Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 37?40,
Suntec, Singapore, 3 August 2009. c?2009 ACL and AFNLP
A NLG-based Application for Walking Directions 
 
 
Michael Roth and Anette Frank 
Department of Computational Linguistics 
Heidelberg University 
69120 Heidelberg, Germany 
{mroth,frank}@cl.uni-heidelberg.de 
 
  
 
Abstract 
This work describes an online application 
that uses Natural Language Generation 
(NLG) methods to generate walking di-
rections in combination with dynamic 2D 
visualisation. We make use of third party 
resources, which provide for a given 
query (geographic) routes and landmarks 
along the way. We present a statistical 
model that can be used for generating 
natural language directions. This model 
is trained on a corpus of walking direc-
tions annotated with POS, grammatical 
information, frame-semantics and mark-
up for temporal structure. 
1 Introduction 
The purpose of route directions is to inform a 
person, who is typically not familiar with his cur-
rent environment, of how to get to a designated 
goal. Generating such directions poses difficul-
ties on various conceptual levels such as the 
planning of the route, the selection of landmarks 
along the way (i.e. easily recognizable buildings 
or structures) and generating the actual instruc-
tions of how to navigate along the route using the 
selected landmarks as reference points. 
As pointed out by Tom & Denis (2003), the 
use of landmarks in route directions allows for 
more effective way-finding than directions rely-
ing solely on street names and distance measures. 
An experiment performed in Tom & Denis? work 
also showed that people tend to use landmarks 
rather than street names when producing route 
directions themselves.  
The application presented here is an early re-
search prototype that takes a data-driven genera-
tion approach, making use of annotated corpora 
collected in a way-finding study. In contrast to 
previously developed NLG systems in this area 
(e.g. Dale et. al, 2002), one of our key features is 
the integration of a number of online resources to 
compute routes and to find salient landmarks. 
The information acquired from these resources 
can then be used to generate natural directions 
that are both easier to memorise and easier to 
follow than directions given by a classic route 
planner or navigation system. 
The remainder of this paper is structured as 
follows: In Section 2 we introduce our system 
and describe the resources and their integration 
in the architecture. Section 3 describes our cor-
pus-based generation approach, with Section 4 
outlining our integration of text generation and 
visualisation. Finally, Section 5 gives a short 
conclusion and discusses future work. 
2 Combining Resources  
The route planner used in our system is provided 
by the Google Maps API1. Given a route com-
puted in Google Maps, our system queries a 
number of online resources to determine land-
marks that are adjacent to this route. At the time 
of writing, these resources are: OpenStreetMaps2 
for public transportation, the Wikipedia WikiPro-
ject Geographical coordinates3 for salient build-
ings, statues and other objects, Google AJAX 
Search API4 for ?yellow pages landmarks? such 
as hotels and restaurants, and Wikimapia 5  for 
squares and other prominent places.  
All of the above mentioned resources can be 
queried for landmarks either by a single GPS 
                                                 
1 http://code.google.com/apis/maps/  
2 http://www.openstreetmap.org 
3 http://en.wikipedia.org/wiki/Wikipedia:WikiProject 
Geographical_coordinates 
4 http://code.google.com/apis/ajaxsearch 
5 http://www.wikimapia.org  
37
coordinate (using the LocalSearch method in 
Google AJAX Search and web tools in Wikipe-
dia) or an area of GPS coordinates (using URL 
based queries in Wikimapia and OpenStreet-
Maps). The following list describes the data for-
mats returned by the respective services and how 
they were integrated: 
? Wikimapia and OpenStreetMaps ? Both 
resources return landmarks in the queried 
area as an XML file that specifies GPS 
coordinates and additional information. 
The XML files are parsed using a Java-
Script implementation of a SAX parser. 
The coordinates and names of landmarks 
are then used to add objects within the 
Google Maps API. 
? Wikipedia ? In order to integrate land-
marks from Wikipedia, we make use of a 
community created tool called search-a-
place 6 , which returns landmarks from 
Wikipedia in a given radius of a GPS 
coordinate. The results are returned in an 
HTML table that is converted to an XML 
file similar to the output of Wikimapia. 
Both the query and the conversion are im-
plemented in a Yahoo! Pipe7 that can be 
accessed in JavaScript via its URL. 
? Google AJAX Search ? The results re-
turned by the Google AJAX Search API 
are JavaScript objects that can be directly 
inserted in the visualisation using the 
Google Maps API.   
3 Using Corpora for Generation 
A data-driven generation approach achieves a 
number of advantages over traditional ap-
proaches for our scenario. First of all, corpus 
data can be used to learn directly how certain 
events are typically expressed in natural lan-
guage, thus avoiding the need of manually speci-
fying linguistic realisations. Secondly, variations 
of discourse structures found in naturally given 
directions can be learned and reproduced to 
avoid monotonous descriptions in the generation 
part. Last but not least, a corpus with good cov-
erage can help us determine the correct selection 
restrictions on verbs and nouns occurring in di-
rections. The price to pay for these advantages is 
                                                 
6 http://toolserver.org/~kolossos/wp-
world/umkreis.php  
7 http://pipes.yahoo.com/pipes/pipe.info?_id=BBI0x8
G73RGbWzKnBR50VA  
the cost of annotation; however we believe that 
this is a reasonable trade-off, in view of the fact 
that a small annotated corpus and reasonable 
generalizations in data modelling will likely 
yield enough information for the intended navi-
gation applications. 
3.1 Data Collection 
We currently use the data set from (Marciniak & 
Strube, 2005) to learn linguistic expressions for 
our generation approach. The data is annotated 
on the following levels: 
? Token and POS level 
? Grammatical level (including annotations 
of main verbs, arguments and connectives) 
? Frame-semantics level (including semantic 
roles and frame annotations in the sense of 
(Fillmore, 1977)) 
? Temporal level (including temporal rela-
tions between discourse units) 
3.2 Our Generation Approach 
At the time of writing, our system only makes 
use of the first three annotation levels. The lexi-
cal selection is inspired by the work of Ratna-
parkhi (2000) with the overall process designed 
as follows: given a certain situation on a route, 
our generation component receives the respective 
frame name and a list of semantic role filling 
landmarks as input (cf. Section 4). The genera-
tion component then determines a list of poten-
tial lexical items to express this frame using the 
relative frequencies of verbs annotated as evok-
ing the particular frame with the respective set of 
semantic roles (examples in Table 1). 
 
SELF_MOTION 
PATH 
17% walk, 13% follow, 10% 
cross, 7% continue, 6% take, ? 
GOAL 
18% get, 18% enter, 9% con-
tinue, 7% head, 5% reach, ? 
SOURCE 14% leave, 14% start, ? 
DIRECTION 
25% continue, 13% make,  
13% walk, 6% go, 3% take, ? 
DISTANCE 15% continue, 8% go, ? 
PATH + GOAL 29% continue, 14% take, ? 
DISTANCE + 
GOAL 
100% walk 
DIRECTION + 
PATH 
23% continue, 23% walk,  
8% take, 6% turn, 6% face, ? 
Table 1: Probabilities of lexical items for the frame 
SELF_MOTION and different frame elements 
38
For frame-evoking elements and each associated 
semantic role-filler in the situation, the gram-
matical knowledge learned from the annotation 
level determines how these parts can be put to-
gether in order to generate a full sentence (cf. 
Table 2). 
 
SELF_MOTION 
walk +  
[building PATH] 
walk ? walk + PP 
PP ? along + NP  
NP ? the + building 
get +  
[building GOAL] 
get ? get + to + NP 
NP ? the + building 
take +  
[left DIRECTION] 
take ? take + NP 
NP ? a + left 
Table 2: Examples of phrase structures for the frame 
SELF_MOTION and different semantic role fillers 
4 Combining Text and Visualisation 
As mentioned in the previous section, our model 
is able to compute single instructions at crucial 
points of a route. At the time of writing the ac-
tual integration of this component consists of a 
set of hardcoded rules that map route segments to 
frames, and landmarks within the segment to role 
fillers of the considered frame. The rules are 
specified as follows: 
? A turning point given by the Google Maps 
API is mapped to the SELF_MOTION frame 
with the actual direction as the semantic 
role direction. If there is a landmark adja-
cent to the turning point, it is added to the 
frame as the role filler of the role source. 
? If a landmark is adjacent or within the 
starting point of the route, it will be 
mapped to the SELF_MOTION frame with 
the landmark filling the semantic role 
source. 
? If a landmark is adjacent or within the 
goal of a route, it will be mapped to the 
SELF_MOTION frame with the landmark 
filling the semantic role goal. 
? If a landmark is adjacent to a route or a 
route segment is within a landmark, the 
respective segment will be mapped to the 
SELF_MOTION frame with the landmark 
filling the semantic role path. 
5 Conclusions and Outlook 
We have presented the technical details of an 
early research prototype that uses NLG methods 
to generate walking directions for routes com-
puted by an online route planner. We outlined 
the advantages of a data-driven generation ap-
proach over traditional rule-based approaches 
and implemented a first-version application, 
which can be used as an initial prototype exten-
sible for further research and development. 
Our next goal in developing this system is to 
enhance the generation component with an inte-
grated model based on machine learning tech-
niques that will also account for discourse level 
phenomena typically found in natural language 
directions. We further intend to replace the cur-
rent hard-coded set of mapping rules with an 
automatically induced mapping that aligns 
physical routes and landmarks with the semantic 
representations. The application is planned to be 
used in web experiments to acquire further data 
for alignment and to study specific effects in the 
generation of walking instructions in a multimo-
dal setting.  
The prototype system described above will be 
made publicly available at the time of publica-
tion. 
Acknowledgements 
This work is supported by the DFG-financed in-
novation fund FRONTIER as part of the Excel-
lence Initiative at Heidelberg University (ZUK 
49/1).  
References 
Dale, R., Geldof, S., & Prost, J.-P. (2002). Generating 
more natural route descriptions. Proceedings of the 
2002 Australasian Natural Language Processing 
Workshop. Canberra, Australia. 
Fillmore, C. (1977). The need for a frame semantics 
in linguistics. Methods in Linguistics , 12, 2-29. 
Marciniak, T., & Strube, M. (2005). Using an 
annotated corpus as a knowledge source for 
language generation. Proceedings of the Workshop 
on Using Corpora for Natural Language 
Generation, (pp. 19-24). Birmingham, UK. 
Ratnaparkhi, A. (2000). Trainable Methods for 
Surface Natural Language Generation. Proceedings 
of the 6th Applied Natural Language Processing 
Conference. Seattle, WA, USA. 
Tom, A., & Denis, M. (2003). Referring to landmark 
or street information in route directions: What 
difference does it make? In W. Kuhn, M. Worboys, 
& S. Timpf (Eds.), Spatial Information Theory (pp. 
384-397). Berlin: Springer. 
39
 Figure 1: Visualised route from Rohrbacher Stra?e 6 to Hauptstrasse 22, Heidelberg. Left: GoogleMaps 
directions; Right: GoogleMaps visualisation enriched with landmarks and directions generated by our system 
(The directions were manually inserted here as they are actually presented step-by-step following the route) 
Script Outline 
Our demonstration is outlined as follows: At first 
we will have a look at the textual outputs of 
standard route planners and discuss at which 
points the respective instructions could be im-
proved in order to be better understandable or 
easier to follow. We will then give an overview 
of different types of landmarks and argue how 
their integration into route directions is a valu-
able step towards better and more natural instruc-
tions.  
Following the motivation of our work, we will 
present different online resources that provide 
landmarks of various sorts. We will look at the 
information provided by these resources, exam-
ine the respective input and output formats, and 
state how the formats are integrated into a com-
mon data representation in order to access the 
information within the presented application. 
Next, we will give a brief overview of the cor-
pus in use and point out which kinds of annota-
tions were available to train the statistical gen-
eration component. We will discuss which other 
annotation levels would be useful in this scenario 
and which disadvantages we see in the current 
corpus. Subsequently we outline our plans to 
acquire further data by collecting directions for 
routes computed via Google Maps, which would 
allow an easier alignment between the instruc-
tions and routes. 
Finally, we conclude the demonstration with a 
presentation of our system in action. During the 
presentation, the audience will be given the pos-
sibility to ask questions and propose routes for 
which we show our system?s computation and 
output (cf. Figure 1).  
System Requirements 
The system is currently developed as a web-
based application that can be viewed with any 
JavaScript supporting browser. A mid-end CPU 
is required to view the dynamic route presenta-
tion given by the application. Depending on the 
presentation mode, we can bring our own laptop 
so that the only requirements to the local organ-
isers would be a stable internet connection (ac-
cess to the resources mentioned in the system 
description is required) and presentation hard-
ware (projector or sufficiently large display). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
40
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 72?76,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Creating an Annotated Corpus for Generating Walking Directions
Stephanie Schuldes
?
Michael Roth
?
?EML Research gGmbH
Heidelberg, Germany
http://www.eml-research.de/nlp
Anette Frank
?
Michael Strube
?
?Department of Computational Linguistics
University of Heidelberg, Germany
http://www.cl.uni-heidelberg.de
Abstract
This work describes first steps towards
building a system that synchronously gen-
erates multimodal (textual and visual)
route directions for pedestrians. We pur-
sue a corpus-based approach for building a
generation model that produces natural in-
structions in multiple languages. We con-
ducted an empirical study to collect ver-
bal route directions, and annotated the ac-
quired texts on different levels. Here we
describe the experimental setting and an
analysis of the collected data.
1 Introduction
Route directions guide a person unfamiliar with
the environment to their designated goal. We plan
to generate route instructions that are similar to
those given by humans by referring to landmarks
and by structuring the route in a way that it is easy
to memorize (Denis, 1997).
We develop a system for synchronously gen-
erating natural language route directions and 3D
scenes of a route. The core of the architecture
is a unified representation providing information
for both verbal and graphical output. The direct
correspondence between linguistic references and
shown objects facilitates the identification of the
visual scene in the real world and the choice of the
correct action while following the route. To cre-
ate a reusable system that is adaptable to different
navigational domains and languages, we use ma-
chine learning techniques to build a statistical gen-
eration model from annotated corpora. We report
on an empirical study to collect human-produced
walking directions to be used for statistical gener-
ation from underlying semantic structures. While
our scenario is ultimately multilingual, here we
give an analysis of the German dataset.
2 Related Work
The task of analyzing and generating cognitively
adequate route instructions has been addressed by
a number of authors (Taylor & Tversky, 1996;
Tappe, 2000; Habel, 2003; Richter, 2008; Vi-
ethen & Dale, 2008; Kelleher & Costello, 2009).
Marciniak & Strube (2005) showed that a system
for generating route directions can be successfully
trained on a small set of 75 route direction texts
(8418 tokens). In their approach directions are
represented in a graph, which encodes informa-
tion on various conceptual levels. While their ap-
proach is restricted to reproducing directions for
the learned graphs, we will generate directions for
a wide range of possible routes. Dale et al (2005)
developed a system that takes GIS data as input
and uses a pipeline architecture to generate verbal
route directions. In contrast to their approach, our
approach will be based on an integrated architec-
ture allowing for more interaction between the dif-
ferent stages of generation. The idea of combining
verbal directions with scenes from a virtual 3D en-
vironment has recently lead to a new framework
for evaluating NLG systems: The Challenge on
Generating Instructions in Virtual Environments
(GIVE) (Byron et al, 2009) is planned to become
a regular event for the NLG community.
3 Corpus Acquisition
For collecting naturally produced route instruc-
tions, we conducted a study with 29 native speak-
ers of German (66% female and 33% male). The
participants in our study were students from var-
ious fields aged between 20 and 34 years. We
designed two different settings: one on-site set-
ting, in which participants walked around in a real
world situation (specifically our university cam-
pus), and one desk-based setting, in which they
interacted with a web application. The former
was further divided into indoor and outdoor routes,
72
32
1
7
6
4
5
Figure 1: Example route from the indoor setting
(first task), leading from a room with photocopiers
(1) across an open space and downstairs (3) to a
students? union room (6), passing an information
board (4) and a coffee machine (5). A lecture room
(2) and a glass wall (7) are visible from the route.
while the latter was restricted to an outdoor sce-
nario. This design enables us to study possible
differences and commonalities between linguistic
realizations obtained for different environments as
well as different presentation modes.
For both scenarios, the task was to give written
directions to a person unfamiliar with the area as
to how to get to the destination the participants just
reached, taking the same route. First, participants
were led along a route to a given destination point
(on-site). Each participant was asked to give di-
rections for two routes inside buildings of the uni-
versity campus (e.g. from an office to a seminar
room, cf. Figure 1), and one outside route (e.g.
from the building entrance to a bus stop).
Second, participants were shown a web appli-
cation that guided them along a route by means of
a 2D animation (desk-based). Subjects were al-
lowed to use all information displayed by the web
application: named places, buildings, street and
bridge names, etc. (cf. Figure 2).
Setting GM CI CO Total
physical routes 9 6 3 18
directions 59 58 28 145
tokens 5353 4119 2674 12146
tokens/dir. (?) 91 71 96
Table 1: Number of routes, directions, and tokens
for the different settings. GM = Google Maps, CI
= Campus Indoor, CO = Campus Outdoor.
4 Corpus Annotation
The acquired texts were processed in several steps.
To ensure that all route directions consist of syn-
tactically and semantically correct sentences, we
Figure 2: Web application used in the second task.
Landmarks were introduced successively via pop-
ups as the animated walker encountered them.
manually corrected spelling mistakes, omissions
resulting in grammatical errors, and removed el-
liptical and unclear directions.
The preprocessed texts were annotated on the
following three levels:
pos lemma ? part-of-speech and lemma
syn dep ? dependency relations
sem frame ? frames and semantic roles
For the pos lemma and syn dep levels, we used
TreeTagger (Schmid, 1997) and XLE (Maxwell
& Kaplan, 1993). The corpus was parsed
with the German ParGram LFG grammar (Forst,
2007). The outputs were corrected manually
by two annotators. On the sem frame level an-
notation was carried out using the annotation
tool SALTO (Burchardt et al, 2006) and fol-
lowing the definiton of the FrameNet frames
SELF MOTION, PERCEPTION, BEING LOCATED
and LOCATIVE RELATION (Baker et al, 1998). In
terms of accuracy for unlabeled/labeled relations,
the annotation agreement was 78.88%/65.17% on
the syn dep level and 79.27%/68.39% for frames
and semantic roles.
5 Data Analysis
5.1 Corpus Statistics
We examined word frequencies with respect to the
experimental settings in order to determine simi-
larities and dissimilarities in lexical choice. Table
2 shows the three most frequent verbs and nouns
found in each corpus part.
The data reveals that the most frequent verbs are
typical among all settings. However, we found a
number of lower-frequency verbs that are rather
73
Top verbs (Campus) GM CI CO
gehen ?to walk? 11% 18% 14%
sein ?to be? 3.9% 8.2% 6.6%
stehen ?to stand? 0.0% 6.3% 5.3%
Top verbs (GM) GM CI CO
folgen ?to follow? 12% 2.9% 2.6%
gehen ?to walk? 11% 18% 14%
abbiegen ?to turn into? 9.0% 3.8% 8.9%
Top nouns (Campus) GM CI CO
T?ur ?door? 0.0% 12% 0.9%
Treppe ?stairs? 0.0% 8.3% 0.0%
Gang ?hallway? 0.0% 6.6% 0.0%
Top nouns (GM) GM CI CO
...stra?e ?. . . Street? 28% 0.0% 2.2%
Richtung ?direction? 3.5% 2.8% 2.6%
...platz ?. . . Square? 3.4% 0.0% 6.1%
Table 2: Relative frequency of the three most com-
mon verbs and nouns in both studies
scenario-specific. In many cases, the occurrence
or absence of a verb can be attributed to a verb?s
selectional restrictions. For example, some of
the verbs describing movements along streets (e.g.
folgen ?to follow?, abbiegen ?to turn into?) do not
occur within the indoor corpus whereas verbs de-
scribing ?3D movements? (e.g. durchqueren ?to
walk through?, hinuntergehen ?to walk down?) are
not mentioned with the Google Maps setting.
The most frequent nouns significantly differ be-
tween the indoor and outdoor settings. This corre-
lation does not come as a surprise, as most of the
mentioned objects cannot be found in all scenar-
ios. On the other hand, nouns that are common
to both indoor and outdoor scenarios can be di-
vided into two categories: Nouns denoting (1) ob-
jects that appear in both scenarios (e.g. Geb?aude
?building?) and (2) abstract concepts typical for
route directions in general, e.g. Richtung ?direc-
tion?, Nummer ?number?, Ziel ?goal?, and Start-
punkt ?starting point?.
5.2 Landmark Alignment
Landmark alignment serves the purpose of de-
tecting objects that are most frequently men-
tioned across directions, and how the same ob-
ject is referred to differently. We created a graph-
based representation of the landmarks mentioned
in each route instruction (single route representa-
tion, SRR) for use in two types of alignment. Fig-
ure 3 shows an example from the indoor study.
First, we created a combined graph for each phys-
ical route by merging the respective SRRs, taking
into account several criteria:
String matching of landmark names;
Semantic similarity using GermaNet (Lemnitzer
& Kunze, 2002), a lexical-semantic network
for German similar to WordNet;
Frequency of references across all directions;
Spatio-temporal proximity of references to the
same object;
Number of landmarks mentioned in a single di-
rection (i.e. length of the SRR).
The combined graphs show that there are strong
correspondences between the directions for the
same route. We also found that, in the campus
settings, there was a small number of frequently
used general objects and a large number of less
frequently used specific objects. This facilitates
merging and shows the importance of the objects
for people?s orientation, and at the same time sup-
ports our claim that other modalities are needed
to disambiguate references during navigation. For
generating informative referential expressions, the
combined graph needs to be refined so that object
properties are represented (Krahmer et al, 2003).
Second, we aligned the SRRs with the physical
route graph. Comparing the landmarks mentioned
in the campus settings revealed that, in 97.8% of
the cases, people adhere to the sequence in which
objects are encountered. Reversed order was only
found in special cases like distant objects.
5.3 Discourse Phenomena
We analyzed the use of anaphora, the temporal or-
der of instructions, and occurrences of prototypi-
cal event chains in the collected texts in order to
identify coherence-inducing elements.
Spatio-temporal adverbials: Most anaphors
mention intermediate goals on the route in order
to refer to the starting point of a new action (e.g.
da/hier ?here?, dort ?there?). This finding goes
hand in hand with the observation that the col-
lected route directions are typically structured in
a linear temporal order (cf. Table 3) as for ex-
ample indicated by the use of adverbs indicat-
ing temporal succession (e.g. jetzt ?now?, dann
?then? and danach ?afterwards?) and conjunctions
(e.g. bis ?until?, wenn ?when?). Interestingly, a re-
versed order can be found in a few cases, where
74
R?cken Raum Kopierer Treppe Treppe Brett Getr?nkeautomat
Treppe
H?rsaal
Treppe Kaffeeautomat T?r Fachschaft
Kopierer Treppe
H?rsaal
Brett S?ule
Glast?r
Getr?nkeautomat T?r
Fachschafts-
raum
Druckerraum Theoretikum 180-Grad-Kurve Fachschaftstafel
Glaswand Glaswand
Kopf
Medizin-
Fachschaft
Kopierzentrum R?cken Treppe Treppe Richtung
Glasfenster Glasfront
Eingang Fachschaft
3
2
1
7
64 5
Figure 3: Each line shows one SRR for the route in Figure 1. Correspondences are indicated by identical
node shapes, black dots substitute non-matched tokens. The bottom graph shows the physical route seen
as sequence of landmarks. Node size reflects the importance of the referred object as conveyed by SRRs.
Adverbs >
t
GM CI CO
dann ?then? 55 43 30
jetzt ?now? 4 7 5
danach ?afterwards? 12 5 3
Adverbs <
t
GM CI CO
vorher ?beforehand? 0 1 0
davor ?before? 1 0 2
Table 3: Frequencies of temporal adverbs indicat-
ing linear (>
t
) and reversed linear order (<
t
)
the following action or situation is not supposed
to take place (e.g. Gehen Sie vorher rechts ?be-
forehand turn right?).
Backward-looking event anaphors and refer-
ences to result states: We also found explicit
references to past events (e.g. Nach dem Durch-
queren ?after traversing?) and result states of
events, e.g. the adverbial phrase unten angekom-
men (here: ?downstairs?) was frequently used fol-
lowing an instruction to ?walk downstairs?.
6 Conclusions and Future Work
The lexical corpus analysis confirms our hypoth-
esis that there are strong commonalities in lexi-
cal choice for directions that persist across sce-
narios and presentation modes, with a small num-
ber of focused differences, and obvious domain-
dependent lexical differences regarding the nature
of objects in the respective scenarios. While our
current corpus data is rather broad, environment-
specific data can be extended quickly by setting up
web studies using 2D and 3D environments.
The alignment of the physical routes and ver-
bal instructions shows a clear tendency that linear
route structure is observed in verbal realization,
with only few exceptions. Since temporal order
is observed by default, temporal annotation can be
restricted to capture exceptional orderings, which
are recoverable from linguistic cues. The study of
discourse coherence effects yielded a number of
elements that will be given special attention in the
surface generation model. We observed a variety
of coherence-inducing elements that are generic
in nature and thus seem well-suited for a corpus-
based generation model. As other languages are
known to exhibit differences in verbal realization
of directions (von Stutterheim et al, 2002), we
have to extend our data collection in order to gen-
erate systematic linguistic variations from a single
underlying semantic structure for all languages.
The linguistic annotation levels of frames and
roles, syntactic dependencies, and basic word cat-
egories have been tested successfully with a sim-
ilar corpus (Roth & Frank, 2009). The next steps
will consist in the alignment of physical routes and
landmarks with semantic representations in an in-
tegrated generation architecture.
Acknowledgements: This work is supported by
the DFG-financed innovation fund FRONTIER as
part of the Excellence Initiative at Heidelberg Uni-
versity (ZUK 49/1) and partially funded by the
Klaus Tschira Foundation, Heidelberg, Germany.
We thank the participants in our study, our anno-
tators Tim Krones and Anna Schmidt, and student
assistants Jonathan Geiger and Carina Silberer.
75
References
Baker, Collin F., Charles J. Fillmore & John B. Lowe (1998).
The Berkeley FrameNet Project. In Proceedings of the
17th International Conference on Computational Linguis-
tics and 36th Annual Meeting of the Association for Com-
putational Linguistics, Montr?eal, Qu?ebec, Canada, 10?14
August 1998, pp. 86?90.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, Andrea
Kowalski & Sebastian Pado (2006). SALTO: A versatile
multi-level annotation tool. In Proceedings of the 5th In-
ternational Conference on Language Resources and Eval-
uation, Genoa, Italy, 22?28 May 2006, pp. 517?520.
Byron, Donna, Alexander Koller, Kristina Striegnitz, Justine
Cassell, Robert Dale, Johanna Moore & Jon Oberlander
(2009). Report on the First NLG Challenge on Generating
Instructions in Virtual Environments (GIVE). In Proceed-
ings of the 12th European Workshop on Natural Language
Generation (ENLG 2009), Athens, Greece, 30-31 March
2009, pp. 165?173.
Dale, Robert, Sabine Geldof & Jean-Philippe Prost (2005).
Using natural language generation in automatic route de-
scription. Journal of Research and Practice in Information
Technology, 37(1):89?106.
Denis, Michel (1997). The description of routes: A cognitive
approach to the production of spatial discourse. Current
Psychology of Cognition, 16:409?458.
Forst, Martin (2007). Filling statistics with linguistics ?
Property design for the disambiguation of German LFG
parses. In Proceedings of the ACL 2007 Workshop on
Deep Linguistic Processing, Prague, Czech Republic, 28
June 2007, pp. 17?24.
Habel, Christopher (2003). Incremental generation of mul-
timodal route instructions. In Reva Freedman & Charles
Callaway (Eds.), Working Papers of the 2003 AAAI Spring
Symposium on Natural Language Generation in Spoken
and Written Dialogue, pp. 44?51. Menlo Park, California:
AAAI Press.
Kelleher, John D. & Fintan J. Costello (2009). Applying com-
putational models of spatial prepositions to visually situ-
ated dialog. Computational Linguistics, 35(2):271?306.
Krahmer, Emiel, Sebastiaan van Erk & Andr?e Verleg (2003).
Graph-based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
Lemnitzer, Lothar & Claudia Kunze (2002). GermaNet ? rep-
resentation, visualization, application. In Proceedings of
the 3rd International Conference on Language Resources
and Evaluation, Las Palmas, Canary Islands, Spain, 29?31
May 2002, pp. 1485?1491.
Marciniak, Tomacz & Michael Strube (2005). Beyond the
pipeline: Discrete optimization in NLP. In Proceedings of
the 9th Conference on Computational Natural Language
Learning, Ann Arbor, Mich., USA, 29?30 June 2005, pp.
136?145.
Maxwell, John T. & Ronald M. Kaplan (1993). The inter-
face between phrasal and functional constraints. Compu-
tational Linguistics, 19(4):571?590.
Richter, Kai-Florian (2008). Context-Specific Route Direc-
tions ? Generation of Cognitively Motivated Wayfinding
Instructions. Amsterdam: IOS Press.
Roth, Michael & Anette Frank (2009). A NLG-based appli-
cation for walking directions. In Companion Volume to
the Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Interna-
tional Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Processing,
Singapore, 2-7 August 2009. To appear.
Schmid, Helmut (1997). Probabilistic Part-of-Speech tagging
using decision trees. In Daniel Jones & Harold Somers
(Eds.), New Methods in Language Processing, pp. 154?
164. London, U.K.: UCL Press.
Tappe, Heike (2000). Perspektivenwahl in Beschreibun-
gen dynamischer und statischer Wegeskizzen. [Choice of
perspective in descriptions of dynamic and static sketch-
maps]. In Christopher Habel & Christiane v. Stutterheim
(Eds.), R?aumliche Konzepte und sprachliche Strukturen,
pp. 69?97. T?ubingen: Niemeyer.
Taylor, Holly & Barbara Tversky (1996). Perspective in
spatial descriptions. Journal of Memory and Language,
35:371?391.
Viethen, Jette & Robert Dale (2008). The use of spatial re-
lations in referring expression generation. In Proceedings
of the Fifth International Natural Language Generation
Conference, Salt Fork OH, USA, 12?14 June 2008, pp.
59?67.
von Stutterheim, Christiane, Ralf N?use & Jorge M. Serra
(2002). Crosslinguistic differences in the conceptuali-
sation of events. In Hilde Hasselg?ard, Stig Johansson,
Bergljot Behrens & Cathrine Fabricius-Hansen (Eds.), In-
formation Structure in a Cross-lingustic Perspective, pp.
179?198. Amsterdam: Rodopi.
76
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 457?465,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic induction of FrameNet lexical units
Marco Pennacchiotti(?), Diego De Cao(?), Roberto Basili(?), Danilo Croce(?), Michael Roth(?)
(?) Computational Linguistics
Saarland University
Saarbru?cken, Germany
{pennacchiotti,mroth}@coli.uni-sb.de
(?) DISP
University of Roma Tor Vergata
Roma, Italy
{decao,basili,croce}@info.uniroma2.it
Abstract
Most attempts to integrate FrameNet in NLP
systems have so far failed because of its lim-
ited coverage. In this paper, we investigate the
applicability of distributional and WordNet-
based models on the task of lexical unit induc-
tion, i.e. the expansion of FrameNet with new
lexical units. Experimental results show that
our distributional and WordNet-based models
achieve good level of accuracy and coverage,
especially when combined.
1 Introduction
Most inference-based NLP tasks require a large
amount of semantic knowledge at the predicate-
argument level. This type of knowledge allows to
identify meaning-preserving transformations, such
as active/passive, verb alternations and nominal-
izations, which are crucial in several linguistic in-
ferences. Recently, the integration of NLP sys-
tems with manually-built resources at the predi-
cate argument-level, such as FrameNet (Baker et
al., 1998) and PropBank (Palmer et al, 2005) has
received growing interest. For example, Shen and
Lapata (2007) show the potential improvement that
FrameNet can bring on the performance of a Ques-
tion Answering (QA) system. Similarly, several
other studies (e.g. (Bar-Haim et al, 2005; Garoufi,
2007)) indicate that frame semantics plays a central
role in Recognizing Textual Entailment (RTE). Un-
fortunately, most attempts to integrate FrameNet or
similar resources in QA and RTE systems have so
far failed, as reviewed respectively in (Shen and La-
pata, 2007) and (Burchardt and Frank, 2006). These
studies indicate limited coverage as the main reason
of insuccess. Indeed, the FrameNet database only
contains 10,000 lexical units (LUs), far less than
the 210,000 entries in WordNet 3.0. Also, frames
are based on more complex information than word
senses, so that their manual development is much
more demanding (Burchardt et al, 2006; Subirats
and Petruck, 2003).
Therefore, there is nowadays a pressing need to
adopt learning approaches to extend the coverage
of the FrameNet lexicon by automatically acquiring
new LUs, a task we call LU induction, as recently
proposed at SemEval-2007 (Baker et al, 2007). Un-
fortunately, research in this area is still somehow
limited and fragmentary. The aim of our study is
to pioneer in this field by proposing two unsuper-
vised models for LU induction, one based on dis-
tributional techniques and one using WordNet as a
support; and a combined model which mixes the
two. The goal is to investigate to what extent distri-
butional and WordNet-based models can be used to
induce frame semantic knowledge in order to safely
extend FrameNet, thus limiting the high costs of
manual annotation.
In Section 2 we introduce the LU induction task
and present related work. In Sections 3, 4 and 5 we
present our distributional, WordNet-based and com-
bined models. Then, in Section 6 we report experi-
mental results and comparative evaluations. Finally,
in Section 7 we draw final conclusions and outline
future work.
2 Task Definition and Related Work
As defined in (Fillmore, 1985), a frame is a con-
ceptual structure modeling a prototypical situation,
evoked in texts through the occurrence of its lex-
ical units. A lexical unit (LU) is a predicate that
linguistically expresses the situation of the frame.
Lexical units of the same frame share semantic ar-
guments. For example the frame KILLING has lex-
ical units such as assassin, assassinate, blood-bath,
fatal, murderer, kill, suicide that share semantic ar-
guments such as KILLER, INSTRUMENT, CAUSE,
VICTIM. Building on this frame-semantic model,
the Berkeley FrameNet project (Baker et al, 1998)
has been developing a frame-semantic lexicon for
457
the core vocabulary of English since 1997. The
current FrameNet release contains 795 frames and
about 10,000 LUs. Part of FrameNet is also a cor-
pus of 135,000 annotated example sentences from
the British National Corpus (BNC).
LU induction is a fairly new task. Formally,
it can be defined as the task of assigning a
generic lexical unit not yet present in the FrameNet
database (hereafter called unknown LU) to the cor-
rect frame(s). As the number of frames is very
large (about 800) the task is intuitively hard to solve.
A further complexity regards multiple assignments.
Lexical units are sometimes ambiguous and can then
be mapped to more than one frame (for example
the word tea could map both to FOOD and SO-
CIAL EVENT). Also, even unambiguous words can
be assigned to more than one frame ? e.g. child maps
to both KINSHIP and PEOPLE BY AGE.
LU induction is relevant to many NLP tasks, such
as the semi-automatic creation of new FrameNets,
and semantic role labelling. LU induction has been
integrated at SemEval-2007 as part of the Frame Se-
mantic Structure Extraction shared task (Baker et
al., 2007), where systems are requested to assign
the correct frame to a given LU, even when the
LU is not yet present in FrameNet. Johansson and
Nugues (2007) approach the task as a machine learn-
ing problem: a Support Vector Machine trained on
existing LUs is applied to assign unknown LUs to
the correct frame, using features derived from the
WordNet hierarchy. Tested on the FrameNet gold
standard, the method achieves an accuracy of 0.78,
at the cost of a low coverage of 31% (i.e. many LUs
are not assigned). Johansson and Nugues (2007)
also experiment with a simple model based on stan-
dard WordNet similarity measures (Pedersen et al,
2004), achieving lower performance. Burchardt and
colleagues (2005) present Detour, a rule-based sys-
tem using words in a WordNet relation with the un-
known LU to find the correct frame. The system
achieves an accuracy of 0.39 and a coverage of 87%.
Unfortunately this algorithm requires the LU to be
previously disambiguated, either by hand or using
contextual information.
In a departure from previous work, our first model
leverages distributional properties to induce LUs, in-
stead of relying on pre-existing lexical resources as
WordNet. This guarantees two main advantages.
First, it can predict a frame for any unknown LU,
while WordNet based approaches can be applied
only to words having a WordNet entry. Second, it
allows to induce LUs in languages for which Word-
Net is not available or has limited coverage. Our
second WordNet-based model uses sense informa-
tion to characterize the frame membership for un-
known LU, by adopting a semantic similarity mea-
sure which is sensitive to all the known LUs of a
frame.
3 Distributional model
The basic idea behind the distributional approach is
to induce new LUs by modelling existing frames and
unknown LUs in a semantic space, where they are
represented as distributional co-occurrence vectors
computed over a corpus.
Semantic spaces are widely used in NLP for rep-
resenting the meaning of words or other lexical en-
tities. They have been successfully applied in sev-
eral tasks, such as information retrieval (Salton et al,
1975) and harvesting thesauri (Lin, 1998). The intu-
ition is that the meaning of a word can be described
by the set of textual contexts in which it appears
(Distributional Hypothesis (Harris, 1964)), and that
words with similar vectors are semantically related.
In our setting, the goal is to find a semantic space
model able to capture the notion of frame ? i.e. the
property of ?being characteristic of a frame?. In
such a model, an unknown LU is induced by first
computing the similarity between its vector and the
vectors of the existing frames, and then assigning the
LU to the frame with the highest similarity.
3.1 Assigning unknown LUs to frames
In our model, a LU l is represented by a vector ~l
whose dimensions represent the set of contexts C
of the semantic space. The value of each dimen-
sion is given by the co-occurrence value of the LU
with a contextual feature c ? C, computed over a
large corpus using an association measure. We ex-
periment with two different association measures:
normalized frequency and pointwise mutual infor-
mation. We approximate these measures by using
Maximum Likelihood Estimation, as follows:
458
F (l, c) =MLE |l, c||?, ?|
MI(l, c) =MLE |l, c||?, ?||?, c||l, ?|
(1)
where |l, c| denotes the co-occurrence counts
of the pair (l, c) in the corpus, |?, c| =?
l?L |l, c|, |l, ?| =
?
c?C |l, c| and finally |?, ?| =?
l?L,c?C |l, c|.
A frame f is modeled by a vector ~f , representing
the distributional profile of the frame in the seman-
tic space. We here assume that a frame can be fully
described by the set of its lexical units F . We imple-
ment this intuition by computing ~f as the weighted
centroid of the set F , as follows:
~f =
?
l?F
wlf ?~l (2)
where wlf is a weighting factor, accounting for
the relevance of a given lexical unit with respect to
the frame, estimated as:
wlf = |l|?
l?F
|l|
(3)
where |l| denotes the counts of l in the corpus.
From a more cognitive perspective, the vector ~f rep-
resents the prototypical lexical unit of the frame.
Given the set of all framesN and an unknown lex-
ical unit ul, we assign ul to the frame fmaxul which
is distributionally most similar ? i.e. we intuitively
map an unknown lexical unit to the frame whose
prototypical lexical unit ~f has the highest similarity
with ~ul:
fmaxul = argmaxf?N simD(~ul, ~f) (4)
In our model, we used the traditional cosine simi-
larity:
simcos(ul, f) =
~ul ? ~f
|~ul| ? |~f |
(5)
3.2 Choosing the space
Different types of contexts C define spaces with dif-
ferent semantic properties. We are here looking for
a space able to capture the properties which charac-
terise a frame. The most relevant of these properties
is that LUs in the same frame tend to be either co-
occurring or substitutional words (e.g. assassin/kill
or assassinate/kill) ? i.e. they are either in paradig-
matic and syntagmatic relation. In an ideal space,
a high similarity value simD would be then given
both to assassinate/kill and to assassin/kill. We ex-
plore three spaces which seem to capture the above
property well:
Word-based space: Contexts are words appear-
ing in a n-window of the lexical unit. Such spaces
model a generic notion of semantic relatedness.
Two LUs close in the space are likely to be re-
lated by some type of generic semantic relation,
either paradigmatic (e.g. synonymy, hyperonymy,
antonymy) or syntagmatic (e.g. meronymy, concep-
tual and phrasal association).1
Syntax-based space: Contexts are syntactic re-
lations (e.g. X-VSubj-man where X is the LU), as
described in (Pado?, 2007). These spaces are good
at modeling semantic similarity. Two LUs close in
the space are likely to be in a paradigmatic relation,
i.e. to be close in a is-a hierarchy (Budanitsky and
Hirst, 2006; Lin, 1998; Pado?, 2007). Indeed, as con-
texts are syntactic relations, targets with the same
part of speech are much closer than targets of differ-
ent types.
Mixed space: In a combination of the two above
spaces, contexts are words connected to the LU by a
dependency path of at most length n. Unlike word-
based spaces, contexts are selected in a more princi-
pled way: only syntactically related words are con-
texts, while other (possibly noisy) material is filtered
out. Unlike syntax-based spaces, the context c does
not explicitly state the type of syntactic relation with
the LU: this usually allows to capture both paradig-
matic and syntagmatic relations.
4 WordNet-based model
In a departure from previous work, our WordNet-
based model does not rely on standard WordNet sim-
ilarity measures (Pedersen et al, 2004), as these
measures can only be applied to pairs of words,
while we here need to capture the meaning of whole
frames, which typically consist of larger sets of LUs.
Our intuition is that senses able to evoke a frame can
be detected via WordNet, by jointly considering the
WordNet synsets activated by all LUs of the frame.
We implement this intuition in a weakly-
supervised model, where each frame f is repre-
sented as a set of specific sub-graphs of the WordNet
1See (Pado?, 2007; Sahlgren, 2006) for an in depth analysis.
459
hyponymy hierarchy. As different parts of speech
have different WordNet hierarchies, we build a sub-
graph for each of them: Snf for nouns, Svf for verbs
and Saf for adjectives.2 These sub-graphs repre-
sent the lexical semantic properties characterizing
the frame. An unknown LU ul of a given part of
speech is assigned to the frame whose correspond-
ing sub-graph is semantically most similar to one of
the senses of ul:
fmaxul = argmaxf?N simWN (ul, f) (6)
where simWN is a WordNet-based similarity
measure. In the following subsections we will de-
scribe how we build sub-graphs and model the sim-
ilarity measure for the different part of speech.
Figure 1 reports an excerpt of the noun sub-
graph for the frame PEOPLE BY AGE, cover-
ing the suitable senses of its nominal LUs
{adult, baby, boy, kid, youngster, youth}. The
relevant senses (e.g. sense 1 of youth out of the 6
potential ones) are generally selected, as they share
the most specific generalizations in WordNet with
the other words.
Nouns. To compute similarity for nouns we adopt
conceptual density (cd) (Agirre and Rigau, 1996),
a semantic similarity model previously applied to
word sense disambiguation tasks.
Given a frame f and its set of nominal lexical
units Fn, the nominal subgraph Snf is built as fol-
lows. All senses of all words in Fn are activated
in WordNet. All hypernyms Hnf of these senses are
then retrieved. Every synset ? ? Hnf is given a cd
score, representing the density of the WordNet sub-
hierarchy rooted at ? in representing the set of nouns
Fn. The intuition behind this model is that the larger
the number of LUs in Fn that are generalized by ? is,
the better it captures the lexical semantics intended
by the frame f . Broader generalizations are penal-
ized as they give rise to bigger hierarchies, not well
correlated with the full set of targets Fn.
To build the final sub-graph Snf , we apply the
greedy algorithm proposed by Basili and colleagues
(2004). It first computes the set of WordNet synsets
that generalize at least two LUs in Fn, and then se-
lects the subset of most dense ones Snf ? Hnf that
2Our WordNet model does not cover the limited number of
LUs which are not nouns, verbs or adjectives.
cover Fn. If a LU has no common hypernym with
other members of Fn, it is not represented in Snf , and
its similarity is set to 0 . Snf disambiguates words in
Fn as only the lexical senses with at least one hyper-
nym in Snf are considered.
Figure 1 shows the nominal sub-graph automati-
cally derived using conceptual density for the frame
PEOPLE BY AGE. The word boy is successfully dis-
ambiguated, as its only hypernym in the sub-graph
refers to its third sense (a male human offspring)
which correctly maps to the given frame. Notice
that this model departs from the first sense heuris-
tics largely successful in word sense disambigua-
tion: most frames in fact are characterized by non
predominant senses. The only questionable disam-
biguation is for the word adult: the wrong sense
(adult mammal) is selected. However, even in these
cases, the cd values are very low (about 10?4), so
that they do not impact much on the quality of the
resulting inference.
Figure 1: The noun sub-graph for the frame PEO-
PLE BY AGE as evoked by a subset of the words. Sense
numbers #n refers to WordNet 2.0.
Using this model, LU induction is performed as
follows. Given an unknown lexical unit ul, for each
frame f ? N we first build the sub-graph Snf from
the set Fn ? {ul}. We then compute simWN (f, ul)
as the maximal cd of any synset ? ? Snf that gener-
alizes one of the lexical senses of ul. In the example
baby would receive a score of 0.117 according to its
first sense in WordNet 2.0 (?baby,babe,infant?). In
a final step, we assign the LU to the most similar
frame, according to Eq. 6
Verbs and Adjectives. As the conceptual density
algorithm can be used only for nouns, we apply dif-
ferent similarity measures for verbs and adjectives.
460
For verbs we exploit the co-hyponymy relation:
the sub-graph Svf is given by all hyponyms of all
verbs Fv in the frame f . Similarity simWN (f, ul)
is computed as follows:
simWN (ul, f) =
?
???
???
1 iff ?K ? F such that
|K| > ? AND
?l ? K, l is a co-hyponym of ul
? otherwise
(7)
As for adjectives, WordNet does not provide a hy-
ponymy hierarchy. We then compute similarity sim-
ply on the basis of the synonymy relation, as fol-
lows:
simWN (ul, f) =
?
?
?
1 iff ?l ? F such that
l is a synonym of ul
? otherwise
(8)
5 Combined model
The methods presented so far use two independent
information sources to induce LUs: distributional
similarity simD and WordNet similarity simWN .
We also build a joint model, leveraging both ap-
proaches: we expect the combination of different
information to raise the overall performance. We
here choose to combine the two approaches using a
simple back-off model, that uses the WordNet-based
model as a default and backs-off to the distributional
one when no frame is proposed by the former. The
intuition is that WordNet should guarantee the high-
est precision in the assignment, while distributional
similarity should recover cases of low coverage.
6 Experiments
In this section we present a comparative evaluation
of our models on the task of inducing LUs, in a
leave-one-out setting over a reference gold standard.
6.1 Experimental Setup
Our gold standard is the FrameNet 1.3 database,
containing 795 frames and a set L of 7,522 unique
LUs (in all there are 10,196 LUs possibly assigned
to more than one frame). Given a lexical unit l ? L,
we simulate the induction task by executing a leave-
one-out procedure, similarly to Burchardt and col-
leagues (2005). First, we remove l from all its origi-
nal frames. Then, we ask our models to reassign it to
the most similar frame(s) f , according to the simi-
larity measure3. We repeat this procedure for all lex-
ical units. Though our experiment is not completely
realistic (we test over LUs already in FrameNet), it
has the advantage of a reliable gold standard pro-
duced by expert annotators. A second, more re-
alistic, small-scale experiment is described in Sec-
tion 6.2.
We compute accuracy as the fraction of LUs in L
that are correctly re-assigned to the original frame.
Accuracy is computed at different levels k: a LU l is
correctly assigned if its gold standard frame appears
among the best-k frames f ranked by the model us-
ing the sim(l, f) measure. As LUs can have more
than one correct frame, we deem as correct an as-
signment for which at least one of the correct frames
is among the best-k.
We also measure coverage, intended as the per-
centage of LUs that have been assigned to at least
one frame by the model. Notice that when no
sense preference can be found above the threshold ?,
the WordNet-based model cannot predict any frame,
thus decreasing coverage.
We present results for the following models and
parametrizations (further parametrizations have re-
vealed comparable performance).
Dist-word : the word-based space described in
Section 3. Contextual features correspond to the
set of the 4,000 most frequent words in the BNC.4
The association measure between LUs and contexts
is the pointwise mutual information. Valid contexts
for LUs are fixed to a 20-window.
Dist-syntax : the syntax-based space described
in Section 3. Context features are the 10,000 most
frequent syntactic relations in the BNC5. As associ-
ation measure we apply log-likelihood ratio (Dun-
ning, 1993) to normalized frequency. Syntactic rela-
tions are extracted using the Minipar parser.
Dist-mixed : the mixed space described in Sec-
3In the distributional model, we recompute the centroids for
each frame f in which the LU appeared, applying Eq. 2 to the
set F ? {l}.
4We didn?t use the FrameNet corpus directly, as it is too
small to obtain reliable statistics.
5Specifically, we use the minimum context selection func-
tion and the plain path value function described in Pado (2007).
461
tion 3. As for the Dist-word model, contextual fea-
tures are 4,000 and pointwise mutual information is
the association measure. The maximal dependency
path length for selecting each context word is 3.
Syntactic relations are extracted using Minipar.
WNet-full : the WordNet based model described
in Section 4.
WNet-bsense : this model is computed as WNet-
full but using only the most frequent sense for each
LU as defined in WordNet.
Combined : the combined method presented in
Section 5. Specifically, it uses WNet-full as a default
and Dist-word as back-off.
Baseline-rnd : a baseline model, randomly as-
signing LUs to frames.
Baseline-mostfreq : a model predicting as best-k
frames the most likely ones in FrameNet ? i.e. those
containing the highest number of LUs.
6.2 Experimental Results
Table 1 reports accuracy and coverage results for the
different models, considering only 6792 LUs with
frequency higher than 5 in the BNC, and frames
with more than 2 lexical units (to allow better gen-
eralizations in all models). Results show that all our
models largely outperform both baselines, achieving
a good level of accuracy and high coverage. In
particular, accuracy for the best-10 frames is high
enoungh to support tasks such as the semi-automatic
creation of new FrameNets. This claim is supported
by a further task-driven experiment, in which we
asked 3 annotators to assign 60 unknown LUs (from
the Detour system log) to frames, with and without
the support of the Dist-word model?s predictions as
suggestions6. We verified that our model guarantee
an annotation speed-up of 25% ? i.e. in average an
annotator saves 25% of annotation time by using
the system?s suggestions.
Distributional vs. WordNet-based models.
WordNet-based models are significantly better than
distributional ones, for several reasons. First, distri-
butional models acquire information only from the
contexts in the corpus. As we do not use a FrameNet
annotated corpus, there is no guarantee that the us-
age of a LU in the texts reflects exactly the semantic
6For this purpose, the dataset is evenly split in two parts.
properties of the LU in FrameNet. In the extreme
cases of polysemous LUs, it may happen that the
textual contexts refer to senses which are not ac-
counted for in FrameNet. In our study, we explicitly
ignore the issue of polisemy, which is a notoriously
hard task to solve in semantics spaces (see (Schu?tze,
1998)), as the occurrences of different word senses
need to be clustered separately. We will approach
the problem in future work. The WordNet-based
model suffers from the problem of polisemy to a
much lesser extent, as all senses are explicitly rep-
resented and separated in WordNet, including those
related to the FrameNet gold standard.
A second issue regards data sparseness. The vec-
torial representation of LUs with few occurrences in
the corpus is likely to be semantically incomplete,
as not enough statistical evidence is available. Par-
ticularly skewed distributions can be found when
some frames are very rarely represented in the cor-
pus. A more in-depth descussion on these two issues
is given later in this section.
Regarding the WordNet-based models, WNet-full
in most cases outperforms WNet-bsense. The first
sense heuristic does not seem to be as effective as
in other tasks, such as Word Sense Disambigua-
tion. Although sense preferences (or predominance)
across two general purpose resources, such as Word-
Net and FrameNet, should be a useful hint, the con-
ceptual density algorithm seems to produce better
distributions (i.e. higher accuracy), especially when
several solutions are considered. Indeed, for many
LUs the first WordNet sense is not the one repre-
sented in the FrameNet database.
As for distributional models, results show that the
Dist-word model performs best. In general, syntac-
tic relations (Dist-syntax model) do not help to cap-
ture frame semantic properties better than a simple
window-based approach. This seems to indicate that
LUs in a same frame are related both by paradig-
matic and syntagmatic relations, in accordance to
the definition given in Section 3.2 ? i.e. they are
mostly semantically related, but not similar.
Coverage. Distributional models show a coverage
15% higher than WordNet-based ones. Indeed, as far
as corpus evidence is available (i.e. the unknown LU
appears in the corpus), distributional methods are al-
ways able to predict a frame. WordNet-based mod-
462
MODEL B-1 B-2 B-3 B-4 B-5 B-6 B-7 B-8 B-9 B-10 COVERAGE
Dist-word 0.27 0.36 0.42 0.46 0.49 0.51 0.53 0.55 0.56 0.57 95%
Dist-syntax 0.22 0.29 0.34 0.38 0.41 0.44 0.46 0.48 0.50 0.51 95%
Dist-mixed 0.25 0.35 0.40 0.44 0.47 0.49 0.51 0.53 0.54 0.56 95%
WNet-full 0.47 0.59 0.65 0.69 0.72 0.73 0.75 0.76 0.77 0.78 80%
WNet-bsense 0.52 0.61 0.64 0.66 0.67 0.68 0.69 0.69 0.70 0.70 72%
Combined 0.43 0.54 0.60 0.64 0.66 0.68 0.70 0.71 0.72 0.73 95%
Baseline-rnd 0.02 0.03 0.05 0.06 0.08 0.10 0.11 0.12 0.14 0.15
Baseline-mostfreq 0.02 0.05 0.07 0.08 0.10 0.11 0.13 0.14 0.15 0.17
Table 1: Accuracy and coverage of different models on best-k ranking with frequency threshold 5 and frame threshold
2
els cannot make predictions in two specific cases.
First, when the LU is not present in WordNet. Sec-
ond, when the function simWN does not has suffi-
cient relational information to find a similar frame.
This second factor is particularly evident for adjec-
tives, as Eq. 8 assigns a frame only when a synonym
of the unknown LU is found. It is then not surpris-
ing that 68% of the missed assignment are indeed
adjectives.
Results for the Combined model suggest that
the integration of distributional and WordNet-based
methods can offer a viable solution to the cover-
age problem, as it achieves an accuracy comparable
to the pure WordNet approaches, while keeping the
coverage high.
Figure 2: Dist-word model accuracy at different LU fre-
quency cuts.
Data Sparseness. A major issue when using dis-
tributional approaches is that words with low fre-
quency tend to have a very sparse non-meaningful
representation in the vector space. This highly im-
pacts on the accuracy of the models. To measure
the impact of data sparseness, we computed the ac-
curacy at different frequency cuts ? i.e. we exclude
LUs below a given frequency threshold from cen-
troid computation and evaluation. Figure 2 reports
the results for best-10 assignment at different cuts,
for the Dist-word model. As expected, accuracy im-
proves by excluding infrequent LUs. Only at a fre-
quency cut of 200 performance becomes stable, as
statistical evidence is enough for a reliable predic-
tion. Yet, in a real setting the improvement in accu-
racy implies a lower coverage, as the system would
not classify LUs below the threshold. For example,
by discarding LUs occurring less than 200 times in
the corpus, we obtain a +0.12 improvement in accu-
racy, but the coverage decreases to 57%. However,
uncovered LUs are also the most rare ones and their
relevance in an application may be negligible.
Lexical Semantics, Ambiguity and Plausible As-
signments. The overall accuracies achieved by
our methods are ?pessimistic?, in the sense that they
should be intended as lower-bounds. Indeed, a qual-
itative analysis of erroneous predictions reveals that
in many cases the frame assignments produced by
the models are semantically plausible, even if they
are considered incorrect in the leave-one-out test.
Consider for example the LU guerrilla, assigned in
FrameNet to the frame PEOPLE BY VOCATION. Our
mixed model proposes as two most similar frames
MILITARY and TERRORISM, which could still be
considered plausible assignment. The same holds
for the LU caravan, for which the most similar
frame is VEHICLE, while in FrameNet the LU is as-
signed only to the frame BUILDINGS. These cases
are due to the low FrameNet coverage, i.e LUs are
not fully annotated and they appear only in a subset
of their potential frames. The real accuracy of our
463
models is therefore expected to be higher.
To explore the issue, we carried out a qualita-
tive analysis of 5 words (i.e. abandon.v, accuse.v,
body.n, charge.v and partner.n). For each of them,
we randomly picked 60 sentences from the BNC
corpus, and asked two human annotators to assign
to the correct frame the occurrence of the word in
the given sentence. For 2 out of 5 words, no frame
could be found for most of the sentences, suggesting
that the most frequent frames for these words were
missing from FrameNet7. We can then conclude that
100% accuracy cannot be considered as the upper-
bound of our experiment, as word usage in texts is
not well reflected in the FrameNet modelling.
Further experiments. We also tested our models
on a realistic gold-standard set of 24 unknown LUs
extracted from the SemEval-2007 corpus (Baker et
al., 2007). These are words not present in FrameNet
1.3 which have been assigned by human annotators
to an existing frame8. WNet-full achieves an accu-
racy of 0.25 for best-1 and 0.69 for best-10, with a
coverage of 67%. A qualitative analysis showed that
the lower performance wrt to our main experiment is
due to higher ambiguity of the LUs (e.g. we assign
tea to SOCIAL EVENT instead of FOOD).
Comparison to other approaches. We compare
our models to the system presented by Johans-
son and Nugues (2007) and Burchardt and col-
leagues (2005). Johansson and Nugues (2007) eval-
uate their machine learning system using 7,000
unique LUs to train the Support Vector Machine, and
the remaining LUs as test. They measure accuracy at
different coverage levels. At 80% coverage accuracy
is about 0.42, 10 points below our best WordNet-
based system. At 90% coverage, the system shows
an accuracy below 0.10 and is significantly out-
performed by both our distributional and combined
methods. These results confirm that WordNet-based
approaches, while being highly accurate wrt dis-
tributional ones, present strong weaknesses as far
as coverage is concerned. Furthermore, Johansson
and Nugues (2007) show that their machine learn-
7Note that the need of new frames to account for seman-
tic phenomena in free texts has been also demonstrated by the
SemEval-2007 competition.
8The set does not contain 4 LUs which have no frame in
FrameNet.
ing approach outperforms a simple approach based
on WordNet similarity: thus, our results indirectly
prove that our WordNet-based method is more ef-
fective than the application of the similarity measure
presented in (Pedersen et al, 2004).
We also compare our results to those reported
by Burchardt and colleagues (2005) for Detour.
Though the experimental setting is slightly different
(LU assignment is done at the text-level), they use
the same gold standard and leave-one-out technique,
reporting a best-1 accuracy of 0.38 and a coverage
of 87%. Our WordNet-based models significantly
outperform Detour on best-1 accuracy, at the cost of
lower coverage. Yet,our combined model is signifi-
cantly better both on accuracy (+5%) and coverage
(+8%). Also, in most cases Detour cannot predict
more than one frame (best-1), while our accuracies
can be improved by relaxing to any best-k level.
7 Conclusions
In this paper we presented an original approach for
FrameNet LU induction. Results show that mod-
els combining distributional and WordNet informa-
tion offer the most viable solution to model the no-
tion of frame, as they allow to achieve a reasonable
trade-off between accuracy and coverage. We also
showed that in contrast to previous work, simple se-
mantic spaces are more helpful than complex syn-
tactic ones. Results are accurate enough to support
the creation and the development of new FrameNets.
As future work, we will evaluate new types of
spaces (e.g. dimensionality reduction methods) to
improve the generalization capabilities of the space
models. We will also address the data sparseness is-
sue, by testing smoothing techniques to better model
low frequency LUs. Finally, we will implement
the presented models in a complex architecture for
semi-supervised FrameNets development, both for
specializing the existing English FrameNet in spe-
cific domains, and for creating new FrameNets in
other languages.
Acknowledgements
This work has partly been funded by the German Re-
search Foundation DFG (grant PI 154/9-3). Thanks
to Richard Johansson and Aljoscha Burchardt for
providing the data of their systems.
464
References
E. Agirre and G. Rigau. 1996. Word Sense Disam-
biguation using Conceptual Density. In Proceedings
of COLING-96, Copenhagen, Denmark.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval-2007 Task 19: Frame Semantic Structure
Extraction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99?104, Prague, Czech Republic, June.
Roy Bar-Haim, Idan Szpektor, and Oren Glickman.
2005. Definition and Analysis of Intermediate Entail-
ment Levels. In ACL-05 Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, Michigan.
R. Basili, M. Cammisa, and F.M. Zanzotto. 2004. A
semantic similarity measure for unsupervised semantic
disambiguation. In Proceedings of LREC-04, Lisbon,
Portugal.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of PASCAL RTE2 Workshop.
Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005.
A WordNet Detour to FrameNet. In Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8 of Computer Studies in Language
and Speech. Peter Lang, Frankfurt/Main.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC, Genova,
Italy.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 18(1):61?74.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222?254.
K. Garoufi. 2007. Towards a better understanding of
applied textual entailment: Annotation and evaluation
of the rte-2 dataset. M.Sc. thesis, saarland university.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Phi-
losophy of Linguistics, New York. Oxford University
Press.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic Re-
sources for Scandinavian and Baltic Languages, at
NODALIDA, Tartu, Estonia, May 24.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Sebastian Pado?. 2007. Cross-Lingual Annotation Projec-
tion Models for Role-Semantic Information. Saarland
University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1).
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Magnus Sahlgren. 2006. The Word-Space Model. De-
partment of Linguistics, Stockholm University.
G. Salton, A. Wong, and C. Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18:613620.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?124.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of EMNLP-CoNLL, pages 12?21, Prague.
C. Subirats and M. Petruck. 2003. Surprise! Spanish
FrameNet! In Proceedings of the Workshop on Frame
Semantics at the XVII. International Congress of Lin-
guists, Prague.
465
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 958?966,
Beijing, August 2010
Computing EM-based Alignments of Routes and Route Directions as a
Basis for Natural Language Generation
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Route directions are natural language
(NL) statements that specify, for a given
navigational task and an automatically
computed route representation, a se-
quence of actions to be followed by the
user to reach his or her goal. A corpus-
based approach to generate route direc-
tions involves (i) the selection of elements
along the route that need to be mentioned,
and (ii) the induction of a mapping from
route elements to linguistic structures that
can be used as a basis for NL generation.
This paper presents an Expectation-Maxi-
mization (EM) based algorithm that aligns
geographical route representations with
semantically annotated NL directions, as
a basis for the above tasks. We formu-
late one basic and two extended models,
the latter capturing special properties of
the route direction task. Although our
current data set is small, both extended
models achieve better results than the sim-
ple model and a random baseline. The
best results are achieved by a combination
of both extensions, which outperform the
random baseline and the simple model by
more than an order of magnitude.
1 Introduction
The purpose of route directions is to inform a per-
son, who is typically not familiar with his cur-
rent environment, of how to get to a designated
goal. Generating such directions poses difficul-
ties on various conceptual levels such as planning
the route, selecting landmarks (i.e., recognizable
buildings or structures) and splitting the task into
appropriate single instructions of how to navigate
along the route using the selected landmarks as
reference points.
Previously developed natural language genera-
tion (NLG) systems make use of simple heuristics
for the task of content selection for route direc-
tions (Dale et al, 2005; Roth and Frank, 2009).
In our work, we aim for a corpus-based approach
that can be flexibly modeled after natural, human-
produced directions for varying subtasks (e.g., in-
door vs. outdoor navigation), and that facilitates
multilingual extensions. By employing salient
landmarks and allowing for variation in NL real-
ization, such a system is expected to generate nat-
ural sounding directions that are easier to memo-
rize and easier to follow than directions given by
a classical route planner or navigation system.
NLG for route directions crucially differs from
other generation tasks such as document summa-
rization (Mani, 2001) in that the selection and or-
dering of input structures for language generation
is heavily situation-dependent, i.e., dependent on
the specific properties of a given route to be fol-
lowed.
In line with a corpus-based NLG approach, we
propose to automatically align geographical route
representations as produced by a route planner
with an annotated corpus of NL directions given
by humans for the respective routes. The induced
alignments will (i) serve to identify which ele-
ments of a route to select for verbalization, and (ii)
deliver correspondences between route segments
and linguistic input structures that can be used as
a basis for statistical NL generation. We investi-
958
gate a minimally supervised method for inducing
such alignments to ensure maximal flexibility for
adaptations to different scenarios.
The remainder of this paper is structured as fol-
lows: In Section 2 we discuss related work. Sec-
tion 3 introduces the task, and the representation
formats and resources we use. Section 4 intro-
duces a basic Expectation-Maximization model
and two extensions for the alignment task. Sec-
tion 5 outlines the experiments and presents the
evaluation results. In Section 6 we conclude and
discuss future work.
2 Related Work
Various aspects of route directions have been sub-
ject of research in computational linguistics, rang-
ing from instructional dialogues in MapTask (An-
derson et al, 1991) to recent work on learning
to follow route directions (Vogel and Jurafsky,
2010). However, little work has been done on
generating NL directions based on data from Geo-
graphical Information Systems (Dale et al, 2005;
Roth and Frank, 2009).
NLG systems are typically realized as pipeline
architectures (Reiter and Dale, 2000). As a first
step, they compute a set of messages that rep-
resent the information to be conveyed to a user,
given a specific communicative task (Content Se-
lection). Selecting appropriate content for a task
can be defined heuristically, by manually crafted
rules or by learning content selection rules auto-
matically from corpus data. Previous work by
Dale et al (2005) and Roth and Frank (2009)
on generating NL directions used hand-crafted
heuristics. Duboue and McKeown (2003) were
the first to model content selection as a machine
learning task, in which selection rules are induced
from pairs of human-written text and associated
sets of database entries. They induce baseline se-
lection rules from exact matches of NL expres-
sions with database entries; in addition, class-
based rules are computed by matching database
entry types against NL expressions, using statis-
tical co-occurrence clusters. Barzilay and Lapata
(2005) incorporate the interplay between multiple
events and entities when learning content selec-
tion rules using a special link function.
Recent work by Liang et al (2009) focuses on
modeling grounded language, by aligning real-
world representations with NL text that references
corresponding world states. They show how a
generative model can be used to segment text into
utterances and to identify relevant facts with min-
imal supervision. Both tasks are handled jointly
in a unified framework by training a hierarchical
semi-Markov model on pairs of text and world
states, thereby modeling sequencing effects in the
presentation of facts. While their work is not pri-
marily concerned with NLG, the learned corre-
spondences and their probabilities could be ap-
plied to induce content selection rules and lin-
guistic mappings in a NLG task. The approach is
shown to be effective in scenarios typical for NLG
settings (weather forecasts, RoboCup sportscast-
ing, NFL recaps) that differ in the amount of avail-
able data, length of textual descriptions, and den-
sity of alignments.
In the following, we will adapt ideas from their
EM-based approach to align (segments of) route
representations and NL route directions in a min-
imally supervised manner. We will investigate in-
creasingly refined models that are tailored to the
nature of our task and underlying representations.
In particular, we extend their approach by exploit-
ing semantic markup in the NL direction corpus.
3 Aligning Routes and Directions
In this work we explore the possibility of using
an implementation of the EM algorithm (Demp-
ster et al, 1977) to learn correspondences between
(segments of) the geographical representation of
a route and linguistic instructions of how to fol-
low this route in order to arrive at a designated
goal. We are specifically interested in identifying
which parts of a route are realized in natural lan-
guage and which kinds of semantic constructions
are used to express them.
As a data source for inducing such correspon-
dences we use a parallel corpus of route repre-
sentations and corresponding route directions that
were collected in a controlled experiment for nav-
igation in an urban street network (cf. Schuldes
et al (2009)). For the alignment task, the routes
were compiled to a specification format that has
been realized in an internal version of an online
route planner. Figure 1 displays the route rep-
959
Figure 1: A (partial) route representation of the route segment displayed on the right.
resentation for a small route segment (a junction
connecting ?Hauptstra?e? and ?Leyergasse?). The
corresponding part of a NL route direction is dis-
played in Figure 2. The route representation and
the NL direction share some common concepts:
For example, both contain references to a land-
mark called ?Sudpfanne? (marked as [1]) and a
street named ?Leyergasse? (marked as [2]). Using
pairs of route representations and directions, we
aim to automatically induce alignments between
such correspondences. In the following we de-
scribe our data in more detail.
3.1 Route Representation Format
The route representation format we use (illus-
trated in Figure 1) is an extended version of
the OpenGIS Location Service (OpenLS) Imple-
mentation Standards, a set of XML-based rep-
resentations specified by the Open Geospatial
Consortium1. Previous approaches on extend-
ing the latter with landmarks in an interopera-
1http://www.opengeospatial.org/standards/is
ble way have been presented by Neis and Zipf
(2008). The representation format of our data
has been developed in close collaboration with re-
searchers from Geoinformatics at Heidelberg Uni-
versity2 and adopts ideas previously proposed in
the Cognitive OpenLS specification by Hansen et
al. (2006). The resulting specification will be im-
plemented in an extended (internal) version of the
online route planner OpenRouteService.org.
Our work revolves around two kinds of ele-
ments in this format: so-called maneuvers, i.e., el-
ements that describe a decision point including the
required action and the following route segment,
and landmarks that occur along the route. For the
alignment task we focus on the following types of
attributes that are part of the XML specification,
specified here as Attribute (Element):
directionOfTurn (Maneuver) ? the direction of
movement for the current maneuver, i.e.,
?left?, ?right? or ?straight?
2Chair of GIScience, Alexander Zipf,
http://www.geog.uni-heidelberg.de/lehrstuehle/gis/
960
Figure 2: Directions for the route segment displayed in Figure 1 annotated with frame-semantic markup
and alignment information. The directions translate to ?You start walking from Hauptstra?e towards
Gaststa?tte Sudpfanne, then you turn right onto Leyergasse?
junctionType (Maneuver) ? the type of junction
at the current maneuver, e.g., ?intersection?,
?crossing?
name (JunctionCategory) ? the name of the
junction at the current maneuver, e.g.,
?Hauptstra?e/Leyergasse?
name (NextSegment) ? the name of the street of
the next route segment, e.g., ?Hauptstra?e?
streetName (RouteBranch) ? the street name of
a branch along which the route continues,
e.g., ?Leyergasse?
streetName (NoRouteBranch) ? the street name
of a branch that is not part of the route, e.g.,
?Kisselgasse?
name (Landmark) ? the name of a landmark,
e.g., ?Hotel Sudpfanne?
spatialRelation (UsedLandmark) ? the spatial
relation between a landmark and the current
maneuver, e.g., ?left?, ?right?, ?before?
3.2 A Parallel Corpus of Route Directions
The corpus of route directions used in this work
is a subset of the data collected by Schuldes et al
(2009) in a desk-based experiment. To elicit NL
route directions, subjects were shown a web appli-
cation that guided them along a route by means of
a 2D animation. Subsequently they had to write
NL route directions in German for the shown
routes. The subjects were allowed to use all infor-
mation displayed by the web application: named
places, buildings, bridges and street names, etc.
The resulting directions were POS-tagged with
TreeTagger (Schmid, 1997), dependency-parsed
with XLE (Maxwell and Kaplan, 1993), and man-
ually revised. Additionally, we annotated frame-
semantic markup (Fillmore et al, 2003) and gold
standard alignments to the route representation us-
ing the SALTO annotation tool (Burchardt et al,
2006).
Frame semantic markup. The texts are an-
notated with an inventory of 4 frames relevant
for directions (SELF MOTION, PERCEPTION, BE-
ING LOCATED, LOCATIVE RELATION), with se-
mantic roles (frame elements) such as DIREC-
TION, GOAL, PATH, LOCATION. Figure 2 il-
lustrates a typical example for the use of the
SELF MOTION frame, once with the elements
SOURCE and DIRECTION, and once with the el-
ements DIRECTION and GOAL. Our alignment
model uses the frame semantic annotation as
structuring information.
Gold standard alignments. For evaluation we
constructed gold alignments. We asked two an-
notators to align text parts with corresponding
attributes in the respective route representation3.
The information about corresponding attributes
was added to a single word by manually insert-
3The alignments have not been double annotated, hence
no measure for inter-annotator agreement can be provided.
961
#S #W #FE #aligned FE
avg. per direction 8 98 28 14 (50%)
overall 412 5298 1519 750
Table 1: Corpus statistics: number of sentences
(S), words (W), frame elements (FE) and align-
ments.
#attributes #aligned attr.
avg. per route 115 14 (12%)
overall 921
Table 2: Corpus statistics: total number and per-
centage of relevant attribute alignments.
ing XPATH expressions that unambiguously refer
to the aligned attribute in the route representation
format. For learning the alignment model, the an-
notations were spread to all words in the span of
the respective frame element.
Corpus statistics. We made use of a corpus of
54 NL directions collected for 8 routes in an urban
street network. Tables 1 and 2 give some statis-
tics about the number of words (W) and frame
elements (FE) in the parallel corpus. Comparing
the total number of relevant attributes (as listed in
Section 3.1) and attributes annotated in the gold
alignments (aligned attr.) we note that only 12%
are actually mentioned in NL directions. Thus it
is necessary to select the most salient attributes to
avoid the generation of overly redundant text.
4 Alignment Model
For the induction of alignments between (parts of)
route structures and semantic representations, we
adopt ideas from the models presented in Liang et
al. (2009) (cf. Section 2).
We start from a basic frame alignment model.
It specifies a conditional probability distribution
p(f |a) for the alignment to a frame element f of
type ft (e.g., source, goal, direction) in the frame-
semantic annotation layer given an attribute a of
type at (e.g., streetName, directionOfTurn) in the
route representation format. Note that this model
does not take into account the actual value av of
the attribute a nor the words that are annotated as
part of f . We assume that the frame annotation
represents a reliable segmentation for this align-
ment. This allows us to omit modeling segmenta-
tion explicitly.
As extensions to the basic frame alignment
model, we specify two further models that cap-
ture properties that are specific to the task of di-
rection alignment. As route directions are typi-
cally presented in a linear order with respect to
the route, we incorporate an additional distance
model ? in our alignment. We further account
for word choice within a frame element as an ad-
ditional factor. The word choice model p(w|a)
will exploit attribute type and value information
in the route representations that are reflected in
word choice in the linguistic instructions. Both
extensions are inspired by and share similarities
with models that have been successfully applied
in work on text alignment for the task of machine
translation (Vogel et al, 1996; Tiedemann, 2003).
Our full model is a distribution over frame el-
ements f and words w that factorizes the three
above mentioned parts under the assumption of
independence between each component and each
attribute:
p(f, w|a) = p(f |a)?(dist(f, a)) p(w|a) (1)
The individual models are described in more
detail in the following subsections.
4.1 Frame Alignment Model
This basic frame alignment model specifies the
probabilities p(f |a) for aligning an attribute a of
type at (i.e., one of the types listed in Section 3.1)
to a frame element f labeled as type ft. This
alignment model is initialized as a uniform distri-
bution over f and trained using a straight-forward
implementation of the EM algorithm, following
the well-known IBM Model 1 for alignment in
machine translation (Brown et al, 1993). The ex-
pectation step (E-step) computes expected counts
given occurrences of ft and at under the assump-
tion that all alignments are independent 1:1 corre-
spondences:
count(ft, at) =
?
{?f ?,a??|f ?t=ft?a?t=at} p(f
?|a?)
?
{?f ?,y?|f ?t=ft} p(f
?|y)
(2)
The probabilities are re-estimated to maximize
the overall alignment probability by normalizing
962
the estimated counts (M-step):
p(f |a) = count(ft, at)?
x count(xt, at)
(3)
4.2 Distance Model
We hypothesize that the order of route directions
tends to be consistent with the order of maneuvers
encoded by the route representation. We include
this information in our alignment model by defin-
ing a distance measure dist(f, a) between the rel-
ative position of a frame element f in the text and
the relative position of an attribute a in the route
representation. The probabilities are specified in
form of a distance distribution ?(i) over normal-
ized distances i ? [0 : 1] and learned during EM
training. The weights are initialized as a uniform
distribution and re-estimated in each M-step by
normalizing the estimated counts:
?(i) =
?
{?x,y?| dist(x,y)=i} count(x, y)?
{?x,y?} count(x, y)
(4)
4.3 Word Choice Model
We define a word choice model for word us-
age within a frame element. This additional fac-
tor is necessary to distinguish between various
occurrences of the same type of frame element
with different surface realizations. For exam-
ple, assuming that the frame alignment model
correctly aligns directionOfTurn attributes to a
frame element of type DIRECTION, the word
choice model will provide an additional weight
for the alignment between the value of an attribute
(e.g., ?left?) and the corresponding words within
the frame element (e.g., ?links?). Similarly to
the word choice model within fields in (Liang
et al, 2009), our model specifies a distribution
over words given the attribute a. Depending on
whether the attribute is typed for strings or cate-
gorial values, two different distributions are used.
String Attributes. For string attributes, we de-
termine a weighting factor based on the longest
common subsequence ratio (LCSR). The reason
for using this measure is that we want to allow for
spelling variants and the use of synonymous com-
mon nouns in the description of landmarks and
street names (e.g., ?Main St.? vs. ?Main Street?,
?Texas Steakhouse? vs. ?Texas Restaurant?). The
weighting factor pstr(w|a) for an alignment pair
?f, a? is a constant in the E-step and is calculated
as the LCSR of the considered attribute value av
and the content words w = cw(f) in an anno-
tated frame element f divided by the sum over the
LCSR values of all alignment candidates for a:
pstr(w|a) =
LCSR(av, w)?
x LCSR(av, cw(x))
(5)
Categorial Attributes. We define categorial at-
tributes as attributes that can only take a finite
and prescribed set of values. For these we do
not expect to find matching strings in NL direc-
tions as the attribute values are defined indepen-
dently of the language in use (e.g., values for di-
rectionOfTurn are ?left?, ?right? and ?straight?.
However, the directions in our data set are in Ger-
man, thus containing the lexemes ?links?, ?rechts?
und ?geradeaus? instead). As the set of values
{av ? Dat} for a categorial attribute type at is
finite, we can define and train probability distri-
butions over words for each of them during EM
training. The models are initialized as uniform
distributions and are used as a weighting factor
in the E-Step. We re-calculate the parameters of
a distribution pcat(w|a) in each EM iteration by
normalizing the estimated counts during M-step:
pcat(w|a) =
count(av, w)?
x count(av, x)
(6)
5 Experiments and Results
5.1 Setting
We test the performance of different combinations
of these EM-based models on our data, starting
from a simple baseline model (EM), combined
with the distance (EM+dst) and word choice
models (EM+wrd) and finally the full model
(Full). We perform additional experiments to ex-
amine the impact of different corpus sizes and an
alignment threshold (+thld).
EM is a baseline model that consists of a simple
EM implementation for aligning attributes
and frame elements (equation (3)).
EM+dst consists of the simple EM model and the
additional distance factor (equation (4)).
963
Model P (+thld) R (+thld) F1 (+thld)
Random 2.7 (2.7) 3.9 (3.9) 3.2 (3.2)
EM 2.0 (3.6) 2.9 (3.7) 2.34 (3.6)
EM+dst 7.3 (11.6) 10.8 (11.7) 8.7 (11.6)
EM+wrd 26.8 (36.3) 39.5 (35.5) 32.0 (35.9)
Full 28.9 (38.9) 42.5 (37.9) 34.4 (38.4)
Table 3: Precision (P), Recall (R) and F1 measure
results with and without threshold (+thld) on the
alignment task (all numbers in percentages).
EM+wrd consists of the simple EM model with
the word choice model (equations (5) and
(6), respectively).
Full is the full alignment model including dis-
tance and word choice as described in Sec-
tion 4 (cf. equation (1)).
We use the data set described in Section 3. The
predictions made by the different models are eval-
uated against the gold standard alignments (cf. Ta-
bles 1 and 2). We run a total number of 30 iter-
ations4 of EM training on the complete data set
to learn the parameters of the probability distri-
butions. From the set of all possible 1-to-1 align-
ments, we select the most probable alignments ac-
cording to the model in a way that no attribute and
no frame element is aligned twice.
5.2 Results
We measure precision as the number of predicted
alignments also annotated in the gold standard di-
vided by the total number of alignments generated
by our model. Recall is measured as the number
of correctly predicted alignments divided by the
total number of alignment annotations. As base-
lines we consider a random baseline (obtained
from the average results measured over 1,000 ran-
dom alignment runs) and the simple EM model.
The results in Table 3 show that the simple
EM model performs below the random baseline.
The individual extended models achieve signifi-
cant improvement over the simple model and the
random baseline. While the distance model has a
smaller impact, the influence of the word choice
4This number was determined by experiments as a gen-
eral heuristics.
# directions Precison Recall F1
1 28.94% 42.31% 34.38%
2 29.04% 41.90% 34.31%
3 29.01% 42.18% 34.38%
4 28.75% 41.81% 34.07%
5 29.36% 42.69% 34.79%
6 30.18% 43.91% 35.77%
Table 4: Average results when using only a spe-
cific number of directions for each route with the
model Full (-thld).
model is considerable. Applying the full model
yields further performance gains. We note that for
all models recall is higher compared to precision.
One of the reasons for this phenomenon may be
that the EM-based models align as many attributes
as possible to frame elements in the route direc-
tions. In our gold standard, however, only around
12% of all relevant attributes correspond to frame
elements in the route directions (cf. Section 3.2).
We estimate this quota from a part of the corpus
and use it as an alignment threshold, i.e., for eval-
uation we select the best alignments proposed by
the models, until we reach the threshold. With this
we achieve a F1 measure of 38.40% in a 6-fold
cross validation test. This represents an improve-
ment of 3.97 points and considerably boosts preci-
sion, yielding overall balanced precision (38.90%)
and recall (37.92%).
A general problem of the current setup is the
small amount of available data. With a total of 54
route directions, the data consists of 6 to 8 direc-
tions for each route. We compute a learning curve
by using only exactly 1 to 6 directions per route to
examine whether performance improves with in-
creasing data size. The results are computed as
an average over multiple runs with different data
partitions (see Table 4). The results indicate small
but consistent improvements with increasing data
sizes, however, the differences are minimal. Thus
we are not able to conclude at this point whether
performance increases are possible with the addi-
tion of more data.
5.3 Error Analysis
In an error analysis on the results of the full model,
we found that 363 out of 784 (46%) misalign-
964
ments are related to attributes not aligned in our
gold standard. This is due to the fact that not
all relevant attributes are realized in natural lan-
guage directions. By addressing this problem in
the model Full+threshold, we are able to reduce
these errors, as evidenced by a gain of almost 10
points in precision and 4 points in F1 measure.
We further observe that the word choice model
does not correctly reflect the distribution of cat-
egorial attributes in the parallel corpus. In the
data, we observe that humans often aggregate
multiple occurrences of the same attribute value
into one single utterance. An example of such a
phenomenon can be seen with the attribute type
?directionOfTurn?: Even though ?straight? is the
most common value for this attribute, it is only re-
alized in directions in 33 (5%) cases (compared
to 65% and 47% for ?left? and ?right? respec-
tively). While our EM implementation maximizes
the likelihood for all alignment probabilities based
on expected counts, many pairs are not ? or not
frequently ? found in the corpus. This results in
the model often choosing incorrect alignments for
categorial attributes and makes up for 23% of the
misaligned attributes in total.
We found that further 5% of the attributes are
misaligned with frame elements containing pro-
nouns that actually refer to a different attribute.
As our word choice model does not account for
the use of anaphora, none of the affected frame
elements are aligned correctly. Given the genre
of our corpus, integrating simple heuristics to re-
solve anaphora (e.g., binding to the closest pre-
ceding mention) could solve this problem for the
majority of the cases.
6 Conclusion
We presented a weakly supervised method for
aligning route representations and natural lan-
guage directions on the basis of parallel corpora
using EM-based learning. Our models adopt ideas
from Liang et al (2009) with special adaptations
to the current application scenario. As a major
difference to their work, we make use of frame-
semantic annotations on the NL side as a basis for
segmentation.
While we can show that the extended mod-
els significantly outperform a simple EM-based
model, the overall results are still moderate. We
cannot draw a direct comparison to the results pre-
sented in Liang et al (2009) due to the different
scenarios and data sets. However, the corpus they
used for the NFL recaps scenario is the closest to
ours in terms of available data size and percentage
of aligned records (in our case attributes). For this
kind of corpus, they achieve an F1 score of 39.9%
with the model that is closest to ours (Model 2?).
Their model achieves higher performance for sce-
narios with more available data and a higher per-
centage of alignments. Thus we expect that our
model benefits from additional data sets, which
we plan to gather in web-based settings.
Still, we do not expect to achieve near to per-
fect alignments due to speaker variation, a factor
we also observe in the current data. As our ul-
timate goal is to generate NL instructions from
given route representations, we can nevertheless
make use of imperfectly aligned data for the com-
pilation of high-confidence rules to compute se-
mantic input structures for NLG. Following previ-
ous work by Barzilay and Lee (2002), we can also
exploit the fact that our data consists of multiple
directions for each route to identify alternative re-
alization patterns for the same route segments. In
addition, (semi-)supervised models could be used
to assess the gain we may achieve in comparison
to the minimally supervised setting.
However, we still see potential for improv-
ing our current models by integrating refinements
based on the observations outlined above: Miss-
ing alignment targets on the linguistic side ? es-
pecially due to anaphora, elliptical or aggregating
constructions ? constitute the main error source.
We aim to capture these phenomena within the
linguistic markup in order to provide hidden align-
ment targets. Also, our current model only consid-
ers frame elements as alignment targets. This can
be extended to include their verbal predicates.
Acknowledgements: This work is supported by
the DFG-financed innovation fund FRONTIER as
part of the Excellence Initiative at Heidelberg Uni-
versity (ZUK 49/1). We thank Michael Bauer and
Pascal Neis for the specification of the route repre-
sentation format and Carina Silberer and Jonathan
Geiger for annotation.
965
References
Anderson, Anne H., Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, Catherine Sotillo, Henry Thompson, and
Regina Weinert. 1991. The HCRC Map Task cor-
pus. Language and Speech, 34(4):351?366.
Barzilay, Regina and Mirella Lapata. 2005. Collective
content selection for concept-to-text-generation. In
Proceedings of the Human Language Technology
Conference and the 2005 Conference on Empirical
Methods in Natural Language Processing, Vancou-
ver, B.C., Canada, 6?8 October 2005, pages 331?
338.
Barzilay, Regina and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, Penn., 6?7 July 2002, pages 164?171.
Brown, Peter F., Vincent J. Della Pietra, Stephan
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19:263?311.
Burchardt, Aljoscha, Katrin Erk, Anette Frank, Andrea
Kowalski, and Sebastian Pado. 2006. SALTO: A
versatile multi-level annotation tool. In Proceedings
of the 5th International Conference on Language
Resources and Evaluation, Genoa, Italy, 22?28 May
2006, pages 517?520.
Dale, Robert, Sabine Geldof, and Jean-Philippe Prost.
2005. Using natural language generation in auto-
matic route description. Journal of Research and
Practice in Information Technology, 37(1):89?106.
Dempster, Arthur P., Nan M. Laird, and Donald B.
Rubin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the
Royal Statistics Society, Series B (Methodological),
39(1):1?38.
Duboue, Pablo A. and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, Sapporo,
Japan, 11?12 July 2003, pages 121?128.
Fillmore, Charles J., Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16(3):235?250.
Hansen, Stefan, Kai-Florian Richter, and Alexander
Klippel. 2006. Landmarks in OpenLS: A data
structure for cognitive ergonomic route directions.
In Proceedings of the 4th International Conference
on Geographic Information Science, Mu?nster, Ger-
many, 20-23 September 2006.
Liang, Percy, Michael Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of ACL-IJCNLP 2009, pages
91?99, August.
Mani, Inderjeet. 2001. Automatic Summarization.
John Benjamins, Amsterdam, Philadelphia.
Maxwell, John T. and Ronald M. Kaplan. 1993.
The interface between phrasal and functional con-
straints. Computational Linguistics, 19(4):571?
590.
Neis, Pascal and Alexander Zipf. 2008. Extending the
OGC OpenLS route service to 3D for an interoper-
able realisation of 3D focus maps with landmarks.
Journal of Location Based Services, 2(2):153?174.
Reiter, Ehud and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge, U.K.:
Cambridge University Press.
Roth, Michael and Anette Frank. 2009. A NLG-based
Application for Walking Directions. In Companion
Volume to the Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing,
Singapore, 2?7 August 2009, pages 37?40.
Schmid, Helmut. 1997. Probabilistic Part-of-Speech
tagging using decision trees. In Jones, Daniel and
Harold Somers, editors, New Methods in Language
Processing, pages 154?164. London, U.K.: UCL
Press.
Schuldes, Stephanie, Michael Roth, Anette Frank, and
Michael Strube. 2009. Creating an annotated cor-
pus for generating walking directions. In Proceed-
ings of the ACL-IJCNLP 2009 Workshop on Lan-
guage Generation and Summarisation, Singapore,
6 August 2009, pages 72?76.
Tiedemann, Jo?rg. 2003. Combining Clues for Word
Alignment. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 339?346,
Budapest, Hungary.
Vogel, Adam and Dan Jurafsky. 2010. Learning to
Follow Navigational Directions. In Proceedings of
ACL-2010, Uppsala, Sweden.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based Word Alignment in Sta-
tistical Translation. In Proceedings of the 16h Inter-
national Conference on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark.
966
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 171?182, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Aligning Predicates across Monolingual Comparable Texts
using Graph-based Clustering
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University
Germany
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Generating coherent discourse is an important
aspect in natural language generation. Our
aim is to learn factors that constitute coherent
discourse from data, with a focus on how to re-
alize predicate-argument structures in a model
that exceeds the sentence level. We present
an important subtask for this overall goal, in
which we align predicates across compara-
ble texts, admitting partial argument struc-
ture correspondence. The contribution of this
work is two-fold: We first construct a large
corpus resource of comparable texts, includ-
ing an evaluation set with manual predicate
alignments. Secondly, we present a novel ap-
proach for aligning predicates across compa-
rable texts using graph-based clustering with
Mincuts. Our method significantly outper-
forms other alignment techniques when ap-
plied to this novel alignment task, by a margin
of at least 6.5 percentage points in F1-score.
1 Introduction
Discourse coherence is an important aspect in natu-
ral language generation (NLG) applications. A num-
ber of theories have investigated coherence inducing
factors. A prominent example is Centering Theory
(Grosz et al1995), which models local coherence
by relating the choice of referring expressions to the
importance of an entity at a certain stage of a dis-
course. A data-driven model based on this theory
is the entity-based approach by Barzilay and Lap-
ata (2008), which models coherence phenomena by
observing sentence-to-sentence transitions of entity
occurrences.
Barzilay and Lapata show that their approach can
discriminate between a coherent and a non-coherent
set of ordered sentences. However, their model is
not able to generate alternative entity realizations by
itself. Furthermore, the entity-based approach only
investigates realization patterns for individual enti-
ties in discourse in terms of core grammatical func-
tions. It does not investigate the interplay between
entity transitions and realization patterns for full-
fledged semantic structures. This interplay, how-
ever, is an important factor for a semantics-based,
generative model of discourse coherence.
The main hypothesis of our work is that we can
automatically learn context-specific realization pat-
terns for predicate argument structures (PAS) from a
semantically parsed corpus of comparable text pairs.
Our assumption builds on the success of previous
research, where comparable and parallel texts have
been exploited for a range of related learning tasks,
e.g., unsupervised discourse segmentation (Barzilay
and Lee, 2004) and bootstrapping semantic analyz-
ers (Titov and Kozhevnikov, 2010).
For our purposes, we are interested in finding cor-
responding PAS across comparable texts that are
known to talk about the same events, and hence in-
volve the same set of underlying event participants.
By aligning predicates in such texts, we can inves-
tigate the factors that determine discourse coher-
ence in the realization patterns for the involved argu-
ments. These include the specific forms of argument
realization, as a pronoun or a specific type of refer-
ential expression, as studied in prior work in NLG
(Belz et al2009, inter alia). The specific set-up
we examine, however, allows us to further investi-
171
gate the factors that govern the non-realization of
an argument position, as a special form of coher-
ence inducing element in discourse. Example (1),
extracted from our corpus of aligned texts,illustrates
this point: Both texts report on the same event of
locating victims in an avalanche. While (1.a) explic-
itly talks about the location of this event, the role re-
mains implicit in the second sentence of (1.b), given
that it can be recovered from the preceding sentence.
In fact, realization of this argument role would im-
pede the fluency of discourse by being overly repet-
itive.
(1) a. . . . The official said that [no bodies]Arg1 had
been recovered [from the avalanches]Arg2 which
occurred late Friday in the Central Asian coun-
try near the Afghan border some 300 kilometers
(185 miles) southeast of the capital Dushanbe.
b. Three other victims were trapped in an
avalanche in the village of Khichikh. [None
of the victims bodies]Arg1 have been found
[ ]Argm-loc.
This phenomenon clearly relates to the problem
of discourse-linking of implicit roles, a very chal-
lenging task in discourse processing.1 In our work,
we consider this problem from a content-based gen-
eration perspective, concentrating on the discourse
factors that allow for the omission of a role.
Thus, our aim is to identify comparable predica-
tions across aligned texts, and to study the discourse
coherence factors that determine the realization pat-
terns of arguments in the respective discourses. This
can be achieved by considering the full set of argu-
ments that can be recovered from the aligned pred-
ications. This paper focuses on the first of these
tasks, henceforth called predicate alignment.2
In line with data-driven approaches in NLP, we
automatically align predicates in a suitable corpus of
paired texts. The induced alignments will (i) serve to
identify events described in both comparable texts,
and (ii) provide information about the underlying ar-
gument structures and how they are realized in each
context to establish a coherent discourse. We in-
vestigate a graph-based clustering method for induc-
1See the recent SemEval 2010 task: Linking Events and
their Participants in Discourse, (Ruppenhofer et al2010).
2Note that we provide details regarding the construction of
a suitable data set and further examples involving non-realized
arguments in a complementary paper (Roth and Frank, 2012).
ing such alignments as clustering provides a suitable
framework to implicitly relate alignment decisions
to one another, by exploiting global information en-
coded in a graph.
The remainder of this paper is structured as fol-
lows: In Section 2, we discuss previous work in re-
lated tasks. Section 3 describes our task and a suit-
able data set. Section 4 introduces a graph-based
clustering model using Mincuts for the alignment of
predicates. Section 5 outlines the experiments and
presents evaluation results. Finally, we conclude in
Section 6 and discuss future work.
2 Related Work
The task of aligning words in general has been stud-
ied extensively in previous work, for example as part
of research in statistical machine translation (SMT).
Typically, alignment models in SMT are trained by
observing and (re-)estimating co-occurrence counts
of word pairs in parallel sentences (Brown et al
1993). The same methods have also been applied
in monolingual settings, for example to align words
in paraphrases (Cohn et al2008). In contrast to
traditional word alignment tasks, our focus is not on
pairs of isolated sentences but on aligning predicates
within the discourse contexts in which they are sit-
uated. Furthermore, text pairs for our task should
not be strictly parallel as we are specifically inter-
ested in the impact of different discourse contexts.
In Section 5, we will show that this particular set-
ting indeed constitutes a more challenging task com-
pared to traditional word alignment in parallel or
paraphrasing sentences.
Another set of related tasks is found in the area of
textual inference. Since 2006, there have been reg-
ular challenges on the task of Recognizing Textual
Entailment (RTE). In the original task description,
Dagan et al2006) define textual entailment ?as a
directional relationship between pairs of text expres-
sions, denoted by T - the entailing ?Text? -, and H
- the entailed ?Hypothesis?. (. . . ) T entails H if the
meaning of H can be inferred from the meaning of
T, as would typically be interpreted by people.? Al-
though this relation does not necessarily require the
presence of corresponding predicates, previous work
by MacCartney et al2008) shows that word align-
ments can serve as a good indicator of entailment.
172
As a matter of fact, the same holds true for the task
of detecting paraphrases. In contrast to RTE, this lat-
ter task requires bi-directional entailments, i.e., each
of the two phrases must entail the other. Wan et al
(2006) show that a simple approach solely based on
word (and lemmatized n-gram) overlap can already
achieve an F1-score of up to 83% for detecting para-
phrases in the Microsoft Research Paraphrase Cor-
pus (Dolan and Brockett, 2005, MSRPC). In fact,
this is just 0.6% points below the state-of-the-art re-
sults recently reported by Socher et al2011).
The MSRPC and data sets from the first RTE
challenges only consisted of isolated pairs of sen-
tences. The Fifth PASCAL Recognizing Textual En-
tailment Challenge (Bentivogli et al2009) intro-
duced a ?Search Task?, where entailing sentences
for a hypothesis have to be found in a set of full
documents. This new task first opened the doors for
assessing the role of discourse (Mirkin et al2010a;
Mirkin et al2010b) in RTE. However, this setting is
still limited as discourse contexts are only provided
for the entailing part (T ) of each text pair but not for
the hypothesis H .
A further task related to ours is the detection
of event coreference. The goal of this task is to
identify all mentions of the same event within a
document and, in some settings, also across docu-
ments. However, the task setting is typically more
restricted than ours in that its focus lies on iden-
tical events/references (cf. Walker et al2006),
Weischedel et al2011), inter alia). In particular,
verbalizations of different aspects of an event (e.g.,
?buy???sell?, ?kill???die?, ?recover???find?) are gen-
erally not linked in this paradigm. In contrast to co-
reference methods that identify chains of events, we
are interested in pairs of corresponding predicates
(and their argument structure), for which we can ob-
serve alternative realizations in discourse.
3 Aligning Predicates Across Texts
This section summarizes how we built a large cor-
pus of comparable texts, as a basis for the predicate
alignment task. We motivate the choice of the cor-
pus and present a strategy for extracting comparable
text pairs. Subsequently, we report on the prepara-
tion of an evaluation data set with manual predicate
alignments across the paired texts. We conclude this
section with an example that showcases the poten-
tial of using aligned predicates for the study of co-
herence phenomena. More detailed information re-
garding corpus creation, annotation guidelines and
additional examples illustrating the potential of this
corpus can be found in Roth and Frank (2012).
3.1 Corpus Creation
The goal of our work is to investigate coherence fac-
tors for argument structure realization, using com-
parable texts that describe the same events, but that
include variation in textual presentation. This re-
quirement fits well with the news domain, for which
we can trace varying textual sources that describe
the same underlying events. The English Gigaword
Fifth Edition (Parker et al2011) corpus (henceforth
just Gigaword) is one of the largest corpus collec-
tions for English. It comprises a total of 9.8 million
newswire articles from seven distinct sources.
In previous work (Roth and Frank, 2012), we in-
troduced GigaPairs, a sub-corpus extracted from Gi-
gaword that includes over 160,000 pairs of newswire
articles from distinct sources. GigaPairs has been
derived from Gigaword using the pairwise similar-
ity method on headlines presented by Wubben et al
(2009). In addition to calculating the similarity of
news titles, we impose an additional date constraint
to further increase the precision of extracted pairs of
texts. Random inspection of about 100 documents
revealed only two texts describing different events.
Overall, we extracted 167,728 document pairs con-
taining a total of 50 million word tokens. Each doc-
ument in this corpus consists of up to 7.564 words
with a mean and median of 301 and 213 words, re-
spectively. All texts have been pre-processed us-
ing MATE tools (Bjo?rkelund et al2010; Bohnet,
2010), a pipeline of NLP modules including a state-
of-the-art semantic role labeler that computes Prop-
Bank/NomBank annotations (Palmer et al2005;
Meyers et al2008).
3.2 Gold Standard Annotation
We selected 70 text pairs from the GigaPairs cor-
pus for manual predicate alignment. All document
pairs were randomly chosen with the constraint that
each text consists of 100 to 300 words.3 Predi-
3This constraint is satisfied by 75.3% of all documents in
GigaPairs.
173
cates identified by the semantic parser are provided
as pre-labeled annotations for alignment. We asked
two students4 to tag corresponding predicates across
each text pair. Following standard practice in word
alignment tasks (cf. Cohn et al2008)) the annota-
tors were instructed to distinguish between sure and
possible alignments, depending on how certainly, in
their opinion, two predicates describe verbalizations
of the same event. The following examples show
predicate pairings marked as sure (2) and as possi-
ble alignments (3).
(2) a. The regulator ruled on September 27 that Nas-
daq too was qualified to bid for OMX [. . . ]
b. The authority [. . . ] had already approved a sim-
ilar application by Nasdaq.
(3) a. Myanmar?s military government said earlier this
year it has released some 220 political prisoners
[. . . ]
b. The government has been regularly releasing
members of Suu Kyi?s National League for
Democracy party [. . . ]
In total, the annotators (A/B) aligned 487/451 sure
and 221/180 possible alignments with a Kappa score
(Cohen, 1960) of 0.86.5 For the construction of a
gold standard, we merged the alignments from both
annotators by taking the union of all possible align-
ments and the intersection of all sure alignments.
Cases which involved a sure alignment on which the
annotators disagreed were resolved in a group dis-
cussion with the first author.
We split the final corpus into a development set
of 10 document pairs and a test set of 60 document
pairs. The test set contains a total of 3,453 predicates
(1,531 nouns and 1,922 verbs). Its gold standard an-
notation consists of 446 sure and 361 possible align-
ments, which corresponds to an average of 7.4 sure
(6.0 possible) alignments per document pair. Most
of the gold alignments (82.4%) are between predi-
cates of the same part-of-speech (242 noun and 423
verb pairs). A total of 383 gold alignments (47.5%)
have been annotated between predicates with iden-
tical lemma form. Diverging numbers of realized
arguments can be observed in 320 pairs (39.7%).
4Both annotators are students in computational linguistics,
one undergraduate (A) and one postgraduate (B) student.
5Following Brockett (2007), we computed agreement on la-
beled annotations, including unaligned predicate pairs as an ad-
ditional null category.
3.3 Potential for Discourse Coherence
This section presents an example of an aligned
predicate pair from our development set that il-
lustrates the potential of aggregating corresponding
PAS across comparable texts. The example repre-
sents one of eleven cases involving unrealized argu-
ments that can be found in our development set of
only ten document pairs.
(4) a. The Chadians said theyArg0 had fled in fear of
their lives.
b. The United Nations says some 20,000
refugeesArg0 have fled into CameroonArg1.
In both sentences, the Arg0 role of the predicate flee
is filled, but Arg1 (here: the goal) has not been real-
ized in (4.a). However, sentence (4.a) is still part of a
coherent discourse, as a role filler for the omitted ar-
gument can be inferred from the preceding context.
For the goal of our work, we are interested in factors
that license such omissions of an argument. Poten-
tial factors on the discourse level include the infor-
mation status of the entity filling an argument posi-
tion, and its salience at the corresponding point in
discourse. Roth and Frank (2012) discuss additional
examples that demonstrate the importance of fac-
tors on further linguistic levels, e.g., lexical choice
of predicates and their syntactic realization.
In the example above, the aggregation of aligned
PAS presents an effective means to identify appro-
priate fillers for unrealized roles. Hence, we can uti-
lize each such pair as one positive and one negative
training instance for a model of discourse coherence
that controls the omissibility of arguments. In what
follows, we introduce an alignment approach that
can be used to automatically acquire more training
data using the entire GigaPairs corpus.
4 Model
For the automatic induction of predicate alignments
across texts, we opt for an unsupervised graph-based
clustering method. In this section, we first define a
graph representation for pairs of documents. In par-
ticular, predicates are represented as nodes in such a
graph and similarities between predicates as edges.
We then proceed to describe various similarity mea-
sures that can be used to identify similar predicate
instances. Finally, we introduce the clustering algo-
rithm that we apply to graphs (representing pairs of
174
documents) in order to induce alignments between
corresponding predicates.
4.1 Graph representation
We build a bipartite graph representation for each
pair of texts, using as vertices the predicate argu-
ment structures assigned in pre-processing (cf. Sec-
tion 3.1). We represent each predicate as a node and
integrate information about arguments only implic-
itly. Given the sets of predicates P1 and P2 of two
comparable texts T1 and T2, respectively, we for-
mally define an undirected graph GP1,P2 as follows:
GP1,P2 = ?V,E? where
V = P1 ? P2
E = P1 ? P2
(1)
Edge weights. We specify the edge weight be-
tween two nodes representing predicates p1 ? P1
and p2 ? P2 as a weighted linear combination of
four similarity measures described in the next sec-
tion: WordNet and VerbNet similarity, Distributional
similarity and Argument similarity.
wp1p2 = ?1 ? simWN(p1, p2)
+ ?2 ? simVN(p1, p2)
+ ?3 ? simDist(p1, p2)
+ ?4 ? simArg(p1, p2)
(2)
Initially we set aleighting parameters ?1 . . . ?4 to
have uniform weights by default. In Section 5, we
define an optimized weighting setting for the indi-
vidual similarity measures.
4.2 Similarity Measures
We employ a number of similarity measures
that make use of complementary information
that is type-based (simWN/VN/Dist) or token-based
(simArg).6 Given two lemmatized predicates p1, p2
and their set of arguments A1 = args(p1), A2 =
args(p2), we define the following measures.
WordNet similarity. Given all pairs of synsets s1,
s2 that contain the predicates p1, p2, respectively,
we compute the maximal similarity using the infor-
mation theoretic measure described in Lin (1998).
Our implementation exploits the WordNet hierarchy
6All token-based frequency counts (i.e., freq() and idf())
are computed over all documents from the AFP and APW parts
of the English Gigaword Fifth Edition.
(Fellbaum, 1998) to find the synset of the least com-
mon subsumer (lcs) and uses the pre-computed In-
formation Content (IC) files from Pedersen et al
(2004) to compute Lin?s measure:
simWN(p1, p2) =
IC(lcs(s1, s2))
IC(s1) ? IC(s2)
(3)
In order to compute similarities between verbal and
nominal predicates, we further use derivation infor-
mation from NomBank (Meyers et al2008): if a
noun represents a nominalization of a verbal pred-
icate, we resort to the corresponding verb synset.
If no relation can be found between two predicates,
we set a default value of simWN = 0. This applies
in particular to all cases that involve a predicate not
present in WordNet.
VerbNet similarity. To overcome systematic
problems with the WordNet verb hierarchy (cf.
Richens (2008)), we further compute similarity
between verbal predicates using VerbNet (Kipper
et al2008). Verbs in VerbNet are categorized into
semantic classes according to their syntactic behav-
ior. A class C can recursively embed sub-classes
Cs ? sub(C) that represent finer semantic and
syntactic distinctions. We define a simple similarity
function that defines fixed similarity scores between
0 and 1 for pairs of predicates p1, p2 depending on
their relatedness within the VerbNet class hierarchy:
simVN(p1, p2) =
?
????
????
1.0 if ?C : p1, p2 ? C
0.8 if ?C,Cs : Cs ? sub(C)
? p1, p2 ? C ? Cs
0.0 else
(4)
Distributional similarity. As some predicates
may not be covered by the WordNet and VerbNet hi-
erarchies, we additionally calculate similarity based
on distributional meaning in a semantic space (Lan-
dauer and Dumais, 1997). Following the traditional
bag-of-words approach that has been applied in re-
lated tasks (Guo and Diab, 2011; Mitchell and La-
pata, 2010), we consider the 2,000 most frequent
context words c1, . . . , c2000 ? C as dimensions of
a vector space and define predicates as vectors using
their Pointwise Mutual Information (PMI):
~p = (PMI(p, c1), . . . ,PMI(p, c2000) (5)
175
with PMI(x, y) =
freq(x, y)
freq(x) ? freq(y)
Given the vector representations of two predicates,
we calculate their similarity as the cosine of the an-
gle between the two vectors:
simDist(p1, p2) =
~p1 ? ~p2
|~p1| ? |~p2|
(6)
Argument similarity. While the previous similar-
ity measures are purely type-based, argument simi-
larity integrates token-based, i.e., discourse-specific,
similarity information about predications by taking
into account the similarity of their arguments. This
measure calculates the association between the ar-
guments A1 of the first and the arguments A2 of the
second predicate by determining the ratio of over-
lapping words in both argument sets.
simArg(p1, p2) =
?
w?A1?A2 idf(w)?
w?A1 idf(w) +
?
w?A2 idf(w)
(7)
In order to give higher weight to (rare) content
words, we weight each word by its Inverse Docu-
ment Frequency (IDF), which we calculate over all
documents d from the AFP and APW sections of the
Gigaword corpus:
idf(w) = log
|D|
|{d : w ? D|}
(8)
Normalization. In order to make the outputs of all
similarity measures comparable, we normalize their
value ranges on the development set to have a mean
and standard deviation of 1.0.
4.3 Mincut-based Clustering
Our graph clustering method uses minimum cuts (or
Mincut) in order to partition the bipartite text graph
into clusters of aligned predicates. A Mincut op-
eration divides a given graph into two disjoint sub-
graphs. Each minimum cut is performed as a cut
between some source node s and some target node
t, such that (i) each of the two nodes will be in a
different sub-graph and (ii) the sum of weights of all
removed edges will be as small as possible. Our sys-
tem determines each Mincut using an implementa-
tion of the method by Goldberg and Tarjan (1986).7
7Basic graph operations are performed using the freely
available Java library JGraph, cf. http://jgrapht.org/.
function CLUSTER(G)
clusters? ?
E ? GETEDGES(G) . Step 1
e? GETEDGEWITHLOWESTWEIGHT(E)
s? GETSOURCENODE(e)
t? GETTARGETNODE(e)
G? ? MINCUT(G, s, t) . Step 2
C ? GETCONNECTEDCOMPONENTS(G?)
for all Gs ? C do . Step 3
if SIZE(Gs) <= 2 then
clusters? clusters ?Gs
else
clusters? clusters ? CLUSTER(Gs)
end if
end for
return clusters;
end function
Figure 2: Pseudo code of our clustering algorithm
As our goal is to induce clusters that correspond to
pairs of similar predicates, we set a maximum num-
ber of two nodes per cluster as stopping criterion.
Given an input graph G, our algorithm recursively
applies Mincuts in three steps as described in Figure
2. Step 1 identifies the edge e with lowest weight in
the given graph G. Step 2 performs the actual Min-
cut operation on G. Finally, the stopping criterion
and recursion are applied in Step 3. An example of
a clustered graph is illustrated in Figure 1.
The advantage of our method compared to off-
the-shelf clustering techniques is two-fold: On the
one hand, the clustering algorithm is free of any pa-
rameters, such as the number of clusters or a clus-
tering threshold, that require fine-tuning. On the
other hand, the approach makes use of a termina-
tion criterion that very well represents the nature of
the goal of our task, namely to align pairs of predi-
cates across comparable texts. The next section pro-
vides empirical evidence for the advantage of this
approach.
5 Experiments
This section evaluates our graph-clustering model
on the task of aligning predicates across compara-
ble texts. For comparison to related tasks and meth-
ods, we describe different evaluation settings, vari-
176
Figure 1: The predicates of two sentences (white: ?The company has said it plans to restate its earnings for 2000
through 2002.?; grey: ?The company had announced in January that it would have to restate earnings (. . . )?) from the
Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.
ous baselines, as well as results for these baselines
and the model presented above.
5.1 Settings
In order to benchmark our model against tradi-
tional methods for word alignment, we first apply
our graph-based alignment model (Full) on three
sentence-based paraphrase corpora. This model uses
the similarity measures defined in Section 4.2 and
the clustering algorithm introduced in Section 4.3.
In a second experiment, we evaluate Full on our
novel task of inducing predicate alignments across
comparable monolingual texts, using the GigaPairs
data set described in Section 3. We evaluate against
the manually annotated gold alignments in the test
data set described in Section 3.2. To gain more in-
sight into the performance of the various similar-
ity measures included in the Full model, we eval-
uate simplified versions that omit individual similar-
ity measures (Full?[measure name]).
The relative differences in performance against
various baselines will help us quantify the differ-
ences and difficulties between a traditional sentence-
based word alignment setting and our novel align-
ment task that operates on full texts.
5.1.1 Sentence-level Alignment Setting
For sentence-based predicate alignment we make
use of the following three corpora that are word-
aligned subsets of the paraphrase collections de-
scribed in (Cohn et al2008): MTC consists of 100
sentence pairs from the Multiple-Translation Chi-
nese Corpus (Huang et al2002), Leagues contains
100 sentential paraphrases from two translations of
Jules Verne?s ?Twenty Thousand Leagues Under
the Sea?, and MSR is a sub-set of the Microsoft
Research Paraphrase Corpus (Dolan and Brockett,
2005), consisting of 130 sentence pairs. All three
paraphrase collections are in English.
Results for these experiments are reported in Sec-
tion 5.3.1. Note that in order to determine alignment
candidates, we apply the same pre-processing steps
as used for the annotation of our corpus. The se-
mantic parser identified an average number of 3.8,
5.1 and 4.7 predicates per text (i.e., per paraphrase
sentence) in MTC, Leagues and MSR, respectively.
All models are evaluated against the subset of gold
standard alignments (cf. Cohn et al2008)) between
pairs of words marked as predicates.
5.1.2 Text-level Alignment Setting
Results for our own data set, GigaPairs, are reported
in Section 5.3.2. In this setting, models are evaluated
against the annotated gold standard alignments be-
tween predicates as described in Section 3.2. Since
all text pairs in GigaPairs comprise multiple sen-
tences each, the average number of predicates per
text to consider (27.5) is much higher than in the
paraphrase settings. As the full graph representa-
tion becomes rather inefficient to handle (by default,
edges are inserted between all predicate pairs), we
use the development set of 10 text pairs to estimate
177
MTC Leagues MSR
Precision Recall F1 Precision Recall F1 Precision Recall F1
LemmaId 25.1** 74.9 37.6** 31.5** 67.2 42.9** 42.3** 90.8 57.7**
Greedy 74.8** 88.3** 81.0 75.0** 86.0** 80.1 80.7** 97.0** 88.1
WordAlign 99.3 86.6 92.5 98.7 78.5 87.4 99.5 96.0* 97.7*
Full 92.3 72.2 81.1 92.7 69.4 79.4 94.5 88.3 91.3
Table 1: Results for sentence-based predicte alignment in the three benchmark settings MTC, Leagues and MSR (all
numbers in %); results that significantly differ from Full are marked with asterisks (* p<0.05; ** p<0.01).
a threshold on predicate similarity for adding edges.
We tested all thresholds from 1.5 to 4.0 with a step-
size of 0.25 and found 2.5 to perform best. This
threshold is applied in the evaluation of all graph-
based models.
5.2 Baselines
A simple baseline for both settings is to align all
predicates whose lemmas are identical. This base-
line, henceforth called LemmaId, is computed as a
lower bound for all settings. In order to assess the
benefits of the clustering step, we propose a second
baseline that uses the same similarity measures and
thresholds as our Full model, but omits the cluster-
ing step described in Section 4.3. Instead, it greed-
ily computes as many 1-to-1 alignments as possible,
starting from the highest similarity to the learned
threshold (Greedy).
As a more sophisticated baseline, we make
use of alignment tools commonly used in sta-
tistical machine translation (SMT). For the three
sentence-based paraphrase settings MTC, Leagues
and MSR, Cohn et al2008) readily provide
GIZA++ (Och and Ney, 2003) alignments as part
of their word-aligned paraphrase corpus. For the
experiments in the GigaPairs setting, we train our
own word alignment model using the state-of-the-
art word alignment tool Berkeley Aligner (Liang et
al., 2006). As word alignment tools require pairs of
sentences as input, we first extract paraphrases in the
latter setting using a re-implementation of the para-
phrase detection system by Wan et al2006).8 In
the following section, we abbreviate both baselines
using SMT alignment tools as WordAlign.
8Note that the performance of this system lies slightly be-
low the state-of-the-art results reported by Socher et al2011)
However, we were not able to reproduce the results of Socher et
al. using the publicly available release of their software.
5.3 Results
We measure precision as the number of predicted
alignments that are annotated in the gold standard
divided by the total number of predictions. Recall
is measured as the number of correctly predicted
sure alignments divided by the total number of sure
alignments in the gold standard. This conforms to
evaluation measures used for word alignment mod-
els in SMT (Och and Ney, 2003). Following Cohn
et al2008), we subsequently compute the F1-score
as the harmonic mean between precision and recall.
We compute statistical significance of result dif-
ferences with a paired t-test (Cohen, 1995) over the
affected test set documents and provide correspond-
ing significance levels where appropriate.
5.3.1 Sentence-level Predicate Alignment
The results for MTC, Leagues and MSR are pre-
sented in Table 1. The numbers indicate that
WordAlign consistently outperforms all other mod-
els on the three data sets in terms of F1-score. Sta-
tistical significance of result differences between
WordAlign and Full can only be observed for recall
and F1-score on the MSR data set (p<0.05). Other
differences are not significant due to high variance
of results compared to data set sizes.
The overall performance of WordAlign does not
come much as a surprise, seeing that all three data
sets consist of highly parallel sentence pairs. In
fact, the results for LemmaId show that by align-
ing all predicates with identical lemmas, most of the
sure alignments in the three settings are already cov-
ered. The reason for the low precision lies in the
fact that the same lemma can occur multiple times
in the same paraphrase, a phenomenon that is bet-
ter handled by WordAlign, Greedy and Full. In-
terestingly, the Greedy model achieves the highest
recall in all settings but it performs below our Full
178
model in terms of precision and F1-score. The per-
formance differences between Greedy and Full are
statistically significant (p<0.01) regarding precision
and recall.
5.3.2 Text-level Predicate Alignment
We now turn to the experiments on our own data
set, GigaPairs, which comprises full documents
of unequal lengths instead of pairs of single sen-
tences. Table 2 presents the results for our full model
and the three baselines. From all four approaches,
WordAlign yields lowest performance. We observe
two main reasons for this: On the one hand, sen-
tence paraphrase detection does not perform per-
fectly. Hence, the extracted sentence pairs do not
always contain gold alignments. On the other hand,
even sentence pairs that contain gold alignments are
generally less parallel than in the previous settings,
which make them harder to align. The increased dif-
ficulty can also be seen in the results for the Greedy
baseline, which only achieves an F1-score of 20.1%
in this setting. In contrast, we observe that the ma-
jority of all sure alignments (60.3%) can be retrieved
by applying the LemmaId model.
The Full model achieves a recall of 46.6%, but
it significantly outperforms LemmaId (p<0.01) in
terms of precision (58.7%, +18.4 percentage points).
This is an important factor for us, as we plan to use
the alignments in subsequent tasks. With 52.0%,
Full achieves the best overall F1-score.
Ablating similarity measures. All aforemen-
tioned results were conducted in experiments with
a uniform weighting scheme of similarity measures
as introduced in Section 4.3. Table 3 shows the per-
formance impact of individual similarity measures
by removing them completely (i.e., setting their
weight to 0.0). The numbers indicate that not all
measures contribute positively to the overall perfor-
mance when using equal weights. However, a signif-
icant difference can only be observed when remov-
ing the argument similarity measure, which drasti-
cally reduces the results. This clearly highlights the
importance of incorporating the context of individ-
ual predications in this task.
Tuning weights. Subsequently, we tested various
combinations of weights on our development set in
order to estimate a good overall weighting scheme.
Precision Recall F1
LemmaId 40.3** 60.3** 48.3
Greedy 19.6** 20.6** 20.1**
WordAlign 19.7** 15.2** 17.2**
Full 58.7 46.6 52.0
Table 2: Results for GigaPairs (all numbers in %); re-
sults that significantly differ from Full are marked with
asterisks (* p<0.05; ** p<0.01).
Precision Recall F1
Full?WN 58.9 48.0 52.9
Full?VN 57.3 48.7 52.6
Full?Dist 54.3 42.8 47.9
Full?Args 40.1** 24.0** 30.0**
Full 58.7 46.6 52.0
Full+tuned 59.7** 50.7** 54.8**
Table 3: Impact of removing individual measures and us-
ing a tuned weighting scheme (all numbers in %); results
that significantly differ from Full are marked with aster-
isks (* p<0.05; ** p<0.01).
This tuning procedure is implemented as a brute-
force technique, in which we fix the weight of one
similarity measure and allow all other measures to
receive a weight assignment between 0.25 to 5.0
times the fixed weight. Finally, the resulting weights
are normalized to sum to 1.0. We found the best per-
forming weighting scheme to be 0.09, 0.48, 0.24 and
0.19 for ?1, . . . , ?4, respectively (cf. Eq. (2), Section
4). The performance gains of the resulting model
(Full+tuned) can be seen in Table 3. Comput-
ing statistical significance of the result differences
between Full+tuned and all baseline models con-
firmed significant improvements (p<0.01) for both
precision and F1-score.
5.4 Error Analysis
We perform an error analysis on the output of
Full+tuned on the development set of GigaPairs
in order to determine re-occurring problems. In to-
tal, the model missed 13 out of 35 sure alignments
(Type I errors) and predicted 23 alignments not an-
notated in the gold standard (Type II errors).
Six Type I errors (46%) occurred when the lemma
of an affected predicate occurred more than once in a
text and the model missed a correct link. Vice versa,
identical predicates that refer to different events have
179
been the source of 8 Type II errors (35%). We ob-
serve that these errors are frequently related to pred-
icates, such as ?say? and ?appear?, that often occur
in news texts. Altogether, we find 15 Type II errors
(65%) that are due to high predicate similarity de-
spite low argument overlap (cf. Example (5)).
(5) a. The US alert (. . . ) followed intelligence reports
that . . .
b. The Foreign Ministry announcement called on
Japanese citizens to be cautious . . .
We observe that argument overlap itself can be low
even for correct alignments. This clearly indicates
that a better integration of context is needed. Ex-
ample (6.a) illustrates a case in which the agent of
a warning event is not realized. Here, contextual in-
formation is required to correctly align it to the first
warning event in (6.b). This involves inference be-
yond the local PAS.
(6) a. The US alert (. . . ) is one step down from a full
[travel]Arg1 warning [ ]Arg0.
b. Japan has issued a travel alert . . . (which)
follows similar warnings [from Ameri-
can and British authorities]Arg0. (. . . ) An offi-
cial said it was highly unusual for [Tokyo]Arg0
to issue such a warning . . .
6 Conclusion
We presented a novel task for predicate alignment
across comparable monolingual texts, which we ad-
dress using graph-based clustering with Mincuts.
The motivation for this task is to acquire empirical
data for studying discourse coherence factors related
to argument structure realization.
As a first step, we constructed a data set of com-
parable texts that provide full discourse contexts
for alternative verbalizations of the same underlying
events. The data set is derived from all newswire
pairs found in the English Gigaword Fifth Edition
and contains a total of more than 160,000 paired
documents.
A subset of these pairs forms an evaluation set,
annotated with gold alignments that relate predica-
tions, which exhibit a (possibly partial) correspond-
ing argument structure. We established that the an-
notation task, while difficult, can be performed with
good inter-annotator agreement (? at 0.86).
Our main contribution is a novel clustering ap-
proach using Mincuts for aligning predications
across comparable texts. Our experiments estab-
lished that recursive clustering improves on greedy
selection methods by profiting from global infor-
mation encoded in the graph representation. While
the Mincut-based method is in itself unsupervised, a
small amount of development data is needed to tune
parameters for the construction of particularly suit-
able input graphs.
We tested our full model against two additional
baselines: simple heuristic alignment based on iden-
tical lemma forms and a combination of techniques
from SMT and paraphrase detection. The evalua-
tion for our novel task was complemented by a tra-
ditional word alignment task using established para-
phrase data sets. We determined clear differences in
performance for all models for the two types of task
settings. While word alignment methods from SMT
outperform the competing models in the sentence-
based alignment tasks, they perform poorly in the
discourse setting.
In future work, we will enhance our model by
incorporating more refined similarity measures in-
cluding discourse-based criteria. We will further ex-
plore tuning techniques, e.g., a more suitable pre-
selection method for edges in graph construction, in
order to increase either precision or recall. The deci-
sion of optimizing towards one measure or another
is clearly task-dependent. In our case, high preci-
sion is favorable as we plan to learn accurate dis-
course model parameters from the computed align-
ments. Even though such an optimization will result
in an overall lower recall, application of the align-
ment model on the entire GigaPairs corpus can still
provide us with a large amount of precise predicate
alignments. Using this set of alignments, we will
then proceed to exploit contextual information in or-
der to learn a semantic model for discourse coher-
ence in argument structure realization.
Acknowledgements
We are grateful to the Landesgraduiertenfo?rderung
Baden-Wu?rttemberg for funding within the research
initiative ?Coherence in language processing? at
Heidelberg University. We thank Danny Rehl and
Lukas Funk for annotation.
180
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tions, Montreal, Canada, June. to appear.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, Mass., 2?7 May 2004,
pages 113?120.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2009. The grec main subject reference generation
challenge 2009: overview and evaluation results. In
Proceedings of the 2009 Workshop on Language Gen-
eration and Summarisation, pages 79?87.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of TAC.
Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33?36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Microsoft Research.
Peter F. Brown, Vincent J. Della Pietra, Stephan A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37?46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing Corpora for Development and
Evaluation of Paraphrase Systems. 34(4).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In J. Quin?onero-Candela, I. Dagan, and
B. Magnini, editors, Machine Learning Challenges,
pages 177?190. Springer, Heidelberg, Germany.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Adrew V. Goldberg and Robert E. Tarjan. 1986. A
new approach to the maximum flow problem. In Pro-
ceedings of the eighteenth annual ACM symposium on
Theory of computing, pages 136?146, New York, NY,
USA.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Weiwei Guo and Mona Diab. 2011. Semantic topic mod-
els: Combining word distributional statistics and dic-
tionary definitions. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 552?561, July.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium, Philadelphia.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A Large-scale Classification
of English Verbs. 42(1):21?40.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic Anal-
ysis theory of the acquisition, induction, and represen-
tation of knowledge. Psychological Review, 104:211?
240.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In North American Associ-
ation for Computational Linguistics (NAACL), pages
104?111.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, Madison,
Wisc., 24?27 July 1998, pages 296?304.
Bill MacCartney, Michael Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, Waikiki, Honolulu, Hawaii, 25-
27 October 2008.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Shachar Mirkin, Jonathan Berant, Ido Dagan, and Eyal
Shnarch. 2010a. Recognising entailment within dis-
181
course. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Shachar Mirkin, Ido Dagan, and Sebastian Pado?. 2010b.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. 34(8):1388?
1429.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
29(1):19?51.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
105.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity ? Measuring the re-
latedness of concepts. In Companion Volume to the
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, Boston, Mass.,
2?7 May 2004, pages 267?270.
Tom Richens. 2008. Anomalies in the wordnet verb hier-
archy. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 729?736. Association for Computational Lin-
guistics.
Michael Roth and Anette Frank. 2012. Aligning pred-
icate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal, Canada, June.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems (NIPS 2011).
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, 11?16 July 2010, pages 958?967.
Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. ACE 2005 Multilin-
gual Training Corpus. Linguistic Data Consortium,
Philadelphia.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
?Para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, pages
131?138.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-
wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, Mohammed El-Bachouti, Robert Belvin, and
Ann Houston. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium, Philadelphia.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG 2009), pages 122?
125, Athens, Greece, March. Association for Compu-
tational Linguistics.
182
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 407?413,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Composition of Word Representations
Improves Semantic Role Labelling
Michael Roth and Kristian Woodsend
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
{mroth,kwoodsen}@inf.ed.ac.uk
Abstract
State-of-the-art semantic role labelling
systems require large annotated corpora to
achieve full performance. Unfortunately,
such corpora are expensive to produce and
often do not generalize well across do-
mains. Even in domain, errors are often
made where syntactic information does
not provide sufficient cues. In this pa-
per, we mitigate both of these problems
by employing distributional word repre-
sentations gathered from unlabelled data.
While straight-forward word representa-
tions of predicates and arguments improve
performance, we show that further gains
are achieved by composing representa-
tions that model the interaction between
predicate and argument, and capture full
argument spans.
1 Introduction
The goal of semantic role labelling (SRL) is to
discover the relations that hold between a pred-
icate and its arguments in a given input sen-
tence (e.g., ?who? did ?what? to ?whom?, ?when?,
?where?, and ?how?). This semantic knowl-
edge at the predicate-argument level is required
by inference-based NLP tasks in order to iden-
tify meaning-preserving transformations, such as
active/passive, verb alternations and nominaliza-
tions. Several manually-build semantic resources,
including FrameNet (Ruppenhofer et al., 2010)
and PropBank (Palmer et al., 2005), have been
developed with the goal of documenting and pro-
viding examples of such transformations and how
they preserve semantic role information. Given
that labelled corpora are inevitably restricted in
size and coverage, and that syntactic cues are not
by themselves unambiguous or sufficient, the suc-
cess of systems that automatically provide corre-
sponding analyses has been limited in practice.
Recent work on SRL has explored approaches
that can leverage unlabelled data, following a
semi-supervised (F?urstenau and Lapata, 2012;
Titov and Klementiev, 2012) or unsupervised
learning paradigm (Abend et al., 2009; Titov and
Klementiev, 2011). Unlabelled data provides ad-
ditional statistical strength and can lead to more
consistent models. For instance, latent representa-
tions of words can be computed, based on distri-
butional similarity or language modelling, which
can be used as additional features during tradi-
tional supervised learning. Although we would
expect that extra features would improve classifier
performance, this seems in part counter-intuitive.
Just because one word has a specific representa-
tion does not mean that it should be assigned a
specific argument label. Instead, one would ex-
pect a more complex interplay between predicate,
argument and the context they appear in.
In this paper, we investigate the impact of dis-
tributional word representations for SRL. Initially,
we augment the feature space with word repre-
sentations for a predicate and its argument head.
Furthermore, we use a compositional approach to
model a representation of the full argument, by
composing a joint representation of all words in
the argument span, and we also investigate the in-
teraction between predicate and argument, using
a compositional representation of the dependency
path. We demonstrate the benefits of these com-
positional features using a state-of-the-art seman-
tic role labeller, which we evaluate on the English
part of the CoNLL-2009 data set.
2 Related Work
Research into using distributional information
in SRL dates back to Gildea and Jurafsky
(2002), who used distributions over verb-object
co-occurrence clusters to improve coverage in ar-
gument classification. The distribution of a word
over these soft clusters assignments was added as
407
features to their classifier. The SRL system by
Croce et al. (2010) combines argument clustering
based on co-occurrence frequencies with a lan-
guage model. Collobert et al. (2011) used dis-
tributional word representations in a neural net-
work model that can update representations dur-
ing training. Zapirain et al. (2013) suggested dis-
tributional information as a basis for a selectional
preference model that can be used as a single addi-
tional feature for classifying potential arguments.
Most recently, Hermann et al. (2014) used distri-
butional word representations within pre-defined
syntactic contexts as input to a classifier which
learns to distinguish different predicate senses.
A complementary line of research explores the
representation of sequence information. Promi-
nent examples are the works by Deschacht and
Moens (2009) and Huang and Yates (2010) who
learned and applied Hidden Markov Models to
assign state variables to words and word spans,
which serve as supplementary features for classifi-
cation. One drawback of this approach is that state
variables are discrete and the number of states
(i.e., their granularity) has to be chosen in advance.
The popularity of distributional methods for
word representation has been a motivation for de-
veloping representations of larger constructions
such as phrases and sentences, and there have
been several proposals for computing the meaning
of word combinations in vector spaces. Mitchell
and Lapata (2010) introduced a general frame-
work where composition is formulated as a func-
tion f of two vectors u and v. Depending on
how f is chosen, different composition models
arise, the simplest being an additive model where
f(u, v) = u + v. To capture relational functions,
Baroni and Zamparelli (2010) expanded on this
approach by representing verbs, adjectives and ad-
verbs by matrices which can modify the properties
of nouns (represented by vectors). Socher et al.
(2012) combined word representations with syn-
tactic structure information, through a recursive
neural network that learns vector space represen-
tations for multi-word phrases and sentences. An
empirical comparison of these composition meth-
ods was provided in (Blacoe and Lapata, 2012).
In this work, we use type-based continuous rep-
resentations of words to compose representations
of multiple word sequences and spans, which can
then be incorporated directly as features into SRL
systems.
Distributional Feature Computation
Argument a ~a
Predicate p ~p
Predicate-argument Interaction ~a + ~p
Argument Span w
1
. . . w
n
?
i
~w
i
Dependency Path from a to p ?
w?path(a,p)
~w
Table 1: Features based on distributional word
representations and additive composition. Vector
~w denotes the representation of word w.
3 Method
Following the set-up of the CoNLL shared task
in 2009, we consider predicate-argument struc-
tures that consist of a verbal or nominal pred-
icate p and PropBank-labelled arguments a
i
?
{a
1
. . . a
n
}, where each a
i
corresponds to the head
word of the phrase that constitutes the respective
argument. Traditional semantic role labelling ap-
proaches compute a set of applicable features on
each pair ?p, a
i
?, such as the observed lemma type
of a word and the grammatical relation to its head,
that serve as indicators for a particular role label.
The disadvantage of this approach lies in the
fact that indicator features such as word and
lemma type are often sparse in training data and
hence do not generalize well across domains. In
contrast, features based on distributional represen-
tations (e.g., raw co-occurrence frequencies) can
be computed for every word, given that it occurs
in some unlabelled corpus. In addition to this ob-
vious advantage for out-of-domain settings, dis-
tributional representations can provide a more ro-
bust input signal to the classifier, for instance by
projecting a matrix of co-occurrence frequencies
to a lower-dimensional space. We hence hypoth-
esize that such features enable the model to be-
come more robust out-of-domain, while providing
higher precision in-domain.
Although simply including the components of
a word representation as features to a classifier
can lead to immediate improvements in SRL per-
formance, this observation seems in part counter-
intuitive. Just because one word has a specific
representation does not mean that it should be as-
signed a specific argument label. In fact, one
would expect a more complex interplay between
the representation of an argument a
i
and the con-
text it appears in. To model aspects of this inter-
play, we define an extended set of features that
408
further includes representations for the combina-
tion of p and a
i
, the set of words in the depen-
dency path between p and a
i
, and the set of words
in the full span of a
i
. We compute additive com-
positional representations of multiple words, us-
ing the simplest method of Mitchell and Lapata
(2010) where the composed representation is the
uniformly weighted sum of each single represen-
tation. Our full set of feature types based on distri-
butional word representations is listed in Table 1.
4 Experimental Setup
We evaluate the impact of different types of fea-
tures by performing experiments on a benchmark
dataset for semantic role labelling. To assess the
gains of distributional representations realistically,
we incorporate the features described in Section 3
into a state-of-the-art SRL system. The follow-
ing paragraphs summarize the details of our ex-
perimental setup.
Semantic Role Labeller. In all our experi-
ments, we use the publicly available system by
Bj?orkelund et al. (2010).
1
This system com-
bines the first-ranked SRL system and the first-
ranked syntactic parser in the CoNLL 2009 shared
task for English (Bj?orkelund et al., 2009; Bohnet,
2010). To the best of our knowledge, this
combination represents the current state-of-the-art
for semantic role labelling following the Prop-
Bank/NomBank paradigm (Palmer et al., 2005;
Meyers et al., 2004). To re-train and evaluate mod-
els with different feature sets, we use the same
training, development and test sets as provided
in the CoNLL shared task (Haji?c et al., 2009).
Although the employed system features a full
syntactic-semantic parsing pipeline, we only mod-
ify the feature sets of the two components directly
related to the actual role labelling task, namely ar-
gument identification and argument classification.
Word Representations. As a baseline, we sim-
ply added as features the word representations of
the predicate and argument head involved in a
classification decision (first two lines in Table 1).
We experimented with a range of publicly avail-
able sets of word representations, including em-
beddings from various neural language models
1
http://code.google.com/p/mate-tools/
2
http://metaoptimize.com/projects/wordreprs/
3
http://ai.stanford.edu/%7eehhuang/
4
http://lebret.ch/words/
5
http://www.cis.upenn.edu/%7eungar/eigenwords/
Development dims P R F
1
None ? 86.1 81.0 83.5
Brown clusters
2
320 86.2 81.3 83.7
Neural LM
2
50 86.2 81.4 83.7
Neural LM+Global
3
50 86.2 81.4 83.7
HLBL
2
50 86.3 81.3 83.7
H-PCA
4
50 86.2 81.3 83.7
Eigenwords
5
50 86.2 81.3 83.6
Table 2: Results on the CoNLL-2009 develop-
ment set, using off-the-shelf word representations
for predicates and argument as additional features.
Performance numbers in percent.
(Mnih and Hinton, 2009; Collobert et al., 2011;
Huang et al., 2012), eigenvectors (Dhillon et al.,
2011), Brown clusters (Brown et al., 1992), and
post-processed co-occurrence counts (Lebret and
Collobert, 2014). Results on the development set
for various off-the-shelf representations are shown
in Table 2. The numbers reveal that any kind of
word representation can be employed to improve
results. We choose to perform all follow-up exper-
iments using the 50-dimensional embeddings in-
duced by Turian et al. (2010), using the method by
Collobert et al., as they led to slightly better results
in F
1
-score than other representations. No signif-
icant differences were observed, however, using
other types of representations or vector sizes.
5 Results
We evaluate our proposed set of additional fea-
tures on the CoNLL-2009 in-domain and out-of-
domain test sets, using the aforementioned SRL
system and word representations. All results are
computed using the system?s built-in preprocess-
ing pipeline and re-trained models for argument
identification and classification. We report la-
belled precision, recall and semantic F1-score as
computed by the official scorer.
The upper part of Table 3 shows SRL perfor-
mance on the in-domain CoNLL-2009 test set,
with and without (Original) additional features
based on distributional representations. The re-
sults reveal that any type of additional feature
helps to improve precision and recall in this setting
(from 85.2% F
1
-score up to 85.5%), with signifi-
cant gains for 4 of the 5 additional features (com-
puted using a randomization test; cf. Yeh, 2000).
Interestingly, we find that the features do not seem
409
In-domain P R F
1
Original 87.4 83.1 85.2
Original + Argument 87.6 83.3 85.4**
Original + Predicate 87.4 83.2 85.2
Original + Interaction 87.5 83.3 85.3**
Original + Span 87.6 83.5 85.5**
Original + Path 87.5 83.4 85.4**
Original + All 87.6 83.4 85.5**
Out-of-domain P R F
1
Original 76.9 71.7 74.2
Original + Argument 77.4 71.9 74.5
Original + Predicate 77.3 72.2 74.7*
Original + Interaction 77.2 72.0 74.5
Original + Span 77.3 72.3 74.7*
Original + Path 77.2 72.3 74.7*
Original + All 77.5 73.0 75.2**
Table 3: Results on both CoNLL-2009 test sets.
All numbers in percent. Significant differences
from Original in terms of F
1
-score are marked by
asterisks (* p<0.05, ** p<0.01).
to have a cumulative effect here, as indicated by
the results with all features (+All, 85.5% F
1
). We
conjecture that this is due to the high volume of
existing in-domain training data, which renders
our full feature set redundant. To test this conjec-
ture, we further assess performance on the out-of-
domain test set of the CoNLL-2009 shared task.
The results for the out-of-domain experiment
are summarized in the lower part of Table 3.
We again observe that each single feature type
improves classification, with absolute gains be-
ing slightly higher than in the in-domain setting.
More interestingly though, we find that the com-
plete feature set boosts performance even further,
achieving an overall gain in precision and recall
of 0.6 and 1.3 percentage points, respectively. The
resulting F
1
-score of 75.2 lies even higher than the
top score for this particular data set reported in the
CoNLL shared task (Zhao et al., 2009; 74.6 F
1
).
We next investigate the benefits of compo-
sitional representations over features for single
words by assessing their impact on the overall re-
sult in an ablation study. Table 4 shows results
of ablation tests performed for the three composi-
tional feature types Interaction, Span and Path
on the out-of-domain test set. The results reveal
Out-of-domain P R F
1
Original 76.9 71.7 74.2
Full (Original+All) 77.5 73.0 75.2
Full ?Interaction 77.2 72.5 74.8
Full ?Span 77.2 72.3 74.7
Full ?Path 77.6 72.3 74.8
Table 4: Results of an ablation study over features
based on compositional representations. All num-
bers in percent.
a considerable loss in recall, indicating the impor-
tance of including compositional word represen-
tations and confirming our intuition that they can
provide additional gains over simple type-level
representations. In the next section, we discuss
this result in more detail and provide examples of
improved classification decisions.
6 Discussion
As a more detailed qualitative analysis, we exam-
ined the impact of word representations on SRL
performance with respect to different argument la-
bels and predicate types. Results on the in-domain
data set, shown in the upper part of Table 5, sug-
gest that most improvements in terms of preci-
sion are gained for verbal predicates, while nom-
inal predicates primarily benefit from higher re-
call. One reason for the latter observation might
be that arguments of nominal predicates are gen-
erally much harder to identify for the Original
model, as the cues provided by indicator features
on words and syntax are often inconclusive. For
verbal predicates, the word representations mainly
provide reinforcing signals to the classifier, im-
proving its precision at a slight cost of recall.
The results on the out-of-domain data set pro-
vide more insights regarding the suitability of
word representations for generalization. As shown
in the lower half of Table 5, the additional features
on average have a positive impact on precision and
recall. For verbal predicates, we observe only one
case, namely A0, in which improvements in recall
came with a decline in precision. Regarding nomi-
nal predicates, the trend is similar to what we have
seen in the in-domain setting, with most gains be-
ing achieved in terms of recall.
Apart from assessing quantitative effects, we
further examined cases that directly show the qual-
itative gains of the compositional features defined
410
Sentence with predicate and [gold argument
label
] Original Features required for correction
(1) He did not resent [their
A0
] supervision A1 Interaction
(2) [He
A1
] is getting plenty of rest no label Interaction, Path
(3) [He
A0
] rose late and went down to have breakfast. no label Path
(4) He was able to sit [for hours
AM-TMP
]. A2 Span
(5) Because he had spoken [too softly
AM-MNR
]. AM-TMP Span
Table 6: Example sentences in which distributional features compensated for errors made by Original.
In-domain verbal nominal
Label P R P R
A0 +0.4 +0.4 ?0.1 +2.4
A1 +0.2 ?0.4 +0.6 +1.5
A2 +1.7 ?1.5 ? +2.5
AM-ADV +0.8 +0.2 ?9.9 ?3.1
AM-DIS +0.3 ?3.2 ? ?
AM-LOC +0.8 +1.1 +0.6 +3.0
AM-MNR ?0.5 ?1.2 +2.7 +0.3
AM-TMP ?1.2 ?0.7 ?1.9 +3.3
Out-of-domain verbal nominal
Label P R P R
A0 ?0.9 +2.5 ?2.5 ?0.4
A1 +1.7 +0.8 +1.0 +3.7
A2 +1.4 +0.7 ?2.5 +3.2
AM-ADV +5.6 +0.7 ? ?
AM-DIS +7.3 ? ? ?
AM-LOC +0.7 +2.4 ? +15.0
AM-MNR +6.4 +10.5 +9.7 +10.7
AM-TMP +1.6 +1.8 ?6.7 +1.1
Table 5: Differences in precision and recall per
argument label and predicate word category. All
numbers represent absolute percentage points.
in Section 3. Table 6 lists examples from the
out-of-domain data set that were misclassified by
the Original model but could be correctly pre-
dicted using our enhanced feature set. As illus-
trated by Examples (1) and (2), the Interaction
feature seems to help recall by guiding classifica-
tion decisions towards more meaningful and com-
plete structures.
Improvements using the Path feature can be ob-
served in cases where nested syntactic structures
need to be processed, as required in Example (2).
In another instance, Example (3), the following
path is predicted between argument and predicate:
He
SBJ
?? rose
COORD
????and
CONJ
???went
OPRD
??? to
IM
??have.
Such cases are particularly problematic for the
Original model because long and potentially er-
roneous paths are sparse in the training data.
Further gains in performance are achieved using
the Span feature, which enables the model to bet-
ter handle infrequent and out-of-vocabulary words
occurring in an argument span, including ?hours?
and ?softly? in Example (4) and (5), respectively.
7 Conclusions
In this paper, we proposed to enhance the feature
space of a state-of-the-art semantic role labeller by
applying and composing distributional word rep-
resentations. Our results indicate that combining
such features with standard syntactic cues leads to
more precise and more robust models, with sig-
nificant improvements both in-domain and out-of-
domain. Ablation tests on an out-of-domain data
set have shown that gains in recall are mostly due
to features based on composed representations.
Given the novelty of these features for SRL, we
believe that this insight is remarkable and deserves
further investigation. In future work, we plan to
apply more sophisticated models of composition-
ality to better represent predicate-argument struc-
tures and to guide classification decisions towards
outcomes that are semantically more plausible.
We anticipate that this line of research will also be
of interest for a range of related tasks beyond tra-
ditional SRL, including predicate-argument struc-
ture alignment (Roth and Frank, 2012) and im-
plicit argument linking (Gerber and Chai, 2012).
Acknowledgements
This work has been supported by the FP7 Col-
laborative Project S-CASE (Grant Agreement No
610717) funded by the European Commission
(Michael Roth), and by EPSRC (EP/K017845/1)
in the framework of the CHIST-ERA READERS
project (Kristian Woodsend).
411
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 28?36, Sun-
tec, Singapore, August.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association
for Computational Linguistics.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43?48. Association for Computational Lin-
guistics.
Anders Bj?orkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syn-
tactic and semantic dependency parser. In Coling
2010: Demonstration Volume, pages 33?36, Beijing,
China, August. Coling 2010 Organizing Committee.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Jeju Island, Korea,
July. Association for Computational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain se-
mantic role labeling. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 237?246, Uppsala, Sweden, July.
Association for Computational Linguistics.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the
Latent Words Language Model. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 21?29, Singapore,
August.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
CCA. In Advances in Neural Information Process-
ing Systems, pages 199?207.
Hagen F?urstenau and Mirella Lapata. 2012. Semi-
supervised semantic role labeling via structural
alignment. Computational Linguistics, 38(1):135?
171.
Matthew Gerber and Joyce Chai. 2012. Semantic Role
Labeling of Implicit Arguments for Nominal Predi-
cates. Computational Linguistics, 38(4):755?798.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame
identification with distributed word representations.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics, pages
1448?1458, Baltimore, Maryland, June. Association
for Computational Linguistics.
Fei Huang and Alexander Yates. 2010. Open-domain
semantic role labeling by modeling word spans. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 968?
978, Uppsala, Sweden, July.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882, Jeju Island,
Korea, July.
R?emi Lebret and Ronan Collobert. 2014. Word em-
beddings through hellinger pca. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, pages
482?490, Gothenburg, Sweden, April. Association
for Computational Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The nombank project: An interim report. In
A. Meyers, editor, HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May.
412
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1429.
Andriy Mnih and Geoffrey Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
volume 21, pages 1081?1088.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Michael Roth and Anette Frank. 2012. Aligning
predicate argument structures in monolingual com-
parable texts: A new corpus for a new task. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics, pages 218?227,
Montreal, Canada, June.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211, Jeju Island, Korea, July. Association for
Computational Linguistics.
Ivan Titov and Alexandre Klementiev. 2011. A
bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1445?1455, Port-
land, Oregon, USA, June.
Ivan Titov and Alexandre Klementiev. 2012. Semi-
supervised semantic role labeling: Approaching
from an unsupervised perspective. In Proceedings
of COLING 2012, pages 2635?2652, Mumbai, In-
dia, December.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Alexander Yeh. 2000. More accurate tests for
the statistical significance of result differences.
In Proceedings of the 18th International Confer-
ence on Computational Linguistics, pages 947?953,
Saarbr?ucken, Germany, August.
Be?nat Zapirain, Eneko Agirre, Llu??s M`arquez, and Mi-
hai Surdeanu. 2013. Selectional preferences for se-
mantic role classification. Computational Linguis-
tics, 39(3):631?663.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 61?66, Boulder,
Colorado, USA, June.
413
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524?530,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Combining Word Patterns and Discourse Markers
for Paradigmatic Relation Classification
Michael Roth
ILCC, School of Informatics
University of Edinburgh
mroth@inf.ed.ac.uk
Sabine Schulte im Walde
Institut f?ur Maschinelle Sprachverarbeitung
Universit?at Stuttgart
schulte@ims.uni-stuttgart.de
Abstract
Distinguishing between paradigmatic rela-
tions such as synonymy, antonymy and hy-
pernymy is an important prerequisite in a
range of NLP applications. In this paper,
we explore discourse relations as an alter-
native set of features to lexico-syntactic
patterns. We demonstrate that statistics
over discourse relations, collected via ex-
plicit discourse markers as proxies, can be
utilized as salient indicators for paradig-
matic relations in multiple languages, out-
performing patterns in terms of recall and
F
1
-score. In addition, we observe that
markers and patterns provide complemen-
tary information, leading to significant
classification improvements when applied
in combination.
1 Introduction
Paradigmatic relations (such as synonymy,
antonymy and hypernymy; cf. Murphy, 2003) are
notoriously difficult to distinguish automatically,
as first-order co-occurrences of the related words
tend to be very similar across the relations. For
example, in The boy/girl/person loves/hates the
cat, the nominal co-hyponyms boy, girl and their
hypernym person as well as the verbal antonyms
love and hate occur in identical contexts, respec-
tively. Vector space models, which represent
words by frequencies of co-occurring words to
enable comparisons in terms of distributional
similarity (Sch?utze, 1992; Turney and Pantel,
2010), hence perform below their potential when
inferring the type of relation that holds between
two words. This distinction is crucial, however,
in a range of tasks: in sentiment analysis, for
example, words of the same and opposing polarity
need to be distinguished; in textual entailment,
systems further need to identify hypernymy
because of directional inference requirements.
Accordingly, while there is a rich tradition on
identifying word pairs of a single paradigmatic re-
lation, there is little work that has addressed the
distinction between two or more paradigmatic re-
lations (cf. Section 2 for details). In more gen-
eral terms, previous approaches to distinguish-
ing between several semantic relations have pre-
dominantly relied on manually created knowledge
sources, or lexico-syntactic patterns that can be
automatically extracted from text. Each option
comes with its own shortcomings: knowledge
bases, on the one hand, are typically developed for
a single language or domain, meaning that they
might not generalize well; word patterns, on the
other hand, are noisy and can be sparse for infre-
quent word pairs.
In this paper, we propose to strike a balance
between availability and restrictedness by mak-
ing use of discourse markers. This approach has
several advantages: markers are frequently found
across genres (Webber, 2009), they exist in many
languages (Jucker and Yiv, 1998), and capture
various semantic properties (Hutchinson, 2004).
We implement discourse markers within a vector
space model that aims to distinguish between the
three paradigmatic relations synonymy, antonymy
and hypernymy in German and in English, across
the three word classes of nouns, verbs, adjectives.
We examine the performance of discourse markers
as vector space dimensions in isolation and also
explore their contribution in combination with lex-
ical patterns.
2 Related Work
As mentioned above, there is a rich tradition of
research on identifying a single paradigmatic rela-
tions. Work on synonyms includes Edmonds and
Hirst (2002), who employed a co-occurrence net-
work and second-order co-occurrence, and Cur-
ran (2003), who explored word-based and syntax-
based co-occurrence for thesaurus construction.
524
Van der Plas and Tiedemann (2006) compared
a standard distributional approach against cross-
lingual alignment; Erk and Pad?o (2008) defined
a vector space model to identify synonyms and
the substitutability of verbs. Most computational
work on hypernyms was performed for nouns, cf.
the lexico-syntactic patterns by Hearst (1992) and
an extension of the patterns by dependency paths
(Snow et al, 2004). Weeds et al (2004), Lenci
and Benotto (2012) and Santus et al (2014) identi-
fied hypernyms in distributional spaces. Computa-
tional work on antonyms includes approaches that
tested the co-occurrence hypothesis (Charles and
Miller, 1989; Fellbaum, 1995), and approaches
driven by text understanding efforts and contradic-
tion frameworks (Harabagiu et al, 2006; Moham-
mad et al, 2008; de Marneffe et al, 2008).
Among the few approaches that distinguished
between paradigmatic semantic relations, Lin et al
(2003) used patterns and bilingual dictionaries to
retrieve distributionally similar words, and relied
on clear antonym patterns such as ?either X or Y?
in a post-processing step to distinguish synonyms
from antonyms. The study by Mohammad et al
(2013) on the identification and ranking of oppo-
sites also included synonym/antonym distinction.
Yih et al (2012) developed an LSA approach in-
corporating a thesaurus, to distinguish the same
two relations. Chang et al (2013) extended this
approach to induce vector representations that can
capture multiple relations. Whereas the above
mentioned approaches rely on additional knowl-
edge sources, Turney (2006) developed a corpus-
based approach to model relational similarity, ad-
dressing (among other tasks) the distinction be-
tween synonyms and antonyms. More recently,
Schulte im Walde and K?oper (2013) proposed to
distinguish between the three relations antonymy,
synonymy and hyponymy based on automatically
acquired word patterns.
Regarding pattern-based approaches to iden-
tify and distinguish lexical semantic relations in
more general terms, Hearst (1992) was the first
to propose lexico-syntactic patterns as empirical
pointers towards relation instances, focusing on
hyponymy. Girju et al (2003) applied a sin-
gle pattern to distinguish pairs of nouns that are
in a causal relationship from those that are not,
and Girju et al (2006) extended the work to-
wards part?whole relations, applying a super-
vised, knowledge-intensive approach. Chklovski
and Pantel (2004) were the first to apply pattern-
based relation extraction to verbs, distinguish-
ing five non-disjoint relations (similarity, strength,
antonymy, enablement, happens-before). Pantel
and Pennacchiotti (2006) developed Espresso, a
weakly-supervised system that exploits patterns in
large-scale web data to distinguish between five
noun-noun relations (hypernymy, meronymy, suc-
cession, reaction, production). Similarly to Girju
et al (2006), they used generic patterns, but relied
on a bootstrapping cycle combined with reliability
measures, rather than manual resources. Whereas
each of the aforementioned approaches considers
only one word class and clearly disjoint categories,
we distinguish between paradigmatic relations that
can be distributionally very similar and propose a
unified framework for nouns, verbs and adjectives.
3 Baseline Model and Data Set
The task addressed in this work is to distin-
guish between synonymy, antonymy and hyper-
nymy. As a starting point, we build on the ap-
proach and data set used by Schulte im Walde
and K?oper (2013, henceforth just S&K). In their
work, frequency statistics over automatically ac-
quired co-occurrence patterns were found to be
good indicators for the paradigmatic relation that
holds between two given words of the same word
class. They further experimented with refinements
of the vector space model, for example, by only
considering patterns of a specific length, weight-
ing by pointwise mutual information and applying
thresholds based on frequency and reliability.
Baseline Model. We re-implemented the best
model from S&K with the same setup: word pairs
are represented by vectors, with each entry corre-
sponding to one out of almost 100,000 patterns of
lemmatized word forms (e.g., X affect how
you Y ). Each value is calculated as the log fre-
quency of the corresponding pattern occurring be-
tween the word pairs in a corpus, based on exact
match. For English, we use the ukWaC corpus
(Baroni et al, 2009); for German, we rely on the
COW corpus instead of deWaC, as it is larger and
better balanced (Sch?afer and Bildhauer, 2012).
Data Set. The evaluation data set by S&K is a
collection of target and response words in Ger-
man that has been collected via Amazon Mechan-
ical Turk. The data contains a balanced amount
of instances across word categories and relations,
also taking into account corpus frequency, degree
of ambiguity and semantic classes. In total, the
525
S&K Reimplemented
P R F
1
P R F
1
Nouns
SYN?ANT 77.4 65.0 70.7 76.7 62.2 68.7
SYN?HYP 75.0 57.0 64.8 73.3 59.5 65.7
Verbs
SYN?ANT 70.6 40.0 51.1 84.6 36.7 51.2
SYN?HYP 42.0 26.7 32.6 52.6 33.3 40.8
Adjectives
SYN?ANT 88.9 66.7 76.2 94.1 66.7 78.0
SYN?HYP 68.4 54.2 60.5 65.0 54.2 59.1
Table 1: 2-way classification results by Schulte
im Walde and K?oper (2013) and our re-
implementation. All numbers in percent.
data set consists of 692 pairs of instances, dis-
tributed over three word classes (nouns, verbs,
adjectives) and three paradigmatic relations (syn-
onymy, antonymy, hypernymy).
Intermediate Evaluation. We compare our re-
implementation to the model by S&K using their
80% training and 20% test split, focusing on 2-
way classifications involving synonymy. The re-
sults, summarized in Table 1, confirm that our re-
implementation achieves similar results. Observed
differences are probably an effect of the distinct
corpora applied to induce patterns and counts.
We notice that the performance of both models
strongly depends on the affected pair of relations
and word category. For example, precision varies
in the 2-way classification between synonymy and
antonymy from 70.6% to 94.1%. Given the small
amount of test data, some of the 80/20 splits might
be better suited for the model than others. To avoid
resulting bias effects, we perform our final evalua-
tion using 5-fold cross-validation on a merged set
of all training and test instances. To illustrate the
performance of models in multiple languages, we
further conduct experiments on a data set for En-
glish relation pairs that has been collected by Giu-
lia Benotto and Alessandro Lenci, following the
same methodology as the German collection. The
English data set consists of 648 pairs of instances,
also distributed over nouns, verbs, adjectives, and
covering synonymy, antonymy, hypernymy.
4 Markers for Relation Classification
The aim of this work is to establish corpus statis-
tics over discourse relations as a salient source of
CONTRAST but, altough, rather . . .
RESTATEMENT indeed, specifically, . . .
INSTANTIATION (for) example, instance, . . .
Table 2: Examples of discourse relations/markers.
information to distinguish between paradigmatic
relations. Our approach is motivated by linguis-
tic studies that indicated a connection between dis-
course relations and lexical relations of words oc-
curring in the respective discourse segments: Mur-
phy et al (2009) have shown, for example, that
antonyms frequently serve as indicators for con-
trast relations in English and Swedish. More gen-
erally, pairs of word tokens have been identified as
strong features for classifying discourse relations
when no explicit discourse markers are available
(Pitler et al, 2009; Biran and McKeown, 2013).
Whereas word pairs have frequently been used
as features for disambiguating discourse relations,
to the best of our knowledge, our approach is novel
in that we are the first to apply discourse relations
as features for classifying lexical relations. One
reason for this might be that discourse relations in
general are only available in manually annotated
corpora. Previous work has shown, however, that
such relations can be classified reliably given the
presence of explicit discourse markers.
1
We hence
rely on such markers as proxies for discourse rela-
tions (for examples, cf. Table 2).
4.1 Model and Hypothesis
We propose a vector space model that represents
pairs of words using as features the discourse
markers that occur between them. The under-
lying hypothesis of this model is as follows: if
two phrases frequently co-occur with a specific
discourse marker, then the discourse relation ex-
pressed by the corresponding marker should also
indicate the relation between the words in the af-
fected phrases. Following this hypothesis, contrast
relations might indicate antonymy, whereas elab-
orations may indicate synonymy or hyponymy.
Although such relations will not hold between
every pair of words in two connected discourse
segments, we hypothesize that correct instances
(of all considered word classes) can be identified
based on high relative frequency.
In our model, frequency statistics are com-
puted over sentence-internal co-occurrences of
1
Pitler et al (2008) report an accuracy of up to 93%.
526
word pairs and discourse markers. Since discourse
relations are typically directed, we take into con-
sideration whether a word occurs to the left or
to the right of the respective marker. Accord-
ingly, the features of our model are special cases of
single-word patterns with an arbitrary number of
wild card tokens (e.g., the marker feature ?though?
corresponds to the pattern ?X ? though ? Y ?).
Yet, our specific choice of features has several ad-
vantages: Whereas strict and potentially long pat-
terns can be rare in text, discourse markers such as
?however?, ?for example? and ?additionally? are
frequently found across genres (Webber, 2009).
Although combinations of tokens could also be re-
placed by wild cards in any automatically acquired
pattern, this would generally lead to an exponen-
tially growing feature space. In contrast, the set
of discourse markers in our work is fixed: for En-
glish, we use 61 markers annotated in the Penn
Discourse TreeBank 2.0 (Prasad et al, 2008); for
German, we use 155 one-word translations of the
English markers, as obtained from an online dic-
tionary.
23
Taking directionality into account, our
vector space model consists of 2x61 and 2x155
features, respectively.
4.2 Development Set and Hyperparameters
We select the hyperparameters of our model using
an independent development set, which we extract
from the lexical resource GermaNet (Hamp and
Feldweg, 1997). For each considered word cate-
gory, we extract instances of synonymy, antonymy
and hypernymy. In total, 1502 instances are iden-
tified, with 64 of them overlapping with the evalu-
ation data set described in Section 3. Note though
that the development set is not used for evaluation
but only to select the following hyperparameters.
We experimented with different vector values
(absolute frequency, log frequency, pointwise mu-
tual information (PMI)), distance measures (co-
sine, euclidean) and normalization schemes. In
contrast to S&K, who did not observe any im-
provements using PMI, we found it to perform
best, combined with euclidean distance and no
additional normalization. This finding might be
an immediate effect of discourse markers being
2
http://dict.leo.org
3
We also experimented with larger sets of markers, in-
cluding conjunctions and adverbials in sentence-initial posi-
tions, but did not notice any considerable effect. Future work
could use manual sets of markers, e.g. those by Pasch et al
(2003), though such sets are only available in few languages.
generally more frequent than strict word patterns,
which also leads to more reliable PMI values.
5 Evaluation
In our evaluation, we assess the performance of the
marker-based model and demonstrate the benefits
of incorporating discourse markers into a pattern-
based model, which we apply as a baseline. We
evaluate on several data sets: the collection of
target-response pairs in German from previous
work, and a similar data set that was collected for
English target words (cf. Section 3); for compari-
son reasons, we also apply our models to the bal-
anced data set of related and unrelated noun pairs
by Yap and Baldwin (2009).
4
We perform 3-way
and 2-way relation classification experiments, us-
ing 5-fold cross-validation and a nearest centroid
classifier (as applied by S&K).
Results. The 3-way classification results of the
baseline and our marker-based model are summa-
rized in Table 3, with best results for each set-
ting marked in bold. On the German data set,
our model always outperforms a random baseline
(33% F
1
-score). The results on the English data
set are overall a bit lower, possibly due to corpus
size. In almost all classification tasks, our marker-
based model achieves a higher recall and F
1
-score
than the pattern-based approach. The precision
results of the marker-based model are overall be-
low the pattern-based model. This drop in perfor-
mance does not come as a surprise though, con-
sidering that the model only makes use of 122 and
310 features, in comparison to tens of thousands
of features in the pattern approach.
A randomized significance test over classified
instances (cf. Yeh, 2000) revealed that only two
differences in results are significant. We hypoth-
esize that one reason for this outcome might be
that both models cover complementary sets of in-
stances. To verify this hypothesis, we apply a
combined model, which is based on a weighted
linear combination of distances computed by the
two individual models.
5
As displayed in Table 3,
this combined model yields further improvements
4
Note that we could, in principle, also apply our models to
unbalanced data. Our main focus lies however on examining
the direct impact of different feature sets. We hence decided
to keep the evaluation setup simple and used a classifier that
does not take into account class frequency.
5
We determined the best weights on the development set
and found these to be 0.9 and 0.1 for the output of the pattern-
based and marker-based model, respectively.
527
Nouns Verbs Adjectives
P R F
1
P R F
1
P R F
1
G
e
r
m
a
n
Patterns 55.6 40.8 47.0 55.6 35.6 43.4 53.5 41.1 46.5
Markers 42.6 38.7 40.5 48.4 46.2** 47.3 51.1 48.6 49.9
Combined 50.4 45.7* 48.0 52.6 50.2** 51.4** 53.4 50.8** 52.1
E
n
g
l
i
s
h Patterns 46.4 28.0 34.9 44.7 28.5 34.8 56.6 32.1 41.0
Markers 39.0 34.3 36.5 38.3 36.3 37.2 50.0 41.2** 45.2
Combined 43.0 37.8** 40.3* 41.8 39.6** 40.7* 53.5 44.4** 48.5**
Table 3: 3-way classification results using 5-fold cross-validation. All numbers in percent. Asterisks
indicate significant differences to the pattern-based baseline model (* p<0.10, ** p<0.05).
Combined
model
German English
P R F
1
P R F
1
Nouns
SYN?ANT 61.7 55.7 58.5 52.9 44.2 48.2
SYN?HYP 66.5 60.4 63.3 62.2 58.6 60.4
ANT?HYP 70.9 64.6 67.6 59.1 50.6 54.5
Verbs
SYN?ANT 58.9 55.0 56.8 49.6 45.8 47.6
SYN?HYP 67.6 64.0 65.8 66.4 63.0 64.7
ANT?HYP 67.3 66.4 66.9 62.9 60.7 61.8
Adjectives
SYN?ANT 74.8 69.4 72.0 67.0 56.6 61.3
SYN?HYP 58.0 56.1 57.0 56.4 46.0 50.7
ANT?HYP 73.7 70.7 72.2 69.8 57.8 63.2
Table 4: 2-way results of the combined model.
Bold numbers indicate improvements over both
individual models. All numbers in percent.
in recall and F
1
-score, leading to the best 3-way
classification results. All gains in recall are sig-
nificant, confirming that the single models in-
deed contribute complementary information. For
example, only the pattern-based model classifies
?intentional???accidental? as antonyms, and only
the marker-based model predicts the correct rela-
tion for ?double???multiple? (hypernymy). The
combined model classifies both pairs correctly.
Table 4 further assesses the strength of the com-
bined model on the 2-way classifications. The
table highlights results indicating improvements
over both individual models. We observe that the
combined model achieves the best recall and F
1
-
score in 15 out of 18 cases.
Relation SYN ANT HYP
Patterns 0.97 0.97 0.94
Markers 0.77* 0.82* 0.91*
Combined 0.93* 0.98 0.96*
Table 5: Results in F
1
-score on the balanced data
set by Yap and Baldwin (* p<0.05).
A final experiment is performed on the data set
by Yap and Baldwin (2009) to see whether our
models can also distinguish word pairs of individ-
ual relations from unrelated pairs of words. The
results, listed in Table 5, show that the marker-
based model cannot perform this task as well as
the pattern-based model. The combined model,
however, outperforms both individual models in 2
out of 3 cases. Despite their simplicity, our models
achieve results close to the F
1
-scores reported by
Yap and Baldwin (0.98?0.99), who employed syn-
tactic pre-processing and an SVM-based classifier,
and experimented with different corpora.
6 Conclusions
In this paper, we proposed to use discourse mark-
ers as indicators for paradigmatic relations be-
tween words and demonstrated that a small set
of such markers can achieve higher recall than a
pattern-based model with tens of thousands of fea-
tures. Combining patterns and markers can further
improve results, leading to significant gains in re-
call and F
1
. As our new model only relies on a raw
corpus and a fixed list of discourse markers, it can
easily be extended to other languages.
Acknowledgments
The research presented in this paper was funded
by the DFG grant SCHU-2580/2-1 and the DFG
Heiselberg Fellowship SCHU-2580/1-1.
528
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209?226.
Or Biran and Kathleen McKeown. 2013. Aggre-
gated word pair features for implicit discourse rela-
tion disambiguation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 69?73,
Sofia, Bulgaria, August.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Walter G. Charles and George A. Miller. 1989. Con-
texts of antonymous adjectives. Applied Psycholin-
guistics, 10(3):357?375.
Tim Chklovski and Patrick Pantel. 2004. VerbOcean:
Mining the Web for fine-grained semantic verb re-
lations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Process-
ing, Barcelona, Spain, 25?26 July 2004, pages 33?
40.
James Curran. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, Institute for Communica-
tion and Collaborative Systems, School of Informat-
ics, University of Edinburgh.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contra-
dictions in text. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1039?1047, Columbus, Ohio, USA.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Lin-
guistics, 28(2):105?144.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25-27 October 2008.
Christiane Fellbaum. 1995. Co-occurrence and
antonymy. International Journal of Lexicography,
8(4):281?303.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Edmonton, Al-
berta, Canada, 27 May ?1 June 2003, pages 80?87.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a lexical-semantic net for German. In Proceedings
of the Workshop on Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications at ACL/EACL-97, Madrid, Spain,
12 July 1997, pages 9?15.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text
processing. In In Proceedings of the 21st National
Conference on Artificial Intelligence, pages 755?
762.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 15th International Conference on Computa-
tional Linguistics, Nantes, France, 23-28 August
1992, pages 539?545.
Ben Hutchinson. 2004. Acquiring the meaning of dis-
course markers. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, 21?26 July 2004, pages
685?692.
Andreas H. Jucker and Zael Yiv, editors. 1998. Dis-
course Markers: Descriptions and Theory, vol-
ume 57 of Discourse & Beyond New Series. John
Benjamin Publishing Company.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantic, pages 75?79.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of the 18th
International Joint Conference on Artificial Intelli-
gence, pages 1492?1493. Morgan Kaufmann Pub-
lishers Inc.
Saif M. Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 982?991,
Honolulu, Hawaii, USA.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics, 39(3):555?590.
M. Lynne Murphy, Carita Paradis, Caroline Will-
ners, and Steven Jones. 2009. Discourse func-
tions of antonymy: A cross-linguistic investigation
of Swedish and English. Journal of Pragmatics,
41(11):2159?2184.
M. Lynne Murphy. 2003. Semantic relations and the
lexicon. Cambridge University Press.
529
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, 17?21 July 2006, pages 113?120.
Renate Pasch, Ursula Brausse, Eva Breindl, and Ulrich
Wassner. 2003. Handbuch der deutschen Konnek-
toren. Walter de Gruyter, Berlin.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 186?195, Honolulu, Hawaii, October.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 683?691,
Suntec, Singapore, August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC-2008), Marrakesh, Marocco, May.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine
Schulte im Walde. 2014. Chasing hypernyms in
vector spaces with entropy. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 38?42, Gothenburg, Sweden.
Roland Sch?afer and Felix Bildhauer. 2012. Build-
ing large corpora from the web using a new effi-
cient tool chain. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC-2012), pages 486?493, Istanbul,
Turkey, May.
Sabine Schulte im Walde and Maximilian K?oper. 2013.
Pattern-based distinction of paradigmatic relations
for German nouns, verbs, adjectives. In Language
Processing and Knowledge in the Web, pages 184?
198. Springer.
Hinrich Sch?utze. 1992. Dimensions of meaning. In In
Proceedings of Supercomputing, pages 787?796.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems, volume 17, pages 1297?1304.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Lonneke Van der Plas and J?org Tiedemann. 2006.
Finding synonyms using automatic word alignment
and measures of distributional similarity. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 866?873.
Bonnie Webber. 2009. Genre distinctions for dis-
course in the Penn TreeBank. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
674?682, Suntec, Singapore, August.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference on Computational Linguistics, pages
1015?1021.
Willy Yap and Timothy Baldwin. 2009. Experiments
on pattern-based relation learning. In Proceedings
of the 18th ACM Conference on Information and
Knowledge Management, pages 1657?1660.
Alexander Yeh. 2000. More accurate tests for
the statistical significance of result differences.
In Proceedings of the 18th International Confer-
ence on Computational Linguistics, pages 947?953,
Saarbr?ucken, Germany, August.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1212?1222, Jeju Island, Korea, July.
530
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 218?227,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Aligning Predicate Argument Structures in Monolingual Comparable Texts:
A New Corpus for a New Task
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University
Germany
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Discourse coherence is an important aspect of
natural language that is still understudied in
computational linguistics. Our aim is to learn
factors that constitute coherent discourse from
data, with a focus on how to realize predicate-
argument structures (PAS) in a model that ex-
ceeds the sentence level. In particular, we aim
to study the case of non-realized arguments
as a coherence inducing factor. This task can
be broken down into two subtasks. The first
aligns predicates across comparable texts, ad-
mitting partial argument structure correspon-
dence. The resulting alignments and their con-
texts can then be used for developing a coher-
ence model for argument realization.
This paper introduces a large corpus of com-
parable monolingual texts as a prerequisite for
approaching this task, including an evaluation
set with manual predicate alignments. We il-
lustrate the potential of this new resource for
the empirical investigation of discourse coher-
ence phenomena. Initial experiments on the
task of predicting predicate alignments across
text pairs show promising results. Our findings
establish that manual and automatic predicate
alignments across texts are feasible and that
our data set holds potential for empirical re-
search into a variety of discourse-related tasks.
1 Introduction
Research in the fields of discourse and pragmatics
has led to a number of theories that try to explain and
formalize the effect of discourse coherence induc-
ing elements either locally or globally. For exam-
ple, Centering Theory (Grosz et al, 1995) provides
a framework to model local coherence by relating
the choice of referring expressions to the salience of
an entity at certain stages of a discourse. An exam-
ple for a global coherence model would be Rhetori-
cal Structure Theory (Mann and Thompson, 1988),
which addresses overall text structure by means of
coherence relations between the parts of a text.
In addition to such theories, computational ap-
proaches have been proposed to capture correspond-
ing phenomena empirically. A prominent example
is the entity-based model by Barzilay and Lapata
(2008). In their approach, local coherence is mod-
eled by the observation of sentence-to-sentence re-
alization patterns of individual entities. The learned
model reflects a key idea from Centering Theory,
namely that adjacent sentences in a coherent dis-
course are likely to involve the same entities.
One shortcoming of Barzilay and Lapata?s model
(and extensions of it) is that it only investigates overt
realization patterns in terms of grammatical func-
tions. These functions reflect explicit realizations of
predicate argument structures (PAS), but they do not
capture the full range of salience factors. In partic-
ular, the model does not reflect the importance of
discourse entities that fill core roles of the predicate,
but that remain implicit in the predicate?s local argu-
ment structure. We develop a specific set-up that al-
lows us to further investigate the factors that govern
such a null-instantiations of argument positions (cf.
Fillmore et al (2003)), as a special form of coher-
ence inducing element in discourse. We henceforth
refer to such cases as non-realized arguments.
Our main hypothesis is that context specific re-
alization patterns for PAS can be automatically
218
learned from a semantically parsed corpus of com-
parable text pairs. This assumption builds on
the success of previous research, where compara-
ble and parallel texts have been exploited for a
range of related learning tasks, e.g., unsupervised
discourse segmentation (Barzilay and Lee, 2004)
and bootstrapping semantic analyzers (Titov and
Kozhevnikov, 2010).
For our purposes, we are interested in finding cor-
responding PAS across comparable texts that are
known to talk about the same events, and hence in-
volve the same set of underlying event participants.
By aligning predicates in such texts, we can investi-
gate the factors that determine discourse coherence
in the realization patterns for the involved partici-
pants. As a first step towards this overall goal, we
describe the construction of a resource that contains
more than 160,000 document pairs that are known to
talk about the same events and participants. Exam-
ple (1), extracted from our corpus of aligned texts,
illustrates this point: Both texts report on the same
event, in particular the (aligned) event of locating
victims in an avalanche. While (1.a) explicitly talks
about the location of this event, the role remains im-
plicit in the second sentence of (1.b), given that it
can be recovered from the preceding sentence. In
fact, realization of this argument would impede the
fluency of discourse by being overly repetitive.
(1) a. . . . The official said that [no bodies]Arg1 had
been recovered [from the avalanches]Arg2 which
occurred late Friday in the Central Asian coun-
try near the Afghan border some 300 kilometers
(185 miles) southeast of the capital Dushanbe.
b. Three other victims were trapped in an
avalanche in the village of Khichikh. [None
of the victims bodies]Arg1 have been found
[ ]Argm-loc.
Our aim is to identify comparable predications
across pairs of texts, and to study the coherence
factors that determine the realization patterns of ar-
gument structures (including roles that remain im-
plicit) in discourse. This can be achieved by consid-
ering the full set of arguments that can be recovered
from the aligned predications, including both core
and non-core (i.e. adjunct) roles. However, in order
to relate PAS across texts to one another, we first
need to identify corresponding predicates.
In this paper, we construct a large data set to be
used for the induction of a coherence model for ar-
gument structure realization and related tasks. We
discuss the prospects of this data set for the study
of coherence factors in PAS realization. Finally, we
present first results on the initial task of predicate
alignment across comparable monolingual texts.
The remainder of this paper is structured as fol-
lows: In Section 2, we discuss previous work in re-
lated tasks. Section 3 introduces the new task to-
gether with a description of how we prepared a suit-
able data set. Section 4 discusses the potential bene-
fits of the created resource in more detail. Section 5
presents experiments on predicate alignment using
this new data set and outlines first results. Finally,
we conclude in Section 6 and discuss future work.
2 Related Work
Data sets comprising parallel texts have been re-
leased for various different tasks, including para-
phrase extraction and statistical machine translation
(SMT). While corpora for SMT are typically mul-
tilingual (e.g. Europarl, Koehn (2005)), there also
exist monolingual parallel corpora that consist of
multiple translations of one text into the same lan-
guage (Barzilay and McKeown, 2001; Huang et
al., 2002, inter alia). Each translation can pro-
vide alternative verbalizations of the same events
but little variation can be observed in context, as
the overall discourse remains the same. A higher
degree of variation can be found in the Microsoft
Research Paraphrase Corpus (e.g. MSRPC, Dolan
and Brockett (2005)), which consists of paraphrases
automatically extracted from different sources. In
the MSRPC, however, original discourse contexts
are not provided for each sentence. In contrast to
truly parallel monolingual corpora, there also exist
a range of comparable corpora that have been used
for tasks such as (multi-document) summarization
(McKeown and Radev, 1995, inter alia). Corpora for
this task are collected manually and hence are rather
small. Our work presents a method to automatically
construct a large corpus of text pairs describing the
same underlying events.
In this novel corpus, we identify common events
across texts and investigate the argument structures
that were realized in each context to establish a co-
219
herent discourse. Different aspects related to this
setting have been studied in previous work. For ex-
ample, Filippova and Strube (2007) and Cahill and
Riester (2009) examine factors that determine con-
stituent order and Belz et al (2009) study the con-
ditions for the use of different types of referring ex-
pressions. The specific set-up we examine allows
us to further investigate the factors that govern the
non-realization of an argument position, as a special
form of coherence inducing element in discourse.
As in the aforementioned work, we are specifically
interested in the generation of coherent discourses
(e.g. for summarization). Yet, our work also com-
plements research in discourse analysis. A recent
example for such work is the Semeval 2010 Task 10
(Ruppenhofer et al, 2010), which aims at linking
events and their participants in discourse. The pro-
vided data sets for this task, however, are critically
small (438 train and 525 test sentences). Eventu-
ally, the corpus we present in this paper could also
be beneficial for data-driven approaches to role link-
ing in discourse.
3 A Corpus for Aligning Predications
across Comparable Texts
Our aim is to construct a corpus of comparable texts
that can be assumed to be about the same events,
but include variation in textual presentation. This re-
quirement fits well with the news domain, for which
we can trace varying textual sources for the same
underlying events.
The English Gigaword Fifth Edition (Parker et al,
2011) corpus (henceforth just Gigaword) is one of
the largest corpus collections for English. It com-
prises a total of 9.8 million newswire articles from
seven distinct sources. For construction of our cor-
pus we make use of all combinations of agency pairs
in Gigaword.
3.1 Corpus Creation
In order to extract pairs of articles describing the
same news event, we implemented the pairwise sim-
ilarity method presented by Wubben et al (2009).
The method is based on measuring word overlap in
news headlines, weighting each word by its TF*IDF
score to give a higher impact to words occurring
with lower frequency. As our focus is to provide
a high-quality data set for predicate alignment and
follow-up tasks, we impose an additional date con-
straint to favor precision over recall. We apply this
constraint by requiring a pair of articles to be pub-
lished within a two-day time frame in order to be
considered as pairs of comparable news items.
Following this two-step procedure, we extracted a
total of 167,728 document pairs, an overall collec-
tion of 50 million word tokens. We inspected about
100 randomly selected document pairs and found
only two of them describing different events. This
is in line with the results of Wubben et al who re-
ported a precision of 93% without explicitly impos-
ing a date constraint. Overall, we found that most
text pairs share a high degree of similarity and vary
only in length (up to 7.564 words with a mean and
median of 301 and 213 words, respectively) and de-
tail. Closer examination of a development set of
10 document pairs (described below) revealed that
we can indeed find multiple cases where roles are
not locally filled in predicate argument structures.
We show instances of this phenomenon, in which
aligned PAS help to resolve implicit role references,
in Section 4.
3.2 Gold Standard Annotation
We pre-processed all texts using MATE tools
(Bohnet, 2010; Bjo?rkelund et al, 2010), a pipeline
of natural language processing modules including a
state-of-the-art semantic role labeler that computes
Prop/NomBank annotations (Palmer et al, 2005;
Meyers et al, 2008). The output was used to provide
pre-labeled verbal and nominal predicates for anno-
tation. We asked two students1 to tag alignments
of corresponding predicates in 70 text pairs derived
from the created corpus. All document pairs were
randomly chosen from the AFP and APW sections
of Gigaword with the constraint that each text con-
sists of 100 to 300 words2. We chose this constraint
as longer text pairs contain a high number of unre-
lated predicates, making this task difficult to manage
for the annotators.
Sure and possible links. Following standard prac-
tice in word alignment tasks (cf. Cohn et al (2008))
1Both annotators are students in Computational Linguistics,
one undergraduate (A) and one postgraduate (B) student.
2This constraint is satisfied by 75.3% of the documents.
220
the annotators were instructed to distinguish be-
tween sure (S) and possible (P) alignments, depend-
ing on how certainly, in their opinion, two predi-
cates (including their arguments) describe the same
event. The following examples show cases of predi-
cate pairings marked as sure (S link) (2) and as pos-
sible (P link) alignments (3):
(2) a. The regulator ruled on September 27 that Nas-
daq too was qualified to bid for OMX [. . . ]3
b. The authority [. . . ] had already approved a sim-
ilar application by Nasdaq.4
(3) a. Myanmar?s military government said earlier this
year it has released some 220 political prisoners
[. . . ]5
b. The government has been regularly releasing
members of Suu Kyi?s National League for
Democracy party [. . . ]6
Replaceability. As a guideline for deciding
whether two predicates are to be aligned, the
annotators were given the following two criteria: 1)
whether the predicates are replaceable in a given
context and 2) whether they share (potentially
implicit) arguments.
Missing context. In case one text does not provide
enough context to decide whether two predicates in
the paired documents refer to the same event, an
alignment should not be marked as sure.
Similar predicates. Annotators were told explic-
itly that sure links can be used even if two predicates
are semantically different but have the same mean-
ing in context. Example (4) illustrates such a case:
(4) a. The volcano roared back to life two weeks ago.
b. It began erupting last month.
1-to-1 vs. n-to-m. We asked the annotators to find
as many 1-to-1 correspondences as possible and to
prefer 1-to-1 matches over n-to-m alignments. In
case of multiple mentions (cf. Example (5)) of the
same event, we further asked the annotators to pro-
vide only one S link per predicate and mark remain-
ing cases as P links. If possible, the S link should
3Source document ID: AFP ENG 20071112.0235
4Source document ID: APW ENG 20071112.0645
5Source document ID: AFP ENG 20020301.0041
6Source document ID: APW ENG 20020301.0132
be used for the pairing of PAS with the highest in-
formation overlap (e.g. ?performa3???performb2? in
(5)). If there is no difference in information over-
lap, the predicate pair that occurs first in both texts
should be marked as a sure alignment (e.g. ?singa1??
?performb1? in (5)). The intuition behind this guide-
line is that the first mention introduces the actual
event while later mentions just (co-)refer or add fur-
ther information.
(5) a. Susan Boyle said she will singa1 in front of
Britain?s Prince Charles (. . . ) ?It?s going to be
a privilege to be performinga2 before His Royal
Highness,? the singer said (. . . ) British copy-
right laws will allow her to performa3 the hit in
front of the prince and his wife.7
b. British singing sensation Susan Boyle is going
to performb1 for Prince Charles (. . . ) The show
star will performb2 her version of Perfect Day
for Charles and his wife Camilla.8
3.3 Development and Evaluation Data Sets
In total, the annotators (A/B) aligned 487/451 sure
and 221/180 possible alignments with a Kappa score
(Cohen, 1960) of 0.86. Following Brockett (2007),
we computed agreement on labeled annotations, in-
cluding unaligned predicate pairs as an additional
null category. For the construction of a gold stan-
dard, we merged the alignments from both annota-
tors by taking the union of all possible alignments
and the intersection of all sure alignments. Cases
which involved a sure alignment on which the anno-
tators disagreed were resolved in a group discussion
with the first author. We split the final corpus into a
development set of 10 document pairs and a test set
of 60 document pairs.
Table 1 summarizes information about the result-
ing annotations in the development and test sets,
respectively. It gives information about the paired
texts (PT): number of predicates marked in prepro-
cessing (nouns and verbs), the set of manual predi-
cate alignments (PA): sure and possible, as well as
information about whether they were annotated for
predicates of the same PoS (N,V) or lemma.
Finally, as a rough indicator for diverging ar-
gument structures captured in the annotated align-
7Source document ID: AFP ENG 20101102.0028
8Source document ID: APW ENG 20101102.0923
221
Dev Set Test Set
nb. of PT 10 60
nb. marked predicates 395 3,453
nb. marked nouns 168 1,531
nb. marked verbs 227 1,922
sure PA/PT: avg. (total) 3.9 (35) 7.4 (446)
poss. PA/PT: avg. (total) 4.8 (43) 6.0 (361)
same PoS in PA (N/V) 88.5% (24/42) 82.4% (242/423)
same lemma in PA 53.8% (42) 47.5% (383)
unequal nb. args in PA 30.8% (24) 39.7% (320)
Table 1: Information on Paired Texts (PT) and manual
Predicate Alignments (PA) in development and test set
ments, we analyzed the number of PAs that involve
a different number of arguments.
4 Potential of Aggregation
In this section, we analyze the predicate alignments
in our manually annotated data set, to illustrate the
potential of aggregating corresponding PAS across
comparable texts.
We are particularly interested in cases of non-
realization of arguments, and thus take a closer look
at alignments involving roles that are not filled in
their local PAS. We extract a subset of such cases
by extracting pairs of aligned predicates that con-
tain a different number of realized arguments. We
deliberately focus on the more restricted core roles
in this exposition, but will consider the full range
of roles for developing a comprehensive coherence
model for argument structure realization.9 Our se-
lection of alignment examples is drawn from the de-
velopment set.
The following excerpts are from a pair of com-
parable texts describing a news report on Chadian
refugees crossing into Nigeria:
(6) a. The Chadians said [they]Arg0 had fled [ ]Arg1 in
fear of their lives.10
b. The United Nations says
[some 20,000 refugees]Arg0 have fled
[into Cameroon]Arg1.11
In both examples, the Arg0 role of the predicate fled
is filled, but Arg1 has not been realized in (6.a). Note
9Accordingly, the number of PAs involving diverging role
realizations in Table 1 is strongly underestimated.
10Source document ID: AFP ENG 20080205.0230
11Source document ID: APW ENG 20080206.0766
that the sentence is still part of a coherent discourse
as fillers for the omitted role can be inferred from
the preceding discourse context. Aggregating the
aligned PAS presents an effective means to identify
such appropriate fillers.
Example (7) presents another text pair, reporting
on elections in Iraq, in which role realizations differ
for the same hold event.
(7) a. He said (. . . ) [elections]Arg1 will be held [ ]Arg0
to form a government.12
b. The president (. . . ) said Wednesday
[his country]Arg0 will definitely hold
[elections]Arg1 in 2004.13
Here, the changes in argument realization go
along with a diathesis alternation, while the pair in
(6) exemplifies a case of lexical licensing for omis-
sion of a role.14
Example (8.b) illustrates a case in which the Arg1
of a decline event is involved in a preceding pred-
ication (rise) and thus has already been overtly re-
alized. The constructional properties of the subse-
quent predicates decline as a participle and noun, re-
spectively, are more adverse to overt realization of
the Arg1 role. Suppression of Arg1 in such cases
yields a much more coherent discourse as compared
to their realization. This is brought out by the con-
structed examples in (a?/b?), which are both highly
repetitive.
(8) a. The closely watched [index]Arg1 rose to 93.7
. . . after declining for . . . months.15
a?. ? . . . after the index declining for . . . months.
b. Consumer confidence rose . . . following three
months of dramatic decline [ ]Arg1.16
b?. ? . . . following three months of dramatic decline
[of consumer confidence]Arg1.
As showcased by the previous examples, the de-
cision on whether to realize a role filler in a lo-
cal PAS can be rather complex. Obviously, the
12Source document ID: AFP ENG 20031015.0353
13Source document ID: APW ENG 20031015.0236
14These different configurations are termed constructional
vs. lexical licensors in the SemEval 2010 Task 10 (Ruppen-
hofer et al, 2010).
15Source document ID: AFP ENG 20011228.0365
16Source document ID: APW ENG 20011228.0572
222
Figure 1: The predicates of two sentences (white: ?The company has said it plans to restate its earnings for 2000
through 2002.?; gray: ?The company had announced in January that it would have to restate earnings (. . . )?) from the
Microsoft Research Paragraph Corpus are aligned by computing clusters with minimum cuts.
above instances do not provide exhaustive informa-
tion for grounding all such decisions. A comprehen-
sive model of discourse coherence will need to esti-
mate the argument realization potential of different
predicates and roles from larger corpora. But as can
be seen from the discussed examples, training a se-
mantic model with suitable discourse features on all
predicate argument structures in a large corpus such
as ours will provide indicative range of realization
decisions.
5 Experiments
This section presents an initial experiment using an
unsupervised graph-based clustering method for the
task of aligning predicates across comparable texts.
We describe the alignment model, two baselines as
well as the experimental setting and results.17
5.1 Clustering Model
Similarity Measures. We define a number of sim-
ilarity measures between predicates, which make
use of complementary lexical information. One
source of information are token-based frequency
counts, which we compute over all documents
from the AFP and APW sections of Gigaword18.
Given two lemmatized predicates and their respec-
tive PAS, we employ the following four similarity
17The technicalities of this model, including detailed def-
initions of the similarity measures, are described elsewhere
(manuscript, under submission).
18These sections make up 56.6% of documents in Gigaword.
measures: Similarity in WordNet (simWN) and Verb-
Net (simVN), distributional similarity (simDist) and
bag-of-word similarity of arguments (simArgs). The
first three measures are type-based, whereas the lat-
ter is token-based.
Graph Representation. The input for graph clus-
tering is a bi-partite graph representation for pairs
of texts to be predicate-aligned. In this graph, each
node represents a PAS that was assigned during pre-
processing (cf. Section 3). Edges are inserted be-
tween pairs of predicates that are from two distinct
texts. A weight is assigned to each edge by a com-
bination of the introduced similarity measures.
Clustering algorithm. The graph clustering
method uses minimum cuts (or Mincuts) in order
to partition the bipartite text graph into clusters of
aligned predicates. Each Mincut operation divides
a graph into two disjoint sub-graphs, such that the
sum of weights of removed edges will be minimal.
As the goal is to induce clusters consisting of pairs
of similar predicates, a maximum number of two
nodes per cluster is set as stopping criterion. We
apply Mincut recursively to the input graph and
resulting sub-graphs until we reach the stopping
criterion. Figure 1 shows an example of a graph
clustered by the Mincut approach.
5.2 Setting
We perform evaluations of the graph-based align-
ment model (henceforth called Clustering) on the
223
task of inducing predicate alignments across com-
parable monolingual texts. We evaluate on the man-
ually annotated gold alignments in the test data set
described in Section 3.2.
Parameter Tuning. As the graph representation
becomes rather inefficient to handle using edges be-
tween all predicate pairs, we use the development
set of 10 text pairs to estimate a threshold for adding
edges. We found the best similarity threshold to be
an edge weight of 2.5. Note that the edge weights are
calculated as a weighted linear combination of four
different similarity measures. Subsequently, we also
tune the weighting scheme for similarity measures
on the development set. We found the best perform-
ing combination of weights to be 0.09, 0.19, 0.48
and 0.24 for simWN, simVN, simDist and simArgs, re-
spectively.
Baselines. A simple baseline for this task is to
align all predicates whose lemmas are identical
(SameLemma). As a more sophisticated baseline,
we make use of alignment tools commonly used in
statistical machine translation (SMT). We train our
own word alignment model using the state-of-the-art
tool Berkeley Aligner (Liang et al, 2006). As word
alignment tools require pairs of sentences as input,
we first extract paraphrases for this baseline using a
re-implementation of the paraphrase detection sys-
tem by Wan et al (2006). In the following sections,
we abbreviate this model as WordAlign.
5.3 Results
Following Cohn et al (2008) we measure precision
as the number of predicted alignments also anno-
tated in the gold standard divided by the total num-
ber of predictions. Recall is measured as the num-
ber of correctly predicted sure alignments devided
by the total number of sure alignments in the gold
standard. We subsequently compute the F1-score as
the harmonic mean between precision and recall.
Table 2 presents the results for our model and
the two baselines. From all four approaches,
WordAlign performs worst. We identify two main
reasons for this: On the one hand, the paraphrase
detection does not perform perfectly. Hence, the
extracted sentence pairs do not always contain gold
alignments. On the other hand, even sentence pairs
that contain gold alignments are generally less paral-
Precision Recall F1
WordAlign 19.7% 15.2% 17.2%
SameLemma 40.3% 60.3% 48.3%
Clustering 59.7% 50.7% 54.8%
Table 2: Results for all models on our test set; significant
improvements (p<0.005) over the results given in each
previous line are marked in bold face.
lel compared to a typical SMT setting, which makes
them harder to align.
We observe that the majority of all sure align-
ments (60.3%) can be retrieved by applying the
SameLemma model, yet at a low precision (40.3%).
While the Clustering model only recalls 50.7% of
all cases, it clearly outperforms SameLemma in
terms of precision (+19.4% points), an important
factor for us as we plan to use the alignments in
subsequent tasks. With 54.8%, Clustering also
achieves the best overall F1-score. We computed
statistical significance of result differences with a
paired t-test (Cohen, 1995), yielding significance at
the 99.5% level for precision and F1-score.
5.4 Analysis of Results
We perform an analysis of the output of the Clus-
tering model on the development set to categorize
correct and incorrect alignment decisions.19 In to-
tal, the model missed 13 out of 35 sure alignments
(Type I errors) and predicted 23 alignments not an-
notated in the gold standard (Type II errors). Six
Type I errors (46%) occurred when the lemma of an
affected predicate occurred more than once in a text
and the model missed the correct link. Vice versa,
we find 18 Type II errors (78%) that were made be-
cause of a high predicate similarity despite low ar-
gument overlap. An example is given in (9).
(9) a. The US alert (. . . ) followed intelligence reports
that . . . 20
b. The Foreign Ministry announcement called on
Japanese citizens to be cautious . . . 21
While argument overlap itself can be low even for
correct alignments, the results clearly indicate that
19We decided to leave the test set untouched for further exper-
iments. Due to parameter tuning, the results on the development
set alo provide us with an upper bound of the proposed model.
20Source document ID: AFP ENG 20101004.0367
21Source document ID: APW ENG 20101004.0207
224
a better integration of context is necessary: Exam-
ple (10.a) illustrates a case in which the agent of a
warning event is not realized. Here, contextual in-
formation is required to correctly align it to one of
the warning events in (10.b). This involves inference
beyond the local PAS.
(10) a. The US alert (. . . ) is one step down from a full
[travel]Arg1 warning [ ]Arg0.20
b. Japan has issued a travel alert . . . (which)
follows similar warnings [from Ameri-
can and British authorities]Arg0. (. . . ) An offi-
cial said it was highly unusual for [Tokyo]Arg0
to issue such a warning . . . 21
On the positive side, Clustering achieves a precision
of 61.4% and a recall of 65.7% on the development
set. Example (11) shows a correctly aligned PAS
pair that involves non-realized arguments:
(11) a. . . . the Governing Council has established
[a committee]Arg0 to draft [a constitution]Arg1.22
b. A .. resolution calls on the Governing
Council for elections and the drafting [ ]Arg0
[of a new constitution]Arg1.23
In (11.a), the follow-up sentences will refer back to
the committee that will draft the new Iraqi constitu-
tion, hence the institution has to be introduced in the
discourse at this point. In contrast, excerpt (11.b) is
the last sentence of a news report. Since it presents
a summary, introducing new (omissible) entities at
this point would not concord with general coherence
principles.
6 Conclusion
In this paper, we presented a novel corpus of compa-
rable texts that provides full discourse contexts for
alternative verbalizations. The motivation for the
construction of this corpus is to acquire empirical
data for studying discourse coherence factors related
to argument structure realization. A special phe-
nomenon we are interested in are discourse-related
factors that license the omission of argument roles.
Our data set satisfies two conditions that are es-
sential for the purported task: the texts are about
22Source document ID: AFP ENG 20031015.0353
23Source document ID: APW ENG 20031015.0236.
the same events and constitute alternative verbaliza-
tions. Selected from the Gigaword corpus, the doc-
uments pertain to the news domain, and satisfy the
further constraint that we have access to the full sur-
rounding discourse context. The constructed corpus
could thus be profitable for a range of other tasks
that need to investigate factors for knowledge aggre-
gation, such as summarization, or inference in dis-
course, such as textual entailment.
In total, we derived more than 160,000 document
pairs from all pairwise combinations of newswire
sources in the English Gigaword Fifth Edition. Us-
ing a subset of these pairs, we constructed a devel-
opment and an evaluation data set with gold align-
ments that relate predications with (possibly partial)
PAS correspondence. We established that the anno-
tation task, while difficult, can be performed with
good inter-annotator agreement (? at 0.86).
We presented first experiments on the task of au-
tomatically predicting predicate alignments. This
step is essential to gather empirical evidence of dif-
ferent PAS realizations for the same event, given
varying discourse contexts. Analysis of the data
shows that the aligned predications capture a wide
variety of sources and variations of coherence ef-
fects, including constructional, lexical and discourse
phenomena.
In future work, we will enhance our model by in-
corporating more refined semantic similarity mea-
sures including discourse-based criteria for estab-
lishing cross-document alignments. Given that our
data set includes sets of aligned documents from
several newswire sources, we will explore transitiv-
ity constraints across multiple document pairs in or-
der to further enhance the precision of the alignment
model. We will then proceed to the ultimate aim of
our work: the development of a coherence model for
argument structure realization, including the design
of an appropriate task and evaluation setting.
Acknowledgements
We are grateful to the Landesgraduiertenfo?rderung
Baden-Wu?rttemberg for funding within the research
initiative ?Coherence in language processing? of
Heidelberg University. We thank Danny Rehl and
Lukas Funk for annotation and Kathrin Spreyer, Tae-
Gil Noh and Carina Silberer for helpful discussion.
225
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, Mass., 2?7 May 2004,
pages 113?120.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics, Toulouse, pages
50?57.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2009. The grec main subject reference generation
challenge 2009: overview and evaluation results. In
Proceedings of the 2009 Workshop on Language Gen-
eration and Summarisation, pages 79?87.
Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33?36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Chris Brockett. 2007. Aligning the RTE 2006 Corpus.
Microsoft Research.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 817?825, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37?46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing Corpora for Development and
Evaluation of Paraphrase Systems. 34(4).
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pages 320?327.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16.3:235?250.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium, Philadelphia.
Philipp Koehn, 2005. Europarl: A parallel corpus for
statistical machine translation, volume 5, pages 79?
86.
Percy Liang, Benjamin Taskar, and Dan Klein. 2006.
Alignment by agreement. In North American Associ-
ation for Computational Linguistics (NAACL), pages
104?111.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory. Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Kathleen R. McKeown and Dragomir Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of the 18th Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval, Seattle, Wash., 9?13 July 1995, pages
74?82. Reprinted in Advances in Automatic Text Sum-
marization, Mani, I. and Maybury, M.T. (Eds.), Cam-
bridge, Mass.: MIT Press, 1999, pp.381-389.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
105.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Ivan Titov and Mikhail Kozhevnikov. 2010. Bootstrap-
ping semantic analyzers from non-contradictory texts.
226
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, Uppsala, Swe-
den, 11?16 July 2010, pages 958?967.
Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris.
2006. Using dependency-based features to take the
?Para-farce? out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop, pages
131?138.
Sander Wubben, Antal van den Bosch, Emiel Krahmer,
and Erwin Marsi. 2009. Clustering and matching
headlines for automatic paraphrase acquisition. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG 2009), pages 122?
125, Athens, Greece, March. Association for Compu-
tational Linguistics.
227
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 306?316, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Automatically Identifying Implicit Arguments to
Improve Argument Linking and Coherence Modeling
Michael Roth and Anette Frank
Department of Computational Linguistics
Heidelberg University, Germany
{mroth,frank}@cl.uni-heidelberg.de
Abstract
Implicit arguments are a discourse-level phe-
nomenon that has not been extensively stud-
ied in semantic processing. One reason for
this lies in the scarce amount of annotated data
sets available. We argue that more data of
this kind would be helpful to improve exist-
ing approaches to linking implicit arguments
in discourse and to enable more in-depth stud-
ies of the phenomenon itself. In this paper, we
present a range of studies that empirically val-
idate this claim. Our contributions are three-
fold: we present a heuristic approach to auto-
matically identify implicit arguments and their
antecedents by exploiting comparable texts;
we show how the induced data can be used as
training data for improving existing argument
linking models; finally, we present a novel ap-
proach to modeling local coherence that ex-
tends previous approaches by taking into ac-
count non-explicit entity references.
1 Introduction
Semantic role labeling systems traditionally process
text in a sentence-by-sentence fashion, construct-
ing local structures of semantic meaning (Palmer et
al., 2010). Information relevant to these structures,
however, can be non-local in natural language texts
(Palmer et al, 1986; Fillmore, 1986, inter alia). In
this paper, we view instances of this phenomenon,
also referred to as implicit arguments, as elements
of discourse. In a coherent discourse, each utter-
ance focuses on a salient set of entities, also called
?foci? (Sidner, 1979) or ?centers? (Joshi and Kuhn,
1979). According to the theory of Centering (Grosz
et al, 1995), the salience of an entity in a discourse
is reflected by linguistic factors such as choice of
referring expression and syntactic form. Both ex-
tremes of salience, i.e., contexts of referential conti-
nuity (Brown, 1983) and irrelevance, can also be re-
flected by the non-realization of an entity. Altough
specific instances of non-realization, so-called zero
anaphora, have been well-studied in discourse anal-
ysis (Sag and Hankamer, 1984; Tanenhaus and Carl-
son, 1990, inter alia), this phenomenon has widely
been ignored in computational approaches to entity-
based coherence modeling. It could, however, pro-
vide an explanation for local coherence in cases that
are not covered by current models of Centering (cf.
Louis and Nenkova (2010)). In this work, we pro-
pose a new model to predict whether realizing an
argument contributes to local coherence in a given
position in discourse. Example (1) shows a text frag-
ment, in which argument realization is necessary in
the first sentence but redundant in the second.
(1) El Salvador is now the only Latin Ameri-
can country which still has troops in [Iraq].
Nicaragua, Honduras and the Dominican
Republic have withdrawn their troops [?].
From a semantic processing perspective, a human
reader can easily infer that ?Iraq?, the marked en-
tity in the first sentence of Example (1), is also an
implicit argument of the predicate ?withdraw? in the
second sentence. This inference step is, however,
difficult to model computationally as it involves an
interplay of two challenging sub-tasks: first, a se-
mantic processor has to determine that an argument
is not realized (but inferrable); and second, a suit-
306
able antecedent has to be found within the discourse
context. For the remainder of this paper, we refer to
these steps as identifying and linking implicit argu-
ments to discourse antecedents.
As indicated by Example (1), implicit arguments
are an important aspect in semantic processing, yet
they are not captured in traditional semantic role la-
beling systems. The main reasons for this are the
scarcity of annotated data, and the inherent difficulty
of inferring discourse antecedents automatically.
In this paper, we propose to induce implicit ar-
guments and discourse antecedents by exploiting
complementary (explicit) information obtained from
monolingual comparable texts (Section 3). We ap-
ply the empirically acquired data in argument link-
ing (Section 4) and coherence modeling (Section 5).
We conclude with a discussion on the advantages of
our data set and outline directions for future work
(Section 6).
2 Related work
The most prominent approach to entity-based coher-
ence modeling nowadays is the entity grid model by
Barzilay and Lapata (2005). It has originally been
proposed for automatic sentence ordering but has
also been applied in coherence evaluation and read-
ability assessment (Barzilay and Lapata, 2008; Pitler
and Nenkova, 2008), and story generation (McIntyre
and Lapata, 2009). Based on the original model,
a few extensions have been proposed: for exam-
ple, Filippova and Strube (2007) and Elsner and
Charniak (2011b) suggested additional features to
characterize semantic relatedness between entities
and features specific to single entities, respectively.
Other entity-based approaches to coherence model-
ing include the pronoun model by Charniak and El-
sner (2009) and the discourse-new model by Elsner
and Charniak (2008). All of these approaches are,
however, based on explicitly realized entity men-
tions only, ignoring references that are inferrable.
The role of implicit arguments has been studied
early on in the context of semantic processing (Fill-
more, 1986; Palmer et al, 1986). Yet, the phe-
nomenon has mostly been ignored in semantic role
labeling. First data sets, focusing on implicit argu-
ments, have only recently become available: Rup-
penhofer et al (2010) organized a SemEval shared
task on ?linking events and participants in dis-
course?, Gerber and Chai (2012) made available im-
plicit argument annotations for the NomBank corpus
(Meyers et al, 2008) and Moor et al (2013) pro-
vide annotations for parts of the OntoNotes corpus
(Weischedel et al, 2011). However, these resources
are very limited: The annotations by Moor et al and
Gerber and Chai are restricted to 5 and 10 predi-
cate types, respectively. The training set of the Se-
mEval task contains only 245 resolved implicit argu-
ments in total. As pointed out by Silberer and Frank
(2012), additional training data can be heuristically
created by treating anaphoric mentions as implicit
arguments. Their experimental results showed that
artificial training data can indeed improve results,
but only when obtained from corpora with manual
semantic role annotations (on the sentence level) and
gold coreference chains.
3 Identifying and linking implicit
arguments
The aim of this work is to automatically construct
a data set of implicit arguments and their discourse
antecedents. We propose an induction approach that
exploits complementary information obtained from
pairs of comparable texts. As a basis for this ap-
proach, we rely on several preparatory steps pro-
posed in the literature that first identify informa-
tion two documents have in common (cf. Figure 1).
In particular, we align corresponding predicate-
argument structures (PAS) using graph-based clus-
tering (Roth and Frank, 2012b). We then determine
co-referring entities across the texts using corefer-
ence resolution techniques on concatenated docu-
ment pairs (Lee et al, 2012). These preprocessing
steps are described in more detail in Section 3.1.
Given the preprocessed comparable texts and
aligned PAS, we propose to heuristically iden-
tify implicit arguments and link them to their
antecedents via the cross-document coreference
chains. We describe the details of this approach in
Section 3.2.
3.1 Data preparation
The starting point for our approach is the data set of
automatically aligned predicate pairs that has been
released by Roth and Frank (2012a).1 This data
1cf. http://www.cl.uni-heidelberg.de/%7Emroth/
307
Sentence that comprises a PAS with an (correctly predicted) implicit argument induced antecedent
The [?A0] [operatingA3] loss, as measured by . . . widened to 189 million euros . . . T-Online[?s]
It was handed over to Mozambican control . . . 33 years after [?A0] independence. Mozambique[?s]
. . . [local officials A0] failed to immediately report [the accident A1] [?A2] . . . [to] the government
Table 1: Three positive examples of automatically induced implicit argument and antecedent pairs.
Figure 1: Illustration of the induction approach: texts
consist of PAS (represented by overlapping circles);
we exploit alignments between corresponding predicates
across texts (marked by solid lines) and co-referring enti-
ties (marked by dotted lines) to infer implicit arguments
(marked by ?i?) and link antecedents (curly dashed line)
set, henceforth just R&F data, is a collection of
283,588 predicate pairs that have been aligned ?with
high precision?2 across comparable newswire arti-
cles from the Gigaword corpus (Parker et al, 2011).
To use these documents for our argument induc-
tion technique, we apply a couple of pre-processing
tools on each single document and perform cross-
document entity coreference on pairs of documents.
Single document pre-processing. We apply sev-
eral preprocessing steps to all documents in
the R&F data: we use the Stanford CoreNLP
package3 for tokenization and sentence split-
ting. We then apply MATE tools (Bohnet, 2010;
Bjo?rkelund et al, 2010), including the integrated
PropBank/NomBank-style semantic parser, to re-
construct local predicate-argument structures for
aligned predicates. Finally, we resolve pronouns that
occur in a PAS using the coreference resolution sys-
tem by Martschat et al (2012).
2The used method achieved a precision of 86.2% at a recall
of 29.1% on the Roth and Frank (2012a) test set.
3http://nlp.stanford.edu/software/
Cross-document coreference. We apply cross-
document coreference resolution to induce an-
tecedents for implicit arguments. In practice, we
use the Stanford Coreference System (Lee et al,
2013) and run it on pairs of texts by simply pro-
viding a single document as input, comprising of a
concatenation of the two texts. To perform this step
with high precision, we only use the most precise
resolution sieves: ?String Match?, ?Relaxed String
Match?, ?Precise Constructs?, ?Strict Head Match
[A-C]?, and ?Proper Head Noun Match?.
3.2 Identification and linking approach
Given a pair of aligned predicates from two compa-
rable texts, we examine the parser output to identify
the arguments in each predicate-argument structure
(PAS). We compare the set of realized argument po-
sitions in both structures to determine whether one
PAS contains an argument position (explicit) that
has not been realized in the other PAS (implicit).
For each implicit argument, we identify appropri-
ate antecedents by considering the cross-document
coreference chain of its explicit counterpart. As our
goal is to link arguments within discourse, we re-
strict candidate antecedents to mentions that occur
in the same document as the implicit argument.
We apply a number of restrictions to the resulting
pairs of implicit arguments and antecedents to mini-
mize the impact of errors from preprocessing:
- The aligned PAS should consist of a different
number of arguments (to minimize the impact
of argument labeling errors)
- The antecedent should not be a resolved pro-
noun (to avoid errors resulting from incorrect
pronoun resolution)
- The antecedent should not be in the same sen-
tence as the implicit argument (to circumvent
cases, in which an implicit argument is actu-
ally explicit but has not been recognized by the
parser)
308
3.3 Resulting data set
We apply the identification and linking approach to
the full R&F data set of aligned predicates. As a re-
sult, we induce a total of 701 implicit argument and
antecedent pairs, each in a separate document, in-
volving 535 different predicates. Examples are dis-
played in Table 1. Note that 701 implicit arguments
from 283,588 pairs of predicate-argument structures
seem to represent a fairly low recall. Most predicate
pairs in the high precision data set of Roth and Frank
(2012a) do, however, consist of identical argument
positions (84.5%). In the remaining cases, in which
an implicit argument can be identified (15.5%), an
antecedent in discourse cannot always be found us-
ing the high precision coreference sieves. This does
not mean that implicit arguments are a rare phe-
nomenon in general. In fact, 38.9% of all manually
aligned predicate pairs in Roth and Frank (2012a)
involved a different number of arguments.
We manually evaluated a subset of 90 induced im-
plicit arguments and found 80 discourse antecedents
to be correct (89%). Some incorrectly linked in-
stances still result from preprocessing errors. In Ta-
ble 2, we present a range of different error types that
occurred when extracting implicit arguments with-
out any restrictions.
4 Experiment 1: Linking implicit
arguments
Our first experiment assesses the utility of automat-
ically induced implicit arguments and antecedent
pairs for the task of implicit argument linking. For
evaluation, we use the data sets from the SemEval
2010 task on Linking Events and their Participants
in Discourse (Ruppenhofer et al, 2010, henceforth
just SemEval). For direct comparison with previous
results and heuristic acquisition techniques (cf. Sec-
tion 2), we apply the implicit argument identifica-
tion and linking model by Silberer and Frank (2012,
henceforth S&F) for training and testing.
4.1 Task summary
Both the training and test sets of the SemEval task
are text corpora extracted from Sherlock Holmes
novels, with manual frame semantic annotations in-
cluding implicit arguments. In the actual linking
task (?NI-only?), labels are provided for local argu-
ments and participating systems have to perform the
following three sub-tasks: (1) identify implicit argu-
ments (IA), (2) predict whether each IA is resolvable
and, if so, (3) find an appropriate antecedent.
The task organizers provide two versions of their
data sets: one based on FrameNet annotations and
one based on PropBank/NomBank annotations. We
found that the latter, however, only contains a sub-
set of the implicit argument annotations from the
FrameNet-based version. As all previous results in
this task have been reported on the FrameNet data
set, we adopt the same setting. Note that our addi-
tional training data is automatically labeled with a
PropBank/NomBank-style parser. That is, we need
to map our annotations to FrameNet. The organizers
of the SemEval shared task provide a manual map-
ping dictionary for predicates in the annotated data
set. We make use of this manual mapping and ad-
ditionally use SemLink 1.14 for mapping predicates
and arguments not in the dictionary.
4.2 Model details
We make use of the system by S&F to train a new
model for the NI-only task. As mentioned in the pre-
vious sub-section, this task consists of three steps:
In step (1), implicit arguments are identified as un-
filled FrameNet core roles that are not competing
with roles that are already filled; in step (2), a SVM
classifier is used to predict whether implicit argu-
ments are resolvable based on a small amount of
features ? semantic type of the affected Frame Ele-
ment, the relative frequency of its realization type in
the SemEval training corpus, and a boolean feature
that indicates whether the affected sentence is in pas-
sive voice and does not contain a (deep) subject. In
step (3), we apply the same features and classifier as
S&F, i.e., the BayesNet implementation from Weka
(Witten and Frank, 2005), to find appropriate an-
tecedents for (predicted) resolvable arguments. S&F
report that their best results were obtained when
considering all entities as candidate antecedents that
are syntactic constituents from the present and the
past two sentences, or entities that occurred at least
five times in the previous discourse (?Chains+Win?
setting). In their evaluation, the latter of these two
restrictions crucially depended on gold coreference
chains. As the automatic coreference chains in our
4http://verbs.colorado.edu/semlink/
309
Sentence that comprises a PAS with an (incorrectly predicted) implicit argument induced antecedent
(1) .. [Statistics?] released [Tuesday TMP ] [?A0] showed the death toll dropped . . . official statistics
(2) A [French LOC?] [?A0] draft resolution . . . demands full . . . compliance . . . France
(3) An earthquake . . . is capable of causing .. [heavy EXT ] damage [?A2?] major
Table 2: Examples of erroneous pairs of implicit arguments and antecedents. In (1), the parser did not recognize
?Statistics? as an argument of showed; in (2), the parser mislabeled ?French? as a locative modifier; both errors lead
to incorrectly identified implicit arguments. In (3), the implicit argument is correct but the wrong antecedent was
identified because ?major? had been mislabeled in the aligned predicate-argument structure
data are rather sparse (and noisy), we only consider
syntactic constituents from the present and the past
two sentences as antecedents (?SentWin? setting).
Before training and testing a new model with
our own data, we perform feature selection us-
ing 10-fold cross validation. We run the feature
selection on a combination of the SemEval train-
ing data and our additional data set in order to
find a set of features that generalizes best across
the two different corpora. We found these to be
features regarding ?prominence?, selectional pref-
erences (?sp supersense?), the POS tags of entity
mentions, and semantic types of argument positions
(?semType dni.entity?). Note that the S&F system
does not make use of any lexicalized information.
Instead, semantic features are computed based on
the highest abstraction level in WordNet (Fellbaum,
1998). For detailed description of all features, see
Silberer and Frank (2012).
4.3 Results
For direct comparison in the full task, both with
S&F?s model and other previously published results,
we adopt the precision, recall and F1 measures as
defined in Ruppenhofer et al (2010). We compare
our results with those previously reported on the Se-
mEval task (see Table 3 for a summary): Chen et
al. (2010) adapted SEMAFOR, the best performing
system that participated in the actual task in 2010.
Tonelli and Delmonte (2011) presented a revised
version of their SemEval system (Tonelli and Del-
monte, 2010), which outperformed SEMAFOR in
terms of recall (6%) and F1 score (8%). The best
results in terms of recall and F1 score up to date
have been reported by Laparra and Rigau (2012),
with 25% and 19%, respectively. Our model outper-
forms their state-of-the-art system in terms of preci-
sion (21%) but at a higher cost of recall (8%). Two
P R F
Chen et al (2010)5 0.25 0.01 0.02
Tonelli and Delmonte (2011) 0.13 0.06 0.08
Laparra and Rigau (2012) 0.15 0.25 0.19
Laparra and Rigau (2013) 0.14 0.18 0.16
Gorinski et al (2013)6 0.14 0.12 0.13
S&F (no additional data) 0.06 0.09 0.07
S&F (best additional data) 0.09 0.11 0.10
This paper 0.21 0.08 0.12
Table 3: Results in terms of precision (P), recall (R) and
F1 score (F) for identifying and linking implicit argu-
ments in the SemEval test set.
influencing factors for their high recall are probably
(1) their improved method for identifying (resolv-
able) implicit arguments, and (2) their addition of
lexicalized and ontological features.
Comparison to the original results reported by
S&F, whose system we use, shows that our addi-
tional data improves precision (from 6% to 21%)
and F1 score (from 7% to 12%). The loss in recall
is marginal (-1%) given the size of the test set (259
resolvable cases in total). The result in precision is
the second highest score reported on this task. Inter-
estingly, the improvements are higher than those of
the best training set used in the original study by Sil-
berer and Frank (2012), even though their additional
data set is three times bigger than ours and is based
on manual semantic annotations. We conjecture that
their low gain in precision could be a side effect trig-
gered by two factors: on the one hand, their model
crucially relies on coreference chains, which are au-
tomatically generated for the test set and hence are
rather noisy. On the other hand, their heuristically
created training data might not represent implicit ar-
gument instances adequately.
310
5 Experiment 2: Implicit arguments in
coherence modeling
In our second experiment, we examine the effect of
implicit arguments on local coherence, i.e., the ques-
tion of how well a local argument (non-)realization
fits into a given context. We approach this question
as follows: first, we assemble a data set of document
pairs that differ only with respect to a single realiza-
tion decision (Section 5.1). Given each pair in this
data set, we ask human annotators to indicate their
preference for the implicit or explicit argument re-
alization in the pre-specified context (Section 5.2).
Second, we attempt to emulate the decision pro-
cess computationally using a discriminative model
based on discourse and entity-specific features (Sec-
tion 5.3).
5.1 Data compilation
We use the induced data set (henceforth source
data), as described in Section 3, as a starting point
for composing a set of document pairs that involve
implicit and explicit arguments. To make sure that
each document pair in this data set only differs with
respect to a single realization decision, we first cre-
ate two copies of each document from the source
data: one copy remains in its original form, and the
other copy will be modified with respect to a sin-
gle argument realization. Example (2) illustrates an
example of an original and modified (marked by an
asterik) sentence:
(2) [The Dalai Lama?sA0] visit [to FranceA1] ends
on Tuesday.
* [The Dalai Lama?sA0] visit ends on Tuesday.
Note that adding and removing arguments at ran-
dom can lead to structures that are semantically
implausible. Hence, we restrict this procedure to
predicate-argument structures (PAS) that actually
occur and are aligned across two texts, and create
modifications by replacing a single argument posi-
tion in one text with the corresponding argument po-
sition in the comparable text. Examples (2) and (3)
5Results as reported in Tonelli and Delmonte (2011)
6Results computed as an average over the scores given for
both test files; rounded towards the number given for the test
file that contained more instances.
show two such comparable texts. The original PAS
in Example (2) contains an explicit argument that is
implicit in the aligned PAS and hence removed in
the modified version. Vice versa, the original text
in (3) involves an implicit argument, which is made
explicit in the modified version.
(3) [The Dalai Lama?sA0] visit coincides with the
Beijing Olympics.
* [The Dalai Lama?sA0] visit [to FranceA1] co-
incides with the Beijing Olympics.
We ensure that the modified structure fits into
the given context grammatically by only consid-
ering PAS with identical predicate form and con-
stituent order. We found that this restriction con-
strains affected arguments to be modifiers, prepo-
sitional phrases and direct objects. We argue that
this is actually a desirable property because more
complicated alternations could affect coherence by
themselves; resulting interplays would make it diffi-
cult to distinguish between the isolated effect of ar-
gument realization itself and other effects, triggered
for example by sentence order (Gordon et al, 1993).
5.2 Annotation
We set up a web experiment using the NLTK pack-
age (Belz and Kow, 2011) to collect (local) coher-
ence ratings for implicit and explicit arguments. For
this experiment, we compiled a data set of 150 doc-
ument pairs. As described in Section 5.1, each text
pair consists of mostly the same text, with the only
difference being one argument realization.
We presented all 150 pairs to two annotators7 and
asked them to indicate their preference for one al-
ternative over the other using a continuous slider
scale. The annotators got to see the full texts, with
the alternatives presented next to each other. To
make texts easier to read and differences easier to
spot, we collapsed all identical sentences into one
column and highlighted the aligned predicate (in
both texts) and the affected argument (in the explicit
case). An example is shown in Figure 2. To avoid
any bias in the annotation process, we shuffled the
sequence of text pairs and randomly assigned the
side of display (left/right) of each realization type
7Both annotators are undergraduate students in Computa-
tional Linguistics.
311
Figure 2: Texts as displayed to the annotators.
(explicit/implicit). Note that instead of providing a
definition of local coherence ourselves, we simply
asked the annotators to rate how ?natural? a realiza-
tion sounds given the discourse context.
We found that annotators made use of the full rat-
ing scale, which spans from -50 to +50, with the ex-
tremes indicating either a strong preference for the
text on the left hand side or the right hand side, re-
spectively. Most ratings are, however, concentrated
more towards the center of the scale (i.e., around
zero). This seems to imply that the use of im-
plicit or explicit arguments did not make a consid-
erable difference most of the time. The first author
confirmed this assumption and resolved disagree-
ments between annotators in several group discus-
sions. The annotators also affirmed that some cases
do not read naturally when a specific argument is or
is not realized at a given position in discourse. Ex-
amples (4) and (5) illustrate two cases, in which a
redundant argument is realized (A4, or destination)
or a coherence establishing argument has been omit-
ted (A2, or co-signer).8
(4) ? The remaining contraband was picked up at
Le Havre. The containers had arrived [in
Le Havre] from China.
(5) ? Lt.-Gen. Mohamed Lamari (. . . ) denied
his country wanted South African weapons
to fight Muslim rebels fighting the govern-
ment. ?We are not going to fight a flea with
8Note that both examples are only excerpts from the affected
texts. The annotators got to see the full context.
a hammer,? Lamari told reporters after sign-
ing the agreement of intent [?].
Following discussions with the annotators, we
discarded all items from the final data set, for which
no clear preference could be established (72%) or
the annotators had different preferences (9%). We
mapped all remaining items into two classes accord-
ing to whether the affected argument had to be im-
plicit (9 texts) or explicit (20 texts). All 29 uniquely
classified texts are used as a small gold standard test
set for evaluation.
5.3 Coherence model
We model the decision process that underlies the
(non-)realization of arguments using a SVM classi-
fier and a range of discourse features. The features
can be classified into three groups: features specific
to the affected predicate-argument structure (Parg),
the (automatic) coreference chain of the affected ar-
gument (Coref), and the discourse context (Disc).
Parg includes the absolute and relative number of
realized arguments; the number of modifiers in the
PAS; and the total length (in words) of the PAS and
the complete sentence.
Coref includes the number of previous/follow-up
mentions in a fixed sentence window; the distance
(in number of words/sentences) to the previous/next
mention; the distribution of occurrences over the
previous/succeeding two sentences;9 and the POS of
previous/follow-up mentions.
Disc includes the total number of coreference
chains in the text; the occurrence of pronouns
in the current sentence; lexical repetitions in the
previous/follow-up sentence; the current position in
discourse (begin, middle, end); and a feature indi-
cating whether the affected argument occured in the
first sentence.
Note that most of these features overlap with
those successfully applied in previous work. For
example, Pitler and Nenkova (2008) also use text
9This type of feature is very similar to the transition pat-
terns in the original entity grid. The only difference is that our
features are not typed with respect to the grammatical function
of explicit realizations. The reason for skipping this informa-
tion lies in the insignificant amount of relevant samples in our
(noisy) training data.
312
length, sentence-to-sentence transitions, word over-
lap and pronoun occurrences as features for predict-
ing readability. Our own contribution lies in the defi-
nition of PAS-specific features and the adaptation of
all features to the task of predicting (non-)realization
of arguments in a predicate-argument structure.
5.4 Training data
We do not make use of any manually annotated data
for training. Instead, our model relies solely on the
automatically induced source data, described in Sec-
tion 3, for learning. We prepare this data set as fol-
lows: first, we remove all data points that also occur
in the test set. Second, we split all pairs of texts into
two groups ? texts that contain a predicate-argument
structure in which an implicit argument has been
identified (IA), and their comparable counterparts
that contain the aligned PAS with an explicit argu-
ment (EA). All texts are labelled according to their
group. For all texts in group EA, we remove the ex-
plicit argument from the aligned PAS. This way, the
feature extractor always gets to see the text and au-
tomatic annotations as if the realization decision had
not been performed and can thus extract unbiased
feature values for the affected entity and argument
position.
5.5 Evaluation setting
The goal of this task is to correctly predict the re-
alization type (implicit or explicit) of an argument
that maximizes the coherence of the document. As
a proxy for coherence, we use the naturalness rat-
ings given by our annotators. We evaluate classifica-
tion performance on the part of our test set for which
clear preferences have been established. We report
results in terms of precision, recall and F1 score. We
compute precision as the fraction of correct classifier
decisions divided by the total number of classifica-
tions; and recall as the fraction of correct classifier
decisions divided by the total number of test items.
Note that precision and recall are identical when the
model provides a class label for every test item. We
compute F1 as the harmonic mean between precision
and recall.
For comparison with previous work, we further
apply a couple of previously proposed local co-
herence models: the original entity grid model by
Barzilay and Lapata (2005), a modified version that
uses topic models (Elsner and Charniak, 2011a) and
an extended version that includes entity-specific fea-
tures (Elsner and Charniak, 2011b). We further ap-
ply the discourse-new model by Elsner and Charniak
(2008) and the pronoun-based model by Charniak
and Elsner (2009). For all of the aforementioned
models, we use their respective implementation pro-
vided with the Brown Coherence Toolkit10. Note
that the toolkit only returns one coherence score for
each document. To use the toolkit for argument clas-
sification, we use two documents per data point ?
one that contains the affected argument explicitly
and one that does not (implicit argument) ? and treat
the higher scoring variant as classification output. If
both documents achieve the same score, we neither
count the test item as correctly nor as incorrectly
classified. In contrast, we apply our own model only
on the document that contains the implicit argument,
and use the classifier to predict whether this realiza-
tion type fits into the given context or not. Note that
our model has an advantage here because it is specif-
ically designed for this task. Yet, all models com-
pute local coherence ratings based on entity occur-
rences and should thus be able to predict which re-
alization type coheres best with the given discourse
context.11
5.6 Results
The results are summarized in Table 4. As all mod-
els provided class labels for almost all test instances,
we focus our discussion on F1 scores. The majority
class in our test set is the explicit realization type,
making up 20 of the 29 test items (69%).
The original entity grid model produced differing
scores for the two realization types only in 26 cases.
The model exhibits a strong preference for the im-
plicit realization type: it predicts this class in 22
cases, resulting in an F1 score of only 15%. Tak-
ing a closer look at the features of the model reveals
that this an expected outcome: in its original set-
ting, the entity grid learns realization patterns in the
form of sentence-to-sentence transitions. Most enti-
ties are, however, only mentioned a few times in a
10cf. http://www.ling.ohio-state.edu/%7Emelsner/
11Recall that input document pairs are identical except for the
affected argument position. Consequently, the resulting coher-
ence scores only differ with respect to affected entity realiza-
tions.
313
P R F
Entity grid models ? ? ?
Baseline entity grid 0.15** 0.14** 0.15**
Extended entity grid 0.19** 0.17** 0.18**
Topical entity grid 0.34** 0.34** 0.34**
Other models ? ? ?
Pronouns 0.43** 0.34** 0.38**
Discourse-newness 0.48** 0.48** 0.48**
This paper ? ? ?
Our (full) model 0.90 0.90 0.90
Simplified model 0.83 0.83 0.83
Majority class 0.69* 0.69* 0.69*
Table 4: Results in terms of precision (P), recall (R) and
F1 score for correctly predicting argument realization; re-
sults that significantly differ from our (full) model are
marked with asterisks (* p<0.1; ** p<0.01)
text, which means that non-realizations make up the
?most probable? class ? independently of whether
they are relevant in a given context or not. The mod-
els by Charniak and Elsner (2009) and Elsner and
Charniak (2011a), which are not based on an entity
grid, do not suffer from this effect and achieve bet-
ter results, with F1 scores of 38% and 48%, respec-
tively. The topical and entity-specific refinements to
the entity grid model also alleviate the bias towards
non-realizations, resulting in improved F1 scores of
18% and 34%, respectively.
To counter-balance this issue altogether, we train
a simplified version of our own model that only
uses features that involve occurrence patterns. The
main difference between this simplified model and
the original entity grid model lies in the different
use of training data: while entity grid models treat
all non-realized items equally, our model gets to
?see? actual examples of entities that are implicit.
In other words, our simplified model takes into ac-
count implicit mentions of entities, not only explicit
ones. The results show that this extra information
has a significant (p<0.01, using a randomization test
(Yeh, 2000)) impact on test set performance, basi-
cally raising F1 from 15% to 83%. Using all features
of our model further increases F1 score to 90%, the
highest score achieved overall.
The highest weighted features in our model in-
clude all three feature groups: for example, the
number of coreferent mentions within the preceed-
ing/following two sentences (Coref), the number
of words already realized in the affected predicate-
argument structure (Parg), and the total number of
coreference chains in the document (Disc).
6 Conclusions
In this paper, we presented a novel approach to ac-
curately induce implicit arguments and discourse an-
tecedents from comparable texts (cf. Section 3). We
demonstrated the benefit of this kind of data for link-
ing implicit arguments and modeling local coher-
ence. Our experiments revealed three particularly
interesting results.
Firstly, a small data set of (automatically induced)
implicit arguments can have a greater impact on ar-
gument linking models than a bigger data set of ar-
tificially created instances (cf. Section 4). Secondly,
the use of implicit vs. explicit arguments, while be-
ing a subtle difference in most contexts, can have a
clear impact on text ratings. Thirdly, our automat-
ically created training data enables models to learn
features that considerably improve prediction of lo-
cally coherent argument realizations (cf. Section 5).
For the task of implicit argument linking, more
training data will be needed to further advance
the state-of-the-art. Our method for inducing
this kind of data, by exploiting aligned predicate-
argument structures from comparable texts, has
shown promising results. Future work will have
to explore this direction more fully, for example,
by identifying ways to induce data with higher re-
call. Integrating argument (non-)realization into a
full model of local coherence also remains part of
future work. In this paper, we presented a suitable
basis for such work: a training set that contains em-
pirical data on implicit arguments in discourse; and
a feature set that models argument realization with
high accuracy.
Acknowledgments
We are grateful to the Landesgraduiertenfo?rderung
Baden-Wu?rttemberg for funding within the research
initiative ?Coherence in language processing? at
Heidelberg University. We thank our annotators and
four anonymous reviewers.
314
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor, Michi-
gan, USA, 25?30 June 2005, pages 141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Anja Belz and Eric Kow. 2011. Discrete vs. continuous
rating scales for language evaluation in nlp. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 230?235, Portland, Oregon, USA,
June.
Anders Bjo?rkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntac-
tic and semantic dependency parser. In Coling 2010:
Demonstration Volume, pages 33?36, Beijing, China,
August. Coling 2010 Organizing Committee.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 89?97, Beijing, China,
August.
Cheryl Brown. 1983. Topic continuity in written english
narrative. In Talmy Givon, editor, Topic Continuity
in Discourse: A Quantitative Cross-Language Study.
John Benjamins, Amsterdam, The Netherlands.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 148?156, Athens, Greece,
March.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 264?267, Uppsala, Sweden, July.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of ACL-
08: HLT, Short Papers, pages 41?44, Columbus, Ohio,
June.
Micha Elsner and Eugene Charniak. 2011a. Disentan-
gling chat with local coherence models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1179?1189, Portland, Oregon, USA,
June.
Micha Elsner and Eugene Charniak. 2011b. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 125?129, Portland, Oregon, USA,
June.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Katja Filippova and Michael Strube. 2007. Extending
the entity-grid coherence model to semantically re-
lated entities. In Proceedings of the 11th European
Workshop on Natural Language Generation, Schloss
Dagstuhl, Germany, 17?20 June 2007, pages 139?142.
C. J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the twelfth annual meet-
ing of the Berkeley Linguistics Society, pages 95?107.
Matthew Gerber and Joyce Chai. 2012. Semantic Role
Labeling of Implicit Arguments for Nominal Predi-
cates. Computational Linguistics, 38(4):755?798.
Peter C. Gordon, Barbara J. Grosz, and Laura A. Gilliom.
1993. Pronouns, names, and the centering of attention
in discourse. Cognitive Science, 17:311?347.
Philip Gorinski, Josef Ruppenhofer, and Caroline
Sporleder. 2013. Towards weakly supervised resolu-
tion of null instantiations. In Proceedings of the 10th
International Conference on Computational Semantics
(IWCS 2013) ? Long Papers, pages 119?130, Potsdam,
Germany, March.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Aravind K. Joshi and Steve Kuhn. 1979. Centered logic:
The role of entity centered sentence representation in
natural language inferencing. In Proceedings of the
6th International Joint Conference on Artificial Intel-
ligence, Tokyo, Japan, August, pages 435?439.
Egoitz Laparra and German Rigau. 2012. Exploiting ex-
plicit annotations and semantic types for implicit argu-
ment resolution. In Proceedings of the Sixth IEEE In-
ternational Conference on Semantic Computing (ICSC
2010), pages 75?78, Palermo, Italy, September. IEEE
Computer Society.
Egoitz Laparra and German Rigau. 2013. Sources of ev-
idence for implicit argument resolution. In Proceed-
ings of the 10th International Conference on Compu-
tational Semantics (IWCS 2013) ? Long Papers, pages
155?166, Potsdam, Germany, March.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489?500, Jeju Island, Korea, July.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
315
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, 39(4). Accepted for publication.
Annie Louis and Ani Nenkova. 2010. Creating local
coherence: An empirical assessment. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 313?316, Los An-
geles, California, June.
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va
Mu?jdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task,
pages 100?106, Jeju Island, Korea, July.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference
on Natural Language Processing, Singapore, 2?7 Au-
gust 2009, pages 217?225.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v1.0. Linguistic Data Consortium,
Philadelphia.
Tatjana Moor, Michael Roth, and Anette Frank. 2013.
Predicate-specific annotations for implicit role bind-
ing: Corpus annotation, data analysis and evaluation
experiments. In Proceedings of the 10th International
Conference on Computational Semantics (IWCS 2013)
? Short Papers, pages 369?375, Potsdam, Germany,
March.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and John
Dowding. 1986. Recovering implicit information. In
Proceedings of the 24th Annual Meeting of the Associ-
ation for Computational Linguistics, New York, N.Y.,
10?13 June 1986, pages 10?19.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Synthesis Lectures on Human Language Technolo-
gies. Morgan & Claypool.
Robert Parker, David Graff, Jumbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. Linguistic Data Consortium, Philadelphia.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages
186?195, Honolulu, Hawaii, October.
Michael Roth and Anette Frank. 2012a. Aligning pred-
icate argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, pages 218?227, Montreal, Canada,
June.
Michael Roth and Anette Frank. 2012b. Aligning
predicates across monolingual comparable texts us-
ing graph-based clustering. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 171?182, Jeju Island, Ko-
rea, July.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations, pages 45?50, Up-
psala, Sweden, July.
Ivan A. Sag and Jorge Hankamer. 1984. Towards a The-
ory of Anaphoric Processing. Linguistics and Philos-
ophy, 7:325?345.
Candace L. Sidner. 1979. Towards a computational the-
ory of definite anaphora comprehension in English.
Technical Report AI-Memo 537, Massachusetts Insti-
tute of Technology, AI Lab, Cambridge, Mass.
Carina Silberer and Anette Frank. 2012. Casting implicit
role linking as an anaphora resolution task. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012), pages 1?10,
Montre?al, Canada, 7-8 June.
Michael K. Tanenhaus and Greg N. Carlson. 1990. Com-
prehension of Deep and Surface Verbphrase Anaphors.
Language and Cognitive Processes, 5(4):257?280.
Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++:
Adapting a deep semantic processing system to the
identification of null instantiations. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 296?299, Uppsala, Sweden, July.
Sara Tonelli and Rodolfo Delmonte. 2011. Desperately
seeking implicit arguments in text. In Proceedings of
the ACL 2011 Workshop on Relational Models of Se-
mantics, pages 54?62, Portland, Oregon, USA, June.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-
wen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-
chini, Mohammed El-Bachouti, Robert Belvin, and
Ann Houston. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium, Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, California, USA, 2nd
edition.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics, pages 947?953, Saarbru?cken, Ger-
many, August.
316
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 50?54,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Software Requirements: A new Domain for Semantic Parsers
Michael Roth? Themistoklis Diamantopoulos? Ewan Klein? Andreas Symeonidis?
?ILCC, School of Informatics
University of Edinburgh
{mroth,ewan}@inf.ed.ac.uk
?Electrical & Computer Engineering Department
Aristotle University of Thessaloniki
thdiaman@issel.ee.auth.gr
asymeon@eng.auth.gr
Abstract
Software requirements are commonly
written in natural language, making them
prone to ambiguity, incompleteness and
inconsistency. By converting require-
ments to formal semantic representations,
emerging problems can be detected at an
early stage of the development process,
thus reducing the number of ensuing errors
and the development costs. In this paper,
we treat the mapping from requirements to
formal representations as a semantic pars-
ing task. We describe a novel data set for
this task that involves two contributions:
first, we establish an ontology for formally
representing requirements; and second, we
introduce an iterative annotation scheme,
in which formal representations are de-
rived through step-wise refinements.
1 Introduction
During the process of software development, de-
velopers and customers typically discuss and
agree on requirements that specify the function-
ality of a system that is being developed.
1
Such
requirements play a crucial role in the develop-
ment lifecycle, as they form the basis for actual
implementations, corresponding work plans, cost
estimations and follow-up directives (van Lam-
sweerde, 2009). In general, software requirements
can be expressed in various different ways, includ-
ing the use of UML diagrams and storyboards.
Most commonly, however, expectations are ex-
pressed in natural language (Mich et al., 2004), as
shown in Example (1):
(1) A user should be able to login to his account.
1
Although software engineering can also involve non-
functional requirements, which describe general quality cri-
teria of a system, this paper is only concerned with functional
requirements, i.e., requirements that specify the behavior of a
system.
While requirements expressed in natural lan-
guage have the advantage of being intelligible to
both clients and developers, they can of course
also be ambiguous, vague and incomplete. Al-
though formal languages could be used as an alter-
native that eliminates some of these problems, cus-
tomers are rarely equipped with the mathematical
and technical expertise for understanding highly
formalised requirements. To benefit from the ad-
vantages of both natural language and formal rep-
resentations, we propose to induce the latter au-
tomatically from text in a semantic parsing task.
Given the software requirement in Example (1),
for instance, we would like to construct a represen-
tation that explicitly specifies the types of the en-
tities involved (e.g., object(account)) and that cap-
tures explicit and inferable relationships among
them (e.g., owns(user, account)). We expect such
formal representations to be helpful in detecting
errors at an early stage of the development process
(e.g., via logical inference and verification tools),
thus avoiding the costs of finding and fixing prob-
lems at a later and hence more expensive stage
(Boehm and Basili, 2001).
Given the benefits of formal representations,
we believe that software requirements constitute
a useful application domain for semantic parsers.
Requirement texts naturally occur in the real world
and appropriate data sets can thus be constructed
without setting up artificial tasks to collect them.
Parsing requirements of different software projects
also poses interesting challenges as texts exhibit a
considerable amount of lexical variety, while fre-
quently also containing more than one relation per
sentence.
2 Related Work
A range of methods have been proposed in previ-
ous work to (semi-)automatically process require-
ments written in plain, natural language text and
map them to formal representations. To the best
50
of our knowledge, Abbott (1983) was the first to
introduce a technique for extracting data types,
variables and operators from informal texts de-
scribing a problem. The proposed method fol-
lows a simple rule-based setup, in which common
nouns are identified as data types, proper nouns
as objects and verbs as operators between them.
Booch (1986) described a method of similar com-
plexity that extends Abbot?s approach to object-
oriented development. Saeki et al. (1989) imple-
mented a first prototype that automatically con-
structs object-oriented models from informal re-
quirements. As proposed by Abbott and Booch,
the system is based on automatically extracted
nouns and verbs. Although Saeki et al. found re-
sulting object diagrams of reasonable quality, they
concluded that human intervention was still nec-
essary to distinguish between words that are rele-
vant for the model and irrelevant nouns and verbs.
Nanduri and Rugaber (1995) proposed to further
automate object-oriented analysis of requirement
texts by applying a syntactic parser and a set of
post-processing rules. In a similar setting, Mich
(1996) employed a full NLP pipeline that con-
tains a semantic analysis module, thus omitting the
need for additional post-processing rules. More
recent approaches include those by Harmain and
Gaizauskas (2003) and Kof (2004), who relied on
a combination of NLP components and human in-
teraction. Whereas most approaches in previous
work aim to derive class diagrams, Ghosh et al.
(2014) proposed a pipeline architecture that con-
verts syntactic parses to logical expressions via a
set of heuristic post-processing rules.
Despite this seemingly long tradition, previ-
ous methods for processing software requirements
have tended to depend on domain-specific heuris-
tics and knowledge bases or have required addi-
tional user intervention. In contrast, we propose
to utilize annotated data to learn how to perform
semantic parsing of requirements automatically.
3 Data Set
Given our conviction that mapping natural lan-
guage software requirements to formal representa-
tions provides an attractive challenge for semantic
parsing research, we believe that there is a more
general benefit in building a corpus of annotated
requirements. One immediate obstacle is that soft-
ware requirements can drastically differ in quality,
style and granularity. To cover a range of possible
#sentences #tokens #types
student projects 270 3130 604
industrial prototypes 55 927 286
Our dataset (total) 325 4057 765
GEOQUERY880 880 6656 279
FREE917 917 6769 2035
Table 1: Statistics on our requirements collection
and existing semantic parsing data sets.
differences, we asked lecturers from several uni-
versities to provide requirement documents writ-
ten by students. We received requirement docu-
ments on student projects from various domains,
including embedded systems, virtual reality and
web applications.
2
From these documents, we ex-
tracted lists of requirements, each of which is ex-
pressed within a single sentence. We addition-
ally collected single sentence requirements within
the S-CASE project, describing industrial proto-
types of cloud-based web services.
3
Table 1 gives
an overview of the quantity of requirements col-
lected. We observe that the number of require-
ments received for student projects is much higher.
The token counts reveal however that require-
ments written for industrial prototypes are longer
on average (16.6 vs. 11.6 words). This observa-
tion might be related to the fact that students in
software engineering classes are often provided
with explicit guidelines on how to concisely ex-
press requirements in natural language. As a con-
sequence, we also find their requirement texts to
be more regimented and stylised than those writ-
ten by senior software engineers. Examples (2)
and (3) show examples of a student-written and
developer-written requirement, respectively.
(2) The user must be able to vote on polls.
(3) For each user contact, back-end must perform
a check to determine whether the contact is a
registered user or not.
In comparison to two extant data sets, namely
GeoQuery880 (Tang, 2003) and Free917 (Cai and
Yates, 2013), we find that our collection is still rel-
atively small in terms of example sentences. The
2
The majority of collected requirements are
from a software development course organized
jointly by several European universities, cf.
http://www.fer.unizg.hr/rasip/dsd
3
http://www.scasefp7.eu/
51
Concept
OperationType
ThingType
Action
Emergence
Status
Ownership
Property
Participant
Object
Actor
level 1 level 2 level 3
Figure 1: Class hierarchy of our conceptual ontol-
ogy for modeling software requirements.
difference in total number of tokens is not as cru-
cial, however, given that sentences in our data set
are much longer on average. We further observe
that the token/type ratio in our texts lies some-
where between ratios reported in previous work.
Based on the observed lexical variety and average
sentence length, we expect our texts to be chal-
lenging but not too difficult to parse using existing
methods.
4 Modeling Requirements Conceptually
Different representations have been proposed for
modeling requirements in previous work: whereas
early work focused on deriving simple class dia-
grams, more recent approaches suggest represent-
ing requirements via logical forms (cf. Section 2).
In this paper, we propose to model requirements
using a formal ontology that captures general con-
cepts from different application domains. Our pro-
posed ontology covers the same properties as ear-
lier work and provides a means to represent re-
quirements in logical form. In practice, such logi-
cal forms can be induced by semantic parsers and
in subsequent steps be utilized for automatic infer-
ence. The class hierarchy of our ontology is shown
in Figure 1. At the highest level of the class hierar-
chy, we distinguish between ?things? (ThingType)
and ?operations? (OperationType).
4.1 ThingType
We define the following subclasses of ThingType:
? A Participant is a thing that is involved in an
operation. We further subdivide Participants
into Actors, which can be users of a system
or the system itself, and Objects.
? A Property is an attribute of an Object or a
characteristic of an OperationType.
4.2 OperationType
We further divide operations into the following
subclasses:
? An Action describes an operation that is per-
formed by an Actor on one or several Ob-
ject(s).
? A State is an operation that describes the sta-
tus of an Actor.
? Ownership is used to model operations that
express possession.
? Emergence represent operations that undergo
passive transformation.
4.3 Relations
In addition to the class hierarchy, we define a set
of relations between classes, which describe and
constrain how different operations and things can
interact with each other.
On the level of OperationType, every opera-
tion can be assigned one Actor via the relations
HAS ACTOR or HAS OWNER, respectively. Ob-
jects can participate in Actions, States and Owner-
ships via the relations ACTS ON, HAS STATE and
OWNS, respectively. Every instance of Opera-
tionType and Object can further have an arbitrary
number of properties assigned to it via the relation
HAS PROPERTY.
5 Annotation Process
In preliminary annotation experiments, we found
that class diagrams may be too simple to repre-
sent requirements conceptually. Logical forms, on
the other hand, can be difficult to use for anno-
tators without sufficient background knowledge.
To keep the same level of expressiveness as log-
ical forms and the simplicity of object-oriented
annotations, we propose a multi-step annotation
scheme, in which decisions in one iteration are fur-
ther refined in later iterations.
By adopting the class hierarchy introduced in
Section 4, we can naturally divide each annotation
iteration according to a level in the ontology. This
means that in the first iteration, we ask annotators
52
A user that is logged in to his account must be able to update his password.
Actor(user) ? Action(login) ? Action(update)
? Object(account) ? HAS ACTOR(login,user) ? HAS ACTOR(update,user)
? Object(password) ? ACTS ON(login,account) ? ACTS ON(update,password)
? Ownership(o
1
) ? Ownership(o
2
)
? HAS OWNER(o
1
,user) ? HAS OWNER(o
2
,user)
? OWNS(o
1
,account) ? OWNS(o
2
,password)
The system must be able to forward and rewind a playing program.
Actor(system) ? Action(forward) ? Action(rewind)
? Object(program) ? HAS ACTOR(forward,system) ? HAS ACTOR(rewind,system)
? ACTS ON(forward,program) ? ACTS ON(rewind,program)
? Property(playing) ? HAS PROPERTY(program,playing)
Table 2: Example requirements from different domains and logical forms derived from annotations.
A user should be able
login
to his account
ThingType
OperationType
ThingType
Participant
Action
Participant
Actor Object
HAS ACTOR ACTS ON
(implicit)
Ownership
HAS OWNER OWNS
Figure 2: Annotation process: instances are
marked in text (dashed), class assignments are re-
fined (dotted), and relations are added (solid).
to simply mark all instances of ThingType and Op-
erationType that are explicitly expressed in a given
requirement. We then resolve conflicting annota-
tions and present the resulting instances from the
first level to annotators for the next iteration. In
each iteration, we add one layer of sophistication
from the class hierarchy, resulting in step-wise re-
finements. In the final iteration, we add relations
between instances of concepts, including implicit
but inferable cases.
An illustration of the overall annotation process,
based on Example (1), is depicted in Figure 2. The
last iteration in this example involves the addition
of an Ownership instance that is indicated (by the
phrase ?his account?) but not explicitly realized in
text. Although identifying and annotating such in-
stances can be more challenging than the previous
annotation steps, we can directly populate our on-
tology at this stage (e.g., via conversion to RDF
tuples) and run verification tools to check whether
they are consistent with the annotation schema.
6 Discussion
The annotation scheme introduced in Section 4 is
designed with the goal of covering a wide range
of different application domains. Although this
means that many of the more fine-grained distinc-
tions within a domain are not considered here, we
believe that the scheme already provides sufficient
information for a range of tasks. By storing pro-
cessed requirements in a relational database, for
example, they can be retrieved using structured
queries and utilized for probabilistic inference.
Given the hierarchical structure of our annota-
tion process, as defined in Section 5, it is possible
to extend existing annotations with additional lev-
els of granularity provided by domain ontologies.
As an example, we have defined a domain ontol-
ogy for web services, which contains subclasses
of Action to further distinguish between the HTTP
methods get, put, post and delete. Similar exten-
sions can be defined for other domains.
Regarding the task of semantic parsing itself,
we are currently in the process of annotating sev-
eral hundreds of instances of requirements (cf.
Section 3) following the proposed ontology. We
will release an initial version of this data set at
the Semantic Parsing workshop. The initial re-
lease will serve as a basis for training and eval-
uating parsers in this domain, for which we are
also planning to collect more examples through-
out the year. We believe that requirements form
an interesting domain for the parsing community
53
as the texts involve a fair amount of variation and
challenging semantic phenomena (such as infer-
able relations), while also serving a practical and
valuable purpose.
Acknowledgements
Parts of this work have been supported by the FP7
Collaborative Project S-CASE (Grant Agreement
No 610717), funded by the European Commis-
sion. We thank our project partners for data sup-
port and useful discussions on the proposed ontol-
ogy.
References
Russell J Abbott. 1983. Program design by informal
english descriptions. Communications of the ACM,
26(11):882?894.
Barry Boehm and Victor R. Basili. 2001. Software
defect reduction top 10 list. Computer, 34:135?137.
Grady Booch. 1986. Object-oriented develop-
ment. IEEE Transactions on Software Engineering,
(2):211?221.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 423?433, Sofia,
Bulgaria, August.
Shalini Ghosh, Daniel Elenius, Wenchao Li, Patrick
Lincoln, Natarajan Shankar, and Wilfried Steiner.
2014. Automatically extracting requirements spec-
ifications from natural language. arXiv preprint
arXiv:1403.3142.
H. M. Harmain and Robert Gaizauskas. 2003. Cm-
builder: A natural language-based case tool for
object-oriented analysis. Automated Software Engi-
neering, 10(2):157?181.
Leonid Kof. 2004. Natural language processing for
requirements engineering: Applicability to large re-
quirements documents. In 19th International Con-
ference on Automated Software Engineering, Work-
shop Proceedings.
Luisa Mich, Franch Mariangela, and Novi Inverardi
Pierluigi. 2004. Market research for requirements
analysis using linguistic tools. Requirements Engi-
neering, 9(1):40?56.
Luisa Mich. 1996. NL-OOPS: From natural language
to object oriented requirements using the natural lan-
guage processing system LOLITA. Natural Lan-
guage Engineering, 2(2):161?187.
Sastry Nanduri and Spencer Rugaber. 1995. Re-
quirements validation via automated natural lan-
guage parsing. In Proceedings of the Twenty-Eighth
Hawaii International Conference on System Sci-
ences, volume 3, pages 362?368.
Motoshi Saeki, Hisayuki Horai, and Hajime Enomoto.
1989. Software development process from natural
language specification. In Proceedings of the 11th
International Conference on Software Engineering,
pages 64?73.
Lappoon R. Tang. 2003. Integrating Top-down and
Bottom-up Approaches in Inductive Logic Program-
ming: Applications in Natural Language Processing
and Relational Data Mining. Ph.D. thesis, Depart-
ment of Computer Sciences, University of Texas,
Austin, Texas, USA, August.
Axel van Lamsweerde. 2009. Requirements Engineer-
ing: From System Goals to UML Models to Software
Specifications. Wiley.
54
