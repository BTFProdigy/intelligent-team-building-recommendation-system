The Problem of Precision in Restricted-Domain Question-Answering.  
Some Proposed Methods of Improvement 
DOAN-NGUYEN Hai and Leila KOSSEIM 
CLaC Laboratory, Department of Computer Science, Concordia University 
Montreal, Quebec, H3G-1M8, 
Canada 
haidoan@cs.concordia.ca, kosseim@cs.concordia.ca 
 
 
Abstract 
This paper discusses some main difficulties of 
restricted-domain question-answering systems, 
in particular the problem of precision 
performance. We propose methods for 
improving the precision, which can be 
classified into two main approaches: 
improving the Information Retrieval module, 
and improving its results. We present the 
application of these methods in a real QA 
system for a large company, which yielded 
very good results. 
1 Introduction 
Restricted-domain Question-Answering 
(RDQA) works on specific domains and often uses 
document collections restricted in subject and 
volume. It has some characteristics that make 
techniques developed recently for open-domain 
QA, particularly those within TREC (Text 
REtrieval Conference, e.g. (TREC, 2002)) 
competitions, become less helpful. First, in RDQA, 
correct answers to a question may often be found 
in only very few documents. Light et al(2001) 
give evidence that the performance on precision of 
a system depends greatly on the redundancy of 
answer occurrences in the document collection1. 
Second, a RDQA system has often to work with 
domain-specific terminology, including domain-
specific word meaning. Lexical and semantic 
techniques based on general lexicons and thesauri, 
such as WordNet, may not apply well here. Third, 
if a QA system is to be used for a real application, 
e.g. answering questions from clients of a 
company, it should accept complex questions, of 
                                                 
1 For example, they estimate that only about 27% of 
the systems participating in TREC-8 produced a correct 
answer for questions with exactly one answer 
occurrence, while about 50% of systems produced a 
correct answer for questions with 7 answer occurrences. 
(7 is the average answer occurrences per question in the 
TREC-8 collection.) 
various forms and styles. The system should then 
return a complete answer, which can be long and 
complex, because it has to, e.g., clarify the context 
of the problem posed in the question, explain the 
options of a service, give instructions, procedures, 
or suggestions, etc. Contrarily, techniques from 
TREC competitions, aiming at finding short and 
precise answers, are often based on the hypothesis 
that the questions are constituted by a single, and 
often simple, sentence, and can be categorized into 
a well-defined and simple semantic classification 
(e.g. Person, Time, Location, Quantity, etc.). 
RDQA has a long history, beginning with 
systems working over databases (e.g., BASEBALL 
(Green et al 1961) and LUNAR (Woods, 1973)). 
Recently, research in QA has concentrated mostly 
on open-domain QA, in particular on how to find a 
very precise and short answer. Nonetheless, RDQA 
seems to be regaining attention, as shown by this 
ACL workshop. Researchers are also beginning to 
recognize the importance of long and complete 
answers. Lin et al(2003) carried out experiments 
showing that users prefer an answer within context, 
e.g., an answer within its containing paragraph. 
Buchholz and Daelemans (2001) defined some 
types of complex answers, and proposed that the 
system presents a list of good candidates to the 
user, and let him construct the reply by himself. 
Harabagiu et al(2001) mentioned the class of 
questions that need a listing answer. 
One well-known approach for RDQA was 
semantic grammars (Brown and Burton, 1975), 
which build pre-defined patterns of questions for a 
specific task. Simple and easy to implement, this 
approach can only deal with very small tasks, and a 
restricted set of questions. The most popular class 
of techniques for QA ? whether it is restricted-
domain or open-domain, includes using thesauri 
and lexicons, classifying documents, and 
categorizing the questions. Harabagiu et al(2000), 
for example, use WordNet extensively to generate 
keyword alternations and infer the expected answer 
category of a question. 
In this paper, we present several methods to 
improve the precision of a RDQA system which 
should accept freely complex questions and return 
complete answers. We use our experiments in 
developing a real system as demonstration. 
2 Overview of the demonstration system 
The objective of this system is to reply to clients' 
questions on services offered by a large company, 
here Bell Canada. The company provides wide-
range services on telephone, wireless, Internet, 
Web, etc. for personal and enterprise clients. The 
document collection was derived from HTML and 
PDF files from the company's website 
(www.bell.ca). As the structure of these files was 
so complicated, documents were saved as pure text 
with no mark-ups, sacrificing some important 
formatting cues like titles, listings, tables. The 
collection comprises more than 220 documents, of 
a total of about 560K characters. 
The available question set has 140 questions. It 
was assured that every question has an answer 
from the contents of the collection. The form and 
style of the questions vary freely. Most questions 
are composed of one sentence, but some are 
composed of several sentences. The average length 
of questions is 11.3 words (to compare, that of 
TREC questions is 7.3 words). The questions ask 
about what a service is, its details, whether a 
service exists for a certain need, how to do 
something with a service, etc. For the project, we 
divided the question set at random into 80 
questions for training and 60 for testing. Below are 
some examples of questions: 
Do I have a customized domain name even 
with the Occasional Plan of Business Internet 
Dial? 
With the Web Live Voice service, is it possible 
that a visitor activates a call to our company 
from our web pages, but then the call is 
connected over normal phone line? 
It seems that the First Rate Plan is only good 
if most of my calls are in the evenings or 
weekends. If so, is there another plan for long 
distance calls anytime during the day? 
Although our collection was not very large, it 
was not so small either so that a strategy of 
searching the answers directly in the collection 
could be obvious. Hence we first followed the 
classic two-step strategy of QA: information 
retrieval (IR), and then candidate selection and 
answer extraction. For the first step, we used 
Okapi, a well-known generic IR engine 
(www.soi.city.ac.uk/~andym/OKAPI-PACK/, also 
(Beaulieu et al 1995)). For each question, Okapi 
returns an ordered list of answer candidates, 
together with a relevance score for each candidate 
and the name of the document containing it. An 
answer candidate is a paragraph which Okapi 
considers most relevant to the question.2 
The candidates were then evaluated by a human 
judge using a binary scale: correct or incorrect. 
This kind of judgment is recommended in the 
context of communications between a company 
and its clients, because the conditions and technical 
details of a service should be edited as clearly as 
possible in the reply to the client. However we did 
also accept some tolerance in the evaluation. If a 
question is ambiguous, e.g., it asks about phones 
but does not specify whether it pertains to wired 
phones or wireless phones, all correct candidates of 
either case will be accepted. If a candidate is good 
but incomplete as a reply, it will be judged correct 
if it contains the principal theme of the supposed 
answer, and if missing information can be found in 
paragraphs around the candidate's text in the 
containing document. 
Table 1 shows Okapi's performance on the 
training question set. We kept at most the 10 best 
candidates for each question, because after rank 10 
a good answer was very rare. C(n) is the number of 
candidates at rank n which are judged correct. Q(n) 
is the number of questions in the training set which 
have at least one correct answer among the first n 
ranks. As for answer redundancy, among the 45 
questions having at least a correct answer (see 
Q(10)), there were 33 questions (41.3% of the 
entire training set) having exactly 1 correct answer, 
10 questions (12.5%) having 2, and 2 questions 
(2.5%) having 3 correct answers. Table 2 gives 
Okapi's precision on the test question set. 
The results show that Okapi's performance on 
precision was not satisfying, conforming to our 
discussion about characteristics of RDQA above. 
The precision was particularly weak for n's from 1 
to 5. Unfortunately, these are cases that the system 
aims at. n=1 means that only one answer will be 
returned ? a totally automatic system. n=2 to 5 
correspond to more practical scenarios of a semi-
automatic system, where an agent of the company 
chooses the best one among the n candidates, edits 
it, and sends it to the client. We stopped at n=5 
because a greater number of candidates seems too 
heavy psychologically to the human agent. Also 
note that the rank of the candidates is not important 
here, because they would be equally examined by 
the agent. This explains why we used Q(n) to 
measure the precision performance rather than 
                                                 
2 A paragraph is a block of text separated by double 
newlines. As formatted files were saved in plain text, 
original "logical" paragraphs may be joined up into one 
paragraph, which may affect the precision of the 
candidates. 
other well-known scoring such as mean reciprocal 
rank (MRR). 
Examining the correct candidates, we found that 
they were generally good enough to be sent to the 
user as an understandable reply. About 25% of 
them contained superfluous information for the 
corresponding question, while 15% were lacking 
of information. However, only 2/3 of the latter 
(that is 10% of all) looked difficult to be completed 
automatically. Building the answer from a good 
candidate therefore seemed less important than 
improving the precision of the IR module. We 
therefore concentrated on how to improve Q(n), n= 
1 to 5, of the system. 
 
 
n 1 2 3 4 5 6 7 8 9 10 
C(n) 20 11 5 4 9 3 1 1 4 1 
%C(n) 25% 13.8% 6.3% 5% 11.3% 3.8% 1.3% 1.3% 5% 1.3% 
Q(n) 20 26 28 32 39 41 42 43 44 45 
%Q(n) 25% 32.5% 35% 40% 48.8% 51.3% 52.5% 53.8% 55% 56.3% 
Table 1: Precision performance of Okapi on the training question set (80 questions). 
n 1 2 3 4 5 6 7 8 9 10 
C(n) 18 8 7 2 4 3 3 2 1 1 
%C(n) 30% 13.3% 11.7% 3.3% 6.7% 5% 5% 3.3% 1.7% 1.7% 
Q(n) 18 23 28 29 32 33 35 36 36 37 
%Q(n) 30% 38.3% 46.7% 48.3% 53.3% 55% 58.3% 60% 60% 61.7% 
Table 2: Precision performance of Okapi on the test question set (60 questions). 
 
3 Methods for Improving Precision 
Performance 
The first approach to improve the precision 
performance of the IR module is to use a better 
engine, e.g. by adjusting the parameters, modifying 
the formulas of the engine, or replacing a generic 
engine by a more domain-specific one, etc. 
Now suppose that the IR engine is already fixed, 
e.g. because we have achieved the best engine, or, 
more practically, because we cannot make changes 
or afford another engine. The second approach 
consists in improving the results returned by the IR 
engine. One main direction is candidate re-ranking, 
i.e. pushing good candidates in the returned 
candidate list to the first ranks as much as possible, 
thus increasing Q(n). To do this, we need some 
information that can characterize the relevance of a 
candidate to the corresponding question better than 
the IR engine did. The most prominent kind of 
such information may be the domain-specific 
language used in the working domain of the QA 
system, particularly its vocabulary, or even more 
narrowly, its terminological set. 
In the following, we will present our 
development of the second approach on the Bell 
Canada QA system first, because it seems less 
costly than the first one. However, we will present 
some implementations of the first approach later. 
4 Improving Precision by Re-ranking 
Candidates 
We experimented with two methods of re-
ranking, one with a strongly specific 
terminological set, and one with a good document 
characterization. 
4.1 Re-ranking using specific vocabulary 
In the first experiment, we noted that the names 
of specific Bell services, such as 'Business Internet 
Dial', 'Web Live Voice', etc., could be used as a 
relevance characterizing information, because they 
occurred very often in almost every document and 
question, and a service was often presented or 
mentioned in only one or a few documents, making 
these terms very discriminating. To have a generic 
concept, let's call these names 'special terms'. 
Luckily, these special terms occurred normally in 
capital letters, and could be automatically extracted 
easily. After a manual filtering, we obtained more 
than 450 special terms. 
We designed a new scoring system which raises 
the score of the candidates containing occurrences 
of special terms found in the corresponding 
question, as follows: 
(1) Score_of_candidate[i] = DC ? (OW ? 
Okapi_score + RC[i] ? Term_score + 1) 
Thus, the score of candidate i in the ranked list 
returned by Okapi depends on: (i) The original 
Okapi_score given by Okapi, weighted by some 
integer value OW. (ii) A Term_score that 
measures the importance of common occurrences 
of special terms, and, with less emphasis, other 
noun phrases and open-class words, in the question 
and the candidate. It is weighted by some integer 
value RC[i] (for rank coefficient) that represents 
the role of the relative ranking of Okapi. (iii) A 
document coefficient DC that indicates the relative 
importance of a candidate i coming or not coming 
from a document which contains at least a special 
term occurring in the question. DC is thus 
represented by a 2-value pair; e.g., the pair (1, 0) 
corresponds to the extreme case of keeping only 
candidates coming from a document which 
contains at least one special term in the question, 
and throwing out all others. We ran the system 
with 20 different values of DC, 50 of RC, and OW 
from 0 to 60, on the training question set. See 
(Doan-Nguyen and Kosseim, 2004) for a detailed 
explanation of how formula (1) was derived, and 
how to design the values of DC, RC, and OW. 
Formula (1) gave very good improvements on 
the training set (Table 3), but just modest results 
when running the system with optimal training 
parameters on the test set (Table 4). Note: ?Q(n) = 
System's Q(n) ? Okapi?s Q(n); %?Q(n) = 
?Q(n)/Okapi?s Q(n).3 
 
n 1 2 3 4 5 
Q(n) 30 40 42 43 44 
?Q(n) 10 14 14 11 5 
%?Q(n) 50% 53.8% 50% 34.4% 12.8%
Table 3: Best results of formula (1) on the 
training set. 
n 1 2 3 4 5 
Q(n) 22 29 32 33 34 
?Q(n) 4 6 4 4 2 
%?Q(n) 22.2% 26.1% 14.3% 13.8% 6.3% 
Table 4: Results of formula (1) on the test set. 
                                                 
3 Okapi allows one to give it a list of phrases as 
indices, in addition to indices automatically created 
from single words. In fact, the results in Tables 1 and 2 
correspond to this kind of indexing, in which we 
provided Okapi with the list of special terms. These 
results are much better than those of standard indexing, 
i.e. without the special term list.  
 
4.2 Re-ranking with a better document 
characterization 
In formula (1), the coefficient DC represents an 
estimate of the relevance of a document to a 
question based only on special terms; it cannot 
help when the question and document do not 
contain special terms. To find another document 
characterization which can complement this, we 
tried to map the documents into a system of 
concepts. Each document says things about a set of 
concepts, and a concept is discussed in a set of 
documents. Building such a concept system seems 
feasible within closed-domain applications, 
because the domain of the document collection is 
pre-defined, the number of documents is in a 
controlled range, and the documents are often 
already classified topically, e.g. by their creator. If 
no such classification existed, one can use 
techniques of building hierarchies of clusters (e.g. 
those summarized in (Kowalski, 1997)). 
We used the original document classification of 
Bell Canada, represented in the web page URLs, as 
the basis for constructing the concept hierarchy 
and the mapping between it and the document 
collection. Below is a small excerpt from the 
hierarchy: 
BellAll 
 Personal 
  Personal-Phone 
   Personal-Phone-LongDistance 
    Personal-Phone-LongDistance-BasicRate 
    Personal-Phone-LongDistance-FirstRate 
In general, a leaf node concept corresponds to 
one or very few documents talking about it. A 
parent concept corresponds to the union of 
documents of its child concepts. Note that although 
many concepts coincide in fact with a special term, 
e.g. 'First Rate', many others are not special terms, 
e.g. 'phone', 'wireless', 'long distance', etc. 
The use of the concept hierarchy in the QA 
system was based on the following assumption: A 
question can be well understood only when we can 
recognize the concepts implicit in it. For example, 
the concepts in the question: 
It seems that the First Rate Plan is only good 
if most of my calls are in the evenings or 
weekends. If so, is there another plan for long 
distance calls anytime during the day? 
include Personal-Phone-LongDistance and 
Personal-Phone-LongDistance-FirstRate. 
Once the concepts are recognized, it is easy to 
determine a small set of documents relevant to 
these concepts, and carry out the search of answers 
in this set. 
To map a question to the concept hierarchy, we 
postulated that the question should contain words 
expressing the concepts. These words may be those 
constituting the concepts, e.g., 'long', 'distance', 
'first', 'rate', etc., or synonyms/near synonyms of 
them, e.g., 'telephone' to 'phone'; 'mobile', 
'cellphone' to 'wireless'. For every concept, we 
built a bag of words which make up the concept, 
e.g., the bag of words for Personal-Phone-
LongDistance-FirstRate is {'personal', 'phone', 
'long', 'distance', 'first', 'rate'}. We also built 
manually a small lexicon of (near) synonyms as 
mentioned above. 
Now, a question will be analyzed into separate 
words (stop words removed), and we look for 
concepts whose bags of words have elements in 
common with them. (Here we used the Porter 
stemmed form of words in comparison, and also 
counted cases of synonyms/near synonyms.) A 
concept is judged more relevant to a question if: (i) 
its bag of words has more elements in common 
with the question's set of words; (ii) the quotient of 
the size of the common subset mentioned in (i) 
over the size of the entire bag of words is larger; 
and (iii) the question contains more occurrences of 
words in that subset. 
From the relevant concept set, it is 
straightforward to derive the relevant document set 
for a given question. The documents will be ranked 
according to the order of the deriving concepts. (If 
a document is derived from several concepts, the 
highest rank will be used.) As for the coverage of 
the mapping, there were only 4 questions in the 
training set and 6 in the test set (7% of the entire 
question set) having an empty relevant document 
set. In fact, these questions seemed to need a 
context to be understood, e.g., a question like 
'What does Dot org mean?' should be posed in a 
conversation about Internet services. 
Now the score of a candidate is calculated by: 
(2) Score_of_candidate[i] = (CC + DC) ? (OW 
? Okapi_score + RC[i] ? Term_score + 1) 
The value of CC (concept-related coefficient) 
depends on the document that provides the 
candidate. CC should be high if the rank of the 
document is high, e.g. CC=1 if rank=1, CC=0.9 if 
rank=2, CC=0.8 if rank=3, etc. If the document 
does not occur in the concept-derived list, its CC 
should be very small, e.g. 0. The sum (CC + DC) 
represents a combination of the two kinds of 
document characterization. We ran the system with 
15 different values of the CC vector, with CC for 
rank 1 varying from 0 to 7, and CC for other ranks 
decreasing accordingly. Values for other 
coefficients are the same as in the previous 
experiment using formula (1). Results (Tables 5 
and 6) are uniformly better than those of formula 
(1). Good improvements show that the approach is 
appropriate and effective. 
 
n 1 2 3 4 5 
Q(n) 32 41 44 44 44 
?Q(n) 12 15 16 12 5 
%?Q(n) 60% 57.7% 57.1% 37.5% 12.8%
Table 5: Best results of formula (2) on the 
training set. 
n 1 2 3 4 5 
Q(n) 30 32 35 35 36 
?Q(n) 12 9 7 6 4 
%?Q(n) 66.6% 39.1% 25% 20.7% 12.5%
Table 6: Results of formula (2) on the test set. 
5 Two-Level Candidate Searching 
As the mapping in the previous section seems to 
be able to point out the documents relevant to a 
given question with a high precision, we tried to 
see how to combine it with the IR engine Okapi. In 
the previous experiments, the entire document 
collection was indexed by Okapi. Now indexing 
will be carried out separately for each question: 
only the document subset returned by the mapping, 
which usually contains no more than 20 
documents, is indexed, and Okapi will search for 
candidate answers for the question only in this 
subset. We hoped that Okapi could achieve higher 
precision in working with a much smaller 
document set. This strategy can be considered as a 
kind of two-level candidate searching. 
 
n 1 2 3 4 5 
MO Q(n) 18 33 38 45 46 
Q(n) 31 42 48 48 48 
?Q(n) 11 16 20 16 9 
%?Q(n) 55% 61.5% 71.4% 50% 23.1%
Table 7: Best results of two-level search 
combined with re-ranking on the training set. 
n 1 2 3 4 5 
MO Q(n) 20 25 26 29 31 
Q(n) 24 28 32 32 33 
?Q(n) 6 5 4 3 1 
%?Q(n) 33.3% 21.8% 14.3% 10.3% 3.1% 
Table 8: Results of two-level search combined 
with re-ranking on the test set. 
Results show that Okapi did not do better in this 
case than when it worked with the entire document 
collection (compare MO Q(n) in Tables 7 and 8 
with Q(n) in Tables 1 and 2. MO means 'mapping-
then-Okapi'). We then applied formula (2) to 
rearrange the candidate list as in the previous 
section. Although results on the training set (Table 
7) are generally better than those of the previous 
section, results on the test set (Table 8) are worse, 
which leads to an unfavorable conclusion for this 
method. (Note that ?Q(n) and %?Q(n) are always 
comparisons of the new Q(n) with the original 
Okapi Q(n) in Tables 1 and 2.) 
6 Re-implementing the IR engine  
The precision of the question-document mapping 
was good, but the performance of the two-level 
system based on Okapi in the previous section was 
not very persuasive. This led us back to the first 
approach mentioned in Section 3, i.e. replacing 
Okapi by another IR engine. We would not look 
for another generic engine because it was not 
interesting theoretically, but would instead 
implement a two-level engine using the question-
document mapping. As already known, the 
mapping returns just a small set of relevant 
documents for a given question; the new engine 
will search for candidate answers in this set. If the 
document set is empty, the system takes the 
candidates proposed by Okapi as results ("Okapi as 
Last Resort"). 
We implemented just a simple IR engine. First 
the question is analyzed into separate words (stop 
words removed). For every document in the set 
returned by the question-document mapping, the 
system scores each paragraph by counting in this 
paragraph the number of occurrences of words 
which also appear in the question (using the 
stemmed form of words). Here 'paragraph' means a 
block of text separated by one newline, not two as 
in Okapi sense. Note that texts in the Bell Canada 
collection contain a lot of short and empty 
paragraphs. The candidate passage is extracted by 
taking the five consecutive paragraphs which have 
the highest score sum. However, if the document is 
"small", i.e. contains less than 2000 characters, the 
entire document is taken as the candidate and its 
score is the sum of scores of all paragraphs. 
This choice seemed unfair to previous 
experiments because about 60% of the collection 
are such small documents. However, we decided to 
have a more realistic notion of answer candidates 
which reflects the nature of the collection and of 
our current task: in fact, those small documents are 
often dedicated to a very specific topic, and it 
seems necessary to present its contents in its 
entirety to any related question for reasons of 
understandability, or because of important 
additional information in the document. Also, a 
size of 2000 characters (which are normally 70% 
of a page) seems acceptable for a human 
judgement in the scenario of semi-automatic 
systems.4 
Let's call the score calculated as above 
Occurrence_score. We also considered the role of 
the rank of the document in the list returned by the 
question-document mapping. The final score 
formula is as follows: 
(3) Score_of_candidate = RC ? (21 - 
Document_Rank) + Occurrence_score 
The portion (21 - Document_Rank) guarantees 
that high-rank documents contribute high scores. 
That portion is always positive because we 
retained no more than 20 documents for every 
question. RC is a coefficient representing the 
importance of the document rank. Due to time 
limit ? judgement of candidates has to be done 
manually and is very time consuming, we carried 
out the experiment with only RC=0, 1, 1.5, and 2, 
and achieved the best results with RC=1.5. 
Results (Tables 9 and 10) show that except the 
case of n=1 in the test set, the new system 
performs well in precision. This might be 
explained partly because it tolerates larger 
candidates than previous experiments. However 
what is interesting here is that the engine is very 
simple but efficient because it does searching on a 
well selected and very small document subset. 
 
n 1 2 3 4 5 
Q(n) 42 55 60 60 61 
?Q(n) 22 29 32 28 22 
%?Q(n) 110% 112% 114% 88% 56% 
Table 9: Best results of the specific engine on 
the training set. 
n 1 2 3 4 5 
Q(n) 23 37 41 42 42 
?Q(n) 5 14 13 13 10 
%?Q(n) 27.8% 60.9% 46.4% 44.8% 31.3%
Table 10: Results of the specific engine on the 
test set. 
                                                 
4 In fact, candidates returned by Okapi are not 
uniform in length. Some are very short (e.g. one line), 
some are very long (more than 2000 characters). 
7 Second Approach Revisited: Extending 
Answer Candidates 
The previous experiment has shown that 
extending the size of answer candidates can greatly 
ease the task. This can be considered as another 
method belonging to the second approach ? that of 
improving precision performance by improving the 
results returned by the IR engine. To be fair, it may 
be necessary to see how precision performance 
will be improved if this extending is used in other 
experiments. We did two small experiments. In the 
first one, any candidates returned by Okapi (cf. 
Tables 1 and 2) which came from a document of 
less than 2000 characters were extended into the 
entire document. Table 11 shows that 
improvements are not as good as those obtained by 
other methods. 
 
n 1 2 3 4 5 
Q(n) - A 24 32 35 39 47 
?Q(n) 4 6 7 7 8 
%?Q(n) 20% 23.1% 25% 21.9% 20.5%
Q(n) - B 20 27 32 34 37 
?Q(n) 2 4 4 5 5 
%?Q(n) 11.1% 17.4% 14.3% 17.2% 15.6%
Table 11: Results of extending Okapi candidates 
on the training set (A) and test set (B). 
In the second experiment, we similarly extended 
candidates returned by the two-level search process 
"mapping-then-Okapi" in Section 5. Improvements 
(Table 12) seem comparable to those of the 
experiment in Section 5 (Tables 7 and 8), but less 
good than those of experiments in Sections 4.2 and 
6. The two experiments of this section suggest that 
extending candidates helps improve the precision, 
but not so much unless it is combined with other 
methods. We have not yet, however, carried out 
experiments of combining candidate extending 
with re-ranking. 
 
n 1 2 3 4 5 
Q(n) - A 25 43 48 57 60 
?Q(n) 5 17 20 25 21 
%?Q(n) 25% 65.4% 71.4% 78.1% 53.8% 
Q(n) - B 24 31 32 38 41 
?Q(n) 6 8 4 9 9 
%?Q(n) 25% 34.8% 14.3% 31% 23.1% 
Table 12: Results of extending two-level search 
candidates on the training set (A) and test set (B). 
8 Discussions and Conclusions 
RDQA, working on small document collections 
and restricted subjects, seems to be a task no less 
difficult than open-domain QA. Due to candidate 
scarcity, the precision performance of a RDQA 
system, and in particular that of its IR module, 
becomes a problematic issue. It affects seriously 
the entire success of the system, because if most of 
the retrieved candidates are incorrect, it is 
meaningless to apply further techniques of QA to 
refine the answers. 
In this paper, we have discussed several methods 
to improve the precision performance of the IR 
module. They include the use of domain-specific 
terminology to rearrange the candidate list and to 
better characterize the question-document 
relevance relationship. Once this relationship has 
been well established, one can expect to obtain a 
small set of (almost) all relevant documents for a 
given question, and use this to guide the IR engine 
in a two-level search strategy. 
Also, long and complex answers may be a 
common characteristic of RDQA systems. Being 
aware of this, one can design appropriate systems 
which are more tolerant on answer size to achieve 
a higher precision, and to avoid the need of 
expanding a short but insufficient answer into a 
complete one. However, what a good answer 
should be is still an open question, which would 
need a lot more study to clarify. 
We have also presented applications of these 
methods in the real QA system for Bell Canada. 
Good improvements achieved compared to results 
of the original IR module show that these methods 
are applicable and effective. 
Many other problems on the precision 
performance of a RDQA system have not been 
tackled in this paper. Some of them relate to the 
free form of the questions: how to identify the 
category of the question (e.g. the mapping 'Who' ? 
Person, 'When' ? Time, 'How many' ? Quantity, 
etc.), how to analyze the question into pragmatic 
parts (pre-suppositions, problem context, question 
focus), etc. Certainly, they are also problems of 
open-domain QA if one wants to go further than 
pre-defined question pattern tasks. 
 
9 Acknowledgements 
This project was funded by Bell University 
Laboratories (BUL) and the Canada Natural 
Science and Engineering Research Council 
(NSERC). 
References 
Beaulieu M., M. Gatford, X. Huang, S.E. 
Robertson, S. Walker, P. Williams (1995). Okapi 
at TREC-3. In: Overview of the Third Text 
REtrieval Conference (TREC-3). Edited by D.K. 
Harman. Gaithersburg, MD: NIST, April 1995. 
Brown, J., Burton, R. (1975). Multiple 
representations of knowledge for tutorial 
reasoning. In Bobrow and Collins (Eds), 
Representation and Understanding. Academic 
Press, New York. 
Buchholz, S., Daelemans, W. (2001). Complex 
Answers: A Case Study using a WWW Question 
Answering System. Natural Language 
Engineering, 7(4), 2001. 
Doan-Nguyen, H., Kosseim, L. (2004). Improving 
the Precision of a Closed-Domain Question-
Answering System with Semantic Information. 
Proceedings of RIAO (Recherche d'Information 
Assist?e par Ordinateur (Computer Assisted 
Information Retrieval)) 2004. Avignon, France. 
pp. 850-859. 
Green, W., Chomsky, C., Laugherty, K. (1961). 
BASEBALL: An automatic question answerer. 
Proceedings of the Western Joint Computer 
Conference, pp. 219-224. 
Harabagiu, S., D. Moldovan, M. Pasca, R. 
Mihalcea, M. Surdeanu, R. Bunescu, R. G?rju, V. 
Rus, P. Morarescu (2000). FALCON: Boosting 
Knowledge for Answer Engines. Proceedings of 
the Ninth Text REtrieval Conference (TREC 
2000). 
Harabagiu, S., D. Moldovan, M. Pasca, M. 
Surdeanu, R. Mihalcea, R. Girju, V. Rus, F. 
Lactusu, P. Morarescu, R. Bunescu (2001). 
Answering Complex, List and Context Questions 
with LCC's Question-Answering Server. 
Proceedings of the Tenth Text REtrieval 
Conference (TREC 2001). 
Kowalski, G. (1997). Information Retrieval 
Systems ? Theory and Implementation. Kluwer 
Academic Publishers, Boston/Dordrecht/London. 
Light, M., Mann, G., Riloff, E., Breck, E. (2001). 
Analyses for Elucidating Current Question 
Answering Technology. Natural Language 
Engineering, 7(4), 2001. 
Lin, J., Quan, D., Sinha, V., Bakshi, K., Huynh, D., 
Katz, B., Karger, D. (2003). The Role of Context 
in Question Answering Systems. Proceedings of 
the 2003 Conference on Human Factors in 
Computing Systems (CHI 2003), April 2003, Fort 
Lauderdale, Florida. 
TREC (2002). Proceedings of The Eleventh Text 
Retrieval Conference. NIST Special Publication: 
SP 500-251. E. M. Voorhees and L. P. Buckland 
(Eds). 
Woods W. A. (1973). Progress in natural language 
understanding: An application to lunar geology. 
AFIPS Conference Proceedings, Vol. 42, pp. 
441-450. 
Simple Features for Statistical Word Sense Disambiguation
Abolfazl K. Lamjiri, Osama El Demerdash, Leila Kosseim
CLaC Laboratory
Department of Computer Science
Concordia University, Montreal, Canada
{a keigho,osama el,kosseim}@cs.concordia.ca
Abstract
In this paper, we describe our experiments on
statistical word sense disambiguation (WSD)
using two systems based on different ap-
proaches: Na??ve Bayes on word tokens and Max-
imum Entropy on local syntactic and seman-
tic features. In the first approach, we consider
a context window and a sub-window within it
around the word to disambiguate. Within the
outside window, only content words are con-
sidered, but within the sub-window, all words
are taken into account. Both window sizes are
tuned by the system for each word to disam-
biguate and accuracies of 75% and 67% were re-
spectively obtained for coarse and fine grained
evaluations. In the second system, sense res-
olution is done using an approximate syntac-
tic structure as well as semantics of neighbor-
ing nouns as features to a Maximum Entropy
learner. Accuracies of 70% and 63% were ob-
tained for coarse and fine grained evaluations.
1 Introduction
In this paper, we present the two systems we
built for our first participation in the English
lexical sample task at Senseval-3. In the first
system, a Na??ve Bayes learner based on context
words as features is implemented. In the second
system, an approximate syntactic structure, in
addition to semantics of the nouns around the
ambiguous word are selected as features to learn
with a Maximum Entropy learner.
In Section 2, a brief overview of related work
on WSD is presented. Sections 3 and 4 provide
specifications of our two systems. Section 5
discusses the results obtained and remarks on
them, and finally in Section 6, conclusion and
our future work direction are given.
2 Related Work
In 1950, Kaplan carried out one of the earliest
WSD experiments and showed that the accu-
racy of sense resolution does not improve when
more than four words around the target are
considered (Ide and Ve?ronis, 1998). While re-
searchers such as Masterman (1961), Gougen-
heim and Michea (1961), agree with this ob-
servation (Ide and Ve?ronis, 1998), our results
demonstrate that this does not generally ap-
ply to all words. A large context window pro-
vides domain information which increases the
accuracy for some target words such as bank.n,
but not others like different.a or use.v (see Sec-
tion 3). This confirms Mihalcea?s observations
(Mihalcea, 2002). In our system we allow a
larger context window size and for most of the
words such context window is selected by the
system.
Another trend consists in defining and us-
ing semantic preferences for the target word.
For example, the verb drink prefers an ani-
mate subject in its imbibe sense. Boguraev
shows that this does not work for polysemous
verbs because of metaphoric expressions (Ide
and Ve?ronis, 1998).
Furthermore, the grammatical structures the
target word takes part in can be used as a distin-
guishing tool: ?the word ?keep?, can be disam-
biguated by determining whether its object is
gerund (He kept eating), adjectival phrase (He
kept calm), or noun phrase (He kept a record)?
(Reifler, 1955). In our second system we ap-
proximate the syntactic structures of a word, in
its different senses.
Mooney (Mooney, 1996) has discussed the
effect of bias on inductive learning methods.
In this work we also show sensitivity of Na??ve
Bayes to the distribution of samples.
3 Na??ve Bayes for Learning Context
Words
In our approach, a large window and a smaller
sub-window are centered around the target
word. We account for all words within the sub-
window but use a POS filter as well as a short
stop-word list to filter out non-content words
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Figure 1: The effect of choosing different win-
dow and sub-window sizes for the word bank.n.
The best accuracy is achieved with a window
and sub-window size of around 450 and 50 char-
acters respectively, while for example 50 and 25
provide very low accuracy.
from the context. The filter retains only open
class words, i.e. nouns, adjectives, adverbs, and
verbs, and rejects words tagged otherwise.
3.1 Changing the context window size
Figure 1 shows the effect of selecting differ-
ent window and sub-window sizes for the word
bank.n. It is clear that precision is very sensitive
to the selected window size. Other words also
have such variations in their precision results.
The system decides on the best window sizes
for every word by examining possible window
size values ranging from 25 to 750 characters1.
Table 1 shows the optimal window sizes se-
lected for a number of words from different word
classes. The baseline is considered individually
for every word as the ratio of the most com-
mon sense in the training samples. We used the
Senseval-3 training set for the English lexical
sample task for training. It includes a total of
7860 tagged samples for 57 ambiguous words.
15% of this data was used for validation, while
the rest was used for training.
3.2 Approximate Smoothing
During the testing phase, given the context of
the target word, the score of every sense is com-
puted using the Na??ve Bayes formula:
Scoresensei = log p(sensei) +
?
k
log p(wordk)
where, wordk is every word inside the context
window (recall that these are all the words in
1For technical reasons, character is used instead of
word as the unit, making sure no word is cut at the
extremities.
the sub-window, and filtered words in the large
window).
Various smoothing algorithms could be used
to reduce the probability of seen words and dis-
tributing them among unseen words. However,
tuning various smoothing parameters is delicate
as it involves keeping an appropriate amount
of held-out data. Instead, we implemented an
approximate smoothing method, which seems
to perform better compared to Ng?s (Ng, 1997)
approximate smoothing. In our simple approxi-
mate smoothing the probability of seen words
is not discounted to compensate for those of
unseen words2. Finding a proper value to
assign to unseen words was done experimen-
tally; for a relatively large training data set,
p(an unseen word) = 10?10 and for a small
set, 10?9 resulted in the highest accuracy with
our 15% validation set3. The intuition is that,
with a small training set, more unseen words
are likely to be seen during the testing phase,
and in order to prevent the accumulating score
penalty value from becoming relatively high, a
lower probability value is selected. Additionally,
the selection should not result in large differ-
ences in the computed scores of different senses
of the target word.
A simple function assigns 10?10 in any of the
following conditions: the total number of words
seen is larger than 4300, the number of train-
ing instances is greater than 230, or the context
window size is larger than 400 characters. The
function returns 10?9 otherwise.
4 Maximum Entropy learning of
syntax and semantics
Syntactic structures as well as semantics of the
words around the ambiguous word are strong
clues for sense resolution in many words. How-
ever, deriving and using exact syntactic infor-
mation introduces its own difficulties. So, we
tried to use approximate syntactic structures by
learning the following features in a context win-
dow bounded by the last punctuation before and
the first punctuation after the ambiguous word:
1. Article Bef: If there is any article before,
the string token is considered as the value
of this feature.
2. POS, POS Bef, POS Aft: The part of
speech of the target, the part of speech of
the word before (after) if any.
2This results in a total probability mass larger than
1; but still allows us to rank the probability of the senses.
3The logarithm was taken in base 10.
Word WS SW Diff Base Bys Ent
add.v 100 25 4.1 46 69 82
argument.n 175 75 3.1 47 45 54
ask.v 725 150 5.2 36 37 65
decide.v 725 375 5.2 77 65 75
different.a 175 0 4.0 47 34 48
eat.v 550 150 3.1 81 76 86
miss.v 425 125 5.1 28 40 53
simple.a 400 25 9.0 40 11 33
sort.n 175 75 4.0 66 60 71
use.v 50 25 5.6 58 57 79
wash.v 50 25 5.5 56 62 71
Table 1: Optimal window configuration and
performance of both systems for the words
on which Max Entropy has performed bet-
ter than Na??ve Bayes. (WS=Optimal win-
dow size; SW=Optimal sub-window size;
Diff=Average absolute difference between the
distribution of training and test samples;
Accuracy (Base=Baseline; Bys=Na??ve Bayes;
Ent=Max Entropy)).
3. Prep Bef, Prep Aft: The last preposition
before, and the first preposition after the
target, if any.
4. Sem Bef, Sem Aft: The general semantic
category of the noun before (after) the tar-
get. The category, which can be ?animate?,
?inanimate?, or ?abstract?, is computed by
traversing hypernym synsets of WordNet
for all the senses of that noun. The first
semantic category observed is returned, or
?inanimate? is returned as the default value.
The first three items are taken from Mihal-
cea?s work (Mihalcea, 2002) which are useful
features for most of the words. The range of all
these features are closed sets; so Maximum En-
tropy is not biased by the distribution of train-
ing samples among senses, which is a side-effect
of Na??ve Bayes learners (see Section 5.2)4.
The following is an example of the fea-
tures extracted for sample miss.v.bnc.00045286:
? . . . ? I?ll miss the kids. But . . . ?:
Article Bef=null,
POS Bef="MD", POS="VB", POS Aft="DT",
Prep Bef=null, Prep Aft=null,
Sem Bef=null, Sem After="animate"
4The Maximum Entropy program we used to learn
these features was obtained from the OpenNLP site:
http://maxent.sourceforge.net/index.html
Word Na??ve Bayes Max Entropy
Category coarse fine coarse fine
nouns 76% 70% 70% 61%
verbs 76% 67% 74% 66%
adjectives 59% 45% 59% 47%
Total 75% 67% 70% 63%
Table 2: Results of both approaches in fine and
coarse grain evaluation.
5 Results and Discussion
The Word Sense Disambiguator program has
been written as a Processing Resource in the
Gate Architecture5. It uses the ANNIE Tok-
enizer and POS Tagger which are provided as
components of Gate.
Table 2 shows the results of both systems for
each category of words. It can be seen that ap-
proximate syntactic information has performed
relatively better with adjectives which are gen-
erally harder to disambiguate.
5.1 Window size and the commonest
effect
The optimal window size seems to be related
to the distribution of the senses in the train-
ing samples and the number of training sam-
ples available for a word. Indeed, a large win-
dow size is selected when the number of samples
is large, and the samples are not evenly dis-
tributed among senses. Basically because the
words in Senseval are not mostly topical words,
Na??ve Bayes is working strongly with the com-
monest effect. On the other hand, when a small
window size is selected, the commonest effect
mostly vanishes and instead, collocations are re-
lied upon.
5.2 Distribution of samples
A Na??ve Bayes method is quite sensitive to the
proportion of training and test samples: if the
commonest class presented as test is different
from the commonest class in training for ex-
ample, this method performs poorly. This is
a serious problem of Na??ve Bayes towards real
world WSD. For testing this claim, we made the
following hypothesis: When the mean of abso-
lute difference of the test samples and training
samples among classes of senses is more than
4%, Na??ve Bayes method performs at most 20%
above baseline6. Table 3 shows that this hypoth-
esis is confirmed in 82% of the cases (41 words
5http://www.gate.ac.uk/
6The following exceptional cases are not considered:
1) When baseline is above 70%, getting 20% above base-
Acc ? 20 Acc > 20
Dist ? 4.0 5 26
Dist > 4.0 15 4
Table 3: Sensitivity of Na??ve Bayes to the dis-
tribution of samples (Acc=Accuracy amount
higher than baseline; Dist=Mean of distribution
change.)
out of 50 ambiguous words that satisfy the con-
ditions). Furthermore, such words are not nec-
essarily difficult words. Our Maximum Entropy
method performed on average 25% above the
baseline on 7 of them (ask.v, decide.v, differ-
ent.a, difficulty.n, sort.n, use.v, wash.v some of
which are shown in Table 1).
5.3 Rare samples
Na??ve Bayes mostly ignores the senses with a
few samples in the training and gets its score
on the senses with large number of training in-
stances, while Maximum Entropy exploits fea-
tures from senses which have had a few training
samples.
5.4 Using lemmas and synsets
We tried working with word lemmas instead
of derivated forms; however, for some words
it causes loss in accuracy. For example, for
the adjective different.a, with window and sub-
window size of 175 and 0, it reduces the accu-
racy from 60% to 46% with the validation set.
However, for the noun sort.n, precision increases
from 62% to 72% with a window size of 650 and
sub-window size of 50. We believe that some
senses come with a specific form of their neigh-
boring tokens and lemmatization removes this
distinguishing feature.
We also tried storing synsets of words as fea-
tures for the Na??ve Bayes learner, but obtained
no significant change in the results.
6 Conclusion and Future Work
There is no fixed context window size applica-
ble to all ambiguous words in the Na??ve Bayes
approach: keeping a large context window pro-
vides domain information which increases the
resolution accuracy for some target words but
not others. For non-topical words, large win-
dow size is selected only in order to exploit the
distribution of samples.
line is really difficult, 2) When the difference is mostly
on the commonest sense being seen more than expected,
so the score is favored (7 words out of 57 satisfy these
conditions.)
Rough syntactic information performed well
in our second system using Maximum Entropy
modeling. This suggests that some senses can
be strongly identified by syntax, leaving resolu-
tion of other senses to other methods. A simple,
rough heuristic for recognizing when to rely on
syntactic information in our system is when the
selected window size by Na??ve Bayes is relatively
small.
We tried two simple methods for combining
the two methods: considering context words as
features in Max Entropy learner, and, establish-
ing a separate Na??ve Bayes learner for each syn-
tactic/semantic feature and adding their scores
to the basic contextual Na??ve Bayes. These pre-
liminary experiments did not result in any no-
ticeable improvement.
Finally, using more semantic features from
WordNet, such as verb sub-categorization
frames (which are not consistently available)
may help in distinguishing the senses.
Acknowledgments
Many thanks to Glenda B. Anaya and Michelle
Khalife? for their invaluable help.
References
N. Ide and J. Ve?ronis. 1998. Introduction to the
special issue on word sense disambiguation:
the state of the art. Computational Linguis-
tics, 24(1):1?40.
R. Mihalcea. 2002. Instance based learning
with automatic feature selection applied to
word sense disambiguation. In Proceedings of
COLING?02, Taiwan.
R. J. Mooney. 1996. Comparative experiments
on disambiguating word senses: An illustra-
tion of the role of bias in machine learning. In
Proceedings of EMNLP-96, pages 82?91, PA.
H. T. Ng. 1997. Exemplar-based word sense
disambiguation: Some recent improvements.
In Proceedings of EMNLP-97, pages 208?213.
NJ.
E. Reifler. 1955. The mechanical determination
of meaning. In Machine translation of lan-
guages, pages 136?164, New York. John Wi-
ley and Sons.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 610?619, Dublin, Ireland, August 23-29 2014.
Inducing Discourse Connectives from Parallel Texts
Majid Laali and Leila Kosseim
Department of Computer Science and Software Engineering,
Concordia University, Montreal, Quebec, Canada
{m laali, kosseim}@cse.concordia.ca
Abstract
Discourse connectives (e.g. however, because) are terms that explicitly express discourse rela-
tions in a coherent text. While a list of discourse connectives is useful for both theoretical and
empirical research on discourse relations, few languages currently possess such a resource. In
this article, we propose a new method that exploits parallel corpora and collocation extraction
techniques to automatically induce discourse connectives. Our approach is based on identifying
candidates and ranking them using Log-Likelihood Ratio. Then, it relies on several filters to fil-
ter the list of candidates, namely: Word-Alignment, POS patterns, and Syntax. Our experiment
to induce French discourse connectives from an English-French parallel text shows that Syntac-
tic filter achieves a much higher MAP value (0.39) than the other filters, when compared with
LEXCONN resource.
1 Introduction
Discourse relations are often categorized as being implicit or explicit depending on how they are marked
linguistically (Prasad et al., 2008). Implicit relations between two text spans are inferred by the reader
even if they are not explicitly connected through lexical cues. On the other hand, explicit relations
are explicitly identified with syntactically well-defined terms, so called discourse markers or discourse
connectives (DCs). A list of DCs is a valuable resource to help the automatic detection of discourse
relations in a text. Discourse parsers (e.g. (Lin et al., 2010)) often use DCs as a powerful distinguishing
feature to tag discourse relations (Pitler and Nenkova, 2009). A list of DCs is also instrumental in
generating annotated training data which, in turn, is critical for training data-driven parsers (Prasad et al.,
2010).
In this article, we propose an automatic method to induce a list of DCs for one language from a parallel
corpus. We present an experiment in inducing a French DC list from an English-French parallel text. Our
approach is based on the hypothesis that discourse relations are retained during the translation process.
Therefore, if a reliable discourse tagger exists in a language, we can produce a corpus with discourse
annotation labels in any language that has a parallel text with that language. Fortunately, according
to (Versley, 2011), in English, the discourse usage of DCs can be automatically identified and labeled
with their relation with 84% precision; a result that is close to the reported inter-annotator agreement.
Moreover, with the advancement of statistical machine translation, today English parallel corpora for
several languages are publicly available.
Although we can expect little variability in the usage of discourse relations in parallel texts, this is
not the case for DCs. In other words, translated texts may not always reproduce DCs of the source
texts. Since discourse relations can be conveyed either explicitly with a DC or implicitly, a translator
may choose to remove explicit DCs in the source text and express the relation in the translated text
implicitly. In fact, Meyer and Webber (2013) has shown that DCs drop out up to 18% of the times in
human reference translations.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
610
To alleviate noisy data (i.e. sentences whose DCs are dropped during the translation), we have been
inspired by work in collocation extraction (e.g. (Seretan, 2010)). As such, our approach consists of two
main steps: candidate identification and candidate ranking and filtering. We have used several types of
information to filter out incorrect DC candidates and used Log-Likelihood ratio to rank them. These
filters include Part-of-speech tags, syntactic tree and word-alignment. Our results show that syntactic
information outperforms the other filtering methods for the DC identification task.
This paper is organized as follow. Section 2 reviews related work. Section 3 describes our approach
to extract DCs from a parallel text. Section 4 reports detailed experimental results, and finally Section 5
presents our conclusion and future work.
2 Related Work
Currently, publicly available lists of DCs already exist for English (Knott, 1996), Spanish (Alonso Ale-
many et al., 2002), German (Stede and Umbach, 1998), and French (Roze et al., 2012). Typically, these
lists have been manually constructed by applying systematic linguistic tests to a list of potential DCs. For
example, (Roze et al., 2012) gathered a potential list of DCs (about 600 expressions) from English DC
translations and various lists of subordinate conjunctions and prepositions. Then, they applied syntactic,
semantic, and discourse tests to filter this initial list and identify DCs and their associated relations.
A list of DCs can also be created automatically by analyzing lexically-grounded discourse annotated
corpora. The Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) is the largest resource to date that
provides a discourse annotated corpus in English. In this corpus, discourse relations between two text
spans are labeled with a DC. If a discourse relation is expressed without any explicit DC, an inferred DC
which conveys the same discourse relation has been inserted between the text spans. This approach has
been widely adopted to create discourse tree banks in several other languages such as Turkish (Zeyrek et
al., 2010), Chinese (Zhou and Xue, 2012), Arabic (Al-Saif and Markert, 2010), Czech (Mladov?a et al.,
2008), and Hindi (Oza et al., 2009).
Several work have already investigated the use of discourse relations in machine translation (e.g.
(Meyer and Webber, 2013; Meyer, 2011)). Others have attempted to generate discourse annotated cor-
pora from parallel corpora (e.g. (Cartoni, 2013; Meyer, 2011; Popescu-Belis et al., 2012; Versley, 2010;
Zhou et al., 2012)). Among these, the most similar approach to ours is Versley (2010) who has projected
English DCs to their counterparts in German in a parallel corpus. Doing this, he produced a corpus where
discourse vs. non-discourse usage of German DCs were annotated and built a discourse parser from the
corpus. Although Versley (2010) used a list of DCs in generating the dataset, he also tried to automat-
ically induce the DCs from his corpus. However, Versley (2010) did not explicitly evaluate his list of
DCs, but rather focused on his parser. The main difference between our work and Versley (2010) is that
he has solely employed word alignment to find DCs, which as mentioned in his paper, is not sufficient
to align discourse connectives. In contrast, we have used and compared three approaches for inducing a
DC list: word-alignment, POS patterns and syntactic information.
3 Method
Our approach to the extraction of DCs consists of two steps. The first step is the preparation of the
parallel corpus with discourse annotations; the next step is the mining of the parallel corpus to identify
DCs.
3.1 Preparing the Parallel Corpus
Our experiment has focused on building a French list of DCs from English. In order to build the English-
French parallel corpus with discourse annotations, we used the Europarl corpus (Koehn, 2005). The
Europarl corpus contains sentence-aligned texts in 21 European languages that have been extracted from
the proceeding of the European parliament. For our study, we have only considered the English-French
part of this corpus.
To label discourse relations in the parallel text, we have automatically parsed the English side of
the parallel text and assumed that the same relation existed in the French translation. Although this
611
assumption is not directly addressed in previous work, it has been implicitly used by many (e.g. (Cartoni,
2013; Meyer et al., 2011; Popescu-Belis et al., 2012; Versley, 2010; Prasad et al., 2010)). In particular,
Prasad et al. (2010) have suggested to the use of the back-translation technique (translating a text from
language A to language B, then back translate the same text from language B to language A again) to
discover new DCs. In this work, the authors have implicitly assumed that the discourse relations of the
initial text are maintained in the back-translation. We argue that since discourse relations are semantic
and rhetorical in nature, they usually transfer from source language to target language. We have used the
PDTB-style End-To-End Discourse parser (Lin et al., 2010) to parse the English text. This parser has
been trained on Section 02-22 of the PDTB corpus (Prasad et al., 2008) and can identify and label a DC
with its relation with 81.19% precision when tested on Section 23 of the PDTB.
After tagging the English text, we have only kept parallel sentences whose English translation had
exactly one discourse relation. This was done to ensure that no ambiguity would exist in the discourse
relation of the French sentences, once we transfer the discourse relation from English to French. In
other words, we can label each French sentence with a single discourse relation, that of its English
translation. In addition, we have also removed sentences whose discourse relations were expressed
implicitly. Although the (Lin et al., 2010) parser is able to identify both implicit and explicit discourse
relations, we have only considered relations expressed with a DC. This has been done, since not only
the precision of the parser in detecting discourse relation in the absence of DC is very low (24.54%),
but also we would not expect implicit relations to help us to identify DCs in French. In other words, a
translator only occasionally inserts DCs in a translation and therefore we would not expect that too many
DCs would exist in the translation of sentences with an implicit discourse relation.
Table 1 provides statistics on the original English-French Parallel Corpus and the corpus extracted
with exactly one explicit discourse relation per sentence. Initially, the Europarl corpus contained 2,054K
sentences (57 million and 63 million words in the English and the French sides respectively). However,
after removing the sentences with more than one discourse relation, the corpus was reduced to 543K
sentences automatically annotated with discourse relations. The English part of these sentences contains
14 million words, while the French part contains 15 million words.
# Parallel Sentences # English Words # French Words
Original Europarl Corpus 2,054K 57M 63M
Extracted Corpus 543K 14M 15M
Table 1: Statistics on the Parallel Corpora
Although this new annotated corpus represents only 26% of the original French Europarl, the corpus
still represents a large annotated corpus with respect to existing discourse-annotated corpora. For exam-
ple, the corpus is almost 30 times bigger than PDTB. Therefore, due to the large size of the corpus, it
can be expected that eventual errors in the corpus (e.g. sentences whose discourse relations have been
changed during the translation) should not affect the results significantly.
3.2 Mining the Parallel Corpus
Once the aligned corpus has been built, we have mined the French side to identify DCs. To do this, we
have produced an initial list of DC candidates from the corpus; then we have ranked the list based on the
Log-Likelihood Ratio (LLR). Finally, we have applied several filters to refine the final list.
To produce the initial DC candidates, we have extracted n-grams (unigrams, bigrams, ..., and six-
grams) from all French sentences as a potential candidate for a DC. Then, we have stored each potential
candidate with its discourse relation as a pair. For example, in sentence (1) below, the English sentence
contains an ALTERNATIVE relation signaled with the ?So? English DC. We have therefore produced the
pairs ?{ALTERNATIVE, Donc}?, ?{ALTERNATIVE, Donc d}?, ?{ALTERNATIVE, Done d un}?, etc. from
its corresponding French sentence.
612
(1) So, judicially, something needs to be done./ALTERNATIVE
Donc, d?un point de vue judiciaire, il convient de prendre des mesures.
Once the initial list of DC candidates has been extracted, we have used the LLR to rank the DCs
1
.
LLR evaluates association strength between a pair of events based on their frequency. This measure,
for example, has been largely used in collocation extraction (e.g. (Seretan, 2010)). According to Evert
(2004), LLR is equivalent to the average mutual information that one event conveys about the other.
For the sake of completeness, Figure 1 shows the formula used to calculate LLR for two binary random
variables X and Y. Note that in Figure 1, O refers to the observed frequencies, E refers to the expected
frequencies and N refers to the total number of observations.
LLR(X ,Y ) = 2?
2
?
i=1
2
?
j=1
O
i j
? log(
O
i j
E
i j
)
E
i j
=
?
2
k=1
O
ik
?
?
2
k=1
O
k j
N
, N =
2
?
i=1
2
?
j=1
O
i j
Y = v Y = ?v
X = u O
11
O
12
X = ?u O
21
O
22
Figure 1: The formula used to calculate LLR.
In our configuration, our pairs of events consist of the observation of a discourse relation and a DC
candidate. We have computed contingency tables of frequencies of these pairs from the French corpus
and then used the NSP package (Pedersen et al., 2011) to calculate the LLR for each candidate to rank
them. Once the initial list of DCs has been ranked, we have experimented with several types of filters to
refine it.
Frequency Filter: This simple filter tries to account for the fact that low frequent events may affect
the reliability of the LLRmeasure. Therefore, as a simple baseline filter, we have removed DC candidates
that appear less than a certain number of times in the French corpus.
Word-Alignment Filter: This filter removes any DC candidate that does not align with any part of an
English DC. In other words, this filter keeps any consecutive words in the French text if at least one of
its composing words aligns to at least one word of an English DC when using a word-alignment model.
A word-alignment model maps each word in the target text to its translation in the source text (creating
an n-to-one mapping). Therefore, two word-alignment models can be produced (i.e. when the target
text is French (En2Fr) or when the target text is English (Fr2En)). In addition, Och and Ney (2003)
have also presented another word-alignment model called Intersect word-alignment that uses a heuristic
to combine En2Fr and Fr2En word alignments. Figure 2 presents the later alignment for two parallel
sentences. An alignment between two words is shown by a line connecting them. For example, in these
sentences, the connective ?therefore? is aligned to the three French words ?raison pour laquelle?. We
have used MGIZA++ (Gao and Vogel, 2008) to generate En2Fr and Fr2En word-alignments; then used
Moses (Koehn et al., 2007) to compute the Intersect word alignment. In this article, we only consider
Intersect word-alignment, as it is able to map n-to-m mapping
2
.
Syntactic Filters: DCs are defined as syntactically well-defined terms (Prasad et al., 2008). The
syntactic filters exploit this property and remove any constituent that is not categorized as a DC. In other
words, these filters keep only Prepositional Phrases (PP), Coordinate Phrases (CP) or Adverbial Phrases
(ADVP). We have implemented two types of Syntactic Filters. The first one (called POS Filter) uses
predefined POS patterns to filter out incorrect candidates. We have manually defined POS patterns based
on an analysis of the French DCs in the LEXCONN resource (Roze et al., 2012). Table 2 shows the
POS patterns we have used along with an example. The second approach (called Syntax Tree Filter)
makes use of Syntax Trees to filter unlikely syntactic combinations. Therefore, after parsing all the
1
We have also used other association measures, such as PMI, t-score test, and Chi-square test, but LLR achieved the best
results in terms of mean average precision.
2
We have also experimented with other word-alignments but their performances were not better. The Intersect model
outperformed the Fr2En word-alignment model and acheived similar results as the En2Fr word-alignment model.
613
 
French: Le Livre blanc pr?tend r?soudre ces probl?mes , raison pour laquelle nous soutenons les  
 
English: The White Paper intends to resolve these problems and we therefore support these  
 
propositions qu'il contient.  
 
proposals. 
 
Figure 2: Example of Word-Alignments between English and French Texts.
3
French sentences, the Syntax Tree Filter only kept PPs, CPs and ADVPs. We have used the Stanford
POS Tagger (Toutanova et al., 2003) and the Stanford PCFG Parser (Green et al., 2011) for POS tagging
and parsing the French text, respectively.
POS Pattern Example POS Pattern Example
ADV alors P ADV apr`es tout
C et P N par exemple
P comme P P avant de
ADV C encore que V C consid?erant que
ADV P en outre N D P de ce fait
C C parce que P N P de mani`ere `a
N P histoire de P D N dans ce cas
Table 2: POS Patterns Used in the POS Filter.
3.3 Gold Dataset
To evaluate our final ranked list of French DCs candidates and compare the four filters, we have used the
LEXCONN dataset (Roze et al., 2012). This manually constructed dataset includes 467 French discourse
connectives with their syntactic categories and the discourse relations that they express
4
. Table 3 provides
some statistics about LEXCONN. We also provide statistics about the DCs in PDTB for comparative
purposes. Each row of Table 3 indicates the number of DCs and the average number of relations per
DC in parenthesis. For example, in LEXCONN, 70 DCs are unigrams and on average they indicate 1.66
different discourse relations. Table 3 also shows statistics on the length of DCs (in number of words). It
is interesting to note that French tends to have longer DCs than English. Indeed LEXCONN contains 69
DCs that contain four words (e.g.?au m?eme titre que?, ?dans l?espoir de?, etc.) while there are only 4
four-gram DCs in English (e.g. ?as it turns out? or ?on the other hand??).
Although there are fewer relations in PDTB, English DCs tend to be more ambiguous. As Table 3
shows, each English DC conveys 3.05 relations on average, while this number is 1.29 for French DCs.
We also notice that the longer the DC, the less ambiguous it is in terms of discourse relations it can
convey. For example, unigram DCs in French convey on average 1.66 relations, however the number of
relations decreases when the length of the DC increases, so that for a trigram DC, on average, there are
1.22 relations.
3.4 Evaluation Metric
Since our task is very similar to a collocation extraction task, we have used a similar evaluation method-
ology to evaluate our results. We have modeled the task of inducing DCs as a binary classification and
tried to evaluate it using precision and recall. In other words, by choosing a threshold for LLR, we can
3
The examples in this figure are taken from the Europarl corpus.
4
LEXCONN has 431 DCs, however if we consider different spelling of each DC (e.g. ?alors que?? and ?alors qu??), the
number increases to 467.
5
As the parser labels relations at the second level of the PDTB hierarchy, we here report only the number of second level
relations.
614
LEXCONN (French) PDTB DCs (English)
# Discourse relation 29 16
5
# Total number of DCs 467 (1.29) 133 (3.05)
# Unigram DCs 70 (1.66) 76 (3.50)
# Bigram DCs 169 (1.25) 33 (2.70)
# Trigram DCs 139 (1.22) 18 (2.11)
# Four-gram DCs 69 (1.17) 4 (2.50)
# Five-gram DCs 14 (1.07) 1 (1.00)
# Six-gram DCs 5 (1.20) 0 (-)
# Seven-gram DCs 1 (2.00) 1 (1.00)
Table 3: Statistics on Discourse Connectives in LEXCONN and PDTB v.2.
label each potential DC candidate as ?DC? if its LLR is above the threshold or ?non-DC? otherwise.
However, choosing the LLR threshold depends on the application and there is no principled way to de-
termine an ideal value for the threshold. Therefore, we measured the performance of the ranked list of
DCs with 11-point interpolated average precision curve (Manning et al., 2008). This curve shows high-
est precision at the 11 recall levels of 0.0, 0.1, 0.2, ..., 1.0. Using this methodology, we can evaluate the
ranked list without considering any threshold.
In addition to the 11-point interpolated average precision, we also used Mean Average Precision
(MAP) (Manning et al., 2008). As Pecina (2010) noted for the evaluation of collocation extraction,
since the precision is not reliable at low recall levels and changes frequently at high recall levels, we only
consider average precision in the interval of <0.1, 0.9>when we are calculating MAP.
Another consideration when evaluating our final ranked lists is how to evaluate DC fragments. For
example, when evaluating the candidate ?`a ce point?, we have to label it as a wrong DC because it is not
repertoried in LEXCONN. However, it is a segment of the French DC ?`a ce point que? and only one word
is missing in the expression. This issue has been also addressed in the field of collocation extraction; in
particular, Kilgarriff et al. (2010) suggested to consider a partial collocation as a true positive, since it
signals the presence of the longer collocation. However, this ?was not a decision that human evaluators
were comfortable with? (Kilgarriff et al., 2010). In our evaluation, we have used two approaches to
evaluate fragment DCs. In the first approach, the Exact Match approach, we have considered fragment
DCs as an incorrect DC. In the other approach, the Exclude-From-The-List approach, we have removed
them from our list, so that when we analyzed the find list, they do not appear as an incorrect DC.
4 Results
To evaluate the DC extraction approach, we first analyzed the candidate generation step without any
filtering. Table 4 provides the frequency distribution of LEXCONN?s DCs in the annotated corpus.
This table shows that the longer the DCs, the less frequent they are in our corpus. For example, all
one-word DCs of LEXCONN appear in the corpus, while 21% of LEXCONN?s five-gram and 60% of
LEXCONN?s six-gram DCs never occur in the corpus. Overall, 14% of all LEXCONN DCs do not
appear in the corpus.
Recall that the Frequency filter removes DCs that do not appear enough times in order to use LLR
to rank candidates. In our experiment, we used a minimum threshold of 10 for this filter. Therefore,
the filter removed additional 20% DCs, so that overall only 66% of LEXCONN?s DCs are considered
in the corpus. Most of these removed DCs are not common or rather formal expressions in French
such as ?cons?equemment??, ?hormis que?, ?tout bien consid?er?e?. However, several more informal DCs
commonly used in French were also removed, especially in the trigram and more groups of DCs (e.g. ?`a
part c?a?).
Once we calculated the number of available DCs in the corpus, we evaluated the ranked list of DCs
after applying each filter. Table 5 shows the MAP values of each filter using both the Exact Match
615
freq> 10 10? freq> 0 freq = 0
# Unigram DCs 93% 7% 0%
# Bigram DCs 76% 16% 8%
# Trigram DCs 60% 24% 16%
# Four-gram DCs 36% 31% 33%
# Five-gram DCs 50% 29% 21%
# Six-gram DCs 20% 20% 60%
Overall 66% 20% 14%
Table 4: Distribution of LEXCONN DCs in the Extracted Corpus.
Filter MAP with Exact Match MAP with Exclude-From-The-List
LLR only 0.06 0.07
LLR + Word-Alignment Filter 0.10 0.12
LLR + POS Pattern Filter 0.12 0.14
LLR + Syntax Tree Filter 0.39 0.44
Table 5: MAP of Each Filter.
and Exclude-From-The-List approaches to judge fragment DCs
6
(see Section 3.4). With all four filters,
we first used the Frequency Filter and then ranked the candidates using LLR. Our results show that
using the POS Pattern Filters outperforms the Word-Alignment filter. For example, if we consider the
Exact Match metric, the MAP value of the Word-Alignment is 0.10 while it is 0.12 for the POS-Pattern
Filter. As Table 5 shows, the best MAP values are achieved using the Syntax Tree Filter. For the rest
of document, we only consider the Exclude-From-The-List approach to judge fragment DCs, since we
would like to focus on other sources of errors in the ranked list of DCs in addition to the fragment DCs.
After analyzing the list of DCs generated by all approaches, we noted that the size of a DC affects
the performance of our approach. Figure 3 shows the performance of each filter in detecting unigram
(Figure 3a) and bigram (Figure 3b) DCs. These figures shows that except for the Syntax Tree filter, the
performance of the identification of bigram DCs drops rapidly when compared with the identification of
unigram DCs. To better understand why longer DCs are more difficult to identify, we manually analyzed
the errors of each filters. The most significant proportion of errors with bigram DCs is generated from
a unigram DC and a noisy word. For example, ?mais je? is composed of the French DC ?mais? and a
noisy word ?je?. As these errors usually do not create a syntactic well-defined constituent, they can only
be filtered out by the Syntax Tree Filter.
The POS pattern filter cannot detect noisy syntactic components since detecting such components
needs contextual syntactic information. When we analyzed negative examples of this filter, we noticed
that most of bigram errors are comprised of two words that belong to two different chunks. For example,
in sentence (2) below, the POS pattern ?ADV C? extracts ?donc que?, but these two words belong to two
different syntactic constituents (i.e ADV and Ssub).
(2) VN [Je demande] ADV [donc] Ssub[que l?on soutienne l?Irlande dans ce cas particulier].
It is interesting to note that the ranked list created with the Syntax Tree Filter includes several DCs
that do not appear in the LEXCONN lexicon but are nevertheless correct DCs in French. Among the
top 100 candidates labeled as an incorrect DC, we have found 31 correct DCs which are not listed in
LEXCONN, such as?toutefois?, ?certes? and ?au lieu de cela?. The work of (Roze et al., 2012) (or
any manually curated list of DCs) constitutes an invaluable resource. However, as Prasad et al. (2010)
mentioned, DCs are open-class terms. Therefore, our approach to induce DCs from parallel texts can be
6
When calculating recall points, we only considered the available DCs in the dataset after applying the Frequency Filter (i.e.
66% of DCs).
616
617
References
Amal Al-Saif and Katja Markert. 2010. The Leeds Arabic Discourse Treebank: Annotating Discourse Connec-
tives for Arabic. In LREC, pages 2046?2053, Valletta, Malta.
Laura Alonso Alemany, Irene Castell?on Masalles, and Llu`?s Padr?o Cirera. 2002. Lexic?on computacional de
marcadores del discurso. Procesamiento del Lenguaje Natural, 29.
Bruno Cartoni. 2013. Annotating the meaning of discourse connectives by looking at their translation: The
translation-spotting technique. Dialogue & Discourse, 4(2):65?86.
Stefan Evert. 2004. The Statistics of Word Cooccurrences: Word Pairs and Collocations. PhD dissertation,
Institut fr Maschinelle Sprachverarbeitung, University of Stuttgart.
Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. In Software Engineering,
Testing, and Quality Assurance for Natural Language Processing, pages 49?57, Columbus, OH, USA.
Spence Green, Marie-Catherine de Marneffe, John Bauer, and Christopher D. Manning. 2011. Multiword Expres-
sion Identification with Tree Substitution Grammars: A Parsing tour de force with French. In Proceedings of
the Conference on Empirical Methods in Natural Language Processing, pages 725?735, Edinburgh, Scotland,
UK. Association for Computational Linguistics.
Adam Kilgarriff, Vojtch Kov, Simon Krek, Irena Srdanovi, and Carole Tiberius. 2010. A Quantitative Evaluation
of Word Sketches. In Proceedings of the 14th EURALEX International Congress, Leeuwarden, The Nether-
lands.
Alistair Knott. 1996. A data-driven methodology for motivating a set of coherence relations. PhD dissertation,
University of Edinburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual
Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177?180. ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5,
pages 79?86.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(02):151?184.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to Information Retrieval,
volume 1. Cambridge University Press.
Thomas Meyer and Bonnie Webber. 2013. Implicitation of Discourse Connectives in (machine) Translation. In
Proceedings of the 1st DiscoMT Workshop at ACL 2013 (51st Annual Meeting of the Association for Computa-
tional Linguistics), pages 19?26, Sofia, Bulgaria.
ThomasMeyer, Charlotte Roze, Bruno Cartoni, L. Danlos, and A. Popescu-Belis. 2011. Disambiguating discourse
connectives using parallel corpora: senses vs. translations. In Proceedings of Corpus Linguistics.
Thomas Meyer. 2011. Disambiguating Temporal-Contrastive Discourse Connectives for Machine Translation. In
Proceedings of ACL-HLT, pages 46?51, Portland, OR, USA.
Lucie Mladov?a, Sarka Zikanova, and Eva Hajicov?a. 2008. From Sentence to Discourse: Building an Annota-
tion Scheme for Discourse Based on Prague Dependency Treebank. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation (LREC?08), pages 28?30, Morocco, Marrakech.
F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational
linguistics, 29(1):19?51.
Umangi Oza, Rashmi Prasad, Sudheer Kolachina, Dipti Misra Sharma, and Aravind Joshi. 2009. The Hindi
Discourse Relation Bank. In Proceedings of the Third Linguistic Annotation Workshop, pages 158?161, Suntec,
Singapore.
P. Pecina. 2010. Lexical association measures and collocation extraction. Language Resources and Evaluation,
44(1):137?158.
618
T. Pedersen, S. Banerjee, B. T. McInnes, S. Kohli, M. Joshi, and Y. Liu. 2011. The Ngram Statistics Package
(text:: NSP)-A Flexible Tool for Identifying Ngrams, Collocations, and Word Associations. In Workshop on
Multiword Expression: from Parsing and Generation to the Real World (MWE 2011), pages 131?133, Portland,
OR, USA.
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers , pages, pages 13?16, Suntec, Singapore.
Andrei Popescu-Belis, Thomas Meyer, Jeevanthi Liyanapathirana, Bruno Cartoni, and Sandrine Zufferey. 2012.
Discourse-level Annotation over Europarl for Machine Translation: Connectives and Pronouns. In Proceed-
ings of the Eight International Conference on Language Resources and Evaluation (LREC?12), pages 23?25,
Istanbul, Turkey.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K. Joshi, and Bonnie L.
Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC?08), pages 28?30, Marrakech, Morocco.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2010. Realization of Discourse Relations by Other Means:
Alternative Lexicalizations. In COLING ?10, pages 1023?1031, Beijing, China.
Charlotte Roze, Laurence Danlos, and Philippe Muller. 2012. LEXCONN: a French lexicon of discourse connec-
tives. Discours [En ligne], (10).
V. Seretan. 2010. Syntax-Based Collocation Extraction, volume 44. Springer-Verlag.
Manfred Stede and Carla Umbach. 1998. DiMLex: A lexicon of discourse markers for text generation and
understanding. In Proceeding of the 17th international conference on Computational Linguistics (COLING-
98), pages 1238?1242, Montreal, Canada. Association for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich Part-of-speech
Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pages 173?180, Edmonton.
Association for Computational Linguistics.
Yannick Versley. 2010. Discovery of ambiguous and unambiguous discourse connectives via annotation projec-
tion. In Proceedings of Workshop on Annotation and Exploitation of Parallel Corpora (AEPC), pages 83?82,
Tartu, Estonia. Northern European Association for Language Technology (NEALT).
Yannick Versley. 2011. Towards Finer-grained Tagging of Discourse Connectives. In Beyond Semantics: Corpus-
based investigations of pragmatic and discourse phenomena.
Deniz Zeyrek, In Demirahin, Ay Sevdik-all, Hale gel Balaban, hsan Yalnkaya, and mit Deniz Turan. 2010. The an-
notation scheme of the Turkish Discourse Bank and an evaluation of inconsistent annotations. In Proceedings of
the Fourth Linguistic Annotation Workshop, pages 282?289, Uppsala, Sweden. Association for Computational
Linguistics.
Yuping Zhou and Nianwen Xue. 2012. PDTB-style Discourse Annotation of Chinese Text. In Proceedings of the
50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 69?77,
Jeju, Republic of Korea. Association for Computational Linguistics.
Lanjun Zhou, Wei Gao, Binyang Li, Zhongyu Wei, and Kam-Fai Wong. 2012. Cross-lingual identification of
ambiguous discourse connectives for resource-poor language. In Proceedings of COLING 2012.
619
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 108?113, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ClaC: Semantic Relatedness of Words and Phrases
Reda Siblini
Concordia University
1400 de Maisonneuve Blvd. West
Montreal, Quebec, Canada, H3G 1M8
r_sibl@encs.concordia.ca
Leila Kosseim
Concordia University
1400 de Maisonneuve Blvd. West
Montreal, Quebec, Canada, H3G 1M8
kosseim@encs.concordia.ca
Abstract
The measurement of phrasal semantic relat-
edness is an important metric for many nat-
ural language processing applications. In this
paper, we present three approaches for mea-
suring phrasal semantics, one based on a se-
mantic network model, another on a distribu-
tional similarity model, and a hybrid between
the two. Our hybrid approach achieved an F-
measure of 77.4% on the task of evaluating
the semantic similarity of words and compo-
sitional phrases.
1 Introduction
Phrasal semantic relatedness is a measurement of
how multiword expressions are related in meaning.
Many natural language processing applications
such as textual entailment, question answering, or
information retrieval require a robust measurement
of phrasal semantic relatedness. Current approaches
to address this problem can be categorized into three
main categories: those that rely on a knowledge
base and its structure, those that use the distribu-
tional hypothesis on a large corpus, and hybrid
approaches. In this paper, we propose supervised
approaches for comparing phrasal semantics that are
based on a semantic network model, a distributional
similarity model, and a hybrid between the two.
Those approaches have been evaluated on the task
of semantic similarity of words and compositional
phrases and on the task of evaluating the composi-
tionality of phrases in context.
2 Semantic Similarity of Words and
Compositional Phrases
The semantic similarity of words and compositional
phrases is the task of evaluating the similarity of a
word and a short phrase of two or more words; for
example, the word Interview and the phrase Formal
Meeting. In the next section we present our seman-
tic network model for computing phrasal semantic
relatedness between a word and a phrase, followed
by a distributional similarity model, that we evalu-
ate on the task of semantic similarity of words and
compositional phrases.
2.1 Semantic Network Model
Knowledge-based approaches to semantic related-
ness use the features of the knowledge base to mea-
sure the relatedness. One of most frequently used
semantic network is the Princeton?s WordNet (Fell-
baum, 1998) which groups words into synonyms
sets (called synsets) and includes 26 semantic rela-
tions between those synsets, including: hypernymy,
hyponymy, meronymy, entailment . . .
To measure relatedness, most of those approaches
rely on the structure of the semantic network, such
as the semantic link path, depth (Leacock and
Chodorow, 1998; Wu and Palmer, 1994), direction
(Hirst and St-Onge, 1998), or type (Tsatsaronis et
al., 2010). Our phrasal semantic relatedness ap-
proach is inspired from those methods. However,
our approach is based on the idea that the combi-
nation of the least costly types of relations that re-
late one concept to a set of concepts are a suitable
indicator of their semantic relatedness. The type
of relations considered includes not only the hy-
108
Figure 1: Example of the semantic network around the word car.
ponym/hypernym relations but also all 26 available
semantic relations found in WordNet in addition to
relations extracted from each of the eXtended Word-
Net (Harabagiu et al, 1999) synset?s logical form.
To implement our idea, we created a weighted and
directed semantic network based on the relations of
WordNet and eXtended WordNet. We used Word-
Net?s words and synsets as the nodes of the network.
Each word is connected by an edge to its synsets,
and each synset is in turn connected to other synsets
based on the semantic relations included in Word-
Net. In addition each synset is connected by a la-
beled edge to the predicate arguments that are ex-
tracted from the eXtended WordNet synset?s logical
form. Every synset in the eXtended WordNet is re-
lated to a logical form, which contains a set of pred-
icate relations that relates the synset to set of words.
Each predicate in this representation is added as an
edge to the graph connecting the synset to a word.
For example, Figure 1 shows part of the semantic
network created around the word car. In this graph,
single-line ovals represent words, while double-line
ovals represent synsets.
To compute the semantic relatedness between
nodes in the semantic network, it is necessary to take
into consideration the semantic relation involved be-
tween two nodes. Indeed, WordNet?s 26 semantic
relations are not equally distributed nor do they con-
tribute equally to the semantic relatedness between
concept. In order to indicate the contribution of each
relation, we have classified them into seven cate-
gories: Similar, Hypernym, Sense, Predicate, Part,
Instance, and Other. By classifying WordNet?s re-
lations into these classes, we are able to weight
the contribution of a relation based on the class it
belongs to, as opposed to assigning a contributory
weight to each relations. The weights were assigned
by manually comparing the semantic features of a
set of concepts that are related by a specific seman-
tic relations. Table 1 shows the seven semantic cat-
egories that we defined, their corresponding weight,
and the relations they include. For example the cat-
egory Similar includes WordNet?s relations of en-
tailment, cause, verb group, similar to, participle of
verb, antonym, and pertainym. This class of rela-
tions has the most common semantic features when
comparing two concepts related with any of those
relations and hence was assigned the lowest weight1
of 1. All the 26 relations in the table are the ones
found in WordNet, for the exception of the predicate
(and inverse predicate) relations which are the predi-
cate relations extracted from the eXtended WordNet.
This can be seen in Figure 1, for example, where the
word car is related to the word Engine with the Pred-
icate relation extracted from the eXtended WordNet
logical form and more specifically the predicate pro-
pel by.
The computation of semantic relatedness be-
tween a word and a compositional phrase is then
the combination of weights of the shortest weighted
path2 in the weighted semantic network between
that word and every word in that phrase, normalized
by the maximum path cost.
1The weight can be seen as the cost of traversing an edge;
hence a lower weight is assigned to a highly contributory rela-
tion.
2The shortest path is based on an implementation of Dijk-
stras graph search algorithm (Dijkstra, 1959)
109
Category Weight Semantic Relations in WordNet or xWordnet
Similar 1 similar to, pertainym, participle of verb, entailment, cause,
antonym, verb group
Hypernym 2 hypernym, instance hypernym, derivationally related
Sense 4 lemma-synset
Predicate 6 predicate (extracted from Extended WordNet)
Part 8 holonym (instance, member, substance), meronym (instance,
member, substance), inverse predicate (extracted from Extended
WordNet)
Instance 10 hyponym, instance hyponym
Other 12 attribute, also see, domain of synset (topic, region, usage), member
of this domain (topic, region, usage)
Table 1: Relations Categories and Corresponding Weights.
Figure 2 shows an extract of the network involv-
ing the words Interview and the phrase Formal Meet-
ing. For the shortest path from Interview to Formal,
the word Interview is connected with a Sense rela-
tion to the synset #107210735 [Interview]. As in-
dicated in Table 1, the weight of this relation is de-
fined as 4, This synset is connected to the synset Ex-
amination through a Hypernym relation type with a
weight of 2, which is connected to the word Formal
with a predicate (IS) relation of weight 6. Overall,
the sum of the shortest path from Interview to For-
mal Meeting is hence equal to the sum of the edges
shown in Figure 1 (4+2+6+4+6+4+6 = 32). By nor-
malizing the sum to the maximum, In our approach,
24 is maximum path cost after which we assume
that two words are not related (which we assume to
be traversing two times maximum weighted path, 2
* maximum path weight of 12) and 8 is the mini-
mum number of edges between 2 words (which is
equal to traversing from the word to itself, 2 * sense
weight of 4)). Taking into consideration the number
of words in the phrase, the semantic relatedness will
be (24*2 - (32-8*2))/24*2 = 66.7%. In the next sec-
tion, we will introduce our distributional similarity
model.
2.2 Distributional Similarity Model
Distributional similarity models rely on the distribu-
tional hypothesis (Harris, 1954) to represent a word
by its context in order to compare word semantics.
There are various approach for the selection, repre-
sentation, and comparison of contextual data. Most
use the vector space model to represent the context
as dimensions in a vector space, where the feature
are frequency of co-occurrence of the context words,
and the comparison is usually the cosine similar-
ity. To go beyond lexical semantics and to repre-
sent phrases, a compositional model is created, some
use the addition or multiplication of vectors such
as Mitchell and Lapata (2008), or the use of tensor
product to account for word order as in the work of
Widdows (2008), or a more complex model as the
work of Grefenstette and Sadrzadeh (2011). In our
model, we are inspired by those various work, and
more specifically by the work of Mitchell and Lapata
(2008). The compositional model is based on phrase
words vectors addition, where each vector is com-
posed of the collocation pointwise mutual informa-
tion of the word up to a window of 3 words left and
right of the main word. The corpus used to collect
the features and their frequencies is the Web 1TB
corpus (Brants and Franz, 2006). For the Interview
to Formal Meeting example, the vector of the word
interview is first created from the corpus of the top
1000 words collocating interview between the win-
dow of 1 to 3 words with their frequencies. A similar
vector is created for the word Formal and the word
Meeting, the vector representing Formal Meeting is
then the addition of vector Formal to vector Meet-
ing. The comparison of vector Interview to vector
Formal Meeting is then the cosine of both vectors.
110
2.3 Evaluation
We evaluated our approaches for word-phrase se-
mantic relatedness on the SemEval task of evalu-
ating phrasal semantics, and more specifically on
the sub-task of evaluating the semantic similarity
between words and phrases. The task provided an
English dataset of 15,628 word-phrases, 60% an-
notated for training and 40% for testing, with the
goal of classifying each word-phrase as either pos-
itive or negative. To transform the semantic relat-
edness measure to a semantic similarity classifica-
tion one, we first calculated the semantic relatedness
of each word-phrase in the training set, and used
JRip, WEKA?s (Witten et al, 1999) implementation
of Cohen?s RIPPER rule learning algorithm (Cohen
and Singer, 1999), in order to learn a set of rules that
can differentiate between a positive semantic simi-
larity and a negative one. The classifier resulted in
rules for the semantic network model based related-
ness that could be summarized as follows: If the se-
mantic relatedness of the word-phrase is over 61%
then the similarity is positive, otherwise it is nega-
tive. So for the example Interview - Formal meeting,
which resulted in a semantic relatedness of 66.7% in
the semantic network approach, it will be classified
positively by the generated rule. This method was
our first submitted test run to this task, which re-
sulted in a recall of 63.79%, a precision of 91.01%,
and an F-measure of 75.00% on the testing set.
For the second run, we trained the distributional
similarity model using the same classifier. This re-
sulted with the following rule that could be summa-
rized as follows: If the semantic relatedness of the
word-phrase is over 40% then the similarity is pos-
itive, otherwise it is negative. It was obvious from
the training set that the semantic network model
was more accurate than the distributional similarity
model, but the distributional model had more cover-
age. So for our second submitted test run, we used
the semantic network approach as the main result,
but used the distributional model as a backup ap-
proach if one of the words in the phrase was not
available in WordNet, thus combining the precision
and coverage of both approaches. This method re-
sulted in a recall of 69.48%, a precision of 86.70%,
and an F-measure of 77.14% on the testing set.
For the last run, we used the same classifier
but this time we training it using two features:
the semantic network model relatedness measure
(SN), and the distributional similarity model (DS).
This training resulted in a set of rules that could
be summarized as follows: if SN > 61% then the
similarity is positive, else if DS > 40% then the
similarity is also positive, and lastly if SN > 53%
and DS > 31% then also in this case the similarity
is positive, otherwise the similarity is negative. This
was our third submitted test run, which resulted a
recall of 70.66%, a precision of 85.55%, and an
F-measure of 77.39% on the testing set.
3 Semantic Compositionality in Context
The semantic compositional in context is the task of
evaluating if a phrase is used literally or figuratively
in context. For example, the phrase big picture is
used literally in the sentence Click here for a bigger
picture and figuratively in To solve this problem, you
have to look at the bigger picture.
Our approach for this task is a supervised ap-
proached based on two main components: first, the
availability of the phrases most frequent collocating
expressions in a large corpus, and more specifically
the top 1000 phrases by frequency in Web 1TB cor-
pus (Brants and Franz, 2006). For example, for the
phrase big picture, we collect the top 1000 phrases
that come before and after the phrase in a corpus,
those includes look at the, see the, understand the
..... If the context contain any of those phrase, then
this component returns 1, indicating that the phrase
is most probably used figuratively. The intuition is
that, the use of phrases figuratively is more frequent
than their use in a literal meaning, and hence the
most frequent use will be collocated with phrases
that indicate this use.
The second component, is the phrase compositional-
ity. We calculate the semantic relatedness using the
semantic network model relatedness measure, that
was explained in Section 2.1, between the phrase
and the first content word before it and after it. The
intuition here is that the semantic relatedness of the
figurative use of the phrase to its context should be
different than the relatedness to its literal use. So
for the example, the phrase old school in the con-
text he is one of the old school versus the hall of
111
Figure 2: Shortest Path Between the Word Interview and the Phrase Formal Meeting.
the old school, we can notice that hall will be more
related to old school than the word one. This compo-
nent will result in two features: the relatedness to the
word before the phrase (SRB) and the relatedness to
word after the phrase in context (SRA).
To combine both componenets, we evaluated our
approaches on the data set presented by the Se-
mEval task of evaluating phrasal semantics, and
more specifically on the sub task of evaluating se-
mantic compositionality in context. The data set
contains a total of 1114 training instances, and 518
test instances. We use the training data and com-
puted the three features (Frequent Collocation (FC),
Semantic Relatedness word Before (SRB), and Se-
mantic Relatedness word After (SRA), and used
JRip, WEKA?s (Witten et al, 1999) implementation
of Cohen?s RIPPER rule learning algorithm (Cohen
and Singer, 1999) to learn a set of rule that differen-
tiate between a figurative and literal phrase use. This
method resulted in a set of rules that can be summa-
rized as follows: if FC is equal to 0 and SRB < 75%
then it is used literally in this context, else if FC is
equal to 0 and SRA < 75% then it is is also used lit-
erally, otherwise it is used figuratively. This method
resulted in an accuracy of 55.01% on the testing set.
4 Conclusion
In this paper we have presented state of the art
word-phrase semantic relatedness approaches that
are based on a semantic network model, a distribu-
tional model, and a combination of the two. The
novelty of the semantic network model approach is
the use of the sum of the shortest path between a
word and a phrase from a weighted semantic net-
work to calculate word-phrase semantic relatedness.
We evaluated the approach on the SemEval task of
evaluating phrasal semantics, once in a supervised
standalone configuration, another with a backup dis-
tributional similarity model, and last in a hybrid con-
figuration with the distributional model. The hy-
brid model achieved the highest f-measure in those
three configuration of 77.4% on the task of evaluat-
ing the semantic similarity of words and composi-
tional phrases. We also evaluated this approach on
the subtask of evaluating the semantic composition-
ality in context with less success, and an accuracy of
of 55.01%.
Acknowledgments
We would like to thank the reviewers for their sug-
gestions and valuable comments.
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1.
William W Cohen and Yoram Singer. 1999. A simple,
fast, and effective rule learner. In Proceedings of the
National Conference on Artificial Intelligence, pages
335?342. John Wiley & Sons Ltd.
Edsger W Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische mathematik,
1(1):269?271.
112
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1394?1404. Association for
Computational Linguistics.
Sanda Harabagiu, George Miller, and Dan Moldovan.
1999. Wordnet 2- a morphologically and semantically
enhanced resource. In Proceedings of SIGLEX, vol-
ume 99, pages 1?8.
Zellig S Harris. 1954. Distributional structure. Word.
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. WordNet An electronic lexi-
cal database, pages 305?332, April.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265?283.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236?244.
George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-
giannis. 2010. Text relatedness based on a word
thesaurus. Journal of Artificial Intelligence Research,
37(1):1?40.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In To appear in Second
AAAI Symposium on Quantum Interaction, volume 26,
page 28th. Citeseer.
Ian H Witten, Eibe Frank, Leonard E Trigg, Mark A Hall,
Geoffrey Holmes, and Sally Jo Cunningham. 1999.
Weka: Practical machine learning tools and techniques
with java implementations.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics, pages 133?138, New Mexico, June.
113
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 44?52,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Discrepancy Between Automatic and Manual Evaluation of Summaries
Shamima Mithun, Leila Kosseim, and Prasad Perera
Concordia University
Department of Computer Science and Software Engineering
Montreal, Quebec, Canada
{s mithun, kosseim, p perer}@encs.concordia.ca
Abstract
Today, automatic evaluation metrics such as
ROUGE have become the de-facto mode of
evaluating an automatic summarization sys-
tem. However, based on the DUC and the TAC
evaluation results, (Conroy and Schlesinger,
2008; Dang and Owczarzak, 2008) showed
that the performance gap between human-
generated summaries and system-generated
summaries is clearly visible in manual eval-
uations but is often not reflected in automated
evaluations using ROUGE scores. In this pa-
per, we present our own experiments in com-
paring the results of manual evaluations ver-
sus automatic evaluations using our own text
summarizer: BlogSum. We have evaluated
BlogSum-generated summary content using
ROUGE and compared the results with the
original candidate list (OList). The t-test re-
sults showed that there is no significant differ-
ence between BlogSum-generated summaries
and OList summaries. However, two man-
ual evaluations for content using two different
datasets show that BlogSum performed signif-
icantly better than OList. A manual evaluation
of summary coherence also shows that Blog-
Sum performs significantly better than OList.
These results agree with previous work and
show the need for a better automated sum-
mary evaluation metric rather than the stan-
dard ROUGE metric.
1 Introduction
Today, any NLP task must be accompanied by a
well-accepted evaluation scheme. This is why, for
the last 15 years, to evaluate automated summariza-
tion systems, sets of evaluation data (corpora, topics,
. . . ) and baselines have been established in text sum-
marization competitions such as TREC1, DUC2, and
TAC3. Although evaluation is essential to verify the
quality of a summary or to compare different sum-
marization approaches, the evaluation criteria used
are by no means universally accepted (Das and Mar-
tins, 2007). Summary evaluation is a difficult task
because no ideal summary is available for a set of
input documents. In addition, it is also difficult to
compare different summaries and establish a base-
line because of the absence of standard human or
automatic summary evaluation metrics. On the other
hand, manual evaluation is very expensive. Accord-
ing to (Lin, 2004), large scale manual evaluations of
all participants? summaries in the DUC 2003 confer-
ence would require over 3000 hours of human efforts
to evaluate summary content and linguistic qualities.
The goal of this paper is to show that the literature
and our own work empirically point out the need for
a better automated summary evaluation metric rather
than the standard ROUGE metric4 (Lin, 2004).
2 Current Evaluation Schemes
The available summary evaluation techniques can
be divided into two categories: manual and auto-
matic. To do a manual evaluation, human experts as-
sess different qualities of the system generated sum-
maries. On the other hand, for an automatic eval-
1Text REtrieval Conference: http://trec.nist.gov
2Document Understanding Conference: http://duc.nist.gov
3Text Analysis Conference: http://www.nist.gov/tac
4http://berouge.com/default.aspx
44
uation, tools are used to compare the system gen-
erated summaries with human generated gold stan-
dard summaries or reference summaries. Although
they are faster to perform and result in consistent
evaluations, automatic evaluations can only address
superficial concepts such as n-grams matching, be-
cause many required qualities such as coherence and
grammaticality cannot be measured automatically.
As a result, human judges are often called for to
evaluate or cross check the quality of the summaries,
but in many cases human judges have different opin-
ions. Hence inter-annotator agreement is often com-
puted as well.
The quality of a summary is assessed mostly on its
content and linguistic quality (Louis and Nenkova,
2008). Content evaluation of a query-based sum-
mary is performed based on the relevance with the
topic and the question and the inclusion of important
contents from the input documents. The linguistic
quality of a summary is evaluated manually based on
how it structures and presents the contents. Mainly,
subjective evaluation is done to assess the linguis-
tic quality of an automatically generated summary.
Grammaticality, non-redundancy, referential clarity,
focus, structure and coherence are commonly used
factors considered to evaluate the linguistic quality.
A study by (Das and Martins, 2007) shows that eval-
uating the content of a summary is more difficult
compared to evaluating its linguistic quality.
There exist different measures to evaluate an
output summary. The most commonly used metrics
are recall, precision, F-measure, Pyramid score,
and ROUGE/BE.
Automatic versus Manual Evaluation
Based on an analysis of the 2005-2007 DUC data,
(Conroy and Schlesinger, 2008) showed that the
ROUGE evaluation and a human evaluation can sig-
nificantly vary due to the fact that ROUGE ignores
linguistic quality of summaries, which has a huge in-
fluence in human evaluation. (Dang and Owczarzak,
2008) also pointed out that automatic evaluation is
rather different than the one based on manual assess-
ment. They explained this the following way: ?auto-
matic metrics, based on string matching, are unable
to appreciate a summary that uses different phrases
than the reference text, even if such a summary is
perfectly fine by human standards?.
To evaluate both opinionated and news article
based summarization approaches, previously men-
tioned evaluation metrics such as ROUGE or Pyra-
mid are used. Shared evaluation tasks such as
DUC and TAC competitions also use these methods
to evaluate participants? summary. Table 1 shows
Table 1: Human and Automatic System Performance at
Various TAC Competitions
Model (Human) Automatic
Pyr. Resp. Pyr. Resp.
2010 Upd. 0.78 4.76 0.30 2.56
2009 Upd. 0.68 8.83 0.26 4.14
2008 Upd. 0.66 4.62 0.26 2.32
2008 Opi. 0.44 Unk. 0.10 1.31
the evaluation results of automatic systems? average
performance at the TAC 2008 to 2010 conferences
using the pyramid score (Pyr.) and responsiveness
(Resp.). In this evaluation, the pyramid score was
used to calculate the content relevance and the re-
sponsiveness of a summary was used to judge the
overall quality or usefulness of the summary, con-
sidering both the information content and linguistic
quality. These two criteria were evaluated manually.
The pyramid score was calculated out of 1 and the
responsiveness measures were calculated on a scale
of 1 to 5 (1, being the worst). However, in 2009,
responsiveness was calculated on a scale of 10. Ta-
ble 1 also shows a comparison between automatic
systems and human participants (model). In Table
1, the first 3 rows show the evaluation results of the
TAC Update Summarization (Upd.) initial summary
generation task (which were generated for news arti-
cles) and the last row shows the evaluation results of
the TAC 2008 Opinion Summarization track (Opi.)
where summaries were generated from blogs. From
Table 1, we can see that in both criteria, automatic
systems are weaker than humans. (Note that in the
table, Unk. refers to unknown.)
Interestingly, in an automatic evaluation, often,
not only is there no significant gap between models
and systems, but in many cases, automatic systems
scored higher than some human models.
Table 2 shows the performance of human (H.)
and automated systems (S.) (participants) using au-
tomated and manual evaluation in the TAC 2008 up-
45
Table 2: Automated vs. Manual Evaluation at TAC 2008
Automated Manual
R-2 R-SU4 Pyr. Ling. Resp.
H. Mean 0.12 0.15 0.66 4.79 4.62
S. Mean 0.08 0.12 0.26 2.33 2.32
H. Best 0.13 0.17 0.85 4.91 4.79
S. Best 0.11 0.14 0.36 3.25 2.29
date summarization track. In the table, R-2 and R-
SU4 refer to ROUGE-2 and ROUGE-SU4 and Pyr.,
Ling., and Resp. refer to Pyramid, linguistic, and
responsiveness, respectively. A t-test of statistical
significance applied to the data in Table 2 shows that
there is no significant difference between human and
participants in automated evaluation but that there is
a significant performance difference between them
in the manual evaluation.
These findings indicate that ROUGE is not the
most effective tool to evaluate summaries. Our own
experiments described below arrive at the same con-
clusion.
3 BlogSum
We have designed an extractive query-based summ-
rizer called BlogSum. In BlogSum, we have devel-
oped our own sentence extractor to retrieve the ini-
tial list of candidate sentences (we called it OList)
based on question similarity, topic similarity, and
subjectivity scores. Given a set of initial candidate
sentences, BlogSum generates summaries using dis-
course relations within a schema-based framework.
Details of BlogSum is outside the scope of this pa-
per. For details, please see (Mithun and Kosseim,
2011).
4 Evaluation of BlogSum
BlogSum-generated summaries have been evaluated
for content and linguistic quality, specifically dis-
course coherence. The evaluation of the content was
done both automatically and manually and the evalu-
ation of the coherence was done manually. Our eval-
uation results also reflect the discrepancy between
automatic and manual evaluation schemes of sum-
maries described above.
In our evaluation, BlogSum-generated summaries
were compared with the original candidate list gen-
erated by our approach without the discourse re-
ordering (OList). However, we have validated our
original candidate list with a publicly available sen-
tence ranker. Specifically, we have conducted an ex-
periment to verify whether MEAD-generated sum-
maries (Radev et al, 2004), a widely used publicly
available summarizer5, were better than our candi-
date list (OList). In this evaluation, we have gener-
ated summaries using MEAD with centroid, query
title, and query narrative features. In MEAD, query
title and query narrative features are implemented
using cosine similarity based on the tf-idf value. In
this evaluation, we used the TAC 2008 opinion sum-
marization dataset (described later in this section)
and summaries were evaluated using the ROUGE-2
and ROUGE-SU4 scores. Table 3 shows the results
of the automatic evaluation using ROUGE based on
summary content.
Table 3: Automatic Evaluation of MEAD based on Sum-
mary Content on TAC 2008
System R-2 (F) R-SU4 (F)
MEAD 0.0407 0.0642
Average 0.0690 0.0860
OList 0.1020 0.1070
Table 3 shows that MEAD-generated summaries
achieved weaker ROUGE scores compared to that
of our candidate list (OList). The table also shows
that MEAD performs weaker than the average per-
formance of the participants of TAC 2008 (Average).
We suspect that these poor results are due to sev-
eral reasons. First, in MEAD, we cannot use opin-
ionated terms or polarity information as a sentence
selection feature. On the other hand, most of the
summarizers, which deal with opinionated texts, use
opinionated terms and polarity information for this
purpose. In addition, in this experiment, for some of
the TAC 2008 questions, MEAD was unable to cre-
ate any summary. This evaluation results prompted
us to develop our own candidate sentence selector.
5MEAD: http://www.summarization.com/mead
46
4.1 Evaluation of Content
4.1.1 Automatic Evaluation of Content
First, we have automatically evaluated the sum-
maries generated by our approach for content. As
a baseline, we used the original ranked list of can-
didate sentences (OList), and compared them to the
final summaries (BlogSum). We have used the data
from the TAC 2008 opinion summarization track for
the evaluation.
The dataset consists of 50 questions on 28 topics;
on each topic one or two questions are asked and 9 to
39 relevant documents are given. For each question,
one summary was generated by OList and one by
BlogSum and the maximum summary length was re-
stricted to 250 words. This length was chosen cause
in the DUC conference from 2005 to 2007, in the
main summarization task, the summary length was
250 words. In addition, (Conroy and Schlesinger,
2008) also created summaries of length 250 words
in their participation in the TAC 2008 opinion sum-
marization task and performed well. (Conroy and
Schlesinger, 2008) also pointed out that if the sum-
maries were too long this adversely affected their
scores. Moreover, according to the same authors
shorter summaries are easier to read. Based on these
observations, we have restricted the maximum sum-
mary length to 250 words. However, in the TAC
2008 opinion summarization track, the allowable
summary length is very long (the number of non-
whitespace characters in the summary must not ex-
ceed 7000 times the number of questions for the tar-
get of the summary). In this experiment, we used
the ROUGE metric using answer nuggets (provided
by TAC), which had been created to evaluate par-
ticipants? summaries at TAC, as gold standard sum-
maries. F-scores are calculated for BlogSum and
OList using ROUGE-2 and ROUGE-SU4. In this
experiment, ROUGE scores are also calculated for
all 36 submissions in the TAC 2008 opinion sum-
marization track.
The evaluation results are shown in Table 4. Note
that in the table Rank refers to the rank of the system
compared to the other 36 systems.
Table 4 shows that BlogSum achieved a better F-
Measure (F) for ROUGE-2 (R-2) and ROUGE-SU4
(R-SU4) compared to OList. From the results, we
can see that BlogSum gained 18% and 16% in F-
Table 4: Automatic Evaluation of BlogSum based on
Summary Content on TAC 2008
System R-2 (F) R-SU4 (F) Rank
Best 0.130 0.139 1
BlogSum 0.125 0.128 3
OList 0.102 0.107 10
Average 0.069 0.086 N/A
Measure over OList using ROUGE-2 and ROUGE-
SU4, respectively.
Compared to the other systems that participated to
the TAC 2008 opinion summarization track, Blog-
Sum performed very competitively; it ranked third
and its F-Measure score difference from the best sys-
tem is very small. Both BlogSum and OList per-
formed better than the average systems.
However, a further analysis of the results of
Table 4 shows that there is no significant differ-
ence between BlogSum-generated summaries and
OList summaries using the t-test with a p-value
of 0.228 and 0.464 for ROUGE-2 and ROUGE-
SU4, respectively. This is inline with (Conroy and
Schlesinger, 2008; Dang and Owczarzak, 2008) who
showed that the performance gap between human-
generated summaries and system-generated sum-
maries at DUC and TAC is clearly visible in a man-
ual evaluation, but is often not reflected in automated
evaluations using ROUGE scores. Based on these
findings, we suspected that there might be a perfor-
mance difference between BlogSum-generated sum-
maries and OList which is not reflected in ROUGE
scores. To verify our suspicion, we have conducted
manual evaluations for content.
4.1.2 Manual Evaluation of Content using the
Blog Dataset
We have conducted two manual evaluations using
two different datasets to better quantify BlogSum-
generated summary content.
Corpora and Experimental Design
In the first evaluation, we have again used the TAC
2008 opinion summarization track data. For each
question, one summary was generated by OList and
one by BlogSum and the maximum summary length
was again restricted to 250 words. To evaluate
47
content, 3 participants manually rated 50 summaries
from OList and 50 summaries from BlogSum using
a blind evaluation. These summaries were rated
on a likert scale of 1 to 5 where 1 refers to ?very
poor? and 5 refers to ?very good?. Evaluators rated
each summary with respect to the question for
which it was generated and against the reference
summary. In this experiment, we have used the
answer nuggets provided by TAC as the reference
summary, which had been created to evaluate
participants? summaries at TAC. Annotators were
asked to evaluate summaries based on their content
without considering their linguistic qualities.
Results
In this evaluation, we have calculated the average
scores of all 3 annotators? ratings to a particular
question to compute the score of BlogSum for a
particular question. Table 5 shows the performance
comparison between BlogSum and OList. The re-
sults show that 58% of the time BlogSum summaries
were rated better than OList summaries which im-
plies that 58% of the time, our approach has im-
proved the question relevance compared to that of
the original candidate list (OList).
Table 5: Comparison of OList and BlogSum based on the
Manual Evaluation of Summary Content on TAC 2008
Comparison %
BlogSum Score > OList Score 58%
BlogSum Score = OList Score 30%
BlogSum Score < OList Score 12%
Table 6 shows the performance of BlogSum ver-
sus OList on each likert scale; where ? shows the
difference in performance. Table 6 demonstrates
that 52% of the times, BlogSum summaries were
rated as ?very good? or ?good?, 26% of the times
they were rated as ?barely acceptable? and 22% of
the times they were rated as ?poor? or ?very poor?.
From Table 6, we can also see that BlogSum out-
performed OList in the scale of ?very good? and
?good? by 8% and 22%, respectively; and improved
the performance in ?barely acceptable?, ?poor?, and
?very poor? categories by 12%, 8%, and 10%, re-
spectively.
In this evaluation, we have also calculated
Table 6: Manual Evaluation of BlogSum and OList based
on Summary Content on TAC 2008
Category OList BlogSum ?
Very Good 6% 14% 8%
Good 16% 38% 22%
Barely Acceptable 38% 26% -12%
Poor 26% 18% -8%
Very Poor 14% 4% -10%
whether there is any performance gap between Blog-
Sum and OList. The t-test results show that in a two-
tailed test, BlogSum performed significantly better
than OList with a p-value of 0.00281.
Whenever human performance is computed by
more than one person, it is important to compute
inter-annotator agreement. This ensures that the
agreement between annotators did not simply occur
by chance. In this experiment, we have also cal-
culated the inter-annotator agreement using Cohen?s
kappa coefficient to verify the annotation subjectiv-
ity. We have found that the average pair-wise inter-
annotator agreement is moderate according to (Lan-
dis and Koch, 1977) with the kappa-value of 0.58.
4.1.3 Manual Evaluation of Content using the
Review Dataset
We have conducted a second evaluation using
the OpinRank dataset6 and (Jindal and Liu, 2008)?s
dataset to evaluate BlogSum-generated summary
content.
Corpora and Experimental Design
In this second evaluation, we have used a subset of
the OpinRank dataset and (Jindal and Liu, 2008)?s
dataset. The OpinRank dataset contains reviews on
cars and hotels collected from Tripadvisor (about
259,000 reviews) and Edmunds (about 42,230 re-
views). The OpinRank dataset contains 42,230 re-
views on cars for different model-years and 259,000
reviews on different hotels in 10 different cities. For
this dataset, we created a total of 21 questions in-
cluding 12 reason questions and 9 suggestions. For
each question, 1500 to 2500 reviews were provided
6OpinRank Dataset: http://kavita-ganesan.com/entity-
ranking-data
48
as input documents to create the summary.
(Jindal and Liu, 2008)?s dataset consists of 905
comparison and 4985 non-comparison sentences.
Four human annotators labeled these data manually.
This dataset consists of reviews, forum, and news ar-
ticles on different topics from different sources. We
have created 9 comparison questions for this dataset.
For each question, 700 to 1900 reviews were pro-
vided as input documents to create the summary.
For each question, one summary was generated
by OList and one by BlogSum and the maximum
summary length was restricted to 250 words again.
To evaluate question relevance, 3 participants
manually rated 30 summaries from OList and 30
summaries from BlogSum using a blind evaluation.
These summaries were again rated on a likert scale
of 1 to 5. Evaluators rated each summary with
respect to the question for which it was generated.
Results
Table 7 shows the performance comparison between
BlogSum and OList. The results show that 67% of
the time BlogSum summaries were rated better than
OList summaries. The table also shows that 30%
of the time both approaches performed equally well
and 3% of the time BlogSum was weaker than OList.
Table 7: Comparison of OList and BlogSum based on the
Manual Evaluation of Summary Content on the Review
Dataset
Comparison %
BlogSum Score > OList Score 67%
BlogSum Score = OList Score 30%
BlogSum Score < OList Score 3%
Table 8 demonstrates that 44% of the time Blog-
Sum summaries were rated as ?very good?, 33% of
the time rated as ?good?, 13% of the time they were
rated as ?barely acceptable? and 10% of the time
they were rated as ?poor? or ?very poor?. From Ta-
ble 8, we can also see that BlogSum outperformed
OList in the scale of ?very good? by 34% and im-
proved the performance in ?poor? and ?very poor?
categories by 23% and 10%, respectively.
In this evaluation, we have also calculated
whether there is any performance gap between Blog-
Table 8: Manual Evaluation of BlogSum and OList based
on Summary Content on the Review Dataset
Category OList BlogSum ?
Very Good 10% 44% 34%
Good 37% 33% -4%
Barely Acceptable 10% 13% 3%
Poor 23% 0% -23%
Very Poor 20% 10% -10%
Sum and OList. The t-test results show that in a two-
tailed test, BlogSum performed significantly very
better than OList with a p-value of 0.00236. In ad-
dition, the average pair-wise inter-annotator agree-
ment is substantial according to (Landis and Koch,
1977) with the kappa-value of 0.77.
4.1.4 Analysis
In both manual evaluation for content, BlogSum
performed significantly better than OList. We can
see that even though there was not any signifi-
cant performance gap between BlogSum and OList-
generated summaries in the automatic evaluation of
Section 4.1.1, both manual evaluations show that
BlogSum and OList-generated summaries signifi-
cantly vary at the content level. For content, our re-
sults support (Conroy and Schlesinger, 2008; Dang
and Owczarzak, 2008)?s findings and points out for
a better automated summary evaluation tool.
4.2 Evaluation of Linguistic Quality
Our next experiments were geared at evaluating the
linguistic quality of our summaries.
4.2.1 Automatic Evaluation of Linguistic
Quality
To test the linguistic qualities, we did not use
an automatic evaluation because (Blair-Goldensohn
and McKeown, 2006) found that the ordering of con-
tent within the summaries is an aspect which is not
evaluated by ROUGE. Moreover, in the TAC 2008
opinion summarization track, on each topic, answer
snippets were provided which had been used as sum-
marization content units (SCUs) in pyramid evalua-
tion to evaluate TAC 2008 participants summaries
but no complete summaries is provided to which we
can compare BlogSum-generated summaries for co-
49
herence. As a result, we only performed two man-
ual evaluations using two different datasets again to
see whether BlogSum performs significantly better
than OList for linguistic qualities too. The pos-
itive results of the next experiments will ensure
that BlogSum-generated summaries are really sig-
nificantly better than OList summaries.
4.2.2 Manual Evaluation of Discourse
Coherence using the Blog Dataset
In this evaluation, we have again used the TAC
2008 opinion summarization track data. For each
question, one summary was generated by OList and
one by BlogSum and the maximum summary length
was restricted to 250 words again. Four participants
manually rated 50 summaries from OList and 50
summaries from BlogSum for coherence. These
summaries were again rated on a likert scale of 1 to
5.
Results
To compute the score of BlogSum for a particular
question, we calculated the average scores of all an-
notators? ratings to that question. Table 9 shows
the performance comparison between BlogSum and
OList. We can see that 52% of the time BlogSum
Table 9: Comparison of OList and BlogSum based on the
Manual Evaluation of Discourse Coherence on TAC 2008
Comparison %
BlogSum Score > OList Score 52%
BlogSum Score = OList Score 30%
BlogSum Score < OList Score 18%
summaries were rated better than OList summaries;
30% of the time both performed equally well; and
18% of the time BlogSum was weaker than OList.
This means that 52% of the time, our approach has
improved the coherence compared to that of the
original candidate list (OList).
From Table 10, we can see that BlogSum outper-
formed OList in the scale of ?very good? and ?good?
by 16% and 8%, respectively; and improved the per-
formance in ?barely acceptable? and ?poor? cate-
gories by 12% and 14%, respectively.
The t-test results show that in a two-tailed test,
BlogSum performed significantly better than OList
Table 10: Manual Evaluation of BlogSum and OList
based on Discourse Coherence on TAC 2008
Category OList BlogSum ?
Very Good 8% 24% 16%
Good 22% 30% 8%
Barely Acceptable 36% 24% -12%
Poor 22% 8% -14%
Very Poor 12% 14% 2%
with a p-value of 0.0223. In addition, the average
pair-wise inter-annotator agreement is substantial
according to with the kappa-value of 0.76.
4.2.3 Manual Evaluation of Discourse
Coherence using the Review Dataset
In this evaluation, we have again used the Opin-
Rank dataset and (Jindal and Liu, 2008)?s dataset
to conduct the second evaluation of content. In
this evaluation, for each question, one summary
was generated by OList and one by BlogSum and
the maximum summary length was restricted to
250 words. Three participants manually rated 30
summaries from OList and 30 summaries from
BlogSum for coherence.
Results
To compute the score of BlogSum for a particular
question, we calculated the average scores of all an-
notators? ratings to that question. Table 11 shows
the performance comparison between BlogSum and
OList. We can see that 57% of the time BlogSum
Table 11: Comparison of OList and BlogSum based on
the Manual Evaluation of Discourse Coherence on the
Review Dataset
Comparison %
BlogSum Score > OList Score 57%
BlogSum Score = OList Score 20%
BlogSum Score < OList Score 23%
summaries were rated better than OList summaries;
20% of the time both performed equally well; and
23% of the time BlogSum was weaker than OList.
50
Table 12: Manual Evaluation of BlogSum and OList
based on Discourse Coherence on the Review Dataset
Category OList BlogSum ?
Very Good 13% 23% 10%
Good 27% 43% 16%
Barely Acceptable 27% 17% -10%
Poor 10% 10% 0%
Very Poor 23% 7% -16%
From Table 12, we can see that BlogSum outper-
formed OList in the scale of ?very good? and ?good?
by 10% and 16%, respectively; and improved the
performance in ?barely acceptable? and ?very poor?
categories by 10% and 16%, respectively.
We have also evaluated if the difference in perfor-
mance between BlogSum and OList was statistically
significant. The t-test results show that in a two-
tailed test, BlogSum performed significantly better
than OList with a p-value of 0.0371.
In this experiment, we also calculated the inter-
annotator agreement using Cohen?s kappa coeffi-
cient. We have found that the average pair-wise
inter-annotator agreement is substantial according to
(Landis and Koch, 1977) with the kappa-value of
0.74.
The results of both manual evaluations of dis-
course coherence also show that BlogSum performs
significantly better than OList.
5 Conclusion
Based on the DUC and TAC evaluation re-
sults, (Conroy and Schlesinger, 2008; Dang and
Owczarzak, 2008) showed that the performance gap
between human-generated summaries and system-
generated summaries, which is clearly visible in the
manual evaluation, is often not reflected in auto-
mated evaluations using ROUGE scores. In our
content evaluation, we have used the automated
measure ROUGE (ROUGE-2 & ROUGE-SU4) and
the t-test results showed that there was no signif-
icant difference between BlogSum-generated sum-
maries and OList summaries with a p-value of 0.228
and 0.464 for ROUGE-2 and ROUGE-SU4, respec-
tively. We suspected that there might be a perfor-
mance difference between BlogSum-generated sum-
maries and OList which is not reflected in ROUGE
scores. To verify our suspicion, we have conducted
two manual evaluations for content using two dif-
ferent datasets. The t-test results for both datasets
show that in a two-tailed test, BlogSum performed
significantly better than OList with a p-value of
0.00281 and 0.00236. Manual evaluations of co-
herence also show that BlogSum performs signifi-
cantly better than OList. Even though there was no
significant performance gap between BlogSum and
OList-generated summaries in the automatic evalua-
tion, the manual evaluation results clearly show that
BlogSum-generated summaries are better than OList
significantly. Our results supports (Conroy and
Schlesinger, 2008; Dang and Owczarzak, 2008)?s
findings and points out for a better automated sum-
mary evaluation tool.
Acknowledgement
The authors would like to thank the anonymous ref-
erees for their valuable comments on a previous ver-
sion of the paper.
This work was financially supported by NSERC.
References
Annie Louis and Ani Nenkova. 2008. Automatic Sum-
mary Evaluation without Human Models. Proceedings
of the First Text Analysis Conference (TAC 2008),
Gaithersburg, Maryland (USA), November.
Chin-Y. Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July.
Dipanjan Das and Andre F. T. Martins. 2007. A
Survey on Automatic Text Summarization. Available
from: http://www.cs.cmu.edu/? nasmith/LS2/ das-
martins.07.pdf, Literature Survey for the Language
and Statistics II course at Carnegie Mellon University.
Dragomir Radev et al 2004. MEAD -A Platform for
Multidocument Multilingual Text Summarization. Pro-
ceedings of the the 4th International Conference on
Language Resources and Evaluation, pages 1?4, Lis-
bon, Portugal.
Hoa T. Dang and Karolina Owczarzak. 2008. Overview
of the TAC 2008 Update Summarization Task. Pro-
ceedings of the Text Analysis Conference, Gaithers-
burg, Maryland (USA), November.
John M. Conroy and Judith D. Schlesinger. 2008.
CLASSY and TAC 2008 Metrics. Proceedings of the
51
Text Analysis Conference, Gaithersburg, Maryland
(USA), November.
John M. Conroy and Hoa T. Dang. 2008. Mind the Gap:
Dangers of Divorcing Evaluations of Summary Con-
tent from Linguistic Quality. Proceedings of the the
22nd International Conference on Computational Lin-
guistics Coling, pages 145?152, Manchester, UK.
Nitin Jindal and Bing Liu. 2006. Identifying Compar-
ative Sentences in Text Documents. SIGIR?06 Pro-
ceedings of the 29th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 244?251, Seattle, Washington,
USA, August.
Richard J. Landis and Gary G. Koch. 1977. A One-way
Components of Variance Model for Categorical Data.
Journal of Biometrics, 33(1):671?679.
Sasha Blair-Goldensohn and Kathleen McKeown. 2006.
Integrating Rhetorical-Semantic Relation Models for
Query-Focused Summarization. Proceedings of the
Document Understanding Conference (DUC) Work-
shop at NAACL-HLT 2006, New York, USA, June.
Shamima Mithun and Leila Kosseim. 2011. Discourse
Structures to Reduce Discourse Incoherence in Blog
Summarization. Proceedings of Recent Advances in
Natural Language Processing, pages 479?486, Hissar,
Bulgaria, September.
52
