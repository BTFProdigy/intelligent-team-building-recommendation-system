Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 545?552
Manchester, August 2008
Comparative Parser Performance Analysis across Grammar Frameworks
through Automatic Tree Conversion using Synchronous Grammars
Takuya Matsuzaki 1 Jun?ichi Tsujii 1,2,3
1. Department of Computer Science, University of Tokyo, Japan
2. School of Computer Science, University of Manchester, UK
3. National Center for Text Mining, UK
{matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a methodology for the
comparative performance analysis of the
parsers developed for different grammar
frameworks. For such a comparison, we
need a common representation format of
the parsing results since the representation
of the parsing results depends on the gram-
mar frameworks; hence they are not di-
rectly comparable to each other. We first
convert the parsing result to a shallow CFG
analysis by using an automatic tree con-
verter based on synchronous grammars.
The use of such a shallow representation as
a common format has the advantage of re-
duced noise introduced by the conversion
in comparison with the noise produced by
the conversion to deeper representations.
We compared an HPSG parser with sev-
eral CFG parsers in our experiment and
found that meaningful differences among
the parsers? performance can still be ob-
served by such a shallow representation.
1 Introduction
Recently, there have been advancement made in
the parsing techniques for large-scale lexicalized
grammars (Clark and Curran, 2004; Ninomiya et
al., 2005; Ninomiya et al, 2007), and it have
presumably been accelerated by the development
of the semi-automatic acquisition techniques of
large-scale lexicalized grammars from parsed cor-
pora (Hockenmaier and Steedman, 2007; Miyao
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
et al, 2005). In many of the studies on lexical-
ized grammar parsing, the accuracy of the pars-
ing results is evaluated in terms of the accuracy of
the semantic representations output by the parsers.
Since the formalisms for the semantic representa-
tion are different across the grammar frameworks,
it has been difficult to directly compare the perfor-
mance of the parsers developed for different gram-
mar frameworks.
Several researchers in the field of lexicalized
grammar parsing have recently started to seek a
common representation of parsing results across
different grammar frameworks (Clark and Curran,
2007; Miyao et al, 2007). For example, Clark
and Curran (2007) developed a set of mapping
rules from the output of a Combinatorial Catego-
rial grammar parser to the Grammatical Relations
(GR) (Carroll et al, 1998). They found that the
manual development of such mapping rules is not a
trivial task; their mapping rules covered only 85%
of the GRs in a GR-annotated corpus; i.e., 15% of
the GRs in the corpus could not be covered by the
mapping from the gold-standard CCG analyses of
those sentences.
We propose another method for the cross-
framework performance analysis of the parsers
wherein the output of parsers are first converted
to a CFG tree. Specifically, we use CFG trees of
the style used in the Penn Treebank (PTB) (Mar-
cus et al, 1994), in which the non-terminal labels
are simple phrasal categories (i.e., we do not use
function-tags, empty nodes, and co-indexing). We
hereafter name such CFG trees, ?PTB-CFG trees.?
We use an automatic tree converter based on a
stochastic synchronous grammar in order to make
the PTB-CFG trees from the analyses based on a
lexicalized grammar.
In such a shallow representation, some infor-
545
mation given by the lexicalized parsers is lost.
For instance, long-distance dependency and con-
trol/raising distinction cannot be directly repre-
sented in the PTB-CFG tree. From the viewpoint
of NLP-application developer, the parser evalua-
tion based on such a shallow representation may
be not very informative because performance met-
rics based on the shallow representation, e.g., la-
beled bracketing accuracy, do not serve as a direct
indicator of the usefulness of the parser in their
applications. Nevertheless, we consider the parser
performance analysis based on the shallow repre-
sentation is still very useful from the viewpoint
of parser developers because the accuracy of the
structure of the CFG-trees is, though not an ideal
one, a good indicator of the parsers? structural dis-
ambiguation performance.
In addition, there are at least two advantages in
using the CFG-trees as the common representation
for the evaluation. The first advantage is that the
conversion from the parser?s output to the CFG-
trees can be achieved with much higher accuracy
than to deeper representations like GRs; we ob-
tained a conversion accuracy of around 98% in our
experiments using an HPSG grammar. The accu-
racy of the conversion is critical in the quantita-
tive comparison of parsers that have similar per-
formances because the difference in the parsers?
ability would soon be masked by the errors intro-
duced in the conversion process. The second ad-
vantage is that we can compare the converted out-
put directly against the outputs of the well-studied
CFG-parsers derived from PTB.
In the experiments, we applied the conversion to
an HPSG parser, and compared the results against
several CFG parsers. We found that the parsing
accuracy of the HPSG parser is a few points lower
than state-of-the-art CFG parsers in terms of the
labeled bracketing accuracy. By further investi-
gating the parsing results, we have identified a
portion of the reason for the discrepancy, which
comes from the difference in the architecture of the
parsers.
2 Background
In this section, we first give a brief overview of
the semi-automatic acquisition framework of lex-
icalized grammars. Although our methodology is
also applicable to manually developed grammars,
in this paper, we concentrate on the evaluation of
the parsers developed for lexicalized grammars de-
rived from a CFG treebank. Next, we introduce
a specific instance of the treebank-derived lexical-
ized grammars used in our experiment: the Enju
English HPSG grammar. Using the Enju grammar
as a concrete example, we present the motivations
for our tree conversion method based on a stochas-
tic synchronous grammar. We also provide a sum-
mary of the basic concepts and terminologies of
the stochastic synchronous grammar.
2.1 Semi-automatic Acquisition of
Lexicalized Grammars
A lexicalized grammar generally has two compo-
nents: a small set of grammar rules and a large
set of lexical items. The grammar rules represent
generic grammatical constraints while the lexical
items represent word-specific characteristics. An
analysis of a sentence is created by iteratively com-
bining lexical items assigned to to the words in the
sentence by applying the grammar rules.
Several researchers have suggested to extract the
lexicon; i.e., the set of lexical items, from a tree-
bank such as PTB. Most of the lexicon acquisition
methods proceed as follows:
1. Fix the the grammar rules and the basic de-
sign of the lexical items.
2. Re-analyse the sentences in terms of the tar-
get grammar framework, exploiting the anal-
ysis given in the source treebank. A re-
analysis is generally represented as a deriva-
tion of the sentence; i.e., a history of rule ap-
plications.
3. Find a lexical item for each word in the sen-
tences so that it matches the re-analysis of the
sentence, and extract it.
We used the pairs of the original trees and the re-
analyses of the same sentence as a parallel tree-
bank, from which we extract a synchronous gram-
mar.
2.2 The Enju HPSG Grammar
We used the Enju English HPSG grammar (Miyao
et al, 2005) 1 in the experiments. The design of
the grammar basically follows the definition in the
text by Pollard and Sag (1994). A program called
Mayz is distributed with the grammar, which was
1Version 2.2., publicly available from http://www-
tsujii.is.s.u-tokyo.ac.jp/enju
546
used to make the HPSG treebank (i.e., a set of re-
analyses based on the HPSG grammar) from PTB;
the lexicon was extracted from the HPSG treebank.
We reproduced the HPSG treebank using the pro-
gram.
An analysis of a sentence in the HPSG for-
malism is represented by a phrasal tree, in which
each node is assigned a data structure called
typed feature structure (TFS). The TFS represents
syntactic/semantic structures of the corresponding
phrase. To convert an HPSG analysis to a corre-
sponding PTB-CFG trees, we first map the TFSs to
atomic symbols like PP, NP, NX, etc. (33 symbols
in total). We hereafter name such HPSG trees af-
ter the TFS-to-symbol mapping, ?simplified HPSG
trees.? Similarly to the PTB-CFG trees, the simpli-
fied HPSG trees do not include empty categories,
co-indexing, and function-tags. However, we can-
not attain a PTB-CFG tree by simply mapping
those atomic symbols to the corresponding PTB
non-terminal symbols, because the analyses by the
PTB-CFG and the HPSG yield different tree struc-
tures for the same sentence.
The conversion of the tree structure from HPSG
trees to PTB-CFG trees can be regarded as the
inverse-mapping of the transformation from PTB
trees to HPSG trees implemented in the Mayz pro-
gram. A most notable transformation is the bina-
rization of the PTB trees; all the branches in the
HPSG treebank are unary or binary. The binariza-
tion scheme used in Mayz is similar to the head-
centered binarization, which is often used for the
extraction of ?Markovised? PCFGs from the tree-
bank. Mayz identifies the head daughters by using
a modified version of Collins? head finding rules
(Collins, 1999). It is also notable that the PTB-
to-HPSG transformation by Mayz often makes a
bracketing in the HPSG analyses that crosses with
the original bracketing in the PTB. Such a trans-
formation is used, for instance, to change the at-
tachment level of an article to a noun phrase with
a post-modifier (Figure 1).
The tree transformation by Mayz is achieved
by sequentially applying many tree transformation
rules to an input PTB tree. Although each of the
rules operates on a relatively small region of the
tree, the net result can be a very complex transfor-
mation. It is thus very difficult, if not impossible,
to invert the transformation programmatically.
NP
the NX
cat PP
on NP
the wall
NP
NP
the cat
PP
on NP
the wall
Figure 1: Different attachment level of the arti-
cles: HPSG analysis (left) and PTB-CFG analysis
(right).
 
  



 
   


 

 
	

	


   	

	
 

 

 


   

  









 



 



Figure 2: An example of synchronous CFG
2.3 Stochastic Synchronous Tree-Substitution
Grammar for Tree Conversion
For the purpose of the inverted transformation
of simplified HPSG trees to PTB-CFG trees, we
use a statistical approach based on the stochastic
synchronous grammars. Stochastic synchronous
grammars are a family of probabilistic models that
generate a pair of trees by recursively applying
synchronous productions, starting with a pair of
initial symbols. See e.g., Eisner (2003) for a more
formal definition. Figure 2 shows an example of
synchronous CFG, which generates the pairs of
strings of the form (abmc, cbma). Each non-
terminal symbol on the yields of the synchronous
production is linked to a non-terminal symbol on
the other rule?s yield. In the figure, the links are
represented by subscripts. A linked pair of the non-
terminal symbols is simultaneously expanded by
another synchronous production.
The probability of a derivation D of a tree pair
?S, T ? is defined as the product of the probability
of the pair of initial symbols (i.e., the root nodes of
S and T ), and the probabilities of the synchronous
productions used in the derivation:
P (D) = P
(?
R
1
, R
2
?)
?
?t1
i
,t
2
i
??D
P
(?
t
1
i
, t
2
i
?)
,
where ?R1, R2? is the pair of the symbols of the
root nodes of S and T , and ?t1
i
, t
2
i
? is a syn-
chronous production.
547

Proceedings of the 12th Conference of the European Chapter of the ACL, pages 603?611,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Deterministic shift-reduce parsing for unification-based grammars by 
using default unification 
 
 
Takashi Ninomiya 
Information Technology Center 
University of Tokyo, Japan 
ninomi@r.dl.itc.u-tokyo.ac.jp 
Takuya Matsuzaki 
Department of Computer Science 
University of Tokyo, Japan 
matuzaki@is.s.u-tokyo.ac.jp 
  
Nobuyuki Shimizu 
Information Technology Center 
University of Tokyo, Japan 
shimizu@r.dl.itc.u-tokyo.ac.jp 
Hiroshi Nakagawa 
Information Technology Center 
University of Tokyo, Japan 
nakagawa@dl.itc.u-tokyo.ac.jp
 
 
Abstract 
Many parsing techniques including pa-
rameter estimation assume the use of a 
packed parse forest for efficient and ac-
curate parsing.  However, they have sev-
eral inherent problems deriving from the 
restriction of locality in the packed parse 
forest.  Deterministic parsing is one of 
solutions that can achieve simple and fast 
parsing without the mechanisms of the 
packed parse forest by accurately choos-
ing search paths.  We propose (i) deter-
ministic shift-reduce parsing for unifica-
tion-based grammars, and (ii) best-first 
shift-reduce parsing with beam threshold-
ing for unification-based grammars.  De-
terministic parsing cannot simply be ap-
plied to unification-based grammar pars-
ing, which often fails because of its hard 
constraints.  Therefore, it is developed by 
using default unification, which almost 
always succeeds in unification by over-
writing inconsistent constraints in gram-
mars. 
1 Introduction 
Over the last few decades, probabilistic unifica-
tion-based grammar parsing has been investi-
gated intensively.  Previous studies (Abney, 
1997; Johnson et al, 1999; Kaplan et al, 2004; 
Malouf and van Noord, 2004; Miyao and Tsujii, 
2005; Riezler et al, 2000) defined a probabilistic 
model of unification-based grammars, including 
head-driven phrase structure grammar (HPSG), 
lexical functional grammar (LFG) and combina-
tory categorial grammar (CCG), as a maximum 
entropy model (Berger et al, 1996).  Geman and 
Johnson (Geman and Johnson, 2002) and Miyao 
and Tsujii (Miyao and Tsujii, 2002) proposed a 
feature forest, which is a dynamic programming 
algorithm for estimating the probabilities of all 
possible parse candidates.  A feature forest can 
estimate the model parameters without unpack-
ing the parse forest, i.e., the chart and its edges.  
Feature forests have been used successfully 
for probabilistic HPSG and CCG (Clark and Cur-
ran, 2004b; Miyao and Tsujii, 2005), and its 
parsing is empirically known to be fast and accu-
rate, especially with supertagging (Clark and 
Curran, 2004a; Ninomiya et al, 2007; Ninomiya 
et al, 2006).  Both estimation and parsing with 
the packed parse forest, however, have several 
inherent problems deriving from the restriction 
of locality.  First, feature functions can be de-
fined only for local structures, which limit the 
parser?s performance.  This is because parsers 
segment parse trees into constituents and factor 
equivalent constituents into a single constituent 
(edge) in a chart to avoid the same calculation.  
This also means that the semantic structures must 
be segmented.  This is a crucial problem when 
we think of designing semantic structures other 
than predicate argument structures, e.g., syn-
chronous grammars for machine translation.  The 
size of the constituents will be exponential if the 
semantic structures are not segmented.  Lastly, 
we need delayed evaluation for evaluating fea-
ture functions.  The application of feature func-
tions must be delayed until all the values in the 
603
segmented constituents are instantiated.  This is 
because values in parse trees can propagate any-
where throughout the parse tree by unification.  
For example, values may propagate from the root 
node to terminal nodes, and the final form of the 
terminal nodes is unknown until the parser fi-
nishes constructing the whole parse tree.  Conse-
quently, the design of grammars, semantic struc-
tures, and feature functions becomes complex.  
To solve the problem of locality, several ap-
proaches, such as reranking (Charniak and John-
son, 2005), shift-reduce parsing (Yamada and 
Matsumoto, 2003), search optimization learning 
(Daum? and Marcu, 2005) and sampling me-
thods (Malouf and van Noord, 2004; Nakagawa, 
2007), were studied. 
In this paper, we investigate shift-reduce pars-
ing approach for unification-based grammars 
without the mechanisms of the packed parse for-
est.  Shift-reduce parsing for CFG and dependen-
cy parsing have recently been studied (Nivre and 
Scholz, 2004; Ratnaparkhi, 1997; Sagae and La-
vie, 2005, 2006; Yamada and Matsumoto, 2003), 
through approaches based essentially on deter-
ministic parsing.  These techniques, however, 
cannot simply be applied to unification-based 
grammar parsing because it can fail as a result of 
its hard constraints in the grammar.  Therefore, 
in this study, we propose deterministic parsing 
for unification-based grammars by using default 
unification, which almost always succeeds in 
unification by overwriting inconsistent con-
straints in the grammars.  We further pursue 
best-first shift-reduce parsing for unification-
based grammars. 
Sections 2 and 3 explain unification-based 
grammars and default unification, respectively.  
Shift-reduce parsing for unification-based gram-
mars is presented in Section 4.  Section 5 dis-
cusses our experiments, and Section 6 concludes 
the paper. 
2 Unification-based grammars 
A unification-based grammar is defined as a pair 
consisting of a set of lexical entries and a set of 
phrase-structure rules.  The lexical entries ex-
press word-specific characteristics, while the 
phrase-structure rules describe constructions of 
constituents in parse trees.  Both the phrase-
structure rules and the lexical entries are 
represented by feature structures (Carpenter, 
1992), and constraints in the grammar are forced 
by unification.  Among the phrase-structure rules, 
a binary rule is a partial function: ? ? ? ? ? , 
where ? is the set of all possible feature struc-
tures.  The binary rule takes two partial parse 
trees as daughters and returns a larger partial 
parse tree that consists of the daughters and their 
mother.  A unary rule is a partial function: 
? ? ?, which corresponds to a unary branch. 
In the experiments, we used an HPSG (Pollard 
and Sag, 1994), which is one of the sophisticated 
unification-based grammars in linguistics.  Gen-
erally, an HPSG has a small number of phrase-
structure rules and a large number of lexical en-
tries.  Figure 1 shows an example of HPSG pars-
ing of the sentence, ?Spring has come.?  The up-
per part of the figure shows a partial parse tree 
for ?has come,? which is obtained by unifying 
each of the lexical entries for ?has? and ?come? 
with a daughter feature structure of the head-
complement rule.  Larger partial parse trees are 
obtained by repeatedly applying phrase-structure 
rules to lexical/phrasal partial parse trees.  Final-
ly, the parse result is output as a parse tree that 
dominates the sentence. 
3 Default unification 
Default unification was originally investigated in 
a series of studies of lexical semantics, in order 
to deal with default inheritance in a lexicon.  It is 
also desirable, however, for robust processing, 
because (i) it almost always succeeds and (ii) a 
feature structure is relaxed such that the amount 
of information is maximized (Ninomiya et al, 
2002).  In our experiments, we tested a simpli-
fied version of Copestake?s default unification.  
Before explaining it, we first explain Carpenter?s 
 
Figure 1: Example of HPSG parsing. 
 
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
HEAD  noun
SUBJ  <      SUBJ  <>     >
COMPS <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
head-comp
Spring has come
1
1 12
2
HEAD  verb
SUBJ  <>
COMPS <>
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
subject-head
head-comp
Spring has come
1
1 11 2
2
604
two definitions of default unification (Carpenter, 
1993). 
 
(Credulous Default Unification) 
? ?? ? ? =  ?? ? ????
? ? ? is maximal such
that ? ? ? ?is defined ? 
 
(Skeptical Default Unification) 
? ?? ? ? =  ?(? ?
?
? ?) 
 
?  is called a strict feature structure, whose in-
formation must not be lost, and ? is called a de-
fault feature structure, whose information can be 
lost but as little as possible so that ? and ? can 
be unified. 
Credulous default unification is greedy, in that 
it tries to maximize the amount of information 
from the default feature structure, but it results in 
a set of feature structures.  Skeptical default un-
ification simply generalizes the set of feature 
structures resulting from credulous default unifi-
cation.  Skeptical default unification thus leads to 
a unique result so that the default information 
that can be found in every result of credulous 
default unification remains.  The following is an 
example of skeptical default unification: 
 
[F: ?]  ?? ? ?
F: 1 ?
G: 1
H: ?
? =  ???
F: ?
G: ?
H: ?
? , ?
F: 1 ?
G: 1
H: ?
?? = ?
F: ?
G: ?
H: ?
?. 
 
Copestake mentioned that the problem with 
Carpenter?s default unification is its time com-
plexity (Copestake, 1993).  Carpenter?s default 
unification takes exponential time to find the op-
timal answer, because it requires checking the 
unifiability of the power set of constraints in a 
default feature structure.  Copestake thus pro-
posed another definition of default unification, as 
follows. Let ??(?) be a function that returns a 
set of path values in ?, and let ??(?) be a func-
tion that returns a set of path equations, i.e., in-
formation about structure sharing in ?. 
 
(Copestake?s default unification) 
? ?? ? ? =  ? ? ? ???
? ? ??(?)and there is no ?? ? ??(?)
such that ? ? ??is defined and
? ? ? ? ??is not defined
?, 
where ? = ? ? ???(?). 
 
Copestake?s default unification works effi-
ciently because all path equations in the default 
feature structure are unified with the strict fea-
ture structures, and because the unifiability of 
path values is checked one by one for each node 
in the result of unifying the path equations.  The 
implementation is almost the same as that of 
normal unification, but each node of a feature 
structure has a set of values marked as ?strict? or 
?default.?  When types are involved, however, it 
is not easy to find unifiable path values in the 
default feature structure.  Therefore, we imple-
mented a more simply typed version of Corpes-
take?s default unification. 
Figure 2 shows the algorithm by which we 
implemented the simply typed version.  First, 
each node is marked as ?strict? if it belongs to a 
strict feature structure and as ?default? otherwise. 
The marked strict and default feature structures 
procedure forced_unification(p, q) 
   queue := {?p, q?}; 
   while( queue is not empty ) 
      ?p, q? := shift(queue); 
      p := deref(p); q := deref(q); 
      if p ? q 
         ?(p) ?  ?(p) ? ?(q); 
         ?(q) ? ptr(p); 
         forall f ? feat(p)? feat(q) 
            if f ? feat(p) ? f ? feat(q) 
               queue := queue ? ??(f, p), ?(f, q)?; 
            if f ? feat(p) ? f ? feat(q) 
               ?(f, p) ?  ?(f, q); 
procedure mark(p, m) 
   p := deref(p); 
   if p has not been visited 
      ?(p) := {??(p),m?}; 
      forall f ? feat(p) 
         mark(?(f, p), m); 
procedure collapse_defaults(p) 
   p := deref(p); 
   if p has not been visited 
      ts := ?; td := ?; 
      forall ?t, ??????? ? ?(p) 
         ts := ts ? t; 
      forall ?t, ???????? ? ?(p) 
         td := td ? t; 
      if ts is not defined 
         return false; 
      if ts ? td is defined 
         ?(p) := ts ? td; 
      else 
         ?(p) := ts; 
      forall f ? feat(p) 
         collapse_defaults(?(f, p)); 
procedure default_unification(p, q) 
   mark(p, ??????); 
   mark(q, ???????); 
   forced_unification(p, q); 
   collapse_defaults(p); 
 
?(p) is (i) a single type, (ii) a pointer, or (iii) a set of pairs of 
types and markers in the feature structure node p. 
A marker indicates that the types in a feature structure node 
originally belong to the strict feature structures or the default 
feature structures. 
A pointer indicates that the node has been unified with other 
nodes and it points the unified node.  A function deref tra-
verses pointer nodes until it reaches to non-pointer node. 
?(f, p) returns a feature structure node which is reached by 
following a feature f from p. 
 
Figure 2: Algorithm for the simply typed ver-
sion of Corpestake?s default unification. 
605
are unified, whereas the types in the feature 
structure nodes are not unified but merged as a 
set of types.  Then, all types marked as ?strict? 
are unified into one type for each node.  If this 
fails, the default unification also returns unifica-
tion failure as its result.  Finally, each node is 
assigned a single type, which is the result of type 
unification for all types marked as both ?default? 
and ?strict? if it succeeds or all types marked 
only as ?strict? otherwise. 
4 Shift-reduce parsing for unification-
based grammars 
Non-deterministic shift-reduce parsing for unifi-
cation-based grammars has been studied by Bris-
coe and Carroll (Briscoe and Carroll, 1993).  
Their algorithm works non-deterministically with 
the mechanism of the packed parse forest, and 
hence it has the problem of locality in the packed 
parse forest.  This section explains our shift-
reduce parsing algorithms, which are based on 
deterministic shift-reduce CFG parsing (Sagae 
and Lavie, 2005) and best-first shift-reduce CFG 
parsing (Sagae and Lavie, 2006).  Sagae?s parser 
selects the most probable shift/reduce actions and 
non-terminal symbols without assuming explicit 
CFG rules.  Therefore, his parser can proceed 
deterministically without failure.  However, in 
the case of unification-based grammars, a deter-
ministic parser can fail as a result of its hard con-
straints in the grammar.  We propose two new 
shift-reduce parsing approaches for unification-
based grammars: deterministic shift-reduce pars-
ing and shift-reduce parsing by backtracking and 
beam search.  The major difference between our 
algorithm and Sagae?s algorithm is that we use 
default unification.  First, we explain the deter-
ministic shift-reduce parsing algorithm, and then 
we explain the shift-reduce parsing with back-
tracking and beam search. 
4.1 Deterministic shift-reduce parsing for 
unification-based grammars 
The deterministic shift-reduce parsing algorithm 
for unification-based grammars mainly compris-
es two data structures: a stack S, and a queue W.  
Items in S are partial parse trees, including a lex-
ical entry and a parse tree that dominates the 
whole input sentence.  Items in W are words and 
POSs in the input sentence.  The algorithm de-
fines two types of parser actions, shift and reduce, 
as follows. 
? Shift: A shift action removes the first item 
(a word and a POS) from W.  Then, one 
lexical entry is selected from among the 
candidate lexical entries for the item.  Fi-
nally, the selected lexical entry is put on 
the top of the stack. 
Common features: Sw(i), Sp(i), Shw(i), Shp(i), Snw(i), Snp(i), 
Ssy(i), Shsy(i), Snsy(i), wi-1, wi,wi+1, pi-2, pi-1, pi, pi+1, 
pi+2, pi+3 
Binary reduce features: d, c, spl, syl, hwl, hpl, hll, spr, syr, 
hwr, hpr, hlr 
Unary reduce features: sy, hw, hp, hl 
 
Sw(i) ? head word of i-th item from the top of the stack 
Sp(i) ? head POS of i-th item from the top of the stack 
Shw(i) ? head word of the head daughter of i-th item from the 
top of the stack 
Shp(i) ? head POS of the head daughter of i-th item from the 
top of the stack 
Snw(i) ? head word of the non-head daughter of i-th item 
from the top of the stack 
Snp(i) ? head POS of the non-head daughter of i-th item from 
the top of the stack 
Ssy(i) ? symbol of phrase category of the i-th item from the 
top of the stack 
Shsy(i) ? symbol of phrase category of the head daughter of 
the i-th item from the top of the stack 
Snsy(i) ? symbol of phrase category of the non-head daughter 
of the i-th item from the top of the stack 
d ? distance between head words of daughters 
c ? whether a comma exists between daughters and/or inside 
daughter phrases 
sp ? the number of words dominated by the phrase 
sy ? symbol of phrase category 
hw ? head word 
hp ? head POS 
hl ? head lexical entry 
 
Figure 3: Feature templates. 
Shift Features 
  [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1, 
wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi] 
[pi-1, pi, pi+1] [pi, pi+1, pi+2] [pi-2, pi-1] [pi-1, pi] [pi, 
pi+1] [pi+1, pi+2] 
 
Binary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d, 
c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c, 
sp, hw,hl] [c, sp, sy, hw] [d, c, hp, hl] [d, c, hp] [d, c, hl] [d, 
c, sy] [c, sp, hp, hl] [c, sp, hp] [c, sp, hl] [c, sp, sy] 
 
Unary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [hw, hp, hl] [hw, hp] [hw, hl] 
[sy, hw] [hp, hl] [hp] [hl] [sy]
 
Figure 4: Combinations of feature templates. 
606
? Binary Reduce: A binary reduce action 
removes two items from the top of the 
stack.  Then, partial parse trees are derived 
by applying binary rules to the first re-
moved item and the second removed item 
as a right daughter and left daughter, re-
spectively.  Among the candidate partial 
parse trees, one is selected and put on the 
top of the stack. 
? Unary Reduce: A unary reduce action re-
moves one item from the top of the stack.  
Then, partial parse trees are derived by 
applying unary rules to the removed item.  
Among the candidate partial parse trees, 
one is selected and put on the top of the 
stack. 
Parsing fails if there is no candidate for selec-
tion (i.e., a dead end).  Parsing is considered suc-
cessfully finished when W is empty and S has 
only one item which satisfies the sentential con-
dition: the category is verb and the subcategori-
zation frame is empty.  Parsing is considered a 
non-sentential success when W is empty and S 
has only one item but it does not satisfy the sen-
tential condition. 
In our experiments, we used a maximum en-
tropy classifier to choose the parser?s action.  
Figure 3 lists the feature templates for the clas-
sifier, and Figure 4 lists the combinations of fea-
ture templates.  Many of these features were tak-
en from those listed in (Ninomiya et al, 2007), 
(Miyao and Tsujii, 2005) and (Sagae and Lavie, 
2005), including global features defined over the 
information in the stack, which cannot be used in 
parsing with the packed parse forest.  The fea-
tures for selecting shift actions are the same as 
the features used in the supertagger (Ninomiya et 
al., 2007).  Our shift-reduce parsers can be re-
garded as an extension of the supertagger. 
The deterministic parsing can fail because of 
its grammar?s hard constraints.  So, we use de-
fault unification, which almost always succeeds 
in unification.  We assume that a head daughter 
(or, an important daughter) is determined for 
each binary rule in the unification-based gram-
mar.   Default unification is used in the binary 
rule application in the same way as used in Ni-
nomiya?s offline robust parsing (Ninomiya et al, 
2002), in which a binary rule unified with the 
head daughter is the strict feature structure and 
the non-head daughter is the default feature 
structure, i.e.,  (? ? ?) ?? ??, where R is a bi-
nary rule, H is a head daughter and NH is a non-
head daughter.  In the experiments, we used the 
simply typed version of Copestake?s default un-
ification in the binary rule application1.  Note 
that default unification was always used instead 
of normal unification in both training and evalua-
tion in the case of the parsers using default unifi-
cation.  Although Copestake?s default unification 
almost always succeeds, the binary rule applica-
tion can fail if the binary rule cannot be unified 
with the head daughter, or inconsistency is 
caused by path equations in the default feature 
structures.  If the rule application fails for all the 
binary rules, backtracking or beam search can be 
used for its recovery as explained in Section 4.2.  
In the experiments, we had no failure in the bi-
nary rule application with default unification. 
4.2 Shift-reduce parsing by backtracking 
and beam-search 
Another approach for recovering from the pars-
ing failure is backtracking.  When parsing fails 
or ends with non-sentential success, the parser?s 
state goes back to some old state (backtracking), 
and it chooses the second best action and tries 
parsing again.  The old state is selected so as to 
minimize the difference in the probabilities for 
selecting the best candidate and the second best 
candidate.  We define a maximum number of 
backtracking steps while parsing a sentence.  
Backtracking repeats until parsing finishes with 
sentential success or reaches the maximum num-
ber of backtracking steps.  If parsing fails to find 
a parse tree, the best continuous partial parse 
trees are output for evaluation. 
From the viewpoint of search algorithms, pars-
ing with backtracking is a sort of depth-first 
search algorithms.  Another possibility is to use 
the best-first search algorithm.  The best-first 
parser has a state priority queue, and each state 
consists of a tree stack and a word queue, which 
are the same stack and queue explained in the 
shift-reduce parsing algorithm.  Parsing proceeds 
by applying shift-reduce actions to the best state 
in the state queue.  First, the best state is re-
                                                 
1 We also implemented Ninomiya?s default unification, 
which can weaken path equation constraints.  In the prelim-
inary experiments, we tested binary rule application given 
as (? ? ?) ?? ?? with Copestake?s default unification, 
(? ? ?) ?? ?? with Ninomiya?s default unification, and 
(? ? ??) ?? ? with Ninomiya?s default unification.  How-
ever, there was no significant difference of F-score among 
these three methods.  So, in the main experiments, we only 
tested (? ? ?) ?? ?? with Copestake?s default unification 
because this method is simple and stable. 
607
moved from the state queue, and then shift-
reduce actions are applied to the state.  The new-
ly generated states as results of the shift-reduce 
actions are put on the queue.  This process re-
peats until it generates a state satisfying the sen-
tential condition.  We define the probability of a 
parsing state as the product of the probabilities of 
selecting actions that have been taken to reach 
the state.  We regard the state probability as the 
objective function in the best-first search algo-
rithm, i.e., the state with the highest probabilities 
is always chosen in the algorithm.  However, the 
best-first algorithm with this objective function 
searches like the breadth-first search, and hence, 
parsing is very slow or cannot be processed in a 
reasonable time.  So, we introduce beam thre-
sholding to the best-first algorithm.  The search 
space is pruned by only adding a new state to the 
state queue if its probability is greater than 1/b of 
the probability of the best state in the states that 
has had the same number of shift-reduce actions.  
In what follows, we call this algorithm beam 
search parsing. 
In the experiments, we tested both backtrack-
ing and beam search with/without default unifi-
cation.  Note that, the beam search parsing for 
unification-based grammars is very slow com-
pared to the shift-reduce CFG parsing with beam 
search.  This is because we have to copy parse 
trees, which consist of a large feature structures, 
in every step of searching to keep many states on 
the state queue.  In the case of backtracking, co-
pying is not necessary. 
5 Experiments 
We evaluated the speed and accuracy of parsing 
with Enju 2.3?, an HPSG for English (Miyao and 
Tsujii, 2005).  The lexicon for the grammar was 
extracted from Sections 02-21 of the Penn Tree-
bank (39,832 sentences).  The grammar consisted 
of 2,302 lexical entries for 11,187 words.  Two 
probabilistic classifiers for selecting shift-reduce 
actions were trained using the same portion of 
the treebank.  One is trained using normal unifi-
cation, and the other is trained using default un-
ification. 
We measured the accuracy of the predicate ar-
gument relation output of the parser.  A predi-
cate-argument relation is defined as a tuple 
??, ??, ?, ???, where ? is the predicate type (e.g., 
  Section 23 (Gold POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non- 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 87.26 86.50 86.88 604 - - - - - 
(Ninomiya et al, 2007) 89.78 89.28 89.53 234 - - - - - 
Ours 
det 76.45 82.00 79.13 122 0 - 867 35 1514 
det+du 87.78 87.45 87.61 256 0 - 0 117 2299 
back40 81.93 85.31 83.59 519 18986 - 386 23 2007 
back10 + du 87.79 87.46 87.62 267 574 - 0 45 2371 
beam(7.4) 86.17 87.77 86.96 510 - 226 369 30 2017 
beam(20.1)+du 88.67 88.79 88.48 457 - 205 0 16 2400 
beam(403.4) 89.98 89.92 89.95 10246 - 2822 71 14 2331 
           
  Section 23 (Auto POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 84.96 84.25 84.60 674 - - - - - 
(Ninomiya et al, 2007) 87.28 87.05 87.17 260 - - - - - 
(Matsuzaki et al, 2007)  86.93 86.47 86.70 30 - - - - - 
(Sagae et al, 2007)  88.50 88.00 88.20 - - - - - - 
Ours 
det 74.13 80.02 76.96 127 0 - 909 31 1476 
det+du 85.93 85.72 85.82 252 0 - 0 124 2292 
back40 78.71 82.86 80.73 568 21068 - 438 27 1951 
back10 + du 85.96 85.75 85.85 270 589 - 0 46 2370 
beam(7.4) 83.84 85.82 84.82 544 - 234 421 33 1962 
beam(20.1)+du 86.59 86.36 86.48 550 - 222 0 21 2395 
beam(403.4) 87.70 87.86 87.78 16822 - 4553 89 16 2311 
 
Table 1: Experimental results for Section 23. 
608
adjective, intransitive verb), ?? is the head word 
of the predicate, ? is the argument label (MOD-
ARG, ARG1, ?, ARG4), and ??  is the head 
word of the argument.  The labeled precision 
(LP) / labeled recall (LR) is the ratio of tuples 
correctly identified by the parser, and the labeled 
F-score (LF) is the harmonic mean of the LP and 
LR. This evaluation scheme was the same one 
used in previous evaluations of lexicalized 
grammars (Clark and Curran, 2004b; Hocken-
maier, 2003; Miyao and Tsujii, 2005).  The expe-
riments were conducted on an Intel Xeon 5160 
server with 3.0-GHz CPUs. Section 22 of the 
Penn Treebank was used as the development set, 
and the performance was evaluated using sen-
tences of ? 100 words in Section 23.  The LP, 
LR, and LF were evaluated for Section 23. 
Table 1 lists the results of parsing for Section 
23.  In the table, ?Avg. time? is the average pars-
ing time for the tested sentences.  ?# of backtrack? 
is the total number of backtracking steps that oc-
curred during parsing.  ?Avg. # of states? is the 
average number of states for the tested sentences.  
?# of dead end? is the number of sentences for 
which parsing failed.  ?# of non-sentential suc-
cess? is the number of sentences for which pars-
ing succeeded but did not generate a parse tree 
satisfying the sentential condition.  ?det? means 
the deterministic shift-reduce parsing proposed 
in this paper.  ?back?? means shift-reduce pars-
ing with backtracking at most ? times for each 
sentence.  ?du? indicates that default unification 
was used.  ?beam?? means best-first shift-reduce 
parsing with beam threshold ?.  The upper half 
of the table gives the results obtained using gold 
POSs, while the lower half gives the results ob-
tained using an automatic POS tagger.  The max-
imum number of backtracking steps and the 
beam threshold were determined by observing 
the performance for the development set (Section 
22) such that the LF was maximized with a pars-
ing time of less than 500 ms/sentence (except 
?beam(403.4)?). The performance of 
?beam(403.4)? was evaluated to see the limit of 
the performance of the beam-search parsing. 
Deterministic parsing without default unifica-
tion achieved accuracy with an LF of around 
79.1% (Section 23, gold POS).  With backtrack-
ing, the LF increased to 83.6%.  Figure 5 shows 
the relation between LF and parsing time for the 
development set (Section 22, gold POS).  As 
seen in the figure, the LF increased as the parsing 
time increased.  The increase in LF for determi-
nistic parsing without default unification, how-
ever, seems to have saturated around 83.3%.  
Table 1 also shows that deterministic parsing 
with default unification achieved higher accuracy, 
with an LF of around 87.6% (Section 23, gold 
POS), without backtracking.  Default unification 
is effective: it ran faster and achieved higher ac-
curacy than deterministic parsing with normal 
unification.  The beam-search parsing without 
default unification achieved high accuracy, with 
an LF of around 87.0%, but is still worse than 
deterministic parsing with default unification.  
However, with default unification, it achieved 
the best performance, with an LF of around 
88.5%, in the settings of parsing time less than 
500ms/sentence for Section 22. 
For comparison with previous studies using 
the packed parse forest, the performances of 
Miyao?s parser, Ninomiya?s parser, Matsuzaki?s 
parser and Sagae?s parser are also listed in Table 
1.  Miyao?s parser is based on a probabilistic 
model estimated only by a feature forest.  Nino-
miya?s parser is a mixture of the feature forest 
 
Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). 
 
82.00%
83.00%
84.00%
85.00%
86.00%
87.00%
88.00%
89.00%
90.00%
0 1 2 3 4 5 6 7 8
LF
Avg. parsing time (s/sentence)
back
back+du
beam
beam+du
609
and an HPSG supertagger.  Matsuzaki?s parser 
uses an HPSG supertagger and CFG filtering.  
Sagae?s parser is a hybrid parser with a shallow 
dependency parser.  Though parsing without the 
packed parse forest is disadvantageous to the 
parsing with the packed parse forest in terms of 
search space complexity, our model achieved 
higher accuracy than Miyao?s parser. 
?beam(403.4)? in Table 1 and ?beam? in Fig-
ure 5 show possibilities of beam-search parsing.  
?beam(403.4)? was very slow, but the accuracy 
was higher than any other parsers except Sagae?s 
parser. 
Table 2 shows the behaviors of default unifi-
cation for ?det+du.?  The table shows the 20 
most frequent path values that were overwritten 
by default unification in Section 22.  In most of 
the cases, the overwritten path values were in the 
selection features, i.e., subcategorization frames 
(COMPS:, SUBJ:, SPR:, CONJ:) and modifiee 
specification (MOD:).  The column of ?Default 
type? indicates the default types which were 
overwritten by the strict types in the column of 
?Strict type,? and the last column is the frequency 
of overwriting.  ?cons? means a non-empty list, 
and ?nil? means an empty list.  In most of the 
cases, modifiee and subcategorization frames 
were changed from empty to non-empty and vice 
versa.  From the table, overwriting of head in-
formation was also observed, e.g., ?noun? was 
changed to ?verb.? 
6 Conclusion and Future Work 
We have presented shift-reduce parsing approach 
for unification-based grammars, based on deter-
ministic shift-reduce parsing.  First, we presented 
deterministic parsing for unification-based 
grammars.  Deterministic parsing was difficult in 
the framework of unification-based grammar 
parsing, which often fails because of its hard 
constraints.  We introduced default unification to 
avoid the parsing failure.  Our experimental re-
sults have demonstrated the effectiveness of de-
terministic parsing with default unification.  The 
experiments revealed that deterministic parsing 
with default unification achieved high accuracy, 
with a labeled F-score (LF) of 87.6% for Section 
23 of the Penn Treebank with gold POSs.  
Second, we also presented the best-first parsing 
with beam search for unification-based gram-
mars.  The best-first parsing with beam search 
achieved the best accuracy, with an LF of 87.0%, 
in the settings without default unification.  De-
fault unification further increased LF from 
87.0% to 88.5%.  By widening the beam width, 
the best-first parsing achieved an LF of 90.0%. 
References 
Abney, Steven P. 1997. Stochastic Attribute-Value 
Grammars. Computational Linguistics, 23(4), 597-
618. 
Path Strict 
type 
Default 
type 
Freq
SYNSEM:LOCAL:CAT:HEAD:MOD: cons nil 434
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD:MOD: cons nil 237
SYNSEM:LOCAL:CAT:VAL:SUBJ: nil cons 231
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: nil cons 125
SYNSEM:LOCAL:CAT:HEAD: verb noun 110
SYNSEM:LOCAL:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT: 
HEAD:MOD: 
cons nil 101
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:
hd:LOCAL:CAT:HEAD:MOD: 
cons nil 96
SYNSEM:LOCAL:CAT:HEAD:MOD: nil cons 92
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: verb noun 91
SYNSEM:LOCAL:CAT:VAL:SUBJ: cons nil 79
SYNSEM:LOCAL:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD: nominal verb 75
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:HEAD:MOD: cons nil 74
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:HEAD:MOD: cons nil 69
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:COMPS:hd:LOCAL:CAT:HEAD: nominal verb 63
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: cons nil 63
? ? ? ?
Total   10,598
 
Table 2: Path values overwritten by default unification in Section 22. 
610
Berger, Adam, Stephen Della Pietra, and Vincent Del-
la Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Lin-
guistics, 22(1), 39-71. 
Briscoe, Ted and John Carroll. 1993. Generalized 
probabilistic LR-Parsing of natural language (cor-
pora) with unification-based grammars. Computa-
tional Linguistics, 19(1), 25-59. 
Carpenter, Bob. 1992. The Logic of Typed Feature 
Structures: Cambridge University Press. 
Carpenter, Bob. 1993. Skeptical and Credulous De-
fault Unification with Applications to Templates 
and Inheritance. In Inheritance, Defaults, and the 
Lexicon. Cambridge: Cambridge University Press. 
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative 
Reranking. In proc. of ACL'05, pp. 173-180. 
Clark, Stephen and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG 
parsing. In proc. of COLING-04, pp. 282-288. 
Clark, Stephen and James R. Curran. 2004b. Parsing 
the WSJ using CCG and log-linear models. In proc. 
of ACL'04, pp. 104-111. 
Copestake, Ann. 1993. Defaults in Lexical Represen-
tation. In Inheritance, Defaults, and the Lexicon. 
Cambridge: Cambridge University Press. 
Daum?, Hal III and Daniel Marcu. 2005. Learning as 
Search Optimization: Approximate Large Margin 
Methods for Structured Prediction. In proc. of 
ICML 2005. 
Geman, Stuart and Mark Johnson. 2002. Dynamic 
programming for parsing and estimation of sto-
chastic unification-based grammars. In proc. of 
ACL'02, pp. 279-286. 
Hockenmaier, Julia. 2003. Parsing with Generative 
Models of Predicate-Argument Structure. In proc. 
of ACL'03, pp. 359-366. 
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi 
Chi, and Stefan Riezler. 1999. Estimators for Sto-
chastic ``Unification-Based'' Grammars. In proc. of 
ACL '99, pp. 535-541. 
Kaplan, R. M., S. Riezler, T. H. King, J. T. Maxwell 
III, and A. Vasserman. 2004. Speed and accuracy 
in shallow and deep stochastic parsing. In proc. of 
HLT/NAACL'04. 
Malouf, Robert and Gertjan van Noord. 2004. Wide 
Coverage Parsing with Stochastic Attribute Value 
Grammars. In proc. of IJCNLP-04 Workshop 
``Beyond Shallow Analyses''. 
Matsuzaki, Takuya, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Supertag-
ging and CFG-filtering. In proc. of IJCAI 2007, pp. 
1671-1676. 
Miyao, Yusuke and Jun'ichi Tsujii. 2002. Maximum 
Entropy Estimation for Feature Forests. In proc. of 
HLT 2002, pp. 292-297. 
Miyao, Yusuke and Jun'ichi Tsujii. 2005. Probabilistic 
disambiguation models for wide-coverage HPSG 
parsing. In proc. of ACL'05, pp. 83-90. 
Nakagawa, Tetsuji. 2007. Multilingual dependency 
parsing using global features. In proc. of the 
CoNLL Shared Task Session of EMNLP-CoNLL 
2007, pp. 915-932. 
Ninomiya, Takashi, Takuya Matsuzaki, Yusuke 
Miyao, and Jun'ichi Tsujii. 2007. A log-linear 
model with an n-gram reference distribution for ac-
curate HPSG parsing. In proc. of IWPT 2007, pp. 
60-68. 
Ninomiya, Takashi, Takuya Matsuzaki, Yoshimasa 
Tsuruoka, Yusuke Miyao, and Jun'ichi Tsujii. 2006. 
Extremely Lexicalized Models for Accurate and 
Fast HPSG Parsing. In proc. of EMNLP 2006, pp. 
155-163. 
Ninomiya, Takashi, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2002. Lenient Default Unification for Robust 
Processing within Unification Based Grammar 
Formalisms. In proc. of COLING 2002, pp. 744-
750. 
Nivre, Joakim and Mario Scholz. 2004. Deterministic 
dependency parsing of English text. In proc. of 
COLING 2004, pp. 64-70. 
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar: University of Chicago 
Press. 
Ratnaparkhi, Adwait. 1997. A linear observed time 
statistical parser based on maximum entropy mod-
els. In proc. of EMNLP'97. 
Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and 
Mark Johnson. 2000. Lexicalized Stochastic Mod-
eling of Constraint-Based Grammars using Log-
Linear Measures and EM Training. In proc. of 
ACL'00, pp. 480-487. 
Sagae, Kenji and Alon Lavie. 2005. A classifier-based 
parser with linear run-time complexity. In proc. of 
IWPT 2005. 
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In proc. of COL-
ING/ACL on Main conference poster sessions, pp. 
691-698. 
Sagae, Kenji, Yusuke Miyao, and Jun'ichi Tsujii. 
2007. HPSG parsing with shallow dependency 
constraints. In proc. of ACL 2007, pp. 624-631. 
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector 
Machines. In proc. of IWPT-2003. 
 
611
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56?64,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Discriminative Latent Variable Chinese Segmenter
with Hybrid Word/Character Information
Xu Sun
Department of Computer Science
University of Tokyo
sunxu@is.s.u-tokyo.ac.jp
Yaozhong Zhang
Department of Computer Science
University of Tokyo
yaozhong.zhang@is.s.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
School of Computer Science
University of Manchester
yoshimasa.tsuruoka@manchester.ac.uk
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo, Japan
School of Computer Science, University of Manchester, UK
National Centre for Text Mining, UK
tsujii@is.s.u-tokyo.ac.jp
Abstract
Conventional approaches to Chinese word
segmentation treat the problem as a character-
based tagging task. Recently, semi-Markov
models have been applied to the problem, in-
corporating features based on complete words.
In this paper, we propose an alternative, a
latent variable model, which uses hybrid in-
formation based on both word sequences and
character sequences. We argue that the use of
latent variables can help capture long range
dependencies and improve the recall on seg-
menting long words, e.g., named-entities. Ex-
perimental results show that this is indeed the
case. With this improvement, evaluations on
the data of the second SIGHAN CWS bakeoff
show that our system is competitive with the
best ones in the literature.
1 Introduction
For most natural language processing tasks, words
are the basic units to process. Since Chinese sen-
tences are written as continuous sequences of char-
acters, segmenting a character sequence into a word
sequence is the first step for most Chinese process-
ing applications. In this paper, we study the prob-
lem of Chinese word segmentation (CWS), which
aims to find these basic units (words1) for a given
sentence in Chinese.
Chinese character sequences are normally am-
biguous, and out-of-vocabulary (OOV) words are a
major source of the ambiguity. Typical examples
of OOV words include named entities (e.g., orga-
nization names, person names, and location names).
Those named entities may be very long, and a dif-
ficult case occurs when a long word W (|W | ? 4)
consists of some words which can be separate words
on their own; in such cases an automatic segmenter
may split the OOV word into individual words. For
example,
(Computer Committee of International Federation of
Automatic Control) is one of the organization names
in the Microsoft Research corpus. Its length is 13
and it contains more than 6 individual words, but it
should be treated as a single word. Proper recogni-
tion of long OOV words are meaningful not only for
word segmentation, but also for a variety of other
purposes, e.g., full-text indexing. However, as is il-
lustrated, recognizing long words (without sacrific-
ing the performance on short words) is challenging.
Conventional approaches to Chinese word seg-
mentation treat the problem as a character-based la-
1Following previous work, in this paper, words can also refer
to multi-word expressions, including proper names, long named
entities, idioms, etc.
56
beling task (Xue, 2003). Labels are assigned to each
character in the sentence, indicating whether the
character xi is the start (Labeli = B), middle or end
of a multi-character word (Labeli = C). A popu-
lar discriminative model that have been used for this
task is the conditional random fields (CRFs) (Laf-
ferty et al, 2001), starting with the model of Peng
et al (2004). In the Second International Chinese
Word Segmentation Bakeoff (the second SIGHAN
CWS bakeoff) (Emerson, 2005), two of the highest
scoring systems in the closed track competition were
based on a CRF model (Tseng et al, 2005; Asahara
et al, 2005).
While the CRF model is quite effective compared
with other models designed for CWS, it may be lim-
ited by its restrictive independence assumptions on
non-adjacent labels. Although the window can in
principle be widened by increasing the Markov or-
der, this may not be a practical solution, because
the complexity of training and decoding a linear-
chain CRF grows exponentially with the Markov or-
der (Andrew, 2006).
To address this difficulty, a choice is to relax the
Markov assumption by using the semi-Markov con-
ditional random field model (semi-CRF) (Sarawagi
and Cohen, 2004). Despite the theoretical advan-
tage of semi-CRFs over CRFs, however, some pre-
vious studies (Andrew, 2006; Liang, 2005) explor-
ing the use of a semi-CRF for Chinese word seg-
mentation did not find significant gains over the
CRF ones. As discussed in Andrew (2006), the rea-
son may be that despite the greater representational
power of the semi-CRF, there are some valuable fea-
tures that could be more naturally expressed in a
character-based labeling model. For example, on
a CRF model, one might use the feature ?the cur-
rent character xi is X and the current label Labeli
is C?. This feature may be helpful in CWS for gen-
eralizing to new words. For example, it may rule
out certain word boundaries if X were a character
that normally occurs only as a suffix but that com-
bines freely with some other basic forms to create
new words. This type of features is slightly less nat-
ural in a semi-CRF, since in that case local features
?(yi, yi+1, x) are defined on pairs of adjacent words.
That is to say, information about which characters
are not on boundaries is only implicit. Notably, ex-
cept the hybrid Markov/semi-Markov system in An-
drew (2006)2, no other studies using the semi-CRF
(Sarawagi and Cohen, 2004; Liang, 2005; Daume?
III and Marcu, 2005) experimented with features of
segmenting non-boundaries.
In this paper, instead of using semi-Markov mod-
els, we describe an alternative, a latent variable
model, to learn long range dependencies in Chi-
nese word segmentation. We use the discrimina-
tive probabilistic latent variable models (DPLVMs)
(Morency et al, 2007; Petrov and Klein, 2008),
which use latent variables to carry additional infor-
mation that may not be expressed by those original
labels, and therefore try to build more complicated
or longer dependencies. This is especially meaning-
ful in CWS, because the used labels are quite coarse:
Label(y) ? {B,C}, where B signifies beginning a
word and C signifies the continuation of a word.3
For example, by using DPLVM, the aforementioned
feature may turn to ?the current character xi is X ,
Labeli = C, and LatentV ariablei = LV ?. The
current latent variable LV may strongly depend on
the previous one or many latent variables, and there-
fore we can model the long range dependencies
which may not be captured by those very coarse la-
bels. Also, since character and word information
have their different advantages in CWS, in our latent
variable model, we use hybrid information based on
both character and word sequences.
2 A Latent Variable Segmenter
2.1 Discriminative Probabilistic Latent
Variable Model
Given data with latent structures, the task is to
learn a mapping between a sequence of observa-
tions x = x1, x2, . . . , xm and a sequence of labels
y = y1, y2, . . . , ym. Each yj is a class label for the
j?th character of an input sequence, and is a mem-
ber of a set Y of possible class labels. For each se-
quence, the model also assumes a sequence of latent
variables h = h1, h2, . . . , hm, which is unobserv-
able in training examples.
The DPLVM is defined as follows (Morency et al,
2The system was also used in Gao et al (2007), with an
improved performance in CWS.
3In practice, one may add a few extra labels based on lin-
guistic intuitions (Xue, 2003).
57
2007):
P (y|x,?) =?
h
P (y|h,x,?)P (h|x,?), (1)
where ? are the parameters of the model. DPLVMs
can be seen as a natural extension of CRF models,
and CRF models can be seen as a special case of
DPLVMs that have only one latent variable for each
label.
To make the training and inference efficient, the
model is restricted to have disjoint sets of latent vari-
ables associated with each class label. Each hj is a
member in a set Hyj of possible latent variables for
the class label yj . H is defined as the set of all pos-
sible latent variables, i.e., the union of all Hyj sets.
Since sequences which have any hj /? Hyj will by
definition have P (y|x,?) = 0, the model can be
further defined4 as:
P (y|x,?) = ?
h?Hy1?...?Hym
P (h|x,?), (2)
where P (h|x,?) is defined by the usual conditional
random field formulation:
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
in which f(h,x) is a feature vector. Given a training
set consisting of n labeled sequences, (xi,yi), for
i = 1 . . . n, parameter estimation is performed by
optimizing the objective function,
L(?) =
n?
i=1
log P (yi|xi,?) ? R(?). (4)
The first term of this equation is the conditional log-
likelihood of the training data. The second term is
a regularizer that is used for reducing overfitting in
parameter estimation.
For decoding in the test stage, given a test se-
quence x, we want to find the most probable label
sequence, y?:
y? = argmaxyP (y|x,??). (5)
For latent conditional models like DPLVMs, the best
label path y? cannot directly be produced by the
4It means that Eq. 2 is from Eq. 1 with additional definition.
Viterbi algorithm because of the incorporation of
hidden states. In this paper, we use a technique
based on A? search and dynamic programming de-
scribed in Sun and Tsujii (2009), for producing the
most probable label sequence y? on DPLVM.
In detail, an A? search algorithm5 (Hart et al,
1968) with a Viterbi heuristic function is adopted to
produce top-n latent paths, h1,h2, . . .hn. In addi-
tion, a forward-backward-style algorithm is used to
compute the exact probabilities of their correspond-
ing label paths, y1,y2, . . .yn. The model then tries
to determine the optimal label path based on the
top-n statistics, without enumerating the remaining
low-probability paths, which could be exponentially
enormous.
The optimal label path y? is ready when the fol-
lowing ?exact-condition? is achieved:
P (y1|x,?) ? (1 ?
?
yk?LPn
P (yk|x,?)) ? 0, (6)
where y1 is the most probable label sequence in
current stage. It is straightforward to prove that
y? = y1, and further search is unnecessary. This
is because the remaining probability mass, 1 ??
yk?LPn P (yk|x,?), cannot beat the current op-timal label path in this case. For more details of the
inference, refer to Sun and Tsujii (2009).
2.2 Hybrid Word/Character Information
We divide our main features into two types:
character-based features and word-based features.
The character-based features are indicator functions
that fire when the latent variable label takes some
value and some predicate of the input (at a certain
position) corresponding to the label is satisfied. For
each latent variable label hi (the latent variable la-
bel at position i), we use the predicate templates as
follows:
? Input characters/numbers/letters locating at po-
sitions i ? 2, i ? 1, i, i + 1 and i + 2
? The character/number/letter bigrams locating
at positions i ? 2, i ? 1, i and i + 1
5A? search and its variants, like beam-search, are widely
used in statistical machine translation. Compared to other
search techniques, an interesting point of A? search is that it
can produce top-n results one-by-one in an efficient manner.
58
? Whether xj and xj+1 are identical, for j = (i?
2) . . . (i + 1)
? Whether xj and xj+2 are identical, for j = (i?
3) . . . (i + 1)
The latter two feature templates are designed to de-
tect character or word reduplication, a morphologi-
cal phenomenon that can influence word segmenta-
tion in Chinese.
The word-based features are indicator functions
that fire when the local character sequence matches
a word or a word bigram. A dictionary containing
word and bigram information was collected from the
training data. For each latent variable label unigram
hi, we use the set of predicate template checking for
word-based features:
? The identity of the string xj . . . xi, if it matches
a word A from the word-dictionary of training
data, with the constraint i?6 < j < i; multiple
features will be generated if there are multiple
strings satisfying the condition.
? The identity of the string xi . . . xk, if it matches
a word A from the word-dictionary of training
data, with the constraint i < k < i+6; multiple
features could be generated.
? The identity of the word bigram (xj . . . xi?1,
xi . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
? The identity of the word bigram (xj . . . xi,
xi+1 . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
All feature templates were instantiated with val-
ues that occur in positive training examples. We
found that using low-frequency features that occur
only a few times in the training set improves perfor-
mance on the development set. We hence do not do
any thresholding of the DPLVM features: we simply
use all those generated features.
The aforementioned word based features can in-
corporate word information naturally. In addition,
following Wang et al (2006), we found using a
very simple heuristic can further improve the seg-
mentation quality slightly. More specifically, two
operations, merge and split, are performed on the
DPLVM/CRF outputs: if a bigram A B was not ob-
served in the training data, but the merged one AB
was, then A B will be simply merged into AB; on
the other hand, if AB was not observed but A B ap-
peared, then it will be split into A B. We found this
simple heuristic on word information slightly im-
proved the performance (e.g., for the PKU corpus,
+0.2% on the F-score).
3 Experiments
We used the data provided by the second Inter-
national Chinese Word Segmentation Bakeoff to
test our approaches described in the previous sec-
tions. The data contains three corpora from different
sources: Microsoft Research Asia (MSR), City Uni-
versity of Hong Kong (CU), and Peking University
(PKU).
Since the purpose of this work is to evaluate the
proposed latent variable model, we did not use ex-
tra resources such as common surnames, lexicons,
parts-of-speech, and semantics. For the generation
of word-based features, we extracted a word list
from the training data as the vocabulary.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the de-
coder), precision (P , the percentage of words in the
decoder output that are segmented correctly), bal-
anced F-score (F ) defined by 2PR/(P + R), recall
of OOV words (R-oov). For more detailed informa-
tion on the corpora and these metrics, refer to Emer-
son (2005).
3.1 Training the DPLVM Segmenter
We implemented DPLVMs in C++ and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions. We
employ the feature templates defined in Section 2.2,
taking into account those 3,069,861 features for the
MSR data, 2,634,384 features for the CU data, and
1,989,561 features for the PKU data.
As for numerical optimization, we performed
gradient decent with the Limited-Memory BFGS
59
(L-BFGS)6 optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order Quasi-
Newton method that numerically estimates the cur-
vature from previous gradients and updates. With
no requirement on specialized Hessian approxima-
tion, L-BFGS can handle large-scale problems in an
efficient manner.
Since the objective function of the DPLVM model
is non-convex, we randomly initialized parameters
for the training.7 To reduce overfitting, we employed
an L2 Gaussian weight prior8 (Chen and Rosen-
feld, 1999). During training, we varied the L2-
regularization term (with values 10k, k from -3 to
3), and finally set the value to 1. We use 4 hidden
variables per label for this task, compromising be-
tween accuracy and efficiency.
3.2 Comparison on Convergence Speed
First, we show a comparison of the convergence
speed between the objective function of DPLVMs
and CRFs. We apply the L-BFGS optimization algo-
rithm to optimize the objective function of DPLVM
and CRF models, making a comparison between
them. We find that the number of iterations required
for the convergence of DPLVMs are fewer than for
CRFs. Figure 1 illustrates the convergence-speed
comparison on the MSR data. The DPLVM model
arrives at the plateau of convergence in around 300
iterations, with the penalized loss of 95K when
#passes = 300; while CRFs require 900 iterations,
with the penalized loss of 98K when #passes =
900.
However, we should note that the time cost of the
DPLVM model in each iteration is around four times
higher than the CRF model, because of the incorpo-
ration of hidden variables. In order to speed up the
6For numerical optimization on latent variable models, we
also experimented the conjugate-gradient (CG) optimization al-
gorithm and stochastic gradient decent algorithm (SGD). We
found the L-BFGS with L2 Gaussian regularization performs
slightly better than the CG and the SGD. Therefore, we adopt
the L-BFGS optimizer in this study.
7For a non-convex objective function, different parame-
ter initializations normally bring different optimization results.
Therefore, to approach closer to the global optimal point, it
is recommended to perform multiple experiments on DPLVMs
with random initialization and then select a good start point.
8We also tested the L-BFGS with L1 regularization, and we
found the L-BFGS with L2 regularization performs better in
this task.
0
300K
600K
900K
1200K
1500K
1800K
 100  200  300  400  500  600  700  800  900
O
bj.
 Fu
nc
. V
alu
e
Forward-Backward Passes
DPLVM
CRF
Figure 1: The value of the penalized loss based on the
number of iterations: DPLVMs vs. CRFs on the MSR
data.
Style #W.T. #Word #C.T. #Char
MSR S.C. 88K 2,368K 5K 4,050K
CU T.C. 69K 1,455K 5K 2,403K
PKU S.C. 55K 1,109K 5K 1,826K
Table 1: Details of the corpora. W.T. represents word
types; C.T. represents character types; S.C. represents
simplified Chinese; T.C. represents traditional Chinese.
training speed of the DPLVM model in the future,
one solution is to use the stochastic learning tech-
nique9. Another solution is to use a distributed ver-
sion of L-BFGS to parallelize the batch training.
4 Results and Discussion
Since the CRF model is one of the most successful
models in Chinese word segmentation, we compared
DPLVMs with CRFs. We tried to make experimen-
tal results comparable between DPLVMs and CRF
models, and have therefore employed the same fea-
ture set, optimizer and fine-tuning strategy between
the two. We also compared DPLVMs with semi-
CRFs and other successful systems reported in pre-
vious work.
4.1 Evaluation Results
Three training and test corpora were used in the test,
including the MSR Corpus, the CU Corpus, and the
9We have tried stochastic gradient decent, as described pre-
viously. It is possible to try other stochastic learning methods,
e.g., stochastic meta decent (Vishwanathan et al, 2006).
60
MSR data P R F R-oov
DPLVM (*) 97.3 97.3 97.3 72.2
CRF (*) 97.1 96.8 97.0 72.0
semi-CRF (A06) N/A N/A 96.8 N/A
semi-CRF (G07) N/A N/A 97.2 N/A
CRF (Z06-a) 96.5 96.3 96.4 71.4
Z06-b 97.2 96.9 97.1 71.2
ZC07 N/A N/A 97.2 N/A
Best05 (T05) 96.2 96.6 96.4 71.7
CU data P R F R-oov
DPLVM (*) 94.7 94.4 94.6 68.8
CRF (*) 94.3 93.9 94.1 65.8
CRF (Z06-a) 95.0 94.2 94.6 73.6
Z06-b 95.2 94.9 95.1 74.1
ZC07 N/A N/A 95.1 N/A
Best05 (T05) 94.1 94.6 94.3 69.8
PKU data P R F R-oov
DPLVM (*) 95.6 94.8 95.2 77.8
CRF (*) 95.2 94.2 94.7 76.8
CRF (Z06-a) 94.3 94.6 94.5 75.4
Z06-b 94.7 95.5 95.1 74.8
ZC07 N/A N/A 94.5 N/A
Best05 (C05) 95.3 94.6 95.0 63.6
Table 2: Results from DPLVMs, CRFs, semi-CRFs, and
other systems.
PKU Corpus (see Table 1 for details). The results
are shown in Table 2. The results are grouped into
three sub-tables according to different corpora. Each
row represents a CWS model. For each group, the
rows marked by ? represent our models with hy-
brid word/character information. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; A06 represents the semi-CRF model in An-
drew (2006)10, which was also used in Gao et al
(2007) (denoted as G07) with an improved perfor-
mance; Z06-a and Z06-b represents the pure sub-
word CRF model and the confidence-based com-
bination of CRF and rule-based models, respec-
tively (Zhang et al, 2006); ZC07 represents the
word-based perceptron model in Zhang and Clark
(2007); T05 represents the CRF model in Tseng et
al. (2005); C05 represents the system in Chen et al
10It is a hybrid Markov/semi-Markov CRF model which
outperforms conventional semi-CRF models (Andrew, 2006).
However, in general, as discussed in Andrew (2006), it is essen-
tially still a semi-CRF model.
(2005). The best F-score and recall of OOV words
of each group is shown in bold.
As is shown in the table, we achieved the best
F-score in two out of the three corpora. We also
achieved the best recall rate of OOV words on those
two corpora. Both of the MSR and PKU Corpus use
simplified Chinese, while the CU Corpus uses the
traditional Chinese.
On the MSR Corpus, the DPLVM model reduced
more than 10% error rate over the CRF model us-
ing exactly the same feature set. We also compared
our DPLVM model with the semi-CRF models in
Andrew (2006) and Gao et al (2007), and demon-
strate that the DPLVM model achieved slightly bet-
ter performance than the semi-CRF models. Andrew
(2006) and Gao et al (2007) only reported the re-
sults on the MSR Corpus.
In summary, tests for the Second International
Chinese Word Segmentation Bakeoff showed com-
petitive results for our method compared with the
best results in the literature. Our discriminative la-
tent variable models achieved the best F-scores on
the MSR Corpus (97.3%) and PKU Corpus (95.2%);
the latent variable models also achieved the best re-
calls of OOV words over those two corpora. We will
analyze the results by varying the word-length in the
following subsection.
4.2 Effect on Long Words
One motivation of using a latent variable model for
CWS is to use latent variables to more adequately
learn long range dependencies, as we argued in Sec-
tion 1. In the test data of the MSR Corpus, 19% of
the words are longer than 3 characters; there are also
8% in the CU Corpus and 11% in the PKU Corpus,
respectively. In the MSR Corpus, there are some ex-
tremely long words (Length > 10), while the CU
and PKU corpus do not contain such extreme cases.
Figure 2 shows the recall rate on different groups
of words categorized by their lengths (the number
of characters). As we expected, the DPLVM model
performs much better on long words (Length ? 4)
than the CRF model, which used exactly the same
feature set. Compared with the CRF model, the
DPLVM model exhibited almost the same level of
performance on short words. Both models have
the best performance on segmenting the words with
the length of two. The performance of the CRF
61
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-M
SR
 (%
)
Length of Word (MSR)
DPLVM
CRF
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-C
U 
(%
)
Length of Word (CU)
DPLVM
CRF
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-P
KU
 (%
)
Length of Word (PKU)
DPLVM
CRF
Figure 2: The recall rate on words grouped by the length.
model deteriorates rapidly as the word length in-
creases, which demonstrated the difficulty on mod-
eling long range dependencies in CWS. Compared
with the CRF model, the DPLVM model performed
quite well in dealing with long words, without sacri-
ficing the performance on short words. All in all, we
conclude that the improvement of using the DPLVM
model came from the improvement on modeling
long range dependencies in CWS.
4.3 Error Analysis
Table 3 lists the major errors collected from the la-
tent variable segmenter. We examined the collected
errors and found that many of them can be grouped
into four types: over-generalization (the top row),
errors on named entities (the following three rows),
errors on idioms (the following three rows) and er-
rors from inconsistency (the two rows at the bottom).
Our system performed reasonably well on very
complex OOV words, such as
(Agricultural Bank of China,
Gold Segmentation Segmenter Output
//
Co-allocated org. names
(Chen Yao) //
(Chen Fei) //
(Vasillis) //
//
//
// //
Idioms
// (propagandist)
(desertification) //
Table 3: Error analysis on the latent variable seg-
menter. The errors are grouped into four types: over-
generalization, errors on named entities, errors on idioms
and errors from data-inconsistency.
Shijiazhuang-city Branch, the second sales depart-
ment) and (Science
and Technology Commission of China, National In-
stitution on Scientific Information Analysis). How-
ever, it sometimes over-generalized to long words.
For example, as shown in the top row,
(National Department of Environmental Protection)
and (The Central Propaganda Department)
are two organization names, but they are incorrectly
merged into a single word.
As for the following three rows, (Chen Yao)
and (Chen Fei) are person names. They are
wrongly segmented because we lack the features to
capture the information of person names (such use-
ful knowledge, e.g., common surname list, are cur-
rently not used in our system). In the future, such
errors may be solved by integrating open resources
into our system. (Vasillis) is a transliter-
ated foreign location name and is also wrongly seg-
mented.
For the corpora that considered 4 character idioms
as a word, our system successfully combined most
of new idioms together. This differs greatly from the
results of CRFs. However, there are still a number
of new idioms that failed to be correctly segmented,
as listed from the fifth row to the seventh row.
Finally, some errors are due to inconsistencies in
the gold segmentation. For example, // (pro-
pagandist) is two words, but a word with similar
62
structure, (theorist), is one word.
(desertification) is one word, but its synonym,
// (desertification), is two words in the gold seg-
mentation.
5 Conclusion and Future Work
We presented a latent variable model for Chinese
word segmentation, which used hybrid information
based on both word and character sequences. We
discussed that word and character information have
different advantages, and could be complementary
to each other. Our model is an alternative to the ex-
isting word based models and character based mod-
els.
We argued that using latent variables can better
capture long range dependencies. We performed
experiments and demonstrated that our model can
indeed improve the segmentation accuracy on long
words. With this improvement, tests on the data
of the Second International Chinese Word Segmen-
tation Bakeoff show that our system is competitive
with the best in the literature.
Since the latent variable model allows a wide
range of features, so the future work will consider
how to integrate open resources into our system. The
latent variable model handles latent-dependencies
naturally, and can be easily extended to other label-
ing tasks.
Acknowledgments
We thank Kun Yu, Galen Andrew and Xiaojun Lin
for the enlightening discussions. We also thank the
anonymous reviewers who gave very helpful com-
ments. This work was partially supported by Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word segmen-
tation. Proceedings of the fourth SIGHAN workshop,
pages 134?137.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word seg-
mentation. Proceedings of the fourth SIGHAN work-
shop.
Hal Daume? III and Daniel Marcu. 2005. Learn-
ing as search optimization: approximate large mar-
gin methods for structured prediction. Proceedings of
ICML?05.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of the
fourth SIGHAN workshop, pages 123?133.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL?07), pages 824?831.
P.E. Hart, N.J. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
path. IEEE Trans. On System Science and Cybernet-
ics, SSC-4(2):100?107.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. Proceed-
ings of ICML?01, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings of
CVPR?07, pages 1?8.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
F. Peng and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random
fields. Proceedings of COLING?04.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings of
NIPS?08.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Proceedings of ICML?04.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm and
its efficient approximation. Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL?09).
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bakeoff
63
2005. Proceedings of the fourth SIGHAN workshop,
pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. Proceedings of ICML?06, pages 969?
976.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop, pages
138?141, July.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Computa-
tional Linguistics and Chinese Language Processing,
8(1).
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. Pro-
ceedings of ACL?07.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. Proceedings of
HLT/NAACL?06 companion volume short papers.
64
Proceedings of the 43rd Annual Meeting of the ACL, pages 75?82,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Probabilistic CFG with latent annotations
Takuya Matsuzaki
 
Yusuke Miyao
 
Jun?ichi Tsujii  
 
Graduate School of Information Science and Technology, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033

CREST, JST(Japan Science and Technology Agency)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012

matuzaki, yusuke, tsujii  @is.s.u-tokyo.ac.jp
Abstract
This paper defines a generative probabilis-
tic model of parse trees, which we call
PCFG-LA. This model is an extension of
PCFG in which non-terminal symbols are
augmented with latent variables. Fine-
grained CFG rules are automatically in-
duced from a parsed corpus by training a
PCFG-LA model using an EM-algorithm.
Because exact parsing with a PCFG-LA is
NP-hard, several approximations are de-
scribed and empirically compared. In ex-
periments using the Penn WSJ corpus, our
automatically trained model gave a per-
formance of 86.6% (F  , sentences  40
words), which is comparable to that of an
unlexicalized PCFG parser created using
extensive manual feature selection.
1 Introduction
Variants of PCFGs form the basis of several broad-
coverage and high-precision parsers (Collins, 1999;
Charniak, 1999; Klein and Manning, 2003). In those
parsers, the strong conditional independence as-
sumption made in vanilla treebank PCFGs is weak-
ened by annotating non-terminal symbols with many
?features? (Goodman, 1997; Johnson, 1998). Exam-
ples of such features are head words of constituents,
labels of ancestor and sibling nodes, and subcatego-
rization frames of lexical heads. Effective features
and their good combinations are normally explored
using trial-and-error.
This paper defines a generative model of parse
trees that we call PCFG with latent annotations
(PCFG-LA). This model is an extension of PCFG
models in which non-terminal symbols are anno-
tated with latent variables. The latent variables work
just like the features attached to non-terminal sym-
bols. A fine-grained PCFG is automatically induced
from parsed corpora by training a PCFG-LA model
using an EM-algorithm, which replaces the manual
feature selection used in previous research.
The main focus of this paper is to examine the
effectiveness of the automatically trained models in
parsing. Because exact inference with a PCFG-LA,
i.e., selection of the most probable parse, is NP-hard,
we are forced to use some approximation of it. We
empirically compared three different approximation
methods. One of the three methods gives a perfor-
mance of 86.6% (F  , sentences  40 words) on the
standard test set of the Penn WSJ corpus.
Utsuro et al (1996) proposed a method that auto-
matically selects a proper level of generalization of
non-terminal symbols of a PCFG, but they did not
report the results of parsing with the obtained PCFG.
Henderson?s parsing model (Henderson, 2003) has a
similar motivation as ours in that a derivation history
of a parse tree is compactly represented by induced
hidden variables (hidden layer activation of a neu-
ral network), although the details of his approach is
quite different from ours.
2 Probabilistic model
PCFG-LA is a generative probabilistic model of
parse trees. In this model, an observed parse tree
is considered as an incomplete data, and the corre-
75
	 

:

:

 
 
Proceedings of ACL-08: HLT, pages 46?54,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Task-oriented Evaluation of Syntactic Parsers and Their Representations
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a comparative evalua-
tion of several state-of-the-art English parsers
based on different frameworks. Our approach
is to measure the impact of each parser when it
is used as a component of an information ex-
traction system that performs protein-protein
interaction (PPI) identification in biomedical
papers. We evaluate eight parsers (based on
dependency parsing, phrase structure parsing,
or deep parsing) using five different parse rep-
resentations. We run a PPI system with several
combinations of parser and parse representa-
tion, and examine their impact on PPI identi-
fication accuracy. Our experiments show that
the levels of accuracy obtained with these dif-
ferent parsers are similar, but that accuracy
improvements vary when the parsers are re-
trained with domain-specific data.
1 Introduction
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame-
works (Charniak, 2000; Klein and Manning, 2003;
Charniak and Johnson, 2005; Petrov and Klein,
2007), but also include dependency parsers (Mc-
Donald and Pereira, 2006; Nivre and Nilsson, 2005;
Sagae and Tsujii, 2007) and deep parsers (Kaplan
et al, 2004; Clark and Curran, 2004; Miyao and
Tsujii, 2008). However, efforts to perform extensive
comparisons of syntactic parsers based on different
frameworks have been limited. The most popular
method for parser comparison involves the direct
measurement of the parser output accuracy in terms
of metrics such as bracketing precision and recall, or
dependency accuracy. This assumes the existence of
a gold-standard test corpus, such as the Penn Tree-
bank (Marcus et al, 1994). It is difficult to apply
this method to compare parsers based on different
frameworks, because parse representations are often
framework-specific and differ from parser to parser
(Ringger et al, 2004). The lack of such comparisons
is a serious obstacle for NLP researchers in choosing
an appropriate parser for their purposes.
In this paper, we present a comparative evalua-
tion of syntactic parsers and their output represen-
tations based on different frameworks: dependency
parsing, phrase structure parsing, and deep pars-
ing. Our approach to parser evaluation is to mea-
sure accuracy improvement in the task of identify-
ing protein-protein interaction (PPI) information in
biomedical papers, by incorporating the output of
different parsers as statistical features in a machine
learning classifier (Yakushiji et al, 2005; Katrenko
and Adriaans, 2006; Erkan et al, 2007; S?tre et al,
2007). PPI identification is a reasonable task for
parser evaluation, because it is a typical information
extraction (IE) application, and because recent stud-
ies have shown the effectiveness of syntactic parsing
in this task. Since our evaluation method is applica-
ble to any parser output, and is grounded in a real
application, it allows for a fair comparison of syn-
tactic parsers based on different frameworks.
Parser evaluation in PPI extraction also illu-
minates domain portability. Most state-of-the-art
parsers for English were trained with the Wall Street
Journal (WSJ) portion of the Penn Treebank, and
high accuracy has been reported for WSJ text; how-
ever, these parsers rely on lexical information to at-
tain high accuracy, and it has been criticized that
these parsers may overfit to WSJ text (Gildea, 2001;
46
Klein and Manning, 2003). Another issue for dis-
cussion is the portability of training methods. When
training data in the target domain is available, as
is the case with the GENIA Treebank (Kim et al,
2003) for biomedical papers, a parser can be re-
trained to adapt to the target domain, and larger ac-
curacy improvements are expected, if the training
method is sufficiently general. We will examine
these two aspects of domain portability by compar-
ing the original parsers with the retrained parsers.
2 Syntactic Parsers and Their
Representations
This paper focuses on eight representative parsers
that are classified into three parsing frameworks:
dependency parsing, phrase structure parsing, and
deep parsing. In general, our evaluation methodol-
ogy can be applied to English parsers based on any
framework; however, in this paper, we chose parsers
that were originally developed and trained with the
Penn Treebank or its variants, since such parsers can
be re-trained with GENIA, thus allowing for us to
investigate the effect of domain adaptation.
2.1 Dependency parsing
Because the shared tasks of CoNLL-2006 and
CoNLL-2007 focused on data-driven dependency
parsing, it has recently been extensively studied in
parsing research. The aim of dependency pars-
ing is to compute a tree structure of a sentence
where nodes are words, and edges represent the re-
lations among words. Figure 1 shows a dependency
tree for the sentence ?IL-8 recognizes and activates
CXCR1.? An advantage of dependency parsing is
that dependency trees are a reasonable approxima-
tion of the semantics of sentences, and are readily
usable in NLP applications. Furthermore, the effi-
ciency of popular approaches to dependency pars-
ing compare favorable with those of phrase struc-
ture parsing or deep parsing. While a number of ap-
proaches have been proposed for dependency pars-
ing, this paper focuses on two typical methods.
MST McDonald and Pereira (2006)?s dependency
parser,1 based on the Eisner algorithm for projective
dependency parsing (Eisner, 1996) with the second-
order factorization.
1http://sourceforge.net/projects/mstparser
Figure 1: CoNLL-X dependency tree
Figure 2: Penn Treebank-style phrase structure tree
KSDEP Sagae and Tsujii (2007)?s dependency
parser,2 based on a probabilistic shift-reduce al-
gorithm extended by the pseudo-projective parsing
technique (Nivre and Nilsson, 2005).
2.2 Phrase structure parsing
Owing largely to the Penn Treebank, the mainstream
of data-driven parsing research has been dedicated
to the phrase structure parsing. These parsers output
Penn Treebank-style phrase structure trees, although
function tags and empty categories are stripped off
(Figure 2). While most of the state-of-the-art parsers
are based on probabilistic CFGs, the parameteriza-
tion of the probabilistic model of each parser varies.
In this work, we chose the following four parsers.
NO-RERANK Charniak (2000)?s parser, based on a
lexicalized PCFG model of phrase structure trees.3
The probabilities of CFG rules are parameterized on
carefully hand-tuned extensive information such as
lexical heads and symbols of ancestor/sibling nodes.
RERANK Charniak and Johnson (2005)?s rerank-
ing parser. The reranker of this parser receives n-
best4 parse results from NO-RERANK, and selects
the most likely result by using a maximum entropy
model with manually engineered features.
BERKELEY Berkeley?s parser (Petrov and Klein,
2007).5 The parameterization of this parser is op-
2http://www.cs.cmu.edu/?sagae/parser/
3http://bllip.cs.brown.edu/resources.shtml
4We set n = 50 in this paper.
5http://nlp.cs.berkeley.edu/Main.html#Parsing
47
Figure 3: Predicate argument structure
timized automatically by assigning latent variables
to each nonterminal node and estimating the param-
eters of the latent variables by the EM algorithm
(Matsuzaki et al, 2005).
STANFORD Stanford?s unlexicalized parser (Klein
and Manning, 2003).6 Unlike NO-RERANK, proba-
bilities are not parameterized on lexical heads.
2.3 Deep parsing
Recent research developments have allowed for ef-
ficient and robust deep parsing of real-world texts
(Kaplan et al, 2004; Clark and Curran, 2004; Miyao
and Tsujii, 2008). While deep parsers compute
theory-specific syntactic/semantic structures, pred-
icate argument structures (PAS) are often used in
parser evaluation and applications. PAS is a graph
structure that represents syntactic/semantic relations
among words (Figure 3). The concept is therefore
similar to CoNLL dependencies, though PAS ex-
presses deeper relations, and may include reentrant
structures. In this work, we chose the two versions
of the Enju parser (Miyao and Tsujii, 2008).
ENJU The HPSG parser that consists of an HPSG
grammar extracted from the Penn Treebank, and
a maximum entropy model trained with an HPSG
treebank derived from the Penn Treebank.7
ENJU-GENIA The HPSG parser adapted to
biomedical texts, by the method of Hara et al
(2007). Because this parser is trained with both
WSJ and GENIA, we compare it parsers that are
retrained with GENIA (see section 3.3).
3 Evaluation Methodology
In our approach to parser evaluation, we measure
the accuracy of a PPI extraction system, in which
6http://nlp.stanford.edu/software/lex-parser.
shtml
7http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
This study demonstrates that IL-8 recognizes and
activates CXCR1, CXCR2, and the Duffy antigen
by distinct mechanisms.
The molar ratio of serum retinol-binding protein
(RBP) to transthyretin (TTR) is not useful to as-
sess vitamin A status during infection in hospi-
talised children.
Figure 4: Sentences including protein names
ENTITY1(IL-8) SBJ?? recognizes OBJ?? ENTITY2(CXCR1)
Figure 5: Dependency path
the parser output is embedded as statistical features
of a machine learning classifier. We run a classi-
fier with features of every possible combination of a
parser and a parse representation, by applying con-
versions between representations when necessary.
We also measure the accuracy improvements ob-
tained by parser retraining with GENIA, to examine
the domain portability, and to evaluate the effective-
ness of domain adaptation.
3.1 PPI extraction
PPI extraction is an NLP task to identify protein
pairs that are mentioned as interacting in biomedical
papers. Because the number of biomedical papers is
growing rapidly, it is impossible for biomedical re-
searchers to read all papers relevant to their research;
thus, there is an emerging need for reliable IE tech-
nologies, such as PPI identification.
Figure 4 shows two sentences that include pro-
tein names: the former sentence mentions a protein
interaction, while the latter does not. Given a pro-
tein pair, PPI extraction is a task of binary classi-
fication; for example, ?IL-8, CXCR1? is a positive
example, and ?RBP, TTR? is a negative example.
Recent studies on PPI extraction demonstrated that
dependency relations between target proteins are ef-
fective features for machine learning classifiers (Ka-
trenko and Adriaans, 2006; Erkan et al, 2007; S?tre
et al, 2007). For the protein pair IL-8 and CXCR1
in Figure 4, a dependency parser outputs a depen-
dency tree shown in Figure 1. From this dependency
tree, we can extract a dependency path shown in Fig-
ure 5, which appears to be a strong clue in knowing
that these proteins are mentioned as interacting.
48
(dep_path (SBJ (ENTITY1 recognizes))
(rOBJ (recognizes ENTITY2)))
Figure 6: Tree representation of a dependency path
We follow the PPI extraction method of S?tre et
al. (2007), which is based on SVMs with SubSet
Tree Kernels (Collins and Duffy, 2002; Moschitti,
2006), while using different parsers and parse rep-
resentations. Two types of features are incorporated
in the classifier. The first is bag-of-words features,
which are regarded as a strong baseline for IE sys-
tems. Lemmas of words before, between and after
the pair of target proteins are included, and the linear
kernel is used for these features. These features are
commonly included in all of the models. Filtering
by a stop-word list is not applied because this setting
made the scores higher than S?tre et al (2007)?s set-
ting. The other type of feature is syntactic features.
For dependency-based parse representations, a de-
pendency path is encoded as a flat tree as depicted in
Figure 6 (prefix ?r? denotes reverse relations). Be-
cause a tree kernel measures the similarity of trees
by counting common subtrees, it is expected that the
system finds effective subsequences of dependency
paths. For the PTB representation, we directly en-
code phrase structure trees.
3.2 Conversion of parse representations
It is widely believed that the choice of representa-
tion format for parser output may greatly affect the
performance of applications, although this has not
been extensively investigated. We should therefore
evaluate the parser performance in multiple parse
representations. In this paper, we create multiple
parse representations by converting each parser?s de-
fault output into other representations when possi-
ble. This experiment can also be considered to be
a comparative evaluation of parse representations,
thus providing an indication for selecting an appro-
priate parse representation for similar IE tasks.
Figure 7 shows our scheme for representation
conversion. This paper focuses on five representa-
tions as described below.
CoNLL The dependency tree format used in the
2006 and 2007 CoNLL shared tasks on dependency
parsing. This is a representation format supported by
several data-driven dependency parsers. This repre-
Figure 7: Conversion of parse representations
Figure 8: Head dependencies
sentation is also obtained from Penn Treebank-style
trees by applying constituent-to-dependency conver-
sion8 (Johansson and Nugues, 2007). It should be
noted, however, that this conversion cannot work
perfectly with automatic parsing, because the con-
version program relies on function tags and empty
categories of the original Penn Treebank.
PTB Penn Treebank-style phrase structure trees
without function tags and empty nodes. This is the
default output format for phrase structure parsers.
We also create this representation by converting
ENJU?s output by tree structure matching, although
this conversion is not perfect because forms of PTB
and ENJU?s output are not necessarily compatible.
HD Dependency trees of syntactic heads (Fig-
ure 8). This representation is obtained by convert-
ing PTB trees. We first determine lexical heads of
nonterminal nodes by using Bikel?s implementation
of Collins? head detection algorithm9 (Bikel, 2004;
Collins, 1997). We then convert lexicalized trees
into dependencies between lexical heads.
SD The Stanford dependency format (Figure 9).
This format was originally proposed for extracting
dependency relations useful for practical applica-
tions (de Marneffe et al, 2006). A program to con-
vert PTB is attached to the Stanford parser. Although
the concept looks similar to CoNLL, this representa-
8http://nlp.cs.lth.se/pennconverter/
9http://www.cis.upenn.edu/?dbikel/software.
html
49
Figure 9: Stanford dependencies
tion does not necessarily form a tree structure, and is
designed to express more fine-grained relations such
as apposition. Research groups for biomedical NLP
recently adopted this representation for corpus anno-
tation (Pyysalo et al, 2007a) and parser evaluation
(Clegg and Shepherd, 2007; Pyysalo et al, 2007b).
PAS Predicate-argument structures. This is the de-
fault output format for ENJU and ENJU-GENIA.
Although only CoNLL is available for depen-
dency parsers, we can create four representations for
the phrase structure parsers, and five for the deep
parsers. Dotted arrows in Figure 7 indicate imper-
fect conversion, in which the conversion inherently
introduces errors, and may decrease the accuracy.
We should therefore take caution when comparing
the results obtained by imperfect conversion. We
also measure the accuracy obtained by the ensem-
ble of two parsers/representations. This experiment
indicates the differences and overlaps of information
conveyed by a parser or a parse representation.
3.3 Domain portability and parser retraining
Since the domain of our target text is different from
WSJ, our experiments also highlight the domain
portability of parsers. We run two versions of each
parser in order to investigate the two types of domain
portability. First, we run the original parsers trained
with WSJ10 (39832 sentences). The results in this
setting indicate the domain portability of the original
parsers. Next, we run parsers re-trained with GE-
NIA11 (8127 sentences), which is a Penn Treebank-
style treebank of biomedical paper abstracts. Accu-
racy improvements in this setting indicate the pos-
sibility of domain adaptation, and the portability of
the training methods of the parsers. Since the parsers
listed in Section 2 have programs for the training
10Some of the parser packages include parsing models
trained with extended data, but we used the models trained with
WSJ section 2-21 of the Penn Treebank.
11The domains of GENIA and AImed are not exactly the
same, because they are collected independently.
with a Penn Treebank-style treebank, we use those
programs as-is. Default parameter settings are used
for this parser re-training.
In preliminary experiments, we found that de-
pendency parsers attain higher dependency accuracy
when trained only with GENIA. We therefore only
input GENIA as the training data for the retraining
of dependency parsers. For the other parsers, we in-
put the concatenation of WSJ and GENIA for the
retraining, while the reranker of RERANK was not re-
trained due to its cost. Since the parsers other than
NO-RERANK and RERANK require an external POS
tagger, a WSJ-trained POS tagger is used with WSJ-
trained parsers, and geniatagger (Tsuruoka et al,
2005) is used with GENIA-retrained parsers.
4 Experiments
4.1 Experiment settings
In the following experiments, we used AImed
(Bunescu and Mooney, 2004), which is a popular
corpus for the evaluation of PPI extraction systems.
The corpus consists of 225 biomedical paper ab-
stracts (1970 sentences), which are sentence-split,
tokenized, and annotated with proteins and PPIs.
We use gold protein annotations given in the cor-
pus. Multi-word protein names are concatenated
and treated as single words. The accuracy is mea-
sured by abstract-wise 10-fold cross validation and
the one-answer-per-occurrence criterion (Giuliano
et al, 2006). A threshold for SVMs is moved to
adjust the balance of precision and recall, and the
maximum f-scores are reported for each setting.
4.2 Comparison of accuracy improvements
Tables 1 and 2 show the accuracy obtained by using
the output of each parser in each parse representa-
tion. The row ?baseline? indicates the accuracy ob-
tained with bag-of-words features. Table 3 shows
the time for parsing the entire AImed corpus, and
Table 4 shows the time required for 10-fold cross
validation with GENIA-retrained parsers.
When using the original WSJ-trained parsers (Ta-
ble 1), all parsers achieved almost the same level
of accuracy ? a significantly better result than the
baseline. To the extent of our knowledge, this is
the first result that proves that dependency parsing,
phrase structure parsing, and deep parsing perform
50
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 53.2/56.5/54.6 N/A N/A N/A N/A
KSDEP 49.3/63.0/55.2 N/A N/A N/A N/A
NO-RERANK 50.7/60.9/55.2 45.9/60.5/52.0 50.6/60.9/55.1 49.9/58.2/53.5 N/A
RERANK 53.6/59.2/56.1 47.0/58.9/52.1 48.1/65.8/55.4 50.7/62.7/55.9 N/A
BERKELEY 45.8/67.6/54.5 50.5/57.6/53.7 52.3/58.8/55.1 48.7/62.4/54.5 N/A
STANFORD 50.4/60.6/54.9 50.9/56.1/53.0 50.7/60.7/55.1 51.8/58.1/54.5 N/A
ENJU 52.6/58.0/55.0 48.7/58.8/53.1 57.2/51.9/54.2 52.2/58.1/54.8 48.9/64.1/55.3
Table 1: Accuracy on the PPI task with WSJ-trained parsers (precision/recall/f-score)
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 49.1/65.6/55.9 N/A N/A N/A N/A
KSDEP 51.6/67.5/58.3 N/A N/A N/A N/A
NO-RERANK 53.9/60.3/56.8 51.3/54.9/52.8 53.1/60.2/56.3 54.6/58.1/56.2 N/A
RERANK 52.8/61.5/56.6 48.3/58.0/52.6 52.1/60.3/55.7 53.0/61.1/56.7 N/A
BERKELEY 52.7/60.3/56.0 48.0/59.9/53.1 54.9/54.6/54.6 50.5/63.2/55.9 N/A
STANFORD 49.3/62.8/55.1 44.5/64.7/52.5 49.0/62.0/54.5 54.6/57.5/55.8 N/A
ENJU 54.4/59.7/56.7 48.3/60.6/53.6 56.7/55.6/56.0 54.4/59.3/56.6 52.0/63.8/57.2
ENJU-GENIA 56.4/57.4/56.7 46.5/63.9/53.7 53.4/60.2/56.4 55.2/58.3/56.5 57.5/59.8/58.4
Table 2: Accuracy on the PPI task with GENIA-retrained parsers (precision/recall/f-score)
WSJ-trained GENIA-retrained
MST 613 425
KSDEP 136 111
NO-RERANK 2049 1372
RERANK 2806 2125
BERKELEY 1118 1198
STANFORD 1411 1645
ENJU 1447 727
ENJU-GENIA 821
Table 3: Parsing time (sec.)
equally well in a real application. Among these
parsers, RERANK performed slightly better than the
other parsers, although the difference in the f-score
is small, while it requires much higher parsing cost.
When the parsers are retrained with GENIA (Ta-
ble 2), the accuracy increases significantly, demon-
strating that the WSJ-trained parsers are not suffi-
ciently domain-independent, and that domain adap-
tation is effective. It is an important observation that
the improvements by domain adaptation are larger
than the differences among the parsers in the pre-
vious experiment. Nevertheless, not all parsers had
their performance improved upon retraining. Parser
CoNLL PTB HD SD PAS
baseline 424
MST 809 N/A N/A N/A N/A
KSDEP 864 N/A N/A N/A N/A
NO-RERANK 851 4772 882 795 N/A
RERANK 849 4676 881 778 N/A
BERKELEY 869 4665 895 804 N/A
STANFORD 847 4614 886 799 N/A
ENJU 832 4611 884 789 1005
ENJU-GENIA 874 4624 895 783 1020
Table 4: Evaluation time (sec.)
retraining yielded only slight improvements for
RERANK, BERKELEY, and STANFORD, while larger
improvements were observed for MST, KSDEP, NO-
RERANK, and ENJU. Such results indicate the dif-
ferences in the portability of training methods. A
large improvement from ENJU to ENJU-GENIA shows
the effectiveness of the specifically designed do-
main adaptation method, suggesting that the other
parsers might also benefit from more sophisticated
approaches for domain adaptation.
While the accuracy level of PPI extraction is
the similar for the different parsers, parsing speed
51
RERANK ENJU
CoNLL HD SD CoNLL HD SD PAS
KSDEP CoNLL 58.5 (+0.2) 57.1 (?1.2) 58.4 (+0.1) 58.5 (+0.2) 58.0 (?0.3) 59.1 (+0.8) 59.0 (+0.7)
RERANK CoNLL 56.7 (+0.1) 57.1 (+0.4) 58.3 (+1.6) 57.3 (+0.7) 58.7 (+2.1) 59.5 (+2.3)
HD 56.8 (+0.1) 57.2 (+0.5) 56.5 (+0.5) 56.8 (+0.2) 57.6 (+0.4)
SD 58.3 (+1.6) 58.3 (+1.6) 56.9 (+0.2) 58.6 (+1.4)
ENJU CoNLL 57.0 (+0.3) 57.2 (+0.5) 58.4 (+1.2)
HD 57.1 (+0.5) 58.1 (+0.9)
SD 58.3 (+1.1)
Table 5: Results of parser/representation ensemble (f-score)
differs significantly. The dependency parsers are
much faster than the other parsers, while the phrase
structure parsers are relatively slower, and the deep
parsers are in between. It is noteworthy that the
dependency parsers achieved comparable accuracy
with the other parsers, while they are more efficient.
The experimental results also demonstrate that
PTB is significantly worse than the other represen-
tations with respect to cost for training/testing and
contributions to accuracy improvements. The con-
version from PTB to dependency-based representa-
tions is therefore desirable for this task, although it
is possible that better results might be obtained with
PTB if a different feature extraction mechanism is
used. Dependency-based representations are com-
petitive, while CoNLL seems superior to HD and SD
in spite of the imperfect conversion from PTB to
CoNLL. This might be a reason for the high per-
formances of the dependency parsers that directly
compute CoNLL dependencies. The results for ENJU-
CoNLL and ENJU-PAS show that PAS contributes to a
larger accuracy improvement, although this does not
necessarily mean the superiority of PAS, because two
imperfect conversions, i.e., PAS-to-PTB and PTB-to-
CoNLL, are applied for creating CoNLL.
4.3 Parser ensemble results
Table 5 shows the accuracy obtained with ensembles
of two parsers/representations (except the PTB for-
mat). Bracketed figures denote improvements from
the accuracy with a single parser/representation.
The results show that the task accuracy significantly
improves by parser/representation ensemble. Inter-
estingly, the accuracy improvements are observed
even for ensembles of different representations from
the same parser. This indicates that a single parse
representation is insufficient for expressing the true
Bag-of-words features 48.2/54.9/51.1
Yakushiji et al (2005) 33.7/33.1/33.4
Mitsumori et al (2006) 54.2/42.6/47.7
Giuliano et al (2006) 60.9/57.2/59.0
S?tre et al (2007) 64.3/44.1/52.0
This paper 54.9/65.5/59.5
Table 6: Comparison with previous results on PPI extrac-
tion (precision/recall/f-score)
potential of a parser. Effectiveness of the parser en-
semble is also attested by the fact that it resulted in
larger improvements. Further investigation of the
sources of these improvements will illustrate the ad-
vantages and disadvantages of these parsers and rep-
resentations, leading us to better parsing models and
a better design for parse representations.
4.4 Comparison with previous results on PPI
extraction
PPI extraction experiments on AImed have been re-
ported repeatedly, although the figures cannot be
compared directly because of the differences in data
preprocessing and the number of target protein pairs
(S?tre et al, 2007). Table 6 compares our best re-
sult with previously reported accuracy figures. Giu-
liano et al (2006) and Mitsumori et al (2006) do
not rely on syntactic parsing, while the former ap-
plied SVMs with kernels on surface strings and the
latter is similar to our baseline method. Bunescu and
Mooney (2005) applied SVMs with subsequence
kernels to the same task, although they provided
only a precision-recall graph, and its f-score is
around 50. Since we did not run experiments on
protein-pair-wise cross validation, our system can-
not be compared directly to the results reported
by Erkan et al (2007) and Katrenko and Adriaans
52
(2006), while S?tre et al (2007) presented better re-
sults than theirs in the same evaluation criterion.
5 Related Work
Though the evaluation of syntactic parsers has been
a major concern in the parsing community, and a
couple of works have recently presented the com-
parison of parsers based on different frameworks,
their methods were based on the comparison of the
parsing accuracy in terms of a certain intermediate
parse representation (Ringger et al, 2004; Kaplan
et al, 2004; Briscoe and Carroll, 2006; Clark and
Curran, 2007; Miyao et al, 2007; Clegg and Shep-
herd, 2007; Pyysalo et al, 2007b; Pyysalo et al,
2007a; Sagae et al, 2008). Such evaluation requires
gold standard data in an intermediate representation.
However, it has been argued that the conversion of
parsing results into an intermediate representation is
difficult and far from perfect.
The relationship between parsing accuracy and
task accuracy has been obscure for many years.
Quirk and Corston-Oliver (2006) investigated the
impact of parsing accuracy on statistical MT. How-
ever, this work was only concerned with a single de-
pendency parser, and did not focus on parsers based
on different frameworks.
6 Conclusion and Future Work
We have presented our attempts to evaluate syntac-
tic parsers and their representations that are based on
different frameworks; dependency parsing, phrase
structure parsing, or deep parsing. The basic idea
is to measure the accuracy improvements of the
PPI extraction task by incorporating the parser out-
put as statistical features of a machine learning
classifier. Experiments showed that state-of-the-
art parsers attain accuracy levels that are on par
with each other, while parsing speed differs sig-
nificantly. We also found that accuracy improve-
ments vary when parsers are retrained with domain-
specific data, indicating the importance of domain
adaptation and the differences in the portability of
parser training methods.
Although we restricted ourselves to parsers
trainable with Penn Treebank-style treebanks, our
methodology can be applied to any English parsers.
Candidates include RASP (Briscoe and Carroll,
2006), the C&C parser (Clark and Curran, 2004),
the XLE parser (Kaplan et al, 2004), MINIPAR
(Lin, 1998), and Link Parser (Sleator and Temperley,
1993; Pyysalo et al, 2006), but the domain adapta-
tion of these parsers is not straightforward. It is also
possible to evaluate unsupervised parsers, which is
attractive since evaluation of such parsers with gold-
standard data is extremely problematic.
A major drawback of our methodology is that
the evaluation is indirect and the results depend
on a selected task and its settings. This indicates
that different results might be obtained with other
tasks. Hence, we cannot conclude the superiority of
parsers/representations only with our results. In or-
der to obtain general ideas on parser performance,
experiments on other tasks are indispensable.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Grant-in-Aid for Young Scientists (MEXT, Japan).
References
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
T. Briscoe and J. Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In COLING/ACL 2006 Poster Session.
R. Bunescu and R. J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL 2004, pages 439?446.
R. C. Bunescu and R. J. Mooney. 2005. Subsequence
kernels for relation extraction. In NIPS 2005.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
ACL 2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL-2000, pages 132?139.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In 42nd ACL.
S. Clark and J. R. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In ACL
2007.
A. B. Clegg and A. J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC Bioinformatics,
8:24.
53
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In 35th ACL.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COLING
1996.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interac-
tion sentences using dependency parsing. In EMNLP
2007.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP 2001, pages 167?202.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In EACL 2006.
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating im-
pact of re-training a lexical disambiguation model on
domain adaptation of an HPSG parser. In IWPT 2007.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA 2007.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell, and
A. Vasserman. 2004. Speed and accuracy in shallow
and deep stochastic parsing. In HLT/NAACL?04.
S. Katrenko and P. Adriaans. 2006. Learning relations
from biomedical corpora using dependency trees. In
KDECB, pages 61?80.
J.-D. Kim, T. Ohta, Y. Teteisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19:i180?182.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL 2003.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In LREC Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL 2005.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In EACL
2006.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with SVM. IEICE - Trans.
Inf. Syst., E89-D(8):2464?2466.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Y. Miyao, K. Sagae, and J. Tsujii. 2007. Towards
framework-independent evaluation of deep linguistic
parsers. In Grammar Engineering across Frameworks
2007, pages 238?258.
A. Moschitti. 2006. Making tree kernels practical for
natural language processing. In EACL 2006.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In ACL 2005.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL 2007.
S. Pyysalo, T. Salakoski, S. Aubin, and A. Nazarenko.
2006. Lexical adaptation of link grammar to the
biomedical sublanguage: a comparative evaluation of
three approaches. BMC Bioinformatics, 7(Suppl. 3).
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007a. BioInfer: a cor-
pus for information extraction in the biomedical do-
main. BMC Bioinformatics, 8(50).
S. Pyysalo, F. Ginter, V. Laippala, K. Haverinen, J. Hei-
monen, and T. Salakoski. 2007b. On the unification of
syntactic annotations under the Stanford dependency
scheme: A case study on BioInfer and GENIA. In
BioNLP 2007, pages 25?32.
C. Quirk and S. Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical ma-
chine translation. In EMNLP 2006.
E. K. Ringger, R. C. Moore, E. Charniak, L. Vander-
wende, and H. Suzuki. 2004. Using the Penn Tree-
bank to evaluate non-treebank parsers. In LREC 2004.
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic
features for protein-protein interaction extraction. In
LBM 2007 short papers.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In EMNLP-CoNLL 2007.
K. Sagae, Y. Miyao, T. Matsuzaki, and J. Tsujii. 2008.
Challenges in mapping of syntactic representations
for framework-independent parser evaluation. In the
Workshop on Automated Syntatic Annotations for In-
teroperable Language Resources.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a Link Grammar. In 3rd IWPT.
Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. Develop-
ing a robust part-of-speech tagger for biomedical text.
In 10th Panhellenic Conference on Informatics.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In First International
Symposium on Semantic Mining in Biomedicine.
54
An Efficient Clustering Algorithm for Class-based Language Models
Takuya Matsuzaki Yusuke Miyao
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
matuzaki,yusuke,tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Abstract
This paper defines a general form for class-
based probabilistic language models and pro-
poses an efficient algorithm for clustering
based on this. Our evaluation experiments re-
vealed that our method decreased computation
time drastically, while retaining accuracy.
1 Introduction
Clustering algorithms have been extensively studied in
the research area of natural language processing because
many researchers have proved that ?classes? obtained by
clustering can improve the performance of various NLP
tasks. Examples have been class-based -gram models
(Brown et al, 1992; Kneser and Ney, 1993), smooth-
ing techniques for structural disambiguation (Li and Abe,
1998) and word sense disambiguation (Shu?tze, 1998).
In this paper, we define a general form for class-based
probabilistic language models, and propose an efficient
and model-theoretic algorithm for clustering based on
this. The algorithm involves three operations, CLAS-
SIFY, MERGE, and SPLIT, all of which decreases the
optimization function based on the MDL principle (Ris-
sanen, 1984), and can efficiently find a point near the lo-
cal optimum. The algorithm is applicable to more general
tasks than existing studies (Li and Abe, 1998; Berkhin
and Becher, 2002), and computational costs are signifi-
cantly small, which allows its application to very large
corpora.
Clustering algorithms may be classified into three
types. The first is a type that uses various heuristic mea-
sure of similarity between the elements to be clustered
and has no interpretation as a probabilitymodel (Widdow,
2002). The resulting clusters from this type of method
are not guaranteed to work effectively as a component
of a statistical language model, because the similarity
used in clustering is not derived from the criterion in the
learning process of the statistical model, e.g. likelihood.
The second type has clear interpretation as a probability
model, but no criteria to determine the number of clusters
(Brown et al, 1992; Kneser and Ney, 1993). The perfor-
mance of methods of this type depend on the number of
clusters that must be specified before the clustering pro-
cess. It may prove rather troublesome to determine the
proper number of clusters in this type of method. The
third has interpretation as a probability model and uses
some statistically motivated model selection criteria to
determine the proper number of clusters. This type has
a clear advantage compared to the second. AutoClass
(Cheeseman and Stutz, 1996), the Bayesian model merg-
ing method (Stolcke and Omohundro, 1996) and Li?s
method (Li, 2002) are examples of this type. AutoClass
and the Bayesian model merging are based on soft clus-
tering models and Li?s method is based on a hard clus-
tering model. In general, computational costs for hard
clustering models are lower than that for soft clustering
models. However, the time complexity of Li?s method is
of cubic order in the size of the vocabulary. Therefore, it
is not practical to apply it to large corpora.
Our model and clustering algorithm provide a solution
to these problems with existing clustering algorithms.
Since the model has clear interpretation as a probability
model, the clustering algorithm uses MDL as clustering
criteria and using a combination of top-down clustering,
bottom-up clustering, and a K-means style exchange al-
gorithm, the method we propose can perform the cluster-
ing efficiently.
We evaluated the algorithm through experiments on
a disambiguation task of Japanese dependency analysis.
In the experiments, we observed that the proposed algo-
rithm?s computation time is roughly linear to the size of
the vocabulary, and it performed slightly better than the
existing method. Our main intention in the experiments
was to see improvements in terms of computational cost,
not in performance in the test task. We will show, in Sec-
tions 2 and 3, that the proposed method can be applied
to a broader range of tasks than the test task we evalu-
ate in the experiments in Section 4. We need further ex-
periments to determine the performance of the proposed
method with more general tasks.
2 Probability model
2.1 Class-based language modeling
Our probability model is a class-based model and it is an
extension of the model proposed by Li and Abe (1998).
We extend their two-dimensional class model to a multi-
dimensional class model, i.e., we incorporate an arbitrary
number of random variables in our model.
Although our probabilitymodel and learning algorithm
are general and not restricted to particular domains, we
mainly intend to use them in natural language process-
ing tasks where large amounts of lexical knowledge are
required. When we incorporate lexical information into
a model, we inevitably face the data-sparseness problem.
The idea of ?word class? (Brown et al, 1992) gives a gen-
eral solution to this problem. A word class is a group
of words which performs similarly in some linguistic
phenomena. Part-of-speech are well-known examples of
such classes. Incorporating word classes into linguistic
models yields good smoothing or, hopefully, meaningful
generalization from given samples.
2.2 Model definition
Let us introduce some notations to define our model. In
our model, we have considered  kinds of discrete ran-
dom variables 

 

     

and their joint distribu-
tion. 

denotes a set of possible values for the -th vari-
able 

. Our probability model assumes disjunctive par-
titions of each 

, which are denoted by 

?s. A disjunc-
tive partition   

 

     

 of  is a subset of


, and satisfies 

 

 	 
   and   



.
We call elements in a partition 

classes of elements in


. 


, or 

for short, denotes a class in 

which
contains an element   

.
With these notations, our probability model is ex-
pressed as:
 

 

 

 

     

 


  


 


     





 

 


 (1)
In this paper, we have considered a hard clusteringmodel,
i.e.,     for any   . Li & Abe?s model
(1998) is an instance of this joint probability model,
where   . Using more than 2 variables the model can
represent the probability for the co-occurrence of triplets,
such as subject, verb, object.
2.3 Clustering criterion
To determine the proper number of classes in each par-
tition 

     

, we need criteria other than the maxi-
mum likelihood criterion, because likelihood always be-
come greater when we use smaller classes. We can see
this class number decision problem as a model selection
problem and apply some statistically motivated model
selection criteria. As mentioned previously (following
Li and Abe (1998)) we used the MDL principle as our
clustering criterion.
Assume that we have  samples of co-occurrence
data:
  

 

 

     

  
        
The objective function in both clustering and parame-
ter estimations in our method is the description length,
, which is defined as follows:
   	

 
  (2)
where  denotes the model and 

 is the likelihood
of samples  under model  :


 
	


 

 

     

 (3)
The first term in Eq.2,  	

, is called the data
description length. The second term, , is called the
model description length, and when sample size  is
large, it can be approximated as
 


	
where  is the number of free parameters in model  .
We used this approximated form throughout this paper.
Given the number of classes, 

 

 for each  
     , we have





  free parameters for joint
probabilities    . Also, for each class , we
have   free parameters for conditional probabilities
 , where   . Thus, we have
 







   





 






 

 





 
Our learning algorithm tries to minimize  by
adjusting the parameters in the model, selecting partition


of each 

, and choosing the numbers of classes, 

in each partition 

.
3 Clustering algorithm
Our clustering algorithm is a combination of three ba-
sic operations: CLASSIFY, SPLIT and MERGE. We it-
eratively invoke these until a terminate condition is met.
Briefly, these three work as follows. The CLASSIFY
takes a partition  in  as input and improves the par-
tition by moving the elements in  from one class to an-
other. This operation is similar to one iteration in the K-
means algorithm. The MERGE takes a partition  as in-
put and successively chooses two classes 

and 

from
 and replaces themwith their union,



. The SPLIT
takes a class, , and tries to find the best division of 
into two new classes, which will decrease the description
length the most.
All of these three basic operations decrease the de-
scription length. Consequently, our overall algorithm
also decreases the description length monotonically and
stops when all three operations cause no decrease in de-
scription length. Strictly, this termination does not guar-
antee the resulting partitions to be even locally opti-
mal, because SPLIT operations do not perform exhaus-
tive searches in all possible divisions of a class. Doing
such an exhaustive search is almost impossible for a class
of modest size, because the time complexity of such an
exhaustive search is of exponential order to the size of the
class. However, by properly selecting the number of tri-
als in SPLIT, we can expect the results to approach some
local optimum.
It is clear that the way the three operations are com-
bined affects the performance of the resulting class-based
model and the computation time required in learning. In
this paper, we basically take a top-down, divisive strat-
egy, but at each stage of division we do CLASSIFY op-
erations on the set of classes at each stage. When we
cannot divide any classes and CLASSIFY cannot move
any elements, we invoke MERGE to merge classes that
are too finely divided. This top-down strategy can drasti-
cally decrease the amount of computation time compared
to the bottom-up approaches used by Brown et al (1992)
and Li and Abe (1998).
The following is the precise algorithm for our main
procedure:
Algorithm 1 MAIN PROCEDURE()
INPUT
 : an integer specifying the number of trials in a
SPLIT operation
OUTPUT
Partitions 

  

and estimated parameters in the
model
PROCEDURE
Step 0 

  

 	 INITIALIZE

 

 
Step 1 Do Step 2 through Step 3 until no change is made
through one iteration
Step 2 For     , do Step 2.1 through Step 2.2
Step 2.1 Do Step 2.1.1 until no change occurs through it
Step 2.1.1 For      , 

	 CLASSIFY


Step 2.2 For each   

,  	 SPLIT 
Step 3 For     , 

	 MERGE


Step 4 Return the resulting partitions with the parame-
ters in the model
In the Step 0 of the algorithm, INITIALIZE creates
the initial partitions of 

     

. It first divides each


     

into two classes and then applies CLASSIFY
to each partition 

     

one by one, while any ele-
ments can move.
The following subsections explain the algorithm for
the three basic operations in detail and show that they
decrease  monotonically.
3.1 Iterative classification
In this subsection, we explain a way of finding a local
optimum in the possible classification of elements in 

,
given the numbers of classes in partitions 

.
Given the number of classes, optimization in terms of
the description length (Eq.2) is just the same as optimiz-
ing the likelihood (Eq.3). We used a greedy algorithm
which monotonically increases the likelihood while
updating classification. Our method is a generalized
version of the previously reported K-means/EM-
algorithm-style, iterative-classification methods in
Kneser and Ney (1993), Berkhin and Becher (2002) and
Dhillon et al (2002). We demonstrate that the method is
applicable to more generic situations than those previ-
ously reported, where the number of random variables is
arbitrary.
To explain the algorithmmore fully, we define ?counter
functions? Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155?163,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extremely Lexicalized Models for Accurate and Fast HPSG Parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
Yoshimasa Tsuruoka
School of Informatics
University of Manchester
Yusuke Miyao
Department of Computer Science
University of Tokyo
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
SORST, Japan Science and Technology Agency
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
{ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes an extremely lexi-
calized probabilistic model for fast and
accurate HPSG parsing. In this model,
the probabilities of parse trees are de-
fined with only the probabilities of select-
ing lexical entries. The proposed model
is very simple, and experiments revealed
that the implemented parser runs around
four times faster than the previous model
and that the proposed model has a high
accuracy comparable to that of the previ-
ous model for probabilistic HPSG, which
is defined over phrase structures. We
also developed a hybrid of our probabilis-
tic model and the conventional phrase-
structure-based model. The hybrid model
is not only significantly faster but also sig-
nificantly more accurate by two points of
precision and recall compared to the pre-
vious model.
1 Introduction
For the last decade, accurate and wide-coverage
parsing for real-world text has been intensively
and extensively pursued. In most of state-of-the-
art parsers, probabilistic events are defined over
phrase structures because phrase structures are
supposed to dominate syntactic configurations of
sentences. For example, probabilities were de-
fined over grammar rules in probabilistic CFG
(Collins, 1999; Klein and Manning, 2003; Char-
niak and Johnson, 2005) or over complex phrase
structures of head-driven phrase structure gram-
mar (HPSG) or combinatory categorial grammar
(CCG) (Clark and Curran, 2004b; Malouf and van
Noord, 2004; Miyao and Tsujii, 2005). Although
these studies vary in the design of the probabilistic
models, the fundamental conception of probabilis-
tic modeling is intended to capture characteristics
of phrase structures or grammar rules. Although
lexical information, such as head words, is known
to significantly improve the parsing accuracy, it
was also used to augment information on phrase
structures.
Another interesting approach to this problem
was using supertagging (Clark and Curran, 2004b;
Clark and Curran, 2004a; Wang and Harper, 2004;
Nasr and Rambow, 2004), which was originally
developed for lexicalized tree adjoining grammars
(LTAG) (Bangalore and Joshi, 1999). Supertag-
ging is a process where words in an input sen-
tence are tagged with ?supertags,? which are lex-
ical entries in lexicalized grammars, e.g., elemen-
tary trees in LTAG, lexical categories in CCG,
and lexical entries in HPSG. Supertagging was,
in the first place, a technique to reduce the cost
of parsing with lexicalized grammars; ambiguity
in assigning lexical entries to words is reduced
by the light-weight process of supertagging be-
fore the heavy process of parsing. Bangalore and
Joshi (1999) claimed that if words can be assigned
correct supertags, syntactic parsing is almost triv-
ial. What this means is that if supertags are cor-
rectly assigned, syntactic structures are almost de-
155
termined because supertags include rich syntac-
tic information such as subcategorization frames.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated
in the case of a CCG parser (Clark and Curran,
2004a) with the result of a drastic improvement in
the parsing speed. Wang and Harper (2004) also
demonstrated the effects of supertagging with a
statistical constraint dependency grammar (CDG)
parser. They achieved accuracy as high as the
state-of-the-art parsers. However, a supertagger it-
self was used as an external tagger that enumerates
candidates of lexical entries or filters out unlikely
lexical entries just to help parsing, and the best
parse trees were selected mainly according to the
probabilistic model for phrase structures or depen-
dencies with/without the probabilistic model for
supertagging.
We investigate an extreme case of HPSG pars-
ing in which the probabilistic model is defined
with only the probabilities of lexical entry selec-
tion; i.e., the model is never sensitive to charac-
teristics of phrase structures. The model is simply
defined as the product of the supertagging proba-
bilities, which are provided by the discriminative
method with machine learning features of word
trigrams and part-of-speech (POS) 5-grams as de-
fined in the CCG supertagging (Clark and Curran,
2004a). The model is implemented in an HPSG
parser instead of the phrase-structure-based prob-
abilistic model; i.e., the parser returns the parse
tree assigned the highest probability of supertag-
ging among the parse trees licensed by an HPSG.
Though the model uses only the probabilities of
lexical entry selection, the experiments revealed
that it was as accurate as the previous phrase-
structure-based model. Interestingly, this means
that accurate parsing is possible using rather sim-
ple mechanisms.
We also tested a hybrid model of the su-
pertagging and the previous phrase-structure-
based probabilistic model. In the hybrid model,
the probabilities of the previous model are mul-
tiplied by the supertagging probabilities instead
of a preliminary probabilistic model, which is in-
troduced to help the process of estimation by fil-
tering unlikely lexical entries (Miyao and Tsujii,
2005). In the previous model, the preliminary
probabilistic model is defined as the probability
of unigram supertagging. So, the hybrid model
can be regarded as an extension of supertagging
from unigram to n-gram. The hybrid model can
also be regarded as a variant of the statistical CDG
parser (Wang, 2003; Wang and Harper, 2004), in
which the parse tree probabilities are defined as
the product of the supertagging probabilities and
the dependency probabilities. In the experiments,
we observed that the hybrid model significantly
improved the parsing speed, by around three to
four times speed-ups, and accuracy, by around two
points in both precision and recall, over the pre-
vious model. This implies that finer probabilistic
model of lexical entry selection can improve the
phrase-structure-based model.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemata describe gen-
eral construction rules, and a large number of
lexical entries express word-specific characteris-
tics. The structures of sentences are explained us-
ing combinations of schemata and lexical entries.
Both schemata and lexical entries are represented
by typed feature structures, and constraints repre-
sented by feature structures are checked with uni-
fication.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
parse result is output as a phrasal sign that domi-
nates the sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries de-
termine the dominant syntactic structures.
156
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model
or maximum entropy model (Berger et al, 1996).
The probability that a parse result T is assigned to
a given sentence w = ?w1, . . . , wn? is
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature
function that represents a characteristic of parse
tree T , and Zw is the sum over the set of all pos-
sible parse trees for the sentence. Intuitively, the
probability is defined as the normalized product
of the weights exp(?u) when a characteristic cor-
responding to fu appears in parse result T . The
model parameters, ?u, are estimated using numer-
ical optimization methods (Malouf, 2002) to max-
imize the log-likelihood of the training data.
However, the above model cannot be easily es-
timated because the estimation requires the com-
putation of p(T |w) for all parse candidates as-
signed to sentence w. Because the number of
parse candidates is exponentially related to the
length of the sentence, the estimation is intractable
for long sentences. To make the model estimation
tractable, Geman and Johnson (Geman and John-
son, 2002) and Miyao and Tsujii (Miyao and Tsu-
jii, 2002) proposed a dynamic programming algo-
rithm for estimating p(T |w). Miyao and Tsujii
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(2005) also introduced a preliminary probabilistic
model p0(T |w) whose estimation does not require
the parsing of a treebank. This model is intro-
duced as a reference distribution of the probabilis-
tic HPSG model; i.e., the computation of parse
trees given low probabilities by the model is omit-
ted in the estimation stage. We have
(Previous probabilistic HPSG)
phpsg?(T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in T
and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model
with the probabilistic HPSG model of Miyao and
Tsujii (2005). The features used in their model are
combinations of the feature templates listed in Ta-
ble 1. The feature templates fbinary and funary
are defined for constituents at binary and unary
branches, froot is a feature template set for the
root nodes of parse trees, and flex is a feature tem-
plate set for calculating the preliminary probabilis-
tic model. An example of features applied to the
parse tree for the sentence ?Spring has come? is
shown in Figure 2.
157
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Features.
3 Extremely lexicalized probabilistic
models
In the experiments, we tested parsing with the pre-
vious model for the probabilistic HPSG explained
in Section 2 and other three types of probabilis-
tic models defined with the probabilities of lexi-
cal entry selection. The first one is the simplest
probabilistic model, which is defined with only
the probabilities of lexical entry selection. It is
defined simply as the product of the probabilities
of selecting all lexical entries in the sentence; i.e.,
the model does not use the probabilities of phrase
structures like the previous models.
Given a set of lexical entries, L, a sentence,
w = ?w1, . . . , wn?, and the probabilistic model
of lexical entry selection, p(li ? L|w, i), the first
model is formally defined as follows:
(Model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi
in T and p(li|w, i) is the probability of selecting
lexical entry li for wi.
The second model is defined as the product of
the probabilities of selecting all lexical entries in
the sentence and the root node probability of the
parse tree. That is, the second model is also de-
fined without the probabilities on phrase struc-
tures:
(Model 2)
pmodel2(T |w) =
1
Zmodel2 pmodel1(T |w) exp
?
??
?
u
(fu?froot)
?ufu(T )
?
??
Zmodel2 =
?
T ?
pmodel1(T ?|w) exp
?
??
?
u
(fu?froot)
?ufu(T ?)
?
?? ,
where Zmodel2 is the sum over the set of all pos-
sible parse trees for the sentence.
The third model is a hybrid of model 1 and the
previous model. The probabilities of the lexical
entries in the previous model are replaced with the
probabilities of lexical entry selection:
(Model 3)
pmodel3(T |w) =
1
Zmodel3 pmodel1(T |w) exp
(?
u
?ufu(T )
)
Zmodel3 =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In this study, the same model parameters used
in the previous model were used for phrase struc-
tures.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic Model of Lexical Entry Selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
158
fexlex =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
combinations of feature templates
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Features for the probabilities of lexical
entry selection.
procedure Parsing(?w1, . . . , wn?, ?L,R?, ?, ?, ?, ?, ?)
for i = 1 to n
foreach F ? ? {F |?wi, F ? ? L}
p =
?
u ?ufu(F
?)
pi[i? 1, i] ? pi[i? 1, i] ? {F ?}
if (p > ?[i? 1, i, F ?]) then
?[i? 1, i, F ?] ? p
LocalThresholding(i? 1, i,?, ?)
for d = 1 to n
for i = 0 to n? d
j = i+ d
for k = i+ 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if F = r(Fs, Ft) has succeeded
p = ?[i, k, Fs] + ?[k, j, Ft] +
?
u ?ufu(F )
pi[i, j] ? pi[i, j] ? {F}
if (p > ?[i, j, F ]) then
?[i, j, F ] ? p
LocalThresholding(i, j,?, ?)
GlobalThresholding(i, n, ?)
procedure IterativeParsing(w, G, ?0, ?0, ?0, ?0, ?0, ??, ??, ??,
??, ??, ?last, ?last, ?last, ?last, ?last)? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0;
loop while ? ? ?last and ? ? ?last and ? ? ?last and ? ? ?last
and ? ? ?last
call Parsing(w, G, ?, ?, ?, ?, ?)
if pi[1, n] 6= ? then exit
? ? ?+??; ? ? ? +??;
? ? ?+??; ? ? ? +??; ? ? ? +??;
Figure 3: Pseudo-code of iterative parsing for
HPSG.
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical en-
tries for the word wi. The feature templates used
in our model are listed in Table 2 and are word
trigrams and POS 5-grams.
4 Experiments
4.1 Implementation
We implemented the iterative parsing algorithm
(Ninomiya et al, 2005) for the probabilistic HPSG
models. It first starts parsing with a narrow beam.
If the parsing fails, then the beam is widened, and
parsing continues until the parser outputs results
or the beam width reaches some limit. Though
the probabilities of lexical entry selection are in-
troduced, the algorithm for the presented proba-
bilistic models is almost the same as the original
iterative parsing algorithm.
The pseudo-code of the algorithm is shown in
Figure 3. In the figure, the pi[i, j] represents
the set of partial parse results that cover words
wi+1, . . . , wj , and ?[i, j, F ] stores the maximum
figure-of-merit (FOM) of partial parse result F
at cell (i, j). The probability of lexical entry
F is computed as ?u ?ufu(F ) for the previous
model, as shown in the figure. The probability
of a lexical entry for models 1, 2, and 3 is com-
puted as the probability of lexical entry selection,
p(F |w, i). The FOM of a newly created partial
parse, F , is computed by summing the values of
? of the daughters and an additional FOM of F if
the model is the previous model or model 3. The
FOM for models 1 and 2 is computed by only sum-
ming the values of ? of the daughters; i.e., weights
exp(?u) in the figure are assigned zero. The terms
? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs
in the chart cell. The terms ? and ? are the thresh-
olds of the number and the beam width of lexical
entries, and ? is the beam width for global thresh-
olding (Goodman, 1997).
4.2 Evaluation
We evaluated the speed and accuracy of parsing
with extremely lexicalized models by using Enju
2.1, the HPSG grammar for English (Miyao et al,
2005; Miyao and Tsujii, 2005). The lexicon of
the grammar was extracted from Sections 02-21 of
the Penn Treebank (Marcus et al, 1994) (39,832
sentences). The grammar consisted of 3,797 lex-
ical entries for 10,536 words1. The probabilis-
tic models were trained using the same portion of
the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and other tech-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by ap-
plying lexical rules to observed lexical entries in the HPSG
treebank (Nakanishi et al, 2004). The lexicon, however, in-
cluded many lexical entries that do not appear in the HPSG
treebank. The HPSG treebank is used for training the prob-
abilistic model for lexical entry selection, and hence, those
lexical entries that do not appear in the treebank are rarely
selected by the probabilistic model. The ?effective? tag set
size, therefore, is around 1,361, the number of lexical entries
without those never-seen lexical entries.
159
No. of tested sentences Total No. of Avg. length of tested sentences
? 40 words ? 100 words sentences ? 40 words ? 100 words
Section 23 2,162 (94.04%) 2,299 (100.00%) 2,299 20.7 22.2
Section 24 1,157 (92.78%) 1,245 (99.84%) 1,247 21.2 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (? 40 + Gold POSs) Section 23 (? 100 + Gold POSs)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 87.65 86.97 91.13 90.42 468 87.26 86.50 90.73 89.93 604
model 1 87.54 86.85 90.38 89.66 111 87.23 86.47 90.05 89.27 129
model 2 87.71 87.02 90.51 89.80 109 87.38 86.62 90.17 89.39 130
model 3 89.79 88.97 92.66 91.81 132 89.48 88.58 92.33 91.40 152
Section 23 (? 40 + POS tagger) Section 23 (? 100 + POS tagger)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 85.33 84.83 89.93 89.41 509 84.96 84.25 89.55 88.80 674
model 1 85.26 84.31 89.17 88.18 133 85.00 84.01 88.85 87.82 154
model 2 85.37 84.42 89.25 88.26 134 85.08 84.09 88.91 87.88 155
model 3 87.66 86.53 91.61 90.43 155 87.35 86.29 91.24 90.13 183
Table 4: Experimental results for Section 23.
niques for deep parsing2. The parameters for beam
searching were determined manually by trial and
error using Section 22: ?0 = 4,?? = 4, ?last =
20, ?0 = 1.0,?? = 2.5, ?last = 11.0, ?0 =
12,?? = 4, ?last = 28, ?0 = 6.0,?? =
2.25, ?last = 15.0, ?0 = 8.0,?? = 3.0, and
?last = 20.0. With these thresholding parame-
ters, the parser iterated at most five times for each
sentence.
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tu-
ple ??,wh, a, wa?, where ? is the predicate type
(e.g., adjective, intransitive verb), wh is the head
word of the predicate, a is the argument label
(MODARG, ARG1, ..., ARG4), and wa is the
head word of the argument. Labeled precision
(LP)/labeled recall (LR) is the ratio of tuples cor-
rectly identified by the parser3. Unlabeled pre-
cision (UP)/unlabeled recall (UR) is the ratio of
tuples without the predicate type and the argu-
ment label. This evaluation scheme was the
same as used in previous evaluations of lexicalized
grammars (Hockenmaier, 2003; Clark and Cur-
2Deep parsing techniques include quick check (Malouf
et al, 2000) and large constituent inhibition (Kaplan et al,
2004) as described by Ninomiya et al (2005), but hybrid
parsing with a CFG chunk parser was not used. This is be-
cause we did not observe a significant improvement for the
development set by the hybrid parsing and observed only a
small improvement in the parsing speed by around 10 ms.
3When parsing fails, precision and recall are evaluated,
although nothing is output by the parser; i.e., recall decreases
greatly.
ran, 2004b; Miyao and Tsujii, 2005). The ex-
periments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the
Treebank was used as the development set, and
the performance was evaluated using sentences of
? 40 and 100 words in Section 23. The perfor-
mance of each parsing technique was analyzed us-
ing the sentences in Section 24 of ? 100 words.
Table 3 details the numbers and average lengths of
the tested sentences of ? 40 and 100 words in Sec-
tions 23 and 24, and the total numbers of sentences
in Sections 23 and 24.
The parsing performance for Section 23 is
shown in Table 4. The upper half of the table
shows the performance using the correct POSs in
the Penn Treebank, and the lower half shows the
performance using the POSs given by a POS tag-
ger (Tsuruoka and Tsujii, 2005). The left and
right sides of the table show the performances for
the sentences of ? 40 and ? 100 words. Our
models significantly increased not only the pars-
ing speed but also the parsing accuracy. Model
3 was around three to four times faster and had
around two points higher precision and recall than
the previous model. Surprisingly, model 1, which
used only lexical information, was very fast and
as accurate as the previous model. Model 2 also
improved the accuracy slightly without informa-
tion of phrase structures. When the automatic POS
tagger was introduced, both precision and recall
dropped by around 2 points, but the tendency to-
wards improved speed and accuracy was again ob-
160
76.00%
78.00%
80.00%
82.00%
84.00%
86.00%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F
-
s
c
o
r
e
previous model
model 1
model 2
model 3
Figure 4: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
served.
The unlabeled precisions and recalls of the pre-
vious model and models 1, 2, and 3 were signifi-
cantly different as measured using stratified shuf-
fling tests (Cohen, 1995) with p-values < 0.05.
The labeled precisions and recalls were signifi-
cantly different among models 1, 2, and 3 and
between the previous model and model 3, but
were not significantly different between the previ-
ous model and model 1 and between the previous
model and model 2.
The average parsing time and labeled F-score
curves of each probabilistic model for the sen-
tences in Section 24 of? 100 words are graphed in
Figure 4. The superiority of our models is clearly
observed in the figure. Model 3 performed sig-
nificantly better than the previous model. Models
1 and 2 were significantly faster with almost the
same accuracy as the previous model.
5 Discussion
5.1 Supertagging
Our probabilistic model of lexical entry selection
can be used as an independent classifier for select-
ing lexical entries, which is called the supertag-
ger (Bangalore and Joshi, 1999; Clark and Curran,
2004b). The CCG supertagger uses a maximum
entropy classifier and is similar to our model.
We evaluated the performance of our probabilis-
tic model as a supertagger. The accuracy of the re-
sulting supertagger on our development set (Sec-
tion 22) is given in Table 5 and Table 6. The test
sentences were automatically POS-tagged. Re-
sults of other supertaggers for automatically ex-
test data accuracy (%)
HPSG supertagger 22 87.51
(this paper)
CCG supertagger 00/23 91.70 / 91.45
(Curran and Clark, 2003)
LTAG supertagger 22/23 86.01 / 86.27
(Shen and Joshi, 2003)
Table 5: Accuracy of single-tag supertaggers. The
numbers under ?test data? are the PTB section
numbers of the test data.
? tags/word word acc. (%) sentence acc. (%)
1e-1 1.30 92.64 34.98
1e-2 2.11 95.08 46.11
1e-3 4.66 96.22 51.95
1e-4 10.72 96.83 55.66
1e-5 19.93 96.95 56.20
Table 6: Accuracy of multi-supertagging.
tracted lexicalized grammars are listed in Table 5.
Table 6 gives the average number of supertags as-
signed to a word, the per-word accuracy, and the
sentence accuracy for several values of ?, which is
a parameter to determine how many lexical entries
are assigned.
When compared with other supertag sets of au-
tomatically extracted lexicalized grammars, the
(effective) size of our supertag set, 1,361 lexical
entries, is between the CCG supertag set (398 cat-
egories) used by Curran and Clark (2003) and the
LTAG supertag set (2920 elementary trees) used
by Shen and Joshi (2003). The relative order based
on the sizes of the tag sets exactly matches the or-
der based on the accuracies of corresponding su-
pertaggers.
161
5.2 Efficacy of extremely lexicalized models
The implemented parsers of models 1 and 2 were
around four times faster than the previous model
without a loss of accuracy. However, what sur-
prised us is not the speed of the models, but
the fact that they were as accurate as the previ-
ous model, though they do not use any phrase-
structure-based probabilities. We think that the
correct parse is more likely to be selected if the
correct lexical entries are assigned high probabil-
ities because lexical entries include specific infor-
mation about subcategorization frames and syn-
tactic alternation, such as wh-movement and pas-
sivization, that likely determines the dominant
structures of parse trees. Another possible rea-
son for the accuracy is the constraints placed by
unification-based grammars. That is, incorrect
parse trees were suppressed by the constraints.
The best performer in terms of speed and ac-
curacy was model 3. The increased speed was,
of course, possible for the same reasons as the
speeds of models 1 and 2. An unexpected but
very impressive result was the significant improve-
ment of accuracy by two points in precision and
recall, which is hard to attain by tweaking param-
eters or hacking features. This may be because
the phrase structure information and lexical in-
formation complementarily improved the model.
The lexical information includes more specific in-
formation about the syntactic alternation, and the
phrase structure information includes information
about the syntactic structures, such as the dis-
tances of head words or the sizes of phrases.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. We ex-
emplified the dominance of lexical information in
real syntactic parsing, i.e., syntactic parsing with-
out gold-supertags, by showing that the proba-
bilities of lexical entry selection dominantly con-
tributed to syntactic parsing.
The CCG supertagging demonstrated fast and
accurate parsing for the probabilistic CCG (Clark
and Curran, 2004a). They used the supertag-
ger for eliminating candidates of lexical entries,
and the probabilities of parse trees were calcu-
lated using the phrase-structure-based model with-
out the probabilities of lexical entry selection. Our
study is essentially different from theirs in that the
probabilities of lexical entry selection have been
demonstrated to dominantly contribute to the dis-
ambiguation of phrase structures.
We have not yet investigated whether our results
can be reproduced with other lexicalized gram-
mars. Our results might hold only for HPSG be-
cause HPSG has strict feature constraints and has
lexical entries with rich syntactic information such
as wh-movement.
6 Conclusion
We developed an extremely lexicalized probabilis-
tic model for fast and accurate HPSG parsing.
The model is very simple. The probabilities of
parse trees are defined with only the probabili-
ties of selecting lexical entries, which are trained
by the discriminative methods in the log-linear
model with features of word trigrams and POS 5-
grams as defined in the CCG supertagging. Ex-
periments revealed that the model achieved im-
pressive accuracy as high as that of the previous
model for the probabilistic HPSG and that the im-
plemented parser runs around four times faster.
This indicates that accurate and fast parsing is pos-
sible using rather simple mechanisms. In addi-
tion, we provided another probabilistic model, in
which the probabilities for the leaf nodes in a parse
tree are given by the probabilities of supertag-
ging, and the probabilities for the intermediate
nodes are given by the previous phrase-structure-
based model. The experiments demonstrated not
only speeds significantly increased by three to four
times but also impressive improvement in parsing
accuracy by around two points in precision and re-
call.
We hope that this research provides a novel ap-
proach to deterministic parsing in which only lex-
ical selection and little phrasal information with-
out packed representations dominates the parsing
strategy.
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23(4):597?
618.
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
162
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL?05, pages 173?180.
Stephen Clark and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Proc.
of ACL?04, pages 104?111.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Univ. of Pennsylvania.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proc. of EACL?03, pages 91?98.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Proc.
of ACL ?99, pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL?03,
pages 423?430.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. of IJCNLP-04 Workshop ?Be-
yond Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Journal of Natural Language En-
gineering, 6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii,
2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok Lee
and Oi Yee Kwong (Eds.), Natural Language Pro-
cessing - IJCNLP 2004 LNAI 3248, chapter Corpus-
oriented Grammar Development for Acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lex-
ical rules on parsing with a treebank grammar. In
Proc. of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic hpsg parsing. In Proc. of IWPT
2005, pages 103?114.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling
of constraint-based grammars using log-linear mea-
sures and EM training. In Proc. of ACL?00, pages
480?487.
Libin Shen and Aravind K. Joshi. 2003. A SNoW
based supertagger with application to NP chunking.
In Proc. of ACL?03, pages 505?512.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proc. of HLT/EMNLP
2005, pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statisti-
cal constraint dependency grammar (CDG) parser.
In Proc. of ACL?04 Incremental Parsing work-
shop: Bringing Engineering and Cognition To-
gether, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Gram-
mar. Ph.D. thesis, Purdue University.
163
Proceedings of the 10th Conference on Parsing Technologies, pages 60?68,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A log-linear model with an n-gram reference distribution for accurate HPSG
parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
ninomi@r.dl.itc.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
NaCTeM (National Center for Text Mining)
tsujii@is.s.u-tokyo.ac.jp
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
Abstract
This paper describes a log-linear model with
an n-gram reference distribution for accurate
probabilistic HPSG parsing. In the model,
the n-gram reference distribution is simply
defined as the product of the probabilities
of selecting lexical entries, which are pro-
vided by the discriminative method with ma-
chine learning features of word and POS
n-gram as defined in the CCG/HPSG/CDG
supertagging. Recently, supertagging be-
comes well known to drastically improve
the parsing accuracy and speed, but su-
pertagging techniques were heuristically in-
troduced, and hence the probabilistic mod-
els for parse trees were not well defined.
We introduce the supertagging probabilities
as a reference distribution for the log-linear
model of the probabilistic HPSG. This is the
first model which properly incorporates the
supertagging probabilities into parse tree?s
probabilistic model.
1 Introduction
For the last decade, fast, accurate and wide-coverage
parsing for real-world text has been pursued in
sophisticated grammar formalisms, such as head-
driven phrase structure grammar (HPSG) (Pollard
and Sag, 1994), combinatory categorial grammar
(CCG) (Steedman, 2000) and lexical function gram-
mar (LFG) (Bresnan, 1982). They are preferred
because they give precise and in-depth analyses
for explaining linguistic phenomena, such as pas-
sivization, control verbs and relative clauses. The
main difficulty of developing parsers in these for-
malisms was how to model a well-defined proba-
bilistic model for graph structures such as feature
structures. This was overcome by a probabilistic
model which provides probabilities of discriminat-
ing a correct parse tree among candidates of parse
trees in a log-linear model or maximum entropy
model (Berger et al, 1996) with many features for
parse trees (Abney, 1997; Johnson et al, 1999; Rie-
zler et al, 2000; Malouf and van Noord, 2004; Ka-
plan et al, 2004; Miyao and Tsujii, 2005). Follow-
ing this discriminative approach, techniques for effi-
ciency were investigated for estimation (Geman and
Johnson, 2002; Miyao and Tsujii, 2002; Malouf and
van Noord, 2004) and parsing (Clark and Curran,
2004b; Clark and Curran, 2004a; Ninomiya et al,
2005).
An interesting approach to the problem of parsing
efficiency was using supertagging (Clark and Cur-
60
ran, 2004b; Clark and Curran, 2004a; Wang, 2003;
Wang and Harper, 2004; Nasr and Rambow, 2004;
Ninomiya et al, 2006; Foth et al, 2006; Foth and
Menzel, 2006), which was originally developed for
lexicalized tree adjoining grammars (LTAG) (Ban-
galore and Joshi, 1999). Supertagging is a process
where words in an input sentence are tagged with
?supertags,? which are lexical entries in lexicalized
grammars, e.g., elementary trees in LTAG, lexical
categories in CCG, and lexical entries in HPSG. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated in
the case of a CCG parser (Clark and Curran, 2004a)
with the result of a drastic improvement in the pars-
ing speed. Wang and Harper (2004) also demon-
strated the effects of supertagging with a statisti-
cal constraint dependency grammar (CDG) parser
by showing accuracy as high as the state-of-the-art
parsers, and Foth et al (2006) and Foth and Menzel
(2006) reported that accuracy was significantly im-
proved by incorporating the supertagging probabili-
ties into manually tuned Weighted CDG. Ninomiya
et al (2006) showed the parsing model using only
supertagging probabilities could achieve accuracy as
high as the probabilistic model for phrase structures.
This means that syntactic structures are almost de-
termined by supertags as is claimed by Bangalore
and Joshi (1999). However, supertaggers themselves
were heuristically used as an external tagger. They
filter out unlikely lexical entries just to help parsing
(Clark and Curran, 2004a), or the probabilistic mod-
els for phrase structures were trained independently
of the supertagger?s probabilistic models (Wang and
Harper, 2004; Ninomiya et al, 2006). In the case of
supertagging of Weighted CDG (Foth et al, 2006),
parameters for Weighted CDG are manually tuned,
i.e., their model is not a well-defined probabilistic
model.
We propose a log-linear model for probabilistic
HPSG parsing in which the supertagging probabil-
ities are introduced as a reference distribution for
the probabilistic HPSG. The reference distribution is
simply defined as the product of the probabilities of
selecting lexical entries, which are provided by the
discriminative method with machine learning fea-
tures of word and part-of-speech (POS) n-gram as
defined in the CCG/HPSG/CDG supertagging. This
is the first model which properly incorporates the su-
pertagging probabilities into parse tree?s probabilis-
tic model. We compared our model with the proba-
bilistic model for phrase structures (Miyao and Tsu-
jii, 2005). This model uses word and POS unigram
for its reference distribution, i.e., the probabilities of
unigram supertagging. Our model can be regarded
as an extension of a unigram reference distribution
to an n-gram reference distribution with features that
are used in supertagging. We also compared with a
probabilistic model in (Ninomiya et al, 2006). The
probabilities of their model are defined as the prod-
uct of probabilities of supertagging and probabilities
of the probabilistic model for phrase structures, but
their model was trained independently of supertag-
ging probabilities, i.e., the supertagging probabili-
ties are not used for reference distributions.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical entries
express word-specific characteristics. The structures
of sentences are explained using combinations of
schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly applying
schemata to lexical/phrasal signs. Finally, the parse
result is output as a phrasal sign that dominates the
sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
61
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries deter-
mine the dominant syntactic structures.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability that a parse result T is assigned to a
given sentence w = ?w1, . . . , wn? is
(Probabilistic HPSG)
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature func-
tion that represents a characteristic of parse tree T ,
and Zw is the sum over the set of all possible parse
trees for the sentence. Intuitively, the probability
is defined as the normalized product of the weights
exp(?u) when a characteristic corresponding to fu
appears in parse result T . The model parameters, ?u,
are estimated using numerical optimization methods
(Malouf, 2002) to maximize the log-likelihood of
the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the compu-
tation of p(T |w) for all parse candidates assigned
to sentence w. Because the number of parse can-
didates is exponentially related to the length of the
sentence, the estimation is intractable for long sen-
tences. To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). Miyao and Tsujii (2005) also introduced a
preliminary probabilistic model p0(T |w) whose es-
timation does not require the parsing of a treebank.
This model is introduced as a reference distribution
(Jelinek, 1998; Johnson and Riezler, 2000) of the
probabilistic HPSG model; i.e., the computation of
parse trees given low probabilities by the model is
omitted in the estimation stage (Miyao and Tsujii,
2005), or a probabilistic model can be augmented
by several distributions estimated from the larger
and simpler corpus (Johnson and Riezler, 2000). In
(Miyao and Tsujii, 2005), p0(T |w) is defined as the
product of probabilities of selecting lexical entries
with word and POS unigram features:
(Miyao and Tsujii (2005)?s model)
puniref (T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in
T and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model with
other two types of probabilistic models using a su-
pertagger (Ninomiya et al, 2006). The first one is
the simplest probabilistic model, which is defined
with only the probabilities of lexical entry selec-
tion. It is defined simply as the product of the prob-
abilities of selecting all lexical entries in the sen-
tence; i.e., the model does not use the probabilities
of phrase structures like the probabilistic models ex-
plained above. Given a set of lexical entries, L, a
sentence, w = ?w1, . . . , wn?, and the probabilistic
model of lexical entry selection, p(li ? L|w, i), the
first model is formally defined as follows:
62
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(Ninomiya et al (2006)?s model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi in T
and p(li|w, i) is the probability of selecting lexical
entry li for wi.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic model of lexical entry selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical entries
for the word wi.
The second model is a hybrid model of supertag-
ging and the probabilistic HPSG. The probabilities
are given as the product of Ninomiya et al (2006)?s
model 1 and the probabilistic HPSG.
(Ninomiya et al (2006)?s model 3)
pmodel3(T |w) = pmodel1(T |w)phpsg(T |w)
In the experiments, we compared our model with
Miyao and Tsujii (2005)?s model and Ninomiya et
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
fsptag =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Feature templates.
al. (2006)?s model 1 and 3. The features used in our
model and their model are combinations of the fea-
ture templates listed in Table 1 and Table 2. The
feature templates fbinary and funary are defined for
constituents at binary and unary branches, froot is a
feature template set for the root nodes of parse trees.
flex is a feature template set for calculating the uni-
gram reference distribution and is used in Miyao and
Tsujii (2005)?s model. fsptag is a feature template
set for calculating the probabilities of selecting lex-
ical entries in Ninomiya et al (2006)?s model 1 and
3. The feature templates in fsptag are word trigrams
and POS 5-grams. An example of features applied
to the parse tree for the sentence ?Spring has come?
is shown in Figure 2.
3 Probabilistic HPSG with an n-gram
reference distribution
In this section, we propose a probabilistic model
with an n-gram reference distribution for probabilis-
tic HPSG parsing. This is an extension of Miyao
and Tsujii (2005)?s model by replacing the unigram
reference distribution with an n-gram reference dis-
tribution. Our model is formally defined as follows:
63
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
combinations of feature templates for fsptag
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Combinations of feature templates.
(Probabilistic HPSG with an n-gram reference distribution)
pnref (T |w) =
1
Znref pmodel1(T |w) exp
(?
u
?ufu(T )
)
Znref =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In our model, Ninomiya et al (2006)?s model 1
is used as a reference distribution. The probabilis-
tic model of lexical entry selection and its feature
templates are the same as defined in Ninomiya et al
(2006)?s model 1.
The formula of our model is the same as Ni-
nomiya et al (2006)?s model 3. But, their model
is not a probabilistic model with a reference distri-
bution. Both our model and their model consist of
the probabilities for lexical entries (= pmodel1(T |w))
and the probabilities for phrase structures (= the rest
of each formula). The only difference between our
model and their model is the way of how to train
model parameters for phrase structures. In both our
model and their model, the parameters for lexical en-
tries (= the parameters of pmodel1(T |w)) are first es-
timated from the word and POS sequences indepen-
dently of the parameters for phrase structures. That
is, the estimated parameters for lexical entries are
the same in both models, and hence the probabilities
of pmodel1(T |w) of both models are the same. Note
that the parameters for lexical entries will never be
updated after this estimation stage; i.e., the parame-
ters for lexical entries are not estimated in the same
time with the parameters for phrase structures. The
difference of our model and their model is the esti-
mation of parameters for phrase structures. In our
model, given the probabilities for lexical entries, the
parameters for phrase structures are estimated so as
to maximize the entire probabilistic model (= the
product of the probabilities for lexical entries and
the probabilities for phrase structures) in the train-
ing corpus. In their model, the parameters for phrase
structures are trained without using the probabili-
ties for lexical entries, i.e., the parameters for phrase
structures are estimated so as to maximize the prob-
abilities for phrase structures only. That is, the pa-
rameters for lexical entries and the parameters for
phrase structures are trained independently in their
model.
Miyao and Tsujii (2005)?s model also uses a ref-
erence distribution, but with word and POS unigram
features, as is explained in the previous section. The
only difference between our model and Miyao and
Tsujii (2005)?s model is that our model uses se-
quences of word and POS tags as n-gram features
for selecting lexical entries in the same way as su-
pertagging does.
4 Experiments
We evaluated the speed and accuracy of parsing
by using Enju 2.1, the HPSG grammar for English
(Miyao et al, 2005; Miyao and Tsujii, 2005). The
lexicon of the grammar was extracted from Sec-
tions 02-21 of the Penn Treebank (Marcus et al,
1994) (39,832 sentences). The grammar consisted
of 3,797 lexical entries for 10,536 words1. The prob-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by apply-
ing lexical rules to observed lexical entries in the HPSG tree-
bank (Nakanishi et al, 2004). The lexicon, however, included
many lexical entries that do not appear in the HPSG treebank.
64
No. of tested sentences Total No. of sentences Avg. length of tested sentences
Section 23 2,299 (100.00%) 2,299 22.2
Section 24 1,245 (99.84%) 1,247 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (Gold POSs)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 87.26 86.50 86.88 90.73 89.93 90.33 604
Ninomiya et al (2006)?s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129
Ninomiya et al (2006)?s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152
our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234
our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379
Section 23 (POS tagger)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 84.96 84.25 84.60 89.55 88.80 89.17 674
Ninomiya et al (2006)?s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154
Ninomiya et al (2006)?s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183
Matsuzaki et al (2007)?s model 86.93 86.47 86.70 - - - 30
our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260
our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821
Table 4: Experimental results for Section 23.
abilistic models were trained using the same portion
of the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and quick check
(Malouf et al, 2000).
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tuple
??,wh, a, wa?, where ? is the predicate type (e.g.,
adjective, intransitive verb), wh is the head word of
the predicate, a is the argument label (MODARG,
ARG1, ..., ARG4), and wa is the head word of
the argument. Labeled precision (LP)/labeled re-
call (LR) is the ratio of tuples correctly identified
by the parser2. Unlabeled precision (UP)/unlabeled
recall (UR) is the ratio of tuples without the pred-
icate type and the argument label. This evaluation
scheme was the same as used in previous evaluations
of lexicalized grammars (Hockenmaier, 2003; Clark
The HPSG treebank is used for training the probabilistic model
for lexical entry selection, and hence, those lexical entries that
do not appear in the treebank are rarely selected by the proba-
bilistic model. The ?effective? tag set size, therefore, is around
1,361, the number of lexical entries without those never-seen
lexical entries.
2When parsing fails, precision and recall are evaluated, al-
though nothing is output by the parser; i.e., recall decreases
greatly.
and Curran, 2004b; Miyao and Tsujii, 2005). The
experiments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the Tree-
bank was used as the development set, and the per-
formance was evaluated using sentences of ? 100
words in Section 23. The performance of each
model was analyzed using the sentences in Section
24 of ? 100 words. Table 3 details the numbers
and average lengths of the tested sentences of ? 100
words in Sections 23 and 24, and the total numbers
of sentences in Sections 23 and 24.
The parsing performance for Section 23 is shown
in Table 4. The upper half of the table shows the per-
formance using the correct POSs in the Penn Tree-
bank, and the lower half shows the performance us-
ing the POSs given by a POS tagger (Tsuruoka and
Tsujii, 2005). LF and UF in the figure are labeled
F-score and unlabeled F-score. F-score is the har-
monic mean of precision and recall. We evaluated
our model in two settings. One is implemented with
a narrow beam width (?our model 1? in the figure),
and the other is implemented with a wider beam
width (?our model 2? in the figure)3. ?our model
3The beam thresholding parameters for ?our model 1? are
?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? = 2.5, ?last =15.0, ?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? =2.5, ?last = 15.0, ?0 = 6.0,?? = 3.5, and ?last = 20.0.
65
83.00%
83.50%
84.00%
84.50%
85.00%
85.50%
86.00%
86.50%
87.00%
87.50%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F-sc
ore
Miyao and Tsujii(2005)'s modelNinomiya et al(2006)'s model 1Ninomiya et al(2006)'s model 3
our model
Figure 3: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
1? was introduced to measure the performance with
balanced F-score and speed, which we think appro-
priate for practical use. ?our model 2? was intro-
duced to measure how high the precision and re-
call could reach by sacrificing speed. Our mod-
els increased the parsing accuracy. ?our model 1?
was around 2.6 times faster and had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model. ?our model 2? was around 2.3 times slower
but had around 2.9 points higher F-score than Miyao
and Tsujii (2005)?s model. We must admit that the
difference between our models and Ninomiya et al
(2006)?s model 3 was not as great as the differ-
ence from Miyao and Tsujii (2005)?s model, but ?our
model 1? achieved 0.56 points higher F-score, and
?our model 2? achieved 0.8 points higher F-score.
When the automatic POS tagger was introduced, F-
score dropped by around 2.4 points for all models.
We also compared our model with Matsuzaki et
al. (2007)?s model. Matsuzaki et al (2007) pro-
The terms ? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs in the chart
cell. The terms ? and ? are the thresholds of the number and
the beam width of lexical entries, and ? is the beam width for
global thresholding (Goodman, 1997). The terms with suffixes
0 are the initial values. The parser iterates parsing until it suc-
ceeds to generate a parse tree. The parameters increase for each
iteration by the terms prefixed by ?, and parsing finishes when
the parameters reach the terms with suffixes last. Details of the
parameters are written in (Ninomiya et al, 2005). The beam
thresholding parameters for ?our model 2? are ?0 = 18,?? =
6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0, ?0 =18,?? = 6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0.
In ?our model 2?, the global thresholding was not used.
posed a technique for efficient HPSG parsing with
supertagging and CFG filtering. Their results with
the same grammar and servers are also listed in the
lower half of Table 4. They achieved drastic im-
provement in efficiency. Their parser ran around 6
times faster than Ninomiya et al (2006)?s model 3,
9 times faster than ?our model 1? and 60 times faster
than ?our model 2.? Instead, our models achieved
better accuracy. ?our model 1? had around 0.5 higher
F-score, and ?our model 2? had around 0.8 points
higher F-score. Their efficiency is mainly due to
elimination of ungrammatical lexical entries by the
CFG filtering. They first parse a sentence with a
CFG grammar compiled from an HPSG grammar,
and then eliminate lexical entries that are not in the
parsed CFG trees. Obviously, this technique can
also be applied to the HPSG parsing of our mod-
els. We think that efficiency of HPSG parsing with
our models will be drastically improved by applying
this technique.
The average parsing time and labeled F-score
curves of each probabilistic model for the sentences
in Section 24 of ? 100 words are graphed in Fig-
ure 3. The graph clearly shows the difference of
our model and other models. As seen in the graph,
our model achieved higher F-score than other model
when beam threshold was widen. This implies that
other models were probably difficult to reach the F-
score of ?our model 1? and ?our model 2? for Section
23 even if we changed the beam thresholding param-
eters. However, F-score of our model dropped eas-
66
ily when we narrow down the beam threshold, com-
pared to other models. We think that this is mainly
due to its bad implementation of parser interface.
The n-gram reference distribution is incorporated
into the kernel of the parser, but the n-gram fea-
tures and a maximum entropy estimator are defined
in other modules; n-gram features are defined in a
grammar module, and a maximum entropy estimator
for the n-gram reference distribution is implemented
with a general-purpose maximum entropy estimator
module. Consequently, strings that represent the n-
gram information are very frequently changed into
feature structures and vice versa when they go in and
out of the kernel of the parser. On the other hand, Ni-
nomiya et al (2006)?s model 3 uses the supertagger
as an external module. Once the parser acquires the
supertagger?s outputs, the n-gram information never
goes in and out of the kernel. This advantage of Ni-
nomiya et al (2006)?s model can apparently be im-
plemented in our model, but this requires many parts
of rewriting of the implemented parser. We estimate
that the overhead of the interface is around from 50
to 80 ms/sentence. We think that re-implementation
of the parser will improve the parsing speed as esti-
mated. In Figure 3, the line of our model crosses the
line of Ninomiya et al (2006)?s model. If the esti-
mation is correct, our model will be faster and more
accurate so that the lines in the figure do not cross.
Speed-up in our model is left as a future work.
5 Conclusion
We proposed a probabilistic model in which su-
pertagging is consistently integrated into the prob-
abilistic model for HPSG. In the model, the n-gram
reference distribution is simply defined as the prod-
uct of the probabilities of selecting lexical entries
with machine learning features of word and POS n-
gram as defined in the CCG/HPSG/CDG supertag-
ging. We conducted experiments on the Penn Tree-
bank with a wide-coverage HPSG parser. In the ex-
periments, we compared our model with the prob-
abilistic HPSG with a unigram reference distribu-
tion (Miyao and Tsujii, 2005) and the probabilistic
HPSG with supertagging (Ninomiya et al, 2006).
Though our model was not as fast as Ninomiya
et al (2006)?s models, it achieved the highest ac-
curacy among them. Our model had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.56 points higher F-score than
the Ninomiya et al (2006)?s model 3. When we sac-
rifice parsing speed, our model achieved around 2.9
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.8 points higher F-score than Ni-
nomiya et al (2006)?s model 3. Our model achieved
higher F-score because parameters for phrase struc-
tures in our model are trained with the supertagging
probabilities, which are not in other models.
References
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Joan Bresnan. 1982. The Mental Representation of
Grammatical Relations. MIT Press, Cambridge, MA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proc. of
ACL?04, pages 104?111.
Killian Foth and Wolfgang Menzel. 2006. Hybrid pars-
ing: Using probabilistic models as predictors for a
symbolic parser. In Proc. of COLING-ACL 2006.
Killian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a constraint dependency parser with su-
pertags. In Proc. of COLING-ACL 2006.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
F. Jelinek. 1998. Statistical Methods for Speech Recog-
nition. The MIT Press.
67
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proc. of NAACL-2000, pages 154?161.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proc. of ACL ?99,
pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In Proc. of IJCNLP-04 Workshop ?Beyond
Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake. 2000.
Efficient feature structure operations without compi-
lation. Journal of Natural Language Engineering,
6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG parsing with supertagging and
CFG-filtering. In Proc. of IJCAI 2007, pages 1671?
1676.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proc. of HLT
2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii, 2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok
Lee and Oi Yee Kwong (Eds.), Natural Language
Processing - IJCNLP 2004 LNAI 3248, chapter
Corpus-oriented Grammar Development for Acquir-
ing a Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lexi-
cal rules on parsing with a treebank grammar. In Proc.
of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
and Jun?ichi Tsujii. 2005. Efficacy of beam threshold-
ing, unification filtering and hybrid parsing in proba-
bilistic HPSG parsing. In Proc. of IWPT 2005, pages
103?114.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proc. of EMNLP 2006, pages 155?163.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proc. of ACL?00, pages 480?487.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proc. of HLT/EMNLP 2005,
pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc.
of ACL?04 Incremental Parsing workshop: Bringing
Engineering and Cognition Together, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Grammar.
Ph.D. thesis, Purdue University.
68
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 210?213,
Paris, October 2009. c?2009 Association for Computational Linguistics
HPSG Supertagging: A Sequence Labeling View
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining, UK
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
Supertagging is a widely used speed-up
technique for deep parsing. In another
aspect, supertagging has been exploited
in other NLP tasks than parsing for
utilizing the rich syntactic information
given by the supertags. However, the
performance of supertagger is still a
bottleneck for such applications. In this
paper, we investigated the relationship
between supertagging and parsing, not
just to speed up the deep parser; We
started from a sequence labeling view
of HPSG supertagging, examining how
well a supertagger can do when separated
from parsing. Comparison of two types
of supertagging model, point-wise model
and sequential model, showed that the
former model works competitively well
despite its simplicity, which indicates
the true dependency among supertag
assignments is far more complex than the
crude first-order approximation made in
the sequential model. We then analyzed
the limitation of separated supertagging
by using a CFG-filter. The results showed
that big gains could be acquired by resort-
ing to a light-weight parser.
1 Introduction
Supertagging is an important part of lexicalized
grammar parsing. A high performance supertag-
ger greatly reduces the load of a parser and ac-
celerates its speed. A supertag represents a lin-
guistic word category, which encodes syntactic be-
havior of the word. The concept of supertagging
was first proposed for lexicalized tree adjoining
grammar (LTAG) (Bangalore and Joshi, 1999) and
then extended to other lexicalized grammars, such
as combinatory categorial grammar (CCG) (Clark,
2002) and Head-driven phrase structure grammar
(HPSG) (Ninomiya et al, 2006). Recently, syn-
tactic information in supertags has been exploited
for NLP tasks besides parsing, such as NP chunk-
ing (Shen and Joshi, 2003), semantic role label-
ing (Chen and Rambow, 2003) and machine trans-
lation (Hassan et al, 2007). Supertagging serves
there as an implicit and convenient way to incor-
porate rich syntactic information in those tasks.
Improving the performance of supertagging can
thus benefit these two aspects: as a preproces-
sor for deep parsing and as an independent, al-
ternative technique for ?almost? parsing. How-
ever, supertags are derived from a grammar and
thus have a strong connection to parsing. To fur-
ther improve the supertagging accuracy, the rela-
tion between supertagging and parsing is crucial.
With this motivation, we investigate how well a se-
quence labeling model can do when it is separated
from a parser, and to what extent the ignorance of
long distance dependencies in the sequence label-
ing formulation affects the supertagging results.
Specifically, we evaluated two different types
of supertagging model, point-wise model and se-
quential model, for HPSG supertagging. CFG-
filter was then used to empirically evaluate the
effect of long distance dependencies in supertag-
ging. The point-wise model achieved competitive
result of 92.53% accuracy on WSJ-HPSG tree-
bank with fast training speed, while the sequen-
tial model augmented with supertag edge features
did not give much further improvement over the
point-wise model. Big gains acquired by using
CFG-filter indicates that further improvement may
be achieved by resorting to a light-weight parser.
2 HPSG Supertags
HPSG (Pollard and Sag, 1994) is a kind of lexi-
calized grammar. In HPSG, many lexical entries
are used to express word-specific characteristics,
210
while only small amount of rule schemas are used
to describe general constructions. A supertag in
HPSG corresponds to a template of lexical entry.
For example, one possible supertag for ?big? is
?[<ADJP>]N lxm?, which indicates that the syn-
tactic category of ?big? is adjective and it modi-
fies a noun to its right. The number of supertags
is generally much larger than the number of labels
used in other sequence labeling tasks; Comparing
to 45 POS tags used in PennTreebank, the HPSG
grammar used in our experiments includes 2,308
supertags. Because of this, it is often very hard or
even impossible to apply computationary demand-
ing methods to HPSG supertagging.
3 Perceptron and Bayes Point Machine
Perceptron is an efficient online discriminative
training method. We used perceptron with weight-
averaging (Collins, 2002) as the basis of our su-
pertagging model. We also use perceptron-based
Bayes point machine (BPM) (Herbrich et al,
2001) in some of the experiments. In short, a BPM
is an average of a number of averaged perceptrons?
weights. We use average of 10 averaged percep-
trons, each of which is trained on a different ran-
dom permutation of the training data.
3.1 Formulation
Here we follow the definition of Collins? per-
ceptron to learn a mapping from the input space
(w, p) ? W ? P to the supertag space s ? S. We
use function GEN(w,p) to indicate all candidates
given input (w, p). Feature function f maps a train-
ing sample (w, p, s) ?W ?P ?S to a point in the
feature space Rd. To get feature weights ? ? Rd
of feature function, we used the averaged percep-
tron training method described in (Collins, 2002),
and the average of its 10 different runs (i.e., BPM).
For decoding, given an input (w, p) and a vector
of feature weights ?, we want to find an output s
which satisfies:
F (w, p) = argmax
s?GEN(w, p)
? ? f(w, p, s)
For the input (w, p), we treat it in two fash-
ions: one is (w, p) representing a single word
and a POS tag. Another is (w, p) representing
whole word and POS tags sequence. We call them
point-wise model and sequential model respec-
tively. Viterbi algorithm is used for decoding in
sequential model.
template type template
Word wi,wi?1,wi+1,
wi?1&wi, wi&wi+1
POS pi, pi?1, pi?2, pi+1,
pi+2, pi?1&pi, pi?2&pi?1,
pi?1&pi+1, pi&pi+1, pi+1&pi+2
Word-POS pi?1&wi, pi&wi, pi+1&wi
Supertag? si?1 , si?2&si?1
Substructure {ssi,1, ..., ssi,N}?Word
{ssi,1, ..., ssi,N}? POS
{ssi,1, ..., ssi,N}?Word-POS
{ssi?1,1, ..., ssi?1,N}?
{ssi,1, ..., ssi,N}?
Table 1: Feature templates for point-wise model
and sequential model. Templates with ? are only
used by sequential model. ssi,j represents j-th
substructure of supertag at i. For briefness, si is
omitted for each template. ??? means set-product.
e.g., {a,b}?{A,B}={a&A,a&B,b&A,b&B}
3.2 Features
Feature templates are listed in Table 1. To make
the results comparable with previous work, we
adopt the same feature templates as Matsuzaki et.
al. (2007). For sequential model, supertag con-
texts are added to the features. Because of the
large number of supertags, those supertag edge
features could be very sparse. To alleviate this
sparseness, we extracted sub-structures from the
lexical template of each supertag, and use them for
making generalized node/edge features as shown
in Table 1. The sub-structures we used include
subcategorization frames (e.g., subject=NP, ob-
ject=NP PP), direction and category of modifiee
phrase (e.g., mod left=VP), voice and tense of a
verb (e.g., passive past).
3.3 CFG-filter
Long distance dependencies are also encoded in
supertags. For example, when a transitive verb
gets assigned a supertag that specifies it has a PP-
object, in most cases a preposition to its right must
be assigned an argument (not adjunct) supertag,
and vice versa. Such kind of long distance context
information might be important for supertag dis-
ambiguation, but is not easy to incorporate into a
sequence labeling model separated from a parser.
To examine the limitation of supertagging sep-
arated from a parser, we used CFG-filter as an ap-
211
Model Name Acc%
PW-AP 92.29
SEQ-AP 92.53
PW-AP+CFG 93.57
SEQ-AP+CFG 93.68
Table 2: Averaged 10-cross validation of averaged
perceptron on Section 02-21.
proximation of an HPSG parser. We firstly cre-
ated a CFG that approximates the original HPSG
grammar, using the iterative method by Kiefer
and Krieger (2000). Given the supertags as pre-
terminals, the approximating CFG was then used
for finding a maximally scored sequence of su-
pertags which satisfies most of the grammatical
constraints in the original HPSG grammar (Mat-
suzaki et al, 2007). By comparing the supertag-
ging results before and after CFG-filtering, we can
quantify how many errors are caused by ignorance
of the long-range dependencies in supertagger.
4 Experiments and Analysis
We conducted experiments on WSJ-HPSG tree-
bank corpus (Miyao, 2006), which was semi-
automatically converted from the WSJ portion of
PennTreebank. The number of training iterations
was set to 5 for all models. Gold-standard POS
tags are used as input. The performance is evalu-
ated by accuracy1 and speed of supertagging on an
AMD Opteron 2.4GHz server.
Table 2 shows the averaged results of 10-
fold cross-validation of averaged perceptron (AP)
models2 on section 02-21. We can see the dif-
ference between point-wise AP model and se-
quential AP model is small (0.24%). It becomes
even smaller after CFG-filtering (0.11%). Table
3 shows the supertagging accuracy on section 22
based on BPM. Although not statistically signif-
icantly different from previous ME model (Mat-
suzaki et al, 2007), point-wise model (PW-BPM)
achieved competitive result 92.53% with faster
training. In addition, 0.27% and 0.29% gains were
brought by using BPM from PW-AP (92.26%) and
PW-SEQ (92.54%) with P-values less than 0.05.
The improvement by using sequential mod-
els (PW-AP?SEQ-AP: 0.24%, PW-BPM?SEQ-
BPM: 0.3%, statistically significantly different),
1?UNK? supertags are ignored in evaluation as previous.
2For time limitation, cross validation for BPM was not
conducted.
Model Name Acc% Training/
Testing Time ?
ME (Matsuzaki 07?) 92.45 ? 3h / 12s
PW-BPM 92.53 285s / 10s
SEQ-BPM 92.83 1721s / 13s
PW-BPM+SUB 92.68 1275s / 25s
SEQ-BPM+SUB 92.99 9468s / 107s
PW-BPM+CFG 93.60 285s / 78s
SEQ-BPM+CFG 93.70 1721s / 195s
PW-BPM+SUB+CFG 93.72 1275s / 170s
SEQ-BPM+SUB+CFG 93.88 9468s / 1011s
Table 3: Supertagging accuracy and training&
testing speed on section 22. (?) Test time was cal-
culated on totally 1648 sentences.
compared to point-wise models, were not so large,
but the training time was around 6 times longer.
We think the reason is twofold. First, as previous
research showed, POS sequence is very informa-
tive in supertagging (Clark, 2004). A large amount
of local syntactic information can be captured in
POS tags of surrounding words, although a few
long-range dependencies are of course not. Sec-
ond, the number of supertags is large and the su-
pertag edge features used in sequential model are
inevitably suffered from data sparseness. To alle-
viate this, we extracted sub-structure from lexical
templates (i.e., lexical items corresponding to su-
pertags) to augment the supertag edge features, but
only got 0.16% improvement (SEQ-BPM+SUB).
Furthermore, we also got 0.15% gains with P-
value less than 0.05 by incorporating the sub-
structure features into point-wise model (PW-
BPM+SUB). We hence conclude that the contri-
bution of the first-order edge features is not large
in sequence modeling for HPSG supertagging.
As we explained in Section 3.3, sequence label-
ing models have inherent limitation in the ability
to capture long distance dependencies between su-
pertags. This kind of ambiguity could be easier to
solve in a parser. To examine this, we added CFG-
filter which works as an approximation of a full
HPSG parser, after the sequence labeling model.
As expected, there came big gains of 1.26% (from
PW-AP to PW-AP+CFG) and 1.15% (from PW-
BPM to PW-BPM+CFG). Even for the sequen-
tial model we also got 1.15% (from SEQ-AP to
SEQ-AP+CFG) and 0.87% (from SEQ-BPM to
SEQ-BPM+CFG) respectively. All these models
were statistically significantly different from orig-
212
inal ones.
We also gave error analysis on test results.
Comparing SEQ-AP with SEQ-AP+CFG, one of
the most frequent types of ?correct supertag? by
the CFG-filter was for word ?and?, wherein a su-
pertag for NP-coordination (?NP and NP?) was
corrected to one for VP-coordination (?VP and
VP? or ?S and S?). It means the disambiguation
between the two coordination type is difficult for
supertaggers, presumably because they looks very
similar with a limited length of context since the
sequence of the NP-object of left conjunct, ?and?,
the NP subject of right conjunct looks very similar
to a NP coordination. The different assignments
by SEQ-AP+CFG from SEQ-AP include 725 right
corrections, while it changes 298 correct predic-
tions by SEQ-AP to wrong assignments. One pos-
sible reason for some of ?wrong correction? is re-
lated to the approximation of grammar. But this
gives clue that for supertagging task: just using
sequence labeling models is limited, and we can
resort to use some light-weight parser to handle
long distance dependencies.
Although some of the ambiguous supertags
could be left for deep parsing, like multi-tagging
technique (Clark, 2004), we also consider the
tasks where supertags can be used while conduct-
ing deep parsing is too computationally costly. Al-
ternatively, focusing on supertagging, we could
treat it as a sequence labeling task, while a conse-
quent light-weight parser is a disambiguator with
long distance constraint.
5 Conclusions
In this paper, through treating HPSG supertag-
ging in a sequence labeling way, we examined
the relationship between supertagging and parsing
from an angle. In experiment, even for sequential
models, CFG-filter gave much larger improvement
than one gained by switching from a point-wise
model to a sequential model. The accuracy im-
provement given by the CFG-filter suggests that
we could gain further improvement by combining
a supertagger with a light-weight parser.
Acknowledgments
Thanks to the anonymous reviewers for valuable
comments. The first author was partially sup-
ported by University of Tokyo Fellowship (UT-
Fellowship). This work was partially supported
by Grant-in-Aid for Specially Promoted Research
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25:237?265.
John Chen and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling
of semantic arguments. In Proceedings of EMNLP-
2003, pages 41?48.
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
and Related Frameworks (TAG+ 6), pages 19?24.
Stephen Clark. 2004. The importance of supertagging
for wide-coverage ccg parsing. In Proceedings of
COLING-04, pages 282?288.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. pages 1?8.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine transla-
tion. In Proceedings of ACL 2007, pages 288?295.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes point machines. Journal of Machine
Learning Research, 1:245?279.
Bernd Kiefer and Hans-Ulrich Krieger. 2000. A
context-free approximation of head-driven phrase
structure grammar. In Proceedings of IWPT-2000,
pages 135?146.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient hpsg parsing with supertagging
and cfg-filtering. In Proceedings of IJCAI-07, pages
1671?1676.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. Disserta-
tion, The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Mat-
suzaki, and Yusuke Miyao. 2006. Extremely lex-
icalized models for accurate and fast hpsg parsing.
In Proceedings of EMNLP-2006, pages 155?163.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University of Chicago /
CSLI.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505?512.
213
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1281?1289,
Beijing, August 2010
Forest-guided Supertagger Training
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
Supertagging is an important technique
for deep syntactic analysis. A super-
tagger is usually trained independently
of the parser using a sequence labeling
method. This presents an inconsistent
training objective between the supertagger
and the parser. In this paper, we pro-
pose a forest-guided supertagger training
method to alleviate this problem by incor-
porating global grammar constraints into
the supertagging process using a CFG-
filter. It also provides an approach to
make the supertagger and the parser more
tightly integrated. The experiment shows
that using the forest-guided trained super-
tagger, the parser got an absolute 0.68%
improvement from baseline in F-score
for predicate-argument relation recogni-
tion accuracy and achieved a competi-
tive result of 89.31% with a faster pars-
ing speed, compared to a state-of-the-art
HPSG parser.
1 Introduction
Deep syntactic analysis by lexicalized grammar
parsing, which provides linguistic-rich informa-
tion for many NLP tasks, has recently received
more and more attention from the NLP commu-
nity. To use a deep parser in real large-scale ap-
plications, speed is an important issue to take into
consideration. Supertagging is one of the speed-
up technique widely used for lexicalized grammar
parsing. A supertagger is used to limit the number
of plausible lexical entries fed to the parser, this
can greatly reduce the search space for the parser.
Supertagging was first proposed for Lexicalized
Tree Adjoining Grammar (LTAG) (Bangalore and
Joshi, 1999), and then successfully applied to
Combinatory Categorial Grammar (CCG) (Clark,
2002) and Head-driven Phrase Structure Gram-
mar (HPSG) (Ninomiya et al, 2006). In addi-
tion, supertags can also be used for other NLP
tasks besides parsing, such as semantic role label-
ing (Chen and Rambow, 2003) and machine trans-
lation (Birch et al, 2007; Hassan et al, 2007) to
utilize syntactic information in the supertags.
In lexicalized grammar parsing, supertagging is
usually treated as a sequence labeling task inde-
pendently trained from the parser. Previous re-
search (Clark, 2002) showed that even a point-
wise classifier not considering context edge fea-
tures is effective when used as a supertagger. To
make up for the insufficient accuracy as a single-
tagger, more than one supertag prediction is re-
served and the parser takes the burden of resolving
the rest of the supertag ambiguities.
A non-trivial problem raised by the separate
training of the supertagger is that the prediction
score provided by the supertagger might not be
suitable for direct use in the parsing process, since
a separately trained supertagger that does not take
into account grammar constraints has a training
objective which is inconsistent with the parser.
Although the scores provided by the supertagger
can be ignored (e.g., in some CCG parsers), this
may also discard some useful information for ef-
fective beam search and accurate disambiguation.
Based on this observation, we assume that
considering global grammar constraints during
the supertagger training process would make the
supertagger and the parser more tightly integrated.
1281
In this paper, we propose an on-line forest-guided
training method for a supertagger to make the
training objective of a supertagger more closely
related to the parsing task. We implemented this
method on a large-scale HPSG grammar. We
used a CFG grammar to approximate the original
HPSG grammar in the supertagging stage and ap-
plied best-first search to select grammar-satisfying
supertag sequences for the parameter updating.
The experiments showed that the HPSG parser is
improved by considering structure constraints in
the supertagging training process. For the stan-
dard test set (Penn Treebank Section 23), we ac-
complished an absolute 0.68% improvement from
baseline in F-score for predicate-argument rela-
tion recognition and got a competitive result of
89.31% with a faster parsing speed, compared to
a state-of-the-art HPSG parser.
The remainder of the paper is organized as
follows: in section 2 we provide the necessary
background regarding HPSG parsing. In section
3, we introduce the on-line forest-guided super-
tagger training method. Section 4 shows the ex-
periment results and the related analysis. Section
5 compares the proposed approach with related
work and section 6 presents our conclusions and
future work.
2 Background
2.1 Statistical HPSG Parsing
HPSG (Pollard and Sag, 1994) is a lexicalist
grammar framework. In HPSG, a large number
of lexical entries are used to express word-specific
characteristics, while only a small number of rule
schemata are used to describe general construc-
tion rules. Typed feature structures named ?signs?
are used to represent both lexical entries and
phrasal constituents. A classic efficient statisti-
cal HPSG parsing process is depicted in Figure 1.
Given a word and part-of-speech sequence (w, p)
as input, the first step (called ?supertagging?) in
HPSG parsing is to assign possible lexical entries.
In practice, for each word, more than one super-
tag is reserved for the parser. Then, the parser
searches the given lexical entry space to construct
a HPSG tree using the rule schemata to com-
bine possible signs. Constituent-based methods
and transition-based methods can be used for tree
structure disambiguation. This parsing framework
using supertagging is also used in other lexical-
ized grammars, such as LTAG and CCG.
2.2 HPSG Supertagging
Like other lexicalized grammar, the lexical en-
tries defined in HPSG are referred to as ?super-
tags?. For example, the word ?like? is assigned
a lexical entry for transitive verbs in non-3rd per-
son present form, which indicates that the head
syntactic category of ?like? is verb and it has
an NP subject and an NP complement. With
such fine-grained grammatical type distinctions,
the number of supertags is very large. Compared
to the 45 part-of-speech (POS) tags defined in the
PennTreebank, the HPSG grammar we used con-
tains 2,308 supertags. The large number and the
complexity of the supertags makes supertagging
harder than the POS tagging task.
Supertagging can be formulated as a sequence
labeling task. Here, we follow the definition of
Collins? perceptron (Collins, 2002). The train-
ing objective of supertagging is to learn the map-
ping from a POS-tagged word sentence w =
(w1/p1, ..., wn/pn) to a sequence of supertags
s = (s1, ..., sn). We use function GEN(w)
to indicate all candidates of supertag sequences
given input w. Feature function ? maps a sam-
ple (w, s) to a point in the feature space Rd. ? is
the vector of feature weights. Given an input w,
the most plausible supertag sequence is found by
the prediction function defined as follows:
F (w) = argmax
s?GEN(w)
? ? ?(w, s) (1)
2.3 CFG-filtering
CFG-filtering (Kiefer and Krieger, 2000) is a tech-
nique to find a superset of (packed) HPSG parse
trees that satisfy the constraints in a grammar. A
CFG that approximates the original HPSG gram-
mar is used for efficiently finding such trees with-
out doing full-fledged HPSG parsing that is com-
putationally demanding because the schema ap-
plication involves unification operations among
large feature structures (signs). The number of
possible signs is infinite in general and hence
1282
Figure 1: HPSG parsing for the sentence ?They like coffee.?
some features (e.g., the number agreement fea-
ture) are ignored in the approximating CFG so that
the set of possible signs can be approximated by
a finite set of non-terminal symbols in the CFG.
By this construction, some illegal trees may be
included in the set of trees licensed by the ap-
proximating CFG, but none of the well-formed
trees (i.e., those satisfying all constraints in the
grammar) are excluded by the approximation. We
use the algorithm described by Kiefer and Krieger
(2000) to obtain the approximating CFG for the
original HPSG. The technical details regarding
the algorithm can be found in Kiefer and Krieger
(2000).
3 Forest-guided Training for
Supertagging
3.1 Motivation
In lexicalized grammar parsing, a parser aims to
find the most plausible syntactic structure for a
given sentence based on the supertagging results.
One efficient parsing approach is to use predic-
tion scores provided by the supertagger. Usu-
ally, the supertagger is trained separately from the
structure disambiguation in a later stage. This
pipeline parsing strategy poses a potential prob-
lem in that the training objective of a supertagger
can deviate from the final parser, if the global
grammar constraints are not considered. For ex-
ample, the supertag predictions for some words
can contribute to high supertagging accuracy, but
cause the parser to fail. Therefore, considering the
global grammar constraints in the supertagging
training stage can make the supertagger and the
Algorithm 1: Forest-guided supertagger training
Input: Training Sample (wi, si)i=1,...,N ,
Number of iterations T
1: ? ? (0, ..., 0), ?sum ? (0, ..., 0)
2: for iterNum? 1 to T do
3: for i ? 1 to N do
4: Generate supertag lattice using
the point-wise classifier with current ?
5: Select s?i from the lattice
which can construct a tree
with largest sequence score
6: if( No s?i satisfied grammar constraints)
s?i ? argmaxs?GEN(wi) ?i ? ?(wi, si)
7: if s?i "= si then
8: ?i+1 ? ?i + ?(wi, si)? ?(wi, s?i)
9: ?sum ? ?sum + ?i+1
Return: ?sum/NT
parser more tightly related, which will contribute
towards the performance of the parser.
3.2 Training Algorithm
Based on the motivation above, we propose
a forest-guided supertagger training method to
make the supertagger more tightly integrated with
the parser. This method is based on the averaged
perceptron training algorithm. The training pro-
cess is given in Algorithm 1.
The most important difference of the proposed
algorithm compared to the traditional supertagger
training method is that the current best-scored
supertag sequence is searched only within the
space of the supertag sequences that are allowed
by the grammar. As for whether the grammar
1283
constraints are satisfied, we judge it by whether
a possible syntactic tree can be constructed using
the given supertag sequence. We do not require
the constructed syntactic tree to be identical to the
gold tree in the corpus. For this reason we call it
?forest-guided?.
In the forest-guided training of the supertagger,
an approximating CFG is used to filter out the
supertag sequences from which no well-formed
tree can be built. It is implemented as a best-first
CFG parser wherein the score of a constituent is
the score of the supertag (sub-)sequence on the
fringe of the constituent, which is calculated us-
ing the current value of the parameters. Note that
the best-first parser can find the best-scored super-
tag sequence very efficiently given proper scoring
for the candidate supertag set for each token; this
is actually the case in the course of training except
for the initial phase of the training, wherein the pa-
rameter values are not well-tuned. The efficiency
is due to the sparseness of the approximating CFG
(i.e., the production rule set includes only a tiny
fraction of the possible parent-children combina-
tions of symbols) and highest-scored supertags of-
ten have a well-formed tree on top of them.
As is clear from the above description, the use
of CFG-filter in the forest-guided training of the
supertagger is not essential but is only a subsidiary
technique to make the training faster. The im-
provement by the forest-guided training should
however depend on whether the CFG approxi-
mation is reasonably tight or not. Actually, we
managed to obtain a manageable size out of a
CFG grammar, which includes 80 thousand non-
terminal symbols and 10 million rules, by elimi-
nating only a small number of features (semantics,
case and number agreement, and fine distinctions
in nouns, adjectives and complementizers). We
thus believe that the approximation is fairly tight.
This training algorithm can also be explained
in a search-based learning framework (Hal Daume?
III and Daniel Marcu, 2005). In this framework,
the objective of learning is to optimize the ? for
the enqueue function to make the good hypothe-
ses rank high in the search queue. The rank score
r consists of two components: path score g and
heuristic score h. In the forest-guided training
method, r can be rewritten as follows:
r = g + h
= ? ? ?(x, y?) + [Tree(y?)] ? Penalty (2)
The heuristic part h checks whether the super-
tag candidate sequence satisfies the grammar con-
straints: if no CFG tree can be constructed, -?
penalty is imposed to the candidate sequence in
the forest-guided training method.
4 Experiments
We mainly evaluated the proposed forest-guided
supertagger training method on HPSG parsing.
Supertagging accuracy1 using different training
methods was also investigated.
4.1 Corpus Description
The HPSG grammar used in the experiments is
Enju version 2.32. It is semi-automatically con-
verted from the WSJ portion of PennTreebank
(Miyao, 2006). The grammar consists of 2,308
supertags in total. Sections 02-21 were used to
train different supertagging models and the HPSG
parser. Section 22 and section 23 were used as
the development set and the test set respectively.
We evaluated the HPSG parser performance by la-
beled precision (LP) and labeled recall (LR) of
predicate-argument relations of the parser?s out-
put as in previous works (Miyao, 2005). All ex-
periments were conducted on an AMD Opteron
2.4GHz server.
Template Type Template
Word wi,wi?1,wi+1,
wi?1&wi, wi&wi+1
POS pi, pi?1, pi?2, pi+1,
pi+2, pi?1&pi, pi?2&pi?1,
pi?1&pi+1, pi&pi+1,
pi+1&pi+2
Word-POS pi?1&wi, pi&wi, pi+1&wi
Table 1: Feature templates used for supertagging
models.
1?UNK? supertags are ignored in evaluation as in previ-
ous works.
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
1284
4.2 Baseline Models and Settings
We used a point-wise averaged perceptron (PW)
to train a baseline supertagger. Point-wise classi-
fiers have been reported to be very effective and
with competitive results for the supertagging task
(Clark, 2002; Zhang et al, 2009). The number of
training iterations was set to 5. The features used
in the supertaggers are described in Table 1. For
comparison, these features are identical to the fea-
tures used in the previous works (Matsuzaki et al,
2007; Ninomiya et al, 2007). To make the train-
ing efficient, we set the default chart size limit for
the forest-guided supertagger training to be 20k
by tuning it on the development set.
We combined the supertagger trained under
forest-guidance with a supertagging-based HPSG
parser (Matsuzaki et al, 2007) and evaluated the
contribution of the improved supertagger train-
ing procedure for the final HPSG parsing by the
accuracy of the predicate-argument relations out-
put of the parser. The parser crucially depends
on the supertagger?s performance in that it out-
puts the first well-formed tree successfully con-
structed on the highest scored supertag sequence.
The highest-scored supertag sequences are enu-
merated one by one in descending order in re-
gards to their score. The enumeration is actu-
ally implemented as n-best parsing on the super-
tag candidates using an approximating CFG. The
HPSG tree construction on a supertag sequence is
done using a shift-reduce style parsing algorithm
equipped with a classifier-based action selection
mechanism.
The automatically assigned POS tags were
given by a maximum entropy tagger with roughly
97% accuracy.
4.3 Supertagging Results
Although we mainly focused on improving the fi-
nal HPSG parsing performance through the im-
proved supertagger training, it is also very inter-
esting to investigate the supertagger performance
using different training methods. To evaluate the
forest-guided training method for a supertagger,
we also need to incorporate structure constraints
in the test stage. To make fair comparisons,
for the averaged perceptron trained supertagger
we also add structure constraints in its testing.
Model Name Acc%
FT+CFG 92.77
auto-POS PW+CFG 92.47
PW 91.14
ME 91.45
FT+CFG 93.98
gold-POS PW+CFG 93.70
PW 92.48
ME 92.78
Table 2: Supertagging results in section 23. ?FT?
represents the forest-guided trained supertagger.
?PW? is the baseline average perceptron trained
supertagger. ?ME? is the supertagger trained by
using the maximum entropy method. ?+CFG? in-
dicates the use of the CFG-filter for the super-
tagger results. The accuracy of automatically as-
signed POS tags in this section is 97.39%.
For simplicity, throughout this paper, we call the
forest-guided trained supertagger ?FT? in short,
while the ?PW? is used to represent the base-
line point-wise averaged perceptron supertagger.
?ME? is the re-implemented maximum entropy
supertagger described in Matsuzaki et al (2007).
For the PW supertagger, the performance was
roughly 0.3% below the ME supertagger. Simi-
lar results were reported by Zhang et al (2009),
which used a Bayes point machine to reduce the
gap between the averaged perceptron supertagger
and the maximum entropy supertagger. Although
we expected the ME supertagger using CFG-filter
to give better results than the PW supertagger, im-
plementing forest-guided supertagger training in
a maximum entropy framework is different and
more sophisticated than the current on-line train-
ing method. Considering that the performance of
the PW supertagger and the ME supertagger were
at a similar level, we chose the PW supertagger as
our baseline.
We used a CFG-filter to incorporate global
grammar constraints into both the training and
the testing phase. Compared to the PW super-
tagger, the PW+CFG supertagger incorporated
global grammar constraints only in the test phase,
while for the FT+CFG supertagger, the global
grammar constraints were incorporated both in
1285
!!!!!!!!!!!!!!!!Training Method
Iter NUM
1 2 3 4 5 Total Time
FT 6684s 4189s 3524s 3285s 3086s ? 5.8h
PW 99s 116s 117s 117s 117s ? 10 min
ME / ? 3h
Table 3: Supertagger training time on section 02-21. ?FT? and ?PW? represent forest-guided training
and point-wise averaged perceptron training separately. ?ME? is the point-wise maximum entropy
training reported in Matsuzaki et al (2007).
the training and the testing stage. The super-
tagging accuracy for different models is shown
in Table 2. Firstly, incorporating grammar con-
straints only in the testing phase (PW+CFG) gave
an absolute 1.22% (gold POS) and 1.33% (auto
POS) increase in F-score compared to the PW
supertagger. Secondly, incorporating grammar
constraints into both the training and the testing
stage (FT+CFG) gave an additional 0.28% (gold
POS) and 0.3% (auto POS) improvement over the
PW+CFG supertagger with p-values 0.0018 (gold
POS) and 0.0016 (auto POS).
This also indicates that the supertagger and the
parser are closely related to each other. The orig-
inal motivation for supertagging is using simple
models to resolve lexical ambiguities, which can
efficiently reduce the search space of the parser.
A better supertagger can contribute to more ef-
ficient and more accurate lexicalized grammar
parsing. Actually, a supertagger can act as a
coarse parser for the whole parsing process as
well, as long as the coarse parser is efficient. Since
supertag disambiguation is highly constrained by
the grammar, incorporating grammar constraints
into supertagging (including training and testing)
by using the CFG-filter can further improve the
supertagging performance, as shown in Table 2.
As for the supertagger training time, incorpo-
rating grammar constraints inevitably increases
the training time. As shown in Table 3, the to-
tal training time of forest-guided training (default
settings, with chart size limited to 20k) was about
5.8 hours. For each iteration of the FT model,
we find that the training time gradually decreases
with each successive iteration. This hints that we
can do better model initialization to further reduce
the training time.
4.4 HPSG Parsing Results
We evaluated the HPSG parsers using different
supertagger training methods. For the baseline
HPSG parser, a CFG-filter is already incorporated
to accelerate the parsing process. In the follow-
ing experiments, we fed the parser all the possi-
ble supertag candidates with the prediction scores
generated by the supertaggers. We controlled the
upper bound of the chart size in the CFG-filter to
make the parser more efficient.
Table 4 shows the results of the different pars-
ing models. We first compared the baseline
parsers using different supertaggers. The forest-
guided supertagger improved the final FT parser?s
F-score by 0.68% (statistically significant) over
the PW parser using the PW supertagger, which
did not consider global grammar constraints dur-
ing the supertagger training process. The parsing
time of the FT parser was very close to that of the
PW parser (108s vs. 106s), which was also ef-
ficient. The result empirically reflects that incor-
porating the global grammar constraints into the
supertagger training process can refine supertag
predicting scores, which become more consistent
and compatible with the parser.
We also compared our results with a state-of-
the-art HPSG parser using the same grammar.
Enju (Miyao, 2005; Ninomiya et al, 2007) is
a log-linear model based HPSG parser, which
uses a maximum entropy model for the struc-
ture disambiguation. In contrast to our baseline
parser, full HPSG grammar is directly used with
CKY algorithm in the parsing stage. As for the
parsing performance, our baseline PW parser us-
ing the PW supertagger was 0.23% below the
Enju parser. However, by using the forest-guided
trained supertagger, our improved FT parser per-
1286
Parser UP UR LP LR F-score Time ?
FT Parser 92.28 92.14 89.38 89.23 89.31 108s
PW Parser 91.88 91.63 88.75 88.51 88.63 106s
Enju 2.3 92.26 92.21 88.89 88.84 88.86 775s
Table 4: Parser performance on Section 23. ?FT Parser? represents baseline parser which uses forest-
guided trained supertagger. ?PW Parser? represents the baseline parser which uses the point-wise av-
eraged perceptron trained supertagger. (?) The time is the total time of both supertagging and parsing
and it was calculated on all 2291 sentences of the Section 23.
 85
 85.5
 86
 86.5
 87
 87.5
 88
 88.5
 89
0.5k
1k 1.5k
2k 2.5k
3k 3.5k
4k 10k
15k
20k
H
PS
G
 P
ar
si
ng
 F
-s
co
re
Chart size limit in the parsing
Parser using the PW supertagger
Parser using the 10k-train FT supertagger
Parser using the 20k-train FT supertagger
Figure 2: The F-score of the HPSG parsers on sec-
tion 22 using different settings for the chart size
limit in supertagger training and parsing.
formed 0.45% better than the Enju parser (default
settings) in F-score. In addition, our shift-reduce
style parser was faster than the Enju parser.
Beam size plays an important role for the
forest-guided supertagger training method, since a
larger beam size reduces the possibility of search
errors. Precisely speaking, we control the beam
size by limiting the number of edges in the chart
in both the forest-guided supertagger training pro-
cess and the final parsing. Figure 2 shows the re-
sults of setting different limits for the chart size
during supertagger training and parsing on the de-
velopment set. The X-axis represents the chart
size limitation for the parsing. ?10k-train? rep-
resents the chart size to be limited to 10k dur-
ing FT supertagger training phase. A similar
representation is used for ?20k-train?. There is
no tree structure search process for the baseline
PW supertagger. We evaluated the F-score of the
parsers using different supertaggers. As shown in
Figure 2, when the chart size of the parser was
more than 10k, the benefit of using forest-guided
supertaggers were obvious (around an absolute
0.5% improvement in F-score, compared to the
parser using the baseline PW supertagger). The
performance of the parser using ?10k-train? FT
supertagger was already approaching to that of the
parser using ?20k-train? FT supertagger. When
the chart size of the parser was less than 2000, the
forest-guided supertaggers were not work. Simi-
lar to the results showed in previous research (Hal
Daume? III and Daniel Marcu, 2005), it is better to
use the same chart size limit in the forest-guided
supertagger training and the final parsing.
5 Related Work
Since the supertagging technique is well known
to drastically improve the parsing speed and ac-
curacy, there is work concerned with tightly in-
tegrating a supertagger with a lexicalized gram-
mar parser. Clark and Curran (2004) investigated
a multi-tagger supertagging technique for CCG.
Based on the multi-tagging technique, supertagger
and parser are tightly coupled, in the sense that the
parser requests more supertags if it fails. They
(Clark and Curran, 2007) also used the percep-
tron algorithm to train a CCG parser. Differ-
ent from their work, we focused on improving
the performance of the deep parser by refining
the training method for supertagging. Ninomiya
et al (2007) used the supertagging probabili-
ties as a reference distribution for the log-linear
model for HPSG, which aimed to consistently
integrate supertagging into probabilistic HPSG
parsing. Prins et al (2001) trained a POS-
tagger on an automatic parser-generated lexical
entry corpus as a filter for Dutch HPSG parsing
to improve the parsing speed and accuracy.
1287
The existing work most similar to ours is
Boullier (2003). He presented a non-statistical
parsing-based supertagger for LTAG. Similar to
his method, we used a CFG to approximate the
original lexicalized grammar. The main difference
between these two methods is that we consider
the grammar constraints in the training phase of
the supertagger, not only in the supertagging test
phase and our main objective is to improve the
performance of the final parser.
6 Conclusions and Future Work
In this paper, based on the observation that su-
pertaggers are commonly trained separately from
lexicalized parsers without global grammar con-
straints, we proposed a forest-guided supertagger
training method to integrate supertagging more
tightly with deep parsing. We applied this method
to HPSG parsing and made further significant im-
provement for both supertagging (0.28%) and the
HPSG parsing (0.68%) compared to the baseline.
The improved parser also achieved a competitive
result (89.31%) with a faster parsing speed, com-
pared to a state-of-the-art HPSG parser.
For future work, we will try to weight the for-
est trees for the supertagger training and extend
this method to other lexicalized grammars, such
as LTAG and CCG.
Acknowledgments
We are grateful to the anonymous reviewers for
their valuable comments. We also thank Goran
Topic and Pontus Stenetorp for their help proof-
reading this paper. The first author was sup-
ported by The University of Tokyo Fellowship
(UT-Fellowship). This work was partially sup-
ported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan).
References
Bangalore, Srinivas and Aravind K. Joshi. 1999.
Supertagging: An approach to almost parsing.
Computational Linguistics, 25:237?265.
Birch, Alexandra, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 9?16.
Boullier, P. 2003. Supertagging: A non-statistical
parsing-based approach. In In Proceedings IWPT-
2003, volume 3, pages 55?65.
Chen, John and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling
of semantic arguments. In Proceedings of EMNLP-
2003, pages 41?48.
Clark, Stephen and James R. Curran. 2004. The
importance of supertagging for wide-coverage ccg
parsing. In Proceedings of COLING-04, pages 282?
288.
Clark, S. and J.R. Curran. 2007. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser.
In Proceedings of the Workshop on Deep Linguistic
Processing, pages 9?16.
Clark, Stephen. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th In-
ternational Workshop on Tree Adjoining Grammars
and Related Frameworks (TAG+ 6), pages 19?24.
Collins, M. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP-2002, pages 1?8.
Hal Daume? III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In International
Conference on Machine Learning (ICML), pages
169?176.
Hassan, Hany, Mary Hearne, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of ACL-2007, pages 288?
295.
Kiefer, Bernd and Hans-Ulrich Krieger. 2000. A
context-free approximation of head-driven phrase
structure grammar. In Proceedings of IWPT-2000,
pages 135?146.
Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Super-
tagging and CFG-filtering. In Proceedings of
IJCAI-07, pages 1671?1676.
Miyao, Yusuke. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Pro-
ceedings of the 43rd AnnualMeeting on Association
for Computational Linguistics, pages 83?90.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. Disserta-
tion, The University of Tokyo.
1288
Ninomiya, Takashi, Yoshimasa Tsuruoka, Takuya
Matsuzaki, and Yusuke Miyao. 2006. Extremely
lexicalized models for accurate and fast HPSG pars-
ing. In Proceedings of EMNLP-2006, pages 155?
163.
Ninomiya, T., T. Matsuzaki, Y. Miyao, and J. Tsujii.
2007. A log-linear model with an n-gram reference
distribution for accurate HPSG parsing. In Proceed-
ings of IWPT-2007, pages 60?68.
Pollard, Carl and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University of Chicago
/ CSLI.
Prins, R. and G. Van Noord. 2001. Unsupervised
Pos-Tagging Improves Parsing Accuracy And Pars-
ing Efficiency. In Proceedings of IWPT-2001, pages
154?165.
Zhang, Yao-zhong, Takuya Matsuzaki, and Jun?ichi
Tsujii. 2009. HPSG Supertagging: A Sequence La-
beling View. In Proceedings of IWPT-2009, pages
210?213.
1289
Coling 2010: Poster Volume, pages 1417?1425,
Beijing, August 2010
Semi-automatically Developing Chinese HPSG Grammar from the Penn Chinese Treebank for Deep Parsing 
Kun Yu1 Yusuke Miyao2 Xiangli Wang1 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo 2. National Institute of Informatics yusuke@nii.ac.jp {kunyu, xiangli, matuzaki, tsujii} @is.s.u-tokyo.ac.jp 3. The University of Manchester  Abstract In this paper, we introduce our recent work on Chinese HPSG grammar development through treebank conversion. By manually defining grammatical constraints and anno-tation rules, we convert the bracketing trees in the Penn Chinese Treebank (CTB) to be an HPSG treebank. Then, a large-scale lexi-con is automatically extracted from the HPSG treebank. Experimental results on the CTB 6.0 show that a HPSG lexicon was successfully extracted with 97.24% accu-racy; furthermore, the obtained lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. 1 Introduction Precise, in-depth syntactic and semantic analysis has become important in many NLP applications. Deep parsing provides a way of simultaneously obtaining both the semantic relation and syntac-tic structure. Thus, the method has become more popular among researchers recently (Miyao and Tsujii, 2006; Matsuzaki et al, 2007; Clark and Curran, 2004; Kaplan et al, 2004).  This paper introduces our recent work on deep parsing for Chinese, specifically focusing on the development of a large-scale grammar, based on the HPSG theory (Pollard and Sag, 1994). Be-cause it takes a decade to manually develop an HPSG grammar that achieves sufficient coverage for real-world text, we use a semi-automatic ap-proach, which has successfully been pursued for English (Miyao, 2006; Miyao et al, 2005; Xia, 1999; Hockenmaier and Steedman, 2002; Chen and Shanker, 2000; Chiang, 2000) and other lan-guages (Guo et al, 2007; Cramer and Zhang, 2009; Hockenmaier, 2006; Rehbei and Genabith, 2009; Schluter and Genabith, 2009).  The following lists our method of approach: (1) define a skeleton of the grammar (in this 
work, the structure of sign, grammatical princi-ples and schemas), (2) convert the CTB (Xue et al, 2002) into an HPSG-style treebank, (3) automatically extract a large-scale lexicon from the obtained treebank. Experiments were performed to evaluate the quality of the grammar developed from the CTB 6.0. More than 95% of the sentences in the CTB could be successfully converted, and the ex-tracted lexicon was 97.24% accurate. The ex-tracted lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. Since grammar engineering has many specific problems in each language, although we used the similar method applied in other languages to de-velop a Chinese HPSG grammar, it is very dif-ferent from applying, such as statistical parsing models, to a new language. Lots of efforts have been done for the specific characteristics of Chi-nese. The contribution of our work is to describe these issues. As a result, a skeleton design of Chinese HPSG is proposed, and for the first time, a robust and wide-coverage Chinese HPSG grammar is developed from real-world text.  2 Design of Grammatical Constraints for Chinese HPSG Because of the lack of a comprehensive HPSG-based syntactic theory for Chinese, we extended the original HPSG (Pollard and Sag, 1994) to analyze the specific linguistic phenomena in Chinese. Due to space limitations, we will pro-vide a brief sampling of our extensions, and dis-cuss several selected constructions.  2.1 Sign, Principles, and Schemas Sign, which is a data structure to express gram-matical constraints of words/phrases, is modified and extended for the analysis of Chinese specific constructions, as shown in Figure 1. PHON, MOD, SPEC, SUBJ, MARKING, and SLASH are 
1417
features defined in the original HPSG, and they represent the phonological information of a word, the constraints on the modifiee, the speci-fiee, the subject, the marker, and the long-distance dependency, respectively. COMPS, which represents the constraints on comple-ments, is divided into LCOMPS and RCOMPS, to distinguish between left and right comple-ments. Aspect, question, and negation particles are treated as markers as done in (Gao, 2000), which are distinguished by ASPECT, QUESTION, and NEGATION. CONT is also originated from Pollard and Sag (1994), although it is used to represent semantic structures with predicate-argument dependencies. TOPIC and CONJ are extended features that represent the constraints on the topic and the conjuncts of co-ordination. FILLER is another extended feature that records the grammatical function of the moved argument in a long-distance dependency. 
 Figure 1. HPSG sign for Chinese. The principles, including Phonology Princi-ple, Valence Principle, Head Feature Principle, and Nonlocal Feature Principle, are imple-mented in our Chinese HPSG grammar as de-fined in (Pollard and Sag, 1994). Semantic Principle is slightly modified so that it composes predicate-argument structures. 14 schemas are defined in our grammar, among which the Coord-Empty-Conj Schema, Relative-Head Schema, Empty-Relativizer Schema, and Topic-Head Schema are designed specifically for Chinese. The other 10 schemas are borrowed from the original HPSG theory. 15 Chinese constructions are considered in our current grammar (refer to Table 1). A detailed description of some particular constructions will be provided in the following subsection.  2.2 An HPSG Analysis for Chinese  2.2.1 BA Construction The BA construction moves the object of a verb to the pre-verbal position. For example, the sen-
tence in Figure 2 with the original word order is ??/I ?/read ? ?/book?. There were three popular ways to address the BA construction: as a verb (Huang, 1991; Bender, 2000), preposition (Gao, 1992), and case marker (Gao, 2000). Since the aspect markers, such as ???, cannot attach to BA, we exclude the analysis of treating BA as a verb. Because BA, like prepositions, always ap-pears before a noun phrase, we therefore follow the analysis in Gao (1992), and treat BA as a preposition. As shown in Figure 2, BA takes a moved object as a complement, and attaches to the verb as a left-complement. 
 (I read the book.) Figure 21. Analysis of BA construction. 2.2.2 BEI Construction The BEI construction is used to make the passive voice of a sentence. Because the aspect marker also cannot attach to BEI, we do not treat BEI as a verb, as done in the CTB. Similar to the analy-sis of BA construction, we regard BEI as a preposition that attaches to the verb as a left-complement. Additionally, because we can insert a clause ???/Li ?/send ?/person? between the moved object ??/he? and the verb ??/beat?, as is the case for ??/he ?/BEI ??/Li ?/send ?/person ?/beat ? (He was beaten by the person that is sent by Li)?, we treat the relation between the moved object and the verb as a long-distance dependency. Figure 3 exem-plifies our analysis of the BEI construction, in which the Filler-Head Schema is used to handle the long-distance dependency, and the FILLER feature is used to record that the role of the moved argument. 
 (The book is read by me.)  Figure 3. Analysis of BEI construction.  2.2.3 Topic Construction As indicated in Li and Thompson (1989), a topic refers to the theme of a sentence, which always                                                            1 In the figures in this paper, we will show only selected features that are relevant to the explanation.  
1418
appears before the subject. The difference be-tween the topic and subject is the subject must always have a direct semantic relationship with the verb in a sentence, whereas the topic does not. There are two types of topic constructions. In the first type, the topic does not fill any argu-ment slots of the verb, such as the topic ???/elephant? in Figure 4. In the second type, the topic has a semantic relationship with the verb. For example, in the sentence ??/he ?/I ??/like (I like him)?, the topic ??/he? is also an object of ???/like?. For the first type, we define the Topic-Head Schema to describe the topic construction (refer to Figure 4). For the second type, we follow the same analysis as in English, and use the Filler-Head Schema.   
 (The nose of an elephant is long.) Figure 4. Analysis of topic construction. 2.2.4 Serial Verb Construction In contrast to the definition of serial verb con-struction in Li and Thompson (1989), we specify a serial verb construction as a special type of verb phrase coordination, which describes sev-eral separate events with no conjunctions inside. Similar to ordinary coordination, the verb phrases in a serial verb construction share the same syntactic subject (Muller and Lipenkova, 2009), topic, and left-complement. We define Coord-Empty-Conj Schema to deal with it. Fig-ure 5 shows an example analysis. 
 (I go to the book store and buy a book.) Figure 5. Analysis of serial verb construction. 2.2.5 Relative Clause In Chinese, a relative clause is marked by a rela-tivizer ??? and exists in the left of the head noun. Because Chinese noun phrases are right-headed in general, we analyze a relative clause as a nominalization that modifies a head noun (Li and Thompson, 1989). Inside of a relative clause, the relativizer is treated as head. When the rela-tivizer is omitted, we define a unary schema, Empty-Relativizer Schema, which functions by combining a relative clause with an empty rela-
tivizer. Furthermore, we introduce a Relative-Head Schema to handle the long-distance de-pendency for the extracted argument2 (refer to Figure 6).  
 (the book that I buy) Figure 6. Analysis of relative clause. 3 Converting the CTB into an HPSG Treebank 3.1 Partially-specified Derivation Tree Annotation In order to convert the CTB into an HPSG tree-bank, we first annotate the bracketing trees in the CTB to be partially-specified derivation trees3, which conform to the grammatical constraints designed in Section 2. Three types of rules are defined to fulfill this annotation. 
 (I read the book that he wrote.) Figure 7. The CTB annotation for a sentence. 
 Figure 8. Partially-specified derivation tree for Figure 7. For example, Figure 7 shows the bracketing tree of a sentence in the CTB, while Figure 8 shows the partially-specified derivation tree after re-annotation.                                                            2 The extracted adjunct is not treated as a long-distance dependency in our current grammar. 3 Partially-specified derivation tree means a tree structure that is annotated with schema names and some features of the HPSG signs (Miyao, 2006). 
1419
3.1.1 Rules for Annotation Conversion  In the CTB, there exist some annotations that do not coincide with our HPSG analysis for Chi-nese. Therefore, we define pattern rules to con-vert the annotations in the CTB to fit with our HPSG analysis. 76 annotation rules are defined for 15 Chinese constructions (refer to Table 2). Due to page constraints, we focus on the constructions that we discussed in Section 2. Construction Rule # Relative clause 20 BEI construction 21 Coordination 7 Subject/object control 5 Non-verbal predicate 4 Logical subject 3 Right node raising 3 Parenthesis 3 BA construction 3 Aspect/question/negation particle 2 Subordination 1 Serial Verb construction 1 Modal verb 1 Topic construction 1 Apposition 1 Table 1. Chinese constructions and annotation rules. Rules for BA and BEI Construction As analyzed in Section 2, we treat BA and BEI as prepositions that attach to the verb as left-complements. However, in the CTB, BA and BEI are annotated as verbs that take a sentential complement (Xue and Xia, 2000). By applying the annotation rules, the BA/BEI and the subject of the sentential complement of BA/BEI are re-annotated as a prepositional phrase (as indicated in the dash-boxed part in Figure 9). 
 (I read the book.) Figure 9. Conversion of BA construction. 
 (He is regarded as a friend by me.) Figure 10. Verb division in BEI construction. In addition, in the CTB, some BA/BEI con-structions are not annotated with trace, which 
makes it difficult to retrieve the semantic relation between the verb and the moved object. The principal reason for this is that the moved object in these constructions has a semantic relation with only part of the verb. For example, in Fig-ure 10, the moved noun ??/he? is the object of ??/regard?, but not for ???/regard as?. Analy-sis shows that only a closed set of characters (e.g. ??/as?)  can be attached to verbs in such a case. Therefore, we manually collect these char-acters from the CTB, and then define pattern rules to automatically split the verb, which ends with the collected characters, in the BA and BEI construction. Finally, we annotate trace for the split verb. Figure 10 exemplifies the conversion of an example sentence. Rules for Topic Construction In the CTB, a functional tag ?TPC? is used to indicate a topic (Xue and Xia, 2000). Therefore, we use this functional tag to detect topic phrases during conversion. Rules for Serial Verb Construction We define pattern rules to detect the parallel verb phrases with no conjunction inside (as shown in Figure 11), and treat these verb phrases as a se-rial verb construction. However, when the verb in the first phrase is a modal verb, such as the case of ??/I ?/want to ??/sing (I want to sing)?, the parallel verb phrases should not be treated as a serial verb construction. Therefore, a list of modal verbs is manually collected from the CTB to filter out these exceptional cases dur-ing conversion.  
 (go downstairs and eat meal) Figure 11. An example of parallel verb phrases. Rules for Relative Clause  
 (the book that he wrote) Figure 12. Conversion of relative clause. We define annotation rules to slightly modify the annotation of a relative clause in CTB, as shown in Figure 12, to make the tree structure easy to be 
1420
analyzed. Furthermore, in CTB, relative clauses are annotated with both extracted arguments and extracted adjuncts. But in our grammar, we only deal with extracted arguments, and the gap in a relative clause (as indicated in the dash-boxed part in Figure 12). When the extracted phrase is an adjunct of the relative clause, we simply view the clause as a modifier of the extracted phrase. 3.1.2 Rules for Correcting Inconsistency  There are some inconsistencies in the annotation of the CTB, which presents difficulties for per-forming the derivation tree annotation. There-fore, we define 49 rules, as done in (Hockenmaier and Steedman, 2002) for English, to mitigate inconsistencies before annotation (re-fer to Table 3).  3.1.3 Rules for Assisting Annotation We also define 48 rules (refer to Table 2), which are similar to the rules used in (Miyao, 2006) for English, to help the derivation tree annotation. For example, 12 pattern rules are defined to as-sign the schemas to corresponding constituents. Rule Type Rule Description Rule # Fix tree annotation 37 Fix phrase tag annotation 5 Fix functional tag annotation 5 Rules for correcting inconsistent annotation Fix POS tag annotation 2 Slash recognization 27 Schema assignment 12 Head/Argument/Modifier marking 8 Rules for assisting  annotation Binarization 1 Table 2. Rules for correcting inconsistency and assisting annotation. 3.2 HPSG Treebank Acquisition In this phase, the schemas and principles are ap-plied to the annotated partially-specified trees, in order to fill out unspecified constraints and vali-date the consistency of the annotated constraints. In effect, an HPSG treebank is obtained. For instance, by applying the Head-Complement Schema to the dash-boxed nodes in Figure 8, the constraints of the right daughter are percolated to RCOMPS of the left daughter (as indicated as 4 in Figure 13). After applying the schemas and the principles to the whole tree in Figure 8, a HPSG derivation tree is acquired (re-fer to Figure 13).  3.3 Lexicon Extraction  With the HPSG treebank acquired in Section 3.2, we automatically collect lexical entries as the combination of words and lexical entry templates from the terminal nodes of the derivation trees. For example, from the HPSG derivation tree 
shown in Figure 13, we obtain a lexical entry for the word ??/write? as shown in Figure 14. 
 Figure 13. HPSG derivation tree for Figure 8. 
 Figure 14. Lexical entry extracted for the word ??/write?. 3.3.1 Lexical Entry Template Expansion 
 (a) Lexical entry template for the verb in BEI construction 
 (b) Lexical entry template for the verb in original word order Figure 15. Application of a lexical rule. Some Chinese constructions change the word order of sentences, such as the BA/BEI construc-tions. Therefore, we apply lexical rules (Naka-nishi et al, 2004) to the lexical entry templates to convert them into those for the original word order, and expand the lexical entry templates consequently. 18 lexical rules are defined for the verbs in the BA/BEI constructions. For example, by applying a lexical rule to the lexical entry template in Figure 15(a), the moved object indi-
1421
cated by SLASH is restored into RCOMPS, and the subject introduced by BEI in LCOMPS is restored into SUBJ (refer to Figure 15(b)). 3.3.2 Mapping of Semantics In our grammar, we use predicate-argument de-pendencies for semantic representation. 44 types of predicate-argument relations are defined to represent the semantic structures of 13 classes of words. For example, we define a predicate-argument relation ?verb_arg12?, in which a verb takes two arguments ?ARG1? and ?ARG2?, to ex-press the semantics of transitive verbs. 72 se-mantics mapping rules are defined to associate these predicate-argument relations with the lexi-cal entry templates. Figure 16 exemplifies a se-mantics mapping rule. The input of this rule is the lexical entry template (as shown in the left part), and the output is a predicate-argument rela-tion ?verb_arg12? (as shown in the right part), which associates the syntactic arguments SUBJ and SLASH with the semantic arguments ARG1 and ARG2 (as indicated by 1 and 2 in Figure 16). 
 Figure 16. A semantics mapping rule. 4 Evaluation 4.1 Experimental Setting We used the CTB 6.0 for HPSG grammar devel-opment and evaluation. We split the corpus into development, testing, and training data sets, fol-lowing the recommendation from the corpus author. The development data was used to tune the design of grammar constraints and the anno-tation rules. However, the testing data set was reserved for further evaluation on parsing. Thus, the training data was further divided into two parts for training and testing in this work. During the evaluation, unknown words were handled in the same way as done in (Hockenmaier and Steedman, 2002).  4.2 Evaluation Metrics In order to verify the quality of the grammar de-veloped in our work, we evaluated the extracted lexicon by the accuracy for assessing the semi-automatic conversion process, and the coverage for quantifying the upper-bound coverage of the future HPSG parser based on this grammar.  The accuracy of the extracted lexicon was evaluated by lexical accuracy, which counts the 
number of the correct lexical entries among all the obtained lexical entries.  In addition, two evaluation metrics as used in (Hockenmaier and Steedman, 2002; Xia, 1999; Miyao, 2006) were used to evaluate the coverage of the obtained lexicon. The first one is lexical coverage (Hockenmaier and Steedman, 2002; Xia, 1999), which means that the percentage that the lexical entries extracted from the testing data are covered by the lexical entries acquired from the training data. The second one is sentential coverage (Miyao, 2006): a sentence is consid-ered to be covered only when the lexical entries of all the words in this sentence are covered.  4.3 Results of Accuracy Since there was no gold standard data for the automatic evaluation of accuracy, we randomly selected 100 sentences from the testing data, and manually checked the lexical entries extracted from these sentences. Results show that 1,558 lexical entries were extracted at 97.24% (1,515/1,558) accuracy.  Error analysis shows all the incorrect lexical entries came from the error in the derivation tree annotation. For example, our current design failed to find the correct boundary of coordinated noun phrases when the word ??/etc? was at-tached at the end, such as ???/property right ??/selling ? ??/assets ??/renting ?/etc (property right selling and assets renting etc.)?. We will improve the derivation tree anno-tation to solve this issue. 4.4 Results of Coverage Table 3 shows the coverage of the extracted lexi-cal entries, which indicates that a large HPSG lexicon was successfully extracted from the CTB for unseen text, with reasonable coverage. The statistics of the HPSG lexicon extraction in our experiments (refer to Table 4) also indicates that we successfully extracted lexical entries from more than 95% of the sentences in the CTB.  Among all the uncovered lexical entries, 78.55% are for content words, such as verb and noun. In addition, the classification of uncovered lexical entries in Table 4 indicates that about 1/3 of the uncovered lexical entries came from the unknown lexical entry templates (?+w/-t?). We analyzed the 193 ?+w/-t? failures in the testing data, among which 169 failures resulted from the shortage of training data, which indicated that the correct lexical entry template did not appear in 
1422
the training data. The learning curve in Figure 17 shows that we can resolve this issue by enlarging the training data. The other 24 failures came from the error in the derivation tree annotation. For example, our current grammar failed at de-tecting the coordinated clauses when they were separated by a colon. We will be able to reduce this type of failure by improving the derivation tree annotation. Uncovered Lexical Entries Sent. Cov. Lex. Cov. +w/+t +w/-t 76.51% 98.51% 1.05% 0.43% Table 34. Coverage of extracted HPSG lexicon. Data Set Total Sent # Succeed Sent # Word # Lexical Entry Template # Training 20,230 19,257(95.19%) 510,815 4,836 Develop 2,067 2,009(97.19%) 55,714 1,582 Testing 2,000 1,941(97.05%) 44,924 1,163 Table 4. Statistics of HPSG lexicon extraction.  
 Figure 17. Lexical coverage (Y axis) vs. corpus size (X axis). 
 Figure 18. A lexical entry template extracted from testing data. The other type of failures (?+w/+t?) indicate that a word was incorrectly associated with a lexical entry template, even though both of them existed in the training data. Error analysis shows that 64.39% of failures were related to verbs. For example, for a relative clause ???/invest ??/Taiwan ? ??/businessman (the busi-nessman that invests Taiwan)? in the testing data, we associated a lexical entry template as shown in Figure 18 with the verb ???/invest?. In the training data, however, the lexical entry template shown in Figure 18 cannot be extracted for ???/invest?, since this word never appears in a relative clause with an extracted subject. Intro-ducing lexical rules to expand the lexical entry template of verbs in a relative clause is a possible way to solve this problem. 4.5 Comparison with Previous Work Guo?s work (Guo et al, 2007; Guo, 2009) is the only previous work on Chinese lexicalized                                                            4 ?+w/+t? means both the word and lexical entry template have been seen in the lexicon. ?+w/-t? means only the word has been seen in the lexicon (Hockenmaier and Steedman, 2002). 
grammar development from the CTB, which in-duced wide-coverage LFG resources from the CTB. By using the hand-made gold-standard f-structures of 200 sentences from the CTB 5.1, the LFG f-structures developed in Guo?s work achieved 96.34% precision and 96.46% recall for unseen text (Guo, 2009). In our work, we applied the similar strategy in evaluating the accuracy of the developed Chinese HPSG grammar, which achieved 97.24% lexical accuracy on 100 unseen sentences from the CTB 6.0. When evaluating the coverage of our grammar, we used a much larger data set (including 2,000 unseen sen-tences), and achieved 98.51% lexical coverage. Although these results cannot be compared to Guo?s work directly because of the different size and content of data set, it indicates that the Chi-nese HPSG grammar developed in our work is comparable in quality with Guo?s work. In addition, there were previous works about developing lexicalized grammar for English. Considering the small size of the CTB, in com-parison to the Penn Treebank used in the previ-ous works, the results listed in Table 5 verify that, the quality of the Chinese HPSG grammar developed in our work is comparable to these previous works.  Previous Work Sent. Cov. Lex. Cov. Miyao (2006) 82.50% 98.97% Hockenmaier and Steedman (2002) - 98.50% Xia (1999) - 96.20% Table 5. Evaluation results of previous work.  4.6 Discussion There are still some sentences in the CTB from which we failed to extract lexical entries. We analyzed the 59 failed sentences in the testing data and listed the reasons in Table 6.  Reason Sent # Error in the derivation tree annotation 31 Short of semantics mapping rule 23 Inconsistent annotation in the CTB 5 Table 6. Reasons for lexicon extraction failures. The principal reason for 31 sentence failures, is the error in the derivation tree annotation. For instance, our current annotation rules could con-vert the regular relative clause shown in Figure 12. Nonetheless, when the relative clause is in-side of a parenthesis, such as ?? ??/primitive ? ???/method (the method that is primi-tive)?, the annotation rules failed at finding the extracted head noun to create a derivation tree. This type of failure can be reduced by improving the annotation rules. 
1423
The second reason, for which 23 sentences failed, is the shortage of the semantics mapping rules. For example, we did not define semantics mapping rule for a classifier that acts as a predi-cate with two topics. This type of failure can be reduced by adding semantic mapping rules.  The last reason for sentence failures is incon-sistencies in the CTB annotation. In our future work, these inconsistencies will be collected to enrich our inconsistency correction rules.  In addition to the reasons above, some sen-tences with special constructions in the devel-opment and training data also could not be analyzed by our current grammar, since the spe-cial construction is difficult for the current HPSG to analyze. The special constructions include the argument-cluster coordination shown in Figure 19. Introducing the similar rules used in CCG (Hockenmaier and Steedman, 2002) could be a possible solution to this problem. 
 (have 177 intrant projects and 6.4 billion investments) Figure 19. An argument-cluster coordination in CTB. 5 Related Work  To the extent of our knowledge, the only previ-ous work about developing Chinese lexicalized grammar from treebanks is Guo?s work (Guo et al, 2007; Guo, 2009). An LFG-based parsing using wide-coverage LFG approximations in-duced from the CTB was done in this work. However, they did not train a deep parser based on the LFG resources obtained in their work, but relied on an external PCFG parser to create c-structure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). In contrast to Guo?s work, we paid particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other lan-guages. For example, by using the Penn Tree-bank, Miyao et al (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG 
specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) ac-quired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained wide-coverage LFG resources from a French Tree-bank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a compara-tive result to state-of-the-art works for English.  There are some researchers who worked on Chinese HPSG grammar development manually. Zhang (2004) implemented a Chinese HPSG grammar using the LinGO Grammar matrix (Bender et al, 2002). Only a few basic construc-tions were considered, and a small lexicon was constructed in this work. Li (1997) and Wang et al (2009) designed frameworks for Chinese HPSG grammar; however, only small grammars were implemented in these works. Furthermore, some linguistic works focused mainly on the discussion of specific Chinese constructions in the HPSG or LFG framework, without implementing a grammar for real-world text (Bender, 2000; Gao, 2000; Li and McFe-tridge, 1995; Li, 1995; Xue and McFetridge, 1995; Wang and Liu, 2007; Ng, 1997; Muller and Lipenkova, 2009; Liu, 1996; Kit, 1998). 6 Conclusion and Future Work  In this paper, we described the semi-automatic development of a Chinese HPSG grammar from the CTB. Grammatical constraints are first de-signed by hand. Then, we convert the bracketing trees in the CTB into an HPSG treebank, by us-ing pre-defined annotation rules. Lastly, we automatically extract lexical entries from the HPSG treebank. We evaluated our work on the CTB 6.0. Results indicated that a large HPSG lexicon was successfully extracted with a 97.24% accuracy. Furthermore, our grammar achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text.  This is an ongoing work, and there are some future works under consideration, including en-riching the design of annotation rules, introduc-ing more semantics mapping rules, and adding lexical rules. In addition, the work on Chinese HPSG parsing is on-going, within which the Chinese HPSG grammar developed in this work will be available soon. 
1424
References  Emily Bender. 2000. The Syntax of Madarin Ba: Reconsid-ering the Verbal Analysis. Journal of East Asian Lin-guistics. 9(2): 105-145. Emily Bender, Dan Flickinger, and Stephan Oepen. 2002. The Grammar Matrix: An Open-source Starter-lit for the Rapid Development of Cross-linguistically Consistent Broad-coverage Precision Grammars. Procedings of the Workshop on Grammar Engineering and Evaluation. John Chen and Vijay K. Shanker. 2004. Automated Extrac-tion of TAGs from the Penn Treebank. Proceedings of the 6th IWPT. David Chiang. 2000. Statistical Parsing with an Automati-cally-extracted Tree Adjoining Grammar. Proceedings of the 38th ACL. 456-463. Stephen Clark and James R. Curran. 2004. Parsing the WSJ Using CCG and Log-linear Models. Proceedings of the 42nd ACL. Bart Cramer and Yi Zhang. 2009. Construction of a German HPSG Grammar from a Detailed Treebank. Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks. Qian Gao. 1992. Chinese Ba Construction: its Syntax and Semantics. Technical report.  Qian Gao. 2000. Argument Structure, HPSG and Chinese Grammar. Ph.D. Thesis. Ohio State University. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. The-sis. Dublin City University. Yuqing Guo, Josef van Genabith and Haifeng Wang. 2007. Acquisition of Wide-Coverage, Robust, Probabilistic Lexical-Functional Grammar Resources for Chinese. Proceedings of the 12th International Lexical Functional Grammar Conference (LFG 2007). 214-232. Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German Proceedings of COLING/ACL 2006. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. C-R Huang. 1991. Madarin Chinese and the Lexical Map-ping Theory: A Study of the Interaction of Morphology and Argument Changing. Bulletin of the Institute of His-tory and Philosophy 62. Ronald M. Kaplan et al 2004. Speed and Accuracy in Shal-low and Deep Stochastic Parsing. Proceedings of HLT/NAACL 2004. Chunyu Kit. 1998. Ba and Bei as Multi-valence Preposi-tions in Chinese. Studia Linguistica Sinica: 497-522.  Wei Li. 1995. Esperanto Inflection and its Interface in HPSG. Proceedings of the 11th North West Linguistics Conference. Wei Li. 1997. Outline of an HPSG-style Chinese Reversible Grammar. Proceedings of the 13th North West Linguis-tics Conference. Wei Li and Paul McFetridge. 1995. Handling Chinese NP Predicate in HPSG. Proceedings of PACLING-II. Charles N. Li and Sandra A. Thompson. 1989. Mandarin Chinese: A Functional Reference Grammar. University of California Press, London, England. 
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2007. Efficient HPSG Parsing with Supertagging and CFG-filtering. Proceedings of the 20th IJCAI. Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D. Thesis. The University of Tokyo. Yusuke Miyao, Takashi Ninomiya and Junichi Tsujii. 2005. Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. Natural Language Processing - IJCNLP 2005: 684-693. Yusuke Miyao and Junichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics. 34(1): 35-80. Stefan Muller and Janna Lipenkova. 2009. Serial Verb Con-structions in Chinese: A HPSG Account. Proceedings of the 16th International Conference on Head-Driven Phrase Structure Grammar. 234-254. Hiroko Nakanishi, Yusuke Miyao and Junichi Tsujii. 2004. An Empirical Investigation of the Effect of Lexical Rules on Parsing with a Treebank Grammar. Proceed-ings of the 3rd TLT. 103-114. Say K. Ng. 1997. A Double-specifier Account of Chinese NPs Using Head-driven Phrase Structure Grammar. Master Thesis. Department of Linguistics, University of Edinburgh. Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. The University of Chicago Press and CSLI Publications, Chicago, IL and Stanford, CA. Ines Rehbein and Josef van Genabith. 2009. Automatic Acquisition of LFG Resources for German ? As Good as it Gets. Proceedings of the 14th International Lexical Functional Grammar Conference (LFG 2009). Natalie Schluter and Josef van Genabith. 2008. Treebank-based Acquisition of LFG Parsing Resources for French. Proceedings of the 6th LREC. Mark Steedman. 2000. The Syntactic Process. The MIT Press. Xiangli Wang et al 2009. Design of Chinese HPSG Frame-work for Data-driven Parsing. Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation. Lulu Wang and Haitao Liu. 2007. A Description of Chinese NPs Using Head-driven Phrase Structure Grammar. Pro-ceedings of the 14th International Conference on Head-Driven Phrase Structure Grammar. 287-305. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-scale Annotated Chinese Corpus. Pro-ceedings of COLING 2002. Ping Xue and Paul McFetridge. 1995. DP Structure, HPSG, and the Chinese NP. Proceedings of the 14th Annual Conference of Canadian Linguistics Association. Nianwen Xue and Fei Xia. 2000. The Bracketing Guidelines for the Penn Chinese Treebank. Yi Zhang. 2004. Starting to Implement Chinese Resource Grammar using LKB and LinGO Grammar Matrix. Technical report.   
1425
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 430?438,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Coordination Structure Analysis using Dual Decomposition
Atsushi Hanamoto 1 Takuya Matsuzaki 1
1. Department of Computer Science, University of Tokyo, Japan
2. Web Search & Mining Group, Microsoft Research Asia, China
{hanamoto, matuzaki}@is.s.u-tokyo.ac.jp
jtsujii@microsoft.com
Jun?ichi Tsujii 2
Abstract
Coordination disambiguation remains a dif-
ficult sub-problem in parsing despite the
frequency and importance of coordination
structures. We propose a method for disam-
biguating coordination structures. In this
method, dual decomposition is used as a
framework to take advantage of both HPSG
parsing and coordinate structure analysis
with alignment-based local features. We
evaluate the performance of the proposed
method on the Genia corpus and the Wall
Street Journal portion of the Penn Tree-
bank. Results show it increases the per-
centage of sentences in which coordination
structures are detected correctly, compared
with each of the two algorithms alone.
1 Introduction
Coordination structures often give syntactic ambi-
guity in natural language. Although a wrong anal-
ysis of a coordination structure often leads to a
totally garbled parsing result, coordination disam-
biguation remains a difficult sub-problem in pars-
ing, even for state-of-the-art parsers.
One approach to solve this problem is a gram-
matical approach. This approach, however, of-
ten fails in noun and adjective coordinations be-
cause there are many possible structures in these
coordinations that are grammatically correct. For
example, a noun sequence of the form ?n0 n1
and n2 n3? has as many as five possible struc-
tures (Resnik, 1999). Therefore, a grammatical
approach is not sufficient to disambiguate coor-
dination structures. In fact, the Stanford parser
(Klein and Manning, 2003) and Enju (Miyao and
Tsujii, 2004) fail to disambiguate a sentence I am
a freshman advertising and marketing major. Ta-
ble 1 shows the output from them and the correct
coordination structure.
The coordination structure above is obvious to
humans because there is a symmetry of conjuncts
(-ing) in the sentence. Coordination structures of-
ten have such structural and semantic symmetry
of conjuncts. One approach is to capture local
symmetry of conjuncts. However, this approach
fails in VP and sentential coordinations, which
can easily be detected by a grammatical approach.
This is because conjuncts in these coordinations
do not necessarily have local symmetry.
It is therefore natural to think that consider-
ing both the syntax and local symmetry of con-
juncts would lead to a more accurate analysis.
However, it is difficult to consider both of them
in a dynamic programming algorithm, which has
been often used for each of them, because it ex-
plodes the computational and implementational
complexity. Thus, previous studies on coordina-
tion disambiguation often dealt only with a re-
stricted form of coordination (e.g. noun phrases)
or used a heuristic approach for simplicity.
In this paper, we present a statistical analysis
model for coordination disambiguation that uses
the dual decomposition as a framework. We con-
sider both of the syntax, and structural and se-
mantic symmetry of conjuncts so that it outper-
forms existing methods that consider only either
of them. Moreover, it is still simple and requires
onlyO(n4) time per iteration, where n is the num-
ber of words in a sentence. This is equal to that
of coordination structure analysis with alignment-
based local features. The overall system still has a
quite simple structure because we need just slight
modifications of existing models in this approach,
430
Stanford parser/Enju
I am a ( freshman advertising ) and (
marketing major )
Correct coordination structure
I am a freshman ( ( advertising and mar-
keting ) major )
Table 1: Output from the Stanford parser, Enju and the
correct coordination structure
so we can easily add other modules or features for
future.
The structure of this paper is as follows. First,
we describe three basic methods required in the
technique we propose: 1) coordination structure
analysis with alignment-based local features, 2)
HPSG parsing, and 3) dual decomposition. Fi-
nally, we show experimental results that demon-
strate the effectiveness of our approach. We com-
pare three methods: coordination structure anal-
ysis with alignment-based local features, HPSG
parsing, and the dual-decomposition-based ap-
proach that combines both.
2 Related Work
Many previous studies for coordination disam-
biguation have focused on a particular type of NP
coordination (Hogan, 2007). Resnik (1999) dis-
ambiguated coordination structures by using se-
mantic similarity of the conjuncts in a taxonomy.
He dealt with two kinds of patterns, [n0 n1 and
n2 n3] and [n1 and n2 n3], where ni are all nouns.
He detected coordination structures based on sim-
ilarity of form, meaning and conceptual associa-
tion between n1 and n2 and between n1 and n3.
Nakov and Hearst (2005) used the Web as a train-
ing set and applied it to a task that is similar to
Resnik?s.
In terms of integrating coordination disam-
biguation with an existing parsing model, our ap-
proach resembles the approach by Hogan (2007).
She detected noun phrase coordinations by find-
ing symmetry in conjunct structure and the depen-
dency between the lexical heads of the conjuncts.
They are used to rerank the n-best outputs of the
Bikel parser (2004), whereas two models interact
with each other in our method.
Shimbo and Hara (2007) proposed an
alignment-based method for detecting and dis-
ambiguating non-nested coordination structures.
They disambiguated coordination structures
based on the edit distance between two conjuncts.
Hara et al(2009) extended the method, dealing
with nested coordinations as well. We used their
method as one of the two sub-models.
3 Background
3.1 Coordination structure analysis with
alignment-based local features
Coordination structure analysis with alignment-
based local features (Hara et al 2009) is a hy-
brid approach to coordination disambiguation that
combines a simple grammar to ensure consistent
global structure of coordinations in a sentence,
and features based on sequence alignment to cap-
ture local symmetry of conjuncts. In this section,
we describe the method briefly.
A sentence is denoted byx = x1...xk, where xi
is the i-th word of x. A coordination boundaries
set is denoted by y = y1...yk, where
yi =
?
?
?
?
?
?
?
?
?
?
?
(bl, el, br, er) (if xi is a coordinating
conjunction having left
conjunct xbl ...xel and
right conjunct xbr ...xer)
null (otherwise)
In other words, yi has a non-null value
only when it is a coordinating conjunction.
For example, a sentence I bought books and
stationary has a coordination boundaries set
(null, null, null, (3, 3, 5, 5), null).
The score of a coordination boundaries set is
defined as the sum of score of all coordinating
conjunctions in the sentence.
score(x,y) =
k
?
m=1
score(x, ym)
=
k
?
m=1
w ? f(x, ym) (1)
where f(x, ym) is a real-valued feature vector of
the coordination conjunct xm. We used almost the
same feature set as Hara et al(2009): namely, the
surface word, part-of-speech, suffix and prefix of
the words, and their combinations. We used the
averaged perceptron to tune the weight vector w.
Hara et al(2009) proposed to use a context-
free grammar to find a properly nested coordina-
tion structure. That is, the scoring function Eq (1)
431
COORD Coordination.
CJT Conjunct.
N Non-coordination.
CC Coordinating conjunction like ?and?.
W Any word.
Table 2: Non-terminals
Rules for coordinations:
COORDi,m ? CJTi,jCCj+1,k?1CJTk,m
Rules for conjuncts:
CJTi,j ? (COORD|N)i,j
Rules for non-coordinations:
Ni,k ? COORDi,jNj+1,k
Ni,j ?Wi,i(COORD|N)i+1,j
Ni,i ?Wi,i
Rules for pre-terminals:
CCi,i ? (and|or|but|, |; |+|+/?)i
CCi,i+1 ? (, |; )i(and|or|but)i+1
CCi,i+2 ? (as)i(well)i+1(as)i+2
Wi,i ? ?i
Table 3: Production rules
is only defined on the coordination structures that
are licensed by the grammar. We only slightly ex-
tended their grammar for convering more variety
of coordinating conjunctions.
Table 2 and Table 3 show the non-terminals and
production rules used in the model. The only ob-
jective of the grammar is to ensure the consistency
of two or more coordinations in a sentence, which
means for any two coordinations they must be ei-
ther non-overlapping or nested coordinations. We
use a bottom-up chart parsing algorithm to out-
put the coordination boundaries with the highest
score. Note that these production rules don?t need
to be isomorphic to those of HPSG parsing and
actually they aren?t. This is because the two meth-
ods interact only through dual decomposition and
the search spaces defined by the methods are con-
sidered separately.
This method requires O(n4) time, where n is
the number of words. This is because there are
O(n2) possible coordination structures in a sen-
tence, and the method requires O(n2) time to get
a feature vector of each coordination structure.
3.2 HPSG parsing
HPSG (Pollard and Sag, 1994) is one of the
linguistic theories based on lexicalized grammar
sign
PHON list of string
SYNSEM
synsem
LOCAL
local
CAT
category
HEAD
head
MODL synsem
MODR synsem
SUBJ list of synsem
COMPS list of synsem
SEM semantics
NONLOC
nonlocal
REL list of local
SLASH list of local
Figure 1: HPSG sign
2SUBJ    < >COMPS   < >2
HEAD
SUBJ    <    >
COMPS   < >
1
HEAD
SUBJ    < >
COMPS  < >
1
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
Figure 2: Subject-Head Schema (left) and Head-
Complement Schema (right)
and unbounded dependencies. SEM feature rep-
resents the semantics of a constituent, and in this
study it expresses a predicate-argument structure.
Figure 2 presents the Subject-Head Schema
and the Head-Complement Schema1 defined in
(Pollard and Sag, 1994). In order to express gen-
eral constraints, schemata only provide sharing of
feature values, and no instantiated values.
Figure 3 has an example of HPSG parsing
of the sentence ?Spring has come.? First, each
of the lexical entries for ?has? and ?come? are
unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
phrasal sign of the entire sentence is output on the
top of the derivation tree.
3 Acquiring HPSG from the Penn
Treebank
As discussed in Section 1, our grammar devel-
opment requires each sentence to be annotated
with i) a history of rule applications, and ii) ad-
ditional annotations to make the grammar rules
be pseudo-injective. In HPSG, a history of rule
applications is represented by a tree annotated
with schema names. Additional annotations are
1The value of category has been presented for simplicity,
while the other portions of the sign have been omitted.
Spring
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >
COMPS <                        >
5
has
HEAD  verb
SUBJ  <                         >
COMPS  < >
come
HEAD  verb
SUBJ  <    >
COMPS  < >
5
HEAD  noun
SUBJ < >
COMPS  < >
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
UnifyUnify
Head-complement
schema
Lexical entries
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 3: HPSG parsing
required because HPSG schemata are not injec-
tive, i.e., daughters? signs cannot be uniquely de-
termined given the mother. The following annota-
tions are at least required. First, the HEAD feature
of each non-head daughter must be specified since
this is not percolated to the mother sign. Second,
SLASH/REL features are required as described in
our previous study (Miyao et al 2003a). Finally,
the SUBJ feature of the complement daughter in
the Head-Complement Schema must be specified
since this schema may subcategorize an unsatu-
rated constituent, i.e., a constituent with a non-
empty SUBJ feature. When the corpus is anno-
tated with at least these features, the lexical en-
tries required to explain the sentence are uniquely
determined. In this study, we define partially-
specified derivation trees as tree structures anno-
tated with schema names and HPSG signs includ-
ing the specifications of the above features.
We describe the process of grammar develop-
ment in terms of the four phases: specification,
externalization, extraction, and verification.
3.1 Specification
General grammatical constraints are defined in
this phase, and in HPSG, they are represented
through the design of the sign and schemata. Fig-
ure 1 shows the definition for the typed feature
structure of a sign used in this study. Some more
features are defined for each syntactic category al-
Figure 1: subject-head schema (left) and head-
complement schema (right); taken from Miyao et al
(2004).
formalism. In a lexicalized grammar, quite a
small numbers of schemata are used to explain
general grammatical constraints, compared with
other theories. On the other hand, rich word-
specific characteristics are embedded in lexical
entries. Both of schemata and lexical entries
are represented by typed feature structures, and
constraints in parsing are checked by unification
among them. Figure 1 shows examples of HPSG
schema.
Figure 2 shows an HPSG parse tree of the s n-
tence ?Spring has come.? Fi st, the lexical en-
tries of ?has? and ?come? are joined by head-
complement schema. Unification gives the HPSG
sign of mother. After applying schemata to HPSG
signs repeatedly, the HPSG sign of the whole sen-
tence is output.
We use Enju for an English HPSG parser
(Miyao et al 2004). Figure 3 shows how a co-
ordination tructure is built in the Enju grammar.
First, a coordinating conju ction and the right
conjunct are joined by coord right schema. Af-
terwards, the parent and the left conjunct are
joined by coord left schema.
The Enju parser is equipped with a disam-
biguation model trained by the maximum entropy
method (Miyao and Tsujii, 2008). Since we do
not need the probability of each parse tree, we
treat the model just as a linear model that defines
the score of a parse tree as the sum of feature
weights. The features of the model are defined
on local subtrees of a parse tree.
The Enju parser takes O(n3) time since it uses
the CKY algorithm, and each cell in the CKY
parse table has at most a constant number of edges
because we use beam search algorithm. Thus, we
can regard the parser as a decoder for a weighted
CFG.
3.3 Dual decomposition
Dual decomposition is a classical method to solve
complex optimization problems that can be de-
432
sign
PHON list of string
SYNSEM
synsem
LOCAL
local
CAT
category
HEAD
head
MODL synsem
MODR synsem
SUBJ list of synsem
COMPS list of synsem
SEM semantics
NONLOC
nonlocal
REL list of local
SLASH list of local
Figure 1: HPSG sign
2SUBJ    < >COMPS   < >2
HEAD
SUBJ    <    >
COMPS   < >
1
HEAD
SUBJ    < >
COMPS  < >
1
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
Figure 2: Subject-Head Schema (left) and Head-
Complement Schema (right)
and unbounded dependencies. SEM feature rep-
resents the semantics of a constituent, and in this
study it expresses a predicate-argument structure.
Figure 2 presents the Subject-Head Schema
and the Head-Complement Schema1 defined in
(Pollard and Sag, 1994). In order to express gen-
eral constraints, schemata only provide sharing of
feature values, and no instantiated values.
Figure 3 has an example of HPSG parsing
of the sentence ?Spring has come.? First, each
of the lexical entries for ?has? and ?come? are
unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
phrasal sign of the entire sentence is output on the
top of the derivation tree.
3 Acquiring HPSG from the Penn
Treebank
As discussed in Section 1, our grammar devel-
opment requires each sentence to be annotated
with i) a history of rule applications, and ii) ad-
ditional annotations to make the grammar rules
be pseudo-injective. In HPSG, a history of rule
applications is represented by a tree annotated
with schema names. Additional annotations are
1The value of category has been presented for simplicity,
while the other portions of the sign have been omitted.
Spring
HEAD  noun
SUBJ  < >
COMPS  < >
HEAD  verb
SUBJ  <    >
COMPS <                        >
5
has
HEAD  verb
SUBJ  <                         >
COMPS  < >
come
HEAD  verb
SUBJ  <    >
COMPS  < >
5
HEAD  noun
SUBJ < >
COMPS  < >
HEAD
SUBJ  
COMPS   <    |    >
1
COMPS   < >
HEAD
SUBJ  
COMPS   
1
2
3 4
3
4
2
UnifyUnify
Head-complement
schema
Lexical entries
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 3: HPSG parsing
required because HPSG schemata are not injec-
tive, i.e., daughters? signs cannot be uniquely de-
termined given the mother. The following annota-
tions are at least required. First, the HEAD feature
of each non-head daughter must be specified since
this is not percolated to the mother sign. Second,
SLASH/REL features are required as described in
our previous study (Miyao et al 2003a). Finally,
the SUBJ feature of the complement daughter in
the Head-Complement Schema must be specified
since this schema may subcategorize an unsatu-
rated constituent, i.e., a constituent with a non-
empty SUBJ feature. When the corpus is anno-
tated with at least these features, the lexical en-
tries required to explain the sentence are uniquely
determined. In this study, we define partially-
specified derivation trees as tree structures anno-
tated with schema names and HPSG signs includ-
ing the specifications of the above features.
We describe the process of grammar develop-
ment in terms of the four phases: specification,
externalization, extraction, and verification.
3.1 Specification
General grammatical constraints are defined in
this phase, and in HPSG, they are represented
through the design of the sign and schemata. Fig-
ure 1 shows the definition for the typed feature
structure of a sign used in this study. Some more
features are defined for each syntactic category al-
Figure 2: HPSG parsing; taken from Miyao et al
(2004).
Coordina(on
Le3,Conjunct Par(al,Coordina(on
Coordina(ng,Conjunc(on Right,Conjunct
? coord_right_schema 
? coord_left_schema 
Figure 3: Construction of coordination in Enju
composed into efficiently solvable sub-problems.
It is becoming popular in the NLP community
and has been shown to work effectively on sev-
eral NLP tasks (Rush et al 2010).
We consider an optimization problem
argmax
x
(f(x) + g(x)) (2)
which is difficult to solve (e.g. NP-hard), while
argmaxx f(x) and argmaxx g(x) are effectively
solvable. In dual decomposition, we solve
min
u
max
x,y
(f(x) + g(y) + u(x? y))
instead of the original problem.
To find the minimum value, we can use a sub-
gradient method (Rush et al 2010). The subgra-
dient method is given in Table 4. As the algorithm
u(1) ? 0
for k = 1 to K do
x(k) ? argmaxx(f(x) + u(k)x)
y(k) ? argmaxy(g(y)? u(k)y)
if x = y then
return u(k)
end if
u(k+1) ? uk ? ak(x(k) ? y(k))
end for
return u(K)
Table 4: The subgradient method
shows, you can use existing algorithms and don?t
need to have an exact algorithm for the optimiza-
tion problem, which are features of dual decom-
position.
If x(k) = y(k) occurs during the algorithm, then
we simply take x(k) as the primal solution, which
is the exact answer. If not, we simply take x(K),
the answer of coordination structure analysis with
alignment-based features, as an approximate an-
swer to the primal solution. The answer does not
always solve the original problem Eq (2), but pre-
vious works (e.g., (Rush et al 2010)) has shown
that it is effective in practice. We use it in this
paper.
4 Proposed method
In this section, we describe how we apply dual
decomposition to the two models.
4.1 Notation
We define some notations here. First we describe
weighted CFG parsing, which is used for both
coordination structure analysis with alignment-
based features and HPSG parsing. We follows the
formulation by Rush et al (2010). We assume a
context-free grammar in Chomsky normal form,
with a set of non-terminals N . All rules of the
grammar are either the form A? BC or A? w
where A,B,C ? N and w ? V . For rules of the
form A? w we refer to A as the pre-terminal for
w.
Given a sentence with n words, w1w2...wn, a
parse tree is a set of rule productions of the form
?A ? BC, i, k, j? where A,B,C ? N , and
1 ? i ? k ? j ? n. Each rule production rep-
resents the use of CFG rule A? BC where non-
terminal A spans words wi...wj , non-terminal B
433
spans word wi...wk, and non-terminal C spans
word wk+1...wj if k < j, and the use of CFG
rule A? wi if i = k = j.
We now define the index set for the coordina-
tion structure analysis as
Icsa = {?A? BC, i, k, j? : A,B,C ? N,
1 ? i ? k ? j ? n}
Each parse tree is a vector y = {yr : r ? Icsa},
with yr = 1 if rule r is in the parse tree, and yr =
0 otherwise. Therefore, each parse tree is repre-
sented as a vector in {0, 1}m, where m = |Icsa|.
We use Y to denote the set of all valid parse-tree
vectors. The set Y is a subset of {0, 1}m.
In addition, we assume a vector ?csa = {?csar :
r ? Icsa} that specifies a score for each rule pro-
duction. Each ?csar can take any real value. The
optimal parse tree is y? = argmaxy?Y y ? ?csa
where y ? ?csa =
?
r yr ? ?csar is the inner product
between y and ?csa.
We use similar notation for HPSG parsing. We
define Ihpsg , Z and ?hpsg as the index set for
HPSG parsing, the set of all valid parse-tree vec-
tors and the weight vector for HPSG parsing re-
spectively.
We extend the index sets for both the coor-
dination structure analysis with alignment-based
features and HPSG parsing to make a constraint
between the two sub-problems. For the coor-
dination structure analysis with alignment-based
features we define the extended index set to be
I ?csa = Icsa
?
Iuni where
Iuni = {(a, b, c) : a, b, c ? {1...n}}
Here each triple (a, b, c) represents that word
wc is recognized as the last word of the right
conjunct and the scope of the left conjunct or
the coordinating conjunction is wa...wb1. Thus
each parse-tree vector y will have additional com-
ponents ya,b,c. Note that this representation is
over-complete, since a parse tree is enough to
determine unique coordination structures for a
sentence: more explicitly, the value of ya,b,c is
1This definition is derived from the structure of a co-
ordination in Enju (Figure 3). The triples show where
the coordinating conjunction and right conjunct are in
coord right schema, and the left conjunct and partial coor-
dination are in coord left schema. Thus they alone enable
not only the coordination structure analysis with alignment-
based features but Enju to uniquely determine the structure
of a coordination.
1 if rule COORDa,c ? CJTa,bCC , CJT ,c or
COORD ,c ? CJT , CCa,bCJT ,c is in the parse
tree; otherwise it is 0.
We apply the same extension to the HPSG in-
dex set, also giving an over-complete representa-
tion. We define za,b,c analogously to ya,b,c.
4.2 Proposed method
We now describe the dual decomposition ap-
proach for coordination disambiguation. First, we
define the set Q as follows:
Q = {(y, z) : y ? Y, z ? Z, ya,b,c = za,b,c
for all (a, b, c) ? Iuni}
Therefore, Q is the set of all (y, z) pairs that
agree on their coordination structures. The coor-
dination structure analysis with alignment-based
features and HPSG parsing problem is then to
solve
max
(y,z)?Q
(y ? ?csa + ?z ? ?hpsg) (3)
where ? > 0 is a parameter dictating the relative
weight of the two models and is chosen to opti-
mize performance on the development test set.
This problem is equivalent to
max
z?Z
(g(z) ? ?csa + ?z ? ?hpsg) (4)
where g : Z ? Y is a function that maps a
HPSG tree z to its set of coordination structures
z = g(y).
We solve this optimization problem by using
dual decomposition. Figure 4 shows the result-
ing algorithm. The algorithm tries to optimize
the combined objective by separately solving the
sub-problems again and again. After each itera-
tion, the algorithm updates the weights u(a, b, c).
These updates modify the objective functions for
the two sub-problems, encouraging them to agree
on the same coordination structures. If y(k) =
z(k) occurs during the iterations, then the algo-
rithm simply returns y(k) as the exact answer. If
not, the algorithm returns the answer of coordina-
tion analysis with alignment features as a heuristic
answer.
It is needed to modify original sub-problems
for calculating (1) and (2) in Table 4. Wemodified
the sub-problems to regard the score of u(a, b, c)
as a bonus/penalty of the coordination. The mod-
ified coordination structure analysis with align-
ment features adds u(k)(i, j,m) and u(k)(j+1, l?
434
u(1)(a, b, c)? 0 for all (a, b, c) ? Iuni
for k = 1 to K do
y(k) ? argmaxy?Y(y ? ?csa ?
?
(a,b,c)?Iuni u
(k)(a, b, c)ya,b,c) ... (1)
z(k) ? argmaxz?Z(z ? ?hpsg +
?
(a,b,c)?Iuni u
(k)(a, b, c)za,b,c) ... (2)
if y(k)(a, b, c) = z(k)(a, b, c) for all (a, b, c) ? Iuni then
return y(k)
end if
for all (a, b, c) ? Iuni do
u(k+1)(a, b, c)? u(k)(a, b, c)? ak(y(k)(a, b, c)? z(k)(a, b, c))
end for
end for
return y(K)
Figure 4: Proposed algorithm
w ? f(x, (i, j, l,m)) to the score of the sub-
tree, when the rule production COORDi,m ?
CJTi,jCCj+1,l?1CJTl,m is applied.
The modified Enju adds u(k)(i, j, l) when co-
ord left schema is applied, where word wc
is recognized as a coordinating conjunction
and left side of its scope is wa...wb, or co-
ord right schema is applied, where word wc
is recognized as a coordinating conjunction and
right side of its scope is wa...wb.
5 Experiments
5.1 Test/Training data
We trained the alignment-based coordination
analysis model on both the Genia corpus (?)
and the Wall Street Journal portion of the Penn
Treebank (?), and evaluated the performance of
our method on (i) the Genia corpus and (ii) the
Wall Street Journal portion of the Penn Treebank.
More precisely, we used HPSG treebank con-
verted from the Penn Treebank and Genia, and
further extracted the training/test data for coor-
dination structure analysis with alignment-based
features using the annotation in the Treebank. Ta-
ble ?? shows the corpus used in the experiments.
The Wall Street Journal portion of the Penn
Treebank has 2317 sentences from WSJ articles,
and there are 1356 COOD tags in the sentences,
while the Genia corpus has 1754 sentences from
MEDLINE abstracts, and there are 1848 COOD
tags in the sentences. COOD tags are further
subcategorized into phrase types such as NP-
COOD or VP-COOD. Table ?? shows the per-
centage of each phrase type in all COOD tags.
It indicates the Wall Street Journal portion of the
COORD WSJ Genia
NP 63.7 66.3
VP 13.8 11.4
ADJP 6.8 9.6
S 11.4 6.0
PP 2.4 5.1
Others 1.9 1.5
Table 6: The percentage of each conjunct type (%) of
each test set
Penn Treebank has more VP-COOD tags and S-
COOD tags, while the Genia corpus has more
NP-COOD tags and ADJP-COOD tags.
5.2 Implementation of sub-problems
We used Enju (?) for the implementation of
HPSG parsing, which has a wide-coverage prob-
abilistic HPSG grammar and an efficient parsing
algorithm, while we re-implemented Hara et al
(2009)?s algorithm with slight modifications.
5.2.1 Step size
We used the following step size in our algo-
rithm (Figure ??). First, we initialized a0, which
is chosen to optimize performance on the devel-
opment set. Then we defined ak = a0 ? 2??k ,
where ?k is the number of times that L(u(k
?)) >
L(u(k
??1)) for k? ? k.
5.3 Evaluation metric
We evaluated the performance of the tested meth-
ods by the accuracy of coordination-level brack-
eting (?); i.e., we count each of the coordination
scopes as one output of the system, and the system
Figure 4: Proposed algorithm
1,m), as well as adding w ? f(x, (i, j, l,m)) to
the score of the subtree, when the rule produc-
tion COORDi,m ? CJTi,jCCj+1,l?1CJTl,m is
applied.
The modified Enju adds u(k)(a, b, c) when
coord right schema is applied, where word
wa...wb is recognized as a coordinating conjunc-
tion and the last word of the right conjunct is
wc, or coord left schema is applied, where word
wa...wb is recognized as the left conjunct and the
last word of the right conjunct is wc.
5 Experiments
5.1 Test/Training data
We trained the alignment-based coordination
analysis model on both the Geni corpus (Kim
et al 2003) and the Wall Street Jour al p rtion
of the Penn Treebank (Marcus et al 1993), and
evaluated the performance of our method on (i)
the Genia corpus and (ii) the Wall Street Jour-
nal portion of the Penn Tre bank. More precisely,
we used HPSG treebank onverted from the Penn
Treebank and Genia, and further extracted the
training/test data for c ordinati n structure analy-
sis with alignment-based features usi g the anno-
tation in the reebank. Table 5 shows the corpus
used in the experiments.
The Wall Street Journal portion of the Penn
Treebank in the test set ha 2317 sentences from
WSJ articles, and there are 1356 coordinations
in the sentences, while the Genia corpus in the
test set has 1764 sentences from MEDLINE ab-
stracts, and there are 1848 coordinations in the
sentences. Coor inations are further subcatego-
COORD WSJ Genia
NP 63.7 66.3
VP 13.8 11.4
ADJP 6.8 9.6
S 11.4 6.0
PP 2.4 5.1
Others 1.9 1.5
Table 6: The percentage of each conjunct type (%) of
each test set
rized into phrase types such as a NP coordination
or PP coordination. Table 6 shows the percentage
of each phrase type in all coordianitons. It indi-
cates the Wall Street Journal portion of the Penn
Treebank has more VP coordinations and S co-
ordianitons, while the Genia corpus has more NP
coordianitons and ADJP coordiations.
5.2 Implementation of sub-problems
We used Enju (Miyao and Tsujii, 2004) for
the implementation of HPSG parsing, which has
a wide-coverage probabilistic HPSG grammar
and an efficient parsing algorithm, while we re-
implemented Hara t al., (2009)?s algorithm with
slight modificatio s.
5.2.1 Step size
We used the following step size in our algo-
rithm (Figure 4). First, we initialized a0, which
is chosen to optimize performance on th devel-
opment set. Then we defined ak = a0 ? 2??k ,
where ?k is the number of times that L(u(k
?)) >
L(u(k??1)) for k? ? k.
435
Task (i) Task (ii)
Training WSJ (sec. 2?21) + Genia (No. 1?1600) WSJ (sec. 2?21)
Development Genia (No. 1601?1800) WSJ (sec. 22)
Test Genia (No. 1801?1999) WSJ (sec. 23)
Table 5: The corpus used in the experiments
Proposed Enju CSA
Precision 72.4 66.3 65.3
Recall 67.8 65.5 60.5
F1 70.0 65.9 62.8
Table 7: Results of Task (i) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)
5.3 Evaluation metric
We evaluated the performance of the tested meth-
ods by the accuracy of coordination-level bracket-
ing (Shimbo and Hara, 2007); i.e., we count each
of the coordination scopes as one output of the
system, and the system output is regarded as cor-
rect if both of the beginning of the first output
conjunct and the end of the last conjunct match
annotations in the Treebank (Hara et al 2009).
5.4 Experimental results of Task (i)
We ran the dual decomposition algorithm with a
limit of K = 50 iterations. We found the two
sub-problems return the same answer during the
algorithm in over 95% of sentences.
We compare the accuracy of the dual decompo-
sition approach to two baselines: Enju and coor-
dination structure analysis with alignment-based
features. Table 7 shows all three results. The dual
decomposition method gives a statistically signif-
icant gain in precision and recall over the two
methods2.
Table 8 shows the recall of coordinations of
each type. It indicates our re-implementation of
CSA and Hara et al(2009) have a roughly simi-
lar performance, although their experimental set-
tings are different. It also shows the proposed
method took advantage of Enju and CSA in NP
coordination, while it is likely just to take the an-
swer of Enju in VP and sentential coordinations.
This means we might well use dual decomposi-
2p < 0.01 (by chi-square test)
60%$
65%$
70%$
75%$
80%$
85%$
90%$
95%$
100%$
1$ 3$ 5$ 7$ 9$ 11$13$15$17$19$21$23$25$27$29$31$33$35$37$39$41$43$45$47$49$accuracy certificates 
Figure 5: Performance of the approach as a function of
K of Task (i) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is obtained.
tion only on NP coordinations to have a better re-
sult.
Figure 5 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The graphs show
that values of K much less than 50 produce al-
most identical performance to K = 50 (with
K = 50, the accuracy of the method is 73.4%,
with K = 20 it is 72.6%, and with K = 1 it
is 69.3%). This means you can use smaller K in
practical use for speed.
5.5 Experimental results of Task (ii)
We also ran the dual decomposition algorithm
with a limit of K = 50 iterations on Task (ii).
Table 9 and 10 show the results of task (ii). They
show the proposed method outperformed the two
methods statistically in precision and recall3.
Figure 6 shows performance of the approach as
a function of K, the maximum number of iter-
ations of dual decomposition. The convergence
speed for WSJ was faster than that for Genia. This
is because a sentence of WSJ often have a simpler
coordination structure, compared with that of Ge-
nia.
3p < 0.01 (by chi-square test)
436
COORD # Proposed Enju CSA # Hara et al(2009)
Overall 1848 67.7 63.3 61.9 3598 61.5
NP 1213 67.5 61.4 64.1 2317 64.2
VP 208 79.8 78.8 66.3 456 54.2
ADJP 193 58.5 59.1 54.4 312 80.4
S 111 51.4 52.3 34.2 188 22.9
PP 110 64.5 59.1 57.3 167 59.9
Others 13 78.3 73.9 65.2 140 49.3
Table 8: The number of coordinations of each type (#), and the recall (%) for the proposed method, Enju,
coordination structure analysis with alignment-based features (CSA) , and Hara et al(2009) of Task (i) on the
development set. Note that Hara et al(2009) uses a different test set and different annotation rules, although its
test data is also taken from the Genia corpus. Thus we cannot compare them directly.
Proposed Enju CSA
Precision 76.3 70.7 66.0
Recall 70.6 69.0 60.1
F1 73.3 69.9 62.9
Table 9: Results of Task (ii) on the test set. The preci-
sion, recall, and F1 (%) for the proposed method, Enju,
and Coordination structure analysis with alignment-
based features (CSA)
COORD # Proposed Enju CSA
Overall 1017 71.6 68.1 60.7
NP 573 76.1 71.0 67.7
VP 187 62.0 62.6 47.6
ADJP 73 82.2 75.3 79.5
S 141 64.5 62.4 42.6
PP 19 52.6 47.4 47.4
Others 24 62.5 70.8 54.2
Table 10: The number of coordinations of each type
(#), and the recall (%) for the proposed method, Enju,
and coordination structure analysis with alignment-
based features (CSA) of Task (ii) on the development
set.
6 Conclusion and Future Work
In this paper, we presented an efficient method for
detecting and disambiguating coordinate struc-
tures. Our basic idea was to consider both gram-
mar and symmetries of conjuncts by using dual
decomposition. Experiments on the Genia corpus
and the Wall Street Journal portion of the Penn
Treebank showed that we could obtain statisti-
cally significant improvement in accuracy when
using dual decomposition.
We would need a further study in the follow-
ing points of view: First, we should evaluate our
60%$
65%$
70%$
75%$
80%$
85%$
90%$
95%$
100%$
1$ 3$ 5$ 7$ 9$ 11$13$15$17$19$21$23$25$27$29$31$33$35$37$39$41$43$45$47$49$accuracy certificates 
Figure 6: Performance of the approach as a function of
K of Task (ii) on the development set. accuracy (%):
the percentage of sentences that are correctly parsed.
certificates (%): the percentage of sentences for which
a certificate of optimality is provided.
method with corpus in different domains. Be-
cause characteristics of coordination structures
differs from corpus to corpus, experiments on
other corpus would lead to a different result. Sec-
ond, we would want to add some features to coor-
dination structure analysis with alignment-based
local features such as ontology. Finally, we can
add other methods (e.g. dependency parsing) as
sub-problems to our method by using the exten-
sion of dual decomposition, which can deal with
more than two sub-problems.
Acknowledgments
The second author is partially supported by KAK-
ENHI Grant-in-Aid for Scientific Research C
21500131 and Microsoft CORE project 7.
437
References
Kazuo Hara, Masashi Shimbo, Hideharu Okuma, and
Yuji Matsumoto. 2009. Coordinate structure analy-
sis with global structural constraints and alignment-
based local features. In Proceedings of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP, pages 967?975, Aug.
Deirdre Hogan. 2007. Coordinate noun phrase dis-
ambiguation in a generative parsing model. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL 2007),
pages 680?687.
Jun-Dong Kim, Tomoko Ohta, and Jun?ich Tsujii.
2003. Genia corpus - a semantically annotated cor-
pus for bio-textmining. Bioinformatics, 19.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15:3?10.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19:313?330.
Yusuke Miyao and Jun?ich Tsujii. 2004. Deep lin-
guistic analysis for the accurate identification of
predicate-argument relations. In Proceeding of
COLING 2004, pages 1392?1397.
Yusuke Miyao and Jun?ich Tsujii. 2008. Feature
forest models for probabilistic hpsg parsing. MIT
Press, 1(34):35?80.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP 2004).
Preslav Nakov and Marti Hearst. 2005. Using the web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language (HLT-
EMNLP 2005), pages 835?842.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
phrase structure grammar. University of Chicago
Press.
Philip Resnik. 1999. Semantic similarity in a takon-
omy. Journal of Artificial Intelligence Research,
11:95?130.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natu-
ral language processing. In Proceeding of the con-
ference on Empirical Methods in Natural Language
Processing.
Masashi Shimbo and Kazuo Hara. 2007. A discrimi-
native learning model for coordinate conjunctions.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 610?619, Jun.
438
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 645?648,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Simple Approach for HPSG Supertagging Using Dependency Information
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining, UK
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
In a supertagging task, sequence labeling
models are commonly used. But their lim-
ited ability to model long-distance informa-
tion presents a bottleneck to make further im-
provements. In this paper, we modeled this
long-distance information in dependency for-
malism and integrated it into the process of
HPSG supertagging. The experiments showed
that the dependency information is very in-
formative for supertag disambiguation. We
also evaluated the improved supertagger in the
HPSG parser.
1 Introduction
Supertagging is a widely used speed-up technique
for lexicalized grammar parsing. It was first
proposed for lexicalized tree adjoining grammar
(LTAG) (Bangalore and Joshi, 1999), then extended
to combinatory categorial grammar (CCG) (Clark,
2002) and head-driven phrase structure grammar
(HPSG) (Ninomiya et al, 2006). For deep parsing,
supertagging is an important preprocessor: an ac-
curate supertagger greatly reduces search space of
a parser. Not limited to parsing, supertags can be
used for NP chunking (Shen and Joshi, 2003), se-
mantic role labeling (Chen and Rambow, 2003) and
machine translation (Birch et al, 2007; Hassan et
al., 2007) to explore rich syntactic information con-
tained in them.
Generally speaking, supertags are lexical tem-
plates extracted from a grammar. These templates
encode possible syntactic behavior of a word. Al-
though the number of supertags is far larger than the
45 POS tags defined in Penn Treebank, sequence la-
beling techniques are still effective for supertagging.
Previous research (Clark, 2002) showed that a POS
sequence is very informative for supertagging, and
some extent of local syntactic information can be
captured by the context of surrounding words and
POS tags. However, since the context window
length is limited for the computational cost reasons,
there are still long-range dependencies which are not
easily captured in sequential models (Zhang et al,
2009). In practice, the multi-tagging technique pro-
posed by Clark (2002) assigned more than one su-
pertag to each word and let the ambiguous supertags
be selected by the parser. As for other NLP applica-
tions which use supertags, resolving more supertag
ambiguities in supertagging stage is preferred. With
this consideration, we focus on supertagging and
aim to make it as accurate as possible.
In this paper, we incorporated long-distance in-
formation into supertagging. First, we used depen-
dency parser formalism to model long-distance re-
lationships between the input words, which is hard
to model in sequence labeling models. Then, we
combined the dependency information with local
context in a simple point-wise model. The experi-
ments showed that dependency information is very
informative for supertagging and we got a compet-
itive 93.70% on supertagging accuracy (fed golden
POS). In addition, we also evaluated the improved
supertagger in the HPSG parser.
2 HPSG Supertagging and Dependency
2.1 HPSG Supertags
HPSG (Pollard and Sag, 1994) is a lexicalist gram-
mar framework. In HPSG, a large number of
lexical entries is used to describe word-specific
syntactic characteristics, while only a small num-
ber of schemas is used to explain general con-
struction rules. These lexical entries are called
?HPSG supertags?. For example, one possi-
ble supertag for the word ?like? is written like
?[NP.nom<V.bse>NP.acc] lxm?, which indicates
645
the head syntactic category of ?like? is verb in base
form. It has a NP subject and a NP complement.
With such fine-grained grammatical type distinc-
tions, the number of supertags is much larger than
the number of tags used in other sequence labeling
tasks. The HPSG grammar used in our experiment
includes 2,308 supertags. This increases computa-
tional cost of sequence labeling models.
2.2 Why Use Dependency in Supertagging
By analyzing the internal structure of the supertags,
we found that subject and complements are two im-
portant syntactic properties for each supertag. If
we could predict subject and complements of the
word well, supertagging would be an easier job to
do. However, current widely used sequence labeling
models have the limited ability to catch these long-
distance syntactic relations. In supertagging stage,
tree structures are still not constructed. Dependency
formalism is an alternative way to describe these two
syntactic properties. Based on this observation, we
think dependency information could assist supertag
prediction.
Figure 1: Model structure of incorporating dependency
information into the supertagging stage. Dotted arrows
describe the augmented long distance dependency infor-
mation provided for supertag prediction.
3 Our Method
3.1 Modeling Dependency for Supertags
First of all, we need to characterize the dependency
between words for supertagging. Since exact de-
pendency locations are not encoded in supertags, to
make use of state-of-the-art dependency parser, we
recover HPSG supertag dependencies with the aid
of HPSG treebanks. The dependencies are extracted
from each branch in the HPSG trees by regarding
the non-head daughter as the modifier of the head-
daughter. HPSG schemas are expressed in depen-
dency arcs.
To model the dependency, we follow mainstream
dependency parsing formalism. Two representa-
tive methods for dependency parsing are transition-
based model like MaltParser (Nivre, 2003) and
graph-based model like MSTParser1 (McDonald et
al., 2005). Previous research (Nivre and McDon-
ald, 2008) showed that MSTParser is more accurate
than MaltParser for long dependencies. Since our
motivation is to capture long-distance dependency
as a complement for local supertagging models, we
use the projective MSTParser formalism to model
dependencies.
{(pi ? pj)&sj |(j, i) ? E}
MOD-IN {(pi ? wj)&sj|(j, i) ? E}
{(wi ? pj)&sj|(j, i) ? E}
{(wi ? wj)&sj |(j, i) ? E}
{(pi ? pj)&si|(i, j) ? E}
MOD-OUT {(pi ? wj)&si|(i, j) ? E}
{(wi ? pj)&si|(i, j) ? E}
{(wi ? wj)&si|(i, j) ? E}
Table 1: Non-local feature templates used for super-
tagging. Here, p, w and s represent POS, word
and schema respectively. Direction (Left/Right) from
MODIN/MODOUTword to the current word is also con-
sidered in the feature templates.
3.2 Integrating Dependency into Supertagging
There are several ways to combine long-distance
dependency into supertagging. Integrating depen-
dency information into training process would be
more intuitive. Here, we use feature-based integra-
tion. The base model is a point-wise averaged per-
ceptron (PW-AP) which has been shown very ef-
fective (Zhang et al, 2009). The improved model
structure is described in Figure 1. The long-distance
information is formalized as first-order dependency.
For the word being predicted, we extract its modi-
fiers (MODIN) and its head (MODOUT) (Table 1)
based on first-order dependency arcs. Then MODIN
and MODOUT relations are combined as features
with local context for supertag prediction. To com-
pare with previous work, the basic local context fea-
tures are the same as in Matsuzaki et al (2007).
1http://sourceforge.net/projects/mstparser/
646
4 Experiments
We evaluated dependency-informed supertagger
(PW-DEP) both by supertag accuracy 2 and by a
HPSG parser. The experiments were conducted on
WSJ-HPSG treebank (Miyao, 2006). Sections 02-
21 were used to train the dependency parser, the
dependency-informed supertagger and the HPSG
parser. Section 23 was used as the testing set. The
evaluation metric for HPSG parser is the accuracy
of predicate-argument relations in the parser?s out-
put, as in previous work (Sagae et al, 2007).
Model Dep Acc%? Acc%
PW-AP / 91.14
PW-DEP 90.98 92.18
PW-AP (gold POS) / 92.48
PW-DEP (gold POS) 92.05 93.70
100 97.43
Table 2: Supertagging accuracy on section 23. (?)
Dependencies are given by MSTParser evaluated with
labeled accuracy. PW-AP is the baseline point-wise
averaged perceptron model. PW-DEP is point-wise
dependency-informed model. The automatically tagged
POS tags were given by a maximum entropy tagger with
97.39% accuracy.
4.1 Results on Supertagging
We first evaluated the upper-bound of dependency-
informed supertagging model, given gold standard
first-order dependencies. As shown in Table 2,
with such long-distance information supertagging
accuracy can reach 97.43%. Comparing to point-
wise model (PW-AP) which only used local con-
text (92.48%), this absolute 4.95% gain indicated
that dependency information is really informative
for supertagging. When automatically predicted de-
pendency relations were given, there still were ab-
solute 1.04% (auto POS) and 1.22% (gold POS) im-
provements from baseline PW-AP model.
We also compared supertagging results with pre-
vious works (reported on section 22). Here we
mainly compared the dependency-informed point-
wise models with perceptron-based Bayes point ma-
chine (BPM) plus CFG-filter (Zhang et al, 2009).
To the best of our knowledge, these are the state-of-
the-art results on the same dataset with gold POS
2?UNK? supertags are ignored in evaluation as previous.
Figure 2: HPSG Parser F-score on section 23, given au-
tomatically tagged POS.
tags. CFG-filtering can be considered as an al-
ternative way of incorporating long-distance con-
straints on supertagging results. Although our base-
line system was slightly behind (PW-AP: 92.16%
vs. BPM:92.53%), the final accuracies of grammati-
cally constrained models were very close (PW-DEP:
93.53% vs. BPM-CFG: 93.60%); They were not sta-
tistically significantly different (P-value is 0.26). As
the result of oracle PW-DEP indicated, supertagging
accuracy can be further improved with better depen-
dency modeling (e.g., with a semi-supervised de-
pendency parser), which makes it more extensible
and attractive than using CFG-filter after the super-
tagging process.
4.2 HPSG parsing results
We also evaluated the dependency-informed su-
pertagger in a HPSG parser. Considering the effi-
ciency, we use the HPSG parser3 described by Ma-
tsuzaki et al (2007).
In practice, several supertag candidates are re-
served for each word to avoid parsing failure. To
evaluate the quality of the two supertaggers, we re-
stricted the number of each word?s supertag candi-
dates fed to the HPSG parser. As shown in Figure 2,
for the case when only one supertag was predicted
for each word, F-score of the HPSG parser using
dependency-informed supertagger is 5.06% higher
than the parser using the baseline supertagger mod-
ule. As the candidate number increased, the gap nar-
rowed: when all candidates were given, the gains
gradually came down to 0.2%. This indicated that
3Enju v2.3.1, http://www-tsujii.is.s.u-tokyo.ac.jp/enju.
647
improved supertagger can optimize the search space
of the deep parser, which may contribute to more ac-
curate and fast deep parsing. From another aspect,
supertagging can be viewed as an interface to com-
bine different types of parsers.
As for the overall parsing time, we didn?t opti-
mize for speed in current setting. The parsing time4
saved by using the improved supertagger (around
6.0 ms/sen, 21.5% time reduction) can not compen-
sate for the extra cost of MSTParser (around 73.8
ms/sen) now. But there is much room to improve the
final speed (e.g., optimizing the dependency parser
for speed or reusing acquired dependencies for ef-
fective pruning). In addition, small beam-size can be
?safely? used with improved supertagger for speed.
Using shallow dependencies in deep HPSG pars-
ing has been previously explored by Sagae et al
(2007), who used dependency constraints in schema
application stage to guide HPSG tree construction
(F-score was improved from 87.2% to 87.9% with
a single shift-reduce dependency parser). Since the
baseline parser is different, we didn?t make a direct
comparison here. However, it would be interesting
to compare these two different ways of incorporat-
ing the dependency parser into HPSG parsing. We
left it as further work.
5 Conclusions
In this paper, focusing on improving the accu-
racy of supertagging, we proposed a simple but
effective way to incorporate long-distance depen-
dency relations into supertagging. The experiments
mainly showed that these long-distance dependen-
cies, which are not easy to model in traditional se-
quence labeling models, are very informative for su-
pertag predictions. Although these were preliminary
results, the method shows its potential strength for
related applications. Not limited to HPSG, it can be
extended to other lexicalized grammar supertaggers.
Acknowledgments
Thanks to the anonymous reviewers for valuable
comments. We also thank Goran Topic for his self-
less help. The first author was supported by The
University of Tokyo Fellowship (UT-Fellowship).
4Tested on section 23 (2291 sentences) using an AMD
Opteron 2.4GHz server, given all supertag candidates.
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Super-
tagging: An approach to almost parsing. Computa-
tional Linguistics, 25:237?265.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003.
Stephen Clark. 2002. Supertagging for combinatory cat-
egorial grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammars and Re-
lated Frameworks (TAG+ 6), pages 19?24.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL 2007, pages 288?295.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI-07.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL-05.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Develop-
ment and Feature Forest Model. Ph.D. Dissertation,
The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsu-
zaki, and Yusuke Miyao. 2006. Extremely lexicalized
models for accurate and fast hpsg parsing. In Proceed-
ings of EMNLP-2006, pages 155?163.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of IWPT-03, pages
149?160. Citeseer.
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago / CSLI.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints. In
Proceedings of ACL-07.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505?512.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi Tsu-
jii. 2009. Hpsg supertagging: A sequence labeling
view. In Proceedings of IWPT-09, Paris, France.
648
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 325?334,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fine-grained Tree-to-String Translation Rule Extraction
Xianchao Wu? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
?School of Computer Science, University of Manchester
?National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
Tree-to-string translation rules are widely
used in linguistically syntax-based statis-
tical machine translation systems. In this
paper, we propose to use deep syntac-
tic information for obtaining fine-grained
translation rules. A head-driven phrase
structure grammar (HPSG) parser is used
to obtain the deep syntactic information,
which includes a fine-grained description
of the syntactic property and a semantic
representation of a sentence. We extract
fine-grained rules from aligned HPSG
tree/forest-string pairs and use them in
our tree-to-string and string-to-tree sys-
tems. Extensive experiments on large-
scale bidirectional Japanese-English trans-
lations testified the effectiveness of our ap-
proach.
1 Introduction
Tree-to-string translation rules are generic and ap-
plicable to numerous linguistically syntax-based
Statistical Machine Translation (SMT) systems,
such as string-to-tree translation (Galley et al,
2004; Galley et al, 2006; Chiang et al, 2009),
tree-to-string translation (Liu et al, 2006; Huang
et al, 2006), and forest-to-string translation (Mi et
al., 2008; Mi and Huang, 2008). The algorithms
proposed by Galley et al (2004; 2006) are fre-
quently used for extracting minimal and composed
rules from aligned 1-best tree-string pairs. Deal-
ing with the parse error problem and rule sparse-
ness problem, Mi and Huang (2008) replaced the
1-best parse tree with a packed forest which com-
pactly encodes exponentially many parses for tree-
to-string rule extraction.
However, current tree-to-string rules only make
use of Probabilistic Context-Free Grammar tree
fragments, in which part-of-speech (POS) or
koroshita korosareta
(active) (passive)
VBN(killed) 6 (6/10,6/6) 4 (4/10,4/4)
VBN(killed:active) 5 (5/6,5/6) 1 (1/6,1/4)
VBN(killed:passive) 1 (1/4,1/6) 3 (3/4,3/4)
Table 1: Bidirectional translation probabilities of
rules, denoted in the brackets, change when voice
is attached to ?killed?.
phrasal tags are used as the tree node labels. As
will be testified by our experiments, we argue that
the simple POS/phrasal tags are too coarse to re-
flect the accurate translation probabilities of the
translation rules.
For example, as shown in Table 1, sup-
pose a simple tree fragment ?VBN(killed)? ap-
pears 6 times with ?koroshita?, which is a
Japanese translation of an active form of ?killed?,
and 4 times with ?korosareta?, which is a
Japanese translation of a passive form of ?killed?.
Then, without larger tree fragments, we will
more frequently translate ?VBN(killed)? into ?ko-
roshita? (with a probability of 0.6). But,
?VBN(killed)? is indeed separable into two fine-
grained tree fragments of ?VBN(killed:active)?
and ?VBN(killed:passive)?1. Consequently,
?VBN(killed:active)? appears 5 times with ?ko-
roshita? and 1 time with ?korosareta?; and
?VBN(killed:passive)? appears 1 time with ?ko-
roshita? and 3 times with ?korosareta?. Now, by
attaching the voice information to ?killed?, we are
gaining a rule set that is more appropriate to reflect
the real translation situations.
This motivates our proposal of using deep syn-
tactic information to obtain a fine-grained trans-
lation rule set. We name the information such as
the voice of a verb in a tree fragment as deep syn-
tactic information. We use a head-driven phrase
structure grammar (HPSG) parser to obtain the
1For example, ?John has killed Mary.? versus ?John was
killed by Mary.?
325
deep syntactic information of an English sentence,
which includes a fine-grained description of the
syntactic property and a semantic representation
of the sentence. We extract fine-grained trans-
lation rules from aligned HPSG tree/forest-string
pairs. We localize an HPSG tree/forest to make
it segmentable at any nodes to fit the extraction
algorithms described in (Galley et al, 2006; Mi
and Huang, 2008). We also propose a linear-time
algorithm for extracting composed rules guided
by predicate-argument structures. The effective-
ness of the rules are testified in our tree-to-string
and string-to-tree systems, taking bidirectional
Japanese-English translations as our test cases.
This paper is organized as follows. In Section 2,
we briefly review the tree-to-string and string-to-
tree translation frameworks, tree-to-string rule ex-
traction algorithms, and rich syntactic information
previously used for SMT. The HPSG grammar and
our proposal of fine-grained rule extraction algo-
rithms are described in Section 3. Section 4 gives
the experiments for applying fine-grained transla-
tion rules to large-scale Japanese-English transla-
tion tasks. Finally, we conclude in Section 5.
2 Related Work
2.1 Tree-to-string and string-to-tree
translations
Tree-to-string translation (Liu et al, 2006; Huang
et al, 2006) first uses a parser to parse a source
sentence into a 1-best tree and then searches for
the best derivation that segments and converts the
tree into a target string. In contrast, string-to-tree
translation (Galley et al, 2004; Galley et al, 2006;
Chiang et al, 2009) is like bilingual parsing. That
is, giving a (bilingual) translation grammar and a
source sentence, we are trying to construct a parse
forest in the target language. Consequently, the
translation results can be collected from the leaves
of the parse forest.
Figure 1 illustrates the training and decoding
processes of bidirectional Japanese-English trans-
lations. The English sentence is ?John killed
Mary? and the Japanese sentence is ?jyon ha mari
wo koroshita?, in which the function words ?ha?
and ?wo? are not aligned with any English word.
2.2 Tree/forest-based rule extraction
Galley et al (2004) proposed the GHKM algo-
rithm for extracting (minimal) tree-to-string trans-
lation rules from a tuple of ?F,Et, A?, where F =
 
x0 ? x1 
x0 x1 
x1 ? x0 
NP 
John 
??? 
V 
killed ??? 
NP 
Mary ??? 
NP 
V NP 
VP 
S 
John killed Mary 
??? ? ??? ? ??? 
NP VP 
S 
V NP 
VP 
x0 x1 
Training 
Aligned tree-string pair: 
Extract 
rules  
John killed Mary 
??? ? ??? ? ??? 
CKY decoding 
Testing 
NP V NP 
VP 
S 
John killed Mary 
NP 
VP 
V NP 
Apply  
rules  
?? 
 
jyon   ha    mari  wo  koroshita 
parsing 
Bottom-up 
decoding 
tree-to-string string-to-tree 
Figure 1: Illustration of the training and decod-
ing processes for tree-to-string and string-to-tree
translations.
fJ1 is a sentence of a foreign language other than
English,Et is a 1-best parse tree of an English sen-
tence E = eI1, and A = {(j, i)} is an alignment
between the words in F and E.
The basic idea of GHKM algorithm is to de-
compose Et into a series of tree fragments, each
of which will form a rule with its corresponding
translation in the foreign language. A is used as a
constraint to guide the segmentation procedure, so
that the root node of every tree fragment of Et ex-
actly corresponds to a contiguous span on the for-
eign language side. Based on this consideration, a
frontier set (fs) is defined to be a set of nodes n in
Et that satisfies the following constraint:
fs = {n|span(n) ? comp span(n) = ?}. (1)
Here, span(n) is defined by the indices of the first
and last word in F that are reachable from a node
n, and comp span(n) is defined to be the comple-
ment set of span(n), i.e., the union of the spans
of all nodes n? in Et that are neither descendants
nor ancestors of n. span(n) and comp span(n)
of each n can be computed by first a bottom-up
exploration and then a top-down traversal of Et.
By restricting each fragment so that it only takes
326
John 
CAT       N 
POS      NNP 
BASE    john 
LEXENTRY [D< 
N.3sg>]_lxm 
PRED  noun_arg0  
t0 
HEAD           t0 
SEM_HEAD t0   
CAT           NX   
XCAT  
 
c2 
killed 
CAT     V 
POS     VBD 
BASE   kill 
LEXENTRY [NP.nom  
<V.bse> NP.acc] 
_lxm-past_verb_rule 
PRED  verb_arg12   
TENSE     past 
ASPECT   none 
VOICE      active 
AUX          minus 
ARG1       c1 
ARG2       c5 
t1 
HEAD           t1 
SEM_HEAD t1  
CAT           VX  
XCAT    
 
c4 
HEAD           c6 
SEM_HEAD c6   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c5 
HEAD           t2 
SEM_HEAD t2  
CAT          NX  
XCAT    
 
c6 
HEAD           c3 
SEM_HEAD c3   
CAT              S   
XCAT    
SCHEMA subj_head  
 
c0 
HEAD           c2 
SEM_HEAD c2   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c1 
HEAD           c4 
SEM_HEAD c4   
CAT              VP   
XCAT    
SCHEMA  head_comp  
c3 
Mary 
CAT       N 
POS      NNP 
BASE    mary 
LEXENTRY  
[D<N.3sg>]_lxm 
PRED    noun_arg0  
 
t2 
??? ? ??? ? ??? 
1. c0(x0:c1, x1:c3)  x0 ? x1 
2. c1(x0:c2)  x0 
3. c2(t0)  ??? 
4. c3(x0:c4, x1:c5)  x1 ? x0 
5. c4(t1)  ??? 
6. c5(x0:c6)  x0 
7. c6(t2)  ??? 
c0 
c1 c3 
c4 c5 
t1 
minimum  
covering tree 
x0 ? x1 ? ??? 
An HPSG-tree based minimal rule set A PAS-based composed rule  
John killed Mary 
HEAD           c8 
SEM_HEAD c8   
CAT              S   
XCAT    
SCHEMA head_mod  
 
c7 
HEAD           c9 
SEM_HEAD c9   
CAT              S   
XCAT    
SCHEMA  subj_head  
 
c8 
killed 
CAT     V 
POS     VBD 
BASE   kill 
LEXENTRY [NP.nom 
<V.bse>]_lxm-
past_verb_rule 
PRED  verb_arg1   
TENSE     past 
ASPECT   none 
VOICE      active 
AUX          minus 
ARG1       c1 
t3 
HEAD           t3 
SEM_HEAD t3  
CAT           VP  
XCAT    
 
c9 
HEAD           c11 
SEM_HEAD c11   
CAT              NP   
XCAT    
SCHEMA empty_spec_head  
 
c10 
HEAD           t4 
SEM_HEAD t4  
CAT          NX  
XCAT    
 
c11 
Mary 
CAT       N 
POS      NNP 
BASE    mary 
LEXENTRY  
V[D<N.3sg>] 
PRED    noun_arg0  
 
t4 
 
2.77 4.52 
0.81 2.25 
0 
0.00 
-3.47 -0.03 
0 
-2.82 
-0.07 -0.001 
Figure 2: Illustration of an aligned HPSG forest-string pair. The forest includes two parse trees by taking
?Mary? as a modifier (t3, t4) or an argument (t1, t2) of ?killed?. Arrows with broken lines denote the PAS
dependencies from the terminal node t1 to its argument nodes (c1 and c5). The scores of the hyperedges
are attached to the forest as well.
the nodes in fs as the root and leaf nodes, a well-
formed fragmentation of Et is generated. With
fs computed, rules are extracted through a depth-
first traversal of Et: we cut Et at all nodes in fs
to form tree fragments and extract a rule for each
fragment. These extracted rules are calledminimal
rules (Galley et al, 2004). For example, the 1-
best tree (with gray nodes) in Figure 2 is cut into 7
pieces, each of which corresponds to the tree frag-
ment in a rule (bottom-left corner of the figure).
In order to include richer context information
and account for multiple interpretations of un-
aligned words of foreign language, minimal rules
which share adjacent tree fragments are connected
together to form composed rules (Galley et al,
2006). For each aligned tree-string pair, Gal-
ley et al (2006) constructed a derivation-forest,
in which composed rules were generated, un-
aligned words of foreign language were consis-
tently attached, and the translation probabilities
of rules were estimated by using Expectation-
Maximization (EM) (Dempster et al, 1977) train-
ing. For example, by combining the minimal rules
of 1, 4, and 5, we obtain a composed rule, as
shown in the bottom-right corner of Figure 2.
Considering the parse error problem in the
1-best or k-best parse trees, Mi and Huang
(2008) extracted tree-to-string translation rules
from aligned packed forest-string pairs. A for-
est compactly encodes exponentially many trees
327
rather than the 1-best tree used by Galley et al
(2004; 2006). Two problems were managed to
be tackled during extracting rules from an aligned
forest-string pair: where to cut and how to cut.
Equation 1 was used again to compute a frontier
node set to determine where to cut the packed
forest into a number of tree-fragments. The dif-
ference with tree-based rule extraction is that the
nodes in a packed forest (which is a hypergraph)
now are hypernodes, which can take a set of in-
coming hyperedges. Then, by limiting each frag-
ment to be a tree and whose root/leaf hypernodes
all appearing in the frontier set, the packed forest
can be segmented properly into a set of tree frag-
ments, each of which can be used to generate a
tree-to-string translation rule.
2.3 Rich syntactic information for SMT
Before describing our approaches of applying
deep syntactic information yielded by an HPSG
parser for fine-grained rule extraction, we would
like to briefly review what kinds of deep syntactic
information have been employed for SMT.
Two kinds of supertags, from Lexicalized Tree-
Adjoining Grammar and Combinatory Categorial
Grammar (CCG), have been used as lexical syn-
tactic descriptions (Hassan et al, 2007) for phrase-
based SMT (Koehn et al, 2007). By introduc-
ing supertags into the target language side, i.e.,
the target language model and the target side
of the phrase table, significant improvement was
achieved for Arabic-to-English translation. Birch
et al (2007) also reported a significant improve-
ment for Dutch-English translation by applying
CCG supertags at a word level to a factorized SMT
system (Koehn et al, 2007).
In this paper, we also make use of supertags
on the English language side. In an HPSG
parse tree, these lexical syntactic descriptions
are included in the LEXENTRY feature (re-
fer to Table 2) of a lexical node (Matsuzaki
et al, 2007). For example, the LEXEN-
TRY feature of ?t1:killed? takes the value of
[NP.nom<V.bse>NP.acc]_lxm-past
_verb_rule in Figure 2. In which,
[NP.nom<V.bse>NP.acc] is an HPSG
style supertag, which tells us that the base form
of ?killed? needs a nominative NP in the left hand
side and an accessorial NP in the right hand side.
The major differences are that, we use a larger
feature set (Table 2) including the supertags for
fine-grained tree-to-string rule extraction, rather
than string-to-string translation (Hassan et al,
2007; Birch et al, 2007).
The Logon project2 (Oepen et al, 2007) for
Norwegian-English translation integrates in-depth
grammatical analysis of Norwegian (using lexi-
cal functional grammar, similar to (Riezler and
Maxwell, 2006)) with semantic representations in
the minimal recursion semantics framework, and
fully grammar-based generation for English using
HPSG. A hybrid (of rule-based and data-driven)
architecture with a semantic transfer backbone is
taken as the vantage point of this project. In
contrast, the fine-grained tree-to-string translation
rule extraction approaches in this paper are to-
tally data-driven, and easily applicable to numer-
ous language pairs by taking English as the source
or target language.
3 Fine-grained rule extraction
We now introduce the deep syntactic informa-
tion generated by an HPSG parser and then de-
scribe our approaches for fine-grained tree-to-
string rule extraction. Especially, we localize an
HPSG tree/forest to fit the extraction algorithms
described in (Galley et al, 2006; Mi and Huang,
2008). Also, we propose a linear-time com-
posed rule extraction algorithm by making use of
predicate-argument structures.
3.1 Deep syntactic information by HPSG
parsing
Head-driven phrase structure grammar (HPSG) is
a lexicalist grammar framework. In HPSG, lin-
guistic entities such as words and phrases are rep-
resented by a data structure called a sign. A sign
gives a factored representation of the syntactic fea-
tures of a word/phrase, as well as a representation
of their semantic content. Phrases and words rep-
resented by signs are composed into larger phrases
by applications of schemata. The semantic rep-
resentation of the new phrase is calculated at the
same time. As such, an HPSG parse tree/forest
can be considered as a tree/forest of signs (c.f. the
HPSG forest in Figure 2).
An HPSG parse tree/forest has two attractive
properties as a representation of an English sen-
tence in syntax-based SMT. First, we can carefully
control the condition of the application of a trans-
lation rule by exploiting the fine-grained syntactic
2http://www.emmtee.net/
328
Feature Description
CAT phrasal category
XCAT fine-grained phrasal category
SCHEMA name of the schema applied in the node
HEAD pointer to the head daughter
SEM HEAD pointer to the semantic head daughter
CAT syntactic category
POS Penn Treebank-style part-of-speech tag
BASE base form
TENSE tense of a verb (past, present, untensed)
ASPECT aspect of a verb (none, perfect,
progressive, perfect-progressive)
VOICE voice of a verb (passive, active)
AUX auxiliary verb or not (minus, modal,
have, be, do, to, copular)
LEXENTRY lexical entry, with supertags embedded
PRED type of a predicate
ARG?x? pointer to semantic arguments, x = 1..4
Table 2: Syntactic/semantic features extracted
from HPSG signs that are included in the output
of Enju. Features in phrasal nodes (top) and lexi-
cal nodes (bottom) are listed separately.
description in the English parse tree/forest, as well
as those in the translation rules. Second, we can
identify sub-trees in a parse tree/forest that cor-
respond to basic units of the semantics, namely
sub-trees covering a predicate and its arguments,
by using the semantic representation given in the
signs. We expect that extraction of translation
rules based on such semantically-connected sub-
trees will give a compact and effective set of trans-
lation rules.
A sign in the HPSG tree/forest is represented by
a typed feature structure (TFS) (Carpenter, 1992).
A TFS is a directed-acyclic graph (DAG) wherein
the edges are labeled with feature names and the
nodes (feature values) are typed. In the original
HPSG formalism, the types are defined in a hierar-
chy and the DAG can have arbitrary shape (e.g., it
can be of any depth). We however use a simplified
form of TFS, for simplicity of the algorithms. In
the simplified form, a TFS is converted to a (flat)
set of pairs of feature names and their values. Ta-
ble 2 lists the features used in this paper, which
are a subset of those in the original output from an
HPSG parser, Enju3. The HPSG forest shown in
Figure 2 is in this simplified format. An impor-
tant detail is that we allow a feature value to be a
pointer to another (simplified) TFS. Such pointer-
valued features are necessary for denoting the se-
mantics, as explained shortly.
In the Enju English HPSG grammar (Miyao et
3http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
 
 
She 
ignore 
 fact 
want 
I 
dispute 
ARG1 
ARG2 
ARG1 ARG1 
ARG2 
ARG2 
John 
kill 
 Mary ARG2 
ARG1 
Figure 3: Predicate argument structures for the
sentences of ?John killed Mary? and ?She ignored
the fact that I wanted to dispute?.
al., 2003) used in this paper, the semantic content
of a sentence/phrase is represented by a predicate-
argument structure (PAS). Figure 3 shows the PAS
of the example sentence in Figure 2, ?John killed
Mary?, and a more complex PAS for another sen-
tence, ?She ignored the fact that I wanted to dis-
pute?, which is adopted from (Miyao et al, 2003).
In an HPSG tree/forest, each leaf node generally
introduces a predicate, which is represented by
the pair of LEXENTRY (lexical entry) feature and
PRED (predicate type) feature. The arguments of
a predicate are designated by the pointers from the
ARG?x? features in a leaf node to non-terminal
nodes.
3.2 Localize HPSG forest
Our fine-grained translation rule extraction algo-
rithm is sketched in Algorithm 1. Considering that
a parse tree is a trivial packed forest, we only use
the term forest to expand our discussion, hereafter.
Recall that there are pointer-valued features in the
TFSs (Table 2) which prevent arbitrary segmenta-
tion of a packed forest. Hence, we have to localize
an HPSG forest.
For example, there are ARG pointers from t1 to
c1 and c5 in the HPSG forest of Figure 2. How-
ever, the three nodes are not included in one (min-
imal) translation rule. This problem is caused
by not considering the predicate argument depen-
dency among t1, c1, and c5 while performing the
GHKM algorithm. We can combine several min-
imal rules (Galley et al, 2006) together to ad-
dress this dependency. Yet we have a faster way
to tackle PASs, as will be described in the next
subsection.
Even if we omit ARG, there are still two kinds
of pointer-valued features in TFSs, HEAD and
SEM HEAD. Localizing these pointer-valued fea-
tures is straightforward, since during parsing, the
HEAD and SEM HEAD of a node are automati-
cally transferred to its mother node. That is, the
syntactic and semantic head of a node only take
329
Algorithm 1 Fine-grained rule extraction
Input: HPSG tree/forest Ef , foreign sentence F , and align-
ment A
Output: a PAS-based rule set R1 and/or a tree-rule set R2
1: if Ef is an HPSG tree then
2: E
?
f = localize Tree(Ef )
3: R1 = PASR extraction(E
?
f , F , A) ? Algorithm 2
4: E
??
f = ignore PAS(E
?
f )
5: R2 = TR extraction(E
??
f , F , A) ? composed rule ex-
traction algorithm in (Galley et al, 2006)
6: else if Ef is an HPSG forest then
7: E
?
f = localize Forest(Ef );
8: R2 = forest based rule extraction(E
?
f , F , A) ? Algo-
rithm 1 in (Mi and Huang, 2008)
9: end if
the identifier of the daughter node as the values.
For example, HEAD and SEM HEAD of node c0
take the identical value to be c3 in Figure 2.
To extract tree-to-string rules from the tree
structures of an HPSG forest, our solution is to
pre-process an HPSG forest in the following way:
? for a phrasal hypernode, replace its HEAD
and SEM HEAD value with L, R, or S,
which respectively represent left daughter,
right daughter, or single daughter (line 2 and
7); and,
? for a lexical node, ARG?x? and PRED fea-
tures are ignored (line 4).
A pure syntactic-based HPSG forest without any
pointer-valued features can be yielded through this
pre-processing for the consequent execution of the
extraction algorithms (Galley et al, 2006; Mi and
Huang, 2008).
3.3 Predicate-argument structures
In order to extract translation rules from PASs,
we want to localize a predicate word and its ar-
guments into one tree fragment. For example, in
Figure 2, we can use a tree fragment which takes
c0 as its root node and c1, t1, and c5 on its yield (=
leaf nodes of a tree fragment) to cover ?killed? and
its subject and direct object arguments. We define
this kind of tree fragment to be a minimum cov-
ering tree. For example, the minimum covering
tree of {t1, c1, c5} is shown in the bottom-right
corner of Figure 2. The definition supplies us a
linear-time algorithm to directly find the tree frag-
ment that covers a PAS during both rule extracting
and rule matching when decoding an HPSG tree.
Algorithm 2 PASR extraction
Input: HPSG tree Et, foreign sentence F , and alignment A
Output: a PAS-based rule set R
1: R = {}
2: for node n ? Leaves(Et) do
3: if Open(n.ARG) then
4: Tc = MinimumCoveringTree(Et, n, n.ARGs)
5: if root and leaf nodes of Tc are in fs then
6: generate a rule r using fragment Tc
7: R.append(r)
8: end if
9: end if
10: end for
See (Wu, 2010) for more examples of minimum
covering trees.
Taking a minimum covering tree as the tree
fragment, we can easily build a tree-to-string
translation rule that reflects the semantic depen-
dency of a PAS. The algorithm of PAS-based
rule (PASR) extraction is sketched in Algorithm
2. Suppose we are given a tuple of ?F,Et, A?.
Et is pre-processed by replacing HEAD and
SEM HEAD to be L, R, or S, and computing the
span and comp span of each node.
We extract PAS-based rules through one-time
traversal of the leaf nodes in Et (line 2). For each
leaf node n, we extract a minimum covering tree
Tc if n contains at least one argument. That is, at
least one ARG?x? takes the value of some node
identifier, where x ranges 1 over 4 (line 3). Then,
we require the root and yield nodes of Tc being in
the frontier set of Et (line 5). Based on Tc, we can
easily build a tree-to-string translation rule by fur-
ther completing the right-hand-side string by sort-
ing the spans of Tc?s leaf nodes, lexicalizing the
terminal node?s span(s), and assigning a variable
to each non-terminal node?s span. Maximum like-
lihood estimation is used to calculate the transla-
tion probabilities of each rule.
An example of PAS-based rule is shown in the
bottom-right corner of Figure 2. In the rule, the
subject and direct-object of ?killed? are general-
ized into two variables, x0 and x1.
4 Experiments
4.1 Translation models
We use a tree-to-string model and a string-to-tree
model for bidirectional Japanese-English transla-
tions. Both models use a phrase translation table
(PTT), an HPSG tree-based rule set (TRS), and
a PAS-based rule set (PRS). Since the three rule
sets are independently extracted and estimated, we
330
use Minimum Error Rate Training (MERT) (Och,
2003) to tune the weights of the features from the
three rule sets on the development set.
Given a 1-best (localized) HPSG tree Et, the
tree-to-string decoder searches for the optimal
derivation d? that transforms Et into a Japanese
string among the set of all possible derivations D:
d? =argmax
d?D
{?1 log pLM (?(d)) + ?2|?(d)|
+ log s(d|Et)}. (2)
Here, the first item is the language model (LM)
probability where ?(d) is the target string of
derivation d; the second item is the translation
length penalty; and the third item is the transla-
tion score, which is decomposed into a product of
feature values of rules:
s(d|Et) =
?
r?d
f(r?PTT )f(r?TRS)f(r?PRS).
This equation reflects that the translation rules in
one d come from three sets. Inspired by (Liu et
al., 2009b), it is appealing to combine these rule
sets together in one decoder because PTT provides
excellent rule coverages while TRS and PRS offer
linguistically motivated phrase selections and non-
local reorderings. Each f(r) is in turn a product of
five features:
f(r) = p(s|t)?3 ? p(t|s)?4 ? l(s|t)?5 ? l(t|s)?6 ? e?7 .
Here, s/t represent the source/target part of a rule
in PTT, TRS, or PRS; p(?|?) and l(?|?) are transla-
tion probabilities and lexical weights of rules from
PTT, TRS, and PRS. The derivation length penalty
is controlled by ?7.
In our string-to-tree model, for efficient decod-
ing with integrated n-gram LM, we follow (Zhang
et al, 2006) and inversely binarize all translation
rules into Chomsky Normal Forms that contain
at most two variables and can be incrementally
scored by LM. In order to make use of the bina-
rized rules in the CKY decoding, we add two kinds
of glues rules:
S ? Xm(1), Xm(1);
S ? S(1)Xm(2), S(1)Xm(2).
Here Xm ranges over the nonterminals appearing
in a binarized rule set. These glue rules can be
seen as an extension from X to {Xm}of the two
glue rules described in (Chiang, 2007).
The string-to-tree decoder searches for the op-
timal derivation d? that parses a Japanese string
F into a packed forest of the set of all possible
derivations D:
d? =argmax
d?D
{?1 log pLM (?(d)) + ?2|?(d)|
+ ?3g(d) + log s(d|F )}. (3)
This formula differs from Equation 2 by replacing
Et with F in s(d|?) and adding g(d), which is the
number of glue rules used in d. Further definitions
of s(d|F ) and f(r) are identical with those used
in Equation 2.
4.2 Decoding algorithms
In our translation models, we have made use
of three kinds of translation rule sets which are
trained separately. We perform derivation-level
combination as described in (Liu et al, 2009b) for
mixing different types of translation rules within
one derivation.
For tree-to-string translation, we use a bottom-
up beam search algorithm (Liu et al, 2006) for
decoding an HPSG tree Et. We keep at most 10
best derivations with distinct ?(d)s at each node.
Recall the definition of minimum covering tree,
which supports a faster way to retrieve available
rules from PRS without generating all the sub-
trees. That is, when node n fortunately to be the
root of someminimum covering tree(s), we use the
tree(s) to seek available PAS-based rules in PRS.
We keep a hash-table with the key to be the node
identifier of n and the value to be a priority queue
of available PAS-based rules. The hash-table is
easy to be filled by one-time traversal of the termi-
nal nodes in Et. At each terminal node, we seek
its minimum covering tree, retrieve PRS, and up-
date the hash-table. For example, suppose we are
decoding an HPSG tree (with gray nodes) shown
in Figure 2. At t1, we can extract its minimum
covering tree with the root node to be c0, then take
this tree fragment as the key to retrieve PRS, and
consequently put c0 and the available rules in the
hash-table. When decoding at c0, we can directly
access the hash-table looking for available PAS-
based rules.
In contrast, we use a CKY-style algorithm with
beam-pruning and cube-pruning (Chiang, 2007)
to decode Japanese sentences. For each Japanese
sentence F , the output of the chart-parsing algo-
rithm is expressed as a hypergraph representing a
set of derivations. Given such a hypergraph, we
331
Train Dev. Test
# of sentences 994K 2K 2K
# of Jp words 28.2M 57.4K 57.1K
# of En words 24.7M 50.3K 49.9K
Table 3: Statistics of the JST corpus.
use the Algorithm 3 described in (Huang and Chi-
ang, 2005) to extract its k-best (k = 500 in our
experiments) derivations. Since different deriva-
tions may lead to the same target language string,
we further adopt Algorithm 3?s modification, i.e.,
keep a hash-table to maintain the unique target
sentences (Huang et al, 2006), to efficiently gen-
erate the unique k-best translations.
4.3 Setups
The JST Japanese-English paper abstract corpus4,
which consists of one million parallel sentences,
was used for training and testing. This corpus
was constructed from a Japanese-English paper
abstract corpus by using the method of Utiyama
and Isahara (2007). Table 3 shows the statistics
of this corpus. Making use of Enju 2.3.1, we suc-
cessfully parsed 987,401 English sentences in the
training set, with a parse rate of 99.3%. We mod-
ified this parser to output a packed forest for each
English sentence.
We executed GIZA++ (Och and Ney, 2003) and
grow-diag-final-and balancing strategy (Koehn et
al., 2007) on the training set to obtain a phrase-
aligned parallel corpus, from which bidirectional
phrase translation tables were estimated. SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) was em-
ployed to train 5-gram English and Japanese LMs
on the training set. We evaluated the translation
quality using the case-insensitive BLEU-4 metric
(Papineni et al, 2002). The MERT toolkit we used
is Z-mert5 (Zaidan, 2009).
The baseline system for comparison is Joshua
(Li et al, 2009), a freely available decoder for hi-
erarchical phrase-based SMT (Chiang, 2005). We
respectively extracted 4.5M and 5.3M translation
rules from the training set for the 4K English and
Japanese sentences in the development and test
sets. We used the default configuration of Joshua,
expect setting the maximum number of items/rules
and the k of k-best outputs to be the identical
4http://www.jst.go.jp. The corpus can be conditionally
obtained from NTCIR-7 patent translation workshop home-
page: http://research.nii.ac.jp/ntcir/permission/ntcir-7/perm-
en-PATMT.html.
5http://www.cs.jhu.edu/ ozaidan/zmert/
PRS CS3 C3 FS F
tree nodes TFS POS TFS POS TFS
# rules 0.9 62.1 83.9 92.5 103.7
# tree types 0.4 23.5 34.7 40.6 45.2
extract time 3.5 - 98.6 - 121.2
Table 4: Statistics of several kinds of tree-to-string
rules. Here, the number is in million level and the
time is in hour.
200 for English-to-Japanese translation and 500
for Japanese-to-English translation.
We used four dual core Xeon machines
(4?3.0GHz?2CPU, 4?64GB memory) to run all
the experiments.
4.4 Results
Table 4 illustrates the statistics of several transla-
tion rule sets, which are classified by:
? using TFSs or simple POS/phrasal tags (an-
notated by a superscript S) to represent tree
nodes;
? composed rules (PRS) extracted from the
PAS of 1-best HPSG trees;
? composed rules (C3), extracted from the tree
structures of 1-best HPSG trees, and 3 is the
maximum number of internal nodes in the
tree fragments; and
? forest-based rules (F ), where the packed
forests are pre-pruned by the marginal
probability-based inside-outside algorithm
used in (Mi and Huang, 2008).
Table 5 reports the BLEU-4 scores achieved by
decoding the test set making use of Joshua and our
systems (t2s = tree-to-string and s2t = string-to-
tree) under numerous rule sets. We analyze this
table in terms of several aspects to prove the effec-
tiveness of deep syntactic information for SMT.
Let?s first look at the performance of TFSs. We
take CS3 and FS as approximations of CFG-based
translation rules. Comparing the BLEU-4 scores
of PTT+CS3 and PTT+C3, we gained 0.56 (t2s)
and 0.57 (s2t) BLEU-4 points which are signifi-
cant improvements (p < 0.05). Furthermore, we
gained 0.50 (t2s) and 0.62 (s2t) BLEU-4 points
from PTT+FS to PTT+F , which are also signif-
icant improvements (p < 0.05). The rich fea-
tures included in TFSs contribute to these im-
provements.
332
Systems BLEU-t2s Decoding BLEU-s2t
Joshua 21.79 0.486 19.73
PTT 18.40 0.013 17.21
PTT+PRS 22.12 0.031 19.33
PTT+CS3 23.56 2.686 20.59
PTT+C3 24.12 2.753 21.16
PTT+C3+PRS 24.13 2.930 21.20
PTT+FS 24.25 3.241 22.05
PTT+F 24.75 3.470 22.67
Table 5: BLEU-4 scores (%) achieved by Joshua
and our systems under numerous rule configura-
tions. The decoding time (seconds per sentence)
of tree-to-string translation is listed as well.
Also, BLEU-4 scores were inspiringly in-
creased 3.72 (t2s) and 2.12 (s2t) points by append-
ing PRS to PTT, comparing PTT with PTT+PRS.
Furthermore, in Table 5, the decoding time (sec-
onds per sentence) of tree-to-string translation by
using PTT+PRS is more than 86 times faster than
using the other tree-to-string rule sets. This sug-
gests that the direct generation of minimum cover-
ing trees for rule matching is extremely faster than
generating all subtrees of a tree node. Note that
PTT performed extremely bad compared with all
other systems or tree-based rule sets. The major
reason is that we did not perform any reordering
or distorting during decoding with PTT.
However, in both t2s and s2t systems, the
BLEU-4 score benefits of PRS were covered by
the composed rules: both PTT+CS3 and PTT+C3
performed significant better (p < 0.01) than
PTT+PRS, and there are no significant differences
when appending PRS to PTT+C3. The reason is
obvious: PRS is only a small subset of the com-
posed rules, and the probabilities of rules in PRS
were estimated by maximum likelihood, which is
fast but biased compared with EM based estima-
tion (Galley et al, 2006).
Finally, by using PTT+F , our systems achieved
the best BLEU-4 scores of 24.75% (t2s) and
22.67% (s2t), both are significantly better (p <
0.01) than that achieved by Joshua.
5 Conclusion
We have proposed approaches of using deep syn-
tactic information for extracting fine-grained tree-
to-string translation rules from aligned HPSG
forest-string pairs. The main contributions are the
applications of GHKM-related algorithms (Galley
et al, 2006; Mi and Huang, 2008) to HPSG forests
and a linear-time algorithm for extracting com-
posed rules from predicate-argument structures.
We applied our fine-grained translation rules to a
tree-to-string system and an Hiero-style string-to-
tree system. Extensive experiments on large-scale
bidirectional Japanese-English translations testi-
fied the significant improvements on BLEU score.
We argue the fine-grained translation rules are
generic and applicable to many syntax-based SMT
frameworks such as the forest-to-string model (Mi
et al, 2008). Furthermore, it will be interesting
to extract fine-grained tree-to-tree translation rules
by integrating deep syntactic information in the
source and/or target language side(s). These tree-
to-tree rules are applicable for forest-to-tree trans-
lation models (Liu et al, 2009a).
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan) and Japanese/Chinese Machine Translation
Project in Special Coordination Funds for Pro-
moting Science and Technology (MEXT, Japan),
and Microsoft Research Asia Machine Translation
Theme. The first author thanks Naoaki Okazaki
and Yusuke Miyao for their help and the anony-
mous reviewers for improving the earlier version.
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. Ccg supertags in factored statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 9?
16, June.
Bob Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 218?
226, June.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Lingustics, 33(2):201?228.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, 39:1?38.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL.
333
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, Sydney.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of ACL, pages 288?295, June.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of 7th AMTA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL 2007 Demo and Poster Ses-
sions, pages 177?180.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Demonstration of joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 25?28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009a. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, Au-
gust.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI, pages 1671?
1676, January.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206?214, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2003. Probabilistic modeling of argument struc-
tures including non-local dependencies. In Proceed-
ings of the International Conference on Recent Ad-
vances in Natural Language Processing, pages 285?
291, Borovets.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Stephan Oepen, Erik Velldal, Jan Tore L?nning, Paul
Meurer, and Victoria Rose?n. 2007. Towards hy-
brid quality-oriented machine translation - on lin-
guistics and probabilities in mt. In Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI-07), September.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Stefan Riezler and John T. Maxwell, III. 2006. Gram-
matical machine translation. In Proceedings of HLT-
NAACL, pages 248?255.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Masao Utiyama and Hitoshi Isahara. 2007. A
japanese-english patent parallel corpus. In Proceed-
ings of MT Summit XI, pages 475?482, Copenhagen.
Xianchao Wu. 2010. Statistical Machine Transla-
tion Using Large-Scale Lexicon and Deep Syntactic
Structures. Ph.D. dissertation. Department of Com-
puter Science, The University of Tokyo.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT-NAACL,
pages 256?263, June.
334
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 22?31,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Effective Use of Function Words for Rule Generalization
in Forest-Based Translation
Xianchao Wu? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, The University of Tokyo
?School of Computer Science, University of Manchester
?National Centre for Text Mining (NaCTeM)
{wxc, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
In the present paper, we propose the ef-
fective usage of function words to generate
generalized translation rules for forest-based
translation. Given aligned forest-string pairs,
we extract composed tree-to-string translation
rules that account for multiple interpretations
of both aligned and unaligned target func-
tion words. In order to constrain the ex-
haustive attachments of function words, we
limit to bind them to the nearby syntactic
chunks yielded by a target dependency parser.
Therefore, the proposed approach can not
only capture source-tree-to-target-chunk cor-
respondences but can also use forest structures
that compactly encode an exponential num-
ber of parse trees to properly generate target
function words during decoding. Extensive
experiments involving large-scale English-to-
Japanese translation revealed a significant im-
provement of 1.8 points in BLEU score, as
compared with a strong forest-to-string base-
line system.
1 Introduction
Rule generalization remains a key challenge for
current syntax-based statistical machine translation
(SMT) systems. On the one hand, there is a ten-
dency to integrate richer syntactic information into
a translation rule in order to better express the trans-
lation phenomena. Thus, flat phrases (Koehn et al,
2003), hierarchical phrases (Chiang, 2005), and syn-
tactic tree fragments (Galley et al, 2006; Mi and
Huang, 2008; Wu et al, 2010) are gradually used in
SMT. On the other hand, the use of syntactic phrases
continues due to the requirement for phrase cover-
age in most syntax-based systems. For example,
Mi et al (2008) achieved a 3.1-point improvement
in BLEU score (Papineni et al, 2002) by including
bilingual syntactic phrases in their forest-based sys-
tem. Compared with flat phrases, syntactic rules are
good at capturing global reordering, which has been
reported to be essential for translating between lan-
guages with substantial structural differences, such
as English and Japanese, which is a subject-object-
verb language (Xu et al, 2009).
Forest-based translation frameworks, which make
use of packed parse forests on the source and/or tar-
get language side(s), are an increasingly promising
approach to syntax-based SMT, being both algorith-
mically appealing (Mi et al, 2008) and empirically
successful (Mi and Huang, 2008; Liu et al, 2009).
However, forest-based translation systems, and, in
general, most linguistically syntax-based SMT sys-
tems (Galley et al, 2004; Galley et al, 2006; Liu
et al, 2006; Zhang et al, 2007; Mi et al, 2008;
Liu et al, 2009; Chiang, 2010), are built upon word
aligned parallel sentences and thus share a critical
dependence on word alignments. For example, even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, and un-
aligned words can result in an exponentially large
set of extractable rules for the interpretation of these
unaligned words (Galley et al, 2006).
What makes word alignment so fragile? In or-
der to investigate this problem, we manually ana-
lyzed the alignments of the first 100 parallel sen-
tences in our English-Japanese training data (to be
shown in Table 2). The alignments were generated
by running GIZA++ (Och and Ney, 2003) and the
grow-diag-final-and symmetrizing strategy (Koehn
et al, 2007) on the training set. Of the 1,324 word
alignment pairs, there were 309 error pairs, among
22
which there were 237 target function words, which
account for 76.7% of the error pairs1. This indicates
that the alignments of the function words are more
easily to be mistaken than content words. More-
over, we found that most Japanese function words
tend to align to a few English words such as ?of?
and ?the?, which may appear anywhere in an English
sentence. Following these problematic alignments,
we are forced to make use of relatively large English
tree fragments to construct translation rules that tend
to be ill-formed and less generalized.
This is the motivation of the present approach of
re-aligning the target function words to source tree
fragments, so that the influence of incorrect align-
ments is reduced and the function words can be gen-
erated by tree fragments on the fly. However, the
current dominant research only uses 1-best trees for
syntactic realignment (Galley et al, 2006; May and
Knight, 2007; Wang et al, 2010), which adversely
affects the rule set quality due to parsing errors.
Therefore, we realign target function words to a
packed forest that compactly encodes exponentially
many parses. Given aligned forest-string pairs, we
extract composed tree-to-string translation rules that
account for multiple interpretations of both aligned
and unaligned target function words. In order to con-
strain the exhaustive attachments of function words,
we further limit the function words to bind to their
surrounding chunks yielded by a dependency parser.
Using the composed rules of the present study in
a baseline forest-to-string translation system results
in a 1.8-point improvement in the BLEU score for
large-scale English-to-Japanese translation.
2 Backgrounds
2.1 Japanese function words
In the present paper, we limit our discussion
on Japanese particles and auxiliary verbs (Martin,
1975). Particles are suffixes or tokens in Japanese
grammar that immediately follow modified con-
tent words or sentences. There are eight types of
Japanese function words, which are classified de-
pending on what function they serve: case markers,
parallel markers, sentence ending particles, interjec-
1These numbers are language/corpus-dependent and are not
necessarily to be taken as a general reflection of the overall qual-
ity of the word alignments for arbitrary language pairs.
tory particles, adverbial particles, binding particles,
conjunctive particles, and phrasal particles.
Japanese grammar also uses auxiliary verbs to
give further semantic or syntactic information about
the preceding main or full verb. Alike English, the
extra meaning provided by a Japanese auxiliary verb
alters the basic meaning of the main verb so that the
main verb has one or more of the following func-
tions: passive voice, progressive aspect, perfect as-
pect, modality, dummy, or emphasis.
2.2 HPSG forests
Following our precious work (Wu et al, 2010), we
use head-drive phrase structure grammar (HPSG)
forests generated by Enju2 (Miyao and Tsujii, 2008),
which is a state-of-the-art HPSG parser for English.
HPSG (Pollard and Sag, 1994; Sag et al, 2003) is a
lexicalist grammar framework. In HPSG, linguistic
entities such as words and phrases are represented
by a data structure called a sign. A sign gives a
factored representation of the syntactic features of
a word/phrase, as well as a representation of their
semantic content. Phrases and words represented by
signs are collected into larger phrases by the appli-
cations of schemata. The semantic representation of
the new phrase is calculated at the same time. As
such, an HPSG parse forest can be considered to
be a forest of signs. Making use of these signs in-
stead of part-of-speech (POS)/phrasal tags in PCFG
results in a fine-grained rule set integrated with deep
syntactic information.
For example, an aligned HPSG forest3-string pair
is shown in Figure 1. For simplicity, we only draw
the identifiers for the signs of the nodes in the HPSG
forest. Note that the identifiers that start with ?c? de-
note non-terminal nodes (e.g., c0, c1), and the iden-
tifiers that start with ?t? denote terminal nodes (e.g.,
t3, t1). In a complete HPSG forest given in (Wu et
al., 2010), the terminal signs include features such
as the POS tag, the tense, the auxiliary, the voice of
a verb, etc.. The non-terminal signs include features
such as the phrasal category, the name of the schema
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
3The forest includes three parse trees rooted at c0, c1, and
c2. In the 1-best tree, ?by? modifies the passive verb ?verified?.
Yet in the 2- and 3-best tree, ?by? modifies ?this result was ver-
ified?. Furthermore, ?verified? is an adjective in the 2-best tree
and a passive verb in the 3-best tree.
23
 
 
jikken niyotte kono kekka ga sa re ta kensyou 
Realign target function words 
?? 0 
 
???? 1 
 
?? 2 
 
?? 3 
 
? 4 ? 6 
 
? 7 
 
? 8 
 
?? 5 
 
this 
 
result 
 
was 
 
verified 
 
by 
 
the 
 
experiments 
 
t3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9 
c9 c10 c16 c22 c4 c21 c12 c18 c19 c14 c15
c23 
c8 
c13 
c5 c17 
c3 
c6 
c2 
c7 
c11 
c0 
c20 
c1 
1-best tree 2-best tree 3-best tree 
experiments 
 
by 
 
this 
 
result 
 
 
 
verified 
 
c1 
?? 0 
 
???? 1 
 
?? 2 
 
?? 3 
 
? 4 
 
? 6 
 
? 7 
 
? 8 
 
?? 5 
 C1 C2 C3 C4 
this 
 
result 
 
was 
 
verified 
 
by 
 
the 
 
experiments 
 
t3 t1 t4 t8 t10 t7 t0 t6 t5 t2 t9 
c9 c16 c22 c45-7 | 5-8 c125-7 | 5-8  c18 c19 c14 c15
c2 c0 
c215-7 | 5-8 
c23 c8 
c13 
c5 c17 
c3 
c6 
c7 
c11 c20 
c103 | 3-4 
Figure 1: Illustration of an aligned HPSG forest-string pair for English-to-Japanese translation. The chunk-level
dependency tree for the Japanese sentence is shown as well.
applied in the node, etc..
3 Composed Rule Extraction
In this section, we first describe an algorithm that
attaches function words to a packed forest guided
by target chunk information. That is, given a triple
?FS , T, A?, namely an aligned (A) source forest
(FS) to target sentence (T ) pair, we 1) tailor the
alignment A by removing the alignments for tar-
get function words, 2) seek attachable nodes in the
source forest FS for each function word, and 3) con-
struct a derivation forest by topologically travers-
ing FS . Then, we identify minimal and composed
rules from the derivation forest and estimate the
probabilities of rules and scores of derivations us-
ing the expectation-maximization (EM) (Dempster
et al, 1977) algorithm.
3.1 Definitions
In the proposed algorithm, we make use of the fol-
lowing definitions, which are similar to those de-
scribed in (Galley et al, 2004; Mi and Huang, 2008):
? s(?): the span of a (source) node v or a (target)
chunk C, which is an index set of the words that
24
v or C covers;
? t(v): the corresponding span of v, which is an
index set of aligned words on another side;
? c(v): the complement span of v, which is the
union of corresponding spans of nodes v? that
share an identical parse tree with v but are nei-
ther antecedents nor descendants of v;
? PA: the frontier set of FS , which contains
nodes that are consistent with an alignment A
(gray nodes in Figure 1), i.e., t(v) ?= ? and
closure(t(v)) ? c(v) = ?.
The function closure covers the gap(s) that may
appear in the interval parameter. For example,
closure(t(c3)) = closure({0-1, 4-7}) = {0-7}.
Examples of the applications of these functions can
be found in Table 1. Following (Galley et al,
2006), we distinguish between minimal and com-
posed rules. The composed rules are generated by
combining a sequence of minimal rules.
3.2 Free attachment of target function words
3.2.1 Motivation
We explain the motivation for the present research
using an example that was extracted from our train-
ing data, as shown in Figure 1. In the alignment of
this example, three lines (in dot lines) are used to
align was and the with ga (subject particle), and was
with ta (past tense auxiliary verb). Under this align-
ment, we are forced to extract rules with relatively
large tree fragments. For example, by applying the
GHKM algorithm (Galley et al, 2004), a rule rooted
at c0 will take c7, t4, c4, c19, t2, and c15 as the
leaves. The final tree fragment, with a height of 7,
contains 13 nodes. In order to ensure that this rule
is used during decoding, we must generate subtrees
with a height of 7 for c0. Suppose that the input for-
est is binarized and that |E| is the average number
of hyperedges of each node, then we must generate
O(|E|26?1) subtrees4 for c0 in the worst case. Thus,
4For one (binarized) hyperedge e of a node, suppose there
are x subtrees in the left tail node and y subtrees in the right tail
node. Then the number of subtrees guided by e is (x + 1) ?
(y+1). Thus, the recursive formula is Nh = |E|(Nh?1 +1)2,
where h is the height of the hypergraph and Nh is the number
of subtrees. When h = 1, we let Nh = 0.
the existence of these rules prevents the generaliza-
tion ability of the final rule set that is extracted.
In order to address this problem, we tailor the
alignment by ignoring these three alignment pairs in
dot lines. For example, by ignoring the ambiguous
alignments on the Japanese function words, we en-
large the frontier set to include from 12 to 19 of the
24 non-terminal nodes. Consequently, the number
of extractable minimal rules increases from 12 (with
three reordering rules rooted at c0, c1, and c2) to
19 (with five reordering rules rooted at c0, c1, c2,
c5, and c17). With more nodes included in the fron-
tier set, we can extract more minimal and composed
monotonic/reordering rules and avoid extracting the
less generalized rules with extremely large tree frag-
ments.
3.2.2 Why chunking?
In the proposed algorithm, we use a target chunk
set to constrain the attachment explosion problem
because we use a packed parse forest instead of a 1-
best tree, as in the case of (Galley et al, 2006). Mul-
tiple interpretations of unaligned function words for
an aligned tree-string pair result in a derivation for-
est. Now, we have a packed parse forest in which
each tree corresponds to a derivation forest. Thus,
pruning free attachments of function words is prac-
tically important in order to extract composed rules
from this ?(derivation) forest of (parse) forest?.
In the English-to-Japanese translation test case of
the present study, the target chunk set is yielded
by a state-of-the-art Japanese dependency parser,
Cabocha v0.535 (Kudo and Matsumoto, 2002). The
output of Cabocha is a list of chunks. A chunk con-
tains roughly one content word (usually the head)
and affixed function words, such as case markers
(e.g., ga) and verbal morphemes (e.g., sa re ta,
which indicate past tense and passive voice). For
example, the Japanese sentence in Figure 1 is sepa-
rated into four chunks, and the dependencies among
these chunks are identified by arrows. These arrows
point out the head chunk that the current chunk mod-
ifies. Moreover, we also hope to gain a fine-grained
alignment among these syntactic chunks and source
tree fragments. Thereby, during decoding, we are
binding the generation of function words with the
generation of target chunks.
5http://chasen.org/?taku/software/cabocha/
25
Algorithm 1 Aligning function words to the forest
Input: HPSG forest FS , target sentence T , word alignment
A = {(i, j)}, target function word set {fw} appeared in
T , and target chunk set {C}
Output: a derivation forest DF
1: A? ? A \ {(i, s(fw))} ? fw ? {fw}
2: for each node v ? PA? in topological order do
3: Tv ? ? ? store the corresponding spans of v
4: for each function word fw ? {fw} do
5: if fw ? C and t(v)?(C) ?= ? and fw are not attached
to descendants of v then
6: append t(v) ? {s(fw)} to Tv
7: end if
8: end for
9: for each corresponding span t(v) ? Tv do
10: R ? IDENTIFYMINRULES(v, t(v), T ) ? range
over the hyperedges of v, and discount the factional
count of each rule r ? R by 1/|Tv|
11: create a node n in DF for each rule r ? R
12: create a shared parent node ? when |R| > 1
13: end for
14: end for
3.2.3 The algorithm
Algorithm 1 outlines the proposed approach to
constructing a derivation forest to include multiple
interpretations of target function words. The deriva-
tion forest is a hypergraph as previously used in
(Galley et al, 2006), to maintain the constraint that
one unaligned target word be attached to some node
v exactly once in one derivation tree. Starting from
a triple ?FS , T, A?, we first tailor the alignment A
to A? by removing the alignments for target function
words. Then, we traverse the nodes v ? PA? in topo-
logical order. During the traversal, a function word
fw will be attached to v if 1) t(v) overlaps with the
span of the chunk to which fw belongs, and 2) fw
has not been attached to the descendants of v.
We identify translation rules that take v as the root
of their tree fragments. Each tree fragment is a fron-
tier tree that takes a node in the frontier set PA?
of FS as the root node and non-lexicalized frontier
nodes or lexicalized non-frontier nodes as the leaves.
Also, a minimal frontier tree used in a minimal rule
is limited to be a frontier tree such that all nodes
other than the root and leaves are non-frontier nodes.
We use Algorithm 1 described in (Mi and Huang,
2008) to collect minimal frontier trees rooted at v in
FS . That is, we range over each hyperedges headed
at v and continue to expand downward until the cur-
A ? (A?)
node s(?) t(?) c(?) consistent
c0 0-6 0-8(0-3,5-7) ? 1
c1 0-6 0-8(0-3,5-7) ? 1
c2 0-6 0-8(0-3,5-7) ? 1
c3 3-6 0-1,4-7(0-1, 5-7) 2,8 0
c4 3 5-7 0,8(0-3) 1
c5* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c6* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c7 0-1 2-3 0-1,4-8(0-1,5-7) 1
c8* 2-3 4-8(5-7) 0-4(0-3) 0(1)
c9 0 2 0-1,3-8(0-1,3,5-7) 1
c10 1 3 0-2,4-8(0-2,5-7) 1
c11 2-6 0-1,4-8(0-1,5-7) 2-3 0
c12 3 5-7 0,8(0-3) 1
c13* 5-6 0,4(0) 1-8(1-3,5-7) 0(1)
c14 5 4(?) 0-8(0-3,5-7) 0
c15 6 0 1-8(1-3,5-7) 1
c16 2 4,8(?) 0-7(0-3,5-7) 0
c17* 4-6 0,4(0-1) 2-8(2-3,5-7) 0(1)
c18 4 1 0,2-8(0,2-3,5-7) 1
c19 4 1 0,2-8(0,2-3,5-7) 1
c20* 0-3 2-8(2-3,5-7) 0,4(0-1) 0(1)
c21 3 5-7 0,8(0-3) 1
c22 2 4,8(?) 0-7(0-3,5-7) 0
c23* 2-3 4-8(5-7) 0-4(0-3) 0(1)
Table 1: Change of node attributes after alignment modi-
fication from A to A? of the example in Figure 1. Nodes
with * superscripts are consistent with A? but not consis-
tent with A.
rent set of hyperedges forms a minimal frontier tree.
In the derivation forest, we use ? nodes to man-
age minimal/composed rules that share the same
node and the same corresponding span. Figure 2
shows some minimal rule and ? nodes derived from
the example in Figure 1.
Even though we bind function words to their
nearby chunks, these function words may still be at-
tached to relative large tree fragments, so that richer
syntactic information can be used to predict the
function words. For example, in Figure 2, the tree
fragments rooted at node c0?80 can predict ga and/or
ta. The syntactic foundation behind is that, whether
to use ga as a subject particle or to use wo as an ob-
ject particle depends on both the left-hand-side noun
phrase (kekka) and the right-hand-side verb (kensyou
sa re ta). This type of node v? (such as c0?80 ) should
satisfy the following two heuristic conditions:
? v? is included in the frontier set PA? of FS , and
? t(v?) covers the function word, or v? is the root
node ofFS if the function word is the beginning
or ending word in the target sentence T .
Starting from this derivation forest with minimal
26
 c103-4 
t13: result 
kekka ga  
 
* c103 
t13: result 
kekka  
 
c92 
t32: the 
kono  
 
c72-3 
c103 c92 
x0 x1 
 x0 
 x1  
c72-4 
c103-4 c92 
x0 x1 
 x0  x1  *
c62-7 
c85-7 c72-3 
x0 ga x1  
 x0 
 x1  
* c62-7 
c85-7 c72-4 
x0 x1  
 x0 
 x1  * 
c00-8 
c16 
c45-7 c50-1 
c3 
c72-4 c11 x2 x0 x1 ta 
 
x0 
 
x1 
 
x2 
 
+ 
* 
c00-8 
c16 
c45-8 c50-1 
c3 
c72-4 c11 x2 x0 x1  
 
x0 
 
x1 
 
x2 
 + 
* 
c00-8 
c16 
c45-7 c50-1 
c3 
c72-3 c11 x2 x0 ga x1 ta 
 x0  
x1 
 
x2 
 
* + c00-8 
c16 
c45-8 c50-1 
c3 
c72-3 c11 x2 x0 ga x1 
 x0  
x1 
 
x2 
 
* 
+ 
t4{}:was t4{}:was t4{}:was t4{}:was 
Figure 2: Illustration of a (partial) derivation forest. Gray nodes include some unaligned target function word(s).
Nodes annotated by ?*? include ga, and nodes annotated by ?+? include ta.
rules as nodes, we can further combine two or more
minimal rules to form composed rules nodes and can
append these nodes to the derivation forest.
3.3 Estimating rule probabilities
We use the EM algorithm to jointly estimate 1)
the translation probabilities and fractional counts of
rules and 2) the scores of derivations in the deriva-
tion forests. As reported in (May and Knight, 2007),
EM, as has been used in (Galley et al, 2006) to es-
timate rule probabilities in derivation forests, is an
iterative procedure and prefers shorter derivations
containing large rules over longer derivations con-
taining small rules. In order to overcome this bias
problem, we discount the fractional count of a rule
by the product of the probabilities of parse hyper-
edges that are included in the tree fragment of the
rule.
4 Experiments
4.1 Setup
We implemented the forest-to-string decoder de-
scribed in (Mi et al, 2008) that makes use of forest-
based translation rules (Mi and Huang, 2008) as
the baseline system for translating English HPSG
forests into Japanese sentences. We analyzed the
performance of the proposed translation rule sets by
Train Dev. Test
# sentence pairs 994K 2K 2K
# En 1-best trees 987,401 1,982 1,984
# En forests 984,731 1,979 1,983
# En words 24.7M 50.3K 49.9K
# Jp words 28.2M 57.4K 57.1K
# Jp function words 8.0M 16.1K 16.1K
Table 2: Statistics of the JST corpus. Here, En = English
and Jp = Japanese.
using the same decoder.
The JST Japanese-English paper abstract corpus6
(Utiyama and Isahara, 2007), which consists of one
million parallel sentences, was used for training,
tuning, and testing. Table 2 shows the statistics of
this corpus. Note that Japanese function words oc-
cupy more than a quarter of the Japanese words.
Making use of Enju 2.3.1, we generated 987,401
1-best trees and 984,731 parse forests for the En-
glish sentences in the training set, with successful
parse rates of 99.3% and 99.1%, respectively. Us-
ing the pruning criteria expressed in (Mi and Huang,
2008), we continue to prune a parse forest by set-
ting pe to be 8, 5, and 2, until there are no more than
e10 = 22, 026 trees in a forest. After pruning, there
are an average of 82.3 trees in a parse forest.
6http://www.jst.go.jp
27
C3-T M&H-F Min-F C3-F
free fw Y N Y Y
alignment A? A A? A?
English side tree forest forest forest
# rule 86.30 96.52 144.91 228.59
# reorder rule 58.50 91.36 92.98 162.71
# tree types 21.62 93.55 72.98 120.08
# nodes/tree 14.2 42.1 26.3 18.6
extract time 30.2 52.2 58.6 130.7
EM time 9.4 - 11.2 29.0
# rules in dev. 0.77 1.22 1.37 2.18
# rules in test 0.77 1.23 1.37 2.15
DT(sec./sent.) 2.8 15.7 22.4 35.4
BLEU (%) 26.15 27.07 27.93 28.89
Table 3: Statistics and translation results for four types of
tree-to-string rules. With the exception of ?# nodes/tree?,
the numbers in the table are in millions and the time is in
hours. Here, fw denotes function word, and DT denotes
the decoding time, and the BLEU scores were computed
on the test set.
We performed GIZA++ (Och and Ney, 2003)
and the grow-diag-final-and symmetrizing strategy
(Koehn et al, 2007) on the training set to obtain
alignments. The SRI Language Modeling Toolkit
(Stolcke, 2002) was employed to train a five-gram
Japanese LM on the training set. We evaluated the
translation quality using the BLEU-4 metric (Pap-
ineni et al, 2002).
Joshua v1.3 (Li et al, 2009), which is a
freely available decoder for hierarchical phrase-
based SMT (Chiang, 2005), is used as an external
baseline system for comparison. We extracted 4.5M
translation rules from the training set for the 4K En-
glish sentences in the development and test sets. We
used the default configuration of Joshua, with the ex-
ception of the maximum number of items/rules, and
the value of k (of the k-best outputs) is set to be 200.
4.2 Results
Table 3 lists the statistics of the following translation
rule sets:
? C3-T: a composed rule set extracted from the
derivation forests of 1-best HPSG trees that
were constructed using the approach described
in (Galley et al, 2006). The maximum number
of internal nodes is set to be three when gen-
erating a composed rule. We free attach target
function words to derivation forests;
0
5
10
15
20
25
2 12 22 32 42 52 62 72 82 92
# o
f ru
les 
(M
)
# of tree nodes in rule
M&H-F
Min-F
C3-T
C3-F
Figure 3: Distributions of the number of tree nodes in the
translation rule sets. Note that the curves of Min-F and
C3-F are duplicated when the number of tree nodes being
larger than 9.
? M&H-F: a minimal rule set extracted from
HPSG forests using the extracting algorithm of
(Mi and Huang, 2008). Here, we make use of
the original alignments. We use the two heuris-
tic conditions described in Section 3.2.3 to at-
tach unaligned words to some node(s) in the
forest;
? Min-F: a minimal rule set extracted from the
derivation forests of HPSG forests that were
constructed using Algorithm 1 (Section 3).
? C3-F: a composed rule set extracted from the
derivation forests of HPSG forests. Similar to
C3-T, the maximum number of internal nodes
during combination is three.
We investigate the generalization ability of these
rule sets through the following aspects:
1. the number of rules, the number of reordering
rules, and the distributions of the number of
tree nodes (Figure 3), i.e., more rules with rel-
atively small tree fragments are preferred;
2. the number of rules that are applicable to the
development and test sets (Table 3); and
3. the final translation accuracies.
Table 3 and Figure 3 reflect that the generalization
abilities of these four rule sets increase in the or-
der of C3-T < M&H-F < Min-F < C3-F. The ad-
vantage of using a packed forest for re-alignment is
verified by comparing the statistics of the rules and
28
0
10
20
30
40
50
0.0
0.5
1.0
1.5
2.0
2.5
C3-T M&H-F Min-F C3-F
Dec
odi
ng t
ime
 (se
c./s
ent
.)
# o
f ru
les 
(M)
# rules (M)
DT
Figure 4: Comparison of decoding time and the number
of rules used for translating the test set.
the final BLEU scores of C3-T with Min-F and C3-
F. Using the composed rule set C3-F in our forest-
based decoder, we achieved an optimal BLEU score
of 28.89 (%). Taking M&H-F as the baseline trans-
lation rule set, we achieved a significant improve-
ment (p < 0.01) of 1.81 points.
In terms of decoding time, even though we used
Algorithm 3 described in (Huang and Chiang, 2005),
which lazily generated the N-best translation can-
didates, the decoding time tended to be increased
because more rules were available during cube-
pruning. Figure 4 shows a comparison of decoding
time (seconds per sentence) and the number of rules
used for translating the test set. Easy to observe that,
decoding time increases in a nearly linear way fol-
lowing the increase of the number of rules used dur-
ing decoding.
Finally, compared with Joshua, which achieved
a BLEU score of 24.79 (%) on the test set with
a decoding speed of 8.8 seconds per sentence, our
forest-based decoder achieved a significantly better
(p < 0.01) BLEU score by using either of the four
types of translation rules.
5 Related Research
Galley et al (2006) first used derivation forests of
aligned tree-string pairs to express multiple inter-
pretations of unaligned target words. The EM al-
gorithm was used to jointly estimate 1) the trans-
lation probabilities and fractional counts of rules
and 2) the scores of derivations in the derivation
forests. By dealing with the ambiguous word align-
ment instead of unaligned target words, syntax-
based re-alignment models were proposed by (May
and Knight, 2007; Wang et al, 2010) for tree-based
translations.
Free attachment of the unaligned target word
problem was ignored in (Mi and Huang, 2008),
which was the first study on extracting tree-to-string
rules from aligned forest-string pairs. This inspired
the idea to re-align a packed forest and a target sen-
tence. Specially, we observed that most incorrect or
ambiguous word alignments are caused by function
words rather than content words. Thus, we focus on
the realignment of target function words to source
tree fragments and use a dependency parser to limit
the attachments of unaligned target words.
6 Conclusion
We have proposed an effective use of target function
words for extracting generalized transducer rules for
forest-based translation. We extend the unaligned
word approach described in (Galley et al, 2006)
from the 1-best tree to the packed parse forest. A
simple yet effective modification is that, during rule
extraction, we account for multiple interpretations
of both aligned and unaligned target function words.
That is, we chose to loose the ambiguous alignments
for all of the target function words. The consider-
ation behind is in order to generate target function
words in a robust manner. In order to avoid gener-
ating too large a derivation forest for a packed for-
est, we further used chunk-level information yielded
by a target dependency parser. Extensive experi-
ments on large-scale English-to-Japanese translation
resulted in a significant improvement in BLEU score
of 1.8 points (p < 0.01), as compared with our
implementation of a strong forest-to-string baseline
system (Mi et al, 2008; Mi and Huang, 2008).
The present work only re-aligns target function
words to source tree fragments. It will be valuable
to investigate the feasibility to re-align all the tar-
get words to source tree fragments. Also, it is in-
teresting to automatically learn a word set for re-
aligning7. Given source parse forests and a target
word set for re-aligning beforehand, we argue our
approach is generic and applicable to any language
pairs. Finally, we intend to extend the proposed
approach to tree-to-tree translation frameworks by
7This idea comes from one reviewer, we express our thank-
fulness here.
29
re-aligning subtree pairs (Liu et al, 2009; Chiang,
2010) and consistency-to-dependency frameworks
by re-aligning consistency-tree-to-dependency-tree
pairs (Mi and Liu, 2010) in order to tackle the rule-
sparseness problem.
Acknowledgments
The present study was supported in part by a Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan), by the Japanese/Chinese Machine Transla-
tion Project through Special Coordination Funds for
Promoting Science and Technology (MEXT, Japan),
and by Microsoft Research Asia Machine Transla-
tion Theme.
Wu (wu.xianchao@lab.ntt.co.jp) has
moved to NTT Communication Science Laborato-
ries and Tsujii (junichi.tsujii@live.com)
has moved to Microsoft Research Asia.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
39:1?38.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL-2002, pages 63?69. Taipei, Taiwan.
Zhifei Li, Chris Callison-Burch, Chris Dyery, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Demonstration of joshua: An open source
toolkit for parsing-based machine translation. In Pro-
ceedings of the ACL-IJCNLP 2009 Software Demon-
strations, pages 25?28, August.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Samuel E. Martin. 1975. A Reference Grammar of
Japanese. New Haven, Conn.: Yale University Press.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 360?368, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214, October.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1433?1442, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Lingustics, 34(1):35?80.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
30
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
Number 152 in CSLI Lecture Notes. CSLI Publica-
tions.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In Proceedings of MT
Summit XI, pages 475?482, Copenhagen.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247?277.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 325?334, Uppsala, Sweden, July. Association
for Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceedings
of HLT-NAACL, pages 245?253.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT Summit XI, pages 535?542, Copen-
hagen, Denmark, September.
31
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045?1053,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Incremental Joint Approach to Word Segmentation, POS Tagging, and
Dependency Parsing in Chinese
Jun Hatori1 Takuya Matsuzaki2 Yusuke Miyao2 Jun?ichi Tsujii3
1University of Tokyo / 7-3-1 Hongo, Bunkyo, Tokyo, Japan
2National Institute of Informatics / 2-1-2 Hitotsubashi, Chiyoda, Tokyo, Japan
3Microsoft Research Asia / 5 Danling Street, Haidian District, Beijing, P.R. China
hatori@is.s.u-tokyo.ac.jp
{takuya-matsuzaki,yusuke}@nii.ac.jp jtsujii@microsoft.com
Abstract
We propose the first joint model for word segmen-
tation, POS tagging, and dependency parsing for
Chinese. Based on an extension of the incremental
joint model for POS tagging and dependency pars-
ing (Hatori et al, 2011), we propose an efficient
character-based decoding method that can combine
features from state-of-the-art segmentation, POS
tagging, and dependency parsing models. We also
describe our method to align comparable states in
the beam, and how we can combine features of dif-
ferent characteristics in our incremental framework.
In experiments using the Chinese Treebank (CTB),
we show that the accuracies of the three tasks can
be improved significantly over the baseline models,
particularly by 0.6% for POS tagging and 2.4% for
dependency parsing. We also perform comparison
experiments with the partially joint models.
1 Introduction
In processing natural languages that do not include
delimiters (e.g. spaces) between words, word seg-
mentation is the crucial first step that is necessary
to perform virtually all NLP tasks. Furthermore, the
word-level information is often augmented with the
POS tags, which, along with segmentation, form the
basic foundation of statistical NLP.
Because the tasks of word segmentation and POS
tagging have strong interactions, many studies have
been devoted to the task of joint word segmenta-
tion and POS tagging for languages such as Chi-
nese (e.g. Kruengkrai et al (2009)). This is because
some of the segmentation ambiguities cannot be re-
solved without considering the surrounding gram-
matical constructions encoded in a sequence of POS
tags. The joint approach to word segmentation and
POS tagging has been reported to improve word seg-
mentation and POS tagging accuracies by more than
1% in Chinese (Zhang and Clark, 2008). In addition,
some researchers recently proposed a joint approach
to Chinese POS tagging and dependency parsing (Li
et al, 2011; Hatori et al, 2011); particularly, Ha-
tori et al (2011) proposed an incremental approach
to this joint task, and showed that the joint approach
improves the accuracies of these two tasks.
In this context, it is natural to consider further
a question regarding the joint framework: how
strongly do the tasks of word segmentation and de-
pendency parsing interact? In the following Chinese
sentences:
S? ?sV  ?s ?Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 127?132,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Akamon: An Open Source Toolkit
for Tree/Forest-Based Statistical Machine Translation?
Xianchao Wu?, Takuya Matsuzaki?, Jun?ichi Tsujii?
? Baidu Inc.
?National Institute of Informatics
? Microsoft Research Asia
wuxianchao@gmail.com,takuya-matsuzaki@nii.ac.jp,jtsujii@microsoft.com
Abstract
We describe Akamon, an open source toolkit
for tree and forest-based statistical machine
translation (Liu et al, 2006; Mi et al, 2008;
Mi and Huang, 2008). Akamon implements
all of the algorithms required for tree/forest-
to-string decoding using tree-to-string trans-
lation rules: multiple-thread forest-based de-
coding, n-gram language model integration,
beam- and cube-pruning, k-best hypotheses
extraction, and minimum error rate training.
In terms of tree-to-string translation rule ex-
traction, the toolkit implements the tradi-
tional maximum likelihood algorithm using
PCFG trees (Galley et al, 2004) and HPSG
trees/forests (Wu et al, 2010).
1 Introduction
Syntax-based statistical machine translation (SMT)
systems have achieved promising improvements in
recent years. Depending on the type of input, the
systems are divided into two categories: string-
based systems whose input is a string to be simul-
taneously parsed and translated by a synchronous
grammar (Wu, 1997; Chiang, 2005; Galley et al,
2006; Shen et al, 2008), and tree/forest-based sys-
tems whose input is already a parse tree or a packed
forest to be directly converted into a target tree or
string (Ding and Palmer, 2005; Quirk et al, 2005;
Liu et al, 2006; Huang et al, 2006; Mi et al, 2008;
Mi and Huang, 2008; Zhang et al, 2009; Wu et al,
2010; Wu et al, 2011a).
?Work done when all the authors were in The University of
Tokyo.
Depending on whether or not parsers are explic-
itly used for obtaining linguistically annotated data
during training, the systems are also divided into two
categories: formally syntax-based systems that do
not use additional parsers (Wu, 1997; Chiang, 2005;
Xiong et al, 2006), and linguistically syntax-based
systems that use PCFG parsers (Liu et al, 2006;
Huang et al, 2006; Galley et al, 2006; Mi et al,
2008; Mi and Huang, 2008; Zhang et al, 2009),
HPSG parsers (Wu et al, 2010; Wu et al, 2011a), or
dependency parsers (Ding and Palmer, 2005; Quirk
et al, 2005; Shen et al, 2008). A classification1 of
syntax-based SMT systems is shown in Table 1.
Translation rules can be extracted from aligned
string-string (Chiang, 2005), tree-tree (Ding and
Palmer, 2005) and tree/forest-string (Galley et al,
2004; Mi and Huang, 2008; Wu et al, 2011a)
data structures. Leveraging structural and linguis-
tic information from parse trees/forests, the latter
two structures are believed to be better than their
string-string counterparts in handling non-local re-
ordering, and have achieved promising translation
results. Moreover, the tree/forest-string structure is
more widely used than the tree-tree structure, pre-
sumably because using two parsers on the source
and target languages is subject to more problems
than making use of a parser on one language, such
as the shortage of high precision/recall parsers for
languages other than English, compound parse error
rates, and inconsistency of errors. In Table 1, note
that tree-to-string rules are generic and applicable
to many syntax-based models such as tree/forest-to-
1This classification is inspired by and extends the Table 1 in
(Mi and Huang, 2008).
127
Source-to-target Examples (partial) Decoding Rules Parser
tree-to-tree (Ding and Palmer, 2005) ? dep.-to-dep. DG
forest-to-tree (Liu et al, 2009a) ? ?? tree-to-tree PCFG
tree-to-string (Liu et al, 2006) ? tree-to-string PCFG
(Quirk et al, 2005) ? dep.-to-string DG
forest-to-string (Mi et al, 2008) ? ?? tree-to-string PCFG
(Wu et al, 2011a) ? ?? tree-to-string HPSG
string-to-tree (Galley et al, 2006) CKY tree-to-string PCFG
(Shen et al, 2008) CKY string-to-dep. DG
string-to-string (Chiang, 2005) CKY string-to-string none
(Xiong et al, 2006) CKY string-to-string none
Table 1: A classification of syntax-based SMT systems. Tree/forest-based and string-based systems are split by a line.
All the systems listed here are linguistically syntax-based except the last two (Chiang, 2005) and (Xiong et al, 2006),
which are formally syntax-based. DG stands for dependency (abbreviated as dep.) grammar. ? and ? denote top-down
and bottom-up traversals of a source tree/forest.
string models and string-to-tree model.
However, few tree/forest-to-string systems have
been made open source and this makes it diffi-
cult and time-consuming to testify and follow exist-
ing proposals involved in recently published papers.
The Akamon system2, written in Java and follow-
ing the tree/forest-to-string research direction, im-
plements all of the algorithms for both tree-to-string
translation rule extraction (Galley et al, 2004; Mi
and Huang, 2008; Wu et al, 2010; Wu et al, 2011a)
and tree/forest-based decoding (Liu et al, 2006; Mi
et al, 2008). We hope this system will help re-
lated researchers to catch up with the achievements
of tree/forest-based translations in the past several
years without re-implementing the systems or gen-
eral algorithms from scratch.
2 Akamon Toolkit Features
Limited by the successful parsing rate and coverage
of linguistic phrases, Akamon currently achieves
comparable translation accuracies compared with
the most frequently used SMT baseline system,
Moses (Koehn et al, 2007). Table 2 shows the auto-
matic translation accuracies (case-sensitive) of Aka-
mon and Moses. Besides BLEU and NIST score, we
further list RIBES score3, , i.e., the software imple-
mentation of Normalized Kendall?s ? as proposed by
(Isozaki et al, 2010a) to automatically evaluate the
translation between distant language pairs based on
rank correlation coefficients and significantly penal-
2Code available at https://sites.google.com/site/xianchaowu2012
3Code available at http://www.kecl.ntt.co.jp/icl/lirg/ribes
izes word order mistakes.
In this table, Akamon-Forest differs from
Akamon-Comb by using different configurations:
Akamon-Forest used only 2/3 of the total training
data (limited by the experiment environments and
time). Akamon-Comb represents the system com-
bination result by combining Akamon-Forest and
other phrase-based SMT systems, which made use
of pre-ordering methods of head finalization as de-
scribed in (Isozaki et al, 2010b) and used the total 3
million training data. The detail of the pre-ordering
approach and the combination method can be found
in (Sudoh et al, 2011) and (Duh et al, 2011).
Also, Moses (hierarchical) stands for the hi-
erarchical phrase-based SMT system and Moses
(phrase) stands for the flat phrase-based SMT sys-
tem. For intuitive comparison (note that the result
achieved by Google is only for reference and not a
comparison, since it uses a different and unknown
training data) and following (Goto et al, 2011), the
scores achieved by using the Google online transla-
tion system4 are also listed in this table.
Here is a brief description of Akamon?s main fea-
tures:
? multiple-thread forest-based decoding: Aka-
mon first loads the development (with source
and reference sentences) or test (with source
sentences only) file into memory and then per-
form parameter tuning or decoding in a paral-
lel way. The forest-based decoding algorithm
is alike that described in (Mi et al, 2008),
4http://translate.google.com/
128
Systems BLEU NIST RIBES
Google online 0.2546 6.830 0.6991
Moses (hierarchical) 0.3166 7.795 0.7200
Moses (phrase) 0.3190 7.881 0.7068
Moses (phrase)* 0.2773 6.905 0.6619
Akamon-Forest* 0.2799 7.258 0.6861
Akamon-Comb 0.3948 8.713 0.7813
Table 2: Translation accuracies of Akamon and the base-
line systems on the NTCIR-9 English-to-Japanese trans-
lation task (Wu et al, 2011b). * stands for only using
2 million parallel sentences of the total 3 million data.
Here, HPSG forests were used in Akamon.
i.e., first construct a translation forest by ap-
plying the tree-to-string translation rules to the
original parsing forest of the source sentence,
and then collect k-best hypotheses for the root
node(s) of the translation forest using Algo-
rithm 2 or Algorithm 3 as described in (Huang
and Chiang, 2005). Later, the k-best hypothe-
ses are used both for parameter tuning on addi-
tional development set(s) and for final optimal
translation result extracting.
? language models: Akamon can make use of
one or many n-gram language models trained
by using SRILM5 (Stolcke, 2002) or the Berke-
ley language model toolkit, berkeleylm-1.0b36
(Pauls and Klein, 2011). The weights of multi-
ple language models are tuned under minimum
error rate training (MERT) (Och, 2003).
? pruning: traditional beam-pruning and cube-
pruning (Chiang, 2007) techniques are incor-
porated in Akamon to make decoding feasi-
ble for large-scale rule sets. Before decoding,
we also perform the marginal probability-based
inside-outside algorithm based pruning (Mi et
al., 2008) on the original parsing forest to con-
trol the decoding time.
? MERT: Akamon has its own MERT module
which optimizes weights of the features so as
to maximize some automatic evaluation metric,
such as BLEU (Papineni et al, 2002), on a de-
velopment set.
5http://www.speech.sri.com/projects/srilm/
6http://code.google.com/p/berkeleylm/
 
 
 
e.tok 
corpus 
f.seg 
tokenize word segment 
e.tok.lw f.seg.lw 
lowercase lowercase 
clean 
e.clean f.clean 
GIZA++ 
alignment 
Rule set 
rule extraction 
SRILM 
Akamon Decoder (MERT) 
N-gram LM 
e.tok 
dev.e 
tokenize 
e.tok.lw 
lowercase 
e.forests 
Enju 
e.forests 
Enju 
dev 
f.seg 
dev.f 
word 
segmentation 
f.seg.lw 
lowercase 
pre-processing 
Figure 1: Training and tuning process of the Akamon sys-
tem. Here, e = source English language, f = target foreign
language.
? translation rule extraction: as former men-
tioned, we extract tree-to-string translation
rules for Akamon. In particular, we imple-
mented the GHKM algorithm as proposed by
Galley et al (2004) from word-aligned tree-
string pairs. In addition, we also implemented
the algorithms proposed by Mi and Huang
(2008) and Wu et al (2010) for extracting rules
from word-aligned PCFG/HPSG forest-string
pairs.
3 Training and Decoding Frameworks
Figure 1 shows the training and tuning progress of
the Akamon system. Given original bilingual par-
allel corpora, we first tokenize and lowercase the
source and target sentences (e.g., word segmentation
of Chinese and Japanese, punctuation segmentation
of English).
The pre-processed monolingual sentences will be
used by SRILM (Stolcke, 2002) or BerkeleyLM
(Pauls and Klein, 2011) to train a n-gram language
model. In addition, we filter out too long sentences
129
here, i.e., only relatively short sentence pairs will be
used to train word alignments. Then, we can use
GIZA++ (Och and Ney, 2003) and symmetric strate-
gies, such as grow-diag-final (Koehn et al, 2007),
on the tokenized parallel corpus to obtain a word-
aligned parallel corpus.
The source sentence and its packed forest, the tar-
get sentence, and the word alignment are used for
tree-to-string translation rule extraction. Since a 1-
best tree is a special case of a packed forest, we will
focus on using the term ?forest? in the continuing
discussion. Then, taking the target language model,
the rule set, and the preprocessed development set
as inputs, we perform MERT on the decoder to tune
the weights of the features.
The Akamon forest-to-string system includes the
decoding algorithm and the rule extraction algorithm
described in (Mi et al, 2008; Mi and Huang, 2008).
4 Using Deep Syntactic Structures
In Akamon, we support the usage of deep syn-
tactic structures for obtaining fine-grained transla-
tion rules as described in our former work (Wu et
al., 2010)7. Similarly, Enju8, a state-of-the-art and
freely available HPSG parser for English, can be
used to generate packed parse forests for source
sentences9. Deep syntactic structures are included
in the HPSG trees/forests, which includes a fine-
grained description of the syntactic property and a
semantic representation of the sentence. We extract
fine-grained rules from aligned HPSG forest-string
pairs and use them in the forest-to-string decoder.
The detailed algorithms can be found in (Wu et al,
2010; Wu et al, 2011a). Note that, in Akamon, we
also provide the codes for generating HPSG forests
from Enju.
Head-driven phrase structure grammar (HPSG) is
a lexicalist grammar framework. In HPSG, linguis-
tic entities such as words and phrases are represented
by a data structure called a sign. A sign gives a
7However, Akamon still support PCFG tree/forest based
translation. A special case is to yield PCFG style trees/forests
by ignoring the rich features included in the nodes of HPSG
trees/forests and only keep the POS tag and the phrasal cate-
gories.
8http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
9Until the date this paper was submitted, Enju supports gen-
erating English and Chinese forests.
Feature Description
CAT phrasal category
XCAT fine-grained phrasal category
SCHEMA name of the schema applied in the node
HEAD pointer to the head daughter
SEM HEAD pointer to the semantic head daughter
CAT syntactic category
POS Penn Treebank-style part-of-speech tag
BASE base form
TENSE tense of a verb (past, present, untensed)
ASPECT aspect of a verb (none, perfect,
progressive, perfect-progressive)
VOICE voice of a verb (passive, active)
AUX auxiliary verb or not (minus, modal,
have, be, do, to, copular)
LEXENTRY lexical entry, with supertags embedded
PRED type of a predicate
ARG?x? pointer to semantic arguments, x = 1..4
Table 3: Syntactic/semantic features extracted from
HPSG signs that are included in the output of Enju. Fea-
tures in phrasal nodes (top) and lexical nodes (bottom)
are listed separately.
factored representation of the syntactic features of
a word/phrase, as well as a representation of their
semantic content. Phrases and words represented by
signs are composed into larger phrases by applica-
tions of schemata. The semantic representation of
the new phrase is calculated at the same time. As
such, an HPSG parse tree/forest can be considered
as a tree/forest of signs (c.f. the HPSG forest in Fig-
ure 2 in (Wu et al, 2010)).
An HPSG parse tree/forest has two attractive
properties as a representation of a source sentence
in syntax-based SMT. First, we can carefully control
the condition of the application of a translation rule
by exploiting the fine-grained syntactic description
in the source parse tree/forest, as well as those in the
translation rules. Second, we can identify sub-trees
in a parse tree/forest that correspond to basic units
of the semantics, namely sub-trees covering a pred-
icate and its arguments, by using the semantic rep-
resentation given in the signs. Extraction of trans-
lation rules based on such semantically-connected
sub-trees is expected to give a compact and effective
set of translation rules.
A sign in the HPSG tree/forest is represented by a
typed feature structure (TFS) (Carpenter, 1992). A
TFS is a directed-acyclic graph (DAG) wherein the
edges are labeled with feature names and the nodes
130
  
She 
ignore 
 fact 
want 
I 
dispute 
ARG1 
ARG2 
ARG1 ARG1 
ARG2 
ARG2 
John 
kill 
 Mary ARG2 
ARG1 
Figure 2: Predicate argument structures for the sentences
of ?John killed Mary? and ?She ignored the fact that I
wanted to dispute?.
(feature values) are typed. In the original HPSG for-
malism, the types are defined in a hierarchy and the
DAG can have arbitrary shape (e.g., it can be of any
depth). We however use a simplified form of TFS,
for simplicity of the algorithms. In the simplified
form, a TFS is converted to a (flat) set of pairs of
feature names and their values. Table 3 lists the fea-
tures used in our system, which are a subset of those
in the original output from Enju.
In the Enju English HPSG grammar (Miyao et
al., 2003) used in our system, the semantic content
of a sentence/phrase is represented by a predicate-
argument structure (PAS). Figure 2 shows the PAS
of a simple sentence, ?John killed Mary?, and a more
complex PAS for another sentence, ?She ignored the
fact that I wanted to dispute?, which is adopted from
(Miyao et al, 2003). In an HPSG tree/forest, each
leaf node generally introduces a predicate, which
is represented by the pair of LEXENTRY (lexical
entry) feature and PRED (predicate type) feature.
The arguments of a predicate are designated by the
pointers from the ARG?x? features in a leaf node
to non-terminal nodes. Consequently, Akamon in-
cludes the algorithm for extracting compact com-
posed rules from these PASs which further lead to
a significant fast tree-to-string decoder. This is be-
cause it is not necessary to exhaustively generate the
subtrees for all the tree nodes for rule matching any
more. Limited by space, we suggest the readers to
refer to our former work (Wu et al, 2010; Wu et al,
2011a) for the experimental results, including the
training and decoding time using standard English-
to-Japanese corpora, by using deep syntactic struc-
tures.
5 Content of the Demonstration
In the demonstration, we would like to provide a
brief tutorial on:
? describing the format of the packed forest for a
source sentence,
? the training script on translation rule extraction,
? the MERT script on feature weight tuning on a
development set, and,
? the decoding script on a test set.
Based on Akamon, there are a lot of interesting
directions left to be updated in a relatively fast way
in the near future, such as:
? integrate target dependency structures, espe-
cially target dependency language models, as
proposed by Mi and Liu (2010),
? better pruning strategies for the input packed
forest before decoding,
? derivation-based combination of using other
types of translation rules in one decoder, as pro-
posed by Liu et al (2009b), and
? taking other evaluation metrics as the opti-
mal objective for MERT, such as NIST score,
RIBES score (Isozaki et al, 2010a).
Acknowledgments
We thank Yusuke Miyao and Naoaki Okazaki for
their invaluable help and the anonymous reviewers
for their comments and suggestions.
References
Bob Carpenter. 1992. The Logic of Typed Feature Struc-
tures. Cambridge University Press.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Lingustics, 33(2):201?228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammers. In Proceedings of ACL, pages 541?
548, Ann Arbor.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2011. Generalized
minimum bayes risk system combination. In Proceed-
ings of IJCNLP, pages 1356?1360, November.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
131
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961?968, Sydney.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent ma-
chine translation task at the ntcir-9 workshop. In Pro-
ceedings of NTCIR-9, pages 559?578.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of 7th AMTA.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic eval-
uation of translation quality for distant language pairs.
In Proc.of EMNLP, pages 944?952.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMT-MetricsMATR, pages 244?251, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions, pages
177?180.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
transaltion. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009a. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL-IJCNLP, pages 558?566, August.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b.
Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, August.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP, pages
206?214, October.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of ACL,
pages 1433?1442, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003. Probabilistic modeling of argument structures
including non-local dependencies. In Proceedings of
RANLP, pages 285?291, Borovets.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL-HLT,
pages 258?267, June.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08:HLT, pages 577?585.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, pages
901?904.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki
Nagata, Xianchao Wu, Takuya Matsuzaki, and
Jun?ichi Tsujii. 2011. Ntt-ut statistical machine trans-
lation in ntcir-9 patentmt. In Proceedings of NTCIR-9
Workshop Meeting, pages 585?592, December.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2010. Fine-grained tree-to-string translation rule ex-
traction. In Proceedings of ACL, pages 325?334, July.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2011a. Effective use of function words for rule gen-
eralization in forest-based translation. In Proceedings
of ACL-HLT, pages 22?31, June.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.
2011b. Smt systems in the university of tokyo for
ntcir-9 patentmt. In Proceedings of NTCIR-9 Work-
shop Meeting, pages 666?672, December.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proceedings of COLING-
ACL, pages 521?528, July.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of ACL-
IJCNLP, pages 172?180, Suntec, Singapore, August.
132
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042?1051,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Integrating Multiple Dependency Corpora
for Inducing Wide-coverage Japanese CCG Resources
Sumire Uematsu?
uematsu@cks.u-tokyo.ac.jp
Takuya Matsuzaki?
takuya-matsuzaki@nii.ac.jp
Hiroki Hanaoka?
hanaoka@nii.ac.jp
Yusuke Miyao?
yusuke@nii.ac.jp
Hideki Mima?
mima@t-adm.t.u-tokyo.ac.jp
?The University of Tokyo
Hongo 7-3-1, Bunkyo, Tokyo, Japan
?National Institute of Infomatics
Hitotsubashi 2-1-2, Chiyoda, Tokyo, Japan
Abstract
This paper describes a method of in-
ducing wide-coverage CCG resources for
Japanese. While deep parsers with corpus-
induced grammars have been emerging
for some languages, those for Japanese
have not been widely studied, mainly be-
cause most Japanese syntactic resources
are dependency-based. Our method first
integrates multiple dependency-based cor-
pora into phrase structure trees and then
converts the trees into CCG derivations.
The method is empirically evaluated in
terms of the coverage of the obtained lexi-
con and the accuracy of parsing.
1 Introduction
Syntactic parsing for Japanese has been domi-
nated by a dependency-based pipeline in which
chunk-based dependency parsing is applied and
then semantic role labeling is performed on the de-
pendencies (Sasano and Kurohashi, 2011; Kawa-
hara and Kurohashi, 2011; Kudo and Matsumoto,
2002; Iida and Poesio, 2011; Hayashibe et al,
2011). This dominance is mainly because chunk-
based dependency analysis looks most appropriate
for Japanese syntax due to its morphosyntactic ty-
pology, which includes agglutination and scram-
bling (Bekki, 2010). However, it is also true that
this type of analysis has prevented us from deeper
syntactic analysis such as deep parsing (Clark and
Curran, 2007) and logical inference (Bos et al,
2004; Bos, 2007), both of which have been sur-
passing shallow parsing-based approaches in lan-
guages like English.
In this paper, we present our work on induc-
ing wide-coverage Japanese resources based on
combinatory categorial grammar (CCG) (Steed-
man, 2001). Our work is basically an extension of
a seminal work on CCGbank (Hockenmaier and
Steedman, 2007), in which the phrase structure
trees of the Penn Treebank (PTB) (Marcus et al,
1993) are converted into CCG derivations and a
wide-coverage CCG lexicon is then extracted from
these derivations. As CCGbank has enabled a va-
riety of outstanding works on wide-coverage deep
parsing for English, our resources are expected to
significantly contribute to Japanese deep parsing.
The application of the CCGbank method to
Japanese is not trivial, as resources like PTB are
not available in Japanese. The widely used re-
sources for parsing research are the Kyoto corpus
(Kawahara et al, 2002) and the NAIST text corpus
(Iida et al, 2007), both of which are based on the
dependency structures of chunks. Moreover, the
relation between chunk-based dependency struc-
tures and CCG derivations is not obvious.
In this work, we propose a method to integrate
multiple dependency-based corpora into phrase
structure trees augmented with predicate argument
relations. We can then convert the phrase structure
trees into CCG derivations. In the following, we
describe the details of the integration method as
well as Japanese-specific issues in the conversion
into CCG derivations. The method is empirically
evaluated in terms of the quality of the corpus con-
version, the coverage of the obtained lexicon, and
the accuracy of parsing with the obtained gram-
mar. Additionally, we discuss problems that re-
main in Japanese resources from the viewpoint of
developing CCG derivations.
There are three primary contributions of this pa-
per: 1) we show the first comprehensive results for
Japanese CCG parsing, 2) we present a methodol-
ogy for integrating multiple dependency-based re-
1042
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 1: A CCG derivation.
X/Y : f Y : a ? X : fa (>)
Y : a X\Y : a ? X : fa (<)
X/Y : f Y/Z : g ? X/Z : ?x.f(gx) (> B)
Y\Z : g X\Y : f ? X\Z : ?x.f(gx) (< B)
Figure 2: Combinatory rules (used in the current
implementation).
sources to induce CCG derivations, and 3) we in-
vestigate the possibility of further improving CCG
analysis by additional resources.
2 Background
2.1 Combinatory Categorial Grammar
CCG is a syntactic theory widely accepted in the
NLP field. A grammar based on CCG theory con-
sists of categories, which represent syntactic cat-
egories of words and phrases, and combinatory
rules, which are rules to combine the categories.
Categories are either ground categories like S and
NP or complex categories in the form of X/Y or
X\Y , where X and Y are the categories. Cate-
gory X/Y intuitively means that it becomes cat-
egory X when it is combined with another cat-
egory Y to its right, and X\Y means it takes a
category Y to its left. Categories are combined
by applying combinatory rules (Fig. 2) to form
categories for larger phrases. Figure 1 shows a
CCG analysis of a simple English sentence, which
is called a derivation. The verb give is assigned
category S\NP/NP/NP , which indicates that it
takes two NPs to its right, one NP to its left, and fi-
nally becomes S. Starting from lexical categories
assigned to words, we can obtain categories for
phrases by applying the rules recursively.
An important property of CCG is a clear inter-
face between syntax and semantics. As shown in
Fig. 1, each category is associated with a lambda
term of semantic representations, and each com-
binatory rule is associated with rules for semantic
composition. Since these rules are universal, we
can obtain different semantic representations by
switching the semantic representations of lexical
categories. This means that we can plug in a vari-
Sentence S Verb S\$ (e.g. S\NPga)
Noun phrase NP Post particle NPga|o|ni|to\NP
Auxiliary verb S\S
Table 1: Typical categories for Japanese syntax.
Cat. Feature Value Interpretation
NP case ga nominal
o accusative
ni dative
to comitative, complementizer, etc.
nc none
S form stem stem
base base
neg imperfect or negative
cont continuative
vo s causative
Table 2: Features for Japanese syntax (those used
in the examples in this paper).
ety of semantic theories with CCG-based syntactic
parsing (Bos et al, 2004).
2.2 CCG-based syntactic theory for Japanese
Bekki (2010) proposed a comprehensive theory
for Japanese syntax based on CCG. While the the-
ory is based on Steedman (2001), it provides con-
crete explanations for a variety of constructions of
Japanese, such as agglutination, scrambling, long-
distance dependencies, etc. (Fig. 3).
The ground categories in his theory are S, NP,
and CONJ (for conjunctions). Table 1 presents
typical lexical categories. While most of them
are obvious from the theory of CCG, categories
for auxiliary verbs require an explanation. In
Japanese, auxiliary verbs are extensively used to
express various semantic information, such as
tense and modality. They agglutinate to the main
verb in a sequential order. This is explained in
Bekki?s theory by the category S\S combined with
a main verb via the function composition rule
(<B). Syntactic features are assigned to categories
NP and S (Table 2). The feature case represents a
syntactic case of a noun phrase. The feature form
denotes an inflection form, and is necessary for
constraining the grammaticality of agglutination.
Our implementation of the grammar basically
follows Bekki (2010)?s theory. However, as a first
step in implementing a wide-coverage Japanese
parser, we focused on the frequent syntactic con-
structions that are necessary for computing pred-
icate argument relations, including agglutination,
inflection, scrambling, case alternation, etc. Other
details of the theory are largely simplified (Fig. 3),
1043
I
NP: I?
NPy: I?
NP: I?
NP: I?
give
S\NP/NP/NP :
?x?y?z.give?yxz
them
NP :them?
NP :ythem? >S\NP/NP :?y?z.give?y them?z
money
NP :money?
NP :money?
NP :money? >S\NP :?z.give?money?them?z <S :give?money?them?I ?
??
ambassador
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPni
??
participation
Sstem\NPga\NPni
?
do-CONT
Scont\Sstem <BScont\NPga\NPni
?
PAST-BASE
Sbase\Scont
Sbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase
??
government
NPnc
?
NOM
NPga\NPnc <NPga
NPga
NPga
??
ambassador
NPnc
?
ACC
NPwo\NPnc <NPwo
NPwo
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
???
participation
Svo s\NPga\NPni
?
CAUSE
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
government
NP
?
NOM
NPga\NP <NPga
NPga
NPga
???
ambassador-ACC
NPwo
NPga
NPga
???
negotiation-DAT
NPni
NPga
???
join
Svo s\NPga\NPni
?
cause
Scont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont
??
negotiation
NPnc
?
DAT
NPni\NPnc <NPni
NPga
NPga
??
participation
Sstem\NPni
?
do
Svo s\Sstem <BSvo s\NPni
?
CAUSE
Scont\Svo s
Scont\Svo s <BScont\NPni
?
PAST
Sbase\Scont
Scont\Svo s
Scont <BSbase\NPni <Sbase
1
Figure 3: A simplified CCG analysis of the sentence ?The ambassador participated in the negotiation.?.
S ? NP/NP (RelExt)
S\NP1 ? NP1/ P1 (RelIn)
S ? S1/S1 (Con)
S\$1\NP1 ? (S1\$1\NP1)/(S1\$1\NP1) (ConCoord)
Figure 4: Type changing rules. The upper two are
for relative clauses and the others for continuous
clauses.
coordination and semantic representation in par-
ticular. The current implementation recognizes
coordinated verbs in continuous clauses (e.g., ??
???????????/he played the pia o and
sang?), but the treatment of other types of coor-
dination is largely simplified. For semantic repre-
sentation, we define predicate argument structures
(PASs) rather than the theory?s formal representa-
tion based on dynamic logic. Sophisticating our
semantic representation is left for future work.
For parsing efficiency, we modified the treat-
ment of some constructions so that empty el-
ements are excluded from the implementation.
First, we define type changing rules to produce
relative and continuous clauses (shown in Fig. 4).
The rules produce almost the same results as the
theory?s treatment, but without using empty ele-
ments (pro, etc.). We also used lexical rules to
treat pro-drop and scrambling. For the sentence in
Fig. 3, the deletion of the nominal phrase (??
?), the dative phrase (???), or both results in
valid sentences, and shuffling the two phrases does
so as well. Lexical entries with the scrambled or
dropped arguments are produced by lexical rules
in our implementation.
2.3 Linguistic resources for Japanese parsing
As described in Sec. 1, dependency-based analysis
has been accepted for Japanese syntax. Research
on Japanese parsing also relies on dependency-
based corpora. Among them, we used the follow-
ing resources in this work.
Kyoto corpus A news text corpus annotated
with morphological information, chunk bound-
Kyoto Corpus 
Chunk 
?? ? government NOM ?? ? ambassador ACC ?? ? negotiation DAT ?? ? ? ? participation do cause PAST 
NAIST Corpus 
Dep. 
Causer ARG-ga ARG-ni 
Figure 5: The Kyoto and NAIST annotations for
?The government had the ambassador participate
in the negotiation.?. Accusatives are labeled as
ARG-ga in causative (see Sec. 3.2).
aries, and dependency relations among chunks
(Fig. 5). The dependencies are classified into four
types: Para (coordination), A (apposition), I (ar-
gument cluster), and Dep (default). Most of the
dependencies are annotated as Dep.
NAIST text corpus A corpus annotated with
anaphora and coreference relations. The same set
as the Kyoto corpus is annotated.1 The corpus
only focuses on three cases: ?ga? (subject), ?o?
(direct object), and ?ni? (indirect object) (Fig. 5).
Japanese particle corpus (JP) (Hanaoka et al,
2010) A corpus annotated with distinct gram-
matical functions of the Japanese particle (postpo-
sition) ?to?. In Japanese, ?to? has many functions,
including a complementizer (similar to ?that?), a
subordinate conjunction (similar to ?then?), a co-
ordination conjunction (similar to ?and?), and a
case marker (similar to ?with?).
2.4 Related work
Research on Japanese deep parsing is fairly lim-
ited. Formal theories of Japanese syntax were
presented by Gunji (1987) based on Head-driven
Phrase Structure Grammar (HPSG) (Sag et al,
2003) and by Komagata (1999) based on CCG, al-
though their implementations in real-world pars-
ing have not been very successful. JACY (Siegel
1In fact, the NAIST text corpus includes additional texts,
but in this work we only use the news text section.
1044
and Bender, 2002) is a large-scale Japanese gram-
mar based on HPSG, but its semantics is tightly
embedded in the grammar and it is not as easy
to systematically switch them as it is in CCG.
Yoshida (2005) proposed methods for extracting
a wide-coverage lexicon based on HPSG from a
phrase structure treebank of Japanese. We largely
extended their work by exploiting the standard
chunk-based Japanese corpora and demonstrated
the first results for Japanese deep parsing with
grammar induced from large corpora.
Corpus-based acquisition of wide-coverage
CCG resources has enjoyed great success for En-
glish (Hockenmaier and Steedman, 2007). In
that method, PTB was converted into CCG-based
derivations from which a wide-coverage CCG lex-
icon was extracted. CCGbank has been used for
the development of wide-coverage CCG parsers
(Clark and Curran, 2007). The same methodology
has been applied to German (Hockenmaier, 2006),
Italian (Bos et al, 2009), and Turkish (C?ak?c?,
2005). Their treebanks are annotated with depen-
dencies of words, the conversion of which into
phrase structures is not a big concern. A notable
contribution of the present work is a method for in-
ducing CCG grammars from chunk-based depen-
dency structures, which is not obvious, as we dis-
cuss later in this paper.
CCG parsing provides not only predicate argu-
ment relations but also CCG derivations, which
can be used for various semantic processing tasks
(Bos et al, 2004; Bos, 2007). Our work consti-
tutes a starting point for such deep linguistic pro-
cessing for languages like Japanese.
3 Corpus integration and conversion
For wide-coverage CCG parsing, we need a)
a wide-coverage CCG lexicon, b) combinatory
rules, c) training data for parse disambiguation,
and d) a parser (e.g., a CKY parser). Since d) is
grammar- and language-independent, all we have
to develop for a new language is a)?c).
As we have adopted the method of CCGbank,
which relies on a source treebank to be converted
into CCG derivations, a critical issue to address is
the absence of a Japanese counterpart to PTB. We
only have chunk-based dependency corpora, and
their relationship to CCG analysis is not clear.
Our solution is to first integrate multiple
dependency-based resources and convert them
into a phrase structure treebank that is independent
ProperNoun 
????? Yeltsin 
NP 
ProperNoun 
??? Russia 
Noun 
???president 
PostP 
? DAT 
PP 
NP 
Aux 
??? not 
VP 
Verb 
?? forgive 
VerbSuffix 
? PASSIVE 
VP Aux 
? PAST 
VP 
?to Russian president Yeltsin? ?(one) was not forgiven? 
Figure 6: Internal structures of a nominal chunk
(left) and a verbal chunk (right).
of CCG analysis (Step 1). Next, we translate the
treebank into CCG derivations (Step 2). The idea
of Step 2 is similar to what has been done with
the English CCGbank, but obviously we have to
address language-specific issues.
3.1 Dependencies to phrase structure trees
We first integrate and convert available Japanese
corpora?namely, the Kyoto corpus, the NAIST
text corpus, and the JP corpus ?into a phrase
structure treebank, which is similar in spirit to
PTB. Our approach is to convert the depen-
dency structures of the Kyoto corpus into phrase
structures and then augment them with syntac-
tic/semantic roles from the other two corpora.
The conversion involves two steps: 1) recogniz-
ing the chunk-internal structures, and (2) convert-
ing inter-chunk dependencies into phrase struc-
tures. For 1), we don?t have any explicit infor-
mation in the Kyoto corpus although, in princi-
ple, each chunk has internal structures (Vadas and
Curran, 2007; Yamada et al, 2010). The lack of
a chunk-internal structure makes the dependency-
to-constituency conversion more complex than a
similar procedure by Bos et al (2009) that con-
verts an Italian dependency treebank into con-
stituency trees since their dependency trees are an-
notated down to the level of each word. For the
current implementation, we abandon the idea of
identifying exact structures and instead basically
rely on the following generic rules (Fig. 6):
Nominal chunks Compound nouns are first
formed as a right-branching phrase and
post-positions are then attached to it.
Verbal chunks Verbal chunks are analyzed as
left-branching structures.
The rules amount to assume that all but the last
word in a compound noun modify the head noun
(i.e., the last word) and that a verbal chunk is typ-
ically in a form V A1 . . . An, where V is a verb
1045
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP 
PP 
Noun 
?? process  
PostPcm  
? ACC  PP 
NP 
PP 
Noun 
?? birth  
PostPcm  
??from  
PP 
Noun 
? death  
PostPcm  
?? to 
PostPadnom  
? adnominal 
PP PP 
Noun 
?? process  
PostPcm  
? ACC  
Para Dep 
?$ proFess froP EirtK to deatK? 
Figure 7: From inter-chunk dependencies to a tree.
(or other predicative word) and Ais are auxiliaries
(see Fig. 6). We chose the left-branching structure
as default for verbal chunks because the semantic
scopes of the auxiliaries are generally in that or-
der (i.e., A1 has the narrowest scope). For both
cases, phrase symbols are percolated upward from
the right-most daughters of the branches (except
for a few cases like punctuation) because in almost
all cases the syntactic head of a Japanese phrase is
the right-most element.
In practice, we have found several patterns of
exceptions for the above rules. We implemented
exceptional patterns as a small CFG and deter-
mined the chunk-internal structures by determin-
istic parsing with the generic rules and the CFG.
For example, two of the rules we came up with are
rule A: Number ? PrefixOfNumber Number
rule B: ClassifierPhrase ? Number Classifier
in the precedence: rule A > B > generic rules.
Using the above, we bracket a compound noun
? ? ? ??
approximately thousand people death
PrefixOfNumber Number Classifier CommonNoun
?death of approximately one thousand people?
as in
(((? ?) ?) ??)
(((approximately thousand) people) death)
We can improve chunk-internal structures to some
extent by refining the CFG rules. A complete solu-
tion like the manual annotation by Vadas and Cur-
ran (2007) is left for future work.
The conversion of inter-chunk dependencies
into phrase structures may sound trivial, but it is
not necessarily easy when combined with chunk-
internal structures. The problem is to which node
in the internal structure of the head the dependent
dep modifier-type precedence
Para ??/PostPcm ??/PostPcm, */(Verb|Aux), ...
Dep */PostPcm */(Verb|Aux), */Noun, ...
Dep */PostPadnom */Noun, */(Verb|Aux), ...
Table 3: Rules to determine adjoin position.
PP 
Noun 
? dog 
PostP 
? DAT 
VP 
NP 
Adj  
?? 
white  
NP 
VP 
Noun 
? 
cat  
Verb 
?? say  
Aux 
? PAST 
VP PP 
Verb 
?? go!  
PostP 
? 
CMP  
ARG - to 
ARG - ni 
ARG - ga 
ARG - ga 
ARG - ga 
ARG - ga ARG - ni 
ARG - CLS  
NAIST 
JP 
Figure 8: Overlay of pred-arg structure annotation
(?The white cat who said ?Go!? to the dog.?).
tree is adjoined (Fig. 7). In the case shown in the
figure, three chunks are in the dependency relation
indicated by arrows on the top. The dotted arrows
show the nodes to which the subtrees are adjoined.
Without any human-created resources, we can-
not always determine the adjoin positions cor-
rectly. Therefore, as a compromise, we wound up
implementing approximate heuristic rules to deter-
mine the adjoin positions. Table 3 shows examples
of such rules. A rule specifies a precedence of the
possible adjoin nodes as an ordered list of patterns
on the lexical head of the subtree under an ad-
join position. The precedence is defined for each
combination of the type of the dependent phrase,
which is determined by its lexical head, and the
dependency type in the Kyoto corpus.
To select the adjoin position for the left-most
subtree in Fig. 7, for instance, we look up the
rule table using the dependency type, ?Para?, and
the lexical head of the modifier subtree, ? ??
/PostPcm?, as the key, and find the precedence ??
?/PostPcm, */(Verb|Aux), ...?. We thus select the
PP-node on the middle subtree indicated by the
dotted arrow because its lexical head (the right-
most word), ? ??/PostPcm?, matches the first
pattern in the precedence list. In general, we seek
for an adjoin node for each pattern p in the prece-
dence list, until we find a first match.
The semantic annotation given in the NAIST
corpus and the JP corpus is overlaid on the phrase
structure trees with slight modifications (Fig. 8).
1046
PP 
Noun 
?? negotiation 
PostPcm  
? DAT VP Noun 
?? participation  
Verb 
? do 
VerbSuffix 
? CAUSE  
Aux 
? PAST 
VP 
VP 
S 
NPni 
NP 
?? negotiation 
T1 
? DAT T4 
T5 
?? participation  
S?S 
? do 
S?S 
? CAUSE  
S?S 
? PAST 
T3 
T2 
S ? 
? 
?  or   ?B   
?  or   ?B   
?  or   ?B   
NPni 
NPnc 
?? negotiation 
NPni?NPnc 
? DAT 
Svo_s?NPni 
Svo_s?NPni 
?? participation  
Svo_s?Svo_s 
? do 
Scont?Svo_s 
? CAUSE  
Sbase?Scont 
? PAST 
Scont?NPni 
Sbase?NPni 
Sbase 
Step 2 - 1  
Step 2 - 2, 2 - 3  
Figure 9: A phrase structure into a CCG deriva-
tion.
In the figure, the annotation given in the two cor-
pora is shown inside the dotted box at the bottom.
We converted the predicate-argument annotations
given as labeled word-to-word dependencies into
the relations between the predicate words and their
argument phrases. The results are thus similar to
the annotation style of PropBank (Palmer et al,
2005). In the NAIST corpus, each pred-arg re-
lation is labeled with the argument-type (ga/o/ni)
and a flag indicating that the relation is medi-
ated by either a syntactic dependency or a zero
anaphora. For a relation of a predicate wp and its
argument wa in the NAIST corpus, the boundary
of the argument phrase is determined as follows:
1. If wa precedes wp and the relation is medi-
ated by a syntactic dep., select the maximum
PP that is formed by attaching one or more
postpositions to the NP headed by wa.
2. If wp precedes wa or the relation is mediated
by a zero anaphora, select the maximum NP
headed by wa that does not include wp.
In the figure, ??/dog?/DAT? is marked as the ni-
argument of the predicate ???/say? (Case 1), and
???/white ?/cat? is marked as its ga-argument
(Case 2). Case 1 is for the most basic construction,
where an argument PP precedes its predicate. Case
VP 
??    ? 
friend- DAT 
PP VP 
?? 
meet - BASE  
NPni ? 
VP 
10 ?    ? 
10 o?clock - TIME  
PP VP 
?? 
meet - BASE  
T/T ? 
X  
S 
??    ? 
friend- DAT 
NPni S?NPni 
?? 
meet - BASE  
S 
10 ?    ? 
10 o?clock - TIME  
S?S S 
?? 
meet - BASE  
?(to) Peet at ten? 
?(to) Peet a friend? 
Figure 10: An argument post particle phrase (PP)
(upper) and an adjunct PP (lower).
2 covers the relative clause construction, where a
relative clause precedes the head NP, the modifi-
cation of a noun by an adjective, and the relations
mediated by zero anaphora.
The JP corpus provides only the function label
to each particle ?to? in the text. We determined
the argument phrases marked by the ?to? particles
labeled as (nominal or clausal) argument-markers
in a similar way to Case 1 above and identified the
predicate words as the lexical heads of the phrases
to which the PPto phrases attach.
3.2 Phrase structures to CCG derivations
This step consists of three procedures (Fig. 9):
1. Add constraints on categories and features
to tree nodes as far as possible and assign a
combinatory rule to each branching.
2. Apply combinatory rules to all branching and
obtain CCG derivations.
3. Add feature constraints to terminal nodes.
3.2.1 Local constraint on derivations
According to the phrase structures, the first proce-
dure in Step 2 imposes restrictions on the resulting
CCG derivations. To describe the restrictions, we
focus on some of the notable constructions and il-
lustrate the restrictions for each of them.
Phrases headed by case marker particles A
phrase of this type must be either an argument
(Fig. 10, upper) or a modifier (Fig. 10, lower) of a
predicative. Distinction between the two is made
based on the pred-arg annotation of the predica-
tive. If a phrase is found to be an argument, 1) cat-
egory NP is assigned to the corresponding node,
2) the case feature of the category is given accord-
ing to the particle (in the case of Fig. 10 (upper),
1047
VP 
Verb 
?? 
Speak - NEG  
Aux 
??? 
not- CONT  
Aux 
? 
PAST- BASE  
VP 
S cont ?S  
S ba s e ?S  
?did not speak? 
? or ?B  
? or ?B  S cont ?NPg a  
Sneg ?NPg a  
?? 
Speak - NEG  
S cont ?Sneg  
??? 
not- CONT  
S b ase ?S cont  
? 
PAST- BASE  
S b ase ?NPg a  
Figure 11: An auxiliary verb and its conversion.
VP 
Verb 
?? inquire - NEG  
VerbSuffix 
??? cause - BASE  
??    ? her - DAT 
PP 
VP 
ARG - ga  
?(to) Kave Ker inTuire? 
? 
S?NPni[1 ] 
S?S 
??? cause - BASE  
??    ? her - DAT 
NPni[1 ] 
S 
S?NPni[1 ] 
?? inquire - NEG  
ga         [1]  
NPni[1 ] 
ga: [1 ] 
Figure 12: A causative construction.
ni for dative), and 3) the combinatory rule that
combines the particle phrase and the predicative
phrase is assigned backward function application
rule (<). Otherwise, a category T/T is assigned to
the corresponding modifier node and the rule will
be forward function application (>).
Auxiliary verbs As described in Sec. 2.2, an
auxiliary verb is always given the category S\S
and is combined with a verbal phrase via < or <B
(Fig. 11). Furthermore, we assign the form feature
value of the returning category S according to the
inflection form of the auxiliary. In the case shown
in the figure, Sbase\S is assigned for ??/PAST-
BASE? and Scont\S for ????/not-CONT?. As
a result of this restriction, we can obtain condi-
tions for every auxiliary agglutination because the
two form values in S\S are both restricted after
applying combinatory rules (Sec. 3.2.2).
Case alternations In addition to the argu-
ment/adjunct distinction illustrated above, a pro-
cess is needed for argument phrases of predicates
involving case alternation. Such predicates are
either causative (see Fig. 12) or passive verbs
and can be detected by voice annotation from the
NAIST corpus. For an argument of that type of
verb, its deep case (ga for Fig. 12) must be used
to construct the semantic representation, namely
the PAS. As well as assigning the shallow case
value (ni in Fig. 12) to the argument?s category
NP, as usual, we assign a restriction to the PAS
S?NPo[1] 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
NP 
? 
book 
NP 
NP[1]?NP[1] 
VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
Noun 
? 
book 
NP 
S?NP[1] 
NP[1]?NP[1] 
Noun 
? 
store 
NP 
?   ? 
book-ACC 
PP VP 
Verb 
?? 
buy-CONT 
Aux 
? 
PAST-ATTR 
VP X 
S 
NP?NP 
NP 
? 
store 
NP 
NP?NP 
?    ? 
book-ACC 
NPo 
S 
S?NPo 
S?NPo 
?? 
buy-CONT 
S?S 
? 
PAST-ATTR 
?a store wKere (,) EougKt tKe EooN? 
?a EooN wKiFK (,) EougKt? 
Figure 13: A relative clause with/without argu-
ment extraction (upper/lower, respectively).
of the verb so that the semantic argument corre-
sponding to the deep case is co-indexed with the
argument NP. These restrictions are then utilized
for PAS construction in Sec. 3.2.3.
Relative clauses A relative clause can be de-
tected as a subtree that has a VP as its left child
and an NP as its right child, as shown in Fig. 13.
The conversion of the subtree consists of 1) in-
serting a node on the top of the left VP (see the
right-hand side of Fig. 13), and 2) assigning the
appropriate unary rule to make the new node. The
difference between candidate rules RelExt and Re-
lIn (see Fig. 4) is whether the right-hand NP is
an obligatory argument of the VP or not, which
can be determined by the pred-arg annotation on
the predicate in the VP. In the upper example in
Fig. 13, RelIn is assigned because the right NP
?book? is annotated as an accusative argument of
the predicate ?buy?. In contrast, RelExt is as-
signed in the lower side in the figure because the
right NP ?store? is not annotated as an argument.
Continuous clauses A continuous clause can be
detected as a subtree with a VP of continuous form
as its left child and a VP as its right child. Its
conversion is similar to that of a relative clause,
and only differs in that the candidate rules are Con
and ConCoord. ConCoord generates a continu-
ous clause that shares arguments with the main
clause while Con produces one without shared ar-
guments. Rule assignment is done by comparing
the pred-arg annotations of the two phrases.
1048
Training Develop. Test
#Sentences 24,283 4,833 9,284
#Chunks 234,685 47,571 89,874
#Words 664,898 136,585 255,624
Table 4: Statistics of input linguistic resources.
3.2.2 Inverse application of rules
The second procedure in Step 2 begins with as-
signing a category S to the root node. A combi-
natory rule assigned to each branching is then ?in-
versely? applied so that the constraint assigned to
the parent transfers to the children.
3.2.3 Constraints on terminal nodes
The final process consists of a) imposing restric-
tions on the terminal category in order to instan-
tiate all the feature values, and b) constructing a
PAS for each verbal terminal. An example of pro-
cess a) includes setting the form features in the
verb category, such as S\NPni, according to the
inflection form of the verb. As for b), arguments
in a PAS are given according to the category and
the partial restriction. For instance, if a category
S\NPni is obtained for ???/inquire? (Fig. 12),
the PAS for ?inquire? is unary because the cate-
gory has one argument category (NPni), and the
category is co-indexed with the semantic argument
ga in the PAS due to the partial restriction depicted
in Sec. 3.2.1. As a result, a lexical entry is ob-
tained as?? ` S\NPni[1]: inquire([1]).
3.3 Lexical entries
Finally, lexical rules are applied to each of the ob-
tained lexical entries in order to reduce them to
the canonical form. Since words in the corpus (es-
pecially verbs) often involve pro-drop and scram-
bling, there are a lot of obtained entries that have
slightly varied categories yet share a PAS. We as-
sume that an obtained entry is a variation of the
canonical one and register the canonical entries in
the lexicon. We treat only subject deletion for pro-
drop because there is not sufficient information to
judge the deletion of other arguments. Scrambling
is simply treated as permutation of arguments.
4 Evaluation
We used the following for the implementation of
our resources: Kyoto corpus ver. 4.02, NAIST text
2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?
Kyoto\%20University\%20Text\%20Corpus
Training Develop. Test
St.1 St.2 St.1 St.2 St.1 St.2
Sent. 24,283 24,116 4,833 4,803 9,284 9,245
Converted 24,116 22,820 4,803 4,559 9,245 8,769
Con. rate 99.3 94.6 99.4 94.9 99.6 94.9
Table 5: Statistics of corpus conversion.
Sentential Coverage
Covered Uncovered Cov. (%)
Devel. 3,920 639 85.99
Test 7,610 1,159 86.78
Lexical Coverage
Word Known Unknown
combi. cat. word
Devel. 127,144 126,383 682 79 0
Test 238,083 236,651 1,242 145 0
Table 6: Sentential and lexical coverage.
corpus ver. 1.53, and JP corpus ver. 1.04. The
integrated corpus is divided into training, devel-
opment, and final test sets following the standard
data split in previous works on Japanese depen-
dency parsing (Kudo and Matsumoto, 2002). The
details of these resources are shown in Table 4.
4.1 Corpus conversion and lexicon extraction
Table 5 shows the number of successful conver-
sions performed by our method. In total, we ob-
tained 22,820 CCG derivations from 24,283 sen-
tences (in the training set), resulting in the to-
tal conversion rate of 93.98%. The table shows
we lost more sentences in Step 2 than in Step 1.
This is natural because Step 2 imposed more re-
strictions on resulting structures and therefore de-
tected more discrepancies including compounding
errors. Our conversion rate is about 5.5 points
lower than the English counterpart (Hockenmaier
and Steedman, 2007). Manual investigation of the
sampled derivations would be beneficial for the
conversion improvement.
For the lexicon extraction from the CCGbank,
we obtained 699 types of lexical categories from
616,305 word tokens. After lexical reduction, the
number of categories decreased to 454, which in
turn may produce 5,342 categories by lexical ex-
pansion. The average number of categories for a
word type was 11.68 as a result.
4.2 Evaluation of coverage
Following the evaluation criteria in (Hockenmaier
and Steedman, 2007), we measured the coverage
3http://cl.naist.jp/nldata/corpus/
4https://alaginrc.nict.go.jp/resources/tocorpus/
tocorpusabstract.html
1049
of the grammar on unseen texts. First, we obtained
CCG derivations for evaluation sets by applying
our conversion method and then used these deriva-
tions as gold standard. Lexical coverage indicates
the number of words to which the grammar assigns
a gold standard category. Sentential coverage indi-
cates the number of sentences in which all words
are assigned gold standard categories 5.
Table 6 shows the evaluation results. Lexical
coverage was 99.40% with rare word treatment,
which is in the same level as the case of the En-
glish CCG parser C&C (Clark and Curran, 2007).
We also measured coverage in a ?weak? sense,
which means the number of sentences that are
given at least one analysis (not necessarily cor-
rect) by the obtained grammar. This number was
99.12 % and 99.06 % for the development and the
test set, respectively, which is sufficiently high for
wide-coverage parsing of real-world texts.
4.3 Evaluation of parsing accuracy
Finally, we evaluated the parsing accuracy. We
employed the parser and the supertagger of
(Miyao and Tsujii, 2008), specifically, its gen-
eralized modules for lexicalized grammars. We
trained log-linear models in the same way as
(Clark and Curran, 2007) using the training set as
training data. Feature sets were simply borrowed
from an English parser; no tuning was performed.
Following conventions in research on Japanese de-
pendency parsing, gold morphological analysis re-
sults were input to a parser. Following C&C, the
evaluation measure was precision and recall over
dependencies, where a dependency is defined as a
4-tuple: a head of a functor, a functor category, an
argument slot, and a head of an argument.
Table 7 shows the parsing accuracy on the de-
velopment and the test sets. The supertagging ac-
curacy is presented in the upper table. While our
coverage was almost the same as C&C, the perfor-
mance of our supertagger and parser was lower.
To improve the performance, tuning disambigua-
tion models for Japanese is a possible approach.
Comparing the parser?s performance with previ-
ous works on Japanese dependency parsing is dif-
ficult as our figures are not directly comparable
to theirs. Sassano and Kurohashi (2009) reported
the accuracy of their parser as 88.48 and 95.09
5Since a gold derivation can logically be obtained if gold
categories are assigned to all words in a sentence, sentential
coverage means that the obtained lexicon has the ability to
produce exactly correct derivations for those sentences.
Supertagging accuracy
Lex. Cov. Cat. Acc.
Devel. 99.40 90.86
Test 99.40 90.69
C&C 99.63 94.32
Overall performance
LP LR LF UP UR UF
Devel. 82.55 82.73 82.64 90.02 90.22 90.12
Test 82.40 82.59 82.50 89.95 90.15 90.05
C&C 88.34 86.96 87.64 93.74 92.28 93.00
Table 7: Parsing accuracy. LP, LR and LF refer to
labeled precision, recall, and F-score respectively.
UP, UR, and UF are for unlabeled.
in unlabeled chunk-based and word-based F1 re-
spectively. While our score of 90.05 in unlabeled
category dependency seems to be lower than their
word-based score, this is reasonable because our
category dependency includes more difficult prob-
lems, such as whether a subject PP is shared by
coordinated verbs. Thus, our parser is expected to
be capable of real-world Japanese text analysis as
well as dependency parsers.
5 Conclusion
In this paper, we proposed a method to induce
wide-coverage Japanese resources based on CCG
that will lead to deeper syntactic analysis for
Japanese and presented empirical evaluation in
terms of the quality of the obtained lexicon and
the parsing accuracy. Although our work is basi-
cally in line with CCGbank, the application of the
method to Japanese is not trivial due to the fact that
the relationship between chunk-based dependency
structures and CCG derivations is not obvious.
Our method integrates multiple dependency-
based resources to convert them into an integrated
phrase structure treebank. The obtained treebank
is then transformed into CCG derivations. The
empirical evaluation in Sec. 4 shows that our cor-
pus conversion successfully converts 94 % of the
corpus sentences and the coverage of the lexicon
is 99.4 %, which is sufficiently high for analyz-
ing real-world texts. A comparison of the parsing
accuracy with previous works on Japanese depen-
dency parsing and English CCG parsing indicates
that our parser can analyze real-world Japanese
texts fairly well and that there is room for improve-
ment in disambiguation models.
1050
References
Daisuke Bekki. 2010. Formal Theory of Japanese Syn-
tax. Kuroshio Shuppan. (In Japanese).
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING 2004, pages
1240?1246.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cate-
gorial grammar treebank for Italian. In Proceedings
of the Eighth International Workshop on Treebanks
and Linguistic Theories (TLT8), pages 27?38.
Johan Bos. 2007. Recognising textual entailment and
computational semantics. In Proceedings of Seventh
International Workshop on Computational Seman-
tics IWCS-7, page 1.
Ruken C?ak?c?. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of ACL Stu-
dent Research Workshop, pages 73?78.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Takao Gunji. 1987. Japanese Phrase Structure Gram-
mar: A Unification-based Approach. D. Reidel.
Hiroki Hanaoka, Hideki Mima, and Jun?ichi Tsujii.
2010. A Japanese particle corpus built by example-
based annotation. In Proceedings of LREC 2010.
Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-
sumoto. 2011. Japanese predicate argument struc-
ture analysis exploiting argument position and type.
In Proceedings of IJCNLP 2011, pages 201?209.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In Pro-
ceedings of the Joint Conference of COLING/ACL
2006.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Pro-
ceedings of ACL-HLT 2011, pages 804?813.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text
corpus with predicate-argument and coreference re-
lations. In Proceedings of Linguistic Annotation
Workshop, pages 132?139.
Daisuke Kawahara and Sadao Kurohashi. 2011. Gen-
erative modeling of coordination by factoring paral-
lelism and selectional preferences. In Proceedings
of IJCNLP 2011.
Daisuke Kawahara, Sadao Kurohashi, and Koiti
Hasida. 2002. Construction of a Japanese
relevance-tagged corpus. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 495?498. (In Japanese).
Nobo Komagata. 1999. Information Structure in Texts:
A Computational Analysis of Contextual Appropri-
ateness in English and Japanese. Ph.D. thesis, Uni-
versity of Pennsylvania.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analyisis using cascaded chunking. In
Proceedings of CoNLL 2002.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction, 2nd
Edition. CSLI Publications.
Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to Japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
Proceedings of IJCNLP 2011.
Manabu Sassano and Sadao Kurohashi. 2009. A uni-
fied single scan algorithm for Japanese base phrase
chunking and dependency parsing. In Proceedings
of ACL-IJCNLP 2009.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of Japanese. In Proceedings of the
3rd Workshop on Asian Language Resources and In-
ternational Standardization.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of ACL 2007, pages 240?247.
Emiko Yamada, Eiji Aramaki, Takeshi Imai, and
Kazuhiko Ohe. 2010. Internal structure of a disease
name and its application for ICD coding. Studies
in health technology and informatics, 160(2):1010?
1014.
Kazuhiro Yoshida. 2005. Corpus-oriented develop-
ment of Japanese HPSG parsers. In Proceedings of
the ACL Student Research Workshop.
1051
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 79?89,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Logical Inference on Dependency-based Compositional Semantics
Ran Tian Yusuke Miyao Takuya Matsuzaki
National Institute of Informatics, Japan
{tianran,yusuke,takuya-matsuzaki}@nii.ac.jp
Abstract
Dependency-based Compositional Se-
mantics (DCS) is a framework of natural
language semantics with easy-to-process
structures as well as strict semantics. In
this paper, we equip the DCS framework
with logical inference, by defining ab-
stract denotations as an abstraction of
the computing process of denotations in
original DCS. An inference engine is built
to achieve inference on abstract denota-
tions. Furthermore, we propose a way to
generate on-the-fly knowledge in logical
inference, by combining our framework
with the idea of tree transformation.
Experiments on FraCaS and PASCAL
RTE datasets show promising results.
1 Introduction
Dependency-based Compositional Semantics
(DCS) provides an intuitive way to model seman-
tics of questions, by using simple dependency-like
trees (Liang et al, 2011). It is expressive enough
to represent complex natural language queries on
a relational database, yet simple enough to be
latently learned from question-answer pairs. In
this paper, we equip DCS with logical inference,
which, in one point of view, is ?the best way
of testing an NLP system?s semantic capacity?
(Cooper et al, 1996).
It should be noted that, however, a framework
primarily designed for question answering is not
readily suited for logical inference. Because, an-
swers returned by a query depend on the specific
database, but implication is independent of any
databases. For example, answers to the question
?What books are read by students??, should al-
ways be a subset of answers to ?What books are
ever read by anyone??, no matter how we store the
data of students and how many records of books
are there in our database.
Thus, our first step is to fix a notation which ab-
stracts the calculation process of DCS trees, so as
to clarify its meaning without the aid of any exist-
ing database. The idea is to borrow a minimal set
of operators from relational algebra (Codd, 1970),
which is already able to formulate the calculation
in DCS and define abstract denotation, which is
an abstraction of the computation of denotations
guided by DCS trees. Meanings of sentences then
can be represented by primary relations among
abstract denotations. This formulation keeps the
simpleness and computability of DCS trees mostly
unaffected; for example, our semantic calculation
for DCS trees is parallel to the denotation compu-
tation in original DCS.
An inference engine is built to handle inference
on abstract denotations. Moreover, to compensate
the lack of background knowledge in practical in-
ference, we combine our framework with the idea
of tree transformation (Bar-Haim et al, 2007), to
propose a way of generating knowledge in logical
representation from entailment rules (Szpektor et
al., 2007), which are by now typically considered
as syntactic rewriting rules.
We test our system on FraCaS (Cooper et al,
1996) and PASCAL RTE datasets (Dagan et al,
2006). The experiments show: (i) a competi-
tive performance on FraCaS dataset; (ii) a big
impact of our automatically generated on-the-fly
knowledge in achieving high recall for a logic-
based RTE system; and (iii) a result that outper-
forms state-of-the-art RTE system on RTE5 data.
Our whole system is publicly released and can
be downloaded from http://kmcs.nii.ac.
jp/tianran/tifmo/.
2 The Idea
In this section we describe the idea of represent-
ing natural language semantics by DCS trees, and
achieving inference by computing logical relations
among the corresponding abstract denotations.
79
SUBJreadstudent bookOBJARG ARG
Figure 1: The DCS tree of ?students read books?
student
ARG
Mark
John
Emily
...
book
ARG
A Tale of Two Cities
Ulysses
...
read
SUBJ OBJ
Mark New York Times
Mary A Tale of Two Cities
John Ulysses
... ...
Table 1: Databases of student, book, and read
2.1 DCS trees
DCS trees has been proposed to represent natu-
ral language semantics with a structure similar to
dependency trees (Liang et al, 2011) (Figure 1).
For the sentence ?students read books?, imagine
a database consists of three tables, namely, a set
of students, a set of books, and a set of ?reading?
events (Table 1). The DCS tree in Figure 1 is in-
terpreted as a command for querying these tables,
obtaining ?reading? entries whose ?SUBJ? field
is student and whose ?OBJ? field is book. The
result is a set {John reads Ulysses, . . .}, which is
called a denotation.
DCS trees can be extended to represent linguis-
tic phenomena such as quantification and coref-
erence, with additional markers introducing addi-
tional operations on tables. Figure 2 shows an ex-
ample with a quantifier ?every?, which is marked
as ??? on the edge (love)OBJ-ARG(dog) and in-
terpreted as a division operator q
OBJ
?
(?2.2). Op-
timistically, we believe DCS can provide a frame-
work of semantic representation with sufficiently
wide coverage for real-world texts.
The strict semantics of DCS trees brings us the
idea of applying DCS to logical inference. This is
not trivial, however, because DCS works under the
assumption that databases are explicitly available.
Obviously this is unrealistic for logical inference
on unrestricted texts, because we cannot prepare
a database for everything in the world. This fact
fairly restricts the applicable tasks of DCS.
Our solution is to redefine DCS trees without
the aid of any databases, by considering each node
of a DCS tree as a content word in a sentence (but
may no longer be a table in a specific database),
while each edge represents semantic relations be-
tween two words. The labels on both ends of
an edge, such as SUBJ (subject) and OBJ (ob-
ject), are considered as semantic roles of the cor-
SUBJreadstu enbueoOBJARG ARGotadARGOBJ
SUBJotadke?? ?t?OBJARG ARG
stu SUBJread ??OBJARG ARG ke??SUBJARG
T: H:?
Figure 2: DCS trees of ?Mary loves every dog?
(Left-Up), ?Tom has a dog? (Left-Down), and
?Tom has an animal that Mary loves? (Right).
responding words
1
. To formulate the database
querying process defined by a DCS tree, we pro-
vide formal semantics to DCS trees by employing
relational algebra (Codd, 1970) for representing
the query. As described below, we represent mean-
ings of sentences with abstract denotations, and
logical relations among sentences are computed
as relations among their abstract denotations. In
this way, we can perform inference over formulas
of relational algebra, without computing database
entries explicitly.
2.2 Abstract denotations
Abstract denotations are formulas constructed
from a minimal set of relational algebra (Codd,
1970) operators, which is already able to formu-
late the database queries defined by DCS trees.
For example, the semantics of ?students read
books? is given by the abstract denotation:
F
1
= read ? (student
SUBJ
? book
OBJ
),
where read, student and book denote sets repre-
sented by these words respectively, and w
r
repre-
sents the set w considered as the domain of the
semantic role r (e.g. book
OBJ
is the set of books
considered as objects). The operators? and? rep-
resent intersection and Cartesian product respec-
tively, both borrowed from relational algebra. It
is not hard to see the abstract denotation denotes
the intersection of the ?reading? set (as illustrated
by the ?read? table in Table 1) with the product of
?student? set and ?book? set, which results in the
same denotation as computed by the DCS tree in
Figure 1, i.e. {John reads Ulysses, . . .}. However,
the point is that F
1
itself is an algebraic formula
that does not depend on any concrete databases.
Formally, we introduce the following constants:
? W : a universal set containing all entities.
1
The semantic role ARG is specifically defined for denot-
ing nominal predicate.
80
example phrase abstract denotation / statement
compound noun pet fish pet ? fish
modification nice day day ? (W
ARG
? nice
MOD
)
temporal relation boys study at night study ? (boy
SUBJ
? night
TIME
)
relative clause books that book ? pi
OBJ
(read
students read ?(student
SUBJ
?W
OBJ
))
quantification all men die man ? pi
SUBJ
(die)
hypernym dog ? animal
derivation all criminals commit criminal ? pi
SUBJ
(commit?
a crime (W
SUBJ
? crime
OBJ
))
antonym rise ? fall
negation no dogs are hurt dog ? pi
OBJ
(hurt)
Table 2: Abstract denotations and statements
? Content words: a content word (e.g. read)
defines a set representing the word (e.g.
read={(x, y) | read(x, y)}).
In addition we introduce following functions:
? ?: the Cartesian product of two sets.
? ?: the intersection of two sets.
? pi
r
: projection onto domain of semantic role
r (e.g. pi
OBJ
(read) = {y | ?x; read(x, y)}).
Generally we admit projections onto multiple
semantics roles, denoted by pi
R
where R is a
set of semantic roles.
? ?
r
: relabeling (e.g. ?
OBJ
(book) = book
OBJ
).
? q
r
?
: the division operator, where q
r
?
(A,B) is
defined as the largest set X which satisfies
B
r
?X ? A.
2
This is used to formulate uni-
versal quantifiers, such as ?Mary loves every
dog? and ?books read by all students?.
An abstract denotation is then defined as finite ap-
plications of functions on either constants or other
abstract denotations.
2.3 Statements
As the semantics of DCS trees is formulated by
abstract denotations, the meanings of declarative
sentences are represented by statements on ab-
stract denotations. Statements are declarations
of some relations among abstract denotations, for
which we consider the following set relations:
Non-emptiness A 6= ?: the set A is not empty.
Subsumption A ? B: set A is subsumed by B.
3
Roughly speaking, the relations correspond to the
logical concepts satisfiability and entailment.
2
If A and B has the same dimension, q
?
(A,B) is either
? or {?} (0-dimension point set), depending on if A ? B.
3
Using division operator, subsumption can be represented
by non-emptiness, since for setsA,B of the same dimension,
q
?
(A,B) 6= ? ? A ? B.
Abstract denotations and statements are conve-
nient for representing semantics of various types
of expressions and linguistic knowledge. Some
examples are shown in Table 2.
4
2.4 Logical inference on DCS
Based on abstract denotations, we briefly describe
our process to apply DCS to textual inference.
2.4.1 Natural language to DCS trees
To obtain DCS trees from natural language, we
use Stanford CoreNLP
5
for dependency parsing
(Socher et al, 2013), and convert Stanford depen-
dencies to DCS trees by pattern matching on POS
tags and dependency labels.
6
Currently we use
the following semantic roles: ARG, SUBJ, OBJ,
IOBJ, TIME and MOD. The semantic role MOD
is used for any restrictive modifiers. Determiners
such as ?all?, ?every? and ?each? trigger quanti-
fiers, as shown in Figure 2.
2.4.2 DCS trees to statements
A DCS tree T = (N , E) is defined as a rooted tree,
where each node ? ? N is labeled with a content
word w(?) and each edge (?, ?
?
) ? E ? N ?
N is labeled with a pair of semantic roles (r, r
?
)
7
.
Here ? is the node nearer to the root. Furthermore,
for each edge (?, ?
?
) we can optionally assign a
quantification marker.
Abstract denotation of a DCS tree can be cal-
culated in a bottom-up manner. For example, the
abstract denotation of H in Figure 2 is calculated
from the leaf node Mary, and then:
Node love (Mary loves):
F
2
= love ? (Mary
SUBJ
?W
OBJ
)
Node animal (Animal that Mary loves):
F
3
= animal ? pi
OBJ
(F
2
)
Node have (Tom has an animal that Mary loves):
F
4
= have ? (Tom
SUBJ
? (F
3
)
OBJ
).
Formally, suppose the root ? of a DCS tree T has
children ?
1
, . . . , ?
n
, and edges (?, ?
1
), . . . , (?, ?
n
)
labeled by (r
1
, r
?
1
), . . . , (r
n
, r
?
n
), respectively. The
abstract denotation of T is defined as:
[[T ]]=w(?) ? (
n
?
i=1
?
r
i
(pi
r
?
i
([[T
?
i
]]))?W
R
?
\r
i
),
4
Negation and disjointness (???) are explained in ?2.5.
5
http://nlp.stanford.edu/software/
corenlp.shtml
6
In (Liang et al, 2011) DCS trees are learned from QA
pairs and database entries. We obtain DCS trees from depen-
dency trees, to bypass the need of a concrete database.
7
The definition differs slightly from the original Liang et
al. (2011), mainly for the sake of simplicity and clarity.
81
piOBJ
(F
4
) = F
3
? F
7
pi
OBJ
(F
6
) = dog ? F
7
T
F
6
6= ? Axiom 4
dog ? F
7
6= ?
T
dog ? pi
OBJ
(F
2
) dog ? animal Axiom 8
dog ? F
3
dog ? F
7
? F
3
? F
7
Axiom 6
F
3
? F
7
6= ? Axiom 4
F
4
6= ?
Figure 3: An example of proof using abstract denotations
1. W 6= ?
2. A ? B ? A
3. B
r
? q
r
?
(A,B) ? A
4. pi
R
(A) 6= ? ? A 6= ?
5. (A ? B & B ? C)? A ? C
6. (A ? B & A 6= ?)? B 6= ?
7. A ? B ? pi
R
(A) ? pi
R
(B)
8. (C ? A & C ? B)? C ? A ? B
Table 3: An excerpt of axioms
where T
?
i
is the subtree of T rooted at ?
i
, and
R
?
is the set of possible semantic roles for con-
tent word w(?) (e.g. R
love
= {SUBJ,OBJ}), and
W
R
?
\r
i
is the product of W which has dimension
R
?
\ r
i
(e.g. W
{SUBJ,OBJ}\SUBJ
= W
OBJ
).
When universal quantifiers are involved, we
need to add division operators to the formula.
If (?, ?
i
) is assigned by a quantification marker
???
8
, then the abstract denotation is
9
[[T ]]=q
r
i
?
(pi
R
?
\{r
1
,...,r
i?1
}
([[T
?
]]), pi
r
?
i
([[T
?
i
]])),
where T
?
is the same tree as T except that the
edge (?, ?
i
) is removed. For example, the ab-
stract denotation of the first sentence of T in Fig-
ure 2 (Mary loves every dog) is calculated from F
2
(Mary loves) as
F
5
= q
OBJ
?
(pi
OBJ
(F
2
),dog).
After the abstract denotation [[T ]] is calcu-
lated, the statement representing the meaning of
the sentence is defined as [[T ]] 6= ?. For ex-
ample, the statement of ?students read books?
is read ? (student
SUBJ
? book
OBJ
) 6= ?, and
the statement of ?Mary loves every dog? is
q
OBJ
?
(pi
OBJ
(F
2
),dog) 6= ?, which is logically
equivalent to dog ? pi
OBJ
(F
2
).
10
2.4.3 Logical inference
Since meanings of sentences are represented by
statements on abstract denotations, logical infer-
ence among sentences is reduced to deriving new
relations among abstract denotations. This is done
by applying axioms to known statements, and ap-
proximately 30 axioms are implemented (Table 3).
8
Multiple quantifiers can be processed similarly.
9
The result of [[T ]] depends on the order of the children
?
1
, . . . , ?
n
. Different orders correspond to readings of differ-
ent quantifier scopes.
10
See Footnote 2,3.
These are algebraic properties of abstract denota-
tions, among which we choose a set of axioms that
can be handled efficiently and enable most com-
mon types of inference seen in natural language.
For the example in Figure 2, by constructing the
following abstract denotations:
Tom has a dog:
F
6
= have ? (Tom
SUBJ
? dog
OBJ
)
Objects that Tom has:
F
7
= pi
OBJ
(have ? (Tom
SUBJ
?W
OBJ
)),
we can use the lexical knowledge dog ? animal,
the statements of T (i.e. dog ? pi
OBJ
(F
2
) and
F
6
6= ?), and the axioms in Table 3,
11
to prove
the statement of H (i.e. F
4
6= ?) (Figure 3).
We built an inference engine to perform logical
inference on abstract denotations as above. In this
logical system, we treat abstract denotations as
terms and statements as atomic sentences, which
are far more easier to handle than first order pred-
icate logic (FOL) formulas. Furthermore, all im-
plemented axioms are horn clauses, hence we can
employ forward-chaining, which is very efficient.
2.5 Extensions
Further extensions of our framework are made
to deal with additional linguistic phenomena, as
briefly explained below.
Negation To deal with negation in our forward-
chaining inference engine, we introduce one more
relation on abstract denotations, namely disjoint-
ness A ? B, meaning that A and B are dis-
joint sets. Using disjointness we implemented two
types of negations: (i) atomic negation, for each
content word w we allow negation w? of that word,
characterized by the property w ? w?; and (ii) root
negation, for a DCS tree T and its denotation [[T ]],
the negation of T is represented by T ? T , mean-
ing that T = ? in its effect.
Selection Selection operators in relational alge-
bra select a subset from a set to satisfy some spe-
11
Algebraic identities, such as pi
OBJ
(F
4
) = F
3
? F
7
and
pi
OBJ
(F
6
) = dog ? F
7
, are also axioms.
82
cific properties. This can be employed to rep-
resent linguistic phenomena such as downward
monotonicity and generalized quantifiers. In the
current system, we implement (i) superlatives,
e.g. s
highest
(mountain? (W
ARG
?Asia
MOD
)) (the
highest mountain in Asia) and (ii) numerics, e.g.
s
two
(pet ? fish) (two pet fish), where s
f
is a se-
lection marker. Selection operators are imple-
mented as markers assigned to abstract denota-
tions, with specially designed axioms. For ex-
ample superlatives satisfy the following property:
A ? B & s
highest
(B) ? A ? s
highest
(B) =
s
highest
(A). New rules can be added if necessary.
Coreference We use Stanford CoreNLP to re-
solve coreferences (Raghunathan et al, 2010),
whereas coreference is implemented as a special
type of selection. If a node ? in a DCS tree T be-
longs to a mention cluster m, we take the abstract
denotation [[T
?
]] and make a selection s
m
([[T
?
]]),
which is regarded as the abstract denotation of that
mention. Then all selections of the same mention
cluster are declared to be equal.
3 Generating On-the-fly Knowledge
Recognizing textual entailment (RTE) is the task
of determining whether a given textual statement
H can be inferred by a text passage T. For this,
our primary textual inference system operates as:
1. For a T-H pair, apply dependency parsing
and coreference resolution.
2. Perform rule-based conversion from depen-
dency parses to DCS trees, which are trans-
lated to statements on abstract denotations.
3. Use statements of T and linguistic knowledge
as premises, and try to prove statements of H
by our inference engine.
However, this method does not work for real-
world datasets such as PASCAL RTE (Dagan et
al., 2006), because of the knowledge bottleneck:
it is often the case that the lack of sufficient lin-
guistic knowledge causes failure of inference, thus
the system outputs ?no entailment? for almost all
pairs (Bos and Markert, 2005).
The transparent syntax-to-semantics interface
of DCS enables us to back off to NLP techniques
during inference for catching up the lack of knowl-
edge. We extract fragments of DCS trees as para-
phrase candidates, translate them back to linguis-
  
T/H DCS trees AbstractdenotationsParsingCoreference InferenceYes/NoOn-the-flyknowledge Axioms
Languageresources
Figure 4: RTE system
tic expressions, and apply distributional similar-
ity to judge their validity. In this way, our frame-
work combines distributional and logical seman-
tics, which is also the main subject of Lewis and
Steedman (2013) and Beltagy et al (2013).
As follows, our full system (Figure 4) addition-
ally invokes linguistic knowledge on-the-fly:
4. If H is not proven, compare DCS trees of T
and H, and generate path alignments.
5. Aligned paths are evaluated by a similar-
ity score to estimate their likelihood of be-
ing paraphrases. Path alignments with scores
higher than a threshold are accepted.
6. Convert accepted path alignments into state-
ments on abstract denotations, use them in
logical inference as new knowledge, and try
to prove H again.
3.1 Generating path alignments
On-the-fly knowledge is generated by aligning
paths in DCS trees. A path is considered as joining
two germs in a DCS tree, where a germ is defined
as a specific semantic role of a node. For example,
Figure 5 shows DCS trees of the following sen-
tences (a simplified pair from RTE2-dev):
T: Tropical storm Debby is blamed for deaths.
H: A storm has caused loss of life.
The germ OBJ(blame) and germ ARG(death) in
DCS tree of T are joined by the underscored path.
Two paths are aligned if the joined germs are
aligned, and we impose constraints on aligned
germs to inhibit meaningless alignments, as de-
scribed below.
3.2 Aligning germs by logical clues
Two germs are aligned if they are both at leaf
nodes (e.g. ARG(death) in T and ARG(life) in H,
Figure 5), or they already have part of their mean-
ings in common, by some logical clues.
83
  
readsT?: H?: ARGtunsb obnek?btt?ARG ARGOBJreadsARG ARG
IOBJ
eda??nuARG MOD
??b uarru?b
ARG
SUBJ
ARG
MOD
OBJ
Figure 5: Aligned paths (underscored by the solid
lines) and aligned germs (joined by the dotted line)
To formulate this properly, we define the ab-
stract denotation of a germ, which, intuitively, rep-
resents the meaning of the germ in the specific sen-
tence. The abstract denotation of a germ is defined
in a top-down manner: for the root node ? of a
DCS tree T , we define its denotation [[?]]
T
as the
denotation of the entire tree [[T ]]; for a non-root
node ? and its parent node ?, let the edge (?, ?) be
labeled by semantic roles (r, r
?
), then define
[[? ]]
T
= [[T
?
]] ? (?
r
?
(pi
r
([[?]]
T
))?W
R
?
\r
?).
Now for a germ r(?), the denotation is defined as
the projection of the denotation of node ? onto the
specific semantic role r: [[r(?)]]
T
= pi
r
([[?]]
T
).
For example, the abstract denotation of germ
ARG(book) in Figure 1 is defined as pi
ARG
(book?
pi
OBJ
(read?(student
SUBJ
?book
OBJ
))), meaning
?books read by students?. Similarly, denotation
of germ OBJ(blame) in T of Figure 5 indicates
the object of ?blame? as in the sentence ?Tropi-
cal storm Debby is blamed for death?, which is
a tropical storm, is Debby, etc. Technically, each
germ in a DCS tree indicates a variable when the
DCS tree is translated to a FOL formula, and the
abstract denotation of the germ corresponds to the
set of consistent values (Liang et al, 2011) of that
variable.
The logical clue to align germs is: if there exists
an abstract denotation, other than W , that is a su-
perset of both abstract denotations of two germs,
then the two germs can be aligned. A simple ex-
ample is that ARG(storm) in T can be aligned
to ARG(storm) in H, because their denotations
have a common superset other than W , namely
pi
ARG
(storm). A more complicated example is that
OBJ(blame) and SUBJ(cause) can be aligned,
because inference can induce [[OBJ(blame)]]
T
=
[[ARG(Debby)]]
T
= [[ARG(storm)]]
T
, as well as
[[SUBJ(cause)]]
H
= [[ARG(storm)]]
H
, so they also
have the common superset pi
ARG
(storm). How-
ever, for example, logical clues can avoid align-
ing ARG(storm) to ARG(loss), which is obviously
  
T?: T'?:
What is tropical storm, Debby,       and is blamed for death ]][[ What is tropical storm, Debby,           and cause loss of life ]][[?blame deathDebbyARG ARGOBJstormARG ARG
IOBJ
tropicalARG MOD
cause losslife
ARG
SUBJ
ARG
MOD
OBJDebbyARGstormARG ARGtropicalARG MOD
Figure 6: Tree transformation and generated on-
the-fly knowledge (subsumption of denotations
shown above the trees)
meaningless.
3.3 Scoring path alignments by similarity
Aligned paths are evaluated by a similarity score,
for which we use distributional similarity of the
words that appear in the paths (?4.1). Only path
alignments with high similarity scores can be ac-
cepted. Also, we only accept paths of length ? 5,
to prevent too long paths to be aligned.
3.4 Applying path alignments
Accepted aligned paths are converted into state-
ments, which are used as new knowledge. The
conversion is done by first performing a DCS tree
transformation according to the aligned paths, and
then declare a subsumption relation between the
denotations of aligned germs. For example, to ap-
ply the aligned path pair generated in Figure 5,
we use it to transform T into a new tree T? (Fig-
ure 6), and then the aligned germs, OBJ(blame)
in T and SUBJ(cause) in T?, will generate
the on-the-fly knowledge: [[OBJ(blame)]]
T
?
[[SUBJ(cause)]]
T?
.
Similar to the tree transformation based ap-
proach to RTE (Bar-Haim et al, 2007), this pro-
cess can also utilize lexical-syntactic entailment
rules (Szpektor et al, 2007). Furthermore, since
the on-the-fly knowledge is generated by trans-
formed pairs of DCS trees, all contexts are pre-
served: in Figure 6, though the tree transformation
can be seen as generated from the entailment rule
?X is blamed for death? X causes loss of life?, the
generated on-the-fly knowledge, as shown above
the trees, only fires with the additional condition
that X is a tropical storm and is Debby. Hence,
the process can also be used to generate knowl-
edge from context sensitive rules (Melamud et al,
2013), which are known to have higher quality
(Pantel et al, 2007; Clark and Harrison, 2009).
However, it should be noted that using on-the-
fly knowledge in logical inference is not a trivial
84
task. For example, the FOL formula of the rule ?X
is blamed for death? X causes loss of life? is:
?x; (?a; blame(x, a) & death(a))?
(?b, c; cause(x, b) & loss(b, c) & life(c)),
which is not a horn clause. The FOL formula for
the context-preserved rule in Figure 6 is even more
involved. Still, it can be efficiently treated by our
inference engine because as a statement, the for-
mula [[OBJ(blame)]]
T
? [[SUBJ(cause)]]
T?
is an
atomic sentence, more than a horn clause.
4 Experiments
In this section, we evaluate our system on FraCaS
(?4.2) and PASCAL RTE datasets (?4.3).
4.1 Language Resources
The lexical knowledge we use are synonyms, hy-
pernyms and antonyms extracted from WordNet
12
.
We also add axioms on named entities, stopwords,
numerics and superlatives. For example, named
entities are singletons, so we add axioms such as
?x; (x ? Tom & x 6= ?)? Tom ? x.
To calculate the similarity scores of path align-
ments, we use the sum of word vectors of the
words from each path, and calculate the cosine
similarity. For example, the similarity score of the
path alignment ?OBJ(blame)IOBJ-ARG(death)
? SUBJ(cause)OBJ-ARG(loss)MOD-ARG(life)? is
calculated as the cosine similarity of vectors
blame+death and cause+loss+life. Other struc-
tures in the paths, such as semantic roles, are ig-
nored in the calculation. The word vectors we
use are from Mikolov et al (2013)
13
(Mikolov13),
and additional results are also shown using Turian
et al (2010)
14
(Turian10). The threshold for ac-
cepted path alignments is set to 0.4, based on pre-
experiments on RTE development sets.
4.2 Experiments on FraCaS
The FraCaS test suite contains 346 inference prob-
lems divided into 9 sections, each focused on a cat-
egory of semantic phenomena. We use the data by
MacCartney and Manning (2007), and experiment
on the first section, Quantifiers, following Lewis
and Steedman (2013). This section has 44 single
premise and 30 multi premise problems. Most of
12
http://wordnet.princeton.edu/
13
http://code.google.com/p/word2vec/
14
http://metaoptimize.com/projects/
wordreprs/
Single Prem. Multi Prem.
Lewis13 70 50
MacCartney07 84.1 -
MacCartney08 97.7 -
Our Sys. 79.5 80.0
Table 4: Accuracy (%) on FraCaS
the problems do not require lexical knowledge, so
we use our primary textual inference system with-
out on-the-fly knowledge nor WordNet, to test the
performance of the DCS framework as formal se-
mantics. To obtain the three-valued output (i.e.
yes, no, and unknown), we output ?yes? if H is
proven, or try to prove the negation of H if H is
not proven. To negate H, we use the root negation
as described in ?2.5. If the negation of H is proven,
we output ?no?, otherwise we output ?unknown?.
The result is shown in Table 4. Since our sys-
tem uses an off-the-shelf dependency parser, and
semantic representations are obtained from sim-
ple rule-based conversion from dependency trees,
there will be only one (right or wrong) interpre-
tation in face of ambiguous sentences. Still, our
system outperforms Lewis and Steedman (2013)?s
probabilistic CCG-parser. Compared to MacCart-
ney and Manning (2007) and MacCartney and
Manning (2008), our system does not need a pre-
trained alignment model, and it improves by mak-
ing multi-sentence inferences. To sum up, the re-
sult shows that DCS is good at handling universal
quantifiers and negations.
Most errors are due to wrongly generated DCS
trees (e.g. wrongly assigned semantic roles) or
unimplemented quantifier triggers (e.g. ?neither?)
or generalized quantifiers (e.g. ?at least a few?).
These could be addressed by future work.
4.3 Experiments on PASCAL RTE datasets
On PASCAL RTE datasets, strict logical inference
is known to have very low recall (Bos and Markert,
2005), so on-the-fly knowledge is crucial in this
setting. We test the effect of on-the-fly knowledge
on RTE2, RTE3, RTE4 and RTE5 datasets, and
compare our system with other approaches.
4.3.1 Impact of on-the-fly knowledge
Results on test data are shown in Table 5. When
only primary knowledge is used in inference (the
first row), recalls are actually very low; After we
activate the on-the-fly knowledge, recalls jump to
over 50%, with a moderate fall of precision. As a
result, accuracies significantly increase.
85
RTE2 RTE3 RTE4 RTE5
Prec. Rec. Acc. Prec. Rec. Acc. Prec. Rec. Acc. Prec. Rec. Acc.
Primary 70.9 9.8 52.9 73.2 7.3 51.1 89.7 5.2 52.3 82.6 6.3 52.5
+On-the-fly 57.6 66.5 58.8 63.7 64.6 63.0 60.0 57.4 59.6 69.9 55.7 65.8
Table 5: Impact of on-the-fly knowledge
RTE2 RTE3 RTE4 RTE5
Bos06 60.6 - - -
MacCartney08 - 59.4 - -
Clark08 - - 56.5 -
Wang10 63.0 61.1 - -
Stern11 61.6 67.1 - 63.5
Stern12 - - - 64.0
Our Sys. 58.8 63.0 59.6 65.8
Table 6: Comparison with other systems
4.3.2 Comparison to other RTE systems
A comparison between our system and other RTE
systems is shown in Table 6. Bos06 (Bos and
Markert, 2006) is a hybrid system combining
deep features from a theorem prover and a model
builder, together with shallow features such as lex-
ical overlap and text length. MacCartney08 (Mac-
Cartney and Manning, 2008) uses natural logic to
calculate inference relations between two superfi-
cially aligned sentences. Clark08 (Clark and Har-
rison, 2008) is a logic-based system utilizing vari-
ous resources including WordNet and DIRT para-
phrases (Lin and Pantel, 2001), and is tolerant to
partially unproven H sentences in some degree.
All of the three systems pursue a logical approach,
while combining various techniques to achieve ro-
bustness. The result shows that our system has
comparable performance. On the other hand,
Wang10 (Wang and Manning, 2010) learns a tree-
edit model from training data, and captures entail-
ment relation by tree edit distance. Stern11 (Stern
and Dagan, 2011) and Stern12 (Stern et al, 2012)
extend this framework to utilize entailment rules
as tree transformations. These are more tailored
systems using machine learning with many hand-
crafted features. Still, our unsupervised system
outperforms the state-of-the-art on RTE5 dataset.
4.3.3 Analysis
Summing up test data from RTE2 to RTE5, Fig-
ure 7 shows the proportion of all proven pairs and
their precision. Less than 5% pairs can be proven
primarily, with a precision of 77%. Over 40%
pairs can be proven by one piece of on-the-fly
knowledge, yet pairs do exist in which more than
2 pieces are necessary. The precisions of 1 and 2
pieces on-the-fly knowledge application are over
  
0 1 2 >=30
0.10.2
0.30.4
0.50.6
0.70.8
0.9 Proportion of proven pairsPrecision
Applied on-the-fly knowledge
Figure 7: Proportion of proven pairs and their pre-
cision, w.r.t. pieces of on-the-fly knowledge.
60%, which is fairly high, given our rough estima-
tion of the similarity score. As a comparison, Dinu
and Wang (2009) studied the proportion of proven
pairs and precision by applying DIRT rules to tree
skeletons in RTE2 and RTE3 data. The proportion
is 8% with precision 65% on RTE2, and propor-
tion 6% with precision 72% on RTE3. Applied
by our logical system, the noisy on-the-fly knowl-
edge can achieve a precision comparable to higher
quality resources such as DIRT.
A major type of error is caused by the igno-
rance of semantic roles in calculation of simi-
larity scores. For example, though ?Italy beats
Kazakhstan? is not primarily proven from ?Italy
is defeated by Kazakhstan?, our system does
produce the path alignment ?SUBJ(beat)OBJ ?
OBJ(defeat)SUBJ? with a high similarity score.
The impact of such errors depends on the data
making methodology, though. It lowers precisions
in RTE2 and RTE3 data, particularly in ?IE? sub-
task (where precisions drop under 0.5). On the
other hand, it occurs less often in ?IR? subtask.
Finally, to see if we ?get lucky? on RTE5 data
in the choice of word vectors and thresholds, we
change the thresholds from 0.1 to 0.7 and draw
the precision-recall curve, using two types of word
vectors, Mikolov13 and Turian10. As shown in
Figure 8, though the precision drops for Turian10,
both curves show the pattern that our system keeps
gaining recall while maintaining precision to a cer-
tain level. Not too much ?magic? in Mikolov13 ac-
tually: for over 80% pairs, every node in DCS tree
of H can be covered by a path of length ? 5 that
86
  0 012 01> 01= 013 01. 014 01501.
01..
014
014.
015
015.
016
016. What tisropchlms,
789Prr
op89t
itn 
Figure 8: Precision-Recall curve.
has a corresponding path of length ? 5 in T with
a similarity score > 0.4.
5 Conclusion and Discussion
We have presented a method of deriving abstract
denotation from DCS trees, which enables logi-
cal inference on DCS, and we developed a textual
inference system based on the framework. Exper-
imental results have shown the power of the rep-
resentation that allows both strict inference as on
FraCaS data and robust reasoning as on RTE data.
Exploration of an appropriate meaning repre-
sentation for querying and reasoning on knowl-
edge bases has a long history. Description logic,
being less expressive than FOL but featuring more
efficient reasoning, is used as a theory base for Se-
mantic Web (W3C, 2012). Ideas similar to our
framework, including the use of sets in a repre-
sentation that benefits efficient reasoning, are also
found in description logic and knowledge repre-
sentation community (Baader et al, 2003; Sowa,
2000; Sukkarieh, 2003). To our knowledge, how-
ever, their applications to logical inference beyond
the use for database querying have not been much
explored in the context of NLP.
The pursue of a logic more suitable for natural
language inference is not new. For instance, Mac-
Cartney and Manning (2008) has implemented a
model of natural logic (Lakoff, 1970). While
being computationally efficient, various inference
patterns are out of the scope of their system.
Much work has been done in mapping natu-
ral language into database queries (Cai and Yates,
2013; Kwiatkowski et al, 2013; Poon, 2013).
Among these, the (?-)DCS (Liang et al, 2011;
Berant et al, 2013) framework defines algorithms
that transparently map a labeled tree to a database
querying procedure. Essentially, this is because
DCS trees restrict the querying process to a very
limited subset of possible operations. Our main
contribution, the abstract denotation of DCS trees,
can thus be considered as an attempt to charac-
terize a fragment of FOL that is suited for both
natural language inference and transparent syntax-
semantics mapping, through the choice of opera-
tions and relations on sets.
We have demonstrated the utility of logical in-
ference on DCS through the RTE task. A wide
variety of strategies tackling the RTE task have
been investigated (Androutsopoulos and Malaka-
siotis, 2010), including the comparison of surface
strings (Jijkoun and De Rijke, 2005), syntactic and
semantic structures (Haghighi et al, 2005; Snow
et al, 2006; Zanzotto et al, 2009; Burchardt et
al., 2009; Heilman and Smith, 2010; Wang and
Manning, 2010), semantic vectors (Erk and Pad?o,
2009) and logical representations (Bos and Mark-
ert, 2005; Raina et al, 2005; Tatu and Moldovan,
2005). Acquisition of basic knowledge for RTE
is also a huge stream of research (Lin and Pantel,
2001; Shinyama et al, 2002; Sudo et al, 2003;
Szpektor et al, 2004; Fujita et al, 2012; Weis-
man et al, 2012; Yan et al, 2013). These previ-
ous works include various techniques for acquir-
ing and incorporating different kinds of linguistic
and world knowledge, and further fight against the
knowledge bottleneck problem, e.g. by back-off
to shallower representations.
Logic-based RTE systems employ various ap-
proaches to bridge knowledge gaps. Bos and
Markert (2005) proposes features from a model
builder; Raina et al (2005) proposes an abduction
process; Tatu and Moldovan (2006) shows hand-
crafted rules could drastically improve the perfor-
mance of a logic-based RTE system.
As such, our current RTE system is at a proof-
of-concept stage, in that many of the above tech-
niques are yet to be implemented. Nonetheless,
we would like to emphasize that it already shows
performance competitive to state-of-the-art sys-
tems on one data set (RTE5). Other directions of
our future work include further exploitation of the
new semantic representation. For example, since
abstract denotations are readily suited for data
querying, they can be used to verify newly gen-
erated assumptions by fact search in a database.
This may open a way towards a hybrid approach
to RTE wherein logical inference is intermingled
with large scale database querying.
Acknowledgments This research was supported
by the Todai Robot Project at National Institute of
Informatics.
87
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. J. Artif. Int. Res., 38(1).
Franz Baader, Diego Calvanese, Deborah L. McGuin-
ness, Daniele Nardi, and Peter F. Patel-Schneider,
editors. 2003. The Description Logic Handbook:
Theory, Implementation, and Applications. Cam-
bridge University Press, New York, NY, USA.
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI 2007.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of EMNLP
2013.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of EMNLP 2005.
Johan Bos and Katja Markert. 2006. When logical
inference helps determining textual entailment (and
when it doesnt). In Proceedings of the 2nd PASCAL
RTE Challenge Workshop.
Aljoscha Burchardt, Marco Pennacchiotti, Stefan
Thater, and Manfred Pinkal. 2009. Assessing the
impact of frame semantics on textual entailment.
Nat. Lang. Eng., 15(4).
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL 2013.
Peter Clark and Phil Harrison. 2008. Recognizing tex-
tual entailment with logical inference. In Proceed-
ings of 2008 Text Analysis Conference (TAC?08).
Peter Clark and Phil Harrison. 2009. Large-scale ex-
traction and use of knowledge from text. In Pro-
ceedings of the Fifth International Conference on
Knowledge Capture (K-CAP?09).
E. F. Codd. 1970. A relational model of data for large
shared data banks. Commun. ACM, 13(6).
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris
Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp,
David Milward, Manfred Pinkal, Massimo Poesio,
and et al 1996. Using the framework. FraCaS De-
liverable D, 16.
Ido Dagan, O. Glickman, and B. Magnini. 2006. The
pascal recognising textual entailment challenge. In
Machine Learning Challenges. Evaluating Predic-
tive Uncertainty, Visual Object Classification, and
Recognising Tectual Entailment.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of EACL 2009.
Katrin Erk and Sebastian Pad?o. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proceedings of the Work-
shop on Geometrical Models of Natural Language
Semantics.
Atsushi Fujita, Pierre Isabelle, and Roland Kuhn.
2012. Enlarging paraphrase collections through
generalization and instantiation. In Proceedings of
EMNLP 2012.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching.
In Proceedings of EMNLP 2005.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings
of NAACL 2010.
Valentin Jijkoun and Maarten De Rijke. 2005. Rec-
ognizing textual entailment: Is word similarity
enough? In Machine Learning Challenge Work-
shop, volume 3944 of LNCS, Springer.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
EMNLP 2013.
George Lakoff. 1970. Linguistics and natural logic.
Synthese, 22(1-2).
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of ACL, 1.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL 2011.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for question-answering. Nat. Lang.
Eng., 7(4).
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of Col-
ing 2008.
88
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of ACL 2013.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL
2013.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of NAACL 2007.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In Proceedings of ACL 2013.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Ju-
rafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of EMNLP 2010.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning
and abductive reasoning. In Proceedings of AAAI
2005.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of HLT 2002.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of NAACL 2006.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compo-
sitional vector grammars. In Proceedings of ACL
2013.
John F. Sowa. 2000. Knowledge Representation:
Logical, Philosophical and Computational Founda-
tions. Brooks/Cole Publishing Co., Pacific Grove,
CA, USA.
Asher Stern and Ido Dagan. 2011. A confidence model
for syntactically-motivated entailment proofs. In
Proceedings of RANLP 2011.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL 2012.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic ie pattern acquisition. In
Proceedings of ACL 2003.
JanaZ. Sukkarieh. 2003. An expressive efficient rep-
resentation: Bridging a gap between nlp and kr.
In Vasile Palade, RobertJ. Howlett, and Lakhmi
Jain, editors, Knowledge-Based Intelligent Informa-
tion and Engineering Systems. Springer Berlin Hei-
delberg.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP
2004.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of ACL 2007.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of EMNLP 2005.
Marta Tatu and Dan Moldovan. 2006. A logic-
based semantic approach to recognizing textual en-
tailment. In Proceedings of the COLING/ACL 2006.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL 2010.
W3C. 2012. Owl 2 web ontology language document
overview (second edition). www.w3.org/TR/owl2-
overview/.
Mengqiu Wang and Christopher Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of Coling 2010.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido
Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings of
EMNLP 2012.
Yulan Yan, Chikara Hashimoto, Kentaro Torisawa,
Takao Kawai, Jun?ichi Kazama, and Stijn De Saeger.
2013. Minimally supervised method for multilin-
gual paraphrase extraction from definition sentences
on the web. In Proceedings of NAACL 2013.
Fabio massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learn-
ing approach to textual entailment recognition. Nat.
Lang. Eng., 15(4).
89
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 123?126,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Deep Re-annotation in a Chinese Scientific Treebank  
Kun Yu1 Xiangli Wang1 Yusuke Miyao2 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {kunyu, xiangli, matuzaki, tsujii}@is.s.u-tokyo.ac.jp 2. National Institute of Informatics, Hitotsubashi 2-1-2, Chiyoda-ku, Tokyo, 101-8430, Japan yusuke@nii.ac.jp 3. The University of Manchester, Oxford Road, Manchester, M13 9PL, UK  
Abstract 
In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific tree-bank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described. 1 Introduction A Chinese scientific Treebank (called the NICT Chinese Treebank) has been developed by the National Institute of Information and Communi-cations Technology of Japan (NICT). This tree-bank annotates the word segmentation, pos-tags, and bracketing structures according to the anno-tation guideline of the Penn Chinese Treebank (Xia, 2000(a); Xia, 2000(b); Xue and Xia, 2000). Contrary to the Penn Chinese Treebank in news domain, the NICT Chinese Treebank includes sentences that are manually translated from Japanese scientific papers. Currently, the NICT Chinese Treebank includes around 8,000 Chinese sentences. The annotation of more sen-tences in the science domain is ongoing.  The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep informa-tion, which includes both the grammatical func-tional tags and the traces, are omitted in the an-notation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies.  Gabbard et al (2006) and Blaheta and Charniak (2000) applied machine learning mod-els to automatically assign the empty categories and functional tags to an English treebank.  
However, considering about the different do-mains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chi-nese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be used not only for the shallow natural language processing tasks, but also as a resource for deep applica-tions, such as the lexicalized grammar develop-ment from treebanks (Miyao 2006; Guo 2009; Xia 1999; Hockenmaier and Steedman 2002).  Considering that the translation quality of the sentences in the NICT Chinese Treebank may affect the quality of re-annotation, in the current phase, we only selected 2,363 sentences that are of good translation quality, for re-annotation. In the future, with the expansion of the NICT Chi-nese Treebank, we will continue this re-annotation work on large-scale sentences.  2 Content of Re-annotation Because the NICT Chinese Treebank follows the annotation guideline of the Penn Chinese Treebank, our re-annotation uses similar annota-tion criteria in the Penn Chinese Treebank.  Figure 1 exemplifies our re-annotation to a sentence in the NICT Chinese Treebank. In this example, we first re-annotate the trace (as indi-cated by the italicized part in Figure 1(b)) for the extracted head noun ??/word?. Furthermore, we re-annotate the functional tag of the trace (as indicated by the dashed-box in Figure 1(b)), to indicate that the extracted head noun should be restored into the relative clause as a topic. There are 26 functional tags in the Penn Chi-nese Treebank (Xue and Xia, 2000), in which seven functional tags describe the grammatical 
123
roles and one functional tag (i.e. LGS) indicates a logical subject. Since the eight functional tags are crucial for obtaining the grammatical func-tion of constituents, we re-annotate the eight functional tags (refer to Table 1) to the NICT Chinese Treebank. (NP (CP (IP (NP (NN ??)                             (NN ???))                               (VP (VA ?)))                                  (DEC ?))                           (NP (NN ?))) (the word of which the word cohesion is high) (a) A relative clause in the NICT Chinese Treebank          (NP (CP (WHNP-1 (-NONE- *OP*)      (CP (IP (NP-TPC (-NONE- *T*-1))                   (NP (NN ??)                           (NN ???))                   (VP (VA ?)))                                (DEC ?)))                (NP (NN ?))) (b) The relative clause after re-annotation Figure 1.  Our re-annotation to a relative clause. Functional Tag Description IO indirect object OBJ direct object EXT post-verbal complement that describes the extent, frequency, or quantity FOC object fronted to a pre-verbal but post-subject position PRD non-verbal predicate SBJ surface subject TPC topic LGS logical subject Table 1. Functional tags that we re-annotate.                 (IP (NP-TPC-1 (NN ??))                 (VP (ADVP (AD ??))                        (VP (ADVP (AD ??))                               (VP (VV ??)                                      (NP-OBJ (-NONE- *T*-1))))))                            (It is easier to obtain information.) (a) A topic construction with long-distance dependency after re-annotation of functional tag and trace          (IP (NP-TPC (DP (DT ?))                                (NP (NN ??)))                (NP-SBJ (NP (PN ?))                               (NP (NN ???)))                (VP (ADVP (AD ?))                        (VP (VV ??)                               (VV ??))))                (The rationality of this algorithm has been verified.)  (b) A topic construction without long-distance dependency after re-annotation of functional tag Figure 2. Our re-annotation to topic constructions. In addition, in the annotation guideline of the Penn Chinese Treebank, four constructions are annotated with traces: BA-construction, BEI-construction, topic construction and relative clause. The BEI-construction and relative 
clause introduce long-distance dependency. Therefore, we re-annotate the traces for the two constructions. The topic construction introduces the topic phrase. For the topic constructions that contain long-distance dependency, we re-annotate both the traces and the functional tags (refer to the italicized part in Figure 2(a)). Some topic constructions, however, do not include long-distance dependency. In such cases, we only re-annotate the functional tag to indicate that it is a topic (refer to the italicized part in Figure 2(b)). In addition, the BA-construction moves the object to a pre-verbal position. Al-though the BA-construction does not contain long-distance dependency, we still re-annotate the trace to acquire the original position of the moved object in the sentence. 3 Issues and Solutions 3.1 Trace re-annotation in the BA/BEI construction The NICT Chinese Treebank follows the word segmentation and pos-tag annotation guideline of the Penn Chinese Treebank. Therefore, there are some BA-constructions and BEI-constructions that cannot be re-annotated with traces. The principle reason for this is that the moved object has semantic relations with only part of the verb. For example, in the sentence shown in Figure 3(a), the moved head noun ???/hometown? is the object of ??/construct?, but not for ???/construct to be?.  (VP (BA ?)                (IP (NP (NN ??))                      (VP (VV ??)                             (NP (NN ??))))) (construct the hometown to be a garden) (a) The annotation in the NICT Chinese Treebank  (VP (BA ?)                            (IP (NP-SBJ-1 (NN ??))                                  (VP (VV ?)                                         (NP-OBJ (-NONE- *-1))                                         (AM ?)                                         (NP (NN ??))))) (b) Our proposed re-annotation of functional tag and trace Figure 3.  Our re-annotation to a BA construction with split verb. Our analysis of the Penn Chinese Treebank shows that only a closed list of characters (such as ??/to be?) can be attached to verbs in such a case. Therefore, we solve the problem by fol-lowing four steps (for an example, refer to Fig-ure 3(b)): 
124
(1) A linguist manually collects the characters that can be attached to verbs in such a case from the Penn Chinese Treebank and assigns them a new pos-tag ?AM (argument marker)?.  (2) The annotators use the character list as a reference during the re-annotation. When the verb in a BA/BEI construction ends with a char-acter in the list, and the annotators think the verb should be split, the annotators record the sentence ID without performing any re-annotation.  (3) The linguist collects all of the recorded sentences, and defines pattern rules to automati-cally split the verbs in the BA/BEI construc-tions. (4) The annotators annotate trace for the sen-tences with the split verbs. This step will be fin-ished in our future work. 3.2 Topic detection In the annotation guideline of the Penn Chinese Treebank, a topic is defined as ?the element that appears before the subject in a declarative sen-tence?. However, the NICT Chinese Treebank does not annotate the omitted subject. Therefore, we could not use the position of the subject as a criterion for topic detection.  In order to resolve this issue, we define some heuristic rules based on both the meaning and the bracketing structure of phrases, to help de-tect the topic phrase. Only the phrase that satis-fies all the rules will be re-annotated as a topic. The following exemplifies some rules: (1) If there is a phrase before a subject, the phrase is probably a topic. (2) A topic phrase must be parallel to the fol-lowing verb phrase. (3) The preposition phrase and localization phrase describing the location or time are not topics. 3.3 Inconsistent annotation in the NICT Chinese Treebank There are some inconsistent annotations in the NICT Chinese Treebank, which makes our re-annotation work difficult.  These inconsistencies include: (1) Inconsistent word segmentation, such as segmenting the word ???? /corresponding? into two words ???/opposite? and ??/ought?. (2) Inconsistent pos-tag annotation. For ex-ample, when the word  ???  exists between two noun phrases, it should be tagged as an associa-tive marker (i.e. DEG), according to the guide-
line of the Penn Chinese Treebank. However, in the NICT Chinese Treebank, sometimes it is tagged as a nominalizer (i.e. DEC).  (3) Inconsistent bracketing annotation. Fig-ure 4(a) shows the annotation of a relative clause in the NICT Chinese Treebank. In this annotation, the noun phrase ???/Osaka ??/subway? is incorrectly treated as the extracted head; furthermore, the adverb ???/by hand? that modifies the verb ???/make? is incor-rectly annotated as an adjective that modifies the noun ????/deformation graph?. After cor-recting these inconsistencies, the relative clause should be annotated as shown in Figure 4(b). (NP (QP (CD ??))              (ADJP (JJ ??))              (DNP (NP (CP (IP (VP (VV ??)))                                       (DEC ?))                                (NP (NR ??)                                       (NN ??)))                         (DEG ?))              (NP (NN ???))) (many deformation graphs of Osaka subway that are made by hand)  (a) The inconsistent annotation of a relative clause (NP (QP (CD ??))        (NP (CP (IP (VP (ADVP (AD ??))                                     (VP (VV ??))))                       (DEC ?))                (NP (DNP (NP (NR ??)                                         (NN ??))                                  (DEG ?))                       (NP (NN ???)))))  (b) The annotation after correcting the inconsistencies Figure 4. An inconsistent annotation in the NICT Chinese Treebank and its correction. In our re-annotation, these inconsistently an-notated sentences in the NICT Chinese Tree-bank were recorded by the annotators. We then sent them back to NICT for further verification. 4 Process of Re-annotation 4.1 Annotation Guideline  During the re-annotation, we basically follow the annotation guideline of the Penn Chinese Treebank (Xue and Xia, 2000). However, in order to fit with the characteristics of scientific sentences in the NICT Chinese Treebank, some constraints are added to the guideline.  For example, in the science domain, the rela-tive clause is often used to describe a phenome-non, in which the extracted head noun is usually an abstract noun, and the relative clause is an appositive of the extracted head noun. Figure 5 shows an example in which the relative clause ???/system ??/stop ??/working? is a de-
125
scription of the extracted head noun ???/phenomenon?. In such a case, the head noun cannot be restored into the clause. Therefore, we add the following restriction in our re-annotation guideline: Do not re-annotate the trace when the head noun of a relative clause is an abstract noun and it is an appositive of the relative clause.         (NP (CP (IP (NP (NN ??))                              (VP (VV ??)                                      (NP (NN ??))))                        (DEC ?))                 (NP (NN ??))) (the phenomenon that the system stops working) Figure 5. A relative clause in the NICT Chinese Treebank. 4.2 Quality Control Several processes were undertaken to guarantee the quality of our re-annotation:  (1) We chose graduate students who major in Chinese for all of the annotators.  (2) A visualization tool - XConc Suite (Kim et al, 2008) was used as assistance during the re-annotation.  (3) Only 2,363 sentences with good transla-tion quality in the NICT Chinese Treebank were chosen for re-annotation in the current phase.   (4) Before starting the re-annotation, a lin-guist selected 200 representative sentences, which contain all the linguistic phenomena that we want to re-annotate, from among the 2,363 sentences in the NICT Chinese Treebank. The selected 200 sentences were manually re-annotated by the linguist, and were split into two sets for training the annotators sequentially. We evaluated the annotation quality of the anno-tators during training. The average annotation quality of all the annotators after training is shown in Table 2. Annotation Quality Inter-annotator Consistency Precision Recall Precision Recall 70.71% 70.75% 61.59% 61.59% Table 2. The average annotation quality of the annotators after training.      (5) After training, the remaining sentences were split into several parts and assigned to the annotators for re-annotation. In each part, there were around 20% sentences that were shared by all of the annotators. These shared sentences were used to check and guarantee inter-annotator consistency during the re-annotation.  5 Conclusion and Future Work  We re-annotated the deep information, which includes eight types of grammatical functional 
tags and the traces in four constructions, to a Chinese scientific treebank, i.e. the NICT Chi-nese Treebank. Since the NICT Chinese Tree-bank is based on manually translated sentences, only 2,363 sentences with good translation qual-ity were re-annotated in the current phase to guarantee the re-annotation quality.  In the future, we will finish the trace annota-tion for the BA and BEI constructions with split verbs. Furthermore, we will continue our re-annotation on more sentences in the NICT Chi-nese Treebank. Acknowledgments We would like to thank Dr. Kiyotaka Uchimoto and Dr. Junichi Kazama for providing the NICT Chinese Treebank. References  Don Blaheta and Eugene Charniak. 2000. Assigning Func-tion Tags to Parsed Text. Proceedings of NAACL 2000. Ryan Gabbard, Seth Kulick and Mitchell Marcus. 2006. Fully Parsing the Penn Treebank. Proceedings of HLT-NAACL 2006. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. Thesis. Dublin City University. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. Jindong Kim, Tomoko Ohta, and Junichi Tsujii. 2008. Corpus Annotation for Mining Biomedical Events from Literature. BMC Bioinformatics, 9(10).  Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D Thesis. The University of Tokyo. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Fei Xia. 2000 (a). The Segmentation Guidelines for the Penn Chinese Treebank (3.0). Fei Xia. 2000 (b). The Part-of-speech Tagging Guidelines for the Penn Chinese Treebank (3.0). Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-Scale Annotated Chinese Corpus. Proceedings of COLING 2002. Nianwen Xue and Fei Xia. 2000. The Bracketing Guide-lines for the Penn Chinese Treebank. Shiwen Yu et al 2002. The Basic Processing of Contempo-rary Chinese Corpus at Peking University Specification. Journal of Chinese Information Processing, 16 (5). Qiang Zhou. 2004. Annotation Scheme for Chinese Tree-bank. Journal of Chinese Information Processing, 18 (4). 
126
Proceedings of the Fifth Law Workshop (LAW V), pages 56?64,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
A Collaborative Annotation between Human Annotators and a Statistical
Parser
Shun?ya Iwasawa Hiroki Hanaoka Takuya Matsuzaki
University of Tokyo
Tokyo, Japan
{iwasawa,hkhana,matuzaki}@is.s.u-tokyo.ac.jp
Yusuke Miyao
National Institute of Informatics
Tokyo, Japan
yusuke@nii.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
Beijing, P.R.China
jtsujii@microsoft.com
Abstract
We describe a new interactive annotation
scheme between a human annotator who
carries out simplified annotations on CFG
trees, and a statistical parser that converts
the human annotations automatically into a
richly annotated HPSG treebank. In order
to check the proposed scheme?s effectiveness,
we performed automatic pseudo-annotations
that emulate the system?s idealized behavior
and measured the performance of the parser
trained on those annotations. In addition,
we implemented a prototype system and con-
ducted manual annotation experiments on a
small test set.
1 Introduction
On the basis of the success of the research on the
corpus-based development in NLP, the demand for
a variety of corpora has increased, for use as both a
training resource and an evaluation data-set. How-
ever, the development of a richly annotated cor-
pus such as an HPSG treebank is not an easy task,
since the traditional two-step annotation, in which
a parser first generates the candidates and then an
annotator checks each candidate, needs intensive ef-
forts even for well-trained annotators (Marcus et al,
1994; Kurohashi and Nagao, 1998). Among many
NLP problems, adapting a parser for out-domain
texts, which is usually referred to as domain adap-
tation problem, is one of the most remarkable prob-
lems. The main cause of this problem is the lack
of corpora in that domain. Because it is difficult to
prepare a sufficient corpus for each domain without
reducing the annotation cost, research on annotation
methodologies has been intensively studied.
There has been a number of research projects
to efficiently develop richly annotated corpora with
the help of parsers, one of which is called a
discriminant-based treebanking (Carter, 1997). In
discriminant-based treebanking, the annotation pro-
cess consists of two steps: a parser first generates
the parse trees, which are annotation candidates,
and then a human annotator selects the most plau-
sible one. One of the most important characteristics
of this methodology is to use easily-understandable
questions called discriminants for picking up the fi-
nal annotation results. Human annotators can per-
form annotations simply by answering those ques-
tions without closely examining the whole tree. Al-
though this approach has been successful in break-
ing down the difficult annotations into a set of easy
questions, specific knowledge about the grammar,
especially in the case of a deep grammar, is still re-
quired for an annotator. This would be the bottle-
neck to reduce the cost of annotator training and can
restrict the size of annotations.
Interactive predictive parsing (Sa?nchez-Sa?ez et
al., 2009; Sa?nchez-Sa?ez et al, 2010) is another ap-
proach of annotations, which focuses on CFG trees.
In this system, an annotator revises the currently
proposed CFG tree until he or she gets the correct
tree by using a simple graphical user interface. Al-
though our target product is a more richly anno-
tated treebanks, the interface of CFG can be useful
to develop deep annotations such as HPSG features
by cooperating with a statistical deep parser. Since
CFG is easier to understand than HPSG, it can re-
56
duce the cost of annotator training; non-experts can
perform annotations without decent training. As a
result, crowd-sourcing or similar approach can be
adopted and the annotation process would be accel-
erated.
Before conducting manual annotation, we sim-
ulated the annotation procedure for validating our
system. In order to check whether the CFG-based
annotations can lead to sufficiently accurate HPSG
annotations, several HPSG treebanks were created
with various qualities of CFG and evaluated by their
HPSG qualities.
We further conducted manual annotation experi-
ments by two human annotators to evaluate the ef-
ficiency of the annotation system and the accuracy
of the resulting annotations. The causes of annota-
tion errors were analyzed and future direction of the
further development is discussed.
2 Statistical Deep Parser
2.1 HPSG
Head-Driven Phrase Structure Grammar (HPSG)
is one of the lexicalized grammatical formalisms,
which consists of lexical entries and a collection of
schemata. The lexical entries represent the syntac-
tic and semantic characteristics of words, and the
schemata are the rules that construct larger phrases
from smaller phrases. Figure 1 shows the mecha-
nism of the bottom-up HPSG parsing for the sen-
tence ?Dogs run.? First, a lexical entry is as-
signed to each word, and then, the lexical signs
for ?Dogs? and ?run? are combined by Subject-
Head schema. In this way, lexical signs and phrasal
signs are combined until the whole sentence be-
comes one sign. Compared to Context Free Gram-
mar (CFG), since each sign of HPSG has rich infor-
mation about the phrase, such as subcategorization
frame or predicate-argument structure, a corpus an-
notated in an HPSG manner is more difficult to build
than CFG corpus. In our system, we aim at building
HPSG treebanks with low-cost in which even non-
experts can perform annotations.
2.2 HPSG Deep Parser
The Enju parser (Ninomiya et al, 2007) is a statis-
tical deep parser based on the HPSG formalism. It
produces an analysis of a sentence that includes the
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Dogs
2
64
HEAD verb
SUBJ < noun >
COMPS <>
3
75
Drung
?
2
64
HEAD verb
SUBJ <>
COMPS <>
3
75
Subject
1
2
64
HEAD noun
SUBJ <>
COMPS <>
3
75
Headj
2
664
HEAD verb
SUBJ < 1 >
COMPS <>
3
775
Figure 1: Example of HPSG parsing for ?Dogs run.?
syntactic structure (i.e., parse tree) and the semantic
structure represented as a set of predicate-argument
dependencies. The grammar design is based on
the standard HPSG analysis of English (Pollard and
Sag, 1994). The parser finds a best parse tree
scored by a maxent disambiguation model using a
CKY-style algorithm and beam search. We used
a toolkit distributed with the Enju parser for ex-
tracting a HPSG lexicon from a PTB-style treebank.
The toolkit initially converts the PTB-style treebank
into an HPSG treebank and then extracts the lexi-
con from it. The HPSG treebank converted from the
test section is also used as the gold standard in the
evaluation.
2.3 Evaluation Metrics
In the experiments shown below, we evaluate the ac-
curacy of an annotation result (i.e., an HPSG deriva-
tion on a sentence) by evaluating the accuracy of
the semantic description produced by the deriva-
tion, as well as a more traditional metrics such
as labeled bracketing accuracy of the tree struc-
ture. Specifically, we used labeled and unlabeled
precision/recall/F-score of the predicate-argument
dependencies and the labeled brackets compared
against a gold-standard annotation obtained by using
the Enju?s treebank conversion tool. A predicate-
argument dependency is represented as a tuple of
?wp, wa, r?, where wp is the predicate word, wa
is the argument word, and r is the label of the
predicate-argument relation, such as verb-ARG1
(semantic subject of a verb) and prep-MOD (modi-
57
fiee of a prepositional phrase). As for the bracketing
accuracies, the label of a bracket is obtained by pro-
jecting the sign corresponding to the phrase into a
simple phrasal labels such as S, NP, and VP.
3 Proposed Annotation System
In our system, a human annotator and a statistical
deep parser cooperate to build a treebank. Our sys-
tem uses CFG as user interface and bridges a gap be-
tween CFG and HPSG with a statistical CKY parser.
Following the idea of the discriminant-based tree-
banking model, the parser first generates candidate
trees and then an annotator selects the correct tree in
the form of a packed forest. For selecting the correct
tree, the annotator only edits a CFG tree projected
from an HPSG tree through pre-defined set of oper-
ations, to eventually give the constraints onto HPSG
trees. This is why annotators can annotate HPSG
trees without HPSG knowledge. The current system
is implemented based on the following client-server
model.
3.1 Client: Annotator Interface
The client-side is an annotator?s interface imple-
mented with Ajax technique, on which annotator?s
revision is carried out through Web-Browser. When
the client-side receives the data of the current best
tree from the server-side, it shows an annotator the
CFG representation of the tree. Then, an annotator
adds revisions to the CFG tree using the same GUI,
until the current best tree has the CFG structure that
exactly matches the annotators? interpretation of the
sentence. Finally, the client-side sends the annota-
tor?s revision as a CGI query to the server. Based
on interactive predicative parsing system, two kinds
of operations are implemented in our system: ?span
modification? and ?label substitution?, here abbrevi-
ated as ?S? and ?L? operations:
?S? operation modify span(left, right)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified span, by sequentially clicking the leaf
nodes at the left and right boundaries.
?L? operation modify label(pos, label)
An annotator can specify that a constituent in
the tree after user?s revision must match a spec-
ified label, by inputting a label and clicking the
node position.
In addition to ?S? and ?L? operations, one more
operation, ?tree fixation?, abbreviated ?F?, is imple-
mented for making annotation more efficient. Our
system computes the best tree under the current con-
straints, which are specified by the ?S? and ?L? op-
erations that the annotator has given so far. It means
other parts of the tree that are not constrained may
change after a new operation by the annotator. This
change may lead to a structure that the annotator
does not want. To avoid such unexpected changes,
an annotator can specify a subtree which he or she
does not want to change by ?tree fixation? operation:
?F? operation fix tree(pos = i)
An annotator can specify a subtree as correct
and not to be changed. The specified subtree
does not change and always appears in the best
tree.
3.2 Server: Parsing Constraints
In our annotation system, the server-side carries out
the conversion of annotator?s constraints into HPSG
grammatical constraints on CKY chart and the re-
computation of the current best tree under the con-
straints added so far. The server-side works in the
following two steps. The first step is the conversion
of the annotator?s revision into a collection of dead
edges or dead cells; a dead edge means the edge
must not be a part of the correct tree, and a dead cell
means all edges in the cell are dead. As mentioned
in the background section, Enju creates a CKY chart
during the parsing where all the terminal and non-
terminal nodes are stored with the information of its
sign and links to daughter edges. In our annotation
system, to change the best tree according to the an-
notator?s revision, we determine whether each edge
in the chart is either alive or dead. The server-side
re-constructs the best tree under the constraints that
all the edges used in the tree are alive. The sec-
ond step is the computation of the best tree by re-
constructing the tree from the chart, under the con-
straint that the best tree contains only the alive edges
as its subconstituents. Re-construction includes the
following recursive process:
1. Start from the root edge.
58
2. Choose the link which has the highest probabil-
ity among the links and whose daughter edges
are all alive.
3. If there is such a link, recursively carry out the
process for the daughter edge.
4. If all the links from the edge are dead, go back
to the previous edge.
Note that our system parses a sentence only once,
the first time, instead of re-parsing the sentence after
each revision. Now, we are going to list the revision
operations again and explain how the operations are
interpreted as the constraints in the CKY chart. In
the description below, label(x) means the CFG-
symbol that corresponds to edge x. Note that there
is in principle an infinite variety of possible HPSG
signs. The label function maps this multitude of
signs onto a small set of simple CFG nonterminal
symbols.
?S? operation span(left = i, right = j)
When the revision type is ?S? and the left and
right boundary of the specified span is i and j
in the CGI query, we add the cells which satisfy
the following formula to the list of dead edges.
Suppose the sentence length is L, then the set
of new dead cells is defined as:
{cell(a, b) | 0 ? a < i,i ? b < j }
? {cell(c, d) | i+ 1 ? c ? j,j + 1 ? d ? n },
where the first set means the inhibition of the
edges that span across the left boundary of the
specified span. The second set means a similar
conditions for the right span.
?L? operation fix label(position = i, label = l)
When the revision type is ?L?, the node posi-
tion is i and the label is l in the CGI query, we
determine the set of new dead edges and dead
cells as follows:
1. let cell(a, b) = the cell including i
2. mark those cells that are generated by
span(a, b) as defined above to be dead,
and
3. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= l
?F? operation fix tree(position = i)
(a) prob = 0.4 (b) prob = 0.3 (c) prob = 0.2
NP
NX
NP
Time
NX
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
Time
VP
VP
flies
PP
PX
like
NP
DP
an
NX
arrow
S
NP
NX
NP
Time
NX
flies
VP
VX
like
NP
DP
an
NX
arrow
Figure 2: Three parse tree candidates of ?Time flies like
an arrow.?
When the revision type is ?F? and the target
node position is i in the CGI query, we carry
out the following process to determine the new
dead edges and cells:
1. for each edge e in the subtree rooted at
node i,
2. let cell(a, b) = the cell including e
3. mark those cells that are generated by
span(a, b) as defined above to be dead
4. for each edge e? in cell(a, b), mark e?
to be dead if label(e?) 6= label(e)
The above procedure adds the constraints so
that the correct tree includes a subtree that has
the same CFG-tree representation as the sub-
tree rooted at i in the current tree.
Finally we show how the best tree for the sentence
?Time flies like an arrow.? changes with the anno-
tator?s operations. Let us assume that the chart in-
cludes the three trees shown (in the CFG representa-
tion) in (Figure 2), and that there are no dead edges.
Let us further assume that the probability of each
tree is as shown in the figure and hence the current
best tree is (a). If the annotator wants to select (b)
as the best tree, s/he can apply ?L? operation on the
root node. The operation makes some of the edges
dead, which include the root edge of tree (a) (see
Figure 3). Accordingly, the best tree is now selected
from (b), (c), etc., and tree (b) will be selected as the
next best tree.
4 Validation of CFG-based Annotation
Because our system does not present HPSG anno-
tations to the annotators, there is a risk that HPSG
annotations are wrong even when their projections
to CFG trees are completely correct. Our expecta-
59
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
NP
Time
NX
VP
flies
PX
VX
like
DP
IanI
NX
IarrowI
NP
NX NP
PP
VP
VP
NP
S
fix label
(root,S)
?
Figure 3: Chart constraints by ?L? operation. Solid lines
represent the link of the current best tree and dashed lines
represent the second best one. Dotted lines stand for an
unavailable link due to the death of the source edge.
tion is that the stochastic model of the HPSG parser
properly resolves the remaining ambiguities in the
HPSG annotation within the constraints given by a
part of the CFG trees. In order to check the validity
of this expectation and to measure to what extent the
CFG-based annotations can achieve correct HPSG
annotations, we performed a pseudo-annotation ex-
periment.
In this experiment, we used bracketed sentences
in the Brown Corpus (Kuc?era and Francis, 1967),
and a court transcript portion of the Manually An-
notated Sub-Corpus (MASC) (Ide et al, 2010). We
automatically created HPSG annotations that mimic
the annotation results by an ideal annotator in the
following four steps. First, HPSG treebanks for
these sentences are created by the treebank conver-
sion program distributed with the Enju parser. This
program converts a syntactic tree annotated by Penn
Treebank style into an HPSG tree. Since this pro-
gram cannot convert the sentences that are not cov-
ered by the basic design of the grammar, we used
only those that are successfully converted by the
program throughout the experiments and considered
this converted treebank as the gold-standard tree-
bank for evaluation. Second, the same sentences are
parsed by the Enju parser and the results are com-
pared with the gold-standard treebank. Then, CFG-
level differences between the Enju parser?s outputs
and the gold-standard trees are translated into oper-
ation sequences of the annotation system. For ex-
ample, ?L? operation of NX ? VP at the root node
is obtained in the case of Figure 4. Finally, those
operation sequences are executed on the annotation
system and HPSG annotations are produced.
total size ave. s. l. convertible
Brown 24,243 18.94 22,214
MASC 1,656 14.81 1,353
Table 1: Corpus and experimental data information (s. l.
means ?sentence length.?)
(a) NX
NX PP
PX NP
(b) VP
VP PP
PX NP
Figure 4: CFG representation of parser output (a) and
gold-standard tree (b)
4.1 Relationship between CFG and HPSG
Correctness
We evaluated the automatically produced annota-
tions in terms of three measures: the labeled brack-
eting accuracies of their projections to CFG trees,
the accuracy of the HPSG lexical entry assignments
to the words, and the accuracy of the semantic de-
pendencies extracted from the annotations. The
CFG-labeled bracketing accuracies are defined in
the same way as the traditional PARSEVAL mea-
sures. The HPSG lexical assignment accuracy is
the ratio of words to which the correct HPSG lex-
ical entry is assigned, and the semantic dependency
accuracy is defined as explained in Section 2.3. In
this experiment, we cut off sentences longer than 40
words for time reasons. We split the Brown Cor-
pus into three parts: training, development test and
evaluation, and evaluated the automatic annotation
results only for the training portion.
We created three sets of automatic annotations as
follows:
Baseline No operation; default parsing results are
considered as the annotation results.
S-full Only ?S? operations are used; the tree struc-
tures of the resulting annotations should thus be
identical to the gold-standard annotations.
SL-full ?S? and ?L? operations are used; the la-
beled tree structures of the resulting anno-
tations should thus be identical to the gold-
standard annotations.
Before showing the evaluation results, splitting of
the data should be described here. Our system as-
sumes that the correct tree is included in the parser?s
60
CKY chart; however, because of the beam-search
limitation and the incomplete grammar coverage, it
does not always hold true. In this paper, such sit-
uations are called ?out-chart?. Conversely, the sit-
uations in which the parser does include the cor-
rect tree in the CKY chart are ?in-chart?. The re-
sults of ?in-chart? are here considered to be the re-
sults in the ideal situation of the perfect parser. In
our experimental setting, the training portion of the
Brown Corpus has 10,576 ?in-chart? and 7,208 ?out-
chart? sentences, while the MASC portion has 864
?in-chart? and 489 ?out-chart? sentences (Table 2).
Under ?out-chart? situations, we applied the opera-
tions greedily for calculating S-full and SL-full; that
is, all operations are sequentially applied and an op-
eration is skipped when there are no HPSG trees in
the CKY chart after applying that operation.
Table 3 shows the results of our three measures:
the CFG tree bracketing accuracy, the accuracy of
HPSG lexical entry assignment and that of the se-
mantic dependency. In both of S-full and SL-full,
the improvement from the baseline is significant.
Especially, SL-full for ?in-chart? data has almost
complete agreement with the gold-standard HPSG
annotations. The detailed figures are shown in Ta-
ble 4. Therefore, we can therefore conclude that
high quality CFG annotations lead to high quality
HPSG annotations when the are combined with a
good statistical HPSG parser.
4.2 Domain Adaptation
We evaluated the parser accuracy adapted with the
automatically created treebank on the Brown Cor-
pus. In this experiment, we used the adaptation al-
gorithm by (Hara et al, 2007), with the same hyper-
parameters used there. Table 5 shows the result of
the adapted parser. Each line of this table stands for
the parser adapted with different data. ?Gold? is the
result adapted on the gold-standard annotations, and
?Gold (only covered)? is that adapted on the gold
data which is covered by the original Enju HPSG
grammar that was extracted from the WSJ portion
of the Penn Treebank. ?SL-full? is the result adapted
on our automatically created data. ?Baseline? is the
result by the original Enju parser, which is trained
only on the WSJ-PTB and whose grammar was ex-
tracted from the WSJ-PTB. The table shows SL-full
slightly improves the baseline results, which indi-
#operations
S L F Avg. Time
Brown A. 1 122 1 0 1.19 43.32A. 2 91 4 1 0.94 41.77
MASC A. 1 275 2 5 2.76 33.33A. 2 52 2 0 0.51 35.13
Table 6: The number of operations and annotation time
by human annotators. ?Annotator? is abbreviated as A.
Avg. is the average number of operations per sentence
and Time is annotation time per sentence [sec.].
cates our annotation system can be useful for do-
main adaptation. Because we used mixed data of
?in-chart? and ?out-chart? in this experiment, there
still is much room for improvement by increasing
the ratio of the ?in-chart? sentences using a larger
beam-width.
5 Interactive Annotation on a
Prototype-system
We developed an initial version of the annotation
system described in Section 3, and annotated 200
sentences in total on the system. Half of the sen-
tences were taken from the Brown corpus and the
other half were taken from a court-debate section of
the MASC corpus. All of the sentences were an-
notated twice by two annotators. Both of the anno-
tators has background in computer science and lin-
guistics.
Table 6 shows the statistics of the annotation pro-
cedures. This table indicates that human annotators
strongly prefer ?S? operation to others, and that the
manual annotation on the prototype system is at least
comparable to the recent discriminant-based annota-
tion system by (Zhang and Kordoni, 2010), although
the comparison is not strict because of the difference
of the text.
Table 7 shows the automatic evaluation results.
We can see that the interactive annotation gave slight
improvements in all accuracy metrics. The improve-
ments were however not as much as we desired.
By classifying the remaining errors in the anno-
tation results, we identified several classes of major
errors:
1. Truly ambiguous structures, which require the
context or world-knowledge to correctly re-
solve them.
61
in out in+out
Brown (train.) 10,576 / 10,394 7,190 / 6,464 17,766 / 16,858
MASC 864 / 857 489 / 449 1,353 / 1,306
Table 2: The number of ?in-chart? and ?out-chart? sentences (total / 1-40 length)
in out in+out
Brown
SL-full 100.00 / 99.31 / 99.60 88.67 / 83.95 / 82.00 94.91 / 92.21 / 92.24
S-full 98.46 / 96.64 / 96.83 89.60 / 82.02 / 81.20 94.48 / 89.88 / 90.29
Baseline 92.39 / 92.69 / 90.54 82.10 / 78.38 / 73.80 87.78 / 86.07 / 83.54
MASC
SL-full 100.00 / 99.13 / 99.30 85.91 / 80.75 / 78.80 93.38 / 90.55 / 91.02
S-full 98.71 / 96.88 / 96.73 86.95 / 79.14 / 77.43 93.18 / 88.60 / 88.93
Baseline 93.98 / 93.51 / 91.56 80.00 / 75.89 / 72.22 87.43 / 85.30 / 83.75
Table 3: Evaluation of the automatic annotation sets. Each cell has the score of CFG F1 / Lex. Acc. / Dep. F1.
CFG tree accuracy
Brown MASC
A. 1 90.55 / 90.83 / 90.69 90.62 / 90.80 / 90.71
A. 2 91.01 / 91.09 / 91.05 91.01 / 91.09 / 91.05
Enju 89.70 / 89.74 / 89.72 90.02 / 90.20 / 90.11
PAS dependency accuracy
Brown MASC
A. 1 87.48 / 87.55 / 87.52 86.02 / 86.02 / 86.02
A. 2 88.42 / 88.27 / 88.34 85.28 / 91.01 / 85.32
Enju 87.12 / 86.91 / 87.01 84.81 / 84.26 / 84.53
Table 7: Automatic evaluation of the annotation results
(LP / LR / F1)
CFG tree accuracy
in-chart out-chart
A. 1 94.52 / 94.65 / 94.58 83.95 / 84.44 / 84.19
A. 2 95.07 / 95.14 / 95.10 84.22 / 84.32 / 84.27
Enju 94.44 / 94.37 / 94.40 81.81 / 82.00 / 81.90
PAS dependency accuracy
in-chart out-chart
A. 1 92.85 / 92.85 / 92.85 77.47 / 77.65 / 77.56
A. 2 93.34 / 93.34 / 93.34 79.17 / 78.80 / 78.98
Enju 92.73 / 92.73 / 92.73 76.57 / 76.04 / 76.30
Table 8: Automatic evaluation of the annotation results
(LP/LR/F1); in-chart sentences (left-column) and out-
chart sentences (right column) both from Brown
2. Purely grammar-dependent analyses, which re-
quire in-depth knowledge of the specific HPSG
grammar behind the simplified CFG-tree repre-
sentation given to the annotators.
3. Discrepancy between human intuition and the
convention in the HPSG grammar introduced
by the automatic conversion.
4. Apparently wrong analysis left untouched due
to the limitation of the annotation system.
We suspect some of the errors of type 1 have been
caused by the experimental setting of the annotation;
we gave the test sentences randomly drawn from
the corpus in a randomized order. This would have
made it difficult for the annotators to interpret the
sentences correctly. We thus expect this kind of er-
rors would be reduced by doing the annotation on a
larger chunk of text.
The second type of the errors are due to the fact
that the annotators are not familiar with the details
of the Enju English HPSG grammar. For example,
one of the annotators systematically chose a struc-
ture like (NP (NP a cat) (PP on the mat)). This struc-
ture is however always analysed as (NP a (NP? cat
(PP on the mat))) by the Enju grammar. The style of
the analysis implemented in the grammar thus some-
times conflicts with the annotators? intuition and it
introduces errors in the annotation results.
Our intention behind the design of the annotation
system was to make the annotation system more ac-
cessible to non-experts and reduce the cost of the
annotation. To reduce the type 2 errors, rather than
the training of the annotators for a specific gram-
mar, we plan to introduce another representation
system in which the grammar-specific conventions
become invisible to the annotators. For example, the
above-shown difference in the bracketing structures
of a determiner-noun-PP sequence can be hidden by
showing the noun phrase as a ternary branch on the
three children: (NP a cat (PP on the mat)).
The third type of the errors are mainly due to the
rather arbitrary choice of the HPSG analysis intro-
duced through the semi-automatic treebank conver-
sion used to extract the HPSG grammar. For in-
stance, the Penn Treebank annotates a structure in-
cluding an adverb that intervenes an auxiliary verb
62
Lex-Acc Dep-LP Dep-LR Dep-UP Dep-UR Dep-F1 Dep-EM
Brown 99.26 99.61 99.59 99.69 99.67 99.60 95.80
MASC 99.13 99.26 99.33 99.42 99.49 99.30 95.68
Table 4: HPSG agreement of SL-full for ?in-chart? data (EM means ?Exact Match.?)
LP LR UP UR F1 EM
Gold 85.62 85.41 89.70 69.47 85.51 45.07
Gold (only covered) 84.32 84.01 88.72 88.40 84.17 42.52
SL-full 83.27 82.88 87.93 87.52 83.08 40.19
Baseline 82.64 82.20 87.50 87.03 82.42 37.63
Table 5: Domain Adaptation Results
and a following verb as in (VP is (ADVP already)
installed). The attachment direction of the adverb is
thus left unspecified. Such structures are however
indistinguishably transformed to a binary structure
like (VP (VP? is already) installed) in the course of
the conversion to HPSG analysis since there is no
way to choose the proper direction only with the
information given in the source corpus. This de-
sign could be considered as a best-effort, systematic
choice under the insufficient information, but it con-
flicts with the annotators? intuition in some cases.
We found in the annotation results that the anno-
tators have left apparently wrong analyses on some
sentences, either those remaining from the initial
output proposed by the parser or a wrong structure
appeared after some operations by the annotators
(error type 4). Such errors are mainly due to the
fact that for some sentences a correct analysis cannot
be found in the parser?s CKY chart. This can hap-
pen either when the correct analysis is not covered
by the HPSG grammar, or the correct analysis has
been pruned by the beam-search mechanism in the
parser. To correct a wrong analysis from the insuffi-
cient grammar coverage, an expansion of the gram-
mar is necessary, either in the form of the expan-
sion of the lexicon, or an introduction of a new lex-
ical type. For the other errors from the beam-search
limitation, there is a chance to get a correct analysis
from the parser by enlarging the beam size as nec-
essary. The introduction of a new lexical type def-
initely requires a deep knowledge on the grammar
and thus out of the scope of our annotation frame-
work. The other cases can in principle be handled in
the current framework, e.g., by a dynamic expansion
of the lexicon (i.e., an introduction of a new associ-
ation between a word and known lexical type), and
by a dynamic tuning of the beam size.
To see the significance of the last type of the er-
ror, we re-evaluated the annotation results on the
Brown sentences after classifying them into: (1)
those for which the correct analyses were included
in the parser?s chart (in-chart, 65 sentences) and (2)
those for which the correct analyses were not in the
chart (out-chart, 35 sentences), either because of the
pruning effect or the insufficient grammar coverage.
The results shown in Table 8 clearly show that there
is a large difference in the accuracy of the annota-
tion results between these two cases. Actually, on
the in-chart sentences, the parser has returned the
correct analysis as the initial solution for over 50%
of the sentences, and the annotators saved it without
any operations. Thus, we believe it is quite effective
to add the above-mentioned functionalities to reduce
this type of errors.
6 Conclusion and Future Work
We proposed a new annotation framework for deep
grammars by using statistical parsers. From the the-
oretical point of view, we can achieve significantly
high quality HPSG annotations only by CFG annota-
tions, and the products can be useful for the domain
adaptation task. On the other hand, preliminary ex-
periments of a manual annotation show some diffi-
culties about CFG annotations for non-experts, es-
pecially grammar-specific ones. We hence need to
develop some bridging functions reducing such dif-
ficulties. One possible strategy is to introduce an-
other representation such as flat CFG than binary
CFG. While we adopted CFG interface in our first
prototype system, our scheme can be applied to an-
other interface such as dependency as long as there
exist some relatedness over syntax or semantics.
63
References
David Carter. 1997. The treebanker: a tool for super-
vised training of parsed corpora. In Workshop On
Computational Environments For Grammar Develop-
ment And Linguistic Engineering, pages 9?15.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an hpsg
parser. In Proceedings of the 10th International Con-
ference on Parsing Technologies, pages 11?22, Prague,
Czech Republic.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the peo-
ple. In Proceedings of the ACL 2010 Conference Short
Papers, pages 68?73, Uppsala, Sweden, July.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a japanese parsed corpus while improving the parsing
system. In Proceedings of the NLPRS, pages 719?724.
Henry Kuc?era and W. Nelson Francis. 1967. Compu-
tational Analysis of Present Day American English.
Brown University Press, June.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the Workshop on Human Language
Technology, pages 114?119.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao,
and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate hpsg pars-
ing. In Proceedings of the 10th International Confer-
ence on Parsing Technologies, pages 60?68.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ricardo Sa?nchez-Sa?ez, Joan-Andreu Sa?nchez, and Jose?-
Miguel Bened??. 2009. Interactive predictive parsing.
In Proceedings of the 11th International Conference
on Parsing Technologies, pages 222?225.
Ricardo Sa?nchez-Sa?ez, Luis A. Leiva, Joan-Andreu
Sa?nchez, and Jose?-Miguel Bened??. 2010. Interactive
predictive parsing using a web-based architecture. In
Proceedings of the NAACL HLT 2010 Demonstration
Session, pages 37?40.
Yi Zhang and Valia Kordoni. 2010. Discriminant rank-
ing for efficient treebanking. In Coling 2010: Posters,
pages 1453?1461, Beijing, China, August. Coling
2010 Organizing Committee.
64
Learning the Optimal use of Dependency-parsing Information for Finding
Translations with Comparable Corpora
Daniel Andrade?, Takuya Matsuzaki?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo
{daniel.andrade, matuzaki}@is.s.u-tokyo.ac.jp
?Microsoft Research Asia, Beijing
jtsujii@microsoft.com
Abstract
Using comparable corpora to find new word
translations is a promising approach for ex-
tending bilingual dictionaries (semi-) auto-
matically. The basic idea is based on the
assumption that similar words have similar
contexts across languages. The context of
a word is often summarized by using the
bag-of-words in the sentence, or by using
the words which are in a certain dependency
position, e.g. the predecessors and succes-
sors. These different context positions are
then combined into one context vector and
compared across languages. However, previ-
ous research makes the (implicit) assumption
that these different context positions should be
weighted as equally important. Furthermore,
only the same context positions are compared
with each other, for example the successor po-
sition in Spanish is compared with the suc-
cessor position in English. However, this is
not necessarily always appropriate for lan-
guages like Japanese and English. To over-
come these limitations, we suggest to perform
a linear transformation of the context vec-
tors, which is defined by a matrix. We de-
fine the optimal transformation matrix by us-
ing a Bayesian probabilistic model, and show
that it is feasible to find an approximate solu-
tion using Markov chain Monte Carlo meth-
ods. Our experiments demonstrate that our
proposed method constantly improves transla-
tion accuracy.
1 Introduction
Using comparable corpora to automatically extend
bilingual dictionaries is becoming increasingly pop-
ular (Laroche and Langlais, 2010; Andrade et al,
2010; Ismail and Manandhar, 2010; Laws et al,
2010; Garera et al, 2009). The general idea is
based on the assumption that similar words have
similar contexts across languages. The context of
a word can be described by the sentence in which
it occurs (Laroche and Langlais, 2010) or a sur-
rounding word-window (Rapp, 1999; Haghighi et
al., 2008). A few previous studies, like (Garera et
al., 2009), suggested to use the predecessor and suc-
cessors from the dependency-parse tree, instead of a
word window. In (Andrade et al, 2011), we showed
that including dependency-parse tree context posi-
tions together with a sentence bag-of-words context
can improve word translation accuracy. However
previous works do not make an attempt to find an
optimal combination of these different context posi-
tions.
Our study tries to find an optimal weighting and
aggregation of these context positions by learning
a linear transformation of the context vectors. The
motivation is that different context positions might
be of different importance, e.g. the direct predeces-
sors and successors from the dependency tree might
be more important than the larger context from the
whole sentence. Another motivation is that depen-
dency positions cannot be always compared across
different languages, e.g. a word which tends to oc-
cur as a modifier in English, can tend to occur in
Japanese in a different dependency position.
As a solution, we propose to learn the optimal
combination of dependency and bag-of-words sen-
tence information. Our approach uses a linear trans-
formation of the context vectors, before comparing
10
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 10?18,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
them using the cosine similarity. This can be con-
sidered as a generalization of the cosine similarity.
We define the optimal transformation matrix by the
maximum-a-posterior (MAP) solution of a Bayesian
probabilistic model. The likelihood function for a
translation matrix is defined by considering the ex-
pected achieved translation accuracy. As a prior, we
use a Dirichlet distribution over the diagonal ele-
ments in the matrix and a uniform distribution over
its non-diagonal elements. We show that it is fea-
sible to find an approximation of the optimal so-
lution using Markov chain Monte Carlo (MCMC)
methods. In our experiments, we compare the pro-
posed method, which uses this approximation, with
the baseline method which uses the cosine similarity
without any linear transformation. Our experiments
show that the translation accuracy is constantly im-
proved by the proposed method.
In the next section, we briefly summarize the most
relevant previous work. In Section 3, we then ex-
plain the baseline method which is based on previ-
ous research. Section 4 explains in detail our pro-
posed method, followed by Section 5 which pro-
vides an empirical comparison to the baseline, and
analysis. We summarize our findings in Section 6.
2 Previous Work
Using comparable corpora to find new translations
was pioneered in (Rapp, 1999; Fung, 1998). The ba-
sic idea for finding a translation for a word q (query),
is to measure the context of q and then to compare
the context with each possible translation candidate,
using an existing dictionary. We will call words
for which we have a translation in the given dic-
tionary, pivot words. First, using the source cor-
pus, they calculate the degree of association of a
query word q with all pivot words. The degree of
association is a measure which is based on the co-
occurrence frequency of q and the pivot word in a
certain context position. A context (position) can be
a word-window (Rapp, 1999), sentence (Utsuro et
al., 2003), or a certain position in the dependency-
parse tree (Garera et al, 2009; Andrade et al, 2011).
In this way, they get a context vector for q, which
contains the degree of association to the pivot words
in different context positions. Using the target cor-
pus, they then calculate a context vector for each
possible translation candidate x, in the same way.
Finally, they compare the context vector of q with
the context vector of each candidate x, and retrieve
a ranked list of possible translation candidates. In
the next section, we explain the baseline which is
based on that previous research.
The general idea of learning an appropriate
method to compare high-dimensional vectors is not
new. Related research is often called ?metric-
learning?, see for example (Xing et al, 2003; Basu
et al, 2004). However, for our objective function it
is difficult to find an analytic solution. To our knowl-
edge, the idea of parameterizing the transformation
matrix, in the way we suggest in Section 4, and to
learn an approximate solution with a fast sampling
strategy is new.
3 Baseline
Our baseline measures the degree of association be-
tween the query word q and each pivot word with
respect to several context positions. As a context
position we consider the predecessors, successors,
siblings with respect to the dependency parse tree,
and the whole sentence (bag-of-words). The depen-
dency information which is used is also illustrated in
Figure 1. As a measure of the degree of association
we use the Log-odds-ratio as proposed in (Laroche
and Langlais, 2010).
Figure 1: Example of the dependency information used
by our approach. Here, from the perspective of ?door?.
Next, we define the context vector which contains
the degree of association between the query and each
pivot in several context positions. First, for each
11
context position i we define a vector qi which con-
tains the degree of association with each pivot word
in the context position i. If we number the pivot
words from 1 to n, then this vector can be writ-
ten as qi = (q1i , . . . , qni ). Note that in our case i
ranges from 1 to 4, representing the context posi-
tions predecessors (1), successors (2), siblings (3),
and the sentence bag-of-words (4). Finally, the com-
plete context vector for the query q is a long vector
q which appends each qi, i.e.: q = (q1, . . . ,q4).
Next, in the same way as before, we create a con-
text vector x for each translation candidate x in the
target language. For simplicity, we assume that each
pivot word in the source language has only one cor-
responding translation in the target language. As
a consequence, the dimensions of q and x are the
same. Finally we can score each translation candi-
date by using the cosine similarity between q and
x.
We claim that all of the context positions (1 to 4)
can contain information which is helpful to identify
translation candidates. However, we do not know
about their relative importance, neither do we know
whether these dependency positions can be com-
pared across language pairs as different as Japanese
and English. The cosine similarity simply weights
all dependency position equally important and ig-
nores problems which might occur when comparing
dependency positions across languages.
4 Proposed Method
Our proposed method tries to overcome the short-
comings of the cosine-similarity by using the fol-
lowing generalization:
sim(q,x) = qAx
T
?qAqT?xAxT , (1)
where A is a positive-definite matrix in Rdn?dn, and
T is the transpose of a vector. This can also be con-
sidered as linear transformation of the vectors using?A before using the normal cosine similarity, see
also (Basu et al, 2004).1
The challenge is to find an appropriate matrix A
which is expected to take the correlations between
1Therefore, exactly speaking A is not the transformation
matrix, however it defines uniquely the transformation matrix?
A.
the different dimensions into account, and which op-
timally weights the different dimensions. Note that,
if we set A to the identity matrix, we recover the
normal cosine similarity, which is our baseline.
Clearly, finding an optimal matrix in Rdn?dn is
infeasible due to the high dimensionality. We will
therefore restrict the structure of A.
Let I be the identity matrix in Rn?n , then we
define the matrix A, as follows:
A =
?
???
d1I z1,2I z1,3I z1,4I
z1,2I d2I z2,3I z2,4I
z1,3I z2,3I d3I z3,4I
z1,4I z2,4I z3,4I d4I
?
???
It is clear from this definition that d1, . . . , d4 weights
the context positions 1 to 4. Furthermore, zi,j can
be interpreted as a the confusion coefficient between
context position i and j. For example, a high value
for z2,3 means that a pivot word which occurs in
the sibling position in Japanese (source language),
might not necessarily occur in the sibling position in
English (target language), but instead in the succes-
sor position. However, in order to reduce the dimen-
sionality of the parameter space further, we assume
that each such zi,j has the same value z. Therefore,
matrix A becomes
A =
?
???
d1I zI zI zI
zI d2I zI zI
zI zI d3I zI
zI zI zI d4I
?
??? .
In the next subsection we will explain how we de-
fine an optimal solution for A.
4.1 Optimal solution for A
We use a Bayesian probabilistic model in order to
define the optimal solution for A. Formally we try
to find the maximum-a-posterior (MAP) solution of
A, i.e.:
argmax
A
p(A|data, ?). (2)
The posterior probability is defined by
p(A|data, ?) ? fauc(data|A) ? p(A|?) . (3)
fauc(data|A) is the (unnormalized) likelihood func-
tion. p(A|?) is the prior that captures our prior be-
liefs about A, and which is parameterized by a hy-
perparameter ?.
12
4.1.1 The likelihood function fauc(data|A)
As a likelihood function we use a modification
of the area under the curve (AUC) of the accuracy-
vs-rank graph. The accuracy-vs-rank graph shows
the translation accuracy at different ranks. data
refers to the part of the gold-standard which is used
for training. Our complete gold-standard contains
443 domain-specific Japanese nouns (query words).
Each Japanese noun in the gold standard corre-
sponds to one pair of the form <Japanese noun
(query), English translations (answers)>. We de-
note the accuracy at rank r, by accr. The accuracy
accr is determined by counting how often the cor-
rect answer is listed in the top r translation candi-
dates suggested for a query, divided by the number
of all queries in data. The likelihood function is
now defined as follows:
fauc(data|A) =
20?
r=1
accr ? (21 ? r) . (4)
That means fauc(data|A) accumulates the accura-
cies at the ranks from 1 to 20, where we weight ac-
curacies at top ranks higher.
4.1.2 The prior p(A|?)
The prior over the transformation matrix is factor-
ized in the following manner:
p(A|?) = p(z|d1, . . . , d4) ? p(d1, . . . , d4|?) .
The prior over the diagonal is defined as a Dirichlet
distribution:
p(d1, . . . , d4|?) = 1B(?)
4?
i=1
d??1i
where ? is the concentration parameter of the sym-
metric Dirichlet, and B(?) is the normalization con-
stant. The prior over the non-diagonal value a is de-
fined as:
p(z|d1, . . . , d4) = 1? ? 1[0,?](z) (5)
where ? = min{d1, . . . , d4}.
First, note that our prior limits the possible matri-
ces A to matrices which have diagonal entries which
are between 0 and 1. This is not a restriction since
the ranking of the translation candidates induced by
the parameterized cosine similarity will not change
if A is multiplied by a constant c > 0 . To see this,
note that
sim(q,x) = q(c ?A)x?q(c ?A)q?x(c ?A)x
= qAx?qAq?xAx .
Second, note that our prior limits A further, by re-
quiring, in Equation (5), that every non-diagonal el-
ement is smaller or equal than any diagonal element.
That requirement is sensible since we do not expect
that a optimal similarity measure between English
and Japanese will prefer context which is similar in
different dependency positions, over context which
is similar in the same context positions. To see this,
imagine the extreme case where for example d1 is 0,
and instead z12 is 1. In that case the similarity mea-
sure would ignore any similarity in the predecessor
position, but would instead compare the predeces-
sors in Japanese with the successors in English.
Finally, note that our prior puts probability mass
over a subset of the positive-definite matrices in
R4?4, and puts no probability mass on matrices
which are not positive-definite. As a consequence,
the similarity measure in Equation (1) is ensured to
be well-defined.
4.2 Training
In the following we explain how we use the training
data in order to find a good solution for the matrix
A.
4.2.1 Setting hyperparameter ?
Recall, that ? weights our prior belief about how
strong we think that the different context positions
should be weighted equally. From a practical point-
of-view, we do not know how strong we should
weight that prior belief. We therefore use empirical
Bayes to estimate ?, that is we use part of the train-
ing data to set ?. First, using half of the training
set, we find the A which maximizes p(A|data, ?)
for several ?. Then, the remaining half of the train-
ing set is used to evaluate fauc(data|A) to find the
best ?. Note that the prior p(A|?) can also be con-
sidered as a regularization to prevent overfitting. In
the next sub-section we will explain how to find an
approximation ofAwhich maximizes p(A|data, ?).
13
4.2.2 Finding a MAP solution for A
Recall that matrix A is defined by using only five
parameters. Since the problem is low-dimensional,
we can therefore expect to find a reasonable solution
using sampling methods. For finding an approxima-
tion of the maximum-a-posteriori (MAP) solution of
p(A|data, ?), we use the following Markov chain
Monte Carlo procedure:
1. Initialize d1, . . . , d4 and z.
2. Leave z constant, and run Simulated-
Annealing to find the d1, . . . , d4 which
maximize p(A|data, ?).
3. Given d1, . . . , d4, sample from the uniform dis-
tribution [1,min(d1, . . . d4)] in order to find the
z which maximizes p(A|data, ?).
The steps 2. and 3. are repeated till the convergence
of the parameters.
Concerning step 2., we use Simulated-
Annealing for finding a (local) maximum of
p(d1, . . . , d4|data, ?) with the following settings:
As a jumping distribution we use a Dirichlet distri-
bution which we update every 1000 iterations. The
cooling rate is set to 1iteration .
For step 2. and 3. it is of utmost importance to
be able to evaluate p(A|data, ?) fast. The com-
putationally expensive part of p(A|data, ?) is to
evaluate fauc(data|A). In order to quickly evalu-
ate fauc(data|A), we need to pre-calculate part of
sim(q, x) for all queries q and all translation can-
didates x. To illustrate the basic idea, consider
sim(q, x) without the normalization of q and xwith
respect to A, i.e.:
sim(q, x) = qAxT = (q1, . . . ,q4)A(x1, . . . ,x4)T .
Let us denote I?dn a block matrix in Rdn?dn whichcontains in each n ? n block the identity matrix ex-
cept in its diagonal; the diagonal of I?dn contains the
n ? n matrix which is zero in all entries. We can
now rewrite matrix A as:
A =
?
???
d1I 0 0 0
0 d2I 0 0
0 0 d3I 0
0 0 0 d4I
?
???+ z ? I?dn .
And finally we can factor out the parameters
(d1, . . . d4) and z in the following way:
sim(q, x) = (d1, . . . , d4)?
?
??
q1xT1...
q4xT4
?
??+z?(qI?dnxT )
By pre-calculating
?
??
q1xT1...
q4xT4
?
?? and qI?dnxT , we can
make the evaluation of each sample, in steps 2. and
3., computationally feasible.
5 Experiments
In the experiments of the present study, we used
a collection of complaints concerning automobiles
compiled by the Japanese Ministry of Land, Infras-
tructure, Transport and Tourism (MLIT)2 and an-
other collection of complaints concerning automo-
biles compiled by the USA National Highway Traf-
fic Safety Administration (NHTSA)3. Both corpora
are publicly available. The corpora are non-parallel,
but are comparable in terms of content. The part
of MLIT and NHTSA which we used for our ex-
periments, contains 24090 and 47613 sentences, re-
spectively. The Japanese MLIT corpus was mor-
phologically analyzed and dependency parsed using
Juman and KNP4. The English corpus NHTSA was
POS-tagged and stemmed with Stepp Tagger (Tsu-
ruoka et al, 2005; Okazaki et al, 2008) and depen-
dency parsed using the MST parser (McDonald et
al., 2005). Using the Japanese-English dictionary
JMDic5, we found 1796 content words in Japanese
which have a translation which is in the English cor-
pus. These content words and their translations cor-
respond to our pivot words in Japanese and English,
respectively.6
2http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
3http://www-odi.nhtsa.dot.gov/downloads/index.cfm
4http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman.html and http://www-lab25.kuee.kyoto-
u.ac.jp/nl-resource/knp.html
5http://www.csse.monash.edu.au/ jwb/edict doc.html
6Recall that we assume a one-to-one correspondence be-
tween a pivot in Japanese and English. If a Japanese pivot word
as more than one English translation, we select the translation
for which the relative frequency in the target corpus is closest
to the pivot in the source corpus.
14
5.1 Evaluation
For the evaluation we extract a gold-standard which
contains Japanese and English noun pairs that ac-
tually occur in both corpora.7 The gold-standard
is created with the help of the JMDic dictionary,
whereas we correct apparently inappropriate trans-
lations, and remove general nouns such as ???
(possibility) and ambiguous words such as? (rice,
America). In this way, we obtain a final list of 443
domain-specific Japanese nouns.
Each Japanese noun in the gold-standard corre-
sponds to one pair of the form <Japanese noun
(query), English translations (answers)>. We divide
the gold-standard into two halves. The first half is
used for for learning the matrix A, the second part
is used for the evaluation. In general, we expect that
the optimal transformation matrixA depends mainly
on the languages (Japanese and English) and on the
corpora (MLIT and NHTSA). However, in practice,
the optimal matrix can also vary depending on the
part of the gold-standard which is used for training.
These random variations are especially large, if the
part of the gold-standard which is used for training
or testing is small.
In order to take these random effects into ac-
count, we perform repeated subsampling of the
gold-standard. In detail, we randomly split the gold-
standard into equally-sized training and test set. This
is repeated five times, leading to five training and
five test sets. The performance on each test set is
shown in Table 1. OPTIMIZED-ALL marks the re-
sult of our proposed method, where matrix A is opti-
mized using the training set. The optimization of the
diagonal elements d1, . . . , d4, and the non-diagonal
value z is as described in Section 4.2. Finally, the
baseline method, as described in 3, corresponds to
OPTIMIZED-ALL where d1, . . . , d4 are set to 1,
and z is set to 0. This baseline is denoted as NOR-
MAL. We can see that the overall translation accu-
racy varies across the test sets. However, we see that
in all test sets our proposed method OPTIMIZED-
ALL performs better than the baseline NORMAL.
7Note that if the current query (Japanese noun) is a pivot
word, then the word is not considered as a pivot word.
5.2 Analysis
In the previous section, we showed that the cosine-
similarity is sub-optimal for comparing context vec-
tors which contain information from different con-
text positions. We showed that it is possible to find
an approximation of a matrix A which optimally
weights, and combines the different context posi-
tions. Recall, that the matrix A is described by the
parameters d1 . . . d4 and z, which can interpreted as
context position weights and a confusion coefficient,
respectively. Therefore, by looking at these parame-
ters which we learned using each training set, we can
get some interesting insights. Table 2 shows theses
parameters learned for each training set.
We can see that the parameters, across the train-
ing sets, are not as stable as we wish. For example
the weight for the predecessor position ranges from
0.27 to 0.44. As a consequence, the average values,
shown in the last row of Table 2, have to be inter-
preted with care. We expect that the variance is due
to the limited size of the training set, 220 <query,
answers> pairs.
Nevertheless, we can draw some conclusions with
confidence. For example, we see that the prede-
cessor and successor positions are the most impor-
tant contexts, since the weights for both are al-
ways higher than for the other context positions.
Furthermore, we clearly see that the sibling and
sentence (bag-of-words) contexts, although not as
highly weighted as the former two, can be consid-
ered to be relevant, since each has a weight of around
0.20. Finally, we see that z, the confusion coeffi-
cient, is around 0.03, which is small.8 Therefore,
we verify z?s usefulness with another experiment.
We additionally define the method OPTIMIZED-
DIAG which uses the same matrix as OPTIMIZED-
ALL except that the confusion coefficient z is set
to zero. In Table 1, we can see that the accu-
racy of OPTIMIZED-DIAG is constantly lower than
OPTIMIZED-ALL.
Furthermore, we are interested in the role of the
whole sentence (bag-of-words) information which is
in the context vector (in position d4 of the block vec-
tor). Therefore, we excluded the sentence informa-
8In other words, z is around 17% of its maximal possible
value. The maximal possible value is around 0.18, since, recall
that z is, by definition, smaller or equal to min{d1 . . . d4}.
15
Test Set Method Top-1 Top-5 Top-10 Top-15 Top-20Accuracy Accuracy Accuracy Accuracy Accuracy
1
OPTIMIZED-ALL 0.20 0.37 0.47 0.50 0.54
OPTIMIZED-DIAG 0.20 0.34 0.43 0.48 0.51
NORMAL 0.18 0.32 0.43 0.47 0.50
2
OPTIMIZED-ALL 0.20 0.35 0.43 0.48 0.52
OPTIMIZED-DIAG 0.19 0.33 0.42 0.46 0.52
NORMAL 0.18 0.34 0.42 0.47 0.49
3
OPTIMIZED-ALL 0.17 0.31 0.37 0.44 0.48
OPTIMIZED-DIAG 0.17 0.27 0.36 0.41 0.45
NORMAL 0.16 0.27 0.36 0.41 0.44
4
OPTIMIZED-ALL 0.14 0.30 0.38 0.43 0.46
OPTIMIZED-DIAG 0.14 0.26 0.34 0.4 0.43
NORMAL 0.15 0.29 0.37 0.41 0.44
5
OPTIMIZED-ALL 0.18 0.34 0.42 0.46 0.51
OPTIMIZED-DIAG 0.17 0.30 0.38 0.43 0.48
NORMAL 0.19 0.31 0.40 0.44 0.48
average
OPTIMIZED-ALL 0.18 0.33 0.41 0.46 0.50
OPTIMIZED-DIAG 0.17 0.30 0.39 0.44 0.48
NORMAL 0.17 0.31 0.40 0.44 0.47
Table 1: Shows the accuracy at different ranks for all test sets, and, in the last column, the average over all test sets.
The proposed method OPTIMIZED-ALL is compared to the baseline NORMAL. Furthermore, for analysis, the results
when optimizing only the diagonal are marked as OPTIMIZED-DIAG.
Training Set d1 d2 d3 d4 zpredecessor successor sibling sentence confusion coefficient
1 0.35 0.26 0.19 0.20 0.03
2 0.27 0.29 0.21 0.23 0.03
3 0.35 0.31 0.16 0.18 0.02
4 0.44 0.24 0.17 0.16 0.04
5 0.39 0.28 0.20 0.13 0.03
average 0.36 0.28 0.19 0.18 0.03
Table 2: Shows the parameters which were learned using each training set. d1 . . . d4 are the weights of the context
positions, which sum up to 1. z marks the degree to which it is useful to compare context across different positions.
tion from the context vector. The accuracy results,
averaged over the same test sets as before, are shown
in Table 3. We can see that the accuracies are clearly
lower than before (compare to Table 1). This clearly
justifies to include additionally sentence information
into the context vector. It is also interesting to note
that the average z value is now 0.14.9 This is consid-
erable higher than before, and shows that a bag-of-
words model can partly make the use of z redundant.
However, note that the sentence bag-of-words model
covers a broader context, beyond the direct prede-
cessors, successor and siblings, which explains why
9That is 48% of its maximal possible value. Since for the
dependency positions predecessor, successor and sibling we get
the average weights 0.38, 0.33 and 0.29, respectively.
a small z value is still relevant in the situation where
we include sentence bag-of-words into the context
vector.
Finally, to see why it can be helpful to compare
different dependency positions from the context vec-
tors of Japanese and English, we looked at concrete
examples. We found, for example, that the trans-
lation accuracy of the query word ???? (disc)
improved when using OPTIMIZED-ALL instead of
OPTIMIZED-DIAG. The pivot word ?? (wrap)
tends together with both the Japanese query ??
?? (disc), and with the correct translation ?disc?
in English. However, that pivot word occurs in
Japanese and English in different context positions.
In the Japanese corpus ?? (wrap) tends to occur
16
Method Top-1 Top-5 Top-10 Top-15 Top-20
OPT-DEP 0.13 0.25 0.34 0.38 0.41
NOR-DEP 0.12 0.23 0.29 0.33 0.38
Table 3: The proposed method, but without the sentence
information in the context vector, is denoted OPT-DEP.
The baseline method, but without the sentence informa-
tion in the context vector, is denoted NOR-DEP.
together with the query???? (disc) in sentences
like for example the following:
????? (break)???? (disc)???
(wrap)???? (occured)??
That Japanese sentence can be literally translated as
?A wrap occured in the brake disc.?, where ?wrap?
is the sibling of ?disc? in the dependency tree. How-
ever, in English, considered out of the perspective
of ?disc?, the pivot word ?wrap? tends to occur in a
different dependency position. For example, the fol-
lowing sentence can be found in the English corpus:
?Front disc wraps.?
In English ?wrap? tends to occur as a successor of
?disc?. A non-zero confusion coefficient allows us
to account some degree of similarity to situations
where the query (here ??????(disc)) and the
translation candidate (here ?disc?) tend to occur with
the same pivot word (here ?wrap?), but in different
dependency positions.
6 Conclusions
Finding new translations of single words using com-
parable corpora is a promising method, for exam-
ple, to assist the creation and extension of bilin-
gual dictionaries. The basic idea is to first create
context vectors of the query word, and all the can-
didate translations, and then, in the second step,
to compare these context vectors. Previous work
(Laroche and Langlais, 2010; Fung, 1998; Garera
et al, 2009) suggests that for this task the cosine-
similarity is a good choice to compare context vec-
tors. For example, Garera et al (2009) include the
information of various context positions from the
dependency-parse tree in one context vector, and, af-
terwards, compares these context vectors using the
cosine-similarity. However, this makes the implicit
assumption that all context positions are equally im-
portant, and, furthermore, that context from differ-
ent context positions does not need to be compared
with each other. To overcome these limitations, we
suggested to use a generalization of the cosine simi-
larity which performs a linear transformation of the
context vectors, before applying the cosine similar-
ity. The linear transformation can be described by a
positive-definite matrix A. We defined the optimal
matrix A by using a Bayesian probabilistic model.
We demonstrated that it is feasible to approximate
the optimal matrix A by using MCMC-methods.
Our experimental results suggest that it is bene-
ficial to weight context positions individually. For
example, we found that predecessor and successor
should be stronger weighted than sibling, and sen-
tence information. Whereas, the latter two are also
important, having a total weight of around 40%.
Furthermore, we showed that for languages as dif-
ferent as Japanese and English it can be helpful to
compare also different context positions across both
languages. The proposed method constantly outper-
formed the baseline method. Top 1 accuracy in-
creased by up to 2% percent points and Top 20 by
up to 4% percent points.
For future work, we consider to use different pa-
rameterizations of the matrix A which could lead to
even higher improvement in accuracy. Furthermore,
we consider to include, and weight additional fea-
tures like transliteration similarity.
Acknowledgment
We would like to thank the anonymous reviewers
for their helpful comments. This work was partially
supported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan). The first author is sup-
ported by the MEXT Scholarship and by an IBM
PhD Scholarship Award.
References
D. Andrade, T. Nasukawa, and J. Tsujii. 2010. Robust
measurement and comparison of context similarity for
finding translation pairs. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 19?27.
D. Andrade, T. Matsuzaki, and J. Tsujii. 2011. Effec-
tive use of dependency structure for bilingual lexicon
17
creation. In Proceedings of the International Confer-
ence on Computational Linguistics and Intelligent Text
Processing, Lecture Notes in Computer Science, pages
80?92. Springer Verlag.
S. Basu, M. Bilenko, and R.J. Mooney. 2004. A prob-
abilistic framework for semi-supervised clustering. In
Proceedings of the ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 59?68.
P. Fung. 1998. A statistical view on bilingual lexicon ex-
traction: from parallel corpora to non-parallel corpora.
Lecture Notes in Computer Science, 1529:1?17.
N. Garera, C. Callison-Burch, and D. Yarowsky. 2009.
Improving translation lexicon induction from mono-
lingual corpora via dependency contexts and part-of-
speech equivalences. In Proceedings of the Confer-
ence on Computational Natural Language Learning,
pages 129?137. Association for Computational Lin-
guistics.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and D. Klein.
2008. Learning bilingual lexicons from monolingual
corpora. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, pages
771?779. Association for Computational Linguistics.
A. Ismail and S. Manandhar. 2010. Bilingual lexicon
extraction from comparable corpora using in-domain
terms. In Proceedings of the International Conference
on Computational Linguistics, pages 481 ? 489.
A. Laroche and P. Langlais. 2010. Revisiting context-
based projection methods for term-translation spotting
in comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguistics,
pages 617 ? 625.
F. Laws, L. Michelbacher, B. Dorow, C. Scheible,
U. Heid, and H. Schu?tze. 2010. A linguistically
grounded graph model for bilingual lexicon extrac-
tion. In Proceedings of the International Conference
on Computational Linguistics, pages 614?622. Inter-
national Committee on Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics, pages 91?98. Association
for Computational Linguistics.
N. Okazaki, Y. Tsuruoka, S. Ananiadou, and J. Tsujii.
2008. A discriminative candidate generator for string
transformations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 447?456. Association for Computational Lin-
guistics.
R. Rapp. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 519?526. Asso-
ciation for Computational Linguistics.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Lecture
Notes in Computer Science, 3746:382?392.
T. Utsuro, T. Horiuchi, K. Hino, T. Hamamoto, and
T. Nakayama. 2003. Effect of cross-language IR
in bilingual lexicon acquisition from comparable cor-
pora. In Proceedings of the conference on European
chapter of the Association for Computational Linguis-
tics, pages 355?362. Association for Computational
Linguistics.
E.P. Xing, A.Y. Ng, M.I. Jordan, and S. Russell. 2003.
Distance metric learning with application to clustering
with side-information. Advances in Neural Informa-
tion Processing Systems, pages 521?528.
18
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 71?75,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Efficient Logical Inference for Semantic Processing
Ran Tian
?
Yusuke Miyao Takuya Matsuzaki
National Institute of Informatics, Japan
{tianran,yusuke,takuya-matsuzaki}@nii.ac.jp
Abstract
Dependency-based Compositional Se-
mantics (DCS) provides a precise and
expressive way to model semantics of
natural language queries on relational
databases, by simple dependency-like
trees. Recently abstract denotation is pro-
posed to enable generic logical inference
on DCS. In this paper, we discuss some
other possibilities to equip DCS with
logical inference, and we discuss further
on how logical inference can help textual
entailment recognition, or other semantic
precessing tasks.
1 Introduction
Dependency-based Compositional Semantics
(DCS) was proposed as an interface for querying
relational databases by natural language. It
features DCS trees as semantic representation,
with a structure similar to dependency trees. In
its basic version, a node of a DCS tree indicates
a table in the database, and an edge indicates a
join relation. Both ends of an edge are labeled by
a field of the corresponding table (Liang et al.,
2011). However, when DCS is applied to logical
inference on unrestricted texts, it is unrealistic to
assume an explicit database, because we cannot
prepare a database for everything in the world.
For this reason, DCS trees are detached from any
specific relational database, in a way that each
node of a DCS tree indicates a content word in a
sentence (thus no fixed set of possible word labels
for a DCS tree node), and each edge indicates
?
Current affiliation of the first author: Graduate School
of Information Sciences, Tohoku University, Japan. Email
address: tianran@ecei.tohoku.ac.jp
a semantic relation between two words. Labels
on the two ends of an edge, initially indicating
fields of tables in a database, are considered
as semantic roles of the corresponding words.
Abstract denotation is proposed to capture the
meaning of this abstract version of DCS tree,
and a textual inference system based on abstract
denotation is built (Tian et al., 2014).
It is quite natural to apply DCS trees, a simple
and expressive semantic representation, to textual
inference; however the use of abstract denotations
to convey logical inference is somehow unusual.
There are two seemingly obvious way to equip
DCS with logical inference: (i) at the tree level, by
defining a set of logically sound transformations
of DCS trees; or (ii) at the logic level, by convert-
ing DCS trees to first order predicate logic (FOL)
formulas and then utilizing a theorem prover. For
(i), it may not be easy to enumerate all types of
logically sound transformations, but tree transfor-
mations can be seen as an approximation of logical
inference. For (ii), abstract denotation is more ef-
ficient than FOL formula, because abstract deno-
tation eliminates quantifiers and meanings of nat-
ural language texts can be represented by atomic
sentences.
To elaborate the above discussion and to pro-
vide more topics to the literature, in this paper we
discuss the following four questions: (?2) How
well can tree transformation approximate logical
inference? (?3) With rigorous inference on DCS
trees, where does logic contribute in the system
of Tian et al. (2014)? (?4) Does logical inference
have further potentials in Recognizing Textual En-
tailment (RTE) task? and (?5) How efficient is ab-
stract denotation compared to FOL formula? We
provide examples or experimental results to the
above questions.
71
  
stormT?: H?: ARGblame deathDebbyARG ARGOBJstormARG ARG
IOBJ
tropicalARG MOD
cause losslife
ARG
SBJ
ARG
MOD
OBJ
Figure 1: DCS trees of T: Tropical storm Debby is
blamed for death and H: A storm has caused loss
of life
2 Tree transformation vs. logical
inference
In the tree transformation based approach to RTE,
it has been realized that some gaps between T and
H cannot be filled even by a large number of tree
transformation rules extracted from corpus (Bar-
Haim et al., 2007a). For example in Figure 1, it
is possible to extract the rule blamed for death?
cause loss of life, but not easy to extract tropical
storm Debby? storm, because ?Debby? could be
an arbitrary name which may not even appear in
the corpus.
This kind of gaps was typically addressed by
approximate matching methods, for example by
counting common sub-graphs of T and H, or by
computing a cost of tree edits that convert T to
H. In the example of Figure 1, we would expect
that T is ?similar enough? (i.e. has many common
sub-graphs) with H, or the cost to convert T into H
(e.g. by deleting the node Debby and then add the
node storm) is low. As for how similar is enough,
or how the cost is evaluated, we will need a statis-
tical model to train on RTE development set.
It was neglected that some combinations of tree
edits are logical (while some are not). The entail-
ment pair in Figure 1 can be easily treated by log-
ical inference, as long as the apposition tropical
storm = Debby is appropriately handled. In con-
trast to graph matching or tree edit models which
theoretically admit arbitrary tree transformation,
logical inference clearly discriminate sound trans-
formations from unsound ones. In this sense, there
would be no need to train on RTE data.
When coreference is considered, logically
sound tree transformations can be quite compli-
cated. The following is a modified example from
RTE2-dev:
T: Hurricane Isabel, which caused significant
damage, was a tropical storm when she entered
Virginia.
  
stoormblaedbhDypi
mbtdD
drclrurmblf??bcD
fo?rmby
df??
?rocrlrbd?pi
ARG
DlfDo
DlfDo mbtdD
?b?cD
df??T?: H?:ARG TIMEARG MOD TIME
ARG
SBJ
OBJ
ARG
ARG
MOD
OBJSBJ
ARG ARG
ARGARG SBJ
SBJ
SBJ
OBJ
ARG
OBJ
?rocrlrb
Figure 2: DCS trees with coreference
H: A storm entered Virginia, causing damage.
The corresponding DCS trees are shown in Fig-
ure 2. Though the DCS trees of T and H are
quite different, H can actually be proven from T.
Note the coreference between Hurricane Isabel
and she, suggesting us to copy the subtree of Hur-
ricane Isabel to she, in a tree edit approach. This
is not enough yet, because the head storm in T is
not placed at the subject of cause. The issue is in-
deed very logical: from ?Hurricane Isabel = she?,
?Hurricane Isabel = storm?, ?she = subject of en-
ter? and ?Hurricane Isabel = subject of cause?,
we can imply that ?storm = subject of enter = sub-
ject of cause?.
3 Alignment with logical clues
Tian et al. (2014) proposed a way to generate on-
the-fly knowledge to fill knowledge gaps: if H is
not proven, compare DCS trees of T and H to
generate path alignments (e.g. blamed for death
? cause loss of life, as underscored in Figure 1);
evaluate the path alignments by a similarity score
function; and path alignments with a score greater
than a threshold (0.4) are accepted and converted
to inference rules.
The word vectors Tian et al. (2014) use to
calculate similarities are reported able to cap-
ture semantic compositions by simple additions
and subtractions (Mikolov et al., 2013). This is
also the case when used as knowledge resource
for RTE, for example the similarities between
blamed+death and cause+loss+life, or between
found+shot+dead and killed, are computed >
0.4.
However, generally such kind of similarity is
very noisy. Tian et al. (2014) used some logical
clues to filter out irrelevant path alignments, which
helps to keep a high precision. To evaluate the
effect of such logical filters, we compare it with
some other alignment strategies, the performance
of which on RTE5-test data is shown in Table 1.
Each strategy is described in the following.
72
Strategy Prec. Rec. Acc.
LogicClue + Inference 69.9 55.0 65.7
LexNoun + Inference 64.2 57.3 62.7
LexNoun + Coverage 57.1 75.0 59.3
NoFilter + Coverage 54.2 87.7 56.8
Table 1: Comparison of different alignment strate-
gies
LogicClue + Inference This is the system of
Tian et al. (2014)
1
, which use logical clues to filter
out irrelevant path alignments, and apply accepted
path alignments as inference rules.
LexNoun + Inference The same system as
above, except that we only align paths between
lexically aligned nouns. Two nouns are aligned
if and only if they are synonyms, hyponyms or
derivatively related in WordNet.
LexNoun + Coverage As above, paths between
lexically aligned nouns are aligned, and aligned
paths with similarity score > 0.4 are accepted. If
all nodes in H can be covered by some accepted
path alignments, then output ?Y?. This is very
similar to the system described in Bar-Haim et al.
(2007b).
NoFilter + Coverage Same as above, but all
paths alignments with similarity score > 0.4 are
accepted.
4 How can logical inference help RTE?
Logical inference is shown to be useful for RTE,
as Tian et al. (2014) demonstrates a system with
competitive results. However, despite the expec-
tation that all entailment matters can be explained
logically, our observation is that currently logical
inference only fills very limited short gaps from T
to H. The logical phenomena easily addressed by
Tian et al. (2014)?s framework, namely universal
quantifiers and negations, seems rare in PASCAL
RTE data. Most heavy lifting is done by distribu-
tional similarities between phrases, which may fail
in complicated sentences. An especially complex
example is:
T: Wal-Mart Stores Inc. said Tuesday that a Mas-
sachusetts judge had granted its motion to decer-
tify a class action lawsuit accusing the world?s
largest retailer of denying employees breaks.
H: Employee breaks had been denied by a motion
granted by a Massachusetts judge.
1
http://kmcs.nii.ac.jp/tianran/tifmo/
  100 1000 10000 100000 10000001
2
3
4
5
6
R? = 0.24
?? ? ? ? ?? ??? ? ? ? ? ? ?
???? ??????
Figure 3: Time of forward-chaining (seconds) in
our system, plotted on weights of statements (log-
arithmic scale).
Orig. 3 Sec. Orig. 5 Min. Red. 5 Min.
Proof found 8 16 82
Too many variables 5 24 3
Failed to find proof 0 1 3
Memory limit 0 2 0
Time out 86 57 13
Table 2: Proportion (%) of exit status of Prover9
The system of Tian et al. (2014) generated on-
the-fly knowledge to join several fragments in T
and wrongly proved H. In examples of such com-
plexity, distributional similarity is no longer reli-
able. However, it may be possible to build a pri-
ori logical models at the meta level, such as on
epistemic, intentional and reportive attitudes. The
models then can provide signals for semantic pars-
ing to connect the logic to natural language, such
as the words ?grant?, ?decertify?, and ?accuse? in
the above example. We hope this approach can
bring new progress to RTE and other semantic pro-
cessing tasks.
5 Efficiency of abstract denotations
To evaluate the efficiency of logical inference on
abstract denotations, we took 110 true entailment
pairs from RTE5 development set, which are also
pairs that can be proven with on-the-fly knowl-
edge. We plot the running time of Tian et al.
(2014)?s inference engine (single-threaded) on a
2.27GHz Xeon CPU, with respect to the weighted
sum of all statements
2
, as shown in Figure 3. The
graph shows all pairs can be proven in 6 seconds,
and proof time scales logarithmically on weight of
statements.
On the other hand, we converted statements on
abstract denotations into FOL formulas, and tried
to prove the same pairs using Prover9,
3
a popu-
2
If a statement is translated to FOL formula, the weight of
this statement equals to the weighted sum of all predicates in
the FOL formula, where an n-ary predicate is weighted as n.
3
www.cs.unm.edu/
?
mccune/prover9/
73
lar FOL theorem prover. As the result turns out
(Table 2), only 8% of the pairs can be proven in
3 seconds (the ?Orig. 3 Sec.? column), and only
16% pairs can be proven in 5 minutes (the ?Orig.
5 Min.? column), showing severe difficulties for
an FOL prover to handle textual inferences with
many (usually hundreds of) on-the-fly rules. As
such, we use Tian et al. (2014)?s inference engine
to pin down statements that are actually needed for
proving H (usually just 2 or 3 statements), and try
to prove H by Prover9 again, using only necessary
statements. Proven pairs in 5 minutes then jump
to 82% (the ?Red. 5 Min.? column), showing that
a large number of on-the-fly rules may drastically
increase computation cost. Still, nearly 20% pairs
cannot be proven even in this setting, suggesting
that traditional FOL prover is not suited for tex-
tual inference.
6 Conclusion and future work
We have discussed the role that logical infer-
ence could play in RTE task, and the efficiency
of performing inference on abstract denotations.
Though currently logical inference contributes at
places that are somehow inconspicuous, there is
the possibility that with some meta level logical
models and the methodology of semantic parsing,
we can build systems that understand natural lan-
guage texts deeply: logic implies (in)consistency,
which is in turn used as signals to produce more
accurate semantic interpretation. And after all, as
there may be many possible variations of seman-
tic representations, it is good to have an efficient
inference framework that has the potential to con-
nect them. It would be exciting if we can combine
different types of structured data with natural lan-
guage in semantic processing tasks. Directions of
our future work are described below.
Improvement of similarity score To calculate
phrase similarities, Tian et al. (2014) use the co-
sine similarity of sums of word vectors, which ig-
nores syntactic information. We plan to add syn-
tactic information to words by some supertags,
and learn a vector space embedding for this struc-
ture.
Integration of FreeBase to RTE It would be
exciting if we can utilize the huge amount of Free-
Base data in RTE task. Using the framework of
abstract denotation, meanings of sentences can be
explained as relational database queries; to convert
it to FreeBase data queries is like relational to on-
tology schema matching. In order to make effec-
tive use of FreeBase data, we also need to recog-
nize entities and relations in natural language sen-
tences. Previous research on semantic parsing will
be very helpful for learning such mapping.
Winograd Schema Challenge (WSC) As the
RTE task, WSC (Levesque et al., 2012) also pro-
vides a test bed for textual inference systems. A
Winograd schema is a pair of similar sentences but
contain an ambiguity of pronouns that is resolved
in opposite ways. A complicated partial example
is:
Michael decided to freeze himself in
cryo-stasis even though his father was
against it, because he hopes to be un-
frozen in the future when there is a cure
available.
The logical interplay among decided, hopes,
even though, because, and the realization that he
is coreferent to Michael (but not his father) is in-
triguing. By working on the task, we hope to gain
further understanding on how knowledge can be
gathered and applied in natural language reason-
ing.
Acknowledgments This research was supported
by the Todai Robot Project at National Institute of
Informatics.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007a. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI 2007.
Roy Bar-Haim, Ido Dagan, Iddo Greental, Idan Szpek-
tor, and Moshe Friedman. 2007b. Semantic in-
ference at the lexical-syntactic level for textual en-
tailment recognition. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Hector Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Knowledge Representation and Reasoning Confer-
ence.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of ACL 2011.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL
2013.
74
Ran Tian, Yusuke Miyao, and Matsuzaki Takuya.
2014. Logical inference on dependency-based com-
positional semantics. In Proceedings of ACL 2014.
75
