Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
On statistical parsing of French with supervised and semi-supervised
strategies
Marie Candito*, Beno?t Crabb?* and Djam? Seddah?
* Universit? Paris 7
UFRL et INRIA (Alpage)
30 rue du Ch?teau des Rentiers
F-75013 Paris ? France
? Universit? Paris 4
LALIC et INRIA (Alpage)
28 rue Serpente
F-75006 Paris ? France
Abstract
This paper reports results on grammati-
cal induction for French. We investigate
how to best train a parser on the French
Treebank (Abeill? et al, 2003), viewing
the task as a trade-off between generaliz-
ability and interpretability. We compare,
for French, a supervised lexicalized pars-
ing algorithm with a semi-supervised un-
lexicalized algorithm (Petrov et al, 2006)
along the lines of (Crabb? and Candito,
2008). We report the best results known
to us on French statistical parsing, that we
obtained with the semi-supervised learn-
ing algorithm. The reported experiments
can give insights for the task of grammat-
ical learning for a morphologically-rich
language, with a relatively limited amount
of training data, annotated with a rather
flat structure.
1 Natural language parsing
Despite the availability of annotated data, there
have been relatively few works on French statis-
tical parsing. Together with a treebank, the avail-
ability of several supervised or semi-supervised
grammatical learning algorithms, primarily set up
on English data, allows us to figure out how they
behave on French.
Before that, it is important to describe the char-
acteristics of the parsing task. In the case of sta-
tistical parsing, two different aspects of syntactic
structures are to be considered : their capacity to
capture regularities and their interpretability for
further processing.
Generalizability Learning for statistical parsing
requires structures that capture best the underlying
regularities of the language, in order to apply these
patterns to unseen data.
Since capturing underlying linguistic rules is
also an objective for linguists, it makes sense
to use supervised learning from linguistically-
defined generalizations. One generalization is
typically the use of phrases, and phrase-structure
rules that govern the way words are grouped to-
gether. It has to be stressed that these syntactic
rules exist at least in part independently of seman-
tic interpretation.
Interpretability But the main reason to use su-
pervised learning for parsing, is that we want
structures that are as interpretable as possible, in
order to extract some knowledge from the anal-
ysis (such as deriving a semantic analysis from
a parse). Typically, we need a syntactic analysis
to reflect how words relate to each other. This
is our main motivation to use supervised learn-
ing : the learnt parser will output structures as
defined by linguists-annotators, and thus inter-
pretable within the linguistic theory underlying the
annotation scheme of the treebank. It is important
to stress that this is more than capturing syntactic
regularities : it has to do with the meaning of the
words.
It is not certain though that both requirements
(generalizability / interpretability) are best met in
the same structures. In the case of supervised
learning, this leads to investigate different instan-
tiations of the training trees, to help the learning,
while keeping the maximum interpretability of the
trees. As we will see with some of our experi-
ments, it may be necessary to find a trade-off be-
tween generalizability and interpretability.
Further, it is not guaranteed that syntactic rules
infered from a manually annotated treebank pro-
duce the best language model. This leads to
49
methods that use semi-supervised techniques on
a treebank-infered grammar backbone, such as
(Matsuzaki et al, 2005; Petrov et al, 2006).
The plan of the paper is as follows : in the
next section, we describe the available treebank
for French, and how its structures can be inter-
preted. In section 3, we describe the typical prob-
lems encountered when parsing using a plain prob-
abilistic context-free grammar, and existing algo-
rithmic solutions that try to circumvent these prob-
lems. Next we describe experiments and results
when training parsers on the French data. Finally,
we discuss related work and conclude.
2 Interpreting the French trees
The French Treebank (Abeill? et al, 2003) is a
publicly available sample from the newspaper Le
Monde, syntactically annotated and manually cor-
rected for French.
<SENT>
<NP fct="SUJ">
<w cat="D" lemma="le" mph="ms" subcat="def">le</w>
<w cat="N" lemma="bilan" mph="ms" subcat="C">bilan</w>
</NP>
<VN>
<w cat="ADV" lemma="ne" subcat="neg">n?</w>
<w cat="V" lemma="?tre" mph="P3s" subcat="">est</w>
</VN>
<AdP fct="MOD">
<w compound="yes" cat="ADV" lemma="peut-?tre">
<w catint="V">peut</w>
<w catint="PONCT">-</w>
<w catint="V">?tre</w>
</w>
<w cat="ADV" lemma="pas" subcat="neg">pas</w>
</AdP>
<AP fct="ATS">
<w cat="ADV" lemma="aussi">aussi</w>
<w cat="A" lemma="sombre" mph="ms" subcat="qual">sombre</w>
</AP>
<w cat="PONCT" lemma="." subcat="S">.</w>
</SENT>
Figure 1: Simplified example of the FTB
To encode syntactic information, it uses a com-
bination of labeled constituents, morphological
annotations and functional annotation for verbal
dependents as illustrated in Figure 1. This con-
stituent and functional annotation was performed
in two successive steps : though the original re-
lease (Abeill? et al, 2000) consists of 20,648 sen-
tences (hereafter FTB-V0), the functional annota-
tion was performed later on a subset of 12351 sen-
tences (hereafter FTB). This subset has also been
revised, and is known to be more consistently an-
notated. This is the release we use in our experi-
ments. Its key properties, compared with the Penn
Treebank, (hereafter PTB) are the following :
Size : The FTB is made of 385 458 tokens and
12351 sentences, that is the third of the PTB. The
average length of a sentence is 31 tokens in the
FTB, versus 24 tokens in the PTB.
Inflection : French morphology is richer than En-
glish and leads to increased data sparseness for
statistical parsing. There are 24098 types in the
FTB, entailing an average of 16 tokens occurring
for each type (versus 12 for the PTB).
Flat structure : The annotation scheme is flatter
in the FTB than in the PTB. For instance, there
are no VPs for finite verbs, and only one sentential
level for sentences whether introduced by comple-
mentizer or not. We can measure the corpus flat-
ness using the ratio between tokens and non ter-
minal symbols, excluding preterminals. We obtain
0.69 NT symbol per token for FTB and 1.01 for the
PTB.
Compounds : Compounds are explicitly annotated
(see the compound peut-?tre in Figure 1 ) and very
frequent : 14,52% of tokens are part of a com-
pound. They include digital numbers (written with
spaces in French 10 000), very frozen compounds
pomme de terre (potato) but also named entities
or sequences whose meaning is compositional but
where insertion is rare or difficult (garde d?enfant
(child care)).
Now let us focus on what is expressed in the
French annotation scheme, and why syntactic in-
formation is split between constituency and func-
tional annotation.
Syntactic categories and constituents capture dis-
tributional generalizations. A syntactic category
groups forms that share distributional properties.
Nonterminal symbols that label the constituents
are a further generalizations over sequences of cat-
egories or constituents. For instance about any-
where it is grammatical to have a given NP, it is
implicitly assumed that it will also be grammati-
cal - though maybe nonsensical - to have instead
any other NPs. Of course this is known to be false
in many cases : for instance NPs with or with-
out determiners have very different distributions in
French (that may justify a different label) but they
also share a lot. Moreover, if words are taken into
account, and not just sequences of categories, then
constituent labels are a very coarse generalization.
Constituents also encode dependencies : for in-
stance the different PP-attachment for the sen-
tences I ate a cake with cream / with a fork re-
flects that with cream depends on cake, whereas
with a fork depends on ate. More precisely, a
syntagmatic tree can be interpreted as a depen-
dency structure using the following conventions :
50
for each constituent, given the dominating symbol
and the internal sequence of symbols, (i) a head
symbol can be isolated and (ii) the siblings of that
head can be interpreted as containing dependents
of that head. Given these constraints, the syntag-
matic structure may exhibit various degree of flat-
ness for internal structures.
Functional annotation Dependencies are en-
coded in constituents. While X-bar inspired con-
stituents are supposed to contain all the syntac-
tic information, in the FTB the shape of the con-
stituents does not necessarily express unambigu-
ously the type of dependency existing between a
head and a dependent appearing in the same con-
stituent. Yet this is crucial for example to ex-
tract the underlying predicate-argument structures.
This has led to a ?flat? annotation scheme, com-
pleted with functional annotations that inform on
the type of dependency existing between a verb
and its dependents. This was chosen for French
to reflect, for instance, the possibility to mix post-
verbal modifiers and complements (Figure 2), or
to mix post-verbal subject and post-verbal indi-
rect complements : a post verbal NP in the FTB
can correspond to a temporal modifier, (most of-
ten) a direct object, or an inverted subject, and in
the three cases other subcategorized complements
may appear.
SENT
NP-SUJ
D
une
N
lettre
VN
V
avait
V
?t?
V
envoy?e
NP-MOD
D
la
N
semaine
A
derni?re
PP-AOBJ
P
aux
NP
N
salari?s
SENT
NP-SUJ
D
Le
N
Conseil
VN
V
a
V
notifi?
NP-OBJ
D
sa
N
d?cision
PP-AOBJ
P
?
NP
D
la
N
banque
Figure 2: Two examples of post-verbal NPs : a
direct object and a temporal modifier
3 Algorithms for probabilistic grammar
learning
We propose here to investigate how to apply statis-
tical parsing techniques mainly tested on English,
to another language ? French ?. In this section we
briefly introduce the algorithms investigated.
Though Probabilistic Context Free Grammars
(PCFG) is a baseline formalism for probabilistic
parsing, it suffers a fundamental problem for the
purpose of natural language parsing : the inde-
pendence assumptions made by the model are too
strong. In other words all decisions are local to a
grammar rule.
However as clearly pointed out by (Johnson,
1998) decisions have to take into account non lo-
cal grammatical properties: for instance a noun
phrase realized in subject position is more likely to
be realized by a pronoun than a noun phrase real-
ized in object position. Solving this first method-
ological issue, has led to solutions dubbed here-
after as unlexicalized statistical parsing (Johnson,
1998; Klein and Manning, 2003a; Matsuzaki et
al., 2005; Petrov et al, 2006).
A second class of non local decisions to be
taken into account while parsing natural languages
are related to handling lexical constraints. As
shown above the subcategorization properties of
a predicative word may have an impact on the de-
cisions concerning the tree structures to be asso-
ciated to a given sentence. Solving this second
methodological issue has led to solutions dubbed
hereafter as lexicalized parsing (Charniak, 2000;
Collins, 1999).
In a supervised setting, a third and practical
problem turns out to be critical: that of data
sparseness since available treebanks are generally
too small to get reasonable probability estimates.
Three class of solutions are possible to reduce data
sparseness: (1) enlarging the data manually or au-
tomatically (e.g. (McClosky et al, 2006) uses self-
training to perform this step) (2) smoothing, usu-
ally this is performed using a markovization pro-
cedure (Collins, 1999; Klein and Manning, 2003a)
and (3) make the data more coarse (i.e. clustering).
3.1 Lexicalized algorithm
The first algorithm we use is the lexicalized parser
of (Collins, 1999). It is called lexicalized, as it
annotates non terminal nodes with an additional
latent symbol: the head word of the subtree. This
additional information attached to the categories
aims at capturing bilexical dependencies in order
to perform informed attachment choices.
The addition of these numerous latent sym-
bols to non terminals naturally entails an over-
specialization of the resulting models. To en-
sure generalization, it therefore requires to add
additional simplifying assumptions formulated as
a variant of usual na?ve Bayesian-style simplify-
ing assumptions: the probability of emitting a non
51
head node is assumed to depend on the head and
the mother node only, and not on other sibling
nodes1.
Since Collins demonstrated his models to sig-
nificantly improve parsing accuracy over bare
PCFG, lexicalization has been thought as a ma-
jor feature for probabilistic parsing. However two
problems are worth stressing here: (1) the reason
why these models improve over bare PCFGs is not
guaranteed to be tied to the fact that they capture
bilexical dependencies and (2) there is no guar-
antee that capturing non local lexical constraints
yields an optimal language model.
Concerning (1) (Gildea, 2001) showed that full
lexicalization has indeed small impact on results :
he reimplemented an emulation of Collins? Model
1 and found that removing all references to bilex-
ical dependencies in the statistical model2, re-
sulted in a very small parsing performance de-
crease (PARSEVAL recall on WSJ decreased from
86.1 to 85.6). Further studies conducted by (Bikel,
2004a) proved indeed that bilexical information
were used by the most probable parses. The idea
is that most bilexical parameters are very similar
to their back-off distribution and have therefore a
minor impact. In the case of French, this fact can
only be more true, with one third of training data
compared to English, and with a much richer in-
flection that worsens lexical data sparseness.
Concerning (2) the addition of head word an-
notations is tied to the use of manually defined
heuristics highly dependent on the annotation
scheme of the PTB. For instance, Collins? mod-
els integrate a treatment of coordination that is not
adequate for the FTB-like coordination annotation.
3.2 Unlexicalized algorithms
Another class of algorithms arising from (John-
son, 1998; Klein and Manning, 2003a) attempts
to attach additional latent symbols to treebank cat-
egories without focusing exclusively on lexical
head words. For instance the additional annota-
tions will try to capture non local preferences like
1This short description cannot do justice to (Collins,
1999) proposal which indeed includes more fine grained in-
formations and a backoff model. We only keep here the key
aspects of his work relevant for the current discussion.
2Let us consider a dependent constituent C with head
word Chw and head tag Cht, and let C be governed by a con-
stituent H, with head word Hhw and head tag Hht. Gildea
compares Collins model, where the emission of Chw is con-
ditioned on Hhw, and a ?mono-lexical? model, where the
emission of Chw is not conditioned on Hhw.
the fact that an NP in subject position is more
likely realized as a pronoun.
The first unlexicalized algorithms set up in this
trend (Johnson, 1998; Klein and Manning, 2003a)
also use language dependent and manually de-
fined heuristics to add the latent annotations. The
specialization induced by this additional annota-
tion is counterbalanced by simplifying assump-
tions, dubbed markovization (Klein and Manning,
2003a).
Using hand-defined heuristics remains prob-
lematic since we have no guarantee that the latent
annotations added in this way will allow to extract
an optimal language model.
A further development has been first introduced
by (Matsuzaki et al, 2005) who recasts the prob-
lem of adding latent annotations as an unsuper-
vised learning problem: given an observed PCFG
induced from the treebank, the latent grammar is
generated by combining every non terminal of the
observed grammar to a predefined set H of latent
symbols. The parameters of the latent grammar
are estimated from the observed trees using a spe-
cific instantiation of EM.
This first procedure however entails a combi-
natorial explosion in the size of the latent gram-
mar as |H| increases. (Petrov et al, 2006) (here-
after BKY) overcomes this problem by using the
following algorithm: given a PCFG G0 induced
from the treebank, iteratively create n grammars
G1 . . . Gn (with n = 5 in practice), where each
iterative step is as follows :
? SPLIT Create a new grammar Gi from Gi?1
by splitting every non terminal of Gi in
two new symbols. Estimate Gi?s parameters
on the observed treebank using a variant of
inside-outside. This step adds the latent an-
notation to the grammar.
? MERGE For each pair of symbols obtained
by a previous split, try to merge them back.
If the likelihood of the treebank does not
get significantly lower (fixed threshold) then
keep the symbol merged, otherwise keep the
split.
? SMOOTH This step consists in smoothing the
probabilities of the grammar rules sharing the
same left hand side.
This algorithm yields state-of-the-art results on
52
English3. Its key interest is that it directly aims
at finding an optimal language model without (1)
making additional assumptions on the annotation
scheme and (2) without relying on hand-defined
heuristics. This may be viewed as a case of semi-
supervised learning algorithm since the initial su-
pervised learning step is augmented with a second
step of unsupervised learning dedicated to assign
the latent symbols.
4 Experiments and Results
We investigate how some treebank features impact
learning. We describe first the experimental pro-
tocol, next we compare results of lexicalized and
unlexicalized parsers trained on various ?instan-
tiations? of the xml source files of the FTB, and
the impact of training set size for both algorithms.
Then we focus on studying how words impact the
results of the BKYalgorithm.
4.1 Protocol
Treebank setting For all experiments, the tree-
bank is divided into 3 sections : training (80%),
development (10%) and test (10%), made of
respectively 9881, 1235 and 1235 sentences.
We systematically report the results with the
compounds merged. Namely, we preprocess the
treebank in order to turn each compound into a
single token both for training and test.
Software and adaptation to French For the
Collins algorithm, we use Bikel?s implementation
(Bikel, 2004b) (hereafter BIKEL), and we report
results using Collins model 1 and model 2, with
internal tagging. Adapting model 1 to French
requires to design French specific head propaga-
tion rules. To this end, we adapted those de-
scribed by (Dybro-Johansen, 2004) for extracting
a Stochastic Tree Adjoining Grammar parser on
French. And to adapt model 2, we have further
designed French specific argument/adjunct identi-
fication rules.
For the BKY approach, we use the Berkeley
implementation, with an horizontal markovization
h=0, and 5 split/merge cycles. All the required
knowledge is contained in the treebank used for
training, except for the treatment of unknown or
rare words. It clusters unknown words using ty-
pographical and morphological information. We
3(Petrov et al, 2006) obtain an F-score=90.1 for sentences
of less than 40 words.
adapted these clues to French, following (Arun
and Keller, 2005).
Finally we use as a baseline a standard PCFG
algorithm, coupled with a trigram tagger (we refer
to this setup as TNT/LNCKY algorithm4).
Metrics For evaluation, we use the standard PAR-
SEVAL metric of labeled precision/recall, along
with unlabeled dependency evaluation, which is
known as a more annotation-neutral metric. Unla-
beled dependencies are computed using the (Lin,
1995) algorithm, and the Dybro-Johansen?s head
propagation rules cited above5. The unlabeled
dependency F-score gives the percentage of in-
put words (excluding punctuation) that receive the
correct head.
As usual for probabilistic parsing results, the re-
sults are given for sentences of the test set of less
than 40 words (which is true for 992 sentences of
the test set), and punctuation is ignored for F-score
computation with both metrics.
4.2 Comparison using minimal tagsets
We first derive from the FTB a minimally-
informed treebank, TREEBANKMIN, instantiated
from the xml source by using only the major syn-
tactic categories and no other feature. In each ex-
periment (Table 1) we observe that the BKY al-
gorithm significantly outperforms Collins models,
for both metrics.
parser BKY BIKEL BIKEL TNT/
metric M1 M2 LNCKY
PARSEVAL LP 85.25 78.86 80.68 68.74
PARSEVAL LR 84.46 78.84 80.58 67.93
PARSEVAL F1 84.85 78.85 80.63 68.33
Unlab. dep. Prec. 90.23 85.74 87.60 79.50
Unlab. dep. Rec. 89.95 85.72 86.90 79.37
Unlab. dep. F1 90.09 85.73 87.25 79.44
Table 1: Results for parsers trained on FTB with
minimal tagset
4The tagger is TNT (Brants, 2000), and the parser
is LNCKY, that is distributed by Mark Johnson
(http://www.cog.brown.edu/?mj/Software.htm).
Formally because of the tagger, this is not a strict PCFG
setup. Rather, it gives a practical trade-off, in which the
tagger includes the lexical smoothing for unknown and rare
words.
5For this evaluation, the gold constituent trees are con-
verted into pseudo-gold dependency trees (that may con-
tain errors). Then parsed constituent trees are converted
into parsed dependency trees, that are matched against the
pseudo-gold trees.
53
4.3 Impact of training data size
How do the unlexicalized and lexicalized ap-
proaches perform with respect to size? We com-
pare in figure 3 the parsing performance BKY and
COLLINSM1, on increasingly large subsets of the
FTB, in perfect tagging mode6 and using a more
detailed tagset (CC tagset, described in the next
experiment). The same 1235-sentences test set
is used for all subsets, and the development set?s
size varies along with the training set?s size. BKY
outperforms the lexicalized model even with small
amount of data (around 3000 training sentences).
Further, the parsing improvement that would re-
sult from more training data seems higher for BKY
than for Bikel.
2000 4000 6000 8000 10000
76
78
80
82
84
86
88
Number of training sentences
F?
sc
or
e
Bikel
Berkeley
Figure 3: Parsing Learning curve on FTB with CC-
tagset, in perfect-tagging
This potential increase for BKY results if we
had more French annotated data is somehow con-
firmed by the higher results reported for BKY
training on the Penn Treebank (Petrov et al, 2006)
: F1=90.2. We can show though that the 4 points
increase when training on English data is not only
due to size : we extracted from the Penn Treebank
a subset comparable to the FTB, with respect to
number of tokens and average length of sentences.
We obtain F1=88.61 with BKY training.
4.4 Symbol refinements
It is well-known that certain treebank transfor-
mations involving symbol refinements improve
6For BKY, we simulate perfect tagging by changing
words into word+tag in training, dev and test sets. We ob-
tain around 99.8 tagging accuracy, errors are due to unknown
words.
PCFGs (see for instance parent-transformation of
(Johnson, 1998), or various symbol refinements in
(Klein and Manning., 2003b)). Lexicalization it-
self can be seen as symbol refinements (with back-
off though). For BKY, though the key point is to
automatize symbol splits, it is interesting to study
whether manual splits still help.
We have thus experimented BKY training with
various tagsets. The FTB contains rich mor-
phological information, that can be used to split
preterminal symbols : main coarse category (there
are 13), subcategory (subcat feature refining the
main cat), and inflectional information (mph fea-
ture).
We report in Table 2 results for the four tagsets,
where terminals are made of : MIN: main cat,
SUBCAT: main cat + subcat feature, MAX: cat +
subcat + all inflectional information, CC: cat + ver-
bal mood + wh feature.
Tagset Nb of tags Parseval Unlab. dep Tagging
F1 F1 Acc
MIN 13 84.85 90.09 97.35
SUBCAT 34 85.74 ? 96.63
MAX 250 84.13 ? 92.20
CC 28 86.41 90.99 96.83
Table 2: Tagset impact on learning with BKY (own
tagging)
The corpus instantiation with CC tagset is our
best trade-off between tagset informativeness and
obtained parsing performance7 . It is also the best
result obtained for French probabilistic parsing.
This demonstrates though that the BKY learning
is not optimal since manual a priori symbol refine-
ments significantly impact the results.
We also tried to learn structures with functional
annotation attached to the labels : we obtain PAR-
SEVAL F1=78.73 with tags from the CC tagset +
grammatical function. This degradation, due to
data sparseness and/or non local constraints badly
captured by the model, currently constrains us to
use a language model without functional informa-
tions. As stressed in the introduction, this limits
the interpretability of the parses and it is a trade-
off between generalization and interpretability.
4.5 Lexicon and Inflection impact
French has a rich morphology that allows some
degree of word order variation, with respect to
7The differences are statistically significant : using a stan-
dard t-test, we obtain p-value=0.015 between MIN and SUB-
CAT, and p-value=0.002 between CC and SUBCAT.
54
English. For probabilistic parsing, this can have
contradictory effects : (i) on the one hand, this
induces more data sparseness : the occurrences
of a French regular verb are potentially split into
more than 60 forms, versus 5 for an English
verb; (ii) on the other hand, inflection encodes
agreements, that can serve as clues for syntactic
attachments.
Experiment In order to measure the impact
of inflection, we have tested to cluster word
forms on a morphological basis, namely to partly
cancel inflection. Using lemmas as word form
classes seems too coarse : it would not allow to
distinguish for instance between a finite verb and
a participle, though they exhibit different distri-
butional properties. Instead we use as word form
classes, the couple lemma + syntactic category.
For example for verbs, given the CC tagset, this
amounts to keeping 6 different forms (for the 6
moods).
To test this grouping, we derive a treebank where
words are replaced by the concatenation of lemma
+ category for training and testing the parser.
Since it entails a perfect tagging, it has to be
compared to results in perfect tagging mode :
more precisely, we simulate perfect tagging
by replacing word forms by the concatenation
form+tag.
Moreover, it is tempting to study the impact of
a more drastic clustering of word forms : that of
using the sole syntactic category to group word
forms (we replace each word by its tag). This
amounts to test a pure unlexicalized learning.
Discussion Results are shown in Figure 4.
We make three observations : First, comparing
the terminal=tag curves with the other two, it
appears that the parser does take advantage of
lexical information to rank parses, even for this
?unlexicalized? algorithm. Yet the relatively small
increase clearly shows that lexical information
remains underused, probably because of lexical
data sparseness.
Further, comparing terminal=lemma+tag and ter-
minal=form+tag curves, we observe that grouping
words into lemmas helps reducing this sparseness.
And third, the lexicon impact evolution (i.e.
the increment between terminal=tag and termi-
nal=form+tag curves) is stable, once the training
size is superior to approx. 3000 sentences8 .
This suggests that only very frequent words
matter, otherwise words? impact should be more
and more important as training material augments.
0 2000 4000 6000 8000 10000
76
78
80
82
84
86
88
Number of training sentences
Pa
rs
ev
al
 F
?s
co
re
Bky terminal=form+tag
Bky terminal=lemma+tag
Bky terminal=tag
Figure 4: Impact of clustering word forms (train-
ing on FTB with CC-tagset, in perfect-tagging)
5 Related Work
Previous works on French probabilistic parsing are
those of (Arun and Keller, 2005), (Schluter and
van Genabith, 2007), (Schluter and van Genabith,
2008). One major difficulty for comparison is that
all three works use a different version of the train-
ing corpus. Arun reports results on probabilistic
parsing, using an older version of the FTB and us-
ing lexicalized models (Collins M1 and M2 mod-
els, and the bigram model). It is difficult to com-
pare our results with Arun?s work, since the tree-
bank he has used is obsolete (FTB-V0). He obtains
for Model 1 : LR=80.35 / LP=79.99, and for the
bigram model : LR=81.15 / LP=80.84, with min-
imal tagset and internal tagging. The results with
FTB (revised subset of FTB-V0) with minimal
8 This is true for all points in the curves, except for
the last step, i.e. when full training set is used. We per-
formed a 10-fold cross validation to limit sample effects. For
the BKYtraining with CC tagset, and own tagging, we ob-
tain an average F-score of 85.44 (with a rather high stan-
dard deviation ?=1.14). For the clustering word forms ex-
periment, using the full training set, we obtain : 86.64 for
terminal=form+tag (?=1.15), 87.33 for terminal=lemma+tag
(?=0.43), and 85.72 for terminal=tag (?=0.43). Hence our
conclusions (words help even with unlexicalized algorithm,
and further grouping words into lemmas helps) hold indepen-
dently of sampling.
55
tagset (Table 1) are comparable for COLLINSM1,
and nearly 5 points higher for BKY.
It is also interesting to review (Arun and Keller,
2005) conclusion, built on a comparison with the
German situation : at that time lexicalization was
thought (Dubey and Keller, 2003) to have no siz-
able improvement on German parsing, trained on
the Negra treebank, that uses a flat structures. So
(Arun and Keller, 2005) conclude that since lex-
icalization helps much more for parsing French,
with a flat annotation, then word-order flexibility
is the key-factor that makes lexicalization useful
(if word order is fixed, cf. French and English)
and useless (if word order is flexible, cf. German).
This conclusion does not hold today. First, it can
be noted that as far as word order flexibility is con-
cerned, French stands in between English and Ger-
man. Second, it has been proven that lexicalization
helps German probabilistic parsing (K?bler et al,
2006). Finally, these authors show that markoviza-
tion of the unlexicalized Stanford parser gives al-
most the same increase in performance than lex-
icalization, both for the Negra treebank and the
T?ba-D/Z treebank. This conclusion is reinforced
by the results we have obtained : the unlexicalized,
markovized, PCFG-LA algorithm outperforms the
Collins? lexicalized model.
(Schluter and van Genabith, 2007) aim at learn-
ing LFG structures for French. To do so, and in
order to learn first a Collins parser, N. Schluter
created a modified treebank, the MFT, in order (i)
to fit her underlying theoretical requirements, (ii)
to increase the treebank coherence by error min-
ing and (iii) to improve the performance of the
learnt parser. The MFT contains 4739 sentences
taken from the FTB, with semi-automatic trans-
formations. These include increased rule stratifi-
cation, symbol refinements (for information prop-
agation), coordination raising with some manual
re-annotation, and the addition of functional tags.
MFT has also undergone a phase of error min-
ing, using the (Dickinson and Meurers, 2005) soft-
ware, and following manual correction. She re-
ports a 79.95% F-score on a 400 sentence test
set, which compares almost equally with Arun?s
results on the original 20000 sentence treebank.
So she attributes her results to the increased co-
herence of her smaller treebank. Indeed, we ran
the BKY training on the MFT, and we get F-
score=84.31. While this is less in absolute than
the BKY results obtained with FTB (cf. results in
table 2), it is indeed very high if training data size
is taken into account (cf. the BKY learning curve
in figure 3). This good result raises the open ques-
tion of identifying which modifications in the MFT
(error mining and correction, tree transformation,
symbol refinements) have the major impact.
6 Conclusion
This paper reports results in statistical parsing
for French with both unlexicalized (Petrov et al,
2006) and lexicalized parsers. To our knowledge,
both results are state of the art on French for each
paradigm.
Both algorithms try to overcome PCFG?s sim-
plifying assumptions by some specialization of the
grammatical labels. For the lexicalized approach,
the annotation of symbols with lexical head is
known to be rarely fully used in practice (Gildea,
2001), what is really used being the category of
the lexical head.
We observe that the second approach (BKY)
constantly outperforms the lexicalist strategy ? la
(Collins, 1999). We observe however that (Petrov
et al, 2006)?s semi-supervised learning procedure
is not fully optimal since a manual refinement of
the treebank labelling turns out to improve the
parsing results.
Finally we observe that the semi-supervised
BKY algorithm does take advantage of lexical in-
formation : removing words degrades results. The
preterminal symbol splits percolates lexical dis-
tinctions. Further, grouping words into lemmas
helps for a morphologically rich language such as
French. So, an intermediate clustering standing
between syntactic category and lemma is thought
to yield better results in the future.
7 Acknowledgments
We thank N. Schluter and J. van Genabith for
kindly letting us run BKY on the MFT, and A.
Arun for answering our questions. We also thank
the reviewers for valuable comments and refer-
ences. The work of the second author was partly
funded by the ?Prix Diderot Innovation 2007?,
from University Paris 7.
56
References
Anne Abeill?, Lionel Cl?ment, and Alexandra Kinyon.
2000. Building a treebank for french. In Proceed-
ings of the 2nd International Conference Language
Resources and Evaluation (LREC?00).
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a treebank for French. Kluwer, Dor-
drecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
french. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 306?313, Ann Arbor, MI.
Daniel M. Bikel. 2004a. A distributional analysis
of a lexicalized statistical parsing model. In Proc.
of Empirical Methods in Natural Language Pro-
cessing (EMNLP 2004), volume 4, pages 182?189,
Barcelona, Spain.
Daniel M. Bikel. 2004b. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
NLP Conference (ANLP), Seattle-WA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the Annual Meet-
ing of the North American Association for Com-
putational Linguistics (NAACL-00), pages 132?139,
Seattle, Washington.
Michael Collins. 1999. Head driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
Actes de la 15?me Conf?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN?08), pages
45?54, Avignon.
Markus Dickinson and W. Detmar Meurers. 2005.
Prune diseased branches to get healthy trees! how
to find erroneous local trees in treebank and why
it matters. In Proceedings of the 4th Workshop
on Treebanks and Linguistic Theories (TLT 2005),
Barcelona, Spain.
Amit Dubey and Frank Keller. 2003. Probabilis-
tic parsing for german using sister-head dependen-
cies. In In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 96?103.
Ane Dybro-Johansen. 2004. Extraction automatique
de grammaires ? partir d?un corpus fran?ais. Mas-
ter?s thesis, Universit? Paris 7.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the First Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 167?202.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Dan Klein and Christopher D. Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang
Maier. 2006. Is it really that difficult to parse ger-
man? In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 111?119, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
75?82.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Natalie Schluter and Josef van Genabith. 2007.
Preparing, restructuring, and augmenting a french
treebank: Lexicalised parsers or coherent treebanks?
In Proceedings of PACLING 07.
Natalie Schluter and Josef van Genabith. 2008.
Treebank-based acquisition of lfg parsing resources
for french. In European Language Resources As-
sociation (ELRA), editor, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
57
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 138?141,
Paris, October 2009. c?2009 Association for Computational Linguistics
Improving generative statistical parsing with semi-supervised word
clustering
Marie Candito and Beno?t Crabb?
Universit? Paris 7/INRIA (Alpage), 30 rue du Ch?teau des Rentiers, 75013 Paris
Abstract
We present a semi-supervised method to
improve statistical parsing performance.
We focus on the well-known problem of
lexical data sparseness and present exper-
iments of word clustering prior to pars-
ing. We use a combination of lexicon-
aided morphological clustering that pre-
serves tagging ambiguity, and unsuper-
vised word clustering, trained on a large
unannotated corpus. We apply these clus-
terings to the French Treebank, and we
train a parser with the PCFG-LA unlex-
icalized algorithm of (Petrov et al, 2006).
We find a gain in French parsing perfor-
mance: from a baseline of F1=86.76% to
F1=87.37% using morphological cluster-
ing, and up to F1=88.29% using further
unsupervised clustering. This is the best
known score for French probabilistic pars-
ing. These preliminary results are encour-
aging for statistically parsing morpholog-
ically rich languages, and languages with
small amount of annotated data.
1 Introduction
Lexical information is known crucial in natural
language parsing. For probabilistic parsing, one
main drawback of the plain PCFG approach is to
lack sensitivity to the lexicon. The symbols acces-
sible to context-free rules are part-of-speech tags,
which encode generalizations that are too coarse
for many parsing decisions (for instance subcat-
egorization information is generally absent from
tagsets). The lexicalized models first proposed
by Collins reintroduced words at every depth of a
parse tree, insuring that attachments receive prob-
abilities that take lexical information into account.
On the other hand, (Matsuzaki et al, 2005) have
proposed probabilistic CFG learning with latent
annotation (hereafter PCFG-LA), as a way to au-
tomate symbol splitting in unlexicalized proba-
bilistic parsing (cf. adding latent annotations to
a symbol is comparable to splitting this symbol).
(Petrov et al, 2006) rendered the method usable in
practice, with a tractable technique to retain only
the beneficial splits.
We know that both lexicalized parsing algo-
rithm and PCFG-LA algorithm suffer from lex-
ical data sparseness. For lexicalized parsers,
(Gildea, 2001) shows that bilexical dependencies
parameters are almost useless in the probabilistic
scoring of parser because they are too scarce.
For PCFG-LA, we have previously studied the
lexicon impact on this so-called ?unlexicalized?
algorithm, for French parsing (Crabb? and Can-
dito, 2008), (Candito et al, 2009). We have tested
a totally unlexicalized parser, trained on a treebank
where words are replaced by their POS tags. It ob-
tains a parseval F1=86.28 (note that it induces per-
fect tagging). We compared it to a parser trained
with word+tag as terminal symbols (to simulate a
perfect tagging), achieving F1=87.79. This proves
that lexical information is indeed used by the ?un-
lexicalized? PCFG-LA algorithm: some lexical
information percolates through parse trees via the
latent annotations.
We have also reported a slight improvement
(F1=88.18) when word forms are clustered on a
morphological basis, into lemma+tag clusters. So
PCFG-LA uses lexical information, but it is too
sparse, hence it benefits from word clustering. Yet
the use of lemma+tag terminals supposes tagging
prior to parsing. We propose here to apply rather
a deterministic supervised morphological cluster-
ing that preserves tagging ambiguities, leaving it
to the parser to disambiguate POS tags.
We also investigate the use of unsupervised
word clustering, obtained from unannotated text.
It has been proved useful for parsing by (Koo et
al., 2008) and their work directly inspired ours.
They have shown that parsing improves when
cluster information is used as features in a discrim-
inative training method that learns dependency
parsers. We investigate in this paper the use of
such clusters in a generative approach to proba-
bilistic phrase-structure parsing, simply by replac-
ing each token by its cluster.
138
We present in section 2 the treebank instanti-
ation we use for our experiments, the morpho-
logical clustering in section 3, and the Brown al-
gorithm for unsupervised clustering in section 4.
Section 5 presents our experiments, results and
discussion. Section 6 discusses related work. Sec-
tion 7 concludes with some ideas for future work.
2 French Treebank
For our experiments, we use the French Treebank
(hereafter FTB) (Abeill? et al, 2003), containing
12531 sentences of the newspaper Le Monde. We
started with the treebank instantiation defined in
(Crabb? and Candito, 2008), where the rich origi-
nal annotation containing morphological and func-
tional information is mapped to a plain phrase-
structure treebank with a tagset of 28 POS tags.
In the original treebank, 17% of the tokens be-
long to a compound, and compounds range from
very frozen multi word expressions like y com-
pris (literally there included, meaning including)
to syntactically regular entities like loi agraire
(land law). In most of the experiments with the
FTB, each compound is merged into a single to-
ken: (P (CL y) (A compris)) is merged as (P
y_compris). But because our experiments aim at
reducing lexical sparseness but also at augmenting
lexical coverage using an unannotated corpus, we
found it necessary to make the unannotated cor-
pus tokenisation and the FTB tokenisation consis-
tent. To set up a robust parser, we chose to avoid
recognizing compounds that exhibit syntactically
regular patterns. We create a new instance of the
treebank (hereafter FTB-UC), where syntactically
regular patterns are ?undone? (Figure 1). This re-
duces the number of distinct compounds in the
whole treebank from 6125 to 3053.
NP
D
l?
N
N
Union
A
?conomique
C
et
A
mon?taire
NP
D
l?
N
Union
AP
A
?conomique
COORD
C
et
AP
A
mon?taire
Figure 1: A NP with a compound (left) changed
into a regular structure with simple words (right)
3 Morphological clustering
The aim of this step is to reduce lexical sparseness
caused by inflection, without hurting parsability,
and without committing ourselves as far as ambi-
guity is concerned. Hence, a morphological clus-
tering using lemmas is not possible, since lemma
assignment supposes POS disambiguation. Fur-
ther, information such as mood on verbs is nec-
essary to capture for instance that infinitive verbs
have no overt subject, that participial clauses are
sentence modifiers, etc... This is encoded in the
FTB with different projections for finite verbs
(projecting sentences) versus non finite verbs (pro-
jecting VPpart or VPinf).
We had the intuition that the other inflection
marks in French (gender and number for determin-
ers, adjectives, pronouns and nouns, tense and per-
son for verbs) are not crucial to infer the correct
phrase-structure projected by a given word1.
So to achieve morphological clustering, we de-
signed a process of desinflection, namely of re-
moving some inflection marks. It makes use of
the Lefff, a freely available rich morphological and
syntactic French lexicon (Sagot et al, 2006), con-
taining around 116000 lemmas (simple and com-
pounds) and 535000 inflected forms. The desin-
flection is as follows: for a token t to desin-
flect, if it is known in the lexicon, for all the in-
flected lexical entries le of t, try to get corre-
sponding singular entries. If for all the le, cor-
responding singular entries exist and all have the
same form, then replace t by the corresponding
singular. For instance for wt=entr?es (ambigu-
ous between entrances and entered, fem, plural),
the two lexical entries are [entr?es/N/fem/plu] and
[entr?es/V/fem/plu/part/past]2 , each have a corre-
sponding singular lexical entry, with form entr?e.
Then the same process applies to map feminine
forms to corresponding masculine forms. This
allows to change mang?e (eaten, fem, sing) into
mang? (eaten, masc, sing). But for the form en-
tr?e, ambiguous between N and Vpastpart entries,
only the participle has a corresponding masculine
entry (with form entr?). In that case, in order
to preserve the original ambiguity, entr?e is not
replaced by entr?. Finite verb forms, when un-
ambiguous with other POS, are mapped to sec-
ond person plural present indicative corresponding
forms. This choice was made in order to avoid cre-
ating ambiguity: the second person plural forms
end with a very typical -ez suffix, and the result-
ing form is very unlikely ambiguous. For the first
1For instance, French oral comprehension does not seem
to need plural marks very much, since a majority of French
singular forms have their corresponding plural form pro-
nounced in the same way.
2This is just an example and not the real Lefff format.
139
token of a sentence, if unknown in the lexicon,
the algorithm tries to desinflect the low case cor-
responding form.
This desinflection reduces the number of dis-
tinct tokens in the FTB-UC from 27143 to 20268.
4 Unsupervised word clustering
We chose to use the (Brown et al, 1992) hard clus-
tering algorithm, which has proven useful for var-
ious NLP tasks, such as dependency parsing (Koo
et al, 2008) or named entity recognition (Liang,
2005). The algorithm to obtain C clusters is as
follows: each of the C most frequent tokens of
the corpus is assigned its own distinct cluster. For
the (C+1)th most frequent token, create a (C+1)th
cluster. Then for each pair among the C+1 result-
ing clusters, merge the pair that minimizes the loss
in the likelihood of the corpus, according to a bi-
gram language model defined on the clusters. Re-
peat this operation for the (C+2)th most frequent
token, etc... This results in a hard clustering into
C clusters. The process can be continued to fur-
ther merge pairs of clusters among the C clusters,
ending with a unique cluster for the whole vocab-
ulary. This can be traced to obtain a binary tree
representing the merges of the C clusters. A clus-
ter can be identified by its path within this binary
tree. Hence, clusters can be used at various levels
of granularity.
5 Experiments and discussion
For the Brown clustering algorithm, we used Percy
Liang?s code3, run on the L?Est R?publicain cor-
pus, a 125 million word journalistic corpus, freely
available at CNRTL4. The corpus was tokenised5 ,
segmented into sentences and desinflected using
the process described in section 3. We ran the clus-
tering into 1000 clusters for the desinflected forms
appearing at least 20 times.
We tested the use of word clusters for parsing
with the Berkeley algorithm (Petrov et al, 2006).
Clustering words in this case has a double advan-
tage. First, it augments the known vocabulary,
which is made of all the forms of all the clus-
ters appearing in the treebank. Second, it reduces
sparseness for the latent annotations learning on
the lexical rules of the PCFG-LA grammar.
3http://www.eecs.berkeley.edu/ pliang/software
4http://www.cnrtl.fr/corpus/estrepublicain
5The 200 most frequent compounds of the FTB-UC were
systematically recognized as one token.
We used Petrov?s code, adapted to French by
(Crabb? and Candito, 2008), for the suffixes used
to classify unknown words, and we used the same
training(80%)/dev(10%)/test(10%) partition. We
used the FTB-UC treebank to train a baseline
parser, and three other parsers by changing the ter-
minal symbols used in training data:
desinflected forms: as described in section 3
clusters + cap: each desinflected form is re-
placed by its cluster bit string. If the desinflected
form has no corresponding cluster (it did not ap-
pear 20 times in the unannotated corpus), a spe-
cial cluster UNKC is used. Further, a _C suffix is
added if the form starts with a capital.
clusters + cap + suffixes: same as before, ex-
cept that 9 additional features are used as suffixes
to the cluster: if form is all digits, ends with ant,
or r, or ez (cf. this is how end desinflected forms
of unambiguous finite verbs), ...
We give in table 1 parsing performance in terms
of labeled precision/recall/Fscore, and also the
more neutral unlabeled attachment score (UAS)6.
The desinflection process does help: benefits
from reducing data sparseness exceed the loss
of agreement markers. Yet tagging decreases a
little, and this directly impacts the dependency
score, because the dependency extraction uses
head propagation rules that are sensitive to tag-
ging. In the same way, the use of bare clusters
increases labeled recall/precision, but the tagging
accuracy decreases, and thus the UAS. This can
be due to the coarseness of the clustering method,
which sometimes groups words that have differ-
ent POS (for instance among a cluster of infinite
verbs, one may find a present participle). The
quality of the clusters is more crucial in our case
than when clusters are features, whose informativ-
ity is discriminatively learnt. This observation led
us to append a restricted set of suffixes to the clus-
ters, which gives us the best results for now.
6 Related work
We already mentioned that we were inspired by
the success of (Koo et al, 2008) in using word
clusters as features for the discriminative learning
of dependency parsers. Another approach to aug-
ment the known vocabulary for a generative prob-
6In all metrics punctuation tokens are ignored and all re-
sults are for sentences of less than 40 words. Note that we
used the FTB-UC treebank. There are mors tokens in sen-
tences than in the FTB with all compounds merged, and base-
line F1 scores are a little higher (86.79 versus 86.41).
140
terminal symbols LP LR F1 UAS Vocab. size Tagging Acc.
inflected forms (baseline) 86.94 86.65 86.79 91.00 27143 96.90
desinflected forms 87.42 87.32 87.37 91.14 20268 96.81
clusters + cap 88.08 87.50 87.79 91.12 1201 96.37
clusters + cap + suffixes 88.43 88.14 88.29 91.68 1987 97.04
Table 1: Parsing performance when training and parsing use clustered terminal symbols
abilistic parser is the one pursued in (Goldberg et
al., 2009). Within a plain PCFG, the lexical proba-
bilities for words that are rare or absent in the tree-
bank are taken from an external lexical probabil-
ity distribution, estimated using a lexicon and the
Baulm-Welch training of an HMM tagger. This is
proved useful to better parse Hebrew.
7 Conclusion and future work
We have tested the very simple method of replac-
ing inflected forms by clusters of forms in a gener-
ative probabilistic parser. This crude technique has
surprisingly good results and offers a very cheap
and simple way to augment the vocabulary seen at
training time. It seems interesting to try the tech-
nique on other generative approaches such as lex-
icalized probabilistic parsing.
We plan to optimize the exact shape of termi-
nal symbols to use. Bare unsupervised clusters are
unsatisfactory, and we have seen that adding sim-
ple suffixes to the clusters improved performance.
Learning such suffixes is a path to explore. Also,
the hierarchical organization of the clusters could
be used, in the generative approach adopted here,
by modulating the granularity of the clusters de-
pending on their frequency in the treebank.
We also need to check to what extent the desin-
flection step helps for taking advantage of the very
local information captured by the Brown cluster-
ing.Finally, we could try using other kinds of clus-
tering, such as the approach of (Lin, 1998), which
captures similarity between syntactic dependen-
cies beared by nouns and verbs.
8 Acknowledgements
The authors truly thank Percy Liang and Slav
Petrov for providing their code for respec-
tively Brown clustering and PCFG-LA. This
work was supported by the French National
Research Agency (SEQUOIA project ANR-08-
EMER-013).
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer,
Dordrecht.
Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Compu-
tational linguistics, 18(4):467?479.
Marie Candito, Benoit Crabb?, and Djam? Seddah.
2009. On statistical parsing of french with super-
vised and semi-supervised strategies. In EACL 2009
Workshop Grammatical inference for Computa-
tional Linguistics, Athens, Greece.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
Actes de la 15?me Conf?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN?08), pages
45?54, Avignon, France.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc. of EMNLP?01, pages 167?202,
Pittsburgh, USA.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and
Michael Elhadad. 2009. Enhancing unlexicalized
parsing performance using a wide coverage lexicon,
fuzzy tag-set mapping, and EM-HMM-based lexical
probabilities. In Proc. of EACL-09, pages 327?335,
Athens, Greece.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proc. of ACL-08, Columbus, USA.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. In MIT Master?s thesis, Cambridge,
USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL-98, pages 768?
774, Montreal, Canada.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proc. of ACL-05, pages 75?82, Ann Arbor, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL-06, Syd-
ney, Australia.
Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for french: architecture, acquisi-
tion, use. In Proc. of LREC?06, Genova, Italy.
141
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 150?161,
Paris, October 2009. c?2009 Association for Computational Linguistics
Cross Parser Evaluation and Tagset Variation : a French Treebank Study
Djam? Seddah?, Marie Candito? and Beno?t Crabb??
? Universit? Paris-Sorbonne
LALIC & INRIA (ALPAGE)
28 rue Serpente
F-75006 Paris ? France
? Universit? Paris 7
INRIA (ALPAGE)
30 rue du Ch?teau des Rentiers
F-75013 Paris ? France
Abstract
This paper presents preliminary investiga-
tions on the statistical parsing of French by
bringing a complete evaluation on French
data of the main probabilistic lexicalized
and unlexicalized parsers first designed
on the Penn Treebank. We adapted the
parsers on the two existing treebanks of
French (Abeill? et al, 2003; Schluter and
van Genabith, 2007). To our knowledge,
mostly all of the results reported here are
state-of-the-art for the constituent parsing
of French on every available treebank. Re-
garding the algorithms, the comparisons
show that lexicalized parsing models are
outperformed by the unlexicalized Berke-
ley parser. Regarding the treebanks, we
observe that, depending on the parsing
model, a tag set with specific features
has direct influence over evaluation re-
sults. We show that the adapted lexical-
ized parsers do not share the same sensi-
tivity towards the amount of lexical ma-
terial used for training, thus questioning
the relevance of using only one lexicalized
model to study the usefulness of lexical-
ization for the parsing of French.
1 Introduction
The development of large scale symbolic gram-
mars has long been a lively topic in the French
NLP community. Surprisingly, the acquisition of
probabilistic grammars aiming at stochastic pars-
ing, using either supervised or unsupervised meth-
ods, has not attracted much attention despite the
availability of large manually syntactic annotated
data for French. Nevertheless, the availability
of the Paris 7 French Treebank (Abeill? et al,
2003), allowed (Dybro-Johansen, 2004) to carry
out the extraction of a Tree Adjoining Grammar
(Joshi, 1987) and led (Arun and Keller, 2005)
to induce the first effective lexicalized parser for
French. Yet, as noted by (Schluter and van Gen-
abith, 2007), the use of the treebank was ?chal-
lenging?. Indeed, before carrying out successfully
any experiment, the authors had to perform a deep
restructuring of the data to remove errors and in-
consistencies. For the purpose of building a sta-
tistical LFG parser, (Schluter and van Genabith,
2007; Schluter and van Genabith, 2008) have re-
annotated a significant subset of the treebank with
two underlying goals: (1) designing an annota-
tion scheme that matches as closely as possible
the LFG theory (Kaplan and Bresnan, 1982) and
(2) ensuring a more consistent annotation. On the
other hand, (Crabb? and Candito, 2008) showed
that with a new released and corrected version of
the treebank1 it was possible to train statistical
parsers from the original set of trees. This path
has the advantage of an easier reproducibility and
eases verification of reported results.
With the problem of the usability of the data
source being solved, the question of finding one
or many accurate language models for parsing
French raises. Thus, to answer this question,
this paper reports a set of experiments where
five algorithms, first designed for the purpose of
parsing English, have been adapted to French:
a PCFG parser with latent annotation (Petrov et
al., 2006), a Stochastic Tree Adjoining Grammar
parser (Chiang, 2003), the Charniak?s lexicalized
parser (Charniak, 2000) and the Bikel?s implemen-
tation of Collins? Model 1 and 2 (Collins, 1999)
described in (Bikel, 2002). To ease further com-
parisons, we report results on two versions of the
treebank: (1) the last version made available in
December 2007, hereafter FTB , and described
in (Abeill? and Barrier, 2004) and the (2) LFG
inspired version of (Schluter and van Genabith,
2007).
The paper is structured as follows : After a brief
presentation of the treebanks, we discuss the use-
1This has been made available in December 2007.
150
fulness of testing different parsing frameworks
over two parsing paradigms before introducing
our experimental protocol and presenting our re-
sults. Finally, we discuss and compare with re-
lated works on cross-language parser adaptation,
then we conclude.
2 Treebanks for French
This section provides a brief overview to the cor-
pora on which we report results: the French Tree-
bank (FTB) and the Modified French Treebank
(MFT).
2.1 The French Treebank
THE FRENCH TREEBANK is the first treebank
annotated and manually corrected for French. It
is the result of a supervised annotation project of
newspaper articles from Le Monde (Abeill? and
Barrier, 2004). The corpus is annotated with la-
belled constituent trees augmented with morpho-
logical annotations and functional annotations of
verbal dependents as shown below :
<SENT>
<NP fct="SUJ">
<w cat="D" lemma="le" mph="ms" subcat="def">le</w>
<w cat="N" lemma="bilan" mph="ms" subcat="C">bilan</w>
</NP>
<VN>
<w cat="ADV" lemma="ne" subcat="neg">n?</w>
<w cat="V" lemma="?tre" mph="P3s" subcat="">est</w>
</VN>
<AdP fct="MOD">
<w compound="yes" cat="ADV" lemma="peut-?tre">
<w catint="V">peut</w>
<w catint="PONCT">-</w>
<w catint="V">?tre</w>
</w>
<w cat="ADV" lemma="pas" subcat="neg">pas</w>
</AdP>
<AP fct="ATS">
<w cat="ADV" lemma="aussi">aussi</w>
<w cat="A" lemma="sombre" mph="ms" subcat="qual">sombre</w>
</AP>
<w cat="PONCT" lemma="." subcat="S">.</w>
</SENT>
Figure 1: Simplified example of the FTB: ?Le bi-
lan n?est peut-?tre pas aussi sombre.? (i.e. The
result is perhaps not as bleak)
Though the original release (in 2000) consists
of 20,648 sentences, the subset of 12351 function-
ally annotated sentences is known to be more con-
sistently annotated and therefore is the one used
in this work. Its key properties, compared with
the Penn Treebank (hereafter PTB, (Marcus et al,
1994)), are the following :
Size: The FTB consists of 385,458 tokens and
12,351 sentences, that is the third of the PTB. It
also entails that the average length of a sentence
is 27.48 tokens. By contrast the average sentence
length in the PTB is 24 tokens.
Inflection: French morphology is richer than
English and leads to increased data sparseness is-
sues for the purpose of statistical parsing. There
are 24,098 types in the FTB, entailing an average
of 16 tokens occurring for each type.
A Flat Annotation Scheme: Both the FTB
and the PTB are annotated with constituent trees.
However, the annotation scheme is flatter in the
FTB. For instance, there are no VPs for finite verbs
and only one sentential level for clauses or sen-
tences whether or not introduced by a complemen-
tizer. Only verbal nucleus (VN) is annotated and
comprises the verb, its clitics, auxiliaries, adverbs
and surrounding negation.
While X-bar inspired constituents are supposed
to contain all the syntactic information, in the FTB
the shape of the constituents does not necessar-
ily express unambiguously the type of dependency
existing between a head and a dependent appear-
ing in the same constituent. Yet, this is crucial to
extract the underlying predicate-argument struc-
tures. This has led to a ?flat? annotation scheme,
completed with functional annotations that inform
on the type of dependency existing between a verb
and its dependents. This was chosen for French
to reflect, for instance, the possibility to mix post-
verbal modifiers and complements (Figure 2), or
to mix post-verbal subject and post-verbal indirect
complements : a post verbal NP in the FTB can
correspond to a temporal modifier, (most often) a
direct object, or an inverted subject, and all cases,
other subcategorized complements may appear.
SENT
NP-SUJ
D
une
N
lettre
VN
V
avait
V
?t?
V
envoy?e
NP-MOD
D
la
N
semaine
A
derni?re
PP-AOBJ
P
aux
NP
N
salari?s
(a) A letter had been sent last week to the employees
SENT
NP-SUJ
D
Le
N
Conseil
VN
V
a
V
notifi?
NP-OBJ
D
sa
N
d?cision
PP-AOBJ
P
?
NP
D
la
N
banque
(b) The Council has notified his decision to the bank
Figure 2: Two examples of post-verbal NPs : a
temporal modifier (a) and a direct object (b)
Compounds: Compounds are explicitly anno-
tated and very frequent in the treebank: 14.52% of
tokens are part of a compound (see the compound
peut-?tre ?perhaps? in Figure 1 ). They include
151
digit numbers (written with spaces in French) (e.g.
10 000), frozen compounds (eg. pomme de terre
?potato?) but also named entities or sequences
whose meaning is compositional but where inser-
tion is rare or difficult (e.g. garde d?enfant ?child
care?). As noted by (Arun and Keller, 2005), com-
pounds in French may exhibit ungrammatical se-
quences of tags as in ? la va vite ?in a hurry?
: Prep+ Det+ finite verb + adverb or can in-
clude ?words? which do not exist outside a com-
pound (e.g hui in aujourd?hui ?today?). Therefore,
compounds receive a two-level annotation : con-
stituent parts are described in a subordinate level
using the same POS tagset as the genuine com-
pound POS. This makes it more difficult to extract
a proper grammar from the FTB without merged
compounds2. This is why, following (Arun and
Keller, 2005) and (Schluter and van Genabith,
2007), all the treebanks used in this work contain
compounds.
2.2 The Modified French Treebank
THE MODIFIED FRENCH TREEBANK (MFT) has
been derived from the FTB by (Schluter and van
Genabith, 2008) as a basis for a PCFG-based Lexi-
cal Functional Grammar induction process (Cahill
et al, 2004) for French. The corpus is a subset of
4739 sentences extracted from the original FTB.
The MFT further introduces formal differences of
two kinds with respect to the original FTB: struc-
tural and labeling modifications.
Regarding structural changes, the main transfor-
mations include increased rule stratification (Fig.
3), coordination raising (Fig. 5).
Moreover, the MFT?s authors introduced new
treatments of linguistic phenomena that were not
covered by their initial source treebank. Those
include, for example, analysis for ?It?-cleft con-
structions.3 Since the MFT was designed for the
purpose of improving the task of grammar induc-
tion, the MFT?s authors also refined its tag set by
propagating information (such as mood features
added to VN node labels), and added functional
paths4 to the original function labels. The modifi-
cations introduced in the MFT meet better the for-
mal requirements of the LFG architecture set up
2Consider the case of the compound peut-?tre ?perhaps?
whose POS is ADV, its internal structure (Fig. 1) would lead
to a CFG rule of the form ADV ?? V V.
3See pages 2-3 of (Schluter and van Genabith, 2007) for
details.
4Inspired by the LFG framework (Dalrymple, 2001).
AdP
ADV
encore
ADV
pas
ADV
tr?s
ADV
bien
AdP
ADV
encore
AdP
ADV
pas
AdP ADV
tr?s
AdP ADV
bien
FTB initial analysis MFT modification
Figure 3: Increased stratification in the MFT : ?en-
core pas tr?s bien? (?still not very well?)
XP1
..Y.. X1 ..Z.. COORD
C XP2
XP1
COORD-XP
XP
..Y.. X1 ..Z..
C XP2
Figure 5: Coordinated structures in the general
case, for FTB (up) and MFT (down)
by (Cahill et al, 2004) and reduce the size of the
grammars extracted from the treebank. MFT has
also undergone a phase of error mining and an ex-
tensive manual correction.
2.3 Coordination in French Treebanks
One of the key differences between the two French
treebanks is the way they treat coordinate struc-
tures. Whereas the FTB represents them with an
adjunction of a COORD phrase as a sister or a
daughter of the coordinated element, the MFT in-
troduces a treatment closer to the one used in the
PTB to describe such structures. As opposed to
(Arun and Keller, 2005) who decided to transform
the FTB?s coordinations to match the PTB?s analy-
sis, the COORD label is not removed but extended
to include the coordinated label (Fig. 5).
In Figure 5, we show the general coordination
structure in the FTB, and the corresponding mod-
ified structure in the MFT. A more complicated
modification concerns the case of VP coordina-
tions. (Abeill? et al, 2003) argue for a flat repre-
sentation with no VP-node for French, and this is
152
SENT
VN
CL
Elle
V
ajoute
Ssub
que ...
COORD
CC
et
VN
V
pr?sente
NP
douze points de d?saccord
SENT
NP
CL
Elle
COORD-VP
VP
VN-finite
V-finite
ajoute
Ssub
que ...
C-C
et
VP
VN-finite
V-finite
pr?sente
NP
douze points de d?saccord
Figure 4: Two representations of ?VP coordinations? for the sentence She adds that ... and presents
twelve sticking points: in the FTB (left) and in the MFT (right)
particularly justified in some cases of subject-verb
inversion. Nevertheless, VP phrases are used in
the FTB for non-finite VPs only (nodes VPinf and
VPpart). In the MFT, finite VPs were introduced
to handle VP coordinations. In those cases, the
FTB annotation scheme keeps a flat structure (Fig-
ure 4, left), where the COORD phrase has to be in-
terpreted as a coordinate of the VN node; whereas
finite VP nodes are inserted in the MFT (Figure 4,
right).
2.4 Summary
In Table 2, we describe the annotation schemes of
the treebanks and we provide in Table 1 a numeric
summary of some relevant different features be-
tween these two treebanks. The reported numbers
take into account the base syntactic category labels
without functions, part-of-speech tags without any
morpho-syntactic information (ie. no ?gender? or
number?).
properties FTB MFT
# of sentences 12351 4739
Average sent. length 27.48 28.38
Average node branching 2.60 2.11
PCFG size (without term. prod.) 14874 6944
# of NT symbols 13 39
# of POS tags 15 27
Table 1: Treebanks Properties
3 Parsing Algorithms
Although Probabilistic Context Free Grammars
(PCFG) are a baseline formalism for probabilis-
tic parsing, it is well known that they suffer from
two problems: (a) The independence assumptions
made by the model are too strong, and (b) For Nat-
ural Language Parsing, they do not take into ac-
count lexical probabilities. To date, most of the
results on statistical parsing have been reported
for English. Here we propose to investigate how
to apply these techniques to another language ?
French ? by testing two distinct enhancements
FTB MFT
POS tags A ADV C CL D ET
I N P P+D P+PRO
PONCT PREF PRO
V
A A_card ADV
ADV_int AD-
Vne A_int CC CL
C_S D D_card
ET I N N_card
P P+D PONCT
P+PRO_rel PREF
PRO PRO_card
PRO_int PRO_rel
V_finite V_inf
V_part
NT labels AP AdP COORD NP
PP SENT Sint Srel
Ssub VN VPinf VP-
part
AdP AdP_int AP
AP_int COORD_XP
COORD_UC CO-
ORD_unary NC
NP NP_int NP_rel
PP PP_int PP_rel
SENT Sint Srel Ssub
VN_finite VN_inf
VN_part VP VPinf
VPpart VPpart_rel
Table 2: FTB?s and MFT?s annotation schemes
over the bare PCFG model carried out by two class
of parser models: an unlexicalized model attempt-
ing to overcome problem (a) and 3 different lex-
icalized models attempting to overcome PCFG?s
problems (a) and (b)5.
3.1 Lexicalized algorithms
The first class of algorithms used are lexicalized
parsers of (Collins, 1999; Charniak, 2000; Chi-
ang, 2003). The insight underlying the lexical-
ized algorithms is to model lexical dependencies
between a governor and its dependants in order to
improve attachment choices.
Even though it has been proven numerous times
that lexicalization was useful for parsing the Wall
Street Journal corpus (Collins, 1999; Charniak,
2000), the question of its relevance for other lan-
guages has been raised for German (Dubey and
Keller, 2003; K?bler et al, 2006) and for French
5Except (Chiang, 2003) which is indeed a TREE IN-
SERTION GRAMMAR (Schabes and Waters, 1995) parser but
which must extract a lexicalized grammar from the set of con-
text free rules underlying a treebank.
153
(Arun and Keller, 2005) where the authors ar-
gue that French parsing benefits from lexicaliza-
tion but the treebank flatness reduces its impact
whereas (Schluter and van Genabith, 2007) argue
that an improved annotation scheme and an im-
proved treebank consistency should help to reach
a reasonable state of the art. As only Collins? mod-
els 1 & 2 have been used for French as instances
of lexicalised parsers, we also report results from
the history-based generative parser of (Charniak,
2000) and the Stochastic Tree Insertion Grammar
parser of (Chiang, 2003) as well as (Bikel, 2002)?s
implementation of the Collins? models 1 & 2
(Collins, 1999). Most of the lexicalized parsers
we use in this work are well known and since their
releases, almost ten years ago, their core parsing
models still provide state-of-the-art performance
on the standard test set for English.6 We insist on
the fact that one of the goals of this work was to
evaluate raw performance of well known parsing
models on French annotated data. Thus, we have
not considered using more complex parsing archi-
tectures that makes use of reranking (Charniak and
Johnson, 2005) or self-training (McClosky et al,
2006) in order to improve the performance of a
raw parsing model. Furthermore, studying and de-
signing a set of features for a reranking parser was
beyond the scope of this work. However, we did
use some of these models in a non classical way,
leading us to explore a Collins? model 2 variation,
named model X, and a Stochastic Tree Adjoining
Grammar (Schabes, 1992; Resnik, 1992) variant7 ,
named Spinal Stochastic Tree Insertion Grammars
(hereafter SPINAL STIG), which was first used to
validate the heuristics used by our adaptation of
the Bikel?s parser to French. The next two subsec-
tions introduce these variations.
Collins? Model 2 variation During the ex-
ploratory phase of this work, we found out that a
specific instance of the Collins? model 2 leads to
significantly better performance than the canoni-
cal model when applied to any of the French Tree-
banks. The difference between those two models
relies on the way probabilities associated to so-
called ?modifier non terminals? nodes are handled
by the generative model.
To explain the difference, let us recall that
6Section 23 of the Wall Street Journal section of the PTB.
7The formalism actually used in this parser is a con-
text free variant of Tree Adjoining Grammar, Tree Insertion
Grammars (TIG), first introduced in (Schabes and Waters,
1995).
a lexicalized PCFG can roughly be described
as a set of stochastic rules of the form:
P ? Ln Ln?1 ..L1 H R1 .. Rm?1 Rm
where Li, H , Ri and P are all lexicalized non
terminals; P inherits its head from H (Bikel,
2004). The Collins? model 2 deterministically
labels some nodes of a rule to be arguments of
a given Head and the remaining nodes are con-
sidered to be modifier non terminals (hereafter
MNT).
In this model, given a left-hand side symbol, the
head and its arguments are first generated and then
the MNT are generated from the head outward.
In Bikel?s implementation of Collins?s model 2
(Bikel, 2004), the MNT parameter class is the fol-
lowing (for clarity, we omit the verb intervening,
subcat and side features which are the same in
both classes) :
? model 2 (canonical) :
p(M(t)i|P,H,wh, th,map(Mi?1))
Where M(t)i is the POS tag of the ith MNT,
P the parent node label, H the head node
label, wh the head word and th its POS
tag. map(Mi?1) is a mapped version of
the previously-generated modifier added to
the conditioning context (see below for its
definition).
map(Mi) =
8
>><
>>:
+START+ if i = 0
CC if Mi = CC
+PUNC+ if Mi =,
or Mi =:
+OTHER+ otherwise
9
>>=
>>;
Whereas in the model we call X 8, the mapping
version of the previously generated non terminal is
replaced by a complete list of all previously gen-
erated non terminals.
? Model X :
p(M(t)i|P,H,wh, th, (Mi?1, ...,Mi?k))
The FTB being flatter than the PTB, one can con-
jecture that giving more context to generate MNT
will improve parsing accuracy, whereas clustering
MNT in a X-bar scheme must help to reduce data
sparseness. Note that the Model X, to the best of
our knowledge, is not documented but included in
Bikel?s parser.
8See file NonTerminalModelStructure1.java in Bikel?s
parser source code at http://www.cis.upenn.edu/
~dbikel/download/dbparser/1.2/install.sh.
154
The spinal STIG model In the case of the STIG
parser implementation, having no access to an
argument adjunct table leads it to extract a gram-
mar where almost all elementary trees consist of
a suite of unary productions from a lexical anchor
to its maximal projection (i.e. spine9). Therefore
extracted trees have no substitution node.
Moreover, the probability model, being split
between lexical anchors and tree templates,
allows a very coarse grammar that contains, for
example, only 83 tree templates for one treebank
instantiation, namely the FTB-CC (cf. section 5).
This behavior, although not documented10, is
close to Collins? model 1, which does not use any
argument adjunct distinction information, and led
to results interesting enough to be integrated as
the ?Chiang Spinal? model in our parser set. It
should be noted that, recently, the use of similar
models has been independently proposed in
(Carreras et al, 2008) with the purpose of getting
a richer parsing model that can use non local
features and in (Sangati and Zuidema, 2009) as a
mean of extracting a Lexicalized Tree Substitution
Grammar. In their process, the first extracted
grammar is actually a spinal STIG.
3.2 Unlexicalized Parser
As an instance of an unlexicalized parser, the last
algorithm we use is the Berkeley unlexicalized
parser (BKY) of (Petrov et al, 2006). This algo-
rithm is an evolution of treebank transformation
principles aimed at reducing PCFG independence
assumptions (Johnson, 1998; Klein and Manning,
2003).
Treebank transformations may be of two kinds
(1) structure transformation and (2) labelling
transformations. The Berkeley parser concentrates
on (2) by recasting the problem of acquiring an
optimal set of non terminal symbols as an semi-
supervised learning problem by learning a PCFG
with Latent annotations (PCFG-LA): given an ob-
served PCFG induced from the treebank, the latent
grammar is generated by combining every non ter-
minal of the observed grammar to a predefined set
H of latent symbols. The parameters of the latent
grammar are estimated from the actual treebank
9Not to be confused with the ?spine? in the Tree Adjunct
Grammar (Joshi, 1987) framework which is the path from a
foot node to the root node.
10We mistakenly ?discovered? this obvious property dur-
ing the preliminary porting phase.
trees (or observed trees) using a specific instanci-
ation of EM.
4 Experimental protocol
In this section, we specify the settings of the
parsers for French, the evaluation protocol and the
different instantiations of the treebanks we used
for conducting the experiments.
4.1 Parsers settings
Head Propagation table All lexicalized parsers
reported in this paper use head propagation tables.
Adapting them to the French language requires
to design French specific head propagation
rules. To this end, we used those described by
(Dybro-Johansen, 2004) for training a Stochastic
Tree Adjoining Grammar parser on French. From
this set, we built a set of meta-rules that were
automatically derived to match each treebank
annotation scheme.
As the Collins Model 2 and the STIG model need
to distinguish between argument and adjunct
nodes to acquire subcategorization frames prob-
abilities, we implemented an argument-adjunct
distinction table that takes advantage of the
function labels annotated in the treebank. This is
one of the main differences with the experiments
described in (Arun and Keller, 2005) and (Dybro-
Johansen, 2004) where the authors had to rely
only on the very flat treebank structure without
function labels, to annotate the arguments of a
head.
Morphology and typography adaptation Fol-
lowing (Arun and Keller, 2005), we adapted
the morphological treatment of unknown words
proposed for French when needed (BKY?s and
BIKEL?s parser). This process clusters unknown
words using typographical and morphological in-
formation. Since all lexicalized parsers contain
specific treatments for the PTB typographical con-
vention, we automatically converted the original
punctuation parts of speech to the PTB?s punctua-
tion tag set.
4.2 Experimental details
For the BKY parser, we use the Berkeley imple-
mentation, with an initial horizontal markoviza-
tion h=0, and 5 split/merge cycles. For the
COLLINS? MODEL, we use the standard param-
eters set for the model 2, without any argu-
155
ment adjunct distinction table, as a rough emu-
lation of the COLLINS MODEL 1. The same set
of parameters used for COLLINS? MODEL 2 is
used for the MODEL X except for the parameters
?Mod{Nonterminal,Word}ModelStructureNumber? set to
1 instead of 2.
4.3 Protocol
For all parsers, we report parsing results with the
following experimental protocol: a treebank is di-
vided in 3 sections : test (first 10%), development
(second 10%) and training (remaining 80%). The
MFT partition set is the canonical one (3800 sen-
tences for training, 509 for the dev set and the last
430 for the test set). We systematically report the
results with compounds merged. Namely, we pre-
process the treebank in order to turn each com-
pound into a single token both for training and test.
4.4 Evaluation metrics
Constituency Evaluation: we use the standard
labeled bracketed PARSEVAL metric for evalua-
tion (Black et al, 1991), along with unlabeled
dependency evaluation, which is described as a
more annotation-neutral metric in (Rehbein and
van Genabith, 2007). In the remainder of this pa-
per, we use PARSEVAL as a shortcut for Labeled
Brackets results on sentence of length 40 or less.
Dependency Evaluation: unlabeled dependencies
are computed using the (Lin, 1995) algorithm,
and the Dybro Johansens?s head propagation rules
cited above11. The unlabeled dependency accu-
racy gives the percentage of input words (exclud-
ing punctuation) that receive the correct head. All
reported evaluations in this paper are calculated on
sentences of length less than 40 words.
4.5 Baseline : Comparison using minimal
tagsets
We compared all parsers on three different in-
stances, but still comparable versions, of both the
FTB and the MFT. In order to establish a base-
line, the treebanks are converted to a minimal tag
set (only the major syntactic categories.) without
any other information (no mode propagation as in
the MFT) except for the BIKEL?s parser in Collins?
model 2 (resp. model X) and the STIG parser (i.e.
11For this evaluation, the gold constituent trees are con-
verted into pseudo-gold dependency trees (that may con-
tain errors). Then parsed constituent trees are converted
into parsed dependency trees, that are matched against the
pseudo-gold trees.
STIG-pure) whose models needs function labels to
perform.
Note that by stripping all information from the
node labels in the treebanks, we do not mean
to compare the shape of the treebanks or their
parsability but rather to present an overview of
parser performance on each treebank regardless of
tagset optimizations. However, in each experiment
we observe that the BKY parser significantly out-
performs the other parsers in all metrics.
As the STIG parser presents non statistically sig-
nificant PARSEVAL results differences between its
two modes (PURE & SPINAL) with a f-score p-
value of 0.32, for the remaining of the paper we
will only present results for the STIG?s parser in
?spinal? mode.
FTB-min MFT-min
COLLINS MX PARSEVAL 81.65 79.19
UNLAB. DEP 88.48 84.96
COLLINS M2 PARSEVAL 80.1 78.38
UNLAB. DEP 87.45 84.57
COLLINS M1 PARSEVAL 77.98 76.09
UNLAB. DEP 85.67 82.83
CHARNIAK PARSEVAL 82,44 81.34
UNLAB. DEP 88.42 84.90
CHIANG-SPINAL PARSEVAL 80.66 80.74
UNLAB. DEP 87.92 85,14
BKY PARSEVAL 84,93 83.16
UNLAB. DEP 90.06 87.29
CHIANG-PURE PARSEVAL 80.52 79.56
UNLAB. DEP 87,95 85.02
Table 3: Labeled F1 scores for unlexicalised
and lexicalised parsers on treebanks with minimal
tagsets
5 Cross parser evaluation of tagset
variation
In (Crabb? and Candito, 2008), the authors
showed that it was possible to accurately train the
Petrov?s parser (Petrov et al, 2006) on the FTB us-
ing a more fine grained tag set. This tagset, named
CC12 annotates the basic non-terminal labels with
verbal mood information, and wh-features. Re-
sults were shown to be state of the art with a F1
parseval score of 86.42% on less than 40 words
sentences.
To summarize, the authors tested the impact of
tagset variations over the FTB using constituency
measures as performance indicators.
Knowing that the MFT has been built with PCFG-
based LFG parsing performance in mind (Schluter
12TREEBANKS+ in (Crabb? and Candito, 2008).
156
and van Genabith, 2008) but suffers from a small
training size and yet alows surprisingly high pars-
ing results (PARSEVAL F-score (<=40) of 79.95
% on the MFT gold standard), one would have
wished to verify its
performance with more annotated data.
However, some semi-automatic modifications
brought to the global structure of this treebank
cannot be applied, in an automatic and reversible
way, to the FTB. Anyway, even if we cannot evalu-
ate the influence of a treebank structure to another,
we can evaluate the influence of one tagset to an-
other treebank using handwritten conversion tools.
In order to evaluate the relations between tagsets
and parsing accuracy on a given treebank, we ex-
tract the optimal tagsets13 from the FTB, the CC
tagset and we convert the MFT POS tags to this
tagset. We then do the same for the FTB on which
we apply the MFT?s optimal tagset (ie. SCHLU).
Before introducing the results of our experiments,
we briefly describe these tagsets.
1. min : Preterminals are simply the main cate-
gories, and non terminals are the plain labels
2. cc : (Crabb? and Candito, 2008) best tagset.
Preterminals are the main categories, con-
catenated with a wh- boolean for A, ADV,
PRO, and with the mood for verbs (there are 6
moods). No information is propagated to non
terminal symbols. This tagset is shown in Ta-
ble 4, and described in (Crabb? and Candito,
2008).
ADJ ADJWH ADV ADVWH CC CLO CLR
CLS CS DET DETWH ET I NC NPP P P+D
P+PRO PONCT PREF PRO PROREL PROWH
V VIMP VINF VPP VPR VS
Table 4: CC tagset
3. schlu : N. Schluter?s tagset (Table 2.
Preterminals are the main categories, plus
an inf/finite/part verbal distinction, and
int/card/rel distinction on N, PRO, ADV, A.
These distinctions propagate to non terminal
nodes projected by the lexical head. Non ter-
minals for coordinating structures are split
according to the type of the coordinated
phrases.
Results of these experiments, presented in Table
5, show that BKY displays higher performances
13W.r.t constituent parsing accuracy
in every aspects (constituency and dependency,
except for the MFT-SCHLU). Regardless of the
parser type, we note that unlabeled dependency
scores are higher with the SCHLU tagset than with
the CC tagset. That can be explained by the finest
granularity of the SCHLU based rule set compared
to the other tagset?s rules. As these rules have all
been generated from meta description (a general
COORD label rewrites into COORD_vfinite, CO-
ORD_Sint, etc..) their coverage and global accu-
racy is higher. For example the FTB-CC contains
18 head rules whereas the FTB-SCHLU contains
43 rules.
Interestingly, the ranking of lexicalized parsers
w.r.t PARSEVAL metrics shows that CHARNIAK
has the highest performance over both treebank
tagsets variation even though the MFT?s table (ta-
ble 5) exhibits a non statistically significant vari-
ation between CHARNIAK and STIG-spinal on
PARSEVAL evaluation of the MFT-CC.14
One the other hand, unlabeled dependency evalu-
ations over lexicalized parsers are different among
treebanks. In the case of the FTB, CHARNIAK
exhibits the highest F-score ( FTB-CC: 89.7,
FTB-SCHLU: 89.67) whereas SPINAL STIG per-
forms slightly better on the MFT-SCHLU (MFT-
CC: 86,7, MFT-SCHLU: 87.16). Note that both
tested variations of the Collins? model 2 display
very high unlabeled dependency scores with the
SCHLU tagset.
6 Related Works
As we said in the introduction, the initial work
on the FTB has been carried by (Dybro-Johansen,
2004) in order to extract Tree Adjunct Grammars
from the treebank. Although parsing results were
not reported, she experienced the same argument
adjunct distinction problem than (Arun and Keller,
2005) due to the treebank flatness and the lack of
functional labels in this version. This led Arun
to modify some node annotations (VNG to distin-
guish nodes dominating subcategorized subject cl-
itics and so on) and to add bigrams probabilities to
the language model in order to enhance the over-
all COLLINS? MODEL? performance. Although
our treebanks cannot be compared (20.000 sen-
tences for Arun?s one vs 12351 for the FTB), we
report his best PARSEVAL results (<=40): 80.65
LP, 80.25 LR, 80.45 F1.
However, our results are directly comparable with
14Precision P-value = 0.1272 and Recall = 0.06.
157
Parser
Collins (MX)
Collins (M2)
Collins (M1)
Charniak
Chiang (Sp)
Bky
Parseval Dependency
MFTCC MFTSCH. MFTCC MFTSCH.
80.2 80.96 85.97 87.98
78.56 79.91 84.84 87.43
74 78.49 81.31 85.94
82.5 82.66 86.45 86.94
82.6 81.97 86.7 87.16
83.96 82.86 87.41 86.87
Parseval Dependency
FTBCC FTBSCH. FTBCC FTBSCH.
82.52 82.65 88.96 89.12
80.8 79.56 87.94 87.87
79.16 78.51 86.66 86.93
84.27 83.27 89.7 89.67
81.73 81.54 88.85 89.02
86.02 84.95 90.48 90.73
Table 5: Evaluation Results: MFT-CC vs MFT-SCHLU and FTB-CC vs FTB-SCHLU
(Schluter and van Genabith, 2007) whose best
PARSEVAL F-score on raw text is 79.95 and our
best 82.86 on the MFT-SCHLU.
PARSER FTBARUN MFTSCHLU
Arun (acl05) 80.45 -
Arun (this paper) 81.08 -
Schluter (pacling07) - 79.95
Collins (Mx) 81.5 80,96
Collins (M2) 79.36 79,91
Collins (M1) 77.82 -
Charniak 82.35 82,66
Chiang (Sp) 80.94 81,86
Bky 84.03 82.86
Table 6: Labeled bracket scores on Arun?s FTB
version and on the MFT
In order to favour a ?fair? comparison between
our work and (Arun and Keller, 2005), we also
ran their best adaptation of the COLLINS MODEL
2 on their treebank version using our own head
rules set15 and obtained 81.08% of F1 score (Ta-
ble 6). This shows the important influence of a
fine grained head rules set and argues in favor
of data driven induction of this kind of heuris-
tics. Even though it was established, in (Chiang
and Bikel, 2002), that unsupervised induction of
head rules did not lead to improvement over an
extremely hand crafted head rules set, we believe
that for resource poor languages, such methods
could lead toward significant improvements over
parsing accuracy. Thus, the new unsupervised
head rules induction method presented in (Sangati
and Zuidema, 2009) seems very promising for this
topic.
However, it would be of interest to see how the
Arun?s model would perform using the MODEL X
parameter variations.
7 Discussion
Regarding the apparent lack of success of a gen-
uine COLLINS? MODEL 2 (in most cases, its per-
15Due to the lack of function annotation labels in this tree-
bank, (Arun and Keller, 2005)?s argument distinction table
was used for this experiment.
formance is worse than the other parsers w.r.t to
constituent parsing accuracy) when trained on a
treebank with annotated function labels, we sus-
pect that this is caused by the increased data
sparseness added by these annotations. The same
can be said about the pure STIG model, whose re-
sults are only presented on the FTB-MIN because
the differences between the spinal model and itself
were too small and most of the time not statisti-
cally significant. In our opinion, there might be
simply not enough data to accurately train a pure
COLLINS? MODEL 2 on the FTB with function
labels used for clues to discriminate between argu-
ment and adjuncts. Nevertheless, we do not share
the commonly accepted opinion about the poten-
tial lack of success of lexicalized parsers.
To the best of our knowledge, most adaptations of
a lexicalized model to a western language have
been made with Dan Bikel?s implementation of
COLLINS? MODELS.16
In fact, the adaptations of the CHARNIAK and
BKY?s models exhibit similar magnitudes of per-
formances for French as for English. Evidence of
lexicalization usefulness is shown through a learn-
ing curve (Figure 6) obtained by running some of
our parsers in perfect tagging mode. This experi-
ment was done in the early stages of this work, the
goal was to see how well the parsers would behave
with the same head rules and the same set of pa-
rameters. We only compared the parsers that could
be used without argument adjunct distinction table
(ie. COLLIN?S MODEL 1, SPINAL STIG, CHAR-
NIAK and BKY).
For this earlier experiment, our implementation
of the COLLINS MODEL 1 actually corresponds to
the MODEL X without an argument adjunct dis-
tinction table. More precisely, the absence of ar-
gument nodes, used for the acquisition of subcat-
egorization frames features, makes the MODEL X
parsing model consider all the nodes of a rule, ex-
16Note that the CHARNIAK?s parser has been adapted for
Danish (Zeman and Resnik, 2008) ; the authors report a 80.20
F1 score for a specific instance of the Danish Treebank.
158
2000 4000 6000 8000 10000
76
78
80
82
84
86
88
Number of training sentences
La
be
le
d 
br
a
ck
e
ts
 F
?s
co
re
 (<
=4
0)
Berkeley
Charniak
SpinalTig
Model 1 (emulated)
Figure 6: Learning Curve experiment results for
parsers in perfect tagging mode
cept the head, as Modifier Non Terminal nodes
(MNTs). Hence, because of the impossibility to
extract subcategorization frames, the generation
of a MNT depends mainly on the parent head
word and on the whole list of previously gener-
ated MNTs. One can suppose that training on
small treebanks would lead this distribution to be
sparse, therefore most of the discriminant infor-
mation would come from less specific distribu-
tions. Namely the ones conditioned on the head
pos tag and on the last previously generated MNT
as shown in this model back-off structure (Table
7).
Back-off level p(M(t)i| ? ? ? )
0 P,H,wh, th, ?Mi?1, ...,Mi?k?
1 P,H, th,Mi?1
2 P,H, f
Table 7: MODEL X simplified parameter class for
MNTs
M(t)i is the POS tag of the ith MNT, P the parent node
label, H the head node label, wh the head word, th its POS
tag, ?Mi?1, ...,Mi?k? the list of previously generated MNTs
and f a flag stating if the current node is the first MNT to be
generated.
Interestingly, in the SPINAL STIG model,
almost all the extracted trees are spinal and conse-
quently are handled by an operation called Sister
Adjunction whose probability model for a given
root node of an elementary tree, also conditions
its generation upon the label of the previously
generated tree (Chiang, 2003). Furthermore,
the second component of the Sister Adjunction?s
back-off structure (Table 8) is made coarser by the
removing of the lexical anchor of the tree where a
sister-adjunction is to occur.
Studying in depth the respective impact of these
features on the performance of both models is
outside the scope of this paper, nevertheless we
note that their back-off structures are based on
similar principles: a deletion of the main lexical
information and a context limited to the root label
of the previously generated tree (resp. MNT node
label for the MODEL X). This can explain why
these formally different parsers display almost the
same learning curves (Fig. 6) and more over why
they surprisingly exhibit few sensitivity to the
amount of lexical material used for training.
Back-off level Psa(?| ? ? ? )
0 ??, ??, ??, i,X
1 ??, ??, i,X
2 ??, ??, i
Table 8: SPINAL STIG parameter class for Sister-
adjoining tree templates (Chiang, 2003)
? is the tree to be generated on the sister adjunction site
(??, i) of the tree template ?? , ?? is the lexical anchor of ?? ,
?? is ?? stripped from its anchor POS tag and X is the root
label of the previous tree to sister-adjoin at the site (??, i).
However, the learning curve also shows that the
CHARNIAK?s17 and BKY?s parsers have almost
parallel curves whereas this specific COLLIN?S
MODEL 1 parser and the SPINAL STIG model have
very similar shape and seem to reach an upper
limit very quickly.18 The last two parsers having
also very similar back-off models (Chiang, 2003),
we wonder (1) if we are not actually comparing
them because of data sparseness issues and (2) if
the small size of commonly used treebanks does
not lead the community to consider lexicalized
models, via the lone COLLINS? MODELS, as inap-
propriate to parse other languages than Wall Street
Journal English.
17As opposed to the other parsers, the Charniak?s parser
tagging accuracy did not reach the 100% limit, 98.32% for the
last split. So the comparison is not really fair but we believe
that the visible tendency still stands.
18We are of course aware that the curve?s values are also
function of the amount of new productions brought by the
increased treebank size. That should be of course taken into
account.
159
Regarding the remarkable performance of the
BKY algorithm, it remains unclear why exactly
it systematically outperforms the other lexicalized
algorithms. We can only make a few remarks
about that. First, the algorithm is totally dis-
joint from the linguistic knowledge, that is entirely
taken from the treebank, except for the suffixes
used for handling unknown words. This is not true
of the Collins? or Charniak?s models, that were
set up with the PTB annotation scheme in mind.
Another point concerns the amount of data nec-
essary for an accurate learning. We had the intu-
ition that lexicalized algorithms would have ben-
efited more than BKY from the training data size
increase. Yet the BKY?s learning curve displays a
somewhat faster progression than lexicalized algo-
rithms such as the SPINAL STIG and our specific
instance of the COLLINS? MODEL 1.
In our future work, we plan to conduct
self-training experiments using discriminative
rerankers on very large French corpora to study
the exact impact of the lexicon on this unlexical-
ized algorithm.
8 Conclusion
By adapting those parsers to French and carry-
ing out extensive evaluation over the main char-
acteristics of the treebank at our disposal, we
prove indeed that probabilistic parsing was effi-
cient enough to provide accurate parsing results
for French. We showed that the BKY model estab-
lishes a high performance level on parsing results.
Maybe more importantly we emphasized the im-
portance of tag set model to get distinct state of
the art evaluation metrics for FTB parsing, namely
the SCHLU tagset to get more accurate unlabeled
dependencies and the CC tagset to get better con-
stituency parses. Finally, we showed that the lexi-
calization debate could benefit from the inclusion
of more lexicalized parsing models.
9 Acknowledgments
This work was supported by the ANR Sequoia
(ANR-08-EMER-013). We heartily thank A.
Arun, J. van Genabith an N. Schluter for kindly
letting us use our parsers on their treebanks.
Thanks to the anonymous reviewers for their com-
ments. All remaining errors are ours. We thank J.
Wagner for his help and we would like to acknowl-
edge the Centre for Next Generation Localization
(www.cngl.ie) for providing access to one of its
high-memory nodes.
References
Anne Abeill? and Nicolas Barrier. 2004. Enrich-
ing a french treebank. In Proceedings of Language
Ressources and Evaluation Conference (LREC), Lis-
bon.
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer,
Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
french. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 306?313, Ann Arbor, MI.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Proceedings of the second international conference
on Human Language Technology Research, pages
178?182. Morgan Kaufmann Publishers Inc. San
Francisco, CA, USA.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of english grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop,
pages 306?311, San Mateo (CA). Morgan Kaufman.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages
320?327, Barcelona, Spain.
Xavier Carreras, Mickael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning (CoNLL), pages 9?16.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor (MI).
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Annual
Meeting of the North American Chapter of the ACL
(NAACL), Seattle.
160
David Chiang and Daniel M. Bikel. 2002. Recover-
ing latent information in treebanks. In Proceedings
of COLING?02, 19th International Conference on
Computational Linguistics, Taipei, Taiwan, August.
David Chiang, 2003. Statistical Parsing with an Auto-
matically Extracted Tree Adjoining Grammar, chap-
ter 16, pages 299?316. CSLI Publications.
Michael Collins. 1999. Head Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
Actes de la 15?me Conf?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN?08), pages
45?54, Avignon, France.
Mary Dalrymple. 2001. Lexical-Functional Grammar,
volume 34 of Syntax and Semantics. San Diego,
CA; London. Academic Press.
Amit Dubey and Frank Keller. 2003. Probabilis-
tic parsing for german using sister-head dependen-
cies. In In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 96?103.
Ane Dybro-Johansen. 2004. Extraction automatique
de grammaires ? partir d?un corpus fran?ais. Mas-
ter?s thesis, Universit? Paris 7.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Aravind K. Joshi. 1987. Introduction to tree adjoining
grammar. In A. Manaster-Ramer, editor, The Math-
ematics of Language. J. Benjamins.
R. Kaplan and J. Bresnan. 1982. Lexical-functional
grammar: A formal system for grammarical repre-
sentation. In J. Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
Mass.: MIT Press.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang
Maier. 2006. Is it really that difficult to parse ger-
man? In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 111?119, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Ines Rehbein and Josef van Genabith. 2007. Tree-
bank annotation schemes and parser evaluation for
german. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammars as a framework for statistic natural lan-
guage processing. COLING?92, Nantes, France.
F. Sangati and W. Zuidema. 2009. Unsupervised
methods for head assignments. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 701?709, Athens, Greece.
Association for Computational Linguistics.
Y. Schabes and R.C. Waters. 1995. Tree Insertion
Grammar: Cubic-Time, Parsable Formalism that
Lexicalizes Context-Free Grammar without Chang-
ing the Trees Produced. Computational Linguistics,
21(4):479?513.
Yves Schabes. 1992. Stochastic Lexicalized Tree Ad-
joining Grammars. In Proceedings of the 14th con-
ference on Computational linguistics, pages 425?
432, Nantes, France. Association for Computational
Linguistics.
Natalie Schluter and Josef van Genabith. 2007.
Preparing, restructuring, and augmenting a french
treebank: Lexicalised parsers or coherent treebanks?
In Proceedings of PACLING 07.
Natalie Schluter and Josef van Genabith. 2008.
Treebank-based acquisition of lfg parsing resources
for french. In European Language Resources As-
sociation (ELRA), editor, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of IJCNLP 2008 Work-
shop on NLP for Less Privileged Languages, Haj-
dar?b?du, India.
161
