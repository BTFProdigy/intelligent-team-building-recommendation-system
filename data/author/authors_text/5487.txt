TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 73?80,
Rochester, April 2007 c?2007 Association for Computational Linguistics
DLSITE-2: Semantic Similarity Based on Syntactic
Dependency Trees Applied to Textual Entailment
Daniel Micol, O?scar Ferra?ndez, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{dmicol, ofe, rafael, mpalomar}@dlsi.ua.es
Abstract
In this paper we attempt to deduce tex-
tual entailment based on syntactic depen-
dency trees of a given text-hypothesis pair.
The goals of this project are to provide an
accurate and fast system, which we have
called DLSITE-2, that can be applied in
software systems that require a near-real-
time interaction with the user. To accom-
plish this we use MINIPAR to parse the
phrases and construct their correspond-
ing trees. Later on we apply syntactic-
based techniques to calculate the seman-
tic similarity between text and hypothe-
sis. To measure our method?s precision we
used the test text corpus set from Second
PASCAL Recognising Textual Entailment
Challenge (RTE-2), obtaining an accuracy
rate of 60.75%.
1 Introduction
There are several methods used to determine tex-
tual entailment for a given text-hypothesis pair. The
one described in this paper uses the information
contained in the syntactic dependency trees of such
phrases to deduce whether there is entailment or
not. In addition, semantic knowledge extracted from
WordNet (Miller et al, 1990) has been added to
achieve higher accuracy rates.
It has been proven in several competitions and
other workshops that textual entailment is a complex
task. One of these competitions is PASCAL Recog-
nising Textual Entailment Challenge (Bar-Haim et
al., 2006), where each participating group develops a
textual entailment recognizing system attempting to
accomplish the best accuracy rate of all competitors.
Such complexity is the reason why we use a combi-
nation of various techniques to deduce whether en-
tailment is produced.
Currently there are few research projects related
to the topic discussed in this paper. Some systems
use syntactic tree matching as the textual entailment
decision core module, such as (Katrenko and Adri-
aans, 2006). It is based on maximal embedded syn-
tactic subtrees to analyze the semantic relation be-
tween text and hypothesis. Other systems use syn-
tactic trees as a collaborative module, not being the
core, such as (Herrera et al, 2006). The application
discussed in this paper belongs to the first set of sys-
tems, since syntactic matching is its main module.
The remainder of this paper is structured as fol-
lows. In the second section we will describe the
methods implemented in our system. The third one
contains the experimental results, and the fourth and
last discusses such results and proposes future work
based on our actual research.
2 Methods
The system we have built aims to provide a good
accuracy rate in a short lapse of time, making it
feasible to be included in applications that require
near-real-time responses due to their interaction with
the user. Such a system is composed of few mod-
ules that behave collaboratively. These include tree
construction, filtering, embedded subtree search and
graph node matching. A schematic representation of
the system architecture is shown in Figure 1.
73
Figure 1: DLSITE-2 system architecture.
Each of the steps or modules of DLSITE-2 is de-
scribed in the following subsections, that are num-
bered sequentially according to their execution or-
der.
2.1 Tree generation
The first module constructs the corresponding syn-
tactic dependency trees. For this purpose, MINI-
PAR (Lin, 1998) output is generated and afterwards
parsed for each text and hypothesis of our corpus.
Phrase tokens, along with their grammatical infor-
mation, are stored in an on-memory data structure
that represents a tree, which is equivalent to the men-
tioned syntactic dependency tree.
2.2 Tree filtering
Once the tree has been constructed, we may want
to discard irrelevant data in order to reduce our sys-
tem?s response time and noise. For this purpose we
have generated a database of relevant grammatical
categories, represented in Table 1, that will allow
us to remove from the tree all those tokens whose
category does not belong to such list. The result-
ing tree will have the same structure as the original,
but will not contain any stop words nor irrelevant to-
kens, such as determinants or auxiliary verbs. The
whole list of ignored grammatical categories is rep-
resented in Table 2.
We have performed tests taking into account and
discarding each grammatical category, which has al-
lowed us to generate both lists of relevant and ig-
nored grammatical categories.
Verbs, verbs with one argument, verbs with two ar-
guments, verbs taking clause as complement, verb
Have, verb Be
Nouns
Numbers
Adjectives
Adverbs
Noun-noun modifiers
Table 1: Relevant grammatical categories.
2.3 Graph embedding detection
The next step of our system consists in determining
whether the hypothesis? tree is embedded into the
text?s. Let us first define the concept of embedded
tree (Katrenko and Adriaans, 2006).
Definition 1: Embedded tree A tree
T1 = (V1, E1) is embedded into another
one T2 = (V2, E2) iff
1. V1 ? V2, and
2. E1 ? E2
where V1 and V2 represent the vertices,
and E1 and E2 the edges.
In other words, a tree, T1, is embedded into an-
other one, T2, if all nodes and branches of T1 are
present in T2.
We believe that it makes sense to reduce the strict-
ness of such a definition to allow the appearance
of intermediate nodes in the text?s branches that are
74
Determiners
Pre-determiners
Post-determiners
Clauses
Inflectional phrases
Preposition and preposition phrases
Specifiers of preposition phrases
Auxiliary verbs
Complementizers
Table 2: Ignored grammatical categories.
not present in the corresponding hypothesis? branch,
which means that we allow partial matching. There-
fore, a match between two branches will be pro-
duced if all nodes of the first one, namely ?1 ? E1,
are present in the second, namely ?2 ? E2, and their
respective order is the same, allowing the possibil-
ity of appearance of intermediate nodes that are not
present in both branches. This is also described in
(Katrenko and Adriaans, 2006).
To determine whether the hypothesis? tree is em-
bedded into the text?s, we perform a top-down
matching process. For this purpose we first compare
the roots of both trees. If they coincide, we then pro-
ceed to compare their respective child nodes, which
are the tokens that have some sort of dependency
with their respective root token.
In order to add more flexibility to our system,
we do not require the pair of tokens to be ex-
actly the same, but rather set a threshold that rep-
resents the minimum similarity value between them.
This is a difference between our approach and the
one described in (Katrenko and Adriaans, 2006).
Such a similarity is calculated by using the Word-
Net::Similarity tool (Pedersen et al, 2004), and,
concretely, the Wu-Palmer measure, as defined in
Equation 1 (Wu and Palmer, 1994).
Sim(C1, C2) =
2N3
N1 +N2 + 2N3
(1)
where C1 and C2 are the synsets whose similarity
we want to calculate, C3 is their least common su-
perconcept, N1 is the number of nodes on the path
from C1 to C3, N2 is the number of nodes on the
path from C2 to C3, and N3 is the number of nodes
on the path from C3 to the root. All these synsets
and distances can be observed in Figure 2.
Figure 2: Distance between two synsets.
If the similarity rate is greater or equal than the
established threshold, which we have set empirically
to 80%, we will consider the corresponding hypoth-
esis? token as suitable to have the same meaning
as the text?s token, and will proceed to compare its
child nodes in the hypothesis? tree. On the other
hand, if such similarity value is less than the cor-
responding threshold, we will proceed to compare
the children of such text?s tree node with the actual
hypothesis? node that was being analyzed.
The comparison between the syntactic depen-
dency trees of both text and hypothesis will be com-
pleted when all nodes of either tree have been pro-
cessed. If we have been able to find a match for all
the tokens within the hypothesis, the corresponding
tree will be embedded into the text?s and we will be-
lieve that there is entailment. If not, we will not be
able to assure that such an implication is produced
and will proceed to execute the next module of our
system.
Next, we will present a text-hypothesis pair sam-
ple where the syntactic dependency tree of the hy-
pothesis (Figure 3(b)) is embedded into the text?s
(Figure 3(a)). The mentioned text-hypothesis pair
is the following:
Text: Mossad is one of the world?s most
well-known intelligence agencies, and is
often viewed in the same regard as the CIA
and MI6.
Hypothesis: Mossad is an intelligence
agency.
75
(a) Mossad is one of the world?s most well-known intelligence agencies, and is often viewed
in the same regard as the CIA and MI6.
(b) Mossad is an intelligence
agency.
Figure 3: Representation of a hypothesis? syntactic dependency tree that is embedded into the text?s.
As one can see in Figure 3, the hypothesis? syn-
tactic dependency tree represented is embedded into
the text?s because all of its nodes are present in
the text in the same order. There is one exception
though, that is the word an. However, since it is a
determinant, the filtering module will have deleted
it before the graph embedding test is performed.
Therefore, in this example the entailment would be
recognized.
2.4 Graph node matching
Once the embedded subtree comparison has fin-
ished, and if its result is negative, we proceed to per-
form a graph node matching process, termed align-
ment, between both the text and the hypothesis. This
operation consists in finding pairs of tokens in both
trees whose lemmas are identical, no matter whether
they are in the same position within the tree. We
would like to point out that in this step we do not
use the WordNet::Similarity tool.
Some authors have already designed similar
matching techniques, such as the ones described in
(MacCartney et al, 2006) and (Snow et al, 2006).
However, these include semantic constraints that we
have decided not to consider. The reason of this
decision is that we desired to overcome the textual
entailment recognition from an exclusively syntactic
perspective. Therefore, we did not want this module
to include any kind of semantic knowledge.
The weight given to a token that has been found
in both trees will depend on the depth in the hypoth-
esis? tree and the token?s grammatical relevance.
The first of these factors depends on an empirically-
calculated weight that assigns less importance to a
node the deeper it is located in the tree. This weight
is defined in Equation 2. The second factor gives
different relevance depending on the grammatical
category and relationship. For instance, a verb will
have the highest weight, while an adverb or an ad-
jective will have less relevance. The values assigned
to each grammatical category and relationship are
also empirically-calculated and are shown in Tables
3 and 4, respectively.
Grammatical category Weight
Verbs, verbs with one argument, verbs
with two arguments, verbs taking
clause as complement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun mod-
ifiers
0.5
Verbs Have and Be 0.3
Table 3: Weights assigned to the grammatical cate-
gories.
76
Grammatical relationship Weight
Subject of verbs, surface subject, ob-
ject of verbs, second object of ditran-
sitive verbs
1.0
The rest 0.5
Table 4: Weights assigned to the grammatical rela-
tionships.
Let ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively. We as-
sume we have found members of a synset, namely ?,
present in both ? and ?. Now let ? be the weight as-
signed to ??s grammatical category (defined in Table
3), ? the weight of ??s grammatical relationship (de-
fined in Table 4), ? an empirically-calculated value
that represents the weight difference between tree
levels, and ?? the depth of the node that contains
the synset ? in ?. We define the function ?(?) as
represented in Equation 2.
?(?) = ? ? ? ? ???? (2)
The value obtained by calculating the expression
of Equation 2 would represent the relevance of a
synset in our system. The experiments performed
reveal that the optimal value for ? is 1.1.
For a given pair (? , ?), we define the set ? as the
one that contains the synsets present in both trees:
? = ? ? ? ?? ? ?, ? ? ? (3)
Therefore, the similarity rate between ? and ?, de-
noted by the symbol ?, would be defined as:
?(?, ?) =
?
???
?(?) (4)
One should note that a requirement of our sys-
tem?s similarity measure would be to be independent
of the hypothesis length. Thus, we must define the
normalized similarity rate, as shown in Equation 5.
?(?, ?) =
?(?, ?)
?
???
?(?)
=
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value, ?(?, ?), has been cal-
culated, it will be provided to the user together with
the corresponding text-hypothesis pair identifier. It
will be his responsibility to choose an appropriate
threshold that will represent the minimum similarity
rate to be considered as entailment between text and
hypothesis. All values that are under such a thresh-
old will be marked as not entailed. For this purpose,
we suggest using a development corpus in order to
obtain the optimal threshold value, as it is done in
the RTE challenges.
3 Experimental results
The experimental results shown in this paper were
obtained processing a set of text-hypothesis pairs
from RTE-2. The organizers of this challenge pro-
vide development and test corpora to the partic-
ipants, both of them containing 800 pairs manu-
ally annotated for logical entailment. It is com-
posed of four subsets, each of them correspond-
ing to typical true and false entailments in different
tasks, such as Information Extraction (IE), Informa-
tion Retrieval (IR), Question Answering (QA), and
Multi-document Summarization (SUM). For each
task, the annotators selected the same amount of true
entailments as negative ones (50%-50% split).
The organizers have also defined two measures to
evaluate the participating systems. All judgments
returned by the systems will be compared to those
manually assigned by the human annotators. The
percentage of matching judgments will provide the
accuracy of the system, i.e. the percentage of cor-
rect responses. As a second measure, the average
precision will be computed. This measure evaluates
the ability of the systems to rank all the pairs in the
corpus according to their entailment confidence, in
decreasing order from the most certain entailment to
the least. Average precision is a common evaluation
measure for system rankings that is defined as shown
in Equation 6.
AP =
1
R
n?
i=1
E(i)
#correct up to pair i
i
(6)
where n is the amount of the pairs in the test corpus,
R is the total number of positive pairs in it, i ranges
over the pairs, ordered by their ranking, and E(i) is
defined as follows:
77
E(i) =
?
?
?
1 if the i? th pair is positive,
0 otherwise.
(7)
As we previously mentioned, we tested our sys-
tem against RTE-2 development corpus, and used
the test one to evaluate it.
First, Table 5 shows the accuracy (ACC) and av-
erage precision (AP), both as a percentage, obtained
processing the development corpus from RTE-2 for
a threshold value of 68.9%, which corresponds to
the highest accuracy that can be obtained using our
system for the mentioned corpus. It also provides
the rate of correctly predicted true and false entail-
ments.
Task ACC AP TRUE FALSE
IE 52.00 51.49 54.00 50.00
IR 55.50 58.99 32.00 79.00
QA 57.50 54.72 53.00 62.00
SUM 65.00 81.35 39.00 91.00
Overall 57.50 58.96 44.50 70.50
Table 5: Results obtained for the development cor-
pus.
Next, let us show in Table 6 the results obtained
processing the test corpus, which is the one used
to compare the different systems that participated in
RTE-2, with the same threshold as before.
Task ACC AP TRUE FALSE
IE 50.50 47.33 75.00 26.00
IR 64.50 67.67 59.00 70.00
QA 59.50 58.16 80.00 39.00
SUM 68.50 75.86 49.00 88.00
Overall 60.75 57.91 65.75 55.75
Table 6: Results obtained for the test corpus.
As one can observe in the previous table, our
system provides a high accuracy rate by using
mainly syntactical measures. The number of text-
hypothesis pairs that succeeded the graph embed-
ding evaluation was three for the development cor-
pus and one for the test set, which reflects the strict-
ness of such module. However, we would like to
point out that the amount of pairs affected by the
mentioned module will depend on the corpus na-
ture, so it can vary significantly between different
corpora.
Let us now compare our results with the ones that
were achieved by the systems that participated in
RTE-2. One should note that the criteria for such
ranking is based exclusively on the accuracy, ignor-
ing the average precision value. In addition, each
participating group was allowed to submit two dif-
ferent systems to RTE-2. We will consider here the
best result of both systems for each group. The men-
tioned comparison is shown in Table 7, and contains
only the systems that had higher accuracy rates than
our approach.
Participant Accuracy
(Hickl et al, 2006) 75.38
(Tatu et al, 2006) 73.75
(Zanzotto et al, 2006) 63.88
(Adams, 2006) 62.62
(Bos and Markert, 2006) 61.62
DLSITE-2 60.75
Table 7: Comparison of some of the teams that par-
ticipated in RTE-2.
As it is reflected in Table 7, our system would
have obtained the sixth position out of twenty-four
participants, which is an accomplishment consider-
ing the limited number of resources that it has built-
in.
Since one of our system?s modules is based on
(Katrenko and Adriaans, 2006), we will compare
their results with ours to analyze whether the modi-
fications we introduced perform correctly. In RTE-
2, they obtained an accuracy rate of 59.00% for the
test corpus. The reason why we believe we have
achieved better results than their system is due to
the fact that we added semantic knowledge to our
graph embedding module. In addition, the syntactic
dependency trees to which we have applied such a
module have been previously filtered to ensure that
they do not contain irrelevant words. This reduces
the system?s noise and allows us to achieve higher
accuracy rates.
In the introduction of this paper we mentioned
that one of the goals of our system was to provide
78
a high accuracy rate in a short lapse of time. This is
one of the reasons why we chose to construct a light
system where one of the aspects to minimize was its
response time. Table 8 shows the execution times1
of our system for both development and test text cor-
pora from RTE-2. These include total and average2
response times.
Development Test
Total 1045 1023
Average 1.30625 1.27875
Table 8: DLSITE-2 response times (in seconds).
As we can see, accurate results can be obtained
using syntactic dependency trees in a short lapse of
time. However, there are some limitations that our
system does not avoid. For instance, the tree em-
bedding test is not applicable when there is no verb
entailment. This is reflected in the following pair:
Text: Tony Blair, the British Prime Minis-
ter, met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The root node of the hypothesis? tree would be
the one corresponding to the verb is. Since the en-
tailment here is implicit, there is no need for such a
verb to appear in the text. However, this is not com-
patible with our system, since is would not match
any node of the text?s tree, and thus the hypothesis?
tree would not be found embedded into the text?s.
The graph matching process would not behave
correctly either. This is due to the fact that the main
verb, which has the maximum weight because it is
the root of the hypothesis? tree and its grammatical
category has the maximum relevance, is not present
in the text, so the overall similarity score would have
a considerable handicap.
The example of limitation of our system that we
have presented is an apposition. To avoid this spe-
cific kind of situations that produce an undesired be-
havior in our system, we could add a preprocess-
ing module that transforms the phrases that have the
1The machine we used to measure the response times had an
Intel Core 2 Duo processor at 2GHz.
2Average response times are calculated diving the totals by
the number of pairs in the corpus.
structureX , Y , Z intoX is Y , andZ. For the shown
example, the resulting text and hypothesis would be
as follows:
Text: Tony Blair is the British Prime Min-
ister, and met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The transformed text would still be syntactically
correct, and the entailment would be detected since
the hypothesis? syntactic dependency tree is embed-
ded into the text?s.
4 Conclusions and future work
The experimental results obtained from this research
demonstrate that it is possible to apply a syntactic-
based approach to deduce textual entailment from a
text-hypothesis pair. We can obtain good accuracy
rates using the discussed techniques with very short
response times, which is very useful for assisting
different kinds of tasks that demand near-real-time
responses to user interaction.
The baseline we set for our system was to achieve
better results than the ones we obtained with our last
participation in RTE-2. As it is stated in (Ferra?ndez
et al, 2006), the maximum accuracy value obtained
by then was 55.63% for the test corpus. Therefore,
our system is 9.20% more accurate compared to the
one that participated in RTE-2, which represents a
considerable improvement.
The authors of this paper believe that if higher ac-
curacy rates are desired, a step-based systemmust be
constructed. This would have several preprocessing
units, such as negation detectors, multi-word associ-
ators and so on. The addition of these units would
definitely increase the response time preventing the
system from being used in real-time tasks.
Future work can be related to the cases where no
verb entailment is produced. For this purpose we
propose to extract a higher amount of semantic in-
formation that would allow us to construct a charac-
terized representation based on the input text, so that
we can deduce entailment even if there is no appar-
ent structure similarity between text and hypothesis.
This would mean to create an abstract conceptual-
ization of the information contained in the analyzed
phrases, allowing us to deduce ideas that are not
79
explicitly mentioned in the parsed text-hypothesis
pairs.
In addition, the weights and thresholds defined
in our system have been established empirically. It
would be interesting to calculate those values by
means of a machine learning algorithm and com-
pare them to the ones we have obtained empirically.
Some authors have already performed this compari-
son, being one example the work described in (Mac-
Cartney et al, 2006).
Acknowledgments
The authors of this paper would like to thank pro-
fessors Borja Navarro and Rafael M. Terol for their
help and critical comments.
This research has been supported by the under-
graduate research fellowships financed by the Span-
ish Ministry of Education and Science, the project
TIN2006-15265-C06-01 financed by such ministry,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Rod Adams. 2006. Textual Entailment Through Ex-
tended Lexical Overlap. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Johan Bos, and Katja Markert. 2006. When logical infer-
ence helps determining textual entailment (and when it
doesnt). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
O?scar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic Forms andWordNet relation-
ships to Textual Entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Jesu?s Herrera, Anselmo Pen?as, A?lvaro Rodrigo, and Fe-
lisa Verdejo. 2006. UNED at PASCAL RTE-2 Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCC?s GROUNDHOG
System. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Sophia Katrenko, and Pieter Adriaans. 2006. Using
Maximal Embedded Syntactic Subtrees for Textual En-
tailment Recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06), NewYork City, NewYork, United States of Amer-
ica.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography 1990 3(4):235-
244.
Ted Pedersen, Siddhart Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-04), Boston, Massachus-
sets, United States of America.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American As-
sociation of Computational Linguistics (NAACL-06),
New York City, New York, United States of America.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi,
and DanMoldovan. 2006. COGEX at the Second Rec-
ognizing Textual Entailment Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Zhibiao Wu, and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of the 32nd An-
nual Meeting of the Associations for Computational
Linguistics, pages 133-138, Las Cruces, New Mexico,
United States of America.
Fabio M. Zanzotto, Alessandro Moschitti, Marco Pen-
nacchiotti, and Maria T. Pazienza. 2006. Learning
textual entailment from examples. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Venice, Italy.
80
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 66?71,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Perspective-Based Approach for Solving Textual Entailment Recognition
O?scar Ferra?ndez, Daniel Micol, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{ofe, dmicol, rafael, mpalomar}@dlsi.ua.es
Abstract
The textual entailment recognition system
that we discuss in this paper represents
a perspective-based approach composed of
two modules that analyze text-hypothesis
pairs from a strictly lexical and syntactic
perspectives, respectively. We attempt to
prove that the textual entailment recognition
task can be overcome by performing indi-
vidual analysis that acknowledges us of the
maximum amount of information that each
single perspective can provide. We compare
this approach with the system we presented
in the previous edition of PASCAL Recognis-
ing Textual Entailment Challenge, obtaining
an accuracy rate 17.98% higher.
1 Introduction
Textual entailment recognition has become a popu-
lar Natural Language Processing task within the last
few years. It consists in determining whether one
text snippet (hypothesis) entails another one (text)
(Glickman, 2005). To overcome this problem sev-
eral approaches have been studied, being the Recog-
nising Textual Entailment Challenge (RTE) (Bar-
Haim et al, 2006; Dagan et al, 2006) the most re-
ferred source for determining which one is the most
accurate.
Many of the participating groups in previous edi-
tions of RTE, including ourselves (Ferra?ndez et al,
2006), designed systems that combined a variety of
lexical, syntactic and semantic techniques. In our
contribution to RTE-3 we attempt to solve the tex-
tual entailment recognition task by analyzing two
different perspectives separately, in order to ac-
knowledge the amount of information that an indi-
vidual perspective can provide. Later on, we com-
bine both modules to obtain the highest possible ac-
curacy rate. For this purpose, we analyze the pro-
vided corpora by using a lexical module, namely
DLSITE-1, and a syntactic one, namely DLSITE-2.
Once all results have been obtained we perform a
voting process in order to take into account all sys-
tem?s judgments.
The remainder of this paper is structured as fol-
lows. Section two describes the system we have
built, providing details of the lexical and syntactic
perspectives, and explains the difference with the
one we presented in RTE-2. Third section presents
the experimental results, and the fourth one provides
our conclusions and describes possible future work.
2 System Specification
This section describes the systemwe have developed
in order to participate in RTE-3. It is based on sur-
face techniques of lexical and syntactic analysis. As
the starting point we have used our previous system
presented in the second edition of the RTE Chal-
lenge (Ferra?ndez et al, 2006). We have enriched
it with two independent modules that are intended
to detect some misinterpretations performed by this
system. Moreover, these new modules can also rec-
ognize entailment relations by themselves. The per-
formance of each separate module and their combi-
nation with our previous system will be detailed in
section three.
Next, Figure 1 represents a schematic view of the
system we have developed.
66
Figure 1: System architecture.
As we can see in the previous Figure, our sys-
tem is composed of three modules that are coordi-
nated by an input scheduler. Its commitment is to
provide the text-hypothesis pairs to each module in
order to extract their corresponding similarity rates.
Once all rates for a given text-hypothesis pair have
been calculated, they will be processed by an output
gatherer that will provide the final judgment. The
method used to calculate the final entailment deci-
sion consists in combining the outputs of both lex-
ical and syntactic modules, and these outputs with
our RTE-2 system?s judgment. The output gatherer
will be detailed later in this paper when we describe
the experimental results.
2.1 RTE-2 System
The approach we presented in the previous edition of
RTE attempts to recognize textual entailment by de-
termining whether the text and the hypothesis are re-
lated using their respective derived logic forms, and
by finding relations between their predicates using
WordNet (Miller et al, 1990). These relations have
a specific weight that provide us a score represent-
ing the similarity of the derived logic forms and de-
termining whether they are related or not.
For our participation in RTE-3 we decided to ap-
ply our previous system because it allows us to han-
dle some kinds of information that are not correctly
managed by the new approaches developed for the
current RTE edition.
2.2 Lexical Module
This method relies on the computation of a wide va-
riety of lexical measures, which basically consists of
overlap metrics. Although in other related work this
kind of metrics have already been used (Nicholson
et al, 2006), the main contribution of this module is
the fact that it only deals with lexical features with-
out taking into account any syntactic nor semantic
information. The following paragraphs list the con-
sidered lexical measures.
Simple matching: initialized to zero. A boolean
value is set to one if the hypothesis word appears in
the text. The final weight is calculated as the sum of
all boolean values and normalized dividing it by the
length of the hypothesis.
Levenshtein distance: it is similar to simple match-
ing. However, in this case we use the mentioned
distance as the similarity measure between words.
When the distance is zero, the increment value is
one. On the other hand, if such value is equal to one,
the increment is 0.9. Otherwise, it will be the inverse
of the obtained distance.
Consecutive subsequence matching: this measure
assigns the highest relevance to the appearance of
consecutive subsequences. In order to perform this,
we have generated all possible sets of consecutive
subsequences, from length two until the length in
words, from the text and the hypothesis. If we pro-
ceed as mentioned, the sets of length two extracted
from the hypothesis will be compared to the sets of
the same length from the text. If the same element is
present in both the text and the hypothesis set, then
a unit is added to the accumulated weight. This pro-
cedure is applied for all sets of different length ex-
tracted from the hypothesis. Finally, the sum of the
weight obtained from each set of a specific length is
normalized by the number of sets corresponding to
67
this length, and the final accumulated weight is also
normalized by the length of the hypothesis in words
minus one. This measure is defined as follows:
CSmatch =
|H|?
i=2
f(SHi)
|H| ? 1
(1)
where SHi contains the hypothesis? subsequences
of length i, and f(SHi) is defined as follows:
f(SHi) =
?
j?SHi
match(j)
|H| ? i+ 1
(2)
being match(j) equal to one if there exists an ele-
ment k that belongs to the set that contains the text?s
subsequences of length i, such that k = j.
One should note that this measure does not con-
sider non-consecutive subsequences. In addition, it
assigns the same relevance to all consecutive sub-
sequences with the same length. Furthermore, the
longer the subsequence is, the more relevant it will
be considered.
Tri-grams: two sets containing tri-grams of letters
belonging to the text and the hypothesis were cre-
ated. All the occurrences in the hypothesis? tri-
grams set that also appear in the text?s will increase
the accumulated weight in a factor of one unit. The
weight is normalized by the size of the hypothesis?
tri-grams set.
ROUGE measures: considering the impact of n-
gram overlap metrics in textual entailment, we be-
lieve that the idea of integrating these measures1 into
our system is very appealing. We have implemented
them as defined in (Lin, 2004).
Each measure is applied to the words, lemmas and
stems belonging to the text-hypothesis pair. Within
the entire set of measures, each one of them is con-
sidered as a feature for the training and test stages
of a machine learning algorithm. The selected one
was a Support Vector Machine due to the fact that its
properties are suitable for recognizing entailment.
2.3 Syntactic Module
The syntactic module we have built is composed of
few submodules that operate collaboratively in order
1The considered measures were ROUGE-N with n=2 and
n=3, ROUGE-L, ROUGE-W and ROUGE-S with s=2 and s=3.
to obtain the highest possible accuracy by using only
syntactic information.
The commitment of the first two submodules is
to generate an internal representation of the syntac-
tic dependency trees generated by MINIPAR (Lin,
1998). For this purpose we obtain the output of such
parser for the text-hypothesis pairs, and then process
it to generate an on-memory internal representation
of the mentioned trees. In order to reduce our sys-
tem?s noise and increase its accuracy rate, we only
keep the relevant words and discard the ones that we
believe do not provide useful information, such as
determinants and auxiliary verbs. After this step has
been performed we can proceed to compare the gen-
erated syntactic dependency trees of the text and the
hypothesis.
The graph node matching, termed alignment, be-
tween both the text and the hypothesis consists in
finding pairs of words in both trees whose lemmas
are identical, no matter whether they are in the same
position within the tree. Some authors have already
designed similar matching techniques, such as the
one described in (Snow et al, 2006). However, these
include semantic constraints that we have decided
not to consider. The reason of this decision is that we
desired to overcome the textual entailment recogni-
tion from an exclusively syntactic perspective. The
formula that provides the similarity rate between the
dependency trees of the text and the hypothesis in
our system, denoted by the symbol ?, is shown in
Equation 3:
?(?, ?) =
?
???
?(?) (3)
where ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively, and ? is the
set that contains all synsets present in both trees, be-
ing ? = ? ? ? ?? ? ?, ? ? ?. As we can observe in
Equation 3, ? depends on another function, denoted
by the symbol ?, which provides the relevance of
a synset. Such a weight factor will depend on the
grammatical category and relation of the synset. In
addition, we believe that the most relevant words of
a phrase occupy the highest positions in the depen-
dency tree, so we desired to assign different weights
depending on the depth of the synset. With all these
factors we define the relevance of a word as shown
68
in Equation 4:
?(?) = ? ? ? ? ???? (4)
where ? is a synset present in both ? and ?, ? rep-
resents the weight assigned to ??s grammatical cat-
egory (Table 1), ? the weight of ??s grammatical
relationship (Table 2), ? an empirically calculated
value that represents the weight difference between
tree levels, and ?? the depth of the node that contains
the synset ? in ?. The performed experiments reveal
that the optimal value for ? is 1.1.
Grammatical category Weight
Verbs, verbs with one argument, verbs with
two arguments, verbs taking clause as com-
plement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun modifiers 0.5
Verbs Have and Be 0.3
Table 1: Weights assigned to the relevant grammati-
cal categories.
Grammatical relationship Weight
Subject of verbs, surface subject, object of
verbs, second object of ditransitive verbs
1.0
The rest 0.5
Table 2: Weights assigned to the grammatical rela-
tionships.
We would like to point out that a requirement of
our system?s similarity measure is to be independent
of the hypothesis length. Therefore, we must de-
fine the normalized similarity rate, as represented in
Equation 5:
?(?, ?) =
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value has been calculated, it
will be provided to the user together with the cor-
responding text-hypothesis pair identifier. It will be
his responsibility to choose an appropriate threshold
that will represent the minimum similarity rate to be
considered as entailment between text and hypothe-
sis. All values that are under such a threshold will
be marked as not entailed.
3 System Evaluation
In order to evaluate our system we have generated
several results using different combinations of all
three mentioned modules. Since the lexical one uses
a machine learning algorithm, it has to be run within
a training environment. For this purpose we have
trained our system with the corpora provided in the
previous editions of RTE, and also with the develop-
ment corpus from the current RTE-3 challenge. On
the other hand, for the remainder modules the devel-
opment corpora was used to set the thresholds that
determine if the entailment holds.
The performed tests have been obtained by per-
forming different combinations of the described
modules. First, we have calculated the accuracy
rates using only each single module separately.
Later on we have combined those developed by our
research group for this year?s RTE challenge, which
are DLSITE-1 (the lexical one) and DLSITE-2 (the
syntactic one). Finally we have performed a voting
process between these two systems and the one we
presented in RTE-2.
The combination of DLSITE-1 and DLSITE-2 is
described as follows. If both modules agree, then the
judgement is straightforward, but if they do not, we
then decide the judgment depending on the accuracy
of each one for true and false entailment situations.
In our case, DLSITE-1 performs better while dealing
with negative examples, so its decision will prevail
over the rest. Regarding the combination of the three
approaches, we have developed a voting strategy.
The results obtained by our system are represented
in Table 3. As it is reflected in such table, the high-
est accuracy rate obtained using the RTE-3 test cor-
pus was achieved applying only the lexical module,
namely DLSITE-1. On the other hand, the syntac-
tic one had a significantly lower rate, and the same
happened with the system we presented in RTE-2.
Therefore, a combination of them will most likely
produce less accurate results than the lexical mod-
ule, as it is shown in Table 3. However, we would
like to point out that these results depend heavily on
the corpus idiosyncrasy. This can be proven with the
results obtained for the RTE-2 test corpus, where the
grouping of the three modules provided the highest
accuracy rates of all possible combinations.
69
RTE-2 test RTE-3 dev RTE-3 test
Overall Overall Overall IE IR QA SUM
RTE-2 system 0.5563 0.5523 0.5400 0.4900 0.6050 0.5100 0.5550
DLSITE-1 0.6188 0.7012 0.6563 0.5150 0.7350 0.7950 0.5800
DLSITE-2 0.6075 0.6450 0.5925 0.5050 0.6350 0.6300 0.6000
DLSITE-1&2 0.6212 0.6900 0.6375 0.5150 0.7150 0.7400 0.5800
Voting 0.6300 0.6900 0.6375 0.5250 0.7050 0.7200 0.6000
Table 3: Results obtained with the corpora from RTE-2 and RTE-3.
3.1 Results Analysis
We will now perform an analysis of the results
shown in the previous section. First, we would like
to mention the fact that our system does not be-
have correctly when it has to deal with long texts.
Roughly 11% and 13% of the false positives of
DLSITE-1 and DLSITE-2, respectively, are caused
by misinterpretations of long texts. The underlying
reason of these failures is the fact that it is easier to
find a lexical and syntactic match when a long text
is present in the pair, even if there is not entailment.
In addition, we consider very appealing to show
the accuracy rates corresponding to true and false
entailment pairs individually. Figure 2 represents the
mentioned rates for all system combinations that we
displayed in Table 3.
Figure 2: Accuracy rates obtained for true and false
entailments using the RTE-3 test corpus.
As we can see in Figure 2, the accuracy rates
for true and false entailment pairs vary significantly.
The modules we built for our participation in RTE-3
obtained high accuracy rates for true entailment text-
hypothesis pairs, but in contrast they behaved worse
in detecting false entailment pairs. This is the oppo-
site to the system we presented in RTE-2, since it has
a much higher accuracy rate for false cases than true
ones. When we combinedDLSITE-1 andDLSITE-2,
their accuracy rate for true entailments diminished,
although, on the other hand, the rate for false ones
raised. The voting between all three modules pro-
vided a higher accuracy rate for false entailments be-
cause the system we presented at RTE-2 performed
well in these cases.
Finally, we would like to discuss some examples
that lead to failures and correct forecasts by our two
new approaches.
Pair 246 entailment=YES task=IR
T: Overall the accident rate worldwide for commercial aviation
has been falling fairly dramatically especially during the period
between 1950 and 1970, largely due to the introduction of new
technology during this period.
H: Airplane accidents are decreasing.
Pair 246 is incorrectly classified by DLSITE-1
due to the fact that some words of the hypothesis do
not appear in the same manner in the text, although
they have similar meaning (e.g. airplane and
aviation). However, DLSITE-2 is able to establish a
true entailment for this pair, since the hypothesis?
syntactic dependency tree can be matched within the
text?s, and the similarity measure applied between
lemmas obtains a high score. This fact produces
that, in this case, the voting also achieves a correct
prediction for pair 246.
Pair 736 entailment=YES task=SUM
T: In a security fraud case, Michael Milken was sentenced to 10
years in prison.
H: Milken was imprisoned for security fraud.
Pair 736 is correctly classified by DLSITE-1 since
there are matches for all hypothesis? words (except
imprisoned) and some subsequences. In contrast,
DLSITE-2 does not behave correctly with this exam-
ple because the main verbs do not match, being this
fact a considerable handicap for the overall score.
70
4 Conclusions and Future Work
This research provides independent approaches con-
sidering mainly lexical and syntactic information. In
order to achieve this, we expose and analyze a wide
variety of lexical measures as well as syntactic struc-
ture comparisons that attempt to solve the textual en-
tailment recognition task. In addition, we propose
several combinations between these two approaches
and integrate them with our previous RTE-2 system
by using a voting strategy.
The results obtained reveal that, although the
combined approach provided the highest accuracy
rates for the RTE-2 corpora, it has not accom-
plished the expected reliability in the RTE-3 chal-
lenge. Nevertheless, in both cases the lexical-based
module achieved better results than the rest of the in-
dividual approaches, being the optimal for our par-
ticipation in RTE-3, and obtaining an accuracy rate
of about 70% and 65% for the development and test
corpus, respectively. One should note that these re-
sults depend on the idiosyncrasies of the RTE cor-
pora. However, these corpora are the most reliable
ones for evaluating textual entailment recognizers.
Future work can be related to the development
of a semantic module. Our system achieves good
lexical and syntactic comparisons between texts, but
we believe that we should take advantage of the se-
mantic resources in order to achieve higher accuracy
rates. For this purpose we plan to build a module
that constructs characterized representations based
on the text using named entities and role labeling in
order to extract semantic information from a text-
hypothesis pair. Another future research line could
consist in applying different recognition techniques
depending on the type of entailment task. We have
noticed that the accuracy of our approach differs
when the entailment is produced mainly by lexical
or syntactic implications. We intend to establish an
entailment typology and tackle each type by means
of different points of view or approaches.
Acknowledgments
This research has been partially funded by the
QALL-ME consortium, which is a 6th Framework
Research Programme of the European Union (EU),
contract number FP6-IST-033860 and by the Span-
ish Government under the project CICyT number
TIN2006-1526-C06-01. It has also been supported
by the undergraduate research fellowships financed
by the Spanish Ministry of Education and Science,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, pages 1?9.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Quin?onero-Candela et al, edi-
tors, MLCW 2005, LNAI Volume 3944, pages 177?190.
Springer-Verlag.
Oscar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic forms and wordnet relation-
ships to textual entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 22?26, Venice,
Italy.
Oren Glickman. 2005. Applied Textual Entailment Chal-
lenge. Ph.D. thesis, Bar Ilan University.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Stan Szpakow-
icz Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235?244.
Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.
2006. Detecting Entailment Using an Extended Imple-
mentation of the Basic Elements Overlap Metrics. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment, pages 122?
127, Venice, Italy.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American
Association of Computational Linguistics, New York
City, New York, United States of America.
71
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358?366,
Beijing, August 2010
A Large Scale Ranker-Based System  
for Search Query Spelling Correction 
 
Jianfeng Gao 
Microsoft Research, Redmond 
jfgao@microsoft.com 
Xiaolong Li 
Microsoft Corporation 
xiaolong.li@microsoft.com 
Daniel Micol 
Microsoft Corporation 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research, Redmond 
chrisq@microsoft.com 
Xu Sun 
University of Tokyo 
xusun@mist.i.u-tokyo.ac.jp 
 
 
Abstract 
This paper makes three significant extensions to a 
noisy channel speller designed for standard writ-
ten text to target the challenging domain of search 
queries. First, the noisy channel model is sub-
sumed by a more general ranker, which allows a 
variety of features to be easily incorporated. Se-
cond, a distributed infrastructure is proposed for 
training and applying Web scale n-gram language 
models. Third, a new phrase-based error model is 
presented. This model places a probability distri-
bution over transformations between multi-word 
phrases, and is estimated using large amounts of 
query-correction pairs derived from search logs. 
Experiments show that each of these extensions 
leads to significant improvements over the state-
of-the-art baseline methods. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods. New 
search queries emerge constantly. As a result, 
many queries contain valid search terms, such as 
proper nouns and names, which are not well es-
tablished in the language. Therefore, recent re-
search has focused on the use of Web corpora 
and search logs, rather than human-compiled lex-
icons, to infer knowledge about spellings and 
word usages in search queries (e.g., Whitelaw et 
al., 2009; Cucerzan and Brill, 2004).  
The spelling correction problem is typically 
formulated under the framework of the noisy 
channel model. Given an input query   
       , we want to find the best spelling correc-
tion           among all candidates: 
         
 
       (1) 
Applying Bayes' Rule, we have 
         
 
           (2) 
where the error model        models the trans-
formation probability from C to Q, and the lan-
guage model (LM)      models the likelihood 
that C is a correctly spelled query. 
This paper extends a noisy channel speller de-
signed for regular text to search queries in three 
ways: using a ranker (Section 3), using Web scale 
LMs (Section 4), and using phrase-based error 
models (Section 5). 
First of all, we propose a ranker-based speller 
that covers the noisy channel model as a special 
case. Given an input query, the system first gen-
erates a short list of candidate corrections using 
the noisy channel model. Then a feature vector is 
computed for each query and candidate correc-
tion pair. Finally, a ranker maps the feature vec-
tor to a real-valued score, indicating the likeli-
hood that this candidate is a desirable correction. 
We will demonstrate that ranking provides a flex-
ible modeling framework for incorporating a 
wide variety of features that would be difficult to 
model under the noisy channel framework. 
Second, we explore the use of Web scale LMs 
for query spelling correction. While traditional 
LM research focuses on how to make the model 
?smarter? via how to better estimate the probabil-
ity of unseen words (Chen and Goodman, 1999); 
and how to model the grammatical structure of 
language (e.g., Charniak, 2001), recent studies 
show that significant improvements can be 
achieved using ?stupid? n-gram models trained 
on very large corpora (e.g., Brants et al, 2007). 
We adopt the latter strategy in this study. We pre-
sent a distributed infrastructure to efficiently train 
and apply Web scale LMs. In addition, we ob-
serve that search queries are composed in a lan-
guage style different from that of regular text. We 
thus train multiple LMs using different texts as-
sociated with Web corpora and search queries. 
Third, we propose a phrase-based error model 
that captures the probability of transforming one 
358
multi-term phrase into another multi-term phrase. 
Compared to traditional error models that account 
for transformation probabilities between single 
characters or substrings (e.g., Kernighan et al, 
1990; Brill and Moore, 2000), the phrase-based 
error model is more effective in that it captures 
inter-term dependencies crucial for correcting 
real-word errors, prevalent in search queries. We 
also present a novel method of extracting large 
amounts of query-correction pairs from search 
logs. These pairs, implicitly judged by millions of 
users, are used for training the error models. 
Experiments show that each of the extensions 
leads to significant improvements over its base-
line methods that were state-of-the-art until this 
work, and that the combined method yields a sys-
tem which outperforms the noisy channel speller 
by a large margin: a 6.3% increase in accuracy on 
a human-labeled query set. 
2 Related Work 
Prior research on spelling correction for regular 
text can be grouped into two categories: correct-
ing non-word errors and real-word errors. The 
former focuses on the development of error mod-
els based on different edit distance functions (e.g., 
Kucich, 1992; Kernighan et al, 1990; Brill and 
Moore, 2000; Toutanova and Moore, 2002). Brill 
and Moore?s substring-based error model, con-
sidered to be state-of-the-art among these models, 
acts as the baseline against which we compare 
our models. On the other hand, real-word spelling 
correction tries to detect incorrect usages of a 
valid word based on its context, such as "peace" 
and "piece" in the context "a _ of cake". N-gram 
LMs and na?ve Bayes classifiers are commonly 
used models (e.g., Golding and Roth, 1996; 
Mangu and Brill, 1997; Church et al, 2007). 
While almost all of the spellers mentioned 
above are based on a pre-defined dictionary (ei-
ther a lexicon against which the edit distance is 
computed, or a set of real-word confusion pairs), 
recent research on query spelling correction fo-
cuses on exploiting noisy Web corpora and query 
logs to infer knowledge about spellings and word 
usag in queries (Cucerzan and Brill 2004; Ahmad 
and Kondrak, 2005; Li et al, 2006; Whitelaw et 
al., 2009).  Like those spellers designed for regu-
lar text, most of these query spelling systems are 
also based on the noisy channel framework. 
3 A Ranker-Based Speller 
The noisy channel model of Equation (2) does 
not have the flexibility to incorporate a wide va-
riety of features useful for spelling correction, 
e.g., whether a candidate appears as a Wikipedia 
document title. We thus generalize the speller to 
a ranker-based system. Let f be a feature vector 
of a query and candidate correction pair (Q, C). 
The ranker maps f to a real value y that indicates 
how likely C is a desired correction. For example, 
a linear ranker maps f to y with a weight vector w 
such as      , where w is optimized for accu-
racy on human-labeled       pairs. Since the 
logarithms of the LM and error model probabili-
ties can be included as features, the ranker covers 
the noisy channel model as a special case. 
For efficiency, our speller operates in two dis-
tinct stages: candidate generation and re-ranking. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. For each term 
q, we consult a lexicon to identify a list of 
spelling suggestions c whose edit distance from q 
is lower than some threshold. Our lexicon con-
tains around 430,000 high frequency query uni-
gram and bigrams collected from 1 year of query 
logs. These suggestions are stored in a lattice.  
We then use a decoder to identify the 20-best 
candidates from the lattice according to Equation 
(2), where the LM is a backoff bigram model 
trained on 1 year of query logs, and the error 
model is approximated by weighted edit distance:  
                         (3) 
The decoder uses a standard two-pass algorithm. 
The first pass uses the Viterbi algorithm to find 
the best C according to the model of Equations 
(2) and (3).  The second pass uses the A-star al-
gorithm to find the 20-best corrections, using the 
Viterbi scores computed at each state in the first 
pass as heuristics. 
The core component in the second stage is a 
ranker, which re-ranks the 20-best candidate cor-
rections using a set of features extracted from 
     . If the top C after re-ranking is different 
from Q, C is proposed as the correction. We use 
96 features in this study. In addition to the two 
features derived from the noisy channel model, 
the rest of the features can be grouped into the 
following 5 categories. 
1. Surface-form similarity features, which 
check whether C and Q differ in certain patterns, 
359
e.g., whether C is transformed from Q by adding 
an apostrophe, or by adding a stop word at the 
beginning or end of Q. 
2. Phonetic-form similarity features, which 
check whether the edit distance between the met-
aphones (Philips, 1990) of a query term and its 
correction candidate is below some thresholds. 
3. Entity features, which check whether the 
original query is likely to be a proper noun based 
on an in-house named entity recognizer. 
4. Dictionary features, which check whether 
a query term or a candidate correction are in one 
or more human-compiled dictionaries, such as the 
extracted Wiki, MSDN, and ODP dictionaries. 
5. Frequency features, which check whether 
the frequency of a query term or a candidate cor-
rection is above certain thresholds in different 
datasets, such as query logs and Web documents. 
4 Web Scale Language Models 
An n-gram LM assigns a probability to a word 
string   
            according to  
    
   ? (  |  
   )
 
   
 ? (  |      
   )
 
   
 (4) 
where the approximation is based on a Markov 
assumption that each word depends only upon the 
immediately preceding n-1 words. In a speller, 
the log of n-gram LM probabilities of an original 
query and its candidate corrections are used as 
features in the ranker.  
While recent research reports the benefits of 
large LMs trained on Web corpora on a variety of 
applications (e.g. Zhang et al, 2006; Brants et al, 
2007), it is also clear that search queries are com-
posed in a language style different from that of 
the body or title of a Web document. Thus, in this 
study we developed a set of large LMs from dif-
ferent text streams of Web documents and query 
logs. Below, we first describe the n-gram LM 
collection used in this study, and then present a 
distributed n-gram LM platform based on which 
these LMs are built and served for the speller. 
4.1 Web Scale Language Models 
Table 1 summarizes the data sets and Web scale 
n-gram LMs used in this study. The collection is 
built from high quality English Web documents 
containing trillions of tokens, served by a popular 
commercial search engine. The collection con-
sists of several data sets built from different Web 
sources, including the different text fields from 
the Web documents (i.e., body, title, and anchor 
texts) and search query logs. The raw texts ex-
tracted from these different sources were pre- 
processed in the following manner: texts are to-
kenized based on white-space and upper case let-
ters are converted to lower case. Numbers are 
retained, and no stemming/inflection is per-
formed. The n-gram LMs are word-based backoff 
models, where the n-gram probabilities are esti-
mated using Maximum Likelihood Estimation 
with smoothing. Specifically, for a trigram mod-
el, the smoothed probability is computed as 
                (5) 
{
               (             )
           
                   
                              
 
where      is the count of the n-gram in the train-
ing corpus and   is a normalization factor.      
is a discount function for smoothing. We use 
modified absolute discounting (Gao et al, 2001), 
whose parameters can be efficiently estimated 
and performance converges to that of more elabo-
rate state-of-the-art techniques like Kneser-Ney 
smoothing in large data (Nguyen et al 2007).  
4.2 Distributed N-gram LM Platform 
The platform is developed on a distributed com-
puting system designed for storing and analyzing 
massive data sets, running on large clusters con-
sisting of hundreds of commodity servers con-
nected via high-bandwidth network.  
We use the SCOPE (Structured Computations 
Optimized for Parallel Execution) programming 
model (Chaiken et al, 2008) to train the Web 
scale n-gram LMs shown in Table 1. The SCOPE 
scripting language resembles SQL which many 
programmers are familiar with. It also supports 
Dataset Body Anchor Title Query 
Total tokens 1.3T 11.0B 257.2B 28.1B 
Unigrams 1.2B 60.3M 150M 251.5M 
Bigrams 11.7B 464.1M 1.1B 1.3B 
Trigrams 60.0B 1.4B 3.1B 3.1B 
4-grams 148.5B 2.3B 5.1B 4.6B 
Size on disk# 12.8TB 183GB 395GB 393GB 
# N-gram entries as well as other model parameters are 
stored. 
Table 1: Statistics of the Web n-gram LMs collection (count 
cutoff = 0 for all models). These models will be accessible at 
Microsoft (2010). 
360
C# expressions so that users can easily plug-in 
customized C# classes. SCOPE supports writing 
a program using a series of simple data transfor-
mations so that users can simply write a script to 
process data in a serial manner without wonder-
ing how to achieve parallelism while the SCOPE 
compiler and optimizer are responsible for trans-
lating the script into an efficient, parallel execu-
tion plan. We illustrate the usage of SCOPE for 
building LMs using the following example of 
counting 5-grams from the body text of English 
Web pages. The flowchart is shown in Figure 1.  
The program is written in SCOPE as a step-
by- step of computation, where a command takes 
the output of the previous command as its input. 
ParsedDoc=SELECT docId, TokenizedDoc 
FROM @?/shares/?/EN_Body.txt? 
USING DefaultTextExtractor; 
NGram=PROCESS ParsedDoc 
PRODUCE NGram, NGcount 
USING NGramCountProcessor(-stream       
TokenizedDoc -order 5 ?bufferSize 
20000000); 
NGramCount=REDUCE NGram 
ON NGram 
PRODUCE NGram, NGcount 
USING NGramCountReducer; 
 
OUTPUT TO @?Body-5-gram-count.txt?; 
The first SCOPE command is a SELECT 
statement that extracts parsed Wed body text. The 
second command uses a build-in Processor 
(NGramCountProcessor) to map the parsed doc-
uments into separate n-grams together with their 
counts. It generates a local hash at each node 
(i.e., a core in a multi-core server) to store the (n-
gram, count) pairs. The third command (RE-
DUCE) aggregates counts from different nodes 
according to the key (n-gram string). The final 
command (OUTPUT) writes out the resulting to a 
data file. 
The smoothing method can be implemented 
similarly by the customized smoothing Proces-
sor/Reducer. They can be imported from the ex-
isting C# codes (e.g., developed for building LMs 
in a single machine) with minor changes.  
It is straightforward to apply the built LMs for 
the ranker in the speller. The n-gram platform 
provides a DLL for n-gram batch lookup. In the 
server, an n-gram LM is stored in the form of 
multiple lists of key-value pairs, where the key is 
the hash of an n-gram string and the value is ei-
ther the n-gram probability or backoff parameter.  
5 Phrase-Based Error Models 
The goal of an error model is to transform a cor-
rectly spelled query C into a misspelled query Q. 
Rather than replacing single words in isolation, 
the phrase-based error model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. The training pro-
cedure closely follows Sun et al (2010). For in-
stance, we might learn that ?theme part? can be 
replaced by ?theme park? with relatively high 
probability, even though ?part? is not a mis-
spelled word. We use this generative story: first 
the correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence 
q1, ?, qk, finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S de-
note the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as bi-
phrases. Finally, let M denote a permutation of K 
elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution 
over rewrite pairs. Let B(C, Q) denote the set of S, 
T, M triples that transform C into Q. Assuming a 
uniform probability over segmentations, the 
phrase-based probability can be defined as: 
Recursive 
Reducer
Node 1 Node 2 Node N?...
?...
Output
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
 
Figure 1. Distributed 5-gram counting. 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative procedure 
behind the phrase-based error model. 
361
       ?                    
            
 (6) 
As is common practice in SMT, we use the max-
imum approximation to the sum:  
          
            
                    (7) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs that will act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be identi-
fied with little ambiguity. Thus we restrict our 
attention to those phrase transformations con-
sistent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1?aJ  be a hidden variable representing 
the word alignment between them. Each ai takes 
on a value ranging from 1 to L indicating its cor-
responding word position in C, or 0 if the ith 
word in Q is unaligned. The cost of assigning k 
to ai is equal to the Levenshtein edit distance 
(Levenshtein, 1966) between the ith word in Q 
and the kth word in C, and the cost of assigning 0 
to ai is equal to the length of the i
th word in Q. 
The least cost alignment A* between Q and C is 
computed efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, 
which we denote as B(C, Q, A*). Here, consisten-
cy requires that if two words are aligned in A*, 
then they must appear in the same bi-phrase (ci, 
qi). Once the word alignment is fixed, the final 
permutation is uniquely determined, so we can 
safely discard that factor. Thus we have: 
          
       
       
         (8) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
         ?         
 
   , (9) 
where          is a phrase transformation prob-
ability, the estimation of which will be described 
in Section 5.2.  
To find the maximum probability assignment 
efficiently, we use a dynamic programming ap-
proach, similar to the monotone decoding algo-
rithm described in Och (2002).  
5.2 Training the Error Model  
Given a set of (Q, C) pairs as training data, we 
follow a method commonly used in SMT (Och 
and Ney, 2004) to extract bi- phrases and esti-
mate their replacement probabilities. A detailed 
description is discussed in Sun et al (2010). 
We now describe how (Q, C) pairs are gener-
ated automatically from massive query reformu-
lation sessions of a commercial Web browser. 
A query reformulation session contains a list 
of URLs that record user behaviors that relate to 
the query reformulation functions, provided by a 
Web search engine. For example, most commer-
cial search engines offer the "did you mean" 
function, suggesting a possible alternate interpre-
tation or spelling of a user-issued query. Figure 3 
shows a sample of the query reformulation ses-
sions that record the "did you mean" sessions 
from three of the most popular search engines. 
These sessions encode the same user behavior: A 
user first queries for "harrypotter sheme part", 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+part&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+part& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+part&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 3.  A sample of query reformulation sessions from 3 
popular search engines. These sessions show that a user first 
issues the query "harrypotter sheme part", and then clicks on 
the resulting spell suggestion "harry potter theme park". 
362
and then clicks on the resulting spelling sugges-
tion "harry potter theme park". We can "reverse-
engineer" the parameters from the URLs of these 
sessions, and deduce how each search engine en-
codes both a query and the fact that a user arrived 
at a URL by clicking on the spelling suggestion 
of the query ? an strong indication that the 
spelling suggestion is desired. In this study, from 
1 year of sessions, we extracted ~120 million 
pairs. We found the data set very clean because 
these spelling corrections are actually clicked, 
and thus judged implicitly, by many users. 
In addition to the "did you mean" functionali-
ty, recently some search engines have introduced 
two new spelling suggestion functions. One is the 
"auto-correction" function, where the search en-
gine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results. The other is the "split 
pane" result page, where one half portion of the 
search results are produced using the original 
query, while the other half, usually visually sepa-
rate portion of results, are produced using the 
auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the session data, is already able to 
correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of un-
derestimating the self-transformation probability 
of a query P(Q2=Q1|Q1), because we only includ-
ed in the training data the pairs where the query is 
different from the correction. To deal with this 
problem, we augmented the training data by in-
cluding correctly spelled queries, i.e., the pairs 
(Q1, Q2) where Q1 = Q2.  First, we extracted a set 
of queries from the sessions where no spell sug-
gestion is presented or clicked on. Second, we 
removed from the set those queries that were rec-
ognized as being auto-corrected by a search en-
gine. We do so by running a sanity check of the 
queries against our baseline noisy channel 
speller, which will be described in Section 6. If 
the system consider a query misspelled, we as-
sumed it an obvious misspelling, and removed it. 
The remaining queries were assumed to be cor-
rectly spelled and were added to the training data. 
6 Experiments 
We perform the evaluation using a manually an-
notated data set containing 24,172 queries sam-
pled from one year?s query logs from a commer-
cial search engine. The spelling of each query is 
manually corrected by four independent annota-
tors. The average length of queries in the data 
sets is 2.7 words. We divided the data set into 
non-overlapped training and test data sets. The 
training data contain 8,515       pairs, among 
which 1,743 queries are misspelled (i.e.    ). 
The test data contain 15,657       pairs, among 
which 2,960 queries are misspelled.  
The speller systems we developed in this 
study are evaluated using the following metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, a t-test 
with a significance level of 0.05. 
In our experiments, all the speller systems are 
ranker-based. Unless otherwise stated, the ranker 
is a two-layer neural net with 5 hidden nodes. 
The free parameters of the neural net are trained 
to optimize accuracy on the training data using 
the back propagation algorithm (Burges et al, 
2005) .  
6.1 System Results 
Table 1 summarizes the main results of different 
spelling systems. Row 1 is the baseline speller 
where the noisy channel model of Equations (2) 
363
and (3) is used. The error model is based on the 
weighted edit distance function and the LM is a 
backoff bigram model trained on 1 year of query 
logs, with count cutoff 30. Row 2 is the speller 
using a linear ranker to incorporate all ranking 
features described in Section 3. The weights of 
the linear ranker are optimized using the Aver-
aged Perceptron algorithm (Freund and Schapire, 
1999). Row 3 is the speller where a nonlinear 
ranker (i.e., 2-layer neural net) is trained atop the 
features. Rows 4, 5 and 6 are systems that incor-
porate the additional features derived from the 
phrase-based error model (PBEM) described in 
Section 5 and the four Web scale LMs (WLMs) 
listed in Table 1. 
The results show that (1) the ranker is a very 
flexible modeling framework where a variety of 
fine-grained features can be easily incorporated, 
and a ranker-based speller outperforms signifi-
cantly (p < 0.01) the traditional system based on 
the noisy channel model (Row 2 vs. Row 1); (2) 
the speller accuracy can be further improved by 
using more sophisticated rankers and learning 
algorithms (Row 3 vs. Row 2); (3) both WLMs 
and PBEM bring significant improvements 
(Rows 4 and 5 vs. Row 3); and (4) interestingly, 
the gains from WLMs and PBEM are additive 
and the combined leads to a significantly better 
speller (Row 6 vs. Rows 4 and 5) than that of 
using either of them individually. 
In what follows, we investigate in detail how 
the WLMs and PBEM trained on massive Web 
content and search logs improve the accuracy of 
the speller system. We will compare our models 
with the state-of-the-art models proposed previ-
ously. From now on, the system listed in Row 3 
of Table 1 will be used as baseline. 
6.2 Language Models 
The quality of n-gram LMs depends on the order 
of the model, the size of the training data, and 
how well the training data match the test data. 
Figure 4 illustrates the perplexity results of the 
four LMs trained on different data sources tested 
on a random sample of 733,147 queries. The re-
sults show that (1) higher order LMs produce 
lower perplexities, especially when moving be-
yond unigram models; (2) as expected, the query 
LMs are most predictive for the test queries, 
though they are from independent query log 
snapshots; (3) although the body LMs are trained 
on much larger amounts of data than the title and 
anchor LMs, the former lead to much higher per-
plexity values, indicating that both title and an-
chor texts are quantitatively much more similar to 
queries than body texts. 
Table 2 summarizes the spelling results using 
different LMs. For comparison, we also built a 4-
gram LM using the Google 1T web 5-gram cor-
pus (Brants and Franz, 2006). This model is re-
ferred to as the G1T model, and is trained using 
the ?stupid backoff? smoothing method (Brants et 
al., 2007). Due to the high count cutoff applied 
by the Google corpus (i.e., n-grams must appear 
at least 40 times to be included in the corpus), we 
found the G1T model results to a higher OOV 
rate (i.e., 6.5%) on our test data than that of the 4 
Web scale LMs (i.e., less than 1%). 
The results in Table 2 are more or less con-
sistent with the perplexity results: the query LM 
is the best performer; there is no significant dif-
ference among the body, title and anchor LMs 
though the body LM is trained on a much larger 
amount of data; and all the 4 Web scale LMs out-
perform the G1T model substantially due to the 
significantly lower OOV rates. 
6.3 Error Models 
This section compares the phrase-based error 
model (PBEM) described in Section 5, with one 
of the state-of-the-art error models, proposed by 
Brill and Moore (2000), henceforth referred to as 
# System Accuracy Precision Recall 
1 Noisy channel 85.3 72.1 35.9 
2 Linear ranker 88.0 74.0 42.8 
3 Nonlinear ranker 89.0 74.1 49.6 
4 3 + PBEM 90.7 78.7 58.2 
5 3 + WLMs 90.4 75.1 58.7 
6 3 + PBEM + WLMs  91.6 79.1 63.9 
Table 1. Summary of spelling correction results. 
 
Figure 4. Perplexity results on test queries, using n-
gram LMs with different orders, derived from differ-
ent data sources. 
 
364
the B&M model. B&M is a substring error mod-
el. It estimates        as 
          
    
           
?        
   
   
  (10) 
where R is a partitioning of correction term c into 
adjacent substrings, and T is a partitioning of 
query term q, such that |T|=|R|. The partitions are 
thus in one-to-one alignment. To train the B&M 
model, we extracted 1 billion term-correction 
pairs       from the set of 120 million query-
correction pairs      , derived from the search 
logs as described in Section 5.2.  
Table 3 summarizes the comparison results. 
Rows 1 and 2 are our ranker-based baseline sys-
tems with and without the error model (EM) fea-
ture. The error model is based on weighted edit 
distance of Eq. (3), where the weights are learned 
on some manually annotated word-correction 
pairs (which is not used in this study). Rows 3 
and 4 are the B&M models using different maxi-
mum substring lengths, specified by L. L=1 re-
duces B&M to the weighted edit distance model 
in Row 2. Rows 5 and 6 are PBEMs with differ-
ent maximum phrase lengths. L=1 reduces PBEM 
to a word-based error model. The results show 
the benefits of capturing context information in 
error models. In particular, the significant im-
provements resulting from PBEM demonstrate 
that the dependencies between words are far 
more effective than that between characters 
(within a word) for spelling correction. This is 
largely due to the fact that there are many real-
word spelling errors in search queries. We also 
notice that PBEM is a more powerful model  than   
# # of word pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 1M 89.15 73.71 50.74 
3 10M 89.22 74.11 50.92 
4 100M 89.20 73.60 51.06 
5 1B 89.21 73.72 50.99 
Table 4. The performance of B&M error model (L=3) as a 
function of the size of training data (# of word pairs). 
# # of (Q, C) pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 5M 89.59 77.01 52.34 
3 15M 90.23 77.87 56.67 
4 45M 90.45 78.56 57.02 
5 120M 90.70 78.49 58.12 
Table 5. The performance of PBEM (L=3) as a function of 
the size of training data (# of (Q, C) pairs). 
B&M in that it can benefit more from increasing-
ly larger training data. As shown in Tables 4 and 
5, whilst the performance of B&M saturates 
quickly with the increase of training data, the per-
formance of PBEM does not appear to have 
peaked ? further improvements are likely given a 
larger data set. 
7 Conclusions and Future Work 
This paper explores the use of massive Web cor-
pora and search logs for improving a ranker- 
based search query speller. We show significant 
improvements over a noisy channel speller using 
fine-grained features, Web scale LMs, and a 
phrase-based error model that captures intern- 
word dependencies. There are several techniques 
we are exploring to make further improvements. 
First, since a query speller is developed for im-
proving the Web search results, it is natural to use 
features from search results in ranking, as studied 
in Chen et al (2007). The challenge is efficiency. 
Second, in addition to query reformulation ses-
sions, we are exploring other search logs from 
which we might extract more       pairs for er-
ror model training. One promising data source is 
clickthrough data (e.g., Agichtein et al 2006; 
Gao et al, 2009). For instance, we might try to 
learn a transformation from the title or anchor 
text of a document to the query that led to a click 
on that document. Finally, the phrase-based error 
model is inspired by phrase-based SMT systems. 
We are introducing more SMT techniques such 
as alignment and translation rule exaction. In a 
broad sense, spelling correction can be viewed as 
a monolingual MT problem where we translate 
bad English queries into good ones. 
# System Accuracy Precision Recall 
1 Baseline 89.0 74.1 49.6 
2 1+ query 4-gram 90.1 75.6 56.3 
3 1 + body 4-gram 89.9 75.7 54.4 
4 1 + title 4-gram 89.8 75.4 54.7 
5 1 + anchor 4-gram 89.9 75.1 55.6 
6 1 + G1T 4-gram 89.4 75.1 51.5 
Table 2. Spelling correction results using different LMs 
trained on different data sources. 
# System Accuracy Precision Recall 
1 Baseline w/o EM 88.6 72.0 47.0 
2 Baseline 89.0 74.1 49.6 
3 1 + B&M, L=1 89.0 73.3 50.1 
4 1 + B&M, L=3 89.2 73.7 51.0 
5 1 + PBEM, L=1 90.1 76.7 55.6 
6 1 + PBEM, L=3 90.7 78.5 58.1 
Table 3. Spelling correction results using different error 
models. 
365
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Kuansan Wang for the 
very helpful discussions and collaboration. The 
work was done when Xu Sun was visiting Mi-
crosoft Research Redmond. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Improv-
ing web search ranking by incorporating user be-
havior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a spelling 
error model from search query logs. In HLT-
EMNLP, pp. 955-962. 
Brants, T., and Franz, A. 2006. Web 1T 5-gram corpus 
version 1.1. Technical report, Google Research. 
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. 
2007. Large language models in machine translation. 
In EMNLP-CoNLL, pp. 858 - 867. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In ACL, 
pp. 286-293. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In ICML, 
pp. 89-96.  
 Chaiken, R., Jenkins, B., Larson, P., Ramsey, B., 
Shakib, D., Weaver, S., and Zhou, J. 2008. SCOPE: 
easy and efficient parallel processing f massive data 
sets. In Proceedings of the VLDB Endowment, pp. 
1265-1276. 
Charniak, E. 2001. Immediate-head parsing for lan-
guage models. In ACL/EACL, pp. 124-131. 
Chen, S. F., and Goodman, J. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13(10):359-
394. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving que-
ry spelling correction using web search results. In 
EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compressing 
trigram language models with Golomb coding. In 
EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction as 
an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Freund, Y. and Schapire, R. E. 1999. Large margin 
classification using the perceptron algorithm. In 
Machine Learning, 37(3): 277-296. 
Gao, J., Goodman, J., and Miao, J. 2001. The use of 
clustering techniques for language modeling -
application to Asian languages. Computational Lin-
guistics and Chinese Language Processing, 
6(1):27?60, 2001.  
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web search 
ranking. In SIGIR, pp. 355-362.  
Golding, A. R., and Roth, D. 1996. Applying winnow 
to context-sensitive spelling correction. In ICML, pp. 
182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 127-
133. 
Kucich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Surveys, 
24(4):377-439. 
Levenshtein, V. I. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet 
Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. Ex-
ploring distributional similarity based models for 
query spelling correction. In ACL, pp. 1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule acquisi-
tion for spelling correction. In ICML, pp. 187-194. 
Microsoft Microsoft web n-gram services. 2010. 
http://research.microsoft.com/web-ngram 
Nguyen, P., Gao, J., and Mahajan, M. 2007. MSRLM: 
a scalable language modeling toolkit. Technical re-
port TR-2007-144, Microsoft Research. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Philips, L. 1990. Hanging on the metaphone. Comput-
er Language Magazine, 7(12):38-44. 
Sun, X., Gao, J., Micol, D., and Quirk, C. 2010. 
Learning phrase-based spelling error models from 
clickthrough data. In ACL.  
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In ACL, 
pp. 144-151.  
Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis, 
G. 2009. Using the web for language independent 
spellchecking and autocorrection. In EMNLP, pp. 
890-899. 
Zhang, Y., Hildebrand, Al. S., and Vogel, S. 2006. 
Distributed language modeling for n-best list re-
ranking. In EMNLP, pp. 216-233. 
366
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 266?274,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
  Learning Phrase-Based Spelling Error Models  
from Clickthrough Data 
 
 
 
Xu Sun? 
Dept. of Mathematical Informatics 
University of Tokyo, Tokyo, Japan 
xusun@mist.i.u-tokyo.ac.jp
Jianfeng Gao 
Microsoft Research 
Redmond, WA, USA 
jfgao@microsoft.com 
 
Daniel Micol 
Microsoft Corporation 
Munich, Germany 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research 
Redmond, WA, USA 
chrisq@microsoft.com 
 
                                                     
? The work was done when Xu Sun was visiting Microsoft Research Redmond. 
Abstract 
This paper explores the use of clickthrough data 
for query spelling correction. First, large amounts 
of query-correction pairs are derived by analyzing 
users' query reformulation behavior encoded in 
the clickthrough data. Then, a phrase-based error 
model that accounts for the transformation 
probability between multi-term phrases is trained 
and integrated into a query speller system. Expe-
riments are carried out on a human-labeled data 
set. Results show that the system using the 
phrase-based error model outperforms signifi-
cantly its baseline systems. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods for three 
main reasons (Ahmad and Kondrak, 2004).  First, 
spelling errors are more common in search queries 
than in regular written text: roughly 10-15% of 
queries contain misspelled terms (Cucerzan and 
Brill, 2004). Second, most search queries consist 
of a few key words rather than grammatical sen-
tences, making a grammar-based approach inap-
propriate. Most importantly, many queries con-
tain search terms, such as proper nouns and names, 
which are not well established in the language. 
For example, Chen et al (2007) reported that 
16.5% of valid search terms do not occur in their 
200K-entry spelling lexicon. 
Therefore, recent research has focused on the 
use of Web corpora and query logs, rather than 
human-compiled lexicons, to infer knowledge 
about misspellings and word usage in search 
queries (e.g., Whitelaw et al, 2009). Another 
important data source that would be useful for this 
purpose is clickthrough data. Although it is 
well-known that clickthrough data contain rich 
information about users' search behavior, e.g., 
how a user (re-) formulates a query in order to 
find the relevant document, there has been little 
research on exploiting the data for the develop-
ment of a query speller system. 
In this paper we present a novel method of 
extracting large amounts of query-correction pairs 
from the clickthrough data.  These pairs, impli-
citly judged by millions of users, are used to train 
a set of spelling error models. Among these 
models, the most effective one is a phrase-based 
error model that captures the probability of 
transforming one multi-term phrase into another 
multi-term phrase. Comparing to traditional error 
models that account for transformation probabili-
ties between single characters (Kernighan et al, 
1990) or sub-word strings (Brill and Moore, 
2000), the phrase-based model is more powerful 
in that it captures some contextual information by 
retaining inter-term dependencies. We show that 
this information is crucial to detect the correction 
of a query term, because unlike in regular written 
text, any query word can be a valid search term 
and in many cases the only way for a speller 
system to make the judgment is to explore its 
usage according to the contextual information. 
We conduct a set of experiments on a large 
data set, consisting of human-labeled 
266
query-correction pairs. Results show that the error 
models learned from clickthrough data lead to 
significant improvements on the task of query 
spelling correction. In particular, the speller sys-
tem incorporating a phrase-based error model 
significantly outperforms its baseline systems. 
To the best of our knowledge, this is the first 
extensive study of learning phase-based error 
models from clickthrough data for query spelling 
correction. The rest of the paper is structured as 
follows. Section 2 reviews related work. Section 3 
presents the way query-correction pairs are ex-
tracted from the clickthrough data. Section 4 
presents the baseline speller system used in this 
study. Section 5 describes in detail the phrase- 
based error model. Section 6 presents the expe-
riments. Section 7 concludes the paper. 
2 Related Work 
Spelling correction for regular written text is a 
long standing research topic. Previous researches 
can be roughly grouped into two categories: 
correcting non-word errors and real-word errors. 
In non-word error spelling correction, any 
word that is not found in a pre-compiled lexicon is 
considered to be misspelled.  Then, a list of lexical 
words that are similar to the misspelled word are 
proposed as candidate spelling corrections. Most 
traditional systems use a manually tuned similar-
ity function (e.g., edit distance function) to rank 
the candidates, as reviewed by Kukich (1992). 
During the last two decades, statistical error 
models learned on training data (i.e., 
query-correction pairs) have become increasingly 
popular, and have proven more effective (Ker-
nighan et al, 1990; Brill and Moore, 2000; Tou-
tanova and Moore, 2002; Okazaki et al, 2008).  
Real-word spelling correction is also referred 
to as context sensitive spelling correction (CSSC). 
It tries to detect incorrect usages of a valid word 
based on its context, such as "peace" and "piece" 
in the context "a _ of cake". A common strategy in 
CSSC is as follows. First, a pre-defined confusion 
set is used to generate candidate corrections, then 
a  scoring model, such as a trigram language 
model or na?ve Bayes classifier, is used to rank the 
candidates according to their context (e.g., 
Golding and Roth, 1996; Mangu and Brill, 1997; 
Church et al, 2007). 
When designed to handle regular written text, 
both CSSC and non-word error speller systems 
rely on a pre-defined vocabulary (i.e., either a 
lexicon or a confusion set). However, in query 
spelling correction, it is impossible to compile 
such a vocabulary, and the boundary between the 
non-word and real-word errors is quite vague. 
Therefore, recent research on query spelling 
correction has focused on exploiting noisy Web 
data and query logs to infer knowledge about 
misspellings and word usage in search queries. 
Cucerzan and Brill (2004) discuss in detail the 
challenges of query spelling correction, and 
suggest the use of query logs. Ahmad and Kon-
drak (2005) propose a method of estimating an 
error model from query logs using the EM algo-
rithm. Li et al (2006) extend the error model by 
capturing word-level similarities learned from 
query logs. Chen et al (2007) suggest using web 
search results to improve spelling correction. 
Whitelaw et al (2009) present a query speller 
system in which both the error model and the 
language model are trained using Web data. 
Compared to Web corpora and query logs, 
clickthrough data contain much richer informa-
tion about users? search behavior.  Although there 
has been a lot of research on using clickthrough 
data to improve Web document retrieval (e.g., 
Joachims, 2002; Agichtein et al, 2006; Gao et al, 
2009), the data have not been fully explored for 
query spelling correction. This study tries to learn 
error models from clickthrough data. To our 
knowledge, this is the first such attempt using 
clickthrough data. 
Most of the speller systems reviewed above are 
based on the framework of the source channel 
model. Typically, a language model (source 
model) is used to capture contextual information, 
while an error model (channel model) is consi-
dered to be context free in that it does not take into 
account any contextual information in modeling 
word transformation probabilities. In this study 
we argue that it is beneficial to capture contextual 
information in the error model. To this end, in-
spired by the phrase-based statistical machine 
translation (SMT) systems (Koehn et al, 2003; 
Och and Ney, 2004), we propose a phrase-based 
error model where we assume that query spelling 
correction is performed at the phrase level. 
In what follows, before presenting the phrase- 
based error model, we will first describe the 
clickthrough data and the query speller system we 
used in this study. 
3 Clickthrough Data and Spelling Cor-
rection 
This section describes the way the 
query-correction pairs are extracted from click-
267
through data. Two types of clickthrough data are 
explored in our experiment. 
The clickthrough data of the first type has been 
widely used in previous research and proved to be 
useful for Web search (Joachims, 2002; Agichtein 
et al, 2006; Gao et al, 2009) and query refor-
mulation (Wang and Zhai, 2008; Suzuki et al, 
2009). We start with this same data with the hope 
of achieving similar improvements in our task. 
The data consist of a set of query sessions that 
were extracted from one year of log files from a 
commercial Web search engine. A query session 
contains a query issued by a user and a ranked list 
of links (i.e., URLs) returned to that same user 
along with records of which URLs were clicked. 
Following Suzuki et al (2009), we extract 
query-correction pairs as follows. First, we extract 
pairs of queries Q1 and Q2 such that (1) they are 
issued by the same user; (2) Q2 was issued within 
3 minutes of Q1; and (3) Q2 contained at least one 
clicked URL in the result page while Q1 did not 
result in any clicks.  We then scored each query 
pair (Q1, Q2) using the edit distance between Q1 
and Q2, and retained those with an edit distance 
score lower than a pre-set threshold as query 
correction pairs. 
Unfortunately, we found in our experiments 
that the pairs extracted using the method are too 
noisy for reliable error model training, even with a 
very tight threshold, and we did not see any sig-
nificant improvement. Therefore, in Section 6 we 
will not report results using this dataset. 
The clickthrough data of the second type con-
sists of a set of query reformulation sessions 
extracted from 3 months of log files from a 
commercial Web browser.  A query reformulation 
session contains a list of URLs that record user 
behaviors that relate to the query reformulation 
functions, provided by a Web search engine. For 
example, almost all commercial search engines 
offer the "did you mean" function, suggesting a 
possible alternate interpretation or spelling of a 
user-issued query. Figure 1 shows a sample of the 
query reformulation sessions that record the "did 
you mean" sessions from three of the most pop-
ular search engines. These sessions encode the 
same user behavior: A user first queries for 
"harrypotter sheme park", and then clicks on the 
resulting spelling suggestion "harry potter theme 
park". In our experiments, we "reverse-engineer" 
the parameters from the URLs of these sessions, 
and deduce how each search engine encodes both 
a query and the fact that a user arrived at a URL 
by clicking on the spelling suggestion of the query 
? an important indication that the spelling sug-
gestion is desired. From these three months of 
query reformulation sessions, we extracted about 
3 million query-correction pairs. Compared to the 
pairs extracted from the clickthrough data of the 
first type (query sessions), this data set is much 
cleaner because all these spelling corrections are 
actually clicked, and thus judged implicitly, by 
many users. 
In addition to the "did you mean" function, 
recently some search engines have introduced two 
new spelling suggestion functions. One is the 
"auto-correction" function, where the search 
engine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results for the user.  The other is 
the "split pane" result page, where one half por-
tion of the search results are produced using the 
original query, while the other half, usually vi-
sually separate portion of results are produced 
using the auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+park&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+park& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+park&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 1.  A sample of query reformulation sessions 
from three popular search engines. These sessions 
show that a user first issues the query "harrypotter 
sheme park", and then clicks on the resulting spell 
suggestion "harry potter theme park". 
268
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the clickthrough data, is already able 
to correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of 
underestimating the self-transformation probabil-
ity of a query P(Q2=Q1|Q1), because we only 
included in the training data the pairs where the 
query is different from the correction. To deal 
with this problem, we augmented the training data 
by including correctly spelled queries, i.e., the 
pairs (Q1, Q2) where Q1 = Q2.  First, we extracted a 
set of queries from the sessions where no spell 
suggestion is presented or clicked on. Second, we 
removed from the set those queries that were 
recognized as being auto-corrected by a search 
engine. We do so by running a sanity check of the 
queries against our baseline spelling correction 
system, which will be described in Section 6. If 
the system thinks an input query is misspelled, we 
assumed it was an obvious misspelling, and re-
moved it. The remaining queries were assumed to 
be correctly spelled and were added to the training 
data. 
4 The Baseline Speller System 
The spelling correction problem is typically 
formulated under the framework of the source 
channel model. Given an input query ? ?
??. . . ??, we want to find the best spelling correc-
tion ? ? ??. . . ??  among all candidate spelling 
corrections: 
?? ? argmax
?
???|?? (1) 
Applying Bayes' Rule and dropping the constant 
denominator, we have 
?? ? argmax
?
???|?????? (2) 
where the error model ???|?? models the trans-
formation probability from C to Q, and the lan-
guage model ????  models how likely C is a 
correctly spelled query. 
The speller system used in our experiments is 
based on a ranking model (or ranker), which can 
be viewed as a generalization of the source 
channel model. The system consists of two 
components: (1) a candidate generator, and (2) a 
ranker. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. Then we scan 
the query from left to right, and each query term q 
is looked up in lexicon to generate a list of spel-
ling suggestions c whose edit distance from q is 
lower than a preset threshold. The lexicon we 
used contains around 430,000 entries; these are 
high frequency query terms collected from one 
year of search query logs. The lexicon is stored 
using a trie-based data structure that allows effi-
cient search for all terms within a maximum edit 
distance. 
The set of all the generated spelling sugges-
tions is stored using a lattice data structure, which 
is a compact representation of exponentially many 
possible candidate spelling corrections. We then 
use a decoder to identify the top twenty candi-
dates from the lattice according to the source 
channel model of Equation (2).  The language 
model (the second factor) is a backoff bigram 
model trained on the tokenized form of one year 
of query logs, using maximum likelihood estima-
tion with absolute discounting smoothing.  The 
error model (the first factor) is approximated by 
the edit distance function as 
?log???|?? ? EditDist??, ?? (3) 
The decoder uses a standard two-pass algorithm 
to generate 20-best candidates. The first pass uses 
the Viterbi algorithm to find the best C according 
to the model of Equations (2) and (3).  In the 
second pass, the A-Star algorithm is used to find 
the 20-best corrections, using the Viterbi scores 
computed at each state in the first pass as heuris-
tics. Notice that we always include the input query 
Q in the 20-best candidate list. 
The core of the second component of the 
speller system is a ranker, which re-ranks the 
20-best candidate spelling corrections. If the top 
C after re-ranking is different than the original 
query Q, the system returns C as the correction.   
Let f be a feature vector extracted from a query 
and candidate spelling correction pair (Q, C). The 
ranker maps f to a real value y that indicates how 
likely C is a desired correction of Q.  For example, 
a linear ranker simply maps f to y with a learned 
weight vector w such as ? ? ? ? ?, where w is 
optimized w.r.t. accuracy on a set of hu-
269
man-labeled (Q, C) pairs. The features in f are 
arbitrary functions that map (Q, C) to a real value. 
Since we define the logarithm of the probabilities 
of the language model and the error model (i.e., 
the edit distance function) as features, the ranker 
can be viewed as a more general framework, 
subsuming the source channel model as a special 
case. In our experiments we used 96 features and a 
non-linear model, implemented as a two-layer 
neural net, though the details of the ranker and the 
features are beyond the scope of this paper. 
5 A Phrase-Based Error Model 
The goal of the phrase-based error model is to 
transform a correctly spelled query C into a 
misspelled query Q. Rather than replacing single 
words in isolation, this model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. For instance, we 
might learn that ?theme part? can be replaced by 
?theme park? with relatively high probability, 
even though ?part? is not a misspelled word. We 
assume the following generative story: first the 
correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence q1, 
?, qk, and finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S 
denote the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as 
bi-phrases. Finally, let M denote a permutation of 
K elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution over 
rewrite pairs. Let B(C, Q) denote the set of S, T, M 
triples that transform C into Q. If we assume a 
uniform probability over segmentations, then the 
phrase-based probability can be defined as: 
???|?? ? ? ???|?, ?? ? ???|?, ?, ??
??,?,???
???,??
 (4) 
As is common practice in SMT, we use the 
maximum approximation to the sum:  
???|?? ? max
??,?,???
???,??
???|?, ?? ? ???|?, ?, ?? (5) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs which act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be iden-
tified with little ambiguity. Thus we restrict our 
attention to those phrase transformations consis-
tent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1, ?, aJ be a hidden variable 
representing the word alignment. Each ai takes on 
a value ranging from 1 to L indicating its corres-
ponding word position in C, or 0 if the ith word in 
Q is unaligned. The cost of assigning k to ai is 
equal to the Levenshtein edit distance (Levensh-
tein, 1966) between the ith word in Q and the kth 
word in C, and the cost of assigning 0 to ai is equal 
to the length of the ith word in Q. We can deter-
mine the least cost alignment A* between Q and C 
efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, which 
we denote as B(C, Q, A*). Here, consistency re-
quires that if two words are aligned in A*, then 
they must appear in the same bi-phrase (ci, qi). 
Once the word alignment is fixed, the final per-
mutation is uniquely determined, so we can safely 
discard that factor. Thus we have: 
???|?? ? max
??,?,???
???,?,???
???|?, ?? (6) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative 
procedure behind the phrase-based error model. 
 
270
???|?, ?? ? ? ????|???
?
??? , (7) 
where ????|???  is a phrase transformation 
probability, the estimation of which will be de-
scribed in Section 5.2.  
To find the maximum probability assignment 
efficiently, we can use a dynamic programming 
approach, somewhat similar to the monotone 
decoding algorithm described in Och (2002). 
Here, though, both the input and the output word 
sequences are specified as the input to the algo-
rithm, as is the word alignment. We define the 
quantity ?? to be the probability of the most likely 
sequence of bi-phrases that produce the first j 
terms of Q and are consistent with the word 
alignment and C. It can be calculated using the 
following algorithm: 
1. Initialization:  
 ?? ? 1 (8) 
2. Induction:  
 ?? ? max
????,???
????
???
??
?????????? (9) 
3. Total:   
 ???|?? ? ?? (10) 
The pseudo-code of the above algorithm is 
shown in Figure 3. After generating Q from left to 
right according to Equations (8) to (10), we record 
at each possible bi-phrase boundary its maximum 
probability, and we obtain the total probability at 
the end-position of Q. Then, by back-tracking the 
most probable bi-phrase boundaries, we obtain B*.  
The algorithm takes a complexity of O(KL2), 
where K is the total number of word alignments in 
A* which does not contain empty words, and L is 
the maximum length of a bi-phrase, which is a 
hyper-parameter of the algorithm. Notice that 
when we set L=1, the phrase-based error model is 
reduced to a word-based error model which as-
sumes that words are transformed independently 
from C to Q, without taking into account any 
contextual information. 
 
5.2 Model Estimation 
We follow a method commonly used in SMT 
(Koehn et al, 2003) to extract bi-phrases and 
estimate their replacement probabilities.  From 
each query-correction pair with its word align-
ment (Q, C, A*), all bi-phrases consistent with the 
word alignment are identified. Consistency here 
implies two things. First, there must be at least 
one aligned word pair in the bi-phrase. Second, 
there must not be any word alignments from 
words inside the bi-phrase to words outside the 
bi-phrase. That is, we do not extract a phrase pair 
if there is an alignment from within the phrase 
pair to outside the phrase pair. The toy example 
shown in Figure 4 illustrates the bilingual phrases 
we can generate by this process. 
 After gathering all such bi-phrases from the 
full training data, we can estimate conditional 
relative frequency estimates without smoothing. 
For example, the phrase transformation probabil-
ity ???|?? in Equation (7) can be estimated ap-
proximately as 
Input: biPhraseLattice ?PL? with length = K & height 
= L;  
Initialization: biPhrase.maxProb = 0; 
for (x = 0; x <= K ? 1; x++) 
      for (y = 1; y <= L; y++) 
            for (yPre = 1; yPre <= L; yPre++) 
            { 
                  xPre = x ? y;  
                  biPhrasePre = PL.get(xPre, yPre); 
                  biPhrase = PL.get(x, y); 
                  if (!biPhrasePre || !biPhrase) 
                         continue; 
                  probIncrs = PL.getProbIncrease(biPhrasePre,  
                                                                      biPhrase); 
                  maxProbPre = biPhrasePre.maxProb;  
                  totalProb = probIncrs + maxProbPre; 
                  if  (totalProb > biPhrase.maxProb)  
                  { 
                        biPhrase.maxProb = totalProb;  
                        biPhrase.yPre = yPre; 
                   } 
             } 
Result: record at each bi-phrase boundary its maxi-
mum probability (biPhrase.maxProb) and optimal 
back-tracking biPhrases (biPhrase.yPre). 
 
Figure 3: The dynamic programming algorithm for 
Viterbi bi-phrase segmentation. 
 A B C D E F  a A 
a #       adc ABCD 
d    #    d D 
c   #     dc CD 
f      #  dcf CDEF 
        c C 
        f F 
 
Figure 4: Toy example of (left) a word alignment 
between two strings "adcf" and "ABCDEF"; and (right) 
the bi-phrases containing up to four words that are 
consistent with the word alignment. 
 
 
271
???|?? ?
???, ??
? ???, ?????
 (11) 
where ???, ?? is the number of times that c is 
aligned to q in training data. These estimates are 
useful for contextual lexical selection with suffi-
cient training data, but can be subject to data 
sparsity issues. 
An alternate translation probability estimate 
not subject to data sparsity issues is the so-called 
lexical weight estimate (Koehn et al, 2003). 
Assume we have a word translation distribution 
???|??  (defined over individual words, not 
phrases), and a word alignment A between q and c; 
here, the word alignment contains (i, j) pairs, 
where  ? ? 1. . |?| and ? ? 0. . |?|, with 0 indicat-
ing an inserted word.  Then we can use the fol-
lowing estimate: 
????|?, ?? ??
1
|??|??, ?? ? ??|
? ????| ???
???,????
|?|
???
 (12) 
We assume that for every position in q, there is 
either a single alignment to 0, or multiple align-
ments to non-zero positions in c. In effect, this 
computes a product of per-word translation scores; 
the per-word scores are averages of all the trans-
lations for the alignment links of that word. We 
estimate the word translation probabilities using 
counts from the word aligned corpus: ???|?? ?
???,??
? ???,?????
. Here ???, ?? is the number of times that 
the words (not phrases as in Equation 11) c and q 
are aligned in the training data. These word based 
scores of bi-phrases, though not as effective in 
contextual selection, are more robust to noise and 
sparsity. 
Throughout this section, we have approached 
this model in a noisy channel approach, finding 
probabilities of the misspelled query given the 
corrected query. However, the method can be run 
in both directions, and in practice SMT systems 
benefit from also including the direct probability 
of the corrected query given this misspelled query 
(Och, 2002). 
5.3 Phrase-Based Error Model Features 
To use the phrase-based error model for spelling 
correction, we derive five features and integrate 
them into the ranker-based query speller system, 
described in Section 4. These features are as 
follows. 
? Two phrase transformation features: 
These are the phrase transformation scores 
based on relative frequency estimates in two 
directions. In the correction-to-query direc-
tion, we define the feature as  ?????, ?, ?? ?
log ???|?? , where ???|??  is computed by 
Equations (8) to (10), and ??????? is the rel-
ative frequency estimate of Equation (11).   
? Two lexical weight features: These are the 
phrase transformation scores based on the 
lexical weighting models in two directions. 
For example, in the correction-to-query di-
rection, we define the feature 
as ?????, ?, ?? ? log ???|??, where ???|?? 
is computed by Equations (8) to (10), and the 
phrase transformation probability is the 
computed as lexical weight according to Eq-
uation (12). 
? Unaligned word penalty feature: the feature 
is defined as the ratio between the number of 
unaligned query words and the total number 
of query words. 
6 Experiments 
We evaluate the spelling error models on a large 
scale real world data set containing 24,172 queries 
sampled from one year?s worth of query logs from 
a commercial search engine. The spelling of each 
query is judged and corrected by four annotators.  
We divided the data set into training and test 
data sets. The two data sets do not overlap. The 
training data contains 8,515 query-correction 
pairs, among which 1,743 queries are misspelled 
(i.e., in these pairs, the corrections are different 
from the queries). The test data contains 15,657 
query-correction pairs, among which 2,960 que-
ries are misspelled. The average length of queries 
in the training and test data is 2.7 words.  
The speller systems we developed in this study 
are evaluated using the following three metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, i.e., a t-test 
with a significance level of 0.05. A significant 
difference should be read as significant at the 95% 
level. 
272
In our experiments, all the speller systems are 
ranker-based. In most cases, other than the base-
line system (a linear neural net), the ranker is a 
two-layer neural net with 5 hidden nodes. The free 
parameters of the neural net are trained to optim-
ize accuracy on the training data using the back 
propagation algorithm, running for 500 iterations 
with a very small learning rate (0.1) to avoid 
overfitting. We did not adjust the neural net 
structure (e.g., the number of hidden nodes) or 
any training parameters for different speller sys-
tems. Neither did we try to seek the best tradeoff 
between precision and recall. Since all the sys-
tems are optimized for accuracy, we use accuracy 
as the primary metric for comparison. 
Table 1 summarizes the main spelling correc-
tion results.  Row 1 is the baseline speller system 
where the source-channel model of Equations (2) 
and (3) is used. In our implementation, we use a 
linear ranker with only two features, derived 
respectively from the language model and the 
error model models. The error model is based on 
the edit distance function. Row 2 is the rank-
er-based spelling system that uses all 96 ranking 
features, as described in Section 4. Note that the 
system uses the features derived from two error 
models.  One is the edit distance model used for 
candidate generation. The other is a phonetic 
model that measures the edit distance between the 
metaphones (Philips, 1990) of a query word and 
its aligned correction word. Row 3 is the same 
system as Row 2, with an additional set of features 
derived from a word-based error model. This 
model is a special case of the phrase-based error 
model described in Section 5 with the maximum 
phrase length set to one.  Row 4 is the system that 
uses the additional 5 features derived from the 
phrase-based error models with a maximum 
bi-phrase length of 3. 
In phrase based error model, L is the maxi-
mum length of a bi-phrase (Figure 3). This value 
is important for the spelling performance. We 
perform experiments to study the impact of L; 
the results are displayed in Table 2. Moreover, 
since we proposed to use clickthrough data for 
spelling correction, it is interesting to study the 
impact on spelling performance from the size of 
clickthrough data used for training. We varied 
the size of clickthrough data and the experi-
mental results are presented in Table 3. 
The results show first and foremost that the 
ranker-based system significantly outperforms 
the spelling system based solely on the 
source-channel model, largely due to the richer 
set of features used (Row 1 vs. Row 2).  Second, 
the error model learned from clickthrough data 
leads to significant improvements (Rows 3 and 4 
vs. Row 2).  The phrase-based error model, due to 
its capability of capturing contextual information, 
outperforms the word-based model with a small 
but statistically significant margin (Row 4 vs. 
Row 3), though using phrases longer (L > 3) does 
not lead to further significant improvement (Rows 
6 and 7 vs. Rows 8 and 9). Finally, using more 
clickthrough data leads to significant improve-
ment (Row 13 vs. Rows 10 to 12). The benefit 
does not appear to have peaked ? further im-
provements are likely given a larger data set. 
7 Conclusions 
Unlike conventional textual documents, most 
search queries consist of a sequence of key words, 
many of which are valid search terms but are not 
stored in any compiled lexicon. This presents a 
challenge to any speller system that is based on a 
dictionary.  
This paper extends the recent research on using 
Web data and query logs for query spelling cor-
rection in two aspects. First, we show that a large 
amount of training data (i.e. query-correction 
pairs) can be extracted from clickthrough data, 
focusing on query reformulation sessions. The 
resulting data are very clean and effective for 
error model training. Second, we argue that it is 
critical to capture contextual information for 
query spelling correction. To this end, we propose 
# System Accuracy Precision Recall 
1 Source-channel 0.8526 0.7213 0.3586 
2 Ranker-based 0.8904 0.7414 0.4964 
3 Word model 0.8994 0.7709 0.5413 
4 Phrase model (L=3) 0.9043 0.7814 0.5732 
Table 1. Summary of spelling correction results. 
# System Accuracy Precision Recall 
5 Phrase model (L=1) 0.8994 0.7709 0.5413 
6 Phrase model (L=2) 0.9014 0.7795 0.5605 
7 Phrase model (L=3) 0.9043 0.7814 0.5732 
8 Phrase model (L=5) 0.9035 0.7834 0.5698 
9 Phrase model (L=8) 0.9033 0.7821 0.5713 
Table 2. Variations of spelling performance as a func-
tion of phrase length. 
 
# System Accuracy Precision Recall 
10 L=3; 0 month data 0.8904 0.7414 0.4964 
11 L=3; 0.5 month data 0.8959 0.7701 0.5234 
12 L=3; 1.5 month data 0.9023 0.7787 0.5667 
13 L=3; 3 month data 0.9043 0.7814 0.5732 
Table 3. Variations of spelling performance as a func-
tion of the size of clickthrough data used for training. 
 
 
273
a new phrase-based error model, which leads to 
significant improvement in our spelling correc-
tion experiments.  
There is additional potentially useful informa-
tion that can be exploited in this type of model. 
For example, in future work we plan to investigate 
the combination of the clickthrough data collected 
from a Web browser with the noisy but large 
query sessions collected from a commercial 
search engine. 
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Galen Andrew for the 
very helpful discussions and collaboration. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Im-
proving web search ranking by incorporating user 
behavior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a 
spelling error model from search query logs. In 
HLT-EMNLP, pp 955-962. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In 
ACL, pp. 286-293. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving 
query spelling correction using web search results. 
In EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compress-
ing trigram language models with Golomb cod-
ing. In EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction 
as an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web 
search ranking. In SIGIR.  
Golding, A. R., and Roth, D. 1996. Applying win-
now to context-sensitive spelling correction. In 
ICML, pp. 182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 
127-133. 
Kukich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Sur-
veys. 24(4): 377-439. 
Levenshtein, V. I. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. 
Exploring distributional similarity based models 
for query spelling correction. In ACL, pp. 
1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule ac-
quisition for spelling correction. In ICML, pp. 
187-194. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Okazaki, N., Tsuruoka, Y., Ananiadou, S., and 
Tsujii, J. 2008. A discriminative candidate gene-
rator for string transformations. In EMNLP, pp. 
447-456. 
Philips, L. 1990. Hanging on the metaphone. 
Computer Language Magazine, 7(12):38-44. 
Suzuki, H., Li, X., and Gao, J. 2009. Discovery of 
term variation in Japanese web search queries. In 
EMNLP. 
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In 
ACL, pp. 144-151. 
Wang, X., and Zhai, C. 2008. Mining term associa-
tion patterns from search logs for effective query 
reformulation. In CIKM, pp. 479-488. 
Whitelaw, C., Hutchinson, B., Chung, G. Y., and 
Ellis, G. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In 
EMNLP, pp. 890-899.  
274
