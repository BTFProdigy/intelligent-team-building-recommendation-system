FASIL Email Summarisation System 
 
 
Angelo Dalli, Yunqing Xia, Yorick Wilks 
NLP Research Group 
Department of Computer Science 
University of Sheffield 
{a.dalli, y.xia, y.wilks}@dcs.shef.ac.uk 
 
 
Abstract 
Email summarisation presents a unique set of 
requirements that are different from general 
text summarisation. This work describes the 
implementation of an email summarisation 
system for use in a voice-based Virtual Per-
sonal Assistant developed for the EU FASiL 
Project. Evaluation results from the first inte-
grated version of the project are presented. 
1 Introduction 
Email is one of the most ubiquitous applications used on 
a daily basis by millions of people world-wide, tradi-
tionally accessed over a fixed terminal or laptop com-
puter. In the past years there has been an increasing 
demand for email access over mobile phones. Our work 
has focused on creating an email summarisation service 
that provides quality summaries adaptively and quickly 
enough to cater for the tight constrains imposed by a 
real time text-to-speech system. 
This work has been done as part of the European 
Union FASiL project, which aims to aims to construct a 
conversationally intelligent Virtual Personal Assistant 
(VPA) designed to manage the user?s personal and 
business information through a voice-based interface 
accessible over mobile phones.  
As the quality of life and productivity is to improved 
in an increasingly information dominated society, peo-
ple need access to information anywhere, anytime. The 
Adaptive Information Management (AIM) service in the 
FASiL VPA seeks to automatically prioritise and pre-
sent information that is most pertinent to the mobile 
users and adapt to different user preferences. The AIM 
service is comprised of three main parts: an email sum-
mariser, email categoriser, calendar scheduling/PIM 
interaction and an adaptive prioritisation service that 
optimizes the sequence in which information is pre-
sented, keeping the overall duration of the voice-based 
dialogue to a minimum. 
2 Email Characteristics 
Email Summarisation techniques share many character-
istics with general text summarisation techniques while 
catering for the unique characteristics of email: 
1. short messages usually between 2 to 800 
words in length (after thread-filtering) 
2. frequently do not obey grammatical or con-
ventional stylistic conventions 
3. are a cross between informal mobile text or 
chat styles and traditional writing formats 
4. display unique thread characteristics with 87% 
containing three previous emails or less 
(Fisher and Moody, 2001) 
All these four main characteristics combined to-
gether mean that most document summarisation tech-
niques simply do not work well for email. The voice-
based system also required that summaries be produced 
on demand, with only a short pause allowed for the 
summariser to output a result ? typically a maximum of 
around 1 second per email.  
Another main constraint imposed in the FASiL VPA 
was the presence of two integer parameters ? the pre-
ferred and maximum length of the summary. The 
maximum length constraint had to be obeyed strictly, 
while striving to fit in the summary into the preferred 
length. These performance and size constraints, coupled 
with the four characteristics of email largely determined 
the design of the FASiL Email Summariser. 
2.1 Short Messages 
Email is a form of short, largely informal, written com-
munication that excludes methods that need large 
amounts of words and phrases to work well.  
The main disadvantage is that sometimes the useful 
content of a whole email message is simply a one word 
in case of a yes/no answer to a question or request. The 
summariser exploits this characteristic by filtering out 
threads and other commonly repeated text at the bottom 
of the email text such as standard email text signatures. 
If the resulting text is very short and falls within the 
preferred length of the summary, the message can be 
output in its entirety to users. The short messages also 
make it easier to achieve relevancy in the summaries. 
Inadvertently context is sometimes lost in the sum-
mary due to replies occurring in threaded emails. Also, 
emails containing lots of question-answer pairs can get 
summarised poorly due to the fixed amount of space 
available for the summary. 
2.2 Stylistic Conventions and Grammar 
Email messages often do not follow formal stylistic 
conventions and are may have a substantial level of 
spelling mistakes, abbreviations and other features that 
make text analysis difficult. 
A simple spellchecker using approximate string 
matching and word frequency/occurrence statistics was 
used to match misspelled names automatically.  
Another problem that was encountered was the iden-
tification of sentence boundaries, since more than 10% 
of the emails seen by the summariser frequently had 
missing punctuation and spurious line breaks inserted 
by various different email programs. A set of hand-
coded heuristics managed to produce acceptable results, 
identifying sentence boundaries correctly more than 
90% of the time. 
2.3 Informal and Formal Styles 
Email can often be classified into three categories: in-
formal short messages ? often sent to people whom are 
directly known or with whom there has been a pro-
longed discussion or interaction about a subject, mixed 
formal/informal emails sent to strangers or when re-
questing information or replying to questions, and for-
mal emails that are generally electronic versions of 
formal letter writing. 
The class of emails that cause most problems for 
summarisation purposes are the first two classes of e-
mails. One of the main determining factors for the style 
adopted by people in replying to emails is the amount of 
time that lapses between replies. Generally email gets 
more formal as the time span between replies increases. 
Informal email can also be recognised by excessive 
use of anaphora that need to be resolved properly before 
summarisation can take place. The summariser thus has 
an anaphora resolver that is capable of resolving ana-
phoric references robustly. 
Linguistic theory indicates that as the formality of a 
text increases, the number of words in the deictic cate-
gory will decrease as the number of words in the non-
deictic category increase (and vice-versa). Deictic (or 
anaphoric) word classes include words that have vari-
able meaning whose meaning needs to be resolved 
through the surrounding (usually preceding) context. 
Non-deictic word classes are those words whose mean-
ing is largely context-independent, analogous to predi-
cates in formal logic.  
2.4 Threaded Emails 
Many emails are composed by replying to an original 
email, often including part or whole of the original 
email together with new content, thus creating a thread 
or chain of emails. The first email in the thread will 
potentially be repeated many times over, which might 
mislead the summarisation process. A thread-detection 
filtering tool is used to eliminate unoriginal content in 
the email by comparing the contents of the current email 
with the content of previous emails. A study of over 57 
user?s incoming and outgoing emails found that around 
30% of all emails are threaded. Around 56% of the 
threaded emails contained only one previous email ? i.e. 
a request and reply, and 87% of all emails contained 
only three previous emails apart from the reply (Fisher 
and Moody, 2001). 
Some reply styles also pose a problem when com-
bined with threads. Emails containing a list of questions 
or requests for comments are often edited by the reply-
ing party and answers inserted directly inside the text of 
the original request, as illustrated in Figure 1. 
 
> ? now coming back to the issue 
> of whether to include support for 
> location names in the recogniser 
> I think that we should include 
> this ? your opinions appreciated. 
I agree with this. 
 
Figure 1 Sample Embedded Answer 
 
Figure 1 illustrates the main two difficulties faced by 
the summariser in this situation. While the threaded 
content from the previous reply should be filtered out to 
identify the reply, the reply on its own is meaningless 
without any form of context. The summariser tries to 
overcome this by identifying this style of embedded 
responses when the original content is split into chunks 
or is only partially included in the reply. The text falling 
before the answer is then treated as part of the reply. 
Although this strategy gives acceptable results in some 
cases, more research is needed into finding the optimal 
strategy to extract the right amount of context from the 
thread without either destroying the context or copying 
too much from the original request back into the sum-
mary. 
3 Summarisation Techniques 
Various summarisation techniques were considered in 
the design of the FASiL email summariser. Few opera-
tional email-specific summarisation systems exist, so 
the emphasis was on extracting the best-of-breed tech-
niques from document summarisation systems that are 
applicable to email summarisation. 
3.1 Previous Work 
Many single-document summarisation systems can be 
split according to whether they are extractive or non-
extractive systems. Extractive systems generate summa-
ries by extracting selected segments from the original 
document that are deemed to be most relevant. Non-
extractive systems try to build an abstract representation 
model and re-generate the summary using this model 
and words found in the original document. 
Previous related work on extractive systems in-
cluded the use of semantic tagging and co-
reference/lexical  chains (Saggion et al, 2003; Barzilay 
and Elhadad, 1997; Azzam et al, 1998), lexical occur-
rence/structural statistics (Mathis et al, 1973), discourse 
structure (Marcu, 1998), cue phrases (Luhn, 1958; 
Paice, 1990; Rau et al, 1994), positional indicators 
(Edmunson, 1964) and other extraction methods (Kui-
pec et al, 1995). 
Non-extractive systems are less common ? previous 
related work included reformulation of extracted models 
(McKeown et al, 1999), gist extraction (Berger and 
Mittal, 2000), machine translation-like approaches 
(Witbrock and Mittal, 1999) and generative models (De 
Jong, 1982; Radev and McKeown, 1998; Fum et al,  
1986; Reihmer and Hahn, 1988; Rau et al,  1989). 
A sentence-extraction system was decided for the 
FASiL summariser, with the capability to have phrase-
level extraction in the future. Non-extractive systems 
were not likely to work as robustly and give the high 
quality results needed by the VPA to work as required. 
Another advantage that extractive systems still pose is 
that in general they are more applicable to a wider range 
of arbitrary domains and are more reliable than non-
extractive systems (Teufel, 2003). 
The FASiL summariser uses named entities as an 
indication of the importance of every sentence, and per-
forms anaphora resolution automatically. Sentences are 
selected according to named entity density and also ac-
cording to their positional ranking. 
3.2 Summariser Architecture 
The FASiL Summariser works in conjunction with a 
number of different components to present real-time 
voice-based summaries to users. Figure 2 shows the 
overall architecture of the summariser and its place in 
the FASiL VPA. 
 
 
Figure 2 Summariser and VPA Architecture 
 
An XML-based protocol is used to communicate 
with the Dialogue Manager enabling the system to be 
loosely coupled but to have high cohesion (Sommer-
ville, 1992). 
3.3 Named Entity Recognition 
One of the most important components in the FASiL 
Summariser is the Named Entity Recogniser (NER) 
system. 
The NER uses a very efficient trie-like structure to 
match sub-parts of every name (Gusfield, 1997; 
Stephen, 1994). An efficient implementation enables the 
NER to confirm or reject a word as being a named en-
tity or not in O(n) time. Named entities are automati-
cally classified according to the following list of 11 
classes: 
? Male proper names (M) 
? Female proper names (F) 
? Places (towns, cities, etc.) (P) 
? Locations (upstairs, boardroom, etc.) (L) 
? Male titles (Mr., Esq., etc.) (Mt) 
? Female titles (Ms., Mrs., etc.) (Ft) 
? Generic titles (t) 
? Date and time references (TIME) 
? Male anaphors (Ma) 
? Female anaphors (Fa) 
? Indeterminate anaphors (a)  
 
The gazetteer list for Locations, Titles, and Ana-
phors were compiled manually. Date and time refer-
ences were compiled from data supplied in the IBM 
International Components for Unicode (ICU) project 
(Davis, 2003). Place names were extracted from data 
available online from the U.S. Geological Survey Geo-
graphic Names Information System and the GEOnet 
Names Server (GNS) of the U.S. National Imagery and 
Mapping Agency (USGS, 2003; NIMA, 2003). 
An innovative approach to gathering names for the 
male and female names was adopted using a small cus-
tom-built information extraction system that crawled 
Internet pages to identify likely proper names in the 
texts. Additional hints were provided by the presence of 
anaphora in the same sentence or the following sentence 
as the suspected proper name. The gender of every title 
and anaphora was manually noted and this information 
was used to keep a count of the number of male or fe-
male titles and anaphors associated with a particular 
name. This information enabled the list of names to be 
organised by gender, enabling a rough probability to be 
assigned to suspect words (Azzam et al, 1998; Mitkov, 
2002).  
An Internet-based method that verified the list and 
filtered out likely spelling mistakes and non-existent 
names was then applied to this list, filtering out incor-
rectly spelt names and other features such as online chat 
nicknames (Dalli, 2004). 
A list of over 592,000 proper names was thus ob-
tained by this method with around 284,000 names being 
identified as male and 308,000 names identified as fe-
male. The large size of this list contributed significantly 
to the NER?s resulting accuracy and compares favoura-
bly with previously compiled lists (Stevenson and Gai-
zauskas, 2000). 
3.4 Anaphora Resolution 
Extracting systems suffer from the problem of dangling 
anaphora in summaries. Anaphora resolution is an effec-
tive way of reducing the incoherence in resulting sum-
maries by replacing anaphors with references to the 
appropriate named entities (Mitkov, 2002). This substi-
tution has the direct effect of making the text less con-
text sensitive and implicitly increases the formality of 
the text. 
Cohesion problems due to semantic discontinuities 
where concepts and agents are not introduced are also 
partially solved by placing emphasis on named entities 
and performing anaphora resolution. The major cohe-
sion problem that still has not been fully addressed is 
the coherence of various events mentioned in the text. 
The anaphora resolver is aided by the gender-
categorised named entity classes, enabling it to perform 
better resolution over a wide variety of names. A simple 
linear model is adopted, where the system focuses 
mainly on nominal and clausal antecedents (Cristea et 
al., 2000). The search scope for candidate antecedents is 
set to the current sentence together with the three pre-
ceding sentences as suggested in (Mitkov, 1998) as em-
pirical studies show that more than 85% of all cases are 
handled correctly with this window size (Mitkov, 2002). 
Candidate antecedents being discarded after ten sen-
tences have been processed without the presence of 
anaphora as suggested in (Kameyama, 1997). 
3.5 Sentence Ranking 
After named entity recognition and anaphora resolution, 
the summariser ranks the various sentences/phrases that 
it identifies and selects the best sentences to extract and 
put in the summary. The summariser takes two parame-
ters apart from the email text itself: a preferred length 
and a maximum length. Typical lengths are 160 charac-
ters preferred with 640 characters maximum, which 
compares to the size a mobile text message. 
Ranking takes into account three parameters: named 
entity density and importance of every class, sentence 
position and the preferred and maximum length parame-
ters. 
0
1
2
3
4
5
6
7
8
1 3 5 7 9 11 13 15 17 19
Number of Sentences
Weight
Series1 Series2 Series3
Series4 Series5
 
Figure 3 Positional sentence weight for varying 
summarisation parameters 
 
Positional importance was found to be significant in 
email text since relevant information was often found to 
be in the first few sentences of the email.  
Figure 3 shows how the quadratic positional weight 
function ? changes with position, giving less importance 
to sentences as they occur further from the start (al-
though the weight is always bigger than zero). Different 
kinds of emails were used to calibrate the weight func-
tion. Series 1 (bottom) represents a typical mobile text 
message length summary with a very long message. 
Series 4 and 5 (middle) represent the weight function 
behaviour when the summary maximum length is long 
(approximately more than 1,000 characters), irrelevant 
of the email message length itself. Series 2 and 3 (top) 
represent email messages that fall within the maximum 
length constraints. 
The following ranking function rank(j), where j is 
the sentence number, is used to rank and select excerpts: 
( ) ( )( ) ( ) ( )( )++?= ?
=
? ????
0
,1
i
c iijjrank  
( )? ?( )???
????
? ??+???
?
???
?
+ ?
???? jlength
j
j
1
max  
 
where ? and ? are empirically determined constants, 
? is the preferred summary length, and jmax is the num-
ber of sentences in the email. The NER function ?c 
represents the number of words of type i in sentence j 
and ?(i) gives the weight associated with that type. In 
our case ? equals 10 since there are 11 named entity 
classes. The NER weights ?(i) for every class have 
been empirically determined and optimized. A third 
parameter ? is used to change the values of ? and ? ac-
cording to the maximum and preferred lengths together 
with the email length as shown in Figure 3. 
The first term handles named entity density, the sec-
ond the sentence position and the third biases the rank-
ing towards the preferred length. The sentences are then 
sorted in rank order and the preferred and maximum 
lengths used to determine which sentences to return in 
the summary. 
4 Experimental Results 
The summariser results quality was evaluated against 
manually produced summaries using precision and re-
call, together with a more useful utility-based evaluation 
that uses a fractional model to cater for varying degrees 
of importance for different sentences. 
4.1 Named Entity Recognition Performance 
The performance of the summariser depends signifi-
cantly on the performance of the NER. Speed tests show 
that the NER consistently processes more than 1 million 
wps on a 1.6 GHz machine while keeping resource us-
age to a manageable 300-400 Mb of memory. 
Precision and recall curves were calculated for 100 
emails chosen at random, separated into 10 random 
sample groups from representative subsets of the three 
main types of emails ? short, normal and long emails as 
explained previously. The samples were manually 
marked according to the 11 different named entity 
classes recognised by the NER to act as a comparative 
standard for relevant results. Figures 4 and 5 respec-
tively show the NER precision and recall results. 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Precision
M F P L
Mt Ft t TIME
Ma Fa a  
Figure 4 Precision by Named Entity Class 
 
It is interesting to note that the NER performed 
worst at anaphora identification with an average preci-
sion of 77.5% for anaphora but 96.7% for the rest of the 
named entity classes. 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Recall
M F P L
Mt Ft t TIME
Ma Fa a
 
Figure 5 Recall by Named Entity Class 
 
Figure 6 shows the average precision and recall av-
eraged across all the eleven types of named entity 
classes, for the 10 sample email groups. An average 
precision of 93% was achieved throughout, with 97% 
recall. 
0
0.25
0.5
0.75
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Value
Recall Precision
 
Figure 6 Average Precision and Recall 
 
It is interesting to note that the precision and recall 
curves do not exhibit the commonly observed inverse 
trade-off relationship between precision and recall 
(Buckland and Gey, 1994; Alvarez, 2002). This result is 
explained by the fact that the NER, in this case, can 
actually identify most named entities in the text with 
high precision while neither over-selecting irrelevant 
results nor under-selecting relevant results. 
4.2 Summariser Results Quality 
Quality evaluation was performed by selecting 150 
emails at random and splitting the emails up into 15 
groups of 10 emails at random to facilitate multiple per-
son evaluation. Each sentence in every email was then 
manually ranked using a scale of 1 to 10. For recall and 
precision calculation, any sentence ranked ? 5 was de-
fined as relevant. Figure 7 shows the precision and re-
call values with 74% average precision and 71% aver-
age recall. 
0
0.5
1
1.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Sample Group
Value
Recall Precision
 
Figure 7 Summaries Recall and Precision 
 
A utility-based evaluation was also used to obtain 
more intuitive results than those given by precision and 
recall using the methods reported in (Jing et al, 1998; 
Goldstein et al, 1999; Radev et al, 2000). The average 
score of each summary was compared to the average 
score over infinity expected to be obtained by extracting 
a combination of the first [1..N] sentences at random. 
The summary average score was also compared to the 
score obtained by an averaged pool of 3 human judges. 
Figure 8 shows a comparison between the summariser 
performance and human performance, with the summar-
iser averaging at 86.5% of the human performance, 
ranging from 60% agreement to 100% agreement with 
the gold standard. 
0
0.5
1
1.5
2
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Sample Group
Value
Summariser Utility Gold Standard Utility
 
Figure 8 Utility Score Comparison 
In Figure 8 a random extraction system is expected 
to get a score of 1 averaged across an infinite amount of 
runs. The average sentence compression factor for the 
summariser was 42%, exactly the same as the human 
judges? results. The selected emails had an average 
length of 14 sentences, varying from 7 to 27 sentences. 
5 Conclusion and Future Work 
The FASiL Email Summarisation system represents a 
compact summarisation system optimised for email 
summarisation in a voice-based system context.  
The excellent performance in both speed and accu-
racy of the NER component makes it ideal for re-use in 
projects that need high quality real-time identification 
and classification of named entities. 
A future improvement will incorporate a fast POS 
analyser to enable phrase-level extraction to take place 
while improving syntactic coherence. An additional 
improvement will be the incorporation of co-reference 
chain methods to verify email subject lines and in some 
cases suggest more appropriate subject lines. 
The FASiL summariser validates the suitability of 
the combined sentence position and NER-driven ap-
proach towards email summarisation with encouraging 
results obtained. 
Acknowledgments 
This research is funded under the EU FASiL Project, an 
EU grant in Human Language Technology (IST-2001-
38685) (Website: www.fasil.co.uk). 
References 
Alvarez, S. 2002. ?An exact analytical relation among 
recall, precision, and classification accuracy in in-
formation retrieval.? Boston College, Boston, Tech-
nical Report BCCS-02-01. 
Azzam, S., Humphreys, K. and Gaizauskas, R. 1998. 
?Coreference resolution in a multilingual information 
extraction?, Proc. Workshop on Linguistic Corefer-
ence. Granada, Spain. 
Barzilay, R. Elhadad, M. 1997. ?Using Lexical Chains 
for Text Summarization.?, Proc. ACL Workshop on 
Intelligent Scaleable Text Summarization, Madrid, 
Spain. 10-17. 
Berger, L. Mittal, V. 2000. ?OCELOT: A system for 
summarizing web pages?. Carnegie Mellon Univer-
sity. Just Research. Pittsburgh, Pennsylvania. 
Buckland, M. Gey, F. 1994. ?The relationship between 
recall and precision.? J. American Society for Infor-
mation Science, 45(1):12-19. 
Cristea, D., Ide, N., Marcu, D., Tablan, V. 2000. ?An 
empirical investigation of the relation between dis-
course structure and coreference.?, Proc. 19th Int. 
Conf. on Comp. Linguistics (COLING-2000), Saar-
br?cken, Germany. 208-214. 
Dalli, A. 2004. ?An Internet-based method for Verifica-
tion of Extracted Proper Names?. CICLING-2004. 
David, C. 2003. Information Society Statistics: PCs, 
Internet and mobile phone usage in the EU. Euro-
pean Community, Report KS-NP-03-015-EN-N. 
Davis, M. 2003. ?An ICU overview?. Proc. 24th Unicode 
Conference, Atlanta. IBM Corporation, California. 
De Jong, G. 1982. ?An overview of the FRUMP sys-
tem.?, in: Lehnert and Ringle eds., Strategies for 
Natural Language Processing, Lawrence Erlbaum 
Associates, Hillsdale, New Jersey. 149-176. 
Edmunson, H.P. 1964. ?Problems in automatic extract-
ing.?, Comm. ACM, 7, 259-263. 
Fisher, D., Moody, P. 2001. Studies of Automated Col-
lection of Email Records. University of California, 
Irvine, Technical Report UCI-ISR-02-4. 
Fum, D. Guida, G. Tasso, C. 1986. ?Tailoring impor-
tance evaluation to reader?s goals: a contribution to 
descriptive text summarization.? Proc. COLING-86, 
256-259. 
Goldstein, J. Kantrowitz, M. Mittal, V. Carbonell, 
Jaime. 1999. ?Summarizing Text Documents: Sen-
tence Selection and Evaluation Metrics?, Proc. ACM-
SIGIR 1999, Berkeley, California. 
Gusfield, D.  1997.  Algorithms on Strings, Trees and 
Sequences.  Cambridge University Press, Cambridge, 
UK. 
Halliday, M.A.K. 1985. Spoken and written language. 
Oxford University Press, Oxford. 
Jing, H. Barzilay, R. McKeown, K. Elhadad, M. 1998. 
?Summarization Evaluation Methods: Experiments 
and Analysis?, AAAI Spring Symposium on Intelligent 
Text Summarisation, Stanford, California. 
Kameyama, M. 1997. ?Recognising referential links: an 
information extraction perspective.?, Proc. EACL-97 
Workshop on Operational Factors in Practical, Ro-
bust, Anaphora Resolution, Madrid, Spain. 46-53. 
Kuipec, J. Pedersen, J. Chen, F. 1995. ?A Trainable 
Document Summarizer.?, Proc. 18th ACM SIGIR 
Conference, Seattle, Washington. 68-73. 
Luhn, P.H. 1958. ?Automatic creation of literature ab-
stracts?. IBM J. 159-165. 
Marcu, D. 1998. ?To Build Text Summaries of High 
Quality, Nuclearity is not Sufficient.?  Proc. AAAI 
Symposium on Intelligent Text Summarisation, Stan-
ford University, Stanford, California. 1-8. 
Mathis, B.A. Rush, J.E. Young, C.E. 1973. ?Improve-
ment of automatic abstracts by the use of structural 
analysis.?, J. American Society for Information Sci-
ence, 24, 101-109. 
McKeown, K. Klavens, J. Hatzivassiloglou, V. Barzi-
lay, R. Eskin, E. 1999. ?Towards Multidocument 
Summarization by Reformulation: Progress and 
Prospects.?, AAAI Symposium on Intelligent Text 
Summarisation. 
Mitkov, R. 1998. ?Robust pronoun resolution with lim-
ited knowledge.?, Proc. 17th International Confer-
ence on Comp. Linguistics (COLING-1998), 
Montreal, Canada. 869-875. 
Mitkov, R. 2002. Anaphora Resolution. London, Long-
man. 
National Imagery and Mapping Agency (NIMA). 2003. 
GEOnet Names Server (GNS). 
Paice, C. 1990. ?Constructing literature abstracts by 
computer: techniques and prospects.?, Information 
Processing and Management, 26:171-186. 
Radev, D. McKeown, K. 1998. ?Generating Natural 
Language Summaries from Multiple On-Line 
Sources.?, Computational Linguistics, 24(3):469-500. 
Radev, D. Jing, H. Budzikowska, M. 2000. ?Centroid-
based summarization of multiple documents: sen-
tence extraction, utility-based evaluation, user stud-
ies.? in Automatic Summarisation: ANLP/NAACL 
2000 Workshop, New Brunswick, New Jersey. 
Rau, L. Jacobs, P. Zernick, U. 1989. ?Information ex-
traction and text summarization using linguistic 
knowledge acquisition.?, Information Processing and 
Management, 25(4):419-428. 
Rau, L. Brandow, R. Mitze, K. 1994. ?Domain-
Independent Summarization of News.?, in: Summa-
rizing Text for Intelligent Communication, Dagstuhl, 
Germany. 71-75. 
Reimer, U. Hahn, U. 1988. ?Text condensation as 
knowledge base abstraction.? Proc. 4th Conference on 
Artificial Intelligence Applications. 338-344. 
Saggion, H. Bontcheva, K. Cunningham, H. 2003. ?Ro-
bust Generic and Query-based Summarisation?. Proc. 
EACL-2003, Budapest. 
Sommerville, I. 1992. Software Engineering. 4th ed. 
Addison-Wesley. 
Stephen, Graham A. 1994. String Searching Algorithms. 
World Scientific Publishing, Bangor, Gwynedd, UK. 
Stevenson, M. Gaizauskas, R. 2000. ?Using Corpus-
derived Name Lists for Named Entity Recognition, 
Proc. ANLP-2000, Seattle. 
Teufel, S. 2003. ?Information Retrieval: Automatic 
Summarisation?, University of Cambridge. 24-25. 
Witbrock, M. Mittal, V. 1999. ?Ultra Summarization: A 
Statistical Approach to Generating Non-Extractive 
Summaries.?, Just Research, Pittsburgh. 
United States Geological Survey (USGS). 2003. Geo-
graphic Names Information System (GNIS). 
http://geonames.usgs.gov/ 
75
76
77
78
143
144
145
146
The Interaction of Knowledge Sources 
in Word Sense Disambiguation 
Mark Stevenson 
University of Sheffield 
Yorick Wilks* 
University of Sheffield 
Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the 
tradition of combining different knowledge sources in artificial in telligence research. An important 
step in the exploration of this hypothesis i to determine which linguistic knowledge sources are 
most useful and whether their combination leads to improved results. We present a sense tagger 
which uses several knowledge sources. Tested accuracy exceeds 94% on our evaluation corpus. 
Our system attempts to disambiguate all content words in running text rather than limiting 
itself to treating a restricted vocabulary of words. It is argued that this approach is more likely to 
assist the creation of practical systems. 
1. Introduction 
Word sense disambiguation (WSD) is a problem long recognised in computational 
linguistics (Yngve 1955) and there has been a recent resurgence of interest, including 
a special issue of this journal devoted to the topic (Ide and V4ronis 1998). Despite this 
there is still a considerable diversity of methods employed by researchers, as well as 
differences in the definition of the problems to be tackled. The SENSEVAL evaluation 
framework (Kilgarriff 1998) was a DARPA-style competition designed to bring some 
conformity to the field of WSD, although it has yet to achieve that aim completely. The 
main sources of divergence are the choice of computational paradigm, the proportion 
of text words disambiguated, the granularity of the meanings assigned to them, and 
the knowledge sources used. We will discuss each in turn. 
Resnik and Yarowsky (1997) noted that, for the most part, part-of-speech tagging is 
tackled using the noisy channel model, although transformation rules and grammatico- 
statistical methods have also had some success. There has been far less consensus 
as to the best approach to WSD. Currently, machine learning methods (Yarowsky 
1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992) 
have been popular. This paper reports a WSD system employing elements of both 
approaches. 
Another source of difference in approach is the proportion of the vocabulary dis- 
ambiguated. Some researchers have concentrated on producing WSD systems that 
base results on a limited number of words, for example Yarowsky (1995) and Schtitze 
(1992) who quoted results for 12 words, and a second group, including Leacock, Tow- 
ell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, 
namely interest. But limiting the vocabulary on which a system is evaluated can have 
two serious drawbacks. First, the words used were not chosen by frequency-based 
sampling techniques and so we have no way of knowing whether or not they are 
special cases, a point emphasised by Kilgarriff (1997). Secondly, there is no guarantee 
? Department of Computer Science, 211 Regent Court, Portobello Street, Sheffield $1 4DP, UK 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 3 
that the techniques employed will be applicable when a larger vocabulary is tackled. 
However it is likely that mark-up for a restricted vocabulary can be carried out more 
rapidly since the subject has to learn the possible senses of fewer words. 
Among the researchers mentioned above, one must distinguish between, on the 
one hand, supervised approaches that are inherently limited in performance to the 
words over which they evaluate because of limited training data and, on the other 
hand, approaches whose unsupervised learning methodology is applied to only small 
numbers of words for evaluation, but which could in principle have been used to tag 
all content words in a text. Others, such as Harley and Glennon (1997) and ourselves 
Wilks and Stevenson (1998a, 1998b; Stevenson and Wilks 1999), have concentrated on 
approaches that disambiguate allcontent words. 1In addition to avoiding the problems 
inherent in restricted vocabulary systems, wide coverage systems are more likely to 
be useful for NLP applications, as discussed by Wilks et al (1990). 
A third difference concerns the granularity of WSD attempted, which one can 
illustrate in terms of the two levels of semantic distinctions found in many dictionaries: 
homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992), we 
shall give results at both levels, but it is worth pointing out that the targets of, say, work 
using translation equivalents (e.g., Brown et al 1991; Gale, Church, and Yarowsky 
1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) 
correspond broadly to the wider, homograph, distinctions. 
In this paper we attempt o show that the high level of results more typical of 
systems trained on many instances of a restricted vocabulary can also be obtained 
by large vocabulary systems, and that the best results are to be obtained from an 
optimization of a combination of types of lexical knowledge (see Section 2). 
1.1 Lexical Knowledge and WSD 
Syntactic, semantic, and pragmatic information are all potentially useful for WSD, as 
can be demonstrated by considering the following sentences: 
(1) 
(2) 
(3) 
(4) 
John did not feel well. 
John tripped near the well. 
The bat slept. 
He bought a bat from the sports shop. 
The first two sentences contain the ambiguous word well; as an adjective in (1) 
where it is used in its "state of health" sense, and as a noun in (2), meaning "water 
supply". Since the two usages are different parts of speech they can be disambiguated 
by this syntactic property. 
Sentence (3) contains the word bat, whose nominal readings are ambiguous be- 
tween the "creature" and "sports equipment" meanings. Part-of-speech information 
cannot disambiguate he senses ince both are nominal usages. However, this sentence 
can be disambiguated using semantic information, such as preference r strictions. The 
verb sleep prefers an animate subject and only the "creature" sense of bat is animate. 
So Sentence (3) can be effectively disambiguated by its semantic behaviour but not by 
its syntax. 
1 In this paper we define content words as nouns, verbs, adjectives, and adverbs, although others have 
included other part-of-speech categories (Hirst 1995). 
322 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
A preference restriction will not disambiguate S ntence (4) since the direct object 
preference will be at least as general as physical object, and any restriction on the direct 
object slot of the verb sell would cover both senses. The sentence can be disambiguated 
on pragmatic grounds because it is far more likely that sports equipment will be bought 
in a sports shop. Thus pragmatic information can be used to disambiguate bat to its 
"sports equipment" sense. 
Each of these knowledge sources has been used for WSD and in Section 3 we de- 
scribe a method which performs rough-grained disambiguation using part-of-speech 
information. Wilks (1975) describes a system which performs WSD using semantic 
information in the form of preference restrictions. Lesk (1986) also used semantic in- 
formation for WSD in the form of textual definitions from dictionaries. Pragmatic in- 
formation was used by Yarowsky (1992) whose approach relied upon statistical models 
of categories from Roget's Thesaurus (Chapman, 1977), a resource that had been used 
in much earlier approaches to WSD such as Masterman (1957). 
The remainder of this paper is organised as follows: Section 2 reviews some sys- 
tems which have combined knowledge sources for WSD. In Section 3 we discuss the 
relationship between semantic disambiguation a d part-of-speech tagging, reporting 
an experiment which quantifies the connection. A general WSD system is presented 
in Section 4. In Section 5 we explain the strategy used to evaluate this system, and we 
report the results in Section 6. 
2. Background 
A comprehensive r view of WSD is beyond the scope of this paper but may be 
found in Ide and V4ronis (1998). Combining knowledge sources for WSD is not a 
new idea; in this section we will review some of the systems which have tried to do 
that. 
2.1 McRoy's System 
Early work on coarse-grained WSD based on combining knowledge sources was un- 
dertaken by McRoy (1992). Her work was carried out without the use of machine- 
readable dictionaries (MRD), necessitating the manual creation of the complex set of 
lexicons this system requires. There was a lexicon of 8,775 unique roots, a hierarchy 
of 1,000 concepts, and a set of 1,400 collocational patterns. The collocational patterns 
are automatically extracted from a corpus of text in the same domain as the text being 
disambiguated and senses are manually assigned to each. If the collocation occurs in 
the text being disambiguated, then it is assumed that the words it contains are being 
used in the same senses as were assigned manually. 
Disambiguation makes use of several knowledge sources: frequency information, 
syntactic tags, morphological information, semantic ontext (clusters), collocations and 
word associations, role-related expectations, and selectional restrictions. The knowl- 
edge sources are combined by adding their results. Each knowledge source assigns a 
(possibly negative) numeric value to each of the possible senses. The numerical value 
depends upon the type of knowledge source. Some knowledge sources have only two 
possible values, for example the frequency information has one value for frequent 
senses and another for infrequent ones. The numerical values assigned for each were 
determined manually. The selectional restrictions knowledge source assigns cores in 
the range -10 to +10, with higher scores being assigned to senses that are more specific 
(according to the concept hierarchy). Disambiguation is carried out by summing the 
scores from each knowledge source for all candidate senses and choosing the one with 
the highest overall score. 
323 
Computational Linguistics Volume 27, Number 3 
In a sample of 25,000 words from the Wall Street Journal, the system covered 98% of 
word-occurrences that were not proper nouns and were not abbreviated, emonstrat- 
ing the impressive coverage of the hand-crafted lexicons. No quantitative evaluation 
of the disambiguation quality was carried out due to the difficulty in obtaining an- 
notated test data, a problem made more acute by the use of a custom-built exicon. 
In addition, comparison of system output against manually annotated text had yet to 
become a standard evaluation strategy in WSD research. 
2.2 The Cambridge Language Survey System 
The Cambridge International Dictionary of English (CIDE) (Procter 1995) is a learners' dic- 
tionary which consists of definitions written using a 2,000 word controlled vocabulary. 
(This lexicon is similar to LDOCE, which we use for experiments presented later in this 
paper; it is described in Section 3.1.) The senses in CIDE are grouped by guidewords, 
similar to homographs in LDOCE. It was produced using a large corpus of English 
created by the Cambridge Language Survey (CLS). 
The CLS also produced a semantic tagger (Harley and Glennon 1997), a commer- 
cial product hat tags words in text with senses from their MRD. The tagger consists 
of four sub-taggers running in parallel, with their results being combined after all 
have run. The first tagger uses collocations derived from the CIDE example sentences. 
The second examines the subject codes for all words in a particular sentence and the 
number of matches with other words is calculated. A part-of-speech tagger produced 
in-house by CUP is run over the text and high scores are assigned to senses that 
agree with the syntactic tag assigned. Finally, the selectional restrictions of verbs and 
adjectives are examined. The results of these processes are combined using a simple 
weighting scheme (similar to McRoy's; see Section 2.1). This weighting scheme, in- 
spired by those used in computer chess programs, assigns each sub-process a weight 
in the range -100 to +100 before summing. Unlike McRoy, this approach does not con- 
sider the specificity of a knowledge source in a particular instance but always assigns 
the same overall weight to each. 
Harley and Glennon report 78% correct agging of all content words at the CIDE 
guideword level (which they equate to the LDOCE sense level) and 73% at the sub- 
sense level, as compared to a hand-tagged corpus of 4,000 words. 
2.3 Machine Learning applied to WSD 
An early application of machine learning to the WSD problem was carried out by 
Brown et al (1991). Several different disambiguation cues, such as first noun to the 
left/right and second word to the left/right, were extracted from parallel text. Trans- 
lation differences were used to define the senses, as this approach was used in an 
English-French machine translation system. The parallel text effectively provided su- 
pervised training examples for this algorithm. Nadas et al (1991) used the flip-flop 
algorithm to decide which of the cues was most important for each word by maxi- 
mizing mutual information scores between words. Yarowsky (1996) used an extremely 
rich features et by expanding this set with syntactic relations uch as subject-verb, 
verb-object and adjective-noun relations, part-of-speech n-grams and others. The ap- 
proach was based on the hypothesis that words exhibited "one sense per collocation" 
(Yarowsky 1993). A large corpus was examined to compute the probability of a partic- 
ular collocate occurring with a certain sense and the discriminatory power of each was 
calculated using the log-likelihood ratio. These ratios were used to create a decision 
list, with the most discriminating collocations being preferred. This approach as the 
benefit hat it does not combine the probabilities of the collocates, which are highly 
non-independent knowledge sources. 
324 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Yarowsky (1993) also examined the discriminatory power of the individual knowl- 
edge sources. It was found that each collocation indicated a particular sense with a 
very high degree of reliability, with the most successful--the first word to the left of 
a noun--achieving 99% precision. Yet collocates have limited applicability; although 
precise, they can only be applied to a limited number of tokens. Yarowsky (1995) 
dealt with this problem largely by producing an unsupervised learning algorithm that 
generates probabilistic decision list models of word senses from seed collocates. This 
algorithm achieves 97% correct disambiguation. I  these experiments Yarowsky deals 
exclusively with binary sense distinctions and evaluates his highly effective algorithms 
on small samples of word tokens. 
Ng and Lee (1996) explored an approach to WSD in which a word is assigned 
the sense of the most similar example already seen. They describe this approach as 
"exemplar-based learning" although it is also known as k-nearest neighbor learning. 
Their system is known as LEXAS (LEXical Ambiguity-resolving System), a supervised 
learning approach which requires disambiguated training text. LEXAS was based on 
PEBLS, a publically available xemplar-based learning algorithm. 
A set of features is extracted from disambiguated xample sentences, including 
part-of-speech information, morphological form, surrounding words, local collocates, 
and words in verb-object syntactic relations. When a new, untagged, usage is encoun- 
tered, it is compared with each of the training examples and the distance from each is 
calculated using a metric adopted from Cost and Salzberg (1993). This is calculated as 
the sum of the differences between each pair of features in the two vectors. The differ- 
ences between two values vl and v2 is calculated according to (5), where C1,i represents 
the number of training examples with value Vl that are classified with sense i in the 
training corpus, and C1 the number with value vl in any sense .  C2, i and C2 denote 
similar values and n denotes the total number of senses for the word under consider- 
ation. The sense of the example with the minimum distance from the untagged usage 
is chosen: if there is more than one with the same distance, one is chosen at random 
to break the tie. 
Cl,i C2,i I (5) 
6(Vl, V2) ~- C1 C2 
i=1 
Ng and Lee tested LEXAS on two separate data sets: one used previously in WSD 
research, the other a new, manually tagged, corpus. The common data set was the 
interest corpus constructed by Bruce and Wiebe (1994) consisting of 2,639 sentences 
from the Wall Street Journal, each containing an occurrence of the noun interest. Each 
occurrence is tagged with one of its six possible senses from LDOCE. Evaluation is 
carried out through 100 random trials, each trained on 1,769 sentences and tested on 
the 600 remaining sentences. The average accuracy was 87.4%, significantly higher 
than the figure of 78% reported by Bruce and Wiebe. 
Further evaluation was carried out on a larger data set constructed by Ng and 
Lee. This consisted of 192,800 occurrences of the 121 nouns and 70 verbs that are "the 
most frequently occurring and ambiguous words in English" (Ng and Lee 1996, 44). 
The corpus was made up from the Brown Corpus (Ku~era nd Francis 1967) and the 
Wall Street Journal Corpus and was tagged with the correct senses from WordNet 
by university undergraduates specializing in linguistics. Before training, two subsets 
of the corpus were put aside as test sets: the first (B?50) contains 7,119 occurrences 
of the ambiguous words from the Brown Corpus, while the second (WSd6) contained 
14,139 from the Wall Street Journal Corpus. LEXAS correctly disambiguated 54% of 
words in BCS0 and 68.6% in WSJ6. Ng and Lee point out that both results are higher 
than choosing the first, or most frequent, sense in each of the corpora. The authors 
325 
Computational Linguistics Volume 27, Number 3 
Table 1 
Relative contribution of knowledge sources in LEXAS. 
Knowledge Source Accuracy 
Collocations 80.2% 
PoS and Morphology 77.2% 
Surrounding words 62.0% 
Verb-object 43.5% 
attribute the lower performance on the Brown Corpus to the wider variety of text 
types it contains. 
Ng and Lee attempted to determine the relative contribution of each knowledge 
source. This was carried out by re-running the data from the "interest" corpus through 
the learning algorithm, this time removing all but one set of features. The results are 
shown in Table 1. They found that the local collocations were the most useful knowl- 
edge source in their system. However, it must be remembered that this experiment 
was carried out on a data set consisting of a single word and may, therefore, not be 
generalizable. 
2.4 Discussion 
This review has been extremely brief and has not covered large areas of research into 
WSD. For example, we have not discussed connectionist approaches, as used by Waltz 
and Pollack (1985), V6ronis and Ide (1990), Hirst (1987), and Cottrell (1984), However, 
we have attempted to discuss some of the approaches to combining diverse types of 
linguistic knowledge for WSD and have concentrated on those which are related to 
the techniques used in our own disambiguation system. 
Of central interest to our research is the relative contribution of the various knowl- 
edge sources which have been applied to the WSD problem. Both Ng and Lee (1996) 
and Yarowsky (1993) reported some results in the area. However, Ng and Lee reported 
results for only a single word and Yarowsky considers only words with two possible 
senses. This paper is an attempt to increase the scope of this research by discussing 
a disambiguation algorithm which operates over all content words and combines a
varied set of linguistic knowledge sources. In addition, we examine the relative ffect 
of each knowledge source to gauge which are the most important, and under what 
circumstances. 
We first report an in-depth study of a particular knowledge source, namely part- 
of-speech tags. 
3. Part of Speech and Word Senses 
3.1 LDOCE 
The experiments described in this section use the Longman Dictionary of Contemporary 
English (LDOCE) (Procter 1978). LDOCE is a learners' dictionary, designed for students 
of English, containing roughly 36,000 word types. LDOCE was innovative in its use 
of a defining vocabulary of 2,000 words with which the definitions were written. If 
a learner of English could master this small core then, it was assumed, they could 
understand every entry in the dictionary. 
In LDOCE, the senses for each word type are grouped into homographs: ets of 
senses with related meanings. For example, one of the homographs of bank means 
326 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
bank  1 n I land along the side of a river, lake, etc. 2 earth which is heaped up in a 
field or a garden, often making a border or division 3 a mass of snow, mud, clouds, 
etc.: The banks of dark cloud promised a heavy storln 4 a slope made at bends in a road or 
race-track, so that they are safer for cars to go round 5 SANDBANK: The Dogger Bank 
in the North Sea can be dangerous for ships 
bank  2 v \[If~\] (of a car or aircraft) to move with one side higher than the other, esp. 
when making a turn - see also BANK UP 
bank  3 n 1 a row, esp. of OARs in an ancient boat or KEYs on a TYPEWRITER 
bank  4 n I a place where money is kept and paid out on demand, and where related 
activities go on - see picture at STREET 2 (usu. in comb.) a place where something is
held ready for use, esp. ORGANIC product of human origin for medical use: Hospital 
bloodbanks have saved many lives 3 (a person who keeps) a supply of money or pieces 
for payment or use in a game of chance 4 break the bank to win all the money that 
the BANK4(3) has in a game of chance 
bank  5 v 1\[T1\] to put or keep (money) in a bank 2\[L9, esp. with\] to keep one's money 
(esp. in the stated bank): Where do you bank? 
Figure 1 
The entry for bank in LDOCE (slightly simplified for clarity). 
roughly "things piled up", with different senses distinguishing exactly what is piled 
(see Figure 1). If the senses are sufficiently close together in meaning there will be 
only one homograph for that word, which we then call monohomographic. However, if 
the senses are far enough apart, as in the bank case, they will be grouped into separate 
homographs,  which we call polyhomographic. 
As can be seen from the example ntry, each LDOCE homograph includes informa- 
tion about the part of speech with which the homograph is marked and that applies 
to each of the senses within that homograph.  The vast majority of homographs in 
LDOCE are marked with a single part of speech; however, about 2% of word types in 
the dictionary contain a homograph that is marked with more than one part of speech 
(e.g., noun or verb), meaning that either part of speech may apply. 
Although the granularity of the distinction between homographs in LDOCE is 
rather coarse-grained, they are, as we noted at the beginning of this paper, an appro- 
priate level for many practical computational linguistic applications. For example, bank 
in the sense of "financial institution" translates to banque in French, but when used 
in the "edge of r iver" sense it translates as bord. This level of semantic disambigua- 
tion is frequently sufficient for choosing the correct arget word in an English-to-French 
Machine Translation system and is at a similar level of granularity to the sense distinc- 
tions explored by other researchers in WSD, for example Brown et al (1991), Yarowsky 
(1996), and McRoy (1992) (see Section 2). 
327 
Computational Linguistics Volume 27, Number 3 
3.2 Using Part-of-Speech Information to Resolve Senses 
We began by examining the potential usefulness of part-of-speech information for 
sense resolution. It was found that 34% of the content-word types in LDOCE were 
polysemous, and 12% polyhomographic. (Polyhomographic words are necessarily pol- 
ysemous ince each homograph is a non-empty set of senses.) If we assume that the 
part of speech of each polyhomographic word in context is known, then 88% of word 
types would be disambiguated to the homograph level. (In other words, 88% do not 
have two homographs with the same part of speech.) Some words will be disam- 
biguated to the homograph level if they are used in a certain part of speech but not 
others. For example, beam has 3 homographs in LDOCE; the first two are marked as 
nouns while the third is marked as verb. This word would be disambiguated if used 
as a verb but not if used as a noun. If we assume that every word of this type is 
assigned a part of speech which disambiguates it (i.e., verb in the case of beam), then 
an additional 7% of words in LDOCE could, potentially, be disambiguated. Therefore, 
up to 95% of word types in LDOCE can be disambiguated to the homograph level 
by part-of-speech information alone. However, these figures do not take into account 
either errors in part-of-speech tagging or the corpus distribution of tokens, since each 
word type is counted exactly once. 
The next stage in our analysis was to attempt o disambiguate some texts us- 
ing the information obtained from part-of-speech tags. We took five articles from the 
Wall Street Journal, containing 391 polyhomographic content words. These articles were 
manually tagged with the most appropriate LDOCE homograph by one of the authors. 
The texts were then part-of-speech tagged using Brill's transformation-based l arning 
tagger (Brill, 1995). The tags assigned by the Brill tagger were manually mapped onto 
the simpler part-of-speech tag set used in LDOCE. 2 If a word has more than one ho- 
mograph with the same part of speech, then part-of-speech tags alone cannot always 
identify a single homograph; in such cases we chose the first sense listed in LDOCE 
since this is the one which occurs most frequently. 3 
It was found that 87.4% of the polyhomographic content words were assigned 
the correct homograph. A baseline for this task can be calculated by computing the 
number of tokens that would be correctly disambiguated if the first homograph for 
each was chosen regardless of part of speech. 78% of polyhomographic tokens were 
correctly disambiguated this way using this approach. 
These results show there is a clear advantage to be gained (over 42% reduction in 
error rate) by using the very simple part-of-speech-based method described compared 
with simply choosing the first homograph. However, we felt that it would be useful to 
carry out some further analysis of the data. To do this, it is useful to divide the polyho- 
mographic words into four classes, all based on the assumption that a part-of-speech 
tagger has been run over the text and that homographs which do not correspond to 
the grammatical category assigned have been removed. 
Full disambiguation (by part of speech): If only a single homograph with the 
correct part of speech remains, that word has been fully disambiguated 
by the tagger. 
2 The Brill tagger uses the 48-tag set from the Penn Tree Bank (Marcus, Santorini, and Marcinkiewicz 
1993), while LDOCE uses a set of 17 more general tags. Brill's tagger has a reported error rate of 
around 3%, although we found that mapp ing  the Penn TreeBank tags used by Brill onto the simpler 
LDOCE tag set led to a lower error rate. 
3 In the 3rd Edition of LDOCE the publ ishers claim that the senses are indeed ordered by frequency, 
a l though they make no such claim in the 1st Edition used here. However, Guo (1989) found evidence 
that there is a correspondence b tween the order in which senses are listed and the frequency of 
occurrence in the 1st Edition. 
328 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Partial disambiguation (by part of speech): If there is more than one possible ho- 
mograph with the correct part of speech but some have been removed 
from consideration, that word has been partially disambiguated by part 
of speech. 
No disambiguation (by part of speech): If all the homographs of a word have 
the same part of speech, which is then assigned by the tagger, then none 
can be removed and no disambiguation has been carried out. 
Part-of-speech error: It is possible for the part-of-speech tagger to assign an incor- 
rect part of speech, leading to the correct homograph being removed from 
consideration. It is worth mentioning that this situation has two possible 
outcomes: first, some homographs, with incorrect parts of speech, may 
remain; or second, all homographs may have been removed from consid- 
eration. 
In Table 3 we show in the column labelled Count the number of words in our 
five articles which fall into each of the four categories. The relative performance of
the baseline method (choosing the first sense) compared to the reported algorithm 
(removing homographs using part-of-speech tags) are shown in the rightmost two 
columns. The figures in brackets indicate the percentage of polyhomographic words 
correctly disambiguated by each method on a per-class basis. It can be seen that the 
majority of the polyhomographic words (297 of 342) fall into the "Full disambiguation" 
category, all of which are correctly disambiguated by the method reported here. When 
no disambiguation is carried out, the algorithm described simply chooses the first 
sense and so the results are the same for both methods. The only condition under 
which choosing the first sense is more effective than using part-of-speech information 
is when the part-of-speech tagger makes an error and all the homographs with the 
correct part of speech are removed from consideration. In most cases this means that 
the correct homograph cannot be chosen; however, in a small number of cases, this is 
equivalent to choosing the most frequent sense, since if all possible homographs have 
been removed from consideration, the algorithm reverts to using the simpler heuristic 
of choosing the word's first homograph. 4 
Although this result may seem intuitively obvious, there have, we believe, been no 
other attempts to quantify the benefit o be gained from the application of a part-of- 
speech tagger in WSD (see Wilks and Stevenson 1998a). The method escribed here is 
effective in removing incorrect senses from consideration, thereby reducing the search 
space if combined with other WSD methods. 
In the experiments reported in this section we made use of the particular struc- 
ture of LDOCE, which assigns each sense to a homograph from which its part of 
speech information is inherited. However, there is no reason to believe that the method 
reported here is limited to lexicons with this structure. In fact this approach can 
be applied to any lexicon which assigns part-of-speech information to senses, al- 
though it would not always be possible to evaluate at the homograph level as we 
do here. 
In the remainder of this paper we go on to describe a sense tagger that assigns 
senses from LDOCE using a combination of classifiers. The set of senses considered 
by the classifiers is first filtered using part-of-speech tags. 
4 An example ofthis situation is shown in the bottom row of Table 2. 
329 
Computational Linguistics Volume 27, Number 3 
Table 2 
Examples of the four word types introduced in Section 3.2. The leftmost column indicates the 
full set of homographs for the example words, with upper case indicating the correct 
homograph. The remaining columns how (respectively) the part-of-speech assigned by the 
tagger, the resulting set of senses after filtering, and the type of the word. 
All PoS After Word type 
Homographs Tag tagging 
N, v, v n N Full disambiguation 
n, adj, V v V Full disambiguation 
n, V, v v V, v Partial disambiguation 
n, N, v n n, N Partial disambiguation 
N, n n N, n No disambiguation 
v, V v v, V No disambiguation 
N, v, v v v v PoS error 
N, v, v adj N, v, v PoS error 
Table 3 
Error analysis for the experiment on WSD by part of speech alone. 
Correctly disambiguated by: 
Word Type Count Baseline method PoS method 
Full disambiguation 297 268 (90%) 297 (100%) 
Partial disambiguation 58 22 (38%) 32 (55%) 
No disambiguation 23 10 (43%) 10 (43%) 
Part-of-speech error 13 5 (38%) 3 (23%) 
All polyhomographic 391 305 (78%) 342 (87%) 
4. A Sense Tagger which Combines Knowledge Sources 
We adopt a f ramework in which different knowledge sources are appl ied as separate 
modules. One type of module,  a filter, can be used to remove senses from consideration 
when a knowledge source identifies them as unlikely in context. Another type can be 
used when a knowledge source provides evidence for a sense but cannot identify 
it confidently; we call these partial taggers (in the spirit of McCarthy's notion of 
"partial information" \[McCarthy and Hayes, 1969\]). The choice of whether to apply a 
knowledge source as either a filter or a partial tagger depends on whether it is likely to 
rule out correct senses. If a knowledge source is unlikely to reject the correct sense, then 
it can be safely implemented as a filter; otherwise implementat ion as a partial tagger 
would be more appropriate. In addition, it is necessary to represent he context of 
ambiguous words so that this information can be used in the disambiguation process. 
In the system described here these modules are referred to as feature extractors. 
Our sense tagger is implemented within this modular  architecture, one where 
each module is a filter, partial tagger, or feature extractor. The architecture of the 
system is represented in Figure 2. This system currently incorporates a single fil- 
ter (par t -o f - speech  f i l te r ) ,  three partial taggers (s imulated anneal ing,  sub jec t  
codes, se lec t iona l  res t r i c t ions )  and a single feature extractor (co l locat ion  ex- 
t rac tor ) .  
330 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  p 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Figure 2 
Sense tagger architecture. 
331 
Computational Linguistics Volume 27, Number 3 
4.1 Preprocessing 
Before the filters or partial taggers are applied, the text is tokenized, lemmatized, 
split into sentences, and part-of-speech tagged, again using Brill's tagger. A named 
entity identifier is then run over the text to mark and categorize proper names, which 
will provide information for the selectional restrictions partial tagger (see Section 4.4). 
These preprocessing stages are carried out by modules from Sheffield University's 
Information Extraction system, LaSIE, and are described in more detail by Gaizauskas 
et al (1996). 
Our system disambiguates only the content words in the text, and the part-of- 
speech tags are used to decide which are content words. There is no attempt to dis- 
ambiguate any of the words identified as part of a named entity. These are excluded 
because they have already been analyzed semantically by means of the classification 
added by the named entity identifier (see Section 4.4). Another eason for not attempt- 
ing WSD on named entities is that when words are used as names they are not being 
used in any of the senses listed in a dictionary. For example, Rose and May are names 
but there are no senses in LDOCE for this usage. It may be possible to create a dummy 
entry in the set of LDOCE senses indicating that the word is being used as a name, 
but then the sense tagger would simply repeat work carried out by the named entity 
identifier. 
4.2 Part-of-Speech filtering 
We take the part-of-speech tags assigned by the Brill tagger and use a manually created 
mapping to translate these to the corresponding LDOCE grammatical category (see 
Section 3.2). Any senses which do not correspond to the category returned are removed 
from consideration. I  practice, the filtering is carried out at the same time as the lexical 
lookup phase and the senses whose grammatical categories do not correspond to the 
tag assigned are never attached to the ambiguous word. There is also an option of 
turning off filtering so that all senses are attached regardless of the part-of-speech tag. 
If none of the dictionary senses for a given word agree with the part-of-speech tag 
then all are kept. 
It could be reasonably argued that removing senses is a dangerous strategy since, 
if the part-of-speech tagger made an error, the correct sense could be removed from 
consideration. However, the experiments described in Section 3.2 indicate that part-of- 
speech information isunlikely to reject he correct sense and can be safely implemented 
as a filter. 
4.3 Optimizing Dictionary Definition Overlap 
Lesk (1986) proposed that WSD could be carried out using an overlap count of content 
words in dictionary definitions as a measure of semantic loseness. This method would 
tag all content words in a sentence with their senses from a dictionary that contains 
textual definitions. However, it was found that the computations which would be 
necessary to test every combination of senses, even for a sentence of modest length, 
was prohibitive. 
The approach was made practical by Cowie, Guthrie, and Guthrie (1992) (see 
also (Wilks, Slator, and Guthrie 1996)). Rather than computing the overlap for all 
possible combinations ofsenses, an approximate solution is identified by the simulated 
annealing optimization algorithm (Metropolis et al 1953). Although this algorithm is 
not guaranteed to find the global solution to an optimization problem, it has been 
shown to find solutions that are not significantly different from the optimal one (Press 
et al 1988). Cowie et al used LDOCE for their implementation a d found it correctly 
disambiguated 47% of words to the sense level and 72% to the homograph level 
332 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Z 
(no semantic restriction) 
T, W, X, Y, 2, 4 ~  
(abstract) 
I,W ? ~ ~ Q ' Y ' 5  
(animate) 
S,E, 1,2,5 L,E, 6,7 G, 7 PV '~A,  O/,V H~OO, 
(solid) (liquid) (gas) (plant) (ani~{al) (~umaXn~ 
\] N B,R D~K M,K F,R 
(movable (nonmovable (animal (ammal (human (human 
solid) solid) male) female) male) female) 
Figure 3 
Bruce and Guthrie's hierarchy of LDOCE semantic odes. 
when compared with manually assigned senses. The optimization must be carried out 
relative to a function that evaluates the suitability of a particular choice of senses. In 
the Cowie et al implementation this was done using a simple count of the number 
of words (tokens) in common between all the definitions for a given choice of senses. 
However, this method prefers longer definitions, since they have more words that 
can contribute to the overlap, and short definitions or definitions by synonym are 
correspondingly penalized. We addressed this problem by computing the overlap in a 
different way: instead of each word contributing one, we normalized its contribution 
by the number of words in the definition it came from. In their implementation Cowie 
et al also added pragmatic odes to the overlap computation; however, we prefer to 
keep different knowledge sources eparate and use this information in another partial 
tagger (see Section 4.5). The Cowie et al implementation returned one sense for each 
ambiguous word in the sentence without any indication of the system's confidence 
in its choice, but we adapted the system to return a set of suggested senses for each 
ambiguous word in the sentence. 
4.4 Selectional Preferences 
Our next partial tagger eturns the set of senses for each word that is licensed by 
selectional preferences (in the sense of Wilks 1975). LDOCE senses are marked with 
selectional restrictions expressed by 36 semantic odes not ordered in a hierarchy. 
However, the codes are clearly not of equal evels of generality; for example, the code H 
is used to represent all humans, while M represents human males. Thus for a restriction 
with type H, we would want to allow words with the more specific semantic lass M to 
meet it. This can be computed if the semantic ategories are organized into a hierarchy. 
Then all categories subsumed by another category will be regarded as satisfying the 
restriction. Bruce and Guthrie (1992) manually identified relations between the LDOCE 
semantic lasses, grouping the codes into small sets with roughly the same meaning 
and attached escriptions; for example M, K are grouped as a pair described as "human 
male". The hierarchy produced is shown in Figure 3. 
333 
Computational Linguistics Volume 27, Number 3 
Table 4 
Mapping of named entities onto LDOCE semantic odes. The named entities can be mapped 
to any semantic ode within a particular node of the hierarchy since the disambiguation 
algorithm treats all codes in the same node as equivalent. 
Named Entity Type LDOCE code 
PERSON H (= Human) 
ORGANIZATION T (= Abstract) 
LOCATION N (= Non-movable solid) 
DATE T (---- Abstract) 
TIME T (= Abstract) 
MONEY T (= Abstract) 
PERCENT T (---- Abstract) 
UNKNOWN Z (---- No  semantic restriction) 
The named entities identified as part of the preprocessing phase (Section 4.1) are 
used by this module, which requires first a mapping between the name types and 
LDOCE semantic odes, shown in Table 4. 
Any use of preferences for sense selection requires prior identification of the site 
in the sentence where such a relationship holds. Although prior identification was not 
done by syntactic methods in Wilks (1975), it is often easiest o think of the relation- 
ships as specified in grammatical terms, e.g., as subject-verb, verb-object, adjective- 
noun etc. We perform this step by means of a shallow syntactic analyzer (Stevenson 
1998) which finds the following grammatical relations: the subject, direct and indirect 
object of each verb (if any), and the noun modified by an adjective. Stevenson (1998) 
describes an evaluation of this system in which the relations identified were compared 
with those derived from Penn TreeBank parses (Marcus, Santorini, and Marcinkiewicz 
1993). It was found that the parser achieved 51% precision and 69% recall. 
The preference resolution algorithm begins by examining a verb and the nouns 
it dominates. Each sense of the verb applies a preference to those nouns such that 
some of their senses may be disallowed. Some verb senses will disallow all senses for 
a particular noun it dominates and these senses of the verb are immediately rejected. 
This process leaves us with a set of verb senses that do not conflict with the nouns 
that verb governs, and a set of noun senses licensed by at least one of those verb 
senses. For each noun, we then check whether it is modified by an adjective. If it is, 
we reject any senses of the adjectives which do not agree with any of the remaining 
noun senses. This approach is rather conservative in that it does not reject a sense 
unless it is impossible for it to fit into the preference pattern of the sentence. 
In order to explain this process more fully we provide a walk-through explanation 
of the procedure applied to a toy example shown in Table 5. It is assumed that the 
named-entity identifier has correctly identified John as a person and that the shallow 
parser has found the correct syntactic relations. In order to make this example as 
straightforward aspossible, we consider only the case in which the ambiguous words 
have few senses. The disambiguation process operates by considering the relations 
between the words in known grammatical relations, and before it begins we have 
essentially a set of possible senses for each word related via their syntax. This situation 
is represented by the topmost ree in Figure 4. 
Disambiguation is carried out by considering each verb sense in turn, beginning 
with run(l). As run is being used transitively, it places two restrictions on the sentence: 
first, the subject must satisfy the restriction human and the object abstract. In this 
334 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 5 
Sentence and lexicon for toy example of selectional preference resolution algorithm. 
Example sentence: 
John ran the hilly course. 
Sense Definition and Example Restriction 
John 
ran (1) 
ran (2) 
hilly (1) 
course (1) 
course (2) 
proper name 
to control an organisation run IBM 
to move quickly by foot run a marathon 
undulating terrain hilly road 
route race course 
programme of study physics course 
type:human 
subject:human object:abstract 
subject:human object:inanimate 
modifies:nonmovable sol id 
type:noumovable solid 
type:abstract 
run(l) 
restriction:human restriction:abstract 
John course(2) 
{ run(1 ),run(2) }
~ b j e c t - ~ b  I 
John { course(1),course(2) } 
f 
I adjective-noun~ 
I 
{hilly(l)} 
run(2) 
restriction:human restriction:inanimate 
John course(I) 
type:nonmovable solid 
hilly(l) 
Figure 4 
Restriction resolution in toy example. 
example, John has been identified as a named entity and marked as human, so the 
subject restriction is not broken. Note that, if the restriction were broken, then the 
verb sense run(l) would be marked as incorrect by this partial tagger and no further 
attempt would be made to resolve its restrictions. As this was not the case, we consider 
the direct-object slot, which places the restriction abst rac t  on the noun which fills it. 
course(2) fulfils this criterion, course is modif ied by hilly which expects a noun of type 
noumovable so l id .  However,  course(2) is marked abst rac t ,  which does not comply 
with this restriction. Therefore, assuming that run is being used in its second sense 
leads to a situation in which there is no set of senses which comply with all the 
restrictions placed on them; therefore run(l) is not the correct sense of run and the 
partial tagger marks this sense as wrong. This situation is represented by the tree at 
the bottom left of Figure 4. The sense course(2) is not rejected at this point since it may  
be found to be acceptable in the configuration of senses of another sense of run. 
The algorithm now assumes that run(2) is the correct sense. This implies that 
course(I) is the correct sense as it complies with the inanimate restriction that that verb 
sense places on the direct object. As well as complying with the restriction imposed 
by run(2), course(I) also complies with the one imposed by hilly(i), since nonmovable 
so l id  is subsumed by inanimate.  Therefore, assuming that the senses run(2) and 
335 
Computational Linguistics Volume 27, Number 3 
course(I) are being used does not lead to any restrictions being broken and the algo- 
rithm marks these as correct. 
Before leaving this example it is worth discussing a few additional points. The 
sense course(2) is marked as incorrect because there is no sense of run with which an 
interpretation of the sentence can be constructed using course(2). If there were further 
senses of run in our example, and course(2) was found to be suitable for those extra 
senses, then the algorithm would mark the second sense of course as correct. There is, 
however, no condition under which run(l) could be considered as correct hrough the 
consideration of further verb senses. Also, although John and hilly are not ambiguous in 
this example, they still participate in the disambiguation process. In fact they are vital 
to its success, as the correct senses could not have been identified without considering 
the restrictions placed by the adjective hilly. 
This partial tagger eturns, for all ambiguous noun, verb, and adjective occurrences 
in the text, the set of senses which satisfy the preferences imposed on those words. 
Adverbs do not have any selectional preferences in LDOCE and so are ignored by this 
partial tagger. 
4.5 Subject Codes 
Our final partial tagger is a re-implementation f the algorithm developed by Yarowsky 
(1992). This algorithm is dependent upon a categorization of words in the lexicon 
into subject areas--Yarowsky used the Roget large categories. In LDOCE, primary 
pragmatic odes indicate the general topic of a text in which a sense is likely to be 
used. For example, LN means "Linguistics and Grammar" and this code is assigned 
to some senses of words such as "ellipsis", "ablative", "bilingual" and "intransitive". 
Roget is a thesaurus, o each entry in the lexicon belongs to one of the large categories; 
but over half (56%) of the senses in LDOCE are not assigned a primary code. We 
therefore created a dummy category, denoted by --,  used to indicate a sense which 
is not associated with any specific subject area and this category is assigned to all 
senses without a primary pragmatic ode. These differences between the structures 
of LDOCE and Roget meant that we had to adapt the original algorithm reported in 
Yarowsky (1992). 
In Yarowsky's implementation, the correct subject category is estimated by apply- 
ing (6), which maximizes the sum of a Bayesian term (the fraction on the right) over 
all possible subject categories (SCat) for the ambiguous word over the words in its 
context (w). A context of 50 words on either side of the ambiguous word is used. 
ARGMAX Pr( w\[ S Cat) Pr( SCat) 
scat ~ log Pr(w) (6) 
w e context  
Yarowsky assumed the prior probability of each subject category to be constant, 
so the value Pr(SCat) has no effect on the maximization in (6), and (7) was in effect 
being maximized. 
ARCMAX Pr (w\]SCat) 
SCat ~ log Pr(w) (7) 
w e context  
By including a general pragmatic ode to deal with the lack of coverage, we created 
an extremely skewed distribution of codes across senses and Yarowsky's assumption 
that subject codes occur with equal probability is unlikely to be useful in this ap- 
plication. We gained a rough estimate of the probability of each subject category by 
determining the proportion of senses in LDOCE to which it was assigned and apply- 
ing the maximum likelihood estimate. It was found that results improved when the 
336 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
rough estimate of the likelihood of pragmatic odes was used. This procedure gener- 
ates estimates based on counts of types and it is possible that this estimate could be 
improved by counting tokens, although the problem of polysemy in the training data 
would have to be overcome in some way. 
The algorithm relies upon the calculation of probabilities gained from corpus tatis- 
tics: Yarowsky used the Grolier's Encyclopaedia, which comprised a 10 million word 
corpus. Our implementation used nearly 14 million words from the non-dialogue 
portion of the British National Corpus (Burnard 1995). Yarowsky used smoothing pro- 
cedures to compensate for data sparseness in the training corpus (detailed in Gale, 
Church, and Yarowsky \[1992b\]), which we did not implement. Instead, we attempted 
to avoid this problem by considering only words which appeared at least 10 times 
in the training contexts of a particular word. A context model is created for each 
pragmatic ode by examining 50 words on either side of any word in the corpus con- 
taining a sense marked with that code. Disambiguation is carried out by examining the 
same 100 word context window for an ambiguous word and comparing it against he 
models for each of its possible categories. Further details may be found in Yarowsky 
(1992). 
Yarowsky reports 92% correct disambiguation ver 12 test words, with an average 
of three possible Roget large categories. However, LDOCE has a higher level of aver- 
age ambiguity and does not contain as complete a thesaural hierarchy as Roget, so we 
would not expect such good results when the algorithm is adapted to LDOCE. Con- 
sequently, we implemented the approach as a partial tagger. The algorithm identifies 
the most likely pragmatic ode and returns the set of senses which are marked with 
that code. In LDOCE, several senses of a word may be marked with the same prag- 
matic code, so this partial tagger may return more than one sense for an ambiguous 
word. 
4.6 Collocation Extractor 
The final disambiguation module is the only feature-extractor in our system and is 
based on collocations. A set of 10 collocates are extracted for each ambiguous word 
in the text: first word to the left, first word to the right, second word to the left, 
second word to the right, first noun to the left, first noun to the right, first verb to 
the left, first verb to the right, first adjective to the left, and first adjective to the 
right. Some of these types of collocation were also used by Brown et al (1991) and 
Yarowsky (1993) (see Section 2.3). All collocates are searched for within the sentence 
which contains the ambiguous word. If some particular collocation does not exist for 
an ambiguous word, for example if it is the first or last word in a sentence, then a 
null value (NoColl) is stored instead. Rather than storing the surface form of the co- 
occurrence, morphological roots are stored instead, as this allows for a smaller set of 
collocations, helping to cope with data sparseness. The surface form of the ambiguous 
word is also extracted from the text and stored. The extracted collocations and surface 
form combine to represent the context of each ambiguous word. 
4.7 Combining Disambiguation Modules 
The results from the disambiguation modules (filter, partial taggers, and feature x- 
tractor) are then presented to a machine learning algorithm to combine their results. 
The algorithm we chose was the TIMBL memory-based learning algorithm (Daelemans 
et al 1999). Memory-based learning is another name for exemplar-based learning, as 
employed by Ng and Lee (Section 2.3). The TiMBL algorithm has already been used for 
various NLP tasks including part-of-speech tagging and PP-attachment (Daelemans et 
al. 1996; Zavrel, Daelemans, and Veenstra 1997). 
337 
Computational Linguistics Volume 27, Number 3 
Like PEBLS, which formed the core of Ng and Lee's LEXAS system, TiMBL classifies 
new examples by comparing them against previously seen cases. The class of the most 
similar example is assigned. At the heart of this approach is the distance metric A(X, Y) 
which computes the similarity between instances X and Y. This measure is calculated 
using the weighted overlap metric shown in (8), which calculates the total distance by 
computing the sum of the distance between each position in the feature vector. 
n 
A(X, Y) =- ~_, wi6(xi, yi) (8) 
i=1 
where: 
xl-yi if numeric, else ~ axi-  min~ ?5(xi, y i )  = i f  Xi = y i  (9) 
if xi # yi 
From (9) we can see that TiMBL treats numeric and symbolic features differently. 
For numeric features, the unweighted distance is computed as the difference between 
the values for that feature in each instance, divided by the maximum possible dis- 
tance computed over all pairs of instances in the database. 5 For symbolic features, the 
unweighted istance is 0 if they are identical, and 1 otherwise. For both numeric and 
symbolic features, this distance is multiplied by the weight for the particular feature, 
based on the Gain Ratio measure introduced by Quinlan (1993). This is a measure of 
the difference in uncertainty between the situations with and without knowledge of 
the value of that feature, as in (10). 
H(C) - ~-,v Pr(v) x H(CIv) (10) 
wi = H(v) 
Where C is the set of classifications, v ranges over all values of the feature i and 
H(C) is the entropy of the class labels. Probabilities are estimated from frequency 
of occurrence in the training data. The numerator of this formula determines the 
knowledge about the distribution of classes that is added by knowing the value of 
feature i. However, this measure can overestimate he value of features with large 
numbers of possible values. To compensate, it is divided by H(v), the entropy of the 
feature values. 
Word senses are presented to TiMBL in a feature-vector representation, with each 
sense which was not removed by the part of speech filter being represented by a 
separate vector. The vectors are formed from the following pieces of information in 
order: headword, homograph number, sense number, rank of sense (the order of the 
sense in the lexicon), part of speech from lexicon, output from the three partial tag- 
gers (simulated annealing, subject codes, and selectional restrictions), sur- 
face form of headword from the text, the ten collocates, and an indicator of whether 
the sense is appropriate or not in the context (correct or incorrect). 
Figure 5 shows the feature vectors generated for the word influence in the context 
shown. The final value in the feature vector shows whether the sense is correct or 
not in the particular context. We can see that, in this case, there is one correct sense, 
influence_l_la, the definition of which is "power to gain an effect on the mind of 
5 An earlier version of this system (Stevenson and Wilks 1999) used TiMBL version 1.0 (Daelemans et al 
1998), which supports only symbolic features. 
338 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Context 
Regard ing At lanta's new mil l ion dollar airport, the jury recommended "that when the new management  take 
charge Jan. 1 the airport be operated in a manner  that will  el iminate political influences". 
Feature Vectors 
Learning features Truth 
influence 1 la 1 n influences 1 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate correct 
influence 1 lb 2 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 2 3 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 3 4 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 4 5 n influences 0 12.03 n NoColl manner NoColl eliminate NoCofl in NoColl political NoColl eliminate incorrect 
influence 1 5 6 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoCon political NoColl eliminate incorrect 
influence 1 6 7 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
Figure 5 
Example feature-vector representation. 
or get results from, without asking or doing anything". Features 10-19 are produced 
by the collocation extractor, and these are identical since each vector is taken from 
the same content. Features 7-9 show the results of the partial taggers. The first is the 
output from simulated annealing, the second the subject  code, and the third the 
se lect iona l  res t r i c t ions .  All noun senses of influence share the same pragmatic 
code (--), and consequently this partial tagger eturns the same score for each sense. 
A final point worth noting is that in LDOCE, influence has a verb sense which the 
part-of-speech filter removed from consideration, and consequently this sense is not 
included in the feature-vector representation. 
The TiMBL algorithm is trained on tokens presented in this format. When disam- 
biguating unannotated text, the algorithm is applied to data presented in the same 
format without the classification. The unclassified vectors are then compared with all 
the training examples, and it is assigned the class of the closest one. 
5. Evaluation Strategy 
5.1 Evaluation Corpus 
The evaluation of WSD algorithms has recently become a much-studied area. Gale, 
Church, and Yarowsky (1992a), Resnik and Yarowsky (1997), and Melamed and Resnik 
(2000) each presented arguments for adopting various evaluation strategies, with 
Resnik and Yarowsky's proposal directly influencing the set-up of SENSEVAL (Kil- 
garriff 1998). At the heart of their proposals is the ability of human subjects to mark 
up text with the phenomenon i question (WSD in this case) and evaluate the results 
of computation. This linguistic phenomenon has proved to be far more elusive and 
complex than many others. We have discussed this at length elsewhere (Wilks 1997) 
and will assume here that humans can mark up text for senses to a sufficient degree. 
Kilgarriff (1993) questioned the possibility of creating sense-tagged texts, claiming the 
task to be impossible. However, it should be borne in mind that no alternative has 
yet been widely accepted and that Kilgarriff himself used the markup-and-test model 
for SENSEVAL. In the following discussion we compare the evaluation methodology 
adopted here with those proposed by others. 
339 
Computational Linguistics Volume 27, Number 3 
The standard evaluation procedure for WSD is to compare the output of the sys- 
tem against gold standard texts, but these are very labor-intensive to obtain; lexical 
semantic markup is generally considered to be a more difficult and time-consuming 
task than part-of-speech markup (Fellbaum et al 1998). Rather than expend a vast 
amount of effort on manual tagging we decided to combine two existing resources: 
SEMCOR (Landes, Leacock, and Tengi 1998), and SENSUS (Knight and Luk 1994). 
SEMCOR is a 200,000 word corpus with the content words manually tagged as part 
of the WordNet project. The semantic tagging was carried out by trained lexicogra- 
phers under disciplined conditions that attempted to keep tagging inconsistencies to
a minimum. SENSUS is a large-scale ontology designed for machine-translation a d 
was itself produced by merging the ontological hierarchies of WordNet, LDOCE (as 
derived by Bruce and Guthrie, see Section 4.4), and the Penman Upper Model (Bate- 
man et al, 1990) from ISI. To facilitate the merging of these three resources to produce 
SENSUS, Knight and Luk were required to derive a mapping between the senses in the 
two lexical resources. We used this mapping to translate the WordNet-tagged content 
words in SEMCOR to LDOCE tags. 
The mapping of senses is not one-to-one, and some WordNet synsets are mapped 
onto two or three LDOCE senses when WordNet does not distinguish between them. 
The mapping also contained significant gaps, chiefly words and senses not in the 
translation scheme. SEMCOR contains 91,808 words tagged with WordNet synsets, 
6,071 of which are proper names, which we ignored, leaving 85,737 words which 
could potentially be translated. The translation contains only 36,869 words tagged 
with LDOCE senses; however, this is a reasonable size for an evaluation corpus for the 
task, and it is several orders of magnitude larger than those used by other researchers 
working in large vocabulary WSD, for example Cowie, Guthrie, and Guthrie (1992), 
Harley and Glennon (1997), and Mahesh et al (1997). This corpus was also constructed 
without the excessive cost of additional hand-tagging and does not introduce any of 
the inconsistencies that can occur with a poorly controlled tagging strategy. 
Resnik and Yarowsky (1997) proposed to evaluate large vocabulary WSD systems 
by choosing a set of test words and providing annotated test and training examples 
for just these words, allowing supervised and unsupervised algorithms to be tested 
on the same vocabulary. This model was implemented in SENSEVAL (Kilgarriff 1998). 
However, for the evaluation of the system presented here, there would have been 
no benefit from using this strategy since it still involves the manual tagging of large 
amounts of data and this effort could be used to create a gold standard corpus in 
which all content words are disambiguated. It is possible that some computational 
techniques may evaluate well over a small vocabulary but may not work for a large 
set of words, and the evaluation strategy proposed by Resnik and Yarowsky will not 
discriminate between these cases. 
In our evaluation corpus, the most frequent ambiguous type is have, which appears 
604 times. A large number of words (2407) occur only once, and nearly 95% have 25 
occurrences or less. Table 6 shows the distribution of ambiguous types by number of 
corpus tokens. It is worth noting that, as would be expected, the observed istribution 
is highly Zipfian (Zipf 1935). 
Differences in evaluation corpora makes comparison difficult. However, some idea 
of the difficulty of WSD can be gained by calculating properties of the evaluation cor- 
pus. Gale, Church, and Yarowsky (1992a) suggest hat the lowest level of performance 
which can be reasonably expected from a WSD system is that achieved by assigning 
the most likely sense in all cases. Since the first sense in LDOCE is usually the most 
frequent, we calculate this baseline figure using a heuristic which assumes the first 
sense is always correct. This is the same baseline heuristic we used for the experiments 
340 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 6 
Occurrence of ambiguous words in the evaluation corpus. 
Occurrence Range Count 
1-25 5488 (94.6%) 
26-50 202 (3.5%) 
51-75 67 (1.2%) 
76-100 21 (0.04%) 
100-604 26 (0.4%) 
reported in Section 3, although those were for the homograph level. We applied the 
naive heuristic of always choosing the first sense in our corpus and found that 30.9% 
of senses were correctly disambiguated. 
Another measure that gives insight into an evaluation corpus is to count the av- 
erage polysemy, i.e., the number of possible senses we can expect for each ambiguous 
word in the corpus. The average polysemy is calculated by counting the sum of pos- 
sible senses for each ambiguous token and dividing by the number of tokens. This is 
represented by (11), where w ranges over all ambiguous tokens in the corpus, S(w) is 
the number of possible senses for word w, and N is the number of ambiguous tokens. 
The average polysemy for our evaluation corpus is 14.62. 
Average polysemy = ~w in text S( w) (11) 
N 
Our annotated corpus has the unusual property that more than one sense may 
be marked as correct for a particular token. This is an unavoidable side-effect of a 
mapping between lexicon senses which is not one-to-one. However, it does not imply 
that WSD is easier in this corpus than one in which only a single sense is marked 
for each token, as can be shown from an imaginary example. The worst case for a 
WSD algorithm is when each of the possible semantic tags for a given word occurs 
with equal frequency in a corpus, and so the prior probabilities exhibit a uniform, 
uninformative distribution. Then a corpus with an average polysemy of 5, and 2 senses 
marked correct on each ambiguous token, will have a baseline not less than 40%. 
However, one with an average polysemy of 2, and only a single sense on each, will 
have a baseline of at least 50%. Test corpora in which each ambiguous token has 
exactly two senses were used by Brown et al (1991), Yarowsky (1995) and others. 
Our system was tested using a technique known as 10-fold cross validation. This 
process is carried out by splitting the available data into ten roughly equal subsets. 
One of the subsets is chosen as the test data and the TiMBL algorithm is trained on the 
remainder. This is repeated ten times, so that each subset is used as test data exactly 
once, and results are averaged across all of the test runs. This technique provides two 
advantages: first, the best use can be made of the available data, and secondly, the 
computed results are more statistically reliable than those obtained by simply setting 
aside a single portion of the data for testing. 
5.2 Evaluation Metrics 
The choice of scoring metric is an important one in the evaluation of WSD algorithms. 
The most commonly used metric is the ratio of words for which the system has as- 
signed the correct sense compared to those which it attempted todisambiguate. Resnik 
and Yarowsky (1997) dubbed this the exact match metric, which is usually expressed 
341 
Computational Linguistics Volume 27, Number 3 
as a percentage calculated according to the formula in (12). 
Exact match = Number of correctly assigned senses x 100% (12) 
Number of senses assigned 
Resnik and Yarowsky criticize this metric because it assumes a WSD system com- 
mits to a particular sense. They propose an alternative metric based on cross-entropy 
that compares the probabilities for each sense as assigned by a WSD system against 
those in the gold standard text. The formula in (13) shows the method for computing 
this metric, where the WSD system has processed N words and Pr(csi) is the proba- 
bility assigned to the correct sense of word i. 
N 1 
N ~ l?g2 Pr(csi) (13) 
i=1 
This evaluation metric may be useful for disambiguation systems that assign probabil- 
ities to each sense, such as those developed by Resnik and Yarowsky, since it provides 
more information than the exact match metric. However, for systems which simply 
choose a single sense and do not measure confidence, it provides far less information. 
When a WSD assigns only one sense to a word and that sense is incorrect, hat word is 
scored as ~.  Consequently, the formula in (13) returns c~ if there is at least one word 
in the test set for which the tagger assigns a zero probability to the correct sense. For 
WSD systems which assign exactly one sense to each word, this metric returns 0 if 
all words are tagged correctly, and cx~ otherwise. This metric is potentially very useful 
for the evaluation of WSD systems that return non-zero probabilities for each possible 
sense; however, it is not useful for the metric presented in this paper and others that 
are not based on probabilistic models. 
Melamed and Resnik (2000) propose a metric for scoring WSD output when there 
may be more than one correct sense in the gold standard text, as with the evaluation 
corpus we use. They mention that when a WSD system returns more than one sense 
it is difficult to tell if they are intended to be disjunctive or conjunctive. The score 
for a token is computed by dividing the number of correct senses identified by the 
algorithm by the total it returns, making the metric equivalent to precision in infor- 
mation retrieval (van Rijsbergen 1979). 6For systems which return exactly one sense 
for each word, this equates to scoring a token as 1 if the sense returned is correct, and 
0 otherwise. For the evaluation of the system presented here, the metric proposed by 
Melamed and Resnik is then equivalent to the exact match metric. 
The exact match metric has the advantage of being widely used in the WSD lit- 
erature. In our experiments he exact match figure is computed at the LDOCE sense 
level, where the number of tokens correctly disambiguated to the sense level is di- 
vided by the number ambiguous at that level. At the homograph level, the number 
correctly disambiguated to the homograph is divided by the number which are poly- 
homographic. 
6. Performance 
Using the evaluation procedure described in the previous ection, it was found that the 
system correctly disambiguated 90% of the ambiguous instances to the fine-grained 
sense level, and in excess of 94% to the homograph level. 
6 The metric operates lightly differently for systems that assign probabilities to senses, 
342 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 7 
System results, baselines, and corpus characteristics. Sense level results are calculated over all 
polysemous words in the evaluation corpus while those reported for the homograph level are 
calculated only over polyhomographic ones. 
Entire Subcorpora 
Corpus Noun Verb Adjective Adverb 
Sense level Accuracy 90.37% 91.24% 88.38% 91.09% 70.61% 
Baseline 30.90% 34.56% 18.46% 25.76% 36.73% 
Tokens 36,774 26,091 6,465 3,310 908 
Types 5,804 4.041 1,021 1,006 125 
Average Polysemy 14.62 13.65 24.35 6.07 4.43 
Homograph level Accuracy 94.65% 94.63% 95.26% 96.89% 90.67% 
Baseline 71.24% 73.47% 60.72% 87.10% 86.87% 
Tokens 18,219 11,380 5,194 1,326 319 
Types 1,683 1,264 709 201 34 
Average Polysemy 2.52 2.32 2.81 2.95 3.13 
In order to analyze the effectiveness of our tagger in more detail, we split the 
main corpus into sub-corpora by grammatical category. In other words, we created 
four individual sub-corpora containing the ambiguous words which had been part- 
of-speech tagged as nouns, verbs, adjectives, and adverbs. The figures characterizing 
each of these corpora are shown in Table 7. The majority of the ambiguous words 
were nouns, with far fewer verbs and adjectives, and less than one thousand adverbs. 
The average polysemy for nouns, at both sense and homograph levels, is roughly 
the same as the overall corpus average although it is noticably higher for verbs at 
the sense level. At the sense level the average polysemy figures are much lower for 
adjectives and adverbs. This is because it is common for English words to act as either 
a noun or a verb and, since these are the most polysemous grammatical categories, 
the average polysemy count becomes large due to the cumulative ffect of polysemy 
across grammatical categories. However, words that can act as adjectives or adverbs 
are unlikely to be nouns or verbs. This, plus the fact that adjectives and adverbs are 
generally less polysemous in LDOCE, means that their average polysemy in text is far 
lower than it is for nouns or verbs. 
Table 7 shows the accuracy of our system over the four subcorpora. We can see 
that the tagger achieves higher results at the homograph level than the sense level 
on each of the four subcorpora, which is consistent with the result over the whole 
corpus. 
There is quite a difference in the tagger's results across the different subcorpora--  
91% for nouns and 70% for adverbs. Perhaps the learning algorithm does not perform 
as well on adverbs because that corpus is significantly smaller than the other three. 
This hypothesis was checked by testing our system on portions of each of the three 
subcorpora that were roughly equal in size to the adverb subcorpus. We found that the 
reduced data caused a slight loss of accuracy on each of the three subcorpora; how- 
ever, there was still a marked difference between the results for the adverb subcorpus 
and the other three. Further analysis showed that the differences in performance over 
different subcorpora seem linked to the behavior of different partial taggers when 
used in combination. In the following section we describe this behavior in more de- 
tail. 
343 
Computational Linguistics Volume 27, Number 3 
6.1 Interaction of Knowledge Sources 
In order to gauge the contribution of each knowledge source separately, we imple- 
mented a set of simple disambiguation algorithms, each of which uses the output 
from a single partial tagger. Each algorithm takes the result of its partial tagger and 
checks it against the disambiguated text to see if it is correct. If the partial tagger eturns 
more than one sense, as do the simulated annealing, subject code and se lect iona l  
preference taggers, the first sense is taken to break the tie. For the partial tagger based 
on Yarowsky's ubject-code algorithm, we choose the sense with the highest saliency 
value. If more than one sense has been assigned the maximum value, the tie is again 
broken by choosing the first sense. Therefore, each partial tagger eturns a single sense 
and the exact match metric is used to determine the proportion of tokens for which 
that tagger eturns the correct sense. The part-of-speech filter is run before the partial 
taggers make their decision and so they only consider the set of senses it did not re- 
move. The results of each tagger, computed at both sense and homograph levels over 
the evaluation corpus and four subcorpora, re shown in Table 7. 
We can see that the partial taggers that are most effective are those based on the 
simulated annealing algorithm and Yarowsky's ubject code approach. The success of 
these modules upports our decision to use existing disambiguation algorithms that 
have already been developed rather than creating new ones. 
The most successful of the partial taggers is the one based on Yarowsky's algorithm 
for modelling thesaural categories by wide contexts. This consistently achieves over 
70% correct disambiguation a d seems particularly successful when disambiguating 
adverbs (over 85% correct). It is quite surprising that this algorithm is so successful for 
adverbs, since it would seem quite reasonable to expect an algorithm based on subject 
codes to be more successful on nouns and less so on modifiers uch as adjectives and 
adverbs. 
Yarowsky (1992) reports that his algorithm achieves 92% correct disambiguation, 
which is nearly 13% higher than achieved in our implementation. However, Yarowsky 
tested his implementation  a restricted vocabulary of 12 words, the majority of which 
were nouns, and used Roget large categories as senses. The baseline performance for 
this corpus is 66.5%, considerably higher than the 30.9% computed for the corpus 
used in our experiments. Another possible reason for the difference in results is the 
fact that Yarowsky used smoothing algorithms to avoid problems with the probability 
estimates caused by data sparseness. We did not employ these procedures and used 
simple corpus frequency counts when calculating the probabilities ( ee Section 4.5). It 
is not possible to say for sure that the differences between implementations did not 
lead to the differences in results, but it seems likely that the difference in the semantic 
granularity of LDOCE subject codes and Roget categories was an important factor. 
The second partial tagger based on an existing approach is the one which uses 
simulated annealing to optimize the overlap of words shared by the dictionary defini- 
tions for a set of senses. In Section 4.3 we noted that Cowie et al (1992) reported 47% 
correct disambiguation to the sense level using this technique, while in our adaptation 
over 17% more words are correctly disambiguated. Our application filtered out senses 
with the incorrect part of speech in addition to using a different method to calculate 
overlap that takes account of short definitions. It seems likely that these changes are 
the source of the improved results. 
Our least successful partial tagger is the one based on selectional preferences. 
Although its overall result is slightly below the overall corpus baseline, it is very suc- 
cessful at disambiguating verbs. This is consistent with the work of Resnik (1997), who 
reported that many words do not have strong enough selectional restrictions to carry 
out WSD. We expected preferences to be successful for adjectives as well, although 
344 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 8 
Performance of individual partial taggers (at sense level). 
All Nouns Verbs Adjectives Adverbs 
simulated annealing (I) 65.24% 66.50% 67.51% 49.02% 50.61% 
selectional preferences (2) 44.85% 40.73% 75.80% 27.56% 0% 
subject codes (3) 79.41% 79.18% 72.75% 73.73% 85.50% 
this is not the case in our evaluation. This is because the sense discrimination of ad- 
jectives is carried out after that for nouns in our algorithm (see Section 4.4), and the 
former is hindered by the low results of the latter. Adverbs cannot be disambiguated 
by preference methods against LDOCE because it does not contain the appropriate 
information. 
Our analysis of the behavior of the individual partial taggers provides ome clues 
to the behavior of the overall system, consisting of all taggers, on the different sub- 
corpora, as shown in Table 7. The system performs to roughly the same level over 
the noun, verb, and adjective sub-corpora with only a 3% difference between the best 
and worst performance. The system's worst performance is on the abverb sub-corpus, 
where it disambiguates only slightly more than 70% of tokens successfully. This may 
be due to the fact that only two partial taggers provide evidence for this grammatical 
category. However, the system still manages to disambiguate most of the adverbs to the 
homograph level successfully, and this is probably because the part-of-speech filter has 
ruled out the incorrect homographs, not because the partial taggers performed well. 
One can legitimately wonder whether in fact the different knowledge sources for 
WSD are all ways of encoding the same semantic information, in a similar way that 
one might suspect ransformation rules and statistics encode the same information 
about part-of-speech tag sequences in different formats. However, the fact that an op- 
timized combination ofour partial taggers yields a significantly higher figure than any 
one tagger operating independently, shows that they must be orthogonal information 
sources. 
6.2 The overall value of the part-of-speech filter 
We have already examined the usefulness of part-of-speech tags for semantic disam- 
biguation in Section 3. However, we now want to know the effect it has within a 
system consisting of several disambiguation modules. It was found that accuracy at 
the sense level reduced to 87.87% and to 93.36% at the homograph level when the 
filter was removed. Although the system's performance did not decrease by a large 
amount, the part-of-speech filter brings the additional benefit of reducing the search 
space for the three partial taggers. In addition, the fact that these results are not af- 
fected much by the removal of the part-of-speech filter, shows that the WSD modules 
alone do a reasonable job of resolving part-of-speech ambiguity as a side-effect of 
semantic disambiguation. 
7. Conclusion 
Previously reported WSD systems that enjoyed a high level of accuracy have often 
operated on restricted vocabularies and employed a single WSD methodology. These 
methods have often been pursued for sound reasons to do with evaluation, but have 
been limited in their applicability and also in their persuasiveness regarding the scal- 
345 
Computational Linguistics Volume 27, Number 3 
ability and interaction of the various WSD partial methods. This paper reported a 
system which disambiguated all content words in a text, as defined by a standard 
machine readable dictionary, with a high degree of accuracy. 
Our evaluation shows that disambiguation can be carried out with more accurate 
results when several knowledge sources are combined. It remains unclear exactly what 
it means to optimize the combination of modules within a learning system like T?MBL: 
we could, in further work, treat the part-of-speech tagger as a partial tagger and not 
a filter, and we could allow the system to learn some "optimal" weighting of all 
the partial taggers. It also remains an interesting question whether, because of the 
undoubted existence of novel senses in text, a sense tagger can ever reach the level 
that part-of-speech tagging has. However, we believe we have shown that interesting 
combinations of WSD methods on a substantial training corpus are possible, and that 
this can show, among other things, the relative independence of the types of semantic 
information expressed by the various forms of lexical input. 
Acknowledgments 
The work described here was supported by 
the European Union Language Engineering 
project ECRAN - Extraction of Content: 
Research at Near-market (LE-2110). One of 
the authors was also supported by the 
EPSRC grant MALT (GR/M73521) while 
writing this paper. We are grateful for the 
feedback from many colleagues in Sheffield, 
especially Mark Hepple, and for the 
detailed comments from the anonymous 
reviewers of an earlier version of this paper. 
Gillian Callaghan was extremely helpful in 
the preparation of the final version of this 
paper. Any errors are our own. 
References 
Bateman, John, Robert Kasper, Joharu~a 
Moore, and Richard Whimey. 1990. A 
general organization of knowledge for 
natural language processing: the 
PENMAN upper model, Technical report, 
USC/Information Sciences Institute, 
Marina del Rey, CA. 
Brill, Eric. 1995. Transformation-based 
error-driven learning and natural 
language processing: A case study in part 
of speech tagging. Computational 
Linguistics, 21(4):543-566. 
Brown, Peter, Stephen Della Pietra, Vincent 
Della Pietra, and Robert Mercer. 1991. 
Word sense disambiguation using 
statistical methods. In Proceedings ofthe 
29th Meeting of the Association for 
Computational Linguistics (ACL-91), 
pages 264-270, Berkeley, CA. 
Bruce, Rebecca nd Louise Guthrie. 1992. 
Genus disambiguation: A study in 
weighted performance. In Proceedings of
the 14th International Conference on 
Computational Linguistics (COLING-92), 
pages 1187-1191, Nantes, France. 
Bruce, Rebecca nd Janyce Wiebe. 1994. 
Word-sense disambiguation using 
decomposable models. In Proceedings ofthe 
32nd Annual Meeting of the Association for 
Computational Linguistics (ACL-94), 
pages 139-145, Las Cruces, New Mexico. 
Burnard, Lou. 1995. Users Reference Guide for 
the British National Corpus. Oxford 
University Computing Services. 
Chapman, R. L. 1977. Roget's International 
Thesaurus Fourth Edition, Thomas Y. 
Crowell Company, New York, NY. 
Cost, Scott and Steven Salzberg. 1993. A 
weighted nearest neighbour algorithm for 
learning with symbolic features. Machine 
Learning, 10(1):57-78. 
Cottrell, Garrison. 1984. A model of lexical 
access of ambiguous words. In Proceedings 
of the National Conference on Artificial 
Intelligence (AAAI-84), pages 61-67, 
Austin, TX. 
Cowie, Jim, Louise Guthrie, and Joe 
Guthrie. 1992. Lexical disambiguation 
using simulated annealing. In Proceedings 
of the 14th International Conference on 
Computational Linguistics (COLING-92), 
pages 359-365, Nantes, France. 
Daelemans, Walter, Jakub Zavrel, Peter 
Berck, and Steven Gillis. 1996. MBT: A 
memory-based part of speech tagger 
generator. In Proceedings ofthe Fourth 
Workshop on Very Large Corpora, 
pages 14-27, Copenhagen. 
Daelemans, Walter, Jakub Zavrel, Ko van 
der Sloot, and Antal van den Bosch. 1998. 
TiMBL: Tilburg memory based learner 
version 1.0. Technical report, University 
of Tilburg Technical Report 98-03. 
Daelemans, Walter, Jakub Zavrel, Ko van 
der Sloot, and Antal van den Bosch. 1999. 
TiMBL: Tilburg memory based learner, 
version 2.0, reference guide. Technical 
346 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
report, University of Tilburg Technical 
Report 99-01. Available from ht tp : / / i l k .  
kub. nl/~ ilk/papers/ilk990 I. ps. 
Fellbaum, Christiane, Joachim Grabowski, 
Shari Landes, and A. Baumann. 1998. 
Matching words to senses in WordNet: 
Naive vs. expert differentiation of senses. 
In Christiane Fellbaum, editor, WordNet: 
An Electronic Lexical Database and Some 
Applications. MIT Press, Cambridge, MA. 
Gaizauskas, Robert, Takahiro Wakao, Kevin 
Humphreys, Hamish Cunningham, and 
Yorick Wilks. 1996. Description of the 
LaSIE system as used for MUC-6. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 207-220, San Francisco, CA. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992a. Estimating upper and 
lower bounds on the performance of
word sense disambiguation programs. In 
Proceedings ofthe 30th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-92), pages 249-256, Newark, DE. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992b. A method for 
disambiguating word senses in a large 
corpus. Computers and the Humanities, 
26:415-439. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992c. One sense per discourse. 
In Proceedings ofthe DARPA Speech and 
Natural Language Workshop, pages 233-237, 
Harriman, NY. 
Guo, Cheng-Ming. 1989. Constructing a 
Machine Tractable Dictionary from 
Longman Dictionary of Contemporary 
English. Technical Report MCCS-89-156, 
Computing Research Laboratory, New 
Mexico State University. 
Harley, Andrew and Dominic Glennon. 
1997. Sense tagging in action: Combining 
different ests with additive weights. In 
Proceedings ofthe SIGLEX Workshop 
"Tagging Text with Lexical Semantics", 
pages 74-78, Washington, DC. 
Hirst, Graeme. 1987. Semantic Interpretation 
and the Resolution of Ambiguity. Cambridge 
University Press, Cambridge, UK. 
Hirst, Graeme. 1995. Near-synonymy and 
the structure of lexical knowledge. In 
American Association for Artificial Intelligence 
Spring Symposium on Lexicons, pages 51-56. 
Ide, Nancy and Jean V4ronis. 1998. 
Introduction to the special issue on word 
sense disambiguation: The state of the art. 
Computational Linguistics, 24(1):1-40. 
Kilgarriff, Adam. 1993. Dictionary word 
sense distinctions: An enquiry into their 
nature. Computers and the Humanities, 
26:356-387. 
Kilgarriff, Adam. 1997. Sample the lexicon. 
Technical Report ITRI-97-01, ITRI, 
University of Brighton. 
Kilgarriff, Adam. 1998. SENSEVAL: An 
Exercise in Evaluating Word Sense 
Disambiguation Programs. In Proceedings 
of the First International Conference on 
Language Resources and Evaluation, 
pages 581-585, Granada, Spain. 
Knight, Kevin and Steve K. Luk. 1994. 
Building a large knowledge base for 
machine translation. In Proceedings ofthe 
American Association for Arti~cial 
Intelligence Conference (AAAI-94), 
pages 185-109, Seattle, WA. 
Ku~era, Henri and Winthrop Francis. 1967. 
A Computational Analysis of Present-day 
American English. Brown University Press, 
Providence, RI. 
Landes, Shari, Claudia Leacock, and Randee 
Tengi. 1998. Building a semantic 
concordance of English. In C. Fellbaum, 
editor, WordNet: An Electronic Lexical 
Database and Some Applications. MIT Press, 
Cambridge, MA. 
Leacock, Claudia, Geoffrey Towell, and 
Ellen Voorhees. 1993. Corpus-based 
statistical sense resolution. In Proceedings 
of the ARPA Human Language Technology 
Workshop, pages 260-265, Plainsboro, NJ. 
Lesk, Michael. 1986. Automatic sense 
disambiguation using machine readable 
dictionaries: how to tell a pine cone from 
an ice cream cone. In Proceedings ofACM 
SIGDOC Conference, pages 24-26, Toronto. 
Mahesh, Kavi, Sergei Nirenburg, Stephen 
Beale, Evelyne Viegas, Victor Raskin, and 
Boyan Onyshkevych. 1997. Word sense 
disambiguation: Why have statistics when 
we have these numbers? In Proceedings of
the Seventh International Conference on The- 
oretical and Methodological Issues in Machine 
Translation, pages 151-159, Sante Fe, NM. 
Marcus, Mitchell, Beatrice Santorini, and 
Mary Marcinkiewicz. 1993. Building a 
large annotated corpus of English: The 
Penn Tree Bank. Computational Linguistics, 
19(2):313-330. 
Masterman, Margaret. 1957. The thesaurus 
in syntax and semantics. Mechanical 
Translation, 4:1-2. 
McCarthy, J. and P. Hayes. 1969. Some 
philosophical problems from the 
standpoint of artificial intelligence. In B. 
Meltzer and D. Michie, editors, Machine 
Intelligence 4. Edinburgh, Edinburgh 
University Press. pages 463-502. 
McRoy, Susan. 1992. Using multiple 
knowledge sources for word sense 
disambiguation. Computational Linguistics, 
18(1):1-30. 
347 
Computational Linguistics Volume 27, Number 3 
Melamed, Daniel and Philip Resnik. 2000. 
Evaluation of sense disambiguation given 
hierarchical tag sets. Computers and the 
Humanities, 34:1-2. 
Metropolis, Norbert, Anne Rosenbluth, 
Maya Rosenbluth, Andrew Teller, and 
Edward Teller. 1953. Equation state 
calculations by fast computing machines. 
Journal of Chemical Physics, 21:1087-1092. 
Nadas, Andrew, David Nahamoo, Michael 
Picheny, and Jonathan Powell. 1991. An 
iterative "flip-flop" approximation of the 
most informative split in the construction 
of decision trees. In Proceedings ofthe IEEE 
International Conference on Acoustics, Speech 
and Signal Processing, pages 565-568, 
Toronto. 
Ng, Hwee and Hian Lee. 1996. Integrating 
multiple knowledge sources to 
disambiguate word sense: An 
exemplar-based approach. In Proceedings 
of the 34th Meeting of the Association for 
Computational Linguistics (ACL-96), 
pages 40-47, Santa Cruz, CA. 
Press, William, Saul Teukolsky, William 
Vetterling, and Brian Flannery. 1988. 
Numerical Recipes in C: The Art of Scientific 
Computing. Cambridge University Press, 
Cambridge. 
Procter, Paul, editor. 1978. Longman 
Dictionary of Contemporary English. 
Longman Group, Essex, UK. 
Procter, Paul, editor. 1995. Cambridge 
International Dictionary of English. 
Cambridge University Press, Cambridge. 
Quinlan, J. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, 
CA. 
Resnik, Philip. 1997. Selectional preferences 
and word sense disambiguation. I  
Proceedings ofthe SIGLEX Workshop 
"Tagging Text with Lexical Semantics: What, 
why and how?", pages 52-57, Washington, 
D.C. 
Resnik, Philip and David Yarowsky. 1997. A 
perspective on word sense 
disambiguation techniques and their 
evaluation. In Proceedings ofthe SIGLEX 
Workshop "Tagging Text with Lexical 
Semantics: What, why and how?", 
pages 79-86, Washington, D.C. 
Rigau, German, Jordi Atserias, and Eneko 
Agirre. 1997. Combining unsupervised 
lexical knowledge methods for word 
sense disambiguation. I  35th Meeting of 
the Association for Computational Linguistics 
and the Eighth Meeting of the European 
Chapter of the Association for Computational 
Linguistics (ACL/EACL-97), pages 48-55, 
Madrid, Spain. 
Sch~tze, Hinrich. 1992. Dimensions of 
meaning. In Proceedings ofSupercomputing 
'92, pages 787-796, Minneapolis, MN. 
Stevenson, Mark. 1998. Extracting syntactic 
relations using heuristics. In Proceedings of
the European Summer School on Logic, 
Language and Information '98 Student 
Workshop, ages 248-256, Saarbri~cken, 
Germany. 
Stevenson, Mark and Yorick Wilks. 1999. 
Combining weak knowledge sources for 
sense disambiguation. I  Proceedings ofthe 
Sixteenth International Joint Conference on 
Artificial Intelligence (IJCAI-99), 
pages 884-889, Stockholm, Sweden. 
van Rijsbergen, Keith. 1979. Information 
Retrieval. Butterworths, London. 
V~ronis, Jean and Nancy Ide. 1990. Word 
sense disambiguation with very large 
neural networks extracted from machine 
readable dictionaries. In Proceedings ofthe 
13th International Conference on 
Computational Linguistics (COLING-90), 
pages 389-394, Helsinki. 
Waltz, David and Jordan Pollack. 1985. 
Massively parallel parsing: A strongly 
interactive model of natural anguage 
interpretation. Cognitive Science, 9:51-74. 
Wilks, Yorick. 1975. A preferential 
pattern-seeking semantics for natural 
language inference. Artificial Intelligence, 
6:53-74. 
Wilks, Yorick. 1997. Senses and Texts. 
Computers and the Humanities, 31:77-90. 
Wilks, Yorick, Dan Fass, Cheng-Ming Guo, 
James McDonald, Tony Plate, and Brian 
Slator. 1990. Providing machine tractable 
dictionary tools. Machine Translation, 
5:99-154. 
Wilks, Yorick, Brian Slator, and Louise 
Guthrie. 1996. Electric Words: Dictionaries, 
Computers and Meanings. MIT Press, 
Cambridge, MA. 
Wilks, Yorick and Mark Stevenson. 1998a. 
The grammar of sense: Using 
part-of-speech tags as a first step in 
semantic disambiguation. Journal of 
Natural Language Engineering, 4(2):135-144. 
Wilks, Yorick and Mark Stevenson. 1998b. 
Optimizing combinations of knowledge 
sources for word sense disambiguation. 
In Proceedings ofthe 36th Meeting of the 
Association for Computational Linguistics 
(COLING-ACL-98), pages 1398-1402, 
Montreal. 
Yarowsky, David. 1992. Word-sense 
disambiguation using statistical models of 
Roget's categories trained on large 
corpora. In Proceedings ofthe 14th 
International Conference on Computational 
Linguistics (COLING-92), pages 454-460, 
Nantes, France. 
348 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Yarowsky, David. 1993. One sense per 
collocation. In Proceedings ofthe ARPA 
Human Language Technology Workshop, 
pages 266-271, Princeton, NJ. 
Yarowsky, David. 1995. Unsupervised word 
sense disambiguation rivaling supervised 
methods. In Proceedings ofthe 33rd Annual 
Meeting of the Association for Computational 
Linguistics (ACL-95), pages 189-196, 
Cambridge, MA. 
Yarowsky, David. 1996. Homograph 
disambiguation i  text-to-speech 
synthesis. In J. Hirschberg, R. Sproat, and 
J. van Santen, editors, Progress in Speech 
Synthesis. Springer Verlag, New York, NY, 
pages 159-175. 
Yngve, Victor. 1995. Syntax and the problem 
of multiple meaning. In W. Locke and D. 
Booth, editors, Machine Translation of 
Languages. Wiley, New York. 
Zavrel, Jakub, Walter Daelemans, and Jorn 
Veenstra. 1997. Resolving PP-attachment 
with memory-based learning. In 
Proceedings ofthe Workshop on 
Computational Natural Language Learning 
(CoNLL '97), pages 136-144, Madrid. 
Zipf, Georg. 1935. The Psycho-Biology of 
Language. Houghton Mifflin, Boston, MA. 
349 

ACL Lifetime Achievement Award
On Whose Shoulders?
Yorick Wilks?
University of Sheffield
Introduction
The title of this piece refers to Newton?s only known modest remark: ?If I have seen
farther than other men, it was because I was standing on the shoulders of giants.? Since
he himself was so much greater than his predecessors, he was in fact standing on the
shoulders of dwarfs, a much less attractive metaphor. I intend no comparisons with
Newton inwhat follows: NLP/CL has noNewtons and noNobel Prizes so far, and quite
rightly. I intend only to draw attention to a tendency in our field to ignore its intellectual
inheritance and debt; I intend to discharge a little of this debt in this article, partly as
an encouragement to others to improve our lack of scholarship and knowledge of our
own roots, often driven by the desire for novelty and to name our own systems. Roger
Schank used to argue that it was crucial to name your own NLP system and then have
lots of students to colonize all major CS departments, although time has not been kind to
his many achievements and originalities, even though he did build just such an Empire.
But to me one of the most striking losses from our corporate memory is the man who
is to me the greatest of the first generation and still with us: Vic Yngve. This is the man
who gave us COMIT, the first NLP programming language; the first random generation
of sentences; and the first direct link from syntactic structure to parsing processes and
storage (the depth hypothesis). I find students now rarely recognize his name, and find
that incredible.
This phenomenon is more than corporate bad memory, or being too busy with en-
gineering to do the scholarship. It is something endemic in the wider field of Computer
Science and Artificial Intelligence, although bottom-up wiki techniques are now filling
many historical gaps for those who know where to look, as the generation of pioneers
has time to reminisce in retirement.1 There are costs to us from this general lack of
awareness, though: a difficulty of ?standing on the shoulders? of others and acknowl-
edging debts, let alne passing on software packages. Alan Bundy used to highlight this
in theAISB Quarterlywith a regular columnwhere he located and pilloried reinventions
in the field of AI; he also recommended giving obituaries for one?s own work, and this
paper could be seen in that way, too.
? Department of Computer Science, The University of Sheffield, Regent Court, 211 Portobello Street,
Sheffield, S1 4DP, UK. E-mail: Y.Wilks@dcs.shef.ac.uk. This article is the text of the talk given on receipt of
the ACL?s Lifetime Achievement Award in 2008.
1 See the video interview with Victor Yngve on my Web site at
http://www.dcs.shef.ac.uk/?yorick/YngveInterview.html.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
Early Academic Life
My overwhelming emotion on getting this honor was, after surprise, a feeling of in-
adequacy in measuring up to previous honorees, but nonetheless, I want to grasp at
this moment of autobiography, or at what in his own acceptance paper Martin Kay
called: ?but one chance for such gross indulgence.? I was born in 1939 in London at
just about the moment the Second World War started in Europe; this was, briefly, a
severe career slowdown. However, the British Government had a policy of exporting
most children out of the range of bombs and I was sent to Torquay, a seaside town in
southwest England that happened to have palm trees on all the main streets, a fact it
is often difficult to convince outsiders of. The town had, and has, a Grammar School
for Boys, which had a very good Cambridge-trained mathematician as its headmaster,
and eventually I made my way back across England to Pembroke College, Cambridge,
to study mathematics, a college now for ever associated with my comedian contem-
poraries: Peter Cook, Clive James, Eric Idle, Tim Brooke-Taylor, and similar wastrels. I
began a series of changes of subject of study, downhill towards easier and easier ones:
frommathematics to philosophy to (what in the end after graduation became) NLP/AI.
It was not that I could not do the mathematics, but rather that I experienced the shock
that many do of finding how wide the range of talent in mathematics is, and that being
very good in a provincial grammar school does not make one very good at Cambridge.
This is a feeling peculiar to mathematics, I think, because the talent range is so much
wider than in most subjects, even at the top level.
Margaret Masterman, who was to become the main intellectual influence in my life,
was the philosophy tutor for my college, although her main vocation was running the
Institute she had founded, outside the University in a Cambridge suburb: CLRU, the
Cambridge Language Research Unit. It was an eccentric and informal outfit, housed in
what had been a museum of Buddhist art, some of whose sculptures were built into the
walls. MMB (as she was known) ran the CLRU from the mid 1950s to the early 1980s
on a mix of US, UK, and EU grants and did pioneering work in MT, AI, and IR. Of
those honored by the ACL with this award over the last five years, three have been
graduates of that little Buddhist shed, and include Martin Kay and Karen Spa?rck Jones,
a remarkable tribute to MMB. The lives and work of we three have been quite different
but all in different ways stem from MMB?s interests and vision: She had been a pupil
of Wittgenstein and, had she known it, would have approved of Longuet-Higgins?s
remark that ?AI is the pursuit of metaphysics by other means.? She believed that
practical research into the structure of language could give insight into metaphysics,
but was in no way other-worldly: She was the daughter of a Cabinet Minister and knew
what it was to command.
In a final twist, I found after her death in 1986 that she had made me her literary
executor: She had never written a book andwantedme to construct one from her papers
posthumously. It took me twenty years to get the required permissions but the volume
finally appeared in 2005 (Masterman et al 2005).
Thesis Building and CLRU
When I started work at CLRU in 1962 to do a doctorate, it had no computer in the
normal sense, only a Hollerith card sorter of the sort built for the US census half a
century before. Basically, you put a stack of punched cards into one of these things?
which looked like a metal horse on four legs?and the cards fell into (I think) 10 slots
472
Wilks On Whose Shoulders?
depending on how you had plugged in a set of wires at the back to identify destination
slots for sorted cards with hole patterns on the cards. With some effort, these could
be turned into quite interesting Boolean machines; my first task was to take a notion
of Fred Parker-Rhodes that a Hallidayan grammar could be expressed as a lattice of
typed classes, and then program the card sorter so that repeated sorts of punched cards
could be used to parse a sentence. It was triumph of ingenuity over practicality. Later
the CLRU owned an ICL 1202 computer with 1,200 registers on a drum, but it was
a so-called bini-ten machine designed for UK cash transactions when there were still
12 pennies to a shilling, and so the 1202 has print wheel characters for 10, 11, and 12
(as well as 0?9), a fact on which Parker-Rhodes built a whole world of novel print
conventions for his research. This was the period at CLRU when Karen Spa?rck Jones
was completing her highly original thesis (published twenty years later as Jones [1986])
on unsupervised clustering of thesaurus terms?whose goal was to produce primitives
for MT, it is often forgotten?until she had to move her computations to a real computer
at the University Computing Laboratory, where she eventually created a new career in
IR, essentially using the same clump algorithms?created by Parker-Rhodes and her
husband Roger Needham?to do IR.
My own interests shifted to notions in an early Masterman paper titled ?Semantic
message detection using an interlingua? (Masterman 1961), an area in which Martin
Kay had also originally worked on an interlingua for MT. My thesis computation was
done in LISP 1.6 on an IBM360 (under a one-man US Air Force contract, administered
by E. Mark Gold, who later became famous as the founder of learnability theory), at
SDC in Santa Monica, where I was attached loosely in 1966 to the NLP group there
run by Bob Simmons. My thesis was to be entitled ?Argument and proof in Meta-
physics from an empirical point of view? andmy advisor wasMMB?s husband, Richard
Braithwaite, Knightbridge Professor of Moral Philosophy at the University. He was a
philosopher of science and a logician, and was given the chair of moral philosophy?
a subject about which he knew nothing?because it was the only one available at
Cambridge at the time. This produced an extraordinary inaugural lecture in which
he effectively founded a new subject: ?The theory of games as a tool for the moral
philosopher.?
Unfortunately for me he was not interested in my thesis, and took me on only as
a favor to MMB. My interest was the demarcation of metaphysical text: what it was, if
anything, that distinguished it from ordinary language text. Wittgenstein had once said
that words were ?on holiday? in metaphysical text, but also that he wanted to ?bring
words back from their metaphysical to their everyday usage? (Wittgenstein 1973). This
is exactly what I wanted to capture with computation, and the thesis was eventually
submitted to the Cambridge Philosophy faculty in 1967?then called Moral Sciences?
with a large appendix of LISP program code at the back, something they had never
seen before, or since. The thesis was bound in yellow, though the regulations stipulated
black or brown bindings; I must have had some extraordinary idea that someone might
cruise the long corridors of Cambridge theses looking for one that stood out by color?
the arrogance of youth!
The thesis?s starting point was Carnap?s monumental Logische Syntax der Sprache
(1937) and his claim that meaningfulness in text could be determined by ?logical
syntax??rules of formation and transformation (a notion which may well sound famil-
iar; Chomsky was a student of Carnap). My claim was that this was a bad demarcation
and a better criterion of meaningfulness would be to have one interpretation rather than
many, namely, that word-sense discrimination (WSD) was possible for a given text. On
that view, the ?meaningless? text had too many interpretations rather than none (or
473
Computational Linguistics Volume 34, Number 4
one). A word in isolation is thus often meaningless. Preference Semantics was a WSD
program to do just that, and to provide a new sense where WSD failed.
The other starting point of the thesis was a slim paper by Bosanquet on the nature of
metaphysical discourse, entitled ?Some Remarks on Spinoza?s Ethics.? He argued that
Spinoza?s logical arguments are all false, but that what Spinoza was actually doing is
rhetorical, not logical: imposing a new sense on the reader. The system as implemented
was, of course, a toy system, in the sense that all symbolic NLP systems were in that
era. It consisted of an analysis of five metaphysical texts (by Wittgenstein, Spinoza,
Descartes, Kant, and Leibniz) along with five randomly chosen passages from editorials
in the London Times, as some sort of control texts.
The vocabulary was only about 500 words, but this was many years before
Boguraev declared the average size of vocabularies in working NLP systems to be
36 words. The semantic structures derived?via what we would now call chunk
parsing?consisted of tree structures of primitives (from a set of about 80), one tree
for each participating word sense in the text chunk, that fitted into preformed triples
called templates. These templates were subject?predicate?object triples that defined
well-formed sequences of the triples of trees (i.e., the first tree for the sense of the
subject, the second for the action and so on), whose tree-heads had to fit those of the
template?s three primitive items in order. The overall system selected the word senses
that fitted into these structures by means of a notion of ?semantic preference? (see
subsequent discussion), and then declared those to be the appropriate senses for the
words, thus doing a primitive kind of WSD.
There was in the thesis an additional ?sense constructor? mode, called if the WSD
did not work, which tried to identify some sense of a word in the text whose representa-
tion would fit in the overall structure derived, and so could be declared a suitable ?new?
sense for thewordwhich had previously failed to fit in. Unsurprisingly, it identified, say,
a sense of ?God? in the Spinoza text with an existing sense of ?Nature? so that, after
this substitution, the whole thing fitted together and WSD could proceed, and thus the
passage be declared meaningful, given the criterion of having a single, ambiguity-free,
interpretation. This was the toy procedure that allowed me to argue that Spinoza?s real
aim, whether he knew it or not, was to persuade us that the word ?God? could have
the sense of ?Nature? and that this was the real point of his philosophy?exactly in line
with what Bosanquet had predicted.
The philosophy work was never really published, outside an obscure McGill Uni-
versity philosophy journal, although the meaningfulness criterion appeared inMind in
1971 under the title ?Decidability andNatural Language? (Wilks 1971). Since publishing
inMind was, at the time, the ambition of every young philosopher, I was now satisfied
and could move to the simpler world of NLP. The thesis, shorn of the metaphysics,
appeared asmy first book,Grammar, Meaning and theMachine Analysis of Language (Wilks
1972); the title was intended as a variation on the title of some strange German play,
popular at the time, and whose actual name I can no longer remember.
Preference Semantics
I returned from California to CLRU but left again for the Stanford AI Lab in 1969.
I had fantasized at CLRU about all the things one could do with a methodology of
trying to base a fairly complex compositional semantics on a foundation of superficial
pattern matching. This had earlier produced speculations like my 1964 CLRU paper
?Text searching with templates,? procedures that we could not possibly have carried
474
Wilks On Whose Shoulders?
I.1 ((*ANI 1)((SELF IN)(MOVE CAUSE))(*REAL 2))?(1(*JUDG) 2)
Or, in semi-English:
[animate-1 cause-to-move-in-self real-object-2]?[1 *judges 2]
I.2 (1 BE (GOOD KIND))?((*ANI 2) WANT 1)
Or, again:
[1 is good]?[animate-2 wants 1]
Figure 1
Inference rules in Preference Semantics.
out with the machines then available, but which I now choose to see as wanting to
do Information Extraction: though, of course, it was Naomi Sager who did IE first on
medical texts at NYU (see Sager and Grishman 1975).
At Stanford as a post-doc, I was on the same corridor asWinograd, just arrived from
MIT; Schank, then starting to build his Conceptual Dependency empire; and Colby and
his large team building the PARRY dialogue system, which included Larry Tesler, later
the Apple software architect. Schank and I agreed on far more than we disagreed on and
saw that wewould be stronger together than separately, but neither of us wanted to give
up our notation: He realized, rightly, that there was more persuasive power in diagrams
than in talk of processes like ?preference.? It was an extraordinary period, when AI and
NLP were probably closer than ever before or since: Around 1972 Colmerauer passed
through the Stanford AI Lab, describing Prolog for the first time but, as you may or
may not remember, as a tool for machine translation! I spent my time there defining and
expanding the coherence-based semantics underlying my thesis, calling it ?Preference
Semantics? (PS), adding larger scale structures such as inference rules (see Figure 1)
and thesauri, and building it into the core of a small semantics-based English-to-French
machine translation system programmed in LISP. At one point the code of this MT
system ended up in the Boston Computer Museum, but I have no idea where it is now.
The principles behind PS were as follows:
 an emphasis on processes, not diagrams;
 the notion of affinity and repulsion between sense representations
(cf. Waltz and Pollack?s WSD connectionism [1985]);
 seeking the ?best fit? interpretation?the one with most satisfied
preferences (normally of verbs, prepositions, and adjectives);
 yielding the least informative/effort interpretation;
 using no explicit syntax, only segmentation and order of items;
 meaningfulness as being connected to a unique interpretation/sense
choice;
 meaning seen as represented in other words, since no other equivalent for
the notion works (e.g., objects or concepts);
 gists or templates of utterances as core underlying entities; and
 there is no correct interpretation or set of primitive concepts, only the best
available.
475
Computational Linguistics Volume 34, Number 4
One could put some of these, admittedly programmatic and imprecise, points as
follows:
 Semantics is not necessarily deep but also superficial (see more recent
results on the interrelations between WSD, POS, and IE, e.g. Stevenson
and Wilks [2001]).
 Quantitative phenomena are unavoidable in language: John McCarthy
thought they had no place anywhere in AI, except perhaps in low-level
computer vision.
 Reference structures (like lexicons) are only temporary snapshots of a
language in a particular state (of expansion or contraction).
 What is important is to locate the update mechanism of language,
including crucially the creation of new word senses, which is not
Chomsky?s sense of the creativity of language.
Constructible Belief Systems
I returned to Europe in the mid 1970s, first to the ISSCO institute in Lugano, where
Charniak was and Schank had just left, and then to Edinburgh as a visitor before taking
a job at Essex. I began a long period of interest in belief systems, in particular seeking
some representation of the beliefs of others, down to any required degree of nesting?
for example, A?s belief about B?s belief about C?that could be constructed recursively
at need, rather than being set out in advance, as in the pioneering systems emerging
from the Toronto group under Ray Perrault (Allen and Perrault 1980). I began thinking
about this with Janusz Bien of the University of Warsaw, who had also published a
paper arguing that CL/NLP should consider ?least effort? methods: in the sense that
the brain might well, due to evolution, be a lazy processor and seek methods for
understanding that minimized some value that could be identified with processing
effort. I had argued in PS for choosing shortest chains of inferences between templates,
and that the most connected/preferred template structure for a piece of text should be
the one found first. I am not sure we ever proved any of this: It was just speculation,
as was the preference for the most semantically connected representation, and the
representation with the least information. All this is really only elementary information
theory: a random string of words contains the maximum information, but that is not
very helpful. Clearly, the preferred interpretation of ?He was named after his father?
(i.e., named the same rather than later in time) is not the least informative, since the latter
contains no information at all?being necessarily true?so one would have to adapt
any such slogan to: ?prefer the interpretation with the least information, unless it is
zero!?
The belief work, first with Bien, later with Afzal Ballim (Wilks and Ballim 1987)
and John Barnden, has not been a successful paradigm in terms of take-up, in that
it has not got into the general discourse, even in the way that Fauconnier?s ?Mental
Spaces? (Fauconnier 1985) has. That approach uses the same spatial metaphor, but for
strictly linguistic rather than belief and knowledge purposes. But I think the VIEWGEN
belief paradigm, as it became, had virtues, and I want to exploit this opportunity to
remind people of it. It was meant to capture the intuition that if we want, for language
476
Wilks On Whose Shoulders?
understanding purposes, to construct X?s beliefs about Y?s beliefs?what I called the
environment of Y-for-X?then:
1. It must be a construction that can be done in real time to any level of
nesting required, because we cannot imagine it pre-stored for all future
nestings, as Perrault el al. in effect assumed.
2. It must capture the intuition that much of our belief is accepted by default
from others: As VIEWGEN expresses it, I will accept as a belief what you
say, because I have normally no way of checking, or experimenting on, let
alone refuting, the things you tell me, e.g., that you had eggs for breakfast
yesterday. As someone in politics once put it, ?There is no alternative.?
Unless, that is, what you say contradicts something I believe or can easily
prove from what I believe.
3. We must be able to maintain apparently contradictory beliefs, provided
they are held in separate spaces and will never meet as contradictions. I
can thus maintain within my-space-for-you beliefs of yours (according to
me) that I do not in fact hold.
In VIEWGEN, belief construction is done in terms of a ?push down? metaphor: A
permeable ?container? of your beliefs is pushed into a ?container? of my beliefs and
what percolates through the membrane, from me to you, will be believed and ascribed
to you, unless it is explicitly contradicted, namely, by some contrary belief I already
ascribe to you, and which, as it were, keeps mine from percolating through. The idea
is to construct the appropriate ?inner belief space? at the relevant level of nesting, so
that inference can be done, and to derive consequences (within that constrained content
space) that also serve to model, in this case, you the belief holder in terms of goals
and desires, in addition to beliefs. This approach is quite different not only from the
Perrault/Toronto system of belief-relevant plans but also to AI theories that make use of
sets-of-support premises, since this is about belief-inheritance-by-default. It is also quite
distinct from linguistic theories like Wilson and Sperber?s Relevance Theory, which
take no account at all of belief as relative to individuals, but perform all operations
in some space that is the same for everyone, which is an essentially Chomskyan ideal
competence-style notion of belief that is not relative to individuals?which is of course
absurd.
Mark Lee and a number of my students have created implementations of this
approach and linked it to dialogue and other applications, but there has been no major
application showing its essential role in a functioning conversational theory where
complex belief states are created in real time. However, the field is, I believe, now
moving in that direction (e.g., with POMDP theories [Williams and Young 2007]) since
the possibility of populating belief theories with a realistic base from text by means of
Information Extraction or Semantic Web parsing to RDF format is now real (a matter we
shall return to subsequently).
There were, for me at least, two connections between the VIEWGEN belief work
and Preference Semantics, in terms of meaning and its relation to processes. First,
there was the role of choice and alternatives, crucial to PS, in that an assigned mean-
ing interpretation for a text was no more than a choice of the best available among
alternatives, because preference implies choice, in a way that generative linguistics?
though not of course traditions like Halliday?s?always displayed alternatives but
considered choice between them a matter for mere performance. What was dispensable
477
Computational Linguistics Volume 34, Number 4
to generative linguistics was the heart of the matter, I argued, to NLP/CL. Secondly,
VIEWGEN suggested a view of meaning, consistent locally with PS, dependent on
which individuals or classes one chose to see in terms of each other?the key notion
here was seeing one thing as another and its consequences for meaning. So, if one chose
to identify (as being the same person under two names) Joe (and what one believed
about him) with Fred?s father (and what one knew about him), the hypothesis was that
a belief environment should be constructed for Joe-as-Fred?s-father by percolating one
set of beliefs into the other, just as was done by the basic algorithm for creating A?s-
beliefs-about-B?s-beliefs from the component beliefs of A and B. This process created
a hybrid entity, with intensional meaning captured by the set of propositions in that
inner environment of belief space, but which was now neither Joe nor Fred?s father but
rather the system?s point of view of their directional amalgamation: Joe-as-Fred?s-father
(which might contain different propositions from the result of Fred?s-father-as-Joe).
More natural, and fundable, scenarios were constructed for this technique in those
days, such as knowledge representations for Navy ships? captains genuinely uncertain
as to whether ship-in-my-viewfinder-now was or was not to be identified with the
stored representation for enemy-ship-number-X. The important underlying notion was
one going back to Frege, and which first had an outing in Winograd?s thesis (Winograd
1972), where he showed you could have representations for blocks that did not in fact
exist on the Blocks World table. A semantics must be able to represent things without
knowing whether they exist or not; that is a basic requirement.
Later, and working with John Barnden and Afzal Ballim, this same underly-
ing process of conflating two belief objects was extended to the representation of
?metaphorical objects,? which could be described, quite traditionally in the literature,
as A-viewed-as-B (e.g., an atom viewed as a billiard ball). The metaphorical object
atom-as-billiard-ball was again created by the same push-down or fusion of belief sets
as in the basic belief point-of-view procedure. All this may well have been fanciful,
and was never fully exploited in published work with programs, but it did have a
certain intellectual appeal in wanting to treat belief, points of view, metaphor and
identification of intensional individuals?normally quite separate issues in semantics?
as being modellable by the same simple underlying process (see Ballim, Wilks, and
Barnden 1991). One novel element that did emerge from this analysis was that, in
the construction of these complex intensional identifications, such as between ?today?s
Wimbledon winner? and ?the top male tennis seed,? one could choose directions of
?viewing as? with the belief sets that led to objects which were neither the classic de re
nor de dicto outcomes: Those became just two among a range of choices, and the others
of course had no handy Latin names.
Adapting to the ?Empirical Wave? in NLP
For me, as with many others, especially in Europe, the beginning of the empirical wave
in NLP was the work of Leech and his colleagues at Lancaster: CLAWS4 (a name which
hides a UK political joke), their part-of-speech tagger based on large-scale annotation of
corpora. Such tagging is now the standard first stage of almost every NLP process and it
may be hard for some to realize the skepticsm its arrival provoked: ?What could anyone
want that for?? was a common reaction from those still preoccupied by computational
syntax or semantics. That system was sold to IBM, whose speech group, under Jelinek,
Mercer, and Brown, subsequently astonished the CL/NLP world with their statistical
machine translation system CANDIDE. I wrote critical papers about it at the time, not
totally unconnected to the fact that I was funded by DARPA on the PANGLOSS project
478
Wilks On Whose Shoulders?
at NMSU (along with CMU and ISI/USC) to do MT by competing, but non-statistical,
methods.
In one paper, I used the metaphor of ?stone soup? (Wilks 1996): A reference to the
old peasant folk-tale of the traveler who arrives at a house seeking food and claiming
to have a stone that makes soup from water. He begs a ham bone to stir the water
and stone and eventually cons out of his hosts all the ingredients for real soup. The
aspect of the story I was focusing on was that, in the CANDIDE system, I was not sure
that the ?stone,? namely IBM?s ?fundamental equation of MT,? was in fact producing
the results, and suggested that something else they were doing was giving them their
remarkable success rate of about 50% of sentences correctly translated. As their general
methodology has penetrated the whole of NLP/CL, I no longer stand by my early
criticisms; IBM was of course right, and had everything to teach the rest of us.
Early critics of data-driven, alias empirical, CL found it hard to accept, whatever
its successes in, say, POS tagging, that its methods could extend to the heartland of
semantics and pragmatics. Like others, I came to see this assumption was quite untrue,
and myself moved towards Machine Learning (ML) approaches to word-sense disam-
biguation (e.g., Stevenson and Wilks 2001) and I now work in ML methods applied to
dialogue corpora (as I shall mention subsequently). But the overall shift in approaches
to semantics since 1990 has not only been in the introduction of statistical methods, and
ML in particular, but also in the unexpected advantages that have been gained from
what one might call non-statistical empirical linguistics, and in particular Information
Extraction (IE; see Wilks 1997).
I referred earlier to the fact that my early work could be called, in a general sense,
semantic parsing, and that it was in fact some form of superficial pattern matching
onto language chunks that was then transformed to different layers of compositional
semantic representation. There were obvious relations between that general approach
and what emerged from the DARPA competitions in the early 1990s as IE, a technology
that, when honed by many teams, and especially when ML techniques were added to
it later, had remarkable success and a range of applications; it also expanded out into
other, traditionally separate, NLP areas such as question answering and summarization.
This approach is not in essence statistical at all, however, although it is in a clear
sense ?superficial,? with the assumption that semantics is not necessarily a ?deep?
phenomenon but present on the language surface. I believe the IE movement is also
one of the drivers behind the Semantic Web movement, to which I now turn, and which
I think has brought NLP back to a position nearer the core of AI, from which it drifted
away in the 1980s.
Meaning and the Semantic Web
The Semantic Web (SW; Berners-Lee, Hendler, and Lassila 2001) is what one could call
Berners-Lee?s second big idea, after the World Wide Web; it can be described briefly as
turning the Web into something that can also be understood by computers in the way
that it is understood by people now, as a web of texts and pictures. Depending on one?s
attitude to this enterprise, already well-funded by the European Commission at least, it
can be described as any of the following:
1. As a revival of the traditional AI goal (at least since McCarthy and Hayes
[1969]) of replacing language, with all its vagueness, by some form of
logical representation upon which inference can be done.
479
Computational Linguistics Volume 34, Number 4
2. As a hierarchy of forms of annotation?or what I shall call augmentation
of content?reaching up from simple POS tagging to semantic class
annotation (e.g. CITY, PERSON-NAME) to ontology membership and
logical forms. DARPA/MUC/NIST competitions have worked their way
up precisely this hierarchy over the years and many now consider that
content can be ?annotated onto language? reliably up to any required
level. This can be thought of as extending IE techniques to any linguistic
level by varieties of ML and annotation.
3. As a system of access to trusted databases that ground the meanings of
terms in language; your telephone or social security number might ground
you uniquely (in what is called a URI), or better still?and this is now the
standard view?a unique identifying object number for you over and
above phones and social systems. This is very much Tim Berners-Lee?s
own view of the SW.
There is also a fourth view, much harder to express, that says roughly that, if we keep
our heads, the SW can come into being with any system of coding that will tolerate the
expansion of scale of the system, in the way that, miraculously, the hardware under-
pinnings of the World Wide Web have tolerated its extraordinary expansion without
major breakdown. This is an engineering view that believes there are no fundamental
problems about the meanings and reference of SW terms in, for example, the ontologies
within the SW, and everything will be all right if we just hold tight.
This view may turn out to be true but it is impossible to discuss it. Similarly, view
(3) has no special privilege because it is the WorldWideWeb founder?s own view: Marx
was notoriously not a very consistent Marxist, and one can find multiple examples
of this phenomenon. View (3) is highly interesting and close to philosophical views
of meaning expressed over many years by Putnam, which can be summarized as the
idea that scientists (and Berners-Lee was by origin a database expert and physicist) are
?guardians of meaning? in some sense because they know what terms really mean, in
a way that ordinary speakers do not. Putnam?s standard example is that of metals like
molybdenum and aluminum, which look alike and, to the man in the street, have the
same conceptual, intensional meaning, namely light, white, shiny metal. But only the
scientist (says Putnam) knows the real meanings of those words because he knows
the atomic weights of the two metals and methods for distinguishing them.
No one who takes Wittgenstein?and his view that we, the users of the language,
are in charge of what termsmean, and not any expert?at all seriously can even consider
such a view. On the view we are attributing to Wittgenstein, the terms are synonymous
in a public language, just as water and heavy water are, and any evidence to the contrary
is a private matter for science, not for meaning.
View (1) of the Semantic Web is a well-supported one, particularly by recycled AI
researchers: They have, of course, changed tack considerably and produced formalisms
for the SW, some of which are far closer to the surface of language than logic (what
is known as RDF triples), as well as inference mechanisms like DAML-OIL that gain
advantages over traditional AI methods on the large and practical scale the SW is
intended to work over. On the other hand there are those in AI who say they have
ignored much of the last 40 years of AI research that would have helped them. This
dispute has a conventional flavor and it must be admitted that, in more than 40 years,
AI itself did not come up with such formalisms that stood any chance at all of working
on a large scale on unstructured material (i.e., text).
480
Wilks On Whose Shoulders?
This leaves us with View (2), which is my own: namely, that we should see the SW
partially in NLP terms, however much Berners-Lee rejects such a view and says NLP
is irrelevant to the SW. The whole trend of SW research, in Europe at least, has been
to build up to higher and higher levels of semantic annotation?a technology that has
grown directly out of IE?s success in NLP?as a way of adding content to surface text.
It seems to me obvious that any new SW will evolve from the existing WWW of text
by some such method, and that method is basically a form of large-scale NLP, which
now takes the form of transducers from text to RDF (such as the recently advertised
Reuters API). The idea that the SW can start from scratch in some other place, ignoring
the existing World Wide Web, seems to me unthinkable; successful natural evolution
always adapts the function of what is available and almost never starts again afresh.
I have set out my views on this recently in more detail (Wilks 2008), but it is
important to see that the SW movement?at least as I interpret it herein, and that does
seem pretty close to the way research in it is currently being funded, under calls and
titles like ?semantic content??is one that links to the themes already developed in this
paper in several ways, and which correspond closely to issues in my own early work,
but which have not gone away:
1. The SW takes semantic annotation of content as being a method?whether
done by humans or after machine learning?of recoding content with
special terms, terms close to what have traditionally been called semantic
primitives. It is exactly this that was denied by the early forms of, say,
statistical MT, where there was nothing available to the mechanism except
the words themselves. This is also quite explicit in traditional IR, where,
for example, Karen Spa?rck Jones consistently argued against any form of
content recoding, including the SW. As she put it: ?One of these [simple,
revolutionary IR] ideas is taking words as they stand? (Spa?rck Jones 2003).
2. The SW accords a key role to ontologies as knowledge structures: partially
hierarchical structures containing key terms?primitives again under
another guise?whose meanings must be made clear, particularly at the
more abstract levels. The old AI tradition in logic-based knowledge
structuring?descending from McCarthy and Hayes (1969)?was simply
to declare what these primitive predicates meant. The problem was that
predicates, normally English words written in capital letters (as all
linguistic primitives in the end seem to be), became affected by their
inferential roles over time and the process of coding itself. This became
very clear in the long-term Cyc project (Lenat 1995) where the key
predicates changed their meanings over 30 years of coding, but there was
no way of describing that fact within the system, so as to guarantee
consistency. In Nirenburg and Wilks (2000), Nirenburg and I debate this
issue in depth, and I defend the position that one cannot simply maintain
the meanings of such terms by fiat and independent of their usage?they
look like words and they function like words because, in the end, they are
words. The SW offers a way out of this classic AI dilemma by building up
the hierarchy of annotations with empirical processes like ontology
induction from corpora (e.g., ABRAXAS; see Iria et al 2006); in this way
the meanings of higher level terms are connected back directly to text
usage. Braithwaite, my thesis advisor, described in his classic ?Scientific
explanation? (Braithwaite 1953) a process in the philosophy of science he
481
Computational Linguistics Volume 34, Number 4
called ?semantic ascent? by which the abstract high-level terms in a
scientific theory, seen as a logical hierarchy of deductive processes?terms
such as ?neutron,? possibly corresponding to unobservables?acquired
meaning by an ascent of semantic interpretation up the theory hierarchy
from meanings grounded in experimental terms at the bottom. It is some
such grounding process I envisage the SW as providing for the meanings
of primitive ontological terms in a knowledge structure.
3. The RDF forms, based on triples of surface items, as a knowledge
base?usually with subject?action?object as basic form?can provide a less
formal but more tractable base for knowledge than traditional First Order
Predicate Logic (FOPL). They have a clear relationship back to the crude
templates of my early work and the later templates of IE. I claim no
precedence here, but only note the return of a functioning but plausible
notion of ?superficial semantics.? It seems to me not untrue historically to
claim that RDF, the representational base of the SW, is a return of the level
of representation that Schank (under the name Conceptual Dependency, in
Schank [1975]) and I (under the name Preference Semantics) developed in
the late 1960s and early 1970s (Wilks 1975). I remember that at the Stanford
AI Lab at that time, John McCarthy, a strong advocate of FOPL as the right
level of representation of language content, would comment that
formalisms like these two might have a role as a halfway house on a route
from language to a full logic representation. On one view of the SW that
intermediate stage may prove to be the right stage, because full AI
representations have never been able to deliver in terms of scale and
tractability. Time will tell, and fairly soon.
The most important interest of the SW, from the point of view of this paper, is that
it provides at last a real possibility of a large-scale test of semantic and knowledge
coding: One thing the empirical movement has taught us is the vital importance of scale
and the need to move away from toy systems and illustrative examples. I mentioned
earlier the freely available Reuters API for RDF translation which Slashdot advertised
under the title ?Is the Semantic Web a Reality at Last?? This is exactly the kind of move
to the large scale that we can hope will settle definitively some of these ancient issues
about meaning and knowledge.
A Late Interest in Dialogue: The Companions Project
My only early exposure to dialogue systems was Colby?s PARRY: As I noted earlier, his
team was on the same corridor as me at Stanford AI Lab in the early 1970s. I was a
great admirer of the PARRY system: It seemed to me then, and still does, probably the
most robust dialogue system ever written. It was available over the early ARPANET
and tried out by thousands, usually at night: It was written in LISP and never broke
down; making allowances for the fact it was supposed to be paranoid, it was plausible
and sometimes almost intelligent. In any case it was infinitely more interesting than
ELIZA, and it is one of the great ironies of our subject that ELIZA is so much better
known. PARRY remembered what you had said, had elementary emotion parameters
and, above all, had something to say, which chatbots never do. John McCarthy, who
ran the AI Lab, would never admit that PARRY was AI, even though he tolerated it
under his roof, as it were, for many years; he would say ?It doesn?t even know who
482
Wilks On Whose Shoulders?
the President is,? as if most of the world?s population did! PARRY was in fact a semi-
refutation of the claim that you need knowledge to understand and converse, because
it plainly knew nothing; what it had was primitive ?intentionality,? in the sense that it
had things ?it wanted to say.?
My own introduction to practical work on dialogue was when I was contacted in
the late 1990s by David Levy, who had written 40 books on chess and ran a company
that made chess machines. He already had a footnote in AI as the man who had bet
McCarthy, Michie, and other AI leaders that a chess machine would not beat himwithin
ten years, and he won the bet more than once. In the 1990s he conceived a desire to win
the Loebner Prize2 for the best dialogue program of the year, and came to us at Sheffield
to fund a team to win it for him, which we did in 1997. I designed the system and drew
upon my memories of PARRY, along with obvious advances in the role of knowledge
bases and inference, and the importance of corpora and machine learning. For example,
we took the whole set of winning Loebner dialogues off the Web so as to learn the kinds
of things that the journalist-testers actually said to the trial systems to see if they were
really humans or machines.
Our system, called CONVERSE (see Levy et al 1997), claimed to be Catherine, a
34-year old female British journalist living in New York, and it owed something to
PARRY, certainly in Catherine?s desire to tell people things. It was driven by frames
corresponding to each of about 80 topics that such a person might want to discuss;
death, God, clothes, make-up, sex, abortion, and so on. It was far too top-down and
unwilling to shift from topic to topic but it could seem quite smart on a good day, and
probably won because we had built in news from the night before the competition of
a meeting Bill Clinton had had that day at the White House with Ellen de Generes, a
lesbian actress. This gave a certain immediacy to the responses intended to sway the
judges, as in ?Did you see that meeting Ellen had with Clinton last night??
This was all great fun and gave me an interest in modeling dialogue that has
persisted for a decade and is now exercised through COMPANIONS (Wilks 2004), a
large EU 15-site four-year project that I run. COMPANIONS aims to change the way we
think about the relationships of people to computers and the Internet by developing a
virtual conversational ?Companion.? This will be an agent or ?presence? that stays with
the user for long periods of time, developing a relationship and ?knowing? its owner?s
preferences and wishes. It will communicate with the user primarily by using and un-
derstanding speech, but also using other technologies such as touch screens and sensors.
Another general motivation for the project is the belief that the current Internet
cannot serve all social groups well, and it is one of our objectives to empower citizens
(including the non-technical, the disabled, and the elderly) with a new kind of interface
based on language technologies. The vision of the Senior Companion?currently our
main prototype?is that of an artificial agent that communicates with its user on a
long-term basis, adapting to their voice, needs, and interests: A companion that would
entertain, inform, and react to emergencies. It aims to provide access to information
and services as well as company for the elderly by chatting, remembering past con-
versations, and organizing (and making sense of) the owner?s photographic and image
memories. This Companion would assume a user with a low level of technical knowl-
edge, and who might have lost the ability to read or produce documents themselves
unaided, but who might need help dealing with letters, messages, bills, and getting in-
formation from the Internet. During its conversations with its user or owner, the system
2 See http://www.loebner.net/Prizef/loebner-prize.html.
483
Computational Linguistics Volume 34, Number 4
builds up a knowledge inventory of family relations, family events in photos, places
visited, and so on. This knowledge base is currently stored in RDF, the Semantic Web
format, which has two advantages: first, a very simple inference scheme with which
to drive further conversational inferences, and second, the possibility, not yet fulfilled,
of accessing arbitrary amounts of world information from Wikipedia, already available
in RDF, which could not possibly have been pre-coded in the dialogue manager, nor
elicited in a conversation of reasonable length. So, if the user says a photo was taken in
Paris, the Companion should be able to ask a question about Paris without needing that
knowledge pre-coded, but only using rapidly accessedWikipedia RDFs about Paris. An
ultimate aim of this aspect of the Senior Companion is the provision of a life narrative,
an assisted autobiography for everyone, one that could be given to relatives later if the
owner chose to leave it to them. There is a lot of technical stuff in the Senior Companion:
script-like structures?called DAFs or Dialogue Action Forms?designed to capture the
course of dialogues on specific topics or individuals or images, and these DAFs we are
trying to learn from tiled corpora. The DAFs are pushed and popped on a single stack,
and that simple virtual machine is the Dialogue Manager, where DAFs being pushed,
popped, or reentered at a lower stack point are intended to capture the exits from, and
returns to, abandoned topics and the movement of conversational initiative between
the system and the user. We are halfway through the project and currently have two
prototype Companions: The other, based not at Sheffield but at Tampere, is a Health
and Fitness Companion (HFC).3 It is more task-oriented than the Senior Companion
and aims to advise on exercise and diet. The HFC is on a mobile phone architecture as
well as a PC, andwemay seek to combine the two prototypes later. The central notion of
a Companion is that of the same ?personality,? with its memory and voice being present
no matter what the platform. It is not a robot, and could be embodied later in something
like a chatty furry handbag, being held on a sofa and perhaps reminding you about the
previous episodes of your favorite TV program.
Finale
This article has had something of the form of a life story, and everyone wants to believe
their life is some kind of narrative rather than a random chase from funding agency to
funding agency, with occasional pauses to carry out a successful proposal. But let us
return to Newton for a moment in closing; for us in CL he is the great counter-example,
of why we do not do science or engineering in that classic solitary manner:
. . .where the statue stood
Of Newton, with his prism and silent face,
The marble index of a mind for ever
Voyaging through strange seas of Thought, alone.
?William Wordsworth (1770?1850)
The Prelude, book iii, line 61
The emphasis there for me is on alone, which is pretty much unthinkable in our research
world of teams and research groups. Our form of research is essentially corporate and
cooperative; we may not be sure whose shoulders we are standing on, but we know
whose hands we are holding. I have worked in such a way since my thirties and, at
3 An early demo of a Companion can be seen on YouTube at
http://www.youtube.com/watch?v=SqIP6sTt1Dw.
484
Wilks On Whose Shoulders?
Sheffield, my work would not have been possible without a wide range of colleagues
and former students in the NLP group there over many years and including Louise
Guthrie, Rob Gaizauskas, Hamish Cunningham, Fabio Ciravegna, Mark Stevenson,
Mark Hepple, Kalina Bontcheva, Roberta Catizone, Nick Webb, and many others. In
recent years, what one could call ?DARPA culture??of competitions and cooperation
subtlymixed?aswell as the great repositories of software and data like LDC and ELRA,
have gone a long way to mitigate the personal and group isolation in the field.
But we do have to face the fact that, in many ways, we do not do classic science:
We have no Newtons and will never have any. That is not to deny that we need real
ideas and innovations, and now may be a time for fresh ones. We have stood on the
shoulders of Fred Jelinek, Ken Church, and others for nearly two decades now, and the
strain is beginning to tell as papers still strive to gain that extra 1% in their scores on
some small task. We know that some change is in the air and I have tried to hint in
this article as to some of the places where that might be, even if that will mean a partial
return to older, unfashionable ideas; for there is nothing new under the sun. But locating
them and exploiting themwill not be in my hands but in yours, readers of Computational
Linguistics!
Acknowledgments
First of course to all those who have worked
with me over many years and to whom I
owe so much, particularly in connection
with this award. Then to my current sponsor:
This work was funded by the Companions
project (www.companions-project.org)
sponsored by the European Commission as
part of the Information Society Technologies
(IST) programme under EC grant number
IST-FP6-034434.
References
Allen, James F. and C. Raymond Perrault.
1980. Analyzing intention in utterances.
Artificial Intelligence, 15:143?178.
Ballim, Afzal, Yorick Wilks, and John A.
Barnden. 1991. Belief ascription, metaphor,
and intensional identification. Cognitive
Science, 15(1):133?171.
Berners-Lee, T., J. Hendler, and O. Lassila.
2001, September. The semantic web.
Scientific American, 28?37.
Braithwaite, Richard Bevan. 1953.
Scientific Explanation. A Study of the
Function of Theory, Probability and Law in
Science. Cambridge University Press,
Cambridge, UK.
Carnap, Rudolf. 1937. The Logical Syntax of
Language. Kegan Paul, London.
Fauconnier, Gilles. 1985.Mental Spaces.
Cambridge University Press,
Cambridge, UK.
Iria, Jose?, Christopher Brewster, Fabio
Ciravegna, and Yorick Wilks. 2006. An
incremental tri-partite approach to
ontology learning. In Proceedings of the
Language Resources and Evaluation
Conference (LREC-06), 22?28 May.
Lenat, Douglas B. 1995. CYC: A large-scale
investment in knowledge infrastructure.
Communications of the ACM, 38(11):33?38.
Levy, D., R. Catizone, B. Battacharia,
A. Krotov, and Y. Wilks. 1997. Converse:
A conversational companion. In
Proceedings of the First International
Workshop of Human-Computer
Conversation. Bellagio, Italy.
Masterman, Margaret. 1961. Semantic
message detection for machine
translation, using an interlingua. In
Proceedings of the First International
Conference on Machine Translation of
Languages and Applied Language Analysis,
pages 438?475. HMSO, Teddington,
Middlesex, UK.
Masterman, Margaret. 2005. In Yorick Wilks,
editor, Language, Cohesion and Form (Studies
in Natural Language Processing). Cambridge
University Press, New York.
McCarthy, J. and P. J. Hayes. 1969. Some
philosophical problems from the
standpoint of artificial intelligence. In
B. Meltzer and D. Michie, editors,Machine
Intelligence, volume 4. Edinburgh
University Press, Edinburgh,
pages 463?502.
Nirenburg, Sergei and Yorick Wilks. 2000.
Machine translation. Advances in
Computers, 52:160?189.
Sager, Naomi and Ralph Grishman.
1975. The restriction language for
computer grammars of natural language.
Communications of the ACM,
18(7):390?400.
485
Computational Linguistics Volume 34, Number 4
Schank, Roger C. 1975. Conceptual Information
Processing. Elsevier Science Inc., New York.
Spa?rck Jones, Karen. 1986. Synonymy and
semantic classification. Edinburgh
University Press, Edinburgh, Scotland.
Spa?rck Jones, Karen. 2003. Document
retrieval: Shallow data, deep theories;
historical reflections, potential directions.
In Advances in Information Retrieval, Lecture
Notes in Computer Science, 1?11. Springer,
Berlin/Heidelberg.
Stevenson, Mark and Yorick Wilks. 2001. The
interaction of knowledge sources in word
sense disambiguation. Computational
Linguistics, 27(3):321?349.
Waltz, David L. and Jordan B. Pollack.
1985. Massively parallel parsing: A
strongly interactive model of natural
language interpretation. Cognitive Science,
9(1):51?74.
Wilks, Y. 1975. Preference semantics.
In E. L. Keenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, pages 329?348.
Wilks, Yorick. 1971. Decidability and natural
language.Mind, 80:497?520.
Wilks, Yorick. 1972. Grammar, Meaning and
Machine Analysis of Language. Routledge
and Kegan Paul, London.
Wilks, Yorick. 1996. Statistical versus
knowledge-based machine translation.
IEEE Expert: Intelligent Systems and Their
Applications, 11(2):12?18.
Wilks, Yorick. 1997. Information extraction as
a core language technology. In International
Summer School on Information Extraction: A
Multidisciplinary Approach to an Emerging
Information Technology, volume 1299 of
Lecture Notes In Computer Science,
pages 1?9, Springer, Berlin.
Wilks, Yorick. 2004. Artificial companions. In
Machine Learning for Multimodal Interaction:
First International Workshop, pages 36?45.
Wilks, Yorick. 2008. The semantic web:
Apotheosis of annotation, but what are its
semantics? IEEE Intelligent Systems,
23(3):41?49.
Wilks, Yorick andAfzal Ballim. 1987.Multiple
agents and the heuristic ascription of
belief. In Proceedings of the International
Joint Conference Artificial Intelligence
(IJCAI-87), pages 118?124, Milan, Italy.
Williams, Jason D. and Steve Young. 2007.
Partially observable Markov decision
processes for spoken dialog systems.
Computer Speech and Language,
21(2):393?422.
Winograd, Terry. 1972. Understanding Natural
Language. Academic Press, Orlando, FL.
Wittgenstein, Ludwig. 1973. Philosophical
Investigations. Blackwell Publishers,
Oxford, UK.
486
Proceedings of NAACL HLT 2007, Companion Volume, pages 69?72,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Clustered Sub-matrix Singular Value Decomposition
Fang Huang
School of Computing
Robert Gordon University
Aberdeen, AB25 1HG, UK
f.huang@rgu.ac.uk
Yorick Wilks
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, UK
y.wilks@dcs.shef.ac.uk
Abstract
This paper presents an alternative algo-
rithm based on the singular value decom-
position (SVD) that creates vector rep-
resentation for linguistic units with re-
duced dimensionality. The work was mo-
tivated by an application aimed to repre-
sent text segments for further processing
in a multi-document summarization sys-
tem. The algorithm tries to compensate
for SVD?s bias towards dominant-topic
documents. Our experiments on measur-
ing document similarities have shown that
the algorithm achieves higher average pre-
cision with lower number of dimensions
than the baseline algorithms - the SVD
and the vector space model.
1 Introduction
We present, in this paper, an alternative algorithm
called Clustered Sub-matrix Singular Value Decom-
position(CSSVD) algorithm, which applied cluster-
ing techniques before basis vector calculation in
SVD (Golub and Loan, 1996). The work was
motivated by an application aimed to provide vec-
tor representation for terms and text segments in a
document collection. These vector representations
were then used for further preprocessing in a multi-
document summarization system.
The SVD is an orthogonal decomposition tech-
nique closely related to eigenvector decomposition
and factor analysis. It is commonly used in infor-
mation retrieval as well as language analysis appli-
cations. In SVD, a real m-by-n matrix A is decom-
posed into three matrices, A = U ?V T . ? is an
m-by-n matrix such that the singular value ?i=
?
ii is
the square root of the ith largest eigenvalue of AAT ,
and ?ij = 0 for i 6= j. Columns of orthogonal ma-
trices U and V define the orthonormal eigenvectors
associated with eigenvalues of AAT and ATA, re-
spectively. Zeroing out all but the k, k < rank(A),
largest singular values yields Ak =
?k
i=1 ?iuivTi ,
which is the closest rank-k matrix to A. Let A be a
term-document matrix. Applications such as latent
semantic indexing (Deerwester et al, 1990) apply
the rank-k approximation Ak to the original matrix
A, which corresponds to projecting A onto the k-
dimension subspace spanned by u1, u2, ..., uk. Be-
cause k ? m, in this k-dimension space, minor
terms are ignored, so that terms are not indepen-
dent as they are in the traditional vector space model.
This allows semantically related documents to be re-
lated to each other even though they may not share
terms.
However, SVD tends to wipe out outlier
(minority-class) documents as well as minor terms
(Ando, 2000). Consequently, topics underlying out-
lier documents tend to be lost. In applications such
as multi-document summarization, a set of related
documents are used as the information source. Typ-
ically, the documents describe one broad topic from
several different view points or sub-topics. It is im-
portant for each of the sub-topics underlying the
document collection to be represented well.
Based on the above consideration, we propose the
CSSVD algorithm with the intention of compensat-
69
ing for SVD?s tendency to wipe out minor topics.
The basic idea is to group the documents into a set
of clusters using clustering algorithms. The SVD
is then applied on each of the document clusters.
The algorithm thus selects basis vectors by treat-
ing equally each of the topics. Our experiments
on measuring document similarities have shown that
the algorithm achieves higher average precision with
lower number of dimensions than the SVD.
2 the Algorithm
The input to the CSSVD algorithm is an m?n term-
document matrix A. Documents in matrix A are
grouped into a set of document clusters. Here,
we adopt single-link algorithm to develop the ini-
tial clusters, then use K-means method to refine the
clusters. After clustering, columns in matrix A are
partitioned and regrouped into a set of sub-matrices
A1,A2,...,Aq. Each of these matrices represents a
document cluster. Assume Ai, 1 ? i ? q, is an
m? ni matrix, these sub-matrices are ranked in de-
creasing order of their sizes, i.e., n1 ? n2 ? ... ?
nq, then n1 + n2 + ...+ nq = n.
The algorithm computes basis vectors as follows:
the first basis vector u1 is computed from A1, i.e.,
the first left singular vector of A1 is selected. In or-
der to ensure that the basis vectors are orthogonal,
singular vectors are actually computed on residual
matrices. Rij , the residual matrix of Ai after the se-
lection of basis vectors u1, u2,..., uj , is defined as
Rij =
{ Ai j = 0
Ai ? proj(Aij) otherwise
where, proj(Aij) is the orthogonal projection of the
document vectors in Ai onto the span of u1,u2,...,uj ,
i.e.,
proj(Aij) =
j?
k=1
ukuTkAi
the residual matrix of Ai describes how much the
document vectors in Ai are excluded from the pro-
posed basis vectors u1, u2,..., uj . For the first ba-
sis vector computation, residual matrices are initial-
ized as original sub-matrices. The computation of
the residual matrix makes the remaining vectors per-
pendicular to the previous basis vectors, thus ensures
that the basis vectors are orthogonal, as the eigen-
vector computed next is a linear combination of the
remaining vectors.
After calculating a basis vector, the algorithm
judges whether the sub-matrices have been well rep-
resented by the derived basis vectors. The residual
ratio was defined as a criterion for this judgement,
rrij = ||Rij ||
2
F
ni ? (ki + 1)
where Rij is the residual matrix of Ai after j basis
vectors have been selected1; ni is the number of
the documents in matrix Ai; ki is the number
of singular vectors that have been selected from
matrix Ai. Residual ratios of each sub-matrix are
calculated. The sub-matrix with the largest residual
ratio is assumed to be the one that contains the
most information that has not been represented by
the previous chosen basis vectors. The first left
singular vector of this sub-matrix is computed and
selected as the next basis vector. As described
above, the computation of a basis vector uses the
corresponding residual matrix. Once a basis vector
is selected, its influence from each sub-matrix is
subtracted. The procedure is repeated until an
expected number of basis vectors have been chosen.
The pseudo-code of the algorithm for semantic
space construction is shown as follows:
1. Partition A into matrices A1,...,Aq corresponding
to document clusters, where Ai , 1 ? i ? q, is an
m? ni (n1 ? n2 ? ... ? nq) matrix.
2. For i=1,2,...,q {Ri= Ai; k[i]=0;}
3. j=1; r=1;
4. ur= the first unit eigenvector of RjRTj ;
5. For i=1,2,...,q Ri= Ri - uruTr Ri;
6. k[r]=k[r]+1; r=r+1;
7. For i=1,2,...,q rri= ||Ri||
2
F
(ni?(k[i]+1)) ;8. j=t if rrt > rrp for p=1,2,...,q and p 6= t;
9. If rrj ? threshold then stop else goto step 4.
For the single-link algorithm used in the CSSVD,
we use a threshold 0.2 and cosine measure to cal-
culate the similarity between two clusters in our ex-
periments. The performance of the CSSVD is also
relative to the number of dimensions of the created
1||A||F =
qP
i,j A2ij
70
subspace. As described above, the algorithm uses
the residual ratio as a stopping criterion for the basis
vector computation. In each iteration, after a basis
vector is created, the residual ratio is compared to a
threshold. Once the residual ratio of each sub-matrix
fell below a certain threshold, the process of basis-
vector selection is finished. In our experiments, the
threshold was trained on corpus.
After all the k basis vectors are chosen, a term-
document vector di can be converted to dki , a
vector in the k-dimensional space, by multiply-
ing the matrix of basis vectors following the stan-
dard method of orthogonal transformation,i.e., dki =
[u1, u2, ..., uk]Tdi.
3 Evaluation
3.1 Experimental Setup
For the evaluation of the algorithm, 38 topics from
the Text REtrieval Conference (TREC) collections
were used in our experiments. These topics include
foreign minorities, behavioral genetics, steel pro-
duction, etc. We deleted documents relevant to more
than one topic so that each document is related only
to one topic. The total number of documents used
was 2962. These documents were split into two dis-
joint groups, called ?pool 1? and ?pool 2?. The num-
ber of documents in ?pool 1? and ?pool 2? were 1453
and 1509, respectively. Each of the two groups used
19 topics.
We generated training and testing data by simu-
lating the result obtained by a query search. This
simulation is further simplified by selecting docu-
ments containing same keywords from each docu-
ment group. Thirty document sets were generated
from each of the two document groups, i.e. 60 doc-
ument sets in total. The number of documents for
each set ranges from 51 to 582 with an average of
128; the number of topics ranges from 5 to 19 with
an average of 12. Due to the limited number of the
document sets we created, these sets were used both
for training and evaluation. For the evaluation of the
documents sets from ?pool 1?, ?pool 2? was used for
training, and vice versa.
To construct the original term-document matrix,
the following operations were performed on each of
the documents: 1) filtering out all non-text tags in
the documents; 2) converting all the characters into
lower case; 3) removing stop words - a stoplist con-
taining 319 words was used; and 4) term indexing
- the tf.idf scheme was used to calculate a term?s
weight in a document. Finally, a document set is
represented as a matrix A = [aij ], where aij de-
notes the normalized weight assigned to term i in
document j.
3.2 Evaluation Measures
Our algorithm was motivated by a multi-document
summarization application which is mainly based
on measuring the similarities and differences among
text segments. Therefore, the basic requisite is to ac-
curately measure similarities among texts. Based on
this consideration, we used the CSSVD algorithm to
create the document vectors in a reduced space for
each of the document sets; cosine similarities among
these document vectors were computed; and the re-
sults were then compared with the TREC relevance
judgments. As each of the TREC documents we
used has one specific topic. Assume that similarity
should be higher for any document pair relevant to
the same topic than for any pair relevant to different
topics. The algorithm?s accuracy for measuring the
similarities among documents was evaluated using
average precision taken at various recall levels (Har-
man, 1995). Let pi denote the document pair that
has the ith largest similarity value among all pairs of
documents in the document set. The precision for an
intra-topic pair pk is calculated by
precision(pk) = number of pj where j ? kk
where pj is an intra-topic pair. The average of the
precision values over all intra-topic pairs is com-
puted as the average precision.
3.3 Results
The algorithms are evaluated by the average preci-
sion over 60 document sets. In order to make a com-
parison, two baseline algorithms besides CSSVD are
evaluated. One is the vector space model (VSM)
without dimension reduction. The other is SVD tak-
ing the left singular vectors as the basis vectors.
To treat the selection of dimensions as a separate
issue, we first evaluate the algorithms in terms of
the best average precision. The ?best average preci-
sion? means the best over all the possible numbers
71
of dimensions. The second row of Table 1 shows the
best average precision of our algorithm, VSM, and
SVD. The best average precision on average over 60
document sets of CSSVD is 69.6%, which is 11.5%
higher than VSM and 6.1% higher than SVD.
measure VSM SVD CSSVD
best average
precision (%) 58.1 63.5 69.6
average DR (%) N/A 54.4 32.1
average precision (%) 58.1 59.5 66.8
Table 1: the algorithm performance
In the experiments, we observed that the CSSVD al-
gorithm obtained its best performance with the num-
ber of dimensions lower than that of SVD. The Di-
mensional Ratio (DR) is defined as the number of
dimensions of the derived sub-space compared with
the dimension number of the original space, i.e.,
DR = # of dimensions in derived space# of dimensions in original space
The average dimensional ratio is calculated over all
the 60 document sets. As the algorithms? computa-
tional efficiency is dependent on the number of di-
mensions computed, our interest is in getting good
performance with an average dimensional ratio as
low as possible. The third row of Table 1 shows the
average dimensional ratio that yielded the best av-
erage precision. The average dimensional ratio that
CSSVD yielded the best average precision is 32.1%,
which is 22.3% lower than that of SVD. Thus, our
algorithm has the advantage of being computation-
ally inexpensive, assuming that we can find the op-
timal number of dimensions.
The bottom row of Table 1 shows the average
precision of the algorithms. The threshold used in
CSSVD algorithm was trained on corpus. Let p be
the threshold on residual ratio that yielded the best
average precision on the training data. The value
of p is then used as the threshold on the evaluation
data. For the SVD algorithm, the average dimen-
sional ratio that yielded the best average precision
on training data was used as the dimensional ratio
to determine the subspace dimensionality in evalua-
tion. The performance shown here are the average
of average precision over 60 document sets. Again,
the CSSVD achieves the best performance, which is
7.3% higher than the performance of SVD and 8.7%
higher than VSM.
4 Conclusion
We have presented an alternative algorithm, the
CSSVD, that creates vector representation for lin-
guistic units with reduced dimensionality. The al-
gorithm aims to compensate for SVD?s bias towards
dominant-topic documents by grouping documents
into clusters and selecting basis vectors from each
of the clusters. It introduces a threshold on the resid-
ual ratio of clusters as a stopping criterion of basis
vector selection. It thus treats each topic underly-
ing the document collection equally while focuses
on the dominant documents in each topic. The pre-
liminary experiments on measuring document simi-
larities have shown that the CSSVD achieves higher
average precision with lower number of dimensions
than the baseline algorithms.
Motivated by a multi-document summarization
application, the CSSVD algorithm?s emphasis on
topics and dominant information within each topic
meets the general demand of summarization. We ex-
pect that the algorithm fits the task of summarization
better than SVD. Our future work will focus on more
thorough evaluation of the algorithm and integrating
it into a summarization system.
5 Acknowledgments
We would like to thank Mark Sanderson, Horacio
Saggion, and Robert Gaizauskas for helpful com-
ments at the beginning of this research.
References
Ando R.K. 2000 Latent Sementic Space: Iterative
Scaling Improves Precision of Inter-document Similar-
ity Measurement. Proceedings of ACM SIGIR 2000,
Athens, Greece.
Deerwester S., Dumais S., Furnas G., and Landauer T.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41:391-407.
Golub G. and Loan C.V. 1996. Matrix Computations.
Johns-Hopkins University Press, Maryland, US.
Harman D.K. 1983. Overview of the second Text Re-
trieval Conference (TREC-2). Information Processing
Management, 31(3):271-289.
72
METER: MEasuring TExt Reuse
Paul Clough and Robert Gaizauskas and Scott S.L. Piao and Yorick Wilks
Department of Computer Science
University of She?eld
Regent Court, 211 Portobello Street,
She?eld, England, S1 4DP
finitial.surname@dcs.shef.ac.ukg
Abstract
In this paper we present results from
the METER (MEasuring TExt Reuse)
project whose aim is to explore issues
pertaining to text reuse and derivation,
especially in the context of newspapers
using newswire sources. Although the
reuse of text by journalists has been
studied in linguistics, we are not aware
of any investigation using existing com-
putational methods for this particular
task. We investigate the classication
of newspaper articles according to their
degree of dependence upon, or deriva-
tion from, a newswire source using a
simple 3-level scheme designed by jour-
nalists. Three approaches to measur-
ing text similarity are considered: n-
gram overlap, Greedy String Tiling,
and sentence alignment. Measured
against a manually annotated corpus of
source and derived news text, we show
that a combined classier with fea-
tures automatically selected performs
best overall for the ternary classica-
tion achieving an average F
1
-measure
score of 0.664 across all three cate-
gories.
1 Introduction
A topic of considerable theoretical and practical
interest is that of text reuse: the reuse of existing
written sources in the creation of a new text. Of
course, reusing language is as old as the retelling
of stories, but current technologies for creating,
copying and disseminating electronic text, make
it easier than ever before to take some or all of
any number of existing text sources and reuse
them verbatim or with varying degrees of mod-
ication.
One form of unacceptable text reuse, plagia-
rism, has received considerable attention and
software for automatic plagiarism detection is
now available (see, e.g. (Clough, 2000) for a re-
cent review). But in this paper we present a
benign and acceptable form of text reuse that
is encountered virtually every day: the reuse of
news agency text (called copy) in the produc-
tion of daily newspapers. The question is not
just whether agency copy has been reused, but
to what extent and subject to what transforma-
tions. Using existing approaches from computa-
tional text analysis, we investigate their ability
to classify newspapers articles into categories in-
dicating their dependency on agency copy.
2 Journalistic reuse of a newswire
The process of gathering, editing and publish-
ing newspaper stories is a complex and spe-
cialised task often operating within specic pub-
lishing constraints such as: 1) short deadlines;
2) prescriptive writing practice (see, e.g. Evans
(1972)); 3) limits of physical size; 4) readability
and audience comprehension, e.g. a tabloid's
vocabulary limitations; 5) journalistic bias, e.g.
political and 6) a newspaper's house style. Of-
ten newsworkers, such as the reporter and edi-
tor, will rely upon news agency copy as the basis
of a news story or to verify facts and assess the
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 152-159.
                         Proceedings of the 40th Annual Meeting of the Association for
importance of a story in the context of all those
appearing on the newswire. Because of the na-
ture of journalistic text reuse, dierences will
arise between reused news agency copy and the
original text. For example consider the follow-
ing:
Original (news agency) A drink-driver who
ran into the Queen Mother's o?cial Daim-
ler was ned $700 and banned from driving
for two years.
Rewrite (tabloid) A DRUNK driver who
ploughed into the Queen Mother's limo was
ned $700 and banned for two years yes-
terday.
This simple example illustrates the types of
rewrite that can occur even in a very short
sentence. The rewrite makes use of slang and
exaggeration to capture its readers' attention
(e.g. DRUNK, limo, ploughed). Deletion (e.g.
from driving) has also been used and the addi-
tion of yesterday indicates when the event oc-
curred. Many of the transformations we ob-
served between moving from news agency copy
to the newspaper version have also been re-
ported by the summarisation community (see,
e.g., McKeown and Jing (1999)).
Given the value of the information news agen-
cies supply, the ease with which text can be
reused and commercial pressures, it would be
benecial to be able to identify those news sto-
ries appearing in the newspapers that have relied
upon agency copy in their production. Potential
uses include: 1) monitoring take-up of agency
copy; 2) identifying the most reused stories ; 3)
determining customer dependency upon agency
copy and 4) new methods for charging customers
based upon the amount of copy reused. Given
the large volume of news agency copy output
each day, it would be infeasible to identify and
quantify reuse manually; therefore an automatic
method is required.
3 A conceptual framework
To begin to get a handle on measuring text
reuse, we have developed a document-level clas-
sication scheme, indicating the level at which
a newspaper story as a whole is derived from
agency copy, and a lexical-level classication
scheme, indicating the level at which individ-
ual word sequences within a newspaper story
are derived from agency copy. This framework
rests upon the intuitions of trained journalists
to judge text reuse, and not on an explicit lex-
ical/syntactic denition of reuse (which would
presuppose what we are setting out to discover).
At the document level, newspaper stories
are assigned to one of three possible categories
coarsely reecting the amount of text reused
from the news agency and the dependency of
the newspaper story upon news agency copy
for the provision of \facts". The categories in-
dicate whether a trained journalist can iden-
tify text rewritten from the news agency in
a candidate derived newspaper article. They
are: 1) wholly-derived (WD): all text in
the newspaper article is rewritten only from
news agency copy; 2) partially-derived (PD):
some text is derived from the news agency, but
other sources have also been used; and 3) non-
derived (ND): news agency has not been used
as the source of the article; although words may
still co-occur between the newspaper article and
news agency copy on the same topic, the jour-
nalist is condent the news agency has not been
used.
At the lexical or word sequence level, individ-
ual words and phrases within a newspaper story
are classied as to whether they are used to ex-
press the same information as words in news
agency copy (i.e. paraphrases) and or used to
express information not found in agency copy.
Once again, three categories are used, based on
the judgement of a trained journalist: 1) verba-
tim: text appearing word-for-word to express
the same information; 2) rewrite: text para-
phrased to create a dierent surface appearance,
but express the same information and 3) new:
text used to express information not appearing
in agency copy (can include verbatim/rewritten
text, but being used in a dierent context).
3.1 The METER corpus
Based on this conceptual framework, we have
constructed a small annotated corpus of news
texts using the UK Press Association (PA) as
the news agency source and nine British daily
newspapers
1
who subscribe to the PA as candi-
date reusers. The METER corpus (Gaizauskas
et al, 2001) is a collection of 1716 texts (over
500,000 words) carefully selected from a 12
month period from the areas of law and court
reporting (769 stories) and showbusiness (175
stories). 772 of these texts are PA copy and 944
from the nine newspapers. These texts cover 265
dierent stories from July 1999 to June 2000 and
all newspaper stories have been manually classi-
ed at the document-level. They include 300
wholly-derived, 438 partially-derived and 206
non-derived (i.e. 77% are thought to have used
PA in some way). In addition, 355 have been
classied according to the lexical-level scheme.
4 Approaches to measuring text
similarity
Many problems in computational text analy-
sis involve the measurement of similarity. For
example, the retrieval of documents to full a
user information need, clustering documents ac-
cording to some criterion, multi-document sum-
marisation, aligning sentences from one lan-
guage with those in another, detecting exact and
near duplicates of documents, plagiarism detec-
tion, routing documents according to their style
and identifying authorship attribution. Meth-
ods typically vary depending upon the match-
ing method, e.g. exact or partial, the degree
to which natural language processing techniques
are used and the type of problem, e.g. search-
ing, clustering, aligning etc. We have not had
time to investigate all of these techniques, nor
is there space here to review them. We have
concentrated on just three: ngram overlap mea-
sures, Greedy String Tiling, and sentence align-
ment. The rst was investigated because it of-
fers perhaps the simplest approach to the prob-
lem. The second was investigated because it has
been successfully used in plagiarism detection, a
problem which at least supercially is quite close
1
The newspapers include ve popular papers (e.g. The
Sun, The Daily Mail, Daily Star, Daily Mirror) and four
quality papers (e.g. Daily Telegraph, The Guardian, The
Independent and The Times).
to the text reuse issues we are investigating. Fi-
nally, alignment (treating the derived text as a
\translation" of the rst) seemed an intriguing
idea, and contrasts, certainly with the ngram ap-
proach, by focusing more on local, as opposed to
global measures of similarity.
4.1 Ngram Overlap
An initial, straightforward approach to assessing
the reuse between two texts is to measure the
number of shared word ngrams. This method
underlies many of the approaches used in copy
detection including the approach taken by Lyon
et al (2001).
They measure similarity using the set-
theoretic measures of containment and resem-
blance of shared trigrams to separate texts writ-
ten independently and those with su?cient sim-
ilarity to indicate some form of copying.
We treat each document as a set of overlap-
ping n-word sequences (initially considering only
n-word types) and compute a similarity score
from this. Given two sets of ngrams, we use
the set-theoretic containment score to measure
similarity between the documents for ngrams of
length 1 to 10 words. For a source text A and
a possibly derived text B represented by sets of
ngrams S
n
(A) and S
n
(B) respectively, the pro-
portion of ngrams in B also in A, the ngram con-
tainment C
n
(A;B), is given by:
C
n
(A;B) =
j S
n
(A) \ S
n
(B) j
j S
n
(B) j
(1)
Informally containment measures the number
of matches between the elements of ngram sets
S
n
(A) and S
n
(B), scaled by the size of S
n
(B).
In other words we measure the proportion of
unique n-grams in B that are found in A. The
score ranges from 0 to 1, indicating none to all
newspaper copy shared with PA respectively.
We also compare texts by counting only those
ngrams with low frequency, in particular those
occurring once. For 1-grams, this is the same as
comparing the hapax legomena which has been
shown to discriminate plagiarised texts from
those written independently even when lexical
overlap between the texts is already high (e.g.
70%) (Finlay, 1999). Unlike Finlay's work, we
nd that repetition in PA copy
2
drastically re-
duces the number of shared hapax legomena
thereby inhibiting classication of derived and
non-derived texts. Therefore we compute the
containment of hapax legomena (hapax contain-
ment) by comparing words occurring once in the
newspaper, i.e. those 1-grams in S
1
(B) that oc-
cur once with all 1-grams in PA copy, S
1
(A).
This containment score represents the number
of newspaper hapax legomena also appearing at
least once in PA copy.
4.2 Greedy String-Tiling
Greedy String-Tiling (GST) is a substring
matching algorithm which computes the degree
of similarity between two strings, for exam-
ple software code, free text or biological subse-
quences (Wise, 1996). Compared with previous
algorithms for computing string similarity, such
as the Longest Common Subsequence or
Levenshtein distance, GST is able to deal with
transposition of tokens (in earlier approaches
transposition is seen as a number of single inser-
tions/deletions rather than a single block move).
The GST algorithm performs a 1:1 matching
of tokens between two strings so that as much of
one token stream is covered with maximal length
substrings from the other (called tiles). In our
problem, we consider how much newspaper text
can be maximally covered by words from PA
copy. A minimum match length (MML) can be
used to avoid spurious matches (e.g. of 1 or
2 tokens) and the resulting similarity between
the strings can be expressed as a quantitative
similarity match or a qualitative list of common
substrings. Figure 1 shows the result of GST for
the example in Section 2.
Figure 1: Example GST results (MML=3)
2
As stories unfold, PA release copy with new, as well
as previous versions of the story
Given PA copy A, a newspaper text B and a
set of maximal matches, tiles, of a given length
between A and B, the similarity, gstsim(A,B),
is expressed as:
gstsim(A;B) =
P
i2tiles
length
i
j B j
(2)
4.3 Sentence alignment
In the past decade, various alignment algorithms
have been suggested for aligning multilingual
parallel corpora (Wu, 2000). These algorithms
have been used to map translation equivalents
across dierent languages. In this specic case,
we investigate whether alignment can map de-
rived texts (or parts of them) to their source
texts. PA copy may be subject to various
changes during text reuse, e.g. a single sen-
tence may derive from parts of several source
sentences. Therefore, strong correlations of sen-
tence length between the derived and source
sentences cannot be guaranteed. As a result,
sentence-length based statistical alignment al-
gorithms (Brown et al, 1991; Gale and Church,
1993) are not appropriate for this case. On the
other hand, cognate-based algorithms (Simard
et al, 1992; Melamed, 1999) are more e?cient
for coping with change of text format. There-
fore, a cognate-based approach is adopted for
the METER task. Here cognates are dened as
pairs of terms that are identical, share the same
stems, or are substitutable in the given context.
The algorithm consists of two principal com-
ponents: a comparison strategy and a scoring
function. In brief, the comparison works as fol-
lows (more details may be found in Piao (2001)).
For each sentence in the candidate derived text
DT the sentences in the candidate source text
ST are compared in order to nd the best match.
A DT sentence is allowed to match up to three
possibly non-consecutive ST sentences. The
candidate pair with the highest score (see be-
low) above a threshold is accepted as a true
alignment. If no such candidate is found, the
DT sentence is assumed to be independent of
the ST. Based on individual DT sentence align-
ments, the overall possibility of derivation for
the DT is estimated with a score ranging be-
tween 0 and 1. This score reects the propor-
tion of aligned sentences in the newspaper text.
Note that not only may multiple sentences in
the ST be aligned with a single sentence in the
DT, but also multiple sentences in the DT may
be aligned with one sentence in the ST.
Given a candidate derived sentence DS and
a proposed (set of) source sentence(s) SS, the
scoring function works as follows. Three basic
measures are computed for each pair of candi-
date DS and SS: SNG is the sum of lengths
of the maximum length non-overlapping shared
n-grams with n  2; SWD is the number of
matched words sharing stems not in an n-gram
guring in SNG; and SUB is the number of
substitutable terms (mainly synonyms) not g-
uring in SNG or SWD. Let L
1
be the length of
the candidate DS and L
2
the length of candidate
SS. Then, three scores PD, PS (Dice score) and
PV S are calculated as follows:
PSD =
SWD + SNG + SUB
L
1
PS =
2(SWD + SNG + SUB)
L
1
+ L
2
PSNG =
SNG
SWD + SNG + SUB
These three scores reect dierent aspects of
relations between the candidate DS and SS:
1. PSD: The proportion of the DS which is
shared material.
2. PS: The proportion of shared terms in DS
and SS. This measure prefers SS's which not
only contain many terms in the DS, but also
do not contain many additional terms.
3. PSNG: The proportion of matching n-
grams amongst the shared terms. This
measure captures the intuition that sen-
tences sharing not only words, but word se-
quences are more likely to be related.
These three scores are weighted and combined
together to provide an alignment metric WS
(weighted score), which is calculated as follows:
WS = ?
1
PSD+ ?
2
PS + ?
3
PSNG
where ?
1
+?
2
+?
3
= 1. The three weighting vari-
ables ?
i
(i = 1; 2; 3) have been determined empir-
ically and are currently set to: ?
1
= 0:85; ?
2
=
0:05; ?
3
= 0:1.
5 Reuse Classiers
To evaluate the previous approaches for measur-
ing text reuse at the document-level, we cast the
problem into one of a supervised learning task.
5.1 Experimental Setup
We used similarity scores as attributes for a ma-
chine learning algorithm and used the Weka 3.2
software (Witten and Frank, 2000). Because of
the small number of examples, we used tenfold
cross-validation repeated 10 times (i.e. 10 runs)
and combined this with stratication to ensure
approximately the same proportion of samples
from each class were used in each fold of the
cross-validation. All 769 newspaper texts from
the courts domain were used for evaluation and
randomly permuted to generate 10 sets. For
each newspaper text, we compared PA source
texts from the same story to create results in
the form: newspaper; class; score. These results
were ordered according to each set to create the
same 10 datasets for each approach thereby en-
abling comparison.
Using this data we rst trained ve single-
feature Naive Bayes classiers to do the ternary
classication task. The feature in each case was
a variant of one of the three similarity measures
described in Section 4, computed between the
two texts in the training set. The target classi-
cation value was the reuse classication category
from the corpus. A Naive Bayes classier was
used because of its success in previous classi-
cation tasks, however we are aware of its naive
assumptions that attributes are assumed inde-
pendent and data to be normally distributed.
We evaluated results using the F
1
-measure
(harmonic mean of precision and recall given
equal weighting). For each run, we calculated
the average F
1
score across the classes. The
overall average F
1
-measure scores were com-
puted from the 10 runs for each class (a single
accuracy measure would su?ce but the Weka
package outputs F
1
-measures). For the 10 runs,
the standard deviation of F
1
scores was com-
puted for each class and F
1
scores between all
approaches were tested for statistical signi-
cance using 1-way analysis of variance at a 99%
condence-level. Statistical dierences between
results were identied using Bonferroni analy-
sis
3
.
After examining the results of these single fea-
ture classiers, we also trained a \combined"
classier using a correlation-based lter ap-
proach (Hall and Smith, 1999) to select the com-
bination of features giving the highest classica-
tion score ( correlation-based ltering evaluates
all possible combinations of features). Feature
selection was carried for each fold during cross-
validation and features used in all 10 folds were
chosen as candidates. Those which occurred in
at least 5 of the 10 runs formed the nal selec-
tion.
We also tried splitting the training data into
various binary partitions (e.g. WD/PD vs. ND)
and training binary classiers, using feature se-
lection, to see how well binary classication
could be performed. Eskin and Bogosian (1998)
have observed that using cascaded binary clas-
siers, each of which splits the data well, may
work better on n-ary classication problems
than a single n-way classier. We then com-
puted how well such a cascaded classier should
perform using the best binary classier results.
5.2 Results
Table 1 shows the results of the single ternary
classiers. The baseline F
1
measure is based
upon the prior probability of a document falling
into one of the classes. The gures in parenthe-
sis are the standard deviations for the F
1
scores
across the ten evaluation runs. The nal row
shows the results for combining features selected
using the correlation-based lter.
Table 2 shows the result of training binary
classiers using feature selection to select the
most discriminating features for various binary
splits of the training data.
For both ternary and binary classiers feature
selection produced better results than using all
3
Using SPSS v10.0 for Windows.
Approach Category Avg F-measure
Baseline WD 0.340 (0.000)
PD 0.444 (0.000)
ND 0.216 (0.000)
total 0.333 (0.000)
3-gram WD 0.631 (0.004)
containment PD 0.624 (0.004)
ND 0.549 (0.005)
total 0.601 (0.003)
GST Sim WD 0.669 (0.004)
MML = 3 PD 0.633 (0.003)
ND 0.556 (0.004)
total 0.620 (0.002)
GST Sim WD 0.681 (0.003)
MML = 1 PD 0.634 (0.003)
ND 0.559 (0.008)
total 0.625 (0.004)
1-gram WD 0.718 (0.003)
containment PD 0.643 (0.003)
ND 0.551 (0.006)
total 0.638 (0.003)
Alignment WD 0.774 (0.003)
PD 0.624 (0.005)
ND 0.537 (0.007)
total 0.645 (0.004)
hapax WD 0.736 (0.003)
containment PD 0.654 (0.003)
ND 0.549 (0.010)
total 0.646 (0.004)
hapax cont. WD 0.756 (0.002)
1-gram cont. PD 0.599 (0.006)
alignment ND 0.629 (0.008)
(\combined") total 0.664 (0.004)
Table 1: A summary of classication results
possible features, with the one exception of the
binary classication between PD and ND.
5.3 Discussion
From Table 1, we nd that all classier results
are signicantly higher than the baseline (at
p < 0:01) and all dierences are signicant ex-
cept between hapax containment and alignment.
The highest F-measure for the 3-class problem
is 0.664 for the \combined" classier, which is
signicantly greater than 0.651 obtained with-
out. We notice that highest WD classication
is with alignment at 0.774, highest PD classi-
cation is 0.654 with hapax containment and
highest ND classication is 0.629 with combined
features. Using hapax containment gives higher
results than 1-gram containment alone and in
fact provides results as good as or better than
the more complex sentence alignment and GST
approaches.
Previous research by (Lyon et al, 2001) and
(Wise, 1996) had shown derived texts could be
distinguished using trigram overlap and tiling
with a match length of 3 or more, respectively.
Attributes Category Avg F
1
Correlation- alignment WD 0.942 (0.008)
based ND 0.909 (0.011)
lter total 0.926 (0.010)
alignment PD/ND 0.870 (0.003)
WD 0.770 (0.003)
total 0.820 (0.002)
alignment WD 0.778 (0.003)
PD 0.812 (0.002)
total 0.789 (0.002)
hapax cont. WD/PD 0.882 (0.002)
alignment ND 0.649 (0.007)
1-gram cont. total 0.763 (0.002)
1-gram PD 0.802 (0.002)
GST mml 3 ND 0.638 (0.007)
GST mml 1 total 0.720 (0.004)
alignment
GST mml 1 WD/ND 0.672 (0.002)
alignment PD 0.662 (0.003)
total 0.668 (0.003)
Table 2: Binary Classiers with feature selection
However, our results run counter to this be-
cause the highest classication scores are ob-
tained with 1-grams and an MML of 1, i.e. as
n or MML length increases, the F
1
scores de-
crease. We believe this results from two factors
which are characteristic of reuse in journalism.
First, since even ND texts are thematically sim-
ilar (same events being described) there is high
likelihood of coincidental overlap of ngrams of
length 3 or more (e.g. quoted speech). Secondly,
when journalists rewrite it is rare for them not
to vary the source.
For the intended application { helping the PA
to monitor text reuse { the cost of dierent mis-
classications is not equal. If the classier makes
a mistake, it is better that WD and ND texts are
mis-classied as PD, and PD as WD. Given the
dierence in distribution of documents across
classes where PD contains the most documents,
the classier will be biased towards this class
anyway as required. Table 3 shows the confu-
sion matrix for the combined ternary classier.
WD PD ND
WD 203 55 4
PD 79 192 70
ND 3 53 109
Table 3: Confusion matrix for combined ternary
classier
Although the overall F
1
-measure score is low
(0.664), mis-classication of both WD as ND
and ND as WD is also very low, as most mis-
classications are as PD. Note the high mis-
classication of PD as both WD and ND, re-
ecting the di?culty of separating this class.
From Table 2, we nd alignment is a selected
feature for each binary partition of the data.
The highest binary classication is achieved be-
tween the WD and ND classes using alignment
only, and the highest three scores show WD is
the easiest class to separate from the others.
The PD class is the hardest to isolate, reect-
ing the mis-classications seen in Table 3.
To predict how well a cascaded binary classi-
er will perform we can reason as follows. From
the preceding discussion we see that WD can
be separated most accurately; hence we choose
WD versus PD/ND as the rst binary classier.
This forces the second classier to be PD versus
ND. From the results in Table 2 and the follow-
ing equation to compute the F
1
measure for a
two-stage binary classier
WD + (PD=ND)(
PD+ND
2
)
2
we obtain an overall F
1
measure for ternary clas-
sication of 0.703, which is signicantly higher
than the best single stage ternary classier.
6 Conclusions
In this paper we have investigated text reuse in
the context of the reuse of news agency copy, an
area of theoretical and practical interest. We
present a conceptual framework in which we
measure reuse and based on which the METER
corpus has been constructed. We have presented
the results of using similarity scores, computed
using n-gram containment, Greedy String Tiling
and an alignment algorithm, as attributes for
a supervised learning algorithm faced with the
task of learning how to classify newspaper sto-
ries as to whether they are wholly, partially or
non-derived from a news agency source. We
show that the best single feature ternary clas-
sier uses either alignment or simple hapax con-
tainment measures and that a cascaded binary
classier using a combination of features can
outperform this.
The results are lower than one might like,
and reect the problems of measuring journalis-
tic reuse, stemming from complex editing trans-
formations and the high amount of verbatim
text overlapping as a result of thematic simi-
larity and \expected" similarity due to, e.g., di-
rect/indirect quotes. Given the relative close-
ness of results obtained by all approaches we
have considered, we speculate that any compar-
ison method based upon lexical similarity will
probably not improve classication results by
much. Perhaps improved performance at this
task may possible by using more advanced nat-
ural language processing techniques, e.g. better
modeling of the lexical variation and syntactic
transformation that goes on in journalistic reuse.
Nevertheless the results we have obtained are
strong enough in some cases (e.g. wholly derived
texts can be identied with > 80% accuracy) to
begin to be exploited.
In summary measuring text reuse is an excit-
ing new area that will have a number of appli-
cations, in particular, but not limited to, mon-
itoring and controlling the copy produced by a
newswire.
7 Future work
We are adapting the GST algorithm to deal with
simple rewrites (e.g. synonym substitution) and
to observe the eects of rewriting upon nding
longest common substrings. We are also experi-
menting using the more detailed METER corpus
lexical-level annotations to investigate how well
the GST and ngrams approaches can identify
reuse at this level.
A prototype browser-based demo of both the
GST algorithm and alignment program, allow-
ing users to test arbitrary text pairs for simi-
larity, is now available
4
and will continue to be
enhanced.
Acknowledgements
The authors would like to acknowledge the
UK Engineering and Physical Sciences Re-
search Council for funding the METER project
(GR/M34041). Thanks also to Mark Hepple for
helpful comments on earlier drafts.
4
See http://www.dcs.shef.ac.uk/nlp/meter.
References
P.F. Brown, J.C. Lai, and R.L. Mercer. 1991. Aligning
sentences in parallel corpora. In Proceedings of the
29th Annual Meeting of the Assoc. for Computational
Linguistics, pages 169{176, Berkeley, CA, USA.
P Clough. 2000. Plagiarism in natural and programming
languages: An overview of current tools and technolo-
gies. Technical Report CS-00-05, Dept. of Computer
Science, University of She?eld, UK.
E. Eskin and M. Bogosian. 1998. Classifying text docu-
ments using modular categories and linguistically mo-
tivated indicators. In AAAI-98 Workshop on Learning
for Text Classication.
H. Evans. 1972. Essential English for Journalists, Edi-
tors and Writers. Pimlico, London.
S. Finlay. 1999. Copycatch. Master's thesis, Dept. of
English. University of Birmingham.
R. Gaizauskas, J. Foster, Y. Wilks, J. Arundel,
P. Clough, and S. Piao. 2001. The meter corpus:
A corpus for analysing journalistic text reuse. In Pro-
ceedings of the Corpus Linguistics 2001 Conference,
pages 214|223.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpus. Computational Lin-
guistics, 19:75{102.
M.A. Hall and L.A. Smith. 1999. Feature selection for
machine learning: Comparing a correlation-based l-
ter approach to the wrapper. In Proceedings of the
Florida Articial Intelligence Symposium (FLAIRS-
99), pages 235{239.
C. Lyon, J. Malcolm, and B. Dickerson. 2001. Detecting
short passages of similar text in large document collec-
tions. In Conference on Empirical Methods in Natural
Language Processing (EMNLP2001), pages 118{125.
K. McKeown and H. Jing. 1999. The decomposition of
human-written summary sentences. In SIGIR 1999,
pages 129{136.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics, pages
107{130.
Scott S.L. Piao. 2001. Detecting and measuring text
reuse via aligning texts. Research Memorandum
CS-01-15, Dept. of Computer Science, University of
She?eld.
M. Simard, G. Foster, and P. Isabelle. 1992. Using
cognates to align sentences in bilingual corpora. In
Proceedings of the 4th Int. Conf. on Theoretical and
Methodological Issues in Machine Translation, pages
67{81, Montreal, Canada.
M. Wise. 1996. Yap3: Improved detection of similarities
in computer programs and other texts. In Proceedings
of SIGCSE'96, pages 130{134, Philadelphia, USA.
I.H. Witten and E. Frank. 2000. Datamining - practi-
cal machine learning tools and techniques with Java
implementations. Morgan Kaufmann.
D. Wu. 2000. Alignment. In R. Dale and H. Moisl and
H. Somers (eds.), A Handbook of Natural Language
Processing, pages 415{458. New York: Marcel Dekker.
Abstract
AKT is a major research project
applying a variety of technologies to
knowledge management. Knowledge
is a dynamic, ubiquitous resource,
which is to be found equally in an
expert's head, under terabytes of data,
or explicitly stated in manuals. AKT
will extend knowledge management
technologies to exploit the potential
of the semantic web, covering the use
of knowledge over its entire lifecycle,
from acquisition to maintenance and
deletion. In this paper we discuss how
HLT will be used in AKT and how
the use of HLT will affect different
areas of KM, such as knowledge
acquisition, retrieval and publishing.
1 Introduction
As globalisation reduces the competitive
advantage existing between companies, the role
of proprietary information and its appropriate
management becomes all-important. A
company?s value depends more and more on
?intangible assets?1 which exist in the minds of
employees, in databases, in files and in a
multitude of documents. It is the goal of
knowledge management (KM) technologies to
make computer systems which provide access to
this intangible knowledge present in a company
or organisation.  The system must make it
possible to share, store and retrieve the
collective expertise of all the people in an
organization. At present, many companies spend
                                                     
1 A term coined by Karl-Erik Sveiby
considerable resources on knowledge
management; estimates range between 7 and
10% of revenues (Davenport 1998).
In developing a knowledge management
system, the knowledge must first be captured or
acquired in some form which is usable by a
computer. The knowledge acquisition
bottleneck, so well-known in AI, is just as
important in knowledge management. The
acquisition of knowledge does not become less
difficult in a business environment and often
requires a sea-change in company culture in
order to persuade users to accommodate to the
technology adopted, precisely because
knowledge acquisition is so difficult.
Once knowledge has been acquired, it must be
managed, i.e. modelled, updated and published.
Modelling means representing information in a
way that is both manageable and easy to
integrate with the rest of the company?s
knowledge. Updating is necessary because
knowledge is dynamic. Part of its importance
for a company or individual lies in the fact that
knowledge is ever changing and keeping up
with the change is a crucial dimension in
knowledge management. Publishing is the
process that allows sharing the knowledge
across the company. These needs have
crystallised in efforts to develop the so-called
Semantic Web. It is envisaged that in the future,
the content currently available on the Web (both
Internets and Intranets) as raw data will be
automatically annotated with machine-readable
semantic information.  In such a case, we will
no longer speak of information retrieval but
rather of Knowledge Retrieval because instead
of obtaining thousands of potentially relevant or
irrelevant documents, only the dozen or so
documents that are truly needed by the user will
be presented to them.
Using HLT for Acquiring, Retrieving and Publishing Knowledge in AKT:
Position Paper
K. Bontcheva, C. Brewster, F. Ciravegna, H. Cunningham,
L. Guthrie, R. Gaizauskas, Y. Wilks
Department of Computer Science, the University of Sheffield,
Regent Court, 211 Portobello Street, S1 4DP Sheffield, UK
Email: N.Surname@dcs.shef.ac.uk
 In this paper we present the way Human
Language Technology (HLT) is used to address
several facets of the KM problem:  acquiring,
retrieving, and publishing knowledge. The work
presented in this paper is supported by the AKT
project (Advanced Knowledge Technologies), a
multimillion pound six year research project
funded by the EPSRC in the UK. AKT, started
in 2000, involves the University of
Southampton, the Open University, the
University of Edinburgh, the University of
Aberdeen, and the University of Sheffield
together with a large number of major UK
companies. Its objectives are to develop
technologies to cope with the six main
challenges of knowledge management:
? acquisition ? reuse
? modelling ? publication
? retrieval/extraction ? maintenance
These challenges will be addressed by the
University of Sheffield in the context of AKT
by the application of a variety of human
language technologies. Here, we consider only
the contribution of HLT to the acquisition of
knowledge, its retrieval and extraction, its
publication, and finally the role of appropriate
HLT infrastructure to the completion of these
goals.
2 Knowledge Acquisition
Knowledge acquisition (KA) is concerned with
the process of turning data into coherent
knowledge for a computer program.  The need
for effective KA methods increases as the
quantity of data available electronically
increases year by year, and the importance it
plays in our society is more and more
recognised. The challenge, we believe, lies in
designing effective techniques for acquiring the
vast amounts of (largely) tacit knowledge. KA is
a complex process, which traditionally is
extremely time consuming.
Existing KA methodologies are varied but
almost always require a great deal of manual
input. One methodology, often used in Expert
Systems, involves the time-consuming process
of structured interviews (?protocols?), which are
then analysed by knowledge engineers in order
to codify and model the knowledge of an expert
in a particular domain. Even if a complex expert
system is not required, all forms of KA are very
labour intensive. Yahoo currently employs over
100 people to keep its category hierarchy up to
date (Dom 1999). Some methodologies have
started to appear to automate this process,
although still limited to some steps in the KA
process. They depend on replacing the
introspection of knowledge engineers or the
extended elicitations of the protocol methods
(Ericsson and Simon 1984) by using Human
Language Technologies, more specifically
Information Extraction, Natural Language
Processing and Information Retrieval.
 Although knowledge acquisition produces
data (knowledge) for use by a computer
program, the form and content of that
knowledge is often debated in the research
community.  Ontologies have emerged as one of
the most popular means of modelling the
knowledge of a domain.  The meaning of this
word varies somewhat in the literature, but
minimally it is a hierarchical taxonomy of
categories, concepts or words. Ontologies can
act as an index to the memory of an organisation
and facilitate semantic searches and the retrieval
of knowledge from the corporate memory as it
is embodied in documents and other archives.
Repeated research has shown their usefulness,
especially for specific domains (J?rvelin and
Kek?l?inen 2000). The process of ontology
construction is illustrated in the rest of this
section.
2.1 Taxonomy construction
We propose to introduce automation in the stage
of taxonomy construction mainly in order to
eliminate or reduce the need for extensive
elicitation of data.  In the literature approaches
to construction of taxonomies of concepts have
been proposed (Brown et al 1992, McMahon
and Smith 1996, Sanderson and Croft 1999).
Such approaches either use a large collection of
documents as their sole data source, or they can
attempt to use existing concepts to extend the
taxonomy (Agirre et al2000, Scott 1998).  We
intend to develop a semi-automatic method that,
starting from a seed ontology sketched by the
user, produces the final ontology via a cycle of
refinements by eliciting knowledge from a
collection of texts. In this approach the role of
the user should only be that of proposing an
initial ontology and validating/changing the
different versions proposed by the system.
We intend to integrate a methodology for
automatic hierarchy definition (such as
(Sanderson and Croft 1999)) with a method for
the identification of terms related to a concept
in a hierarchy (such as (Scott 1998)). The
advantage of this integration is that, as
knowledge is continually changing, we can
reconstruct an appropriate domain specific
ontology very rapidly. This does not preclude
incorporating an existing ontology and using the
tools to extend and update it on the basis of
appropriate texts. Finally an ontology defined in
this way has the particular advantage that it
overcomes the well-known ?Tennis problem?
associated with many predefined ontologies
such as WordNet, i.e where terms closely
related in a given domain are structurally very
distant such as ball and court, for example.
In addition we intend to employ classic
Information Extraction techniques (described
below) such as named entity recognition
(Humphreys et al 1998) in order to pre-process
the text, as the identification of complex terms
such as proper names, dates, numbers, etc,
allows to reduce data sparseness in learning
(Ciravegna 2000).
We plan to introduce many cycles of ontology
learning and validation. At each stage the
defined ontology can be: i) validated/corrected
by a user/expert; ii) used to retrieve a larger set
of appropriate documents to be used for further
refinement (J?rvelin and Kek?l?inen 2000); iii)
passed on to the next development stage.
2.2 Learning Other Relations
This stage proceeds to build on the skeletal
ontology in order to specify, as much as
possible without human intervention, relations
among concepts in the ontology, other than
ISAs. In order to flesh the concept relations, we
need to identify relations such as synonymy,
meronymy, antonymy and other relations. We
plan to integrate a variety of methods existing in
the literature, e.g. by using recurrences in verb
subcategorisation as a symptom of general
relations (Basili et al 1998), by using Morin?s
user-guided approach to identify the correct
lexico/syntactic environment (Morin 1999), and
by using methods such as (Hays 1997) to locate
specific cases of synonymy.
3 Knowledge Extraction
Assuming that the shape of knowledge has been
acquired and adequately modelled, it will have
to be stored in a repository from which it is
retrieved as and when needed. On the one hand
there is the problem of retrieving instances in
order to populate the resulting knowledge base.
On the other hand, considering that repositories
could become very substantial in size, there is
the necessity to navigate the repository in order
to extract the knowledge when needed. In this
section we focus on the problem of knowledge
base population, as it is in our opinion the most
challenging from the HLT point of view.
3.1 Knowledge Base  Population
Instance identification for Knowledge Base
population can be performed by HLT-based
document analysis. With the term documents,
we mean a wide variety of types of texts such as
plain texts, web pages, knowledge elicitation
interview transcriptions (protocols), etc.  For the
sake of this paper we limit our analysis to
language related tasks only, ignoring the
problem of multi-media information. As a first
step instance identification requires the
identification of relevant documents containing
citation of the interesting information
(document classification). Then it requires the
ability to identify and extract information from
documents (Information Extraction from text).
3.2 Document Classification
Text classification for IE purposes has been
explored both in the MUC conferences as well
as in some commercially oriented projects
(Ciravegna et al 2000). In concrete terms
classification is used in order to identify the
scenario to apply to a specific set of texts, while
IE will identify (i.e. index) the instances in the
texts.  In most cases of application document
classification is quite straightforward, being
limited to the Boolean classification of a
document between relevant/irrelevant (single
scenario application as in the MUC
conferences). In cases in which knowledge may
be distributed along a number of different
detailed scenarios, full document classification
is then needed. In such cases, two main
characteristics are relevant for the classification
approach: flexibility and refinability (Ciravegna
et al 1999). Flexibility is needed with respect
to both the number of the categories and the
granularity of the classification to be coped
with. Three main types of classification can be
identified: coarse-grained, fine-grained, and
content-based. Coarse-grained classification is
performed among a relatively small number of
classes (e.g., some dozens) that are sharply
different (e.g., sport vs finance). This can be
obtained reliably and efficiently by the
application of statistical classifiers. Fine-
grained classification is performed over a
usually larger number of classes that can be
very similar (e.g., discriminating between news
about private bond issues and news about public
bond issues). This type of classification
generally requires some more knowledge-
oriented approaches such as pattern-based
classification. Sometimes categories are so
similar that classification needs to be content-
based, i.e. it can be performed only by
extracting the news content (e.g., finding news
articles issued by English financial institutions
referring to amounts in excess of 100,000 Euro).
In this case some forms of shallow adaptive
Information Extraction can be used (see next
section). Refinability concerns the possibility
of performing classification in a sequence of
steps, each one providing a more precise
classification (from coarse-grained to content-
based). In the current technological situation
coarse-grained classification can be performed
quickly, while the systems available for more
fine-grained classification are much slower and
less general purpose. When the amount of
textual material is large an incremental
approach, based on some level of coarse-grained
classification further refined by successive
analysis, proves to be very effective. A refinable
classification is generally performed over a
hierarchy of classes. A refinement may revise
the categories assigned to specific texts with
more specialised classes from the hierarchy.
More complex techniques are invoked only
when needed and, in any case, within an already
detected context (Ciravegna et al 1999).
We plan to produce a number of solutions for
text classification, adaptable to different
scenarios and situations, following the criteria
mentioned above.
3.3 Information Extraction
Information extraction from text (IE) is the
process of mapping of texts into fixed format
output (templates) representing the key
information (Gaizauskas 1997). In using IE for
KM, templates represent an intermediate format
for mapping the information in the texts into
ontology instances. Templates can be semi-
automatically derived from the ontology. We
plan to use IE for a number of passes: on the
one hand, we plan to populate a knowledge base
with instances as mentioned above. On the other
hand, IE can be used to monitor relevant
changes in the information, providing a
fundamental contribution to the problem of
knowledge updating. We have a long experience
in IE from texts, Sheffield having actively
participated in the MUC conferences and in the
TIPSTER project, activities that historically
have made a fundamental contribution to
making IE as we now know it.  The new
challenge we are currently addressing is
adaptivity. Adaptivity is a major goal for
Information Extraction, especially in the case of
its application to knowledge management, as
KM is a process that has to be distributed
throughout companies. The real value of IE will
become apparent when it can be adapted to new
applications and scenarios directly by the final
user without the intervention of IE experts. The
goal for research in adaptive IE is to create
systems adaptable to new applications/domains
by using only an analyst?s knowledge, i.e.
knowledge about the domain/scenario.
There are two directions of research in
adaptive IE, both involving the use of Machine
Learning. On the one hand machine learning is
used to automate as much as possible the tasks
an IE expert would perform in application
development (Cardie 1997) (Yangarber et al
2000). The goal here is to reduce the porting
time to a new application (and hence the cost).
This area of research comes mainly from the
MUC community. Currently, the technology
makes use mainly of NLP-intensive
technologies and the type of texts addressed are
mainly journal articles.
On the other hand, there is an attempt to make
IE systems adaptable to new
domains/applications by using only an analyst?s
knowledge, i.e. knowledge about the
domain/scenario only (Kushmerick et al 1997),
(Califf 1998), (Muslea et al 1998), (Freitag and
McCallum 1999), (Soderland 1999), (Freitag
and Kushmerick 2000), (Ciravegna 2001a).
Most research has so far focused on Web-
related texts (e.g. web pages, email, etc.)
Successful commercial products have been
created and there is an increasing interest on IE
in the Web-related market.  Current adaptive
technologies make no use of natural language
processing in the web context, as extra linguistic
structures (e.g. HTML tags, document
formatting, and ungrammatical stereotypical
language) are the elements used to identify
information. Linguistically intensive approaches
are difficult or unnecessary in such cases. When
these non-linguistic approaches are used on
texts with a reduced (or no) structure, they tend
to be ineffective.
There is a technological gap between adaptive
IE on free texts and adaptive IE on web-related
texts. For the purposes of KM, such a gap has to
be bridged so to create a set of technologies able
to cover the whole range of potential
applications for different kinds of texts, as the
type of texts to be analysed for KM may vary
dramatically from case to case. We plan to
bridge this gap via the use of lazy natural
language processing. We intend to use an
approach where the system starts with a range
of potential methodologies (from shallow to
linguistically intensive) and learns from a
training corpus which is the most effective
approach for the particular case under
consideration. A number of factors can
influence the choice: from the type of texts to be
analysed to the type of information the user is
able to provide in adapting the system. In the
first case the system will have to identify what
type of task is under consideration and select the
correct level of analysis  (e.g. language based
for free texts). Formally in this case the level of
language analysis is one of the parameters the
learner will have to learn. Concerning the type
of tagging the user is able to provide: different
users are able to provide different levels of
information in training the system: IE-trained
users are able to provide sophisticated tagging,
maybe inclusive of syntactic, semantic or
pragmatic information. Na?ve users on the other
hand are only able to provide some basic
information (e.g. to spot the relevant
information in the texts and highlight it in
different colours). We plan to develop a system
able to cope with a wide of variety of situations
by starting from the (LP)2 algorithm and
enhancing its learning capabilities on free texts
(Ciravegna 2001) and developing a powerful
human-computer interface for system adaptation
(Ciravegna and Petrelli 2001).
4 Knowledge Publishing
Knowledge is only effective if it is delivered in
the right form, at the right place, to the right
person at the right time. Knowledge publishing
is the process that allows getting knowledge to
the people who need it in a form that they can
use. As a matter of fact, different users need to
see knowledge presented and visualised in quite
different ways. The dynamic construction of
appropriate perspectives is a challenge which, in
AKT, we will address from the perspective of
generating automatically such presentations
from the ontologies acquired by the KA and KE
methods, discussed in the previous sections.
Natural Language Generation (NLG) systems
automatically produce language output (ranging
from a single sentence to an entire document)
from computer-accessible data, usually encoded
in a knowledge or data base (Reiter 2000). NLG
techniques have already been used successfully
in a number of application domains, the most
relevant of which is automatic production of
technical documentation (Reiter et al 1995),
(Paris et al 1996). In the context of KM and
knowledge publishing in particular, NLG is
needed for knowledge diffusion and
documenting ontologies. The first task is
concerned with personalised presentation of
knowledge, in the form needed by each specific
user and tailored to the correct language type
and the correct level of details. The latter is a
very important issue, because as discussed
earlier, knowledge is dynamic and needs to be
updated frequently. Consequently, the
accompanying documentation which is vital for
the understanding and successful use of the
acquired knowledge, needs to be updated in
sync. The use of NLG simplifies the ontology
maintenance and update tasks, so that the
knowledge engineer can concentrate on   the
knowledge itself, because the documentation is
automatically updated as the ontology changes.
The NLG-based knowledge publishing tools
will also utilise the ontology instances extracted
from documents using the IE approaches
discussed in Section 3.3. The dynamically
generated documentation will not only include
these instances, as soon as they get extracted,
but it will also provide examples of their
occurrence in the documents, thus facilitating
users? understanding and use of the ontology.
Our approach to knowledge publishing is based
on an existing framework for generation of user-
adapted hypertext explanations (Bontcheva
2001), (Bontcheva and Wilks 2001). The
framework incorporates a powerful agent
modelling module, which is used to tailor the
explanations to the user?s knowledge, task, and
preferences. We are now also extending the
personalisation techniques to account for user
interests. The main challenge for NLG will be
to develop robust and efficient techniques for
knowledge publishing which can operate on
large-scale knowledge resources and support the
personalised presentation of diverse
information, such as speech, video, text,
graphics (see (Maybury 2001)).
The other challenge in using NLG for
knowledge publishing is to develop tools and
techniques that will enable knowledge
engineers, instead of linguists, to create and
customise the linguistic resources (e.g., domain
lexicon) at the same time as they create and edit
the ontology.  In order to allow such inter-
operability with the KA tools, we will integrate
the NLG tools in the GATE infrastructure,
discussed next.
5 HLT Infrastructure
The range and complexity of the task of
knowledge management make imperative the
need for standardisation. While there has been
much talk about the re-use of knowledge
components such ontologies, much less has
been undertaken to standardise the
infrastructure for tools and their development.
The types of data structures typically involved
are large and complex, and without good tools
to manage and allow succinct viewing of the
data we will continue to work below our
potential. The University of Sheffield has
pioneered in the Gate and Gate 2 projects the
development of an architecture for text
engineering (Cunningham et al 1997),
(Cunningham et al 2000). Given the modular
architecture and component structure of Gate, it
is natural to build on this basis to extend the
capabilities of Gate so as to provide the most
suitable possible environment for tool
development, implementation and evaluation in
AKT. The system will provide a single
interaction and deployment point for the roll-out
of HLT in Knowledge Management. We expect
Gate2 to act as the skeleton for a large range of
knowledge management activities within AKT
and plan to extend its capabilities within the life
of the AKT project by integrating with suitable
ontological and lexical databases in order to
permit the use of  the Gate system with large
bodies of heterogeneous data
6 Conclusion and Future Work
We have presented how we plan to use HLT for
helping KM in AKT. We believe that HLT can
make a substantial contribution to the following
issues in  KM:
? Cost reduction: KM is an expensive task,
especially in the acquisition phase. HLT can
aid in automating both the acquisition of the
structure of the ontology to be learnt and in
populating such ontology with instances. It
will also provide support for automatic
knowledge documentation.
? Time reduction: KM is a slow task: HLT
can help in making it more efficient by
reducing the need for the human effort;
? Subjectivity reduction: this is a main
problem in knowledge identification and
selection. Subjective knowledge is difficult
to integrate with the rest of the company?s
knowledge and its use is somehow difficult.
KM constitutes a challenge for HLT as it
provides a number of fields of application and
in particular it challenges the integration of a set
of techniques for a common goal.
Acknowledgement
This work is supported under the Advanced
Knowledge Technologies (AKT)
Interdisciplinary Research Collaboration (IRC),
which is sponsored by the UK Engineering and
Physical Sciences Research Council under grant
number GR/N15764/01. The AKT IRC
comprises the Universities of Aberdeen,
Edinburgh, Sheffield, Southampton and the
Open University.
References
Agirre, E. O. Ansa, E. Hovy, and  D.
Mart?nez 2000. Enriching very large ontologies
using the WWW, Proceedings of the ECAI 2000
workshop ?Ontology Learning?.
Basili, R., R. Catizone, M. Stevenson, P.
Velardi, M. Vindigni, and Y. Wilks. 1998. ?An
Empirical Approach to Lexical Tuning?.
Proceedings of the Adapting Lexical and
Corpus Resources to Sublanguages and
Applications Workshop, held jointly with 1st
LREC Granada, Spain.
Bontcheva, K. 2001. Generating adaptive
hypertext explainations with a nested agent
model.  Ph. D. Thesis, University of Sheffield.
Bontcheva, K. and Wilks, Y. 2001. Dealing
with Dependencies between Content Planning
and Surface Realisation in a Pipeline
Generation Architecture. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Brown, P.F., Peter F., V. J. Della Pietra, P.
V. DeSouza, J. C. Lai, and R. L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18, 467-479.
 Califf, M. E. 1998. Relational Learning
Techniques for Natural Language Information
Extraction. Ph.D. thesis, Univ. Texas, Austin,
www/cs/utexas.edu/users/mecaliff
C. Cardie, `Empirical methods in
information extraction', AI Journal,18(4), 65-
79, (1997).
F. Ciravegna, A. Lavelli, N. Mana, J.
Matiasek, L. Gilardoni, S. Mazza, M. Ferraro,
W. J. Black F. Rinaldi, and D. Mowatt.
FACILE: Classifying Texts Integrating Pattern
Matching and Information Extraction. In
Proceedings of the 16th International Joint
Conference On Artificial Intelligence
(IJCAI99), Stockholm, Sweden, 1999.
F. Ciravegna, A. Lavelli,, L. Gilardoni, S.
Mazza, W. J. Black, M. Ferraro, N. Mana, J.
Matiasek, F. Rinaldi. Flexible Text
Classification for Financial
Applications: The FACILE System. In
Proceedings of Prestigious Applications sub-
conference (PAIS2000) sub-conference of the
14th European Conference On Artificial
Intelligence (ECAI2000), Berlin, Germany,
August, 2000.
 Ciravegna, F. 2001. Adaptive Information
Extraction from Text by Rule Induction and
Generalisation. Proceedings of the 17th
International Joint Conference on Artificial
Intelligence (IJCAI2001), Seattle.
Ciravegna, F. and D. Petrelli. 2001. User
Involvement in customizing Adaptive
Information Extraction from Texts: Position
Paper. Proceedings of the IJCAI01 Workshop on
Adaptive Text Extraction and Mining, Seattle.
Cunningham, H., K. Humphreys, R.
Gaizauskas and Y. Wilks. 1997. Software
Infrastructure for Natural Language Processing.
Proceedings of the Fifth Conference on Applied
Natural Language Processing (ANLP-97).
Cunningham H., K. Bontcheva, V. Tablan
and Y. Wilks. 2000. Software Infrastructure for
Language Resources: a Taxonomy of Previous
Work and a Requirements Analysis.
Proceedings of the Second Conference on
Language Resources Evaluation, Athens.
Dom, B. 1999. Automatically finding the
best pages on the World Wide Web (CLEVER).
Search Engines and Beyond: Developing
efficient knowledge management systems.
Boston, MA.
Ericsson, K. A. and H. A. Simon. 1984.
Protocol Analysis: verbal reports as data. MIT
Press, Cambridge, Mass.
Freitag, D. and A. McCallum. 1999
Information Extraction with HMMs and
Shrinkage. AAAI-99 Workshop on Machine
Learning for Information Extraction, Orlando,
FL. (www.isi.edu/~muslea/RISE/ML4IE/)
Freitag, D. and N. Kushmerick. 2000.
Boosted wrapper induction. F. Ciravegna, R.
Basili, R. Gaizauskas, ECAI2000 Workshop on
Machine Learning for Information Extraction,
Berlin, 2000, (www.dcs.shef.ac.uk/~fabio/ecai-
workshop.html)
Hays, P. R. 1997. Collocational Similarity:
Emergent Patterns in Lexical Environments,
PhD. Thesis. School of English, University of
Birmingham
Humphreys, K., R. Gaizauskas, S. Azzam, C.
Huyck, B. Mitchell, H. Cunningham and  Y.
Wilks. 1998. Description of the University of
Sheffield LaSIE-II System as used for MUC-7.
Proceedings of the 7th Message Understanding
Conference.
J?rvelin, K.  and J. Kek?l?inen. 2000. IR
evaluation methods for retrieving highly
relevant documents. Proceedings of the 23rd
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, , Athens.
Kushmerick, N., D. Weld, and R.
Doorenbos. 1997. Wrapper induction for
information extraction. Proceedings of 15th
International Conference on Artificial
Intelligence, IJCAI-97.
Manchester, P. 1999. Survey ? Knowledge
Management. Financial Times, 28.04.99.
Maybury, M.. 2001. Human Language
Technologies for Knowledge Management:
Challenges and Opportunities. Workshop on
Human Language Technology and Knowledge
Management. Toulouse, France.
McMahon, J. G. and  F. J. Smith. 1996
Improving Statistical Language Models
Performance with Automatically  Generated
Word Hierarchies. Computational Linguistics,
22(2), 217-247, ACL/MIT.
Morin, E. 1999. Using Lexico-Syntactic
patterns to Extract Semantic Relations between
Terms from Technical Corpus, TKE 99,
Innsbruck, Austria.
Muslea, I., S. Minton, and C. Knoblock.
1998. Wrapper induction for semi-structured,
web-based information sources. Proceedings of
the Conference on Autonomous Learning and
Discovery CONALD-98.
Paris, C. , K. Vander Linden. 1996.
DRAFTER: An interactive support tool for
writing multilingual instructions, IEEE
Computer, Special Issue on Interactive NLP.
Reiter, E. 1995. NLG vs. Templates.
Proceedings of the 5th European workshop on
natural language generation, (ENLG-95),
Leiden.
Reiter, E. , C. Mellish and J. Levine. 1995
Automatic generation of technical
documentation. Journal of Applied Artificial
Intelligence,  9(3) 259-287, 1995
Sanderson, M.  and B. Croft. 1999. Deriving
concept hierarchies from text. Proceedings of
the 22nd ACM SIGIR Conference, 206-213.
Scott, M. 1998. Focusing on the Text and Its
Key Words. TALC 98 Proceedings, Oxford,
Humanities Computing Unit, Oxford University.
Soderland, S. 1999. Learning information
extraction rules for semi-structured and free
text. Machine Learning, (1), 1-44.
Yangarber, R., R. Grishman, P. Tapanainen
and S. Huttunen. 2000. Automatic Acquisition
of Domain Knowledge for Information
Extraction. Proceedings of COLING 2000: The
18th International Conference on
Computational Linguistics, Saarbr?cken.
Multilingual Authoring: the NAMIC approach
R. Basili, M.T. Pazienza
F. Zanzotto
Dept. of Computer Science
University of Rome, Tor Vergata
Via di Tor Vergata,
00133 Roma
Italy
basili@info.uniroma2.it
pazienza@info.uniroma2.it
zanzotto@info.uniroma2.it
R. Catizone, A. Setzer
N. Webb, Y. Wilks
Department of Computer Science
University of Sheffield
Regent Court
211 Portobello Street,
Sheffield S1 4DP, UK
R.Catizone@dcs.shef.ac.uk
A.Setzer@dcs.shef.ac.uk
N.Webb@dcs.shef.ac.uk
Y.Wilks@dcs.shef.ac.uk
L. Padro?, G. Rigau
Dept. Llenguatges i Sistemes Informa`tics
Universitat Polite`cnica de Catalunya
Centre de Recerca TALP
Jordi Girona Salgado 1-3,
08034 Barcelona
Spain
padro@lsi.upc.es
g.rigau@lsi.upc.es
Abstract
With increasing amounts of elec-
tronic information available, and the
increase in the variety of languages
used to produce documents of the
same type, the problem of how to
manage similar documents in dif-
ferent languages arises. This pa-
per proposes an approach to process-
ing/structuring text so that Multi-
lingual Authoring (creating hyper-
text links) can be effectively car-
ried out. This work, funded by
the European Union, is applied to
the Multilingual Authoring of news
agency text. We have applied meth-
ods from Natural Language Process-
ing, especially Information Extrac-
tion technology, to both monolingual
and Multilingual Authoring.
1 Introduction
Modern Information Technologies are faced
with the problem of selecting, filtering and
managing growing amounts of multilingual
information to which access is usually criti-
cal. Traditional Information Retrieval (IR)
approaches are too general in their selection
of relevant documents where as traditional
Information Extraction (IE) (Gaizauskas and
Wilks, 1998; Pazienza, 1997) approaches are
too specific and inflexible. Automatic Au-
thoring is a good example of how these two
methods can be improved and used to cre-
ate a hypertextual organisation of (multilin-
gual) information. This kind of information
is ?added value? to the information embodied
in the text and is not in contrast with other
retrieval paradigms. Automatic Authoring
is the activity of processing news items in
streams, detecting and extracting relevant in-
formation from them and, accordingly, organ-
ising texts in a non-linear fashion.
While IE systems like the ones participat-
ing in the Message Understanding Conference
(MUC, 1998) are oriented towards specific
phenomena (e.g. joint ventures) in restricted
domains, the scope of Automatic Authoring
is wider. In Automatic Authoring, the hy-
pertextual structure has to provide naviga-
tion guidelines to the final user which can also
refuse the system suggestions.
In this paper an architecture for Automatic
Multilingual Authoring is presented based on
knowledge-intensive and large-scale Informa-
tion Extraction. The general architecture
is presented capitalising robust methods of
Information Extraction (Cunningham et al,
1999) and large-scale multilingual resources
(e.g. EuroWordNet). The system is de-
veloped within a European project in the
Human Language Technologies area, called
NAMIC (News Agencies Multilingual Infor-
mation Categorisation)1. It aims to extract
relevant facts from the news streams of large
European news agencies and newspaper pro-
ducers2, to provide hypertextual structures
within each (monolingual) stream and then
produce cross-lingual links between streams.
2 Authoring
2.1 Automatic Authoring
As Automatic Authoring is the task of au-
tomatically deriving a hypertextual structure
from a set of available news articles (in three
different languages English, Spanish and Ital-
ian in our case), the complexity of the overall
framework requires a suitable decomposition:
Text processing requires at least the de-
tection of morphosyntactic information char-
acterising the source texts: recognition, nor-
malisation, and assignment of roles is required
for the main participants for the different
events/facts described.
Event Matching is then the activity of
selecting the relevant facts of a news arti-
cle, in terms of their general type (e.g. sell-
ing or buying companies, winning a football
match), their participants and their related
roles (e.g. the company sold or the winning
football team).
Authoring is thus the activity of gener-
ating links between news articles according
to relationships established among facts de-
tected in the previous phase.
For instance, a company acquisition can be
referred to in one (or more) news items as:
? Intel, the world?s largest chipmaker,
bought a unit of Danish cable maker NKT
that designs high-speed computer chips ...
1See http://namic.itaca.it.
2EFE and ANSA, the major news agencies in Spain
and Italy respectively, and the Financial Times are all
members of the NAMIC consortium.
? The giant chip maker Intel said it ac-
quired the closely held ICP Vortex Com-
putersysteme, a German maker of sys-
tems ...
? Intel ha acquistato Xircom inc. per 748
milioni di dollari.
The hypothesis underlying Authoring is
that all the above news items deal with facts
in the same area of interest to a potential class
of readers. They should be thus linked and
links should suggest to the user that the un-
derlying motivation (used to decide whether
or not to follow an available link) is that they
all refer to Intel acquisitions.
Notice that a link generation process based
only upon words would fail in the above case
as the common word (that could play the role
of anchor in linking) is the proper noun Intel.
As no other information is available, the re-
sulting set of potential matches can be huge
and the connectivity too high.
In order to get the suitable links the equiv-
alence between the senses of bought and ac-
quired in the first two news items must be
known. Although such a relation can be
drawn by mechanisms like query expansion or
thesauri of synonyms (e.g. WordNet (Miller,
1990)), word polysemy and noise may re-
sult in an inherent proliferation of irrelevant
matches. Contextual information is critical
here. Notice that the senses of ?buy? and ?ac-
quire? are constrained by the role played by
Intel as ?agent ? and NKT or ICP Vortex be-
ing the sold companies. In fact, Intel buys
silicon represents an unwanted sense of the
verb and should be distinguished.
The relevant information concerning Intel
should be thus limited to:
? Intel buys a unit of NKT
? Intel acquires ICP Vortex.
These descriptions provide the core infor-
mation able to establish equivalence among
the underlying events. Whenever base event
descriptions are available the linking process
can be carried out via simpler equivalence in-
ferences. The Authoring problem is thus a
side effect of the overall language-processing
task.
According to the suggested decomposition
all the above steps are mandatory. First text
processing is responsible for morpho-syntactic
recognition. Morphological units and syntac-
tic relations are produced for each sentence at
this stage. However, syntactic relations (e.g.
among subjects and verbs) are not sufficient
for proper event characterisation. In the ex-
ample(s), the subject of the verb acquire is
a pronoun only anaphorically referring to In-
tel. Co-reference resolution is usually applied
to this kind of mismatch at the surface level.
This capability is under the responsibility of
the event matching phase. Moreover, in or-
der to keep track of events over syntactic rep-
resentations, references to a target ontology
are required. In such an ontology, equiva-
lence among facts (e.g. buying companies) is
represented. For instance, the relation among
buy and acquire can be encoded under a more
general notion of financial acquisition. On-
tologies also define the set of relevant facts of
the target domain. A financial acquisition is
a perfect example of what is needed in cor-
porate industrial news but is less important,
for example, in sports news, where hiring of
players seems a more relevant event class.
Conceptual differences among facts (de-
tected during event matching) motivate a se-
lective notion of hyperlinking. These links
can be thus generated during the automatic
authoring phase. They are ontologically jus-
tified as their conceptual representation is al-
ready available at this stage. Types as same
acquisition fact, same person, or company can
be used to distinguish links and make expla-
nations available to the user.
2.2 Multilingual Automatic
Authoring
?From a multilingual perspective, the prob-
lem is to establish links among news in dif-
ferent languages. Full-text approaches can
rely only on language independent phenom-
ena (e.g. proper nouns like Intel) that are
very limited in texts. Most of the above-
mentioned inferences require language neu-
tral information (i.e. conceptual and not lexi-
cal constraints). The inherent overgeneration
related to word polysemy affects the results
of translation-based approaches. Again prin-
cipled representations made available by IE
processes (i.e. templates) provide a viable
solution. The different event realisations (in
the different languages) can be handled dur-
ing the overall event matching. A lexical in-
terface to the ontology is able to factor the
language specific information. As syntactic
differences are handled during text process-
ing, the result is a common domain model for
IE plus independent lexical interfaces. The
unified representation of the set of facts ac-
tivates multilingual linking at a conceptual
level, thus making the Authoring a language
independent process. Some challenges of such
a framework are:
? the size of the ontological resources re-
quired in terms of taxonomic (i.e. IS A
relations) and conceptual information
(i.e. classes of events and implied
participant-event relations)
? the size of the lexical interfaces to the
ontology available for the different lan-
guages
? the amount of task dependent knowledge.
For example the definition of the set of
events useful for the target application is
underspecified.
In the following, we propose a complex ar-
chitecture where the above problems are
approached according to well-assessed tech-
niques presented elsewhere. Robust Informa-
tion Extraction is adopted (Humphreys et al,
1998) as an overall method for text process-
ing and event matching. Target events are
semiautomatically derived from domain texts
and represented in the IE engine ontology. Fi-
nally, multilinguality is realised by assuming a
large-scale multilingual lexical hierarchy as a
reference ontology for nominal concepts. The
resulting architecture for Multilingual Auto-
matic Authoring is presented in Section 3.4.
3 The NAMIC system
3.1 Large scale IE for Automatic
Authoring
Information Extraction is a very good ap-
proach to Automatic Authoring for a num-
ber of reasons. The key components of an IE
system are events and objects - the kind of
components that trigger hyperlinks in an Au-
thoring system. Coreference is a significant
part of Information Extraction and indeed a
necessary component in Authoring. Named
Entities - people, places, and organisations,
etc. - play an important part in Authoring
and again are firmly addressed in Information
Extraction systems.
The role of a world model as a method
for event matching and coreferencing
The world model is an ontological represen-
tation of events and objects for a particular
domain or set of domains. The world model
is made up of a set of event and object types,
with attributes. The event types characterise
a set of events in a particular domain and
are usually represented in a text by verbs.
Object Types on the other hand, are best
thought of as characterising a set of people,
places or things and are usually represented
in a text by nouns (both proper and com-
mon). When used as part of an Information
Extraction system, the instances of each type
are inserted/added to the world model. Once
the instances have been added, a procedure
is carried out to link those instances that re-
fer to the same thing - achieving coreference
resolution.
In NAMIC, the world model is created
using the XI cross-classification hierarchy
(Gaizauskas and Humphreys, 1996). The def-
inition of a XI cross-classification hierarchy is
referred to as an ontology, and this together
with an association of attributes with nodes
in the ontology forms the world model. Pro-
cessing a text acts to populate this initially
bare world model with the various instances
and relations mentioned in the text, convert-
ing it into a discourse model specific to the
particular text.
The attributes associated with nodes in
the ontology are simple attribute:value pairs
where the value may either be fixed, as in
the attribute animate:yes which is associ-
ated with the person node, or where the value
may be dependent on various conditions, the
evaluation of which makes reference to other
information in the model.
3.1.1 The Description of LaSIE
LaSIE is a Large-scale Information Ex-
traction system, developed for MUC (Mes-
sage Understanding Conference) competi-
tions, comprised of a variety of modules, see
(Humphreys et al, 1998; MUC, 1998). Al-
though we are not using the complete LaSIE
system in NAMIC, we are using 2 of the key
modules - the Named Entity Matcher and the
Discourse Processor. Below is a description of
each of these modules.
Named Entity Matcher The Named En-
tity Matcher finds named entities through
a secondary phase of parsing which uses a
named entity grammar and a set of gazetteer
lists. It takes as input parsed text from the
first phase of parsing and the named entity
grammar which contains rules for finding a
predefined set of named entities and a set of
gazetteer lists containing proper nouns. The
Name Entity Matcher returns the text with
the Named Entities marked. The Named En-
tities in NAMIC are PERSONS, ORGANI-
SATIONS, LOCATIONS, and DATES. The
Named Entity grammar contains rules for
coreferring abbreviations as well as different
ways of expressing the same named entity
such as Dr. Smith, John Smith and Mr.
Smith occurring in the same article.
Discourse Processor The Discourse Pro-
cessor module translates the semantic rep-
resentation produced by the parser into a
representation of instances, their ontolog-
ical classes and their attributes, in the
XI knowledge representation language (see
Gaizauskas(1996)). XI allows a straightfor-
ward definition of cross-classification hierar-
chies, the association of arbitrary attributes
with classes or instances, and a simple mech-
anism to inherit attributes from classes or in-
stances higher in the hierarchy.
The semantic representation produced by
the parser for a single sentence is processed
by adding its instances, together with their
attributes, to the discourse model which has
been constructed so far for the text.
Following the addition of the instances
mentioned in the current sentence, together
with any presuppositions that they inherit,
the coreference algorithm is applied to at-
tempt to resolve, or in fact merge, each of
the newly added instances with instances cur-
rently in the discourse model.
The merging of instances involves the re-
moval of the least specific instance (i.e. the
highest in the ontology) and the addition of
all its attributes to the other instance. This
results in a single instance with more than one
realisation attribute, which corresponds to a
single entity mentioned more than once in the
text, i.e. a coreference.
3.2 Ontological Modeling
As we have seen in section 3.1, some critical
issues of the NAMIC project rely on the per-
formance of the lexical and conceptual compo-
nents of all linguistic processors. As NAMIC
faces large-scale coverage of news in several
languages we decided to adopt EuroWordNet
(Vossen, 1998) as a common semantic formal-
ism to support:
? lexical semantic inferences (e.g. general-
isation, disambiguation)
? broad coverage (e.g. lexical and semanti-
cal) and
? a common interlingual platform for link-
ing events from different documents.
The NAMIC ontology consists of 40 prede-
fined object classes and 46 attribute types re-
lated to Name Entity objects and nearly 1000
objects relating to EuroWordNet base con-
cepts.
3.2.1 EuroWordNet as a Multilingual
Lexical Knowledge Base
Since the world model aims to describe the
language used in a given domain via events
and objects, the accuracy and breadth of the
model will impact how well the information
extraction works.
EuroWordNet (Vossen, 1998) is a multilin-
gual lexical knowledge base (LKB) with word-
nets for several European languages (Dutch,
Italian, Spanish, German, French, Czech and
Estonian). The wordnets are structured
in the same way as the American wordnet
for English developed at Princeton (Miller,
1990) containing synsets (sets of synonymous
words) with basic semantic relations between
them.
Each wordnet represents a unique
language-internal system of lexicalisa-
tions. In addition, the wordnets are linked
to an Inter-Lingual-Index (ILI), based on
the Princeton WordNet 1.5. WordNet 1.6 is
also connected to the ILI as another English
WordNet (Daude et al, 2000). Via this
index, the languages are interconnected so
that it is possible to go from the words in
one language to words in any other language
having similar meaning. The index also
gives access to a shared top-ontology and
a subset of 1024 Base Concepts (BC). The
Base Concepts provide a common seman-
tic framework for all the languages, while
language specific properties are maintained
in the individual wordnets. The LKB can
be used, among others, for monolingual and
cross-lingual information retrieval, which
has been demonstrated in other projects
(Gonzalo et al, 1998).
3.3 Multilingual Event description
The traditional limitations of a knowledge-
based information extraction system such as
LaSIE have been the need to hand-code in-
formation for the world model - specifically
relating to the event structure of the domain.
For the NAMIC project, we have decided
to semi-automate the process of adding new
?event descriptions? to the World Model. To
us, event descriptions can be categorised as a
set of regularly occurring verbs within our do-
main, complete with their subcategorisation
information.
These verbs can be extracted with simple
statistical techniques and are, for the moment
subjected to hand pruning. Once a list of
verbs has been extracted, subcategorisation
patterns can be generated automatically using
a Galois lattice (as described in (Basili et al,
2000b)). These frames can then be uploaded
into the event hierarchy of the discourse in-
terpreter world model.
The world model can have a structure
which is essentially language independent in
all but the lowest level - at which stage lexi-
calisations relating to each representative lan-
guage are required. Associated with these lex-
icalisations are language dependent scenario
rules which control the behaviour of instances
of these events with a Discourse Model. These
rules are expected to differ across languages in
the way they control coreference for languages
which are constrained to lesser or greater de-
gree.
The lattice generates patterns which refer
to synsets in the WordNet hierarchy. For
our purposes, we will use patterns referring to
Base Concepts in the EuroWordNet hierarchy
- which allows us to exploit the Inter-Lingual-
Index as described in the previous section.
These Base Concepts serve as a level of mul-
tilingual abstraction for the conceptual con-
straints of our events, and allow us to extend
the number of semantic classes from seven
(the MUC Named Entity classifications) to
1024 - the number of base concepts in EWN.
3.4 The NAMIC Architecture
The complexity of the overall NAMIC sys-
tem required the adoption of a distributed
computing paradigm in the design. The sys-
tem is a distributed object oriented system
where services (like text processing or Multi-
lingual Authoring) are provided by indepen-
dent components and asynchronous communi-
cation is allowed. Independent news streams
for the different languages (English, Spanish,
and Italian) are assumed. Language specific
processors (LPs) are thus responsible for text
processing and event matching in indepen-
dent text units in each stream. LPs com-
pile an objective representation (see Fig. 1)
for each source texts, including the detected
morphosyntactic information, categorisation
in news standards (IPTC classes) and descrip-
tion of the relevant events. Any later Au-
thoring activity is based on this canonical
representation of the news. In particular a
monolingual process is carried out within any
stream by the three monolingual Authoring
Engines (English AE, Spanish AE, and Ital-
ian AE). A second phase is foreseen to take
into account links across streams, i.e. multi-
lingual hyper-linking: a Multilingual Author-
ing Engine (M-AE) is here foreseen. Figure
1 represents the overall flow of information.
The Language Processors are composed of a
morphosyntactic (Eng, Ita and Spa MS) and
an event-matching component (EM). The lex-
ical interfaces (ELI, SLI and ItLI) to the uni-
fied Domain model are also used during event
matching.
The linguistic processors are in charge of
producing the objective representation of in-
coming news. This task is performed during
MS analysis by two main subprocessors:
? a modular and lexicalised shallow
morpho-syntactic parser (Basili et al,
2000c), providing name entity match-
ing and extracting dependency graphs
from source sentences. Ambiguity is
controlled by part-of-speech tagging and
domain verb-subcategorisation frames
that guide the dependency recognition
phase.
? a statistical linear text classifier based
upon some of the derived linguistic fea-
tures (Basili et al, 2000a) (lemmas, POS
tags and proper nouns)
The results are then input to the event
matcher that by means of the discourse in-
terpreter (Humphreys et al, 1998) derive the
objective representation. As discussed in sec-
tion 3.1, coreferencing is a side effect of the
discourse interpretation (Humphreys et al,
1998). It is based on the multilingual domain
model where relevant events are described and
nominal concepts represented.
The overall architecture is highly modular
and open to load balancing activity as well as
to adaptation and porting. The communica-
tion interfaces among the MS and EM com-
ponents as well as among the AEs and the M-
AE processors are specified via XML DTDs.
This allows for user-friendly uploading of a
back-end database with the detected material
as well as the easy design and management of
the front-end databases (available for tempo-
rary tasks, like event matching after MS). All
the servers are objects in a distributed archi-
tecture within a CORBA environment. The
current version includes the linguistic proces-
sors (MS and EM) for all the three languages.
The English and Italian linguistic processors
are fully object oriented modules based on
EnglishMS
SpanishMS
ItalianMS
EnglishAE
SpanishAE
ItalianAE
news ObjectiveRepresentation Monolingual Links
Multilingual Links
EnglishEM
SpanishEM
ItalianEM
DomainModel
ELI
SLI
ItLI
Multi-LingualAuthoringEngine
Language Processors
Figure 1: Namic Architecture
Java. They integrate libraries written in C,
C++, Prolog, and Perl for specific functional-
ities (e.g. parsing) running under a Windows
NT platform. The Spanish linguistic proces-
sor shares the discourse interpreter and the
text classifier with the other modules, while
the morpho syntactic component is currently
a Unix server based on Perl. The use of a dis-
tributed architecture under CORBA allowed
a flexible solution to its integration into the
overall architecture. The servers can be in-
stantiated in multiple copies throughout the
network if the amount of required computa-
tion exceeds the capability of a current con-
figuration. As the workload of a news stream
is not easily predictable, distribution and dy-
namic load balancing is the only realistic ap-
proach.
4 Discussion and Future Work
The above sections have provided the out-
line of a general NLP-based approach to auto-
matic authoring. The emphasis given to tra-
ditional capabilities of Information Extraction
depends on the relevance of news content in
the target Web service scenarios as well as
on their inherent multilinguality. The bet-
ter is the generalisation provided by the IE
component, the higher is the independence
from the text source language. As a result,
IE is here seen as a natural approach to cross-
lingual hypertextual authoring. Other works
in this area make extensive use of traditional
IR techniques (e.g. full text search) or rely
on already traced (i.e. manually coded) hy-
perlinks (e.g. (Chakrabarti et al, 1998; Klein-
berg, 1999)). The suggested NAMIC architec-
ture exploits linguistic capabilities for deriv-
ing entirely original (ex novo) resources, over
dynamic, previously unreleased, streams of in-
formation.
The result is a large-scale multilingual NLP
application capitalising existing methods and
resources within an advanced software engi-
neering process. The use of a distributed
Java/CORBA architecture makes the system
very attractive for its scalability and adaptiv-
ity. It results in a very complex (but realis-
tic) NLP architecture. Its organisation (lexi-
cal interfaces with respect to the multilingual
ontology) makes it very well suited for cus-
tomisation and porting to large domains. Al-
though the current version is a prototype, it
realises the complete set of core functionali-
ties, including the main IE steps and the dis-
tributed Java/CORBA layer.
It is worth noticing that a set of extensions
are made viable within the proposed architec-
ture. A first line is the extension of the avail-
able multilingual lexical knowledge. The Dis-
course Model can be used to better reflect on-
tological relationships within a particular do-
main. These relationships could be examined
to confirm known word sense usage as well
as to postulate/propose novel word sense us-
age. Using the mechanism for the addition of
events (as categorised by verbs) to the world
model, users can specify new events which can
be added to the IE system, to achieve User
Driven IE, and deliver a form of adaptive in-
formation extraction.
The instantiated domain models can be
thus used as a basis for ontological resource
expansion as a form of adaptive process.
For example, the stored instantiations of dis-
course models within a specific domain can be
compared: it may be thus possible to recog-
nise new sets of events or objects which are
not currently utilised within the system.
The evaluation strategy that is made possi-
ble within the NAMIC consortium will make
use of the current users (i.e. news agencies)
expertise. The agreed evaluation methods
will provide evidence about the viability of
the proposed large-scale IE-based approach to
authoring, as a valuable paradigm for infor-
mation access.
Acknowledgements
This research is funded by the European
Union, grant number IST-1999-12392. We
would also like to thank all of the partners
in the NAMIC consortium.
References
R. Basili, A. Moschitti, and M.T. Pazienza. 2000a.
Language sensitive text classification. In In
proceeding of 6th RIAO Conference (RIAO
2000), Content-Based Multimedia Information
Access, Coll ge de France, Paris, France.
R. Basili, M.T. Pazienza, and M. Vindigni. 2000b.
Corpus-driven learning of event recognition
rules. In Proc. of Machine Learning for Infor-
mation Extraction workshop, held jointly with
the ECAI2000, Berlin, Germany.
R. Basili, M.T. Pazienza, and F.M. Zanzotto.
2000c. Customizable modular lexicalized pars-
ing. In Proc. of the 6th International Workshop
on Parsing Technology, IWPT2000, Trento,
Italy.
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg,
P. Raghavan, and S. Rajagopalan. 1998. Auto-
matic resource compilation by analysing hyper-
link structure and associated text. In Proceed-
ings of the 7th International World Wide Web
Conference, Brisbane, Australia.
C. Cunningham, R. Gaizauskas, K. Humphreys,
and Y. Wilks. 1999. Experience with a lan-
guage engineering architecture: 3 years of gate.
In Proceedings of the AISB?99 Workshop on
Reference Architectures and Data Standards for
NLP, Edinburgh, UK.
J. Daude, L. Padro, and G. Rigau. 2000. Map-
ping wordnets using structural information.
In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics
ACL?00, Hong Kong, China.
R. Gaizauskas and K. Humphreys. 1996. Xi:
A simple prolog-based language for cross-
classification and inheritance. In Proceedings of
the 6th International Conference on Artificial
Intelligence: Methodologies, Systems, Applica-
tions (AIMSA96), pages 86?95.
R. Gaizauskas and Y. Wilks. 1998. Information
Extraction: Beyond Document Retrieval. Jour-
nal of Documentation, 54(1):70?105.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigar-
ran. 1998. Indexing with wordnet synsets
can improve text retrieval. In Proceedings of
the COLING/ACL?98 Workshop on Usage of
WordNet for NLP, Montreal, Canada.
K. Humphreys, R. Gaizauskas, S. Azzam,
C. Huyck, B. Mitchell, H. Cunningham, and
Y. Wilks. 1998. University of sheffield: De-
scription of the lasie-ii system as used for muc-7.
In Proceedings of the Seventh Message Under-
standing Conferences (MUC-7). Morgan Kauf-
man. Available at http://www.saic.com.
Jon M. Kleinberg. 1999. Authoritative sources
in a hyperlinked environment. Journal of the
ACM, 46(5):604?632.
G. Miller. 1990. Five papers on wordnet. Inter-
national Journal of Lexicography, 4(3).
1998. Proceedings of the Seventh Message Under-
standing Conference (MUC-7). Morgan Kauf-
man. Available at http://www.saic.com.
M.T. Pazienza, editor. 1997. Information Ex-
traction. A Multidisciplinary Approach to an
Emerging Information Technology. Number
1299 in LNAI. Springer-Verlag, Heidelberg,
Germany.
P. Vossen. 1998. EuroWordNet: A Multilin-
gual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht.
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 17?22,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Dating of Documents and Temporal Text Classification 
Angelo Dalli 
NLP Research Group 
University of Sheffield 
United Kingdom 
angelo@dcs.shef.ac.uk 
Yorick Wilks 
NLP Research Group 
University of Sheffield 
United Kingdom 
yorick@dcs.shef.ac.uk 
 
  
 
Abstract 
The frequency of occurrence of words in 
natural languages exhibits a periodic and 
a non-periodic component when analysed 
as a time series. This work presents an 
unsupervised method of extracting perio-
dicity information from text, enabling 
time series creation and filtering to be 
used in the creation of sophisticated lan-
guage models that can discern between 
repetitive trends and non-repetitive writ-
ing patterns. The algorithm performs in 
O(n log n) time for input of length n. The 
temporal language model is used to cre-
ate rules based on temporal-word asso-
ciations inferred from the time series. 
The rules are used to guess automatically 
at likely document creation dates, based 
on the assumption that natural languages 
have unique signatures of changing word 
distributions over time. Experimental re-
sults on news items spanning a nine year 
period show that the proposed method 
and algorithms are accurate in discover-
ing periodicity patterns and in dating 
documents automatically solely from 
their content. 
1 Introduction 
Various features have been used to classify 
and predict the characteristics of text and related 
text documents, ranging from simple word count 
models to sophisticated clustering and Bayesian 
models that can handle both linear and non-linear 
classes. The general goal of most classification 
research is to assign objects from a pre-defined 
domain (such as words or entire documents) to 
two or more classes/categories. Current and past 
research has largely focused on solving problems 
like tagging, sense disambiguation, sentiment 
classification, author and language identification 
and topic classification. In this paper, we intro-
duce an unsupervised method that classifies text 
and documents according to their predicted time 
of writing/creation. The method uses a sophisti-
cated temporal language model to predict likely 
creation dates for a document, hence dating it 
automatically. 
This paper presents the main assumption be-
hind this work together some background infor-
mation about existing techniques and the imple-
mented system, followed by a brief explanation 
of the classification and dating method, and fi-
nally concluding with results and evaluation per-
formed on the LDC GigaWord English Corpus 
(LDC, 2003) together with its implications and 
relevance to temporal-analytical frameworks and 
TimeML applications. 
2 Background and Assumptions 
The main assumption behind this work is that 
natural language exhibits a unique signature of 
varying word frequencies over time. New words 
come into popular use continually, while other 
words fall into disuse either after a brief fad or 
when they become obsolete or archaic. Current 
events, popular issues and topics also affect writ-
ers in their choice of words and so does the time 
period when they create documents. This as-
sumption is implicitly made when people try to 
guess at the creation date of a document ? we 
would expect a document written in Shake-
speare?s time to contain higher frequency counts 
of words and phrases such as ?thou art?, ?be-
twixt?, ?fain?, ?methinks?, ?vouchsafe? and so 
on than would a modern 21st century document. 
Similarly, a document that contains a high fre-
quency of occurrence of the words ?terrorism?, 
?Al Qaeda?, ?World Trade Center?, and so on is 
more likely to be written after 11 September 
2001. New words can also be used to create ab-
solute constraints on the creation dates of docu-
ments, for example, it is highly improbable that a 
17
document containing the word ?blog? was writ-
ten before July 1999 (it was first used in a news-
group in July 1999 as an abbreviation for ?we-
blog?), or a document containing the word 
?Google? to have been written before 1997. 
Words that are now in common use can also be 
used to impose constraints on the creation date; 
for example, the word ?bedazzled? has been at-
tributed to Shakespeare, thus allowing docu-
ments from his time onwards to be identifiable 
automatically. Traditional dictionaries often try 
to record the date of appearance of new words in 
the language and there are various Internet sites, 
such as WordSpy.com, devoted to chronicling 
the appearance of new words and their meanings. 
Our system is building up a knowledge base of 
the first occurrences of various words in different 
languages, enabling more accurate constraints to 
be imposed on the likely document creation date 
automatically. 
Commercial trademarks and company names 
are also useful in dating documents, as their reg-
istration date is usually available in public regis-
tries. Temporal information extracted from the 
documents itself is also useful in dating the docu-
ments ? for example, if a document contains 
many references to the year 2006, it is quite 
likely that the document was written in 2006 (or 
in the last few weeks of December 2005). 
These notions have been used implicitly by re-
searchers and historians when validating the au-
thenticity of documents, but have not been util-
ised much in automated systems. Similar appli-
cations have so far been largely confined to au-
thorship identification, such as (Mosteller and 
Wallace, 1964; Fung, 2003) and the identifica-
tion of association rules (Yarowsky, 1994; 
Silverstein et al, 1997). 
Temporal information is presently under-
utilised for automated document classification 
purposes, especially when it comes to guessing at 
the document creation date automatically. This 
work presents a method of using periodical tem-
poral-frequency information present in docu-
ments to create temporal-association rules that 
can be used for automatic document dating. 
Past and ongoing related research work has 
largely focused on the identification and tagging 
of temporal expressions, with the creation of tag-
ging methodologies such as TimeML/TIMEX 
(Gaizauskas and Setzer, 2002; Pustejovsky et al, 
2003; Ferro et al, 2004), TDRL (Aramburu and 
Berlanga, 1998) and their associated evaluations 
such as the ACE TERN competition (Sundheim 
et al 2004). 
Temporal analysis has also been applied in 
Question-Answering systems (Pustejovsky et al, 
2004; Schilder and Habel, 2003; Prager et al, 
2003), email classification (Kiritchenko et al, 
2004), aiding the precision of Information Re-
trieval results (Berlanga et al, 2001), document 
summarisation (Mani and Wilson, 2000), time 
stamping of event clauses (Filatova and Hovy, 
2001), temporal ordering of events (Mani et al, 
2003) and temporal reasoning from text (Bogu-
raev and Ando, 2005; Moldovan et al, 2005). 
A growing body of related work related to the 
computational treatment of time in language has 
also been building up largely since 2000 (COL-
ING 2000; ACL 2001; LREC 2002; TERQAS 
2002; TANGO 2003, Dagstuhl 2005). 
There is also a large body of work on time se-
ries analysis and temporal logic in Physics, Eco-
nomics and Mathematics, providing important 
techniques and general background information. 
In particular, this work uses techniques adapted 
from Seasonal ARIMA (auto-regressive inte-
grated moving average) models (SARIMA). 
SARIMA models are a class of seasonal, non-
stationary temporal models based on the ARIMA 
process. The ARIMA process is further defined 
as a non-stationary extension of the stationary 
ARMA model. The ARMA model is one of the 
most widely used models when analyzing time 
series, especially in Physics, and incorporate 
both auto-regressive terms and moving average 
terms (Box and Jenkins, 1976). Non-stationary 
ARIMA processes are defined by the following 
equation: 
( ) ( ) ( ) ttd ZBXBB ?? =?1            (1) 
where d is non-negative integer, and ( )X?  
( )X?  polynomials of degrees p and q respec-
tively. The SARIMA extension adds seasonal 
AR and MA polynomials that can handle season-
ally varying data in time series. 
The exact formulation of the SARIMA model 
is beyond the scope of this paper and can be 
found in various mathematics and physics publi-
cations, such as (Chatfield, 2003; Brockwell et 
al., 1991; Janacek, 2001). 
The main drawback of SARIMA modelling 
(and associated models built on the basic ARMA 
model) is that it requires fairly long time series 
before accurate results are obtained. The major-
ity of authors recommend that a time series of at 
least 50 data points is used to build the SARIMA 
model. 
18
0100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
0
100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
 
Time Series for ?January? 
Original (Top Left), Non-Periodic Component (Top 
Right), Periodic Component (Bottom Right) 
0
100
200
300
400
500
600
23
7 46 24 12 17 10 19 30
7 22 3 16 18 13 35 33 31 14 17 5 6
 
  
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
1 164 327 490 653 816 979 1142 1305 1468 1631 1794 1957
 
0
500
1000
1500
2000
2500
3000
3500
4000
1 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081
 
 
Time Series for ?The? 
Original (Top Left), Non-Periodic Component (Top 
Right), Periodic Component (Bottom Right) 
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
1 161 321 481 641 801 961 1121 1281 1441 1601 1761 1921 2081
 
 
Figure 1: Effects of applying the temporal periodical algorithm on time series for "January" (top three 
graphs) and "the" (bottom three graphs) with the original series on the left and the remaining time series 
components after filtering on the right. Y-axis shows frequency count and X-axis shows the day number 
(time). 
 
3 Temporal Periodicity Analysis 
We have created a high-performance system that 
decomposes time series into two parts: a periodic 
component that repeats itself in a predictable 
manner, and a non-periodic component that is 
left after the periodic component has been fil-
tered out from the original time series. Figure 1 
shows an example of the filtering results on time-
series of the words ?January? and ?the?. The 
original series is presented together with two se-
ries representing the periodic and non-periodic 
19
components of the original time series. The time 
series are based on training documents selected 
at random from the GigaWord English corpus. 
10% of all the documents in the corpus were 
used as training documents, with the rest being 
available for evaluation and testing. A total of 
395,944 time series spanning 9 years were calcu-
lated from the GigaWord corpus. The availability 
of 9 years of data also mitigated the negative ef-
fects of using short time series in combination 
with SARIMA models (as up to 3,287 data 
points were available for some words, well above 
the 50 data point minimum recommendation). 
Figure 2 presents pseudo-code for the time series 
decomposition algorithm: 
 
1. Find min/max/mean and standard devia-
tion of time series 
2. Start with a pre-defined maximum win-
dow size (set to 366 days in our pre-
sent system) 
3. While window size bigger than 1 re-
peat steps a. to d. below: 
a. Look at current value in time 
series (starting from first 
value) 
b. Do values at positions cur-
rent, current + window size, 
current + 2 x window size, 
etc. vary by less than half a 
standard deviation? 
c. If yes, mark current 
value/window size pair as be-
ing possible decomposition 
match 
d. Look at next value in time se-
ries until the end is reached 
e. Decrease window size by one 
4. Select the minimum number of decompo-
sition matches that cover the entire 
time series using a greedy algorithm 
 
Figure 2: Time Series Decomposition Algorithm 
 
The time series decomposition algorithm was 
applied to the 395,944 time series, taking an av-
erage of 419ms per series. The algorithm runs in 
O(n log n) time for a time series of length n. 
The periodic component of the time series is 
then analysed to extract temporal association 
rules between words and different ?seasons?, 
including Day of Week, Week Number, Month 
Number, Quarter, and Year. The procedure of 
determining if a word, for example, is predomi-
nantly peaking on a weekly basis, is to apply a 
sliding window of size 7 (in the case of weekly 
periods) and determining if the periodic time se-
ries always spikes within this window. Figure 3 
shows the frequency distribution of the periodic 
time series component of the days of week 
names (?Monday?, ?Tuesday?, etc.) Note that the 
frequency counts peak exactly on that particular 
day of the week. Thus, for example, the word 
?Monday? is automatically associated with Day 
1, and ?April? associated with Month 4. 
The creation of temporal association rules 
generalises the inferences obtained from the pe-
riodic data. Each association rule has the follow-
ing information: 
 
? Word ID 
? Period Type (Week, Month, etc.) 
? Period Number and Score Matrix 
 
The period number and score matrix represent 
a probability density function that shows the 
likelihood of a word appearing on a particular 
period number. Thus, for example, the score ma-
trix for ?January? will have a high score for pe-
riod 1 (and period type set to Monthly). Figure 4 
shows some examples of extracted association 
rules. The probability density function (PDF) 
scores are shown in Figure 4 as they are stored 
internally (as multiples of the standard deviation 
of that time series) and are automatically normal-
ised during the classification process at runtime. 
The standard deviation of values in the time se-
ries is used instead of absolute values in order to 
reduce the variance between fluctuations in dif-
ferent time series for words that occur frequently 
(like pronouns) and those that appear relatively 
less frequently. 
Rule generalisation is not possible in such a 
straightforward manner for the non-periodic data. 
In this paper, the use of non-periodic data to op-
timise the results of the temporal classification 
and automatic dating system is not covered. Non-
periodic data may be used to generate specific 
rules that are associated only with particular 
dates or date ranges. Non-periodic data can also 
use information obtained from hapax words and 
other low-frequency words to generate additional 
refinement rules. However, there is a danger that 
relying on rules extracted from non-periodic data 
will simply reflect the specific characteristics of 
the corpus used to train the system, rather than 
the language in general. Ongoing research is be-
ing performed into calculating relevance levels 
for rules extracted from non-periodic data. 
4 Temporal Classification and Auto-
matic Dating 
The periodic temporal association rules are util-
ised to guess automatically the creation date of 
20
documents. Documents are input into the system 
and the probability density functions for each 
word are weighted and added up. Each PDF is 
weighted according to the inverse document fre-
quency (idf) of each associated word. Periods 
that obtain high score are then ranked for each 
type of period and two guesses per period type 
are obtained for each document. Ten guesses in 
total are thus obtained for Day of Week, Week 
Number, Month Number, Quarter, and Year (5 
period types x 2 guesses each). 
 
 
Su M T W Th F S 
0 22660 10540 7557 772 2130 3264 11672 
1 12461 37522 10335 6599 1649 3222 3414 
2 3394 18289 38320 9352 7300 2543 2261 
3 2668 4119 18120 36933 10427 5762 2147 
4 2052 2602 3910 17492 36094 9098 5667 
5 5742 1889 2481 2568 17002 32597 7849 
6 7994 7072 1924 1428 3050 14087 21468 
 
       
Av 8138 11719 11806 10734 11093 10081 7782 
St 7357 12711 12974 12933 12308 10746 6930 
 
Figure 3: Days of Week Temporal Frequency Dis-
tribution for extracted Periodic Component 
displayed in a Weekly Period Type format 
 
January 
Week 1 2 3 4 5 
Score 1.48 2.20 3.60 3.43 3.52 
 
Month 1 Score 2.95 
Quarter 1 Score 1.50 
 
Christmas 
Week 2 5 36 42 44 
Score 1.32 0.73 1.60 0.83 1.32 
Week 47 49 50 51 52 
Score 1.32 2.20 2.52 2.13 1.16 
 
Month 1 9 10 11 12 
Score 1.10 0.75 1.63 1.73 1.98 
 
Quarter 4 Score 1.07 
 
Figure 4: Temporal Classification Rules for Peri-
odic Components of "January" and "Christmas" 
4.1 TimeML Output 
The system can output TimeML compliant 
markup tags using TIMEX that can be used by 
other TimeML compliant applications especially 
during temporal normalization processes. If the 
base anchor reference date for a document is un-
known, and a document contains relative tempo-
ral references exclusively, our system output can 
provide a baseline date that can be used to nor-
malize all the relative dates mentioned in the 
document. The system has been integrated with a 
fine-grained temporal analysis system based on 
TimeML, with promising results, especially 
when processing documents obtained from the 
Internet. 
5 Evaluation, Results and Conclusion 
The system was trained using 67,000 news items 
selected at random from the GigaWord corpus. 
The evaluation took place on 678,924 news items 
extracted from items marked as being of type 
?story? or ?multi? in the GigaWord corpus. Ta-
ble 1 presents a summary of the evaluation re-
sults. Processing took around 2.33ms per item. 
The actual date was extracted from each news 
item in the GigaWord corpus and the day of 
week (DOW), week number and quarter calcu-
lated from the actual date. 
This information was then used to evaluate the 
system performance automatically. The average 
error for each type of classifier was also calcu-
lated automatically. For a result to be considered 
as correct, the system had to have the predicted 
value ranked in the first position equal to the ac-
tual value (of the type of period). 
 
Type Correct Incorrect Avg. 
Error 
DOW 218,899 
(32.24%) 
 460,025 
(67.75%) 
1.89 
days 
Week 24,660 
(3.53%) 
654,264 
(96.36%) 
14.37 
wks 
Month 122,777 
(18.08%) 
556,147 
(81.91%) 
2.57 
mths 
Quarter 337,384 
(49.69%) 
341,540 
(50.30%) 
1.48 
qts 
Year 596,009  
(87.78%) 
82,915 
(12.21%)  
1.74 
yrs 
 
Table 1: Evaluation Results Summary 
 
The system results show that reasonable accurate 
dates can be guessed at the quarterly and yearly 
levels. The weekly classifier had the worst per-
formance of all classifiers, likely as a result of 
weak association between periodical word fre-
quencies and week numbers. Logical/sanity 
checks can be performed on ambiguous results. 
For example, consider a document written on 4 
January 2006 and that the periodical classifiers 
give the following results for this particular 
document: 
? DOW = Wednesday 
? Week = 52 
? Month = January 
21
? Quarter = 1 
? Year = 2006 
These results are typical of the system, as par-
ticular classifiers sometimes get the period incor-
rect. In this example, the weekly classifier incor-
rectly classified the document as pertaining to 
week 52 (at the end of the year) instead of the 
beginning of the year. The system will use the 
facts that the monthly and quarterly classifiers 
agree together with the fact that week 1 follows 
week 52 if seen as a continuous cycle of weeks 
to correctly classify the document as being cre-
ated on a Wednesday in January 2006. 
The capability to automatically date texts and 
documents solely from its contents (without any 
additional external clues or hints) is undoubtedly 
useful in various contexts, such as the forensic 
analysis of undated instant messages or emails 
(where the Day of Week classifier can be used to 
create partial orderings), and in authorship iden-
tification studies (where the Year classifier can 
be used to check that the text pertains to an ac-
ceptable range of years). 
The temporal classification and analysis sys-
tem presented in this paper can handle any Indo-
European language in its present form. Further 
work is being carried out to extend the system to 
Chinese and Arabic. Evaluations will be carried 
out on the GigaWord Chinese and GigaWord 
Arabic corpora for consistency. Current research 
is aiming at improving the accuracy of the classi-
fier by using the non-periodic components and 
integrating a combined classification method 
with other systems.  
References 
Aramburu, M. Berlanga, R. 1998. A Retrieval Lan-
guage for Historical Documents. Springer Verlag 
LNCS, 1460, pp. 216-225. 
Berlanga, R. Perez, J. Aramburu, M. Llido, D. 2001. 
Techniques and Tools for the Temporal Analysis of 
Retrieved Information. Springer Verlag LNCS, 
2113, pp. 72-81. 
Boguraev, B. Ando, R.K. 2005. TimeML-Compliant 
Text Analysis for Temporal Reasoning. IJCAI-
2005, pp. 997-1003. 
Box, G. Jenkins, G. 1976. Time Series Analysis:  
Forecasting and Control, Holden-Day. 
Brockwell, P.J. Fienberg, S. Davis, R. 1991. Time 
Series: Theory and Methods. Springer-Verlag. 
Chatfield, C. 2003. The Analysis of Time Series. CRC 
Press. 
Ferro, L. Gerber, L. Mani, I. Sundheim, B. Wilson, G. 
2004. TIDES Standard for the Annotation of Tem-
poral Expressions. The MITRE Corporation. 
Filatova, E. Hovy, E. 2001. Assigning time-stamps to 
event-clauses. Proc. EACL 2001, Toulouse. 
Fung, G. 2003. The Disputed Federalist Papers: SVM 
Feature Selection via Concave Minimization. New 
York City, ACM Press. 
Gaizauskas, R. Setzer, A. 2002. Annotation Standards 
for Temporal Information in NL. LREC 2002. 
Janacek, G. 2001. Practical Time Series. Oxford U.P. 
Kiritchenko, S. Matwin, S. Abu-Hakima, S. 2004. 
Email Classification with Temporal Features. 
Proc. IIPWM 2004, Zakopane, Poland. Springer 
Verlag Advances in Soft Computing, pp. 523-534. 
Linguistic Data Consortium (LDC). 2003. English 
Gigaword Corpus. David Graff, ed. LDC2003T05. 
Mani, I. Wilson, G. 2000. Robust temporal processing 
of news. Proc. ACL 2000, Hong Kong. 
Mani, I. Schiffman, B. Zhang, J. 2003. Inferring tem-
poral ordering of events in news. Proc. HLT-
NAACL 2003, Edmonton, Canada. 
Moldovan, D. Clark, C. Harabagiu, S. 2005. Tempo-
ral Context Representation and Reasoning. IJCAI-
2005, pp. 1099-1104. 
Mosteller, F. Wallace, D. 1964. Inference and Dis-
puted Authorship: Federalist. Addison-Wesley. 
Prager, J. Chu-Carroll, J. Brown, E. Czuba, C. 2003. 
Question Answering using predictive annotation. 
In Advances in Question Answering, Hong Kong. 
Pustejovsky, J. Castano, R. Ingria, R. Sauri, R. Gai-
zauskas, R. Setzer, A. Katz, G. 2003. TimeML: 
Robust Specification of event and temporal expres-
sions in text. IWCS-5. 
Pustejovsky, J. Sauri, R. Castano, J. Radev, D. Gai-
zauskas, R. Setzer, A. Sundheim, B. Katz, G. 2004. 
?Representing Temporal and Event Knowledge for 
QA Systems?. New Directions in QA, MIT Press. 
Schilder, F. Habel, C. 2003. Temporal Information 
Extraction for Temporal QA. AAAI Spring Symp., 
Stanford, CA. pp. 35-44. 
Silverstein, C. Brin, S. Motwani, R. 1997. Beyond 
Market Baskets: Generalizing Association Rules to 
Dependence Rules. Data Mining and Knowledge 
Discovery. 
Sundheim, B. Gerber, L. Ferro, L. Mani, I. Wilson, G. 
2004. Time Expression Recognition and Normali-
zation (TERN). MITRE, Northrop Grumman, 
SPAWAR. http://timex2.mitre.org. 
Yarowsky, D. 1994. Decision Lists For Lexical Am-
biguity Resolution: Application to Accent Restora-
tion in Spanish and French. ACL 1994. 
22
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, page 216,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Invited Talk
Artificial Companions as Dialogue Agents
Yorick Wilks
Department of Computer Science
University of Sheffield
Sheffield S1 4DP, UK
www.dcs.shef.ac.uk/?yorick
yorick@dcs.sheffield.ac.uk
COMPANIONS is an EU project that aims to change the way we think about the relationships
of people to computers and the Internet by developing a virtual conversational ?Companion?. This
is intended as an agent or ?presence? that stays with the user for long periods of time, developing
a relationship and ?knowing? its owners preferences and wishes. The Companion communicates
with the user primarily through speech. This paper describes the functionality and system modules
of the Senior Companion, one of two initial prototypes built in the first two years of the project.
The Senior Companion provides a multimodal interface for eliciting and retrieving personal in-
formation from the elderly user through a conversation about their photographs. The Companion
will, through conversation, elicit their life memories, often prompted by discussion of their pho-
tographs; the aim is that the Companion should come to know a great deal about its user, their
tastes, likes, dislikes, emotional reactions etc, through long periods of conversation. It is a further
assumption that most life information will be stored on the internet (as in the Memories for Life
project: http://www.memoriesforlife.org/) and the SC is linked directly to photo in-
ventories in Facebook, to gain initial information about people and relationships, as well as to
Wikipedia to enable it to respond about places mentioned in conversations about images. The
overall aim of the SC, not yet achieved, is to produce a coherent life narrative for its user from
these materials, although its short term goals are to assist, amuse, entertain and gain the trust of
the user. The Senior Companion uses Information Extraction to get content from the speech input,
rather than conventional parsing, and retains utterance content, extracted internet information and
ontologies all in RDF formalism over which it does primitive reasoning about people. It has a
dialogue manager virtual machine intended to capture mixed initiative, between Companion and
user, and which can be a basis for later replacement by learned components.
216
Proceedings of the ACL 2010 System Demonstrations, pages 72?77,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Demonstration of a prototype for a Conversational Companion for  reminiscing about images  Yorick Wilks IHMC, Florida ywilks@ihmc.us Roberta Catizone University of Sheffield, UK r.catizone@dcs.shef.ac.uk Alexiei Dingli University of Malta, Malta alexiei.dingli@um.edu.mt Weiwei Cheng University of Sheffield, UK w.cheng@dcs.shef.ac.uk  Abstract 
This paper describes an initial prototype demonstrator of a Companion, designed as a platform for novel approaches to the following:  1) The use of Informa-tion Extraction (IE) techniques to extract the content of incoming dialogue utterances after an Automatic Speech Recognition (ASR) phase, 2) The conversion of the input to Resource Descriptor Format (RDF)  to allow the generation of new facts from existing ones, under the control of a Dialogue Manger (DM), that also has access to stored knowledge and to open knowledge accessed in real time from the web, all in RDF form, 3) A DM implemented as a stack and net-work virtual machine that models mixed initiative in dialogue control, and 4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this briefly; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning. 1. Introduction This demonstrator Senior Companion (SC) was built during the initial phase of the Companions project and  aims to change the way we think about the relationships of people to computers and the internet by developing a virtual conver-sational 'Companion that will be an agent or 'presence' that stays with the user for long peri-ods of time, developing a relationship and 'know-ing its owners? preferences and wishes. The Companion communicates with the user primar-ily through speech, but also using other tech-nologies such as touch screens and sensors. This paper describes the functionality and system modules of the Senior Companion, one of two initial prototypes built in the first two years of the project. The SC provides a multimodal inter-face for eliciting, retrieving and inferring per-sonal information from elderly users by means of conversation about their photographs. The Com-panion, through conversation, elicits life memo-
ries and reminiscences, often prompted by dis-cussion of their photographs; the aim is that the Companion should come to know a great deal about its user, their tastes, likes, dislikes, emo-tional reactions etc, through long periods of con-versation. It is assumed that most life informa-tion will soon be stored on the internet (as in the Memories for Life project: http://www.memoriesforlife.org/) and we have linked the SC directly to photo inventories in Facebook (see below). The overall aim of the SC project (not yet achieved) is to produce a coher-ent life narrative for its user from conversations about personal photos, although its short-term goals, reported here, are to assist, amuse and en-tertain the user.  The technical content of the project is to use a number of types of machine learning  (ML) to achieve these ends in original ways, initially us-ing a methodology developed in earlier research: first, by means of an Information Extraction (IE) approach to deriving content from user input ut-terances; secondly, using a training method for attaching Dialogue Acts to these utterance and, lastly, using a specific type of dialogue manager (DM) that uses Dialogue Action Forms  (DAF) to determine the context of any utterance. A stack of these DAFs is the virtual machine that models the ongoing dialogue by means of shared user and Companion initiative and generates ap-propriate responses. In this description of the demo, we shall: 
? describe the current SC prototype?s func-tionality; 
? set out its architecture and modules, fo-cusing on the Natural Language Under-standing module and the Dialogue Man-ager. A mini-version of the demo running in real time can be seen at:  URL   
72
http://www.youtube.com/watch?v=-Xx5hgjD-Mw 2. The Senior Companion System The Senior Companion prototype (Wilks, 2010)  was designed to make a rapid advance in the first two years of the project so as to be basis for a second round of prototypes embodying more advanced ML. This strategy was deliberately chosen to avoid a well-known problem with ex-perimental AI systems: that a whole project is spent in design so that a prototype never emerges until the very end, but which is then never fully evaluated and, most importantly, nothing is ever built on the experience obtained in its construc-tion. The central function of the SC is engaging the user in discussion about their photographs: where and when they were taken, details about the people in them and their relationship to the user and each other.  Once a photo is loaded, it is processed with face recognition software to identify any faces in it. The recognition software, OpenCV, provides positional information by identifying the face coordinates and this information is exploited in the Dialogue Manager by making explicit refer-ence to the position of people in the photograph (the person on the left, right, center, etc.) as well as recognizing when there are groups of people. The system discusses properties of the photo as well as properties and relationships of the people in the photos.   The news reading feature adds an interesting ac-companiment to the photo domain and demon-strates the ability of the system to handle more than one kind of application at a time, and news has, of course, an unconstrained vocabulary. The following is a fairly typical example of its cur-rent capacity, depending of course on the images loaded, and comes from the middle part of a sample dialogue generated by the system when discussing a group photo from a vacation in Zan-zibar, and after the system had already stored, from an earlier conversation, that the user has a daughter Zoe (using researchers? children to avoid any problems):  SC: What is your relationship to the doctor? User: There is no relationship. SC: Please tell me how you know the doctor U: Uh, he's a friend. SC: How old was the doctor when this photo was taken? U: I'm not sure but I think about 30. 
SC: How do you know Octavia? U: She's my daughter. SC: Aha! I see. Octavia's sister is Zoe. SC: How old was your daughter Octavia when this photo was taken? U: Octavia was 19 years old. SC: What was the occasion of this photo? U: This was another family holiday. SC: Where was this photo taken? U: This photo was taken in Tanzania. SC: have you ever been to Zanzibar? U: Yes, I have. SC: Great place, I hope you saw the beaches. When was this photo taken? U: last year.  It is a key feature of the SC that it can make sim-ple inferences from family relationships it al-ready knows (e.g. that daughters of the same par-ent are siblings) and that it can access real-time information about places to show that it has some knowledge of what is being talked about, in this case the beaches on Zanzibar, and how this is done is discussed below. This real time access to unconstrained place information on the internet is an attempt to break out of classic AI systems that only know the budget of facts they have been primed with.  This basic system provides the components for future development of the SC, as well as its main use as a device to generate more conversation data for machine learning research in the future. Key features of the SC are listed below followed by a description of the system architecture and modules. The SC: ? Contains a visually appealing multi-modal interface with a character avatar to mediate the system?s functionality to the user. ? Interacts with the user using multiple modalities ? speech and touch. ? Includes face detection software for identifying the position of faces in the photos. ? Accepts pre-annotated (XML) photo in-ventories as a means for creating richer dialogues more quickly.  ? Engages in conversation with the user about topics within the photo domain: when and where the photo was taken, discussion of the people in the photo in-cluding their relationships to the user. ? Reads news from three categories: poli-tics, business and sports. 
73
? Tells jokes taken from an internet-based  joke website. ? Retains all user input for reference in re-peat user sessions, in addition to the knowledge base that has been updated by the Dialogue Manager on the basis of what was said. ? Contains a fully integrated Knowledge Base for maintaining user information including: o Ontological information which is exploited by the Dialogue Manager and provides domain-specific relations between fun-damental concepts.  o A mechanism for storing infor-mation in a triple store (Subject-Predicate-Object) - the RDF Semantic Web format - for han-dling unexpected user input that falls outside of the photo do-main, e.g. arbitrary locations in which photos might have been taken. o A reasoning module for reason-ing over the Knowledge Base and world knowledge obtained in RDF format from the internet; the SC is thus a primitive Se-mantic Web device (see refernce8, 2008) ? Contains basic photo management capa-bility allowing the user, in conversation, to select photos as well as display a set of photos with a particular feature.  
 Figure 1: The Senior Companion Interface 
 3. System Architecture  In this section we will review the components of the SC architecture. As can be seen from Figure 2, the architecture contains three abstract level components ? Connectors, Input Handlers and Application Services ?together with the Dialogue Manager and the Natural Language Understander (NLU). 
  Figure 2: Senior Companion system architecture  Connectors form a communication bridge be-tween the core system and external applications. The external application refers to any modules or systems which provide a specific set of function-alities that might be changed in the future. There is one connector for each external application. It hides the underlying complex communication protocol details and provides a general interface for the main system to use. This abstraction de-couples the connection of external and internal modules and makes changing and adding new external modules easier. At this moment, there are two connectors in the system ? Napier Inter-face Connector and CrazyTalk Avatar Connec-tor. Both of them are using network sockets to send/receive messages.  Input Handlers are a set of modules for process-ing messages according to message types. Each handler deals with a category of messages where categories are coarse-grained and could include one or more message types. The handlers sepa-rate the code handling inputs into different places and make the code easier to locate and change. Three handlers have been implemented in the Senior Companion system ? Setup Handler, Dragon Events Handler and General Handler. The Setup Handler is responsible for loading the photo annotations if any, performing face detec-tion if no annotation file is associated with the photo and checking the Knowledge Base in case 
74
the photo being processed has been discussed in earlier sessions. Dragon Event Handler deals with dragon speech recognition commands sent from the interface while the General Handler processes user utterances and photo change events of the interface.  Application Services are a group of internal modules which provide interfaces for the Dia-logue Action Forms (DAF) to use. It has an easy-to-use high-level interface for general DAF de-signers to code associated tests and actions as well as a low level interface for advanced DAFs. It also provides the communication link between DAFs and the internal system and enables DAFs to access system functionalities. Following is a brief summary of modules grouped into Applica-tion Services.  News Feeders are a set of RSS Feeders for fetch-ing news from the internet. Three different news feeders have been implemented for fetching news from BBC website Sports, Politics and Business channels. There is also a Jokes Feeder to fetch Jokes from internet in a similar way. During the conversation, the user can request news about particular topics and the SC simply reads the news downloaded through the feeds.  The DAF Repository is a list of DAFs loaded from files generated by the DAF Editor.   The Natural Language Generation (NLG) mod-ule is responsible for randomly selecting a sys-tem utterance from a template. An optional vari-able can be passed when calling methods on this module. The variable will be used to replace spe-cial symbols in the text template if applicable.   Session Knowledge is the place where global information for a particular running session is stored. For example, the name of the user who is running the session, the list of photos being dis-cussed in this session and the list of user utter-ances etc.  The Knowledge Base is the data store of persis-tent knowledge. It is implemented as an RDF triplestore using a Jena implementation. The tri-plestore API is a layer built upon a traditional relational database. The application can save/retrieve information as RDF triples rather than table records. The structure of knowledge represented in RDF triples is discussed later.  
The Reasoner is used to perform inference on existing knowledge in the Knowledge Base (see example in next section).  The Output Manager deals with sending mes-sages to external applications. It has been im-plemented in a publisher/subscriber fashion. There are three different channels in the system: the text channel, the interface command channel and the avatar command channel. Those chan-nels could be subscribed to by any connectors and handled respectively.  4. Dialogue understanding and inference Every utterance is passed through the Natural Language Understanding (NLU) module for processing. This module uses a set of well-established natural language processing tools such as those found in the GATE (Cunningham, et al, 1997) system. The basic processes carried out by GATE are: tokenizing, sentence splitting, POS tagging, parsing and Named Entity Recog-nition. These components have been further en-hanced for the SC system by adding 1) new and improved gazetteers including family relations and 2) accompanying extraction rules .The Named Entity (NE) recognizer is a key part of the NLU module and recognizes the significant entities required to process dialogue in the photo domain: PERSON NAMES, LOCATION NAMES, FAMILY RELATIONS and DATES. Although GATE recognizes basic entities, more complex entities are not handled. Apart from the gazetteers mentioned earlier and the hundreds of extraction rules already present in GATE, about 20 new extraction rules using the JAPE rule lan-guage were also developed for the SC module. These included rules which identify complex dates, family relationships, negations and other information related to the SC domain. The fol-lowing is an example of a simple rule used to identify relationship in utterances such as ?Mary is my sister?:  Macro: RELATIONSHIP_IDENTIFIER ( ({To-ken.category=="PRP$"}|{Token.category=="PRP"}|{Lookup.majorType=="person_first"}):person2 ({Token.string=="is"}) ({Token.string=="my"}):person1  ({Lookup.minorType=="Relationship"}):relationship) 
75
Using this rule with the example mentioned ear-lier, the rule interprets person1 as referring to the speaker so, if the name of the user speaking is John (which was known from previous conversa-tions), it is utilized. Person 2 is then the name of the person mentioned, i.e. Mary. This name is recognised by using the gazetteers we have in the system (which contain about 40,000 first names). The relationship is once again identified using the almost 800 unique relationships added to the gazetteer.  With this information, the NLU mod-ule identifies Information Extraction patterns in the dialogue that represent significant content with respect to a user's life and photos.   The information obtained (such as Mary=sister-of John) is passed to the Dialogue Manager (DM) and then stored in the knowledge base (KB). The DM filters what to include and ex-clude from the KB. Given, in the example above, that Mary is the sister of John, the NLU knows that sister is a relationship between two people and is a key relationship. However, the NLU also discovers syntactical information such as the fact the both Mary and John are nouns. Even though this information is important, it is too low level to be of any use by the SC with respect to the user, i.e. the user is not interested in the parts-of-speech of a word. Thus, this information is dis-carded by the DM and not stored in the KB. The NLU module also identifies a Dialogue Act Tag for each user utterance based on the DAMSL set of DA tags and prior work done jointly with the University of Albany (Webb et al, 2008).  The KB is a long-term store of information which makes it possible for the SC to retrieve information stored between different sessions. The information can be accessed anytime it is needed by simply invoking the relevant calls. The structure of the data in the database is an RDF triple, and the KB is more commonly re-ferred to as a triple store. In mathematical terms, a triple store is nothing more than a large data-base of interconnected graphs. Each triple is made up of a subject, a predicate and an object. So, if we took the previous example, Mary sister-of John; Mary would be the subject, sister-of would be the predicate and John would be the object. The inference engine is an important part of the system because it allows us to discover new facts beyond what is elicited from the con-versation with the user.    
Uncle Inference Rule:   (?a sisterOf ?b), (?x sonOf ?a), (?b gender male) -> (?b uncleOf ?x)    Triples: (Mary  sisterOf  John) (Tom   sonOf   Mary)  Triples produced automatically by ANNIE (the semantic tagger): (John  gender   male)  Inference: (Mary  sisterOf  John) (Tom   sonOf   Mary) (John  gender   male)  ->  (John uncleOf Tom)  This kind of inference is already used by the SC and we have about 50 inference rules aimed at producing new data on the relationships domain. This combination of triple store, inference engine and inference rules makes a system which is weak but powerful enough to mimic human rea-soning in this domain and thus simulate basic intelligence in the SC. For our prototype, we are using the JENA Semantic Web Framework for the inference engine together with a MySQL da-tabase as the knowledge base. However, this sys-tem of family relationships is not enough to cover all the possible topics which can crop up during a conversation and, in such circum-stances, the DM switches to an open-world model and instructs the NLU to seek further in-formation online.  5. The Hybrid-world approach When the DM requests further information on a particular topic, the NLU first checks with the KB whether the topic is about something known. At this stage, we have to keep in mind that any topic requested by the DM should be already in the KB since it was preprocessed by the NLU when it was mentioned in the utterance. So, if the user informs the system that the photograph was taken in Paris, (in response to a system question asking where the photo was taken), the utterance is first processed by the NLU which discovers that ?Paris? is a location using its semantic tag-ger ANNIE (A Nearly New Information Extrac-tion engine). The semantic tagger makes use of gazetteers and IE rules in order to accomplish 
76
this task. It also goes through the KB and re-trieves any triples related to ?Paris?. Inference is then performed on this data and the new informa-tion generated by this process is stored back in the KB.   Once the type of information is identified, the NLU can use various predefined strategies: In the case of LOCATIONS, one of the strategies used is to seek for information in Wiki-Travel or Virtual Tourists. The system already knows how to query these sites and interpret their output by using predefined wrappers. This is then used to extract relevant information from the mentioned sites webpages by sending an online query to these sites and storing the information retrieved in the triple-store. This information is then used by the DM to generate a reply. In the previous example, the system manages to extract the best sightseeing spots in Paris. The NLU would then store in the KB triples such as [Paris, sight-seeing, Eiffel Tower] and the DM with the help of the NLG would ask the user ?I?ve heard that the X is a very famous spot. Have you seen it while you were there?? Obviously in this case, X would  be replaced by the ?Eiffel Tower?.  On the other hand, if the topic requested by the DM is unknown, or the semantic tagger is not capable of understanding the semantic category, the system uses a normal search engine (and this is what we call ?hybrid-world?: the move outside the world the system already knows). A query containing the unknown term in context is sent to standard engines and the top pages are retrieved. These pages are then processed using ANNIE and their tagged attributes are analyzed. The standard attributes returned by ANNIE include information about Dialogue Acts, Polarity (i.e. whether a sentence has positive, negative or neu-tral connotations), Named Entities, Semantic Categories (such as dates and currency), etc. The system then filters the information collected by using more generic patterns and generates a reply from the resultant information. ANNIE?s polarity methods have been shown to be an adequate im-plementation of the general word-based polarity methods pioneered by Wiebe and her colleagues (see e.g. Akkaya et al, 2009). 6. Evaluation  The notion of companionship is not yet one with any agreed evaluation strategy or metric, though developing one is part of the main project itself.  
Again, there are established measures for the as-sessment of dialogue programs but they have all been developed for standard task-based dia-logues and the SC is not of that type: there is no specific task either in reminiscing conversations, nor in the elicitation of the content of photos, that can be assessed in standard ways, since there is no clear point at which an informal dialogue need stop, having been completed.  Conventional dialogue evaluations often use measures like ?stickiness? to determine how much a user will stay with or stick with a dialogue system and not leave it, presumably because they are disap-pointed or find it lacking in some feature. But it is hard to separate that feature out from a task rapidly and effectively completed, where sticki-ness would be low not high. Traum (Traum et al, 2004) has developed a methodology for dialogue evaluation based on ?appropriateness? of re-sponses and the Companions project has devel-oped a model of evaluation for the SC based on that (Benyon et al, 2008).  Acknowledgement  This work was funded by the Companions project  (2006-2009) sponsored by the European Commission as part of the Information Society Technologies (IST) programme under EC grant number IST-FP6-034434.  References David Benyon, Prem Hansen and Nick Webb, 2008. Evaluating Human-Computer Conversation in Companions. In: Proc.4th International Workshop on Human-Computer Conversation, Bellagio, Italy. Cem Akkaya, Jan Wiebe, and Rada Mihalcea,. 2009. Subjectivity Word Sense Disambiguation, In:  EMNLP 2009. Hamish Cunningham, Kevin Humphreys, Robert Gai-zauskas, and Yorick Wilks, 1997. GATE -- a TIP-STER based General Architecture for Text Engi-neering. In: Proceedings of the TIPSTER Text Pro-gram (Phase III) 6 Month Workshop. Morgan Kaufmann, CA. David Traum, Susan Robinson, and Jens Stephan. 2004.  Evaluation of multi-party virtual reality dia-logue interaction, In: Proceedings of Fourth International Conference on Language Resources and Evaluation (LREC 2004), pp.1699-1702 Yorick Wilks (ed.) 2010. Artificial Companions in Society: scientific, economic, psychological and philosophical perspectives. John Benjamins: Am-sterdam.  
77
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 13?18,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Is a Companion a distinctive kind of relationship with a machine? Yorick Wilks Florida Institute of Human and Machine Cognition      ywilks@ihmc.us Abstract I start from a perspective close to that of the EC COMPANIONS project, and set out its aim to model a new kind of human-computer relation-ship based on long-term interaction, with some tasks involved although a Companion should not be inherently task-based, since there need be no stopping point to its conversation. Some demon-stration of its functionality will be given but the main purpose here is an analysis of what it is people might want from such a relationship and what evidence we have for whatever we con-clude. Is politeness important? Is an attempt at emotional sympathy important or achievable? Does a user want a consistent personality in a Companion or a variety of personalities? Should we be talking more in terms of a "cognitive pros-thesis (or orthosis)?" ---something to extract, or-ganize, and locate the user's knowledge or per-sonal information---rather than attitudes? 1. Introduction It is convenient to distinguish Companions from both (a) conversational internet agents that carry out specific tasks, such as the train and plane scheduling and ticket ordering speech dialogue applications back to the MIT ATIS systems (Zue et al, 1992), and also from (b) descendants of the early chatbots PARRY and ELIZA, the best of which compete annually in the Loebner competi-tion (Loebner). These have essentially no mem-ory or knowledge but are simple finite state re-sponse sets, although ELIZA had primitive ?scripts? giving some context, and PARRY (Colby, 1971) had parameters like FEAR and ANGER that changed with the conversation and determined which reply was selected at a given point. I take plausible distinguishing features of a Companion agent to be:  1) that it has no central or over-riding task and there is no point at which its conver-sation is complete or has to stop, al-though it may have some tasks it carries out in the course of conversation; 2) That it should be capable of a sustained discourse over a long-period, possibly  
ideally the whole life-time of its princi-pal user; 3) It is essentially the Companion of a par-ticular individual, its principal user, about whom it knows a great deal of per-sonal knowledge, and whose interests it serves?it could, in principle, contain all the information associated with a whole life; 4) It establishes some form of relationship with that user, if that is appropriate, which would have aspects associated with the term ?emotion?, and shared ini-tiative is essential;  5) It is not essentially an internet agent or interface, but since it will have to have access to the internet for information (in-cluding the whole-life information about its user?which could be public data like Facebook, or life information built up by the Companion over long periods of in-teraction with the user) and to act in the world, e.g. to reserve at a restaurant or call a doctor. But a Companion need not be a robot to act in the world in this way, and we may as well assume its internet agent status, with access to open internet knowledge sources.  Given this narrowing of focus in this paper, what questions then arise and what choices does that leave open? We now discuss some obvious ques-tions that have arisen in the literature:  i) Emotion, politeness and affection Cheepen and Monaghan (1997) presented results some thirteen years ago that customers of some automata, such as ATMs, are repelled by exces-sive politeness and endless repetitions of ?thank you for using our service?, because they know they are dealing with a machine and such feigned sincerity is inappropriate. This suggests that po-liteness is very much a matter of judgment in certain situations, just as it is with humans, where inappropriate politeness is often encoun-tered. Wallis (Wallis et al, 2001) has reported results that many find computer conversational-ists ?chippy? or ?cocky? and suggests that this should be avoided as it breeds hostility on the part of users; he believes this is always a major 
13
risk in human-machine interactions.   We know, since the original work of Nass (Reeves and Nass, 1996) and colleagues that people will display some level of feeling for the simplest machines, even PCs in his original ex-periments, and Levy (2007) has argued persua-sively that the trend seems to be towards high levels of ?affectionate? relationships with ma-chines in the next decades, as realistic hardware and sophisticated speech generation make ma-chine interlocutors increasingly lifelike. How-ever, much of this work is about human psychol-ogy, faced with entities known to be artificial, and does not bear directly on the issue of whether Companions should attempt to detect emotion in what they hear from us, or attempt to generate it in what they say back.  The AI area of ?emotion and machines? is con-fused and contradictory: it has established itself as more than an eccentric minority taste, but as yet has nothing concrete to show beyond some better than random algorithms for detecting ?sen-timent? in incoming text (e.g. Wiebe et al, 2005), but even there its success is dependent on effective content extraction techniques. This work began as ?content analysis? (Krippendorff, 2004) at the Harvard psychology department many years ago and, while prose texts may offer enough length to enable a measure of sentiment to be assessed, this is not always the case with short dialogue turns. That technology rested al-most entirely on the supposed sentiment value of individual words, which ignores the fact that their value is content dependent. ?Cancer? may be marked as negative word but the utterance ?I have found a cure for cancer? is presumably positive and detecting the appropriate response to that utterance rests on the ability to do informa-tion extraction beyond single terms. Failure to observe this has led to many of the classic fool-ishnesses of chatbots such as congratulating peo-ple on the death of their relatives, and so on. At deeper levels, there are conflicting theories of emotion for automata, not all of which are con-sistent and which apply only in limited ranges of discourse. So, for example, the classic theory that emotion is a response to the failure and success of the machine?s plans (e.g. Marsella and Gratch, 2003) covers only those situations that are clearly plan driven and, as we noted, Compan-ionship dialogue is not always closely related to plans and tasks. ?Dimensional? theories (Cowie et al, 2001, following Wundt, 1913), display 
emotions along dimensions marked with opposed qualities (such as positive-negative) and nor-mally distribute across the space emotion ?primi-tives?, such as FEAR, and these normally as-signed by manual tagging. All such assignments of tags rest, like the text-sentiment theories above, on human pre-tagging.  The problem with this is that tagging for ?COMPANY? or ?TEM-PERATURE? (in classic NLP) is a quite differ-ent task from tagging for ?FEAR? and ?AN-GER?. These latter terms are not, and probably cannot be, analyzed but rest on the commonsense intuitions of the tagger, which may vary very much from person to person?they have very low consilience between taggers. All this makes many emotion theories look primitive in terms of developments in AI and NLP elsewhere. Appraisal Theory (Scherer et al 2008) seeks to explain why individuals can have quite different emotional reactions to similar situations because they have appraised them dif-ferently, e.g. a death welcomed or regretted. Ap-praisal can also be of the performance of planned activities, in which case this theory approximates to the plan-based one mentioned above. The the-ory itself, like all such theories, has a large-commonsense component, and the issue for computational implementation is how, in assess-ing the emotional state of the Companion?s user to make such concepts quantitatively evaluable. If the Companion conducts long conversations with a user about his or her life, then one might expect there to be ample opportunity to assess the user?s appraisal of, say, a funeral or wedding by means of the application of the sentiment ex-traction techniques to what is said in the presence of the relevant image. In so far as a Companion can be said to have over-arching goals, such as keeping the user happy then, to that degree, it is not difficult to envisage methods (again based on estimates of the happiness, or otherwise, of the user?s utterances) for self-appraisal by the Com-panion of its own performance and some conse-quent causal link to generated demonstrations of its own emotions of satisfaction or guilt.  In speaking of ?language? and Companions, we have so far ignored speech, although that is a communication mode in which a great deal has been done to identify and, more recently, gener-ate, emotion-bearing components (Luneski et al, 2008).  Elements of the above approaches can be found  in the work of Worgan and Moore (see figure below, from REFERENCE REMOVED), where there is the same commitment to the cen-
14
trality of emotion in the communication process, but in a form focusing on an integration of speech and language  (rather than visual and de-sign) technologies. Their argument is for a layer in a dialogue manager over and above local re-sponse management, but one which would seek to navigate the whole conversation across a two-dimensional space onto which Companion and user are mapped using continuous values (rather than discrete values corresponding to primitive but unexplained emotional terms) but in such a way as to both respond to the a user?s demon-strated emotion appropriately, but also----again, if appropriate or chosen by the user----to draw the user back to other more positive emotional areas of the two-dimensional space. It is not yet clear what the right mechanism should be for the integration of this ?landscape? global emotion-based dialogue manager should be with the local dialogue management that generates responses and alters the world context: in the Senior Com-panion this last was sophisticated stack of net-works (see Wilks et al, in press). In some sense, we are just looking for a modern and defensible interface to replace what PARRY had in simple form in 1971 when the sum of two emotion pa-rameters determined which response to select from a stack of alternatives.  This last is a high level issue to be settled in a Companion?s architecture and also, perhaps, to be under the control of the user, namely: should a Companion invariably try to cheer a user up if miserable-----which is trying to ?move? the user to the most naturally desirable (i.e. the top-right) quadrant of the space----or, rather, to track to the part of the space where the user is deemed to be and stay there in roughly the same emotional lo-cation?i.e. be sad with a sad user and happy with a happy one? There is no general answer to this question and, indeed, in an ideal Companion, which tracking method should be used would itself be a conversation topic e.g. ?Do you want me to cheer you up or would you rather stay mis-erable??.   
  ii) What should a Companion look like? A faceless Companion is a plausible candidate for Companionhood: the proverbial furry hand-bag, warm and light to carry, chatty but with full internet access. Such a Companion could always take control of a nearby screen or a phone if it needed to show anything. If there is to be a face, the question of the ?uncanny valley effect? al-ways comes up, where it is argued that users are more uneasy the more something is very like ourselves (Mori, 1970). But many observers do not feel this, and, indeed it cannot in principle apply to an avatar so good that one cannot be sure it is artificial, as many feel about the Emily from Manchester (Emily 2009).  On the other hand, if the quality is not good, and in particular if the lip synch is not perfect, it may be better to go for an abstract avatar ---the Com-panions logo was chosen with that in mind, and without a mouth at all. Non-human avatars seem to avoid some of the problems that arise with valleys and mixed feelings generally, and the best REMOVED demonstration video so far fea-tures REMOVED.  iii) Voice or Typing to communicate with a Companion? At the moment the limitation on the use of voice is two-fold: first, although trained ASR for a sin-gle user?such as a Companion?s user?is now very good and up in the high 90%, it still intro-duces uncertainty into understanding an utter-ance that is far greater than that of spelling er-rors. Secondly, it is currently not possible to store sufficient ASR software locally on a mobile phone to recognize a large vocabulary in real time; access to a remote server takes additional time and can be subject to fluctuations and de-lays. All of which suggests that a web-based Companion may have to use typed input in the immediate future?though using TTS output?
15
which is no problem for most mobile phone us-ers, who have come to find typed chat perfectly natural. However, this is almost certainly only a transitory delay as mobile RAM increases rap-idly and the problem should not determine re-search decisions---there is no doubt that voice will move back to the centre of communication once storage and access size have grown by an-other order of magnitude.  iv) One Companion personality or several? Some (e.g. Pulman, in Wilks, 2010) have argued that having a consistent personality is a condition on Companionhood, but one could differ and argue that, although that is true of people?multiple personalities being a classic psycho-sis?there is no reason why we should expect this of a Companion. Perhaps a Companion should have a personality adapted to its particu-lar relationship to a user at a given moment: Lowe (in Wilks, 2010) has pointed out that one might want a Companion to function as, say, a gym trainer, in which case a rather harsh attitude on the part of the Companion might well be the best one. If a Companion?s emotional attitude were to (figuratively) move across a two dimen-sional emotion space (see diagram above) imitat-ing or correcting what it perceived to be the user?s state over time (as Worgan, see above, has proposed), then that shift in attitude might well seem to be the product of different personalities, as it sometimes can with humans.  It might be better, pace Pulman, to give a user access to, and some control over, the display of a multiple-personality Companion, something one could think of as an ?agency? of Companions, rather than a single ?agent?, all of which shared access to the same knowledge of the world and of the state and history of the user.  v) Ethics and goals in the Companion The issue is very close to the question of what goals a Companion can plausibly have, beyond something very general, such as ?keep the user happy and do what they ask if you can?, which are goals and constraints that directly relate to the standard discussions of the ethics a robot could be considered to have, a discussion started long ago by Asimov (1975). Clearly, there will be need for a Companion to have goals to carry out specific tasks: if it is to place a restaurant table booking on the phone for a user who has 
just said to it ?Get me a table for two tonight at Branca around 8.30?---a phone request well within the bounds of the currently achievable technology-----and the Companion will first have to find the restaurant?s phone number before it phones and ask about availability before choos-ing a reservation time. This is the standard con-tent of goal-driven behavior, with alternatives at every stage if unexpected replies are encountered (such as the restaurant being fully booked to-night).  But one does not need to consider such goals as  ?goals of its own? since they are in-ferred from what it was told and are simply as-sumed, as an agent or slave of the user. But a Companion that finds its user not responding after some minutes of conversation might well have to take an independent decision to call a doctor urgently, based on a stored permanent goal about danger to a user who is unable to an-swer but is not asleep etc.  vi) Safeguards for the information content of a Companion Data protection, privacy, or whatever term one prefers, now captures a crucial concept in the new information society. A Companion that had learned intimate details of a user?s life over months or years would certainly have contents needing protection, and many forces-----commercial, security, governmental, research---might well want access to it, or even to those of all the Companions in a given society. If socie-ties move to a clear legal state where one?s per-sonal data is one?s own, with the owner or origi-nator having rights over sale and distribution of their data---which is not at all the case at the moment in most countries----then the issue of the personal data elicited by a Companion would automatically be covered.    If we ignore the issues of governments and na-tional security---and a Companion would clearly be useful to the police when wanting to know as much as possible about a murder suspect, so that it might then be an issue of whether talking to one?s Companion constituted any kind of self-incrimination, in countries where that form of communication is protected. Some might well want one?s relationship to a Companion put on some basis like that of a relationship to a priest or doctor, or even to a spouse, who cannot al-ways be forced to give evidence in common-law countries.  
16
More realistically, a user might well want to pro-tect parts of his or her Companion?s information, or even an organized life-story based on that, from particular individuals: e.g. ?this must never be told to my children, even when I am gone?. It is not hard to imagine a Companion deciding whom to divulge certain things to, selecting be-tween classes of offspring, relations, friends, col-leagues etc. There will almost certainly need to be a new set of laws covering the ownership, in-heritance and destruction of Companion-objects in the future.  vii) What must a Companion know? There is no clear answer to this question: dogs make excellent Companions and know nothing. More relevantly, Colby?s PARRY program, the best conversationalist of its day (Colby, 1971) and possibly since, famously ?knew? nothing: John McCarthy at Stanford dismissed PARRY?s performance by saying:?It doesn?t even know who the US President is?, forgetting as he said it that most of world?s population did not know that, at least at the time.  On the other hand, it is hard to relate over a long term to an interlocutor who knows little or nothing and has no memory of what it or you have said in the past. It is hard to attribute personality to an entity with no mem-ory and little or no knowledge.  Much of what a Companion knows that is per-sonal it should elicit in conversation from its user; yet much could also be gained from pub-licly available sources, just as the current Senior Companion demo goes off to Facebook, inde-pendently of a conversation, to find out who its user?s friends are. Current information extraction technology (e.g. Ciravegna et al, 2004) allows a reasonable job to be made of going to Wikipedia for general information when, say, a world city is mentioned; the Companion can then glean some-thing about that city from Wikipedia and ask a relevant question such as ?Did you see the Eiffel Tower when you were in Paris?? which again gives a plausible illusion of general knowledge.   A concrete Companion paradigm: the Victorian Companion  The subsections above are mini-discussions of some of the constraints on what it is to be a Companion, the subject of a recent book collec-tion (Wilks, 2010). The upshot of those discus-sions is that there are many dimensions of 
choice, even within an agreed definition of what a Companion is to be, and they will depend on the user?s tastes and needs above all. In the sec-tion that follows, I cut though the choices and make a semi-serious proposal for a model Com-panion, one based on a once well-known social stereotype.  More seriously, and in the spirit of a priori thoughts (and what else can we have at this tech-nological stage of development?) about what a Companion should be, I would suggest we could profitably spend a few moments reminding our-selves of the role of the Victorian lady?s Com-panion. One could, and in no scientific manner, risk a listing of features of the ideal Victorian Companion: 1. Politeness 2. Discretion 3. Knowing their place 4. Dependence 5. Emotions firmly under control 6. Modesty 7. Wit 8. Cheerfulness 9. Well-informed 10. Diverting 11. Looks are irrelevant 12. Long-term relationship if possible 13. Trustworthy 14. Limited socialization between Com-panions permitted off-duty.  The Victorian virtue of discretion here brings to mind the ?confidant? concept that Boden (in Wilks, 2010) explicitly rejected as being a plau-sible one for automated Companions:  Most secrets are secret from some HBs [Human Beings] but not others. If two CCs [Computer Companions] were to share their HB-users? se-crets with each other, how would they know which other CCs (i.e. potentially, users) to ?trust? in this way? The HB could of course say "This is not to be told to Tommy"...... but usually we re-gard it as obvious that our confidant (sic) knows what should not be told to Tommy -- either to avoid upsetting Tommy, or to avoid upsetting the original HB. How is a CC to emulate that?  The HB could certainly say "Tell this to no-one" -- where "no-one" includes other CCs. But would the HB always remember to do that?  How could a secret-sharing CC deal with family feuds? Some family websites have special func-
17
tionalities to deal with this. E.g Robbie is never shown input posted by Billie. Could similar, or more subtle, functionalities be given to CCs??  Boden brings up real difficulties in extending this notion to a computer Companion, but the problems are not all where she thinks. I see no difficulty in programming the notion of explicit secrets for a Companion, or even things to be kept from specific individuals (?Never tell this to Tommy?). Companions will have less problems remembering to be discrete than people do, and I suspect people have less instinctive discretion than Boden believes: they have to be told explic-itly who to say what to, or not, in most cases, unless they are told to tell no one. In any case, much of this will be moot because Companions will normally deal only with one person except when, say, making phone calls to an official, friend or restaurant, where they can try to keep the conversation to limited replies that they can be sure to understand. The notion of a stored fact that must not be disclosed is relatively simple to code. Nonetheless, the Lady?s Companion anal-ogy foresees that Companions will, in time, gos-sip among themselves behind their owners? backs.  I would argue that the ?Lady?s Companion? list above an attractive and plausible one: it assumes emotion will be largely linguistic in expression, it implies care for the mental and emotional state of the user, and I would personally find it hard to abuse any computer with the characteristics listed above. Many of the situations discussed above are, at the moment, wildly speculative: that of a Companion acting as its owner?s agent, on the phone or World Wide Web, perhaps hold-ing power of attorney in case of an owner?s inca-pacity and, with the owner?s advance permission, perhaps even being a source of conversational comfort for relatives after the owner?s death. Companions may not all be nice or even friendly: Companions to stop us falling asleep while driv-ing may tell us jokes but will probably shout at us and make us do stretching exercises. Long-voyage Companions in space will be indispensa-ble cognitive prostheses (or, more correctly, or-thoses) for running a huge vessel and experi-ments above any beyond any personal services---Hollywood already knows all that.   Acknowledgement:  This work was funded by the Companions project sponsored by the European Commission as part of the Information Society Technologies (IST) programme under EC grant number IST-FP6-034434. 
References Colby, K.M. "Artificial Paranoia." Artif. Intell. 2(1) (1971), pp. 1-2 Cheepen, C. and Monaghan, J. 1997, ?Designing Naturalness in Automated Dialogues - some problems and solutions?. In Proceedings ?First International Workshop on Human- Computer Conversation?, Bel-lagio, Italy. Cowie, R., Douglas-Cowie, E., Tsapatsoulis, N., Vot-sis, G., Kollias, S., Fellenz, W. and Taylor, JG. 2001. Emotion recognition in human-computer interaction, Signal Processing Magazine, IEEE, 18(1), pp. 32?80. Emily,2009.http://www.youtube.com/watch?v=UYgLFt5wfP4&feature=player_embedded# http://www.surrealaward.com/avatar/3ddigital12.shtml Krippendorff, K. 2004.  Content Analysis: An Intro-duction to Its Methodology. 2nd edition, Thousand Oaks, CA: Sage. Levy, D. 2007. Love and Sex with Robots: The Evo-lution of Human?Robot Relationships. London: Duckworth. Luneski, A., Moore, R. K., & Bamidis, P. D. (2008). Affective computing and collaborative networks: to-wards emotion-aware interaction. In L. M. Cama-rinha-Matos & W. Picard (Eds.), Pervasive Collabora-tive Networks (Vol. 283, pp. 315-322). Boston: Springer. Marsella, S. and Gratch, J. (2003) Modeling Coping Behavior in Virtual Humans: Don't Worry, Be Happy. 2nd Int Conf on Autonomous Agents and Multiagent Systems (AAMAS), Melbourne, Australia, July 2003. Reeves, B., Nass, C. 1996, The media equation: how people treat computers, television, and new media like real people and places, Cambridge: Cambridge Uni-versity Press, 1996. Scherer, S., Schwenker, F. and Palm, G. 2008. Emo-tion recognition from speech using multi-classifier systems and rbf-ensembles, in Speech, Audio, Image and Biomedical Signal Processing using Neural Net-works, pp. 49?70, Springer: Berlin.  Wallis, P., Mitchard, H., O?Dea, D., and Das J. 2001, Dialogue modelling for a conversational agent. In ?AI-2001: Advances in Artificial Intelligence?, Stumptner, Corbett, and Brooks, (eds.), In Proceed-ings 14th Australian Joint Conference on Artificial Intelligence, Adelaide, Australia. Wiebe, J., Wilson , T., and Cardie, C.  2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, vol-ume 39, issue 2-3, pp. 165-210. Wilks, Y. (ed.) (2010) Artificial Companions in Soci-ety: scientific, economic, psychological and philoso-phical perspectives. John Benjamins: Amsterdam. Wundt, W., 1913. Grundriss der Psychologie, A. Kroner: Berlin. Zue, V., Glass, J., Goddeau, D., Goodine, D., Hirschman, L.  1992. The MIT ATIS system, In Proc. Workshop on speech and natural language, Harriman, New York. 
18
Proceedings of the First Workshop on Metaphor in NLP, pages 36?44,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
 
 
Automatic Metaphor Detection using Large-Scale Lexical Resources and Conventional Metaphor Extraction   Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton Florida Institute for Human and Machine Cognition 15, SE Osceola Ave Ocala, FL, 34471, USA {ywilks,lgalescu,jallen,adalton}@ihmc.us     Abstract The paper presents an experimental algorithm to detect conventionalized metaphors implicit in the lexical data in a resource like WordNet, where metaphors are coded into the senses and so would never be detected by any algorithm based on the violation of preferences, since there would always be a constraint satisfied by such senses. We report an implementation of this algorithm, which was implemented first the preference constraints in VerbNet. We then derived in a systematic way a far more extensive set of constraints based on WordNet glosses, and with this data we reimplemented the detec-tion algorithm and got a substantial improvement in recall. We suggest that this technique could contribute to improve the performance of existing metaphor detec-tion strategies that do not attempt to detect convention-alized metaphors. The new WordNet-derived data is of wider significance because it also contains adjective constraints, unlike any existing lexical resource, and can be applied to any language with a semantic parser (and WN) for it. 1 Introduction Metaphor is ubiquitous in standard language; it is not a fringe or add-on phenomenon. The work de-scribed concerns detecting and interpreting meta-phor on a large scale in corpora. If metaphor is ubiquitous, then locating and interpreting it must be central to any NLP project that aims to under-stand general language. This paper focuses on the initial phase of detection: the identification in text of conceptual combinations that might be deemed metaphoric by a pre-theoretic observer, e.g., ?Bra-zil has economic muscle?, ?Tom is a brick?, or ?The unions have built a fortress round their pen-sions?.  There is a long cultural tradition of de-
scribing and interpreting such phenomena but our goal here is computational: to provide criteria for automatically detecting such cases as candidates for further analysis and interpretation. The key fact is that metaphors are sometimes new and fresh but can be immediately understood: producing them is often the role of poets, creative journalists and writers of all kinds. But many are simply part of the history of the language, and are novel only to those who do not happen to know them already: for example ?Tom is a brick? ? taken to mean that he is a reliable man, but which cannot be literally true ? is actually encoded as a sense of brick in WordNet (WN) (Miller, 1995) even though it is more familiar to UK than US English speakers. This means that lexical resources already con-tain conventionalized metaphors. We propose a simple method for locating and extracting these into the metaphor candidate pool, even when they are not indicated as such in resources like WN (which marks figurative senses very infrequently, unlike some traditional dictionaries). However, we believe these implicit metaphors in WN ? a re-source we intend to use as a semantic/lexical data-base, though transformed as we shall show below ? can be extracted by a simple algorithm, and with-out any need for a priori distinction of literal ver-sus metaphorical.  That distinction, as we noted, depends to a large degree on the temporal snapshot of a language; e.g., no one now would think ?tak-ing a decision? was metaphor, even though deci-sions are not literally taken anywhere. In this paper, we shall present an algorithm for conventionalized metaphor detection, and show results over a standard corpus of examples that 
36
  
demonstrate a possible useful gain in recall of metaphors, our original aim. The algorithm is de-scribed in two implementations (or pipelines) cor-responding, respectively, to the use of WN and VerbNet (Kipper et al, 2000; Kipper et al, 2008) as semantic knowledge-bases, and to their re-placement by our automatically recomputed form of WN, which enables predictions about the pref-erence behavior (see below) of English verbs and adjectives to be better founded than in VerbNet (VN) and on a much larger scale. 2 Background on Metaphor Detection us-ing Preference Violation as Cue In early work on metaphor detection, long preced-ing access to large-scale or annotated corpora, it was suggested as sufficient a criterion for being a metaphor that a ?semantic preference? of a verb or adjective was violated (Wilks, 1978). So, for ex-ample, one might say that the verb drink had a preference for animate agents and liquid objects, in which case ?My car drinks gasoline? violates its subject preference, which might then be a cue to look for metaphor at that point. Similarly, in the ?economic muscle? case mentioned earlier one might say that economic has a preference for ab-stract entities as objects, as in ?economic value?, and muscle is not an abstract entity.  There was discussion in those early days of syntac-tic-semantic interface cases like ?John ran a mile? where a mile might be said to violate the prefer-ence of the (intransitive) verb for a zero object and so again trigger a metaphor. The preference notion was not initially intended to detect metaphor but to semantically disambiguate candidates at those sites by preferring those conceptual entities that did not violate such restrictions. In early work, preferences were largely derived by intuition and sometimes ordered by salience. Later (e.g. Resnik, 1997) there was a range of work on deriving such preferences from corpora; however, in VN the semantic prefer-ences of verbs were again largely intuitive in ori-gin.      Early work linking preference violation to metaphor detection (summarised in Fass and Wilks, 1983, also Martin 1990) worked with hand-crafted resources, but by 1995 Dolan had noted (Dolan, 1995) that large-scale lexical resources would have implications for metaphor detection, and WN was used in conjunction with corpora, by 
(Peters and Wilks, 2003) using symbolic methods and by Mason (2004) and Krishnakumaran and Zhu (2007) using a combination of WN and statis-tical methods. Mason also acquires preferences automatically from corpora, and the latter two pa-pers treat metaphor as a form of anomaly based on rare combinations of surface words and of WN-derived hypernyms, a notion that appears in (Guthrie et al, 2007) but based only on corpus sparsity and not WN codings. Other work on the automatic acquisition of preferences (McCarthy and Carrol, 2003) for WSD has also its considered extension to the detection of classes of metaphor. More recently, work by Shutova (Shutova et al, 2010) has shown that the original preference viola-tion insight can be combined with large-scale in-vestigations, using notions of machine learning and large-scale resources like WN. Our approach is smaller scale and does not involve machine learn-ing: it simply seeks access to implicit metaphors built into the structure of WN by its creators, and which a preference-violation detection criterion cannot, by definition, access. Thus, we view our contribution as complementary to larger efforts on metaphor and interpretation detection, rather than a competing approach. We have not made compari-sons here with the work of (Li and Sporleder, 2010), which is explicitly concerned with idioms, nor with (Markert and Nissim, 2009) which is fo-cused on metonymy. 3 The Conventional Metaphor Detection Hypotheses Where WN codes conventionalized metaphors as senses, as in the initial cases described, then the senses expressing these will NOT violate prefer-ences and so will not be detected by any metaphor-as-violation hypothesis. For example, in ?Jane married a brick? this will not be a preference vio-lation against WN senses because WN explicitly codes brick as a reliable person, though we would almost certainly want to say this sentence contains a metaphor to be detected. The hypothesis we propose is simply this: if we have a word whose main (usually first) sense in WN fails the main preference for the sentence slot it fills, but has a lower, less frequent, sense that satisfies that preference, then we declare that lower sense a metaphorical one. In the case of brick, whose main sense is a PHYSICAL OBJECT, one 
37
  
which clearly fails the equivalence to Tom in the example ?Tom is a brick?. Yet the less frequent listed sense for a reliable person does satisfy the same preference. The work at this stage is not con-cerned with the metaphor-metonymy distinction and this criterion may well capture both, their dis-tinction being, as is well known (e.g. in Fass and Wilks, 1983) hard to establish in the limit. Ours is a purely empirical hypothesis and will work or not, and we argue that it does to a reasonable degree. It does not rest on any assumption of strict ordering of WN senses, only on a tendency (from literal to metaphorical) which is plainly there for any ob-server. 4 Metaphor Detection Experiments We have implemented two versions of conven-tional metaphor detection, using two different lexi-cal resources. We were thus able to divide the hypothesis into two parts, essentially one making use of VN and one within WN only.  In this first pipeline, we use WN together with the verb prefer-ences provided by VN even though those give only patchy coverage of common verbs. At the outset this was the only lexical resource for verb prefer-ences available. VN includes classes of verbs that map members to specific WN senses. VN also provides a hierarchy of verb object/subject inclu-sions, which we use for assessing whether one sen-tence object/subject type appears below another in this simple inclusion hierarchy, and so can be said to be semantically included in it. The selectional restrictions, however, are not linked to any lexi-cons so a mapping was constructed in order to al-low for automated detection of preference violations.  Our first experiment utilizes WN, VN, and the Stanford Parser (de Marneffe et al, 2006) and Named Entity Recognizer (Finkel et al, 2005).  The Stanford Parser identifies the verbs, as well as their corresponding subjects and direct objects. The Stanford Named Entity Recognizer was used to replace sequences of text representing names with WN senses whose hypernyms exist in the se-lectional restriction hierarchy. The first step in determining whether a sentence contains a metaphor is to extract all verbs along with the subject and direct object arguments for each verb.  The Stanford Parser dependencies used to describe the relationships between verbs and 
their arguments include agent, nsubj, and xsubj for subjects and dobj and nsubjpass for direct objects.  The parser also handles copular and prepositional verbs but additional steps are required to link these verbs to their arguments. Once verbs have been extracted and parameter-ized from the sentence, each is checked for prefer-ence violations. A preference is violated if a selectional restriction on one of the thematic roles of a VN class is not satisfied for all VN classes the verb is a member of.  In order for a VN class's preferences to be satisfied, there must be a WN sense for the argument of a verb such that either itself or its hypernym matches the WN senses al-lowed by the selectional restriction in VN class, where the terms in the VN hierarchy have been hand-matched to WN senses. If a sentence contains a verb that does not exist in VN then we must as-sume that it is not violated. 5 Conventionalized Metaphor Detection Closer inspection of false negatives revealed that many of the verbs and the arguments that satisfied their selectional restrictions were unannotated con-ventionalized metaphors.   5.1 Conventionalized Verbs In our approach, a conventionalized verb occurs when two VN Classes have the same member, but one maps to a lower WN sense (in the WN order-ing, which can be taken roughly to mean less fre-quent) than the other.  If the VN Class mapped to the lower sense is satisfied in a sentence, but the other VN Class is not, we say that the verb is used in a conventionalized sense. The verb pour  is a member of four VN classes.  Three of those classes, Pour-9.5, Preparing-26.3-2, and Sub-stance_Emission-43.4 all map to first sense of the word which means to cause to run.  The fourth VN class of pour, Weather-57, maps to the sixth WN sense of the verb, which means to rain heavily.  If we take the example sentence ?Bisciotti has poured money into the team?, we determine that all VN classes that map to the primary WN sense of pour are violated in some way. According to our semantic role labeling heuristic, Pour-9.5 expects money to be a substance, Preparing-26.3-2 ex-pects the team to be an animate, and Sub-stance_Emission-43.4 is violated because Bisciotti is animate.  The only Verb Class that is satisfied is 
38
  
Weather-57, and that class maps to the sixth sense of pour.  Interestingly, there is no VN class mem-ber that maps to the fifth WN sense (supply in large amounts or quantities). The pseudocode for detecting conventional metaphors used as verbs is as follows: ? for each VN Class ? for each member of that class ? for each WN sense of that member with Verb POS ? get the sense number of the WN sense ? associate the sense number to the verb member and selectional re-strictions for the Verb Class ? given a verb in a sentence, decide that the verb is conventionalized if: ? it satisfies the selectional re-strictions of one Verb Class V1 but? ? it violates the selectional restric-tions of another Verb Class V2 and?  ? the sense number of the verb member in V2 is above the sense number of the verb member in V1  5.2 Conventionalized Nouns Let us look again at the example of brick, where the primary sense of the noun is the building mate-rial most are familiar with and the secondary sense refers to a reliable person. For this reason, the noun brick will satisfy any VN class that requires a hu-
man or animate. Without the ability to detect con-ventional metaphors in noun arguments, She married a brick would pass through without detec-tion by preference violation. Here are the WN en-tries for the two senses: ? brick#1 (brick%1:06:00::) (rectangular block of clay baked by the sun or in a kiln; used as a build-ing or paving material)  ? brick#2 (brick%1:18:00::) (a good fellow; helpful and trustworthy)  Less obvious are more abstract words such as zone: ? zone#1 (zone%1:15:00::) (a locally circumscribed place characterized by some distinctive features) ? zone#2 (zone%1:15:02::), geographical zone#1 (geographical_zone%1:15:00::) (any of the re-gions of the surface of the Earth loosely divided ac-cording to latitude or longitude)  ? zone#3 (zone%1:15:01::) (an area or region dis-tinguished from adjacent parts by a distinctive fea-ture or characteristic)  ? zone#4 (zone%1:08:00::), zona#1 (zona%1:08: 00::) ((anatomy) any encircling or beltlike struc-ture)  Zone's primary sense, again, is the anticipated con-cept of circumscribed space. However, the fourth sense deals with anatomy, and therefore is a hypo-nym of body part.  Body part is capable of satisfy-ing any thematic role restricted to animate arguments.  
 Figure 1. Conventionalized verb metaphor detection using WordNet senses  and VerbNet selectional restrictions 
VerbNet WordNet
Parser
Named Entity 
Recognizer
Interface
Metaphor 
Detector
Extract verbs 
and arguments
Is sentence 
a metaphor?
Replace named enitties
Get WordNet hypernym sets for arguments
Find all VerbNet Classes for each verb
Which WordNet senses 
satisfy Selectional 
Restrictions
[None]
Sentence contains 
a metaphor
[One or more]
Set of senses that satisfy selectional restrictions
Does the member of 
the Verb Classes 
satisfied map to the 
primary sense?
[Yes]
No metaphor
[No]
Conventionalized Metaphor
39
  
The pseudocode for detecting conventional metaphors used as nouns is as follows: ? determine if verbs? subjects and di-rect objects satisfy the restriction ? if not, it is a Preference Violation metaphor ? if they do: ? determine if the sense of the sat-isfying word is the primary sense in WN ? if not, it is a conventional metaphor ? otherwise, it is not a metaphor Thus, our overall hypothesis is intended to locate in the very broad WN sense sets those that are ac-tually conventionalized metaphors: we determine that only the first sense, hopefully literal, should be able to satisfy any restriction.  If a lower sense sat-isfies a verb, but the primary sense does not, we classify the satisfaction as being conventionalized, but a metaphor nonetheless.  6 Deriving Preferences and an Ontology from WordNet To date, VerbNet is the most extensive resource for verb roles and restrictions. It provides a rich semantic role taxonomy with some selectional re-strictions. Still, VN has entries for less than 4000 verbs. PropBank (Palmer et al, 2005) has addi-
tional coverage, but uses a more surface oriented role set with no selectional restrictions. On the other hand, WordNet has many more verb entries but they lack semantic role information. However, we believe it is possible to extract automatically a comprehensive lexicon of verbs with semantic roles and selectional restrictions from WN by processing definitions in WN using deep under-standing techniques. Specifically, each verb in WN comes with a gloss that defines the verb sense, and there we can find clues about the semantic roles and their selectional restrictions. Thus, we are test-ing the hypothesis that the semantic roles of the verb being defined are inherited from the roles in its definition, though roles in the latter may be elided or fully specified. For example, consider this entry from WN for one of the senses of the verb kill: S: (v) kill (cause to die; put to death, usually inten-tionally or knowingly) ?This man killed several people when he tried to rob the bank?; ?the farmer killed a pig for the holidays? Let us assume we already know that the verb cause takes three roles, say, a CAUSER, an AFFECTED and an EFFECT role; this leads us to hypothesize that kill would take the same roles. However, the EFFECT role from cause is not inherited by kill as it is fully specified in the definition. The proof of 
  Figure 2. Conventionalized noun metaphor detection using WordNet senses  and VerbNet selectional restrictions 
40
  
this hypothesis is ultimately in how well it predicts the role set. But intuitively, any role in the defini-tion verb (i.e., cause) that is fully filled in the defi-nition has no ?space? for a new argument for that role. Therefore, we conclude that kill takes two roles, filling the CAUSER and AFFECTED roles in the definition. We can now derive selectional restrictions for kill by looking at inherited restrictions from the definition, as well as those that can be derived from the examples. From the definition, the verb cause puts little to no restriction on what the CAUSER role might be. For instance, an animal may cause something, but natural forces cause things as well. Likewise, cause puts little con-straint on what the PATIENT role might be, as one can cause the temperature to rise, or an idea to fade. The restriction from the verb die in the com-plement, however, suggests a restriction of some living object (if we can derive this constraint from die).  We also look at the examples to find more informative restrictions. In the definition of kill, we have two examples of a CAUSER, namely a man and a farmer. Given the hypernym hierarchy of nouns in WordNet, we could look for the most specific subsuming concept in the hierarchy for the concepts MAN and FARMER, finding it to be person%1:03:00.  The fillers for the AFFECTED role in the examples are PEOPLE and PIG, with the most specific WN node being organ-ism%1:03:00). Putting all this together, we pro-duce an entry for kill as follows: kill:  ACTOR/person%1:03:00  PATIENT/organism%1:03:00 To implement this idea we need a number of capa-bilities. First, semantic roles do not appear out of the ether, so we need an initial seed of semantic 
role information. In addition, to process the glosses we need a parser that can build a semantic repre-sentation, including the handling of elided argu-ments. As a start, we use the TRIPS parser (Allen et al, 2008). The TRIPS lexicon provides informa-tion on semantic roles, and the parser can construct the required semantic structures. TRIPS has been shown to be successful at parsing WN glosses in order to build commonsense knowledge bases (Al-len et al, 2011). With around 3000 types, TRIPS offers a reasonable upper-level ontology to serve as the seed for semantic roles. We also use the TRIPS selectional restrictions to bootstrap the process of determining the restrictions for new words. To attain broad lexical coverage, the TRIPS parser uses input from a variety of external re-sources. This includes a subsystem, Wordfinder, for unknown word lookup that accesses WN when an unknown word is encountered. The WN senses have mappings to semantic types in the TRIPS on-tology, although sometimes at a fairly abstract level. When faced with an unknown word, the parser looks up the possible senses in WordNet, maps these to the TRIPS ontology and then uses the verb entries in the TRIPS lexicon associated with these types to suggest possible subcatgoriza-tion frames with mappings to roles. Thus, Word-finder uses the combined information from WN and the TRIPS lexicon and ontology to dynami-cally build lexical entries with approximate seman-tic and syntactic structures for words not in the core lexicon. This process may produce a range of different possibilities based on the different senses and possible subcategorization frames for the verbs that share the same TRIPS type. We feed all of these to the parser and let it determine the entries that best match the definition and examples. While WordNet may have multiple fine-grained senses for a given word, we set a parameter that has the system use only the most frequent sense(s) of the word (cf. McCarthy et al 2004). We use TRIPS to parse the definitions and glosses into a logical form. Figure 3 shows the logical form produced for the definition cause to die. We then search the logical form for structures that signal a potential argument that would fill a role. Besides looking for gaps, we found some other devices that serve the same purpose and oc-cur frequently in WordNet: 
  Figure 3: Abstracted Logical Form for ?cause to die? 
(F CAUSE-MAKE)
(IMPRO LSUBJ)
(IMPRO DOBJ)
(F DIE)
CAUSE
AFFECTED
EFFECT
EXPERIENCER
41
  
? elided arguments (an IMPRO in the logical form); ? indefinite pronouns (e.g., something, some-one); ? prepositional/adverbial forms containing an IMPRO or an indefinite pronoun (e.g., give a benediction to); ? a noun phrase in parentheses (e.g., to re-move (people) from a building). The final condition is probably a WN specific de-vice, and was discovered when working on a 10-verb development set, and occurred twice in that set. Once these arguments are identified, we have a candidate set of roles for the verb. We identify candidate selectional restrictions as described above. Here are a few examples of verbs and their automatically derived roles and restrictions, as computed by our system (here we indicate Word-Net entries by their sense index rather than their sense key, since the index is used in the conven-tional metaphor detection strategy ? see below): bend.v.06: AGENT/being.n.02     PATIENT/physical_entity.n.01 collect.v.03: AGENT /person.n.01     PATIENT/object.n.01 drive.v.01:  AGENT/person.n.01     PATIENT/motor_vehicle.n.01 play.v.13: CAUSE/instrumentality.n.03     EFFECT/music.n.01 walk.v.08: AGENT/being.n.02     GOAL/location.n.01 The techniques described in this section have been used to provide a set of roles with selectional re-strictions for the second IHMC pipeline, described below. The current system takes a list of verbs from a corpus and returns the role names and se-lectional restrictions for every sense of those words in WordNet. The transformations described here all equally able to produce preferences for adjectives, as would be needed to detect ?economic muscle? as a metaphor, which is a form of lexical information not present in any existing database, and the whole process can be applied to any language that pos-sesses a WordNet type lexical resource, and for which we have a capable semantic parser. Hence, these techniques are amenable to being used for detecting metaphorical usage in constructions other 
than just verb-subject and verb-object, as we do here. 7 Conventional Metaphor Detection based on WordNet-Derived Preferences The preferences and ontology derived from WN definitions greatly improve the mapping between selectional restrictions and WN sense keys.  This allows us to replace VN with a new lexical re-source that both improves performance, and re-duces the complexity of discovering preference violations.  In the new pipeline, we can reuse the capabilities developed to extract verbs and their parameters from a sentence.  We also reuse the tie-ins to WN that allow us to determine if one WN sense exists within another's hypernym set. It is the selectional restriction lookup that is greatly simpli-fied in the new lexicon, where verbs are mapped directly to WN senses. The conventional metaphor detection is also simplified because the WN senses are included in the responses to the looked up verbs, allowing us to quickly determine if a satis-fied verb is conventionalized or is satisfied with conventionalized arguments. 8 Results and Conclusion Figure 4 shows the results obtained in a metaphor detection task over a small corpus of 122 sen-tences. Half of these sentences have metaphors and half do not. Of the half that do, approximately half are metaphors about Governance and half are other metaphors. This is not any sort of principled cor-pus but a seed set chosen to give an initial leverage and in a domain chosen by the sponsor (Govern-ance); the selection and implicit annotation were 
	 ? Pipeline	 ?1	 ?(VerbNet	 ?SRs)	 ? Pipeline	 ?2	 ?(WordNet	 ?SRs)	 ?TP 24 50 FP 23 37 TN 48 24 FN 37 11 Precision 0.649 0.575 Recall 0.393 0.82 F1 0.49 0.676  Figure 4. Performance comparison between the first pipeline using VerbNet selectional restrictions (SRs) and the second pipeline using WordNet-derived se-lectional restrictions 
42
  
done by consensus by a large group of twenty or so collaborators. The notion of baseline is irrelevant here, since the choice for every sentence is simply whether it contains a metaphor or not, and could thus be said to be 50% on random assignment of those categories.     From the figures above, it can be seen that the second pipeline does give significant improvement of recall over the first implementation above, even though there is some loss of precision, probably because of the loss of the information in VN. One possibility for integrating a conventional metaphor extraction pipeline like ours with a general meta-phor detection pipeline (including, for example, pattern-based methods and top-down recognition from stored Conceptual Metaphors) would be to OR these two pipelines together and to hope to gain the benefits of both, taking anything as a metaphor that was deemed one by either. However, that is not our aim here: our purpose is only to test the hypothesis that using knowledge derived from existing lexical resources, in combi-nation with some form of the conventionalized metaphor hypothesis, we can achieve good recall performance. On this point we think we have shown the value of the technique. Acknowledgements This work was supported in part by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense US Army Research Labo-ratory contract number W911NF-12-C-0020, and NSF grant IIS 1012205. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copy-right annotation thereon.  Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessar-ily representing the official policies or endorse-ments, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.  References James Allen, William de Beaumont, Nate Blaylock, George Ferguson, Jansen Orfan, and Mary Swift. 2011. Acquiring commonsense knowledge for a cog-nitive agent. In Proceedings of the AAAI Fall Sympo-sium on Advances in Cognitive Systems (ACS 2011), Arlington, Virginia. 
James Allen, Mary Swift, and Will de Beaumont. 2008. Deep semantic analysis of text. In Proceedings of the 2008 Conference on Semantics in Text Processing (STEP '08), Venice, Italy. pp. 343-354. Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pp. 449-454. William B. Dolan. 1995. Metaphor as an emergent property of machine-readable dictionaries. In Pro-ceedings of the AAAI 1995 Spring Symposium Series: Representation and Acquisition of Lexical Knowl-edge: Polysemy, Ambiguity and Generativity, pp. 27?32. Dan Fass and Yorick Wilks. 1983. Preference seman-tics, ill-formedness, and metaphor. American Journal of Computational Linguistics, 9(3):178?187. Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sam-pling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. David Guthrie, Louise Guthrie, Ben Allison and Yorick Wilks. 2007. Unsupervised Anomaly Detection. In Proceedings of the 20th international joint confer-ence on Artifical intelligence (IJCAI'07), San Fran-cisco, CA, pp. 1624-1628. Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of the 17th National Conference on Arti-ficial Intelligence, Austin, Texas. pp. 691-696. Karin Kipper, Anna Korhonen, Neville Ryant, and Mar-tha Palmer. 2008. A large-scale classification of Eng-lish verbs. Language Resources and Evaluation 42(1):21-40. Saisuresh Krishnakumaran and Xiaojin Zhu, 2007. Hunting Elusive Metaphors Using Lexical Re-sources, Proceedings of the Workshop on Computa-tional Approaches to Figurative Language, pp. 13-20.  Linlin Li and Caroline Sporleder. 2010. Linguistic Cues for Distinguishing Literal and Non-Literal Usage. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Beijing, China, pp.  683-691. Katia Markert and  Nissim Malvina. 2009. Data and Models for Metonymy Resolution. In Language Re-sources and Evaluation, 43(2):123-138. James Martin. 1990. A Computational Model of Meta-phor Interpretation. Academic Press. Zachary J. Mason. 2004. Cormet: A computational, cor-pus-based conventional metaphor extraction sys- tem. Computational Linguistics, 30(1):23?44. 
43
  
Diana McCarthy and John Carrol. 2003. Disambiguat-ing nouns, verbs and adjectives using automatically acquired selectional preferences. Computational Lin-guistics. 29(4): 639-654. Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguis-tics (ACL '04), Barcelona, Spain. pp. 280-287. George Miller. 1995. Wordnet: A lexical database for English. Communications of the ACM, 38(11):39-41. Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Se-mantic Roles. Computational Linguistics, 31(1):71-106. Wim Peters and Yorick Wilks. 2003. Data-Driven De-tection of Figurative Language Use in Electronic Language Resources, Metaphor and Symbol, 18(3): 161-174. Philip Resnik, 1997. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What and How?, Washington, DC, pp. 52-57. Ekaterina Shutova, Li-ping Sun and Anna Korhonen. 2010. Metaphor Identification Using Verb and Noun Clustering. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Beijing, China, pp. 1002-1010. Yorick Wilks, 1978. Making Preferences More Active. Artificial Intelligence, 11(3):197-223.  
44
