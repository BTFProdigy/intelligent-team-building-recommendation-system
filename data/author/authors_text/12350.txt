Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 91?95,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A Graph-Theoretic Algorithm for Automatic Extension of Translation
Lexicons
Beate Dorow Florian Laws Lukas Michelbacher Christian Scheible Jason Utt
Institute for Natural Language Processing
Universita?t Stuttgart
{dorowbe,lawsfn,michells,scheibcn,uttjn}@ims.uni-stuttgart.de
Abstract
This paper presents a graph-theoretic
approach to the identification of yet-
unknown word translations. The proposed
algorithm is based on the recursive Sim-
Rank algorithm and relies on the intuition
that two words are similar if they estab-
lish similar grammatical relationships with
similar other words. We also present a for-
mulation of SimRank in matrix form and
extensions for edge weights, edge labels
and multiple graphs.
1 Introduction
This paper describes a cross-linguistic experiment
which attempts to extend a given translation dic-
tionary with translations of novel words.
In our experiment, we use an English and
a German text corpus and represent each cor-
pus as a graph whose nodes are words and
whose edges represent grammatical relationships
between words. The corpora need not be parallel.
Our intuition is that a node in the English and a
node in the German graph are similar (that is, are
likely to be translations of one another), if their
neighboring nodes are. Figure 1 shows part of the
English and the German word graph.
Many of the (first and higher order) neighbors
of food and Lebensmittel translate to one another
(marked by dotted lines), indicating that food and
Lebensmittel, too, are likely mutual translations.
Our hypothesis yields a recursive algorithm for
computing node similarities based on the simi-
larities of the nodes they are connected to. We
initialize the node similarities using an English-
German dictionary whose entries correspond to
known pairs of equivalent nodes (words). These
node equivalences constitute the ?seeds? from
which novel English-German node (word) corre-
spondences are bootstrapped.
We are not aware of any previous work using a
measure of similarity between nodes in graphs for
cross-lingual lexicon acquisition.
Our approach is appealing in that it is language
independent, easily implemented and visualized,
and readily generalized to other types of data.
Section 2 is dedicated to related research on
the automatic extension of translation lexicons. In
Section 3 we review SimRank (Jeh and Widom,
2002), an algorithm for computing similarities of
nodes in a graph, which forms the basis of our
work. We provide a formulation of SimRank in
terms of simple matrix operations which allows
an efficient implementation using optimized ma-
trix packages. We further present a generalization
of SimRank to edge-weighted and edge-labeled
graphs and to inter-graph node comparison.
Section 4 describes the process used for build-
ing the word graphs. Section 5 presents an experi-
ment for evaluating our approach to bilingual lex-
icon acquisition. Section 6 reports the results. We
present our conclusions and directions for future
research in Section 7.
2 Related Work on cross-lingual lexical
acquisition
The work by Rapp (1999) is driven by the idea
that a word and its translation to another lan-
guage are likely to co-occur with similar words.
Given a German and an English corpus, he com-
putes two word-by-word co-occurrence matrices,
one for each language, whose columns span a vec-
tor space representing the corresponding corpus.
In order to find the English translation of a Ger-
man word, he uses a base dictionary to translate
all known column labels to English. This yields
a new vector representation of the German word
in the English vector space. This mapped vector
is then compared to all English word vectors, the
most similar ones being candidate translations.
91
food Lebensmittel
receive erhalten
award Preis
provide liefern
evidence Beweis
buy kaufen
book Buch
publish verlegen
boat Haus
waste ablehnen
Figure 1: Likely translations based on neighboring nodes
Rapp reports an accuracy of 72% for a small
number of test words with well-defined meaning.
Diab and Finch (2000) first compute word sim-
ilarities within each language corpus separately
by comparing their co-occurrence vectors. Their
challenge then is to derive a mapping from one
language to the other (i.e. a translation lexicon)
which best preserves the intra-language word sim-
ilarities. The mapping is initialized with a few seed
?translations? (punctuation marks) which are as-
sumed to be common to both corpora.
They test their method on two corpora written
in the same language and report accuracy rates of
over 90% on this pseudo-translation task. The ap-
proach is attractive in that it does not require a
seed lexicon. A drawback is its high computational
cost.
Koehn and Knight (2002) use a (linear) com-
bination of clues for bootstrapping an English-
German noun translation dictionary. In addition to
similar assumptions as above, they consider words
to be likely translations of one another if they have
the same or similar spelling and/or occur with sim-
ilar frequencies. Koehn and Knight reach an accu-
racy of 39% on a test set consisting of the 1,000
most frequent English and German nouns. The
experiment excludes verbs whose semantics are
more complex than those of nouns.
Otero and Campos (2005) extract English-
Spanish pairs of lexico-syntactic patterns from a
small parallel corpus. They then construct con-
text vectors for all English and Spanish words by
recording their frequency of occurrence in each of
these patterns. English and Spanish vectors thus
reside in the same vector space and are readily
compared.
The approach reaches an accuracy of 89% on a
test set consisting of 100 randomly chosen words
from among those with a frequency of 100 or
higher. The authors do not report results for low-
frequency words.
3 The SimRank algorithm
An algorithm for computing similarities of nodes
in graphs is the SimRank algorithm (Jeh and
Widom, 2002). It was originally proposed for di-
rected unweighted graphs of web pages (nodes)
and hyperlinks (links).
The idea of SimRank is to recursively com-
pute node similarity scores based on the scores
of neighboring nodes. The similarity Sij of two
different nodes i and j in a graph is defined as
the normalized sum of the pairwise similarities of
their neighbors:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Skl. (1)
N(i) and N(j) are the set of i?s and j?s neigh-
bors respectively, and c is a multiplicative factor
smaller than but close to 1 which demotes the con-
tribution of higher order neighbors. Sij is set to 1
if i and j are identical, which provides a basis for
the recursion.
3.1 Matrix formulation of SimRank
We derive a formulation of the SimRank similarity
updates which merely consists of matrix multipli-
cations as follows. In terms of the graph?s (binary)
adjacency matrix A, the SimRank recursion reads:
Sij =
c
|N(i)| |N(j)|
?
k?N(i),l?N(j)
Aik Ajl Skl
(2)
noting that AikAjl = 1, iff k is a neighbor of i
and l is a neighbor of j at the same time. This is
92
equivalent to
Sij = c
?
k,l
Aik
|N(i)|
Ajl
|N(j)| Skl (3)
= c
?
k,l
Aik
?
? Ai?
Ajl
?
? Aj?
Skl.
The Sij can be assembled in a square node sim-
ilarity matrix S, and it is easy to see that the indi-
vidual similarity updates can be summarized as:
Sk = c A? Sk?1A?T (4)
where A? is the row-normalized adjacency matrix
and k denotes the current level of recursion. A? is
obtained by dividing each entry of A by the sum of
the entries in its row. The SimRank iteration is ini-
tialized with S = I , and the diagonal of S, which
contains the node self-similarities, is reset to ones
after each iteration.
This representation of SimRank in closed ma-
trix form allows the use of optimized off-the-shelf
sparse matrix packages for the implementation of
the algorithm. This rendered the pruning strate-
gies proposed in the original paper unnecessary.
We also note that the Bipartite SimRank algorithm
introduced in (Jeh and Widom, 2002) is just a spe-
cial case of Equation 4.
3.2 Extension with weights and link types
The SimRank algorithm assumes an unweighted
graph, i.e. a binary adjacency matrix A. Equa-
tion 4 can equally be used to compute similarities
in a weighted graph by letting A? be the graph?s
row-normalized weighted adjacency matrix. The
entries of A? then represent transition probabili-
ties between nodes rather than hard (binary) adja-
cency. The proof of the existence and uniqueness
of a solution to this more general recursion pro-
ceeds in analogy to the proof given in the original
paper.
Furthermore, we allow the links in the graph to
be of different types and define the following gen-
eralized SimRank recursion, where T is the set of
link types and Nt(i) denotes the set of nodes con-
nected to node i via a link of type t.
Sij =
c
|T |
?
t?T
1
|Nt(i)| |Nt(j)|
?
k?Nt(i),l?Nt(j)
Skl.
(5)
In matrix formulation:
Sk =
c
|T |
?
t?T
A?t Sk?1A?t
T (6)
where At is the adjacency matrix associated with
link type t and, again, may be weighted.
3.3 SimRank across graphs
SimRank was originally designed for the com-
parison of nodes within a single graph. However,
SimRank is readily and accordingly applied to
the comparison of nodes of two different graphs.
The original SimRank algorithm starts off with the
nodes? self-similarities which propagate to other
non-identical pairs of nodes. In the case of two dif-
ferent graphs A and B, we can instead initialize the
algorithm with a set of initially known node-node
correspondences.
The original SimRank equation (2) then be-
comes
Sij =
c
|N(i)| |N(j)|
?
k,l
Aik Bjl Skl, (7)
which is equivalent to
Sk = c A? Sk?1 B?T , (8)
or, if links are typed,
Sk =
c
|T |
?
t?T
A?t Sk?1 B?t
T . (9)
The similarity matrix S is now a rectangular
matrix containing the similarities between nodes
in A and nodes in B. Those entries of S which
correspond to known node-node correspondences
are reset to 1 after each iteration.
4 The graph model
The grammatical relationships were extracted
from the British National Corpus (BNC) (100 mil-
lion words), and the Huge German Corpus (HGC)
(180 million words of newspaper text). We com-
piled a list of English verb-object (V-O) pairs
based on the verb-argument information extracted
by (Schulte im Walde, 1998) from the BNC. The
German V-O pairs were extracted from a syntactic
analysis of the HGC carried out using the BitPar
parser (Schmid, 2004).
We used only V-O pairs because they consti-
tute far more sense-discriminative contexts than,
for example, verb-subject pairs, but we plan to ex-
amine these and other grammatical relationships
in future work.
We reduced English compound nouns to their
heads and lemmatized all data. In English phrasal
93
English German
Low Mid High Low Mid High
N V N V N V N V N V N V
0.313 0.228 0.253 0.288 0.253 0.255 0.232 0.247 0.205 0.237 0.211 0.205
Table 1: The 12 categories of test words, with mean relative ranks of test words
verbs, we attach the particles to the verbs to dis-
tinguish them from the original verb (e.g put off
vs. put). Both the English and German V-O pairs
were filtered using stop lists consisting of modal
and auxiliary verbs as well as pronouns. To reduce
noise, we decided to keep only those relationships
which occurred at least three times in the respec-
tive corpus.
The English and German data alike are then rep-
resented as a bipartite graph whose nodes divide
into two sets, verbs and nouns, and whose edges
are the V-O relationships which connect verbs to
nouns (cf. Figure 1). The edges of the graph are
weighted by frequency of occurrence.
We ?prune? both the English and German graph
by recursively removing all leaf nodes (nodes with
a single neighbor). As these correspond to words
which appear only in a single relationship, there is
only limited evidence of their meaning.
After pruning, there are 4,926 nodes (3,365
nouns, 1,561 verbs) and 43,762 links in the En-
glish, and 3,074 nodes (2,207 nouns, 867 verbs)
and 15,386 links in the German word graph.
5 Evaluation experiment
The aim of our evaluation experiment is to test
the extended SimRank algorithm for its ability to
identify novel word translations given the English
and German word graph of the previous section
and an English-German seed lexicon. We use the
dict.cc English-German dictionary 1.
Our evaluation strategy is as follows. We se-
lect a set of test words at random from among the
words listed in the dictionary, and remove their en-
tries from the dictionary. We run six iterations of
SimRank using the remaining dictionary entries
as the seed translations (the known node equiv-
alences), and record the similarities of each test
word to its known translations. As in the original
SimRank paper, c is set to 0.8.
We include both English and German test words
and let them vary in frequency: high- (> 100),
1http://www.dict.cc/ (May 5th 2008)
mid- (> 20 and ? 100), and low- (? 20) fre-
quent as well as word class (noun, verb). Thus, we
obtain 12 categories of test words (summarized in
Table 1), each of which is filled with 50 randomly
selected words, giving a total of 600 test words.
SimRank returns a matrix of English-German
node-node similarities. Given a test word, we ex-
tract its row from the similarity matrix and sort the
corresponding words by their similarities to the
test word. We then scan this sorted list of words
and their similarities for the test word?s reference
translations (those listed in the original dictionary)
and record their positions (i.e. ranks) in this list.
We then replace absolute ranks with relative ranks
by dividing by the total number of candidate trans-
lations.
6 Results
Table 1 lists the mean relative rank of the reference
translations for each of the test categories. The
values of around 0.2-0.3 clearly indicate that our
approach ranks the reference translations much
higher than a random process would.
Relative rank
Fr
eq
ue
nc
y
0.0 0.2 0.4 0.6 0.8 1.0
0
5
15
25
Figure 2: Distribution of the relative ranks of the
reference translations in the English-High-N test
set.
Exemplary of all test sets, Figure 2 shows the
distribution of the relative ranks of the reference
translations for the test words in English-High-N.
The bulk of the distribution lies below 0.3, i.e. in
the top 30% of the candidate list.
In order to give the reader an idea of the results,
we present some examples of test words and their
94
Test word Top 10 predicted translations Ranks
sanction Ausgangssperre Wirtschaftssanktion
Ausnahmezustand Embargo Moratorium
Sanktion Todesurteil Geldstrafe Bu?geld
Anmeldung
Sanktion(6)
Ma?nahme(1407)
delay anfechten revidieren zuru?ckstellen
fu?llen verku?nden quittieren vertagen
verschieben aufheben respektieren
verzo?gern(78)
aufhalten(712)
Kosten hallmark trouser blouse makup uniform
armour robe testimony witness jumper
cost(285)
o?ffnen unlock lock usher step peer shut guard
hurry slam close
open(12)
undo(481)
Table 2: Some examples of test words, their pre-
dicted translations, and the ranks of their true
translations.
predicted translations in Table 2.
Most of the 10 top-ranked candidate transla-
tions of sanction are hyponyms of the correct
translations. This is mainly due to insufficient
noun compound analysis. Both the English and
German nouns in our graph model are single
words. Whereas the English nouns consist only of
head nouns, the German nouns include many com-
pounds (as they are written without spaces), and
thus tend to be more specific.
Some of the top candidate translations of de-
lay are correct (verschieben) or at least acceptable
(vertagen), but do not count as such as they are
missing in the gold standard dictionary.
The mistranslation of the German noun Kosten
is due to semantic ambiguity. Kosten co-occurs of-
ten with the verb tragen as in to bear costs. The
verb tragen however is ambiguous and may as
well be translated as to wear which is strongly as-
sociated with clothes.
We find several antonyms of o?ffnen among its
top predicted translations. Verb-object relation-
ships alone do not suffice to distinguish synonyms
from antonyms. Similarly, it is extremely difficult
to differentiate between the members of closed
categories (e.g. the days of the week, months of
the year, mass and time units) using only syntactic
relationships.
7 Conclusions and Future Research
The matrix formulation of the SimRank algorithm
given in this paper allows an implementation using
efficient off-the-shelf software libraries for matrix
computation.
We presented an extension of the SimRank
algorithm to edge-weighted and edge-labeled
graphs. We further generalized the SimRank equa-
tions to permit the comparison of nodes from two
different graphs, and proposed an application to
bilingual lexicon induction.
Our system is not yet accurate enough to be
used for actual compilation of translation dictio-
naries. We further need to address the problem of
data sparsity. In particular, we need to remove the
bias towards low-degree words whose similarities
to other words are unduly high.
In order to solve the problem of ambiguity, we
intend to apply SimRank to the incidence repre-
sentation of the word graphs, which is constructed
by putting a node on each link. The proposed al-
gorithm will then naturally return similarities be-
tween the more sense-discriminative links (syn-
tactic relationships) in addition to similarities be-
tween the often ambiguous nodes (isolated words).
References
M. Diab and S. Finch. 2000. A statistical word-
level translation model for comparable corpora. In
In Proceedings of the Conference on Content-Based
Multimedia Information Access (RIAO).
G. Jeh and J. Widom. 2002. Simrank: A measure of
structural-context similarity. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 538?543.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of the ACL-02 Workshop on Unsupervised Lexical
Acquisition, pages 9?16.
P. Gamallo Otero and J. Ramon Pichel Campos. 2005.
An approach to acquire word translations from non-
parallel texts. In EPIA, pages 600?610.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 519?526.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
COLING ?04: Proceedings of the 20th International
Conference on Computational Linguistics, page 162.
Sabine Schulte im Walde. 1998. Automatic Se-
mantic Classification of Verbs According to Their
Alternation Behaviour. Master?s thesis, Insti-
tut fu?r Maschinelle Sprachverarbeitung, Universita?t
Stuttgart.
95
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 151?160,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Regular polysemy: A distributional model
Gemma Boleda
Dept. of Linguistics
University of Texas at Austin
gemma.boleda@upf.edu
Sebastian Pado?
ICL
University of Heidelberg
pado@cl.uni-heidelberg.de
Jason Utt
IMS
University of Stuttgart
uttjn@ims.uni-stuttgart.de
Abstract
Many types of polysemy are not word specific,
but are instances of general sense alternations
such as ANIMAL-FOOD. Despite their perva-
siveness, regular alternations have been mostly
ignored in empirical computational semantics.
This paper presents (a) a general framework
which grounds sense alternations in corpus
data, generalizes them above individual words,
and allows the prediction of alternations for
new words; and (b) a concrete unsupervised
implementation of the framework, the Cen-
troid Attribute Model. We evaluate this model
against a set of 2,400 ambiguous words and
demonstrate that it outperforms two baselines.
1 Introduction
One of the biggest challenges in computational se-
mantics is the fact that many words are polysemous.
For instance, lamb can refer to an animal (as in The
lamb squeezed through the gap) or to a food item (as
in Sue had lamb for lunch). Polysemy is pervasive
in human language and is a problem in almost all
applications of NLP, ranging from Machine Trans-
lation (as word senses can translate differently) to
Textual Entailment (as most lexical entailments are
sense-specific).
The field has thus devoted a large amount of effort
to the representation and modeling of word senses.
The arguably most prominent effort is Word Sense
Disambiguation, WSD (Navigli, 2009), an in-vitro
task whose goal is to identify which, of a set of pre-
defined senses, is the one used in a given context.
In work on WSD and other tasks related to pol-
ysemy, such as word sense induction, sense alter-
nations are treated as word-specific. As a result, a
model for the meaning of lamb that accounts for the
relation between the animal and food senses cannot
predict that the same relation holds between instances
of chicken or salmon in the same type of contexts.
A large number of studies in linguistics and cog-
nitive science show evidence that there are regulari-
ties in the way words vary in their meaning (Apres-
jan, 1974; Lakoff and Johnson, 1980; Copestake
and Briscoe, 1995; Pustejovsky, 1995; Gentner et
al., 2001; Murphy, 2002), due to general analogical
processes such as regular polysemy, metonymy and
metaphor. Most work in theoretical linguistics has
focused on regular, systematic, or logical polysemy,
which accounts for alternations like ANIMAL-FOOD.
Sense alternations also arise from metaphorical use
of words, as dark in dark glass-dark mood, and also
from metonymy when, for instance, using the name
of a place for a representative (as in Germany signed
the treatise). Disregarding this evidence is empiri-
cally inadequate and leads to the well-known lexical
bottleneck of current word sense models, which have
serious problems in achieving high coverage (Navigli,
2009).
We believe that empirical computational semantics
could profit from a model of polysemy1 which (a) is
applicable across individual words, and thus capable
of capturing general patterns and generalizing to new
1Our work is mostly inspired in research on regular polysemy.
However, given the fuzzy nature of ?regularity? in meaning
variation, we extend the focus of our attention to include other
types of analogical sense construction processes.
151
words, and (b) is induced in an unsupervised fashion
from corpus data. This is a long-term goal with many
unsolved subproblems.
The current paper presents two contributions to-
wards this goal. First, since we are working on a
relatively unexplored area, we introduce a formal
framework that can encompass different approaches
(Section 2). Second, we implement a concrete instan-
tiation of this framework, the unsupervised Centroid
Attribute Model (Section 3), and evaluate it on a new
task, namely, to detect which of a set of words in-
stantiate a given type of polysemy (Sections 4 and 5).
We finish with some conclusions and future work
(Section 7).
2 Formal framework
In addition to introducing formal definitions for terms
commonly found in the literature, our framework pro-
vides novel terminology to deal with regular poly-
semy in a general fashion (cf. Table 1; capital letters
designate sets and small letters elements of sets).2
For a lemma l like lamb, we want to know
how well a meta alternation (such as ANIMAL-
FOOD) explains a pair of its senses (such as the
animal and food senses of lamb).3 This is for-
malized through the function score, which maps
a meta alternation and two senses onto a score.
As an example, let lambanm denote the ANIMAL
sense of lamb, lambfod the FOOD sense, and
lambhum the PERSON sense. Then, an appropri-
ate model of meta alternations should predict that
score(animal,food, lambanm, lambfod) is greater
than score(animal,food, lambanm, lambhum).
Meta alternations are defined as unordered pairs
of meta senses, or cross-word senses like ANIMAL.
The meta sensesM can be defined a priori or induced
from data. They are equivalence classes of senses to
which they are linked through the function meta. A
sense s instantiates a meta sense m iff meta(s) =
m. Functions inst and sns allow us to define meta
senses and lemma-specific senses in terms of actual
instances, or occurrences of words in context.
2We re-use inst as a function that returns the set of instances
for a sense: SL ? ?(IL) and assume that senses partition
lemmas? instances: ?l : inst(l) =
?
s?sns(l) inst(s).
3Consistent with the theoretical literature, this paper focuses
on two-way polysemy. See Section 7 for further discussion.
L set of lemmas
IL set of (lemma-wise) instances
SL set of (lemma-wise) senses
inst : L? ?(IL) mapping lemma? instances
sns : L? ?(SL) mapping lemma? senses
M set of meta senses
meta: SL ?M mapping senses?meta senses
A ?M ?M set of meta alternations (MAs)
A set of MA representations
score : A? S2L ? R scoring function for MAs
repA : A? A MA representation function
comp: A?S2L ? R compatibility function
Table 1: Notation and signatures for our framework.
We decompose the score function into two parts:
a representation function repA that maps a meta al-
ternation into some suitable representation for meta
alternations, A, and a compatibility function comp
that compares the relation between the senses of a
word to the meta alternation?s representation. Thus,
comp ? repA = score.
3 The Centroid Attribute Model
The Centroid Attribute Model (CAM) is a simple
instantiation of the framework defined in Section 2,
designed with two primary goals in mind. First, it is
a data-driven model. Second, it does not require any
manual sense disambiguation, a notorious bottleneck.
To achieve the first goal, CAM uses a distribu-
tional approach. It represents the relevant entities as
co-occurrence vectors that can be acquired from a
large corpus (Turney and Pantel, 2010). To achieve
the second goal, CAM represents meta senses using
monosemous words only, that is, words whose senses
all correspond to one meta sense. 4 Examples are
cattle and robin for the meta sense ANIMAL. We
define the vector for a meta sense as the centroid (av-
erage vector) of the monosemous words instantiating
it. In turn, meta alternations are represented by the
centroids of their meta senses? vectors.
This strategy is not applicable to test lemmas,
which instantiate some meta alternation and are by
definition ambiguous. To deal with these without
410.8% of noun types in the corpus we use are monosemous
and 2.3% are disemous, while, on a token level, 23.3% are
monosemous and 20.2% disemous.
152
vecI : IL ? Rk instance vector computation
C : Rk?m ? Rk centroid computation
vecL : L? Rk lemma (type) vector computation
repM : M ? Rk meta sense representation
Table 3: Additional notation and signatures for CAM
explicit sense disambiguation, CAM represents lem-
mas by their type vectors, i.e., the centroid of their
instances, and compares their vectors (attributes) to
those of the meta alternation ? hence the name.
CoreLex: A Semantic Inventory. CAM uses
CoreLex (Buitelaar, 1998) as its meta sense inven-
tory. CoreLex is a lexical resource that was designed
specifically for the study of polysemy. It builds on
WordNet (Fellbaum, 1998), whose sense distinctions
are too fine-grained to describe general sense al-
ternations. CoreLex defines a layer of abstraction
above WordNet consisting of 39 basic types, coarse-
grained ontological classes (Table 2). These classes
are linked to one or more Wordnet anchor nodes,
which define a mapping from WordNet synsets onto
basic types: A synset s maps onto a basic type b if b
has an anchor node that dominates s and there is no
other anchor node on the path from b and s.5
We adopt the WordNet synsets as S, the set of
senses, and the CoreLex basic types as our set of
meta senses M . The meta function (mapping word
senses onto meta senses) is given directly by the an-
chor mapping defined in the previous paragraph. This
means that the set of meta alternations is given by the
set of pairs of basic types. Although basic types do
not perfectly model meta senses, they constitute an
approximation that allows us to model many promi-
nent alternations such as ANIMAL-FOOD.
Vectors for Meta Senses and Alternations. All
representations used by CAM are co-occurrence vec-
tors in Rk (i.e., A := Rk). Table 3 lists new concepts
that CAM introduces to manipulate vector represen-
tations. vecI returns a vector for a lemma instance,
vecL a (type) vector for a lemma, and C the centroid
of a set of vectors.
We leave vecI and C unspecified: we will experi-
ment with these functions in Section 4. CAM does fix
5This is necessary because some classes have non-disjoint
anchor nodes: e.g., ANIMALs are a subset of LIVING BEINGs.
the definitions for vecL and repA. First, vecL defines
a lemma?s vector as the centroid of its instances:
vecL(l) = C{vecI(i) | i ? inst(l)} (1)
Before defining repA, we specify a function repM
that computes vector representations for meta senses
m. In CAM, this vector is defined as the centroid
of the vectors for all monosemous lemmas whose
WordNet sense maps onto m:
repM(m) = C{vecL(l) | meta(sns(l)) = {m}} (2)
Now, repA can be defined simply as the centroid of
the meta senses instantiating a:
repA(m1,m2) = C{repM(m1), repM(m2)} (3)
Predicting Meta Alternations. The final compo-
nent of CAM is an instantiation of comp (cf. Table 1),
i.e., the degree to which a sense pair (s1, s2) matches
a meta alternation a. Since CAM does not represent
these senses separately, we define comp as
comp(a, s1, s2) = sim(a, vecL(l))
so that {s1, s2} = sns(l)
(4)
The complete model, score, can now be stated as:
score(m,m?, s, s?) = sim(repA(m,m
?), vecL(l))
so that {s, s?} = sns(l) (5)
CAM thus assesses how well a meta alternation
a = (m,m?) explains a lemma l by comparing the
centroid of the meta senses m,m? to l?s centroid.
Discussion. The central feature of CAM is that
it avoids word sense disambiguation, although it
still relies on a predefined sense inventory (Word-
Net, through CoreLex). Our use of monosemous
words to represent meta senses and meta alternations
goes beyond previous work which uses monosemous
words to disambiguate polysemous words in context
(Izquierdo et al, 2009; Navigli and Velardi, 2005).
Because of its focus on avoiding disambiguation,
CAM simplifies the representation of meta alterna-
tions and polysemous words to single centroid vec-
tors. In the future, we plan to induce word senses
(Schu?tze, 1998; Pantel and Lin, 2002; Reisinger and
Mooney, 2010), which will allow for more flexible
and realistic models.
153
abs ABSTRACTION ent ENTITY loc LOCATION prt PART
act ACT evt EVENT log GEO. LOCATION psy PSYCHOL. FEATURE
agt AGENT fod FOOD mea MEASURE qud DEFINITE QUANTITY
anm ANIMAL frm FORM mic MICROORGANISM qui INDEFINITE QUANTITY
art ARTIFACT grb BIOLOG. GROUP nat NATURAL BODY rel RELATION
atr ATTRIBUTE grp GROUPING phm PHENOMENON spc SPACE
cel CELL grs SOCIAL GROUP pho PHYSICAL OBJECT sta STATE
chm CHEMICAL hum HUMAN plt PLANT sub SUBSTANCE
com COMMUNICATION lfr LIVING BEING pos POSSESSION tme TIME
con CONSEQUENCE lme LINEAR MEASURE pro PROCESS pro PROCESS
Table 2: CoreLex?s basic types with their corresponding WordNet anchors. CAM adopts these as meta senses.
4 Evaluation
We test CAM on the task of identifying which lem-
mas of a given set instantiate a specific meta alterna-
tion. We let the model rank the lemmas through the
score function (cf. Table (1) and Eq. (5)) and evaluate
the ranked list using Average Precision. While an
alternative would be to rank meta alternations for a
given polysemous lemma, the method chosen here
has the benefit of providing data on the performance
of individual meta senses and meta alternations.
4.1 Data
All modeling and data extraction was carried out on
the written part of the British National Corpus (BNC;
Burnage and Dunlop (1992)) parsed with the C&C
tools (Clark and Curran, 2007). 6
For the evaluation, we focus on disemous words,
words which instantiate exactly two meta senses
according to WordNet. For each meta alternation
(m,m?), we evaluate CAM on a set of disemous tar-
gets (lemmas that instantiate (m,m?)) and disemous
distractors (lemmas that do not). We define three
types of distractors: (1) distractors sharing m with
the targets (but not m?), (2) distractors sharing m?
with the targets (but not m), and (3) distractors shar-
ing neither. In this way, we ensure that CAM cannot
obtain good results by merely modeling the similarity
of targets to either m or m?, which would rather be a
coarse-grained word sense modeling task.
To ensure that we have enough data, we evaluate
CAM on all meta alternations with at least ten targets
that occur at least 50 times in the corpus, discarding
nouns that have fewer than 3 characters or contain
non-alphabetical characters. The distractors are cho-
6The C&C tools were able to reliably parse about 40M words.
sen so that they match targets in frequency. This
leaves us with 60 meta alternations, shown in Ta-
ble 5. For each meta alternation, we randomly select
40 lemmas as experimental items (10 targets and 10
distractors of each type) so that a total of 2,400 lem-
mas is used in the evaluation.7 Table 4 shows four
targets and their distractors for the meta alternation
ANIMAL-FOOD.8
4.2 Evaluation Measure and Baselines
To measure success on this task, we use Average
Precision (AP), an evaluation measure from IR that
reaches its maximum value of 1 when all correct
items are ranked at the top (Manning et al, 2008).
It interpolates the precision values of the top-n pre-
diction lists for all positions n in the list that con-
tain a target. Let T = ?q1, . . . , qm? be the list of
targets, and let P = ?p1, . . . , pn? be the list of pre-
dictions as ranked by the model. Let I(xi) = 1 if
pi ? T , and zero otherwise. Then AP (P, T ) =
1
m
?m
i=1 I(xi)
?i
j=1 I(xi)
i . AP measures the quality
of the ranked list for a single meta alternation. The
overall quality of a model is given by Mean Average
Precision (MAP), the mean of the AP values for all
meta alternations.
We consider two baselines: (1) A random baseline
that ranks all lemmas in random order. This baseline
is the same for all meta alternations, since the distri-
bution is identical. We estimate it by sampling. (2)
A meta alternation-specific frequency baseline which
orders the lemmas by their corpus frequencies. This
7Dataset available at http://www.nlpado.de/
?sebastian/data.shtml.
8Note that this experimental design avoids any overlap be-
tween the words used to construct sense vectors (one meta sense)
and the words used in the evaluation (two meta senses).
154
Targets Distractors with meta sense anm Distractors with meta sense fod Random distractors
carp amphibian (anm-art) mousse (art-fod) appropriation (act-mea)
duckling ape (anm-hum) parsley (fod-plt) scissors (act-art)
eel leopard (anm-sub) pickle (fod-sta) showman (agt-hum)
hare lizard (anm-hum) pork (fod-mea) upholstery (act-art)
Table 4: Sample of experimental items for the meta alternation anm-fod. (Abbreviations are listed in Table 2.)
baseline uses the intuition that frequent words will
tend to exhibit more typical alternations.
4.3 Model Parameters
There are four more parameters to set.
Definition of vector space. We instantiate the vecI
function in three ways. All three are based on
dependency-parsed spaces, following our intuition
that topical similarity as provided by window-based
spaces is insufficient for this task. The functions dif-
fer in the definition of the space?s dimensions, incor-
porating different assumptions about distributional
differences among meta alternations.
The first option, gram, uses grammatical paths
of lengths 1 to 3 as dimensions and thus character-
izes lemmas and meta senses in terms of their gram-
matical context (Schulte im Walde, 2006), with a
total of 2,528 paths. The second option, lex, uses
words as dimensions, treating the dependency parse
as a co-occurrence filter (Pado? and Lapata, 2007),
and captures topical distinctions. The third option,
gramlex, uses lexicalized dependency paths like
obj?see to mirror more fine-grained semantic proper-
ties (Grefenstette, 1994). Both lex and gramlex
use the 10,000 most frequent items in the corpus.
Vector elements. We use ?raw? corpus co-
occurrence frequencies as well as log-likelihood-
transformed counts (Lowe, 2001) as elements of the
co-occurrence vectors.
Definition of centroid computation. There are
three centroid computations in CAM: to combine
instances into lemma (type) vectors (function vecL
in Eq. (1)); to combine lemma vectors into meta
sense vectors (function repM in Eq. (2)); and to com-
bine meta sense vectors into meta alternation vectors
(function repA in Eq. (3)).
For vecL, the obvious definition of the centroid
function is as a micro-average, that is, a simple av-
erage over all instances. For repM and repA, there
is a design choice: The centroid can be computed
by micro-averaging as well, which assigns a larger
weight to more frequent lemmas (repM) or meta
senses (repA). Alternatively, it can be computed
by macro-averaging, that is, by normalizing the in-
dividual vectors before averaging. This gives equal
weight to the each lemma or meta sense, respectively.
Macro-averaging in repA thus assumes that senses
are equally distributed, which is an oversimplifica-
tion, as word senses are known to present skewed
distributions (McCarthy et al, 2004) and vectors for
words with a predominant sense will be similar to the
dominant meta sense vector. Micro-averaging par-
tially models sense skewedness under the assumption
that word frequency correlates with sense frequency.
Similarity measure. As the vector similarity mea-
sure in Eq. (5), we use the standard cosine similar-
ity (Lee, 1999). It ranges between ?1 and 1, with 1
denoting maximum similarity. In the current model
where the vectors do not contain negative counts, the
range is [0; 1].
5 Results
Effect of Parameters The four parameters of Sec-
tion 4.3 (three space types, macro-/micro-averaging
for repM and repA, and log-likelihood transforma-
tion) correspond to 24 instantiations of CAM.
Figure 1 shows the influence of the four parame-
ters. The only significant difference is tied to the use
of lexicalized vector spaces (gramlex / lex are
better than gram). The statistical significance of this
difference was verified by a t-test (p < 0.01). This
indicates that meta alternations can be characterized
better through fine-grained semantic distinctions than
by syntactic ones.
The choice of micro- vs. macro-average does not
have a clear effect, and the large variation observed
in Figure 1 suggests that the best setup is dependent
on the specific meta sense or meta alternation being
155
MACRO MICRO0
.3
5
0.
37
0.
39
repM
MACRO MICRO0
.3
5
0.
37
0.
39
repA
gram gramlex lex0
.3
5
0.
37
0.
39
space type
?
False True0
.3
5
0.
37
0.
39
LL transformation
Figure 1: Effect of model parameters on performance. A
data point is the mean AP (MAP) across all meta alterna-
tions for a specific setting.
modeled. Focusing on meta alternations, whether the
two intervening meta senses should be balanced or
not can be expected to depend on the frequencies of
the concepts denoted by each meta sense, which vary
for each case. Indeed, for AGENT-HUMAN, the alter-
nation which most benefits from the micro-averaging
setting, the targets are much more similar to the HU-
MAN meta sense (which is approximately 8 times as
frequent as AGENT) than to the AGENT meta sense.
The latter contains anything that can have an effect on
something, e.g. emulsifier, force, valium. The targets
for AGENT-HUMAN, in contrast, contain words such
as engineer, manipulator, operative, which alternate
between an agentive role played by a person and the
person herself.
While lacking in clear improvement, log-
likelihood transformation tends to reduce variance,
consistent with the effect previously found in selec-
tional preference modeling (Erk et al, 2010).
Overall Performance Although the performance
of the CAM models is still far from perfect, all 24
models obtain MAP scores of 0.35 or above, while
the random baseline is at 0.313, and the overall fre-
quency baseline at 0.291. Thus, all models con-
sistently outperform both baselines. A bootstrap
resampling test (Efron and Tibshirani, 1994) con-
firmed that the difference to the frequency baseline
is significant at p < 0.01 for all 24 models. The
difference to the random baseline is significant at
p < 0.01 for 23 models and at p < 0.05 for the
remaining model. This shows that the models cap-
ture the meta alternations to some extent. The best
model uses macro-averaging for repM and repA in
a log-likelihood transformed gramlex space and
achieves a MAP of 0.399.
Table 5 breaks down the performance of the best
CAM model by meta alternation. It shows an en-
couraging picture: CAM outperforms the frequency
baseline for 49 of the 60 meta alternations and both
baselines for 44 (73.3%) of all alternations. The per-
formance shows a high degree of variance, however,
ranging from 0.22 to 0.71.
Analysis by Meta Alternation Coherence Meta
alternations vary greatly in their difficulty. Since
CAM is an attribute similarity-based approach, we
expect it to perform better on the alternations whose
meta senses are ontologically more similar. We next
test this hypothesis.
Let Dmi = {dij} be the set of distractors for
the targets T = {tj} that share the meta sense mi,
and DR = {d3j} the set of random distractors. We
define the coherence ? of an alternation a of meta
senses m1,m2 as the mean (?) difference between
the similarity of each target vector to a and the simi-
larity of the corresponding distractors to a, or for-
mally ?(a) = ? sim(repA(m1,m2), vecL(tj)) ?
sim(repA(m1,m2), vecL(dij)), for 1 ? i ? 3 and
1 ? j ? 10. That is, ? measures how much more
similar, on average, the meta alternation vector is to
the target vectors than to the distractor vectors. For a
meta alternation with a higher ?, the targets should
be easier to distinguish from the distractors.
Figure 2 plots AP by ? for all meta alternations.
As we expect from the definition of ?, AP is strongly
correlated with ?. However, there is a marked Y
shape, i.e., a divergence in behavior between high-
? and mid-AP alternations (upper right corner) and
mid-? and high-AP alternations (upper left corner).
In the first case, meta alternations perform worse
than expected, and we find that this typically points
to missing senses, that is, problems in the underlying
lexical resource (WordNet, via CoreLex). For in-
stance, the FOOD-PLANT distractor almond is given
156
grs-psy 0.709 com-evt 0.501 art-com 0.400 atr-com 0.361 art-frm 0.286
pro-sta 0.678 art-grs 0.498 act-pos 0.396 atr-sta 0.361 act-hum 0.281
fod-plt 0.645 hum-psy 0.486 phm-sta 0.388 act-phm 0.339 art-fod 0.280
psy-sta 0.630 hum-nat 0.456 atr-psy 0.384 anm-art 0.335 grs-hum 0.272
hum-prt 0.602 anm-hum 0.448 fod-hum 0.383 art-atr 0.333 act-art 0.267
grp-psy 0.574 com-psy 0.443 plt-sub 0.383 act-psy 0.333 art-grp 0.258
grs-log 0.573 act-grs 0.441 act-com 0.382 agt-hum 0.319 art-nat 0.248
act-evt 0.539 atr-rel 0.440 grp-grs 0.379 art-evt 0.314 act-atr 0.246
evt-psy 0.526 art-qui 0.433 art-psy 0.373 atr-evt 0.312 art-hum 0.240
act-tme 0.523 act-sta 0.413 art-prt 0.364 art-sta 0.302 art-loc 0.238
art-pho 0.520 art-sub 0.412 evt-sta 0.364 act-grp 0.296 art-pos 0.228
act-pro 0.513 art-log 0.407 anm-fod 0.361 com-hum 0.292 com-sta 0.219
Table 5: Meta alternations and their average precision values for the task. The random baseline performs at 0.313 while
the frequency baseline ranges from 0.255 to 0.369 with a mean of 0.291. Alternations for which the model outperforms
the frequency baseline are in boldface (mean AP: 0.399, standard deviation: 0.119).
grs-psy democracy, faculty, humanism, regime,
pro-sta bondage, dehydration, erosion,urbanization
psy-sta anaemia,delight, pathology, sensibility
hum-prt bum, contractor, peter, subordinate
grp-psy category, collectivism, socialism, underworld
Table 6: Sample targets for meta alternations with high
AP and mid-coherence values.
a PLANT sense by WordNet, but no FOOD sense. In
the case of SOCIAL GROUP-GEOGRAPHICAL LOCA-
TION, distractors laboratory and province are miss-
ing SOCIAL GROUP senses, which they clearly pos-
sess (cf. The whole laboratory celebrated Christmas).
This suggests that our approach can help in Word
Sense Induction and thesaurus construction.
In the second case, meta alternations perform bet-
ter than expected: They have a low ?, but a high
AP. These include grs-psy, pro-sta, psy-sta,
hum-prt and grp-psy. These meta alternations
involve fairly abstract meta senses such as PSYCHO-
LOGICAL FEATURE and STATE.9 Table 6 lists a
sample of targets for the five meta alternations in-
volved. The targets are clearly similar to each other
on the level of their meta senses. However, they can
occur in very different semantic contexts. Thus, here
it is the underlying model (the gramlex space) that
can explain the lower than average coherence. It is
striking that CAM can account for abstract words and
meta alternations between these, given that it uses
first-order co-occurrence information only.
9An exception is hum-prt. It has a low coherence because
many WordNet lemmas with a PART sense are body parts.
0.00 0.05 0.10 0.15 0.20 0.250.
2
0.3
0.4
0.5
0.6
0.7
coherence
AP
act?artact?atr
act?com
act?evt
act?grp
act?grs
act?hum
act?phm
act?pos
act?pro
sy
act?sta
act?tme
agt?humanm?art
anm?fod
anm?hum
art?at
art?com
art?evt
art?fodart?frmart?grp
art?grs
art?humart?loc
art?log
r nat
art?pho
art?pos
art?prtart?psy
art?qui
r sta
art?sub
atr?com
atr?evt
a r?psy
atr?rel
sta
com?evt
com?hum
com psy
com?sta
evt?psy
ev ?stafod hum
fod?plt
grp?grs
grp?psy
grs?hum
grs?log
grs?psy
hum?nat
hum?prt
hum?psy
phm?staplt?sub
pro?sta
psy?sta
Figure 2: Average Precision and Coherence (?) for each
meta alternation. Correlation: r = 0.743 (p < 0.001)
6 Related work
As noted in Section 1, there is little work in empiri-
cal computational semantics on explicitly modeling
sense alternations, although the notions that we have
formalized here affect several tasks across NLP sub-
fields.
Most work on regular sense alternations has fo-
cused on regular polysemy. A pioneering study is
Buitelaar (1998), who accounts for regular polysemy
through the CoreLex resource (cf. Section 3). A
similar effort is carried out by Tomuro (2001), but
he represents regular polysemy at the level of senses.
Recently, Utt and Pado? (2011) explore the differences
between between idiosyncratic and regular polysemy
patterns building on CoreLex. Lapata (2000) focuses
157
on the default meaning arising from word combina-
tions, as opposed to the polysemy of single words as
in this study.
Meta alternations other than regular polysemy,
such as metonymy, play a crucial role in Informa-
tion Extraction. For instance, the meta alternation
SOCIAL GROUP-GEOGRAPHICAL LOCATION cor-
responds to an ambiguity between the LOCATION-
ORGANIZATION Named Entity classes which is
known to be a hard problem in Named Entity Recog-
nition and Classification (Markert and Nissim, 2009).
Metaphorical meta alternations have also received
attention recently (Turney et al, 2011)
On a structural level, the prediction of meta al-
ternations shows a clear correspondence to analogy
prediction as approached in Turney (2006) (carpen-
ter:wood is analogous to mason:stone, but not to
photograph:camera). The framework defined in Sec-
tion 2 conceptualizes our task in a way parallel to that
of analogical reasoning, modeling not ?first-order?
semantic similarity, but ?second-order? semantic re-
lations. However, the two tasks cannot be approached
with the same methods, as Turney?s model relies on
contexts linking two nouns in corpus sentences (what
does A do to B?). In contrast, we are interested in
relations within words, namely between word senses.
We cannot expect two different senses of the same
noun to co-occur in the same sentence, as this is dis-
couraged for pragmatic reasons (Gale et al, 1992).
A concept analogous to our notion of meta sense
(i.e., senses beyond single words) has been used in
previous work on class-based WSD (Yarowsky, 1992;
Curran, 2005; Izquierdo et al, 2009), and indeed,
the CAM might be used for class-based WSD as
well. However, our emphasis lies rather on modeling
polysemy across words (meta alternations), some-
thing that is absent in WSD, class-based or not. The
only exception, to our knowledge, is Ando (2006),
who pools the labeled examples for all words from a
dataset for learning, implicitly exploiting regularities
in sense alternations.
Meta senses also bear a close resemblance to the
notion of semantic class as used in lexical acqui-
sition (Hindle, 1990; Merlo and Stevenson, 2001;
Schulte im Walde, 2006; Joanis et al, 2008). How-
ever, in most of this research polysemy is ignored.
A few exceptions use soft clustering for multiple as-
signment of verbs to semantic classes (Pereira et al,
1993; Rooth et al, 1999; Korhonen et al, 2003),
and Boleda et al (to appear) explicitly model regular
polysemy for adjectives.
7 Conclusions and Future Work
We have argued that modeling regular polysemy and
other analogical processes will help improve current
models of word meaning in empirical computational
semantics. We have presented a formal framework
to represent and operate with regular sense alterna-
tions, as well as a first simple instantiation of the
framework. We have conducted an evaluation of dif-
ferent implementations of this model in the new task
of determining whether words match a given sense
alternation. All models significantly outperform the
baselines when considered as a whole, and the best
implementation outperforms the baselines for 73.3%
of the tested alternations.
We have two next steps in mind. The first is to
become independent of WordNet by unsupervised
induction of (meta) senses and alternations from the
data. This will allow for models that, unlike CAM,
can go beyond ?disemous? words. Other improve-
ments on the model and evaluation will be to develop
more informed baselines that capture semantic shifts,
as well as to test alternate weighting schemes for the
co-occurrence vectors (e.g. PMI) and to use larger
corpora than the BNC.
The second step is to go beyond the limited in-vitro
evaluation we have presented here by integrating al-
ternation prediction into larger NLP tasks. Knowl-
edge about alternations can play an important role in
counteracting sparseness in many tasks that involve
semantic compatibility, e.g., testing the applicability
of lexical inference rules (Szpektor et al, 2008).
Acknowledgements
This research is partially funded by the Spanish Min-
istry of Science and Innovation (FFI2010-15006,
TIN2009-14715-C04-04), the AGAUR (2010 BP-
A00070), the German Research Foundation (SFB
732), and the EU (PASCAL2; FP7-ICT-216886). It
is largely inspired on a course by Ann Copestake at
U. Pompeu Fabra (2008). We thank Marco Baroni,
Katrin Erk, and the reviewers of this and four other
conferences for valuable feedback.
158
References
Rie Kubota Ando. 2006. Applying alternating structure
optimization to word sense disambiguation. In Proceed-
ings of the 10th Conference on Computational Natural
Language Learning, pages 77?84, New York City, NY.
Iurii Derenikovich Apresjan. 1974. Regular polysemy.
Linguistics, 142:5?32.
Gemma Boleda, Sabine Schulte im Walde, and Toni Badia.
to appear. Modeling regular polysemy: A study of the
semantic classification of Catalan adjectives. Computa-
tional Linguistics.
Paul Buitelaar. 1998. CoreLex: An ontology of sys-
tematic polysemous classes. In Proceedings of For-
mal Ontologies in Information Systems, pages 221?235,
Amsterdam, The Netherlands.
Gavin Burnage and Dominic Dunlop. 1992. Encoding
the British National Corpus. In Jan Aarts, Pieter de
Haan, and Nelleke Oostdijk, editors, English Language
Corpora: Design, Analysis and Exploitation, Papers
from the Thirteenth International Conference on En-
glish Language Research on Computerized Corpora.
Rodopi, Amsterdam.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-l
inear models. Computational Linguistics, 33(4).
Ann Copestake and Ted Briscoe. 1995. Semi-productive
Polysemy and Sense Extension. Journal of Semantics,
12(1):15?67.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of the
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 26?33, Ann Arbor,
Michigan.
Bradley Efron and Robert Tibshirani. 1994. An Introduc-
tion to the Bootstrap. Monographs on Statistics and
Applied Probability 57. Chapman & Hall.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT, London.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Proceed-
ings of the 1992 ARPA Human Language Technologies
Workshop, pages 233?237, Harriman, NY.
Dedre Gentner, Brian F. Bowdle, Phillip Wolff, and Con-
suelo Boronat. 2001. Metaphor is like analogy. In
D. Gentner, K. J. Holyoak, and B. N. Kokinov, edi-
tors, The analogical mind: Perspectives from Cognitive
Science, pages 199?253. MIT Press, Cambridge, MA.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Donald Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of the 28th Meet-
ing of the Association for Computational Linguistics,
pages 268?275.
Rube?n Izquierdo, Armando Sua?rez, and German Rigau.
2009. An empirical study on class-based word sense
disambiguation. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 389?397, Athens, Greece.
Eric Joanis, Suzanne Stevenson, and David James. 2008.
A general feature space for automatic verb classifica-
tion. Natural Language Engineering, 14(03):337?367.
Anna Korhonen, Yuval Krymolowski, and Zvika Marx.
2003. Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 64?71.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press.
Mirella Lapata. 2000. The Acquisition and Modeling
of Lexical Knowledge: A Corpus-based Investigation
of Systematic Polysemy. Ph.D. thesis, University of
Edinburgh.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 25?32,
College Park, MA.
Will Lowe. 2001. Towards a theory of semantic space. In
Proceedings of the 23rd Annual Meeting of the Cogni-
tive Science Society, pages 576?581, Edinburgh, UK.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK,
1st edition.
Katja Markert and Malvina Nissim. 2009. Data and
models for metonymy resolution. Language Resources
and Evaluation, 43(2):123?138.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Using automatically acquired predominant
senses for word sense disambiguation. In Proceedings
of the ACL SENSEVAL-3 workshop, pages 151?154.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions
of argument structure. Computational Linguistics,
27(3):373?408.
Gregory L. Murphy. 2002. The Big Book of Concepts.
MIT Press, Cambridge, MA.
Roberto Navigli and Paola Velardi. 2005. Structural se-
mantic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 27(7):1075?
1086, July.
159
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Computing Surveys, 41:10:1?10:69,
February.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Mining
2002, pages 613?619, Edmonton.
Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
Proceedings of the 31st Meeting of the Association for
Computational Linguistics, pages 183?190, Columbus,
OH.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge, MA.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of the 11th Annual Conference of the North
American Chapter of the Association for Computational
Linguistics (NAACL-2010), pages 109?117.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, College Park, MD.
Sabine Schulte im Walde. 2006. Experiments on the
automatic induction of German semantic verb classes.
Computational Linguistics, 32(2):159?194.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Gold-
berger. 2008. Contextual preferences. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics, pages 683?691, Columbus,
Ohio.
Noriko Tomuro. 2001. Tree-cut and a lexicon based on
systematic polysemy. In Proceedings of the second
meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
technologies, NAACL ?01, pages 1?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Cohen.
2011. Literal and metaphorical sense identification
through concrete and abstract context. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 680?690, Edinburgh, Scot-
land, UK.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32:379?416.
Jason Utt and Sebastian Pado?. 2011. Ontology-based
distinction between polysemy and homonymy. In Pro-
ceedings of the 9th International Conference on Com-
putational Semantics, Oxford, UK.
David Yarowsky. 1992. Word-sense disambiguation using
statistical models of Roget?s categories trained on large
corpora. In Proceedings of the 14th conference on
Computational linguistics - Volume 2, COLING ?92,
pages 454?460, Stroudsburg, PA, USA. Association for
Computational Linguistics.
160
Ontology-based Distinction between Polysemy and Homonymy
Jason Utt
Institut fu?r Maschinelle Sprachverarbeitung
Universita?t Stuttgart
uttjn@ims.uni-stuttgart.de
Sebastian Pado?
Seminar fu?r Computerlinguistik
Universita?t Heidelberg
pado@cl.uni-heidelberg.de
Abstract
We consider the problem of distinguishing polysemous from homonymous nouns. This distinction
is often taken for granted, but is seldom operationalized in the shape of an empirical model. We
present a first step towards such a model, based on WordNet augmented with ontological classes
provided by CoreLex. This model provides a polysemy index for each noun which (a), accurately
distinguishes between polysemy and homonymy; (b), supports the analysis that polysemy can be
grounded in the frequency of the meaning shifts shown by nouns; and (c), improves a regression
model that predicts when the ?one-sense-per-discourse? hypothesis fails.
1 Introduction
Linguistic studies of word meaning generally divide ambiguity into homonymy and polysemy. Homony-
mous words exhibit idiosyncratic variation, with essentially unrelated senses, e.g. bank as FINANCIAL
INSTITUTION versus as NATURAL OBJECT. In polysemy, meanwhile, sense variation is systematic,
i.e., appears for whole sets of words. E.g., lamb, chicken and salmon have ANIMAL and FOOD senses.
It is exactly this systematicity that represents a challenge for lexical semantics. While homonymy is
assumed to be encoded in the lexicon for each lemma, there is a substantial body of work on dealing with
general polysemy patterns (cf. Nunberg and Zaenen (1992); Copestake and Briscoe (1995); Pustejovsky
(1995); Nunberg (1995)). This work is predominantly theoretical in nature. Examples of questions
addressed are the conditions under which polysemy arises, the representation of polysemy in the semantic
lexicon, disambiguation mechanisms in the syntax-semantics interface, and subcategories of polysemy.
The distinction between polysemy and homonymy also has important potential ramifications for
computational linguistics, in particular for Word Sense Disambiguation (WSD). Notably, Ide and Wilks
(2006) argue that WSD should focus on modeling homonymous sense distinctions, which are easy to
make and provide most benefit. Another case in point is the one-sense-per-discourse hypothesis (Gale
et al, 1992), which claims that within a discourse, instances of a word will strongly tend towards realizing
the same sense. This hypothesis seems to apply primarily to homonyms, as pointed out by Krovetz (1998).
Unfortunately, the distinction between polysemy and homonymy is still very much an unsolved
question. The discussion in the theoretical literature focuses mostly on clear-cut examples and avoids
the broader issue. Work on WSD, and in computational linguistics more generally, almost exclusively
builds on the WordNet (Fellbaum, 1998) word sense inventory, which lists an unstructured set of senses
for each word and does not indicate in which way these senses are semantically related. Diachronic
linguistics proposes etymological criteria; however, these are neither undisputed nor easy to operationalize.
Consequently, there are currently no broad-coverage lexicons that indicate the polysemy status of words,
nor even, to our knowledge, precise, automatizable criteria.
Our goal in this paper is to take a first step towards an automatic polysemy classification. Our approach
is based on the aforementioned intuition that meaning variation is systematic in polysemy, but not in
homonymy. This approach is described in Section 2. We assess systematicity by mapping WordNet senses
onto basic types, a set of 39 ontological categories defined by the CoreLex resource (Buitelaar, 1998),
and looking at the prevalence of pairs of basic types (such as {FINANCIAL INSTITUTION, NATURAL
265
OBJECT} above) across the lexicon. We evaluate this model on two tasks. In Section 3, we apply the
measure to the classification of a set of typical polysemy and homonymy lemmas, mostly drawn from the
literature. In Section 4, we apply it to the one-sense-per-discourse hypothesis and show that polysemous
words tend to violate this hypothesis more than homonyms. Section 5 concludes.
2 Modeling Polysemy
Our goal is to take the first steps towards an empirical model of polysemy, that is, a computational model
which makes predictions for ? in principle ? arbitrary words on the basis of their semantic behavior.
The basis of our approach mirrors the focus of much linguistic work on polysemy, namely the fact
that polysemy is systematic: There is a whole set of words which show the same variation between two
(or more) ontological categories, cf. the ?universal grinder? (Copestake and Briscoe, 1995). There are
different ways of grounding this notion of systematicity empirically. An obvious choice would be to use a
corpus. However, this would introduce a number of problems. First, while corpora provide frequency
information, the role of frequency with respect to systematicity is unclear: should acceptable but rare
senses play a role, or not? We side with the theoretical literature in assuming that they do. Another
problem with corpora is the actual observation of sense variation. Few sense-tagged corpora exist, and
those that do are typically small. Interpreting context variation in untagged corpora, on the other hand,
corresponds to unsupervised WSD, a serious research problem in itself ? see, e.g., Navigli (2009).
We therefore decided to adopt a knowledge-based approach that uses the structure of the WordNet
ontology to calculate how systematically the senses of a word vary. The resulting model sets all senses of
a word on equal footing. It is thus vulnerable to shortcomings in the architecture of WordNet, but this
danger is alleviated in practice by our use of a ?coarsened? version of WordNet (see below).
2.1 WordNet, CoreLex and Basic Types
WordNet provides only a flat list of senses for each word. This list does not indicate the nature of the
sense variation among the senses. However, building on the generative lexicon theory by Pustejovsky
(1995), Buitelaar (1998) has developed the ?CoreLex? resource. It defines a set of 39 so-called basic
types which correspond to coarse-grained ontological categories. Each basic type is linked to one or more
WordNet anchor nodes, which define a complete mapping between WordNet synsets and basic types by
dominance.1 Table 1 shows the set of basic types and their main anchors; Table 2 shows example lemmas
for some basic types.
Ambiguous lemmas are often associated with two or more basic types. CoreLex therefore further
assigns each lemma to what Buitelaar calls a polysemy class, the set of all basic types its synsets belong to;
a class with multiple representatives is considered systematic. These classes subsume both idiosyncratic
and systematic patterns, and thus, despite their name, provide no clue about the nature of the ambiguity.
CoreLex makes it possible to represent the meaning of a lemma not through a set of synsets, but instead
in terms of a set of basic types. This constitutes an important step forward. Our working hypothesis is that
these basic types approximate the ontological categories that are used in the literature on polysemy to
define polysemy patterns. That is, we can define a meaning shift to mean that a lemma possesses one sense
in one basic type, while another sense belongs to another basic type. Naturally, this correspondence is not
perfect: systematic polysemy did not play a role in the design of the WordNet ontology. Nevertheless,
there is a fairly good approximation that allows us to recover many prominent polysemy patterns. Table 3
shows three polysemy patterns characterized in terms of basic types. The first class was already mentioned
before. The second class contains a subset of ?transparent nouns? which can denote a container or a
quantity. The last class contains words which describe a place or a group of people.
1Note that not all of CoreLex anchor nodes are disjoint; therefore a given WordNet synset may be dominated by two CoreLex
anchor nodes. We assign each synset to the basic type corresponding to the most specific dominating anchor node.
266
BT WordNet anchor BT WordNet anchor BT WordNet anchor
abs ABSTRACTION loc LOCATION pho PHYSICAL OBJECT
act ACTION log GEOGRAPHICAL AREA plt PLANT
agt AGENT mea MEASURE pos POSSESSION
anm ANIMAL mic MICROORGANISM pro PROCESS
art ARTIFACT nat NATURAL OBJECT prt PART
atr ATTRIBUTE phm PHENOMENON psy PSYCHOLOGICAL FEATURE
cel CELL frm FORM qud DEFINITE QUANTITY
chm CHEMICAL ELEMENT grb BIOLOGICAL GROUP qui INDEFINITE QUANTITY
com COMMUNICATION grp GROUP rel RELATION
con CONSEQUENCE grs SOCIAL GROUP spc SPACE
ent ENTITY hum PERSON sta STATE
evt EVENT lfr LIVING THING sub SUBSTANCE
fod FOOD lme LINEAR MEASURE tme TIME
Table 1: The 39 CoreLex basic types (BTs) and their WordNet anchor nodes
Basic type WordNet anchor Examples
agt AGENT driver, menace, power, proxy, . . .
grs SOCIAL GROUP city, government, people, state, . . .
pho PHENOMENON life, pressure, trade, work, . . .
pos POSSESSION figure, land, money, right, . . .
qui INDEFINITE QUANTITY bit, glass, lot, step, . . .
rel RELATION function, part, position, series, . . .
Table 2: Basic types with example words
Pattern (Basic types) Examples
ANIMAL, FOOD fowl, hare, lobster, octopus, snail, . . .
ARTIFACT, INDEFINITE QUANTITY bottle, jug, keg, spoon, tub, . . .
ARTIFACT, SOCIAL GROUP academy, embassy, headquarters, . . .
Table 3: Examples of polysemous meaning variation patterns
2.2 Polysemy as Systematicity
Given the intuitions developed in the previous section, we define a basic ambiguity as a pair of basic
types, both of which are associated with a given lemma. The variation spectrum of a word is then the set
of all its basic ambiguities. For example, bottle would have the variation spectrum {{art qui} } (cf.
Table 3); the word course with the three basic types act, art, grs would have the variation spectrum
{{act art}; {act grs}; {art grs} }.
There are 39 basic types and thus 39 ? 38/2 = 741 possible basic ambiguities. In practice, only 663
basic ambiguities are attested in WordNet. We can quantify each basic ambiguity by the number of words
that exhibit it. For the moment, we simply interpret frequency as systematicity.2 Thus, we interpret the
high-frequency (systematic) basic ambiguities as polysemous, and low-frequency (idiosyncratic) basic
ambiguities as homonymous. Table 4 shows the most frequent basic ambiguities, all of which apply to
several hundred lemmas and can safely be interpreted as polysemous. At the other end, 56 of the 663
basic ambiguities are singletons, i.e. are attested by only a single lemma.
In a second step, we extend this classification from basic ambiguities to lemmas. The intuition is again
fairly straightforward: A word whose basic ambiguities are systematic will be perceived as polysemous,
and as homonymous otherwise. This is clearly an oversimplification, both practically, since we depend
on WordNet/CoreLex having made the correct design decisions in defining the ontology and the basic
types; as well as conceptually, since not all polysemy patterns will presumably show the same degree of
systematicity. Nevertheless, we believe that basic types provide an informative level of abstraction, and
that our model is in principle even able to account for conventionalized metaphor, to the extent that the
corresponding senses are encoded in WordNet.
2Note that this is strictly a type-based notion of frequency: corpus (token) frequencies do not enter into our model.
267
Basic ambiguity Examples
{act com} construction, consultation, draft, estimation, refusal, . . .
{act art} press, review, staging, tackle, . . .
{com hum} egyptian, esquimau, kazakh, mojave, thai, . . .
{act sta} domination, excitement, failure, marriage, matrimony, . . .
{art hum} dip, driver, mouth, pawn, watch, wing, . . .
Table 4: Top five basic ambiguities with example lemmas
Noun Basic types Noun Basic types
chicken anm fod evt hum lamb anm fod hum
salmon anm fod atr nat duck anm fod art qud
Table 5: Words exhibiting the ?grinding? (animal ? food) pattern
The exact manner in which the systematicity of the individual basic ambiguities of one lemma are
combined is not a priori clear. We have chosen the following method. Let P be a basic ambiguity, P(w)
the variation spectrum of a lemma w, and freq(P ) the number of WordNet lemmas with basic ambiguity P .
We define the set of polysemous basic ambiguities PN as the N -most frequent bins of basic ambiguities:
PN = {[P1], ..., [PN ]}, where [Pi] = {Pj | freq(Pi) = freq(Pj)} and freq(Pk) > freq(Pl) for k < l.
We call non-polysemous basic ambiguities idiosyncratic. The polysemy index of a lemma w, piN (w), is:
piN (w) =
| PN ?P(w)|
| P(w)| (1)
piN simply measures the ratio of w?s basic ambiguities which are polysemous, i.e., high-frequency basic
ambiguities. piN ranges between 0 and 1, and can be interpreted analogously to the intuition that we
have developed on the level of basic ambiguities: high values of pi (close to 1) mean that the majority
of a lemma?s basic ambiguities are polysemous, and therefore the lemma is perceived as polysemous.
In contrast, low values of pi (close to 0) mean that the lemma?s basic ambiguities are predominantly
idiosyncratic, and thus the lemma counts as homonymous. Again, note that we consider basic ambiguities
at the type level, and that corpus frequency does not enter into the model.
This model of polysemy relies crucially on the distinction between systematic and idiosyncratic basic
ambiguities, and therefore in turn on the parameter N . N corresponds to the sharp cutoff that our model
assumes. At the N -th most frequent basic ambiguity, polysemy turns into homonymy. Since frequency
is our only criterion, we have to lump together all basic ambiguities with the same frequency into 135
bins. If we set N = 0, none of the bins count as polysemous, so pi0(w) = 0 for all w ? all lemmas are
homonymous. In the other extreme, we can set N to 135, the total number of frequency bins, which
makes all basic ambiguities polysemous, and thus all lemmas: pi135(w) = 1 for all w. The optimization
of N will be discussed in Section 3.
2.3 Gradience between Homonymy and Polysemy
We assign each lemma a polysemy index between 0 and 1. We thus abandon the dichotomy that is usually
made in the literature between two distinct categories of polysemy and homonymy. Instead, we consider
polysemy and homonymy the two end points on a gradient, where words in the middle show elements of
both. This type of behavior can be seen even for prototypical examples of either category, such as the
homonym bank, which shows a variation between SOCIAL GROUP and ARTIFACT:
(1) a. The bill would force banks [...] to report such property. (grs)
b. The coin bank was empty. (art)
Note that this is the same basic ambiguity that is often cited as a typical example of polysemous sense
variation, for example for words like newspaper.
On the other hand, many lemmas which are presumably polysemous show rather unsystematic basic
ambiguities. Table 5 shows four lemmas which are instances of the meaning variation between ANIMAL
268
Homonymous nouns ball, bank, board, chapter, china, degree, fall, fame, plane, plant, pole, post, present, rest,
score, sentence, spring, staff, stage, table, term, tie, tip, tongue
Polysemous nouns bottle, chicken, church, classification, construction, cup, development, fish, glass, improve-
ment, increase, instruction, judgment, lamb, management, newspaper, painting, paper, picture,
pool, school, state, story, university
Table 6: Experimental items for the two classes hom and poly
(anm) and FOOD (fod), a popular example of a regular and productive sense extension. Yet each of the
nouns exhibits additional basic types. The noun chicken also has the highly idiosyncratic meaning of a
person who lacks confidence. A lamb can mean a gullible person, salmon is the name of a color and a
river, and a duck a score in the game of cricket. There is thus an obvious unsystematic variety in the words?
sense variations ? a single word can show both homonymic as well as polysemous sense alternation.
3 Evaluating the Polysemy Model
To identify an optimal cutoff value N for our polysemy index, we use a simple supervised approach: we
optimize the quality with which our polysemy index models a small, manually created dataset. More
specifically, we created a two-class, 48-word dataset with 24 homonymous nouns (class hom) and 24
polysemous nouns (class poly) drawn from the literature. The dataset is shown in Table 6.
We now rank these items according to piN for different values of N and observe the ability of piN
to distinguish the two classes. We measure this ability with the Mann-Whitney U test, a nonparametric
counterpart of the t-test.3 In our case, the U statistic is defined as
U(N) =
m
?
i=1
n
?
j=1
1(piN (homi) < piN (polyi))
where 1 is the function function that returns the truth value of its argument (1 for ?true?). Informally,
U(N) counts the number of correctly ranked pairs of a homonymous and a polysemous noun.
The maximum for U is the number of item pairs from the classes (24 ?24 = 576). A score of U = 576
would mean that every piN -value of a homonym is smaller than every polysemous value. U = 0 means
that there are no homonyms with smaller pi-scores. So U can be directly interpreted as the quality of
separation between the two classes. The null hypothesis of this test is that the ranking is essentially
random, i.e., half the rankings are correct4. We can reject the null hypothesis if U is significantly larger.
Figure 1(a) shows the U -statistic for all values of N (between 0 and 135). The left end shows the
quality of separation (i.e. U ) for few basic ambiguities (i.e. small N ) which is very small. As soon as we
start considering the most frequent basic ambiguities as systematic and thus as evidence for polysemy,
hom and poly become much more distinct. We see a clear global maximum of U for N = 81 (U = 436.5).
This U value is highly significant at p < 0.005, which means that even on our fairly small dataset, we can
reject the null hypothesis that the ranking is random. pi81 indeed separates the classes with high confidence:
436.5 of 576 or roughly 75% of all pairwise rankings in the dataset are correct. For N > 81, performance
degrades again: apparently these settings include too many basic ambiguities in the ?systematic? category,
and homonymous words start to be misclassified as polysemous.
The separation between the two classes is visualized in the box-and-whiskers plot in Figure 1(b). We
find that more than 75% of the polysemous words have pi81 > .6. The median value for poly is 1, thus
for more than half of the class pi81 = 1, which can be seen in Figure 2(b) as well. This is a very positive
result, since our hope is that highly polysemous words get high scores. Figure 2(a) shows that homonyms
are concentrated in the mid-range while exhibiting a small number of pi81-values at both extremes.
We take the fact that there is indeed an N which clearly maximizes U as a very positive result that
validates our choice of introducing a sharp cutoff between polysemous and idiosyncratic basic ambiguities.
3The advantage of U over t is that t assumes comparable variance in the two samples, which we cannot guarantee.
4Provided that, like in this case, the classes are of equal size.
269
0 20 40 60 80 100 120
30
0
35
0
40
0
N
U
(a) The U statistic for different values of the cutoff N
l
l
l
l
hom poly
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
(b) Distribution of pi81 values by class
Figure 1: Separation of the hom and poly classes in our dataset
These 81 frequency bins contain roughly 20% of the most frequent basic ambiguities. This corresponds to
the assumption that basic ambiguities are polysemous if they occur with a minimum of about 50 lemmas.
If we look more closely at those polysemous words that obtain low scores (school, glass and cup),
we observe that they also show idiosyncratic variation as discussed in Section 2.3. In the case of school,
we have the senses schooltime of type tme and group of fish of type grb which one would not expect to
alternate regularly with grs and art, the rest of its variation spectrum. The word glass has the unusual
type agt due to its use as a slang term for crystal methamphetamine. Finally, cup is unique in that means
both an indefinite quantity as well as the definite measurement equal to half a pint. Only 10 other words
have this variation in WordNet, including such words as million and billion, which are often used to
describe an indefinite but large number.
On the other hand, those homonyms that have a high score (e.g. tie, staff and china) have somewhat
unexpected regularities due to obscure senses. Both tie and staff are terms used in musical notation. This
leads to basic ambiguities with the com type, something that is very common. Finally, the obviously
unrelated senses for china, China and porcelain, are less idiosyncratic when abstracted to their types, log
and art, respectively. There are 117 words that can mean a location as well as an artifact, (e.g. fireguard,
bath, resort, front, . . . ) which are clearly polysemous in that the location is where the artifact is located.
In conclusion, those examples which are most grossly miscategorized by pi81 contain unexpected
sense variations, a number of which have been ignored in previous studies.
ball
bank
board
chapter
china
degree
fall
game
plane
plant
pole
post
present rest
score
sentence
spring
staffstage
table
term
tie
tip
tongue
0 1
(a) Class hom
classification
chicken bottle constructioncup
development
fish
glass
improvement
increase
instruction
judgment
lamb management
newspaper
painting
paper
picturepool
school
state
story
university
church
0 1
(b) Class poly
Figure 2: Words and their pi81-scores
270
4 The One-Sense-Per-Discourse Hypothesis
The second evaluation that we propose for our polysemy index concerns a broader question on word
sense, namely the so-called one-sense-per-discourse (1spd) hypothesis. This hypothesis was introduced
by Gale et al (1992) and claims that ?[...] if a word such as sentence appears two or more times in
a well-written discourse, it is extremely likely that they will all share the same sense?. The authors
verified their hypothesis on a small experiment with encouraging results (only 4% of discourses broke
the hypothesis). Indeed, if this hypothesis were unreservedly true, then it would represent a very strong
global constraint that could serve to improve word sense disambiguation ? and in fact, a follow-up paper
by Yarowsky (1995) exploited the hypothesis for this benefit.
Unfortunately, it seems that 1spd does not apply universally. At the time (1992), WordNet had
not yet emerged as a widely used sense inventory, and the sense labels used by Gale et al were fairly
coarse-grained ones, motivated by translation pairs (e.g., English duty translated as French droit (tax)
vs. devoir (obligation)), which correspond mostly to homonymous sense distinctions.5 Current WSD, in
contrast, uses the much more fine-grained WordNet sense inventory which conflates homonymous and
polysemous sense distinctions. Now, 1spd seems intuitively plausible for homonyms, where the senses
describe different entities that are unlikely to occur in the same discourse (or if they do, different words
will be used). However, the situation is different for polysemous words: In a discourse about a party, bottle
might felicitously occur both as an object and a measure word. A study by Krovetz (1998) confirmed this
intuition on two sense-tagged corpora, where he found 33% of discourses to break 1spd. He suggests that
knowledge about polysemy classes can be useful as global biases for WSD.
In this section, we analyze the sense-tagged SemCor corpus in terms of the basic type-based framework
of polysemy that we have developed in Section 2 both qualitatively and quantitatively to demonstrate that
basic types, and our polysemy index pi, help us better understand the 1spd hypothesis.
4.1 Analysis by Basic Types and One-Basic-Type-Per-Discourse
The first step in our analysis looks specifically at the basic types and basic ambiguities we observe in
discourses that break 1spd. Our study reanalyses SemCor, a subset of the Brown corpus annotated ex-
haustively with WordNet senses (Fellbaum, 1998). SemCor contains a total of 186 discourses, paragraphs
of between 645 and 1023 words. These 186 discourses, in combination with 1088 nouns, give rise to
7520 lemma-discourse pairs, that is, cases where a sense-tagged lemma occurs more than once within a
discourse.6 These 7520 lemma-discourse pairs form the basis of our analysis. We started by looking at
the relative frequency of 1spd. We found that the hypothesis holds for 69% of the lemma-discourse pairs,
but not for the remaining 31%. This is a good match with Krovetz? findings, and indicates that there are
many discourses where there lemmas are used in different senses.
In accordance with our approach to modeling meaning variation at the level of basic types, we
implemented a ?coarsened? version of 1spd, namely one-basic-type-per-discourse (1btpd). This hypothesis
is parallel to the original, claiming that it is extremely likely that all words in a discourse share the
same basic type. As we have argued before, the basic-type level is a fairly good approximation to the
most important ontological categories, while smoothing over some of the most fine-grained (and most
troublesome) sense distinctions in WordNet. In this vein, 1btpd should get rid of ?spurious? ambiguity,
but preserve meaningful ambiguity, be it homonymous or polysemous. In fact, the basic type with most
of these ?within-basic-type? ambiguities is PSYCHOLOGICAL FEATURE, which contains many subtle
distinctions such as the following senses of perception:
a. a way of conceiving something b. the process of perceiving
c. knowledge gained by perceiving d. becoming aware of something via the senses
Such distinctions are collapsed in 1btpd. In consequence, we expect a noticeable, but limited, reduction in
5Note that Gale et al use the term ?polysemy? synonymously with ?ambiguous?.
6We exclude cases where a lemma occurs once in a discourse, since 1spd holds trivially.
271
Basic ambiguity most common breaking words freq(P breaks 1btpd) freq(P ) N
{com psy} evidence, sense, literature, meaning, style, . . . 89 365 13
{act psy} study, education, pattern, attention, process, . . . 88 588 7
{psy sta} need, feeling, difficulty, hope, fact, . . . 79 338 14
{act atr} role, look, influence, assistance, interest, . . . 79 491 9
{act art} church, way, case, thing, design, . . . 67 753 2
{act sta} operation, interest, trouble, employment, absence, . . . 60 615 4
{act com} thing, art, production, music, literature, . . . 59 755 1
{atr sta} life, level, desire, area, unity, . . . 58 594 6
Table 7: Most frequent basic ambiguities that break the 1btpd hypothesis in SemCor
the cases that break the hypothesis. Indeed, 1btpd holds for 76% of all lemma-discourse pairs, i.e., for 7%
more than 1spd. For the remainder of this analysis, we will test the 1btpd hypothesis instead of 1spd.
The basic type level also provides a good basis to analyze the lemma-discourse pairs where the
hypothesis breaks down. Table 7 shows the basic ambiguities that break the hypothesis in SemCor most
often. The WordNet frequencies are high throughout, which means that these basic ambiguities are poly-
semous according to our framework. It is noticeable that the two basic types PSYCHOLOGICAL FEATURE
and ACTION participate in almost all of these basic ambiguities. This observation can be explained
straightforwardly through polysemous sense extension as sketched above: Actions are associated, among
other things, with attributes, states, and communications, and discussion of an action in a discourse can
fairly effortlessly switch to these other basic types. A very similar situation applies to psychological
features, which are also associated with many of the other categories. In sum, we find that the data bears
out our hypothesis: almost all of the most frequent cases of several-basic-types-per-discourse clearly
correspond to basic ambiguities that we have classified as polysemous rather than homonymous.
4.2 Analysis by Regression Modeling
This section complements the qualitative analysis of the previous section with a quantitative analysis
which predicts specifically for which lemma-discourse pairs 1btpd breaks down. To do so, we fit a logit
mixed effects model (Breslow and Clayton, 1993) to the SemCor data. Logit mixed effects models can
be seen as a generalization of logistic regression models. They explain a binary response variable y in
terms of a set of fixed effects x, but also include a set of random effects x?. Fixed effects correspond to
?ordinary? predictors as in traditional logistic regression, while random effects account for correlations in
the data introduced by groups (such as items or subjects) without ascribing these random effects the same
causal power as fixed effects ? see, e.g., Jaeger (2008) for details.
The contribution of each factor is modelled by a coefficient ?, and their sum is interpreted as the
logit-transformed probability of a positive outcome for the response variable:
p(y = 1) = 11 + e?z with z =
?
?ixi +
?
??jx?j (2)
Model estimation is usually performed using numeric approximations. The coefficients ?? of the random
effects are drawn from a multivariate normal distribution, centered around 0, which ensures that the
majority of random effects are ascribed very small coefficients.
From a linguistic perspective, a desirable property of regression models is that they describe the
importance of the different effects. First of all, each coefficient can be tested for significant difference
to zero, which indicates whether the corresponding effect contributes significantly to modeling the data.
Furthermore, the absolute value of each ?i can be interpreted as the log odds ? that is, as the (logarithmized)
change in the probability of the response variable being positive depending on xi being positive.
In our experiment, each datapoint corresponds to one of the 7520 lemma-discourse pair from SemCor
(cf. Section 4.1). The response variable is binary: whether 1btpd holds for the lemma-discourse pair or
not. We include in the model five predictors which we expect to affect the response variable: three fixed
effects and two random ones. The first fixed effect is the ambiguity of the lemma as measured by the
272
Predictor Coefficient Odds (95% confidence interval) Significance
Number of basic types -0.50 0.61 (0.59?0.63) ***
Log length of discourse (words) -0.60 1.83 (1.14?2.93) ?
Polysemy index (pi81) -0.91 0.40 (0.35?0.46) ***
Table 8: Logit mixed effects model for the response variable ?one-basic-type-per-discourse (1btpd) holds?
(SemCor; random effects: discourse and lemma; significances: ?: p > 0.05; ***: p < 0.001)
number of its basic types, i.e. the size of its variation spectrum. We expect that the more ambiguous a
noun, the smaller the chance for 1btpd. We expect the same effect for the (logarithmized) length of the
discourse in words: longer discourses run a higher risk for violating the hypothesis. Our third fixed effect
is the polysemy index pi81, for which we also expect a negative effect. The two random effects are the
identity of the discourse and the noun. Both of these can influence the outcome, but should not be used as
full explanatory variables.
We build the model in the R statistical environment, using the lme47 package. The main results are
shown in Table 8. We find that the number of basic types has a highly significant negative effect on the
1btpd hypothesis (p < 0.001) . Each additional basic type lowers the odds for the hypothesis by a factor
of e?0.50 ? 0.61. The confidence interval is small; the effect is very consistent. This was to be expected ?
it would have been highly suspicious if we had not found this basic frequency effect. Our expectations are
not met for the discourse length predictor, though. We expected a negative coefficient, but find a positive
one. The size of the confidence interval shows the effect to be insignificant. Thus, we have to assume that
there is no significant relationship between the length of the discourse and the 1btpd hypothesis. Note
that this outcome might result from the limited variation of discourse lengths in SemCor: recall that no
discourse contains less than 645 or more than 1023 words.
However, we find a second highly significant negative effect (p < 0.001) in our polysemy index pi81.
With a coefficient of -0.91, this means that a word with a polysemy index of 1 is only 40% as likely
to preserve 1btpd than a word with a polysemy index of 0. The confidence interval is larger than for
the number of basic types, but still fairly small. To bolster this finding, we estimated a second mixed
effects model which was identical to the first one but did not contain pi81 as predictor. We tested the
difference between the models with a likelihood ratio test and found that the model that includes pi81 is
highly preferred (p < 0.0001;D = ?2?LL = 40; df = 1).
These findings establish that our polysemy index pi can indeed serve a purpose beyond the direct
modeling of polysemy vs. homonymy, namely to explain the distribution of word senses in discourse
better than obvious predictors like the overall ambiguity of the word and the length of the discourse can.
This further validates the polysemy index as a contribution to the study of the behavior of word senses.
5 Conclusion
In this paper, we have approached the problem of distinguishing empirically two different kinds of
word sense ambiguity, namely homonymy and polysemy. To avoid sparse data problems inherent in
corpus work on sense distributions, our framework is based on WordNet, augmented with the ontological
categories provided by the CoreLex lexicon. We first classify the basic ambiguities (i.e., the pairs of
ontological categories) shown by a lemma as either polysemous or homonymous, and then assign the ratio
of polysemous basic ambiguities to each word as its polysemy index.
We have evaluated this framework on two tasks. The first was distinguishing polysemous from
homonymous lemmas on the basis of their polysemy index, where it gets 76% of all pairwise rankings
correct. We also used this task to identify an optimal value for the threshold between polysemous and
homonymous basic ambiguities. We located it at around 20% of all basic ambiguities (113 of 663 in
the top 81 frequency bins), which apparently corresponds to human intuitions. The second task was
an analysis of the one-sense-per-discourse heuristic, which showed that this hypothesis breaks down
7http://cran.r-project.org/web/packages/lme4/index.html
273
frequently in the face of polysemy, and that the polysemy index can be used within a regression model to
predict the instances within a discourse where this happens.
It may seem strange that our continuous index assumes a gradient between homonymy and polysemy.
Our analyses indicate that on the level of actual examples, the two classes are indeed not separated by a
clear boundary: many words contain basic ambiguities of either type. Nevertheless, even in the linguistic
literature, words are often considered as either polysemous or homonymous. Our interpretation of this
contradiction is that some basic types (or some basic ambiguities) are more prominent than others. The
present study has ignored this level, modeling the polysemy index simply on the ratio of polysemous
patterns without any weighting. In future work, we will investigate human judgments of polysemy vs.
homonymy more closely, and assess other correlates of these judgments (e.g., corpus counts).
A second area of future work is more practical. The logistic regression incorporating our polysemous
index predicts, for each lemma-discourse pair, the probability that the one-sense-per-discourse hypothesis
is violated. We will use this information as a global prior on an ?all-words? WSD task, where all
occurrences of a word in a discourse need to be disambiguated. Finally, Stokoe (2005) demonstrates
the chances for improvement in information retrieval systems if we can reliably distinguish between
homonymous and polysemous senses of a word.
References
Breslow, N. and D. Clayton (1993). Approximate inference in generalized linear mixed models. Journal
of the American Statistical Society 88(421), 9?25.
Buitelaar, P. (1998). CoreLex: An ontology of systematic polysemous classes. In Proceedings of FOIS,
Amsterdam, Netherlands, pp. 221?235.
Copestake, A. and T. Briscoe (1995). Semi-productive polysemy and sense extension. Journal of
Semantics 12, 15?67.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Gale, W. A., K. W. Church, and D. Yarowsky (1992). One sense per discourse. In Proceedings of HLT,
Harriman, NY, pp. 233?237.
Ide, N. and Y. Wilks (2006). Making sense about sense. In E. Agirre and P. Edmonds (Eds.), Word Sense
Disambiguation: Algorithms and Applications, pp. 47?74. Springer.
Jaeger, T. (2008). Categorical data analysis: Away from ANOVAs and toward Logit Mixed Models.
Journal of Memory and Language 59(4), 434?446.
Krovetz, R. (1998). More than one sense per discourse. In Proceedings of SENSEVAL, Herstmonceux
Castle, England.
Navigli, R. (2009). Word Sense Disambiguation: a survey. ACM Computing Surveys 41(2), 1?69.
Nunberg, G. (1995). Transfers of meaning. Journal of Semantics 12(2), 109?132.
Nunberg, G. and A. Zaenen (1992). Systematic polysemy in lexicology and lexicography. In Proceedings
of Euralex II, Tampere, Finland, pp. 387?395.
Pustejovsky, J. (1995). The Generative Lexicon. Cambridge MA: MIT Press.
Stokoe, C. (2005). Differentiating homonymy and polysemy in information retrieval. In Proceedings of
the conference on Human Language Technology and Empirical Methods in NLP, Morristown, NJ, pp.
403?410.
Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceed-
ings of ACL, Cambridge, MA, pp. 189?196.
274
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 70?79,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Modeling covert event retrieval in logical metonymy:
probabilistic and distributional accounts
Alessandra Zarcone, Jason Utt
Institut f?r Maschinelle Sprachverarbeitung
Universit?t Stuttgart
{zarconaa,uttjn}@ims.uni-stuttgart.de
Sebastian Pad?
Institut f?r Computerlinguistik
Universit?t Heidelberg
pado@cl.uni-heidelberg.de
Abstract
Logical metonymies (The student finished
the beer) represent a challenge to composi-
tionality since they involve semantic content
not overtly realized in the sentence (covert
events ? drinking the beer). We present a
contrastive study of two classes of computa-
tional models for logical metonymy in German,
namely a probabilistic and a distributional,
similarity-based model. These are built using
the SDEWAC corpus and evaluated against a
dataset from a self-paced reading and a probe
recognition study for their sensitivity to the-
matic fit effects via their accuracy in predicting
the correct covert event in a metonymical con-
text. The similarity-based models allow for
better coverage while maintaining the accuracy
of the probabilistic models.
1 Introduction
Logical metonymies (The student finished the beer)
require the interpretation of a covert event which
is not overtly realized in the sentence (? drinking
the beer). Logical metonymy has received much
attention as it raises issues that are relevant to both
theoretical as well as cognitive accounts of language.
On the theoretical side, logical metonymies consti-
tute a challenge for theories of compositionality (Par-
tee et al, 1993; Baggio et al, in press) since their in-
terpretation requires additional, inferred information.
There are two main accounts of logical metonymy:
According to the lexical account, a type clash be-
tween an event-subcategorizing verb (finish) and an
entity-denoting object (beer) triggers the recovery of
a covert event from complex lexical entries, such as
qualia structures (Pustejovsky, 1995). The pragmatic
account of logical metonymy suggests that covert
events are retrieved through post-lexical inferences
triggered by our world knowledge and communica-
tion principles (Fodor and Lepore, 1998; Cartson,
2002; De Almeida and Dwivedi, 2008).
On the experimental side, logical metonymy leads
to higher processing costs (Pylkk?nen and McEl-
ree, 2006; Baggio et al, 2010). As to covert
event retrieval, it has been found that verbs cue
fillers with a high thematic fit for their argument
positions (e.g. arrest
agent
???? cop, (Ferretti et al,
2001)) and that verbs and arguments combined cue
fillers with a high thematic fit for the remaining
argument slots (e.g. ?journalist , check?
patient
?????
spelling but ?mechanic, check?
patient
????? car (Bick-
nell et al, 2010). The interpretation of logical
metonymy is also highly sensitive to context (e.g.
?confectioner , begin, icing?
covertevent
???????? spread
but ?child , begin, icing?
covertevent
???????? eat (Zarcone
and Pad?, 2011; Zarcone et al, 2012). It thus pro-
vides an excellent test bed for cognitively plausible
computational models of language processing.
We evaluate two classes of computational mod-
els for logical metonymy. The classes represent the
two main current approaches in lexical semantics:
probabilistic and distributional models. Probabilistic
models view the interpretation as the assignment of
values to random variables. Their advantage is that
they provide a straightforward way to include con-
text, by simply including additional random variables.
However, practical estimation of complex models
typically involves independence assumptions, which
70
may or may not be appropriate, and such models
only take first-order co-occurrence into account1. In
contrast, distributional models represent linguistic
entities as co-occurrence vectors and phrase interpre-
tation as a vector similarity maximization problem.
Distributional models typically do not require any
independence assumptions, and include second-order
co-occurrences. At the same time, how to integrate
context into the vector computation is essentially an
open research question (Mitchell and Lapata, 2010).
In this paper, we provide the first (to our knowl-
edge) distributional model of logical metonymy
by extending the context update of Lenci?s ECU
model (Lenci, 2011). We compare this model to
a previous probabilistic approach (Lapata and Las-
carides, 2003a; Lapata et al, 2003b). In contrast
to most experimental studies on logical metonymy,
which deal with English data (with the exception of
Lapata et al (2003b)), we focus on German. We
estimate our models on a large web corpus and eval-
uate them on a psycholinguistic dataset (Zarcone
and Pad?, 2011; Zarcone et al, 2012). The task
we use to evaluate our models is to distinguish
covert events with a high typicality / thematic fit
(e.g. The student finished the beer ?? drinking)
from low typicality / thematic fit covert events
(?? brewing).
2 Probabilistic models of logical metonymy
Lapata et al (2003b; 2003a) model the interpretation
of a logical metonymy (e.g. The student finished the
beer) as the joint distribution P (s, v, o, e) of the vari-
ables s (the subject, e.g. student), v (the metonymic
verb, e.g. finish), o (the object, e.g. beer), e (the
covert event, drinking).
This model requires independence assumptions
for estimation. We present two models with different
independence assumptions.
1This statement refers to the simple probabilistic models we
consider, which are estimated directly from corpus co-occurrence
frequencies. The situation is different for more complex prob-
abilistic models, for example generative models that introduce
latent variables, which can amount to clustering based on higher-
order co-occurrences, as in, e.g., Prescher et al (2000).
2.1 The SOVp model
Lapata et al develop a model which we will refer
to as the SOVp model.2 It assumes a generative pro-
cess which first generates the covert event e and then
generates all other variables based on the choice of e:
P (s, v, o, e) ? P (e) P (o|e) P (v|e) P (s|e)
They predict that the selected covert event e? for
a given context is the event which maximizes
P (s, v, o, e):
e? = argmax
e
P (e) P (o|e) P (v|e) P (s|e)
These distributions are estimated as follows:
P? (e) =
f(e)
N
, P? (o|e) =
f(e
o
?? o)
f(e
o
?? ?)
,
P? (v|e) =
f(v
c
?? e)
f(?
c
?? e)
, P? (s|e) =
f(e
s
?? s)
f(e
s
?? ?)
,
where N is the number of occurrences of full verbs
in the corpus; f(e) is the frequency of the verb e;
f(e
o
?? ?) and f(e
s
?? ?) are the frequencies of e
with a direct object and subject, respectively; and
f(e
c
?? ?) is number of times e is the complement of
another full verb.
2.2 The SOp model
In Lapata et al?s covert event model, v, the
metonymic verb, was used to prime different choices
of e for the same object (begin book?? writing;
enjoy book?? reading). In our dataset (Sec. 4), we
keep v constant and consider e only as a function of
s and o. Thus, the second model we consider is the
SOp model which does not consider v:
P (s, v, o, e) ? P (s, o, e) ? P (e) P (o|e) P (s|e)
Again, the preferred interpretation e? is the one that
maximizes P (s, v, o, e):
e? = argmax
e
P (e) P (o|e) P (s|e)
2In Lapata et al (2003b; 2003a), this model is called the
simplified model to distinguish it from a full model. Since the full
model performs worse, we do not include it into consideration
and use a more neutral name for the simplified model.
71
3 Similarity-based models
3.1 Distributional semantics
Distributional or vector space semantics (Turney and
Pantel, 2010) is a framework for representing word
meaning. It builds on the Distributional Hypothe-
sis (Harris, 1954; Miller and Charles, 1991) which
states that words occurring in similar contexts are
semantically similar. In distributional models, the
meaning of a word is represented as a vector whose
dimensions represent features of its linguistic con-
text. These features can be chosen in different ways;
popular choices are simple words (Sch?tze, 1992) or
lexicalized dependency relations (Lin, 1998; Pad?
and Lapata, 2007). Semantic similarity can then be
approximated by vector similarity using a wide range
of similarity metrics (Lee, 1999).
3.1.1 Distributional Memory
A recent multi-purpose framework in distribu-
tional semantics is Distributional Memory (DM, Ba-
roni and Lenci (2010)). DM does not immedi-
ately construct vectors for words. Instead, it ex-
tracts a three-dimensional tensor of weighted word-
link-word tuples each of which is mapped onto a
score by a function ? : ?w1 l w2? ? R+. For ex-
ample, ?pencil obj use? has a higher weight than
?elephant obj use?. The set of links can be defined in
different ways, yielding various DM instances. Ba-
roni and Lenci present DepDM (mainly syntactic
links such as subj_tr ), LexDM (strongly lexicalized
links, e.g., such_as), or TypeDM (syntactic and lexi-
calized links).3
The benefit of the tensor-based representation is
that it is general, being applicable to many tasks.
Once a task is selected, a dedicated semantic space
for this task can be generated efficiently from the
tensor. For example, the word by link-word space
(W1 ? LW2) contains vectors for the words w1
whose dimensions are labeled with ?l, w2? pairs. The
word-word by link space (W1W2 ? L) contains co-
occurrence vectors for word pairs ?w1, w2? whose
dimensions are labeled with l.
3.2 Compositional Distributional Semantics
Probabilistic models can account for composition-
ality by estimating conditional probabilities. Com-
3l?1 is used to denote the inverse link of l (i.e., exchanging
the positions of w1 and w2).
positionality is less straightforward in a similarity-
based distributional model, because similarity-based
distributional models traditionally model meaning
at word level. Nevertheless, the last years have
seen a wave of distributional models which make
progress at building compositional representations
of higher-level structures such as noun-adjective or
verb-argument combinations (Mitchell and Lapata,
2010; Guevara, 2011; Reddy et al, 2011).
3.2.1 Expectation Composition and Update
Lenci (2011) presents a model to predict the degree
of thematic fit for verb-argument combinations: the
Expectation Composition and Update (ECU) model.
More specifically, the goal of ECU is explain how the
choice of a specific subject for a given verb impacts
the semantic expectation for possible objects. For
example, the verb draw alone might have fair, but not
very high, expectations for the two possible objects
landscape and card. When it is combined with the
subject painter, the resulting phrase painter draw the
expectation for the object landscape should increase,
while it should drop for card.
The idea behind ECU is to first compute the verb?s
own expectations for the object from a TypeDM
W1 ? LW2 matrix and then update it with the sub-
ject?s expectations for the object, as mediated by the
TypeDM verb link type.4 More formally, the verb?s
expectations for the object are defined as
EXV (v) = ?o. ?(
?
v obj?1 o
?
)
The subject?s expectations for the object are
EXS(s) = ?o. ?(?s verb o?)
And the updated expectation is
EXSV (s, v) = ?o.EXV (v)(o) ? EXS(s)(o)
where ? is a composition operation which Lenci in-
stantiates as sum and product, following common
practice in compositional distributional semantics
(Mitchell and Lapata, 2010). The product composi-
tion approximates a conjunction, promoting objects
that are strongly preferred by both verb and subject.
It is, however, also prone to sparsity problems as well
4In DM, verb directly connects the subject and the object of
transitive verb instances, e.g ?marine verb gun?.
72
shortcomings of the scoring function ?. The sum
composition is more akin to a disjunction where it
suffices that an object is strongly preferred by either
the verb or the subject.
It would be possible to use these scores as direct
estimates of expectations, however, sinceEXSV con-
tains three lexical variables, sparsity is a major issue.
ECU thus introduces a distributional generalization
step. It only uses the updated expectations to identify
the 20 most expected nouns for the object position.
It then determines the prototype of the updated ex-
pectations as the centroid of their W1?LW2 vectors.
Now, the thematic fit for any noun can be computed
as the similarity of its vector to the prototype.
Lenci evaluates ECU against a dataset from Bick-
nell et al (2010), where objects (e.g. spelling) are
matched with a high-typicality subject-verb combi-
nations (e.g. ?journalist, check? - high thematic fit)
and with a low-typicality subject-verb combination
(e.g. ?mechanic, check? - low thematic fit). ECU is
in fact able to correctly distinguish between the two
contexts differing in thematic fit with the object.
3.3 Cognitive relevance
Similarity-based models build upon the Distribu-
tional Hypothesis, which, in its strong version, is
a cognitive hypothesis about the form of semantic
representations (Lenci, 2008): the distributional be-
havior of a word reflects its semantic behavior but
is also a direct correlate of its semantic content at
the cognitive level. Also, similarity-based models
are highly compatible with known features of hu-
man cognition, such as graded category member-
ship (Rosch, 1975) or multiple sense activation (Erk,
2010). Their cognitive relevance for language has
been supported by studies of child lexical devel-
opment (Li et al, 2004), category-related deficits
(Vigliocco et al, 2004), selectional preferences (Erk,
2007), event types (Zarcone and Lenci, 2008) and
more (see Landauer et al (2007) and Baroni and
Lenci (2010) for a review).
3.4 Modeling Logical Metonymy with ECU
3.4.1 Logical Metonymy as Thematic Fit
The hypothesis that we follow in this paper is that
the ECU model can also be used, with modifications,
to predict the interpretation of logical metonymy.
The underlying assumption is that the interpretation
of logical metonymy is essentially the recovery of
a covert event with a maximal thematic fit (high-
typicality) and can thus make use of ECU?s mech-
anisms to treat verb-argument composition. Strong
evidence for this assumption has been found in psy-
cholinguistic studies, which have established that
thematic fit dynamically affects processing, with on-
line updates of expectations for typical fillers during
the incremental processing of linguistic input (see
McRae and Matsuki (2009) for a review). Thus, we
can hope to transfer the benefits of similarity-based
models (notably, high coverage) to the interpretation
of logical metonymy.
3.4.2 Extending ECU
The ECU model nevertheless requires some modi-
fications to be applicable to logical metonymy. Both
the entity of interest and the knowledge sources
change. The entity of interest used to be the ob-
ject of the sentence; now it is the covert event, which
we will denote with e. As for knowledge sources,
there are three sources in logical metonymy. These
are (a), the subject (compare the author began the
beer and the reader began the book)); (b), the object
the reader began the book vs. the reader began the
sandwich); and (c), the metonymic verb (compare
Peter began the report vs. Peter enjoyed the report).
The basic equations of ECU can be applied to this
new scenario as follows. We first formulate three
basic equations that express the expectations of the
covert event given the subject, object, and metonymic
verb individually. They are all derived from direct de-
pendency relations in the DM tensor (e.g., the novel
metonymic verb?covert event relation from the ver-
bal complement relation):
EXS(s) = ?e. ?(?s subj e?)
EXO(o) = ?e. ?(?o obj e?)
EXV (v) = ?e. ?(
?
v comp?1 e
?
)
To combine (or update) these basic expectations into
a final expectation, we propose two variants:
ECU SOV In this model, we compose all three
expectations:
EXSOV (s, v, o) = ?e.EXS(s)(e) ?
EXO(o)(e) ? EXV (v)(e)
73
CE
high thematic fit low thematic fit
Der
The
Konditor
baker
begann,
started
die
the
Glasur
icing
aufzutragen.
to spread.
zu essen.
to eat.
Das
The
Kind
child
begann,
started
die
the
Glasur
icing
zu essen.
to eat.
aufzutragen.
to spread.
Table 1: Example materials for the self-paced reading and probe recognition studies
We will refer to this model as SOV? when the
composition function is sum, and as the SOV? model
when the composition function is product.
ECU SO Analogous to the SO probabilistic model,
this model abstracts away from the metonymic verb.
We assume most information about an event to be
determined by the subject and object:
EXSO(n, n
?) = ?e.EXS(n)(e) ? EXO(n
?)(e)
After the update, the prototype computation proceeds
as defined in the original ECU.
We will refer to this model as SO? when the com-
position function is sum, and as the SO? model when
the composition function is product.
4 Experimental Setup
We evaluate the probabilistic models (Sec. 2) and the
similarity-based models (Sec. 3) on a dataset con-
structed from two German psycholinguistic studies
on logical metonymy. One study used self-paced
reading and the second one probe recognition.
Dataset The dataset we use is composed of 96 sen-
tences. There are 24 sets of four ?s, v, o, e? tuples,
where s is the object, v the metonymic verb, o the
object and e the covert event. The materials are illus-
trated in Table 1. As can be seen, all tuples within
a set share the same metonymic verb and the same
object. Each of the two subject e is matched once
with a high-typicality covert event and once with a
low-typicality covert event. This results in 2 high-
typicality tuples and 2 low-typicality tuples in each
set. Typical events (e) were elicited by 20 partici-
pants given the corresponding object o, subjects were
elicited by 10 participants as the prototypical agents
subjects for each e, o combination.
The experiments yielded a main effect of typicality
on self-paced reading times (Zarcone and Pad?, 2011)
and on probe recognition latencies (Zarcone et al,
2012): typical events involved in logical metonymy
interpretation are read faster and take longer to be
rejected as probe words after sentences which evoke
them. The effect is seen early on (after the patient
position in the self-paced reading and at short ISI for
the probe recognition), suggesting that knowledge
of typical events is quickly integrated in processing
and that participants access a broader pool of knowl-
edge than what has traditionally been argued to be
in the lexical entries of nouns (Pustejovsky, 1995).
The finding is in agreement with results of psycholin-
guistic studies which challenge the very distinction
between world knowledge and linguistic knowledge
(Hagoort et al, 2004; McRae and Matsuki, 2009).
DM for German Since DM exists only for English,
we constructed a German analog using the 884M
word SDEWAC web corpus (Faa? et al, 2010) parsed
with the MATE German dependency parser (Bohnet,
2010).
From this corpus, we extract 55M instances of
simple syntactic relations (subj_tr, subj_intr, obj,
iobj, comp, nmod) and 104M instances of lexicalized
patterns such as noun?prep?noun e.g. ?Recht auf
Auskunft? (?right to information?), or adj?noun-(of)-
noun such as ?strittig Entscheidung Schiedsrichter?
(?contested decision referee?). These lexicalized pat-
terns make our model roughly similar to the English
TypeDM model (Sec. 3.1.1).
As for ?, we used local mutual information (LMI)
as proposed by Baroni and Lenci (2010). The LMI
of a triple is defined as Ow1lw2 log(Ow1lw2/Ew1lw2),
where Ow1lw2 is the observed co-occurrence fre-
quency of the triple and Ew1lw2 its expected co-
occurrence frequency (under the assumption of inde-
pendence). Like standard MI, LMI measures the
informativity or surprisal of a co-occurrence, but
74
weighs it by the observed frequency to avoid the
overestimation for low-probability events.
4.1 Task
We evaluate the models using a binary selection
task, similar to Lenci (2011). Given a triple ?s, v, o?
and a pair of covert events e, e? (cf. rows in
Tab. 1), the task is to pick the high-typicality covert
event for the given triple: ?Chauffeur, vermeiden,
Auto? ? fahren/reparieren (?driver, avoid, car? ?
drive/repair). Since our dataset consists of 96 sen-
tences, we have 48 such contexts.
With the probabilistic models, we compare the
probabilities P (s, v, o, e) and P (s, v, o, e?) (ignoring
v in the SO model). Analogously, for the similarity-
based models, we compute the similarities of the
vectors for e and e? to the prototype vectors for the ex-
pectations EXSOV (s, v, o) and predict the one with
higher similarity. For the simplified ECU SO model,
we use EXSO(s, o) as the point of comparison.
4.2 Baseline
Following the baseline choice in Lapata et al
(2003b), we evaluated the probabilistic models
against a baseline (Bp) which, given a ?s, v, o?
triplet (e.g. ?Chauffeur, vermeiden, Auto?), scores
a ?hit? if the P? (e|o) for the high-typicality e is
higher than the P? (e|o) for the low-typicality e. The
similarity-based models were evaluated against a
baseline (Bs) which, given an ?s, v, o? triplet (e.g.
?Chauffeur, vermeiden, Auto?), makes a correct pre-
diction if the prototypical event vector for o has a
higher thematic fit (i.e. similarity) with the high-
typicality e than with the low-typicality e.
Since our dataset is counterbalanced ? that is, each
covert event appears once as the high-typicality event
for a given object (with a congruent subject) and once
as the low-typicality event ? the baseline predicts
the correct covert event in exactly 50% of the cases.
Note, however, that this is not a random baseline: the
choice of the covert event is made deterministically
on the basis of the input parameters.
4.3 Evaluation measures
We evaluate the output of the model with the stan-
dard measures coverage and accuracy. Coverage is
defined as the percentage of datapoints for which
a model can make a prediction. Lack of coverage
arises primarily from sparsity, that is, zero counts for
co-occurrences that are necessary in the estimation
of a model. Accuracy is computed on the covered
contexts only, as the ratio of correct predictions to
the number of predictions of the model. This allows
us to judge the quality of the model?s predictions
independent of its coverage.
We also consider a measure that combines cov-
erage and accuracy, Backoff Accuracy, defined as:
coverage?accuracy+((1?coverage)?0.5). Back-
off Accuracy emulates a backoff procedure: the
model?s predictions are adopted where they are avail-
able; for the remaining datapoints, it assumes base-
line performance (in the current setup, 50%). The
Backoff Accuracy of low-coverage models tends to
degrade towards baseline performance.
We determine the significance of differences be-
tween models with a ?2 test, applied to a 2?2 contin-
gency matrix containing the number of correct and
incorrect answers. Datapoints outside a model?s cov-
erage count half for each category, which corresponds
exactly to the definition of Backoff Accuracy.
5 Results
The results are shown in Table 2. Looking at the
probabilistic models, we find SOp yields better cov-
erage and better accuracy than SOVp (Lapata?s sim-
plified model). It is worth noting the large differ-
ence in coverage, namely .75 as opposed to .44: The
SOVp model is unable to make a prediction for more
than half of all contexts. This is due to the fact that
many ?o, v? combinations are unattested in the cor-
pus. Even on those contexts for which the proba-
bilistic SOVp model can make a prediction, it is less
reliable than the more general SOp model (0.62 ver-
sus 0.75 accuracy). This indicates that, at least on our
dataset, the metonymic verb does not systematically
help to predict the covert event; it rather harms perfor-
mance by introducing noisy estimates. As the lower
half of the Table shows, the SOVp model does not
significantly outperform any other model (including
both baselines Bp and Bs).
The distributional models do not have such cover-
age issues. The main problematic combination for
the similarity model is ?Pizzabote hassen Pizza? (i.e.
?Pizza delivery man hate pizza?) which is paired
with the covert events liefern (deliver) and backen
(bake). The computation of ECU predictions for
75
Probabilistic Models Similarity-based Models
Bp SOVp SOp Bs SOV? SOV? SO? SO?
Accuracy 0.50 0.62 0.75 0.50 0.68 0.56 0.68 0.70
Coverage 1.00 0.44 0.75 1.00 0.98 0.94 0.98 0.98
Backoff Accuracy 0.50 0.55 0.69 0.50 0.68 0.56 0.68 0.70
Probabilistic Models Similarity-based Models
Bp SOVp SOp Bs SOV? SOV? SO? SO?
Bp
P
ro
b. SOVp -
SOp * -
Bs - - *
SOV? * - - *
S
im
il
ar
it
y
SOV? - - - - -
SO? * - - * - -
SO? ** ?? - ** - ?? -
Table 2: Results (above) and significance levels for difference in backoff accuracy determined by ?2-test (below)
for all probabilistic and similarity-based models (**: p<0.01, *: p?0.05, -: p>0.05). For ?? (SO? ? SOVp and
SO? ? SOV?) p was just above 0.05 (p=0.053).
this combination requires corpus transitive corpus
constructions for Pizzabote, in the corpus it is only
attested once as the subject of the intransitive verb
kommen (come).
Among distributional models, the difference be-
tween SO and SOV is not as clear-cut as on the
probabilistic side. We observe an interaction with the
composition operation. Sum is less sensitive to com-
plexity of updating: for sum models, the inclusion
of the metonymic verb (SOV? vs. SOV?) does not
make any difference. On the side of the product mod-
els, there is a major difference similar to the one for
the probabilistic models: SOV? is the worst model
at near-baseline performance, and SO? is the best
one. This supports our interpretation from above that
the metonymic model introduces noisy expectations
which, in the product model, have the potential of
disrupting the update process.
Comparing the best models from the probabilistic
and similarity-based classes (SOp and SO?), we find
that both significantly outperform the baselines. This
shows that the subject contributes to the models with
a significant improvement over the baseline models,
which are only informed by the object. Their back-
off accuracies do not significantly differ from one
another, which is not surprising given the small size
of our dataset, however, the similarity-based model
outperforms the probabilistic model by 1% Backoff
Accuracy. The two models have substantially differ-
ent profiles: the accuracy of the probabilistic model
is 5% higher (0.70 vs. 0.75); at the same time, its
coverage is much lower. It covers only 75% of the
contexts, while the distributional model SO? covers
all but one (98%).
6 Discussion
As mentioned above, the main issue with the proba-
bilistic models is coverage. This is due to the reliance
of these models on first-order co-occurrence.
For example, probabilistic models cannot
assign a probability to any of the triples
?Dieb/Juwelier schmuggeln/schleifen Diamant?
(?thief/jeweler smuggle/cut diamond?), since the
subjects do not occur with either of the verbs in
corpus, even though Diamant does occur as the
object of both.
In contrast, the similarity-based models are able to
compute expectations for these triples from second-
order co-occurrences by taking into account other
verbs that co-occur with Diamant. The ECU model
is not punished by the extra context, as both Dieb and
Diamant are associated with the verbs: stehlen (steal),
76
EXSO(?Chauffeur,Auto?) EXSO(?Mechaniker,Auto?)
fahren (drive) bauen (build)
parken (park) lassen (let/leave)
lassen (let/leave) besitzen (own)
geben (give) reparieren (repair)
sehen (see) brauchen (need)
bringen (bring) sehen (see)
steuern (steer) benutzen (use)
halten (keep/hold) stellen (put)
Table 3: Updated expectations in SO? for Chauffeur
(chauffeur), Mechaniker (mechanic) and Auto (car).
rauben (thieve), holen (get), entwenden (purloin),
erbeuten (snatch), verkaufen (sell), nehmen (take),
klauen (swipe). We also note that these are typical
events for a thief, which fits the intuition that Dieb is
more predictive of the event than Diamant.
For both ?Chauffeur Auto? and ?Mechaniker Auto?
the probabilistic model predicts fahren due to the
high overall frequency of fahren.5 The distributional
model, however, takes the mutual information into
account and is thus able to determine events that
are more strongly associated with Mechaniker (e.g.
bauen, reparieren, etc.) while at the same time dis-
counting the uninformative verb fahren.
There are, however, items that all models have dif-
ficulty with. Three such cases are due to a frequency
disparity between the high and low-typicality event.
E.g. for ?Lehrerin Klausur benoten/schreiben?
(?teacher exam grade/take?), schreiben occurs much
more frequently than benoten. In the case
of ?Sch?ler Geschichte lernen/schreiben? (?student
story learn/write?), none of the models or baselines
correctly assigned lernen. The probabilistic mod-
els are influenced by the very frequent Geschichte
schreiben which is part of an idiomatic expression (to
write history). On the other hand, the distributional
models judge the story and history sense of the word
to have the most informative events, e.g. erz?hlen
(tell), lesen (read), h?ren (hear), erfinden (invent),
and studieren (study), lehren (teach).
The baselines were able to correctly choose
auspacken (unwrap) over einpacken (wrap) for
?Geburtstagskind Geschenk? (?birthday-boy/girl
present?) while the models were not. The prob-
5The combination Mechaniker fahren was seen once more
often than Mechaniker reparieren.
abilistic models lacked coverage and were not
able to make a prediction. For the distributional
models, while both auspacken and verpacken (wrap)
are highly associated with Geschenk, the most
strongly associated actions of Geburtstagskind are
extraordinarily diverse, e.g.: bekommen (receive),
sagen (say), auffuttern (eat up), herumkommandieren
(boss around), ausblasen (blow out). Neither of the
events of interest though were highly associated.
7 Future Work
We see a possible improvement in the choice of the
number of fillers, with which we construct the pro-
totype vectors. A smaller number might lead to less
noisy prototypes.
It has been shown (Bergsma et al, 2010) that the
meaning of the prefix verb can be accurately pre-
dicted using the stem?s vector, when compositional-
ity applies. We suspect covert events that are prefix
verbs to suffer from sparser representations than the
vectors of their stem. E.g., absaugen (vacuum off )
is much less frequent than the semantically nearly
identical saugen (vacuum). Thus, by leveraging the
richer representation of the stem, our distributional
models could more likely assign the correct event.
8 Conclusions
We have presented a contrastive study of two classes
of computational models, probabilistic and distribu-
tional similarity-based ones, for the prediction of
covert events for German logical metonymies.
We found that while both model classes models
outperform baselines which only take into account
information coming from the object, similarity-based
models rival and even outperform probabilistic mod-
els. The reason is that probabilistic models have to
rely on first-order co-occurrence information which
suffers from sparsity issues even in large web corpora.
This is particularly true for languages like German
that have a complex morphology, which tends to ag-
gravate sparsity (e.g., through compound nouns).
In contrast, similarity-based models can take ad-
vantage of higher-order co-occurrences. Provided
that some care is taken to identify reasonable vec-
tor composition strategies, they can maintain the ac-
curacy of probabilistic models while guaranteeing
higher coverage.
77
Acknowledgments
We would like to thank Alessandro Lenci, Siva Reddy
and Sabine Schulte im Walde for useful feedback
and discussion. The research for this paper has
been funded by the German Research Foundation
(Deutsche Forschungsgemeinschaft) as part of the
SFB 732 ?Incremental specification in context? /
project D6 ?Lexical-semantic factors in event inter-
pretation? at the University of Stuttgart.
References
Giosu? Baggio, Travis Chroma, Michiel van Lambalgen,
and Peter Hagoort. 2010. Coercion and composition-
ality. Journal of Cognitive Neuroscience, 22(9):2131?
2140.
Giosu? Baggio, Michiel van Lambalgen, and Peter Ha-
goort. in press. The processing consequences of com-
positionality. In The Oxford Handbook of Composition-
ality. Oxford University Press.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):1?49.
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz
Kondrak. 2010. Predicting the semantic composition-
ality of prefix verbs. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 293?303, Cambridge, MA, October.
Association for Computational Linguistics.
Klinton Bicknell, Jeffrey L. Elman, Mary Hare, Ken
McRae, and Marta Kutas. 2010. Effects of event
knowledge in processing verbal arguments. Journal of
Memory and Language, 63(4):489?505.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 89?97, Beijing, China.
Robyn Cartson. 2002. Thoughts and utterances. Black-
well.
Roberto G. De Almeida and Veena D. Dwivedi. 2008. Co-
ercion without lexical decomposition: Type-shifting
effects revisited. Canadian Journal of Linguistics,
53(2/3):301?326.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of ACL, Prague,
Czech Republic.
Katrin Erk. 2010. What is word meaning, really? (and
how can distributional models help us describe it?).
In Proceedings of the workshop on Geometrical Mod-
els of Natural Language Semantics (GEMS), Uppsala,
Sweden.
Gertrud Faa?, Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-
phological Analysis: SMOR as an Example of Mor-
phological Evaluation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC?10), Valletta, Malta.
T. R. Ferretti, K. McRae, and A. Hatherell. 2001. Integrat-
ing verbs, situation schemas and thematic role concept.
Journal of Memory and Language, 44:516?547.
Jerry A. Fodor and Ernie Lepore. 1998. The emptiness
of the lexicon: Reflections on James Pustejovsky?s The
Generative Lexicon. Linguistic Inquiry, 29(2):269?
288.
Emiliano Raul Guevara. 2011. Computing semantic com-
positionality in distributional semantics. In Proceed-
ings of IWCS-2011, Oxford, UK.
Peter Hagoort, Lea Hald, Marcel Bastiaansen, and
Karl Magnus Petersson. 2004. Integration of word
meaning and world knowledge in language comprehen-
sion. Science, 304:438?441.
Zelig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Thomas K. Landauer, Danielle S. McNamara, Simon Den-
nis, and Walter Kintsch, editors. 2007. Handbook of
Latent Semantic Analysis. Lawrence Erlbaum Asso-
ciates Publishers, Mahwah, NJ, US.
Mirella Lapata and Alex Lascarides. 2003a. A proba-
bilistic account of logical metonymy. Computational
Linguistics, 29(2):263?317.
Mirella Lapata, Frank Keller, and Christoph Scheepers.
2003b. Intra-sentential context effects on the inter-
pretation of logical metonymy. Cognitive Science,
27(4):649?668.
Lillian Lee. 1999. Measures of Distributional Similarity.
In Proceedings of the 37th annual meeting of the Associ-
ation for Computational Linguistics on Computational
Linguistics, College Park, MA.
Alessandro Lenci. 2008. Distributional semantics in lin-
guistic and cognitive research. From context to mean-
ing: Distributional models of the lexicon in linguistics
and cognitive science. Special issue of the Italian Jour-
nal of Linguistics, 20(1):1?31.
Alessandro Lenci. 2011. Composing and updating
verb argument expectations: A distributional semantic
model. In Proceedings of the 2nd Workshop on Cog-
nitive Modeling and Computational Linguistics, pages
58?66, Portland, Oregon.
Ping Li, Igor Farkas, and Brian MacWhinney. 2004. Early
lexical development in a self-organizing neural network.
Neural Networks, 17:1345?1362.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL, pages
768?774, Montreal, QC.
78
Ken McRae and Kazunaga Matsuki. 2009. People use
their knowledge of common events to understand lan-
guage, and do so as quickly as possible. Language and
Linguistics Compass, 3/6:1417?1429.
George A. Miller and Walter G. Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1?28.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Sebastian Pad? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33:161?199, June.
Barbara H. Partee, Alice ter Meulen, and Robert E. Wall.
1993. Mathematical Methods in Linguistics. Kluwer.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a Probabilistic Class-Based Lexicon for Lexical
Ambiguity Resolution. In Proceedings of COLING
2000, Saarbr?cken, Germany.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press.
Liina Pylkk?nen and Brian McElree. 2006. The syntax-
semantic interface: On-line composition of sentence
meaning. In Handbook of Psycholinguistics, pages
537?577. Elsevier.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of IJCNLP 2011, Chiang
Mai, Thailand.
Eleanor Rosch. 1975. Cognitive representations of seman-
tic categories. Journal of Experimental Psychology:
General, 104:192?233.
Hinrich Sch?tze. 1992. Dimensions of meaning. In
Proceedings of Supercomputing ?92, pages 787 ?796.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Gabriella Vigliocco, David P. Vinson, William Lewis,
and Merrill F. Garrett. 2004. Representing the mean-
ings of object and action words: The featural and uni-
tary semantic space hypothesis. Cognitive Psychology,
48(4):422?488.
Alessandra Zarcone and Alessandro Lenci. 2008. Compu-
tational models for event type classification in context.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
Marrakech, Morocco. ELRA.
Alessandra Zarcone and Sebastian Pad?. 2011. General-
ized event knowledge in logical metonymy resolution.
In Proceedings of the 33rd Annual Conference of the
Cognitive Science Society, pages 944?949, Austin, TX.
Alessandra Zarcone, Sebastian Pad?, and Alessandro
Lenci. 2012. Inferring covert events in logical
metonymies: a probe recognition experiment. In Pro-
ceedings of the 34th Annual Conference of the Cogni-
tive Science Society, Austin, TX.
79
