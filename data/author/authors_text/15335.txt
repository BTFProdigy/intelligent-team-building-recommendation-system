Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 21?24,
New York, June 2006. c?2006 Association for Computational Linguistics
Class Model Adaptation for Speech Summarisation
Pierre Chatain, Edward W.D. Whittaker, Joanna Mrozinski and Sadaoki Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552, Japan
{pierre, edw, mrozinsk, furui}@furui.cs.titech.ac.jp
Abstract
The performance of automatic speech
summarisation has been improved in pre-
vious experiments by using linguistic
model adaptation. We extend such adapta-
tion to the use of class models, whose ro-
bustness further improves summarisation
performance on a wider variety of objec-
tive evaluation metrics such as ROUGE-2
and ROUGE-SU4 used in the text sum-
marisation literature. Summaries made
from automatic speech recogniser tran-
scriptions benefit from relative improve-
ments ranging from 6.0% to 22.2% on all
investigated metrics.
1 Introduction
Techniques for automatically summarising written
text have been actively investigated in the field of
natural language processing, and more recently new
techniques have been developed for speech sum-
marisation (Kikuchi et al, 2003). However it is
still very hard to obtain good quality summaries.
Moreover, recognition accuracy is still around 30%
on spontaneous speech tasks, in contrast to speech
read from text such as broadcast news. Spontaneous
speech is characterised by disfluencies, repetitions,
repairs, and fillers, all of which make recognition
and consequently speech summarisation more diffi-
cult (Zechner, 2002). In a previous study (Chatain
et al, 2006), linguistic model (LiM) adaptation us-
ing different types of word models has proved use-
ful in order to improve summary quality. However
sparsity of the data available for adaptation makes it
difficult to obtain reliable estimates of word n-gram
probabilities. In speech recognition, class models
are often used in such cases to improve model ro-
bustness. In this paper we extend the work previ-
ously done on adapting the linguistic model of the
speech summariser by investigating class models.
We also use a wider variety of objective evaluation
metrics to corroborate results.
2 Summarisation Method
The summarisation system used in this paper is es-
sentially the same as the one described in (Kikuchi
et al, 2003), which involves a two step summarisa-
tion process, consisting of sentence extraction and
sentence compaction. Practically, only the sentence
extraction part was used in this paper, as prelimi-
nary experiments showed that compaction had little
impact on results for the data used in this study.
Important sentences are first extracted accord-
ing to the following score for each sentence
W = w1, w2, ..., wn, obtained from the automatic
speech recognition output:
S(W ) =
1
N
N?
i=1
{?CC(wi)+?II(wi)+?LL(wi)},
(1)
where N is the number of words in the sentence
W , and C(wi), I(wi) and L(wi) are the confidence
score, the significance score and the linguistic score
of word wi, respectively. ?C , ?I and ?L are the
respective weighting factors of those scores, deter-
mined experimentally.
For each word from the automatic speech recogni-
21
tion transcription, a logarithmic value of its posterior
probability, the ratio of a word hypothesis probabil-
ity to that of all other hypotheses, is calculated using
a word graph obtained from the speech recogniser
and used as a confidence score.
For the significance score, the frequencies of oc-
currence of 115k words were found using the WSJ
and the Brown corpora.
In the experiments in this paper we modified the
linguistic component to use combinations of dif-
ferent linguistic models. The linguistic component
gives the linguistic likelihood of word strings in
the sentence. Starting with a baseline LiM (LiMB)
we perform LiM adaptation by linearly interpolat-
ing the baseline model with other component mod-
els trained on different data. The probability of a
given n-gram sequence then becomes:
P (wi|wi?n+1..wi?1) = ?1P1(wi|wi?n+1..wi?1)
+... + ?nPn(wi|wi?n+1..wi?1), (2)
where
?
k ?k = 1 and ?k and Pk are the weight and
the probability assigned by model k.
In the case of a two-sided class-based model,
Pk(wi|wi?n+1..wi?1) = Pk(wi|C(wi)) ?
Pk(C(wi)|C(wi?n+1)..C(wi?1)), (3)
where Pk(wi|C(wi)) is the probability of the
word wi belonging to a given class C, and
Pk(C(wi)|C(wi?n+1)..C(wi?1)) the probability of
a certain word class C(wi) to appear after a history
of word classes, C(wi?n+1), ..., C(wi?1).
Different types of component LiM are built, com-
ing from different sources of data, either as word
or class models. The LiMB and component LiMs
are then combined for adaptation using linear inter-
polation as in Equation (2). The linguistic score is
then computed using this modified probability as in
Equation (4):
L(wi) = logP (wi|wi?n+1..wi?1). (4)
3 Evaluation Criteria
3.1 Summarisation Accuracy
To automatically evaluate the summarised speeches,
correctly transcribed talks were manually sum-
marised, and used as the correct targets for evalua-
tion. Variations of manual summarisation results are
merged into a word network, which is considered to
approximately express all possible correct summari-
sations covering subjective variations. The word ac-
curacy of automatic summarisation is calculated as
the summarisation accuracy (SumACCY) using the
word network (Hori et al, 2003):
Accuracy = (Len?Sub?Ins?Del)/Len?100[%],
(5)
where Sub is the number of substitution errors, Ins
is the number of insertion errors, Del is the number
of deletion errors, and Len is the number of words
in the most similar word string in the network.
3.2 ROUGE
Version 1.5.5 of the ROUGE scoring algorithm
(Lin, 2004) is also used for evaluating results.
ROUGE F-measure scores are given for ROUGE-
2 (bigram), ROUGE-3 (trigram), and ROUGE-SU4
(skip-bigram), using the model average (average
score across all references) metric.
4 Experimental Setup
Experiments were performed on spontaneous
speech, using 9 talks taken from the Translanguage
English Database (TED) corpus (Lamel et al, 1994;
Wolfel and Burger, 2005), each transcribed and
manually summarised by nine different humans for
both 10% and 30% summarization ratios. Speech
recognition transcriptions (ASR) were obtained for
each talk, with an average word error rate of 33.3%.
A corpus consisting of around ten years of con-
ference proceedings (17.8M words) on the subject
of speech and signal processing is used to generate
the LiMB and word classes using the clustering al-
gorithm in (Ney et al, 1994).
Different types of component LiM are built and
combined for adaptation as described in Section 2.
The first type of component linguistic models are
built on the small corpus of hand-made summaries
described above, made for the same summarisation
ratio as the one we are generating. For each talk
the hand-made summaries of the other eight talks
(i.e. 72 summaries) were used as the LiM training
corpus. This type of LiM is expected to help gener-
ate automatic summaries in the same style as those
made manually.
22
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 34.4 0.104 0.055 0.142 - - - -
Word 63.1 0.186 0.130 0.227 67.8 0.193 0.140 0.228
Class 65.1 0.195 0.131 0.226 72.6 0.210 0.143 0.234
Mixed 63.6 0.186 0.128 0.218 71.8 0.211 0.139 0.231
30% Random 71.2 0.294 0.198 0.331 - - - -
Word 81.6 0.365 0.271 0.395 83.3 0.365 0.270 0.392
Class 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
Mixed 83.1 0.374 0.279 0.407 92.9 0.415 0.325 0.442
Table 1: TRS baseline and adapted results.
The second type of component linguistic models
are built from the papers in the conference proceed-
ings for the talk we want to summarise. This type
of LiM, used for topic adaptation, is investigated be-
cause key words and important sentences that appear
in the associated paper are expected to have a high
information value and should be selected during the
summarisation process.
Three sets of experiments were made: in the first
experiment (referred to as Word), LiMB and both
component models are word models, as introduced
in (Chatain et al, 2006). For the second one (Class),
both LiMB and the component models are class
models built using exactly the same data as the word
models. For the third experiment (Mixed), the LiMB
is an interpolation of class and word models, while
the component LiMs are class models.
To optimise use of the available data, a rotating
form of cross-validation (Duda and Hart, 1973) is
used: all talks but one are used for development, the
remaining talk being used for testing. Summaries
from the development talks are generated automati-
cally by the system using different sets of parameters
and the LiMB . These summaries are evaluated and
the set of parameters which maximises the develop-
ment score for the LiMB is selected for the remain-
ing talk. The purpose of the development phase is
to choose the most effective combination of weights
?C , ?I and ?L. The summary generated for each
talk using its set of optimised parameters is then
evaluated using the same metric, which gives us our
baseline for this talk. Using the same parameters as
those that were selected for the baseline, we gener-
ate summaries for the lectures in the development set
for different LiM interpolation weights ?k. Values
between 0 and 1 in steps of 0.1, were investigated
for the latter, and an optimal set of ?k is selected.
Using these interpolation weights, as well as the set
of parameters determined for the baseline, we gen-
erate a summary of the test talk, which is evaluated
using the same evaluation metric, giving us our fi-
nal adapted result for this talk. Averaging those re-
sults over the test set (i.e. all talks) gives us our final
adapted result.
This process is repeated for all evaluation metrics,
and all three experiments (Word, Class, and Mixed).
Lower bound results are given by random sum-
marisation (Random) i.e. randomly extracting sen-
tences and words, without use of the scores present
in Equation (1) for appropriate summarisation ratios.
5 Results
5.1 TRS Results
Initial experiments were made on the human tran-
scriptions (TRS), and results are given in Table 1.
Experiments on word models (Word) show relative
improvements in terms of SumACCY of 7.5% and
2.1% for the 10% and 30% summarisation ratios, re-
spectively. ROUGE metrics, however, do not show
any significant improvement.
Using class models (Class and Mixed), for all
ROUGE metrics, relative improvements range from
3.5% to 13.4% for the 10% summarisation ratio, and
from 8.6% to 16.5% on the 30% summarisation ra-
tio. For SumACCY, relative improvements between
11.5% to 12.9% are observed.
5.2 ASR Results
ASR results for each experiment are given in Ta-
ble 2 for appropriate summarisation ratios. As for
23
Baseline Adapted
SumACCY R-2 R-3 R-SU4 SumACCY R-2 R-3 R-SU4
10% Random 33.9 0.095 0.042 0.140 - - - -
Word 48.6 0.143 0.064 0.182 49.8 0.129 0.060 0.173
Class 50.0 0.133 0.063 0.170 55.1 0.156 0.077 0.193
Mixed 48.5 0.134 0.068 0.176 56.2 0.142 0.077 0.191
30% Random 56.1 0.230 0.124 0.283 - - - -
Word 66.7 0.265 0.157 0.314 68.7 0.271 0.161 0.328
Class 66.1 0.277 0.165 0.324 71.1 0.300 0.180 0.348
Mixed 64.9 0.268 0.160 0.312 70.5 0.304 0.192 0.351
Table 2: ASR baseline and adapted results.
the TRS, LiM adaptation showed improvements in
terms of SumACCY, but ROUGE metrics do not cor-
roborate those results for the 10% summarisation ra-
tio. Using class models, for all ROUGE metrics, rel-
ative improvements range from 6.0% to 22.2% and
from 7.4% to 20.0% for the 10% and 30% summari-
sation ratios, respectively. SumACCY relative im-
provements range from 7.6% to 15.9%.
6 Discussion
Compared to previous experiments using only word
models, improvements obtained using class models
are larger and more significant for both ROUGE and
SumACCY metrics. This can be explained by the
fact that the data we are performing adaptation on
is very sparse, and that the nine talks used in these
experiments are quite different from each other, es-
pecially since the speakers also vary in style. Class
models are more robust to this spontaneous speech
aspect than word models, since they generalise bet-
ter to unseen word sequences.
There is little difference between the Class and
Mixed results, since the development phase assigned
most weight to the class model component in the
Mixed experiment, making the results quite similar
to those of the Class experiment.
7 Conclusion
In this paper we have investigated linguistic model
adaptation using different sources of data for an au-
tomatic speech summarisation system. Class mod-
els have proved to be much more robust than word
models for this process, and relative improvements
ranging from 6.0% to 22.2% were obtained on a va-
riety of evaluation metrics on summaries generated
from automatic speech recogniser transcriptions.
Acknowledgements: The authors would like to
thank M. Wo?lfel for the recogniser transcriptions
and C. Hori for her work on two stage summarisa-
tion and gathering the TED corpus data. This work
is supported by the 21st Century COE Programme.
References
P. Chatain, E.W.D. Whittaker, J. Mrozinski, and S. Fu-
rui. 2006. Topic and Stylistic Adaptation for Speech
Summarization. Proc. ICASSP, Toulouse, France.
R. Duda and P. Hart. 1973. Pattern Classification and
Scene Analysis. Wiley, New York.
C. Hori, T. Hori, and S. Furui. 2003. Evaluation
Method for Automatic Speech Summarization. Proc.
Eurospeech, Geneva, Switzerland, 4:2825?2828.
T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic
Speech Summarization based on Sentence Extraction
and Compaction. Proc. ICASSP, Hong Kong, China,
1:236?239.
L. Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Till-
mann. 1994. The Translanguage English Database
(TED). Proc. ICSLP, Yokohama, Japan, 4:1795?1798.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. Proc. WAS, Barcelona,
Spain.
H. Ney, U. Essen, and R. Kneser. 1994. On Structur-
ing Probabilistic Dependences in Stochastic Language
Modelling. Computer Speech and Language, (8):1?
38.
M. Wolfel and S. Burger. 2005. The ISL Baseline Lec-
ture Transcription System for the TED Corpus. Tech-
nical report, Karlsruhe University.
K. Zechner. 2002. Summarization of Spoken Language-
Challenges, Methods, and Prospects. Speech Technol-
ogy Expert eZine, Issue.6.
24
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 288?291,
New York City, June 2006. c?2006 Association for Computational Linguistics
Factoid Question Answering with Web, Mobile and Speech Interfaces
E.W.D. Whittaker J. Mrozinski S. Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1, Ookayama, Meguro-ku
Tokyo 152-8552 Japan
 
edw,mrozinsk,furui  @furui.cs.titech.ac.jp
Abstract
In this paper we describe the web and
mobile-phone interfaces to our multi-
language factoid question answering (QA)
system together with a prototype speech
interface to our English-language QA sys-
tem. Using a statistical, data-driven ap-
proach to factoid question answering has
allowed us to develop QA systems in five
languages in a matter of months. In the
web-based system, which is accessible
at http://asked.jp, we have com-
bined the QA system output with standard
search-engine-like results by integrating it
with an open-source web search engine.
The prototype speech interface is based
around a VoiceXML application running
on the Voxeo developer platform. Recog-
nition of the user?s question is performed
on a separate speech recognition server
dedicated to recognizing questions. An
adapted version of the Sphinx-4 recog-
nizer is used for this purpose. Once the
question has been recognized correctly it
is passed to the QA system and the re-
sulting answers read back to the user by
speech synthesis. Our approach is mod-
ular and makes extensive use of open-
source software. Consequently, each com-
ponent can be easily and independently
improved and easily extended to other lan-
guages.
1 Introduction
The approach to factoid question answering (QA)
that we adopt was first described in (Whittaker et
al., 2005b) where the details of the mathematical
model and how it was trained for English were
given. The approach has been successfully evalu-
ated in the 2005 text retrieval conference (TREC)
question answering track evaluations (Voorhees and
Trang Dang, 2005) where our group placed eleventh
out of thirty participants (Whittaker et al, 2005a).
Although the TREC QA task is substantially differ-
ent to web-based QA this evaluation showed that our
approach works and provides an objective assess-
ment of its quality. Similarly, for our Japanese lan-
guage system we have evaluated the performance of
our approach on the NTCIR-3 QAC-1 task (Whit-
taker et al, 2005c). Although our Japanese ex-
periments were applied retrospectively, the results
would have placed us in the mid-range of partici-
pating systems. In (Whittaker et al, 2006b) we de-
scribed how our approach could be used for the rapid
development of web-based QA systems in five very
different languages. It was shown that a developer
proficient with the tools, and with access to suitable
training data, could build a system in a new language
in around 10 hours. In (Whittaker et al, 2006a) we
evaluated the performance of the systems for four of
our five languages. We give a brief summary of our
approach to QA in Section 2.
In this paper we introduce our web-based
QA system which is publicly accessible at
http://asked.jp, permitting questions in En-
glish, Japanese, Chinese, Russian and Swedish and
288
is discussed in Section 3. Since answers in factoid
QA are inherently well-suited to display on small
screens we have also made a mobile-phone interface
which is accessible at the same address when using
an HTML browser from a mobile phone. This is dis-
cussed in Section 4. There are several other QA sys-
tems on the web including Brainboost (Brainboost,
2005) and Lexxe (Lexxe, 2005) but they only try to
answer questions in English and do not have conve-
nient mobile interfaces.
Entering whole questions rather than just key-
words is tedious especially on a mobile-phone so
we have also begun to look at speech interfaces. In
this paper we describe a prototype speech interface
to our English-language QA system. This prototype
is currently intended primarily as a platform for fur-
ther research into speech recognition and answering
of questions from an acoustic modelling point-of-
view (e.g. low-bandwidth, low-quality VoIP chan-
nel), from a language modelling perspective (e.g. ir-
regular word order in questions vs. text, and very
large out-of-vocabulary problem) and also in terms
of dialog modelling. There have been several at-
tempts at speech interfaces to QA systems in the lit-
erature e.g. (Schofield and Zheng, 2003) but as far
as we know ours is the only system that is publicly
accessible. We discuss this interface in Section 5.
2 Statistical pattern classification
approach to QA
The answer to a question depends primarily on the
question itself but also on many other factors such
as the person asking the question, the location of the
person, what questions the person has asked before,
and so on. For simplicity, we choose to consider
only the dependence of an answer   on the question

. In particular, we hypothesize that the answer  
depends on two sets of features extracted from

:


	
and  

	
as follows:

 

	

 


	
 (1)
where

can be thought of as a set of Proceedings of ACL-08: HLT, pages 443?451,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Collecting a Why-question corpus for development and evaluation of an
automatic QA-system
Joanna Mrozinski Edward Whittaker
Department of Computer Science
Tokyo Institute of Technology
2-12-1-W8-77 Ookayama, Meguro-ku
Tokyo 152-8552 Japan
{mrozinsk,edw,furui}@furui.cs.titech.ac.jp
Sadaoki Furui
Abstract
Question answering research has only recently
started to spread from short factoid questions
to more complex ones. One significant chal-
lenge is the evaluation: manual evaluation is a
difficult, time-consuming process and not ap-
plicable within efficient development of sys-
tems. Automatic evaluation requires a cor-
pus of questions and answers, a definition of
what is a correct answer, and a way to com-
pare the correct answers to automatic answers
produced by a system. For this purpose we
present a Wikipedia-based corpus of Why-
questions and corresponding answers and arti-
cles. The corpus was built by a novel method:
paid participants were contacted through a
Web-interface, a procedure which allowed dy-
namic, fast and inexpensive development of
data collection methods. Each question in the
corpus has several corresponding, partly over-
lapping answers, which is an asset when es-
timating the correctness of answers. In ad-
dition, the corpus contains information re-
lated to the corpus collection process. We be-
lieve this additional information can be used to
post-process the data, and to develop an auto-
matic approval system for further data collec-
tion projects conducted in a similar manner.
1 Introduction
Automatic question answering (QA) is an alternative
to traditional word-based search engines. Instead of
returning a long list of documents more or less re-
lated to the query parameters, the aim of a QA sys-
tem is to isolate the exact answer as accurately as
possible, and to provide the user only a short text
clip containing the required information.
One of the major development challenges is eval-
uation. The conferences such as TREC1, CLEF2
and NTCIR3 have provided valuable QA evaluation
methods, and in addition produced and distributed
corpora of questions, answers and corresponding
documents. However, these conferences have fo-
cused mainly on fact-based questions with short an-
swers, so called factoid questions. Recently more
complex tasks such as list, definition and discourse-
based questions have also been included in TREC in
a limited fashion (Dang et al, 2007). More complex
how- and why-questions (for Asian languages) were
also included in the NTCIR07, but the provided data
comprised only 100 questions, of which some were
also factoids (Fukumoto et al, 2007). Not only is
the available non-factoid data quite limited in size,
it is also questionable whether the data sets are us-
able in development outside the conferences. Lin
and Katz (2006) suggest that training data has to be
more precise, and, that it should be collected, or at
least cleaned, manually.
Some corpora of why-questions have been col-
lected manually: corpora described in (Verberne et
al., 2006) and (Verberne et al, 2007) both com-
prise fewer than 400 questions and corresponding
answers (one or two per question) formulated by na-
tive speakers. However, we believe one answer per
question is not enough. Even with factoid questions
it is sometimes difficult to define what is a correct
1http://trec.nist.gov/
2http://www.clef-campaign.org/
3http://research.nii.ac.jp/ntcir/
443
answer, and complex questions result in a whole new
level of ambiguity. Correctness depends greatly on
the background knowledge and expectations of the
person asking the question. For example, a correct
answer to the question ?Why did Mr. X take Ms. Y
to a coffee shop?? could be very different depending
on whether we knew that Mr. X does not drink cof-
fee or that he normally drinks it alone, or that Mr. X
and Ms. Y are known enemies.
The problem of several possible answers and, in
consequence, automatic evaluation has been tackled
for years within another field of study: automatic
summarisation (Hori et al, 2003; Lin and Hovy,
2003). We believe that the best method of provid-
ing ?correct? answers is to do what has been done in
that field: combine a multitude of answers to ensure
both diversity and consensus among the answers.
Correctness of an answer is also closely related to
the required level of detail. The Internet FAQ pages
were successfully used to develop QA-systems (Jijk-
oun and de Rijke, 2005; Soricut and Brill, 2006), as
have the human-powered question sites such as An-
swers.com, Yahoo Answers and Google Answers,
where individuals can post questions and receive an-
swers from peers (Mizuno et al, 2007). Both re-
sources can be assumed to contain adequately error-
free information. FAQ pages are created so as to
answer typical questions well enough that the ques-
tions do not need to be repeated. Question sites typ-
ically rank the answers and offer bonuses for peo-
ple providing good ones. However, both sites suffer
from excess of information. FAQ-pages tend to also
answer questions which are not asked, and also con-
tain practical examples. Human-powered answers
often contain unrelated information and discourse-
like elements. Additionally, the answers do not al-
ways have a connection to the source material from
which they could be extracted.
One purpose of our project was to take part in
the development of QA systems by providing the
community with a new type of corpus. The cor-
pus includes not only the questions with multiple
answers and corresponding articles, but also certain
additional information that we believe is essential to
enhance the usability of the data.
In addition to providing a new QA corpus, we
hope our description of the data collection process
will provide insight, resources and motivation for
further research and projects using similar collection
methods. We collected our corpus through Amazon
Mechanical Turk service 4 (MTurk). The MTurk
infrastructure allowed us to distribute our tasks to
a multitude of workers around the world, without
the burden of advertising. The system also allowed
us to test the workers suitability, and to reward the
work without the bureaucracy of employment. To
our knowledge, this is the first time that the MTurk
service has been used in equivalent purpose.
We conducted the data collection in three steps:
generation, answering and rephrasing of questions.
The workers were provided with a set of Wikipedia
articles, based on which the questions were created
and the answers determined by sentence selection.
The WhyQA-corpus consists of three parts: original
questions along with their rephrased versions, 8-10
partly overlapping answers for each question, and
the Wikipedia articles including the ones corre-
sponding to the questions. The WhyQA-corpus is
in XML-format and can be downloaded and used
under the GNU Free Documentation License from
www.furui.cs.titech.ac.jp/ .
2 Setup
Question-answer pairs have previously been gen-
erated for example by asking workers to both ask
a question and then answer it based on a given
text (Verberne et al, 2006; Verberne et al, 2007).
We decided on a different approach for two reasons.
Firstly, based on our experience such an approach is
not optimal in the MTurk framework. The tasks that
were welcomed by workers required a short atten-
tion span, and reading long texts was negatively re-
ceived with many complaints, sloppy work and slow
response times. Secondly, we believe that the afore-
mentioned approach can produce unnatural ques-
tions that are not actually based on the information
need of the workers.
We divided the QA-generation task into two
phases: question-generation (QGenHIT) and an-
swering (QAHIT). We also trimmed the amount of
the text that the workers were required to read to cre-
ate the questions. These measures were taken both
in order to lessen the cognitive burden of the task
4http://www.mturk.com
444
and to produce more natural questions.
In the first phase the workers generated the ques-
tions based on a part of Wikipedia article. The re-
sulting questions were then uploaded to the system
as new HITs with the corresponding articles, and
answered by available (different) workers. Our hy-
pothesis is that the questions are more natural if their
answer is not known at the time of the creation.
Finally, in an additional third phase, 5 rephrased
versions of each question were created in order to
gain variation (QRepHIT). The data quality was en-
sured by requiring the workers to achieve a certain
result from a test (or a Qualification) before they
could work on the aforementioned tasks.
Below we explain the MTurk system, and then our
collection process in detail.
2.1 Mechanical Turk
Mechanical Turk is a Web-based service, offered by
Amazon.com, Inc. It provides an API through which
employers can obtain a connection to people to per-
form a variety of simple tasks. With tools provided
by Amazon.com, the employer creates tasks, and up-
loads them to the MTurk Web-site. Workers can then
browse the tasks and, if they find them profitable
and/or interesting enough, work on them. When the
tasks are completed, the employer can download the
results, and accept or reject them. Some key con-
cepts of the system are listed below, with short de-
scriptions of the functionality.
? HIT Human Intelligence Task, the unit of a
payable chore in MTurk.
? Requester An ?employer?, creates and uploads
new HITs and rewards the workers. Requesters
can upload simple HITs through the MTurk Re-
quester web site, and more complicated ones
through the MTurk Web Service APIs.
? Worker An ?employee?, works on the hits
through the MTurk Workers? web site.
? Assignment. One HIT consists of one or more
assignments. One worker can complete a sin-
gle HIT only once, so if the requester needs
multiple results per HIT, he needs to set the
assignment-count to the desired figure. A HIT
is considered completed when all the assign-
ments have been completed.
? Rewards At upload time, each HIT has to be
assigned a fixed reward, that cannot be changed
later. Minimum reward is $0.01. Amazon.com
collects a 10% (or a minimum of $0.05) service
fee per each paid reward.
? Qualifications To improve the data quality,
a HIT can also be attached to certain tests,
?qualifications? that are either system-provided
or created by the requester. An example of
a system-provided qualification is the average
approval ratio of the worker.
Even if it is possible to create tests that workers
have to pass before being allowed to work on a HIT
so as to ensure the worker?s ability, it is impossible
to test the motivation (for instance, they cannot be
interviewed). Also, as they are working through the
Web, their working conditions cannot be controlled.
2.2 Collection process
The document collection used in our research was
derived from the Wikipedia XML Corpus by De-
noyer and Gallinari (2006). We selected a total of
84 articles, based on their length and contents. A
certain length was required so that we could expect
the article to contain enough interesting material to
produce a wide selection of natural questions. The
articles varied in topic, degree of formality and the
amount of details; from ?Horror film? and ?Christ-
mas worldwide? to ?G-Man (Half-Life)? and ?His-
tory of London?. Articles consisting of bulleted lists
were removed, but filtering based on the topic of the
article was not performed. Essentially, the articles
were selected randomly.
2.2.1 QGenHIT
The first phase of the question-answer generation
was to generate the questions. In QGenHIT we pre-
sented the worker with only part of a Wikipedia ar-
ticle, and instructed them to think of a why-question
that they felt could be answered based on the origi-
nal, whole article which they were not shown. This
approach was expected to lead to natural curiosity
and questions. Offering too little information would
have lead to many questions that would finally be
left unanswered, and it also did not give the workers
enough to work on. Giving too much information
445
Qualification The workers were required to pass a test before working on the HITs.
QGenHIT Questions were generated based on partial Wikipedia articles. These questions were
then used to create the QAHITs.
QAHIT Workers were presented with a question and a corresponding article. The task was to
answer the questions (if possible) through sentence selection.
QRepHIT To ensure variation in the questions, each question was rephrased by 5 different workers.
Table 1: Main components of the corpus collection process.
Article topic: Fermi paradox
Original question Why is the moon crucial to the rare earth hypothesis?
Rephrased Q 1 How does the rare earth theory depend upon the moon?
Rephrased Q 2 What makes the moon so important to rare earth theory?
Rephrased Q 3 What is the crucial regard for the moon in the rare earth hypothesis?
Rephrased Q 4 Why is the moon so important in the rare earth hypothesis?
Rephrased Q 5 What makes the moon necessary, in regards to the rare earth hypothesis?
Answer 1. Sentence ids: 20,21. Duplicates: 4. The moon is important because its gravitational pull
creates tides that stabilize Earth?s axis. Without this stability, its variation, known as precession of
the equinoxes, could cause weather to vary so dramatically that it could potentially suppress the more
complex forms of life.
Answer 2. Sentence ids: 18,19,20. Duplicates: 2. The popular Giant impact theory asserts that it
was formed by a rare collision between the young Earth and a Mars-sized body, usually referred to as
Orpheus or Theia, approximately 4.45 billion years ago. The collision had to occur at a precise angle,
as a direct hit would have destroyed the Earth, and a shallow hit would have deflected the Mars-sized
body. The moon is important because its gravitational pull creates tides that stabilize Earth?s axis.
Answer 3. Sentence ids: 20,21,22. Duplicates: 2. The moon is important because its gravitational
pull creates tides that stabilize Earth?s axis. Without this stability, its variation, known as precession
of the equinoxes, could cause weather to vary so dramatically that it could potentially suppress the
more complex forms of life. The heat generated by the Earth/Theia impact, as well as subsequent
Lunar tides, may have also significantly contributed to the total heat budget of the Earth?s interior,
thereby both strengthening and prolonging the life of the dynamos that generate Earth?s magnetic field
Dynamo 1.
Answer 4. Sentence ids: 18,20,21. No duplicates. The popular Giant impact theory asserts that
it was formed by a rare collision between the young Earth and a Mars-sized body, usually referred
to as Orpheus or Theia, approximately 4.45 billion years ago. The moon is important because its
gravitational pull creates tides that stabilize Earth?s axis. Without this stability, its variation, known
as precession of the equinoxes, could cause weather to vary so dramatically that it could potentially
suppress the more complex forms of life.
Answer 5. Sentence ids: 18,21. No duplicates. The popular Giant impact theory asserts that it
was formed by a rare collision between the young Earth and a Mars-sized body, usually referred to as
Orpheus or Theia, approximately 4.45 billion years ago. Without this stability, its variation, known
as precession of the equinoxes, could cause weather to vary so dramatically that it could potentially
suppress the more complex forms of life.
Table 2: Data example: Question with rephrased versions and answers.
446
(long excerpts from the articles) was severely dis-
liked among the workers simply because it took a
long time to read.
We finally settled on a solution where the partial
content consisted of the title and headers of the arti-
cle, along with the first sentences of each paragraph.
The instructions to the questions demanded rigidly
that the question starts with the word ?Why?, as it
was surprisingly difficult to explain what we meant
by why-questions if the question word was not fixed.
The reward per HIT was $0.04, and 10 questions
were collected for each article. We did not force the
questions to be different, and thus in the later phase
some of the questions were removed manually as
they were deemed to mean exactly the same thing.
However, there were less than 30 of these duplicate
questions in the whole data set.
2.2.2 QAHIT
After generating the questions based on partial ar-
ticles, the resulting questions were uploaded to the
system as HITs. Each of these QAHITs presented a
single question with the corresponding original arti-
cle. The worker?s task was to select either 1-3 sen-
tences from the text, or a No-answer-option (NoA).
Sentence selection was conducted with Javascript
functionality, so the workers had no chance to in-
clude freely typed information within the answer (al-
though a comment field was provided). The reward
per HIT was $0.06. At the beginning, we collected
10 answers per question, but we cut that down to 8
because the HITs were not completed fast enough.
The workers for QAHITs were drawn from the
same pool as the workers for QGenHIT, and it was
possible for the workers to answer the questions they
had generated themselves.
2.2.3 QRepHIT
As the final step 5 rephrased versions of each
question were generated. This was done to com-
pensate the rigid instructions of the QGenHIT and
to ensure variation in the questions. We have not yet
measured how well the rephrased questions match
the answers of their original versions. In the final
QRepHIT questions were grouped into groups of 5.
Each HIT consisted of 5 assignments, and a $0.05
reward was offered for each HIT.
QRepHIT required the least amount of design and
trials, and workers were delighted with the task. The
HITs were completed fast and well even in the case
when we accidentally uploaded a set of HITs with
no reward.
As with QAHIT, the worker pool for creating and
rephrasing questions was the same. The questions
were rephrased by their creator in 4 cases.
2.3 Qualifications
To improve the data quality, we used the qualifi-
cations to test the workers. For the QGenHITs we
only used the system-provided ?HIT approval rate?-
qualification. Only workers whose previous work
had been approved in 80% of the cases were able to
work on our HITs.
In addition to the system-provided qualification,
we created a why-question-specific qualification.
The workers were presented with 3 questions, and
they were to answer each by either selecting 1-
3 most relevant sentences from a list of about 10
sentences, or by deciding that there is no answer
present. The possible answer-sentences were di-
vided into groups of essential, OK and wrong, and
one of the questions did quite clearly have no an-
swer. The scoring was such that it was impossible
to get approved results if not enough essential sen-
tences were included. Selecting sentences from the
OK-group only was not sufficient, and selecting sen-
tences from the wrong-group was penalized. A min-
imum score per question was required, but also the
total score was relevant ? component scores could
compensate each other up to a point. However, if
the question with no answer was answered, the score
could not be of an approvable level. This qualifica-
tion was, in addition to the minimum HIT approval
rate of 80%, a prerequisite for both the QRepHITs
and the QAHITs.
A total of 2355 workers took the test, and 1571
(67%) of them passed it, thus becoming our avail-
able worker pool. However, in the end the actual
number of different workers was only 173.
Examples of each HIT, their instructions and the
Qualification form are included in the final corpus.
The collection process is summarised in Table 1.
447
3 Corpus description
The final corpus consists of questions with their
rephrased versions and answers. There are total of
695 questions, of which 159 were considered unan-
swerable based on the articles, and 536 that have 8-
10 answers each. The total cost of producing the
corpus was about $350, consisting of $310 paid in
workers rewards and $40 in Mechanical Turk fees,
including all the trials conducted during the devel-
opment of the final system.
Also included is a set of Wikipedia documents
(WikiXML, about 660 000 articles or 670MB in com-
pressed format), including the ones corresponding to
the questions (84 documents). The source of Wik-
iXML is the English part of the Wikipedia XML
Corpus by Denoyer and Gallinari (2006). In the
original data some of the HTML-structures like lists
and tables occurred within sentences. Our sentence-
selection approach to QA required a more fine-
grained segmentation and for our purpose, much
of the HTML-information was redundant anyway.
Consequently we removed most of the HTML-
structures, and the table-cells, list-items and other
similar elements were converted into sentences.
Apart from sentence-information, only the section-
title information was maintained. Example data is
shown in Table 2.
3.1 Task-related information
Despite the Qualifications and other measures taken
in the collection phase of the corpus, we believe the
quality of the data remains open to question. How-
ever, the Mechanical Turk framework provided addi-
tional information for each assignment, for example
the time workers spent on the task. We believe this
information can be used to analyse and use our data
better, and have included it in the corpus to be used
in further experiments.
? Worker Id Within the MTurk framework, each
worker is assigned a unique id. Worker id can
be used to assign a reliability-value to the work-
ers, based on the quality of their previous work.
It was also used to examine whether the same
workers worked on the same data in different
phases: Of the original questions, only 7 were
answered and 4 other rephrased by the same
worker they were created by. However, it has
to be acknowledged that it is also possible for
one worker to have had several accounts in the
system, and thus be working under several dif-
ferent worker ids.
? Time On Task The MTurk framework also
provides the requester the time it took for the
worker to complete the assignment after ac-
cepting it. This information is also included in
the corpus, although it is impossible to know
precisely how much time the workers actually
spent on each task. For instance, it is possible
that one worker had several assignments open
at the same time, or that they were not concen-
trating fully on working on the task. A high
value of Time On Task thus does not necessar-
ily mean that the worker actually spent a long
time on it. However, a low value indicates that
he/she did only spend a short time on it.
? Reward Over the period spent collecting the
data, we changed the reward a couple of times
to speed up the process. The reward is reported
per HIT.
? Approval Status Within the collection pro-
cess we encountered some clearly unacceptable
work, and rejected it. The rejected work is also
included in the corpus, but marked as rejected.
The screening process was by no means per-
fect, and it is probable that some of the ap-
proved work should have been rejected.
? HIT id, Assignment id, Upload Time HIT and
assignment ids and original upload times of the
HITs are provided to make it possible to retrace
the collection steps if needed.
? Completion Time Completion time is the
timestamp of the moment when the task was
completed by a worker and returned to the sys-
tem. The time between the completion time
and the upload time is presumably highly de-
pendent on the reward, and on the appeal of the
task in question.
3.2 Quality experiments
As an example of the post-processing of the data,
we conducted some preliminary experiments on the
answer agreement between workers.
448
Out of the 695 questions, 159 were filtered out in
the first part of QAHIT. We first uploaded only 3 as-
signments, and the questions that 2 out of 3 work-
ers deemed unanswerable were filtered out. This
left 536 questions which were considered answered,
each one having 8-10 answers from different work-
ers. Even though in the majority of cases (83% of the
questions) one of the workers replied with the NoA,
the ones that answered did agree up to a point: of
all the answers, 72% were such that all of their sen-
tences were selected by at least two different work-
ers. On top of this, an additional 17% of answers
shared at least one sentence that was selected by
more than one worker.
To understand the agreement better, we also cal-
culated the average agreement of selected sentences
based on sentence ids and N-gram overlaps between
the answers. In both of these experiments, only
those 536 questions that were considered answer-
able were included.
3.2.1 Answer agreement on sentence ids
As the questions were answered by means of sen-
tence selection, the simplest method to check the
agreement between the workers was to compare
the ids of the selected sentences. The agreement
was calculated as follows: each answer was com-
pared to all the other answers for the same ques-
tion. For each case, the agreement was defined as
Agreement = CommonIdsAllIds , where CommonIds
is the number of sentence ids that existed in both
answers, and AllIds is the number of different ids
in both answers. We calculated the overall average
agreement ratio (Total Avg) and the average of the
best matches between two assignments within one
HIT (Best Match). We ran the test for two data sets:
The most typical case of the workers cheating was
to mark the question unaswerable. Because of this
the first data set included only the real answers, and
the NoAs were removed (NoA not included, 3872
answers). If an answer was compared with a NoA,
the agreement was 0, and if two NoAs were com-
pared, the agreement was 1. We did, however, also
include the figures for the whole data set (NoA in-
cluded, 4638 answers). The results are shown in Ta-
ble 3.
The Best Match -results were quite high com-
pared to the Total Avg. From this we can conclude
Total Avg Best Match
NoA not included 0.39 0.68
NoA included 0.34 0.68
Table 3: Answer agreement based on sentence ids.
that in the majority of cases, there was at least one
quite similar answer among those for that HIT. How-
ever, comparing the sentence ids is only an indica-
tive measure, and it does not tell the whole story
about agreement. For each document there may ex-
ist several separate sentences that contain the same
kind of information, and so two answers can be alike
even though the sentence ids do not match.
3.2.2 Answer agreement based on ROUGE
Defining the agreement over several passages of
texts has for a long time been a research prob-
lem within the field of automatic summarisation.
For each document it is possible to create several
summarisations that can each be considered cor-
rect. The problem has been approached by using
the ROUGE-metric: calculating the N-gram over-
lap between manual, ?correct? summaries, and the
automatic summaries. ROUGE has been proven to
correlate well with human evaluation (Lin and Hovy,
2003).
Overlaps of higher order N-grams are more usable
within speech summarisation as they take the gram-
matical structure and fluency of the summary into
account. When selecting sentences, this is not an is-
sue, so we decided to use only unigram and bigram
counts (Table 4: R-1, R2), as well as the skip-bigram
values (R-SU) and the longest common N-gram met-
ric R-L. We calculated the figures for two data sets
in the same way as in the case of sentence id agree-
ment. Finally, we set a lower bound for the results
by comparing the answers to each other randomly
(the NoAs were also included).
The final F-measures of the ROUGE results are
presented in Table 4. The figures vary from 0.37 to
0.56 for the first data set, and from 0.28 to 0.42 to
the second. It is debatable how the results should
be interpreted, as we have not defined a theoretical
upper bound to the values, but the difference to the
randomised results is substantial. In the field of au-
tomatic summarisation, the overlap of the automatic
449
results and corresponding manual summarisations is
generally much lower than the overlap between our
answers (Chali and Kolla, 2004). However, it is dif-
ficult to draw detailed conclusions based on compar-
ison between these two very different tasks.
R-1 R-2 R-SU R-L
NoA not included 0.56 0.46 0.37 0.52
NoA included 0.42 0.35 0.28 0.39
Random Answers 0.13 0.01 0.02 0.09
Table 4: Answer agreement: ROUGE-1, -2, -SU and -L.
The sentence agreement and ROUGE-figures do
not tell us much by themselves. However, they are
an example of a procedure that can be used to post-
process the data and in further projects of similar
nature. For example, the ROUGE similarity could
be used in the data collection phase as a tool of au-
tomatic approval and rejection of workers? assign-
ments.
4 Discussion and future work
During the initial trials of data collection we encoun-
tered some unexpected phenomena. For example,
increasing the reward did have a positive effect in
reducing the time it took for HITs to be completed,
however it did not correlate in desirable way with
data quality. Indeed the quality actually decreased
with increasing reward. We believe that this unex-
pected result is due to the distributed nature of the
worker pool in Mechanical Turk. Clearly the moti-
vation of some workers is other than monetary re-
ward. Especially if the HIT is interesting and can
be completed in a short period of time, it seems that
there are people willing to work on them even for
free.
MTurk requesters cannot however rely on this
voluntary workforce. From MTurk Forums it is clear
that some of the workers rely on the money they
get from completing the HITs. There seems to be a
critical reward-threshold after which the ?real work-
force?, i.e. workers who are mainly interested in per-
forming the HITs as fast as possible, starts to partic-
ipate. When the motivation changes from voluntary
participation to maximising the monetary gain, the
quality of the obtained results often understandably
suffers.
It would be ideal if a requester could rely on the
voluntary workforce alone for results, but in many
cases this may result either in too few workers and/or
too slow a rate of data acquisition. Therefore it is of-
ten necessary to raise the reward and rely on efficient
automatic validation of the data.
We have looked into the answer agreement of
the workers as an experimental post-processing step.
We believe that further work in this area will provide
the tools required for automatic data quality control.
5 Conclusions
In this paper we have described a dynamic and inex-
pensive method of collecting a corpus of questions
and answers using the Amazon Mechanical Turk
framework. We have provided to the community
a corpus of questions, answers and corresponding
documents, that we believe can be used in the de-
velopment of QA-systems for why-questions. We
propose that combining several answers from dif-
ferent people is an important factor in defining the
?correct? answer to a why-question, and to that goal
have included several answers for each question in
the corpus.
We have also included data that we believe is
valuable in post-processing the data: the work his-
tory of a single worker, the time spent on tasks, and
the agreement on a single HIT between a set of dif-
ferent workers. We believe that this information, es-
pecially the answer agreement of workers, can be
successfully used in post-processing and analysing
the data, as well as automatically accepting and re-
jecting workers? submissions in similar future data
collection exercises.
Acknowledgments
This study was funded by the Monbusho Scholar-
ship of Japanese Government and the 21st Century
COE Program ?Framework for Systematization and
Application of Large-scale Knowledge Resources
(COE-LKR)?
References
Yllias Chali and Maheedhar Kolla. 2004. Summariza-
tion Techniques at DUC 2004. In DUC2004.
Hoa Trang Dang, Diane Kelly, and Jimmy Lin. 2007.
Overview of the TREC 2007 Question Answering
450
Track. In E. Voorhees and L. P. Buckland, editors, Six-
teenth Text REtrieval Conference (TREC), Gaithers-
burg, Maryland, November.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An Overview of the 4th Ques-
tion Answering Challenge (QAC-4) at NTCIR work-
shop 6. In Proceedings of the Sixth NTCIR Workshop
Meeting, pages 433?440.
Chiori Hori, Takaaki Hori, and Sadaoki Furui. 2003.
Evaluation Methods for Automatic Speech Summa-
rization. In In Proc. EUROSPEECH, volume 4, pages
2825?2828, Geneva, Switzerland.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
Answers from Frequently Asked Questions Pages on
the Web. In CIKM ?05: Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 76?83, New York, NY, USA.
ACM Press.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Eval-
uation of Summaries Using N-gram Co-occurrence
Statistics. In Human Technology Conference (HLT-
NAACL), Edmonton, Canada.
Jimmy Lin and Boris Katz. 2006. Building a Reusable
Test Collection for Question Answering. J. Am. Soc.
Inf. Sci. Technol., 57(7):851?861.
Junta Mizuno, Tomoyosi Akiba, Atsushi Fujii, and
Katunobu Itou. 2007. Non-factoid Question Answer-
ing Experiments at NTCIR-6: Towards Answer Type
Detection for Realworld Questions. In Proceedings of
the 6th NTCIR Workshop Meeting on Evaluation of In-
formation Access Technologies, pages 487?492.
Radu Soricut and Eric Brill. 2006. Automatic Question
Answering Using the Web: Beyond the Factoid. Inf.
Retr., 9(2):191?206.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2006. Data for Question Answering:
the Case of Why. In LREC.
Susan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Discourse-based Answer-
ing of Why-questions. Traitement Automatique des
Langues, 47(2: Discours et document: traitements
automatiques):21?41.
451
Monolingual Web-based Factoid Question Answering in Chinese,
Swedish, English and Japanese
E.W.D. Whittaker J. Hamonic D. Yang T. Klingberg S. Furui
Dept. of Computer Science
Tokyo Institute of Technology
2-12-1, Ookayama, Meguro-ku
Tokyo 152-8552 Japan
 edw,yuuki,raymond,tor,furui@furui.cs.titech.ac.jp
Abstract
In this paper we extend the application
of our statistical pattern classification ap-
proach to question answering (QA) which
has previously been applied successfully
to English and Japanese to develop two
prototype QA systems in Chinese and
Swedish. We show what data is necessary
to achieve this and also evaluate the per-
formance of the two new systems using a
translation of the TREC 2003 factoid QA
task. While performance for Chinese and
Swedish is found to be lower than that for
the more developed English and Japanese
systems we explain why this is the case
and offer solutions for their improvement.
All systems form the basis of our pub-
licly accessible web-based multilingual
QA system at http://asked.jp.
1 Introduction
Much of the research into automatic question an-
swering (QA) has understandably concentrated on
the English language with little regard to portabil-
ity or efficacy in other languages. It is only rela-
tively recently, with the introduction of the CLEF
and NTCIR QA evaluations, that researchers have
started to look at porting and evaluating the tech-
niques that have been shown to work well for En-
glish to other languages.
One of the major drawbacks of porting an En-
glish language QA system or approach to other
languages is often the lack of the corresponding
NLP tools in the target language. For instance,
parsers and named-entity (NE)-taggers, which are
typical components in many QA systems, are cer-
tainly not available for all the world?s languages.
Trainable parsers and NE-taggers similarly require
appropriate training data which, if not available,
is costly and requires specialized knowledge to
produce. Language-specific databases are also a
common feature of many systems, some of which
have taken many man-years to construct and ver-
ify. Such a component in many English-language
systems, for example, is WordNet. While porting
WordNet to other languages has been started in the
Euro and Global WordNet projects they still only
cover a relatively small number of languages.
In this paper we describe the application of our
data-driven approach to QA which was developed
right from the outset with the aim of portability
and robustness in mind. This statistical pattern
classification approach to QA is essentially lan-
guage independent and trainable given appropriate
language-specific training data. No assumptions
about the language are made by the model except
that some notion of words or space-separated to-
kens must exist or be introduced where it is ab-
sent. Our only other requirements to build a QA
system in a new target language are: a large col-
lection of text data in the target language that can
be searched for answers (e.g. the web), and a list
of example questions-and-answers (q-and-a) in the
target language. Given these data sources the re-
maining components can be obtained automati-
cally for each language.
So, in contrast to other contemporary ap-
proaches to QA our English language system
does not use WordNet as in (Hovy et al, 2001;
Moldovan et al, 2002), NE extraction, or any
other linguistic information e.g. from semantic
analysis (Hovy et al, 2001) or from question pars-
ing (Hovy et al, 2001; Moldovan et al, 2002) and
uses capitalised (where appropriate for the lan-
guage) word tokens as the only features for mod-
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
45
elling. For our Japanese system, although we cur-
rently use Chasen to segment Japanese charac-
ter sequences into units that resemble words, we
make no use of any morphological information
as used for example in (Fuchigami et al, 2004).
Moreover, it should be noted that our approach
is at the same time very different to other purely
web-based approaches such as askMSR (Brill et
al., 2002) and Aranea (Lin et al, 2002). For exam-
ple, we use entire documents rather than the snip-
pets of text returned by web search engines; we do
not use structured document sources or databases
and we do not transform the query in any way ei-
ther by term re-ordering or by modifying the tense
of verbs. These basic principles apply to each of
our language-specific QA systems thus simplify-
ing and accelerating development.
The approach to QA that we adopt has previ-
ously been described in (Whittaker et al, 2005a;
Whittaker et al, 2005b; Whittaker et al, 2005c)
where the details of the mathematical model and
how it was trained for English and Japanese were
given. Our approach has also been successfully
evaluated in the text retrieval conference (TREC)
2005 QA track evaluation (Voorhees, 2003) where
our group placed eleventh out of thirty partici-
pants (Whittaker et al, 2005a). Although the
TREC QA task is substantially different to web-
based QA the TREC evaluation confirmed that our
approach works and also provides an objective as-
sessment of its quality. Similarly, for our Japanese
language system we have previously evaluated the
performance of our approach on the NTCIR-3
QAC-1 task (Whittaker et al, 2005c). Although
our Japanese experiments were applied retrospec-
tively, the results would have placed us in the mid-
range of participating systems in that year?s eval-
uation. In this paper we present additional ex-
periments on Chinese and Swedish and explain
how our statistical pattern classification approach
to QA was successfully applied to these two new
languages. Using our approach and given ap-
propriate training data it is found that a reason-
ably proficient developer can build a QA system
in a new language in around 10 hours. Evalua-
tion of the Chinese and Swedish systems is per-
formed using a translation of the first 200 fac-
toid questions from the TREC 2003 evaluation
which we have also made available online. We
compare these results both qualitatively and quan-
titatively against results obtained previously for
English and Japanese. The systems, built us-
ing this method, form the basis of our multi-
language web demo which is publicly available at
http://asked.jp.
An outline of the remainder of this paper is as
follows: we briefly describe our statistical pattern
classification approach to QA in Section 2 repeat-
ing the important elements of our approach as nec-
essary to understand the remainder of the paper.
In Section 3 we describe the basic building blocks
of our QA system and how they can typically be
trained. We also give a breakdown of the data
used to train each language specific QA system.
In Section 4 we present the results of experiments
on Chinese, Swedish, English and Japanese and in
Section 5 we compare and analyse these results.
We wrap up with a conclusion and further work in
Sections 6 and 7.
2 Statistical pattern classification
approach to QA
The answer to a question depends primarily on
the question itself but also on many other factors
such as the person asking the question, the loca-
tion of the person, what questions the person has
asked before, and so on. Although such factors
are clearly relevant in a real-world scenario they
are difficult to model and also to test in an off-
line mode, for example, in the context of the NT-
CIR and TREC evaluations. We therefore choose
to consider only the dependence of an answer  
on the question , where each is considered to
be a string of 
 
words     
 
     

 
and 

words    
 
     


, respectively. In particu-
lar, we hypothesize that the answer   depends on
two sets of features      and     
as follows:
	       	     (1)
where    

 
     



can be thought of as a
set of 

features describing the ?question-type?
part of  such as who, when, where, which, etc.
and    
 
     


is a set of 

features
comprising the ?information-bearing? part of 
i.e. what the question is actually about and what it
refers to. For example, in the questions, ?Where
was Tom Cruise married?? and ?When was
Tom Cruise married?? the information-bearing
component is identical in both cases whereas the
question-type component is different.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
46
Finding the best answer   involves a search
over all   for the one which maximizes the prob-
ability of the above model:

    
 
	     (2)
Using Bayes rule, making further conditional
independence assumptions and assuming uniform
prior probabilities, which therefore do not affect
the optimisation criterion, we obtain the final op-
timisation criterion (see (Whittaker et al, 2005a)
for more details):

 
	    
   
	


 	    
   


 (3)
The 	     model is essentially a language
model which models the probability of an answer
sequence   given a set of information-bearing fea-
tures  . It models the proximity of   to features
in  . We call this model the retrieval model and
examine it further in Section 2.1.
The 	     model matches an answer  
with features in the question-type set  . Roughly
speaking this model relates ways of asking a ques-
tion with classes of valid answers. For example, it
associates dates, or days of the week with when-
type questions. In general, there are many valid
and equiprobable   for a given  so this compo-
nent can only re-rank candidate answers retrieved
by the retrieval model. Consequently, we call it the
filter model and examine it further in Section 2.2.
2.1 Retrieval model
The retrieval model essentially models the prox-
imity of   to features in  . Since    

 
     

 
we are actually modelling the distri-
bution of multi-word sequences. This should be
borne in mind in the following discussion when-
ever   is used. As mentioned above, we currently
use a deterministic information-feature mapping
function     . This mapping only gener-
ates word -tuples (   	 
   ) from single
words in  that are not present in a stoplist of 50-
100 high-frequency words. For more details on
the exact form of the retrieval model please refer
to (Whittaker et al, 2005a).
2.2 Filter model
A set of 
 
 single-word features is extracted
based on frequency of occurrence in question data.
Some examples include: HOW, MANY, WHEN,
WHO, UNTIL etc. The question-type mapping
function   extracts -tuples (   	 
   ) of
question-type features from the question , such
as HOW MANY and UNTIL WHEN.
Modelling the complex relationship between 
and   directly is non-trivial. We therefore intro-
duce an intermediate variable representing classes
of example questions-and-answers (q-and-a) 

for
   	    

 drawn from the set 

, and to fa-
cilitate modelling we say that  is conditionally
independent of   given 

as follows:
	      




 
	   

  	 

   (4)
Given a set  of example q-and-a 

for
   	     where 

  

 

  


 
     




 

 
     


 

 we define a mapping
function     

by 

   . Each
class 

  


 
     





 

 
     


 

 is then
obtained by 

 




 



 


 



. In all the
experiments in this paper no clustering of the q-
and-a is actually performed so each q-and-a ex-
ample forms its own unique class i.e. each 

cor-
responds to a single 

and vice-versa.
Assuming conditional independence of the an-
swer words in class 

given   and making the
modelling assumption that the th answer word 

in the example class 

is dependent only on the
th answer word in   we obtain:
	      




 
	   

 

 


 
	 


 


 




 
	   



 


 

 



 
	 


 


	 


 


(5)
where 


is a concrete class in the set of 
 
 an-
swer classes 
 
, and assuming 

is conditionally
independent of 


given 

. The system using the
formulation of filter model given by Equation (5)
is referred to as model ONE. The model given by
Equation (4) is referred to as model TWO, how-
ever, we are only concerned with model ONE in
this paper.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
47
Answer typing, such as it exists in our model
ONE system, is performed by the filter model and
is effected by matching the input query against
our set of example questions 

. Each one of
these example questions 

has an answer asso-
ciated with it which in turn is expanded via the

 
classes into a set of possible answers for each
question. At each step this matching process is
entirely probabilistic as given by Equation (5). To
take a simple example, if we are presented with
the input query ?Who was the U.S. President who
first used Camp David?? the first step effectively
matches the words (and bigrams, trigrams) in that
query against the words (and bigrams, trigrams)
in our example questions with a probability of its
match assigned to all example questions. Suppose,
for example, that the top-scoring example question
in our set is ?Who was the U.S. President who re-
signed as a result of the Watergate affair?? since
the first six words in each question match each
other and will likely result in a high probability
being assigned. The corresponding answer to this
example question is ?Richard Nixon?. So the next
step expands ?Richard? using 
 
and results in
a high score being assigned to a class of words
which tend to share the feature of being male first
names, one of which is ?Franklin?. Expanding
the second word in the answer ?Nixon? using 
 
will possibly result in a high score being assigned
to a class of words that share the feature of being
the surnames of U.S. presidents, one of which is
?Roosevelt?. In this way, we end up with a high
score assigned by the filter model to ?Franklin
Roosevelt? but also to ?Abraham Lincoln?, ?Bill
Clinton?, etc. In combination with the retrieval
model, we hope that the documents obtained for
this query assign a higher retrieval model score
to ?Franklin Roosevelt? over the names of other
U.S. presidents and thus output it in first place
with the highest overall probability. While this ap-
proach works well for names, dates and short place
names it does fall down, for example, on names
of books, plays and films where there is typically
less of a clear correspondence between the words
in any given position of two answers. This situa-
tion could be avoided by using multi-word answer
strings and not making the position-dependence
modelling assumption that was made to arrive at
Equation (5) but this has its own drawbacks.
The above description of the operation of the
filter model highlights the need for homogeneous
classes of 
 
of sufficiently wide coverage. In the
next section we describe a way in which this can
be achieved in an efficient, data-driven manner for
essentially any language.
2.3 Obtaining 
 
As we saw in the previous section the 
 
are the
closest thing we have to named entities since they
define classes of words that share some similarity
with each other. However, in some sense they are
more flexible than named entities since any word
can actually belong to any class but with a certain
probability. In this way we don?t rule out with a
zero probability the possibility of a word belong-
ing to some class, just that a word is more likely
to belong to some classes than others. In addition,
the entities are not actually named i.e. we do not
impose our own label on the classes so we do not
explicitly have a class of first names, or a class of
days of the week. Although we hope that we will
end up with classes containing such similar words
we do not make it explicit and we do not label what
each class of words is supposed to represent.
In keeping with our data-driven philosophy
and related objective to make our approach as
language-independent as possible we use an ag-
glomerative clustering algorithm to derive classes
automatically from data. The set of potential an-
swer words 

 
that are clustered, should ideally
cover all possible words that might ever be an-
swers to questions. We therefore take the most fre-
quent 

 
 words from a language-specific cor-
pus  comprising   word tokens as our set of
potential answers. The ?seeds? for the clusters are
chosen to be the most frequent 
 
 words in  .
The algorithm then uses the co-occurrence proba-
bilities of words in  to group together words with
similar co-occurrence statistics. For each word 
in 

 
the co-occurrence probability 	 

  
is the probability of 

given  occurring  words
away. If  is positive, 

occurs after , and if
negative, before . We then construct a vector
of co-occurrences with maximum separation be-
tween words , as follows:
     
 
         

     
 
      
 

           
 
        

  
 
 
        

   (6)
Rather than storing 
 

 
elements we can com-
pute most terms efficiently and on-the-fly using
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
48
Language    

 
 

 
 

Chinese TREC Mandarin (Rogers, 2000) 68M 33k 7k 1000
Swedish PAROLE (University, 1997) 19M 367k 5k 1000
English AQUAINT (Voorhees, 2002) 300M 215k 290k 500
Japanese MAINICHI (Fukumoto et al, 2002) 150M 300k 270k 5000
Table 1: Description of each of the four monolingual QA systems.
the Katz back-off method (Katz, 1987) and ab-
solute discounting for estimating the probabilities
of unobserved events. To find the distance be-
tween two vectors, for efficiency, we use an 
 
distance metric: Proceedings of the ACL 2010 Conference Short Papers, pages 236?240,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimizing Question Answering Accuracy by Maximizing Log-Likelihood
Matthias H. Heie, Edward W. D. Whittaker and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552, Japan
{heie,edw,furui}@furui.cs.titech.ac.jp
Abstract
In this paper we demonstrate that there
is a strong correlation between the Ques-
tion Answering (QA) accuracy and the
log-likelihood of the answer typing com-
ponent of our statistical QA model. We
exploit this observation in a clustering al-
gorithm which optimizes QA accuracy by
maximizing the log-likelihood of a set of
question-and-answer pairs. Experimental
results show that we achieve better QA ac-
curacy using the resulting clusters than by
using manually derived clusters.
1 Introduction
Question Answering (QA) distinguishes itself
from other information retrieval tasks in that the
system tries to return accurate answers to queries
posed in natural language. Factoid QA limits it-
self to questions that can usually be answered with
a few words. Typically factoid QA systems em-
ploy some form of question type analysis, so that
a question such as What is the capital of Japan?
will be answered with a geographical term. While
many QA systems use hand-crafted rules for this
task, such an approach is time-consuming and
doesn?t generalize well to other languages. Ma-
chine learning methods have been proposed, such
as question classification using support vector ma-
chines (Zhang and Lee, 2003) and language mod-
eling (Merkel and Klakow, 2007). In these ap-
proaches, question categories are predefined and a
classifier is trained on manually labeled data. This
is an example of supervised learning. In this pa-
per we present an unsupervised method, where we
attempt to cluster question-and-answer (q-a) pairs
without any predefined question categories, hence
no manually class-labeled questions are used.
We use a statistical QA framework, described in
Section 2, where the system is trained with clusters
of q-a pairs. This framework was used in several
TREC evaluations where it placed in the top 10
of participating systems (Whittaker et al, 2006).
In Section 3 we show that answer accuracy is
strongly correlated with the log-likelihood of the
q-a pairs computed by this statistical model. In
Section 4 we propose an algorithm to cluster q-a
pairs by maximizing the log-likelihood of a dis-
joint set of q-a pairs. In Section 5 we evaluate the
QA accuracy by training the QA system with the
resulting clusters.
2 QA system
In our QA framework we choose to model only
the probability of an answer A given a question Q,
and assume that the answer A depends on two sets
of features: W = W (Q) and X = X(Q):
P (A|Q) = P (A|W,X), (1)
where W represents a set of |W | features describ-
ing the question-type part of Q such as who, when,
where, which, etc., and X is a set of features
which describes the ?information-bearing? part of
Q, i.e. what the question is actually about and
what it refers to. For example, in the questions
Where is Mount Fuji? and How high is Mount
Fuji?, the question type features W differ, while
the information-bearing features X are identical.
Finding the best answer A? involves a search over
all A for the one which maximizes the probability
of the above model, i.e.:
A? = arg max
A
P (A|W,X). (2)
Given the correct probability distribution, this
will give us the optimal answer in a maximum
likelihood sense. Using Bayes? rule, assuming
uniform P (A) and that W and X are indepen-
dent of each other given A, in addition to ignoring
P (W,X) since it is independent of A, enables us
to rewrite Eq. (2) as
236
A? = arg max
A
P (A | X)
? ?? ?
retrieval
model
? P (W | A)
? ?? ?
filter
model
. (3)
2.1 Retrieval Model
The retrieval model P (A|X) is essentially a lan-
guage model which models the probability of an
answer sequence A given a set of information-
bearing features X = {x1, . . . , x|X|}. This set
is constructed by extracting single-word features
from Q that are not present in a stop-list of high-
frequency words. The implementation of the re-
trieval model used for the experiments described
in this paper, models the proximity of A to fea-
tures in X . It is not examined further here;
see (Whittaker et al, 2005) for more details.
2.2 Filter Model
The question-type feature set W = {w1, . . . , w|W |}
is constructed by extracting n-tuples (n = 1, 2, . . .)
such as where, in what and when were from the
input question Q. We limit ourselves to extracting
single-word features. The 2522 most frequent
words in a collection of example questions are
considered in-vocabulary words; all other words
are out-of-vocabulary words, and substituted with
?UNK?.
Modeling the complex relationship between
W and A directly is non-trivial. We there-
fore introduce an intermediate variable CE =
{c1, . . . , c|CE |}, representing a set of classes of
example q-a pairs. In order to construct these
classes, given a set E = {t1, . . . , t|E|} of ex-
ample q-a pairs, we define a mapping function
f : E 7? CE which maps each example q-a pair tj
for j = 1 . . . |E| into a particular class f(tj) = ce.
Thus each class ce may be defined as the union of
all component q-a features from each tj satisfy-
ing f(tj) = ce. Hence each class ce constitutes a
cluster of q-a pairs. Finally, to facilitate modeling
we say that W is conditionally independent of A
given ce so that,
P (W | A) =
|CE |?
e=1
P (W | ceW ) ? P (ceA | A), (4)
where ceW and ceA refer to the subsets of question-
type features and example answers for the class ce,
respectively.
P (W | ceW ) is implemented as trigram langu-
age models with backoff smoothing using absolute
discounting (Huang et al, 2001).
Due to data sparsity, our set of example q-a
pairs cannot be expected to cover all the possi-
ble answers to questions that may ever be asked.
We therefore employ answer class modeling rather
than answer word modeling by expanding Eq. (4)
as follows:
P (W | A) =
|CE |?
e=1
P (W | ceW )?
|KA|?
a=1
P (ceA | ka)P (ka | A),
(5)
where ka is a concrete class in the set of |KA|
answer classes KA. These classes are generated
using the Kneser-Ney clustering algorithm, com-
monly used for generating class definitions for
class language models (Kneser and Ney, 1993).
In this paper we restrict ourselves to single-
word answers; see (Whittaker et al, 2005) for the
modeling of multi-word answers. We estimate
P (ceA | kA) as
P (ceA | kA) =
f(kA, ceA)
|CE |?
g=1
f(kA, c
g
A)
, (6)
where
f(kA, ceA) =
?
?i:i?ceA
?(i ? kA)
|ceA|
, (7)
and ?(?) is a discrete indicator function which
equals 1 if its argument evaluates true and 0 if
false.
P (ka | A) is estimated as
P (ka | A) =
1
?
?j:j?Ka
?(A ? j) . (8)
3 The Relationship between Mean
Reciprocal Rank and Log-Likelihood
We use Mean Reciprocal Rank (MRR) as our
metric when evaluating the QA accuracy on a set
of questions G = {g1...g|G|}:
MRR =
?|G|
i=1 1/Ri
|G| , (9)
237
 0.15
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
-1.18 -1.16 -1.14 -1.12
M
R
R
LL
? = 0.86
Figure 1: MRR vs. LL (average per q-a pair) for
100 random cluster configurations.
where Ri is the rank of the highest ranking correct
candidate answer for gi.
Given a set D = (d1...d|D|) of q-a pairs disjoint
from the q-a pairs in CE , we can, using Eq. (5),
calculate the log-likelihood as
LL =
|D|
?
d=1
logP (Wd|Ad)
=
|D|
?
d=1
log
|CE |?
e=1
P (Wd | ceW )?
|KA|?
a=1
P (ceA | ka)P (ka | Ad).
(10)
To examine the relationship between MRR and
LL, we randomly generate configurations CE ,
with a fixed cluster size of 4, and plot the result-
ing MRR and LL, computed on the same data set
D, as data points in a scatter plot, as seen in Fig-
ure 1. We find that LL and MRR are strongly
correlated, with a correlation coefficient ? = 0.86.
This observation indicates that we should be
able to improve the answer accuracy of the QA
system by optimizing the LL of the filter model
in isolation, similar to how, in automatic speech
recognition, the LL of the language model can
be optimized in isolation to improve the speech
recognition accuracy (Huang et al, 2001).
4 Clustering algorithm
Using the observation that LL is correlated with
MRR on the same data set, we expect that opti-
mizing LL on a development set (LLdev) will also
improve MRR on an evaluation set (MRReval).
Hence we propose the following greedy algorithm
to maximize LLdev:
init: c1 ? CE contains all training pairs |E|
while improvement > threshold do
best LLdev ? ??
for all j = 1...|E| do
original cluster = f(tj)
Take tj out of f(tj)
for e = ?1, 1...|CE |, |CE |+ 1 do
Put tj in ce
Calculate LLdev
if LLdev > best LLdev then
best LLdev ? LLdev
best cluster ? e
best pair ? j
end if
Take tj out of ce
end for
Put tj back in original cluster
end for
Take tbest pair out of f(tbest pair)
Put tbest pair into cbest cluster
end while
In this algorithm, c?1 indicates the set of train-
ing pairs outside the cluster configuration, thus ev-
ery training pair will not necessarily be included
in the final configuration. c|C|+1 refers to a new,
empty cluster, hence this algorithm automatically
finds the optimal number of clusters as well as the
optimal configuration of them.
5 Experiments
5.1 Experimental Setup
For our data sets, we restrict ourselves to questions
that start with who, when or where. Furthermore,
we only use q-a pairs which can be answered with
a single word. As training data we use questions
and answers from the Knowledge-Master collec-
tion1. Development/evaluation questions are the
questions from TREC QA evaluations from TREC
2002 to TREC 2006, the answers to which are to
be retrieved from the AQUAINT corpus. In total
we have 2016 q-a pairs for training and 568 ques-
tions for development/evaluation. We are able to
retrieve the correct answer for 317 of the devel-
opment/evaluation questions, thus the theoretical
upper bound for our experiments is an answer ac-
curacy of MRR = 0.558.
Accuracy is evaluated using 5-fold (rotating)
cross-validation, where in each fold the TREC
QA data is partitioned into a development set of
1http://www.greatauk.com/
238
Configuration LLeval MRReval #clusters
manual -1.18 0.262 3
all-in-one -1.32 0.183 1
one-in-each -0.87 0.263 2016
automatic -0.24 0.281 4
Table 1: LLeval (average per q-a pair) and
MRReval (over all held-out TREC years), and
number of clusters (median of the cross-evaluation
folds) for the various configurations.
4 years? data and an evaluation set of one year?s
data. For each TREC question the top 50 doc-
uments from the AQUAINT corpus are retrieved
using Lucene2. We use the QA system described
in Section 2 for QA evaluation. Our evaluation
metric is MRReval, and LLdev is our optimiza-
tion criterion, as motivated in Section 3.
Our baseline system uses manual clusters.
These clusters are obtained by putting all who q-a
pairs in one cluster, all when pairs in a second and
all where pairs in a third. We compare this baseline
with using clusters resulting from the algorithm
described in Section 4. We run this algorithm until
there are no further improvements in LLdev. Two
other cluster configurations are also investigated:
all q-a pairs in one cluster (all-in-one), and each q-
a pair in its own cluster (one-in-each). The all-in-
one configuration is equivalent to not using the fil-
ter model, i.e. answer candidates are ranked solely
by the retrieval model. The one-in-each configura-
tion was shown to perform well in the TREC 2006
QA evaluation (Whittaker et al, 2006), where it
ranked 9th among 27 participants on the factoid
QA task.
5.2 Results
In Table 1, we see that the manual clusters (base-
line) achieves an MRReval of 0.262, while the
clusters resulting from the clustering algorithm
give an MRReval of 0.281, which is a relative
improvement of 7%. This improvement is sta-
tistically significant at the 0.01 level using the
Wilcoxon signed-rank test. The one-in-each clus-
ter configuration achieves an MRReval of 0.263,
which is not a statistically significant improvement
over the baseline. The all-in-one cluster configura-
tion (i.e. no filter model) has the lowest accuracy,
with an MRReval of 0.183.
2http://lucene.apache.org/
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0  400  800  1200  1600  2000
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
LL M
R
R
# iterations
LLdevMRRdev
(a) Development set, 4 year?s TREC.
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0  400  800  1200  1600  2000
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
LL M
R
R
# iterations
LLevalMRReval
(b) Evaluation set, 1 year?s TREC.
Figure 2: MRR and LL (average per q-a pair)
vs. number of algorithm iterations for one cross-
validation fold.
6 Discussion
Manual inspection of the automatically derived
clusters showed that the algorithm had constructed
configurations where typically who, when and
where q-a pairs were put in separate clusters, as in
the manual configuration. However, in some cases
both who and where q-a pairs occurred in the same
cluster, so as to better answer questions like Who
won the World Cup?, where the answer could be a
country name.
As can be seen from Table 1, there are only 4
clusters in the automatic configuration, compared
to 2016 in the one-in-each configuration. Since
the computational complexity of the filter model
described in Section 2.2 is linear in the number of
clusters, a beneficial side effect of our clustering
procedure is a significant reduction in the compu-
tational requirement of the filter model.
In Figure 2 we plot LL and MRR for one of
the cross-validation folds over multiple iterations
(the while loop) of the clustering algorithm in Sec-
239
tion 4. It can clearly be seen that the optimization
of LLdev leads to improvement in MRReval, and
that LLeval is also well correlated with MRReval.
7 Conclusions and Future Work
In this paper we have shown that the log-likelihood
of our statistical model is strongly correlated with
answer accuracy. Using this information, we have
clustered training q-a pairs by maximizing log-
likelihood on a disjoint development set of q-a
pairs. The experiments show that with these clus-
ters we achieve better QA accuracy than using
manually clustered training q-a pairs.
In future work we will extend the types of ques-
tions that we consider, and also allow for multi-
word answers.
Acknowledgements
The authors wish to thank Dietrich Klakow for his
discussion at the concept stage of this work. The
anonymous reviewers are also thanked for their
constructive feedback.
References
[Huang et al2001] Xuedong Huang, Alex Acero and
Hsiao-Wuen Hon. 2001. Spoken Language Pro-
cessing. Prentice-Hall, Upper Saddle River, NJ,
USA.
[Kneser and Ney1993] Reinhard Kneser and Hermann
Ney. 1993. Improved Clustering Techniques for
Class-based Statistical Language Modelling. Pro-
ceedings of the European Conference on Speech
Communication and Technology (EUROSPEECH).
[Merkel and Klakow2007] Andreas Merkel and Diet-
rich Klakow. 2007. Language Model Based Query
Classification. Proceedings of the European Confer-
ence on Information Retrieval (ECIR).
[Whittaker et al2005] Edward Whittaker, Sadaoki Fu-
rui and Dietrich Klakow. 2005. A Statistical Clas-
sification Approach to Question Answering using
Web Data. Proceedings of the International Con-
ference on Cyberworlds.
[Whittaker et al2006] Edward Whittaker, Josef Novak,
Pierre Chatain and Sadaoki Furui. 2006. TREC
2006 Question Answering Experiments at Tokyo In-
stitute of Technology. Proceedings of The Fifteenth
Text REtrieval Conference (TREC).
[Zhang and Lee2003] Dell Zhang and Wee Sun Lee.
2003. Question Classification using Support Vec-
tor Machines. Proceedings of the Special Interest
Group on Information Retrieval (SIGIR).
240
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1210?1219,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Creating a manually error-tagged and shallow-parsed learner corpus
Ryo Nagata
Konan University
8-9-1 Okamoto,
Kobe 658-0072 Japan
rnagata @ konan-u.ac.jp.
Edward Whittaker Vera Sheinman
The Japan Institute for
Educational Measurement Inc.
3-2-4 Kita-Aoyama, Tokyo, 107-0061 Japan
 
whittaker,sheinman  @jiem.co.jp
Abstract
The availability of learner corpora, especially
those which have been manually error-tagged
or shallow-parsed, is still limited. This means
that researchers do not have a common devel-
opment and test set for natural language pro-
cessing of learner English such as for gram-
matical error detection. Given this back-
ground, we created a novel learner corpus
that was manually error-tagged and shallow-
parsed. This corpus is available for research
and educational purposes on the web. In
this paper, we describe it in detail together
with its data-collection method and annota-
tion schemes. Another contribution of this
paper is that we take the first step toward
evaluating the performance of existing POS-
tagging/chunking techniques on learner cor-
pora using the created corpus. These contribu-
tions will facilitate further research in related
areas such as grammatical error detection and
automated essay scoring.
1 Introduction
The availability of learner corpora is still somewhat
limited despite the obvious usefulness of such data
in conducting research on natural language process-
ing of learner English in recent years. In particular,
learner corpora tagged with grammatical errors are
rare because of the difficulties inherent in learner
corpus creation as will be described in Sect. 2. As
shown in Table 1, error-tagged learner corpora are
very few among existing learner corpora (see Lea-
cock et al (2010) for a more detailed discussion
of learner corpora). Even if data is error-tagged,
it is often not available to the public or its access
is severely restricted. For example, the Cambridge
Learner Corpus, which is one of the largest error-
tagged learner corpora, can only be used by authors
and writers working for Cambridge University Press
and by members of staff at Cambridge ESOL.
Error-tagged learner corpora are crucial for devel-
oping and evaluating error detection/correction al-
gorithms such as those described in (Rozovskaya
and Roth, 2010b; Chodorow and Leacock, 2000;
Chodorow et al, 2007; Felice and Pulman, 2008;
Han et al, 2004; Han et al, 2006; Izumi et al,
2003b; Lee and Seneff, 2008; Nagata et al, 2004;
Nagata et al, 2005; Nagata et al, 2006; Tetreault et
al., 2010b). This is one of the most active research
areas in natural language processing of learner En-
glish. Because of the restrictions on their availabil-
ity, researchers have used their own learner corpora
to develop and evaluate error detection/correction
methods, which are often not commonly available
to other researchers. This means that the detec-
tion/correction performance of each existing method
is not directly comparable as Rozovskaya and Roth
(2010a) and Tetreault et al (2010a) point out. In
other words, we are not sure which methods achieve
the best performance. Commonly available error-
tagged learner corpora are therefore essential to fur-
ther research in this area.
For similar reasons, to the best of our knowledge,
there exists no such learner corpus that is manually
shallow-parsed and which is also publicly available,
unlike, say, native-speaker corpora such as the Penn
Treebank. Such a comparison brings up another cru-
cial question: ?Do existing POS taggers and chun-
1210
Name Error-tagged Parsed Size (words) Availability
Cambridge Learner Corpus Yes No 30 million No
CLEC Corpus Yes No 1 million Partially
ETLC Corpus Partially No 2 million Not Known
HKUST Corpus Yes No 30 million No
ICLE Corpus (Granger et al, 2009) No No 3.7 million+ Yes
JEFLL Corpus (Tono, 2000) No No 1 million Partially
Longman Learners? Corpus No No 10 million Not Known
NICT JLE Corpus (Izumi et al, 2003a) Partially No 2 million Partially
Polish Learner English Corpus No No 0.5 million No
Janus Pannoius University Learner Corpus No No 0.4 million Not Known
In Availability, Yes denotes that the full texts of the corpus is available to the public. Partially denotes that it is acces-
sible through specially-made interfaces such as a concordancer. The information in this table may not be consistent
because many of the URLs of the corpora give only sparse information about them.
Table 1: Learner corpus list.
kers work on learner English as well as on edited text
such as newspaper articles?? Nobody really knows
the answer to the question. The only exception in the
literature is the work by Tetreault et al (2010b) who
evaluated parsing performance in relation to prepo-
sitions. Nevertheless, a great number of researchers
have used existing POS taggers and chunkers to ana-
lyze the writing of learners of English. For instance,
error detection methods normally use a POS tagger
and/or a chunker in the error detection process. It is
therefore possible that a major cause of false pos-
itives and negatives in error detection may be at-
tributed to errors in POS-tagging and chunking. In
corpus linguistics, researchers (Aarts and Granger,
1998; Granger, 1998; Tono, 2000) use such tools to
extract interesting patterns from learner corpora and
to reveal learners? tendencies. However, poor per-
formance of the tools may result in misleading con-
clusions.
Given this background, we describe in this paper
a manually error-tagged and shallow-parsed learner
corpus that we created. In Sect. 2, we discuss the
difficulties inherent in learner corpus creation. Con-
sidering the difficulties, in Sect. 3, we describe our
method for learner corpus creation, including its
data collection method and annotation schemes. In
Sect. 4, we describe our learner corpus in detail. The
learner corpus is called the Konan-JIEM learner cor-
pus (KJ corpus) and is freely available for research
and educational purposes on the web1. Another
contribution of this paper is that we take the first
step toward answering the question about the per-
formance of existing POS-tagging/chunking tech-
niques on learner data. We report and discuss the
results in Sect. 5.
2 Difficulties in Learner Corpus Creation
In addition to the common difficulties in creating
any corpus, learner corpus creation has its own dif-
ficulties. We classify them into the following four
categories of the difficulty in:
1. collecting texts written by learners;
2. transforming collected texts into a corpus;
3. copyright transfer; and
4. error and POS/parsing annotation.
The first difficulty concerns the problem in col-
lecting texts written by learners. As in the case
of other corpora, it is preferable that the size of a
learner corpus be as large as possible where the size
can be measured in several ways including the total
number of texts, words, sentences, writers, topics,
and texts per writer. However, it is much more diffi-
cult to create a large learner corpus than to create a
1http://www.gsk.or.jp/index_e.html
1211
large native-speaker corpus. In the case of native-
speaker corpora, published texts such as newspa-
per articles or novels can be used as a corpus. By
contrast, in the case of learner corpora, we must
find learners and then let them write since there
are no such published texts written by learners of
English (unless they are part of a learner corpus).
Here, it should be emphasized that learners often
do not spontaneously write but are typically obliged
to write, for example, in class, or during an exam.
Because of this, learners may soon become tired of
writing. This in itself can affect learner corpus cre-
ation much more than one would expect especially
when creating a longitudinal learner corpus. Thus, it
is crucial to keep learners motivated and focused on
the writing assignments.
The second difficulty arises when the collected
texts are transformed into a learner corpus. This
involves several time-consuming and troublesome
tasks. The texts must be archived in electronic
form, which requires typing every single collected
text since learners normally write on paper. Be-
sides, each text must be archived and maintained
with accompanying information such as who wrote
what text when and on what topic. Optionally, a
learner corpus could include other pieces of infor-
mation such as proficiency, first language, and age.
Once the texts have been electronically archived, it
is relatively easy to maintain and access them. How-
ever, this is not the case when the texts are first col-
lected. Thus, it is better to have an efficient method
for managing such information as well as the texts
themselves.
The third difficulty concerning copyright is a
daunting problem. The copyright for each text
must be transferred to the corpus creator so that the
learner corpus can be made available to the public.
Consider the case when a number of learners par-
ticipate in a learner corpus creation project and ev-
eryone has to sign a copyright transfer form. This is-
sue becomes even more complicated when the writer
does not actually have such a right to transfer copy-
right. For instance, under the Japanese law, those
younger than 20 years of age do not have the right;
instead their parents do. Thus, corpus creators have
to ask learners? parents to sign copyright transfer
forms. This is often the case since the writers in
learner corpus creation projects are normally junior
high school, high school, or college students.
The final difficulty is in error and POS/parsing
annotation. For error annotation, several annota-
tion schemes exist (for example, the NICT JLE
scheme (Izumi et al, 2005)). While designing an an-
notation scheme is one issue, annotating errors is yet
another. No matter how well an annotation scheme
is designed, there will always be exceptions. Every
time an exception appears, it becomes necessary to
revise the annotation scheme. Another issue we have
to remember is that there is a trade-off between the
granularity of an annotation scheme and the level of
the difficulty in error annotation. The more detailed
an annotation scheme is, the more information it can
contain and the more difficult identifying errors is,
and vice versa.
For POS/parsing annotation, there are also a num-
ber of annotation schemes including the Brown tag
set, the Claws tag set, and the Penn Treebank tag
set. However, none of them are designed to be used
for learner corpora. In other words, a variety of lin-
guistic phenomena occur in learner corpora which
the existing annotation schemes do not cover. For
instance, spelling errors often appear in texts writ-
ten by learners of English as in sard year, which
should be third year. Grammatical errors prevent us
applying existing annotation schemes, too. For in-
stance, there are at least three possibilities for POS-
tagging the word sing in the sentence everyone sing
together. using the Penn Treebank tag set: sing/VB,
sing/VBP, or sing/VBZ. The following example is
more complicated: I don?t success cooking. Nor-
mally, the word success is not used as a verb but
as a noun. The instance, however, appears in a po-
sition where a verb appears. As a result, there are
at least two possibilities for tagging: success/NN
and success/VB. Errors in mechanics are also prob-
lematic as in Tonight,we and beautifulhouse (miss-
ing spaces)2. One solution is to split them to obtain
the correct strings and then tag them with a normal
scheme. However, this would remove the informa-
tion that spaces were originally missing which we
want to preserve. To handle these and other phe-
nomena which are peculiar to learner corpora, we
need to develop a novel annotation scheme.
2Note that the KJ corpus consists of typed essays.
1212
3 Method
3.1 How to Collect and Maintain Texts Written
by Learners
Our text-collection method is based on writing exer-
cises. In the writing exercises, learners write essays
on a blog system. This very simple idea of using a
blog system naturally solves the problem of archiv-
ing texts in electronic form. In addition, the use of a
blog system enables us to easily register and main-
tain accompanying information including who (user
ID) writes when (uploaded time) and on what topic
(title of blog item). Besides, once registered in the
user profile, the optional pieces of information such
as proficiency, first language, and age are also easy
to maintain and access.
To design the writing exercises, we consulted
with several teachers of English and conducted pre-
experiments. Ten learners participated in the pre-
experiments and were assigned five essay topics on
average. Based on the experimental results, we
designed the procedure of the writing exercise as
shown in Table 2. In the first step, learners are as-
signed an essay topic. In the second step, they are
given time to prepare during which they think about
what to write on the given topic before they start
writing. We found that this enables the students to
write more. In the third step, they actually write an
essay on the blog system. After they have finished
writing, they submit their essay to the blog system
to be registered.
The following steps were considered optional. We
implemented an article error detection method (Na-
gata et al, 2006) in the blog system as a trial at-
tempt to keep the learners motivated since learners
are likely to become tired of doing the same exercise
repeatedly. To reduce this, the blog system high-
lights where article errors exist after the essay has
been submitted. The hope is that this might prompt
the learners to write more accurately and to continue
the exercises. In the pre-experiments, the detection
did indeed seem to interest the learners and to pro-
vide them with additional motivation. Considering
these results, we decided to include the fourth and
fifth steps in the writing exercises when we created
our learner corpus. At the same time, we should of
course be aware that the use of error detection affects
learners? writing. For example, it may change the
Step Min.
1. Learner is assigned an essay topic ?
2. Learner prepares for writing 5
3. Learner writes an essay 35
4. System detects errors in the essay 5
5. Learner rewrites the essay 15
Table 2: Procedure of writing exercise.
distribution of errors. Nagata and Nakatani (2010)
reported the effects in detail.
To solve the problem of copyright transfer, we
took legal professional advice but were informed
that, in Japan at least, the only way to be sure is
to have a copyright transfer form signed every time.
We considered having it signed on the blog system,
but it soon turned out that this did not work since
participating learners may still be too young to have
the legal right to sign the transfer. It is left for our
long-term future work to devise a better solution to
this legal issue.
3.2 Annotation Scheme
This subsection describes the error and
POS/chunking annotation schemes. Note that
errors and POS/chunking are annotated separately,
meaning that there are two files for any given text.
Due to space restrictions we limit ourselves to only
summarizing our annotation schemes in this section.
The full descriptions are available together with the
annotated corpus on the web.
3.2.1 Error Annotation
We based our error annotation scheme on that used
in the NICT JLE corpus (Izumi et al, 2003a), whose
detailed description is readily available, for exam-
ple, in Izumi et al (2005). In that annotation
scheme and accordingly in ours, errors are tagged
using an XML syntax; an error is annotated by tag-
ging a word or phrase that contains it. For in-
stance, a tense error is annotated as follows: I  v tns
crr=?made?  make  /v tns  pies last year.
where v tns denotes a tense error in a verb. It
should be emphasized that the error tags contain the
information on correction together with error anno-
tation. For instance, crr=?made? in the above ex-
ample denotes the correct form of the verb is made.
For missing word errors, error tags are placed where
1213
a word or phrase is missing (e.g., My friends live
 prp crr=?in?  /prp  these places.).
As a pilot study, we applied the NICT JLE annota-
tion scheme to a learner corpus to reveal what mod-
ifications we needed to make. The learner corpus
consisted of 455 essays (39,716 words) written by
junior high and high school students3. The follow-
ing describes the major modifications deemed nec-
essary as a result of the pilot study.
The biggest difference between the NICT JLE
corpus and our targeted corpus is that the former is
spoken data and the latter is written data. This differ-
ence inevitably requires several modifications to the
annotation scheme. In speech data, there are no er-
rors in spelling and mechanics such as punctuation
and capitalization. However, since such errors are
not usually regarded as grammatical errors, we de-
cided simply not to annotate them in our annotation
schemes.
Another major difference is fragment errors.
Fragments that do not form a complete sentence of-
ten appear in the writing of learners (e.g., I have
many books. Because I like reading.). In written
language, fragments can be regarded as a grammat-
ical error. To annotate fragment errors, we added a
new tag  f  (e.g., I have many books.  f  Because
I like reading.  /f  ).
As discussed in Sect. 2, there is a trade-off be-
tween the granularity of an annotation scheme and
the level of the difficulty in annotating errors. In our
annotation scheme, we narrowed down the number
of tags to 22 from 46 in the original NICT JLE tag
set to facilitate the annotation; the 22 tags are shown
in Appendix A. The removed tags are merged into
the tag for other. For instance, there are only three
tags for errors in nouns (number, lexis, and other) in
our tag set whereas there are six in the NICT JLE
corpus (inflection, number, case, countability, com-
plement, and lexis); the other tag (  n o  ) covers
the four removed tags.
3.2.2 POS/Chunking Annotation
We selected the Penn Treebank tag set, which is
one of the most widely used tag sets, for our
3The learner corpus had been created before this reported
work started. Learners wrote their essays on paper. Unfortu-
nately, this learner corpus cannot be made available to the pub-
lic since the copyrights were not transferred to us.
POS/chunking annotation scheme. Similar to the er-
ror annotation scheme, we conducted a pilot study
to determine what modifications we needed to make
to the Penn Treebank scheme. In the pilot study, we
used the same learner corpus as in the pilot study for
the error annotation scheme.
As a result of the pilot study, we found that the
Penn Treebank tag set sufficed in most cases except
for errors which learners made. Considering this, we
determined a basic rule as follows: ?Use the Penn
Treebank tag set and preserve the original texts as
much as possible.? To handle such errors, we made
several modifications and added two new POS tags
(CE and UK) and another two for chunking (XP and
PH), which are described below.
A major modification concerns errors in mechan-
ics such as Tonight,we and beautifulhouse as already
explained in Sect. 2. We use the symbol ?-? to an-
notate such cases. For instance, the above two ex-
amples are annotated as follows: Tonight,we/NN-
,-PRP and beautifulhouse/JJ-NN. Note that each
POS tag is hyphenated. It can also be used
for annotating chunks in the same manner. For
instance, Tonight,we is annotated as [NP-PH-NP
Tonight,we/NN-,-PRP ]. Here, the tag PH stands for
 chunk label and denotes tokens which are not
normally chunked (cf., [NP Tonight/NN ] ,/, [NP
we/PRP ]).
Another major modification was required to han-
dle grammatical errors. Essentially, POS/chunking
tags are assigned according to the surface informa-
tion of the word in question regardless of the ex-
istence of any errors. For example, There is ap-
ples. is annotated as [NP There/EX ] [VP is/VBZ
] [NP apples/NNS ] ./. Additionally, we define the
CE4 tag to annotate errors in which learners use a
word with a POS which is not allowed such as in I
don?t success cooking. The CE tag encodes a POS
which is obtained from the surface information to-
gether with the POS which would have been as-
signed to the word if it were not for the error. For
instance, the above example is tagged as I don?t
success/CE:NN:VB cooking. In this format, the sec-
ond and third POSs are separated by ?:? which de-
notes the POS which is obtained from the surface
information and the POS which would be assigned
4CE stands for cognitive error.
1214
to the word without an error. The user can select
either POS depending on his or her purposes. Note
that the CE tag is compatible with the basic anno-
tation scheme because we can retrieve the basic an-
notation by extracting only the second element (i.e.,
success/NN). If the tag is unknown because of gram-
matical errors or other phenomena, UK and XP5 are
used for POS and chunking, respectively.
For spelling errors, the corresponding POS and
chunking tag are assigned to mistakenly spelled
words if the correct forms can be guessed (e.g., [NP
sird/JJ year/NN ]); otherwise UK and XP are used.
4 The Corpus
We carried out a learner corpus creation project us-
ing the described method. Twenty six Japanese col-
lege students participated in the project. At the be-
ginning, we had the students or their parents sign
a conventional paper-based copyright transfer form.
After that, they did the writing exercise described in
Sect. 3 once or twice a week over three months. Dur-
ing that time, they were assigned ten topics, which
were determined based on a writing textbook (Ok-
ihara, 1985). As described in Sect. 3, they used a
blog system to write, submit, and rewrite their es-
says. Through out the exercises, they did not have
access to the others? essays and their own previous
essays.
As a result, 233 essays were collected; Table 3
shows the statistics on the collected essays. It turned
out that the learners had no difficulties in using the
blog system and seemed to focus on writing. Out of
the 26 participants, 22 completed the 10 assignments
while one student quit before the exercises started.
We annotated the grammatical errors of all 233
essays. Two persons were involved in the annota-
tion. After the annotation, another person checked
the annotation results; differences in error annota-
Number of essays 233
Number of writers 25
Number of sentences 3,199
Number of words 25,537
Table 3: Statistics on the learner corpus.
5UK and XP stand for unknown and X phrase, respectively.
tion were resolved by consulting the first two. The
error annotation scheme was found to work well on
them. The error-annotated essays can be used for
evaluating error detection/correction methods.
For POS/chunking annotation, we chose 170 es-
says out of 233. We annotated them using our
POS/chunking scheme; hereafter, the 170 essays
will be referred to as the shallow-parsed corpus.
5 Using the Corpus and Discussion
5.1 POS Tagging
The 170 essays in the shallow-parsed corpus was
used for evaluating existing POS-tagging techniques
on texts written by learners. It consisted of 2,411
sentences and 22,452 tokens.
HMM-based and CRF-based POS taggers were
tested on the shallow-parsed corpus. The former was
implemented using tri-grams by the author. It was
trained on a corpus consisting of English learning
materials (213,017 tokens). The latter was CRFTag-
ger6, which was trained on the WSJ corpus. Both
use the Penn Treebank POS tag set.
The performance was evaluated using accuracy
defined by
number of tokens correctly POS-tagged
number of tokens  (1)
If the number of tokens in a sentence was differ-
ent in the human annotation and the system out-
put, the sentence was excluded from the calcula-
tion. This discrepancy sometimes occurred because
the tokenization of the system sometimes differed
from that of the human annotators. As a result, 19
and 126 sentences (215 and 1,352 tokens) were ex-
cluded from the evaluation in the HMM-based and
CRF-based POS taggers, respectively.
Table 4 shows the results. The second column
corresponds to accuracies on a native-speaker cor-
pus (sect. 00 of the WSJ corpus). The third column
corresponds to accuracies on the learner corpus.
As shown in Table 4, the CRF-based POS tagger
suffers a decrease in accuracy as expected. Interest-
ingly, the HMM-based POS tagger performed bet-
ter on the learner corpus. This is perhaps because it
6?CRFTagger: CRF English POS Tagger,? Xuan-Hieu Phan,
http://crftagger.sourceforge.net/, 2006.
1215
was trained on a corpus consisting of English learn-
ing materials whose distribution of vocabulary was
expected to be relatively similar to that of the learner
corpus. By contrast, it did not perform well on the
native-speaker corpus because the size of the train-
ing corpus was relatively small and the distribution
of vocabulary was not similar, and thus unknown
words often appeared. This implies that selecting
appropriate texts as a training corpus may improve
the performance.
Table 5 shows the top five POSs mistakenly
tagged as other POSs. An obvious cause of mis-
takes in both taggers is that they inevitably make
errors in the POSs that are not defined in the Penn
Treebank tag set, that is, UK and CE. A closer
look at the tagging results revealed that phenom-
ena which were common to the writing of learners
were major causes of other mistakes. Errors in cap-
italization partly explain why the taggers made so
many mistakes in NN (singular nouns). They often
identified erroneously capitalized common nouns
as proper nouns as in This Summer/NNP Vaca-
tion/NNP. Spelling errors affected the taggers in the
same way. Grammatical errors also caused confu-
sion between POSs. For instance, omission of a cer-
tain word often caused confusion between a verb and
an adjective as in I frightened/VBD. which should
be I (was) frightened/JJ. Another interesting case
is expressions that learners overuse (e.g., and/CC
so/RB on/RB and so/JJ so/JJ). Such phrases are not
erroneous but are relatively infrequent in native-
speaker corpora. Therefore, the taggers tended to
identify their POSs according to the surface infor-
mation on the tokens themselves when such phrases
appeared in the learner corpus (e.g., and/CC so/RB
on/IN and so/RB so/RB). We should be aware that
tokenization is also problematic although failures in
tokenization were excluded from the accuracies.
The influence of the decrease in accuracy on other
NLP tasks is expected to be task and/or method de-
pendent. Methods that directly use or handle se-
Method Native Corpus Learner Corpus
CRF 0.970 0.932
HMM 0.887 0.926
Table 4: POS-tagging accuracy.
HMM CRF
POS Freq. POS Freq.
NN 259 NN 215
VBP 247 RB 166
RB 163 CE 144
CE 150 JJ 140
JJ 108 FW 86
Table 5: Top five POSs mistakenly tagged.
quences of POSs are likely to suffer from it. An
example is the error detection method (Chodorow
and Leacock, 2000), which identifies unnatural se-
quences of POSs as grammatical errors in the writ-
ing of learners. As just discussed above, existing
techniques often fail in sequences of POSs that have
a grammatical error. For instance, an existing POS
tagger likely tags the sentence I frightened. as I/PRP
frightened/VBD ./. as we have just seen, and in turn
the error detection method cannot identify it as an
error because the sequence PRP VBD is not unnatu-
ral; it would correctly detect it if the sentence were
correctly tagged as I/PRP frightened/JJ ./. For the
same reason, the decrease in accuracy may affect the
methods (Aarts and Granger, 1998; Granger, 1998;
Tono, 2000) for extracting interesting sequences of
POSs from learner corpora; for example, BOS7 PRP
JJ is an interesting sequence but is never extracted
unless the phrase is correctly POS-tagged. It re-
quires further investigation to reveal how much im-
pact the decrease has on these methods. By contrast,
error detection/correction methods based on the bag-
of-word features (or feature vectors) are expected to
suffer less from it since mistakenly POS-tagged to-
kens are only one of the features. At the same time,
we should notice that if the target errors are in the
tokens that are mistakenly POS-tagged, the detec-
tion will likely fail (e.g., verbs should be correctly
identified in tense error detection).
In addition to the above evaluation, we at-
tempted to improve the POS taggers using the
transformation-based POS-tagging technique (Brill,
1994). In the technique, transformation rules are
obtained by comparing the output of a POS tagger
and the human annotation so that the differences be-
tween the two are reduced. We used the shallow-
7BOS denotes a beginning of a sentence.
1216
Method Original Improved
CRF 0.932 0.934
HMM 0.926 0.933
Table 6: Improvement obtained by transformation.
parsed corpus as a test corpus and the other man-
ually POS-tagged corpus created in the pilot study
described in Subsect. 3.2.1 as a training corpus. We
used POS-based and word-based transformations as
Brill (1994) described.
Table 6 shows the improvements together with the
original accuracies. Table 6 reveals that even the
simple application of Brill?s technique achieves a
slight improvement in both taggers. Designing the
templates of the transformation for learner corpora
may achieve further improvement.
5.2 Head Noun Identification
In the evaluation of chunking, we focus on head
noun identification. Head noun identification often
plays an important role in error detection/correction.
For example, it is crucial to identify head nouns to
detect errors in article and number.
We again used the shallow-parsed corpus as a test
corpus. The essays contained 3,589 head nouns.
We implemented an HMM-based chunker using 5-
grams whose input is a sequence of POSs, which
was obtained by the HMM-based POS tagger de-
scribed in the previous subsection. The chunker was
trained on the same corpus as the HMM-based POS
tagger. The performance was evaluated by recall and
precision defined by
number of head nouns correctly identified
number of head nouns (2)
and
number of head nouns correctly identified
number of tokens identified as head noun  (3)
respectively.
Table 7 shows the results. To our surprise, the
chunker performed better than we had expected. A
possible reason for this is that sentences written by
learners of English tend to be shorter and simpler in
terms of their structure.
The results in Table 7 also enable us to quanti-
tatively estimate expected improvement in error de-
tection/correction which is achieved by improving
chunking. To see this, let us define the following
symbols:  : Recall of head noun identification, 	 :
recall of error detection without chunking error, 
	
recall of error detection with chunking error. 	 and

	 are interpreted as the true recall of error detection
and its observed value when chunking error exists,
respectively. Here, note that 
	 can be expressed
as 
		 . For instance, according to Han et al
(2006), their method achieves a recall of 0.40 (i.e.,

	

), and thus 	

assuming that chunk-
ing errors exist and recall of head noun identification
is 

 just as in this evaluation. Improving  to

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1137?1147,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reconstructing an Indo-European Family Tree
from Non-native English texts
Ryo Nagata1,2 Edward Whittaker3
1Konan University / Kobe, Japan
2LIMSI-CNRS / Orsay, France
3Inferret Limited / Northampton, England
nagata-acl@hyogo-u.ac.jp, ed@inferret.co.uk
Abstract
Mother tongue interference is the phe-
nomenon where linguistic systems of a
mother tongue are transferred to another
language. Although there has been plenty
of work on mother tongue interference,
very little is known about how strongly
it is transferred to another language and
about what relation there is across mother
tongues. To address these questions,
this paper explores and visualizes mother
tongue interference preserved in English
texts written by Indo-European language
speakers. This paper further explores lin-
guistic features that explain why certain
relations are preserved in English writing,
and which contribute to related tasks such
as native language identification.
1 Introduction
Transfer of linguistic systems of a mother tongue
to another language, namely mother tongue inter-
ference, is often observable in the writing of non-
native speakers. The reader may be able to deter-
mine the mother tongue of the writer of the fol-
lowing sentence from the underlined article error:
The alien wouldn?t use my spaceship but
the hers.
The answer would probably be French or Span-
ish; the definite article is allowed to modify pos-
sessive pronouns in these languages, and the us-
age is sometimes negatively transferred to English
writing. Researchers such as Swan and Smith
(2001), Aarts and Granger (1998), Davidsen-
Nielsen and Harder (2001), and Altenberg and
Tapper (1998) work on mother tongue interfer-
ence to reveal overused/underused words, part of
speech (POS), or grammatical items.
In contrast, very little is known about how
strongly mother tongue interference is transferred
to another language and about what relation there
is across mother tongues. At one extreme, one
could argue that it is so strongly transferred to
texts in another language that the linguistic rela-
tions between mother tongues are perfectly pre-
served in the texts. At the other extreme, one
can counter it, arguing that other features such as
non-nativeness are more influential than mother
tongue interference. One possible reason for this
is that a large part of the distinctive language sys-
tems of a mother tongue may be eliminated when
transferred to another language from a speaker?s
mother tongue. For example, Slavic languages
have a rich inflectional case system (e.g., Czech
has seven inflectional cases) whereas French does
not. However, the difference in the richness cannot
be transferred into English because English has al-
most no inflectional case system. Thus, one can-
not determine the mother tongue of a given non-
native text from the inflectional case. A similar
argument can be made about some parts of gen-
der, tense, and aspect systems. Besides, Wong and
Dras (2009) show that there are no significant dif-
ferences, between mother tongues, in the misuse
of certain syntactic features such as subject-verb
agreement that have different tendencies depend-
ing on their mother tongues. Considering these,
one could not be so sure which argument is cor-
rect. In any case, to the best of our knowledge, no
one has yet answered this question.
In view of this background, we take the first step
in addressing this question. We hypothesize that:
Hypothesis: Mother tongue interference is so
strong that the relations in a language fam-
ily are preserved in texts written in another
language.
In other words, mother tongue interference is so
strong that one can reconstruct a language fam-
1137
ily tree from non-native texts. One of the major
contributions of this work is to reveal and visual-
ize a language family tree preserved in non-native
texts, by examining the hypothesis. This becomes
important in native language identification1 which
is useful for improving grammatical error correc-
tion systems (Chodorow et al, 2010) or for pro-
viding more targeted feedback to language learn-
ers. As we will see in Sect. 6, this paper reveals
several crucial findings that contribute to improv-
ing native language identification. In addition, this
paper shows that the findings could contribute to
reconstruction of language family trees (Enright
and Kondrak, 2011; Gray and Atkinson, 2003;
Barbanc?on et al, 2007; Batagelj et al, 1992;
Nakhleh et al, 2005), which is one of the central
tasks in historical linguistics.
The rest of this paper is structured as follows.
Sect. 2 introduces the basic approach of this work.
Sect. 3 discusses the methods in detail. Sect. 4 de-
scribes experiments conducted to investigate the
hypothesis. Sect. 5 discusses the experimental re-
sults. Sect. 6 discusses implications for work in
related domains.
2 Approach
To examine the hypothesis, we reconstruct a
language family tree from English texts writ-
ten by non-native speakers of English whose
mother tongue is one of the Indo-European lan-
guages (Beekes, 2011; Ramat and Ramat, 2006).
If the reconstructed tree is sufficiently similar to
the original Indo-European family tree, it will sup-
port the hypothesis. If not, it suggests that some
features other than mother tongue interference are
more influential.
The approach we use for reconstructing a lan-
guage family tree is to apply agglomerative hi-
erarchical clustering (Han and Kamber, 2006) to
English texts written by non-native speakers. Re-
searchers have already performed related work
on reconstructing language family trees. For in-
stance, Kroeber and Chrie?tien (1937) and Ellega?rd
(1959) proposed statistical methods for measuring
the similarity metric between languages. More re-
cently, Batagelj et al (1992) and Kita (1999) pro-
posed methods for reconstructing language fam-
ily trees using clustering. Among them, the
1Recently, native language identification has drawn the at-
tention of NLP researchers. For instance, a shared task on
native language identification took place at an NAACL-HLT
2013 workshop.
most related method is that of Kita (1999). In
his method, a variety of languages are modeled
by their spelling systems (i.e., character-based
n-gram language models). Then, agglomera-
tive hierarchical clustering is applied to the lan-
guage models to reconstruct a language family
tree. The similarity used for clustering is based on
a divergence-like distance between two language
models that was originally proposed by Juang and
Rabiner (1985). This method is purely data-driven
and does not require human expert knowledge for
the selection of linguistic features.
Our work closely follows Kita?s work. How-
ever, it should be emphasized that there is a signif-
icant difference between the two. Kita?s work (and
other previous work) targets clustering of a variety
of languages whereas our work tries to reconstruct
a language family tree preserved in non-native En-
glish. This significant difference prevents us from
directly applying techniques in the literature to our
task. For instance, Batagelj et al (1992) use basic
vocabularies such as belly in English and ventre in
French to measure similarity between languages.
Obviously, this does not work on our task; belly is
belly in English writing whoever writes it. Kita?s
method is also likely not to work well because all
texts in our task share the same spelling system
(i.e., English spelling). Although spelling is some-
times influenced by mother tongues, it involves a
lot more including overuse, underuse, and misuse
of lexical, grammatical, and syntactic systems.
To solve the problem, this work adopts a word-
based language model in the expectation that word
sequences reflect mother tongue interference. At
the same time, its simple application would cause
a serious side effect. It would reflect the topics
of given texts rather than mother tongue interfer-
ence. Unfortunately, there exists no such English
corpus that covers a variety of language speakers
with uniform topics; moreover the availability of
non-native corpora is still somewhat limited. This
also means that available non-native corpora may
be too small to train reliable word-based language
models. The next section describes two methods
(language model-based and vector-based), which
address these problems.
3 Methods
3.1 Language Model-based Method
To begin with, let us define the following symbols
used in the methods. Let Di be a set of English
1138
texts where i denotes a mother tongue i. Similarly,
let Mi be a language model trained using Di.
To solve the problems pointed out in Sect. 2, we
use an n-gram language model based on a mixture
of word and POS tokens instead of a simple word-
based language model. In this language model,
content words in n-grams are replaced with their
corresponding POS tags. This greatly decreases
the influence of the topics of texts, as desired. It
also decreases the number of parameters in the
language model.
To build the language model, the following
three preprocessing steps are applied to Di. First,
texts in Di are split into sentences. Second, each
sentence is tokenized, POS-tagged, and mapped
entirely to lowercase. For instance, the first ex-
ample sentence in Sect. 1 would give:
the/DT alien/NN would/MD not/RB
use/VB my/PRP$ spaceship/NN but/CC
the/DT hers/PRP ./.
Finally, words are replaced with their correspond-
ing POS tags; for the following words, word to-
kens are used as their corresponding POS tags:
coordinating conjunctions, determiners, preposi-
tions, modals, predeterminers, possessives, pro-
nouns, question adverbs. Also, proper nouns are
treated as common nouns. At this point, the spe-
cial POS tags BOS and EOS are added at the begin-
ning and end of each sentence, respectively. For
instance, the above example would result in the
following word/POS sequence:
BOS the NN would RB VB my NN but
the hers . EOS
Note that the content of the original sentence is far
from clear while reflecting mother tongue interfer-
ence, especially in the hers.
Now, the language model Mi can be built from
Di. We set n = 3 (i.e., trigram language model)
following Kita?s work and use Kneser-Ney (KN)
smoothing (Kneser and Ney, 1995) to estimate its
conditional probabilities.
With Mi and Di, we can naturally apply Kita?s
method to our task. The clustering algorithm used
is agglomerative hierarchical clustering with the
average linkage method. The distance2 between
two language models is measured as follows. The
2It is not a distance in a mathematical sense. However,
we will use the term distance following the convention in the
literature.
probability that Mi generates Di is calculated by
Pr(Di|Mi). Note that
Pr(Di|Mi) ?
Pr(w1,i) Pr(w2,i|w1,i)
?
|Di|?
t=3
Pr(wt,i|wt?2,i, wt?1,i) (1)
where wt,i and |Di| denote the tth token in Di and
the number of tokens in Di, respectively, since we
use the trigram language model. Then, the dis-
tance from Mi to Mj is defined by
d(Mi ? Mj) =
1
|Dj |
log Pr(Dj |Mj)Pr(Dj |Mi)
. (2)
In other words, the distance is determined based
on the ratio of the probabilities that each lan-
guage model generates the language data. Because
d(Mi ? Mj) and d(Mj ? Mi) are not symmet-
rical, we define the distance between Mi and Mj
to be their average:
d(Mi,Mj)=
d(Mi ? Mj)+d(Mj ? Mi)
2 . (3)
Equation (3) is used to calculate the distance be-
tween two language models for clustering.
To sum up, the procedure of the language fam-
ily tree construction method is as follows: (i) Pre-
process each Di; (ii) Build Mi from Di; (iii) Cal-
culate the distances between the language models;
(iv) Cluster the language data using the distances;
(v) Output the result as a language family tree.
3.2 Vector-based Method
We also examine a vector-based method for lan-
guage family tree reconstruction. As we will see
in Sect. 5, this method allows us to interpret clus-
tering results more easily than with the language
model-based method while both result in similar
language family trees.
In this method, Di is modeled by a vector. The
vector is constructed based on the relative frequen-
cies of trigrams. As a consequence, the distance
is naturally defined by the Euclidean distance be-
tween two vectors. The clustering procedure is the
same as for the language model-based method ex-
cept that Mi is vector-based and that the distance
metric is Euclidean.
1139
4 Experiments
We selected the ICLE corpus v.2 (Granger et al,
2009) as the target language data. It consists of
English essays written by a wide variety of non-
native speakers of English. Among them, the 11
shown in Table 1 are of Indo-European languages.
Accordingly, we selected the subcorpora of the 11
languages in the experiments. Before the exper-
iments, we preprocessed the corpus data to con-
trol the experimental conditions. Because some of
the writers had more than one native language, we
excluded essays that did not meet the following
three conditions: (i) the writer has only one na-
tive language; (ii) the writer has only one language
at home; (iii) the two languages in (i) and (ii) are
the same as the native language of the subcorpus
to which the essay belongs3. After the selection,
markup tags such as essay IDs were removed from
the corpus data. Also, the symbols ? and ? were
unified into ?4. For reference, we also used na-
tive English (British and American university stu-
dents? essays in the LOCNESS corpus5) and two
sets of Japanese English (ICLE and the NICE cor-
pus (Sugiura et al, 2007)). Table 1 shows the
statistics on the corpus data.
Performance of POS tagging is an important
factor in our methods because they are based on
word/POS sequences. Existing POS taggers might
not perform well on non-native English texts be-
cause they are normally developed to analyze na-
tive English texts. Considering this, we tested
CRFTagger6 on non-native English texts contain-
ing various grammatical errors before the exper-
iments (Nagata et al, 2011). It turned out that
CRFTagger achieved an accuracy of 0.932 (com-
pared to 0.970 on native texts). Although it did not
perform as well as on native texts, it still achieved
a fair accuracy. Accordingly, we decided to use it
in our experiments.
Then, we generated cluster trees from the cor-
pus data using the methods described in Sect. 3.
3For example, because of (iii), essays written by native
speakers of Swedish in the Finnish subcorpus were excluded
from the experiments. This is because they were collected in
Finland and might be influenced by Finnish.
4The symbol ? is sometimes used for ? (e.g., I?m).
5The LOCNESS corpus is a corpus of native En-
glish essays made up of British pupils? essays, British
university students? essays, and American university
students? essays: https://www.uclouvain.be/
en-cecl-locness.html
6Xuan-Hieu Phan, ?CRFTagger: CRF English POS
Tagger,? http://crftagger.sourceforge.net/,
2006.
Native language # of essays # of tokens
Bulgarian 294 219,551
Czech 220 205,264
Dutch 244 240,861
French 273 202,439
German 395 236,841
Italian 346 219,581
Norwegian 290 218,056
Polish 354 251,074
Russian 255 236,748
Spanish 237 211,343
Swedish 301 268,361
English 298 294,357
Japanese1 (ICLE) 171 224,534
Japanese2 (NICE) 340 130,156
Total 4,018 3,159,166
Table 1: Statistics on target corpora.
We used the Kyoto Language Modeling toolkit7
to build language models from the corpus data.
We removed n-grams that appeared less than five
times8 in each subcorpus in the language mod-
els. Similarly, we implemented the vector-based
method with trigrams using the same frequency
cutoff (but without smoothing).
Fig. 1 shows the experimental results. The
tree at the top is the Indo-European family tree
drawn based on the figure shown in Crystal
(1997). It shows that the 11 languages are divided
into three groups: Italic, Germanic, and Slavic
branches. The second and third trees are the clus-
ter trees generated by the language model-based
and vector-based methods, respectively. The num-
ber at each branching node denotes in which step
the two clusters were merged.
The experimental results strongly support the
hypothesis we made in Sect. 1. Fig. 1 reveals
that the language model-based method correctly
groups the 11 Englishes into the Italic, Ger-
manic, and Slavic branches. It first merges
Norwegian-English and Swedish-English into a
cluster. The two languages belong to the North
Germanic branch of the Germanic branch and
thus are closely related. Subsequently, the lan-
guage model-based method correctly merges the
other languages into the three branches. A dif-
7The Kyoto Language Modeling toolkit: http://www.
phontron.com/kylm/
8We found that the results were not sensitive to the value
of frequency cutoff so long as we set it to a small number.
1140
Polish
Italic Germanic Slavic
13
6
7
8
9
10
BulgarianSwedishFrench Spanish Norwegian CzechItalian RussianDutchGerman
French 
English
Spanish 
English
Italian 
English
Swedish 
English
Norwegian 
English
Dutch 
English
German
English
Polish 
English
Bulgarian 
English
Czech
English
Russian 
English
2 45
Indo-European family tree
Cluster tree generated by  LM-based method
13
4
7
6
8
10
French English Spanish English Italian English Swedish EnglishNorwegian EnglishDutch English German English Polish EnglishBulgarian EnglishCzech English Russian English
2 5
9 Cluster tree generated by vector-based clustering
Figure 1: Experimental results.
ference between its cluster tree and the Indo-
European family tree is that there are some mis-
matches within the Germanic and Slavic branches.
While the difference exists, the method strongly
distinguishes the three branches from one an-
other. The third tree shows that the vector-based
method behaves similarly while it mistakenly at-
taches Polish-English into an independent branch.
From these results, we can say that mother tongue
interference is transferred into the 11 Englishes,
strongly enough for reconstructing its language
family tree, which we propose calling the inter-
language Indo-European family tree in English.
Fig. 2 shows the experimental results with na-
tive and Japanese Englishes. It shows that the
same interlanguage Indo-European family tree
was reconstructed as before. More interestingly,
native English was detached from the interlan-
guage Indo-European family tree contrary to the
expectation that it would be attached to the Ger-
manic branch because English is of course a mem-
ber of the Germanic branch. This implies that
non-nativeness common to the 11 Englishes is
more influential than the intrafamily distance is9;
9Admittedly, we need further investigation to confirm this
argument especially because we applied CRFTagger, which is
developed to analyze native English, to both non-native and
native Englishes, which might affect the results.
Interlanguage Indo-European family tree Other family
JapaneseEnglish1 JapaneseEnglish2
3
Native English
12 13
ACL 2013
Figure 2: Experimental results with native and
Japanese Englishes.
otherwise, native English would be included in
the German branch. Fig. 2 also shows that the
two sets of Japanese English were merged into
a cluster and that it was the most distant in the
whole tree. This shows that the interfamily dis-
tance is the most influential factor. Based on
these results, we can further hypothesize as fol-
lows: interfamily distance > non-nativeness >
intrafamily distance.
5 Discussion
To get a better understanding of the interlanguage
Indo-European family tree, we further explore lin-
guistic features that explain well the above phe-
nomena. When we analyze the experimental re-
sults, however, some problems arise. It is al-
most impossible to find someone who has a good
knowledge of the 11 languages and their mother
language interference in English writing. Besides,
there are a large number of language pairs to com-
pare. Thus, we need an efficient and effective way
to analyze the experimental results.
To address these problems, we did the follow-
ing. First, we focused on only a few Englishes
out of the 11. Because one of the authors had
some knowledge of French, we selected French-
English as the main target. This naturally made
us select the other Italic Englishes as its counter-
parts. Also, because we had access to a native
speaker of Russian who had a good knowledge of
English, we included Russian-English in our fo-
cus. We analyzed these Englishes and then ex-
amined whether the findings obtained apply to the
other Englishes or not. Second, we used a method
for extracting interesting trigrams from the cor-
pus data. The method compares three out of the
11 corpora (for example, French-, Spanish-, and
Russian-Englishes). If we remove instances of a
trigram from each set, the clustering tree involving
1141
the three may change. For example, the removal
of but the hers may result in a cluster tree merg-
ing French- and Russian-Englishes before French-
and Spanish-Englishes. Even if it does not change,
the distances may change in that direction. We an-
alyzed what trigrams had contributed to the clus-
tering results with this approach.
To formalize this approach, we will denote a tri-
gram by t. We will also denote its relative fre-
quency in the language data Di by rti. Then, the
change in the distances caused by the removal of t
from Di, Dj , and Dk is quantified by
s = (rtk ? rti)2 ? (rtj ? rti)2 (4)
in the vector-based method. The quantity (rtk ?
rti)2 is directly related to the decrease in the dis-
tance between Di and Dk and similarly, (rtj ?
rti)2 to that between Di and Dj in the vector-
based method. Thus, the greater s is, the higher the
chance that the cluster tree changes. Therefore, we
can obtain a list of interesting trigrams by sorting
them according to s. We could do a similar calcu-
lation in the language model-based method using
the conditional probabilities. However, it requires
a more complicated calculation. Accordingly, we
limit ourselves to the vector-based method in this
analysis, noting that both methods generated sim-
ilar cluster trees.
Table 2 shows the top 15 interesting trigrams
where Di, Dj , and Dk are French-, Spanish-, and
Russian-Englishes, respectively. Note that s is
multiplied by 106 and r is in % for readability. The
list reveals that many of the trigrams contain the
article a or the. Interestingly, their frequencies are
similar in French-English and Spanish-English,
and both are higher than in Russian-English. This
corresponds to the fact that French and Spanish
have articles whereas Russian does not. Actu-
ally, the same argument can be made about the
other Italic and Slavic Englishes (e.g., the JJ NN:
Italian-English 0.82; Polish-English 0.72)10. An
exception is that of trigrams containing the definite
article in Bulgarian-English; it tends to be higher
in Bulgarian-English than in the other Slavic En-
glishes. Surprisingly and interestingly, however, it
reflects the fact that Bulgarian does have the def-
inite article but not the indefinite article (e.g., the
JJ NN: 0.82; a JJ NN: 0.60 in Bulgarian-English).
10Due to the space limitation, other lists were not included
in this paper but are available at http://web.hyogo-u.
ac.jp/nagata/acl/.
Table 3 shows that the differences in article
use exist even between the Italic and Germanic
branches despite the fact that both have the in-
definite and definite articles. The list still con-
tains a number of trigrams containing articles. For
a better understanding of this, we looked further
into the distribution of articles in the corpus data.
It turns out that the distribution almost perfectly
groups the 11 Englishes into the corresponding
branches as shown in Fig. 3. The overall use of
articles is less frequent in the Slavic-Englishes.
The definite article is used more frequently in the
Italic-Englishes than in the Germanic Englishes
(except for Dutch-English). We speculate that
this is perhaps because the Italic languages have a
wider usage of the definite article such as its modi-
fication of possessive pronouns and proper nouns.
The Japanese Englishes form another group (this
is also true for the following findings). This corre-
sponds to the fact that the Japanese language does
not have an article system similar to that of En-
glish.
s Trigram t rti rtj rtk
5.14 the NN of 1.01 0.98 0.78
4.38 a JJ NN 0.85 0.77 0.62
2.74 the JJ NN 0.87 0.86 0.71
2.30 NN of the 0.49 0.52 0.33
1.64 . . . 0.22 0.12 0.05
1.56 NNS . EOS 0.77 0.70 0.92
1.31 NNS and NNS 0.09 0.13 0.21
1.25 BOS RB , 0.25 0.22 0.14
1.22 of the NN 0.42 0.44 0.30
1.17 VBZ to VB 0.26 0.22 0.14
1.09 BOS i VBP 0.07 0.05 0.17
1.03 NN of NN 0.74 0.70 0.63
0.88 NN of JJ 0.15 0.15 0.25
0.67 the JJ NNS 0.28 0.28 0.20
0.65 NN to VB 0.40 0.38 0.31
Table 2: Interesting trigrams (French- (Di),
Spanish- (Dj), and Russian- (Dk) Englishes).
Another interesting trigram, though not as ob-
vious as article use, is NN of NN, which ranks
12th and 2nd in Table 2 and 3, respectively. In the
Italic Englishes, the trigram is more frequent than
the other non-native Englishes as shown in Fig. 4.
This corresponds to the fact that noun-noun com-
pounds are less common in the Italic languages
than in English and that instead, the of -phrase (NN
of NN) is preferred (Swan and Smith, 2001). For
1142
s Trigram t rti rtj rtk
21.49 the NN of 1.01 0.98 0.54
5.70 NN of NN 0.74 0.70 0.50
3.26 NN of the 0.49 0.52 0.30
3.10 the JJ NN 0.87 0.86 0.70
2.62 . . . 0.22 0.12 0.03
1.53 of the NN 0.42 0.44 0.29
1.50 NN , NN 0.30 0.30 0.18
1.50 BOS i VBP 0.07 0.05 0.19
0.85 NNS and NNS 0.09 0.13 0.19
0.81 JJ NN of 0.40 0.39 0.31
0.68 . . EOS 0.13 0.06 0.02
0.63 a JJ NN 0.85 0.77 0.73
0.63 RB . EOS 0.21 0.16 0.31
0.56 NN , the 0.16 0.16 0.08
0.50 NN of a 0.17 0.09 0.06
Table 3: Interesting trigrams (French- (Di),
Spanish- (Dj), and Swedish- (Dk) Englishes).
instance, orange juice is expressed as juice of or-
ange in the Italic languages (e.g., jus d?orange in
French). In contrast, noun-noun compounds or
similar constructions are more common in Russian
and Swedish. As a result, NN of NN becomes rel-
atively frequent in the Italic Englishes. Fig. 4 also
shows that its distribution roughly groups the 11
Englishes into the three branches. Therefore, the
way noun phrases (NPs) are constructed is a clue
to how the three branches were clustered.
This finding in turn reveals that the consecu-
tive repetitions of nouns occur less in the Italic
Englishes. In other words, the length tends to
be shorter than in the others where we define
the length as the number of consecutive repeti-
tions of common nouns (for example, the length
of orange juice is one because a noun is con-
secutively repeated once). To see if this is true,
we calculated the average length for each English.
Fig. 5 shows that the average length roughly dis-
tinguishes the Italic Englishes from the other non-
native Englishes; French-English is the shortest,
which is explained by the discussion above, while
Dutch- and German-Englishes are longest, which
may correspond to the fact that they have a prefer-
ence for noun-noun compounds as Snyder (1996)
argues. For instance, German allows the concate-
nated form as in Orangensaft (equivalently or-
angejuice). This tendency in the length of noun-
noun compounds provides us with a crucial insight
for native language identification, which we will
 2
 3
 4
 5
 6
 1  1.5  2  2.5  3
R
el
at
iv
e 
fre
qu
en
cy
 o
f d
ef
in
ite
 a
rti
cle
 (%
)
Relative frequency of indefinite article (%)
Bulgarian
Czech
Dutch
French
German
Italian
Norwegian
Polish
Russian
Spanish
Swedish
English
Japanese1
Japanese2
Italic
Germanic
Slavic
Japanese
Figure 3: Distribution of articles.
 0
 0.5  1
Relative frequency of NN of NN (%)
French
Italian
Spanish
Italic
Polish
Russian
Bulgarian
Czech
Slavic
English
Dutch
Swedish
German
Norwegian
Germanic
Japanese1
Japanese2 Japanese
Figure 4: Relative frequency of NN of NN in each
corpus (%).
come back to in Sect. 6.
The trigrams BOS RB , in Table 2 and RB . EOS
in Table 3 imply that there might also be a certain
pattern in adverb position in the 11 Englishes (they
roughly correspond to adverbs at the beginning
and end of sentences). Fig. 6 shows an insight into
this. The horizontal and vertical axes correspond
to the ratio of adverbs at the beginning and the end
of sentences, respectively. It turns out that the Ger-
man Englishes form a group. So do the Italic En-
glishes although it is less dense. In contrast, the
Slavic Englishes are scattered. However, the ra-
tios give a clue to how to distinguish Slavic En-
glishes from the others when combined with other
1143
 0
 0.1
Average length of noun-noun compounds
French
Italian
Spanish
Italic
Bulgarian
Czech
Russian
Polish
Slavic
Swedish
Norwegian
German
Dutch
English
Germanic
Japanese1
Japanese2 Japanese
Figure 5: Average length of noun-noun com-
pounds in each corpus.
 5
 10
 15  20  25  30
R
at
io
 o
f a
dv
er
bs
 a
t t
he
 e
nd
 (%
)
Ratio of adverbs at the beginning (%) 
Bulgarian
Czech
Polish
Russian
Dutch
German
Norwegian
Swedish
French
ItalianSpanish
English
Japanese1
Japanese2
Italic
Germanic
Slavic
Japanese
Figure 6: Distribution of adverb position.
trigrams. For instance, although Polish-English
is located in the middle of Swedish-English and
Bulgarian-English in the distribution of articles
(in Fig. 3), the ratios tell us that Polish-English is
much nearer to Bulgarian-English.
6 Implications for Work in Related
Domains
Researchers including Wong and Dras (2009),
Wong et al (2011; 2012), and Koppel et al (2005)
work on native language identification and show
that machine learning-based methods are effec-
tive. Wong and Dras (2009) propose using infor-
mation about grammatical errors such as errors in
determiners to achieve better performance while
they show that its use does not improve the perfor-
mance, contrary to the expectation. Related to this,
other researchers (Koppel and Ordan, 2011; van
Halteren, 2008) show that machine learning-based
methods can also predict the source language of
a given translated text although it should be em-
phasized that it is a different task from native lan-
guage identification because translation is not typ-
ically performed by non-native speakers but rather
native speakers of the target language11.
The experimental results show that n-grams
containing articles are predictive for identify-
ing native languages. This indicates that they
should be used in the native language identifi-
cation task. Importantly, all n-grams contain-
ing articles should be used in the classifier unlike
the previous methods that are based only on n-
grams containing article errors. Besides, no ar-
ticles should be explicitly coded in n-grams for
taking the overuse/underuse of articles into con-
sideration. We can achieve this by adding a spe-
cial symbol such as ? to the beginning of each NP
whose head noun is a common noun and that has
no determiner in it as in ?I like ? orange juice.?
In addition, the length of noun-noun com-
pounds and the position of adverbs should also
be considered in native language identification. In
particular, the former can be modeled by the Pois-
son distribution as follows. The Poisson distribu-
tion gives the probability of the number of events
occurring in a fixed time. In our case, the number
of events in a fixed time corresponds to the num-
ber of consecutive repetitions of common nouns in
NPs, which in turn corresponds to the length. To
be precise, the probability of a noun-noun com-
pound with length l is given by
Pr(l) = ?
l
l! e
??, (5)
where ? corresponds to the average length. Fig. 7
shows that the observed values in the French-
English data very closely fit the theoretical proba-
11For comparison, we conducted a pilot study where we
reconstructed a language family tree from English texts
in European Parliament Proceedings Parallel Corpus (Eu-
roparl) (Koehn, 2011). It turned out that the reconstructed
tree was different from the canonical tree (available at http:
//web.hyogo-u.ac.jp/nagata/acl/). However,
we need further investigation to confirm it because each sub-
corpus in Europarl is variable in many dimensions includ-
ing its size and style (e.g., overuse of certain phrases such as
ladies and gentlemen).
1144
 0
 0.5
 1
 0  1  2  3
Pr
ob
ab
ilit
y
Length of noun-noun compound
Theoretical
Observed
Figure 7: Distribution of noun-noun compound
length for French-English.
bilities given by Equation (5)12. This holds for the
other Englishes although we cannot show them be-
cause of the space limitation. Consequently, Equa-
tion (5) should be useful in native language identi-
fication. Fortunately, it can be naturally integrated
into existing classifiers.
In the domain of historical linguistics, re-
searchers have used computational and corpus-
based methods for reconstructing language fam-
ily trees. Some (Enright and Kondrak, 2011;
Gray and Atkinson, 2003; Barbanc?on et al, 2007;
Batagelj et al, 1992; Nakhleh et al, 2005) ap-
ply clustering techniques to the task of language
family tree reconstruction. Others (Kita, 1999;
Rama and Singh, 2009) use corpus statistics for
the same purpose. These methods reconstruct lan-
guage family trees based on linguistic features that
exist within words including lexical, phonological,
and morphological features.
The experimental results in this paper suggest
the possibility of the use of non-native texts for re-
constructing language family trees. It allows us to
use linguistic features that exist between words, as
seen in our methods, which has been difficult with
previous methods. Language involves the features
between words such as phrase construction and
syntax as well as the features within words and
thus they should both be considered in reconstruc-
12The theoretical and observed values are so close that it
is difficult to distinguish between the two lines in Fig. 7. For
example, Pr(l = 1) = 0.0303 while the corresponding ob-
served value is 0.0299.
tion of language family trees.
7 Conclusions
In this paper, we have shown that mother tongue
interference is so strong that the relations be-
tween members of the Indo-European language
family are preserved in English texts written by
Indo-European language speakers. To show this,
we have used clustering to reconstruct a lan-
guage family tree from 11 sets of non-native
English texts. It turned out that the recon-
structed tree correctly groups them into the Italic,
Germanic, and Slavic branches of the Indo-
European family tree. Based on the resulting
trees, we have then hypothesized that the fol-
lowing relation holds in mother tongue interfer-
ence: interfamily distance > non-nativeness >
intrafamily distance. We have further explored
several intriguing linguistic features that play an
important role in mother tongue interference: (i)
article use, (ii) NP construction, and (iii) adverb
position, which provide several insights for im-
proving the tasks of native language identification
and language family tree reconstruction.
Acknowledgments
This work was partly supported by the Digiteo for-
eign guest project. We would like to thank the
three anonymous reviewers and the following per-
sons for their useful comments on this paper: Ko-
taro Funakoshi, Mitsuaki Hayase, Atsuo Kawai,
Robert Ladig, Graham Neubig, Vera Sheinman,
Hiroya Takamura, David Valmorin, Mikko Vile-
nius.
References
Jan Aarts and Sylviane Granger, 1998. Tag sequences
in learner corpora: a key to interlanguage gram-
mar and discourse, pages 132?141. Longman, New
York.
Bengt Altenberg and Marie Tapper, 1998. The use of
adverbial connectors in advanced Swedish learners?
written English, pages 80?93. Longman, New York.
Franc?ois Barbanc?on, Tandy Warnow, Steven N. Evans,
Donald Ringe, and Luay Nakhleh. 2007. An exper-
imental study comparing linguistic phylogenetic re-
construction methods. Statistics Technical Reports,
page 732.
Vladimir Batagelj, Tomaz? Pisanski, and Dami-
jana Kerz?ic?. 1992. Automatic clustering of lan-
guages. Computational Linguistics, 18(3):339?352.
1145
Robert S.P. Beekes. 2011. Comparative Indo-
European Linguistics: An Introduction (2nd ed.).
John Benjamins Publishing Company, Amsterdam.
Martin Chodorow, Michael Gamon, and Joel R.
Tetreault. 2010. The utility of article and prepo-
sition error correction systems for English language
learners: feedback and assessment. Language Test-
ing, 27(3):419?436.
David Crystal. 1997. The Cambridge Encyclopedia of
Language (2nd ed.). Cambridge University Press,
Cambridge.
Niels Davidsen-Nielsen and Peter Harder, 2001.
Speakers of Scandinavian languages: Danish, Nor-
wegian, Swedish, pages 21?36. Cambridge Univer-
sity Press, Cambridge.
Alvar Ellega?rd. 1959. Statistical measurement of lin-
guistic relationship. Language, 35(2):131?156.
Jessica Enright and Grzegorz Kondrak. 2011. The ap-
plication of chordal graphs to inferring phylogenetic
trees of languages. In Proc. of 5th International
Joint Conference on Natural Language Processing,
pages 8?13.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English v2. Presses universitaires de Lou-
vain, Louvain.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435?438.
Jiawei Han and Micheline Kamber. 2006. Data Min-
ing: Concepts and Techniques (2nd Ed.). Morgan
Kaufmann Publishers, San Francisco.
Bing-Hwang Juang and Lawrence R. Rabiner. 1985.
A probabilistic distance measure for hidden Markov
models. AT&T Technical Journal, 64(2):391?408.
Kenji Kita. 1999. Automatic clustering of languages
based on probabilistic models. Journal of Quantita-
tive Linguistics, 6(2):167?171.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proc. of International Conference on Acoustics,
Speech, and Signal Processing, volume 1, pages
181?184.
Philipp Koehn. 2011. Europarl: A parallel corpus for
statistical machine translation. In Proc. of 10th Ma-
chine Translation Summit, pages 79?86.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of 49th Annual Meeting
of the Association for Computational Linguistics,
pages 1318?1326.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author?s native language by
mining a text for errors. In Proc. of 11th ACM
SIGKDD International Conference on Knowledge
Discovery in Data Mining, pages 624?628.
Alfred L. Kroeber and Charles D. Chrie?tien. 1937.
Quantitative classification of Indo-European lan-
guages. Language, 13(2):83?103.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1210?1219.
Luay Nakhleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A comparison of phyloge-
netic reconstruction methods on an Indo-European
dataset. Transactions of the Philological Society,
103(2):171?192.
Taraka Rama and Anil Kumar Singh. 2009. From bag
of languages to family trees from noisy corpus. In
Proc. of Recent Advances in Natural Language Pro-
cessing, pages 355?359.
Anna Giacalone Ramat and Paolo Ramat, 2006. The
Indo-European Languages. Routledge, New York.
William Snyder. 1996. The acquisitional role of the
syntax-morphology interface: Morphological com-
pounds and syntactic complex predicates. In Proc.
of Annual Boston University Conference on Lan-
guage Development, volume 2, pages 728?735.
Masatoshi Sugiura, Masumi Narita, Tomomi Ishida,
Tatsuya Sakaue, Remi Murao, and Kyoko Muraki.
2007. A discriminant analysis of non-native speak-
ers and native speakers of English. In Proc. of Cor-
pus Linguistics Conference CL2007, pages 84?89.
Michael Swan and Bernard Smith. 2001. Learner En-
glish (2nd Ed.). Cambridge University Press, Cam-
bridge.
Hans van Halteren. 2008. Source language markers
in EUROPARL translations. In Proc. of 22nd Inter-
national Conference on Computational Linguistics,
pages 937?944.
Sze-Meng J. Wong and Mark Dras. 2009. Con-
trastive analysis and native language identification.
In Proc. Australasian Language Technology Work-
shop, pages 53?61.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2011. Exploiting parse structures for native lan-
guage identification. In Proc. Conference on Em-
pirical Methods in Natural Language Processing,
pages 1600?1611.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2012. Exploring adaptor grammars for native lan-
guage identification. In Proc. Joint Conference on
1146
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 699?709.
1147
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 754?764,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Correcting Preposition Errors in Learner English Using Error Case
Frames and Feedback Messages
Ryo Nagata
1?
Mikko Vilenius
2
Edward Whittaker
3
1
Konan University / Kobe, Japan
2
The Japan Institute for Educational Measurement, Inc. / Tokyo, Japan
3
Inferret Limited / Northampton, England
nagata-acl@hyogo-u.ac.jp.
Abstract
This paper presents a novel framework
called error case frames for correcting
preposition errors. They are case frames
specially designed for describing and cor-
recting preposition errors. Their most dis-
tinct advantage is that they can correct er-
rors with feedback messages explaining
why the preposition is erroneous. This pa-
per proposes a method for automatically
generating them by comparing learner and
native corpora. Experiments show (i) au-
tomatically generated error case frames
achieve a performance comparable to con-
ventional methods; (ii) error case frames
are intuitively interpretable and manually
modifiable to improve them; (iii) feedback
messages provided by error case frames
are effective in language learning assis-
tance. Considering these advantages and
the fact that it has been difficult to provide
feedback messages by automatically gen-
erated rules, error case frames will likely
be one of the major approaches for prepo-
sition error correction.
1 Introduction
This paper presents a novel framework for correct-
ing preposition errors. Its most significant advan-
tage over previous methods is that it can provide
learners with feedback messages, that is, explana-
tory notes describing why the detected preposi-
tion is erroneous and should be corrected as in-
dicated, as shown in Fig. 1. Despite the fact that
appropriate feedback messages are essential in
language learning assistance (Ferris and Roberts,
2001; Robb et al, 1986), which is one of the im-
mediate applications of grammatical error correc-
?
Part of this work was performed while the author was a
visiting researcher at LIMSI, Orsay (France).
Target sentence:  In the univerysity, I studied English in the morning.Error: correct preposition atFeedback message:Though both at and in are prepositions of place, at is used to denote the place (university) to which the person belongs and where  the learning activities  take place. 
Target sentence:  When the day is holiday, I go to shopping, singing in Karaoke and talking in cafe. Error: remove toFeedback message:Go directly takes the activity  without a preposition when it means traveling to a place in order to take part in an activity  by go and ?ing,        e.g.,  I went shopping. Similar expressions: go swimming, go fishing, go sightseeing
Figure 1: Error correction and feedback messages
provided by the proposed method.
tion, almost all previous methods are incapable of
providing feedback messages.
Grammatical error correction has been inten-
sively studied in recent years. Current methods
mostly exploit machine learning-based classifiers
to correct target errors; examples are errors in ar-
ticle (Han et al, 2006; Nagata et al, 2006; Ro-
zovskaya and Roth, 2011), preposition (Chodorow
et al, 2007; Felice and Pulman, 2008; Rozovskaya
and Roth, 2011; Tetreault et al, 2010), and
tense (Nagata and Kawai, 2011; Tajiri et al, 2012),
to name a few. Recently, Wu and Ng (2013) and
Rozovskaya and Roth (2013) proposed methods
for simultaneously correcting multiple types of er-
rors using integer linear programming. Another
major approach is to use a language model (LM)
for predicting correct words or phrases for a given
context. Some researchers (Brockett et al, 2006;
Yoshimoto et al, 2013) use statistical machine
translation (SMT) for the same purpose, which
can be regarded as the mixture of a classifier and
an LM. With these diverse techniques, correction
performance has dramatically improved against a
wide variety of target errors.
As noted above, however, one of the crucial
limitations of these previous methods is that they
754
are not capable of providing feedback messages.
They are not suitable for generating open-class
text such as feedback messages by their nature.
Some researchers (Kakegawa et al, 2000; McCoy
et al, 1996) made an attempt to develop hand-
crafted rules for correcting errors with feedback
messages. However, this approach encounters the
tremendous difficulty of covering a wide variety of
errors using hand-crafted rules.
In view of this background, this paper presents a
novel error correction framework called error case
frames an example of which is shown in Fig. 2.
They are case frames specially designed for de-
scribing and correcting errors in preposition at-
tached to a verb; the reader may be able to see that
it describes preposition errors such as *John often
goes shopping to the market with his family. and
that the preposition to should be replaced with at.
This paper proposes a method for automatically
generating them by comparing learner and native
corpora. Achieving a comparable correction per-
formance, they have the following two advantages
over the conventional approaches: (i) they are in-
tuitively interpretable and manually modifiable to
enrich them; (ii) they are capable of providing
feedback messages.
The rest of this paper is structured as fol-
lows. Sect. 2 introduces the definition of error case
frames. Sect. 3 discusses the method for generat-
ing error case frames. Sect. 4 describes how to cor-
rect preposition errors with feedback messages by
error case frames. Sect. 5 describes experiments
conducted to evaluate error case frames. Sect. 6
discusses the experimental results.
2 Error Case Frame
An error case frame consists of a verb, cases, and
a feedback message as shown in Fig. 2
1
. The fol-
lowing explains error case frames in detail based
on this example; occasionally consulting it may
help understanding the following sections.
An error case frame always has a verb. In Fig. 2,
the verb is go.
Cases are arguments the verb takes in an error
case frame. A case consists of a case tag and case
elements. A case tag and case elements describe,
respectively, the role that the case plays in the er-
ror case frame and a set of words that are allowed
1
Fig. 2 shows an example of error case frames for illus-
tration purposes. They are formally expressed in a machine-
readable format such as XML.
go
Feedback messageTo mean traveling to a place in order to take part in an activity, go takes at, in, or on depending on the activity. For example, the activity shoppingtakes place at a store (not  shopping  to a store),    and thus go shopping at a store.cf. We went  sightseeing in Baltimore.
Preposition casePrep_dobj: {shopping} *Prep_to: {store,market}  ?  Prep_at(Prep_with: {family})
Basic caseSubj: {PERSON}
verbcase elementcase tagcases
Figure 2: Example of an error case frame.
to appear as the argument. For instance, in Fig. 2,
?Subj: {PERSON}? is a case where its case tag
and element are ?Subj:? and ?{PERSON},? re-
spectively, denoting that a person such as John
plays a role of the subject of the verb. Note that
tokens in all upper case such as ?PERSON? refer
to a group of words such as {john,he,? ? ? } in this
paper.
Cases are classified into two categories: basic
and preposition cases. Basic cases are either a sub-
ject or a particle, whose case tags are ?Subj:? and
?Ptr:?, respectively. The ?Subj:? case is obliga-
tory while the ?Ptr:? is optional. Preposition cases
correspond to the prepositions the verb takes as its
arguments. Its case tag has the form of ?Prep x?
where x ranges over the target prepositions. It
should be emphasized that direct and indirect ob-
jects are included in the preposition cases for effi-
ciency; their case tags are denoted as ?Prep dobj?
and ?Prep iobj?, respectively. Preposition cases
are classified into those obligatory and optional.
Optional here means that the verb can constitute
a sentence with or without the preposition. Op-
tional prepositions are written in parentheses as in
?(Prep with:{family})?.
Preposition cases describe the information
about an error. An error case frame is constrained
to contain only one erroneous preposition case. It
is marked with the symbol ?*?. So, the preposi-
tion case ?*Prep to:{store,market}? is erroneous
in Fig. 2. The correct preposition is described af-
ter the symbol ??? as in ?? Prep at?.
Error case frames are furnished with feedback
messages. Unlike verbs and cases, which are au-
tomatically filled based on corpus data, they are
manually edited. A human annotator interprets
error case frames and adds explanatory notes to
them. This may seem time-consuming. How-
755
ever, the editing is far more efficient than manually
creating correction rules with feedback messages
from scratch because error case frames are highly
abstracted as explained in Sect. 3. Above all, it is a
significant advantage over the previous classifier-
/LM-based methods considering that there exists
no effective technique for augmenting these meth-
ods with feedback messages.
3 Generating Error Case Frames
The method proposed here exploits two sources
of corpus data: native and learner corpora. Case
frames (error case frames without the information
about an error and a feedback message) can be
automatically extracted from parsed sentences as
Kawahara and Uchimoto (2008) show. The pro-
posed method generates error case frames by com-
paring case frames generated from the learner cor-
pus with those from the native corpus. The basic
approach is to extract, as error case frames, case
frames which appear in the learner corpus but not
in the native corpus. However, this approach is so
simple that it extracts undesirable false error case
frames which do not actually correspond to prepo-
sition errors. To overcome the problem, the fol-
lowing procedures are applied:
(1) Filtering input sentences
(2) Extracting case frames
(3) Recognizing optional cases
(4) Grouping case frames
(5) Selecting candidate error case frames
(6) Determining correct prepositions
(7) Enriching error case frames
(8) Manually editing error case frames
(1) Filtering input sentences: This is a pre-
process to filter out unsuitable input sentences for
case frame generation. Accurate parsing is es-
sential for accurate case frame generation. Pars-
ing errors tend to occur in longer sentences. To
reduce parsing errors, Kawahara and Uchimoto
(2008) propose filtering out sentences which are
longer than 20 words. We adopt this filtering in
our method. We also filter out sentences contain-
ing commas, which often introduce complex struc-
tures. We apply the filtering pre-process only to
the native corpus; the availability of learner cor-
pora is still somewhat limited and therefore we use
all the sentences available in the learner corpus for
better coverage of preposition errors.
(2) Extracting case frames: This procedure
can be viewed as a slot filling task where the
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_to: {market}  
Basic caseSubj: {PERSON}
Input: John went shopping to the market.
wentJohn shoppingsubj dobjprep_to
Dependency parse
PERSONmapping to sense 
marketthedet
Case framegostemming
Figure 3: Example of case frame extraction.
slots are the verb and the cases in a case frame.
To achieve this, the corpus data are first parsed
by a parser. Then, for each verb, the predicate-
argument structures are extracted from the parses
as shown in Fig. 3. Here, only head words are ex-
tracted as arguments. They are reduced to their
base form when extracted. Certain classes of
words are replaced with their corresponding sense
(e.g., John to PERSON); the mapping between
words and their senses is shown in Appendix A.
In the case of the learner corpus, mis-spelt words
are automatically corrected using a spell-checker.
Finally, a case frame is created by filling its slots
with the extracted predicate-argument structures.
Hereafter, case frames generated from the native
and learner corpora will be referred to as the na-
tive and learner case frames, respectively.
(3) Recognizing optional cases: it is crucial
for generating flexible error case frames to recog-
nize optional preposition cases. Optional preposi-
tion cases are determined by the following heuris-
tic rules: (a) Objects are always obligatory; (b)
The number of obligatory preposition cases (ex-
cept objects) is at most one; (c) Prepositions ap-
pearing left of the verb are optional; (d) Preposi-
tions appearing right of the verb are optional ex-
cept the one which is nearest to the verb. Rule (a)
states that objects are always recognized as oblig-
atory
2
. Rule (b) constrains an error case frame to
have at most one obligatory preposition. Certain
verbs sometimes have more than one obligatory
preposition as in range from A to B. However, the
large majority of verbs satisfy rule (b). Rule (c)
states that prepositions appearing left of the verb
2
A sentence can be constituted without objects as in We
sing. Rule (a) always mistakenly recognizes such objects as
obligatory. However, preposition errors never appear in sen-
tences consisting of no object nor prepositions, and thus, the
objects mistakenly recognized as obligatory never cause any
problems in preposition error correction in practice.
756
in the input sentence are optional preposition cases
as in In the morning, he went shopping. Rule (c)
is based on the assumption that obligatory cases
are tied to the verb more strongly than optional
cases. In other words, obligatory cases cannot
easily change their position. Conversely, optional
cases have more freedom of their position, which
enables them to appear left of a verb. Admittedly,
obligatory prepositions can appear left of a verb
as in To school, he went in certain circumstances
such as in poetry. However, this usage is not so
frequent in corpora normally used as training data
such as newspaper articles. Rule (d), together with
rule (b), states that if more than one preposition
appears right of the verb, the one nearest to the
verb is obligatory and the rest are optional. Rule
(d) is based on the same reasoning as in rule (c).
Optional preposition cases are sometimes deter-
mined naturally by comparing two case frames.
In this case, one of them must consist of only
the object(s) as its preposition case(s) as in ?[go
Subj:{PERSON} Prep dobj:{shopping} ].? Then,
the other case frame must consist of the same verb,
the same basic cases, and the same object(s). The
only difference between them is preposition cases
(except the object(s)) (e.g., [go Subj:{PERSON}
Prep dobj:{shopping} Prep at:{market} ]). The
case frame only with the object(s) proves the other
to be valid without the preposition case(s). Thus,
these preposition cases are recognized as optional
(e.g., [go Subj:{PERSON} Prep dobj:{shopping}
(Prep at:{market}) ]).
(4) Grouping case frames: Similar case frames
in the native case frames are grouped into one,
which will play an important role in (7) Enriching
error case frames. Case frames comprising sim-
ilar cases tend to denote similar usage of a verb.
Considering this, case frames are merged into one
if they consist of the same verb, the same basic
cases, and the same case tags of the obligatory
preposition cases. The grouping procedure is illus-
trated in Fig. 4. When preposition cases are oblig-
atory in one case frame and optional in the other,
the discrepancy is resolved by setting the prepo-
sition case to optional in the merged case frame.
Note that this grouping procedure is not applied to
the learner case frames so that erroneous usages in
the learner case frames do not propagate to other
(correct) learner case frames.
(5) Selecting candidate error case frames:
Candidates for error case frames are selected from
the learner case frames. If a learner case frame
does not match, ignoring optional preposition
cases, any native case frame, it is selected as a
candidate for an error case frame on the assump-
tion that case frames corresponding to erroneous
usages do not appear in the native corpus.
Alternatively, an error-annotated learner cor-
pus can be used to select error case frames; sim-
ply extracting case frames of which preposition is
marked as an error gives error case frames. In this
case
3
, procedure (6) may be omitted and proce-
dure (7) is directly applied after procedure (5).
(6) Determining correct prepositions: Now,
correct prepositions for the candidate error case
frames are explored. Each case tag of the prepo-
sition cases in a candidate is replaced, one at a
time, with one of the other target prepositions.
This replacement can be interpreted as error cor-
rection. Take as an example the following can-
didate error case frame: [go Subj:{PERSON}
Prep dobj:{shopping} Prep to:{market} ]. Re-
placing the case tag ?Prep to? with ?Prep at? cor-
responds to correct expressions such as John of-
ten goes shopping at the market. Note that re-
placing a direct object with one of the preposi-
tions corresponds to correcting an omission er-
ror as in ?Prep dobj? with ?Prep to? in ?[go
Subj:{PERSON} Prep dobj:{market} ]?. Simi-
larly, replacing a preposition with an object cor-
responds to correcting an extra-preposition er-
ror (e.g., ?Prep to? with ?Prep dobj? in ?[go
Subj:{PERSON} Prep to:{shopping} ])?.
To examine whether each correction is valid
or not, the native case frames are again used; if
the replaced case frame matches one of the na-
tive case frames, the correction is determined to
be valid. Here, we define the match as the two
case frames consisting of the same verb, the same
basic cases, the same obligatory preposition cases,
and the same preposition case to which the cor-
rection is applied (if it is an optional one). If the
condition is satisfied, the information on the error
and correction is added to the candidate error case
frame. If a valid correction is found, the candi-
date is determined to be a valid error case frame.
In total, their validity is double-checked, once in
(5) and once in (6), by comparing them with the
3
We do not make use of error-annotated learner corpora in
this paper in order to reveal how well the proposed methods
perform without such corpora. In practice, one can use error-
annotated learner corpora together with raw learner corpora
to achieve better performance.
757
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {store}(Prep_with:   {family} ) 
Basic caseSubj: {PERSON}
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {store,market}(Prep_with:   {family} ) 
Basic caseSubj: {PERSON}
go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {market}
Basic caseSubj: {PERSON}
Figure 4: Example of grouping case frames.
native case frames.
(7) Enriching error case frames: The gener-
ated error cases are limited in error coverage be-
cause the procedures so far solely rely on prepo-
sition errors appearing in the learner corpus. In
other words, it is impossible to generate error case
frames corresponding to preposition errors which
do not appear in the learner corpus. To overcome
this limitation, the generated error case frames are
enriched using the native case frames. For each er-
ror case frame, we already know the correspond-
ing native (thus, correct) case frame, which is ob-
tained in (6). The corresponding native case frame
is normally much richer in preposition cases be-
cause of the optional cases and grouping given
by procedures (3) and (4), as shown at the top of
Fig. 5. These additional cases are useful to enrich
error case frames.
For the preposition case which is determined
to be erroneous, its correct preposition is found
in the error case frame (e.g., ?? Prep at? at the
top-left of Fig. 5). Also, its correct preposition
case is found in the corresponding native case
frame (e.g., ?Prep at:{market,store}? at the
top-right). Replacing the case element of the
erroneous case by one of the case elements of
the correct preposition case gives a new can-
didate for an error case frame (e.g., replacing
market of ?*Prep to:{market}? by store gives
?[go Subj:{PERSON} Prep dobj:{shopping}
*Prep to:{store} ].? It should be emphasized that
this new error case frame is still a candidate at this
point and the usage might be correct. To verify
if it really describes an erroneous preposition
use, the native case frames are searched for; if
it matches one of them, that means that the use
of the preposition actually appears in the native
go
Feedback message    
Preposition casePrep_dobj: {shopping} *Prep_to:  {market} ?  Prep_at
Basic caseSubj: {PERSON}
Error case frame go
Feedback message    
Preposition casePrep_dobj: {shopping} Prep_at: {market,store}(Prep_with:   {family} ) 
Basic caseSubj: {PERSON}
Native case frame
Verificationgo
Feedback message    
Preposition casePrep_dobj: {shopping} *Prep_to:       {market,store} ?  Prep_at(Prep_with:   {family})  
Basic caseSubj: {PERSON}
Expanded error case frame
Figure 5: Enriching an error case frame.
corpus. Therefore, it should be discarded. Only
if a match is not found, is the case element added
to the erroneous preposition case in the original
error case frame. This process is illustrated in the
box denoted as Verification in Fig. 5.
For the other preposition cases which are not er-
roneous, the enriching procedure is much simpler.
They are simply added to the error case frame as
shown in Fig. 5. One thing we should take care
of is that there might be a discrepancy in obliga-
tory/optional between the cases of the error case
frame and the native case frame. This discrepancy
is solved by setting the preposition case in the er-
ror case frame to optional. The resulting expanded
error case frame after procedure (7) is shown at
the bottom of Fig. 5 where the enriched cases are
shown in red.
(8) Manually Editing Error Case Frames:
The most important editing is the addition of feed-
back messages. A human annotator interprets the
generated error case frames and adds explanatory
notes to them. Although this basically requires
manual editing, part of feedback messages can be
automatically created to facilitate the procedure.
For example, example sentences corresponding to
an error case frame can be automatically added
to it, whether correct or error examples, because
the original sentences from which the (error) case
frames extracted are available in the native and
learner corpora. Besides, setting a variable to
the feedback message allows it to be adaptable to
correction results as shown in Fig. 6. In Fig. 6,
X
Prep to
is a variable. It is replaced with one
of the case elements of ?Prep to:? depending on
correction results. Also, it will be beneficial to
link similar error case frames each other, which
allows the user to obtain additional information.
758
go
Feedback messageTo mean traveling to a place in order to take part in an activity, go takes at, in, or on depending on the activity. For example, the activity shoppingtakes place at a _ (not  shopping  to a	_ ), and thus go shopping at 		_ .cf. We went  sightseeing in Baltimore.
Preposition casePrep_dobj: {shopping} *Prep_to: {store,market}  ?  Prep_at(Prep_with: {family})
Basic caseSubj: {PERSON}
verbcase elementcase tag
cases
Figure 6: Error case frame with a variable.
For example, the example error case frame in
Fig. 6 may be linked to similar case frames such as
?[ go Subj:{PERSON} Prep dobj:{sightseeing}
*Prep to:{Baltimore} ? Prep in ].? One can re-
trieve similar error case frames from the generated
error case frames where the similarity between
two error case frames are defined by the overlap
in the verb, the basic cases, and the case tags of
the preposition cases.
The generated error case frames may be further
edited to enrich them. As we can see in Fig. 5, the
generated error case frames are easy to interpret.
This property enables us to manually edit them to
enrich their preposition cases. For example, one
might add a case element such as supermarket to
the preposition case ?Prep to:{market,store}? in
the example error case frame. Conversely, one
might discard unnecessary case elements, cases,
or even error case frames.
4 Correcting Preposition Errors
Preposition errors are corrected by applying the
generated error case frames to the target text. Case
frames are first extracted from the target text by the
same procedures (2) and (3) in Sect. 3. Then, each
extracted case frame is examined if it matches one
of the error case frames. If a match is found, the
preposition is detected as an error and the correct
preposition is suggested with the feedback mes-
sage according to the matched error case frame.
The match between a case frame and an error case
frame is defined in the exact same manner as in
procedure (4) in Sect. 3. Sometimes, a case frame
matches more than one error case frame suggest-
ing different corrections. In this case, the most
frequent correction among the candidates is cho-
sen to correct the error, which was applied in the
evaluation described in Sect. 5
4
.
One of the advantages of error case frames is
that they do not require an error-annotated corpus
as explained in the previous section. This means
that the target text itself can be used as part of a
learner corpus for generating error case frames at
the time of error correction. Applying procedures
(2) to (7) to the target text generates additional er-
ror case frames
5
. Although feedback messages are
not available in these additional error case frames,
they are still useful for improving correction per-
formance, especially in recall. Hereafter, this way
of error case frame generation will be referred to
as active generation.
A pre-experiment using a development data set
revealed that there were some preposition errors
for which error case frames were not generated
even though the corresponding erroneous and cor-
rect preposition usages appeared in the learner and
native corpora, respectively. They are preposition
errors where the preposition is incorrectly used
with an adverb as in *John went to there. To be
precise, they are either an adverb denoting a place
(e.g., there) with a preposition concerning a place
(at, in, on, and to) or a noun denoting time, fre-
quency, and duration with a preposition concern-
ing time, frequency, and duration (at, for, in, and
on). In the native corpus, these adverbs or nouns
are correctly used without a preposition and thus
they are not recognized as a prepositional phrase
by a parser. Therefore, corresponding native case
frames are never found for these types of errors
in procedure (6), and in turn error case frames are
never generated for them.
Considering that they are limited in number
because they are independent of verbs and ba-
sic cases, we decided to manually create er-
ror case frames describing these types of er-
rors. In these error case frames, the verb
and the basic cases are filled with ANY denot-
ing any word. The preposition cases are man-
ually filled based on the linguistic knowledge
known as absence of preposition (Quirk et al,
1985). For example, an error case frame for
the above error would be ?[ANY Subj:{ANY}
*Prep to:{here,somewhere,there}? Prep dobj ].?
Certain errors involve a phrase such as *John goes
shopping in every morning. To handle these cases,
4
Ties are broken by random selection.
5
Recall that procedure (1) is only applied to the native
corpus.
759
these manually created error case frames are al-
lowed to have phrases as their case elements (e.g.,
[ANY Subj:{ANY} *Prep in:{every morning} ?
Prep dobj ]).
5 Evaluation
We evaluated the proposed method from two
points of view: correction performance and use-
fulness of feedback messages. We measured cor-
rection performance by recall, precision, and F -
measure. In the evaluation on usefulness of feed-
back messages, three human raters (a teacher of
English at college and two who have a master
degree in TESOL) separately examined whether
each feedback message was useful for learning the
correct usage of the preposition. We defined use-
fulness by the ratio of feedback messages evalu-
ated as useful to the total number of feedback mes-
sages.
We used the following data sets in the evalua-
tion. We selected the Konan-JIEM (KJ) learner
corpus (Nagata et al, 2011) as the target texts. The
KJ learner corpus is fully annotated with grammat-
ical errors. In addition, it includes error correc-
tion results of several benchmark systems. This
means that one can directly compare correction
results of a new method with those of the bench-
mark systems, which reveals where the method is
strong and weak compared to the benchmark sys-
tems. The KJ corpus consists of training and test
sets. We used the training set to generate error case
frames and evaluated correction performance on
the test set. In addition to these data sets, we cre-
ated a development set, which we had collected
to develop the proposed method. We did not use
it in the final evaluation. As a native corpus, we
used the EDR corpus (Japan electronic dictionary
research institute Ltd, 1993), the Reuters-21578
corpus
6
, and the LOCNESS corpus
7
. We used
the lexicalized dependency parser in the Stanford
Statistical Natural Language Parser (ver.2.0.3) (de
Marneffe et al, 2006) to obtain parses for the data
sets. Table 1 shows the statistics on the data sets.
Using these data sets, we implemented three
versions of the proposed method. The first one
was based on error case frames generated from the
training set of the KJ corpus. The second one was
the first one with active generation. To implement
6
Reuters-21578, Distribution 1.0, http://www.
research.att.com/
?
lewis
7
http://www.uclouvain.be/en-cecl.html
Name # of tokens # of errors
KJ training 22,701 327
KJ test 8,065 131
Dev. set 47,217 774
EDR 1,745,863 ?
Reuters 28,431,228 ?
LOCNESS 294,325 ?
Table 1: Statistics on the data sets for evaluation.
the third one, we manually edited the error case
frames of the first version to remove unnecessary
error case frames and case elements (but no addi-
tion) and to add feedback messages to them. Af-
ter this, active generation was applied to augment
the edited error case frames. In implementing the
proposed methods, we selected as target preposi-
tions the ten most frequent prepositions, the same
as in previous work (Rozovskaya and Roth, 2011):
about, at, by, for, from, in, of , on, to, with.
For comparison, we selected two conventional
methods. One was the best-performing sys-
tem among the benchmark systems, which is the
classifier-based method (Sakaguchi et al, 2012)
which had participated in the HOO 2012 shared
task (Dale et al, 2012). The other was the SMT-
based method (Yoshimoto et al, 2013) which was
the best-performing system in preposition error
correction in the CoNLL 2013 shared task (Ng et
al., 2013). In addition, we evaluated performance
of hybrid methods combining the correction re-
sults of the third version of the proposed method
with those of the classifier-/SMT-based method;
we simply took the union of the two.
Table 2 shows the evaluation results. The sim-
ple error case frame-based method achieves an F -
measure of 0.189. It improves recall when com-
bined with active generation, which shows the
effectiveness of active generation for augment-
ing error case frames. It further improves pre-
cision without decreasing recall by manual edit-
ing; note that manual editing was only applied
to the error case frames generated from the train-
ing data but not to those generated by active gen-
eration. The performance is comparable to both
classifier-/SMT-based methods. The hybrid meth-
ods achieve the best performances in F -measure.
In the usefulness evaluation, the third version of
the proposed method was able to provide 20 feed-
back messages for the target texts. The three hu-
man raters evaluated 80%, 80%, and 85% of the
760
Method R P F
ECF 0.107 0.823 0.189
ECF with AG 0.130 0.680 0.218
ME-ECF with AG 0.130 0.708 0.219
Classifier-based 0.167 0.310 0.217
SMT-based 0.115 0.385 0.176
Classifier hybrid 0.235 0.369 0.287
SMT hybrid 0.191 0.446 0.267
ECF: Error Case Frame, ME-ECF: Manually
Edited Error Case Frame, AG: Active Generation
Table 2: Correction performance in recall (R),
precision (P ), and F -measure (F ).
20 feedback messages as useful (82% on average).
The agreement among the raters was ? = 0.67 in
Fleiss?s ?.
6 Discussion
As the experimental results show, the proposed
method achieves a comparable correction perfor-
mance with the classifier-/SMT-based methods. A
closer look at the correction results reveals the dif-
ferences in correction tendencies between these
methods, which explains well why the hybrid
methods achieve better performance.
One of the tendencies is that the proposed
method performs better on preposition errors
where relatively wider contexts are required
to correct them. Error case frames naturally
exploit wider contexts based on the cases
which are extracted by parsing. In contrast,
classifier-/SMT-based methods rely on narrower
contexts such as a few words surrounding the
preposition in question. Take as an example the
following sentence which appeared in the test
set: *In the univerysity, I studied English in the
morning
8
. To confirm that the preposition In
is erroneous requires the verb studied and the
object English. The proposed method successfully
corrected this error by the error case frame ?[study
Subj:{PERSON} Prep dobj:{english,math,? ? ? }
*Prep in:{university} ? at ]? in the evaluation.
This would be difficult for methods relying on
only a few words surrounding the preposition In.
It is also difficult for classifier-/SMT-based
methods to correct missing preposition errors.
Classifier-based methods need to be informed of
8
The word univerysity is a mis-spelt word of university.
Note that mis-spelt words are automatically corrected by a
spell-checker when case frames are extracted.
the position of the preposition to predict a cor-
rect preposition. Because the position of a miss-
ing preposition is implicit, classifier-based meth-
ods would have to make a prediction at every
single position between words, which would be
inefficient. Because of this, the classifier-based
method used in the evaluation (and often other
classifier-based methods) excludes missing prepo-
sition errors from its target. SMT-based methods
do not perform well either on missing preposition
errors because of the fact that they implicitly, but
not directly, handle missing preposition errors. In
contrast, error case frames directly model miss-
ing prepositions by treating objects as one of the
preposition cases (i.e., Prep dobj).
Grammatical errors other than preposition er-
rors influence both the proposed and classifier-
/SMT-based methods, but differently. Grammat-
ical errors appearing around the preposition in
question seem to influence the previous methods
more significantly than the proposed method be-
cause they rely on words surrounding the prepo-
sition. On the other hand, structural errors such
as errors in voice tend to degrade performance of
the proposed method. For instance, if an error in
voice occurs as in *I excited this, correctly, I was
excited by this, error case frames are not properly
applied.
The precisions of the proposed methods are
high compared to those of the previous methods.
To be precise, the number of false positives is only
seven in the third version of the proposed method.
Out of seven, four false positives are due to prob-
lems with the used error case frames themselves.
Two are the influence of other grammatical errors
(e.g., *I like to look beautiful view. was corrected
as look at beautiful view by the proposed method
but as see beautiful view in the error annotation).
Unlike false positives, it is difficult to precisely
point out causes for false negatives, which often
involve several factors. One cause which is theo-
retically clear is errors in preposition attached to
a noun phrase (NP), which amounts to 11 % of
all false negatives. Since error case frames de-
scribe errors in preposition attached to a verb, they
do not target these types of errors. Extending er-
ror case frames to general frames might overcome
this limitation, which will require further investi-
gation. Similarly, error case frames are not gener-
ated for preposition errors where prepositions are
incorrectly used with words other than a noun as
761
in *make me to happy (5 % of all). Although er-
ror case frames can describe these types of errors,
case frames are not extracted for their correspond-
ing correct usages from the native corpus. This
is because the word in question (e.g., happy) cor-
rectly appears without the erroneous preposition in
the native corpus, and thus it is not recognized as
a preposition case. This means that a correspond-
ing correct case frame is never found for any er-
ror of these types in the generation procedure (6).
Accordingly, error case frames are never gener-
ated for these types of errors. The most influen-
tial cause of false negatives, which is also a major
cause of false negatives in the previous methods,
is other grammatical errors (at least 22 % of all).
One of such errors is errors in voice as already ex-
plained (4%). Another is the omission of the ob-
ject of a verb (4%). In these cases, even if an ap-
propriate error case frame exists, it is not applied
because of the grammatical error.
In addition to correction performance, error
case frames are effective in providing feedback
messages; Fig. 1 (on the first page) shows excerpts
of the feedback messages provided in the evalua-
tion. The evaluation shows that 82% of the pro-
vided feedback messages were actually rated as
useful for language learning on average (the rest
were mostly evaluated as not-useful due to false
positive corrections). With the feedback messages
of error case frames, we now have the follow-
ing three choices as the way of error correction:
(a) just indicating the correct preposition (as in
previous methods); (b) indicating the correction
preposition with a feedback message; (c) display-
ing only a feedback message. In (a), the learner
might just copy the correct preposition to correct
his or her writing, which would result in little or
no learning effect. This suggests that the ultimate
goal of grammatical error correction for language
learning assistance is not to correct all errors in the
given text but to maximize learning effect for the
learner. (b) might give a similar result because the
learner can copy the correct preposition without
reading the feedback message. In (c), the learner
has to actually read and understand the feedback
message to select the correct preposition. Taking
these into consideration, (c) will likely give the
learner better learning effect than the other two.
Therefore, we propose applying the feedback (c)
to language learning assistance. To the best of our
knowledge, it is only the error case frame-based
method that is capable of this manner of error cor-
rection.
7 Conclusions
This paper presented a novel framework called
error case frames for correcting preposition er-
rors with feedback messages. The evaluation
showed that (i) automatically generated error case
frames achieve a performance comparable to con-
ventional methods; (ii) they are intuitively in-
terpretable and manually modifiable to improve
them; (iii) feedback messages provided by error
case frames are effective in language learning as-
sistance. Considering these advantages and the
fact that it has been difficult to provide feedback
messages by automatically generated rules, error
case frames will likely be one of the major ap-
proaches for preposition error correction.
Appendix A. Sense mapping
The following list shows the mapping between
words and senses developed based on the Word-
Net (Miller, 1995) and GSK dictionary of places
and facilities (2nd Ed.)
9
. Each line consists of a
token for a sense, its definition, examples of its
member. DRINK (drink): tea, coffee
FOOD (food): cake, sandwich
MONTH (names of months): January, February
MINST (musical instruments): guitar, piano
PERSON (persons): John, he
PLACE (place names): Canada, Paris
SPORT (sports): football, tennis
SPORTING (sporting activities): swimming
WEEK (the days of the week): Monday
VEHICLE (vehicles): train, bus
Acknowledgments
We would like to thank Daisuke Kawahara for his
advice on case frame generation. We also would
like to thank Keisuke Sakaguchi and Mamoru Ko-
machi for providing the authors with their system
outputs. Finally, we would acknowledge the help
from the members of the ILES group at LIMSI,
Orsay (France) where the first author performed
part of this work. This work was partly supported
by Kaken Grant-in-Aid for Young Scientists (B)
(26750091).
9
GSK dictionary of places and facilities second edi-
tion: http://www.gsk.or.jp/en/catalog/
gsk2012-c/
762
References
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
SMT techniques. In Proc. of 21th International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 249?256, Sydney, Aus-
tralia, July.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proc. of 4th ACL-SIGSEM Work-
shop on Prepositions, pages 25?30.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Proc. 7th
Workshop on Building Educational Applications Us-
ing NLP, pages 54?62.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of 5th International Conference on Language
Resources and Evaluation, pages 449?445.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and de-
terminer error correction in L2 English. In Proc.
of 22nd International Conference on Computational
Linguistics, pages 169?176.
Dana Ferris and Barrie Roberts. 2001. Error feed-
back in L2 writing classes: How explicit does it
need to be? Journal of Second Language Writing,
10(3):161?184.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(2):115?129.
Japan electronic dictionary research institute Ltd.
1993. EDR electronic dictionary specifications
guide. Japan electronic dictionary research institute
ltd.
Jun?ichi Kakegawa, Hisayuki Kanda, Eitaro Fujioka,
Makoto Itami, and Kohji Itoh. 2000. Diagnostic
processing of Japanese for computer-assisted second
language learning. In Proc. of 38th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 537?546.
Daisuke Kawahara and Kiyotaka Uchimoto. 2008. A
method for automatically constructing case frames
for English. In Proc. of 6th International Confer-
ence on Language Resources and Evaluation.
Kathleen F. McCoy, Christopher A. Pennington, and
Linda Z. Suri. 1996. English error correction: A
syntactic user model based on principled ?mal-rule?
scoring. In Proc. of 5th International Conference on
User Modeling, pages 69?66.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41.
Ryo Nagata and Atsuo Kawai. 2011. Exploiting learn-
ers? tendencies for detecting English determiner er-
rors. In Lecture Notes in Computer Science, volume
6882/2011, pages 144?153.
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and
Naoki Isu. 2006. A feedback-augmented method
for detecting errors in the writing of learners of En-
glish. In Proc. of 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 241?
248.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proc. of 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1210?1219.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proc. 17th Conference on Computational Natural
Language Learning: Shared Task, pages 1?12.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New York.
Thomas Robb, Steven Ross, and Ian Shortreed. 1986.
Salience of feedback on error and its effect on EFL
writing quality. TESOL QUARTERY, 20(1):83?93.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proc. of 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 924?
933.
Alla Rozovskaya and Dan Roth. 2013. Joint learning
and inference for grammatical error correction. In
Proc. of Conference on Empirical Methods in Natu-
ral Language Processing, pages 791?802.
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo,
Lis Kanashiro, Tomoya Mizumoto, Mamoru Ko-
machi, and Yuji Matsumoto. 2012. NAIST at the
HOO 2012 shared task. In Proc. of 7th Workshop on
the Innovative Use of NLP for Building Educational
Applications, pages 281?288.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction
for ESL learners using global context. In Proc. of
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
198?202.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proc. of 48nd Annual Meet-
ing of the Association for Computational Linguistics
Short Papers, pages 353?358.
763
Yuanbin Wu and Hwee Tou Ng. 2013. Grammatical
error correction using integer linear programming.
In Proc. of 51st Annual Meeting of the Association
for Computational Linguistics, pages 1456?1465.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL grammatical error
correction shared task. In Proc. of 17th Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 26?33.
764
