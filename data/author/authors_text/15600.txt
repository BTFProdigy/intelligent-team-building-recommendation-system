Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 539?549,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Perplexity Minimization for Translation Model Domain Adaptation in
Statistical Machine Translation
Rico Sennrich
Institute of Computational Linguistics
University of Zurich
Binzm?hlestr. 14
CH-8050 Z?rich
sennrich@cl.uzh.ch
Abstract
We investigate the problem of domain
adaptation for parallel data in Statistical
Machine Translation (SMT). While tech-
niques for domain adaptation of monolin-
gual data can be borrowed for parallel data,
we explore conceptual differences between
translation model and language model do-
main adaptation and their effect on per-
formance, such as the fact that translation
models typically consist of several features
that have different characteristics and can
be optimized separately. We also explore
adapting multiple (4?10) data sets with no
a priori distinction between in-domain and
out-of-domain data except for an in-domain
development set.
1 Introduction
The increasing availability of parallel corpora
from various sources, welcome as it may be,
leads to new challenges when building a statis-
tical machine translation system for a specific
domain. The task of determining which par-
allel texts should be included for training, and
which ones hurt translation performance, is te-
dious when performed through trial-and-error.
Alternatively, methods for a weighted combina-
tion exist, but there is conflicting evidence as to
which approach works best, and the issue of de-
termining weights is not adequately resolved.
The picture looks better in language mod-
elling, where model interpolation through per-
plexity minimization has become a widespread
method of domain adaptation. We investigate the
applicability of this method for translation mod-
els, and discuss possible applications.
We move the focus away from a binary com-
bination of in-domain and out-of-domain data. If
we can scale up the number of models whose con-
tributions we weight, this reduces the need for a
priori knowledge about the fitness1 of each poten-
tial training text, and opens new research oppor-
tunities, for instance experiments with clustered
training data.
2 Domain Adaptation for Translation
Models
To motivate efforts in domain adaptation, let us
review why additional training data can improve,
but also decrease translation quality.
Adding more training data to a translation sys-
tem is easy to motivate through the data sparse-
ness problem. Koehn and Knight (2001) show
that translation quality correlates strongly with
how often a word occurs in the training corpus.
Rare words or phrases pose a problem in sev-
eral stages of MT modelling, from word align-
ment to the computation of translation probabil-
ities through Maximum Likelihood Estimation.
Unknown words are typically copied verbatim to
the target text, which may be a good strategy for
named entities, but is often wrong otherwise. In
general, more data allows for a better word align-
ment, a better estimation of translation probabili-
ties, and for the consideration of more context (in
phrase-based or syntactic SMT).
A second effect of additional data is not nec-
essarily positive. Translations are inherently am-
biguous, and a strong source of ambiguity is the
1We borrow this term from early evolutionary biology to
emphasize that the question in domain adaptation is not how
?good? or ?bad? the data is, but how well-adapted it is to the
task at hand.
539
domain of a text. The German word ?Wort? (engl.
word) is typically translated as floor in Europarl,
a corpus of Parliamentary Proceedings (Koehn,
2005), owing to the high frequency of phrases
such as you have the floor, which is translated into
German as Sie haben das Wort. This translation
is highly idiomatic and unlikely to occur in other
contexts. Still, adding Europarl as out-of-domain
training data shifts the probability distribution of
p(t|?Wort?) in favour of p(?floor?|?Wort?), and
may thus lead to improper translations.
We will refer to the two problems as the data
sparseness problem and the ambiguity problem.
Adding out-of-domain data typically mitigates the
data sparseness problem, but exacerbates the am-
biguity problem. The net gain (or loss) of adding
more data changes from case to case. Because
there are (to our knowledge) no tools that predict
this net effect, it is a matter of empirical investi-
gation (or, in less suave terms, trial-and-error), to
determine which corpora to use.2
From this understanding of the reasons for and
against out-of-domain data, we formulate the fol-
lowing hypotheses:
1. A weighted combination can control the con-
tribution of the out-of-domain corpus on the
probability distribution, and thus limit the
ambiguity problem.
2. A weighted combination eliminates the need
for data selection, offering a robust baseline
for domain-specific machine translation.
We will discuss three mixture modelling tech-
niques for translation models. Our aim is to adapt
all four features of the standard Moses SMT trans-
lation model: the phrase translation probabilities
p(t|s) and p(s|t), and the lexical weights lex(t|s)
and lex(s|t).3
2.1 Linear Interpolation
A well-established approach in language mod-
elling is the linear interpolation of several mod-
els, i.e. computing the weighted average of the in-
2A frustrating side-effect is that these findings rarely gen-
eralize. For instance, we were unable to reproduce the find-
ing by Ceaus?u et al(2011) that patent translation systems
are highly domain-sensitive and suffer from the inclusion of
parallel training data from other patent subdomains.
3We can ignore the fifth feature, the phrase penalty,
which is a constant.
dividual model probabilities. It is defined as fol-
lows:
p(x|y;?) =
n?
i=1
?ipi(x|y) (1)
with ?i being the interpolation weight of each
model i, and with (
?
i ?i) = 1.
For SMT, linear interpolation of translation
models has been used in numerous systems. The
approaches diverge in how they set the inter-
polation weights. Some authors use uniform
weights (Cohn and Lapata, 2007), others em-
pirically test different interpolation coefficients
(Finch and Sumita, 2008; Yasuda et al 2008;
Nakov and Ng, 2009; Axelrod et al 2011), others
apply monolingual metrics to set the weights for
TM interpolation (Foster and Kuhn, 2007; Koehn
et al 2010).
There are reasons against all these approaches.
Uniform weights are easy to implement, but give
little control. Empirically, it has been shown that
they often do not perform optimally (Finch and
Sumita, 2008; Yasuda et al 2008). An opti-
mization of BLEU scores on a development set is
promising, but slow and impractical. There is no
easy way to integrate linear interpolation into log-
linear SMT frameworks and perform optimization
through MERT. Monolingual optimization objec-
tives such as language model perplexity have the
advantage of being well-known and readily avail-
able, but their relation to the ambiguity problem
is indirect at best.
Linear interpolation is seemingly well-defined
in equation 1. Still, there are a few implemen-
tation details worth pointing out. If we directly
interpolate each feature in the translation model,
and define the feature values of non-occurring
phrase pairs as 0, this disregards the meaning of
each feature. If we estimate p(x|y) via MLE as in
equation 2, and c(y) = 0, then p(x|y) is strictly
speaking undefined. Alternatively to a naive al-
gorithm, which treats unknown phrase pairs as
having a probability of 0, which results in a defi-
cient probability distribution, we propose and im-
plement the following algorithm. For each value
pair (x, y) for which we compute p(x|y), we re-
place ?i with 0 for all models i with p(y) =
0, then renormalize the weight vector ? to 1.
We do this for p(t|s) and lex(t|s), but not for
p(s|t) and lex(s|t), the reasoning being the con-
540
sequences for perplexity minimization (see sec-
tion 2.4). Namely, we do not want to penalize
a small in-domain model for having a high out-
of-vocabulary rate on the source side, but we do
want to penalize models that know the source
phrase, but not its correct translation. A sec-
ond modification pertains to the lexical weights
lex(s|t) and lex(t|s), which form no true proba-
bility distribution, but are derived from the indi-
vidual word translation probabilities of a phrase
pair (see (Koehn et al 2003)). We propose to
not interpolate the features directly, but the word
translation probabilities which are the basis of the
lexical weight computation. The reason for this is
that word pairs are less sparse than phrase pairs,
so that we can even compute lexical weights for
phrase pairs which are unknown in a model.4
2.2 Weighted Counts
Weighting of different corpora can also be imple-
mented through a modified Maximum Likelihood
Estimation. The traditional equation for MLE is:
p(x|y) =
c(x, y)
c(y)
=
c(x, y)
?
x? c(x
?, y)
(2)
where c denotes the count of an observation, and
p the model probability. If we generalize the for-
mula to compute a probability from n corpora,
and assign a weight ?i to each, we get5:
p(x|y;?) =
?n
i=1 ?ici(x, y)?n
i=1
?
x? ?ici(x
?, y)
(3)
The main difference to linear interpolation is
that this equation takes into account how well-
evidenced a phrase pair is. This includes the dis-
tinction between lack of evidence and negative ev-
idence, which is missing in a naive implementa-
tion of linear interpolation.
Translation models trained with weighted
counts have been discussed before, and have
been shown to outperform uniform ones in some
settings. However, researchers who demon-
strated this fact did so with arbitrary weights (e.g.
(Koehn, 2002)), or by empirically testing differ-
ent weights (e.g. (Nakov and Ng, 2009)). We do
not know of any research on automatically deter-
mining weights for this method, or which is not
limited to two corpora.
4For instance if the word pairs (the,der) and (man,Mann)
are known, but the phrase pair (the man, der Mann) is not.
5Unlike equation 1, equation 3 does not require that
(
?
i ?i) = 1.
2.3 Alternative Paths
A third method is using multiple translation mod-
els as alternative decoding paths (Birch et al
2007), an idea which Koehn and Schroeder (2007)
first used for domain adaptation. This approach
has the attractive theoretical property that adding
new models is guaranteed to lead to equal or bet-
ter performance, given the right weights. At best,
a model is beneficial with appropriate weights. At
worst, we can set the feature weights so that the
decoding paths of one model are never picked for
the final translation. In practice, each translation
model adds 5 features and thus 5 more dimensions
to the weight space, which leads to longer search,
search errors, and/or overfitting. The expectation
is that, at least with MERT, using alternative de-
coding paths does not scale well to a high number
of models.
A suboptimal choice of weights is not the only
weakness of alternative paths, however. Let us
assume that all models have the same weights.
Note that, if a phrase pair occurs in several mod-
els, combining models through alternative paths
means that the decoder selects the path with the
highest probability, whereas with linear interpo-
lation, the probability of the phrase pair would
be the (weighted) average of all models. Select-
ing the highest-scoring phrase pair favours statis-
tical outliers and hence is the less robust decision,
prone to data noise and data sparseness.
2.4 Perplexity Minimization
In language modelling, perplexity is frequently
used as a quality measure for language models
(Chen and Goodman, 1998). Among other appli-
cations, language model perplexity has been used
for domain adaptation (Foster and Kuhn, 2007).
For translation models, perplexity is most closely
associated with EM word alignment (Brown et
al., 1993) and has been used to evaluate different
alignment algorithms (Al-Onaizan et al 1999).
We investigate translation model perplexity
minimization as a method to set model weights
in mixture modelling. For the purpose of opti-
mization, the cross-entropy H(p), the perplexity
2H(p), and other derived measures are equivalent.
The cross-entropy H(p) is defined as:6
6See (Chen and Goodman, 1998) for a short discussion
of the equation. In short, a lower cross-entropy indicates that
the model is better able to predict the development set.
541
H(p) = ?
?
x,y
p?(x, y) log2 p(x|y) (4)
The phrase pairs (x, y) whose probability we
measure, and their empirical probability p? need
to be extracted from a development set, whereas
p is the model probability. To obtain the phrase
pairs, we process the development set with the
same word alignment and phrase extraction tools
that we use for training, i.e. GIZA++ and heuris-
tics for phrase extraction (Och and Ney, 2003).
The objective function is the minimization of the
cross-entropy, with the weight vector ? as argu-
ment:
?? = argmin
?
?
?
x,y
p?(x, y) log2 p(x|y;?) (5)
We can fill in equations 1 or 3 for p(x|y;?). The
optimization itself is convex and can be done with
off-the-shelf software.7 We use L-BFGS with
numerically approximated gradients (Byrd et al
1995).
Perplexity minimization has the advantage that
it is well-defined for both weighted counts and lin-
ear interpolation, and can be quickly computed.
Other than in language modelling, where p(x|y)
is the probability of a word given a n-gram his-
tory, conditional probabilities in translation mod-
els express the probability of a target phrase given
a source phrase (or vice versa), which connects
the perplexity to the ambiguity problem. The
higher the probability of ?correct? phrase pairs,
the lower the perplexity, and the more likely
the model is to successfully resolve the ambigu-
ity. The question is in how far perplexity min-
imization coincides with empirically good mix-
ture weights.8 This depends, among others, on
the other model components in the SMT frame-
work, for instance the language model. We will
not evaluate perplexity minimization against em-
pirically optimized mixture weights, but apply it
in situations where the latter is infeasible, e.g. be-
cause of the number of models.
7A quick demonstration of convexity: equation 1 is
affine; equation 3 linear-fractional. Both are convex in the
domain R>0. Consequently, equation 4 is also convex be-
cause it is the weighted sum of convex functions.
8There are tasks for which perplexity is known to be un-
reliable, e.g. for comparing models with different vocabular-
ies. However, such confounding factors do not affect the op-
timization algorithm, which works with a fixed set of phrase
pairs, and merely varies ?.
Our main technical contributions are as fol-
lows: Additionally to perplexity optimization for
linear interpolation, which was first applied by
Foster et al(2010), we propose perplexity opti-
mization for weighted counts (equation 3), and a
modified implementation of linear interpolation.
Also, we independently perform perplexity mini-
mization for all four features of the standard SMT
translation model: the phrase translation proba-
bilities p(t|s) and p(s|t), and the lexical weights
lex(t|s) and lex(s|t).
3 Other Domain Adaptation Techniques
So far, we discussed mixture modelling for trans-
lation models, which is only a subset of domain
adaptation techniques in SMT.
Mixture-modelling for language models is well
established (Foster and Kuhn, 2007). Language
model adaptation serves the same purpose as
translation model adaptation, i.e. skewing the
probability distribution in favour of in-domain
translations. This means that LM adaptation may
have similar effects as TM adaptation, and that
the two are to some extent redundant. Foster and
Kuhn (2007) find that ?both TM and LM adap-
tation are effective?, but that ?combined LM and
TM adaptation is not better than LM adaptation
on its own?.
A second strand of research in domain adap-
tation is data selection, i.e. choosing a subset of
the training data that is considered more relevant
for the task at hand. This has been done for lan-
guage models using techniques from information
retrieval (Zhao et al 2004), or perplexity (Lin et
al., 1997; Moore and Lewis, 2010). Data selec-
tion has also been proposed for translation mod-
els (Axelrod et al 2011). Note that for transla-
tion models, data selection offers an unattractive
trade-off between the data sparseness and the am-
biguity problem, and that the optimal amount of
data to select is hard to determine.
Our discussion of mixture-modelling is rela-
tively coarse-grained, with 2-10 models being
combined. Matsoukas et al(2009) propose an ap-
proach where each sentence is weighted accord-
ing to a classifier, and Foster et al(2010) ex-
tend this approach by weighting individual phrase
pairs. These more fine-grained methods need not
be seen as alternatives to coarse-grained ones.
Foster et al(2010) combine the two, apply-
ing linear interpolation to combine the instance-
542
weighted out-of-domain model with an in-domain
model.
4 Evaluation
Apart from measuring the performance of the ap-
proaches introduced in section 2, we want to in-
vestigate the following open research questions.
1. Does an implementation of linear interpola-
tion that is more closely tailored to trans-
lation modelling outperform a naive imple-
mentation?
2. How do the approaches perform outside a
binary setting, i.e. when we do not work
with one in-domain and one out-of-domain
model, but with a higher number of models?
3. Can we apply perplexity minimization to
other translation model features such as the
lexical weights, and if yes, does a separate
optimization of each translation model fea-
ture improve performance?
4.1 Data and Methods
In terms of tools and techniques used, we mostly
adhere to the work flow described for the WMT
2011 baseline system9. The main tools are Moses
(Koehn et al 2007), SRILM (Stolcke, 2002), and
GIZA++ (Och and Ney, 2003), with settings as
described in the WMT 2011 guide. We report
two translation measures: BLEU (Papineni et al
2002) and METEOR 1.3 (Denkowski and Lavie,
2011). All results are lowercased and tokenized,
measured with five independent runs of MERT
(Och and Ney, 2003) and MultEval (Clark et al
2011) for resampling and significance testing.
We compare three baselines and four transla-
tion model mixture techniques. The three base-
lines are a purely in-domain model, a purely out-
of-domain model, and a model trained on the con-
catenation of the two, which corresponds to equa-
tion 3 with uniform weights. Additionally, we
evaluate perplexity optimization with weighted
counts and the two implementations of linear in-
terpolation contrasted in section 2.1. The two lin-
ear interpolations that are contrasted are a naive
one, i.e. a direct, unnormalized interpolation of
9http://www.statmt.org/wmt11/baseline.
html
Data set sentences words (fr)
Alpine (in-domain) 220k 4 700k
Europarl 1 500k 44 000k
JRC Acquis 1 100k 24 000k
OpenSubtitles v2 2 300k 18 000k
Total train 5 200k 91 000k
Dev 1424 33 000
Test 991 21 000
Table 1: Parallel data sets for German ? French trans-
lation task.
Data set sentences words
Alpine (in-domain) 650k 13 000k
News-commentary 150k 4 000k
Europarl 2 000k 60 000k
News 25 000k 610 000k
Total 28 000k 690 000k
Table 2: Monolingual French data sets for German ?
French translation task.
all translation model features, and a modified one
that normalizes ? for each phrase pair (s, t) for
p(t|s) and recomputes the lexical weights based
on interpolated word translation probabilites. The
fourth weighted combination is using alternative
decoding paths with weights set through MERT.
The four weighted combinations are evaluated
twice: once applied to the original four or ten par-
allel data sets, once in a binary setting in which
all out-of-domain data sets are first concatenated.
Since we want to concentrate on translation
model domain adaptation, we keep other model
components, namely word alignment and the lex-
ical reordering model, constant throughout the ex-
periments. We contrast two language models. An
unadapted, out-of-domain language model trained
on data sets provided for the WMT 2011 transla-
tion task, and an adapted language model which is
the linear interpolation of all data sets, optimized
for minimal perplexity on the in-domain develop-
ment set.
While unadapted language models are becom-
ing more rare in domain adaptation research, they
allow us to contrast different TM mixtures with-
out the effect on performance being (partially)
hidden by language model adaptation with the
same effect.
The first data set is a DE?FR translation sce-
nario in the domain of mountaineering. The in-
domain corpus is a collection of Alpine Club pub-
543
lications (Volk et al 2010). As parallel out-of-
domain dataset, we use Europarl, a collection of
parliamentary proceedings (Koehn, 2005), JRC-
Acquis, a collection of legislative texts (Stein-
berger et al 2006), and OpenSubtitles v2, a par-
allel corpus extracted from film subtitles10 (Tiede-
mann, 2009). For language modelling, we use in-
domain data and data from the 2011 Workshop
on Statistical Machine Translation. The respec-
tive sizes of the data sets are listed in tables 1 and
2.
As the second data set, we use the Haitian Cre-
ole ? English data from the WMT 2011 featured
translation task. It consists of emergency SMS
sent in the wake of the 2010 Haiti earthquake.
Originally, Microsoft Research and CMU oper-
ated under severe time constraints to build a trans-
lation system for this language pair. This limits
the ability to empirically verify how much each
data set contributes to translation quality, and in-
creases the importance of automated and quick
domain adaptation methods.
Note that both data sets have a relatively high
ratio of in-domain to out-of-domain parallel train-
ing data (1:20 for DE?EN and 1:5 for HT?EN)
Previous research has been performed with ratios
of 1:100 (Foster et al 2010) or 1:400 (Axelrod
et al 2011). Since domain adaptation becomes
more important when the ratio of IN to OUT is
low, and since such low ratios are also realistic11,
we also include results for which the amount of
in-domain parallel data has been restricted to 10%
of the available data set.
We used the same development set for lan-
guage/translation model adaptation and setting
the global model weights with MERT. While it
is theoretically possible that MERT will give too
high weights to models that are optimized on the
same development set, we found no empirical evi-
dence for this in experiments with separate devel-
opment sets.
4.2 Results
The results are shown in tables 5 and 6. In the
DE?FR translation task, results vary between 13.5
and 18.9 BLEU points; in the HT?EN task, be-
tween 24.3 and 33.8. Unsurprisingly, an adapted
10http://www.opensubtitles.org
11We predict that the availability of parallel data will
steadily increase, most data being out-of-domain for any
given task.
Data set units words (en)
SMS (in-domain) 16 500 380 000
Medical 1 600 10 000
Newswire 13 500 330 000
Glossary 35 700 90 000
Wikipedia 8 500 110 000
Wikipedia NE 10 500 34 000
Bible 30 000 920 000
Haitisurf dict 3 700 4000
Krengle dict 1 600 2 600
Krengle 650 4 200
Total train 120 000 1 900 000
Dev 900 22 000
Test 1274 25 000
Table 3: Parallel data sets for Haiti Creole ? English
translation task.
Data set sentences words
SMS (in-domain) 16k 380k
News 113 000k 2 650 000k
Table 4: Monolingual English data sets for Haiti Cre-
ole ? English translation task.
LM performs better than an out-of-domain one,
and using all available in-domain parallel data is
better than using only part of it. The same is not
true for out-of-domain data, which highlights the
problem discussed in the introduction. For the
DE?FR task, adding 86 million words of out-of-
domain parallel data to the 5 million in-domain
data set does not lead to consistent performance
gains. We observe a decrease of 0.3 BLEU points
with an out-of-domain LM, and an increase of 0.4
BLEU points with an adapted LM. The out-of-
domain training data has a larger positive effect
if less in-domain data is available, with a gain of
1.4 BLEU points. The results in the HT?EN trans-
lation task (table 6) paint a similar picture. An
interesting side note is that even tiny amounts of
in-domain parallel data can have strong effects on
performance. A training set of 1600 emergency
SMS (38 000 tokens) yields a comparable perfor-
mance to an out-of-domain data set of 1.5 million
tokens.
As to the domain adaptation experiments,
weights optimized through perplexity minimiza-
tion are significantly better in the majority of
cases, and never significantly worse, than uniform
544
System
out-of-domain LM adapted LM
full IN TM full IN TM small IN TM
BLEU METEOR BLEU METEOR BLEU METEOR
in-domain 16.8 35.9 17.9 37.0 15.7 33.5
out-of-domain 13.5 31.3 14.8 32.3 14.8 32.3
counts (concatenation) 16.5 35.7 18.3 37.3 17.1 35.4
binary in/out
weighted counts 17.4 36.6 18.7 37.9 17.6 36.2
linear interpolation (naive) 17.4 36.7 18.8 37.9 17.6 36.1
linear interpolation (modified) 17.2 36.5 18.9 38.0 17.6 36.2
alternative paths 17.2 36.5 18.6 37.8 17.4 36.0
4 models
weighted counts 17.3 36.6 18.8 37.8 17.4 36.0
linear interpolation (naive) 17.1 36.5 18.5 37.7 17.3 35.9
linear interpolation (modified) 17.2 36.5 18.7 37.9 17.3 36.0
alternative paths 17.0 36.2 18.3 37.4 16.3 35.1
Table 5: Domain adaptation results DE?FR. Domain: Alpine texts. Full IN TM: Using the full in-domain parallel
corpus; small IN TM: using 10% of available in-domain parallel data.
weights.12 However, the difference is smaller for
the experiments with an adapted language model
than for those with an out-of-domain one, which
confirms that the benefit of language model adap-
tation and translation model adaptation are not
fully cumulative. Performance-wise, there seems
to be no clear winner between weighted counts
and the two alternative implementations of lin-
ear interpolation. We can still argue for weighted
counts on theoretical grounds. A weighted MLE
(equation 3) returns a true probability distribution,
whereas a naive implementation of linear interpo-
lation results in a deficient model. Consequently,
probabilities are typically lower in the naively in-
terpolated model, which results in higher (worse)
perplexities. While the deficiency did not affect
MERT or decoding negatively, it might become
problematic in other applications, for instance if
we want to use an interpolated model as a compo-
nent in a second perplexity-based combination of
models.13
When moving from a binary setting with
one in-domain and one out-of-domain transla-
tion model (trained on all available out-of-domain
data) to 4?10 translation models, we observe a
serious performance degradation for alternative
paths, while performance of the perplexity opti-
12This also applies to linear interpolation with uniform
weights, which is not shown in the tables.
13Specifically, a deficient model would be dispreferred by
the perplexity minimization algorithm.
mization methods does not change significantly.
This is positive for perplexity optimization be-
cause it demonstrates that it requires less a priori
information, and opens up new research possibil-
ities, i.e. experiments with different clusterings of
parallel data. The performance degradation for
alternative paths is partially due to optimization
problems in MERT, but also due to a higher sus-
ceptibility to statistical outliers, as discussed in
section 2.3.14
A pessimistic interpretation of the results
would point out that performance gains compared
to the best baseline system are modest or even
inexistent in some settings. However, we want
to stress two important points. First, we often
do not know a priori whether adding an out-of-
domain data set boosts or weakens translation per-
formance. An automatic weighting of data sets re-
duces the need for trial-and-error experimentation
and is worthwhile even if a performance increase
is not guaranteed. Second, the potential impact
of a weighted combination depends on the trans-
lation scenario and the available data sets. Gen-
erally, we expect non-uniform weighting to have
a bigger impact when the models that are com-
bined are more dissimilar (in terms of fitness for
the task), and if the ratio of in-domain to out-of-
domain data is low. Conversely, there are situa-
14We empirically verified this weakness in a synthetic ex-
periment with a randomly split training corpus and identical
weights for each path.
545
System
out-of-domain LM adapted LM
full IN TM full IN TM small IN TM
BLEU METEOR BLEU METEOR BLEU METEOR
in-domain 30.4 30.7 33.4 31.7 29.7 28.6
out-of-domain 24.3 28.0 28.9 30.2 28.9 30.2
counts (concatenation) 30.3 31.2 33.6 32.4 31.3 31.3
binary in/out
weighted counts 31.0 31.6 33.8 32.4 31.5 31.3
linear interpolation (naive) 30.8 31.4 33.7 32.4 31.9 31.3
linear interpolation (modified) 30.8 31.5 33.7 32.4 31.7 31.2
alternative paths 30.8 31.3 33.2 32.4 29.8 30.7
10 models
weighted counts 31.0 31.5 33.5 32.3 31.8 31.5
linear interpolation (naive) 30.9 31.4 33.8 32.4 31.9 31.3
linear interpolation (modified) 31.0 31.6 33.8 32.5 32.1 31.5
alternative paths 25.9 29.2 24.3 29.1 29.8 30.9
Table 6: Domain adaptation results HT?EN. Domain: emergency SMS. Full IN TM: Using the full in-domain
parallel corpus; small IN TM: using 10% of available in-domain parallel data.
tions where we actually expect a simple concate-
nation to be optimal, e.g. when the data sets have
very similar probability distributions.
4.2.1 Individually Optimizing Each TM
Feature
It is hard to empirically show how translation
model perplexity optimization compares to using
monolingual perplexity measures for the purpose
of weighting translation models, as e.g. done by
(Foster and Kuhn, 2007; Koehn et al 2010). One
problem is that there are many different possible
configurations for the latter. We can use source
side or target side language models, operate with
different vocabularies, smoothing techniques, and
n-gram orders.
One of the theoretical considerations that
favour measuring perplexity on the translation
model rather than using monolingual measures
is that we can optimize each translation model
feature separately. In the default Moses transla-
tion model, the four features are p(s|t), lex(s|t),
p(t|s) and lex(t|s).
We empirically test different optimization
schemes as follows. We optimize perplexity on
each feature independently, obtaining 4 weight
vectors. We then compute one model with one
weight vector per feature (namely the feature that
the vector was optimized on), and four models
that use one of the weight vectors for all features.
A further model uses a weight vector that is the
weights
perplexity
BLEU
1 2 3 4
weighted counts
uniform 5.12 7.68 4.84 13.67 30.3
separate 4.68 6.62 4.24 8.57 31.0
1 4.68 6.84 4.50 10.86 30.3
2 4.78 6.62 4.48 10.54 30.3
3 4.86 7.31 4.24 9.15 30.8
4 5.33 7.87 4.52 8.57 30.9
average 4.72 6.71 4.38 9.95 30.4
linear interpolation (modified)
uniform 19.89 82.78 4.80 10.78 30.6
separate 5.45 8.56 4.28 8.85 31.0
1 5.45 8.79 4.40 8.89 30.8
2 5.71 8.56 4.54 8.91 30.9
3 6.46 11.88 4.28 9.07 31.0
4 6.12 10.86 4.47 8.85 30.9
average 5.73 9.72 4.34 8.89 30.9
LM 6.01 9.83 4.56 8.96 30.8
Table 7: Contrast between a separate optimization of
each feature and applying the weight vector optimized
on one feature to the whole model. HT?EN with out-
of-domain LM.
546
average of the other four. For linear interpolation,
we also include a model whose weights have been
optimized through language model perplexity op-
timization, with a 3-gram language model (modi-
fied Knesey-Ney smoothing) trained on the target
side of each parallel data set.
Table 7 shows the results. In terms of BLEU
score, a separate optimization of each feature is a
winner in our experiment in that no other scheme
is better, with 8 of the 11 alternative weighting
schemes (excluding uniform weights) being sig-
nificantly worse than a separate optimization. The
differences in BLEU score are small, however,
since the alternative weighting schemes are gen-
erally felicitious in that they yield both a lower
perplexity and better BLEU scores than uniform
weighting. While our general expectation is that
lower perplexities correlate with higher transla-
tion performance, this relation is complicated by
several facts. Since the interpolated models are
deficient (i.e. their probabilities do not sum to 1),
perplexities for weighted counts and our imple-
mentation of linear interpolation cannot be com-
pard. Also, note that not all features are equally
important for decoding. Their weights in the log-
linear model are set through MERT and vary be-
tween optimization runs.
5 Conclusion
This paper contributes to SMT domain adaptation
research in several ways. We expand on work
by (Foster et al 2010) in establishing transla-
tion model perplexity minimization as a robust
baseline for a weighted combination of translation
models.15 We demonstrate perplexity optimiza-
tion for weighted counts, which are a natural ex-
tension of unadapted MLE training, but are of lit-
tle prominence in domain adaptation research. We
also show that we can separately optimize the four
variable features in the Moses translation model
through perplexity optimization.
We break with prior domain adaptation re-
search in that we do not rely on a binary clustering
of in-domain and out-of-domain training data. We
demonstrate that perplexity minimization scales
well to a higher number of translation models.
This is not only useful for domain adaptation, but
for various tasks that profit from mixture mod-
15The source code is available in the Moses repository
http://github.com/moses-smt/mosesdecoder
elling. We envision that a weighted combination
could be useful to deal with noisy datasets, or ap-
plied after a clustering of training data.
Acknowledgements
This research was funded by the Swiss National
Science Foundation under grant 105215_126999.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
Technical report, Final Report, JHU Summer Work-
shop.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the EMNLP 2011
Workshop on Statistical Machine Translation.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical ma-
chine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
9?16, Prague, Czech Republic, June. Association
for Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263?311.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A limited memory algorithm
for bound constrained optimization. SIAM J. Sci.
Comput., 16:1190?1208, September.
Alexandru Ceaus?u, John Tinsley, Jian Zhang, and
Andy Way. 2011. Experiments on domain adap-
tation for patent machine translation in the PLuTO
project. In Proceedings of the 15th conference of
the European Association for Machine Translation,
Leuven, Belgium.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Computer Speech & Language, 13:359?
393.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176?181, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proceedings of the
547
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 728?735, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In Proceedings of the Third Workshop on
Statistical Machine Translation, StatMT ?08, pages
208?215, Stroudsburg, PA, USA. Association for
Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 128?135, Stroudsburg, PA,
USA. Association for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 451?459,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. In Lil-
lian Lee and Donna Harman, editors, Proceedings
of the 2001 Conference on Empirical Methods in
Natural Language Processing, pages 27?35.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 224?227, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In ACL 2007, Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation
for statistical machine translation. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 115?120, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Philipp Koehn. 2002. Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien,
Keh-Jiann Chen, and Lin-Shan Lee. 1997. Chinese
language model adaptation based on document clas-
sification and multiple domain-specific language
models. In George Kokkinakis, Nikos Fakotakis,
and Evangelos Dermatas, editors, EUROSPEECH.
ISCA.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight esti-
mation for machine translation. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2 - Volume 2,
pages 708?717, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor
languages using related resource-rich languages. In
Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing: Vol-
ume 3 - Volume 3, EMNLP ?09, pages 1358?1367,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC?2006).
548
A. Stolcke. 2002. SRILM ? An Extensible Language
Modeling Toolkit. In Seventh International Confer-
ence on Spoken Language Processing, pages 901?
904, Denver, CO, USA.
J?rg Tiedemann. 2009. News from opus - a col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya
Bangerter, Lenz Furrer, and Beni Ruef. 2010. Chal-
lenges in building a multilingual alpine heritage
corpus. In Proceedings of the Seventh conference
on International Language Resources and Evalu-
ation (LREC?10), Valletta, Malta. European Lan-
guage Resources Association (ELRA).
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP).
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
549
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 832?840,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Multi-Domain Translation Model Framework
for Statistical Machine Translation
Rico Sennrich
Institute of Computational Linguistics
University of Zurich
Binzmu?hlestr. 14
CH-8050 Zu?rich
sennrich@cl.uzh.ch
Holger Schwenk and Walid Aransa
LIUM, University of Le Mans
72085 Le Mans cedex 9, France
lastname@lium.univ-lemans.fr
Abstract
While domain adaptation techniques for
SMT have proven to be effective at im-
proving translation quality, their practical-
ity for a multi-domain environment is of-
ten limited because of the computational
and human costs of developing and main-
taining multiple systems adapted to differ-
ent domains. We present an architecture
that delays the computation of translation
model features until decoding, allowing
for the application of mixture-modeling
techniques at decoding time. We also de-
scribe a method for unsupervised adapta-
tion with development and test data from
multiple domains. Experimental results on
two language pairs demonstrate the effec-
tiveness of both our translation model ar-
chitecture and automatic clustering, with
gains of up to 1 BLEU over unadapted sys-
tems and single-domain adaptation.
1 Introduction
The effectiveness of domain adaptation ap-
proaches such as mixture-modeling (Foster and
Kuhn, 2007) has been established, and has led to
research on a wide array of adaptation techniques
in SMT, for instance (Matsoukas et al, 2009; Shah
et al, 2012). In all these approaches, adaptation is
performed during model training, with respect to a
representative development corpus, and the mod-
els are kept unchanged when then system is de-
ployed. Therefore, when working with multiple
and/or unlabelled domains, domain adaptation is
often impractical for a number of reasons. Firstly,
maintaining multiple systems for each language
pair, each adapted to a different domain, is costly
in terms of computational and human resources:
the full system development pipeline needs to be
performed for all identified domains, all the mod-
els are separately stored and need to be switched at
runtime. This is impractical in many real applica-
tions, in particular a web translation service which
is faced with texts coming from many different do-
mains. Secondly, domain adaptation bears a risk
of performance loss. If there is a mismatch be-
tween the domain of the development set and the
test set, domain adaptation can potentially harm
performance compared to an unadapted baseline.
We introduce a translation model architecture
that delays the computation of features to the de-
coding phase. The calculation is based on a vec-
tor of component models, with each component
providing the sufficient statistics necessary for the
computation of the features. With this framework,
adaptation to a new domain simply consists of up-
dating a weight vector, and multiple domains can
be supported by the same system.
We also present a clustering approach for un-
supervised adaptation in a multi-domain environ-
ment. In the development phase, a set of develop-
ment data is clustered, and the models are adapted
to each cluster. For each sentence that is being
decoded, we choose the weight vector that is op-
timized on the closest cluster, allowing for adap-
tation even with unlabelled and heterogeneous test
data.
2 Related Work
(Ortiz-Mart??nez et al, 2010) delay the compu-
tation of translation model features for the pur-
pose of interactive machine translation with online
training. The main difference to our approach is
that we store sufficient statistics not for a single
model, but a vector of models, which allows us to
832
weight the contribution of each component model
to the feature calculation. The similarity suggests
that our framework could also be used for inter-
active learning, with the ability to learn a model
incrementally from user feedback, and weight it
differently than the static models, opening new re-
search opportunities.
(Sennrich, 2012b) perform instance weighting
of translation models, based on the sufficient
statistics. Our framework implements this idea,
with the main difference that the actual combina-
tion is delayed until decoding, to support adapta-
tion to multiple domains in a single system.
(Razmara et al, 2012) describe an ensemble de-
coding framework which combines several trans-
lation models in the decoding step. Our work is
similar to theirs in that the combination is done
at runtime, but we also delay the computation of
translation model probabilities, and thus have ac-
cess to richer sufficient statistics. In principle,
our architecture can support all mixture operations
that (Razmara et al, 2012) describe, plus addi-
tional ones such as forms of instance weighting,
which are not possible after the translation proba-
bilities have been computed.
(Banerjee et al, 2010) focus on the problem of
domain identification in a multi-domain setting.
They use separate translation systems for each do-
main, and a supervised setting, whereas we aim
for a system that integrates support for multiple
domains, with or without supervision.
(Yamamoto and Sumita, 2007) propose unsu-
pervised clustering at both training and decoding
time. The training text is divided into a number
of clusters, a model is trained on each, and during
decoding, each sentence is assigned to the clos-
est cluster-specific model. Our approach bears re-
semblance to this clustering, but is different in that
Yamamoto and Sumita assign each sentence to the
closest model, and use this model for decoding,
whereas in our approach, each cluster is associ-
ated with a mixture of models that is optimized to
the cluster, and the number of clusters need not be
equal to the number of component models.
3 Translation Model Architecture
This section covers the architecture of the multi-
domain translation model framework. Our transla-
tion model is embedded in a log-linear model as is
common for SMT, and treated as a single transla-
tion model in this log-linear combination. We im-
plemented this architecture for phrase-based mod-
els, and will use this terminology to describe it,
but in principle, it can be extended to hierarchical
or syntactic models.
The architecture has two goals: move the calcu-
lation of translation model features to the decoding
phase, and allow for multiple knowledge sources
(e.g. bitexts or user-provided data) to contribute to
their calculation. Our immediate purpose for this
paper is domain adaptation in a multi-domain en-
vironment, but the delay of the feature computa-
tion has other potential applications, e.g. in inter-
active MT.
We are concerned with calculating four features
during decoding, henceforth just referred to as the
translation model features: p(s|t), lex(s|t), p(t|s)
and lex(t|s). s and t denote the source and target
phrase. We follow the definitions in (Koehn et al,
2003).
Traditionally, the phrase translation probabili-
ties p(s|t) and p(t|s) are estimated through un-
smoothed maximum likelihood estimation (MLE).
p(x|y) = c(x, y)c(y) =
c(x, y)?
x? c(x?, y)
(1)
where c denotes the count of an observation, and
p the model probability.
The lexical weights lex(s|t) and lex(t|s) are
calculated as follows, using a set of word align-
ments a between s and t:1
lex(s|t, a) =
n?
i=1
1
|{j|(i, j) ? a}|
?
?(i,j)?a
w(si|tj)
(2)
A special NULL token is added to t and aligned to
each unaligned word in s. w(si|tj) is calculated
through MLE, as in equation 1, but based on the
word (pair) frequencies.
To combine statistics from a vector of n com-
ponent corpora, we can use a weighted version of
equation 1, which adds a weight vector ? of length
n (Sennrich, 2012b):
p(x|y;?) =
?n
i=1 ?ici(x, y)?n
i=1
?
x? ?ici(x?, y)
(3)
The word translation probabilities w(ti|sj) are de-
fined analogously, and used in equation 2 for a
weighted version.
1The equation shows lex(s|t); lex(t|s) is computed anal-
ogously.
833
In order to compute the translation model fea-
tures online, a number of sufficient statistics need
to be accessible at decoding time. For p(s|t)
and p(t|s), we require the statistics c(s), c(t) and
c(s, t). For accessing them during decoding, we
simply store them in the decoder?s data struc-
ture, rather than storing pre-computed translation
model features. This means that we can use exist-
ing, compact data formats for storing and access-
ing them.2
The statistics are accessed when the decoder
collects all translation options for a phrase s in the
source sentence. We then access all translation op-
tions for each component table, obtaining a vector
of statistics c(s) for the source phrase, and c(t) and
c(s, t) for each potential target phrase. For phrase
pairs which are not found, c(s, t) and c(t) are ini-
tially set to 0.
Note that c(t) is potentially incorrect at this
point, since a phrase pair not being found does
not entail that c(t) is 0. After all tables have been
accessed, and we thus know the full set of possi-
ble translation options (s, t), we perform a second
round of lookups for all c(t) in the vector which
are still set to 0. We introduce a second table for
accessing c(t) efficiently, again storing it in the de-
coder?s data structure. We can easily create such a
table by inverting the source and target phrases,
deduplicating it for compactness (we only need
one entry per target phrase), and storing c(t) as
only feature.
For lex(s|t), we require an alignment a, plus
c(tj) and c(si, tj) for all pairs (i, j) in a. lex(t|s)
can be based on the same alignment a (with the ex-
ception of NULL alignments, which can be added
online), but uses statistics c(sj) and c(ti, sj). For
estimating the lexical probabilities, we load the
frequencies into a vector of four hash tables.3
Both space and time complexity of the lookup
is linear to the number of component tables. We
deem it is still practical because the collection of
translation options is typically only a small frac-
tion of total decoding time, with search making
up the largest part. For storing and accessing the
sufficient statistics (except for the word (pair) fre-
quencies), we use an on-disk data structure pro-
2We have released an implementation of the architecture
as part of the Moses decoder.
3c(s, t) and c(t, s) are not identical since the lexical
probabilities are based on the unsymmetrized word align-
ment frequencies (in the Moses implementation which we re-
implement).
phrase (pair) c1(x) c2(x)
row 300 80
(row, Zeile) 240 20
(row, Reihe) 60 60
? p(Zeile|row) p(Reihe|row)
(1, 1) 0.68 0.32
(1, 10) 0.40 0.60
(10, 1) 0.79 0.21
Table 1: Illustration of instance weighting with
weight vectors for two corpora.
vided by Moses, which reduces the memory re-
quirements. Still, the number of components may
need to be reduced, for instance through clustering
of training data (Sennrich, 2012a).
With a small modification, our framework could
be changed to use a single table that stores a vec-
tor of n statistics instead of a vector of n tables.
While this would be more compact in terms of
memory, and keep the number of table lookups in-
dependent of the number of components, we chose
a vector of n tables for its flexibility. With a vec-
tor of tables, tables can be quickly added to or re-
moved from the system (conceivable even at run-
time), and can be polymorph. One applications
where this could be desirable is interactive ma-
chine translation, where one could work with a
mix of compact, static tables, and tables designed
to be incrementally trainable.
In the unweighted variant, the resulting fea-
tures are equivalent to training on the concatena-
tion of all training data, excepting differences in
word alignment, pruning4 and rounding. The ar-
chitecture can thus be used as a drop-in replace-
ment for a baseline system that is trained on con-
catenated training data, with non-uniform weights
only being used for texts for which better weights
have been established. This can be done either us-
ing domain labels or unsupervised methods as de-
scribed in the next section.
As a weighted combination method, we imple-
mented instance weighting as described in equa-
tion 3. Table 1 shows the effect of weighting two
corpora on the probability estimates for the trans-
lation of row. German Zeile (row in a table) is pre-
dominant in a bitext from the domain IT, whereas
4We prune the tables to the most frequent 50 phrase pairs
per source phrase before combining them, since calculat-
ing the features for all phrase pairs of very common source
phrases causes a significant slow-down. We found that this
had no significant effects on BLEU.
834
0 1 2 3 4 5 60
1
2
3
4
5
6
entropy with KDE LM (IT)
entr
opy
with
Acq
uisL
M(L
EGA
L)
gold clusters
0 1 2 3 4 5 60
1
2
3
4
5
6
entropy with KDE LM (IT)
entr
opy
with
Acq
uisL
M(L
EGA
L)
clustering with Euclidean distance
0 1 2 3 4 5 60
1
2
3
4
5
6
entropy with KDE LM (IT)
entr
opy
with
Acq
uisL
M(L
EGA
L)
clustering with cosine similarity
Figure 1: Clustering of data set which contains sentences from two domains: LEGAL and IT. Compari-
son between gold segmentation, and clustering with two alternative distance/similarity measures. Black:
IT; grey: LEGAL.
Reihe (line of objects) occurs more often in a legal
corpus. Note that the larger corpus (or more pre-
cisely, the one in which row occurs more often)
has a stronger impact on the probability distribu-
tion with uniform weights (or in a concatenation of
data sets). Instance weighting allows us to modify
the contribution of each corpus. In our implemen-
tation, the weight vector is set globally, but can be
overridden on a per-sentence basis. In principle,
using different weight vectors for different phrase
pairs in a sentence is conceivable. The framework
can also be extended to support other combination
methods, such as a linear interpolation of models.
4 Unsupervised Clustering for Online
Translation Model Adaptation
The framework supports decoding each sentence
with a separate weight vector of size 4n, 4 being
the number of translation model features whose
computation can be weighted, and n the number
of model components. We now address the ques-
tion of how to automatically select good weights in
a multi-domain task. As a way of optimizing in-
stance weights, (Sennrich, 2012b) minimize trans-
lation model perplexity on a set of phrase pairs,
automatically extracted from a parallel develop-
ment set. We follow this technique, but want to
have multiple weight vectors, adapted to different
texts, between which the system switches at de-
coding time. The goal is to perform domain adap-
tation without requiring domain labels or user in-
put, neither for development nor decoding.
The basic idea consists of three steps:
1. Cluster a development set into k clusters.
2. Optimize translation model weights for each
cluster.
3. For each sentence in the test set, assign it
to the nearest cluster and use the translation
model weights associated with the cluster.
For step 2, we use the algorithm by (Sennrich,
2012b), implemented in the decoder to allow for a
quick optimization of a running system. We will
here discuss steps 1 and 3 in more detail.
4.1 Clustering the Development Set
We use k-means clustering to cluster the sentences
of the development set. We train a language model
on the source language side of each of the n
component bitexts, and compute an n-dimensional
vector for each sentence by computing its entropy
with each language model. Our aim is not to dis-
criminate between sentences that are more likely
and unlikely in general, but to cluster on the ba-
sis of relative differences between the language
model entropies. For this purpose, we choose
the cosine as our similarity measure. Figure 1
illustrates clustering in a two-dimensional vector
space, and demonstrates that Euclidean distance is
unsuitable because it may perform a clustering that
is irrelevant to our purposes.
As a result of development set clustering, we
obtain a bitext for each cluster, which we use to
optimize the model weights, and a centroid per
cluster. At decoding time, we need only perform
an assignment step. Each test set sentence is as-
signed to the centroid that is closest to it in the
vector space.
4.2 Scalability Considerations
Our theoretical expectation is that domain adapta-
tion will fail to perform well if the test data is from
835
a different domain than the development data, or
if the development data is a heterogeneous mix
of domains. A multi-domain setup can mitigate
this risk, but only if the relevant domain is repre-
sented in the development data, and if the devel-
opment data is adequately segmented for the op-
timization. We thus suggest that the development
data should contain enough data from all domains
that one wants to adapt to, and a high number of
clusters.
While the resource requirements increase with
the number of component models, increasing the
number of clusters is computationally cheap at
runtime. Only the clustering of the develop-
ment set and optimization of the translation model
weights for each clusters is affected by k. This
means that the approach can in principle be scaled
to a high number of clusters, and support a high
number of domains.5
The biggest risk of increasing the number of
clusters is that if the clusters become too small,
perplexity minimization may overfit these small
clusters. We will experiment with different num-
bers of clusters, but since we expect the optimal
number of clusters to depend on the amount of
development data, and the number of domains,
we cannot make generalized statements about the
ideal number of k.
While it is not the focus of this paper, we also
evaluate language model adaptation. We perform
a linear interpolation of models for each clus-
ter, with interpolation coefficients optimized us-
ing perplexity minimization on the development
set. The cost of moving language model interpo-
lation into the decoding phase is far greater than
for translation models, since the number of hy-
potheses that need to be evaluated by the language
model is several orders of magnitudes higher than
the number of phrase pairs used during the trans-
lation. For the experiments with language model
adaptation, we have chosen to perform linear in-
terpolation offline, and perform language model
switching during decoding. While model switch-
ing is a fast operation, it also makes the space com-
plexity of storing the language models linear to the
number of clusters. For scaling the approach to a
high number of clusters, we envision that multi-
5If the development set is labelled, one can also use a gold
segmentation of development sets instead of k-means cluster-
ing. At decoding time, cluster assignment can be performed
by automatically assigning each sentence to the closest cen-
troid, or again through gold labels, if available.
data set sentences words (de)
kde 216 000 1 990 000
kdedoc 2880 41 000
kdegb 51 300 450 000
oo 41 000 434 000
oo3 56 800 432 000
php 38 500 301 000
tm 146 000 2 740 000
acquis 2 660 000 58 900 000
dgt 372 000 8 770 000
ecb 110 000 2 850 000
ep7 1 920 000 50 500 000
nc7 159 000 3 950 000
total (train) 5 780 000 131 000 000
dev (IT) 3500 47 000
dev (LEGAL) 2000 46 800
test (IT) 5520 51 800
test (LEGAL) 9780 250 000
Table 2: Parallel data sets English?German.
data set sentences words (en)
eu 1 270 000 25 600 000
fiction 830 000 13 700 000
navajo 30 000 490 000
news 110 000 2 550 000
paraweb 370 000 3 930 000
subtitles 2 840 000 21 200 000
techdoc 970 000 7 270 000
total (train) 6 420 000 74 700 000
dev 3500 50 700
test 3500 49 600
Table 3: Parallel data sets Czech?English.
pass decoding, with an unadapted language model
in the first phase, and rescoring with a language
model adapted online, could perform adequately,
and keep the complexity independent of the num-
ber of clusters.
5 Evaluation
5.1 Data and Methods
We conduct all experiments with Moses (Koehn et
al., 2007), SRILM (Stolcke, 2002), and GIZA++
(Och and Ney, 2003). Log-linear weights are op-
timized using MERT (Och and Ney, 2003). We
keep the word alignment and lexical reordering
models constant through the experiments to min-
imize the number of confounding factors. We re-
port translation quality using BLEU (Papineni et
836
system TM adaptation LM adaptation TM+LM adaptationIT LEGAL IT LEGAL IT LEGAL
baseline 21.1 49.9 21.1 49.9 21.1 49.9
1 cluster (no split) 21.3* 49.9 21.8* 49.7 21.8* 49.8
2 clusters 21.6* 49.9 22.2* 50.4* 22.8* 50.2*
4 clusters 21.7* 49.9 23.1* 50.2* 22.6* 50.2*
8 clusters 22.1* 49.9 23.1* 50.1* 22.7* 50.3*
16 clusters 21.1 49.9 22.6* 50.3* 21.9* 50.1*
gold clusters 21.8* 50.1* 22.4* 50.1* 23.2* 49.9
Table 4: Translation experiments EN?DE. BLEU scores reported.
al., 2002). We account for optimizer instability
by running 3 independent MERT runs per system,
and performing significance testing with MultEval
(Clark et al, 2011). Systems significantly better
than the baseline with p < 0.01 are marked with
(*).
We conduct experiments on two data sets. The
first is an English?German translation task with
two domains, texts related to information technol-
ogy (IT) and legal documents (LEGAL). We use
data sets from both domains, plus out-of-domain
corpora, as shown in table 2. 7 data sets come from
the domain IT: 6 from OPUS (Tiedemann, 2009)
and a translation memory (tm) provided by our in-
dustry partner. 3 data sets are from the legal do-
main: the ECB corpus from OPUS, plus the JRC-
Acquis (Steinberger et al, 2006) and DGT-TM
(Steinberger et al, 2012). 2 data sets are out-of-
domain, made available by the 2012 Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2012). The development sets are random sam-
ples from the respective in-domain bitexts (held-
out from training). The test sets have been pro-
vided by Translated, our industry partner in the
MATECAT project.
Our second data set is CzEng 0.9, a Czech?
English parallel corpus (Bojar and Zabokrtsky?,
2009). It contains text from 7 different sources, on
which we train separate component models. The
size of the corpora is shown in table 3. As de-
velopment and test sets, we use 500 sentences of
held-out data per source.
For both data sets, language models are trained
on the target side of the bitexts. In all experiments,
we keep the number of component models con-
stant: 12 for EN?DE, 7 for CZ?EN. We vary the
number of clusters k from 1, which corresponds to
adapting the models to the full development set, to
16. The baseline is the concatenation of all train-
Data set ?IT ?LEGAL ?cluster 1 ?cluster 2
kde 1.0 1.0 1.0 1.0
kdedoc 0.64 12.0 86.0 6.4
kdegb 1.6 2.3 1.7 2.7
oo 0.76 1.6 0.73 1.7
oo3 1.8 4.7 2.4 2.7
php 0.79 6.3 0.69 3.5
tm 1.3 1.3 1.5 1.1
acquis 0.024 3.5 0.018 1.9
dgt 0.053 4.5 0.033 2.4
ecb 0.071 2.3 0.039 1.2
ep7 0.037 0.53 0.024 0.29
nc7 0.1 1.1 0.063 0.62
Table 5: Weight vectors for feature p(t|s) opti-
mized on four development sets (from gold split
and clustering with k = 2).
ing data, with no adaptation performed. We also
evaluate the labelled setting, where instead of un-
supervised clustering, we use gold labels to split
the development and test sets, and adapt the mod-
els to each labelled domain.
5.2 Results
Table 4 shows results for the EN?DE data set. For
our clustering experiments, the development set is
the concatenation of the LEGAL and IT develop-
ment sets. However, we always use the gold seg-
mentation between LEGAL and IT for MERT and
testing. This allows for a detailed analysis of the
effect of development data clustering for the pur-
pose of model adaptation. In an unlabelled setting,
one would have to run MERT either on the full de-
velopment set (as we will do for the CZ?EN task)
or separately on each cluster, or use an alternative
approach to optimize log-linear weights in a multi-
domain setting, such as feature augmentation as
described by (Clark et al, 2012).
837
system TM adaptation LM adaptation TM+LM adaptation
baseline 34.4 34.4 34.4
1 cluster (no split) 34.5 33.7 34.1
2 clusters 34.6 34.0 34.4
4 clusters 34.7* 34.3 34.6
8 clusters 34.7* 34.5 34.9*
16 clusters 34.7* 34.7* 35.0*
gold clusters 35.0* 35.0* 35.4*
Table 6: Translation experiments CZ?EN. BLEU scores reported.
We find that an adaptation of the TM and LM
to the full development set (system ?1 cluster?)
yields the smallest improvements over the un-
adapted baseline. The reason for this is that the
mixed-domain development set is not representa-
tive for the respective test sets. Using multiple
adapted systems yields better performance. For
the IT test set, the system with gold labels and TM
adaptation yields an improvement of 0.7 BLEU
(21.1 ? 21.8), LM adaptation yields 1.3 BLEU
(21.1 ? 22.4), and adapting both models outper-
forms the baseline by 2.1 BLEU (21.1 ? 23.2).
The systems that use unsupervised clusters reach
a similar level of performance than those with
gold clusters, with best results being achieved
by the systems with 2?8 clusters. Some sys-
tems outperform both the baseline and the gold
clusters, e.g. TM adaptation with 8 clusters
(21.1 ? 21.8 ? 22.1), or LM adaptation with 4
or 8 clusters (21.1 ? 22.4 ? 23.1).
Results with 16 clusters are slightly worse than
those with 2?8 clusters due to two effects. Firstly,
for the system with adapted TM, one of the three
MERT runs is an outlier, and the reported BLEU
score of 21.1 is averaged from the three MERT
runs achieving 22.1, 21.6, and 19.6 BLEU, respec-
tively. Secondly, about one third of the IT test
set is assigned to a cluster that is not IT-specific,
which weakens the effect of domain adaptation for
the systems with 16 clusters.
For the LEGAL subset, gains are smaller. This
can be explained by the fact that the majority of
training data is already from the legal domain,
which makes it unnecessary to boost its impact on
the probability distribution even further.
Table 5 shows the automatically obtained trans-
lation model weight vectors for two systems,
?gold clusters? and ?2 clusters?, for the feature
p(t|s). It illustrates that all the corpora that we
consider out-of-domain for IT are penalized by
a factor of 10?50 (relative to the in-domain kde
corpus) for the computation of this feature. For
the LEGAL domain, the weights are more uni-
form, which is congruent with our observation that
BLEU changes little.
Table 6 shows results for the CZ?EN data set.
For each system, MERT is performed on the full
development set. As in the first task, adaptation to
the full development set is least effective. The sys-
tems with unsupervised clusters significantly out-
perform the baseline. For the system with 16 clus-
ters, we observe an improvement of 0.3 BLEU for
TM adaptation, and 0.6 BLEU for adapting both
models (34.4 ? 34.7 ? 35.0). The labelled sys-
tem, i.e. the system with 7 clusters corresponding
to the 7 data sources, both for the development and
test set, performs best. We observe gains of 0.6
BLEU (34.4 ? 35.0) for TM or LM adaptation,
and 1 BLEU (34.4 ? 35.4) when both models are
adapted.
We conclude that the translation model archi-
tecture is effective in a multi-domain setting, both
with unsupervised clusters and labelled domains.
The fact that language model adaptation yields an
additional improvement in our experiments sug-
gests that it it would be worthwhile to also inves-
tigate a language model data structure that effi-
ciently supports multiple domains.
6 Conclusion
We have presented a novel translation model ar-
chitecture that delays the computation of trans-
lation model features to the decoding phase, and
uses a vector of component models for this com-
putation. We have also described a usage scenario
for this architecture, namely its ability to quickly
switch between weight vectors in order to serve as
an adapted model for multiple domains. A sim-
ple, unsupervised clustering of development data
is sufficient to make use of this ability and imple-
838
ment a multi-domain translation system. If avail-
able, one can also use the architecture in a labelled
setting.
Future work could involve merging our trans-
lation model framework with the online adapta-
tion of other models, or the log-linear weights.
Our approach is orthogonal to that of (Clark et
al., 2012), who perform feature augmentation to
obtain multiple sets of adapted log-linear weights.
While (Clark et al, 2012) use labelled data, their
approach could in principle also be applied after
unsupervised clustering.
The translation model framework could also
serve as the basis of real-time adaptation of trans-
lation systems, e.g. by using incremental means to
update the weight vector, or having an incremen-
tally trainable component model that learns from
the post-edits by the user, and is assigned a suit-
able weight.
Acknowledgments
This research was partially funded by the
Swiss National Science Foundation under grant
105215 126999, the European Commission
(MATECAT, ICT-2011.4.2 287688) and the
DARPA BOLT project.
References
Pratyush Banerjee, Jinhua Du, Baoli Li, Sudip Kumar
Naskar, Andy Way, and Josef Van Genabith. 2010.
Combining multi-domain statistical machine trans-
lation models using automatic classifiers. In 9th
Conference of the Association for Machine Trans-
lation in the Americas (AMTA 2010), Denver, Col-
orado, USA.
Ondrej Bojar and Zdenek Zabokrtsky?. 2009. Czeng
0.9: Large parallel treebank with rich annotation.
Prague Bull. Math. Linguistics, 92:63?84.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176?181, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Jonathan H. Clark, Alon Lavie, and Chris Dyer. 2012.
One system, many domains: Open-domain statisti-
cal machine translation via feature augmentation. In
Conference of the Association for Machine Transla-
tion in the Americas 2012 (AMTA 2012), San Diego,
California, USA.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 128?135, Prague, Czech
Republic. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Edmonton, Canada. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, pages
708?717, Singapore. Association for Computational
Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In HLT-
NAACL, pages 546?554. The Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA. Association for
Computational Linguistics.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple trans-
lation models in statistical machine translation. In
839
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, Jeju, Re-
public of Korea. Association for Computational Lin-
guistics.
Rico Sennrich. 2012a. Mixture-modeling with unsu-
pervised clusters for domain adaptation in statistical
machine translation. In 16th Annual Conference of
the European Association for Machine Translation
(EAMT 2012), pages 185?192, Trento, Italy.
Rico Sennrich. 2012b. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 539?
549, Avignon, France. Association for Computa-
tional Linguistics.
Kashif Shah, Loc Barrault, and Holger Schwenk.
2012. A general framework to weight heteroge-
neous parallel data for model adaptation in statistical
machine translation. In Conference of the Associa-
tion for Machine Translation in the Americas 2012
(AMTA 2012), San Diego, California, USA.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC?2006),
Genoa, Italy.
Ralf Steinberger, Andreas Eisele, Szymon Klocek,
Spyridon Pilos, and Patrick Schlu?ter. 2012. DGT-
TM: A freely available translation memory in 22 lan-
guages. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey. European Language
Resources Association (ELRA).
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Seventh International
Conference on Spoken Language Processing, pages
901?904, Denver, CO, USA.
Jo?rg Tiedemann. 2009. News from OPUS - a col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Hirofumi Yamamoto and Eiichiro Sumita. 2007. Bilin-
gual cluster based models for statistical machine
translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 514?523, Prague, Czech Republic.
840
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 166?170,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The UZH System Combination System for WMT 2011
Rico Sennrich
Institute of Computational Linguistics
University of Zurich
Binzmu?hlestr. 14
CH-8050 Zu?rich
sennrich@cl.uzh.ch
Abstract
This paper describes the UZH system that was
used for the WMT 2011 system combination
shared task submission. We participated in
the system combination task for the translation
directions DE?EN and EN?DE. The system
uses Moses as a backbone, with the outputs
of the 2?3 best individual systems being inte-
grated through additional phrase tables. The
system compares well to other system com-
bination submissions, with no other submis-
sion being significantly better. A BLEU-based
comparison to the individual systems, how-
ever, indicates that it achieves no significant
gains over the best individual system.
1 Introduction
For our submission to the WMT 2011 shared task,
we built a system with the multi-engine MT ap-
proach described in (Sennrich, 2011), which builds
on the idea by (Chen et al, 2007). A Moses SMT
system (Koehn et al, 2007) is used as a backbone,
trained on the WMT 2011 training data. Translation
hypotheses by other systems are integrated through
a second phrase table. In this second phrase ta-
ble, the phrase translation probabilities and lexical
weights are computed based on the word and phrase
frequencies in both the translation hypotheses and a
parallel training corpus. On the evaluation data in
(Sennrich, 2011), this system significantly outper-
formed MEMT (Heafield and Lavie, 2010), which
was among the best-performing system combination
tools at WMT 2010 (Callison-Burch et al, 2010).
In this paper, we apply the same approach to a dif-
ferent translation scenario, namely the WMT 2011
shared task. We fail to significantly outperform the
best individual system in terms of BLEU score. In
section 2, we describe our system combination ap-
proach. In section 3, we present the results, and dis-
cuss possible reasons why the system fails to show
the same performance gains as in the translation task
on which it was evaluated initially.
2 System Description
We participated in the system combination task DE?
EN and EN?DE. Since the combination is achieved
by integrating translation hypotheses into an existing
Moses system, which we will call the primary sys-
tem, we first describe the methods and data used for
training this primary system. Then, we describe how
the translation hypotheses are selected out of the in-
dividual system submissions and integrated into the
Moses system.
2.1 Primary System
For the training of the primary systems, we mostly
followed the baseline instructions for the transla-
tion task1. We use news-commentary and Europarl
as parallel training data. The language models are
a linear interpolation of the news-commentary, Eu-
roparl and news corpora, optimized for minimal
cross-entropy on the newstest2008 data sets in the
respective target language.
Additionally, we prune the primary phrase table
using statistical significance tests, as described by
(Johnson et al, 2007). For the translation direction
DE?EN, the German source text is reordered based
1described at http://www.statmt.org/wmt11/
baseline.html
166
on syntactic parsing with Pro3GresDE (Sennrich et
al., 2009), and reordering rules similar to those de-
scribed by (Collins et al, 2005).
The Moses phrase table consists of five fea-
tures: phrase translation probabilities in both trans-
lation directions (p(t|s) and p(s|t)), lexical weights
(lex(t|s) and lex(s|t)), and a constant phrase
penalty (Koehn et al, 2003). The computation of the
phrase translation probabilities and lexical weights
is based on the word, phrase and word/phrase pair
frequencies that are extracted from the parallel cor-
pus. We modified the Moses training scripts to col-
lect and store these frequencies for later re-use.
We did not submit the primary system outputs to
the Machine Translation shared task, since we did
not experiment with new techniques. Instead, the
primary system forms the backbone of the system
combination system.
2.2 Integrating Secondary Phrase Tables
To combine the output of several systems, we train a
second phrase table on the translation hypotheses of
these systems. For this, we create a parallel corpus
consisting of n translation hypotheses and n copies
of the corresponding source text, both lowercased
and detokenized.2
We compute the word alignment with MGIZA++
(Gao and Vogel, 2008), based on the word alignment
model from the primary corpus that we have previ-
ously saved to disk.
After training a phrase table from the word-
aligned corpus with Moses, the lexical weights and
translation probabilities are rescored, using the suffi-
cient statistics (i.e. the word, phrase and word/phrase
pair counts) of both the primary and the secondary
corpus. This rescoring step has been shown to
markedly improve performance in (Sennrich, 2011).
We will discuss its effects in section 3.1. The re-
scored phrase table is integrated into the primary
Moses system as an alternative decoding path, and
tuned for maximal BLEU score on newssyscomb-
tune2011 with MERT.
2For convenience and speed, we combined the
translation hypotheses for newssyscombtune2011 and
newssyscombtest2011 into a single corpus. In principle,
we could train separate phrase tables for each data set, or even
for arbitrarily low numbers of sentences, without significant
loss in performance (see (Sennrich, 2011)).
System BLEU
Primary 21.11
Best individual 24.16
Submission 24.44
Vanilla scoring 24.42
Table 1: DE?EN results. Case-insensitive BLEU scores.
2.3 Hypothesis Selection
For the secondary phrase table, we chose to se-
lect the n best individual systems according to their
BLEU score on the tuning set. We determined the
optimal n empirically by trying different n, measur-
ing each system?s BLEU score on the tuning set and
selecting the highest-scoring one. For the DE?EN
translation task, n = 2 turned out to be optimal, for
EN?DE, n = 3.
Chen et al (2009) propose additional, tunable fea-
tures in the phrase table to indicate the origin of
phrase translations. For better comparability with
the results described in (Sennrich, 2011), we did
not add such features. This means that there are
no a priori weights that bias the phrase selection
for or against certain systems, but that decoding
is purely driven by the usual Moses features: two
phrase tables ? the primary one and the re-scored,
secondary one ? the language model, the primary
reordering model, and the corresponding parameters
established through MERT.
3 Results
In the manual evaluation, the system combination
submissions are only compared to each other, not
to the individual systems. According to the manual
evaluation, no other system combination submission
outperforms ours by a statistically significant mar-
gin. In a comparison to individual systems, however,
BLEU scores indicate that our system fails to yield a
significant performance gain over the best individual
system in this translation scenario.
In tables 1 and 2, we present case-insensitive
BLEU scores (Papineni et al, 2002). As statisti-
cal significance test, we applied bootstrap resam-
pling (Riezler and Maxwell, 2005). Tables 1 and
2 show the BLEU scores for the translation direc-
tions DE?EN and EN?DE, respectively. Systems in-
cluded are the primary translation system described
167
System BLEU
Primary 14.99
Best individual 17.44
Submission 17.51
Vanilla scoring 17.32
Table 2: EN?DE results. Case insensitive BLEU scores.
in section 2.1, the best individual system (online-B
in both cases) and the submitted combination sys-
tem. In terms of BLEU score, we achieved no sta-
tistically significant improvement over the best indi-
vidual system.
As contrastive systems, we trained systems with-
out the rescoring step described in section 2.2; we
found no statistically significant difference from the
submission system. In this translation task, the
statistics from the parallel corpus seem to be inef-
fective at improving decoding, contrary to our find-
ings in (Sennrich, 2011), where rescoring the phrase
table improved BLEU scores by 0.7 points. We will
address possible reasons for this discrepancy in the
following section.
3.1 Interpretation
The main characteristic that sets our approach apart
from other system combination software such as
MANY (Barrault, 2010) and MEMT (Heafield and
Lavie, 2010) is its reliance on word and phrase fre-
quencies in a parallel corpus to guide decoding,
whereas MANY and MEMT operate purely on the
target side, without requiring/exploiting the source
text or parallel data. We integrate the information
from a parallel corpus into the decoding process by
extracting phrase translations from the translation
hypotheses and scoring these phrase translations on
the basis of the frequencies from the parallel corpus.
The properties of this re-scored phrase table
proved attractive for the translation task in (Sen-
nrich, 2011), but less so for the WMT 2011 trans-
lation task. To explain why, let us look at p(t|s),
i.e. the probability of a target phrase given a source
phrase, as an example. It is computed as follows,
cprim and csec being the phrase count in the primary
and secondary corpus, respectively.
p(t|s) =
cprim(s, t) + csec(s, t)
cprim(s) + csec(s)
(1)
We can assume that csec(s) and csec(s, t) are
mostly fixed, having values between 1 and the num-
ber of translation hypotheses.3 If cprim(s) is high,
the phrase translation probabilities in the secondary
phrase table will only be marginally different from
those in the primary phrase table (e.g. 5001000 = 0.5 vs.
500+2
1000+2 = 0.501), whereas the secondary corpus has
a stronger effect for phrases that are rare or unseen in
the primary corpus (e.g. 13 = 0.333 vs.
1+2
3+2 = 0.6).
Analogously, the same reasoning applies to p(s|t),
lex(t|s) and lex(s|t).45
In short: the more frequent the phrases and phrase
pairs in the primary corpus, the less effect does the
secondary corpus have on the final feature values.
This is a desirable behaviour if we can ?trust? the
phrase pairs extracted from the primary corpus. In
(Sennrich, 2011), the primary corpus consisted of
in-domain texts, whereas the translation hypothe-
ses came from an out-of-domain SMT system and a
rule-based one. There, it proved an effective strategy
to only consider those translation hypotheses that ei-
ther agreed with the data from the primary corpus, or
for which the primary corpus had insufficient data,
i.e. unknown or rare source words. With a primary
system achieving a BLEU score of 17.18 and two
translation hypotheses, scoring 13.29 and 12.94, we
obtained a BLEU score of 20.06 for the combined
system.
In the WMT 2011 system combination task, the
statistics from the primary corpus failed to effec-
tively improve translation quality. We offer these
explanations based on an analysis of the results.
First, the 2?3 systems whose translation hypothe-
ses we combine obtain higher scores than the pri-
mary system. This casts doubt on whether we should
trust the scores from the primary system more than
the translation hypotheses. And in fact, the results
in table 1 and 2 show that the submission system
3Strictly speaking, this is only true if we build separate
phrase tables for each sentence that is translated, and if there
are no repeated phrases. This slight simplification serves illus-
trative purposes.
4For long phrases, phrase counts are typically low. Still, the
primary corpus plays an important role in the computation of
the lexical weights, which are computed from word frequencies,
and thus typically less sparse than phrase frequencies.
5Rare target words may obtain a undesirably high probabil-
ity, but are penalized in the language model. We set the LM
log-probability of unknown words to -100.
168
(whose phrase table features take into account the
primary corpus) is not better than a contrastive com-
bination system with vanilla scoring, i.e. one that
is solely based on the secondary corpus. We can
also show why the primary corpus does not improve
decoding by way of example. The German phrase
Bei der Wahl [der Matratze] (English: In the choice
[of a mattress]), is translated by the three systems
as in the selection, when choosing and in the elec-
tion. In this context, the last translation hypothesis
is the least correct, but since the political domain
is strongly represented in the training data, it is the
most frequent one in the primary corpus, and the one
being chosen by both the primary and the combined
system.
Second, there seems to be a significant overlap in
training data between the systems that we combine
and our primary system6. We only saw few cases
in which a system produced a translation against
which there was evidence in our primary corpus.
One instance is the German word Kindergarten (En-
glish: kindergarten; nursery), which is translated
as children?s garden by one system. In the com-
bined system, this translation is dispreferred. (Chen
et al, 2009) argue that a combination of dissimi-
lar systems might yield better results. Rule-based
systems could fulfill this role; they are also an at-
tractive choice given their high quality (as judged by
human evaluators) in earlier evaluations (e.g. WMT
2009 (Callison-Burch et al, 2009)). We did not pur-
sue this idea, since we optimized for highest BLEU
score, both during MERT and for the selection of the
submission system, a scoring method that has been
shown to undervalue rule-based systems (Callison-
Burch et al, 2006).
The failure to outperform the individual best sys-
tem in this translation task does not invalidate our
approach. It merely highlights that different con-
ditions call for different tools. Our approach re-
lies strongly on parallel training data, in contrast
to system combination tools such as MANY (Bar-
rault, 2010) and MEMT (Heafield and Lavie, 2010).
In this setting, this brought no benefit. However,
when developing a SMT system for a specific do-
main and when combining an in-domain primary
6This is especially true for all shared task participants build-
ing constrained systems. The amount of overlap between the
anonymous online systems is unknown.
system with out-of-domain translation hypotheses,
we expect that this strong dependence on the pri-
mary SMT system becomes an advantage. It allows
the system to discriminate between source phrases
that are well-documented in the primary training
data, which will give other systems? hypotheses lit-
tle effect, and those that occur rarely or not at all in
the primary data, for which other systems may still
produce a useful translation.
4 Conclusion
We described the UZH system combination submis-
sion to the Workshop of Machine Translation 2011.
It uses the Moses architecture and includes transla-
tion hypotheses through a second phrase table. Its
central characteristic is the extraction of phrase pairs
from translations hypotheses and the scoring thereof
on the basis of another parallel corpus. We find
that, in the WMT 2011 system combination shared
task, this approach fails to result in a significant im-
provement over the best individual system in terms
of BLEU score. However, we argue that it is well
suited for other translation tasks, such as the one de-
scribed in (Sennrich, 2011).
Acknowledgments
This research was funded by the Swiss National Sci-
ence Foundation under grant 105215 126999.
References
Lo??c Barrault. 2010. MANY: Open source MT sys-
tem combination at WMT?10. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 277?281, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In Proceedings the Eleventh Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 249?256, Trento, Italy.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, pages 1?28, Athens, Greece, March. Associa-
tion for Computational Linguistics.
169
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
decoder for statistical machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, StatMT ?07, pages 193?196, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Yu Chen, Michael Jellinghaus, Andreas Eisele, Yi Zhang,
Sabine Hunsicker, Silke Theison, Christian Feder-
mann, and Hans Uszkoreit. 2009. Combining multi-
engine translations with Moses. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
StatMT ?09, pages 42?46, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 531?540, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Kenneth Heafield and Alon Lavie. 2010. CMU multi-
engine machine translation for WMT 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, WMT ?10,
pages 301?306, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL 2007, pages 177?180, Prague, Czech
Republic, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 311?318, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Rico Sennrich, Gerold Schneider, Martin Volk, and Mar-
tin Warin. 2009. A New Hybrid Dependency Parser
for German. In Proceedings of the German Society for
Computational Linguistics and Language Technology
2009 (GSCL 2009), Potsdam, Germany.
Rico Sennrich. 2011. Combining multi-engine ma-
chine translation and online learning through dynamic
phrase tables. In 15th Annual Conference of the Eu-
ropean Association for Machine Translation (EAMT
2011), Leuven, Belgium.
170
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 64?70,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
TerrorCat: a Translation Error Categorization-based MT Quality Metric
Mark Fishel,? Rico Sennrich,? Maja Popovic?,? Ondr?ej Bojar?
? Institute of Computational Linguistics, University of Zurich
{fishel,sennrich}@cl.uzh.ch
? German Research Center for Artificial Intelligence (DFKI), Berlin
maja.popovic@dfki.de
? Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
bojar@ufal.mff.cuni.cz
Abstract
We present TerrorCat, a submission to the
WMT?12 metrics shared task. TerrorCat uses
frequencies of automatically obtained transla-
tion error categories as base for pairwise com-
parison of translation hypotheses, which is in
turn used to generate a score for every trans-
lation. The metric shows high overall corre-
lation with human judgements on the system
level and more modest results on the level of
individual sentences.
1 The Idea
Recently a couple of methods of automatic trans-
lation error analysis have emerged (Zeman et al,
2011; Popovic? and Ney, 2011). Initial experiments
have shown that while agreement with human error
analysis is low, these methods show better perfor-
mance on tasks with a lower granularity, e.g. ranking
error categories by frequency (Fishel et al, 2012).
In this work we apply translation error analysis to a
task with an even lower granularity: ranking transla-
tions, one of the shared tasks of WMT?12.
The aim of translation error analysis is to identify
the errors that translation systems make and catego-
rize them into different types: e.g. lexical, reorder-
ing, punctuation errors, etc. The two tools that we
will use ? Hjerson and Addicter ? both rely on a ref-
erence translation. The hypothesis translation that is
being analyzed is first aligned to the reference on the
word level, and then mistranslated, misplaced, mis-
inflected, missing or superfluous words and other er-
rors are identified.
The main idea of our work is to quantify trans-
lation quality based on the frequencies of different
error categories. The basic assumption is that differ-
ent error categories have different importance from
the point of view of overall translation quality: for
instance, it would be natural to assume that punc-
tuation errors influence translation quality less than
missing words or lexical choice errors. Furthermore,
an error category can be more important for one out-
put language than the other: for example, word or-
der can influence the meaning in an English sentence
more than in a Czech or German one, whereas in-
flection errors are probably more frequent in the lat-
ter two and can thus cause more damage.
In the context of the ranking task, the absolute
value of a numeric score has no importance, apart
from being greater than, smaller than or equal to the
other systems? scores. We therefore start by per-
forming pairwise comparison of the translations ?
the basic task is to compare two translations and re-
port which one is better. To conform with the WMT
submission format we need to generate a numeric
score as the output ? which is obtained by compar-
ing every possible pair of translations and then using
the (normalized) total number of wins per translation
as its final score.
The general architecture of the metric is thus this:
? automatic error analysis is applied to the sys-
tem outputs, yielding the frequencies of every
error category for each sentence
? every possible pair of all system outputs is rep-
resented as a vector of features, based on the
error category frequencies
64
? a binary classifier takes these feature vectors as
input and assigns a win to one of the sentences
in every pair (apart from ties)
? the final score of a system equals to the normal-
ized total number of wins per sentence
? the system-level score is averaged out over the
individual sentence scores
An illustrative example is given in Figure 1.
We call the result TerrorCat, the translation error
categorization-based metric.
2 The Details
In this section we will describe the specifics of
the current implementation of the TerrorCat met-
ric: translation error analysis, lemmatization, binary
classifier and training data for the binary classifier.
2.1 Translation Error Analysis
Addicter (Zeman et al, 2011) and Hjerson (Popovic?
and Ney, 2011) use different methods for automatic
error analysis. Addicter explicitly aligns the hy-
pothesis and reference translations and induces error
categories based on the alignment coverage while
Hjerson compares words encompassed in the WER
(word error rate) and PER (position-independent
word error rate) scores to the same end.
Previous evaluation of Addicter shows that
hypothesis-reference alignment coverage (in terms
of discovered word pairs) directly influences er-
ror analysis quality; to increase alignment cover-
age we used Berkeley aligner (Liang et al, 2006)
and trained it on and applied it to the whole set of
reference-hypothesis pairs for every language pair.
Both tools use word lemmas for their analysis;
we used TreeTagger (Schmid, 1995) for analyzing
English, Spanish, German and French and Morc?e
(Spoustova? et al, 2007) to analyze Czech. The same
tools are used for PoS-tagging in some experiments.
2.2 Binary Classification
Pairwise comparison of sentence pairs is achieved
with a binary SVM classifier, trained via sequential
minimal optimization (Platt, 1998), implemented in
Weka (Hall et al, 2009).
The input feature vectors are composed of fre-
quency differences of every error category; since the
Source: Wir sind Meister!
Translations:
Reference: We are the champions!
HYP-1: Us champions!
HYP-2: The champions we are .
HYP-3: We are the champignons!
Error Frequencies:
HYP-1: 1?inflection, 2?missing
HYP-2: 2?order, 1?punctuation
HYP-3: 1?lex.choice
Classifier Output: (or manually created
input in the training phase)
HYP-1 < HYP-2
HYP-1 < HYP-3
HYP-2 > HYP-3
Scores:
HYP-1: 0
HYP-2: 1
HYP-3: 0.5
Figure 1: Illustration of TerrorCat?s process for a single
sentence: translation errors in the hypothesis translations
are discovered by comparing them to the reference, error
frequencies are extracted, pairwise comparisons are done
by the classifier and then converted to scores. The shown
translation errors correspond to Hjerson?s output.
maximum (normalized) frequency of any error rate
is 1, the feature value range is [?1, 1]. To include
error analysis from both Addicter and Hjerson their
respective features are used side-by-side.
2.3 Data Extraction
Training data for the SVM classifier is taken from
the WMT shared task manual ranking evaluations
of previous years (2007?2011), which consist of tu-
ples of 2 to 5 ranked sentences for every language
pair. Equal ranks are allowed, and translations of
the same sentence by the same pair of systems can
be present in several tuples, possibly having conflict-
ing comparison results.
To convert the WMT manual ranking data into
the training data for the SVM classifier, we collect
all rankings for each pair of translation hypothe-
65
2007-2010 2007-2011
fr-en 34 152 46 070
de-en 36 792 53 790
es-en 30 374 41 966
cs-en 19 268 26 418
en-fr 22 734 35 854
en-de 36 076 56 054
en-es 19 352 35 700
en-cs 31 728 52 954
Table 1: Dataset sizes for every language pair, based
on manual rankings from WMT shared tasks of previ-
ous years: the number of pairs with non-conflicting, non-
equivalent ranks.
ses. Pairs with equal ranks are discarded, conflicting
ranks for the same pairs are resolved with voting. If
the voting is tied, the pair is also discarded.
The kept translation pairs are mirrored (i.e. both
directions of every pair are added to the training set
as independent entries) to ensure no bias towards the
first or second translation in a pair. We will later
present analysis of how well that works.
2.4 TerrorCat+You
TerrorCat is distributed via GitHub; information on
downloading and using it can be found online.1 Ad-
ditionally we are planning to provide more recent
evaluations with new datasets, as well as pre-trained
models for various languages and language pairs.
3 The Experiments
In the experimental part of our work, we search for
the best performing model variant, the aim of which
is to evaluate different input features, score calcula-
tion strategies and other alternations. The search is
done empirically: we evaluate one alternation at a
time, and if it successful, it is added to the system
before proceeding to test further alternations.
Performance of the models is estimated on a held-
out development set, taken from the WMT?11 data;
the training data during the optimization phase is
composed of ranking data from WMT 2007?2010.
In the end we re-trained our system on the whole
data set (WMT 2007?2011) and applied it to the un-
1http://terra.cl.uzh.ch/terrorcat.html
labeled data from this year?s shared task. The result-
ing dataset sizes are given in Table 1.
All of the resulting scores obtained by different
variants of our metric are presented in Tables 2 (for
system-level correlations) and 3 (for sentence-level
correlations), compared to BLEU and other selected
entries in the WMT?11 evaluation shared task. Cor-
relations are computed in the same way as in the
WMT evaluations.
3.1 Model Optimization
The following is a brief description of successful
modifications to the baseline system.
Weighted Wins
In the baseline model, the score of the winning
system in each pairwise comparison is increased by
1. To reduce the impact of low-confidence decisions
of the classifier on the final score we tested replac-
ing the constant rewards to the winning system with
variable ones, proportional to the classifier?s confi-
dence ? a measure of which was obtained by fitting
a logistic regression model to the SVM output.
As the results show, this leads to minor improve-
ments in sentence-level correlation and more notice-
able improvements in system-level correlation (es-
pecially English-French and Czech-English). A pos-
sible explanation for this difference in performance
on different levels is that low classification confi-
dence on the sentence-level does not necessarily af-
fect our ranking for that sentence, but reduces the
impact of that sentence on the system-level ranking.
PoS-Split Features
The original model only makes a difference be-
tween individual error categories as produced by
Hjerson and Addicter. It seems reasonable to assume
that errors may be more or less important, depending
on the part-of-speech of the words they occur in. We
therefore tested using the number of errors per er-
ror category per PoS-tag as input features. In other
words, unlike the baseline, which relied on counts
of missing, misplaced and other erroneous words,
this alternation makes a difference between miss-
ing nouns/verbs/etc., misplaced nouns, misinflected
nouns/adjectives, and so on.
The downside of this approach is that the number
of features is multiplied by the size of the PoS tag
66
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.73 0.74 0.82 0.76 0.76 0.70 0.81 0.69 0.84 0.76
Weighted wins 0.73 0.74 0.82 0.79 0.77 0.75 0.81 0.69 0.84 0.77
PoS-features 0.87 0.76 0.80 0.86 0.82 0.76 0.86 0.74 0.87 0.81
GenPoS-features 0.86 0.77 0.84 0.88 0.84 0.80 0.85 0.75 0.90 0.83
No 2007 data (GenPoS) 0.89 0.80 0.80 0.95 0.86 0.85 0.84 0.81 0.90 0.85
Other:
BLEU 0.85 0.48 0.90 0.88 0.78 0.86 0.44 0.87 0.65 0.70
mp4ibm1 0.08 0.56 0.12 0.91 0.42 0.61 0.91 0.71 0.76 0.75
MTeRater-Plus 0.93 0.90 0.91 0.95 0.92 ? ? ? ? ?
AMBER ti 0.94 0.63 0.85 0.88 0.83 0.84 0.54 0.88 0.56 0.70
meteor-1.3-rank 0.93 0.71 0.88 0.91 0.86 0.85 0.30 0.74 0.65 0.63
Table 2: System-level Spearman?s rank correlation coefficients (?) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
set. Additionally, too specific distinctions can cause
data sparsity, especially on the sentence level.
As shown by the results, PoS-tag splitting of the
features is successful on the system level, but quite
hurtful to the sentence-level correlations. The poor
performance on the sentence level can be attributed
to the aforementioned data sparsity: the number of
different features is higher than the number of words
(and hence, the biggest possible number of errors)
in the sentences. However, we cannot quite ex-
plain, how a sum of these less reliable sentence-level
scores leads to more reliable system-level scores.
To somewhat relieve data sparsity we defined sub-
sets of the original PoS tag sets, mostly leaving out
morphological information and keeping just the gen-
eral word types (nouns, verbs, adjectives, etc.). This
reduced the number of PoS-tags (and thus, the num-
ber of input features) from 2 to 4 times and produced
further increase in system-level and a smaller de-
crease in sentence-level scores, see GenPoS results.
To avoid splitting the metric into different ver-
sions for system-level and sentence-level, we gave
priority to system-level correlations and adopted the
generalized PoS-splitting of the features.
Out-of-Domain Data
The human ranking data from WMT of previ-
ous years do not constitute a completely homo-
geneous dataset. For starters, the test sets are
taken from different domains (News/News Com-
mentary/Europarl), whereas the 2012 test set is from
the News domain only. Added to this, there might be
a difference in the manual data, coming from differ-
ent organization of the competition ? e.g. WMT?07
was the only year when manual scoring of the trans-
lations with adequacy/fluency was performed, and
ranking had just been introduced into the competi-
tion. Therefore we tested whether some subsets of
the training data can result in better overall scores.
Interestingly enough, leaving out News Commen-
tary and Europarl test sets caused decreased correla-
tions, although these account for just around 10%
of the training data. On the other hand, leaving out
the data from WMT?07 led to a significant gain in
overall performance.
3.2 Error Meta-Analysis
To better understand why sentence-level correlations
are low, we analyzed the core of TerrorCat ? its pair-
wise classifier. Here, we focus on the most success-
ful variant of the metric, which uses general PoS-
tags and was trained on the WMT manual rankings
from 2008 to 2010. Table 4 presents the confusion
matrices of the classifier (one for precision and one
for recall), taking into consideration the confidence
estimate.
Evaluation is based on the data from 2011; the
prediction data was mirrored in the same way as for
67
Metric fr-en de-en es-en cs-en *-en en-fr en-de en-es en-cs en-*
TerrorCat:
Baseline 0.20 0.22 0.33 0.25 0.25 0.30 0.19 0.24 0.20 0.23
Weighted wins 0.20 0.23 0.33 0.25 0.25 0.31 0.20 0.24 0.20 0.24
PoS-features 0.13 0.18 0.24 0.15 0.18 0.27 0.15 0.15 0.17 0.19
GenPoS-features 0.16 0.24 0.31 0.22 0.23 0.27 0.18 0.22 0.19 0.22
No 2007 data (GenPoS) 0.21 0.30 0.33 0.23 0.27 0.29 0.20 0.23 0.20 0.23
Other:
mp4ibm1 0.15 0.16 0.18 0.12 0.15 0.21 0.13 0.13 0.06 0.13
MTeRater-Plus 0.30 0.36 0.45 0.36 0.37 ? ? ? ? ?
AMBER ti 0.24 0.26 0.33 0.27 0.28 0.32 0.22 0.31 0.21 0.27
meteor-1.3-rank 0.23 0.25 0.38 0.28 0.29 0.31 0.14 0.26 0.19 0.23
Table 3: Sentence-level Kendall?s rank correlation coefficients (? ) between different variants of TerrorCat and hu-
man judgements, based on WMT?11 data. Other metric submissions are shown for comparison. Highest scores per
language pair are highlighted in bold separately for TerrorCat variants and for other metrics.
the training set. Our aim was to measure the bias
of the classifier towards first or second translations
in a pair (which is obviously an undesired effect).
It can be seen that the confusion matrices are com-
pletely symmetrical, indicating no position bias of
the classifier ? even lower-confidence decisions are
absolutely consistent.
To make sure that this can be attributed to the mir-
roring of the training set, we re-trained the classifier
on non-mirrored training sets. As a result, 9% of the
instances were labelled inconsistently, with the av-
erage confidence of such inconsistent decisions be-
ing extremely low (2.1%, compared to the overall
average of 28.4%). The resulting correlations have
slightly dropped as well ? all indicating that mirror-
ing the training sets does indeed remove the posi-
tional bias and leads to slightly better performance.
Looking at the confusion matrices overall, most
decisions fall within the main diagonals (i.e. the
cells indicating correct decisions of the classifier).
Looking strictly at the classifier?s decisions, the re-
calls and precisions of the non-tied comparison out-
puts (?<? and ?>?) are 57% precision, 69% recall.
However, such strict estimates are too pessimistic in
our case, since the effect of the classifier?s decisions
is proportional to the confidence estimate. On the
sentence level it means that low-confidence decision
errors have less effect on the total score of a system.
A definite source of error is the instability of the in-
dividual translation errors on the sentence level, an
effect both Addicter and Hjerson are known to suffer
from (Fishel et al, 2012).
The precision of the classifier predictably drops
together with the confidence, and almost half of the
misclassifications come from unrecognized equiva-
lent translations ? as a result the recall of such pairs
of equivalent translations is only 20%. This can be
explained by the fact that the binary classifier was
trained on instances with just these two labels and
with no ties allowed.
On the other hand the classifier?s 0-confidence de-
cisions have a high precision (84%) on detecting the
equivalent translations; after re-examining the data
it turned out that 96% of the 0-confidence decisions
were made on input feature vectors containing only
zero frequency differences. Such vectors represent
pairs of sentences with identical translation error
analyses, which are very often simply identical sen-
tences ? in which case the classifier cannot (and in
fact, should not) make an informed decision of one
being better than the other.
4 Related Work
Traditional MT metrics such as BLEU (Papineni et
al., 2002) are based on a comparison of the trans-
lation hypothesis to one or more human references.
TerrorCat still uses a human reference to extract fea-
tures from the error analysis with Addicter and Hjer-
son, but at the core, TerrorCat compares hypotheses
not to a reference, but to each other.
68
Manual Classifier Output and Confidence: Precision
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 81% 60% 45% 8% 32% 23% 10%
= 9% 17% 23% 84% 23% 17% 9%
> 10% 23% 32% 8% 45% 60% 81%
Manual Classifier Output and Confidence: Recall
label < < or > >
0.6?1.0 0.3?0.6 0.0?0.3 0.0 0.0?0.3 0.3?0.6 0.6?1.0
< 23% 18% 28% 1% 20% 7% 3%
= 5% 9% 26% 20% 26% 9% 5%
> 3% 7% 20% 1% 28% 18% 23%
Table 4: The precision and recall confusion matrices of the classifier ? judgements on whether one hypothesis is worse
than, equivalent to or better than another hypothesis are compared to the classifier?s output and confidence.
It is thus most similar to SVM-RANK and Tesla
metrics, submissions to the WMT?10 shared met-
rics task (Callison-Burch et al, 2010) which also
used SVMs for ranking translations. However, both
metrics used SVMrank (Joachims, 2006) directly for
ranking (unlike TerrorCat, which uses a binary clas-
sifier for pairwise comparisons). Their features in-
cluded some of the metric outputs (BLEU, ROUGE,
etc.) for SVM-RANK and similarity scores between
bags of n-grams for Tesla (Dahlmeier et al, 2011).
5 Conclusions
We introduced the TerrorCat metric, which performs
pairwise comparison of translation hypotheses based
on frequencies of automatically obtained error cate-
gories using a binary classifier, trained on manually
ranked data. The comparison outcome is then con-
verted to a numeric score for every sentence or doc-
ument translation by averaging out the number of
wins per translation system.
Our submitted system achieved an average
system-level correlation with human judgements in
the WMT?11 development set of 0.86 for transla-
tion into English and 0.85 for translations from En-
glish into other languages. Particularly good per-
formance was achieved on translations from English
into Czech (0.90) and back (0.95). Sentence-level
scores are more modest: average 0.27 for transla-
tion into English and 0.23 for those out of English.
The scores remain to be checked against the human
judgments from WMT?12.
The introduced TerrorCat metric has certain de-
pendencies. For one thing, in order to apply it to
new languages, a training set of manual rankings is
required ? although this can be viewed as an advan-
tage, since it enables the user to tune the metric to
his/her own preference. Additionally, the metric de-
pends on lemmatization and PoS-tagging.
There is a number of directions to explore in the
future. For one, both Addicter and Hjerson report
MT errors related more to adequacy than fluency, al-
though it was shown last year (Parton et al, 2011)
that fluency is an important component in rating
translation quality. It is also important to test how
well the metric performs if lemmatization and PoS-
tagging are not available.
For this year?s competition, training data was
taken separately for every language pair; it remains
to be tested whether combining human judgements
with the same target language and different source
languages leads to better or worse performance.
To conclude, we have described TerrorCat, one
of the submissions to the metrics shared task of
WMT?12. TerrorCat is rather demanding to apply on
one hand, having more requirements than the com-
mon reference-hypothesis translation pair, but at the
same time correlates rather well with human judge-
ments on the system level.
69
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
Tesla at wmt 2011: Translation evaluation and tunable
metric. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 78?84, Edinburgh,
Scotland.
Mark Fishel, Ondr?ej Bojar, and Maja Popovic?. 2012.
Terra: a collection of translation error-annotated cor-
pora. In Proceedings of the 8th LREC, page in print,
Istanbul, Turkey.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
Philadelphia, USA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the HLT-
NAACL Conference, pages 104?111, New York, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 108?115, Edinburgh, Scot-
land.
John C. Platt. 1998. Using analytic qp and sparseness
to speed training of support vector machines. In Pro-
ceedings of Neural Information Processing Systems
11, pages 557?564, Denver, CO.
Maja Popovic? and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Computational Linguistics, 37(4):657?688.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, Dublin, Ireland.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Daniel Zeman, Mark Fishel, Jan Berka, and Ondr?ej Bo-
jar. 2011. Addicter: What is wrong with my transla-
tions? The Prague Bulletin of Mathematical Linguis-
tics, 96:79?88.
70
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207?214,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Edinburgh?s Syntax-Based Systems at WMT 2014
Philip Williams
1
, Rico Sennrich
1
, Maria Nadejde
1
,
Matthias Huck
1
, Eva Hasler
1
, Philipp Koehn
1,2
1
School of Informatics, University of Edinburgh
2
Center for Speech and Language Processing, The Johns Hopkins University
Abstract
This paper describes the string-to-tree sys-
tems built at the University of Edin-
burgh for the WMT 2014 shared trans-
lation task. We developed systems for
English-German, Czech-English, French-
English, German-English, Hindi-English,
and Russian-English. This year we
improved our English-German system
through target-side compound splitting,
morphosyntactic constraints, and refine-
ments to parse tree annotation; we ad-
dressed the out-of-vocabulary problem us-
ing transliteration for Hindi and Rus-
sian and using morphological reduction
for Russian; we improved our German-
English system through tree binarization;
and we reduced system development time
by filtering the tuning sets.
1 Introduction
For this year?s WMT shared translation task we
built syntax-based systems for six language pairs:
? English-German ? German-English
? Czech-English ? Hindi-English
? French-English ? Russian-English
As last year (Nadejde et al., 2013), our systems are
based on the string-to-tree pipeline implemented
in the Moses toolkit (Koehn et al., 2007).
We paid particular attention to the production of
grammatical German, trying various parsers and
incorporating target-side compound splitting and
morphosyntactic constraints; for Hindi and Rus-
sian, we employed the new Moses transliteration
model to handle out-of-vocabulary words; and for
German to English, we experimented with tree bi-
narization, obtaining good results from right bina-
rization.
We also present our first syntax-based results
for French-English, the scale of which defeated us
last year. This year we were able to train a sys-
tem using all available training data, a task that
was made considerably easier through principled
filtering of the tuning set. Although our system
was not ready in time for human evaluation, we
present BLEU scores in this paper.
In addition to the five single-system submis-
sions described here, we also contributed our
English-German and German-English systems for
use in the collaborative EU-BRIDGE system com-
bination effort (Freitag et al., 2014).
This paper is organised as follows. In Sec-
tion 2 we describe the core setup that is com-
mon to all systems. In subsequent sections we de-
scribe language-pair specific variations and exten-
sions. For each language pair, we present results
for both the development test set (newstest2013
in most cases) and for the filtered test set (new-
stest2014) that was provided after the system sub-
mission deadline. We refer to these as ?devtest?
and ?test?, respectively.
2 System Overview
2.1 Pre-processing
The training data was normalized using the WMT
normalize-punctuation.perl script then
tokenized and truecased. Where the target lan-
guage was English, we used the Moses tokenizer?s
-penn option, which uses a tokenization scheme
that more closely matches that of the parser. For
the English-German system we used the default
Moses tokenization scheme, which is similar to
that of the German parsers.
For the systems that translate into English, we
used the Berkeley parser (Petrov et al., 2006;
Petrov and Klein, 2007) to parse the target-side of
the training corpus. As we will describe in Sec-
tion 3, we tried a variety of parsers for German.
We did not perform any corpus filtering other
than the standard Moses method, which removes
207
sentence pairs with dubious length ratios and sen-
tence pairs where parsing fails for the target-side
sentence.
2.2 Translation Model
Our translation grammar is a synchronous context-
free grammar (SCFG) with phrase-structure labels
on the target side and the generic non-terminal la-
bel X on the source side.
The grammar was extracted from the word-
aligned parallel data using the Moses implemen-
tation (Williams and Koehn, 2012) of the GHKM
algorithm (Galley et al., 2004; Galley et al., 2006).
For word alignment we used MGIZA++ (Gao and
Vogel, 2008), a multi-threaded implementation of
GIZA++ (Och and Ney, 2003).
Minimal GHKM rules were composed into
larger rules subject to parameterized restrictions
on size defined in terms of the resulting target tree
fragment. A good choice of parameter settings
depends on the annotation style of the target-side
parse trees. We used the settings shown in Table 1,
which were chosen empirically during the devel-
opment of last years? systems:
Parameter Value
Rule depth 5
Node count 20
Rule size 5
Table 1: Parameter settings for rule composition.
Further to the restrictions on rule composition,
fully non-lexical unary rules were eliminated us-
ing the method described in Chung et al. (2011)
and rules with scope greater than 3 (Hopkins and
Langmead, 2010) were pruned from the trans-
lation grammar. Scope pruning makes parsing
tractable without the need for grammar binariza-
tion.
2.3 Language Model
We used all available monolingual data to train
5-gram language models. Language models
for each monolingual corpus were trained using
the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) and then interpolated using weights tuned to
minimize perplexity on the development set.
2.4 Feature Functions
Our feature functions are unchanged from the pre-
vious two years. They include the n-gram lan-
guage model probability of the derivation?s target
yield, its word count, and various scores for the
synchronous derivation.
Each grammar rule has a number of pre-
computed scores. For a grammar rule r of the form
C ? ??, ?,??
where C is a target-side non-terminal label, ? is a
string of source terminals and non-terminals, ? is
a string of target terminals and non-terminals, and
? is a one-to-one correspondence between source
and target non-terminals, we score the rule accord-
ing to the following functions:
? p (C, ? | ?,?) and p (? | C, ?,?), the direct
and indirect translation probabilities.
? p
lex
(? | ?) and p
lex
(? | ?), the direct and
indirect lexical weights (Koehn et al., 2003).
? p
pcfg
(pi), the monolingual PCFG probability
of the tree fragment pi from which the rule
was extracted.
? exp(?1/count(r)), a rule rareness penalty.
? exp(1), a rule penalty. The main grammar
and glue grammars have distinct penalty fea-
tures.
2.5 Tuning
The feature weights were tuned using the Moses
implementation of MERT (Och, 2003) for all sys-
tems except English-to-German, for which we
used k-best MIRA (Cherry and Foster, 2012) due
to the larger number of features.
We used tuning sentences drawn from all of
the previous years? test sets (except newstest2013,
which was used as the development test set). In
order to speed up the tuning process, we used sub-
sets of the full tuning sets with sentence pairs up
to length 30 (Max-30) and further applied a fil-
tering technique to reduce the tuning set size to
2,000 sentence pairs for the language pairs involv-
ing German, French and Czech
1
. We also experi-
mented with random subsets of size 2,000.
For the filtering technique, we make the as-
sumption that finding suitable weights for all the
feature functions requires the optimizer to see a
range of feature values and to see hypotheses that
can partially match the reference translations in
order to rank the hypotheses. For example, if a
1
For Russian and Hindi, the development sets are smaller
and no filtering was applied.
208
tuning example contains many out-of-vocabulary
words or is difficult to translate for other reasons,
this will result in low quality translation hypothe-
ses and provide the system with little evidence for
which features are useful to produce good transla-
tions. Therefore, we select high quality examples
using a smooth version of sentence-BLEU com-
puted on the 1-best output of a single decoder run
on the development set. Standard sentence-BLEU
tends to select short examples because they are
more likely to have perfect n-gram matches with
the reference translation. Very short sentence pairs
are less informative for tuning but also tend to have
more extreme source-target length ratios which
can affect the weight of the word penalty. Thus,
we penalize short examples by padding the de-
coder output with a fixed number of non-matching
tokens
2
to the left and right before computing
sentence-BLEU. This has the effect of reducing
the precision of short sentences against the refer-
ence translation while affecting longer sentences
proportionally less. Experiments on phrase-based
systems have shown that the resulting tuning sets
are of comparable diversity as randomly selected
sets in terms of their feature vectors and maintain
BLEU scores in comparison with tuning on the en-
tire development set.
Table 2 shows the size of the full tuning sets
and the size of the subsets with up to length 30,
Table 3 shows the results of tuning with different
sets. Reducing the tuning sets to Max-30 results
in a speed-up in tuning time but affects the per-
formance on some of the devtest/test sets (mostly
for Czech-English). However, tuning on the full
set took more than 18 days using 12 cores for
German-English which is not feasible when try-
ing out several model variations. Further filter-
ing these subsets to a size of 2,000 sentence pairs
as described above maintains the BLEU scores in
most cases and even improves the scores in some
cases. This indicates that the quality of the se-
lected examples is more important than the total
number of tuning examples. However, the exper-
iments with random subsets from Max-30 show
that random selection also yields results which im-
prove over the results with Max-30 in most cases,
though are not always as good as with the filtered
sets.
3
The filtered tuning sets yield reasonable per-
2
These can be arbitrary tokens that do not match any ref-
erence token.
3
For random subsets from the full tuning set the perfor-
mance was similar but resulted in standard deviations of up
formance compared to the full tuning sets except
for the German-English devtest set where perfor-
mance drops by 0.5 BLEU
4
.
Tuning set Cs-En En-De De-En
Full 13,055 13,071 13,071
Max-30 10,392 9,151 10,610
Table 2: Size of full tuning sets and with sentence
length up to 30.
devtest
Tuning set Cs-En En-De De-En
Full 25.1 19.9 26.7
Max-30 24.7 19.8 26.2
Filtered 24.9 19.8 26.2
Random 24.8 19.7 26.4
test
Tuning set Cs-En En-De De-En
Full 27.5 19.2 26.9
Max-30 27.2 19.2 27.0
Filtered 27.5 19.1 27.2
Random 27.3 19.4 27.0
Table 3: BLEU results on devtest and test sets with
different tuning sets: Full, Max-30, filtered subsets
of Max-30 and average of three random subsets of
Max-30 (size of filtered/random subsets: 2,000).
3 English to German
We use the projective output of the dependency
parser ParZu (Sennrich et al., 2013) for the syn-
tactic annotation of our primary submission. Con-
trastive systems were built with other parsers: Bit-
Par (Schmid, 2004), the German Stanford Parser
(Rafferty and Manning, 2008), and the German
Berkeley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
The set of syntactic labels provided by ParZu
has been refined to reduce overgeneralization phe-
nomena. Specifically, we disambiguate the labels
ROOT (used for the root of a sentence, but also
commas, punctuation marks, and sentence frag-
ments), KON and CJ (coordinations of different
constituents), and GMOD (pre- or postmodifying
genitive modifier).
to 0.36 across three random sets.
4
Note however that due to the long tuning times, we are
reporting single tuning runs.
209
NN
SEGMENT
gericht
COMP
JUNC
@s@
SEGMENT
berufung
COMP
JUNC
@es@
SEGMENT
Bund
Figure 1: Syntactic representation of split com-
pound Bundesberufungsgericht (Engl: federal ap-
peals court).
We discriminatively learn non-terminal labels
for unknown words using sparse features, rather
than estimating a probability distribution of non-
terminal labels from singleton statistics in the
training corpus.
We perform target-side compound splitting, us-
ing a hybrid method described by Fritzinger and
Fraser (2010) that combines a finite-state mor-
phology and corpus statistics. As finite-state mor-
phology analyzer, we use Zmorge (Sennrich and
Kunz, 2014). An original contribution of our
experiments is a syntactic representation of split
compounds which eliminates typical problems
with target-side compound splitting, namely er-
roneous reorderings and compound merging. We
represent split compounds as a syntactic tree with
the last segment as head, preceded by a modifier.
A modifier consists of an optional modifier, a seg-
ment and a (possibly empty) joining element. An
example is shown in Figure 1. This hierarchical
representation ensures that compounds can be eas-
ily merged in post-processing (by removing the
spaces and special characters around joining ele-
ments), and that no segments are placed outside of
a compound in the translation.
We use unification-based constraints to model
morphological agreement within German noun
phrases, and between subjects and verbs (Williams
and Koehn, 2011). Additionally, we add con-
straints that operate on the internal tree structure of
the translation hypotheses, to enforce several syn-
tactic constraints that were frequently violated in
the baseline system:
? correct subcategorization of auxiliary/modal
verbs in regards to the inflection of the full
verb.
? passive clauses are not allowed to have ac-
cusative objects.
system
BLEU
devtest test
Stanford Parser 19.0 18.3
Berkeley Parser 19.3 18.6
BitPar 19.5 18.6
ParZu 19.6 19.1
+ modified label set 19.8 19.1
+ discriminative UNK weights 19.9 19.2
+ German compound splitting 20.0 19.8
+ grammatical constraints 20.2 20.1
Table 4: English to German translation results
on devtest (newstest2013) and test (newstest2014)
sets.
? relative clauses must contain a relative (or in-
terrogative) pronoun in their first constituent.
Table 4 shows BLEU scores with systems
trained with different parsers, and for our exten-
sions of the baseline system.
4 Czech to English
For Czech to English we used the core setup de-
scribed in Section 2 without modification. Table 5
shows the BLEU scores.
BLEU
system devtest test
baseline 24.8 27.0
Table 5: Czech to English results on the devtest
(newstest2013) and test (newstest2014) sets.
5 French to English
For French to English, alignment of the parallel
corpus was performed using fast_align (Dyer et
al., 2013) instead of MGIZA++ due to the large
volume of parallel data.
Table 6 shows BLEU scores for the system and
Table 7 shows the resulting grammar sizes after
filtering for the evaluation sets.
BLEU
system devtest test
baseline 29.4 32.3
Table 6: French to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
210
system devtest test
baseline 86,341,766 88,657,327
Table 7: Grammar sizes of the French to En-
glish system after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
6 German to English
German compounds were split using the script
provided with Moses.
For training the primary system, the target parse
trees were restructured before rule extraction by
right binarization. Since binarization strategies
increase the tree depth and number of nodes by
adding virtual non-terminals, we increased the ex-
traction parameters to: Rule Depth = 7, Node
Count = 100, Rule Size = 7. A thorough in-
vestigation of binarization methods for restructur-
ing Penn Treebank style trees was carried out by
Wang et al. (2007).
Table 8 shows BLEU scores for the baseline
system and two systems employing different bi-
narization strategies. Table 9 shows the result-
ing grammar sizes after filtering for the evaluation
sets. Results on the development set showed no
improvement when left binarization was used for
restructuring the trees, although the grammar size
increased significantly.
BLEU
system devtest test
baseline 26.2 27.2
+ right binarization (primary) 26.8 28.2
+ left binarization 26.3 -
Table 8: German to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
system devtest test
baseline 11,462,976 13,811,304
+ right binarization 24,851,982 29,133,910
+ left binarization 21,387,976 -
Table 9: Grammar sizes of the German to En-
glish systems after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
7 Hindi to English
English-Hindi has the least parallel training data
of this year?s language pairs. Out-of-vocabulary
(OOV) input words are therefore a comparatively
large source of translation error: in the devtest set
(newsdev2014) and filtered test set (newstest2014)
the average OOV rates are 1.08 and 1.16 unknown
words per sentence, respectively.
Assuming a significant fraction of OOV words
to be named entities and thus amenable to translit-
eration, we applied the post-processing translitera-
tion method described in Durrani et al. (2014) and
implemented in Moses. In brief, this is an unsuper-
vised method that i) uses EM to induce a corpus of
transliteration examples from the parallel training
data; ii) learns a monotone character-level phrase-
based SMT model from the transliteration corpus;
and iii) substitutes transliterations for OOVs in the
system output by using the monolingual language
model and other features to select between translit-
eration candidates.
5
Table 10 shows BLEU scores with and without
transliteration on the devtest and filtered test sets.
Due to a bug in the submitted system, the language
model trained on the HindEnCorp corpus was used
for transliteration candidate selection rather than
the full interpolated language model. This was
fixed subsequent to submission.
BLEU
system devtest test
baseline 12.9 14.7
+ transliteration (submission) 13.3 15.1
+ transliteration (fixed) 13.6 15.5
Table 10: Hindi to English results with and with-
out transliteration on the devtest (newsdev2014)
and test (newstest2014) sets.
Transliteration increased 1-gram precision from
48.1% to 49.4% for devtest and from 49.1% to
50.6% for test. Of the 2,913 OOV words in test,
938 (32.2%) of transliterations exactly match the
reference. Manual inspection reveals that there are
also many near matches. For instance, translitera-
tion produces Bernat Jackie where the reference is
Jacqui Barnat.
8 Russian to English
Compared to Hindi-English, the Russian-English
language pair has over six times as much parallel
data. Nonetheless, OOVs remain a problem: the
average OOV rates are approximately half those
5
This is the variant referred to as Method 2 in Dur-
rani et al. (2014).
211
of Hindi-English, at 0.47 and 0.51 unknown words
per sentence for the devtest (newstest2013) and fil-
tered test (newstest2014) sets, respectively. We
address this in part using the same transliteration
method as for Hindi-English.
Data sparsity issues for this language pair are
exacerbated by the rich inflectional morphology of
Russian. Many Russian word forms express gram-
matical distinctions that are either absent from En-
glish translations (like grammatical gender) or are
expressed by different means (like grammatical
function being expressed through syntactic config-
uration rather than case). We adopt the widely-
used approach of simplifying morphologically-
complex source forms to remove distinctions that
we believe to be redundant. Our method is simi-
lar to that of Weller et al. (2013) except that ours
is much more conservative (in their experiments,
Weller et al. (2013) found morphological reduc-
tion to harm translation indicating that useful in-
formation was likely to have been discarded).
We used TreeTagger (Schmid, 1994) to obtain
a lemma-tag pair for each Russian word. The tag
specifies the word class and various morphosyn-
tactic feature values. For example, the adjective
??????????????? (?republican?) gets the lemma-
tag pair ??????????????? + Afpfsnf, where
the code A indicates the word class and the re-
maining codes indicate values for the type, degree,
gender, number, case, and definiteness features.
Like Weller et al. (2013), we selectively re-
placed surface forms with their lemmas and re-
duced tags, reducing tags through feature dele-
tion. We restricted morphological reduction to ad-
jectives and verbs, leaving all other word forms
unchanged. Table 11 shows the features that
were deleted. We focused on contextual inflec-
tion, making the assumption that inflectional dis-
tinctions required by agreement alone were the
least likely to be useful for translation (since the
same information was marked elsewhere in the
sentence) and also the most likely to be the source
of ?spurious? variation.
Table 12 shows the BLEU scores for Russian-
English with transliteration and morphological re-
duction. The effect of transliteration was smaller
than for Hindi-English, as might be expected from
the lower baseline OOV rate. 1-gram precision in-
creased from 57.1% to 57.6% for devtest and from
62.9% to 63.6% for test. Morphological reduction
decreased the initial OOV rates by 3.5% and 4.1%
Adjective Verb
Type 7 Type 7
Degree 3 VForm 3
Gender 7 Tense 3
Number 7 Person 3
Case 7 Number 3
Definiteness 7 Gender 7
Voice 3
Definiteness 7
Aspect 3
Case 3
Table 11: Feature values that are retained (3)
or deleted (7) during morphological reduction of
Russian.
BLEU
system devtest test
baseline 23.3 29.7
+ transliteration 23.7 30.3
+ morphological reduction 23.8 30.3
Table 12: Russian to English results on the devtest
(newstest2013) and test (newstest2014) sets.
on the devtest and filtered test sets. After both
morphological and transliteration the 1-gram pre-
cisions for devtest and test were 57.7% and 63.8%.
9 Conclusion
We have described Edinburgh?s syntax-based sys-
tems in the WMT 2014 shared translation task.
Building upon the already-strong string-to-tree
systems developed for previous years? shared
translation tasks, we have achieved substantial im-
provements over our baseline setup: we improved
translation into German through target-side com-
pound splitting, morphosyntactic constraints, and
refinements to parse tree annotation; we have ad-
dressed unknown words using transliteration (for
Hindi and Russian) and morphological reduction
(for Russian); and we have improved our German-
English system through tree binarization.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658 (EU-BRIDGE).
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1_148717.
212
References
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montr?al, Canada, June. Association for
Computational Linguistics.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 413?417, Portland, Oregon, USA, June.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an Unsupervised Translit-
eration Model into Statistical Machine Translation.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the ACL (EACL 2014), Gothenburg,
Sweden, April. To appear.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In In Proc. NAACL/HLT 2013,
pages 644?648.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 224?234, Uppsala,
Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In HLT-NAACL ?04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-
44: Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational
Linguistics, pages 961?968, Morristown, NJ, USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646?655, Cambridge,
MA, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Morristown, NJ, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s Syntax-Based Machine Transla-
tion Systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, ACL ?03, pages
160?167, Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
213
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich and Beat Kunz. 2014. Zmorge: A Ger-
man Morphological Lexicon Extracted from Wik-
tionary. In Proceedings of the 9th International
Conference on Language Resources and Evaluation
(LREC 2014), Reykjavik, Iceland, May.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Intl. Conf. Spoken
Language Processing, Denver, Colorado, September
2002.
Wei Wang, Kevin Knight, Daniel Marcu, and Marina
Rey. 2007. Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy. In
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 746?754.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Rich?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 232?239, Sofia, Bul-
garia, August.
Philip Williams and Philipp Koehn. 2011. Agreement
Constraints for Statistical Machine Translation into
German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 217?226, Ed-
inburgh, Scotland, July.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388?394, Montr?al,
Canada, June.
214
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 94?102,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A CYK+ Variant for SCFG Decoding Without a Dot Chart
Rico Sennrich
School of Informatics
University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB
Scotland, UK
v1rsennr@staffmail.ed.ac.uk
Abstract
While CYK+ and Earley-style variants are
popular algorithms for decoding unbina-
rized SCFGs, in particular for syntax-
based Statistical Machine Translation, the
algorithms rely on a so-called dot chart
which suffers from a high memory con-
sumption. We propose a recursive vari-
ant of the CYK+ algorithm that elimi-
nates the dot chart, without incurring an
increase in time complexity for SCFG de-
coding. In an evaluation on a string-to-
tree SMT scenario, we empirically demon-
strate substantial improvements in mem-
ory consumption and translation speed.
1 Introduction
SCFG decoding can be performed with monolin-
gual parsing algorithms, and various SMT sys-
tems implement the CYK+ algorithm or a close
Earley-style variant (Zhang et al., 2006; Koehn et
al., 2007; Venugopal and Zollmann, 2009; Dyer
et al., 2010; Vilar et al., 2012). The CYK+ algo-
rithm (Chappelier and Rajman, 1998) generalizes
the CYK algorithm to n-ary rules by performing a
dynamic binarization of the grammar during pars-
ing through a so-called dot chart. The construction
of the dot chart is a major cause of space ineffi-
ciency in SCFG decoding with CYK+, and mem-
ory consumption makes the algorithm impractical
for long sentences without artificial limits on the
span of chart cells.
We demonstrate that, by changing the traver-
sal through the main parse chart, we can elimi-
nate the dot chart from the CYK+ algorithm at no
computational cost for SCFG decoding. Our algo-
rithm improves space complexity, and an empiri-
cal evaluation confirms substantial improvements
in memory consumption over the standard CYK+
algorithm, along with remarkable gains in speed.
This paper is structured as follows. As mo-
tivation, we discuss some implementation needs
and complexity characteristics of SCFG decoding
We then describe our algorithm as a variant of
CYK+, and finally perform an empirical evalua-
tion of memory consumption and translation speed
of several parsing algorithms.
2 SCFG Decoding
To motivate our algorithm, we want to highlight
some important differences between (monolin-
gual) CFG parsing and SCFG decoding.
Grammars in SMT are typically several orders
of magnitude larger than for monolingual parsing,
partially because of the large amounts of training
data employed to learn SCFGs, partially because
SMT systems benefit from using contextually rich
rules rather than only minimal rules (Galley et al.,
2006). Also, the same right-hand-side rule on the
source side can be associated with many trans-
lations, and different (source and/or target) left-
hand-side symbols. Consequently, a compact rep-
resentation of the grammar is of paramount impor-
tance.
We follow the implementation in the Moses
SMT toolkit (Koehn et al., 2007) which encodes
an SCFG as a trie in which each node represents
a (partial or completed) rule, and a node has out-
going edges for each possible continuation of the
rule in the grammar, either a source-side termi-
nal symbol or pair of non-terminal-symbols. If a
node represents a completed rule, it is also asso-
ciated with a collection of left-hand-side symbols
and the associated target-side rules and probabil-
ities. A trie data structure allows for an efficient
grammar lookup, since all rules with the same pre-
94
fix are compactly represented by a single node.
Rules are matched to the input in a bottom-up-
fashion as described in the next section. A single
rule or rule prefix can match the input many times,
either by matching different spans of the input, or
by matching the same span, but with different sub-
spans for its non-terminal symbols. Each produc-
tion is uniquely identified by a span, a grammar
trie node, and back-pointers to its subderivations.
The same is true for a partial production (dotted
item).
A key difference between monolingual parsing
and SCFG decoding, whose implications on time
complexity are discussed by Hopkins and Lang-
mead (2010), is that SCFG decoders need to con-
sider language model costs when searching for the
best derivation of an input sentence. This critically
affects the parser?s ability to discard dotted items
early. For CFG parsing, we only need to keep one
partial production per rule prefix and span, or k
for k-best parsing, selecting the one(s) whose sub-
derivations have the lower cost in case of ambigu-
ity. For SCFG decoding, the subderivation with
the higher local cost may be the globally better
choice after taking language model costs into ac-
count. Consequently, SCFG decoders need to con-
sider multiple possible productions for the same
rule and span.
Hopkins and Langmead (2010) provide a run-
time analysis of SCFG decoding, showing that
time complexity depends on the number of choice
points in a rule, i.e. rule-initial, consecutive, or
rule-final non-terminal symbols.
1
The number of
choice points (or scope) gives an upper bound to
the number of productions that exist for a rule and
span. If we define the scope of a grammar G to
be the maximal scope of all rules in the grammar,
decoding can be performed in O(n
scope(G)
) time.
If we retain all partial productions of the same rule
prefix, this also raises the space complexity of the
dot chart from O(n
2
) to O(n
scope(G)
).
2
Crucially, the inclusion of language model costs
both increases the space complexity of the dot
chart, and removes one of its benefits, namely the
ability to discard partial productions early without
risking search errors. Still, there is a second way
1
Assuming that there is a constant upper bound on the
frequency of each symbol in the input sentence, and on the
length of rules.
2
In a left-to-right construction of productions, a rule pre-
fix of a scope-x rule may actually have scope x + 1, namely
if the rule prefix ends in a non-terminal, but the rule does not.
it is a trap
1
2
3
4
5
6
7
8
9
10
it is a trap
1
2
3
4
5
6
7
8
9
10
Figure 1: Traditional CYK/CYK+ chart traversal
order (left) and proposed order (right).
in which a dot chart saves computational cost in
the CYK+ algorithm. The exact chart traversal or-
der is underspecified in CYK parsing, the only re-
quirement being that all subspans of a given span
need to be visited before the span itself. CYK+
or Earley-style parsers typically traverse the chart
bottom-up left-to-right, as in Figure 1 (left). The
same partial productions are visited throughout
time during chart parsing, and storing them in a
dot chart saves us the cost of recomputing them.
For example, step 10 in Figure 1 (left) re-uses par-
tial productions that were found in steps 1, 5 and
8.
We propose to specify the chart traversal order
to be right-to-left, depth-first, as illustrated on the
right-hand-side in Figure 1. This traversal order
groups all cells with the same start position to-
gether, and offers a useful guarantee. For each
span, all spans that start at a later position have
been visited before. Thus, whenever we generate
a partial production, we can immediately explore
all of its continuations, and then discard the par-
tial production. This eliminates the need for a dot
chart, without incurring any computational cost.
We could also say that the dot chart exists in a
minimal form with at most one item at a time, and
a space complexity of O(1). We proceed with a
description of the proposed algorithm, contrasted
with the closely related CYK+ algorithm.
3 Algorithm
3.1 The CYK+ algorithm
We here summarize the CYK+ algorithm, orig-
inally described by Chappelier and Rajman
(1998).
3
3
Chappelier and Rajman (1998) add the restriction that
rules may not be partially lexicalized; our description of
CYK+, and our own algorithm, do not place this restriction.
95
The main data structure during decoding is a
chart with one cell for each span of words in an
input string w
1
...w
n
of length n. Each cell T
i,j
corresponding to the span from w
i
to w
j
contains
two lists of items:
4
? a list of type-1 items, which are non-
terminals (representing productions).
? a list of type-2 items (dotted items), which
are strings of symbols ? that parse the sub-
string w
i
...w
j
and for which there is a rule in
the grammar of the form A ? ??, with ?
being a non-empty string of symbols. Such
an item may be completed into a type-1 item
at a future point, and is denoted ??.
For each cell (i, j) of the chart, we perform the
following steps:
1. if i = j, search for all rules A? w
i
?. If ? is
empty, add A to the type-1 list of cell (i, j);
otherwise, add w
i
? to the type-2 list of cell
(i, j).
2. if j > i, search for all combinations of a type-
2 item ?? in a cell (i, k) and a type-1 item B
in a cell (k+1, j) for which a rule of the form
A? ?B? exists.
5
If ? is empty, add the rule
to the type-1 list of cell (i, j); otherwise, add
?B? to the type-2 list of cell (i, j).
3. for each item B in the type-1 list of the cell
(i, j), if there is a rule of the form A ? B?,
and ? is non-empty, add B? to the type-2 list
of cell (i, j).
3.2 Our algorithm
The main idea behind our algorithm is that we can
avoid the need to store type-2 lists if we process
the individual cells in a right-to-left, depth-first or-
der, as illustrated in Figure 1. Rules are still com-
pleted left-to-right, but processing the rightmost
cells first allows us to immediately extend partial
productions into full productions instead of storing
them in memory.
We perform the following steps for each cell.
1. if i = j, if there is a rule A ? w
i
, add A to
the type-1 list of cell (i, j).
However, our description excludes non-lexical unary rules,
and epsilon rules.
4
For simplicity, we describe a monolingual acceptor.
5
To allow mixed-terminal rules, we also search for B =
w
j
if j = k + 1.
2. if j > i, search for all combinations of a type-
2 item ?? and a type-1 itemB in a cell (j, k),
with j ? k ? n for which a rule of the form
C ? ?B? exists. In the initial call, we allow
?? = A? for any type-1 item A in cell (i, j?
1).
6
If ? is empty, add C to the type-1 list of
cell (i, k); otherwise, recursively repeat this
step, using ?B? as ?? and k + 1 as j.
To illustrate the difference between the two al-
gorithms, let us consider the chart cell (1, 2), i.e.
the chart cell spanning the substring it is, in Fig-
ure 1, and let us assume the following grammar:
S ? NP V NP
NP ? ART NN
NP ? it
V ? is
ART ? a
NN ? trap
In both algorithms, we can combine the sym-
bols NP from cell (1, 1) and V from cell (2, 2) to
partially parse the rule S ? NP V NP. How-
ever, in CYK+, we cannot yet know if the rule can
be completed with a cell (3, x) containing symbol
NP, since the cell (3, 4) may be processed after cell
(1, 2). Thus, the partial production is stored in a
type-2 list for later processing.
In our algorithm, we require all cells (3, x) to
be processed before cell (1, 2), so we can imme-
diately perform a recursion with ? = NP V and
j = 3. In this recursive step, we search for a sym-
bol NP in any cell (3, x), and upon finding it in
cell (3, 4), add S as type-1 item to cell (1, 4).
We provide side-by-side pseudocode of the two
algorithms in Figure 2.
7
The algorithms are
aligned to highlight their similarity, the main dif-
ference between them being that type-2 items are
added to the dot chart in CYK+, and recursively
consumed in our variant. An attractive property
of the dynamic binarization in CYK+ is that each
partial production is constructed exactly once, and
can be re-used to find parses for cells that cover
a larger span. Our algorithm retains this property.
Note that the chart traversal order is different be-
tween the algorithms, as illustrated earlier in Fig-
ure 1. While the original CYK+ algorithm works
with either chart traversal order, our recursive vari-
6
To allow mixed-terminal rules, we also allow ?? = w
i
?
if j = i+ 1, and B = w
j
if k = j.
7
Some implementation details are left out for simplicity.
For instance, note that terminal and non-terminal grammar
trie edges can be kept separate to avoid iterating over all ter-
minal edges.
96
Algorithm 1: CYK+
Input: array w of length N
initialize chart[N,N ], collections[N,N ],
dotchart[N ]
root? root node of grammar trie
for span in [1..N]:
for i in [1..(N-span+1)]:
j? i+span-1
if i = j: #step 1
if (w[i], X) in arc[root]:
addToChart(X, i, j)
else:
for B in chart[i, j-1]: #step 3
if (B, X) in arc[root]:
if arc[X] is not empty:
add (X, j-1) to dotchart[i]
for (a, k) in dotchart[i]: #step 2
if k+1 = j:
if (w[j], X) in arc[a]:
addToChart(X, i, j)
for (B, X) in arc[a]:
if B in chart[k+1, j]:
addToChart(X, i, j)
chart[i, j] = cube_prune(collections[i, j])
def addToChart(trie node X, int i, int j):
if X has target collection:
add X to collections[i, j]
if arc[X] is not empty:
add (X, j) to dotchart[i]
Algorithm 2: recursive CYK+
Input: array w of length N
initialize chart[N,N ], collections[N,N ]
root? root node of grammar trie
for i in [N..1]:
for j in [i..N]:
if i = j: #step 1
if (w[i], X) in arc[root]:
addToChart(X, i, j, false)
else: #step 2
consume(root, i, i, j-1)
chart[i, j] = cube_prune(collections[i, j])
def consume(trie node a, int i, int j, int k):
unary ? i = j
if j = k:
if (w[j], X) in arc[a]:
addToChart(X, i, k, unary)
for (B, X) in arc[a]:
if B in chart[j, k]:
addToChart(X, i, k, unary)
def addToChart(trie node X, int i, int j, bool u):
if X has target collection and u is false:
add X to collections[i, j]
if arc[X] is not empty:
for k in [(j+1)..N]:
consume(X, i, j+1, k)
Figure 2: side-by-side pseudocode of CYK+ (left) and our algorithm (right). Our algorithm uses a new
chart traversal order and recursive consume function instead of a dot chart.
97
ant requires a right-to-left, depth-first chart traver-
sal.
With our implementation of the SCFG as a trie,
a type-2 is identified by a trie node, an array of
back-pointers to antecedent cells, and a span. We
distinguish between type-1 items before and after
cube pruning. Productions, or specifically the tar-
get collections and back-pointers associated with
them, are first added to a collections object, either
synchronously or asynchronously. Cube pruning
is always performed synchronously after all pro-
duction of a cell have been found. Thus, the choice
of algorithm does not change the search space in
cube pruning, or the decoder output. After cube
pruning, the chart cell is filled with a mapping
from a non-terminal symbol to an object that com-
pactly represents a collection of translation hy-
potheses and associated scores.
3.3 Chart Compression
Given a partial production for span (i, j), the num-
ber of chart cells in which the production can be
continued is linear to sentence length. The recur-
sive variant explicitly loops through all cells start-
ing at position j + 1, but this search also exists in
the original CYK+ in the form of the same type-2
item being re-used over time.
The guarantee that all cells (j+1, k) are visited
before cell (i, j) in the recursive algorithm allows
for a further optimization. We construct a com-
pressed matrix representation of the chart, which
can be incrementally updated in O(|V | ?n
2
), V be-
ing the vocabulary of non-terminal symbols. For
each start position and non-terminal symbol, we
maintain an array of possible end positions and
the corresponding chart entry, as illustrated in Ta-
ble 1. The array is compressed in that it does not
represent empty chart cells. Using the previous
example, instead of searching all cells (3, x) for
a symbol NP, we only need to retrieve the array
corresponding to start position 3 and symbol NP
to obtain the array of cells which can continue the
partial production.
While not affecting the time complexity of
the algorithm, this compression technique reduces
computational cost in two ways. If the chart is
sparsely populated, i.e. if the size of the arrays is
smaller than n ? j, the algorithm iterates through
fewer elements. Even if the chart is dense, we only
perform one chart look-up per non-terminal and
partial production, instead of n? j.
cell S NP V ART NN
(3,3) 0x81
(3,4) 0x86
start symbol compressed column
3 ART [(3, 0x81)]
3 NP [(4, 0x86)]
3 S,V,NN []
Table 1: Matrix representation of all chart en-
tries starting at position 3 (top), and equivalent
compressed representation (bottom). Chart entries
are pointers to objects that represent collection of
translation hypotheses and their scores.
4 Related Work
Our proposed algorithm is similar to the work
by Leermakers (1992), who describe a recursive
variant of Earley?s algorithm. While they discuss
function memoization, which takes the place of
charts in their work, as a space-time trade-off, a
key insight of our work is that we can order the
chart traversal in SCFG decoding so that partial
productions need not be tabulated or memoized,
without incurring any trade-off in time complex-
ity.
Dunlop et al. (2010) employ a similar matrix
compression strategy for CYK parsing, but their
method is different to ours in that they employ ma-
trix compression on the grammar, which they as-
sume to be in Chomsky Normal Form, whereas we
represent n-ary grammars as tries, and use matrix
compression for the chart.
An obvious alternative to n-ary parsing is the
use of binary grammars, and early SCFG mod-
els for SMT allowed only binary rules, as in the
hierarchical models by Chiang (2007)
8
, or bina-
rizable ones as in inversion-transduction grammar
(ITG) (Wu, 1997). Whether an n-ary rule can be
binarized depends on the rule-internal reorderings
between non-terminals; Zhang et al. (2006) de-
scribe a synchronous binarization algorithm.
Hopkins and Langmead (2010) show that the
complexity of parsing n-ary rules is determined
by the number of choice points, i.e. non-terminals
that are initial, consecutive, or final, since terminal
symbols in the rule constrain which cells are pos-
sible application contexts of a non-terminal sym-
bol. They propose pruning of the SCFG to rules
8
Specifically, Chiang (2007) allows at most two non-
terminals per rule, and no adjacent non-terminals on the
source side.
98
with at most 3 decision points, or scope 3, as an
alternative to binarization that allows parsing in
cubic time. In a runtime evaluation, SMT with
their pruned, unbinarized grammar offers a bet-
ter speed-quality trade-off than synchronous bi-
narization because, even though both have the
same complexity characteristics, synchronous bi-
narization increases both the overall number of
rules, and the number of non-terminals, which in-
creases the grammar constant. In contrast, Chung
et al. (2011) compare binarization and Earley-style
parsing with scope-pruned grammars, and find
Earley-style parsing to be slower. They attribute
the comparative slowness of Earley-style parsing
to the cost of building and storing the dot chart
during decoding, which is exactly the problem that
our paper addresses.
Williams and Koehn (2012) describe a parsing
algorithm motivated by Hopkins and Langmead
(2010) in which they store the grammar in a com-
pact trie with source terminal symbols or a generic
gap symbol as edge labels. Each path through this
trie corresponds to a rule pattern, and is associated
with the set of grammar rules that share the same
rule pattern. Their algorithm initially constructs a
secondary trie that records all rule patterns that ap-
ply to the input sentence, and stores the position of
matching terminal symbols. Then, chart cells are
populated by constructing a lattice for each rule
pattern identified in the initial step, and traversing
all paths through this lattice. Their algorithm is
similar to ours in that they also avoid the construc-
tion of a dot chart, but they construct two other
auxiliary structures instead: a secondary trie and
a lattice for each rule pattern. In comparison, our
algorithm is simpler, and we perform an empirical
comparison of the two in the next section.
5 Empirical Results
We empirically compare our algorithm to the
CYK+ algorithm, and the Scope-3 algorithm as
described by Williams and Koehn (2012), in a
string-to-tree SMT task. All parsing algorithms
are equivalent in terms of translation output, and
our evaluation focuses on memory consumption
and speed.
5.1 Data
For SMT decoding, we use the Moses toolkit
(Koehn et al., 2007) with KenLM for language
model queries (Heafield, 2011). We use training
algorithm n = 20 n = 40 n = 80
Scope-3 0.02 0.04 0.34
CYK+ 0.32 2.63 51.64
+ recursive 0.02 0.04 0.15
+ compression 0.02 0.04 0.15
Table 2: Peak memory consumption (in GB) of
string-to-tree SMT decoder for sentences of dif-
ferent length n with different parsing algorithms.
data from the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation (WMT) shared trans-
lation task, consisting of 4.5 million sentence pairs
of parallel data and a total of 120 million sen-
tences of monolingual data. We build a string-
to-tree translation system English?German, us-
ing target-side syntactic parses obtained with the
dependency parser ParZu (Sennrich et al., 2013).
A synchronous grammar is extracted with GHKM
rule extraction (Galley et al., 2004; Galley et al.,
2006), and the grammar is pruned to scope 3.
The synchronous grammar contains 38 million
rule pairs with 23 million distinct source-side
rules. We report decoding time for a random sam-
ple of 1000 sentences from the newstest2013/4
sets (average sentence length: 21.9 tokens), and
peak memory consumption for sentences of 20,
40, and 80 tokens. We do not report the time
and space required for loading the SMT models,
which is stable for all experiments.
9
The parsing
algorithm only accounts for part of the cost during
decoding, and the relative gains from optimizing
the parsing algorithm are highest if the rest of the
decoder is fast. For best speed, we use cube prun-
ing with language model boundary word grouping
(Heafield et al., 2013) in all experiments. We set
no limit to the maximal span of SCFG rules, but
only keep the best 100 productions per span for
cube pruning. The cube pruning limit itself is set
to 1000.
5.2 Memory consumption
Peak memory consumption for different sentence
lengths is shown in Table 2. For sentences of
length 80, we observe more than 50 GB in peak
memory consumption for CYK+, which makes
it impractical for long sentences, especially for
multi-threaded decoding. Our recursive variants
keep memory consumption small, as does the
9
The language model consumes 13 GB of memory, and
the SCFG 37 GB. We leave the task of compacting the gram-
mar to future research.
99
0 20 40 60 80
0
100
200
300
400
sentence length
d
e
c
o
d
i
n
g
t
i
m
e
(
s
e
c
o
n
d
s
)
Scope-3 parser
CYK+
+ recursive
+ compression
Figure 3: Decoding time per sentence as a func-
tion of sentence length for four parsing variants.
Regression curves use least squares fitting on cu-
bic function.
algorithm
length 80 random
parse total parse total
Scope-3 74.5 81.1 1.9 2.6
CYK+ 358.0 365.4 8.4 9.1
+ recursive 33.7 40.1 1.5 2.2
+ compression 15.0 21.2 1.0 1.7
Table 3: Parse time and total decoding time per
sentence (in seconds) of string-to-tree SMT de-
coder with different parsing algorithms.
Scope-3 algorithm. This is in line with our theoret-
ical expectation, since both algorithms eliminate
the dot chart, which is the costliest data structure
in the original CYK+ algorithm.
5.3 Speed
While the main motivation for eliminating the dot
chart was to reduce memory consumption, we also
find that our parsing variants are markedly faster
than the original CYK+ algorithm. Figure 3 shows
decoding time for sentences of different length
with the four parsing variants. Table 3 shows se-
lected results numerically, and also distinguishes
between total decoding time and time spent in the
parsing block, the latter ignoring the cost of cube
pruning and language model scoring. If we con-
sider parse time for sentences of length 80, we ob-
serve a speed-up by a factor of 24 between our
fastest variant (with recursion and chart compres-
sion), and the original CYK+.
The gains from chart compression over the re-
cursive variant ? a factor 2 reduction in parse time
for sentences of length 80 ? are attributable to a
reduction in the number of computational steps.
The large speed difference between CYK+ and
the recursive variant is somewhat more surpris-
ing, given the similarity of the two algorithms.
Profiling results show that the recursive variant is
not only faster because it saves the computational
overhead of creating and destroying the dot chart,
but that it also has a better locality of reference,
with markedly fewer CPU cache misses.
Time differences are smaller for shorter sen-
tences, both in terms of time spent parsing, and be-
cause the time spent outside of parsing is a higher
proportion of the total. Still, we observe a factor
5 speed-up in total decoding time on our random
translation sample from CYK+ to our fastest vari-
ant. We also observe speed-ups over the Scope-3
parser, ranging from a factor 5 speed-up (parsing
time on sentences of length 80) to a 50% speed-up
(total time on random translation sample). It is un-
clear to what extent these speed differences reflect
the cost of building the auxiliary data structures in
the Scope-3 parser, and how far they are due to
implementation details.
5.4 Rule prefix scope
For the CYK+ parser, the growth of both memory
consumption and decoding time exceeds our cubic
growth expectation. We earlier remarked that the
rule prefix of a scope-3 rule may actually be scope-
4 if the prefix ends in a non-terminal, but the rule
itself does not. Since this could increase space and
time complexity of CYK+ to O(n
4
), we did addi-
tional experiments in which we prune all scope-3
rules with a scope-4 prefix. This affected 1% of
all source-side rules in our model, and only had
a small effect on translation quality (19.76 BLEU
? 19.73 BLEU on newstest2013). With this addi-
tional pruning, memory consumption with CYK+
is closer to our theoretical expectation, with a peak
memory consumption of 23 GB for sentences of
length 80 (? 2
3
times more than for length 40).
We also observe reductions in parse time as shown
in Table 4. While we do see marked reductions
in parse time for all CYK+ variants, our recursive
variants maintain their efficiency advantage over
the original algorithm. Rule prefix scope is irrel-
evant for the Scope-3 parsing algorithm
10
, and its
10
Despite its name, the Scope-3 parsing algorithm al-
lows grammars of any scope, with a time complexity of
O(n
scope(G)
).
100
algorithm
length 80 random
full pruned full pruned
Scope-3 74.5 70.1 1.9 1.8
CYK+ 358.0 245.5 8.4 6.4
+ recursive 33.7 24.5 1.5 1.2
+ compression 15.0 10.5 1.0 0.8
Table 4: Average parse time (in seconds) of string-
to-tree SMT decoder with different parsing algo-
rithms, before and after scope-3 rules with scope-4
prefix have been pruned from grammar.
speed is only marginally affected by this pruning
procedure.
6 Conclusion
While SCFG decoders with dot charts are still
wide-spread, we argue that dot charts are only of
limited use for SCFG decoding. The core contri-
butions of this paper are the insight that a right-
to-left, depth-first chart traversal order allows for
the removal of the dot chart from the popular
CYK+ algorithm without incurring any computa-
tional cost for SCFG decoding, and the presen-
tation of a recursive CYK+ variant that is based
on this insight. Apart from substantial savings
in space complexity, we empirically demonstrate
gains in decoding speed. The new chart traversal
order also allows for a chart compression strategy
that yields further speed gains.
Our parsing algorithm does not affect the search
space or cause any loss in translation quality,
and its speed improvements are orthogonal to im-
provements in cube pruning (Gesmundo et al.,
2012; Heafield et al., 2013). The algorithmic
modifications to CYK+ that we propose are sim-
ple, but we believe that the efficiency gains of
our algorithm are of high practical importance for
syntax-based SMT. An implementation of the al-
gorithm has been released as part of the Moses
SMT toolkit.
Acknowledgements
I thank Matt Post, Philip Williams, Marcin
Junczys-Dowmunt and the anonymous reviewers
for their helpful suggestions and feedback. This
research was funded by the Swiss National Sci-
ence Foundation under grant P2ZHP1_148717.
References
Jean-C?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. In TAPD, pages 133?137.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Comput. Linguist., 33(2):201?228.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues Concerning Decoding with Syn-
chronous Context-free Grammar. In ACL (Short
Papers), pages 413?417. The Association for Com-
puter Linguistics.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2010. Reducing the grammar constant: an analysis
of CYK parsing efficiency. Technical report CSLU-
2010-02, OHSU.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A Decoder, Alignment, and Learning frame-
work for finite-state and context-free translation
models. In Proceedings of the Association for Com-
putational Linguistics (ACL).
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In HLT-NAACL ?04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-
44: Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational
Linguistics, pages 961?968, Sydney, Australia. As-
sociation for Computational Linguistics.
Andrea Gesmundo, Giorgio Satta, and James Hender-
son. 2012. Heuristic Cube Pruning in Linear Time.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers - Volume 2, ACL ?12, pages 296?300, Jeju
Island, Korea. Association for Computational Lin-
guistics.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping Language Model Boundary Words
to Speed K-Best Extraction from Hypergraphs. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 958?968, Atlanta, Georgia, USA.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, UK. Association for Computational Lin-
guistics.
Mark Hopkins and Greg Langmead. 2010. SCFG
Decoding Without Binarization. In EMNLP, pages
646?655.
101
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL-2007 Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Ren? Leermakers. 1992. A recursive ascent Earley
parser. Information Processing Letters, 41(2):87?
91, February.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Ashish Venugopal and Andreas Zollmann. 2009.
Grammar based statistical MT on Hadoop: An end-
to-end toolkit for large scale PSCFG based MT. The
Prague Bulletin of Mathematical Linguistics, 91:67?
78.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388?394, Montr?al,
Canada, June. Association for Computational Lin-
guistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In HLT-NAACL. The Association
for Computational Linguistics.
102
