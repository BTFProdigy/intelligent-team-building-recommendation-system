Computational Complexity of Statistical Machine Translation
Raghavendra Udupa U.
IBM India Research Lab
New Delhi
India
uraghave@in.ibm.com
Hemanta K. Maji
Dept. of Computer Science
University of Illinois at Urbana-Champaigne
hemanta.maji@gmail.com
Abstract
In this paper we study a set of prob-
lems that are of considerable importance
to Statistical Machine Translation (SMT)
but which have not been addressed satis-
factorily by the SMT research community.
Over the last decade, a variety of SMT
algorithms have been built and empiri-
cally tested whereas little is known about
the computational complexity of some of
the fundamental problems of SMT. Our
work aims at providing useful insights into
the the computational complexity of those
problems. We prove that while IBM Mod-
els 1-2 are conceptually and computation-
ally simple, computations involving the
higher (and more useful) models are hard.
Since it is unlikely that there exists a poly-
nomial time solution for any of these hard
problems (unless P = NP and P#P =
P), our results highlight and justify the
need for developing polynomial time ap-
proximations for these computations. We
also discuss some practical ways of deal-
ing with complexity.
1 Introduction
Statistical Machine Translation is a data driven
machine translation technique which uses proba-
bilistic models of natural language for automatic
translation (Brown et al, 1993), (Al-Onaizan et
al., 1999). The parameters of the models are
estimated by iterative maximum-likelihood train-
ing on a large parallel corpus of natural language
texts using the EM algorithm (Brown et al, 1993).
The models are then used to decode, i.e. trans-
late texts from the source language to the target
language 1 (Tillman, 2001), (Wang, 1997), (Ger-
mann et al, 2003), (Udupa et al, 2004). The
models are independent of the language pair and
therefore, can be used to build a translation sys-
tem for any language pair as long as a parallel
corpus of texts is available for training. Increas-
ingly, parallel corpora are becoming available
for many language pairs and SMT systems have
been built for French-English, German-English,
Arabic-English, Chinese-English, Hindi-English
and other language pairs (Brown et al, 1993), (Al-
Onaizan et al, 1999), (Udupa, 2004).
In SMT, every English sentence e is considered
as a translation of a given French sentence f with
probability Pr (f |e). Therefore, the problem of
translating f can be viewed as a problem of finding
the most probable translation of f :
e? = argmax
e
Pr(e|f) = argmax
e
Pr(f |e)P (e).
(1)
The probability distributions Pr(f |e) and
Pr(e) are known as translation model and lan-
guage model respectively. In the classic work on
SMT, Brown and his colleagues at IBM introduced
the notion of alignment between a sentence f and
its translation e and used it in the development of
translation models (Brown et al, 1993). An align-
ment between f = f1 . . . fm and e = e1 . . . el
is a many-to-one mapping a : {1, . . . ,m} ?
{0, . . . , l}. Thus, an alignment a between f and e
associates the french word fj to the English word
eaj 2. The number of words of f mapped to ei by
a is called the fertility of ei and is denoted by ?i.
Since Pr(f |e) = ?a Pr(f ,a|e), equation 1 can
1In this paper, we use French and English as the prototyp-
ical examples of source and target languages respectively.
2e0 is a special word called the null word and is used to
account for those words in f that are not connected by a to
any of the words of e.
25
be rewritten as follows:
e? = argmax
e
?
a
Pr(f ,a|e)Pr(e). (2)
Brown and his colleagues developed a series
of 5 translation models which have become to be
known in the field of machine translation as IBM
models. For a detailed introduction to IBM trans-
lation models, please see (Brown et al, 1993). In
practice, models 3-5 are known to give good re-
sults and models 1-2 are used to seed the EM it-
erations of the higher models. IBM model 3 is
the prototypical translation model and it models
Pr(f ,a|e) as follows:
P (f ,a|e) ? n
(
?0|
?l
i=1 ?i
)
?l
i=1 n (?i|ei)?i!
??mj=1 t
(
fj|eaj
)
??j: aj 6=0 d (j|i,m, l)
Table 1: IBM Model 3
Here, n(?|e) is the fertility model, t(f |e) is
the lexicon model and d(j|i,m, l) is the distortion
model.
The computational tasks involving IBM Models
are the following:
? Viterbi Alignment
Given the model parameters and a sentence
pair (f , e), determine the most probable
alignment between f and e.
a? = argmax
a
P (f ,a|e)
? Expectation Evaluation
This forms the core of model training via the
EM algorithm. Please see Section 2.3 for
a description of the computational task in-
volved in the EM iterations.
? Conditional Probability
Given the model parameters and a sentence
pair (f , e), compute P (f |e).
P (f |e) =
?
a
P (f ,a|e)
? Exact Decoding
Given the model parameters and a sentence f ,
determine the most probable translation of f .
e? = argmax
e
?
a
P (f ,a|e) P (e)
? Relaxed Decoding
Given the model parameters and a sentence f ,
determine the most probable translation and
alignment pair for f .
(e?,a?) = argmax
(e,a)
P (f ,a|e) P (e)
Viterbi Alignment computation finds applica-
tions not only in SMT but also in other areas
of Natural Language Processing (Wang, 1998),
(Marcu, 2002). Expectation Evaluation is the
soul of parameter estimation (Brown et al, 1993),
(Al-Onaizan et al, 1999). Conditional Proba-
bility computation is important in experimentally
studying the concentration of the probability mass
around the Viterbi alignment, i.e. in determining
the goodness of the Viterbi alignment in compar-
ison to the rest of the alignments. Decoding is
an integral component of all SMT systems (Wang,
1997), (Tillman, 2000), (Och et al, 2001), (Ger-
mann et al, 2003), (Udupa et al, 2004). Exact
Decoding is the original decoding problem as de-
fined in (Brown et al, 1993) and Relaxed Decod-
ing is the relaxation of the decoding problem typ-
ically used in practice.
While several heuristics have been developed
by practitioners of SMT for the computational
tasks involving IBM models, not much is known
about the computational complexity of these tasks.
In their seminal paper on SMT, Brown and his col-
leagues highlighted the problems we face as we go
from IBM Models 1-2 to 3-5(Brown et al, 1993)
3:
?As we progress from Model 1 to Model 5, eval-
uating the expectations that gives us counts be-
comes increasingly difficult. In Models 3 and 4,
we must be content with approximate EM itera-
tions because it is not feasible to carry out sums
over all possible alignments for these models. In
practice, we are never sure that we have found the
Viterbi alignment?.
However, neither their work nor the subsequent
research in SMT studied the computational com-
plexity of these fundamental problems with the
exception of the Decoding problem. In (Knight,
1999) it was proved that the Exact Decoding prob-
lem is NP-Hard when the language model is a bi-
gram model.
Our results may be summarized as follows:
3The emphasis is ours.
26
1. Viterbi Alignment computation is NP-Hard
for IBM Models 3, 4, and 5.
2. Expectation Evaluation in EM Iterations is
#P-Complete for IBM Models 3, 4, and 5.
3. Conditional Probability computation is
#P-Complete for IBM Models 3, 4, and 5.
4. Exact Decoding is #P-Hard for IBM Mod-
els 3, 4, and 5.
5. Relaxed Decoding is NP-Hard for IBM
Models 3, 4, and 5.
Note that our results for decoding are sharper
than that of (Knight, 1999). Firstly, we show that
Exact Decoding is #P-Hard for IBM Models 3-5
and not just NP-Hard. Secondly, we show that
Relaxed Decoding is NP-Hard for Models 3-5
even when the language model is a uniform dis-
tribution.
The rest of the paper is organized as follows.
We formally define all the problems discussed in
the paper (Section 2). Next, we take up each of the
problems discussed in this section and derive the
stated result for them (Section 3). After this, we
discuss the implications of our results (Section 4)
and suggest future directions (Section 5).
2 Problem Definition
Consider the functions f, g : ?? ? {0, 1}. We
say that g ?mp f (g is polynomial-time many-one
reducible to f ), if there exists a polynomial time
reduction r(.) such that g(x) = f(r(x)) for all
input instances x ? ??. This means that given a
machine to evaluate f(.) in polynomial time, there
exists a machine that can evaluate g(.) in polyno-
mial time. We say a function f is NP-Hard, if all
functions in NP are polynomial-time many-one
reducible to f . In addition, if f ? NP, then we
say that f is NP-Complete.
Also relevant to our work are counting func-
tions that answer queries such as ?how many com-
putation paths exist for accepting a particular in-
stance of input?? Let w be a witness for the ac-
ceptance of an input instance x and ?(x,w) be
a polynomial time witness checking function (i.e.
?(x,w) ? P). The function f : ?? ? N such that
f(x) =
?
w???
|w|?p(|x|)
?(x,w)
lies in the class #P, where p(.) is a polynomial.
Given functions f, g : ?? ? N, we say that g is
polynomial-time Turing reducible to f (i.e. g ?T
f ) if there is a Turing machine with an oracle for
f that computes g in time polynomial in the size
of the input. Similarly, we say that f is #P-Hard,
if every function in #P can be polynomial time
Turing reduced to f . If f is #P-Hard and is in
#P, then we say that f is #P-Complete.
2.1 Viterbi Alignment Computation
VITERBI-3 is defined as follows. Given the para-
meters of IBM Model 3 and a sentence pair (f , e),
compute the most probable alignment a? betwen f
and e:
a? = argmax
a
P (f ,a|e).
2.2 Conditional Probability Computation
PROBABILITY-3 is defined as follows. Given
the parameters of IBM Model 3, and a sen-
tence pair (f , e), compute the probability
P (f |e) =?a P (f ,a|e).
2.3 Expectation Evaluation in EM Iterations
(f, e)-COUNT-3, (?, e)-COUNT-3, (j, i,m, l)-
COUNT-3, 0-COUNT-3, and 1-COUNT-3 are de-
fined respectively as follows. Given the parame-
ters of IBM Model 3, and a sentence pair (f , e),
compute the following 4:
c(f |e; f , e) =
?
a
P (a|f , e)
?
j
?(f, fj)?(e, eaj ),
c(?|e; f , e) =
?
a
P (a|f , e)
?
i
?(?, ?i)?(e, ei),
c(j|i,m, l; f , e) =
?
a
P (a|f , e)?(i, aj),
c(0; f , e) =
?
a
P (a|f , e)(m? 2?0), and
c(1; f , e) =
?
a
P (a|f , e)?0.
2.4 Decoding
E-DECODING-3 and R-DECODING-3 are defined
as follows. Given the parameters of IBM Model 3,
4As the counts are normalized in the EM iteration, we can
replace P (a|f , e) by P (f ,a|e) in the Expectation Evaluation
tasks.
27
and a sentence f , compute its most probable trans-
lation according to the following equations respec-
tively.
e? = argmax
e
?
a
P (f ,a|e) P (e)
(e?,a?) = argmax
(e,a)
P (f ,a|e) P (e).
2.5 SETCOVER
Given a collection of sets C = {S1, . . . ,Sl} and
a set X ? ?li=1Si, find the minimum cardinality
subset C? of C such that every element in X be-
longs to at least one member of C?.
SETCOVER is a well-known NP-Complete
problem. If SETCOVER ?mp f , then f is NP-
Hard.
2.6 PERMANENT
Given a matrixM = [Mj,i]n?n whose entries are
either 0 or 1, compute the following:
perm(M) = ?pi
?n
j=1Mj,pij where pi is a per-
mutation of 1, . . . , n.
This problem is the same as that of counting the
number of perfect matchings in a bipartite graph
and is known to be #P-Complete (?). If PERMA-
NENT ?T f , then f is #P-Hard.
2.7 COMPAREPERMANENTS
Given two matrices A = [Aj,i]n?n and B =
[Bj,i]n?n whose entries are either 0 or 1, determine
which of them has a larger permanent. PERMA-
NENT is known to be Turing reducible to COM-
PAREPERMANENTS (Jerrum, 2005) and therefore,
if COMPAREPERMANENTS ?T f , then f is #P-
Hard.
3 Main Results
In this section, we present the main reductions
for the problems with Model 3 as the translation
model. Our reductions can be easily carried over
to Models 4?5 with minor modifications. In order
to keep the presentation of the main ideas simple,
we let the lexicon, distortion, and fertility models
to be any non-negative functions and not just prob-
ability distributions in our reductions.
3.1 VITERBI-3
We show that VITERBI-3 is NP-Hard.
Lemma 1 SETCOVER ?mp VITERBI-3.
Proof: We give a polynomial time many-one
reduction from SETCOVER to VITERBI-3. Given
a collection of sets C = {S1, . . . ,Sl} and a set
X ? ?li=1Si, we create an instance of VITERBI-3
as follows:
For each set Si ? C, we create a word ei (1 ? i ?
l). Similarly, for each element vj ? X we create
a word fj (1 ? j ? |X| = m). We set the model
parameters as follows:
t (fj|ei) =
{
1 if vj ? Si
0 otherwise
n (?|e) =
{
1
2?! if ? 6= 0
1 if ? = 0
d (j|i,m, l) = 1.
Now consider the sentences e =
e1 . . . el and f = f1 . . . fm.
P (f ,a|e) = n
(
?0|
l?
i=1
?i
) l?
i=1
n (?i|ei)?i!
?
m?
j=1
t
(
fj|eaj
) ?
j: aj 6=0
d (j|i,m, l)
=
l?
i=1
1
21??(?i,0)
We can construct a cover for X from the output
of VITERBI-3 by defining C? = {Si|?i > 0}. We
note that P (f ,a|e) = ?ni=1 121??(?i,0) = 2
?|C?|
.
Therefore, Viterbi alignment results in the mini-
mum cover for X.
3.2 PROBABILITY-3
We show that PROBABILITY-3 is #P-Complete.
We begin by proving the following:
Lemma 2 PERMANENT ?T PROBABILITY-3.
Proof: Given a 0, 1-matrix M =
[Mj, i]n?n, we define f = f1 . . . fn and e =
e1 . . . en where each ei and fj is distinct and set
the Model 3 parameters as follows:
t (fj|ei) =
{
1 if Mj,i = 1
0 otherwise
n (?|e) =
{
1 if ? = 1
0 otherwise
d (j|i, n, n) = 1.
28
Clearly, with the above parameter setting,
P (f ,a|e) = ?nj=1Mj, aj if a is a permutation
and 0 otherwise. Therefore,
P (f |e) =
?
a
P (f ,a|e)
=
?
a is a permutation
n?
j=1
Mj, aj = perm (M)
Thus, by construction, PROBABILITY-3 com-
putes perm (M). Besides, the construction con-
serves the number of witnesses. Hence, PERMA-
NENT ?T PROBABILITY-3.
We now prove that
Lemma 3 PROBABILITY-3 is in #P.
Proof: Let (f , e) be the input to
PROBABILITY-3. Let m and l be the lengths
of f and e respectively. With each alignment
a = (a1, a2, . . . , am) we associate a unique num-
ber na = a1a2 . . . am in base l + 1. Clearly,
0 ? na ? (l + 1)m ? 1. Let w be the binary
encoding of na. Conversely, with every binary
string w we can associate an alignment a if the
value of w is in the range 0, . . . , (l + 1)m ? 1. It
requires O (m log (l + 1)) bits to encode an align-
ment. Thus, given an alignment we can compute
its encoding and given the encoding we can com-
pute the corresponding alignment in time polyno-
mial in l and m. Similarly, given an encoding we
can compute P (f ,a|e) in time polynomial in l and
m. Now, if p(.) is a polynomial, then function
f (f , e) =
?
w?{0,1}?
|w|?p(|?f , e?|)
P (f ,a|e)
is in #P. Choose p (x) = dx log2 (x + 1)e.
Clearly, all alignments can be encoded using at
most p (| (f , e) |) bits. Therefore, if (f , e) com-
putes P (f |e) and hence, PROBABILITY-3 is in
#P.
It follows immediately from Lemma 2 and
Lemma 3 that
Theorem 1 PROBABILITY-3 is #P-Complete.
3.3 (f, e)-COUNT-3
Lemma 4 PERMANENT ?T (f, e)-COUNT-3.
Proof: The proof is similar to that of
Lemma 2. Let f = f1 f2 . . . fn f? and e =
e1 e2 . . . en e?. We set the translation model para-
meters as follows:
t (f |e) =
?
??
??
1 if f = fj, e = ei and Mj,i = 1
1 if f = f? and e = e?
0 otherwise.
The rest of the parameters are set as in Lemma 2.
Let A be the set of alignments a, such that an+1 =
n+1 and an1 is a permutation of 1, 2, . . . , n. Now,
c
(
f? |e?; f , e
)
=
?
a
P (f ,a|e)
n+1?
j=1
?(f? , fj)?(e?, eaj )
=
?
a?A
P (f ,a|e)
n+1?
j=1
?(f? , fj)?(e?, eaj )
=
?
a?A
P (f ,a|e)
=
?
a?A
n?
j=1
Mj, aj = perm (M) .
Therefore, PERMANENT ?T COUNT-3.
Lemma 5 (f, e)-COUNT-3 is in #P.
Proof: The proof is essentially the same as
that of Lemma 3. Note that given an encoding w,
P (f ,a|e)?mj=1 ? (fj, f) ?
(
eaj , e
)
can be evalu-
ated in time polynomial in |(f , e)|.
Hence, from Lemma 4 and Lemma 5, it follows
that
Theorem 2 (f, e)-COUNT-3 is #P-Complete.
3.4 (j, i,m, l)-COUNT-3
Lemma 6 PERMANENT ?T (j, i,m, l)-COUNT-
3.
Proof: We proceed as in the
proof of Lemma 4 with some modifica-
tions. Let e = e1 . . . ei?1e?ei . . . en and
f = f1 . . . fj?1f? fj . . . fn. The parameters
are set as in Lemma 4. Let A be the set of
alignments, a, such that a is a permutation of
1, 2, . . . , (n + 1) and aj = i. Observe that
P (f ,a|e) is non-zero only for the alignments in
A. It follows immediately that with these para-
meter settings, c(j|i, n, n; f , e) = perm (M) .
Lemma 7 (j, i,m, l)-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 3 (j, i,m, l)-COUNT-3 is #P-
Complete.
29
3.5 (?, e)-COUNT-3
Lemma 8 PERMANENT ?T (?, e)-COUNT-3.
Proof: Let e = e1 . . . ene? and f =
f1 . . . fn
k
? ?? ?
f? . . . f? . Let A be the set of alignments
for which an1 is a permutation of 1, 2, . . . , n and
an+kn+1 =
k
? ?? ?
(n + 1) . . . (n + 1). We set
n (?|e) =
?
??
??
1 if ? = 1 and e 6= e?
1 if ? = k and e = e?
0 otherwise.
The rest of the parameters are set as in Lemma 4.
Note that P (f ,a|e) is non-zero only for the align-
ments in A. It follows immediately that with these
parameter settings, c(k|e?; f , e) = perm (M) .
Lemma 9 (?, e)-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 4 (?, e)-COUNT-3 is #P-Complete.
3.6 0-COUNT-3
Lemma 10 PERMANENT ?T 0-COUNT-3.
Proof: Let e = e1 . . . en and f = f1 . . . fnf? .
Let A be the set of alignments, a, such that an1 is
a permutation of 1, . . . , n and an+1 = 0. We set
t (f |e) =
?
??
??
1 if f = fj, e = ei and Mj, i = 1
1 if f = f? and e = NULL
0 otherwise.
The rest of the parameters are set as in Lemma 4.
It is easy to see that with these settings, c(0;f ,e)(n?2) =
perm (M) .
Lemma 11 0-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 5 0-COUNT-3 is #P-Complete.
3.7 1-COUNT-3
Lemma 12 PERMANENT ?T 1-COUNT-3.
Proof: We set the parameters as in
Lemma 10. It follows immediately that
c(1; f , e) = perm (M) .
Lemma 13 1-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 6 1-COUNT-3 is #P-Complete.
3.8 E-DECODING-3
Lemma 14 COMPAREPERMANENTS ?T E-
DECODING-3
Proof: Let M and N be the two 0-1 matri-
ces. Let f = f1f2 . . . fn, e(1) = e(1)1 e
(1)
2 . . . e
(1)
n
and e(2) = e(2)1 e
(2)
2 . . . e
(2)
n . Further, let e(1) and
e(2) have no words in common and each word
appears exactly once. By setting the bigram lan-
guage model probabilities of the bigrams that oc-
cur in e(1) and e(2) to 1 and all other bigram prob-
abilities to 0, we can ensure that the only trans-
lations considered by E-DECODING-3 are indeed
e(1) and e(2) and P
(
e(1)
)
= P
(
e(2)
)
= 1. We
then set
t (f |e) =
?
??
??
1 if f = fj, e = e(1)i and Mj,i = 1
1 if f = fj, e = e(2)i and Nj,i = 1
0 otherwise
n (?|e) =
{
1 ? = 1
0 otherwise
d (j|i, n, n) = 1.
Now, P
(
f |e(1)
)
= perm (M), and P
(
f |e(2)
)
=
perm (N ). Therefore, given the output of E-
DECODING-3 we can find out which of M and
N has a larger permanent.
Hence E-DECODING-3 is #P?Hard.
3.9 R-DECODING-3
Lemma 15 SETCOVER ?mp R-DECODING-3
Proof: Given an instance of SETCOVER, we
set the parameters as in the proof of Lemma 1 with
the following modification:
n (?|e) =
{
1
2?! if ? > 0
0 otherwise.
Let e be the optimal translation obtained by solv-
ing R-DECODING-3. As the language model is
uniform, the exact order of the words in e is not
important. Now, we observe that:
? e contains words only from the set
{e1, e2, . . . , el}. This is because, there can-
not be any zero fertility word as n (0|e) = 0
and the only words that can have a non-zero
fertility are from {e1, e2, . . . , el} due to the
way we have set the lexicon parameters.
? No word occurs more than once in e. Assume
on the contrary that the word ei occurs k > 1
30
times in e. Replace these k occurrences by
only one occurrence of ei and connect all the
words connected to them to this word. This
would increase the score of e by a factor of
2k?1 > 1 contradicting the assumption on
the optimality of e.
As a result, the only candidates for e are subsets of
{e1, e2, . . . , el} in any order. It is now straight for-
ward to verify that a minimum set cover can be re-
covered from e as shown in the proof of Lemma 1.
3.10 IBM Models 4 and 5
The reductions are for Model 3 can be easily ex-
tended to Models 4 and 5. Thus, we have the fol-
lowing:
Theorem 7 Viterbi Alignment computation is
NP-Hard for IBM Models 3? 5.
Theorem 8 Expectation Evaluation in the EM
Steps is #P-Complete for IBM Models 3? 5.
Theorem 9 Conditional Probability computation
is #P-Complete for IBM Models 3? 5.
Theorem 10 Exact Decoding is #P-Hard for
IBM Models 3? 5.
Theorem 11 Relaxed Decoding is NP-Hard for
IBM Models 3? 5 even when the language model
is a uniform distribution.
4 Discussion
Our results answer several open questions on the
computation of Viterbi Alignment and Expectation
Evaluation. Unless P = NP and P#P = P,
there can be no polynomial time algorithms for
either of these problems. The evaluation of ex-
pectations becomes increasingly difficult as we go
from IBM Models 1-2 to Models 3-5 exactly be-
cause the problem is #P-Complete for the latter
models. There cannot be any trick for IBM Mod-
els 3-5 that would help us carry out the sums over
all possible alignments exactly. There cannot exist
a closed form expression (whose representation is
polynomial in the size of the input) for P (f |e) and
the counts in the EM iterations for Models 3-5.
It should be noted that the computation of
Viterbi Alignment and Expectation Evaluation is
easy for Models 1-2. What makes these computa-
tions hard for Models 3-5? To answer this ques-
tion, we observe that Models 1-2 lack explicit fer-
tility model unlike Models 3-5. In the former mod-
els, fertility probabilities are determined by the
lexicon and alignment models. Whereas, in Mod-
els 3-5, the fertility model is independent of the
lexicon and alignment models. It is precisely this
freedom that makes computations on Models 3-5
harder than the computations on Models 1-2.
There are three different ways of dealing with
the computational barrier posed by our problems.
The first of these is to develop a restricted fertil-
ity model that permits polynomial time computa-
tions. It remains to be found what kind of parame-
terized distributions are suitable for this purpose.
The second approach is to develop provably good
approximation algorithms for these problems as is
done with many NP-Hard and #P-Hard prob-
lems. Provably good approximation algorithms
exist for several covering problems including Set
Cover and Vertex Cover. Viterbi Alignment is itself
a special type of covering problem and it remains
to be seen whether some of the techniques devel-
oped for covering algorithms are useful for finding
good approximations to Viterbi Alignment. Sim-
ilarly, there exist several techniques for approxi-
mating the permanent of a matrix. It needs to be
explored if some of these ideas can be adapted for
Expectation Evaluation.
As the third approach to deal with complex-
ity, we can approximate the space of all possi-
ble (l + 1)m alignments by an exponentially large
subspace. To be useful such large subspaces
should also admit optimal polynomial time al-
gorithms for the problems we have discussed in
this paper. This is exactly the approach taken
by (Udupa, 2005) for solving the decoding and
Viterbi alignment problems. They show that very
efficient polynomial time algorithms can be de-
veloped for both Decoding and Viterbi Alignment
problems. Not only the algorithms are prov-
ably superior in a computational complexity sense,
(Udupa, 2005) are also able to get substantial im-
provements in BLEU and NIST scores over the
Greedy decoder.
5 Conclusions
IBM models 3-5 are widely used in SMT. The
computational tasks discussed in this work form
the backbone of all SMT systems that use IBM
models. We believe that our results on the compu-
tational complexity of the tasks in SMT will result
in a better understanding of these tasks from a the-
oretical perspective. We also believe that our re-
sults may help in the design of effective heuristics
31
for some of these tasks. A theoretical analysis of
the commonly employed heuristics will also be of
interest.
An open question in SMT is whether there ex-
ists closed form expressions (whose representation
is polynomial in the size of the input) for P (f |e)
and the counts in the EM iterations for models 3-5
(Brown et al, 1993). For models 1-2, closed form
expressions exist for P (f |e) and the counts in the
EM iterations for models 3-5. Our results show
that there cannot exist a closed form expression
(whose representation is polynomial in the size of
the input) for P (f |e) and the counts in the EM
iterations for Models 3-5 unless P = NP.
References
K. Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational
Linguistics.
Brown, P. et al 1993. The Mathematics of Machine
Translation: Parameter Estimation. Computational
Linguistics, 2(19):263?311.
Al-Onaizan, Y. et al 1999. Statistical Machine Trans-
lation: Final Report. JHU Workshop Final Report.
R. Udupa, and T. Faruquie. 2004. An English-Hindi
Statistical Machine Translation System. Proceed-
ings of the 1st IJCNLP.
Y. Wang, and A. Waibel. 1998. Modeling with Struc-
tures in Statistical Machine Translation. Proceed-
ings of the 36th ACL.
D. Marcu and W. Wong. 2002. A Phrase-Based, Joint
Probability Model for Statistical Machine Transla-
tion. Proceedings of the EMNLP.
L. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?
201.
M. Jerrum. 2005. Personal communication.
C. Tillman. 2001. Word Re-ordering and Dynamic
Programming based Search Algorithm for Statistical
Machine Translation. Ph.D. Thesis, University of
Technology Aachen 42?45.
Y. Wang and A. Waibel. 2001. Decoding algorithm in
statistical machine translation. Proceedings of the
35th ACL 366?372.
C. Tillman and H. Ney. 2000. Word reordering and
DP-based search in statistical machine translation.
Proceedings of the 18th COLING 850?856.
F. Och, N. Ueffing, and H. Ney. 2000. An efficient A*
search algorithm for statistical machine translation.
Proceedings of the ACL 2001 Workshop on Data-
Driven Methods in Machine Translation 55?62.
U. Germann et al 2003. Fast Decoding and Optimal
Decoding for Machine Translation. Artificial Intel-
ligence.
R. Udupa, H. Maji, and T. Faruquie. 2004. An Al-
gorithmic Framework for the Decoding Problem in
Statistical Machine Translation. Proceedings of the
20th COLING.
R. Udupa and H. Maji. 2005. Theory of Alignment
Generators and Applications to Statistical Machine
Translation. Proceedings of the 19th IJCAI.
32
An Algorithmic Framework for the Decoding Problem in
Statistical Machine Translation
Raghavendra Udupa U Tanveer A Faruquie
IBM India Research Lab
Block-1A, IIT, Hauz Khas
New Delhi - 110 016
India
{uraghave, ftanveer}@in.ibm.com
Hemanta K Maji
Dept. of Computer Science
and Engineering, IIT Kanpur
Kanpur - 208 016
India,
hkmaji@iitk.ac.in
Abstract
The decoding problem in Statistical Ma-
chine Translation (SMT) is a computation-
ally hard combinatorial optimization prob-
lem. In this paper, we propose a new al-
gorithmic framework for solving the decod-
ing problem and demonstrate its utility. In
the new algorithmic framework, the decod-
ing problem can be solved both exactly and
approximately. The key idea behind the
framework is the modeling of the decod-
ing problem as one that involves alternat-
ing maximization of two relatively simpler
subproblems. We show how the subprob-
lems can be solved efficiently and how their
solutions can be combined to arrive at a so-
lution for the decoding problem. A fam-
ily of provably fast decoding algorithms can
be derived from the basic techniques under-
lying the framework and we present a few
illustrations. Our first algorithm is a prov-
ably linear time search algorithm. We use
this algorithm as a subroutine in the other
algorithms. We believe that decoding algo-
rithms derived from our framework can be
of practical significance.
1 Introduction
Decoding is one of the three fundamental prob-
lems in classical SMT (translation model and
language model being the other two) as pro-
posed by IBM in the early 1990?s (Brown et al,
1993). In the decoding problem we are given the
language and translation models and a source
language sentence and are asked to find the
most probable translation for the sentence. De-
coding is a discrete optimization problem whose
search space is prohibitively large. The chal-
lenge is, therefore, in devising a scheme to ef-
ficiently search the solution space for the solu-
tion.
Decoding is known to belong to a class of com-
putational problems popularly known as NP-
hard problems (Knight, 1999). NP-hard prob-
lems are known to be computationally hard and
have eluded polynomial time algorithms (Garey
and Johnson, 1979). The first algorithms for
the decoding problem were based on what is
known among the speech recognition commu-
nity as stack-based search (Jelinek, 1969). The
original IBM solution to the decoding prob-
lem employed a restricted stack-based search
(Berger et al, 1996). This idea was further ex-
plored by Wang and Waibel (Wang and Waibel,
1997) who developed a faster stack-based search
algorithm. In perhaps the first work on the
computational complexity of Decoding, Kevin
Knight showed that the problem is closely re-
lated to the more famous Traveling Salesman
problem (TSP). Independently, Christoph Till-
man adapted the Held-Karp dynamic program-
ming algorithm for TSP (Held and Karp, 1962)
to Decoding (Tillman, 2001). The original Held-
Karp algorithm for TSP is an exponential time
dynamic programming algorithm and Tillman?s
adaptation to Decoding has a prohibitive com-
plexity of O
(
l3m22m
) ? O (m52m) (where m
and l are the lengths of the source and tar-
get sentences respectively). Tillman and Ney
showed how to improve the complexity of the
Held-Karp algorithm for restricted word re-
ordering and gave a O
(
l3m4
) ? O (m7) algo-
rithm for French-English translation (Tillman
and Ney, 2000). An optimal decoder based on
the well-known A? heuristic was implemented
and benchmarked in (Och et al, 2001). Since
optimal solution can not be computed for prac-
tical problem instances in a reasonable amount
of time, much of recent work has focused on
good quality suboptimal solutions. An O
(
m6
)
greedy search algorithm was developed (Ger-
mann et al, 2003) whose complexity was re-
duced further to O
(
m2
)
(Germann, 2003).
In this paper, we propose an algorithmic
framework for solving the decoding problem and
show that several efficient decoding algorithms
can be derived from the techniques developed in
the framework. We model the search problem
as an alternating search problem. The search,
therefore, alternates between two subproblems,
both of which are much easier to solve in prac-
tice. By breaking the decoding problem into
two simpler search problems, we are able to pro-
vide handles for solving the problem efficiently.
The solutions of the subproblems can be com-
bined easily to arrive at a solution for the orig-
inal problem. The first subproblem fixes an
alignment and seeks the best translation with
that alignment. Starting with an initial align-
ment between the source sentence and its trans-
lation, the second subproblem asks for an im-
proved alignment. We show that both of these
problems are easy to solve and provide efficient
solutions for them. In an iterative search for a
local optimal solution, we alternate between the
two algorithms and refine our solution.
The algorithmic framework provides handles
for solving the decoding problem at several lev-
els of complexity. At one extreme, the frame-
work yields an algorithm for solving the decod-
ing problem optimally. At the other extreme, it
yields a provably linear time algorithm for find-
ing suboptimal solutions to the problem. We
show that the algorithmic handles provided by
our framework can be employed to develop a
very fast decoding algorithm which finds good
quality translations. Our fast suboptimal search
algorithms can translate sentences that are 50
words long in about 5 seconds on a simple com-
puting facility.
The rest of the paper is devoted to the devel-
opment and discussion of our framework. We
start with a mathematical formulation of the
decoding problem (Section 2). We then develop
the alternating search paradigm and use it to
develop several decoding algorithms (Section 3).
Next, we demonstrate the practical utility of our
algorithms with the help of results from our ini-
tial experiments (Section 5).
2 Decoding
The decoding problem in SMT is one of finding
the most probable translation e? in the target
language of a given source language sentence f
in accordance with the Fundamental Equation
of SMT (Brown et al, 1993):
e? = argmaxe Pr(f |e)Pr(e). (1)
In the remainder of this paper we will refer
to the search problem specified by Equation 1
as STRICT DECODING.
Rewriting the translation model Pr(f |e) as
?
a Pr(f ,a|e), where a denotes an alignment
between the source sentence and the target sen-
tence, the problem can be restated as:
e? = argmaxe
?
a
Pr(f ,a|e)Pr(e). (2)
Even when the translation model Pr(f |e) is
as simple as IBM Model 1 and the language
model Pr(e) is a bigram language model, the
decoding problem is NP-hard (Knight, 1999).
Unless P = NP, there is no hope of an efficient
algorithm for the decoding problem. Since the
Fundamental Equation of SMT does not yield
an easy handle to design a solution (exact or
even an approximate one) for the problem, most
researchers have instead worked on solving the
following relatively simpler problem (Germann
et al, 2003):
(e?, a?) = argmax(e,a) Pr(f ,a|e)Pr(e). (3)
We call the search problem specified
by Equation 3 as RELAXED DECODING.
Note that RELAXED DECODING relaxes
STRICT DECODING to a joint optimization
problem. The search in RELAXED DECODING
is for a pair (e?, a?). While RELAXED DECODING
is simpler than STRICT DECODING, it is also,
unfortunately, NP hard for even IBM Model
1 and Bigram language model. Therefore, all
practical solutions to RELAXED DECODING
have focused on finding suboptimal solutions.
The challenge is in devising fast search strate-
gies to find good suboptimal solutions. Table 1
lists the combinatorial optimization problems
in the domain of decoding.
In the remainder of the paper,m and l denote
the length of the source language sentence and
its translation respectively.
3 Framework for Decoding
We begin with a couple of useful observations
about the decoding problem. Although decep-
tively simple, these observations are very cru-
cial for developing our framework. They are
the source for algorithmic handles for breaking
the decoding problem into two relatively eas-
ier search problems. The first of these observa-
tions concerns with solving the problem when
we know in advance the mapping between the
source and target sentences. This leads to the
development of an extremely simple algorithm
for decoding when the alignment is known (or
Problem Search
STRICT DECODING e? = argmaxePr(f |e)Pr(e)
RELAXED DECODING (e?, a?) = argmax(e,a)Pr(f ,a|e)Pr(e)
FIXED ALIGNMENT DECODING e? = argmaxePr(f , a?|e)Pr(e)
VITERBI ALIGNMENT a? = argmaxaPr(f ,a|e?)
Table 1: Combinatorial Search Problems in Decoding
can be guessed). Our second observation is on
finding a better alignment between the source
and target sentences starting with an initial
(possibly suboptimal) alignment. The insight
provided by the two observations are employed
in building a powerful algorithmic framework.
3.1 Handles for attacking the Decoding
Problem
Our goal is to arrive at algorithmic handles
for attacking RELAXED DECODING. In this sec-
tion, we make couple of useful observations and
develop algorithmic handles from the insight
provided by them. The first of the two observa-
tions is:
Observation 1 For a given target length l and
a given alignment a? that maps source words to
target positions, it is easy to compute the opti-
mal target sentence e?.
e? = argmaxe Pr(f , a?|e)Pr(e). (4)
Let us call the search problem specified by
Equation 4 as FIXED ALIGNMENT DECODING.
What Observation 1 is saying is that once the
target sentence length and the source to tar-
get mapping is fixed, the optimal target sen-
tence (with the specified target length and
alignment) can be computed efficiently. As
we will show later, the optimal solution for
FIXED ALIGNMENT DECODING can be com-
puted in O (m) time for IBM models 1-5 using
dynamic programming. As we can always guess
an alignment (as is the case with many decoding
algorithms in the literature), the above observa-
tion provides an algorithmic handle for finding
suboptimal solutions for RELAXED DECODING.
Our second observation is on computing the
optimal alignment between the source sentence
and the target sentence.
Observation 2 For a given target sentence e?,
it is easy to compute the optimal alignment a?
that maps the source words to the target words.
a? = argmaxa Pr(f ,a|e?). (5)
It is easy to determine the optimal (Viterbi)
alignment between the source sentence and its
translation. In fact, for IBM models 1 and 2,
the Viterbi alignment can be computed using a
straight forward algorithm in O (ml) time. For
higher models, an approximate Viterbi align-
ment can be computed iteratively by an iter-
ative procedure called local search. In each it-
eration of local search, we look in the neighbor-
hood of the current best alignment for a better
alignment (Brown et al, 1993). The first itera-
tion can start with any arbitrary alignment (say
the Viterbi alignment of Model 2). It is possi-
ble to implement one iteration of local search in
O (ml) time. Typically, the number of iterations
is bounded in practice by O (m), and therefore,
local search takes O
(
m2l
)
time.
Our framework is not strictly dependent on
the computation of an optimal alignment. Any
alignment which is better than the current
alignment is good enough for it to work. It is
straight forward to find one such alignment us-
ing restricted swaps and moves in O (m) time.
In the remainder of this paper, we use the term
Viterbi to denote any linear time algorithm for
computing an improved alignment between the
source sentence and its translation.
3.2 Illustrative Algorithms
In this section, we show how the handles pro-
vided by the above two observations can be em-
ployed to solve RELAXED DECODING. The two
handles are in some sense complementary to
each other. When the alignment is known, we
can efficiently determine the optimal translation
with that alignment. On the other hand, when
the translation is known, we can efficiently de-
termine a better alignment. Therefore, we can
use one to improve the other. We begin with the
following simple linear time decoding algorithm
which is based on the first observation.
Algorithm NaiveDecode
Input: Source language sentence f of length
m > 0.
Optional Inputs: Target sentence length l,
alignment a? between the source words and tar-
get positions.
Output: Target language sentence e? of length
l.
1. If l is not specified, let l = m.
2. If an alignment is not specified, guess some
alignment a?.
3. Compute the optimal translation e? by solv-
ing FIXED ALIGNMENT DECODING,
i.e., e? = argmaxe Pr(f , a?|e)Pr(e).
4. return e?.
When the length of the translation is not
specified, NaiveDecode assumes that the trans-
lation is of the same length as the source sen-
tence. If an alignment that maps the source
words to target positions is not specified, the
algorithm guesses an alignment a? (a? can be the
trivial alignment that maps the source word fj
to target position j, that is, a?j = j, or can
be guessed more intelligently). It then com-
putes the optimal translation for the source
sentence f , with the length of the target sen-
tence and the alignment between the source and
the target sentences kept fixed to l and a? re-
spectively, by maximizing Pr(f , a?|e)Pr(e). As
FIXED ALIGNMENT DECODING can be solved
in O (m) time, NaiveDecode takes only O(m)
time.
The value of NaiveDecode lies not in itself per
se, but in its instrumental role in designing more
superior algorithms. The power of NaiveDecode
can be demonstrated with the following optimal
algorithm for RELAXED DECODING.
Algorithm NaiveOptimalDecode
Input: Source language sentence f of length
m > 0.
Output: Target language sentence e? of length
l, m2 ? l ? 2m.
1. Let e? = null and a? = null.
2. For each l = m2 , . . . , 2m do
3. For each alignment a between the source
words and the target positions do
(a) Let e = NaiveDecode(f , l,a).
(b) If Pr (f , e,a) > Pr (f , e?, a?) then
i. e? = e
ii. a? = a.
4. return (e?, a?).
NaiveOptimalDecode considers various tar-
get lengths and all possible alignments be-
tween the source words and the target posi-
tions. For each target length l and alignment
a it employs NaiveDecode to find the best so-
lution. There are (l + 1)m candidate align-
ments for a target length l and O (m) can-
didate target lengths. Therefore, NaiveOp-
timalDecode explores ? (m(l + 1)m) alignments.
For each of these candidate alignments, it
makes a call to NaiveDecode. The time com-
plexity of NaiveOptimalDecode is, therefore,
O
(
m2(l + 1)m
)
. Although an exponential time
algorithm, it can compute the optimal solution
for RELAXED DECODING.
With NaiveDecode and NaiveOptimalDecode
we have demonstrated the power of the algo-
rithmic handle provided by Observation 1. It
is important to note that these two algorithms
are at the two extremities of the spectrum.
NaiveDecode is a linear time decoding algorithm
that computes a suboptimal solution for RE-
LAXED DECODING while NaiveOptimalDecode
is an exponential time algorithm that computes
the optimal solution. What we want are algo-
rithms that are close to NaiveDecode in com-
plexity and to NaiveOptimalDecode in qual-
ity. It is possible to reduce the complexity of
NaiveOptimalDecode significantly by carefully
reducing the number of alignments that are ex-
amined. Instead of examining all ?(m(l+1)m)
alignments, if we examine only a small num-
ber, say g (m), alignments in NaiveOptimalDe-
code, we can find a solution in O (mg (m)) time.
In the next section, we show how to restrict
the search to only a small number of promis-
ing alignments.
3.3 Alternating Maximization
We now show how to use the two algorithmic
handles to come up with a fast search paradigm.
We alternate between searching the best trans-
lation given an alignment and searching the
best alignment given a translation. Since the
two subproblems are complementary, they can
be used to improve the solution computed by
one another by alternating between the two
problems.
Algorithm AlternatingSearch
Input: Source language sentence f of length
m > 0.
Output: Target language sentence e(o) of
length l (m/2 ? l ? 2m).
1. Let e(o) = null and a(o) = null.
2. For each l = m/2, . . . , 2m do
(a) Let e = null and a = null.
(b) While there is improvement in solution
do
i. Let e = NaiveDecode (f , l,a).
ii. Let a? = V iterbi (f , e).
(c) If Pr (f , e,a) > Pr (f , e(o),a(o)) then
i. e(o) = e
ii. a(o) = a.
3. return e(o).
AlternatingSearch searches for a good trans-
lation by varying the length of the tar-
get sentence. For a sentence length l,
the algorithm finds a translation of length
l and then iteratively improves the trans-
lation. In each iteration it solves two
subproblems: FIXED ALIGNMENT DECODING
and VITERBI ALIGNMENT. The input to each
iteration are the source sentence f , the tar-
get sentence length l, and an alignment a be-
tween the source and target sentences. So, Al-
ternatingSearch finds a better translation e for
f by solving FIXED ALIGNMENT DECODING.
For this purpose it employs NaiveDecode. Hav-
ing computed e, the algorithm computes a bet-
ter alignment (a?) between e and f by solving
VITERBI ALIGNMENT using Viterbi algorithm.
The new alignment thus found is used by the al-
gorithm in the subsequent iteration. At the end
of each iteration the algorithm checks whether
it has made progress. The algorithm returns the
best translation of the source f across a range
of target sentence lengths.
The analysis of AlternatingSearch is compli-
cated by the fact that the number of iterations
(see step 2.b) depends on the input. It is rea-
sonable to assume that the length of the source
sentence (m) is an upper bound on the number
of iterations. In practice, however, the number
of iterations is typically O (1). There are 3m/2
candidate sentence lengths for the translation
(l varies from m/2 to 2m) and both NaiveDe-
code and Viterbi are O (m). Therefore, the time
complexity of AlternatingSearch is O
(
m2
)
.
4 A Linear Time Algorithm for
FIXED ALIGNMENT DECODING
A key component of all our algorithms is
a linear time algorithm for the problem
FIXED ALIGNMENT DECODING. Recall that in
FIXED ALIGNMENT DECODING, we are given
the target length l and a mapping a? from source
words to target positions. The goal is then to
find the optimal translation with a? as the align-
ment. In this section, we give a dynamic pro-
gramming based solution to this problem. Our
solution is based on a new formulation of IBM
translation models. We begin our discussion
with a few technical definitions.
Alignment a? maps each of the source words
fj, j = 1, . . . ,m to a target position in the range
[0, . . . , l]. Define a mapping ? from [0, . . . , l] to
subsets of {1, . . . ,m} as follows:
?(i) = {j : j ? {1, . . . ,m} ? a?j = i} ? i = 0, . . . , l.
?(i) is the set of source positions which are
mapped to the target location i by the align-
ment a? and the fertility of the target position i
is ?i = |?(i)|.
We can rewrite each of the IBM models
Pr (f , a?|e) as follows:
Pr (f , a?|e) = ?
l
?
i=1
TiDiNi.
Table 2 shows the breaking of Pr (f , a?|e) into
the constituents Ti,Di and Ni. As a conse-
quence, we can write Pr (f , a?|e)Pr (e) as:
Pr (f , a?|e)Pr (e) = ??
l
?
i=1
TiDiNiLi
where Li = trigram(ei|ei?2, ei?1) and ? is the
trigram probability of the boundary word.
The above reformulation of the optimiza-
tion function of the decoding problem allows
us to employ Dynamic Programming for solv-
ing FIXED ALIGNMENT DECODING efficiently.
Note that each word ei has only a constant num-
ber of candidates in the vocabulary. Therefore,
the set of words e1, . . . , el that maximizes the
LHS of the above optimization function can be
found in O (m) time using the standard Dy-
namic Programming algorithm (Cormen et al,
2001).
5 Experiments and Results
In this section we describe our experimental
setup and present the initial results. Our goal
Model ? Ti Di Ni
1 ?(m|l)(l+1)m
?
k??(i) t(fk |ei) 1 1
2 ?(m|l) ?k??(i) t(fk |ei)
?
k??(i) a(i|k,m, l) 1
3 n(?0|m)pm?2?00 p?01
?
k??(i) t(fk |ei)
?
k??(i) d(k|i,m, l) ?i! n(?i|ei)
Table 2: Pr (f, a?|e) for IBM Models
was not only to evaluate the performance of our
algorithms on real data, but also to evaluate
how easy it is to code the algorithm and whether
a straightforward implementation of the algo-
rithm with no parameter tuning can give satis-
factory results.
We implemented the algorithms in C++ and
conducted the experiments on an IBM RS-6000
dual processor machine with 1 GB of RAM. We
built a French-English translation model (IBM
Model 3) by training over a corpus of 100 K sen-
tence pairs from the Hansard corpus. The trans-
lation direction was from French to English. We
built an English language model by training
over a corpus consisting of about 800 million
words. We divided the test sentences into sev-
eral classes based on their length. Each length
class consisted of 300 test French sentences.
We implemented four algorithms -1.1 (NaiveDe-
code), 1.2 (Alternating Search with l restricted
to m), 2.1 (NaiveDecode with l varying from
m/2 to 2m) and 2.2 (Alternating Search). In
order to compare the performance of the al-
gorithms proposed in this paper with a previ-
ous decoding algorithm, we also implemented
the dynamic programming based algorithm by
(Tillman, 2001). For each of the algorithms, we
computed the following:
1. Average time taken for translation for
each length class.
2. NIST score of the translations for each
length class.
3. Average value of the optimization
function for the translations for each
length class.
The results of the experiments are summa-
rized in Plots 1, 2 and 3. In all the plots, the
length class is denoted by the x-axis. 11-20 indi-
cates the class with sentences of length between
11 words to 20 words. 51 indicates the group
of sentences with sentence length 51 or more.
Plot 1 shows the average time taken by the al-
gorithms for translating the sentences in each
length class. Time is shown in seconds on a log
scale. Plot 2 shows the NIST score of the trans-
lations for each length class while Plot 3 shows
the average log score of the translations (-ve log
of Pr (f ,a|e)Pr (e)) again for each length class.
It can be seen from Plot 1 that all of our al-
gorithms are indeed very fast in practice. They
are, in fact, an order faster than the Held-Karp
algorithm. Our algorithms are able to trans-
late even long sentences (50+ words) in a few
seconds.
Plot 3 shows that the log scores of the trans-
lations computed by our algorithms are very
close to those computed by the Held-Karp al-
gorithm. Plot 2 compares the NIST scores ob-
tained with each of the algorithm. Among the
four algorithms based on our framework, Al-
gorithm 2.2 gives the best NIST scores as ex-
pected. Although, the log scores of our algo-
rithms are comparable to those of the Held-
Karp algorithm, our NIST scores are lower. It
should be noted that the mathematical quan-
tity that our algorithm tries to optimize is the
log score. Plot 3 shows that our algorithms are
quite good at finding solutions with good scores.
 0.01
 0.1
 1
 10
 100
 1000
 10000
0-10 11-20 21-30 31-40 41-50 51-
Ti
m
e 
in 
se
co
nd
s
Sentence Length
Decoding Time
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 1: Average decoding time
6 Conclusions
The algorithmic framework developed in this
paper is powerful as it yields several decoding
algorithms. At one end of the spectrum is a
provably linear time algorithm for computing
a suboptimal solution and at the other end is
an exponential time algorithm for computing
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
0-10 11-20 21-30 31-40 41-50 51-
NI
ST
 S
co
re
Sentence Length
NIST Scores
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 2: NIST scores
 0
 50
 100
 150
 200
 250
 300
 350
 400
0-10 11-20 21-30 31-40 41-50 51-
log
sc
or
e
Sentence Length
Logscores
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 3: Log score
the optimal solution. We have also shown that
alternating maximization can be employed to
come up with O
(
m2
)
decoding algorithm. Two
questions in this connection are:
1. Is it possible to reduce the complexity
of AlternatingSearch to O (m)?
2. Instead of exploring each alignment
separately, is it possible to explore a
bunch of alignments in one shot?
Answers to these questions will result in faster
and more efficient decoding algorithms.
7 Acknowledgements
We are grateful to Raghu Krishnapuram for his
insightful comments on an earlier draft of this
paper and Pasumarti Kamesam for his help dur-
ing the course of this work.
References
A. Berger, P. Brown, S. Della Pietra, V. Della
Pietra, A. Kehler, and R. Mercer. 1996. Lan-
guage translation apparatus and method us-
ing context-based translation models. United
States Patent 5,510,981.
P. Brown, S. Della Pietra, V. Della Pietra,
and R. Mercer. 1993. The mathematics of
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
T. H. Cormen, C. E. Leiserson, R. L. Rivest,
and C. Stein. 2001. The MIT Press, Cam-
bridge.
M. R. Garey and D. S. Johnson. 1979. W. H.
Freeman and Company, New York.
U. Germann, M. Jahr, D. Marcu, and K. Ya-
mada. 2003. Fast decoding and optimal de-
coding for machine translation. Artificial In-
telligence.
Ulrich Germann. 2003. Greedy decoding for
statistical machine translation in almost lin-
ear time. In Proceedings of HLT-NAACL
2003. Edmonton, Canada.
M. Held and R. Karp. 1962. A dynamic pro-
gramming approach to sequencing problems.
J. SIAM, 10(1):196?210.
F. Jelinek. 1969. A fast sequential decoding al-
gorithm using a stack. IBM Journal Reseach
and Development, 13:675?685.
Kevin Knight. 1999. Decoding complexity in
word-replacement translation models. Com-
putational Linguistics, 25(4).
F. Och, N. Ueffing, and H. Ney. 2001. An ef-
ficient a* search algorithm for statistical ma-
chine translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Methods in
Machine Translation, pages 55?62. Toulouse,
France.
C. Tillman and H. Ney. 2000. Word reorder-
ing and dp-based search in statistical machine
translation. In Proceedings of the 18th COL-
ING, pages 850?856. Saarbrucken, Germany.
Christoph Tillman. 2001. Word re-ordering
and dynamic programming based search
algorithm for statistical machine transla-
tion. Ph.D. Thesis, University of Technology
Aachen, pages 42?45.
R. Udupa and T. Faruquie. 2004. An english-
hindi statistical machine translation system.
In Proceedings of the 1st IJCNLP, pages 626?
632. Sanya, Hainan Island, China.
Y. Wang and A. Waibel. 1997. Decoding al-
gorithm in statistical machine translation. In
Proceedings of the 35th ACL, pages 366?372.
Madrid, Spain.
