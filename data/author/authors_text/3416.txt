
Using Machine Learning Techniques to Interpret WH-questions
Ingrid Zukerman
School of Computer Science and Software Engineering
Monash University
Clayton, Victoria 3800, AUSTRALIA
ingrid@csse.monash.edu.au
Eric Horvitz
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
horvitz@microsoft.com
Abstract
We describe a set of supervised ma-
chine learning experiments centering
on the construction of statistical mod-
els of WH-questions. These models,
which are built from shallow linguis-
tic features of questions, are employed
to predict target variables which repre-
sent a user?s informational goals. We
report on different aspects of the pre-
dictive performance of our models, in-
cluding the influence of various training
and testing factors on predictive perfor-
mance, and examine the relationships
among the target variables.
1 Introduction
The growth in popularity of the Internet highlights
the importance of developing machinery for gen-
erating responses to queries targeted at large un-
structured corpora. At the same time, the access
of World Wide Web resources by large numbers
of users provides opportunities for collecting and
leveraging vast amounts of data about user activ-
ity. In this paper, we describe research on exploit-
ing data collected from logs of users? queries in
order to build models that can be used to infer
users? informational goals from queries.
We describe experiments which use supervised
machine learning techniques to build statistical
models of questions posed to the Web-based En-
carta encyclopedia service. We focus on mod-
els and analyses of complete questions phrased
in English. These models predict a user?s infor-
mational goals from shallow linguistic features
of questions obtained from a natural language
parser. We decompose these goals into (1) the
type of information requested by the user (e.g.,
definition, value of an attribute, explanation for an
event), (2) the topic, focal point and additional re-
strictions posed by the question, and (3) the level
of detail of the answer. The long-term aim of this
project is to use predictions of these informational
goals to enhance the performance of information-
retrieval and question-answering systems. In this
paper, we report on different aspects of the predic-
tive performance of our statistical models, includ-
ing the influence of various training and testing
factors on predictive performance, and examine
the relationships among the informational goals.
In the next section, we review related research.
In Section 3, we describe the variables being
modeled. In Section 4, we discuss our predic-
tive models. We then evaluate the predictions ob-
tained from models built under different training
and modeling conditions. Finally, we summarize
the contribution of this work and discuss research
directions.
2 Related Research
Our research builds on earlier work on the use
of probabilistic models to understand free-text
queries in search applications (Heckerman and
Horvitz, 1998; Horvitz et al, 1998), and on work
conducted in the IR arena of question answering
(QA) technologies.
Heckerman and Horvitz (1998) and Horvitz et
al. (1998) used hand-crafted models and super-
vised learning to construct Bayesian models that
predict users? goals and needs for assistance in the
context of consumer software applications. Heck-
erman and Horvitz? models considered words,
phrases and linguistic structures (e.g., capitaliza-
tion and definite/indefinite articles) appearing in
queries to a help system. Horvitz et al?s models
considered a user?s recent actions in his/her use of
software, together with probabilistic information
maintained in a dynamically updated user profile.
QA research centers on the challenge of en-
hancing the response of search engines to a user?s
questions by returning precise answers rather than
returning documents, which is the more common
IR goal. QA systems typically combine tradi-
tional IR statistical methods (Salton and McGill,
1983) with ?shallow? NLP techniques. One ap-
proach to the QA task consists of applying the IR
methods to retrieve documents relevant to a user?s
question, and then using the shallow NLP to ex-
tract features from both the user?s question and
the most promising retrieved documents. These
features are then used to identify an answer within
each document which best matches the user?s
question. This approach was adopted in (Kupiec,
1993; Abney et al, 2000; Cardie et al, 2000;
Moldovan et al, 2000).
The NLP components of these systems em-
ployed hand-crafted rules to infer the type of an-
swer expected. These rules were built by con-
sidering the first word of a question as well as
larger patterns of words identified in the question.
For example, the question ?How far is Mars??
might be characterized as requiring a reply of type
DISTANCE. Our work differs from traditional QA
research in its use of statistical models to pre-
dict variables that represent a user?s informational
goals. The variables under consideration include
the type of the information requested in a query,
the level of detail of the answer, and the parts-of-
speech which contain the topic the query and its
focus (which resembles the type of the expected
answer). In this paper, we focus on the predictive
models, rather than on the provision of answers to
users? questions. We hope that in the short term,
the insights obtained from our work will assist
QA researchers to fine-tune the answers generated
by their systems.
3 Data Collection
Our models were built from questions identi-
fied in a log of Web queries submitted to the
Encarta encyclopedia service. These questions
include traditional WH-questions, which begin
with ?what?, ?when?, ?where?, ?which?, ?who?,
?why? and ?how?, as well as imperative state-
ments starting with ?name?, ?tell?, ?find?, ?de-
fine? and ?describe?. We extracted 97,640 ques-
tions (removing consecutive duplicates), which
constitute about 6% of the 1,649,404 queries in
the log files collected during a period of three
weeks in the year 2000. A total of 6,436 questions
were tagged by hand. Two types of tags were col-
lected for each question: (1) tags describing lin-
guistic features, and (2) tags describing high-level
informational goals of users. The former were ob-
tained automatically, while the latter were tagged
manually.
We considered three classes of linguistic fea-
tures: word-based, structural and hybrid.
Word-based features indicate the presence of
specific words or phrases in a user?s question,
which we believed showed promise for predicting
components of his/her informational goals. These
are words like ?make?, ?map? and ?picture?.
Structural features include information ob-
tained from an XML-encoded parse tree gen-
erated for each question by NLPWin (Heidorn,
1999) ? a natural language parser developed by
the Natural Language Processing Group at Mi-
crosoft Research. We extracted a total of 21 struc-
tural features, including the number of distinct
parts-of-speech (PoS) ? NOUNs, VERBs, NPs,
etc ? in a question, whether the main noun is plu-
ral or singular, which noun (if any) is a proper
noun, and the PoS of the head verb post-modifier.
Hybrid features are constructed from structural
and word-based information. Two hybrid fea-
tures were extracted: (1) the type of head verb in
a question, e.g., ?know?, ?be? or action verb;
and (2) the initial component of a question, which
usually encompasses the first word or two of the
question, e.g., ?what?, ?when? or ?how many?,
but for ?how? may be followed by a PoS, e.g.,
?how ADVERB? or ?how ADJECTIVE.?
We considered the following variables rep-
resenting high-level informational goals: Infor-
mation Need, Coverage Asked, Coverage Would
Give, Topic, Focus, Restriction and LIST. Infor-
mation about the state of these variables was pro-
vided manually by three people, with the majority
of the tagging being performed under contract by
a professional outside the research team.
Information Need is a variable that repre-
sents the type of information requested by a
user. We provided fourteen types of informa-
tion need, including Attribute, IDentifica-
tion, Process, Intersection and Topic It-
self (which, as shown in Section 5, are the most
common information needs), plus the additional
category OTHER. As examples, the question ?What
is a hurricane?? is an IDentification query;
?What is the color of sand in the Kalahari?? is an
Attribute query (the attribute is ?color?); ?How
does lightning form?? is a Process query; ?What
are the biggest lakes in New Hampshire?? is an
Intersection query (a type of IDentification,
where the returned item must satisfy a particular
Restriction ? in this case ?biggest?); and ?Where
can I find a picture of a bay?? is a Topic Itself
query (interpreted as a request for accessing an
object directly, rather than obtaining information
about the object).
Coverage Asked and Coverage Would Give are
variables that represent the level of detail in an-
swers. Coverage Asked is the level of detail of
a direct answer to a user?s question. Coverage
Would Give is the level of detail that an infor-
mation provider would include in a helpful an-
swer. For instance, although the direct answer to
the question ?When did Lincoln die?? is a sin-
gle date, a helpful information provider might add
other details about Lincoln, e.g., that he was the
sixteenth president of the United States, and that
he was assassinated. This additional level of de-
tail depends on the request itself and on the avail-
able information. However, here we consider the
former factor, viewing it as an initial filter that
will guide the content planning process of an en-
hanced QA system. The distinction between the
requested level of detail and the provided level of
detail makes it possible to model questions for
which the preferred level of detail in a response
differs from the detail requested by the user. We
considered three levels of detail for both coverage
variables: Precise, Additional and Extended,
plus the additional category OTHER. Precise in-
dicates that an exact answer has been requested,
e.g., a name or date (this is the value of Cover-
age Asked in the above example); Additional
refers to a level of detail characterized by a one-
paragraph answer (this is the value of Coverage
Would Give in the above example); and Extended
indicates a longer, more detailed answer.
Topic, Focus and Restriction contain a PoS in
the parse tree of a user?s question. These variables
represent the topic of discussion, the type of the
expected answer, and information that restricts
the scope of the answer, respectively. These vari-
ables take 46 possible values, e.g., NOUN , VERB
and NP , plus the category OTHER. For each ques-
tion, the tagger selected the most specific PoS that
contains the portion of the question which best
matches each of these informational goals. For in-
stance, given the question ?What are the main tra-
ditional foods that Brazilians eat??, the Topic is
NOUN (Brazilians), the Focus is ADJ +NOUN (tra-
ditional foods) and the restriction is ADJ (main).
As shown in this example, it was sometimes nec-
essary to assign more than one PoS to these tar-
get variables. At present, these composite assign-
ments are classified as the category OTHER.
LIST is a boolean variable which indicates
whether the user is looking for a single answer
(False) or multiple answers (True).
4 Predictive Model
We built decision trees to infer high-level in-
formational goals from the linguistic features
of users? queries. One decision tree was con-
structed for each goal: Information Need, Cov-
erage Asked, Coverage Would Give, Topic, Fo-
cus, Restriction and LIST. Our decision trees were
built using dprog (Wallace and Patrick, 1993)
? a procedure based on the Minimum Message
Length principle (Wallace and Boulton, 1968).
The decision trees described in this section are
those that yield the best predictive performance
(obtained from a training set comprised of ?good?
queries, as described Section 5). The trees them-
selves are too large to be included in this paper.
However, we describe the main attributes iden-
tified in each decision tree. Table 2 shows, for
each target variable, the size of the decision tree
(in number of nodes) and its maximum depth, the
attribute used for the first split, and the attributes
used for the second split. Table 1 shows examples
and descriptions of the attributes in Table 2.1
We note that the decision tree for Focus splits
first on the initial component of a question, e.g.,
?how ADJ?, ?where? or ?what?, and that one of
the second-split attributes is the PoS following the
initial component. These attributes were also used
to build the hand-crafted rules employed by the
QA systems described in Section 2, which con-
centrate on determining the type of the expected
1The meaning of ?Total PRONOUNS? is peculiar in
our context, because the NLPWin parser tags words such
as ?what? and ?who? as PRONOUNs. Also, the clue at-
tributes, e.g., Comparison clues, represent groupings of dif-
ferent clues that at design time where considered helpful in
identifying certain target variables.
Table 1: Attributes in the decision trees
Attribute Example/Meaning
Attribute clues e.g., ?name?, ?type of?, ?called?
Comparison clues e.g., ?similar?, ?differ?, ?relate?
Intersection clues superlative ADJ, ordinal ADJ, relative clause
Topic Itself clues e.g., ?show?, ?picture?, ?map?
PoS after Initial component e.g., NOUN in ?which country is the largest??
verb-post-modifier PoS e.g., NP without PP in ?what is a choreographer?
Total PoS number of occurrences of PoS in a question, e.g., Total NOUNs
First NP plural? Boolean attribute
Definite article in First NP? Boolean attribute
Plural quantifier? Boolean attribute
Length in words number of words in a question
Length in phrases number of NPs + PPs + VPs in a question
Table 2: Summary of decision trees
Target Variable Nodes/Depth First Split Second Split
Information Need 207/13 Initial component Attribute clues, Comparison clues, Topic Itself
clues, PoS after Initial component, verb-post-
modifier PoS, Length in words
Coverage Asked 123/11 Initial component Topic Itself clues, PoS after Initial component,
Head verb
Coverage Would Give 69/6 Topic Itself clues Initial component, Attribute clues
Topic 193/9 Total NOUNs Total ADJs, Total AJPs, Total PRONOUNs
Focus 226/10 Initial component Topic Itself clues, Total NOUNs, Total VERBs,
Total PRONOUNs, Total VPs, Head verb, PoS after
Initial component
Restriction 126/9 Total PPs Intersection clues, PoS after Initial component,
Definite article in First NP?, Length in phrases
LIST 45/7 First NP plural? Plural quantifier?, Initial component
answer (which is similar to our Focus). How-
ever, our Focus decision tree includes additional
attributes in its second split (these attributes are
added by dprog because they improve predictive
performance on the training data).
5 Results
Our report on the predictive performance of the
decision trees considers the effect of various train-
ing and testing factors on predictive performance,
and examines the relationships among the target
variables.
5.1 Training Factors
We examine how the quality of the training data
and the size of the training set affect predictive
performance.
Quality of the data. In our context, the quality
of the training data is determined by the wording
of the queries and the output of the parser. For
each query, the tagger could indicate whether it
was a BAD QUERY or whether a WRONG PARSE
had been produced. A BAD QUERY is incoher-
ent or articulated in such a way that the parser
generates a WRONG PARSE, e.g., ?When its hot it
expand??. Figure 1 shows the predictive perfor-
mance of the decision trees built for two train-
ing sets: All5145 and Good4617. The first set
contains 5145 queries, while the second set con-
tains a subset of the first set comprised of ?good?
queries only (i.e., bad queries and queries with
wrong parses were excluded). In both cases, the
same 1291 queries were used for testing. As a
baseline measure, we also show the predictive ac-
Figure 1: Performance comparison: training with
all queries versus training with good queries;
prior probabilities included as baseline
Small Medium Large X-Large
Train/All 1878 2676 3765 5145
Train/Good 1679 2389 3381 4617
Test 376 662 934 1291
Table 3: Four training and testing set sizes
curacy of using the maximum prior probability to
predict each target variable. These prior probabil-
ities were obtained from the training set All5145.
The Information Need with the highest prior prob-
ability is IDentification, the highest Coverage
Asked is Precise, while the highest Coverage
Would Give is Additional; NOUN contains the
most common Topic; the most common Focus and
Restriction are NONE; and LIST is almost always
False. As seen in Figure 1, the prior probabilities
yield a high predictive accuracy for Restriction
and LIST. However, for the other target variables,
the performance obtained using decision trees is
substantially better than that obtained using prior
probabilities. Further, the predictive performance
obtained for the set Good4617 is only slightly bet-
ter than that obtained for the set All5145. How-
ever, since the set of good queries is 10% smaller,
it is considered a better option.
Size of the training set. The effect of the size
of the training set on predictive performance was
assessed by considering four sizes of training/test
sets: Small, Medium, Large, and X-Large. Ta-
ble 3 shows the number of training and test
queries for each set size for the ?all queries? and
the ?good queries? training conditions.
Figure 2: Predictive performance for four training
sets (1878, 2676, 3765 and 5145) averaged over 5
runs ? All queries
Figure 3: Predictive performance for four training
sets (1679, 2389, 3381 and 4617) ? Good queries
The predictive performance for the all-queries
and good-queries sets is shown in Figures 2 and 3
respectively. Figure 2 depicts the average of the
results obtained over five runs, while Figure 3
shows the results of a single run (similar results
were obtained from other runs performed with the
good-queries sets). As indicated by these results,
for both data sets there is a general improvement
in predictive performance as the size of the train-
ing set increases.
5.2 Testing Factors
We examine the effect of two factors on the pre-
dictive performance of our models: (1) query
length (measured in number of words), and (2) in-
formation need (as recorded by the tagger). These
effects were studied with respect to the predic-
tions generated by the decision trees obtained
from the set Good4617, which had the best per-
formance overall.
Figure 4: Query length distribution ? Test set
Figure 5: Predictive performance by query length
? Good queries
Query length. The queries were divided into
four length categories (measured in number of
words): length , length , length
and length. Figure 4 displays the distribu-
tion of queries in the test set according to these
length categories. According to this distribution,
over 90% of the queries have less than 11 words.
The predictive performance of our decision trees
broken down by query length is shown in Fig-
ure 5. As shown in this chart, for all target vari-
ables there is a downward trend in predictive ac-
curacy as query length increases. Still, for queries
of less than 11 words and all target variables ex-
cept Topic, the predictive accuracy remains over
74%. In contrast, the Topic predictions drop from
88% (for queries of less than 5 words) to 57%
(for queries of 8, 9 or 10 words). Further, the pre-
dictive accuracy for Information Need, Topic, Fo-
cus and Restriction drops substantially for queries
that have 11 words or more. This drop in predic-
tive performance may be explained by two fac-
tors. For one, the majority of the training data
Figure 6: Information need distribution ? Test set
Figure 7: Predictive performance for five most
frequent information needs ? Good queries
consists of shorter questions. Hence, the applica-
bility of the inferred models to longer questions
may be limited. Also, longer questions may exac-
erbate errors associated with some of the indepen-
dence assumptions implicit in our current model.
Information need. Figure 6 displays the dis-
tribution of the queries in the test set ac-
cording to Information Need. The five
most common Information Need categories
are: IDentification, Attribute, Topic It-
self, Intersection and Process, jointly ac-
counting for over 94% of the queries. Figure 7
displays the predictive performance of our models
for these five categories. The best performance
is exhibited for the IDentification and Topic
Itself queries. In contrast, the lowest predictive
accuracy was obtained for the Information Need,
Topic and Restriction of Intersection queries.
This can be explained by the observation that In-
tersection queries tend to be the longest queries
(as seen above, predictive accuracy drops for long
Figure 8: Performance comparison for four pre-
diction models: PerfectInformation, BestRe-
sults, PredictionOnly and Mixed; prior prob-
abilities included as baseline
queries). The relatively low predictive accuracy
obtained for both types of Coverage for Process
queries remains to be explained.
5.3 Relations between target variables
To determine whether the states of our target
variables affect each other, we built three pre-
diction models, each of which includes six tar-
get variables for predicting the remaining vari-
able. For instance, Information Need, Coverage
Asked, Coverage Would Give, Focus, Restriction
and LIST are incorporated as data (in addition to
the observable variables) when training a model
that predicts Focus. Our three models are: Pre-
dictionOnly ? which uses the predicted values
of the six target variables both for the training set
and for the test set; Mixed ? which uses the actual
values of the six target variables for the training
set and their predicted values for the test set; and
PerfectInformation ? which uses actual values
of the six target variables for both training and
testing. This model enables us to determine the
performance boundaries of our methodology in
light of the currently observed attributes.
Figure 8 shows the predictive accuracy of five
models: the above three models, our best model
so far (obtained from the training set Good4617)
? denoted BestResult, and prior probabilities.
As expected, the PerfectInformation model
has the best performance. However, its predic-
tive accuracy is relatively low for Topic and Fo-
cus, suggesting some inherent limitations of our
methodology. The performance of the Predic-
tionOnly model is comparable to that of BestRe-
sult, but the performance of the Mixed model
seems slightly worse. This difference in perfor-
mance may be attributed to the fact that the Pre-
dictionOnly model is a ?smoothed? version of
the Mixed model. That is, the PredictionOnly
model uses a consistent version of the target vari-
ables (i.e., predicted values) both for training and
testing. This is not the case for the Mixed model,
where actual values are used for training (thus the
Mixed model is the same as the PerfectInfor-
mation model), but predicted values (which are
not always accurate) are used for testing.
Finally, Information Need features prominently
both in the PerfectInformation/Mixed model
and the PredictionOnly model, being used in
the first or second split of most of the decision
trees for the other target variables. Also, as ex-
pected, Coverage Asked is used to predict Cov-
erage Would Give and vice versa. These re-
sults suggest using modeling techniques which
can take advantage of dependencies among tar-
get variables. These techniques would enable the
construction of models which take into account
the distribution of the predicted values of one or
more target variables when predicting another tar-
get variable.
6 Discussion and Future Work
We have introduced a predictive model, built
by applying supervised machine-learning tech-
niques, which can be used to infer a user?s key in-
formational goals from free-text questions posed
to an Internet search service. The predictive
model, which is built from shallow linguistic fea-
tures of users? questions, infers a user?s informa-
tion need, the level of detail requested by the user,
the level of detail deemed appropriate by an infor-
mation provider, and the topic, focus and restric-
tions of the user?s question. The performance of
our model is encouraging, in particular for shorter
queries, and for queries with certain information
needs. However, further improvements are re-
quired in order to make this model practically ap-
plicable.
We believe there is an opportunity to identify
additional linguistic distinctions that could im-
prove the model?s predictive performance. For
example, we intend to represent frequent combi-
nations of PoS, such as NOUN +NOUN , which are
currently classified as OTHER (Section 3). We also
propose to investigate predictive models which
return more informative predictions than those re-
turned by our current model, e.g., a distribution
of the probable informational goals, instead of a
single goal. This would enable an enhanced QA
system to apply a decision procedure in order to
determine a course of action. For example, if the
Additional value of the Coverage Would Give
variable has a relatively high probability, the sys-
tem could consider more than one Information
Need, Topic or Focus when generating its reply.
In general, the decision-tree generation meth-
ods described in this paper do not have the abil-
ity to take into account the relationships among
different target variables. In Section 5.3, we in-
vestigated this problem by building decision trees
which incorporate predicted and actual values of
target variables. Our results indicate that it is
worth exploring the relationships between several
of the target variables. We intend to use the in-
sights obtained from this experiment to construct
models which can capture probabilistic depen-
dencies among variables.
Finally, as indicated in Section 1, this project
is part of a larger effort centered on improv-
ing a user?s ability to access information from
large information spaces. The next stage of this
project involves using the predictions generated
by our model to enhance the performance of QA
or IR systems. One such enhancement pertains
to query reformulation, whereby the inferred in-
formational goals can be used to reformulate or
expand queries in a manner that increases the
likelihood of returning appropriate answers. As
an example of query expansion, if Process was
identified as the Information Need of a query,
words that boost responses to searches for infor-
mation relating to processes could be added to the
query prior to submitting it to a search engine.
Another envisioned enhancement would attempt
to improve the initial recall of the document re-
trieval process by submitting queries which con-
tain the content words in the Topic and Focus of a
user?s question (instead of including all the con-
tent words in the question). In the longer term, we
plan to explore the use of Coverage results to en-
able an enhanced QA system to compose an ap-
propriate answer from information found in the
retrieved documents.
Acknowledgments
This research was largely performed during the
first author?s visit at Microsoft Research. The au-
thors thank Heidi Lindborg, Mo Corston-Oliver
and Debbie Zukerman for their contribution to the
tagging effort.
References
S. Abney, M. Collins, and A. Singhal. 2000. Answer
extraction. In Proceedings of the Sixth Applied Nat-
ural Language Processing Conference, pages 296?
301, Seattle, Washington.
C. Cardie, V. Ng, D. Pierce, and C. Buckley.
2000. Examining the role of statistical and lin-
guistic knowledge sources in a general-knowledge
question-answering system. In Proceedings of the
Sixth Applied Natural Language Processing Con-
ference, pages 180?187, Seattle, Washington.
D. Heckerman and E. Horvitz. 1998. Inferring infor-
mational goals from free-text queries: A Bayesian
approach. In Proceedings of the Fourteenth Confer-
ence on Uncertainty in Artificial Intelligence, pages
230?237, Madison, Wisconsin.
G. Heidorn. 1999. Intelligent writing assistance. In
A Handbook of Natural Language Processing Tech-
niques. Marcel Dekker.
E. Horvitz, J. Breese, D. Heckerman, D. Hovel,
and K. Rommelse. 1998. The Lumiere project:
Bayesian user modeling for inferring the goals and
needs of software users. In Proceedings of the
Fourteenth Conference on Uncertainty in Artificial
Intelligence, pages 256?265, Madison, Wisconsin.
J. Kupiec. 1993. MURAX: A robust linguistic ap-
proach for question answering using an on-line en-
cyclopedia. In Proceedings of the 16th Annual
International ACM-SIGIR Conference on Research
and Development in Information Retrieval, pages
181?190, Pittsburgh, Pennsylvania.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The
structure and performance of an open-domain ques-
tion answering system. In ACL2000 ? Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 563?570, Hong
Kong.
G. Salton and M.J. McGill. 1983. An Introduction to
Modern Information Retrieval. McGraw Hill.
C.S. Wallace and D.M. Boulton. 1968. An informa-
tion measure for classification. The Computer Jour-
nal, 11:185?194.
C.S. Wallace and J.D. Patrick. 1993. Coding decision
trees. Machine Learning, 11:7?22.
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 225?234,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Models for Multiparty Engagement in Open-World Dialog 
Dan Bohus 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
dbohus@microsoft.com 
Eric Horvitz 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
horvitz@microsoft.com 
 
 
Abstract 
We present computational models that allow 
spoken dialog systems to handle multi-
participant engagement in open, dynamic envi-
ronments, where multiple people may enter and 
leave conversations, and interact with the sys-
tem and with others in a natural manner. The 
models for managing the engagement process 
include components for (1) sensing the en-
gagement state, actions and intentions of mul-
tiple agents in the scene, (2) making engage-
ment decisions (i.e. whom to engage with, and 
when) and (3) rendering these decisions in a set 
of coordinated low-level behaviors in an embo-
died conversational agent. We review results 
from a study of interactions "in the wild" with a 
system that implements such a model.  
1 Introduction 
To date, nearly all spoken dialog systems research has 
focused on the challenge of engaging single users on 
tasks defined within a relatively narrow context.  Efforts 
in this realm have led to significant progress including 
large-scale deployments that now make spoken dialog 
systems common features in the daily lives of millions 
of people. However, research on dialog systems has 
largely overlooked important challenges with the initia-
tion, maintenance, and suspension of conversations that 
are common in the course of natural communication and 
collaborations among people. In (Bohus and Horvitz, 
2009) we outlined a set of core challenges for extending 
traditional closed-world dialog systems to systems that 
have competency in open-world dialog. The work de-
scribed here is part of a larger research effort aimed at 
addressing these challenges, and constructing computa-
tional models to support the core interaction skills re-
quired for open-world dialog. In particular, we focus our 
attention in this paper on the challenges of managing 
engagement ? ?the process by which two (or more) par-
ticipants establish, maintain and end their perceived 
connection during interactions they jointly undertake?, 
cf. Sidner et al (2004) in open-world settings.  
We begin by reviewing the challenges of managing 
engagement in the open-world in the next section. In 
Section 3, we survey the terrain of related efforts that 
provides valuable context for the new work described in 
this paper. In Section 4, we introduce a computational 
model for multiparty situated engagement. The model 
harnesses components for sensing the engagement state, 
actions, and intentions of people in the scene for making 
high-level engagement decisions (whom to engage with, 
and when), and for rendering these decisions into a set 
of low-level coordinated behaviors (e.g., gestures, eye 
gaze, greetings, etc.). Then, we describe an initial ob-
servational study with the proposed model, and discuss 
some of the lessons learned through this experiment. 
Finally, in Section 6, we summarize this work and out-
line several directions for future research.  
2 Engagement in Open-World Dialog 
In traditional, single-user systems the engagement prob-
lem can often be resolved in a relatively simple manner. 
For instance, in telephony-based applications, it is typi-
cally safe to assume that a user is engaged with a dialog 
system once a call has been received. Similarly, push-
to-talk buttons are often used in multimodal mobile ap-
plications. Although these solutions are sufficient and 
even natural in closed, single-user contexts, they be-
come inappropriate for open-world systems that must 
operate continuously in open, dynamic environments, 
such as robots, interactive billboards, or embodied con-
versational agents.  
Interaction in the open-world is characterized by two 
aspects that capture key departures from assumptions 
traditionally made in spoken dialog systems (Bohus and 
Horvitz, 2009). The first one is the dynamic, multiparty 
nature of the interaction, i.e., the world typically con-
tains not just one, but multiple agents that are relevant 
225
to the interactive system. Engagements in open worlds 
are often dynamic and asynchronous, i.e. relevant agents 
may enter and leave the observable world at any time, 
may interact with the system and with each other, and 
their goals, needs, and intentions may change over time. 
Managing the engagement process in this context re-
quires that a system explicitly represents, models, and 
reasons about multiple agents and interaction contexts, 
and maintains and leverages long-term memory of the 
interactions to provide support and assistance.  
A second important aspect that distinguishes open-
world from closed-world dialog is the situated nature of 
the interaction, i.e., the fact that the surrounding physi-
cal environment provides rich, streaming context that is 
relevant for conducting and organizing the interactions. 
Situated interactions among people often hinge on 
shared information about physical details and relation-
ships, including structures, geometric relationships and 
pathways, objects, topologies, and communication af-
fordances.  The often implicit, yet powerful physicality 
of situated interaction, provides opportunities for mak-
ing inferences in open-world dialog systems, and chal-
lenges system designers to innovate across a spectrum 
of complexity and sophistication. Physicality and em-
bodiment also provide important affordances that can be 
used by a system to support the engagement process. 
For instance, the use of a rendered or physically embo-
died avatar in a spoken dialog system provides a natural 
point of visual engagement between the system and 
people, and allows the system to employ natural signal-
ing about attention and engagement with head pose, 
gaze, facial expressions, pointing and gesturing. 
We present in this paper methods that move beyond 
the realm of closed-world dialog with a situated multi-
party engagement model that can enable a computation-
al system to fluidly engage, disengage and re-engage 
one or multiple people, and support natural interactions 
in an open-world context. 
3 Related Work 
The process of engagement between people, and be-
tween people and computational systems has received a 
fair amount of attention. Observational studies in the 
sociolinguistics and conversational analysis communi-
ties have revealed that engagement is a complex, mixed-
initiative, highly-coordinated process that often involves 
a variety of non-verbal cues and signals, (Goffman, 
1963; Kendon, 1990), spatial trajectory and proximity 
(Hall, 1966; Kendon, 1990b), gaze and mutual attention 
(Argyle and Cook, 1976), head and hand gestures (Ken-
don, 1990), as well as verbal greetings. 
A number of researchers have also investigated is-
sues of engagement in human-computer and human-
robot interaction contexts. Sidner and colleagues (2004) 
define engagement as ?the process by which two (or 
more) participants establish, maintain and end their per-
ceived connection during interactions they jointly un-
dertake?, and focus on the process of maintaining en-
gagement. They show in a user study (Sidner et al, 
2004; 2005) that people directed their attention to a ro-
bot more often when the robot made engagement ges-
tures throughout the interaction (i.e. tracked the user?s 
face, and pointed to relevant objects at appropriate times 
in the conversation.) Peters (2005; 2005b) uses an alter-
native definition of engagement as ?the value that a par-
ticipant in an interaction attributes to the goal of being 
together with the other participant(s) and of continuing 
the interaction,? and present the high-level schematics 
for an algorithm for establishing and maintaining en-
gagement. The algorithm highlights the importance of 
mutual attention and eye gaze and relies on a heuristi-
cally computed ?interest level? to decide when to start a 
conversation. Michalowski and colleagues (2006) pro-
pose and conduct experiments with a model of engage-
ment grounded in proxemics (Hall, 1966) which classi-
fies relevant agents in the scene in four different catego-
ries (present, attending, engaged  and interacting) based 
on their distance to the robot. The robot?s behaviors are 
in turn conditioned on the four categories above.  
In our work, we follow Sidner?s definition of en-
gagement as a process (Sidner et al, 2004) and describe 
a computational model for situated multiparty engage-
ment. The proposed model draws on several ideas from 
the existing body of work, but moves beyond it and 
provides a more comprehensive framework for manag-
ing the engagement process in a dynamic, open-world 
context, where multiple people with different and 
changing goals may enter and leave, and communicate 
and coordinate with each other and with the system.  
4 Models for Multiparty Engagement 
The proposed framework for managing engagement is 
centered on a reified notion of interaction, defined here 
as a basic unit of sustained, interactive problem-solving. 
Each interaction involves two or more participants, and 
this number may vary in time; new participants may 
join an existing interaction, or current participants may 
leave an interaction at any point in time. The system is 
actively engaged in at most one interaction at a time 
(with one or multiple participants), but it can simulta-
neously keep track of additional, suspended interactions. 
In this context, engagement is viewed as the process 
subsuming the joint, coordinated activities by which 
participants initiate, maintain, join, abandon, suspend, 
resume, or terminate an interaction. Appendix A shows 
by means of an example the various stages of an interac-
tion and the role played by the engagement process.  
Successfully modeling the engagement process in a 
situated, multi-participant context requires that the sys-
tem (1) senses and reasons about the engagement state, 
226
 not-engaged engaged 
EA=disengage | 
SEA=disengage 
Figure 2. Engagement state transition diagram. EA is the 
agent?s engagement action; SEA is the system?s action. 
EA=maintain & 
SEA=maintain 
EA=engage & 
SEA=engage 
EA=no-action | 
SEA=no-action 
Figure 3. Graphical model showing key variables and 
dependencies in managing engagement. 
ES 
EA 
EI 
t   t+1 
SEA 
ES 
? 
EI 
A 
? 
a
d
d
it
io
n
a
l 
c
o
n
te
x
t 
e
n
g
a
g
e
m
e
n
t 
s
e
n
s
in
g
 
? 
G 
A 
G 
EA 
SEB 
? 
actions and intentions of multiple agents in the scene, 
(2) makes high-level engagement control decisions (i.e. 
about whom to engage or disengage with, and when) 
and (3) executes and signals these decisions to the other 
participants in an appropriate and expected manner (e.g. 
renders them in a set of coordinated behaviors such as 
gestures, greetings, etc.). The proposed model subsumes 
these three components, which we discuss in more de-
tail in the following subsections. 
4.1 Engagement State, Actions, Intentions 
As a prerequisite for making informed engagement de-
cisions, a system must be able to recognize various en-
gagement cues, and to reason about the engagement 
actions and intentions of relevant agents in the scene. To 
accomplish this, the sensing subcomponent of the pro-
posed engagement model tracks over time three related 
engagement variables for each agent ? and interaction ?: 
the engagement state ???
? (?) , the engagement action 
???
? (?) and the engagement intention ???
? (?).  
The engagement state, ???
? (?), captures whether an 
agent ? is engaged in interaction ? and is modeled as a 
deterministic variable with two possible values: en-
gaged and not-engaged. The state is updated based on 
the joint actions of the agent and the system (see Figures 
3 and 4). Since engagement is a collaborative process, 
the transitions to the engaged state require that both the 
agent and the system take either an engage action (if the 
agent was previously not engaged) or a maintain action 
(if the agent was already engaged); we discuss these 
actions in more detail shortly. On the other hand, disen-
gagement can be a unilateral act: an agent transitions to 
the not-engaged state if either the agent or the system 
take a disengage action or a no-action. 
The second engagement variable, ???
? (?), models the 
actions that an agent takes to initiate, maintain or termi-
nate engagement. There are four engagement actions: 
engage, no-action, maintain, disengage. The first two 
are possible only from the not-engaged state, while the 
last two are possible only from the engaged state. The 
engagement actions are estimated based on a condition-
al probabilistic model of the form: 
 
?(???
? (?)|???
?  ? ,???
?  ? ? 1 , ????
?  ? ? 1 ,?(?)) 
 
The inference is conditioned on the current engage-
ment state, on the previous agent and system actions, 
and on additional sensory evidence ?(t). ? t  includes 
the detection of explicit engagement cues such as: salu-
tations (e.g. ?Hi!?, ?Bye bye?); calling behaviors (e.g. 
?Laura!?); the establishment or the breaking of an F-
formation (Kendon, 1990b), i.e. the agent approaches 
and positions himself in front of the system and attends 
to the system; an expected, opening dialog move (e.g. 
?Come here!?). Note that each of these cues is explicit, 
and marks a committed engagement action.  
A third variable in the proposed model, ???
? (?) , 
tracks the engagement intention of an agent with respect 
to a conversation. Like the engagement state, the inten-
tion can either be engaged or not-engaged. Intentions 
are tracked separately from actions since an agent might 
intend to engage or disengage the system, but not yet 
take an explicit engagement action. For instance, let us 
consider the case in which the system is already en-
gaged in an interaction and another agent is waiting in 
line to interact with the system.  Although the waiting 
agent does not take an explicit, committed engagement 
action, she might still intend to engage in a new conver-
sation with the system once the opportunity arises. She 
might also signal this engagement intention via various 
cues (e.g. pacing around, glances that make brief but 
clear eye contact with the system, etc.) More generally, 
the engagement intention variable captures whether or 
not an agent would respond positively should the system 
initiate engagement. In that sense, it roughly corres-
ponds to Peters? (2005; 2005b) ?interest level?, i.e. to 
the value the agent attaches to being engaged in a con-
versation with the system.  
Like engagement actions, engagement intentions are 
inferred based on a direct conditional model: 
 
227
?(???
? (?)|???
?  ? ,???
?  ? ,????
?  ? ? 1 ,???
?  t? 1 ,?(?)) 
 
This model leverages information about the current 
engagement state, the previous agent and system ac-
tions, the previous engagement intention, as well as ad-
ditional evidence ?(?)  capturing implicit engagement 
cues. Such cues include the spatiotemporal trajectory of 
the participant and the level of sustained mutual atten-
tion. The models for inferring engagement actions and 
intentions are generally independent of the application. 
They capture the typical behaviors and cues by which 
people signal engagement, and, as such, should be reus-
able across different domains. In other work (Bohus and 
Horvitz, 2009b), we describe these models in more de-
tail and show how they can be learned automatically 
from interaction data. 
4.2 Engagement Control Policy 
Based on the inferred state, actions and intentions of the 
agents in the scene, as well as other additional evidence 
to be discussed shortly, the proposed model outputs 
high-level engagement actions, denoted by SEA deci-
sion node in Figure 3. The action-space on the system 
side contains the same four actions previously dis-
cussed: engage, disengage, maintain and no-action. 
Each action is parameterized with a set of agents {??} 
and an interaction ?. Additional parameters that control 
the lower level execution of these actions, such as spe-
cific greetings, waiting times, urgency, etc. may also be 
specified. The actual execution mechanisms are dis-
cussed in more detail in the following subsection. 
In making engagement decisions in an open-world 
setting, a conversational system must balance the goals 
and needs of multiple agents in the scene and resolve 
various tradeoffs (for instance between continuing the 
current interaction or interrupting it temporarily to ad-
dress another agent), all the while observing rules of 
social etiquette in interaction. Apart from the detected 
engagement state, actions and intentions of an agent 
??
? =  ???
? ,???
? ,???
?   , the control policy can be en-
hanced through leveraging additional observational evi-
dence, including high-level information ??  about the 
various agents in the scene, such as their long-term 
goals and activities, as well as other global context (?), 
including the multiple tasks at hand, the history of the 
interactions, relationships between various agents in the 
scene (e.g. which agents are in a group together), etc. 
For instance, a system might decide to temporarily 
refuse engagement even though an agent takes an en-
gage action, because it is currently involved in a higher 
priority interaction. Or, a system might try to take the 
initiative and engage an agent based on the current con-
text (e.g. the system has a message to deliver) and activ-
ity of the agent (e.g. the agent is passing by), even 
though the agent has no intention to engage.  
Engagement control policies have therefore the form,  
 
????({??
? }? ,? ,  ??  ? ,?) 
 
where we have omitted the time index for simplicity. In 
contrast to the models for inferring engagement inten-
tions and action, the engagement control policy can of-
ten be application specific. Such policies can be au-
thored manually to capture the desired system behavior.  
We will discuss a concrete example of this in Section 
5.2. In certain contexts, a more principled solution can 
be developed by casting the control of engagement as an 
optimization problem for scheduling collaborations with 
multiple parties under uncertainties about the estimated 
goals and needs, the duration of the interactions, time 
and frustration costs, social etiquette, etc. We are cur-
rently exploring such models, where the system also 
uses information-gathering actions (e.g. ?Are the two of 
you together?? ?Are you here for X?,? etc.), based on 
value-of-information computations to optimize in the 
nature and flow of attention and collaboration in multi-
party interactions. 
4.3 Behavioral Control Policy 
At the lower level, the engagement decisions taken by 
the system have to be executed and rendered in an ap-
propriate manner. With the use of a rendered or physical 
embodied agent, these actions are translated into a set of 
coordinated lower-level behaviors, such as head ges-
tures, making and breaking eye contact, facial expres-
sions, salutations, interjections, etc. The coordination of 
these behaviors is governed by a behavioral control pol-
icy, conditioned on the estimated engagement state, 
actions and intentions of the considered agents, as well 
as other information extracted from the scene: 
 
????(???, {??
? }? ,? ,?) 
 
For example, in the current implementation, the en-
gage system action subsumes three sub-behaviors per-
formed in a sequence: EstablishAttention, Greeting, and 
Monitor. First, the system attempts to establish sus-
tained mutual attention with the agent(s) to be engaged. 
This is accomplished by directing the gaze towards the 
agents, and if the agent?s focus of attention is not on the 
system, triggering an interjection like ?Excuse me!? 
Once mutual attention is established, on optional Greet-
ing behavior is performed; a greeting can be specified as 
an execution parameter of the engage action. Finally, 
the system enters a Monitor behavior, in which it moni-
tors for the completion of engagement. The action com-
pletes successfully once the agent(s) are in an engaged 
state. Alternatively if a certain period of time elapses 
and the agent(s) have not yet transitioned to the engaged 
state, the engage system action completes with failure 
(which is signaled to the engagement control layer).  
Like the high-level engagement control policies, the 
behavioral control policies can either be authored ma-
nually, or learned from data, either in a supervised (e.g. 
228
from a human-human interaction corpus) or unsuper-
vised learning setting. Also, like the engagement sens-
ing component, the behavioral control component is 
decoupled from the task at hand, and should be largely 
reusable across multiple application domains.  
5 Observational Study 
As an initial step towards evaluating the proposed si-
tuated multiparty engagement models, we conducted a 
preliminary observational study with a spoken dialog 
system that implements these models. The goals of this 
study were (1) to investigate whether a system can use 
the proposed engagement models to effectively create 
and conduct multiparty interactions in an open-world 
setting, (2) to study user behavior and responses in this 
setting, and (3) to identify some of the key technical 
challenges in supporting multiparty engagement and 
dialog in open-world context. In this section, we de-
scribe this study and report on the lessons learned.  
5.1 Experimental platform 
Studying multiparty engagement and more generally 
open-world interaction poses significant challenges. 
Controlled laboratory studies are by their very nature 
closed-world. Furthermore, providing participants with 
instructions, such as ?Go interact with this system?, or 
?Go join the existing interaction? can significantly 
prime and alter the engagement behaviors they would 
otherwise display upon encountering the system in an 
unconstrained setting. This can in turn cast serious 
doubts on the validity of the results. Open-world inte-
raction is best observed in the open-world.  
To provide an ecologically valid basis for studying 
situated, multiparty engagement we therefore developed 
a conversational agent that implements the proposed 
model, and deployed it in the real-world. The system, 
illustrated in Figure 4, takes the form of an interactive 
multi-modal kiosk that displays a realistically rendered 
avatar head which can interact via natural language. The 
avatar can engage with one or more participants and 
plays a simple game, in which the users have to respond 
to multiple-choice trivia questions.  
The system?s hardware and software architecture is 
illustrated in Figure 4. Data gathered from a wide-angle 
camera, a 4-element linear microphone array, and a 19? 
touch-screen is forwarded to a scene analysis module 
that fuses the incoming streams and constructs in real-
time a coherent picture of the dynamics in the surround-
ing environment. The system detects and tracks the lo-
cation of multiple agents in the scene, tracks the head 
pose for engaged agents, tracks the current speaker, and 
infers the focus of attention, activities, and goals of each 
agent, as well as the group relationships among different 
agents. An in-depth description of the hardware and 
scene analysis components falls beyond the scope of 
this paper, but details are available in (Bohus and Hor-
vitz, 2009). The scene analysis results are forwarded to 
the control level, which is structured in a two-layer reac-
tive-deliberative architecture. The reactive layer imple-
ments and coordinates various low-level behaviors, in-
cluding engagement, conversational floor management 
and turn-taking, and coordinating spoken and gestural 
outputs. The deliberative layer plans the system?s dialog 
moves and high-level engagement actions. 
Overall, the game task was purposefully designed to 
minimize challenges in terms of speech recognition or 
dialog management, and allow us to focus our attention 
on the engagement processes. The avatar begins the 
interactions by asking the engaged user if they would 
like to play a trivia game. If the user agrees, the avatar 
goes through four multiple-choice questions, one at a 
time. After each question, the possible answers are dis-
played on the screen (Figure 4) and users can respond 
by either speaking an answer or by touching it. When 
the answer provided by the user is incorrect, the system 
provides a short explanation regarding the correct an-
swer before moving on to the next question.  
The system also supports multi-participant interac-
tions. The engagement policy used to attract and engage 
Dialog Management 
Behavioral Control 
Scene Analysis Output Planning 
Vision Speech Synthesis Avatar 
wide-angle camera 
4-element linear microphone array  
touch screen 
speakers 
Figure 4. Trivia game dialog system: prototype, architectural overview, and runtime scene analysis 
229
multiple users in a game is the focus of this observa-
tional study, and is discussed in more detail in the next 
subsection. Once the system is engaged with multiple 
users, it uses a multi-participant turn taking model 
which allows it to continuously track who the current 
speaker is, and who has the conversational floor (Bohus 
and Horvitz, 2009). At the behavioral level, the avatar 
orients its head pose and gaze towards the current 
speaker, or towards the addressee(s) of its own utter-
ances. During multiplayer games, the avatar alternates 
between the users when asking questions. Also, after a 
response is received from one of the users, the avatar 
confirms the answer with the other user(s), e.g. ?Do you 
agree with that?? A full sample interaction with the sys-
tem is described in Appendix A, and the corresponding 
video is available online (Situated Interaction, 2009).  
5.2 Multiparty Engagement Policy 
The trivia game system implements the situated, multi-
party engagement model described in Section 4. The 
sensing and behavioral control components are applica-
tion independent and were previously described. We 
now describe the system?s engagement policy, which is 
application specific.  
As previously discussed, apart from using the in-
ferred engagement state, actions and intentions for the 
agents in the scene, the proposed model also uses in-
formation about the high-level goals and activities of 
these agents when making engagement decisions. Spe-
cifically, the system tracks the goal of each agent in the 
scene, which can be play, watch, or other, and their cur-
rent activity, which can be passing-by, interacting, play-
ing, watching, or departing. The goal and activity rec-
ognition models are application specific, and in this case 
are inferred based on probabilistic conditional models 
that leverage information about the spatiotemporal tra-
jectory of each agent and their spoken utterances, as 
well as global scene information (e.g. is the system en-
gaged in an active interaction, etc.).  
Initially, when the system is idle, it uses a conserva-
tive engagement policy and waits for the user to initiate 
engagement via an explicit action. Such actions include 
the user approaching and entering in an F-formation 
(Kendon, 1990b) with the system, i.e. standing right in 
front of it, swiping their badge, or pushing the start but-
ton (in the idle state the GUI displays ?swipe your 
badge or press here to begin? below the avatar head).  
While engaged in an interaction, the system attempts 
to engage bystanders in an effort to create a collabora-
tive, multi-participant game. In this case, the engage-
ment policy is conditioned on the inferred activities of 
the agents in the scene. Specifically, if a watching bys-
tander is detected, the system temporarily disengages 
the current participant, and engages and attempts to 
?convince? the watching bystander to join the existing 
game. The prompts in this side interaction depend on 
the current game context, as shown in Table 1. If the 
watching bystander agrees to join in, the system adds 
him to the existing interaction, and continues a multi-
participant game (see Table 1.A.) Conversely, if the 
bystander refuses, the system re-engages the previous 
participant and resumes the single-user game (see Table 
1.B.) Additional examples are available in Appendix A. 
Finally, if the system is already engaged and a watch-
ing bystander is detected but only during the last ques-
tion, the system engages them temporarily to let them 
know that the current game will end shortly and, if they 
wait, they can also start a new game (see Table 1.D).  
5.3 Results and Lessons Learned 
We deployed the system described above for 20 days 
near one of the kitchenettes in our building. The system 
attracted attention of passer-bys with the tracking mo-
tion of its virtual face that followed people as they 
passed by. Most people that interacted with the system 
did so for the first time; only a small number of people 
interacted several times. No instructions were provided 
for interacting with the system. We shall now review 
results from analysis of the collected data.  
Throughout the 20 days of deployment, the system 
engaged in a total of 121 interactive events. Of these, in 
54 cases (44%), a participant engaged the system but 
did not play the game. Typically, the participant would 
approach and enter in an F-formation with the system, 
If a watching bystander B is detected during the first 3 questions  
 and the engaged participant E has not made any mistakes 
A 
 [S to E]: Hold on a second. 
[S to B]: Hi. Excuse me, would you like to join in? 
[B]:  Okay.  
[S to E&B]: Great. So let?s see how well you can do together. 
 Next question [?] 
 and the engaged participant E has just made a mistake, and 
the system did not previously engage the bystander B. 
B 
 [S to E]: Perhaps you can use some help. Hold on a 
 second. 
[S to B]: Hi. Do you think you could help out? 
[B]:  No thanks 
[S to B]: Okay. Well, I guess you can just watch for now 
 and if you wait for a bit we?ll be done and you can 
 start a new game yourself 
[S to E]: Guess you?re still on your own. Next question [?] 
 and  the engaged participant E has just made a mistake, and 
the system did previously attempt to engage the bystander B. 
C 
 [S to E]: I think you could really use some help. Hold on a 
 second. 
[S to B]: Are you sure you don?t want to help out? Come 
 on, this is fun. 
[B]:  Sure 
[S to E&B]: Great. So let?s see how well you can do together. 
 Next question [?] 
If a watching bystander B is detected during the last question 
D 
[S to E]: Excuse me for one moment. 
[S to B]: We?re almost done here. If you wait for a bit we can start 
 a new game right after 
[S to E]: Sorry about that [?] 
 Table 1. Multiparty engagement policy 
230
but, once the system engaged and asked if they would 
like to play the trivia game, they responded negatively 
or left without responding. In 49 cases (40%), a single 
participant engaged and played the game, but no bys-
tanders were observed during these interactions. In one 
case, two participants approached and engaged simulta-
neously; the system played a multi-participant game, but 
no other bystanders were observed. Finally, in the re-
maining 17 cases (14% of all engagements, 25% of ac-
tual interactions), at least one bystander was observed 
and the system engaged in multiparty interaction. These 
multiparty interactions are the focus of our observation-
al analysis, and we will discuss them in more detail.  
In 2 of these 17 cases, bystanders appeared only late 
in the interaction, after the system had already asked the 
last question. In these cases, according to its engage-
ment policy, the system notified the bystander that they 
would be attended to momentarily (see Table 1.D), and 
then proceeded to finish the initial game. In 8 of the 
remaining 15 cases (53%), the system successfully per-
suaded bystanders to join the current interaction and 
carried on a multi-participant game. In the remaining 7 
cases (47%), bystanders turned down the offer to join 
the existing game. Although this corpus is still relatively 
small, these statistics indicate that the system can suc-
cessfully engage bystanders and create and manage 
multi-participant interactions in the open world.  
Next, we analyzed more closely the responses and 
reactions from bystanders and already engaged partici-
pants to the system?s multiparty engagement actions. 
Throughout the 17 multiparty interactions, the system 
planned and executed a total of 23 engagement actions 
soliciting a bystander to enter the game, and 6 engage-
ment actions letting a bystander know that they will be 
engaged momentarily. The system actions and res-
ponses from bystanders and engaged participants are 
visually summarized in Figure 5, and are presented in 
full in Appendix B. Overall, bystanders successfully 
recognize that they are being engaged and solicited by 
the system and respond (either positively or negatively) 
in the large majority of cases (20 out of 23). In 2 of the 
remaining 3 cases, the previously engaged participant 
responded instead of the bystander; finally, in one case 
the bystander did not respond and left the area.  
While bystanders generally respond when engaged 
by the system, the system?s engagement actions towards 
bystanders also frequently elicits spoken responses from 
the already engaged participants; this happened in 14 
out of 23 cases (61%). The responses are sometimes 
addressed to the system e.g. ?Yes he does,? or towards 
the bystander, e.g. ?Say yes!?, or they reflect general 
comments, e.g. ?That?s crazy!? These results show that, 
when creating the side interaction to solicit a bystander 
to join the game, the system should engage both the 
bystander and the existing user in this side interaction, 
or at least allow the previous user to join this side inte-
raction (currently the system engages only the bystander 
in this interaction; see example from Appendix A.)  
Furthermore, we noticed that, in several cases, bys-
tanders provided responses to the system?s questions 
even prior to the point the system engaged them in inte-
raction (sometimes directed toward the system, some-
times toward the engaged participant.) We employed a 
system-initiative engagement policy towards bystanders 
in the current experiment. The initiative being taken by 
participants highlights the potential value of implement-
ing a mixed-initiative policy for engagement. If a rele-
vant response is detected from a bystander, this can be 
interpreted as an engagement action (recall from subsec-
tion 4.1 that engagement actions subsume expected 
opening dialog moves), and a mixed-initiative policy 
can respond by engaging the bystander, e.g. ?Did you 
want to join in?? or ?Please hang on, let?s let him finish. 
We can play a new game right after that.? This policy 
could be easily implemented under the proposed model.  
We also noted side comments by both bystander and 
the existing participant around the time of multiparty 
engagement. These remarks typically indicate surprise 
and excitement at the system?s multiparty capabilities. 
Quotes include: ?That?s awesome!?, ?Isn?t that great!?, 
?That?s funny!?, ?Dude!?, ?Oh my god that?s creepy!?, 
?That?s cool!?, ?It multitasks!?, ?That is amazing!?, 
?That?s pretty funny?, plus an abundance of laughter 
and smiles. Although such surprise might be expected 
today with a first-time exposure to an interactive system 
that is aware of and can engage with multiple parties, 
we believe that expectations will change in the future, as 
these technologies become more commonplace.  
Figure 5. System multiparty engagement actions and responses from bystanders and already engaged participants.  
For bystander responses,     denotes a positive response;      denotes a negative response;    denotes no response. For responses 
from previously engaged participant,      denotes utterances addressed to the bystander,      denotes side comments,      denotes 
responses directed to the system 
response from  
previously engaged 
participant 
Excuse me for one second ? Hi, would you like to join in? 
[12 cases] 
Y Y Y N Y N T N N N N N N N N N N Y Y Y Y 
Perhaps you can use some 
help? Do you think you could 
help out? [6 cases] 
[after non-understanding] 
Sorry, did you want to join 
in? [5 cases] 
We?re almost done here. If you 
wait for a bit we can start a new 
game right after  [6 cases] 
response from  
solicited bystander 
system 
prompt 
B B 
Y N 
B
B B S S S S S S C C C C C 
SC
231
Overall, this preliminary study confirmed that the 
system can effectively initiate engagement in multiparty 
settings, and also highlighted several core challenges for 
managing engagement and supporting multiparty inte-
ractions in the open world. A first important challenge 
we have identified is developing robust models for 
tracking the conversational dynamics in multiparty situ-
ations, i.e. identifying who is talking to whom at any 
given point. Secondly, the experiment has highlighted 
the opportunity for using more flexible, mixed-initiative 
engagement policies. Such policies will rely heavily on 
the ability to recognize engagement intentions; in (Bo-
hus and Horvitz, 2009b), we describe the automated 
learning of engagement intentions from interaction data. 
Finally, another lesson we learned from these initial 
experiments is the importance of accurate face tracking 
for supporting multiparty interaction. Out of the 17 mul-
tiparty interactions, 7 were affected by vision problems 
(e.g. the system momentarily lost a face, or swapped the 
identity of two faces); 4 of these were fatal errors that 
eventually led to interaction breakdowns.  
6 Summary and Future Work 
We have described a computational model for managing 
engagement decisions in open-world dialog. The model 
harnesses components for sensing and reasoning about 
the engagement state, actions, and intentions of multiple 
participants in the scene, for making high-level en-
gagement control decisions about who and when to en-
gage, and for executing and rendering these actions in 
an embodied agent. We reviewed an observational study 
that showed that, when weaved together, these compo-
nents can provide support for effectively managing en-
gagement, and for creating and conducting multiparty 
interactions in an open-world context.  
We believe that the components and policies we have 
presented provide a skeleton for engagement and inte-
raction in open-world settings. However, there are im-
portant challenges and opportunities ahead. Future re-
search includes developing methods for fine tuning and 
optimizing each of these subcomponents and their inte-
ractions. Along these lines, there are opportunities to 
employ machine learning to tune and adapt multiple 
aspects of the operation of the system. In (Bohus and 
Horvitz, 2009b) we introduce and evaluate an approach 
to learning models for inferring engagement actions and 
intentions online, through interaction.  On another direc-
tion, we are investigating the use of decision-theoretic 
approaches for optimizing mixed-initiative engagement 
policies by taking into account the underlying uncertain-
ties, the costs and benefits of interruption versus contin-
uing collaboration, queue etiquette associated with ex-
pectations of fairness, etc. Another difficult challenge is 
the creation of accurate low-level behavioral models, 
including the fine-grained control of pose, gesture, and 
facial expressions. Developing such methods will likely 
have subtle, yet powerful influences on the effectiveness 
of signaling and overall grounding in multiparty set-
tings. We believe that research on these and other prob-
lems of open-world dialog will provide essential and 
necessary steps towards developing computational sys-
tems that can embed interaction deeply into the natural 
flow of everyday tasks, activities, and collaborations. 
Acknowledgments  
We thank George Chrysanthakopoulos, Zicheng Liu, 
Tim Paek, Cha Zhang, and Qiang Wang for discussions 
and feedback in the development of this work.  
References 
M. Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cam-
bridge University Press, New York 
D. Bohus and E. Horvitz, 2009a, Open-World Dialog: Chal-
lenges, Directions and Prototype, to appear in KRPD?09, 
Pasadena, CA 
D. Bohus and E. Horvitz, 2009b, An Implicit-Learning Based 
Model for Detecting Engagement Intentions, submitted to 
SIGdial?09, London, UK 
E. Goffman, 1963, Behaviour in public places: notes on the 
social order of gatherings, The Free Press, New York 
E.T. Hall, 1966, The Hidden Dimension: man?s use of space in 
public and private, New York: Doubleday. 
A. Kendon, 1990, A description of some human greetings, 
Conducting Interaction: Patterns of behavior in focused en-
counters, Studies in International Sociolinguistics, Cam-
bridge University Press 
A. Kendon, 1990b, Spatial organization in social encounters: 
the F-formation system, Conducting Interaction: Patterns of 
behavior in focused encounters, Studies in International 
Sociolinguistics, Cambridge University Press 
M.P. Michalowski, S. Sabanovic, and R. Simmons, A spatial 
model of engagement for a social robot, in 9th IEEE Work-
shop on Advanced Motion Control, pp. 762-767 
C. Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005, 
A model of attention and interest using gaze behavior, Lec-
ture Notes in Computer Science, pp. 229-240. 
C. Peters, 2005b, Direction of Attention Perception for Con-
versation Initiation in Virtual Environments, in Intelligent 
Virtual Agents, 2005, pp. 215-228.  
C.L. Sidner, C.D. Kidd, C. Lee, and N. Lesh, 2004, Where to 
Look: A Study of Human-Robot Engagement, IUI?2004, pp. 
78-84, Madeira, Portugal 
C.L. Sidner, C. Lee, C.D. Kidd, N. Lesh, and C. Rich, 2005, 
Explorations in engagement for humans and robots, Artifi-
cial Intelligence, 166 (1-2), pp. 140-164 
Situated Interaction, 2009, Project page: 
http://research.microsoft.com/~dbohus/research_situated_i
nteraction.html 
R. Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001, 
Eye gaze patterns in conversations: there is more to con-
versational agents than meets the eyes, CHI?01 
232
  
 
1: 
 
S: 
 
 
S: 
 
 
1: 
S: 
 
1:  
S:  
 
 
1: 
S: 
1: 
S: 
 
S: 
 
 
S: 
 
 
S: 
2: 
S: 
2: 
S: 
 
 
 
S: 
 
S: 
 
 
S: 
1: 
S: 
1: 
S: 
1: 
S: 
1: 
S: 
 
 
 
 
 
S: 
 
S: 
 
S: 
 
2: 
S: 
2: 
S: 
 
S: 
 
S: 
 
 
 
 
 
1: 
S: 
2: 
S: 
 
 
 
1: 
S: 
2: 
S: 
2: 
S: 
 
 
1: 
[approaching] 
 
[engaging]  
Hi! 
 
Would you like to play a trivia game? If you answer three 
out of four questions correctly I can enter you in a raffle 
for a Zune player. 
Sure. 
Okay. First, could you please swipe your badge below 
so I can email you the raffle ticket? 
[swipes badge; in the meantime, bystander 2 appears] 
Got it! Here is the first question.  
The state with the most shark attacks each year is 
California, Florida or Hawaii? 
Florida. 
Florida. Is that final? 
Yup 
That?s right 
 
[disengaging] 
Excuse me for one second.  
 
[engaging] 
Hi 
 
Would you like to join in? 
Uh. No thanks. 
Sorry, did you want to join in? 
No.  
Well, you can just watch for now. And if you wait for a 
bit, we?ll be done and then you can start a new game 
yourself 
 
[disengaging] 
 
[engaging] 
Sorry about that.  
 
Next question.Which mammal lives the longest? 
The human 
You think it?s human, right? 
Uh no, no no.  
So what?s the correct answer? 
Elephant.  
Elephant. Is that final? 
Yup.  
Sorry but that?s actually incorrect. Current scientific 
thought is that the longest possible human life span is 
120 years, making humans the longest lived among the 
mammals. Perhaps you could use some help. Hold on 
one more second.  
 
[disengaging] 
 
[engaging] 
 
Are you sure you don?t want to help out? Come on, this 
is fun.  
Okay. 
Sorry, so did you wanna join in? 
Yes.  
Great 
 
[disengaging] 
 
[re-engaging with both 1 and 2] 
 
So let?s see if you can do better together. So far you 
have one correct answer. Now on to the next question. 
Is it true that Black Friday, the Friday after Thanksgiving 
is the busiest shopping day in the year? 
I heard that it wasn?t true actually? false. 
Do you also think that?s false? 
I guess. 
You got it! Okay, so far you have 2 out of 3 correct 
answers. If you get this last question right you can enter 
the raffle to win a Zune. How many people are killed by 
vending machines at the beach in a typical year? 
It?s probably more than one? I?ll say thirteen.  
Do you think that?s correct? 
No, thirteen.  
Is that final? 
Yes.  
Congratulations, you had 3 correct answers out of 4 
questions. I will record an entry for you in the Zune 
raffle.  
Great, thanks 
 
Bye-bye 
 
 
A
ct
iv
e 
t1 
t2 
 
t3 
t4 
t5 
t6 
t7 
t8 
t9 
t10 
t11 
t12 
t13 
t14 
t15 
Engage({1},i1) 
Maintain({1},i1) 
Disengage({1},i1) 
Engage({2},i2) 
Maintain({2},i2) 
Diseng ({2},i2) 
Engage({1},i1) 
Maintain({1},i1) 
Disengage({1},i1) 
Engage({2},i3) 
Maintain({2},i3) 
Diseng({2},i3) 
Engage({1,2},i4) 
Maintain({1,2},i4) 
Disengage({1,2},i4) 
t0 
 
Appendix A. Sample multiparty interaction with trivia game dialog system (not part of the experiment) 
 
 1 
 2 
 1 
 2 
 1 
 2 
 1 
 1 
 1 
 2 
A
ct
iv
e 
S
us
pe
nd
ed
 
S
us
pe
nd
ed
 
 1 
 2 
first person engages -  
around time t2 
bystander appears ? prior to t3 
system engages bystander ~ t5 
participants play together ~ t14 
system 
agent 
active conversa-
tion suspended conver-
sation other conversation 
A
ct
iv
e 
A
ct
iv
e 
A
ct
iv
e 
233
  
Appendix B. User responses to multiparty engagement actions.  
 
S denotes the system, E denotes the already engaged participant, B denotes a watching bystander.  
 
Actions and Resposes Response from B Response from E Timing 
[S to E]: Hold on one second. 
[S to B]: Excuse me, would you like to join in? 
   
4 positive answers from B 
7 negative answers from B 
1 no answer from B (E answers) 
Yes  B only 
Yes Say yes Overlap 
Sure  B only 
Yes [to B]: Would you like to join 
in? 
E first 
No That?s crazy! B first 
Oh, no. No + [moves away] That?s funny! B first 
No thank you  B only 
No No? B first 
Woah, no.  That?s cool! B first 
No + [moves away] + That?s 
pretty funny. 
[laughs looking at B] B first 
[laughs] [laughs] Yes. Oh yes. E only 
[S to E]: Perhaps you could use some help. 
 Excuse me for one second. 
[S to B]: Hi, do you think you could help out? 
 
3 positive answers from B 
2 negative answers from B 
1 no-answer from B (moves away) 
Yes. Yes. B first 
Yes Yes he does. Overlap 
[laughs] + No.  B only 
[to E]: Isn?t that weird? 
[to S]: No. 
[to E]: Isn?t that great? 
[to B]: That is amazing! B first 
[laughs] + [moves out] Quit E only 
[laughs] + Sure Sure B first 
If the initial response from B was not unders-
tood by the system, system asks one more time 
[S to B]: Sorry, did you want to join in? 
 
1 positive answer from B 
3 negative answer from B 
1 no-answer from B (E answers) 
No. 
Please. 
Yes, I don?t know, help me! B first 
No.  B only 
No.  B only 
No.  B only 
 No. E only 
[S to B]: We?re almost done here. If you wait 
 for a bit we can start a new game 
 right after. 
 
1 answer from B 
1 answer from E 
4 no-answer from either B or E 
Great, thanks.  B only 
 That?s awesome E only 
   
   
   
   
 
234
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 244?252,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Learning to Predict Engagement with a                                                        
Spoken Dialog System in Open-World Settings 
Dan Bohus 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
dbohus@microsoft.com 
Eric Horvitz 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052 
horvitz@microsoft.com 
 
 
Abstract 
We describe a machine learning approach that 
allows an open-world spoken dialog system to 
learn to predict engagement intentions in situ, 
from interaction. The proposed approach does 
not require any developer supervision, and le-
verages spatiotemporal and attentional features 
automatically extracted from a visual analysis 
of people coming into the proximity of the sys-
tem to produce models that are attuned to the 
characteristics of the environment the system is 
placed in. Experimental results indicate that a 
system using the proposed approach can learn 
to recognize engagement intentions at low false 
positive rates (e.g. 2-4%) up to 3-4 seconds 
prior to the actual moment of engagement.  
1 Introduction 
We address the challenge of predicting the forthcoming 
engagement of people with open-world conversational 
systems (Bohus and Horvitz, 2009a), i.e. systems that 
operate in relatively unconstrained environments, where 
multiple participants might come and go, establish, 
maintain and break the communication frame, and si-
multaneously interact with a system and with others. 
Examples of such systems include interactive billboards 
in a mall, robots in a home environment, intelligent 
home control systems, interactive systems that provide 
assistance and support during procedural tasks, etc. 
In traditional closed-world dialog systems the en-
gagement problem is generally resolved via simple, un-
ambiguous signals.  For example, engagement is gener-
ally assumed once a phone call is answered by a tele-
phony dialog system. Similarly, a push-to-talk button 
can provide a clear engagement signal for a speech 
enabled mobile application. These solutions are howev-
er inappropriate for systems that must operate conti-
nuously in open, dynamic environments, and engage 
with multiple people and groups over time. Such sys-
tems should ideally be ready to initiate dialog in a fluid, 
natural manner. They should manage engagement with 
participants who are close by, and with those who are at 
a distance, with participants who have a standing plan to 
interact with a system, and with those whom opportu-
nistically decide to engage, in-stream with their other 
ongoing activities.  In recognizing engagement inten-
tions, such systems need to minimize false positives, 
while also minimizing the unnatural delays and discon-
tinuities that come with false negatives about engage-
ment intentions. 
The work described in this paper is set in the larger 
context of a computational model for supporting fluid 
engagement in open-world dialog systems that we have 
previously described in (Bohus and Horvitz, 2009b). 
The above mentioned model harnesses components for 
sensing the engagement state, actions, and intentions of 
multiple participants in the scene, for making engage-
ment control decisions, and for rendering these deci-
sions into coordinated low-level behaviors, such as the 
changing pose and expressions of the face of an embo-
died agent. In this paper, we focus on the sensing sub-
component of this larger model and describe an ap-
proach for automatically learning to detect engagement 
intentions from interaction. 
2 Related Work 
The challenges of engagement between people, and be-
tween people and computational systems, have already 
received some attention in the conversational analysis, 
sociolinguistics, and human-computer interaction com-
munities. For instance, in an early treatise Goffman 
(1963) discusses how people use cues to detect engage-
ment in an effort to avoid the social costs of engaging in 
interaction with an unwilling participant. In later work, 
Kendon (1990a) presents a detailed investigation of 
video sequences of greetings in human-human interac-
tion, and identifies several stages of complex coordi-
nated action (pre-sighting, sighting, distance salutation, 
244
approach, close salutation), together with the head and 
body gestures that they typically involve. In (1990b), 
Kendon also introduces the notion of an F-formation, a 
pattern said to arise when ?two or more people sustain a 
spatial and orientational relationship in which they have 
equal, direct, and exclusive access,? and discusses the 
role of F-formations in establishing and maintaining 
social interactions. Argyle and Cook (1976) as well as 
others (e.g., Duncan, 1972; Vertegaal et al, 2001) have 
identified and discussed the various functions of eye 
gaze in maintaining social and communicative engage-
ment. Overall, this body of work suggests that engage-
ment is a rich, mixed-initiative, and well-coordinated 
process that involves non-verbal cues and signals, such 
as spatial trajectory and proximity, gaze and mutual 
attention, head and hand gestures, and verbal greetings.  
More recently, several researchers have investigated 
issues of engagement in human-computer and human-
robot interaction contexts. Sidner et al (2004; 2005) 
define engagement as ?the process by which two (or 
more) participants establish, maintain and end their per-
ceived connection during interactions they jointly un-
dertake,? and conduct a user study that explores the 
process of maintaining engagement. They show that 
people direct their attention to a robot more often when 
the robot makes engagement gestures throughout an 
interaction, i.e. tracks the user?s face, and points to rele-
vant objects at appropriate times in the conversation. 
Peters et al(2005a; 2005b) use an alternative defini-
tion of engagement as ?the value that a participant in an 
interaction attributes to the goal of being together with 
the other participant(s) and of continuing the interac-
tion,? and present the high-level schematics for an algo-
rithm for establishing and maintaining engagement. The 
proposed algorithm highlights the importance of eye 
gaze and mutual attention in this process and relies on a 
heuristically computed interest level to decide when to 
begin a conversation.  
Michalowski et al(2006) propose and conduct expe-
riments with a spatial model of engagement, grounded 
in proxemics (Hall, 1966). Their model classifies rele-
vant agents in the scene in four different categories 
based on their distance to the robot: present (standing 
far), attending (standing closer), engaged (next to the 
robot), and interacting (standing right in front of the 
robot). The robot?s behaviors are in turn conditioned on 
these categories: the robot turns towards attending 
people, greets engaged people and verbally prompts 
interacting people for input. The authors discuss several 
lessons learned from an observational study conducted 
with this robot in a building lobby.  They find that the 
fast-paced movements of people in the environment 
pose a number of challenges: often the robot greeted 
people too late (earlier anticipation was needed), or 
greeted people that did not intend to engage (more accu-
rate anticipation was needed). The authors recognize 
that these limitations stem partly from their reliance on 
static models, and hypothesize that temporal informa-
tion such as speed and trajectory may provide additional 
cues regarding a person?s engagement with the robot. 
In this paper, we expand on our previous work on a 
situated multiparty engagement model (Bohus and Hor-
vitz, 2009b). Specifically, we focus on a key subcom-
ponent in this model: detecting whether or not a user 
intends to engage in an interaction with a system. We 
introduce an approach that improves upon the existing 
work (Peters 2005a, 2005b; Michalowski et. al, 2006) in 
several significant ways. First, the approach is data-
driven: the use of machine learning techniques allows 
the system to adapt to the specific characteristics of its 
physical location and to the behaviors of the surround-
ing population of potential participants. Second, we 
leverage a wide array of observations, including tem-
poral features. Finally, no developer supervision is re-
quired for training the model: the supervision signal is 
extracted automatically, in-stream with the interactions, 
allowing for online learning and adaptation.  
3 Situated Multiparty Engagement Model 
To set the broader context for the work described in this 
paper, we now briefly review the overall model for 
managing engagement in an open-world setting intro-
duced in (Bohus and Horvitz, 2009b). The model is cen-
tered on a reified notion of interaction, defined as a ba-
sic unit of sustained, interactive problem-solving. Each 
interaction can involve two or more participants, and 
this number may vary in time; new participants may 
join an existing interaction and current participants may 
leave an interaction at any point in time. The system is 
actively engaged in at most one interaction at a time 
(with one or multiple participants), but it can simulta-
neously keep track of additional, suspended interactions. 
In this context, engagement is viewed as the process 
subsuming the joint, coordinated activities by which 
participants initiate, maintain, join, abandon, suspend, 
resume, or terminate an interaction. 
Successfully managing this process requires that the 
system (1) senses and reasons about the engagement 
state, actions and intentions of multiple agents in the 
scene, (2) makes high-level engagement control deci-
sions (i.e. about whom to engage or disengage with, and 
when) and (3) executes and signals these decisions to 
the other participants in an appropriate manner (e.g. via 
a set of coordinated behaviors such as gestures, greet-
ings, etc.) The proposed model, illustrated in Figure 1, 
subsumes these three components. 
The sensing subcomponent in the model tracks the 
engagement state, engagement actions, and engagement 
intention for each agent in the visual scene. The en-
gagement state, ???
? (?), denotes whether an agent ? is 
245
Figure 1. Graphical model showing key variables and 
dependencies in managing engagement. 
ES 
EA 
EI 
t   t+1 
SEA 
ES 
? 
EI 
A 
? 
a
d
d
it
io
n
a
l 
c
o
n
te
x
t 
e
n
g
a
g
e
m
e
n
t 
s
e
n
s
in
g
 
? 
G 
A 
G 
EA 
SEB 
? 
engaged in interaction ? and is modeled as a determinis-
tic variable with two possible values: engaged and not-
engaged. The state is updated based on the joint actions 
of the system and the agent.  
A second engagement variable, ???
? (?), models the 
actions that an agent takes to initiate, maintain or termi-
nate engagement. There are four possible engagement 
actions: engage, no-action, maintain, disengage. These 
actions are tracked by means of a conditional probabilis-
tic model that takes into account the engagement state 
???
? (?), the previous agent and system actions, as well 
as additional sensory evidence ? capturing committed 
engagement actions, such as: salutations (e.g. ?Hi!?); 
calling behaviors (e.g. ?Laura!?); the establishment or 
the breaking of an F-formation (Kendon, 1990b); ex-
pected opening dialog moves (e.g. ?Come here!?) etc.  
A third variable in the proposed model, ???
? (?) , 
tracks whether or not each agent intends to be engaged 
in a conversation with the system. Like the engagement 
state, the intention can either be engaged or not-
engaged. Intentions are tracked separately from actions 
since an agent might intend to engage or disengage the 
system, but not yet take an explicit engagement action. 
For instance, let us consider the case in which the sys-
tem is already engaged in an interaction and another use 
is waiting in line to interact with the system: although 
the waiting user does not take an explicit, committed 
engagement action, she might signal (e.g. via a glance 
that makes brief but clear eye contact with the interac-
tive system) that her intention is to engage in a new 
conversation once the opportunity arises. More general-
ly, the engagement intention captures whether or not an 
agent would respond positively should the system in-
itiate engagement. In that sense, it roughly corresponds 
to Peters? (2005; 2005b) ?interest level?, i.e. to the value 
the agent attaches to being engaged in a conversation 
with the system. Like engagement actions, engagement 
intentions are inferred based on probabilistic models 
that take into account the current engagement state, the 
previous agent and system actions, the previous en-
gagement intention, as well as additional evidence that 
captures implicit engagement cues, e.g. the spatiotem-
poral trajectory of the participant, the level of sustained 
mutual attention, etc.  
Based on the inferred engagement state, actions, and 
intentions of the agents in the scene, as well as other 
additional high-level evidence such as the agents? in-
ferred goals (?), activities (?) and relationships (?), the 
proposed model outputs engagement actions ? denoted 
by the ??? decision node in Figure 1. The action-space 
consists of the same four actions previously discussed: 
engage, disengage, maintain and no-action. At the low-
er level, the engagement decisions taken by the system 
are translated into a set of coordinated lower-level be-
haviors (???) such as head gestures, making eye con-
tact, facial expressions, salutations, interjections, etc. 
In related work (Bohus and Horvitz, 2009a; 2009b), 
we have demonstrated how this model can be used to 
effectively create and support multiparty interactions in 
an open-world context. Here, we focus on one specific 
subcomponent of this framework: the model for detect-
ing engagement intentions.  
4 Approach 
To illustrate the problem of detecting engagement inten-
tions, consider for instance a situated conversational 
system that examines through its sensors the scenes 
from Figure 3. How can such a system detect whether 
the person in the image intends to engage in a conversa-
tion or is just passing-by? Studies of human-human 
conversational engagement (Goffman, 1963; Argyle and 
Cook, 1976; Duncan, 1972; Kendon, 1990, 1990b) indi-
cate that people signal and detect engagement intentions 
by producing and monitoring for a variety of cues, in-
cluding gaze and sustained attention, trajectory and 
proximity, head and hand gestures, body pose, etc.  
In the proposed approach, we use machine learning 
techniques, and leverage a wide array of observations 
from the sensors to create a model that allows an open-
world interactive system to detect the specific patterns 
characterizing an engagement intention. Existing work 
on detecting engagement intentions has focused on stat-
ic heuristic models that leverage proximity and attention 
features (Peters, 2005, 2005b; Michalowski, 2006). As 
previously discussed, psychologists have shown the 
important role played by geometric relationships, trajec-
tories, and sustained attention in signaling and detecting 
engagement. The use of machine learning allows us to 
consider a wide array of such features, including trajec-
tory, speed, and the attention of agents over time. 
246
Figure 3. Placement and visual fields of view for  
side (right) and front (left) orientations. 
pillar 
Kitchenette 
Corridor 
pillar 
Kitchenette 
Corridor 
In general, as discussed in the previous section, the 
engagement intentions of an agent may evolve tempo-
rally under the proposed model, as a function of the 
various system actions and behaviors (e.g. an embodied 
system that makes eye contact, or smiles, or moves to-
ward a participant might alter the engagement intention 
of that participant). In this work we concentrate on a 
simplified problem, in which the system?s behavior is 
fixed (e.g. system always tracks people that pass by), 
and the engagement intention can be assumed constant 
within a limited time window. 
The central idea of the proposed approach is to start 
by using a very conservative (i.e., low false-positives) 
detector for engagement intentions, such as a push-to-
engage button, and automatically gather sensor data 
surrounding the moments of engagement, together with 
labels that indicate whether someone actually engaged 
or not. Note that the system eventually finds out if a 
person becomes engaged with it. If we assume that an 
intention to engage existed for a limited window of time 
prior to the moment of engagement, the collected data 
can be used to learn a model for predicting this intention 
ahead of the actual moment of engagement. The pro-
posed approach therefore enables a system to learn in-
situ models for predicting forthcoming engagement, and 
the models are attuned to the specifics of the environ-
ment the system is in. No explicit developer supervision 
is required, as the training labels are extracted automati-
cally from interaction.  
5 Experimental Setup 
To provide an ecologically valid basis for data collec-
tion and for evaluating the proposed approach, we de-
veloped a situated conversational agent and deployed it 
in the real-world. The system, illustrated in Figure 2, is 
an interactive multimodal kiosk that displays a realisti-
cally rendered avatar head. The avatar can engage and 
interact via natural language with one or more partici-
pants, and plays a simple game in which the users have 
to respond to multiple-choice trivia questions. The sys-
tem, and sample interactions are described in more de-
tail in (Bohus and Horvitz, 2009.) 
The hardware and software architecture is also illu-
strated in Figure 2. Data gathered from a wide-angle 
camera, a 4-element linear microphone array, and a 19? 
touch-screen is forwarded to a scene analysis module 
that fuses the incoming streams and constructs in real-
time a coherent picture of the dynamics in the surround-
ing environment. The system detects and tracks the lo-
cation of multiple agents in the scene, tracks the head 
pose for engaged agents, and infers the focus of atten-
tion, activities, goals and (group) relationships among 
different agents in the scene. An in-depth description of 
these scene analysis components falls beyond the scope 
of this paper, but more details are available in (Bohus 
and Horvitz, 2009). The scene analysis results are for-
warded to the control level, which is structured in a two-
layer reactive-deliberative architecture. The reactive 
layer implements and coordinates low-level behaviors, 
including engagement, conversational floor manage-
ment and turn-taking, and coordinating spoken and ges-
tural outputs. The deliberative layer plans the system?s 
dialog moves and high-level engagement actions. 
We deployed the system described above in an open-
space near the kitchenette area in our building. As we 
were interested in exploring the influence of the spatial 
setup on the engagement models, we deployed the sys-
tem in two different spatial orientations, illustrated to-
gether with the resulting visual fields of view in Figure 
3. Even though the location is similar, the two orienta-
tions create considerable differences in the relative tra-
jectories of people that go by (dashed lines) and people 
that engage with the system (continuous lines). In the 
side orientation, people typically enter the system?s field 
Figure 2. System prototype and architectural overview. 
Dialog Management 
Behavioral Control 
Scene Analysis Output Planning 
Vision Speech Synthesis Avatar 
wide-angle camera 
4-element linear microphone array  
touch screen 
speakers 
247
of view and approach it from the sides. In the front 
orientation, people enter the field of view and approach 
either frontally, or from the immediate right side.  
6 Data and Modeling 
The system was deployed during regular business hours 
for 10 days in each of the two orientations described 
above, for a total of 158 hours and 32 minutes. No in-
structions were provided and most people that interacted 
with the system did so for the first time.  
6.1 Corpus and Implicit Labels 
Throughout the data collection, the system used a con-
servative heuristic to detect engagement intentions: it 
considered that a user wanted to engage when they ap-
proached the system and entered in an F-formation 
(Kendon, 1990b) with it. Specifically, if a sufficiently 
large (close by) frontal face was detected in front of it, 
the system triggered an engaging action and started the 
interaction. We found this F-formation heuristic to be 
fairly robust, having a false-positive rate of 0.18% (6 
false engagements out of 3274 total faces tracked). In 2 
of these cases the face tracker committed an error and 
falsely identified a large nearby face, and in 4 cases a 
person passed by very close to the system but without 
any visible intention to engage.   
Although details on false-negative statistics have not 
yet been calculated (this would require a careful exami-
nation of all 158 hours of data), our experience with the 
face detector suggests this number is near 0. In months 
of usage, we never observed a case where the system 
failed to detect a close by, frontal face. At the same time, 
we note that there is an important distinction between 
people who actually engage with the system, and people 
who intend to engage, but perhaps not come in close-
enough proximity for the system to detect this intention 
(according to the heuristic described above). In this 
sense, while our heuristic can detect people who engage 
at a 0 false-negative rate, the false-negative rate with 
respect to engagement intentions is non-zero. Despite 
these false-negatives, we found that the proposed heu-
ristic still represents a good starting point for learning to 
detect engagement intentions. As we shall see later, em-
pirical results indicate that, by learning to detect who 
actually engages, the system can learn to also detect 
people who might intend to engage, but who ultimately 
do not engage with the dialog system.  
In the experiments described here, we focus on de-
tecting engagement intentions for people that ap-
proached while the system was idle. We therefore au-
tomatically eliminated all faces that were temporally 
overlapping with the periods when the system was al-
ready engaged in an interaction. For the remaining face 
traces, we automatically generate labels as follows: 
? if a person entered in an F-formation and became 
engaged in interaction with the system at time ?? , 
the corresponding face trace was labeled with a 
positive engagement intention label from ??-20sec; 
until ?? ; the initial portion of the trace, from the 
moment it was detected until ??-20sec was marked 
with a negative engagement intention label. Final-
ly, the remainder of the trace (from ??  until the 
face disappeared) was discarded, as the user was 
actively engaged with the system during this time.  
? if the face was never engaged in interaction (i.e. a 
person was just passing by), the entire trace was 
labeled with a negative engagement intention.  
Note that in training the models described below we 
used these automatic labels, which are not entirely accu-
rate: they include a small number of false-positives, as 
discussed above. However, for evaluation purposes, we 
used the corrected labels (no false-positives). 
6.2 Models 
To review, the task at hand is to learn a model for pre-
dicting engagement intentions, based on information 
that can be extracted at runtime from face traces, includ-
ing spatiotemporal trajectory and cues about attention. 
We cast this problem as a frame-by-frame binary classi-
fication task: at each frame, the model must classify 
each visible face as either intending to engage or not. 
We used a maximum entropy model to make this pre-
diction:  
 
? ?? ? =
1
?(?)
???   ?? ? ??(?)
?
  
 
The key role in the proposed maximum entropy 
model is played by the set of features ??(?), which must 
capture cues that are relevant for detecting an engage-
ment intention. We designed several subsets of features, 
summarized in Table 2. The location subset, loc, in-
cludes the x and y location of the detected face in the 
visual scene, and the width and height of the face region, 
which indirectly reflect the proximity of the agent. The 
second feature subset, loc+ff, also includes a probability 
score (and a binarized version of it) produced by the 
face detector which reflects the confidence that the face 
is frontal and thus provides an automatic measure of the 
focus-of-attention of the agent. Apart from these auto-
Table 1. Corpus statistics. 
 Side Front Total 
Size (hours:minutes) 83:16 75:15 158:32 
# face traces 2025 1249 3274 
# engaged 
% engaged  
72 
3.55% 
74 
5.92% 
146 
4.46% 
# false-positive engaged 
% false-positive engaged 
1 
0.04% 
5 
0.40% 
6 
0.18% 
# not-engaged  
% not-engaged  
1953 
96.45% 
1175 
94.08% 
3128 
95.54% 
 
248
matically generated attention features, we also experi-
mented with a manually annotated binary attention 
score, attn. The attention of each detected face was ma-
nually tagged throughout the entire dataset. This infor-
mation is not available to the system at runtime; we use 
it only to identify an upper performance baseline.   
The maximum entropy model is not temporally struc-
tured. The temporal structure of the spatial and atten-
tional trajectory is captured via a set of additional fea-
tures, derived as follows. Given an existing feature f, we 
compute a set of trajectory features traj.w(f) by accumu-
lating aggregate statistics for the feature f over a past 
window of size w frames. We explored windows of size 
5, 10, 20, 30. For continuous features, the trajectory 
statistics include the min, max, mean, and variance of 
the features in the specified window. In addition, we 
performed a linear and a quadratic fit of f in this window, 
and used the resulting coefficients (2 for the linear fit 
and 3 for the quadratic fit) as features (see the example 
in Figure 4). For the binary features, the trajectory sta-
tistics include the number and proportion of times the 
feature had a value of 1 in the given window, and the 
number of frames since the feature last had a value of 1.  
7 Experimental Results 
We trained and evaluated (using a 10-fold cross-
validation process) a set of models for each of the two 
system orientations shown in Figure 3 and for each of 
the 5 feature subsets shown in Table 2. The results on 
the per-frame classification task, including the ROC 
curves for the different models are presented and dis-
cussed in more detail in Appendix A.  
At runtime, the system uses these frame-based mod-
els to predict across time the likelihood that a given 
agent intends to engage (see Figure 5). In this context, 
an evaluation that counts the errors per person (i.e., per 
trace), rather than errors per frame is more informative. 
Furthermore, since early detection is important for sup-
porting a natural engagement process, an informative 
evaluation should also capture how soon a model can 
detect a positive engagement intention (see Figure 5).  
Making decisions about an agent?s engagement in-
tentions typically involves comparing the probability of 
engagement against a preset threshold. Given a thre-
shold, we can compute for each model the number of 
false-positives at the trace level: if the prediction ex-
ceeds the threshold at any point in the trace, we consider 
that a positive detection. We note that, if we aim to 
detect people who will actually engage, there are no 
false negatives at the trace level. The system can use the 
machine learned models in conjunction with the pre-
vious heuristic (a user is detected standing in front of 
the system), to eventually detect when people engage. 
Also, given a threshold, we can identify how early a 
model can correctly detect the intention to engage 
(compared to the existing F-formation heuristic that 
defined the moment of engagement in the training data). 
These durations are illustrated for a threshold of 0.5 in 
Figure 5, and are referred to in the sequel as early detec-
tion time. By varying the threshold between 0 and 1, we 
can obtain a profile that links the false-positive rate at 
the trace level to how early the system can detect en-
gagement, i.e. to the mean early detection time.  
Figure 6 shows the false-positive rate as a function of 
the mean early detection time for models trained using 
each of the five feature subsets shown in Table 2, in the 
side orientation. The model that uses only location in-
formation (including the size of the face and proximity) 
performs worst. Adding automatically extracted infor-
mation about attention leads only to a marginal im-
provement. However, adding information about the tra-
Feature sets Description [total # of features in set] 
Loc location features: x, y, width and height [4] 
loc+ff 
location features plus a confidence score indicat-
ing whether the face is frontal (ff), as well as a 
binary version of this score (ff=1) [6] 
traj(loc) 
location features plus trajectory of location fea-
tures over windows of 5, 10, 20, 30 frames [118] 
traj(loc+ff) 
location and face frontal features, as well as 
trajectory of location and of face-frontal features 
over windows of 5, 10, 20, 30 frames [172] 
traj(loc+attn) 
location and manually labeled attention features, 
as well as trajectory of location and of attention 
over windows of 5, 10, 20, 30 frames [133] 
 
Table 2. Feature sets for detecting engagement intention. 
0 10 20 30 40 50
100
200
300
400
500
600
30 frame window 
current frame 
x 
Figure 4. Trajectory features extracted by fitting linear and 
quadratic functions. 
Figure 5. Example predictions for three different models. 
0 5 10 15
0
0.5
1
0
50
100
0
640
0
0.5
1
x 
width 
frontal 
traj(loc+ff) 
traj(loc) 
loc early detection time = 10.4 sec 
5.4 sec 
4.0 sec 
249
jectory of location and of attention, leads to larger cu-
mulative gains. Adding the more accurate (manually 
tagged) information about attention yields the best mod-
el. The relative performance of these models (which can 
be observed at the frame-level in Appendix A) confirms 
our expectations and the importance of trajectory fea-
tures (both spatial and attentional) in detecting engage-
ment intentions. The results also indicate that the differ-
ences, and hence the importance of these features, are 
larger when trying to detect engagement early on, i.e. at 
larger early detection times. Tables 3 and 4 further high-
light these differences. For instance, when detecting 
engagement intentions at a mean early detection above 3 
seconds, the model that uses trajectory information, 
traj(loc+ff), decreases the false positive rate by a factor of 
3 compared to the location-only model.  
Figure 7 and Tables 5 and 6 show the results for the 
front orientation. The relative trends are similar to those 
observed in the side orientation, highlighting again the 
importance of trajectory features. At the same time, the 
models are performing slightly worse in absolute terms, 
which is consistent with the increased difficulty of the 
task. Several contributing factors can be identified in 
Figure 3: people may simply pass by in closer proximity 
to the system; people who come from the corridor are 
generally frontally oriented towards the system, making 
frontal face cues less informative; and finally, people 
who will engage need to deviate less from the regular 
trajectory of people who are just passing by.   
Next, we review how well the models trained gene-
ralize across the two different setups, by evaluating the 
trajectory models traj(loc+ff) across the two datasets. The 
results indicate that the models are attuned to the dataset 
they are trained on (see Figure 7). As we discussed ear-
lier, we expect this result given the different geometry 
of the relative trajectories of engagement in the two 
orientations. These results highlight the importance of 
learning in situ, and show that the proposed approach 
can be used to learn the specific patterns of engagement 
in a given environment automatically, without explicit 
developer supervision.  
Finally, we performed an error analysis. We focused 
on the side orientation and visually inspected the 79 
(4%) false-positive errors committed by the traj(loc+ff) 
Model 
Early detection time 
FP=2.5% FP=5% FP=10% FP=20% 
loc 1.14 1.97 2.29 2.92 
loc+ff 1.70 2.25 2.74 3.18 
traj(loc) 1.93 2.57 3.13 3.66 
traj(loc+ff) 1.99 2.64 3.44 4.02 
traj(loc+attn) 1.97 2.47 3.52 4.15 
 
Model 
Early detection time 
FP=2.5% FP=5% FP=10% FP=20% 
loc 2.18 2.72 3.09 3.59 
loc+ff 2.25 2.74 3.08 3.63 
traj(loc) 2.51 3.03 3.53 4.07 
traj(loc+ff) 2.68 3.20 3.68 4.22 
traj(loc+attn) 3.08 3.52 4.13 4.49 
 
Figure 6. False-positives vs. early detection time (side). 
F
a
ls
e
 p
o
s
it
iv
e
s
 
Mean early detection time (seconds) 
F
a
ls
e
 p
o
s
it
iv
e
s
 
Mean early detection time (seconds) 
0 1 2 3 4 5
0%
10%
20%
30%
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
traj(loc+attn)
0 1 2 3 4 5
0%
10%
20%
30%
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
raj(loc+attn)
Table 3. *False-positive rate at different EDT (side) Table 5. *False-positive rate at different EDT (front) 
Table 4.*Early detection times at different FP rates (side). Table 6 * Early detection times at different FP rates (front). 
 
Figure 7. False-positives vs. early detection time (front). 
Model 
False positive rate 
EDT=1 EDT=2 EDT=2.5 EDT=3 EDT=3.5 EDT=4 
loc 0.31% 1.6% 4.3% 9.4% 18.4% 32.6% 
loc+ff 0.31% 1.5% 4.1% 8.7% 18.3% 28.6% 
traj(loc) 0.31% 1.1% 2.6% 4.8% 9.3% 18.6% 
traj(loc+ff) 0.15% 0.9% 2.0% 4.0% 7.1% 14.3% 
traj(loc+attn) 0.26% 0.6% 1.1% 2.2% 5.1% 8.9% 
 
Model 
False positive rate 
EDT=1 EDT=2 EDT=2.5 EDT=3 EDT=3.5 EDT=4 
loc 2.3% 5.8% 11.3% 23.0% 35.2% 44.5% 
loc+ff 1.6% 3.7% 7.3% 15.8% 28.5% 41.7% 
traj(loc) 1.1% 3.1% 4.7% 8.2% 15.6% 36.8% 
traj(loc+ff) 1.2% 2.7% 4.7% 7.2% 10.9% 19.8% 
traj(loc+attn) 0.8% 2.9% 5.4% 5.4% 10.3% 16.1% 
 
*shaded cells in Tables 3-6 show statistically significant improvements in performance (p<0.05) over the corresponding model that uses the immediately previous 
feature set (e.g. the cell right above). The traj(loc), traj(loc+ff), traj(loc+attn) always statistically significantly (p<0.05) improve upon the loc models 
250
model when using a threshold corresponding to a mean 
early detection time of 3 seconds. This analysis indi-
cates that in 22 out of these 79 errors (28%) the person 
did actually exhibit behaviors consistent with an inten-
tion to engage the system, such as stopping by or turn-
ing around after passing the system, and approaching 
and maintaining sustained attention for a significant 
amount of time. These cases represent false-negatives 
committed by our conservative F-formation heuristic 
with respect to engagement intention; the user did not 
approach close enough for the system to trigger en-
gagement. The actual false-positive rate of the trained 
model is therefore 2.9% rather than 4%. The system was 
able to correctly identify these cases because the beha-
vioral patterns are similar to the ones exhibited by 
people who did approach close enough for the heuristic 
detector to fire. We plan to assess the false-negative rate 
of the current heuristic more closely and explore how 
many false negatives are actually recovered by the 
trained model.  This analysis will require that multiple 
judges assess engagement intentions on all 3274 traces.  
8 Summary and Future Work 
We described an approach to learning engagement in-
tentions in a situated conversational system. The pro-
posed models fit into a larger framework for supporting 
multiparty, situated engagement and open-world dialog 
(Bohus and Horvitz, 2009a; 2009b). Experimental re-
sults indicate that a system using the proposed approach 
can learn to detect engagement intentions at low false 
positive rates up to 3-4 seconds prior to the actual mo-
ment of engagement. The models leverage features that 
capture spatiotemporal and attentional cues that are 
tuned to the specifics of the physical environment in 
which the system operates. Furthermore, the models can 
be trained in previously unseen environments, without 
any explicit developer supervision. 
We believe the methods and results described 
represent a first step towards supporting fluid, natural 
engagement in open-world interaction. Numerous chal-
lenges remain. While we confirmed the importance of 
spatiotemporal and attentional features in detecting en-
gagement intentions, we believe that leveraging addi-
tional and more accurate sensory information (e.g. body 
pose, eye gaze, more accurate depth information, agent 
identity coupled with longer term memory features) 
may improve performance. Secondly, while the current 
models where trained in a batch fashion, the proposed 
method naturally lends itself to an online approach, 
where the system starts with a prior model for detecting 
engagement intentions, and refines this model online. 
More importantly, rather than just learning to detect 
engagement intentions, we plan to focus on the more 
general problem of controlling the engagement process: 
how should the system time its actions (i.e. gaze and 
sustained attention, smiles, greeting, etc.) to create natu-
ral, fluid engagements in the open world. Introducing 
mobility to dialog systems brings yet another interesting 
dimension to this problem: how can a mobile system, 
such as a robot, detect engagement intentions and re-
spond to support a natural engagement process? We 
believe that there is great opportunity to address these 
challenges by learning predictive models from data.  
References 
M. Argyle and M. Cook, 1976, Gaze and Mutual Gaze, Cam-
bridge University Press, New York 
D. Bohus and E. Horvitz, 2009a, Open-World Dialog: Chal-
lenges, Directions and Prototype, to appear in KRPD?09, 
Pasadena, CA 
D. Bohus and E. Horvitz, 2009b, Computational Models for 
Multiparty Engagement in Open-World Dialog, submitted 
to SIGdial?09, London, UK.  
E. Goffman, 1963, Behaviour in public places: notes on the 
social order of gatherings, The Free Press, New York 
E.T. Hall, 1966, The Hidden Dimension: man?s use of space in 
public and private, New York: Doubleday. 
A. Kendon, 1990a, A description of some human greetings, 
Conducting Interaction: Patterns of behavior in focused en-
counters, Studies in International Sociolinguistics, Cam-
bridge University Press 
A. Kendon, 1990b, Spatial organization in social encounters: 
the F-formation system, Conducting Interaction: Patterns of 
behavior in focused encounters, Studies in International 
Sociolinguistics, Cambridge University Press 
M.P. Michalowski, S. Sabanovic, and R. Simmons, A spatial 
model of engagement for a social robot, in 9th IEEE Work-
shop on Advanced Motion Control, pp. 762-767 
C. Peters, C. Pelachaud, E. Bevacqua, and M. Mancini, 2005a, 
A model of attention and interest using gaze behavior, Lec-
ture Notes in Computer Science, pp. 229-240. 
C. Peters, 2005b, Direction of Attention Perception for Con-
versation Initiation in Virtual Environments, in Intelligent 
Virtual Agents, 2005, pp. 215-228.  
C.L. Sidner, C.D. Kidd, C. Lee, and N. Lesh, 2004, Where to 
Look: A Study of Human-Robot Engagement, IUI?2004, pp. 
78-84, Madeira, Portugal 
C.L. Sidner, C. Lee, C.D. Kidd, N. Lesh, and C. Rich, 2005, 
Explorations in engagement for humans and robots, Artifi-
cial Intelligence, 166 (1-2), pp. 140-164 
R. Vertegaal, R. Slagter, G.C.v.d.Veer, and A. Nijholt, 2001, 
Eye gaze patterns in conversations: there is more to con-
versational agents than meets the eyes, CHI?01  
Figure 7. Model evaluation across orientations. 
 
0 1 2 3 4 5
0%
10%
20%
30%
 
 
traj(loc+ff) trained on side data
traj(loc+ff) trained on front data
F
a
ls
e
 p
o
s
it
iv
e
s
 
F
a
ls
e
 p
o
s
it
iv
e
s
 
Mean early detection time 
0 1 2 3 4 5
0%
10%
20%
30%
 
 
traj(loc+ff) trained on front data
traj(loc+ff) traine  on side data
Mean early detection time 
Evaluation on side data Evaluation on front data 
251
  
 
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
traj(loc+attn)
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
 
 
loc
loc+ff
traj(loc)
traj(loc+ff)
traj(loc+a tn)
Appendix A. Per-frame evaluation of maximum entropy models for detecting engagement intentions 
 
Model Avg. log-likelihood Hard error 
Base Train CV Base Train CV 
loc -0.1651 -0.1222 -0.1259 3.91% 3.22% 3.25% 
loc+ff -0.1651 -0.0962 -0.0984 3.91% 3.01% 3.07% 
traj(loc) -0.1651 -0.0947 -0.1073 3.91% 2.88% 3.06% 
traj(loc+ff) -0.1651 -0.0836 -0.0904 3.91% 2.69% 2.85% 
traj(loc+attn) -0.1651 -0.0765 -0.0810 3.91% 2.47% 2.56% 
 
Figure 1. Per-frame ROC for side orientation models 
T
ru
e
 p
o
s
it
iv
e
s
 (
s
e
n
s
it
iv
it
y
) 
False positives (1-specificity) 
Figure 2. Per-frame ROC for front orientation models 
 
False positives (1-specificity) 
T
ru
e
 p
o
s
it
iv
e
s
 (
s
e
n
s
it
iv
it
y
) 
Model Avg. log-likelihood Hard error 
Base Train CV Base Train CV 
loc -0.1875 -0.1451 -0.1498 4.63% 4.58% 4.72% 
loc+ff -0.1875 -0.1326 -0.1392 4.63% 4.22% 4.39% 
traj(loc) -0.1875 -0.1262 -0.1338 4.63% 3.99% 4.24% 
traj(loc+ff) -0.1875 -0.1159 -0.1298 4.63% 3.91% 4.38% 
traj(loc+attn) -0.1875 -0.1150 -0.1267 4.63% 4.04% 4.47% 
 
Table 1. Baseline, training-set and cross-validation 
performance (data average log-likelihood and classifi-
cation error) for side orientation models 
Table 2. Baseline, training-set and cross-validation 
performance (data average log-likelihood and classifi-
cation error) for front orientation models 
252
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 98?109,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Multiparty Turn Taking in Situated Dialog: 
Study, Lessons, and Directions 
 
 
Dan Bohus Eric Horvitz 
Microsoft Research Microsoft Research 
One Microsoft Way One Microsoft Way 
Redmond, WA, 98052 Redmond, WA, 98052 
dbohus@microsoft.com horvitz@microsoft.com 
 
 
 
 
 
 
Abstract 
We report on an empirical study of a multiparty 
turn-taking model for physically situated spo-
ken dialog systems. We present subjective and 
objective performance measures that show how 
the model, supported with a basic set of sensory 
competencies and turn-taking policies, can en-
able interactions with multiple participants in a 
collaborative task setting. The analysis brings 
to the fore several phenomena and frames chal-
lenges for managing multiparty turn taking in 
physically situated interaction.  
1. Introduction 
Effective dialog relies on the coordination of con-
tributions by participants in a conversation via turn 
taking. The complexity of understanding and man-
aging turns grows significantly in moving from 
dyadic to multiparty settings, including situations 
where groups of people converse as they collabo-
rate on shared goals. We are exploring computa-
tional methods that can endow dialog systems with 
the ability to participate in a natural, fluid manner 
in conversations involving several people.  
In Bohus and Horvitz (2010a), we presented a 
computational model for managing multiparty turn 
taking. The model harnesses multisensory percep-
tion and reasoning and includes a set of compo-
nents and representations.  These include methods 
for tracking multiparty conversational dynamics, 
for making turn-taking decisions, and for rendering 
decisions about turns into an appropriate set of 
low-level, coordinated gaze, gesture and speech 
behaviors. We implemented the model and have 
been testing it in several domains. The investiga-
tions have been aimed at characterizing the sys-
tem?s performance in complex multiparty settings.  
In Bohus and Horvitz (2010b), we examine data 
collected during a user study to evaluate the ability 
of the system to shape the flow of multiparty con-
versational dynamics.  In this paper, we focus our 
attention on the performance of the inference and 
decision-making models.  We analyze the accuracy 
of current turn-taking inferences, the influence of 
inference errors on decisions, and the overall effec-
tiveness of the system?s decision making. We re-
port on subjective and objective measures of the 
system?s turn-taking performance. We find that the 
turn-taking methodology enables our system to 
successfully participate in multiparty interactions, 
even when relying on relatively coarse models for 
inference and decision making.  The analysis high-
lights several general phenomena including stand-
ing bottlenecks and difficulties, and opportunities 
for enhancing multiparty turn taking in dialog sys-
tems. Based on the results, we discuss challenges 
and directions for research on turn taking in physi-
cally situated dialog.   
2. Related Work 
We begin by placing this work within the larger 
context of research on multiparty interaction and 
turn taking. In a seminal paper on turn taking in 
natural conversations, Sacks, Schegloff and Jeffer-
son (1974) proposed a basic model for the organi-
98
zation of turns in conversation. The model is cen-
tered on the notion of turn-constructional-units, 
separated by transition relevance places that pro-
vide opportunities for speaker changes. In later 
work, Schegloff (2000) elaborates on several as-
pects of this model, including interruptions and 
overlap resolution devices. Other researchers in 
conversational analysis and psycho-linguistics 
have highlighted the important role played by gaze, 
gesture, and other non-verbal communication 
channels in regulating turn taking. For instance, 
Duncan (1972) discusses the role of non-verbal 
signals, and proposes that turn taking is mediated 
via a set of verbal and non-verbal cues. Wiemann 
and Knapp (1975) survey prior investigations on 
turn-taking cues in several conversational settings, 
in an effort to elucidate differences. Goodwin 
(1980) discusses various aspects of the relationship 
between turn taking and attention. More recently, 
Hjalmarsson (2011) investigates the additive effect 
turn-taking cues have on listeners in both human 
and synthetic voices.  
 
Figure 1. Components of turn-taking model. 
S(s) A(s) 
Sensing 
CONTRIB 
Decisions 
Behavioral Control 
Behaviors and  
Output Management 
System 
Floor 
Action 
D
ia
lo
g 
m
an
ag
em
en
t 
Contribute Audio-visual 
evidence 
Dialog 
Context 
Speech 
Gaze 
Gesture 
Semantic 
Input 
Semantic 
Output 
FS(p) FI(p) 
FA(p) 
? 
Tu
rn
 m
an
ag
em
en
t 
Within the dialog systems community, efforts 
have been made on designing and implementing 
computational models for managing turn taking 
(e.g., Traum, 1994; Thorriss?n, 2002; Raux and 
Eskenazi, 2009; Selfridge and Heeman, 2010). 
Moving beyond the dyadic setting, Traum and 
Rickel (2002) describe a turn management compo-
nent for supporting dialog between a trainee and 
multiple virtual humans. Kronlid (2006) describes 
a Harel state-chart implementation of the original 
SSJ model. Researchers studying human-robot in-
teraction have developed prototype robots that can 
interact with multiple human participants (e.g. Ma-
tsusaka et al, 2001; Bennewitz et al, 2005). In our 
previous work Bohus and Horvitz (2009; 2010a; 
2010b), we describe a platform that leverages mul-
timodal perception and reasoning to support multi-
party dialog in open-world settings. 
3. Multiparty Turn-Taking Model 
We engaged in a set of experiments to probe the 
inference and decision making competencies of a 
computational model for multiparty turn taking 
(Bohus and Horvitz 2010a; 2010b). To set the 
stage for the analysis to follow, we briefly review 
the proposed approach. 
We model turn taking as an interactive, collabo-
rative process by which participants in a conversa-
tion monitor one another and take coordinated ac-
tions to ensure that (generally) only one person 
speaks at a given time. The participant ratified to 
speak via this process is said to have the floor. 
Each participant engaged in the interaction con-
tinuously produces (i.e. at every time tick) one of 
four floor management actions: a hold action indi-
cates that a participant is maintaining the floor; a 
release action indicates that the participant is 
yielding the floor to a set of other participants 
(which could be void, allowing for self-selection 
next turn allocation); a take action indicates that 
the participant is trying to acquire the floor; finally, 
a null action indicates that a participant is not mak-
ing any floor claims. The floor shifts from one par-
ticipant to another as the result of the joint, coop-
erative floor management actions taken by the par-
ticipants. Specifically, a release action must be met 
with a take action for a floor shift to occur; in all 
other cases the floor stays with the participant that 
currently holds it.  
Figure 1 illustrates the main components and 
key abstractions in the model. The sensing sub-
component tracks the conversational dynamics, 
and includes models for detecting spoken signals s, 
inferring the source S(s) and the set of addressees 
A(s) for each signal, as well as the floor state 
FS(p), actions FA(p) and intentions FI(p) of each 
participant p engaged in a conversation. This in-
formation is used in conjunction with higher-level 
dialog context to decide when the system should 
generate new contributions and which floor action 
should be produced at each point in time. Finally, 
floor actions are rendered by a behavioral compo-
nent into a set of coordinated gaze, gesture and 
speech behaviors. By harnessing these different 
components, the proposed model can enable an 
99
embodied conversational agent to handle a broad 
spectrum of turn-taking phenomena.  
Figure 2.  Questions game: screen and kiosk. 
4. User Study 
We implemented an initial set of turn-taking infer-
ence and decision making models in the context of 
a multiparty dialog system, and we conducted a 
large-scale multiparty interaction user study with 
this system. The study, described in more detail 
below, was designed to fulfill two goals: (1) to as-
certain an initial performance baseline and identify 
current bottlenecks and challenges to be addressed 
moving forward, and (2)  to collect a large set of 
multiparty human-computer dialog data that can be 
used to study and improve multiparty turn taking in 
dialog systems.  
4.1. System 
The platform used in these experiments, described 
in detail in Bohus and Horvitz (2009), takes the 
form of a multimodal interactive kiosk that dis-
plays an avatar head which plays a questions game 
with multiple participants. The system leverages 
audiovisual information and employs components 
for visually tracking multiple people in the scene, 
sound source localization, speech recognition, 
conversational scene analysis, behavioral control 
and dialog management. Figure 2 shows a screen 
generated by the system, with the rendered avatar 
and a sample challenge question. Users can col-
laborate on selecting an answer, and, after a con-
firmation, the system provides an explanation if the 
answer is incorrect, before moving on to the next 
question. Sample interactions are found in Appen-
dix C and videos are available online (Situated In-
teraction, 2011). 
4.2. Turn-Taking Inference and Decisions 
In the current system, a voice activity detector is 
used to identify and segment spoken utterances. 
The source of each utterance is assumed to be the 
participant who is closest in the horizontal plane to 
the sound direction identified by the microphone 
array. The set of addressees is identified by fusing 
information probabilistically about the focus of 
attention of the source, as obtained through face 
detection and head pose tracking, while the utter-
ance is being detected. In addition, the system as-
sumes that non-understandings are addressed to 
other engaged participants, since initial tests indi-
cated that in this domain about 80% of utterances 
that led to non-understandings were in fact ad-
dressed to others. Similarly, the system assumes 
that utterances longer than three seconds are ad-
dressed to others (responses addressed to the sys-
tem tend to be short in this domain) 
Floor management actions are inferred as fol-
lows. If a participant has the floor, we assume they 
are performing a hold action if speaking and a re-
lease action otherwise. The release is assumed to 
be towards the addressees of the last spoken utter-
ance. Although the latter assumption on releases 
may not hold in the most general case, it is a rea-
sonable one for the questions game domain. If a 
participant does not have the floor, the system as-
sumes they perform a take action if speaking or a 
null action otherwise. The system also assumes 
that the floor intentions are fully reflected by the 
floor actions, i.e., a participant intends to have the 
floor if and only if she performs a hold or take ac-
tion. Floor states are updated based on the joint, 
coordinated floor actions of all participants, as de-
scribed earlier.  
Turn-taking decisions are based on a simple 
heuristic policy. The system takes the floor if (1) 
the floor is being released to it or (2) a participant 
releases the floor to someone else, but no one 
claims the floor for a preset duration. In most cas-
es, this duration is set to 3.5 seconds. However, if 
the floor is released to someone else after the sys-
tem is interrupted during a question dialog act, the 
system will try to quickly reacquire the floor 
should no one else be speaking, so as to finish or 
restate its question. The waiting duration is set in 
the latter case to 500 milliseconds. If after 500ms, 
when the system tries to take the floor another con-
flict occurs (followed by a floor release to someone 
else), the waiting duration is increased again to 3.5 
seconds. Finally, if a third consecutive conflict oc-
100
curs when the system tries to acquire the floor, the 
waiting duration is set to a longer, 20 seconds. 
The system releases the floor at the end of its 
own outputs. In addition, it has to decide whether it 
should release the floor when a user performs a 
take action (i.e. barges in) while the system is 
speaking. The heuristic policy currently imple-
mented by the system releases the floor only for 
barge-ins occurring during question dialog acts. 
Finally, the behavioral models employ policies 
informed by the existing literature on the role of 
gaze in regulating turn taking. In particular, the 
system?s gaze is directed towards the speaking par-
ticipant, or, if the system is speaking, towards the 
addressees of the system?s utterance. During si-
lences, the system?s gaze is directed towards the 
participants that the floor is being released to. 
The models and policies described above repre-
sent a starting point for inference and action, con-
structed to enable data collection and an initial 
evaluation in this domain. We are working to up-
date the turn-taking architecture with more sophis-
ticated evidential reasoning and utility-theoretic 
decision making.  Nevertheless, when harnessed as 
an ensemble within the turn-taking approach that 
we have described, the current procedures provide 
for an array of complex, multiparty turn-taking 
behaviors. For instance, the system can address 
each participant individually or all participants as a 
group via controlling the orientation of its head 
pose. When participants talk amongst themselves, 
the system can monitor their exchanges and wait 
until the floor is being released back to it. If an 
answer is heard during such a side conversation 
(e.g., one participant suggests an answer to an-
other), the system highlights it on the screen (see 
Figure 2). If a significant pause is detected during 
this side conversation, the avatar takes the floor 
and the initiative, e.g., ?So, what do you think is 
the correct answer?? Once a participant provides 
an answer, the system seeks confirmation from 
another participant before moving on. In some 
cases, the avatar passes back the floor and seeks 
confirmation non-verbally, by simply turning to-
wards another participant and raising its eyebrows. 
The system can try to require the floor immediately 
after being interrupted, but can also back off, giv-
ing the participants a chance to finish a side con-
versation, if successive floor conflicts occur. Sam-
ple interactions can be viewed in Appendix C and 
online (Situated Interaction, 2011). 
4.3. Study Design 
The user study was conducted in a usability lab 
and involved a total of 60 participants recruited as 
pairs of people from the general population who 
previously knew one another (30 male and 30 fe-
male, with ages between 18 and 61). The study 
was structured in 15 one-hour sessions, with each 
session involving four participants, i.e., two pairs 
of two previously acquainted participants. In each 
session, we formed all possible subgroups of size 
two (6 subgroups) and of size three (4 subgroups) 
with the four participants. Each subgroup played 
one game with the system. This setup allowed us to 
collect a large set of multiparty interactions under 
diverse conditions (e.g., all-male, all-female, 
mixed-gender groups; groups where people were 
previously acquainted vs. not, etc.). At the end of 
each session, participants filled in a subjective as-
sessment survey. 
4.4.  Corpus, Annotations, and Cost Assessment 
In total, 150 multiparty interactions were collected: 
90 with two participants and the system, and 60 
with three participants and the system. A profes-
sional annotator transcribed the utterances detected 
by the system at runtime, and labeled them with 
source and addressee information. 
    The system was noted to commit several types 
of turn-taking errors. To expand the error analysis 
beyond occurrence statistics and to characterize the 
impact of various types of errors, we conducted a 
follow-up study.  In this second study, a set of ad-
ditional participants were recruited to review vid-
eos of interactions from the first study and asked to 
(1) identify the turn-taking errors committed by the 
system and (2) to assess the costliness of the error 
on a five-point scale.  
A total of 9 interactions (5 with two participants 
and system; 4 with three participants and system) 
were randomly sampled from the collected corpus, 
while ensuring that each turn-taking outcome of 
interest (discussed in Section 5 and summarized in 
Table 1) was sufficiently represented. Nine partici-
pants were recruited via an email request to em-
ployees at our organization.  Each participant re-
viewed three interactions, and each interaction was 
reviewed by three different participants. Prior to 
the experiment, each of the annotators received a 
brief review of the turn-taking process in human-
human interaction. Next, they used a multimodal 
101
annotation tool that we created to review the inter-
action videos. As each video played, the annotator 
pushed a button at each point they believed that the 
system had committed a turn-taking error. In a se-
cond pass, each annotator was asked to review the 
errors that they had previously identified and to 
assess the relative cost of the error, on a scale from 
0 (?no error?) to 5 (?worst error?). In a final step, 
the authors manually aligned each identified turn-
taking error with a turn-taking decision made by 
the system and its corresponding outcome. 
5. Evaluation 
We now focus on the various types of turn-taking 
errors, the outcomes that these errors lead to, and 
the costs assessed for the outcomes.  We begin by 
focusing on diarization challenges described in 
Section 5.1.  In Sections 5.2 and 5.3, we review the 
accuracy of the system?s turn-taking inferences and 
decisions, and their corresponding outcomes.  Fi-
nally, in Section 5.4, we turn our attention to the 
subjective assessment results obtained via the post-
experiment user survey.  
Before diving into the details, we note that we 
eliminated 7 out of the total 150 interactions from 
the analysis due to significant problems with 
acoustic echo cancellation. In the remaining 143 
interactions, we also identified and eliminated 24 
utterances in the transitional engagement stages, 
e.g., when the users were not ready or properly 
setup in front of the system. The analysis below is 
based on the remaining 4379 utterances.  
5.1. Diarization 
The system uses a voice activity detector which 
leverages energy, acoustics and grammar to detect 
spoken utterances. Our experiments indicate that 
this type of black-box solution can make diariza-
tion errors, especially in multiparty settings where 
people may speak simultaneously, at a fast pace, 
and address each other with language outside the 
system?s grammar. Results show that only 72% of 
the detected segments contain speech from a single 
participant. Another 2% contain background noises 
incorrectly identified as speech. Most often these 
are instances where the system heard itself due to 
acoustic echo-cancellation problems; the ratio 
grows to about 6% among all utterances detected 
while the system is speaking. The remaining 26% 
contain overlapping or successive utterances from 
multiple speakers. Inspection of the data reveals 
that some utterances spoken softly by participants 
were not detected and that segmentation boundary 
errors are also sometimes present. While such er-
rors may be mitigated by inferences at higher lev-
els in the turn-taking model, they can significantly 
influence the system?s ability to track the conver-
sational dynamics and make appropriate turn-
taking decisions. We plan to pursue more robust 
audiovisual diarization methods that integrate 
sound localization as detected by a microphone 
array, along with higher-level interaction context.  
5.2. Take versus Null  
We now turn our attention to the system?s floor 
control decisions. The analysis below is based on 
the utterances and segmentation detected by the 
system at runtime. We note that a more precise 
analysis could be conducted with a ground truth 
segmentation of utterances. Utterances detected by 
the system can be classified into three categories, 
based on their relationship to system outputs, as 
shown in Figure 3: overlaps, which start and end 
during a system?s output, continuers, which begin 
during but finish after a system output has ended, 
and responses, which do not overlap anywhere.   
With the current policy, the system chooses 
whether it should take the floor following each 
detected continuer and response. The dataset con-
tains a total of 3265 such instances. The system?s 
decision at each of these points hinges on the re-
sults of its inferences about the participants? floor 
actions, and thus of inferences about the addressees 
of each utterance. Table 1 displays a tabulation of 
the release actions performed by the participants 
versus the actions identified by the system. The 
release actions are determined from labels assigned 
manually by the professional annotator. Recall that 
we make an assumption that the release is towards 
the set of addressees of an utterance. For segments 
that were labeled as containing multiple utterances, 
the release is made to the addressee of the last ut-
terance. The last row in Table 1 corresponds to 
background noises and system speech incorrectly 
Figure 3. Schematic of different classes of overlap.
turn-initial overlap (TIO) response continuer turn-internal overlap 
System speech 
Detected utterances 
Actual utterances 
102
Inferred Addressee / Release Action  
To System  Not to System 
 
2063 (64%) 
  
277 (9%) 
Take + Verbal Contribution 
1796 (87%) 
Delayed System Take 
59 (21%) 
To
 S
ys
te
m
 
 
Turn-initial 
overlap 
182 (10%) 
[17 Echo] 
 
 
0.25 No turn-initial 
overlap 
1614 (90%) 
 
 
0.00 
 
 
Take+ 
Non-verbal 
Release 
267 (13%) 
 
 
 
 
0.42 Turn-initial 
overlap  
22 (37%) 
[0 Echo] 
 
1.83 
 
 
No turn-initial 
overlap 
37 (63%) 
 
 
2.58 
 
 
Other 
Takes 
218 (79%) 
 
 
 
 
0.85 
 
305 (9%) 
  
588 (18%) 
Take + Verbal Contribution 
242 (79%) 
Delayed System Take 
131 (22%) 
La
be
le
d 
Ad
dr
es
se
e 
/ R
el
ea
se
 A
ct
io
n 
 
No
t t
o 
Sy
st
em
 
 
Turn-initial 
overlap 
101 (42%) 
[0 Echo] 
 
 
1.76 No turn-initial 
overlap 
141 (58%) 
 
 
0.42 
 
 
Take+ 
Non-verbal 
Release 
63 (21%) 
 
 
 
 
0.00 
Turn-initial 
overlap 
38 (29%) 
[3 Echo] 
 
0.55 
 
 
No turn-initial 
overlap 
93 (71%) 
 
 
0.00 
 
 
Other 
Takes 
457 (78%) 
 
 
 
 
0.03 
 
 
 
10 (<1%) 
  
22 (<1%) 
Take + Verbal Contribution 
9 (90%) 
Delayed System Take 
13 (59%) 
Ba
ck
gr
ou
nd
 
Turn-initial  
overlap 
3 (33%) 
[0 Echo] 
No turn-initial  
overlap 
6 (67%) 
Take+ 
Non-verbal  
Release 
1 (10%) 
Turn-initial  
overlap  
7 (54%) 
[4 Echo] 
No turn-initial  
overlap 
6 (46%) 
Other 
Takes 
9 (41%) 
Table 1. Decisions to take floor (vs. null), outcomes, and estimated costs (bar graph with confidence intervals).  
  Echo denotes cases where the turn initial overlap is created by utterances where the system hears itself because         
of errors with echo cancellation.  
     
  
     
    
    
identified as utterances.  
On the task of detecting addressees, and thus 
floor release actions, the results show an error rate 
of 18%, including 305 false-positives (erroneous 
detections) and 277 false-negatives (missed detec-
tions) of floor releases to the system. These errors 
influence the quality of turn taking in a variety of 
ways and underscore the need for more robust in-
ferences about speech source and target, and floor 
release actions. We believe that more sophisticated 
models learned from audiovisual information (e.g., 
prosody, head and body pose, etc.) and attributes 
of the interaction context (e.g., who spoke last, 
where is the system looking, etc.) can reduce errors 
significantly. 
Table 1 indicates that in 305 (9%) of the cases 
the system incorrectly inferred that the floor was 
being released to it. In 79% of these cases, the sys-
tem took the floor and produced a verbal contribu-
tion. Since the floor was not released to the system, 
such errors can lead to significant turn-taking prob-
lems, which often manifest as floor conflicts 
marked by turn-initial overlaps, where a partici-
pant and the system start speaking around the same 
time (see Figure 3). Operationally, we define turn-
initial overlaps as all detected overlaps with an 
actual onset of less than 300 milliseconds from the 
beginning of the system?s utterance (see discussion 
in Appendix A); the other overlaps are dubbed 
turn-internal. We note that the time at which an 
overlap is detected by the system lags behind the 
actual onset of the utterance by an average of about 
700 milliseconds, due to core latencies in our audio 
and speech processing pipeline. Accounting for 
these computational lags, and others arising at dif-
ferent places in processing pipelines, raise chal-
lenges for turn taking in spoken dialog systems. 
42% of the verbal takes performed incorrectly 
by the system led to turn-initial overlaps. This is 
not surprising, as the system starts speaking when 
the floor was not released to it. In some of these 
cases the same participant continues (e.g., diariza-
tion errors incorrectly segmented the utterance), or 
someone else starts speaking. The cost assessment 
experiment confirmed the impact of these errors ? 
the average estimated cost was 1.76. If no turn-
initial overlap occurred after the system incorrectly 
took the floor, the average cost was 0.42. Clearly 
103
floor conflicts come with a cost. The specific cost 
assessments we obtained are perhaps influenced to 
a degree by the role of game mediator played by 
the system. With this role, taking the floor in cases 
when the system was not addressed is perhaps not 
as costly as it might be in other domains.  
Note that 182 turn-initial overlaps also occur 
when the system takes the floor after correctly 
identifying that the floor was released to it (upper-
left quadrant in Table 1). 17 of them are created by 
the system hearing itself as it starts speaking, due 
to errors in acoustic echo cancellation; these in-
stances are marked Echo in Table 1. While the rel-
ative percentage of turn-initial overlaps is smaller 
after a floor release to the system (~10%), the ma-
jority of all turn-initial overlaps (shaded cells in 
Table 1) occur in this context, because of the larger 
incidence of the situation. Often, these utterances 
contain an immediate answer or a short confirma-
tion from another participant. The cost of these 
turn-initial overlaps is also much lower: 0.25 ver-
sus 1.76 (again, the cost structure is probably sen-
sitive to details of the domain). 
We believe the turn-initial overlaps that occur 
when the floor is released to the system can be ex-
plained in part by the interpretation of the system?s 
short delay in responding (per processing) as a sig-
nal that the system is not taking the floor, leading 
other participants to take initiative. As another fac-
tor, turn taking is a mixed-initiative process, and 
other participants might vie for the floor and issue 
their own contributions immediately after an an-
swer directed to the system. These observations 
bring to the fore two questions: (1) how can we 
minimize the number of turn-initial overlaps, and 
(2) how can the system gracefully handle such 
overlaps once they occur?  
One approach to minimizing turn-initial over-
laps is to reduce the system?s response delays via 
faster processing or via the use of predictive mod-
els to anticipate the end of turns (e.g. Ferrer et al, 
2003; Schlangen, 2006; Raux and Eskenazi, 2008; 
Skantze and Schlangen, 2009). Multiparty settings 
require methods for forecasting not only when a 
current speaker will finish, but also whether any 
participant will try to take (or release) the floor 
within a small window of time in the future, i.e., 
accurately modeling all floor intentions. Our turn-
taking framework includes components for repre-
senting and modeling floor intentions, but these are 
not used in the current system.  We believe there is 
promise in learning models to predict floor inten-
tions and the timing of ends of utterances from in-
teraction data. The availability of such predictions 
can fuel additional turn-taking strategies and also 
pave the way to more graceful handling of turn-
initial overlaps after they occur. For instance, if the 
system can anticipate that someone else might start 
speaking, it might still decide to take the floor but 
it might start with a filler, e.g., ?So [pause] What 
do you think?? constructing a natural opportunity 
for resolving a potential conflict after ?So? We 
plan to investigate the use of decision-theoretic 
methods to anticipate and resolve such conflicts by 
introducing and modulating an array of strategies, 
including the use of fillers, restarts, and acknowl-
edgment gestures.  
In 21% of the 305 incorrectly detected floor re-
leases to the system, our system immediately per-
formed a non-verbal floor release to another par-
ticipant by turning the avatar?s face towards them 
and raising its eyebrows (Take + Non-verbal Release in 
Table 1). These situations are not costly, as the 
system?s action does not interrupt the flow of the 
conversation. Indeed they were never penalized in 
the cost assessment experiment that we conducted. 
However, the same action, performed when the 
floor is actually released to the system (13% of 
2063 cases), has the potential to create problems if 
not properly recognized by the targeted participant 
as a floor release by the system; the average cost 
assessed in this case was 0.42. 
The right-hand column in Table 1 shows cases 
where the system detected that the floor was not 
released to it. In these cases, the system waits (per-
forms null) for a specified duration. The cost as-
sessment indicates that waiting in this situation is 
overall costly, and the cost depends on the ultimate 
outcome. If no one else takes the floor, the system 
will eventually do so (Delayed System Take cases in 
Table 1). In some of these cases, turn-initial over-
laps also occur. The 277 cases in which the system 
fails to detect that the floor was in fact released to 
it lead to no immediate response from the system. 
In these cases the system can be perceived as unre-
sponsive and the participants eventually repeat 
themselves. We believe that performance can be 
improved with the use of an ongoing decision-
theoretic analysis that continuously reassesses the 
situation while the system waits.  Such an analysis 
would consider the delay, floor holder?s previous 
actions, inferences about participants? floor inten-
104
tions, and cost-benefit tradeoffs of different floor 
actions. 
5.3. Release versus Hold 
We now turn our attention to the system?s deci-
sions to release the floor. Recall that, according to 
the current policy, the system performs a floor hold 
while it is speaking and a floor release at the end of 
its outputs. In addition, if an overlap (i.e., barge-in) 
was detected during question dialog acts, the sys-
tem performed a floor release immediately, inter-
rupting its own output and allowing for the user 
barge-in.  
Since such barge-ins were allowed only during 
the question dialog acts, as Table 2 shows, the cur-
rent policy leads to an abundance of cases in which 
the system performs hold when an overlap is de-
tected. Some of these cases are continuers: the 
overlap only happens at the very end of the sys-
tem?s output. These cases do not create significant 
turn-taking problems, as the floor still transitions to 
the participant relatively quickly (the system re-
leases at the end of its output). However, in a sig-
nificant number of cases the system appears to ig-
nore the participants (shaded cells in Table 2). 
About three quarters of these overlaps occur while 
the system is providing an explanation after an in-
correct answer. Observations of the data indicate 
that in these cases participants may discuss or give 
their opinion on the answer or some aspect of the 
system?s explanation, while ignoring the system as 
it blindly continues the explanation.  
We have separated in Table 2 turn-initial from 
turn-internal overlaps. The two types of overlaps 
reflect different phenomena. As we have discussed, 
turn-initial overlaps mark floor conflicts, and vari-
ous strategies could be used to negotiate such con-
flicts (e.g., Yang and Heeman, 2010). In contrast, 
turn-internal overlaps may reflect efforts by other 
participants to take the floor, or might simply be 
backchannels, laughter, exclamations or other lexi-
cal or non-lexical events that do not mark a claim 
for the floor. Making appropriate floor control de-
cisions in this case will require models for reliably 
distinguishing between the two, i.e., between the 
take or null floor actions of the participants. This is 
an especially challenging inference problem as 
decisions need to be made as early as possible after 
the onset of an utterance.  
We note the relatively large incidence of failures 
in echo cancellation in our microphone array. On 
the utterances marked Echo in Table 2, the system 
heard itself and thought a user was speaking. We 
believe these failures could be significantly re-
duced with better acoustic echo cancellation.  
5.4. Subjective Assessment  
Finally, we present results from a subjective as-
sessment of the system by participants, based on a 
post-experiment survey. The survey included sev-
eral 7-point Likert scale questions related to turn 
taking, which are displayed in Figure 4, together 
with the mean user responses and the correspond-
ing 95% confidence intervals. Generally, partici-
pants rated the system?s turn-taking abilities fa-
vorably, with scores around 4.5-5. No statistically 
significant differences were detected in assess-
ments across the participant?s gender or previous 
familiarity with speech recognition systems. We 
also note that a parallel human?human interaction 
study would help us characterize better the sys-
tem?s performance relative to human dialog.  
I knew when the avatar
was addressing me
I knew when the avatar
was addressing others
I knew whom the avatar
was talking to
I knew when it was 
my time to speak
The avatar knew when 
I was speaking to it
The avatar knew when 
I was speaking to others
The avatar knew when 
it was its time to speak
The avatar interrupted 
us at the wrong time
The avatar waited too 
long before taking its turn 
I felt left out or excluded 
during the games
The interaction 
was natural 
I enjoyed playing 
the game
Figure 4. Results of subjective assessments. 
5.1 
5.0 
4.8 
4.6 
4.9 
4.8 
4.0 
3.2 
3.0 
2.0 
4.0 
5.5 
[ lower is better ]
[ lower is better ] 
 [ lower is better ]
1 - Never 7 - Always 2 3 4 5 6 
Action performed by system when overlap detected  
HOLD RELEASE 
315 (23%) 
Tu
rn
 
In
iti
al
 
Overlap 
285 (90%) 
[14 Echo] 
Continuer 
30 (10%) 
[3 Echo] 
43 (3%) 
[7 Echo] 
968 (69%) 
O
ve
rla
p 
Ty
pe
 
Tu
rn
  
In
te
rn
al
 
Overlap 
828 (86%) 
[44 Echo] 
Continuer 
140 (14%) 
[7 Echo] 
73 (5%) 
[13 Echo] 
Table 2. Decisions to release floor (vs. hold).
105
In addition to the survey questions, participants 
were invited to describe in their own words what 
they liked best and the first thing they would 
change about the system. 21 of the 60 participants 
mentioned aspects of multiparty interaction in the 
?what I liked best? category, such as the system?s 
ability to track the speaking participant and address 
people individually. Other frequent answers to this 
question called out the overall experience with the 
integrative intelligence of the system (15 answers), 
the fun/educational nature of the game (14), and 
aspects of speech recognition (11). On the ?first 
thing you would change,? the majority of answers 
(32) included references to shortcomings in render-
ing the avatar, while 13 answers included refer-
ences to problematic aspects of the multiparty turn 
taking. Other answers included task domain sug-
gestions (6) and comments about improving the 
speech recognition (5). A sampling of answers is 
presented in Appendix B.   
6. Summary and Future Work 
We reported on a user study of a multiparty turn-
taking model. Objective measures of system per-
formance and subjective assessments by partici-
pants indicate that the approach can enable suc-
cessful multiparty turn taking in the questions 
game domain. When the correct turn-taking deci-
sions are made, the multiparty interaction is seam-
less and resembles human-human collaboration. 
The conversations exhibit fluid exchanges among 
people and the system, including mixed-initiative, 
multiparty floor control, fluid back offs and re-
starts, natural use of non-verbal cues, such as par-
ticipants? utterances being triggered by a turn of 
the avatar?s head or a lift of the eyebrows. In con-
trast, turn-taking failures lead to a striking loss of 
fluidity and a qualitative jump out of an engaged 
process, where the system rapidly shifts from a 
collaborating participant into a distant and uncoor-
dinated appliance.  
The results we have discussed are based on an 
initial set of coarse perceptual and decision-making 
models and thus reflect an initial baseline; there is 
significant room for improvements. A careful dis-
section of the outcomes demonstrates the subtleties 
of multiparty turn taking and highlights several 
directions we plan to address in future work. First, 
our experiments have highlighted the importance 
of accurate diarization in multiparty dialog set-
tings. Minimizing errors requires rich perceptual 
and inferential competencies, leveraging audiovis-
ual evidence, general patterns of human discourse, 
and attributes of the task-specific goals and con-
text. We plan to explore the use of machine learn-
ing procedures for constructing predictive models 
that harness richer streams of evidence to identify 
and segment utterances, and to make inferences 
about their sources and targets, and the floor state, 
actions and intentions of all participants. Better 
turn-taking decisions can also be supported by in-
ferences about social norms, roles and dynamics, 
pace of interaction, and engagement.  
Although handcrafted turn-taking policies went 
a long way in this domain, enabling more general 
multiparty turn taking will require continuous in-
ference and decision making under uncertainty that 
considers subtleties of intention and timing, and 
that takes into consideration tradeoffs associated 
with different courses of actions. We foresee the 
value of extending the current decision models 
with richer temporal reasoning for performing such 
ongoing analyses. Challenges include a more in-
depth understanding of the cost of different types 
of turn-taking errors; the development of a wider 
array of graded strategies and behaviors for taking, 
releasing, or holding the floor, and for gracefully 
negotiating floor conflicts; and finally, the ability 
to reason about uncertainty in the world as well as 
in the system?s own processing delays in order to 
resolve tradeoffs between taking timely action and 
delaying for additional evidence that promises to 
enhance the accuracies of decisions. 
Much also remains to be done with the corre-
sponding generation of subtle verbal and non-
verbal cues for enhanced signaling and naturalness 
of conversation, including the use of fillers, re-
starts, backchannels, and envelope feedback. We 
are excited about tackling these and other chal-
lenges on the path to fielding systems that can en-
gage in fluid multiparty dialog.  
Acknowledgments 
We thank Anne Loomis Thompson, Ece Kamar, 
Qin Cai, Cha Zhang, and Zicheng Liu for their 
contributions. We also thank our colleagues who 
participated in pilot experiments for the user study. 
106
References 
Bennewitz, M., Faber, F., Joho, D., Schreiber, M., and 
Behnke, S., 2005. Integrating vision and speech for 
Conversations with Multiple Persons, in Proc. of 
IROS?05 
Bohus, D., and Horvitz, E., 2009. Dialog in the Open-
World: Platform and Applications, in Proc ICMI?09. 
Bohus, D., and Horvitz, E., 2010a. Computational 
Models for Multiparty Turn Taking, Microsoft 
Research Technical Report MSR-TR 2010-115. 
Bohus, D., and Horvitz, E., 2010b. Facilitating 
Multiparty Dialog with Gaze, Gesture and Speech, in 
Proc ICMI?10. 
Duncan, S. 1972. Some Signals and Rules for Taking 
Speaking Turns in Conversation, Journal of 
Personality and Social Psychology 23, 283-292. 
Ferrer, L., Shriberg, E., and Stolcke, A. 2003. A 
Prosody-Based Approach to End-Of-Utterance 
Detection That Does Not Require Speech 
Recognition, in Proc. ICASSP?03. 
Goodwin, C. 1980. Restarts, pauses and the 
achievement of mutual gaze at turn-beginning, 
Sociological Inquiry, 50(3-4). 
Hjalmarsson, A., 2011. The additive effect of turn-
taking cues in human and synthetic voice, in Speech 
Communication, vol. 53, issue 1. 
Kronlid, F., 2006. Turn Taking for Artificial Conversa-
tional Agents, in Cooperative Information Agents X, 
LNAI 4149, Springer-Verlag 
Matsusaka, Y., Fujie, S., and Kobayashi, T., 2001. 
Modeling of conversational strategy for the robot 
participating in the group conversation, in Proc of 
EuroSpeech?01. 
Raux, A., and Eskenazi, M. 2008. Optimizing endp-
ointing thresholds using dialogue features in a spoken 
dialogue system, in Proc of SIGdial-2008. 
Raux, A. and Eskenazi, M., 2009. A Finite-State Turn-
Taking Model for Spoken Dialog Systems, in Proc. 
HLT?09. 
Sacks, H., Schegloff. E., and Jefferson, G. 1974. A 
simplest systematics for the organization of turn-
taking in conversation, Language, 50, 696-735.  
Schegloff, E. 2000. Overlapping talk and the 
organization of turn-taking in conversation, 
Language in Society, 29, 1-63. 
Schlangen, D., 2006. From reaction to prediction: 
Experiments with computational models of turn-
taking, in Proc. Interspeech?06, Panel on Prosody of 
Dialogue Acts and Turn-Taking  
Selfridge, E., and Heeman, P., 2010. Importance-Driven 
Turn-Bidding for Spoken Dialogue Systems, in Proc. 
of ACL-2010, Uppsala, Sweden 
Skantze, G., and Schlangen, D., 2009. Incremental 
dialogue processing in a micro-domain, in Proc. of 
EACL-2009.  
Situated Interaction, 2011. Project web page:  
http://research.microsoft.com/~dbohus/si.html 
Thorisson, K.R. 2002. Natural Turn-Taking Needs No 
Manual: Computational Theory and Model, from 
Perceptions to Action, Multimodality in Language 
and Speech Systems, Kluwer Academic Publishers. 
Traum, D., 1994. A Computational Theory of Ground-
ing in Natural Language Conversation, TR-545, U. 
of Rochester. 
Traum, D., and Rickel, J., 2002. Embodied Agents for 
Multi-party Dialogue in Immersive Virtual World, in 
Proc. AAMAS?02.  
Wiemann, J., and Knapp, M., 1975. Turn-taking in 
conversation, Journal of Communication, 25, 75-92. 
Yang, F., and Heeman, P., 2010. Initiative Conflicts in 
Task-Oriented Dialogue, in Computer, Speech and 
Language, vol. 24, issue 2.  
107
Appendix A. Details on derivation of operational definition of turn-initial overlaps.  
 
As described in Section 5.2, we operationally define turn-initial overlaps as 
detected user utterances that have an actual onset of less than 0.3 seconds from 
the beginning of a system utterance. Figure 5 shows the histogram of the onset 
time for user speech with respect to system utterances (start of system utter-
ance is at 0 seconds), for overlapping utterances, where this onset is between -
2 and +5 seconds. If multiple user utterances overlap with a single system 
utterance, only the first user utterance, i.e. the first overlap, is considered in 
computing this histogram. As Figure 5 shows, the onset distribution has a bi-
modal character. We believe that the two modes may reflect two different 
phenomena in terms of the floor transition. The early-onset mode corresponds 
to situations in which a user starts to speak right around (before or immedi-
ately after) the time the system also started speaking; this indicates a situation 
where there is contention for the floor and the system cannot assume it has 
successfully acquired the floor. In contrast, user utterances starting at later 
times represent cases where the floor did first transition to the system and the 
user is aware of this transition.  In producing an utterance the user is attempt-
ing to barge-in and take the floor back from the system (unless the user utter-
ance is a backchannel). The threshold of 0.3 seconds on the onset for turn-
initial overlaps was selected based on the shape of this distribution.  
-2 -1 0 1 2 3 4 5
0%
2%
4%
6%
8%
10%
12%
onset (seconds) 
Figure 5. Histogram of onsets for first  
overlaps. 
 
Appendix B. Sample responses from survey 
 
Category # Example comment 
Please describe what you liked best about interacting with the system 
Multiparty 
interaction 
prowess 
21 
- I enjoyed how it recognized who was speaking and actually looked at you 
- I liked how the avatar tracked the players; how it understood speech 
- It was great to play a game where you don?t have to use your hands, just your mind. The way the avatar would recognize 
position of who spoke was nice. The blinking action at the avatar made her more realistic but she needed more than her face.  
- That it would look right at you and ask a question 
- I liked how the avatar made eye contact with each person playing the game?
Overall  
experience 
with system 
15 
- It was very new and thus it was fun. I don?t play computer games often and I did enjoy this one. Which is rare for me.  
- It was different than any other trivia game I?ve played in the past 
- I think this is a great way for a human to interact with a computer?
- It?s cool interacting with the avatar?
Rewarding 
task 14 
- I liked the challenge of the questions 
- It?s a great fun way to improve knowledge 
- New experience that I found enjoyable. I enjoyed thinking about choices and having an interaction with the avatar 
Speech and 
language 11 
- Voice recognition was fairly accurate, no need to repeat 
- The ability of it to understand what I was saying. Plus it?s pretty cool.  
- I liked it because it wasn?t really hard for the system to understand what we were saying. Even though we have an accent. 
If there was one thing you could change about this system, what would it be? 
Avatar  
rendering 32 
- The avatar should be more friendly ? she came off a bit austere ? she didn?t smile even when we got 5 out of 6 questions 
right, it was only ?pretty good?.  
- The way it moves its lips needs to be better 
- The avatar seemed a little to ?stiff?. It needs to be more natural in movement and speech 
- The face was a ?warmer face?. Smiling perhaps. 
Multiparty 
failures 13 
- Extend the time limit when questions haven?t been fully answered. It would sometimes say we were correct or false before we 
had confirmed our answer 
- Sometimes it skips and pauses and making it difficult to understand 
- Consistency in waiting and asking player to confirm answer instead of overhearing conversations and choosing an answer 
itself 
Task domain  6 - It would be cool if it could remember our names. Also, 6 questions was a little short. I think 8 or 10 questions would be better.  - I think the questions should be more pop culture related 
Speech and 
language 5 
- I enjoyed her. I would like her to understand a little easier. We had to repeat answers on occasion which wasn?t too bad. 
Overall I really liked it. Perhaps it could ask our names and call us by name when speaking to us 
 
 
 
 
 
 
 
 
 
 
108
Appendix C. Excerpts from interactions with the system. We present and discuss two segments from an interaction 
with the questions game system. The segments illustrate challenges for diarization, tracking conversational dynamics (e.g. infer-
ring speech source, target, floor actions, etc.) and decisions making for multiparty turn taking. The video for this entire interac-
tion, as well as an additional interaction are available online at (Situated Interaction, 2011) 
 
1 S?P1 Hi. Would you like to play a questions 
game? 
 
1 S?P16 In the USA hospitals have a red 
cross, what sign do they use in the 
Arab World? 
This segment starts with the system asking a question 
2 P16?P16 Hospitals have a red cross what sign 
do they user in the Arab World? 
3 P17?S Solid blue circle 
4 S?P16 Is that correct? 
5 P17?S No 
6 S?P16,P17 So what?s the correct answer? 
7 P17?P16 Red crescent moon 
8 P17?S Red crescent moon 
9 S?P16 [turns to P16, lifts eyebrows] 
10 P16?S Yes 
11 S?P16,P17 That?s right. Okay, so far you?re two 
out of three [?] 
P16 echoes back the question to himself in a low voice. The system detects this utterance, cor-
rectly inferred that the floor is not being released to it (the utterance leads to a non-
understanding), and the system turns towards the other participant, which produces ?Solid blue 
circle? (3). The utterance and corresponding floor release to the system are correctly understood, 
and the system turns back to P16 for confirmation. Next, the system detects an utterance from P17, 
who has changed her mind (5). The system takes the floor and asks both participants (by directing 
gaze towards them) what the correct answer is (6), and releases the floor to both participants o. 
P17 speaks first towards P16 (7) ? this utterance is low energy and not detected by the system, and 
then towards the system (8). The system recognizes the floor is released to it after (8), takes the 
floor and directs it back towards P16 non-verbally, by turning towards him and raising eyebrows. 
P16 responds immediately, and the system takes the floor and continues to the next question. 
   
17 S?P16,P17 Next question [?] The system then moves on to the next question 
2 P17?S Hi
3 P16?S Hello
4 P17?S Yes 
5 P18?S Yes 
6 S?P16 Here is the first question. The study of 
the size and shape of the skull as an 
indicator of a person?s character is 
known as: telepathy, graphology, or 
phrenology? 
7 P16?S I?ll go? Graphology 
8 S?P17 What do you think? 
9 P17?P17 graph- 
10 P16?P17 That doesn?t sound right but 
11 P17?S Phrenology
12 S?P16,P17 I?m sorry but that?s incorrect. Popular 
during the 19th century but not dis-
credited, phrenology is a theory that a 
person?s character and intelligence 
can be determined by the size and 
shape of their skull.  
13 P16?P17 phrenology
14 P16?P17 She did not wait for your answer
15 P17?P16 I guess not
16 P16?S You didn?t wait for my answer
Immediately after the system?s greeting, the two participants also say ?Hi? and ?Hello?. Their 
greetings are detected as a single utterance by the system which partially overlaps with the be-
ginning of the system?s follow-up question (overlaps are underlined in the examples to the left). 
According to the current policy, the system does not release the floor on this interruption and 
continues with its question. The ?Yes? responses from (4) and (5) are overlapping with each other 
and are detected by the system as a single utterance which is correctly decoded. 
The system correctly infers the floor was addressed to it, and therefore takes the floor and pro-
duces the first question 
Given the pause between ?I?ll go? and ?Graphology? the response in (7) is in fact detected as two 
separate utterances by the system. The first part is non-understood, hence assumed addressed to 
other and the system does not take the floor. The system correctly understands and takes the 
floor after ?Graphology?, and moves to ask for confirmation (8). Next, while the system asks the 
other participant for confirmation, due to imperfections in echo cancellation, the system hears a 
noise at the beginning of its utterance, but ignores the detected ?barge-in?. 
P17 softly says to herself ?graph-?. This utterance is not actually detected by the system. 
Next, the system misunderstands the utterance in (10) as ?that sounds right? and incorrectly infers 
that the utterance was addressed to it. It therefore takes the floor and continues. This leads to a 
turn-initial overlap with the ?Phrenology? utterance immediately produced by P17 (11) 
 
 
The follow-up utterances and discussion between participants (13-16) overlap with portions of the 
system?s explanation. They indicate the high cost of the misunderstanding and of the system?s 
incorrect inference and decision to take the floor (admonished by the user in (16) n, as well as 
the shortcomings of the current policy to not release the floor for barge-ins detected during expla-
nations. This example highlights the need for more robust inferences, but also better policies for 
releasing back the floor and for machinery that would allow the system to gracefully backing from 
detected floor conflicts. 
 
 P arrow shows  
direction of 
attention 
 P P has floor 
 P P is speaking 
 P P is an 
addressee 
n In the first segment, while the system is 
speaking to both participants (12), P17 leans 
in as she produces utterance (16) 
o In the second segment, the system re-
leases the floor to both participants after 
producing (6) 
Illustrations of conversational scene 
analysis performed by the system in real-
time, at runtime. 
 
109
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 13?14,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Towards Situated Collaboration 
 
Dan Bohus, Ece Kamar, Eric Horvitz 
Microsoft Research 
One Microsoft Way 
Redmond, WA, 98052, USA 
{dbohus,eckamar,horvitz@microsoft.com} 
 
 
 
 
 
 
Abstract 
We outline a set of key challenges for dialog 
management in physically situated interactive 
systems, and propose a core shift in perspec-
tive that places spoken dialog in the context of 
the larger collaborative challenge of managing 
parallel, coordinated actions in the open 
world.  
Multiple models for dialog management have been 
proposed, studied, and evaluated in the research 
community (i.a. Allen et al 2001; Bohus and Rud-
nicky, 2009; Rich and Sidner, 1998; Traum and 
Larsson, 2003; Williams and Young, 2007). In the 
process, a diverse set of problems have come to 
light and have been pursued. These include the 
challenges of modeling initiative in interaction, 
contextual interpretation and processing, the man-
agement of uncertainty, grounding, error handling 
and recovery, turn-taking and, more recently, in-
cremental processing in dialog systems. Analyses 
of existing approaches (Allen et. al, 2001; Church-
er et. al, 1997; McTear 2002; Paek and Pieraccini, 
2008) reveal a constellation of benefits but also 
shortcomings along multiple dimensions, where no 
single technique provides the benefits of all. 
While taking incremental, focused steps is im-
portant for making progress within a mature disci-
pline, we believe that the current scope and 
conceptual borders of work in spoken dialog con-
strains thinking about possibilities and gets in the 
way of achieving breakthrough advances. Research 
to date on dialog management has focused almost 
exclusively on dyadic settings, where a single user 
interacts with a system over a relatively narrow, 
speech-only channel. Characteristics of this domi-
nant and shared worldview on dialog research have 
driven modeling and architectural choices, and of-
ten done so in an implicit, hidden manner. For in-
stance, dialog is often viewed as a collection of 
dialog moves that are timed in a relatively well-
structured, sequential fashion. As a consequence, 
dialog management models typically operate on a 
?per-turn? basis: inputs are assumed to arrive se-
quentially and are processed one at a time; for each 
received input, discourse understanding is per-
formed, and a corresponding response is generated.  
In reality, interactions among actors situated in 
the open, physical world depart deeply from com-
mon assumptions made in spoken dialog research 
and bring into focus an array of important, new 
challenges (Horvitz, 2007; Bohus and Horvitz, 
2010; Bohus, Horvitz, Kanda et al, eds., 2010).  
We describe some of the challenges with respect to 
dialog management, and re-frame this problem as 
an instance of the larger collaborative challenge of 
managing parallel, coordinated actions amidst a 
dynamically changing physical world.  
As an example, consider a robot that has been 
given the responsibility of greeting, interacting, 
and escorting visitors in a building. In this setting, 
reasoning about the actors, objects and events and 
relationships in the scene can play a critical role in 
understanding and organizing the interactions. The 
surrounding environment provides rich, continu-
ously streaming situational context that is relevant 
for determining the best way an agent might con-
tribute to interactions. Because the situational con-
text can evolve asynchronously with respect to 
turns in the conversation, systems that operate in 
the open world must be able to plan continuously, 
13
in stream, rather than on a ?per-turn? basis. Inter-
action and collaboration in these settings is best 
viewed as a flow of coordinated, parallel actions. 
The sequential structure of turns in dyadic interac-
tions is but one example of such coordination, fo-
cused solely on linguistic actions. However, to 
successfully interact and collaborate with multiple 
participants in physically situated settings, an agent 
must be able to recognize, plan, and produce both 
linguistic and non-linguistic actions, and reason 
about potentially complex patterns of coordination 
between actions, in-stream?as they are being pro-
duced by the participants in the collaboration. 
We argue that attaining the dream of fluid, 
seamless spoken language interaction with ma-
chines requires a fundamental shift in how we view 
dialog management. First, we need to move from 
per-turn to continual in-stream planning. Second, 
we need to move from reasoning about sequential 
actions to reasoning about parallel and coordinat-
ed actions and their influence on states in the 
world. And third, we need models that can track 
and leverage the streaming situational context, 
from noisy observations, to make decisions about 
how to best contribute to collaborations.  
Spoken dialog is an important channel for ex-
pressing coordinative information. However, we 
need to recognize and begin to tackle head on the 
larger challenge of situated collaborative activity 
management.  We understand that taking this per-
spective introduces new complexities?and that 
some of our colleagues will view diving into the 
larger problems in advance of solving simpler ones 
as being unwise. However, we believe that we 
must embrace the larger goals to make significant 
progress on the struggles with the simpler ones, 
and that the investment in solving challenges with 
physically situated collaboration will have eventual 
payoffs in enabling progress in spoken dialog.   
Making progress on the broader challenge re-
quires technical innovations, tools, and data. Con-
sider for instance one sub-problem of belief 
tracking in these systems: continuously updating 
beliefs over the state of the collaborative activity 
and the situational context requires the develop-
ment of new types of models that can combine 
streaming evidence about context collected 
through sensors, with discrete evidence about the 
actions performed or the turns spoken collected 
through speech, gesture or other action-recognition 
components. In addition, progress hinges on identi-
fying a set of relevant problem domains, and coor-
dinating efforts in the community to collect data, 
and comparatively evaluate proposed approaches. 
New tools geared towards analysis, visualization 
and debugging with streaming multimodal data are 
also required.   
We propose a core shift of perspective and as-
sociated research agenda for moving from dialog 
management to situated collaborative activity 
management. We invite discussion on these ideas.  
References  
Allen, J.F., Byron, D.K., Dzikovska, M., Ferguson, G., 
Galescu, L., and Stent, A. 2001. Towards Conversa-
tional Human-Computer Interaction, AI Magazine, 
22(3) 
Bohus, D., and Rudnicky, A. 2009. The Ravenclaw dia-
log management framework: Architecture and sys-
tems, in Computer, Speech and Language, 23(3). 
Bohus, D., and Horvitz, E. 2010. On the Challenges and 
Opportunities of Physically Situated Dialog, AAAI 
Symposium on Dialog with Robots, Arlington, VA. 
Bohus, D., Horvitz, E., Kanda, T., Mutlu, B., Raux, A., 
editors, 2010. Special Issue on ?Dialog with Robots?, 
AI Magazine 32(4). 
Churcher, G. E., Atwell, E.S, and Souter, C. 1997 Dia-
logue Management Systems: a Survey and Overview, 
Technical Report, University of Leeds, Leeds, UK. 
Horvitz, E., 2007. Reflections on Challenges and Prom-
ises of Mixed-Initiative Interaction, AI Magazine 28, 
pp. 19-22. 
McTear, M.F. 2002. Spoken dialogue technology: ena-
bling the conversational user interface, ACM Compu-
ting Surveys, 34(1):90-169. 
Paek, T., and Pierracini, R. 2008. Automating Spoken 
Dialogue Management design using machine learn-
ing: An industry perspective, Speech Communica-
tion, 50(8-9):716-729. 
Rich, C., and Sidner, C.L. 1998. Collagen: A Collabora-
tion Manager for a Collaborative Interface Agent, 
User Modelling and User Assisted Interaction, 7(3-
4):315-350, Kluwer Academic Publishers. 
Traum, D., and Larsson, S. 2003. The Information State 
Approach to Dialogue Management. Current and 
New Directions in Discourse and Dialogue, Text 
Speech and Language Technology, 22:325-353. 
Williams, J., and Young, S., 2007. Partially Observable 
Markov Decisions Processes for Spoken Dialog Sys-
tems, Computer, Speech and Language, 21(2). 
Young, S. 2006. Using POMDPs for Dialog Manage-
ment, in Proc. of SLT-2006, Palm Beach, Aruba. 
14
