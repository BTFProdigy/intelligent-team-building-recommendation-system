Proceedings of the ACL Student Research Workshop, pages 97?102,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Minimalist Parsing of Subjects Displaced from Embedded Clauses in Free
Word Order Languages
Asad B. Sayeed
Department of Computer Science
University of Maryland at College Park
A. V. Williams Building
MD 20742 USA
asayeed@mbl.ca
Abstract
In Sayeed and Szpakowicz (2004), we
proposed a parser inspired by some as-
pects of the Minimalist Program. This
incremental parser was designed specifi-
cally to handle discontinuous constituency
phenomena for NPs in Latin. We take a
look at the application of this parser to a
specific kind of apparent island violation
in Latin involving the extraction of con-
stituents, including subjects, from tensed
embedded clauses. We make use of ideas
about the left periphery from Rizzi (1997)
to modify our parser in order to handle ap-
parently violated subject islands and simi-
lar phenomena.
1 Introduction
In Sayeed and Szpakowicz (2004), we started by de-
scribing the difficulty of parsing sentences in lan-
guages with discontinuous constituency in a syntac-
tically robust and cognitively realistic manner. We
made the assumption that semantic links between
the words of a sentence are made as soon as they
arrive; we noted that this constrains the kinds of for-
malisms and algorithms that could be used to parse
human sentences. In the spirit of the Minimalist Pro-
gramme, we would like to produce the most eco-
nomical parsing process, where, potentially contro-
versially, we characterize economy as computational
complexity. Discontinuity of phrases (usually noun
phrases) in e.g. Latin provides a specific set of chal-
lenges in the development of a robust syntactic anal-
ysis; for instance, in the process of building parse
trees, nouns must often be committed to positions in
particular structures prior to the arrival of adjectives
in an incremental parsing environment.
Inspired by work such as Stabler (2001), we pro-
posed a formalism and algorithm1 that used fea-
ture set unification rather than feature cancellation,
which Stabler uses to implement basic Minimalist
operations such as MOVE and MERGE. We demon-
strated the workings of the algorithm given sim-
ple declarative sentences?in other words, within
a single, simple clause. What we wish to do now
is demonstrate that our algorithm parses Latin sen-
tences with embedded clauses, and in particular
those with constituents displaced beyond the bound-
aries of embedded clauses where this displacement
does not appear to be legitimate wh-movements;
these are, in a sense, another form of discontinuity.
In doing this, we hope to show that our formalism
works for a wider subset of the Latin language, and
that we have reduced the problem of developing a
grammar to one of choosing the correct features.
2 Background
Noun phrases in Latin can become discontinuous
within clauses. For instance, it is possible to place
a noun before a verb and an adjective that agrees
with the noun after the verb. However, for the most
part, the noun phrase components stay within CP.
Nevertheless, Kessler (1995) noted several instances
where, possibly for intonational effect, Latin prose
writers extracted items into matrix clauses from em-
bedded clauses and clauses embedded within those
embedded clauses. For example,
(1) Tametsi
Although
tu
you-NOM-SG
scio
know-IND-PRES-1SG
quam
how
1For the purpose of clarification, our algorithm can be found
at http://www.umiacs.umd.edu/?asayeed/discont.pdf
97
sis
are-SUBJ-PRES-2SG
curiosus
interested-NOM-SG
?Although I know how interested you are?
(Caelius at Cicero, Fam 8.1.1)
In this and other cases provided by Kessler, a word
is extracted from an embedded clause and moved
to the beginning of the matrix clause. (The itali-
cized words consist of the extracted element and the
clause from which it was extracted.) Note in particu-
lar that 1 involves the dislocation of the subject from
a tensed embedded clause, something that would or-
dinarily be a well-known island violation (Haege-
man, 1994).
According to Kessler, this situation is rare enough
that many contemporary accounts of Latin syntax
neglect discussion of this kind of device. It is likely
that Cicero occasionally wrote this way for prosodic
reasons; however, there is no reason why prosody
should not have syntactic consequences, and we at-
tempt to account for the parsing of such sentences in
this document.
It is interesting to note how in these examples, the
displaced element moves somewhere near to the be-
ginning of the outer clause. Rizzi (1997) suggests a
structure for this ?left periphery? based on observa-
tions from Italian:
(2) . . . Force . . . (Focus) . . . (Topic) . . .
Within Rizzi?s GB-based framework, this is sug-
gested to be the internal structure of CP. In X-bar
terms, it looks something like this:
(3) ForceP
XP Force?
Force FocusP
YP Focus?
Focus TopicP
ZP Topic?
Topic IP
Focus and Topic in most languages have prosodic
effects, so if words displaced from embedded
clauses for prosodic reasons happen to have been
raised to the beginning, it suggests that the word has
become part of some form of articulated CP struc-
ture.
Since our parsing algorithm is inspired by mini-
malism, we cannot make use of the full X-bar sys-
tem. Instead, we use Rizzi?s analysis to develop an
analysis based on features and checking.
3 The Parser in Action
3.1 A Run-through
Our parser (2004) is incremental, meaning that it
does not have access to the end of the sentence at
the beginning of a derivation. It is also ?semanti-
cally greedy?, meaning that it attempts to satisfy the
semantic requirements (through checking) as soon
as possible. So each step in the derivation consists
of attempting to see whether or not checking can be
accomplished using the current items in the ?pro-
cessing buffer? and those in the ?input queue,? and
if not, shifting a word from the input queue onto the
processing buffer. The distinction is marked, in our
notation, by a |: the words and trees before | are in
the processing buffer, and those that are after | are in
the input queue.
The algorithm also prefers move before merge.
This also ensures that trees do not have multiple
pending resolvable semantic dependencies, which
can represent a state of ambiguity in determining
which dependency to resolve and how.
We will now present an example parse of the
above sentence. But we will first present the gen-
eral outline of the parse, rather than the full details
using the formal representation; after that, we will
demonstrate the formalism. We sketch the steps of
the parse first so that we can deduce what features
we would need to make it work with the system.
We first start with everything in the input queue,
after the |:
(4) |tametsi tu scio quam sis curiosus
Now we need to shift (hear) two words for any pars-
ing operations to be performed. So we shift tametsi
and tu. tametsi (?although?) consists of tamen, et,
and si: ?nevertheless?, ?and?, and ?if.? These sug-
gest that tametsi is part of a CP, and, most likely,
Force. Since tu has been displaced from the embed-
ded clause, probably for prosodic reasons, it likely
has features that can be gleaned from the intonation
and the context, such as Focus. Since these are part
of our CP system, we merge them.
(5) tametsi
tametsi tu
scio quam sis curiosus
Now we have to shift scio. But the verb scio does not
have a complement and cannot merge with tametsi
98
until it is a complete VP. The same is true for quam
(?how?) and sis since sis (?you are?) needs a com-
plement: curiosus. So the system waits to shift ev-
erything and then merges sis and curiosus.
(6) tametsi
tametsi tu
scio quam sis
sis curiosus
Now we can merge sis and quam, since sis now has
a complement. Latin is a pro-drop language, so we
can perform the merge without having an explicit
subject, which is currently part of another tree.
(7) tametsi
tametsi tu
scio quam
quam sis
sis curiosus
quam has been given its complement. Now as a com-
plete CP, it is ready to be a complement of scio.
(8) tametsi
tametsi tu
scio
scio quam
quam sis
sis curiosus
We have a CP (the tametsi tree) and a VP (scio), and
we need to merge them to form one CP.
tametsi
(9) tametsi
tametsi tu
scio
scio quam
quam sis
sis curiosus
So this leaves us in the position of having a tu and sis
in one tree. However, we cannot bring them together.
In Sayeed and Szpakowicz (2004), we required (in
order to limit tree searches) that movement during
parsing be to positions that command the trace of
movement. Clearly, tu does not command sis. We
only permitted raising, so what should we raise? If
we raised the entire CP, we would get a tree in which
neither tu nor sis commands the other. We would
have to make another move to get sis to command
tu. So we take a simpler route and just move sis.
tametsi
(10) sisi
sis curiosus
tametsi
tametsi
tametsi tu
scio
scio quam
quam ti
Now sis commands tu. We can now move tu.
tametsi
sis
tuj(11) sisi
sis curiosus
tametsi
tametsi
tametsi tj
scio
scio quam
quam ti
Note that sis still projects after the merge, seeing that
sis holds the requirement for a subject?tu is now
in what would be known as a specifier position. It
does not matter that tu does not presently command
its trace; this is something in our account of pars-
ing that differs from GB and minimalist accounts of
movement in generation. Instead, the position with
which it must be merged after movement can be the
one that commands the original position. This allows
the target position to be the one that projects, as sis
has.
3.2 Now with Features
Now all dependencies are satisfied, and we have a
complete tree. What we need to accomplish next is
an account of the features required for this parse un-
der the system in Sayeed and Szpakowicz (2004).
We add one extra characteristic to Sayeed and Sz-
pakowicz (2004) which we will explain in greater
detail in forthcoming work: optionally-checked fea-
tures; this is required primarily to avoid having to
imagine empty categories when parsing such phe-
nomena as dropped subjects, which exists in Latin.
First of all, let us account for the lexical entries of
the initial two words, tametsi and tu. We need fea-
tures that represent the discursive effect represented
by the displacement of tu. We shall assume that this
is Focus. Also, however, we need a feature that will
prepare tametsi to merge with scio. So we represent
these two as
(12) tametsi: {UNCH?(Disc:Focus), UNCH(Type:V)}
tu: {unch(Disc:Focus) ? unch(Case:Nom, Pers:2, Num:Sg)}
Features are grouped together into feature bun-
dles, which allow simultaneous checking of features.
Note that the ? in one of the feature bundles of
tametsi means that it is optional; it does not have to
be checked with a focus feature on an adjacent con-
stituent if such a feature does not exist, but it must if
there is one.
For tu we are using feature paths as we defined in
Sayeed and Szpakowicz (2004); what is to the right
of a feature path cannot be checked before what is to
99
the left. In this case, we must check the focus feature
before we can check tu as a constituent of its proper
VP (headed by sis).
We express the trees using the same horizontal in-
dented representation as in Sayeed and Szpakowicz
(2004). We use this notation because the nodes of
this tree are too large for the ?normal? tree represen-
tation used above. So we start with
(13) | tametsi tu scio quam sis curiosus
We need to shift two words before we can do any-
thing. We thus create nodes with the above features.
(14) [tametsi {UNCH(Disc:Focus), UNCH(Type:V)}]
[tu {unch(Disc:Focus) ? unch(Case:Nom, Pers:2, Num:Sg)}]
| scio quam sis curiosus
The Focus features can be checked. Using our sys-
tem, unch and UNCH feature bundles are compati-
ble for checking, and the node with the UNCH fea-
ture projects. This form of merge among the items
already shifted can only be performed with the roots
of adjacent trees. We specified this to prevent long-
distance searches of the processing buffer.
(15) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
UNCH(Type:V)}]
tametsi
[tu {ch(Disc:Focus) ? unch(Case:Nom, Pers:2, Num:Sg)}]
| scio quam sis curiosus
When UNCH and unch features bundles are
checked, their features are unified (and replaced with
the result of unification). UNCH and unch become
CH and ch. Meanwhile, tametsi has acquired the
features of tu in the CH bundle. The purpose of this
mechanism is to transfer information up the tree in
order to support incremental parsing of discontinu-
ous NP constituents, but we find an additional use
for this below.
We make one change here to the unification of
feature bundles as described by Sayeed and Sz-
pakowicz (2004): when we replace feature bundles
with the result of unification, we replace them with
the features of the entire path with which we are
checking. This ensures that in the process of check-
ing, we do not ?hide? features that are further on
in the path. So tametsi also gains the gender, per-
son, and case features. This is actually quite a log-
ical extension of the idea we expressed in Sayeed
and Szpakowicz (2004) that a feature being checked
with a feature further down a path should be com-
patible with all the previous features on the path. In
both cases, the system should reflect the idea that
features further down a path are dependent on the
checking status of previous features. As with unifi-
cation in general, compatibility means lack of a con-
flict in ? : ? pairs (i.e., no case conflicts, and so on).
Now, as per 6, we need to shift all the remaining
words into the buffer before we get a compatible set.
So we need to determine lexical entries for all of the
remaining words. First, scio:
(16) scio: {UNCH?(Case:Nom, Pers:1, Num:Sg),
UNCH(Wh:0) ? unch(Type:V)}
We once again use a feature path. In this case, it
means that scio (?know?) must have a wh-phrase
complement2 before it is ready to be checked by
something that takes a VP complement (such as a
complementizer). So this leads us to an entry for
quam:
(17) quam: {UNCH?(Disc:Focus), UNCH(Type:V) ? unch(Wh:0)}
For quam, we also have an optional Focus feature,
because it is the head of a CP as tametsi is above.
(We might have other optional discourse features
there, but they would be superfluous for this discus-
sion.) And, like tametsi, it has a feature that allows
it to take a VP complement. Checking this feature
releases the wh-feature that allows it to become the
complement of scio.
Now we only need entries for sis and curiosus
(18) sis: {UNCH?(Case:Nom, Pers:2, Num:Sg),
UNCH(Case:Acc) ? unch(Type:V)}
curiosus: unch(Case:Acc, Gen:Masc, Num:Sg)
We use an optional feature for the requirement of
a nominative subject on sis, subjects being optional
in Latin. However, we do require it to take an ac-
cusative object. We are able to shift everything as
we did prior to 6.
(19) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
UNCH(Type:V)}]
tametsi
[tu {ch(Disc:Focus) ? unch(Case:Nom, Pers:2, Num:Sg)}]
[scio {UNCH?(Case:Nom, Pers:1, Num:Sg),
UNCH(Wh:0) ? unch(Type:V)}]
[quam {UNCH?(Disc:Focus), UNCH(Type:V) ? unch(Wh:0)}]
[sis {UNCH?(Case:Nom, Pers:2, Num:Sg),
UNCH(Case:Acc) ? unch(Type:V)}]
[curiosus unch(Case:Acc, Gen:Masc, Num:Sg)] |
Now sis and curiosus can merge. The resulting
merger between compatible unch and UNCH fea-
tures, by Sayeed and Szpakowicz (2004), also causes
the contents of those feature bundles to be unified.
(20) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
UNCH(Type:V)}]
tametsi
[tu {ch(Disc:Focus) ? unch(Case:Nom, Pers:2, Num:Sg)}]
2The 0 is just a placeholder meaning that the Wh is a single-
ton, not a pair like many of the other features.
100
[scio {UNCH?(Case:Nom, Pers:1, Num:Sg),
UNCH(Wh:0) ? unch(Type:V)]
[quam {UNCH?(Disc:Focus), UNCH(Type:V) ? unch(Wh:0)}]
[sis {UNCH?(Case:Nom, Pers:2, Num:Sg),
CH(Case:Acc, Gen:Masc, NumSg) ? unch(Type:V)}]
sis
[curiosus ch(Case:Acc, Gen:Masc, Num:Sg)] |
Now that the left feature on the feature path on sis
is checked, the verb type feature is free. It can check
with the corresponding feature on quam.
(21) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
UNCH(Type:V)}]
tametsi
[tu {ch(Disc:Focus) ? unch(Case:Nom, Pers:2, Num:Sg)}]
[scio {UNCH?(Case:Nom, Pers:1, Num:Sg),
UNCH(Wh:0) ? unch(Type:V)]
[quam {UNCH?(Disc:Focus), CH(Type:V) ? unch(Wh:0)}]
quam
[sis {UNCH?(Case:Nom, Pers:2, Num:Sg),
CH(Case:Acc, Gen:Masc, NumSg) ? ch(Type:V)}]
sis
[curiosus ch(Case:Acc, Gen:Masc, Num:Sg)] |
Feature paths allow quam to merge with scio as in 8.
(22) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
UNCH(Type:V)}]
tametsi
[tu {ch(Disc:Focus) ? unch(Case:Nom, Pers:2, Num:Sg)}]
[scio {UNCH?(Case:Nom, Pers:1, Num:Sg),
CH(Wh:0) ? unch(Type:V)]
scio
[quam {UNCH?(Disc:Focus), CH(Type:V) ? ch(Wh:0)}]
quam
[sis {UNCH?(Case:Nom, Pers:2, Num:Sg),
CH(Case:Acc, Gen:Masc, NumSg) ? ch(Type:V)}]
sis
[curiosus ch(Case:Acc, Gen:Masc, Num:Sg)] |
And, lastly, scio merges with the CP headed by
tametsi.
(23) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
CH(Type:V)}]
tametsi
tametsi
[tu {ch(Disc:Focus)
? unch(Case:Nom, Pers:2, Num:Sg)}]
[scio {UNCH?(Case:Nom, Pers:1, Num:Sg),
CH(Wh:0) ? ch(Type:V)]
scio
[quam {UNCH?(Disc:Focus), CH(Type:V) ? ch(Wh:0)}]
quam
[sis {UNCH?(Case:Nom, Pers:2, Num:Sg),
CH(Case:Acc, Gen:Masc, NumSg)
? ch(Type:V)}]
sis
[curiosus ch(Case:Acc, Gen:Masc, Num:Sg)] |
We now have a single tree, but we are in the predica-
ment of 9. We need to be able to move sis to a posi-
tion where it commands tu. And that means moving
it to join with tametsi.
In Sayeed and Szpakowicz (2004), we proposed
a mechanism by which adjuncts displaced from dis-
continuous NPs could reunite with their NPs even if
the NP had already been merged as a constituent of
a verb. This was by allowing adjuncts to merge with
the verb if the verb had a compatible CH feature
(without actually checking the adjunct feature bun-
dle). A CH feature advertises that the verb had pre-
viously merged with a compatible noun, since uni-
fication would have given the noun?s features to the
CH feature bundle.
In this case, tametsi does have a CH feature bun-
dle that appears compatible with sis, but UNCH fea-
tures are not features that cause adjunctions in our
system. We propose a minimal stipulation that will
solve this problem:
(24) UNCH features (i.e., features that indicate a
requirement for a constituent) can be moved
or merged to meet compatible CH features.
The main problem with 24 is the possibility that
unnecessary movements caused by UNCH features
may occur in such a way that the UNCH feature
would be moved out of the way of compatible unch
features.
But this is likely not a problem. Our system
prefers to exhaust all possible movements before
mergers in parsing. So, if an UNCH feature had been
in the tree, and an unch feature is introduced later
at the root (as specified in Sayeed and Szpakow-
icz (2004)), the constituent containing the UNCH
feature would immediately have moved to claim it.
Then if a compatible CH feature arrived, it would
not matter, since the UNCH feature would itself
have been checked. But if a compatible CH feature
had been in the tree before the compatible unch fea-
ture had joined, what then? The constituent contain-
ing the UNCH feature would move to join it. Then
the unch feature would join the tree. It would still
command the UNCH feature, which would move to
claim it.
There is only one unsafe case: if the CH feature
arrives before the unch feature, and it is part of a
head whose constituents contain a compatible unch
feature on the wrong constituent, then the UNCH
feature would be checked with the wrong constituent
according to the mechanism above. After all, the
UNCH feature would command the incorrect unch
feature. This possibility, however, can only exist if
there is another displaced item in the tree containing
the original CH that is compatible with the UNCH
feature but displaced from some other phrase. This
requires further investigation into Latin grammar, as
it seems unlikely that such constructions exist, given
the rarity of displacement in the first place.
101
So let us implement our solution:
(25) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
CH(Type:V)}]
[sis {UNCH?(Case:Nom, Pers:2, Num:Sg),
CH(Case:Acc, Gen:Masc, NumSg) ? ch(Type:V)}]
sis
[curiosus ch(Case:Acc, Gen:Masc, Num:Sg)] |
tametsi
tametsi
tametsi
[tu {ch(Disc:Focus)
? unch(Case:Nom, Pers:2, Num:Sg)}]
[scio {UNCH?(Case:Nom, Pers:1, Num:Sg),
CH(Wh:0) ? ch(Type:V)]
scio
[quam {UNCH?(Disc:Focus), CH(Type:V) ? ch(Wh:0)}]
quam
<sis>
Note that the maximal projections move, not the
heads of constituent trees. The maximal projections
are the highest node containing the features, and we
always take the highest node according to Sayeed
and Szpakowicz (2004). Now sis commands tu. We
can move tu.
(26) [tametsi {CH(Disc:Focus, Case:Nom, Pers:2, Num:Sg),
CH(Type:V)}]
[sis {CH(Case:Nom, Pers:2, Num:Sg),
CH(Case:Acc, Gen:Masc, NumSg) ? ch(Type:V)}]
[tu {ch(Disc:Focus) ? ch(Case:Nom, Pers:2, Num:Sg)}]
sis
sis
[curiosus ch(Case:Acc, Gen:Masc, Num:Sg)] |
tametsi
tametsi
tametsi
<tu>
[scio {UNCH?(Case:Nom, Pers:1, Num:Sg),
CH(Wh:0) ? ch(Type:V)]
scio
[quam {UNCH?(Disc:Focus), CH(Type:V)
? ch(Wh:0)}]
quam
<sis>
All optional unchecked features have been elimi-
nated, and the derivation is complete.
4 Conclusions and Future Work
Using the system of Sayeed and Szpakowicz (2004),
we have demonstrated a means to parse sentences
with constituents extracted from embedded clauses
for prosodic reasons in Latin?constituents that ap-
pear to be able to escape even subject islands. We
were able to maintain the adjacency requirement of
our system by making use of discourse features in-
spired by Rizzi?s analysis of the left periphery in
Italian in a GB framework. Thus, this highly con-
strained incremental system was able to parse a sen-
tence with a long-distance displacement.
In order to do it, though, we had to add a stip-
ulation to the system to allow the constituent that
required the displaced one to move to a command-
ing position. We also took no heed to cyclicity in
this system, which given the apparent island viola-
tion permitted by these constructions, may not seem
so bad, especially since the displaced constituent
only moves over one CP in the examples we gave.
But Kessler finds that there are rare examples where
it moves over two CPs. Of course, these cases are
even more rare than displacement over a single CP.
It could be that the difficulty in violating subjacency
is what makes these cases rare, but the checking of
the discourse feature that causes the displacement is
more important.
One characteristic of our solution and, indeed,
Sayeed and Szpakowicz (2004) in general is that
in order to maintain incrementality, we do not at-
tempt to return items displaced during generation to
their original positions. We still perform only rais-
ing, just as in most GB and minimalist accounts of
movement. This means that if the constituent of a
phrase is higher than its rightful parent in the tree,
the lower subtree raises to claim it. In this case, we
had to stipulate that constituent subtrees searching
for their own constituents could move to interme-
diate locations as adjuncts, something that Sayeed
and Szpakowicz (2004) did not specify. However,
we still maintain an essential property of our system:
movement happens as soon as possible. This means
that the first available compatible intermediate lo-
cation is sought. It becomes an empirical question,
then, whether an intermediate position could ever be
a wrong position.
References
Liliane Haegeman. 1994. Introduction to Government
and Binding Theory. Blackwell, Oxford, 2nd edition.
Brett Kessler. 1995. Discontinuous constituents in latin.
http://www.artsci.wustl.edu/?bkessler/
latin-discontinuity/discontinuity.ps.
Luigi Rizzi. 1997. The fine structure of the left periph-
ery. In L. Haegeman, editor, Elements of Grammar,
pages 281?337. Kluwer, Dordrecht.
Asad Sayeed and Stan Szpakowicz. 2004. Develop-
ing a minimalist parser for free word order languages
with discontinuous constituency. In Jose? Luis Vicedo,
Patricio Mart??nez-Barco, Rafael Mun?oz, and Maxim-
iliano Saiz, editors, EsTAL?Espan?a for Natural Lan-
guage Processing. Springer-Verlag.
Edward P. Stabler. 2001. Minimalist grammars and
recognition. In Christian Rohrer, Antje Ro?deutscher,
and Hans Kamp, editors, Linguistic Form and its Com-
putation. CSLI Publications, Stanford.
102
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 357?360,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Arabic Cross-Document Coreference Detection
Asad Sayeed,
1,2
Tamer Elsayed,
1,2
Nikesh Garera,
1,6
David Alexander,
1,3
Tan Xu,
1,4
Douglas W. Oard,
1,4,5
David Yarowsky,
1,6
Christine Piatko
1
1
Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore,
MD, USA?
2
Dept. of Computer Science, University of Maryland, College Park, MD,
USA?
3
BBN Technologies, Cambridge, MA, USA?
4
College of Information Studies,
University of Maryland, College Park, MD, USA?
5
UMIACS, University of Maryland, College
Park, MD, USA?
6
Dept. of Computer Science, Johns Hopkins University, Baltimore, MD, USA
{asayeed,telsayed}@cs.umd.edu, ngarera@cs.jhu.edu, dalexand@bbn.com,
{tanx,oard}@umd.edu, yarowsky@cs.jhu.edu, Christine.Piatko@jhuapl.edu
Abstract
We describe a set of techniques for Ara-
bic cross-document coreference resolu-
tion. We compare a baseline system of
exact mention string-matching to ones that
include local mention context information
as well as information from an existing
machine translation system. It turns out
that the machine translation-based tech-
nique outperforms the baseline, but local
entity context similarity does not. This
helps to point the way for future cross-
document coreference work in languages
with few existing resources for the task.
1 Introduction
Our world contains at least two noteworthy
George Bushes: President George H. W. Bush and
President George W. Bush. They are both fre-
quently referred to as ?George Bush.? If we wish
to use a search engine to find documents about
one of them, we are likely also to find documents
about the other. Improving our ability to find all
documents referring to one and none referring to
the other in a targeted search is a goal of cross-
document entity coreference detection. Here we
describe some results from a system we built to
perform this task on Arabic documents. We base
our work partly on previous work done by Bagga
and Baldwin (Bagga and Baldwin, 1998), which
has also been used in later work (Chen and Mar-
tin, 2007). Other work such as Lloyd et al (Lloyd,
2006) focus on techniques specific to English.
The main contribution of this work to cross-
document coreference lies in the conditions under
which it was done. Even now, there is no large-
scale resource?in terms of annotated data?for
cross-document coreference in Arabic as there is
in English (e.g. WebPeople (Artiles, 2008)). Thus,
we employed techniques for high-performance
processing in a resource-poor environment. We
provide early steps in cross-document coreference
detection for resource-poor languages.
2 Approach
We treat cross-document entities as a set of graphs
consisting of links between within-document enti-
ties. The graphs are disjoint. Each of our systems
produces a list of such links as within-document
entity pairs (A,B). We obtain within-document
entities by running the corpus through a within-
document coreference resolver?in this case, Serif
from BBN Technologies.
To create the entity clusters, we use a union-
find algorithm over the pairs. If links (A,B)
and (C,B) appear in the system output, then
{A,B,C} are one entity. Similarly, if (X,Y )
and (Z, Y ) appear in the output, then it will find
that {X,Y, Z} are one entity. If the algorithm
later discovers link (B,Z) in the system output, it
will decide that {A,B,C,X, Y, Z} are an entity.
This is efficiently implemented via a hash table
whose keys and values are both within-document
entity IDs, allowing the implementation of easily-
searched linked lists.
2.1 The baseline system
The baseline system uses a string matching cri-
terion to determine whether two within-document
entities are similar enough to be considered as part
of the same cross-document entity. Given within-
document entities A and B, the criterion is imple-
mented as follows:
1. Find the mention strings {a
1
, a
2
, . . .} and
357
{b
1
, b
2
, . . .} of A and B, respectively that are
the longest for that within-document entity
in the given document. (There may be more
than one longest mention of equal length for
a given entity.)
2. If any longest mention strings a
n
and b
m
exist
such that a
n
= b
m
(exact string match), then
A and B are considered to be part of the same
cross-document entity. Otherwise, they are
considered to be different entities.
When the system decides that two within-
document entities are connected as a single cross-
document entity, it emits a link between within-
document entities A and B represented as the pair
(A, B). We maintain a list of such links, but we
omit all links between within-document entities in
the same document.
The output of the system is a list of pairwise
links. The following two experimental systems
also produce lists of pairwise links. Union is per-
formed between the baseline system?s list and the
lists produced by the other systems to create lists
of pairs that include the information in the base-
line. However, each of the following systems?
outputs are merged separately with the baseline.
By including the baseline results in each system,
we are able to clarify the potential of each addi-
tional technique to improve performance over a
technique that is cheap to run under any circum-
stances, especially given that our experiments are
focused on increasing the number of links in an
Arabic context where links are likely to be dis-
rupted by spelling variations.
2.2 Translingual projection
We implement a novel cross-language approach
for Arabic coreference resolution by expanding
the space of exact match comparisons to approxi-
mate matches of English translations of the Arabic
strings. The intuition for this approach is that of-
ten the Arabic strings of the same named entity
may differ due to misspellings, titles, or aliases
that can be corrected in the English space. The
English translations were obtained using a stan-
dard statistical machine translation system (Chi-
ang, 2007; Li, 2008) and then compared using an
alias match.
The algorithm below describes the approach,
applied to any Arabic named entities that fail the
baseline string-match test:
1. For a given candidate Arabic named entity
pair (A,B), we project them into English by
translating the mentions using a standard sta-
tistical machine translation toolkit. Using the
projected English pair, say, (A
?
, B
?
) we per-
form the following tests to determine whether
A and B are co-referent:
(a) We do an exact string-match test in the
English space using the projected enti-
ties (A
?
, B
?
). The exact string match test
is done exactly as in the baseline system,
using the set of longest named entities in
their respective co-reference chains.
(b) If (A
?
, B
?
) fail in the exact string-match
test as in the baseline, then we test
whether they belong to a list of high con-
fidence co-referent named-entity pairs
1
precomputed for English using alias-
lists derived from Wikipedia.
(c) If (A
?
, B
?
) fails (a) and (b) then (A,B)
is deemed as non-coreferent.
While we hypothesize that translingual projection
via English should help in increasing recall since
it can work with non-exact string matches, it may
also help in increasing precision based on the as-
sumption that a name of American or English ori-
gin might have different variants in Arabic and that
translating to English can help in merging those
variants, as shown in figure 1.
 ????? ??????
 ????? 
?????? 
 ??????? 
 ???????
(Ms. Aisha)
(Aisha)
(Clenton)
(Clinton)
(Cilinton)
Aisha
Aisha
Clinton
Clinton
Clinton
Translate
via SMT
Figure 1: Illustration of translingual projection
method for resolving Arabic named entity strings
via English space. The English strings in paren-
theses indicate the literal glosses of the Arabic
strings prior to translation.
2.3 Entity context similarity
The context of mentions can play an important role
in merging or splitting potential coreferent men-
1
For example: (Sean Michael Waltman, Sean Waltman)
are high confidence-matches even though they are not an
exact-string match.
358
tions. We hypothesize that two mentions in two
different documents have a good chance of refer-
ring to the same entity if they are mentioned in
contexts that are topically very similar. A way of
representing a mention context is to consider the
words in the mention?s neighborhood. The con-
text of a mention can be defined as the words that
surround the mention in a window of n (50 in our
experiments) tokens centered by the mention. In
our experiments, we used highly similar contexts
to link mentions that might be coreferent.
Computing context similarity between every
pair of large number of mentions requires a highly
scalable and efficient mechanism. This can be
achieved using MapReduce, a distributed comput-
ing framework (Dean, 2004)
Elsayed et al (Elsayed, 2008) proposed an ef-
ficient MapReduce solution for the problem of
computing the pairwise similarity matrix in large
collections. They considered a ?bag-of-words?
model where similarity of two documents d
i
and d
j
is measured as follows: sim(d
i
, d
j
) =
?
t?d
i
?d
j
w
t,d
i
? w
t,d
j
, where w(t, d) is the weight
of term t in document d. A term contributes to
each pair that contains it. The list of documents
that contain a term is what is contained in the post-
ings of an inverted index. Thus, by processing
all postings, the pairwise similarity matrix can be
computed by summing term contributions. We use
the MapReduce framework for two jobs, inverted
indexing and pairwise similarity.
Elsayed et al suggested an efficient df-cut strat-
egy that eliminates terms that appear in many doc-
uments (having high df ) and thus contribute less
in similarity but cost in computation (e.g., a 99%
df-cut means that the most frequent 1% of the
terms were discarded). We adopted that approach
for computing similarities between the contexts
of two mentions. The processing unit was rep-
resented as a bag of n words in a window sur-
rounding each mention of a within-document en-
tity. Given a relatively small mention context, we
used a high df-cut value of 99.9%.
3 Experiments
We performed our experiments in the context of
the Automatic Content Extraction (ACE) eval-
uation of 2008, run by the National Institute
of Standards and Technology (NIST). The eval-
uation corpus contained approximately 10,000
documents from the following domains: broad-
cast conversation transcripts, broadcast news tran-
scripts, conversational telephone speech tran-
scripts, newswire, Usenet Newsgroup/Discussion
Groups, and weblogs. Systems were required to
process the large source sets completely. For per-
formance measurement after the evaluation, NIST
selected 412 of the Arabic source documents out
of the larger set (NIST, 2008).
For development purposes we used the NIST
ACE 2005 Arabic data with within-document
ground truth. This consisted of 1,245 documents.
We also used exactly 12,000 randomly selected
documents from the LDC Arabic Gigaword Third
Edition corpus, processed through Serif. The Ara-
bic Gigaword corpus was used to select a thresh-
old of 0.4956 for the context similarity technique
via inspection of (A,B) link scores by a native
speaker of Arabic.
It must be emphasized that there was no ground
truth available for this task in Arabic. Performing
this task in the absence of significant training or
evaluation data is one emphasis of this work.
3.1 Evaluation measures
We used NIST?s scoring techniques to evaluate the
performance of our systems. Scoring for the ACE
evaluation is done using an scoring script provided
by NIST which produces many kinds of statistics.
NIST mainly uses a measure called the ACE value,
but it also computes B-cubed.
B-Cubed represents the task of finding cross-
document entities in the following way: if a user
of the system is searching for a particular Bush
and finds document D, he or she should be able to
find all of the other documents with the same Bush
in them as links from D?that is, cross-document
entities represent graphs connecting documents.
Bagga and Baldwin are able to define precision,
recall, and F-measure over a collection of docu-
ments in this way.
The ACE Value represents a score similar to
B-Cubed, except that every mention and within-
document entity is weighted in NIST?s specifica-
tion by a number of factors. Every entity is worth 1
point, a missing entity worth 0, and attribute errors
are discounted by multiplying by a factor (0.75 for
CLASS, 0.5 for TYPE, and 0.9 for SUBTYPE).
Before scoring can be accomplished, the enti-
ties found by the system must be mapped onto
those found in the reference provided by NIST.
The ACE scorer does this document-by-document,
359
selecting the mapping that produces the highest
score. A description of the evaluation method and
entity categorization is available at (NIST, 2008).
3.2 Results and discussion
The results of running the ACE evaluation script
on the system output are shown in table 1. The
translingual projection system achieves higher
scores than all other systems on all measures. Al-
though it achieves only a 2 point improvement
over the baseline ACE value, it should be noted
that this represents a substantial number of at-
tributes per cross-document entity that it is getting
right.
Thresh B-Cubed ACE
System hold Prec Rec F Val.
Baseline 37.5 44.1 40.6 19.2
TrnsProj 38.4 44.8 41.3 21.2
CtxtSim 0.2 37.6 35.2 36.4 15.9
CtxtSim 0.3 37.4 43.8 40.3 18.9
CtxtSim 0.4 37.5 44.1 40.6 19.3
CtxtSim 0.4956 37.5 44.1 40.6 19.3
CtxtSim 0.6 37.5 44.1 40.6 19.2
Table 1: Scores from ACE evaluation script.
On the other hand, as the context similarity
threshold increases, we notice that the B-Cubed
measures reach identical values with the baseline
but never exceed it. But as it decreases, it loses
B-Cubed recall and ACE value.
While two within-document entities whose
longest mention strings match exactly and are le-
gitimately coreferent are likely to be mentioned in
the same contexts, it seems that a lower (more lib-
eral) threshold introduces spurious links and cre-
ates a different entity clustering.
Translingual projection appears to include links
that exact string matching in Arabic does not?
part of its purpose is to add close matches to those
found by exact string matching. It is able to in-
clude these links partly because it allows access to
resources in English that are not available for Ara-
bic such as Wikipedia alias lists.
4 Conclusions and Future Work
We have evaluated and discussed a set of tech-
niques for cross-document coreference in Arabic
that can be applied in the absence of significant
training and evaluation data. As it turns out, an
approach based on machine translation is slightly
better than a string-matching baseline, across all
measures. It worked by using translations from
Arabic to English in order to liberalize the string-
matching criterion, suggesting that using further
techniques via English to discover links may be
a fruitful future research path. This also seems
to suggest that a Bagga and Baldwin-style vector-
space model may not be the first approach to pur-
sue in future work on Arabic.
However, varying other parameters in the con-
text similarity approach should be tried in order
to gain a fuller picture of performance. One of
them is the df-cut of the MapReduce-based sim-
ilarity computation. Another is the width of the
word token window we used?we may have used
one that is too tight to be better than exact Arabic
string-matching.
References
Javier Artiles and Satoshi Sekine and Julio Gonzalo
2008. Web People Search?Results of the first eval-
uation and the plan for the second. WWW 2008.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. COLING-ACL 1998.
Y. Chen and J. Martin. 2007. Towards robust unsuper-
vised personal name disambiguation. EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied Data Processing on Large Clusters. OSDI.
T. Elsayed and J. Lin and D. W. Oard. 2008. Pair-
wise Document Similarity in Large Collections with
MapReduce. ACL/HLT.
Z. Li and S. Khudanpur. 2008. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. ACL SSST.
L. Lloyd and Andrew Mehler and Steven Skiena. 2006.
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching.
NIST. 2008. Automatic Content Extraction 2008 Eval-
uation Plan (ACE08).
360
Coling 2010: Poster Volume, pages 1095?1103,
Beijing, August 2010
?Expresses-an-opinion-about?: using corpus statistics in an information
extraction approach to opinion mining
Asad B. Sayeed, Hieu C. Nguyen,
and Timothy J. Meyer
Department of Computer Science
University of Maryland, College Park
asayeed@cs.umd.edu,
hcnguyen88@gmail.com,
tmeyer1@umd.edu
Amy Weinberg
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland, College Park
weinberg@umiacs.umd.edu
Abstract
We present a technique for identifying the
sources and targets of opinions without
actually identifying the opinions them-
selves. We are able to use an informa-
tion extraction approach that treats opin-
ion mining as relation mining; we iden-
tify instances of a binary ?expresses-an-
opinion-about? relation. We find that
we can classify source-target pairs as be-
longing to the relation at a performance
level significantly higher than two relevant
baselines.
This technique is particularly suited to
emerging approaches in corpus-based so-
cial science which focus on aggregating
interactions between sources to determine
their effects on socio-economically sig-
nificant targets. Our application is the
analysis of information technology (IT)
innovations. This is an example of a
more general problem where opinion is
expressed using either sub- or supersets
of expressive words found in newswire.
We present an annotation scheme and an
SVM-based technique that uses the lo-
cal context as well as the corpus-wide
frequency of a source-target pair as data
to determine membership in ?expresses-
an-opinion-about?. While the presence
of conventional subjectivity keywords ap-
pears significant in the success of this
technique, we are able to find the most
domain-relevant keywords without sacri-
ficing recall.
1 Introduction
Two problems in sentiment analysis consist of
source attribution and target discovery?who has
an opinion, and about what? These problems are
usually presented in terms of techniques that re-
late them to the actual opinion expressed. We have
a social science application in which the identifi-
cation of sources and targets over a large volume
of text is more important than identifying the ac-
tual opinions particularly in experimenting with
social science models of opinion trends. Con-
sequently, we are able to use lightweight tech-
niques to identify sources and targets without us-
ing resource-intensive techniques to identify opin-
ionated phrases.
Our application for this work is the discovery
of networks of influence among opinion leaders
in the IT field. We are interested in answering
questions about who the leaders in the field are
and how their opinion matches the social and eco-
nomic success of IT innovation. Consequently,
it became necessary for us to construct a system
(figure 1) that finds the expressions in text that re-
fer to an opinion leader?s activities in promoting
or deprecating a technology.
In this paper, we demonstrate an information
extraction (Mooney and Bunescu, 2005) approach
based in relation mining (Girju et al, 2007) that
is effective for this purpose. We describe a tech-
nique by which corpus statistics allow us to clas-
sify pairs of entities and sentiment analysis targets
as instances of an ?expresses-an-opinion-about?
relation in documents in the IT business press.
This genre has the characteristic that many enti-
ties and targets are represented within individual
sentences and paragraphs. Features based on the
1095
Figure 1: Opinion relation classification system.
frequency counts of query results allow us to train
classifiers that allow us to extract ?expresses-an-
opinion-about? instances, using a very simple an-
notation strategy to acquire training examples.
In the IT business press, the opinionated lan-
guage is different from the newswire text for
which many extant sentiment tools were devel-
oped. We use an existing sentiment lexicon along-
side other non-sentiment-specific measures that
adapt resources from newswire-developed senti-
ment analysis projects without imposing the full
complexity of those techniques.
1.1 Corpus-based social science
The ?expresses-an-opinion-about? relation is a bi-
nary relation between opinion sources and tar-
gets. Sources include both people?typically
known experts, corporate representatives, and
other businesspeople?as well as organizations
such as corporations and government bodies. The
targets are the innovation terms. Therefore, the
use of named-entity recognition in this project
only focuses on persons and organizations, as the
targets are a fixed list.
1.2 Reifying opinion in an application
context
A hypothesis implicit in our social science task
is that opinion leaders create trends in IT innova-
tion adoption partly by the text that their activi-
ties generate in the IT business press. This text
has an effect on readers, and these readers act in
such a way that in turn may generate more or less
prominence for a given innovation?and may also
generate further text.
Some of these text-generating activities include
expressions of private states in an opinion source
(e.g., ?I believe that Web 2.0 is the future?). These
kinds of expressions suggest a particular ontol-
ogy of opinion analysis involving discourse re-
lations across various types of clauses (Wilson
and Wiebe, 2005; Wilson et al, 2005a). How-
ever, if we are to track the relative adoption of
IT innovations, we must take into account the
effect of the text on the reader?s opinion about
these innovations?there are expressions other
than those of private states that have an effect on
the reader. These can be considered to be ?opin-
ionated acts1.?
Opinionated acts can include things like pur-
chasing and adoption decisions by organizations.
For example:
And like other top suppliers to Wal-
Mart Stores Inc., BP has been in-
volved in a mandate to affix radio
frequency identification tags with em-
bedded electronic product codes to its
crates and pallets. (ComputerWorld,
January 2005)
In this case, both Wal-Mart and BP have expressed
implicit approval for radio frequency identifica-
tion by adopting it. This may affect the reader?s
own likelihood of support or adoption of the tech-
nology. In this context, we do not directly con-
sider the subjectivity of the opinion source, even
though that may be present.
Opinionated acts include things like implica-
tions of technology use, not just adoption. We
thus define opinion expressions as follows: any
expression involving some actor that is likely to
affect a reader?s own potential to adopt, reject, or
speak positively or negatively of a target. This
would include ?conventional? expressions of pri-
vate states as well as opinionated acts.
Our definition of ?expresses-an-opinion-about?
follows immediately. SourceA expresses an opin-
ion about target B if an interested third party C?s
actions towards B may be affected by A?s textu-
ally recorded actions, in a context where actions
1Somasundaran and Wiebe (2009) mention a related cate-
gory of ?pragmatic opinions? that involve world knowledge.
1096
have positive or negative weight (e.g. purchasing,
promotion, etc.).
1.3 Domain-specific sentiment detection
We construct a system that uses named-entity
recognition and supervised machine learning via
SVMs to automatically discover instances of
?expresses-an-opinion-about? as a binary relation
at reasonably high accuracy and precision.
The advantage of our approach is that, outside
of HMM-based named-entity detection (BBN?s
IdentiFinder), we evade the need for resource-
intensive techniques such as sophsticated gram-
matical models, sequence models, and semantic
role labelling (Choi et al, 2006; Kim and Hovy,
2006) by removing the focus on the actual opinion
expressed. Then we can use a simple supervised
discriminative technique with a joint model of lo-
cal term frequency information and corpus-wide
co-occurrence distributions in order to discover
the raw data for opinion trend modelling. The
most complex instrument we use from sentiment
analysis research on conventional newswire is a
sentiment keyword lexicon (Wilson et al, 2005b);
furthermore, our techniques allow us to distin-
guish sentiment keywords that indicate opinion in
this domain from keywords that actually indicate
that there is no opinion relation between source
and target.
While we show that this lightweight technique
works well at a paragraph level, it can also be used
in conjunction with more resource-intensive tech-
niques used to find ?conventional? opinion ex-
pressions. Also, the use of topic aspects (Soma-
sundaran and Wiebe, 2009) in conjunction with
target names has been associated with an improve-
ment in recall. However, our technique still per-
forms well above the baseline without these im-
provements.
2 Methodology
2.1 Article preparation
We have a list of IT innovations on which our
opinion leader research effort is most closely fo-
cused. This list contains common names that re-
fer to these technologies as well as some alternate
names and abbreviations. We selected articles at
random from the ComputerWorld IT journal that
contained mentions of members of the given list.
These direct mentions were tagged in the docu-
ment as XML entities.
Each article was processed by BBN?s Identi-
Finder 3.3 (Bikel et al, 1999), a named entity
recognition (NER) system that tags named men-
tions of person and organization entities2.
The articles were then divided into paragraphs.
For each paragraph, we generated candidate rela-
tions from the entities and innovations mentioned
therein. To generate candidates, we paired every
entity in the paragraph with every innovation. Re-
dundant pairs are sometimes generated when an
entity is mentioned in multiple ways in the para-
graph. We eliminated most of these by removing
entities whose mentions were substrings of other
mentions. For example, ?Microsoft? and ?Mi-
crosoft Corp.? are sometimes found in the same
paragraph; we eliminate ?Microsoft.?
2.2 Annotation
We processed 20 documents containing 157 rela-
tions in the manner described in the previous sec-
tion. Then two domain experts (chosen from the
authors) annotated every candidate pair in every
document according to the following scheme (il-
lustrated in figure 2):
? If the paragraph associated with the candi-
date pair describes a valid source-target rela-
tion, the experts annotated it with Y.
? If the paragraph does not actually contain
that source-target relation, the experts anno-
tated it with N.
? If either the source or the target is misidenti-
fied (e.g., errors in named entity recognition),
the experts annotated it with X.
The Cohen?s ? score was 0.6 for two annotators.
While this appears to be only moderate agree-
ment, we are still able to achieve good perfor-
mance in our experiments with this value.
2In a separate research effort, we found that IdentiFinder
has a high error rate on IT business press documents, so we
built a system to reduce the error post hoc. We ran this sys-
tem over the IdentiFinder annotations.
1097
Davis says she has especially enjoyed work-
ing with the PowerPad?s bluetooth interfaces to
phones and printers. ?It?s nice getting into new
wireless technology,? she says. The bluetooth
capability will allow couriers to transmit data
without docking their devices in their trucks.
Source Target Class
Davis bluetooth Y/N/X
PowerPad bluetooth Y/N/X
Figure 2: Example paragraph annotation exercise.
We then selected 75 different documents for
each annotator and processed and annotated them
as above. At this point we have the instances and
the classes to which they belong. We labelled 466
instances of Y, 325 instances of N, and 280 in-
stances of X, for a total of 1071 relations.
2.3 Feature vector generation
We have four classes of features for every rela-
tion instance. Each type of feature consists of
counts extracted from an index of 77,227 Comput-
erWorld articles from January 1988 to June 2008
generated by the University of Massachusetts
search engine Indri (Metzler and Croft, 2004).
Each vector is normalized to the unit vector. The
index is not stemmed for performance reasons.
The first type of feature consists of simple doc-
ument frequency statistics for source-target pairs
throughout the corpus. The second type consists
of document frequency counts of source-target
pairs when they are in particularly close proxim-
ity to one another. The third type consists of docu-
ment frequency counts of source target pairs prox-
imate to keywords that reflect subjectivity. The
fourth and final type consist of TFIDF scores of
vocabulary items in the paragraph containing the
putative opinion-holding relation (unigram con-
text features). We use the first three features types
to represent the likelihood in the ?world? that the
source has an opinion about the target and the last
feature type to represent the likelihood of the spe-
cific paragraph containing an opinion that reflects
the source-target relation.
We have a total of 7450 features. Each vec-
tor is represented as a sparse array. 806 features
represent queries on the Indri index. For all the
features, we therefore have 863,226 index queries.
We perform the queries in parallel on 25 proces-
sors to generate the full feature array, which takes
approximately an hour on processors running at
8Ghz. We eliminate all values that are smaller in
magnitude than 0.000001 after unit vector normal-
ization.
2.3.1 Frequency statistics
There are two simple frequency statistics fea-
tures generated from Indri queries. The first is
the raw frequency counts of within-document co-
occurrences of the source and target in the rela-
tion. The second is the mean co-occurrence fre-
quency of the source and target per Computer-
World document.
2.3.2 Proximity counts
For every relation, we query Indri to check how
often the source and the target appear in the same
document in the ComputerWorld corpus within
four word ranges: 5, 25, 100, and 500. That is
to say, if a source and a target appear within five
words of one another, this is included in the five-
word proximity feature. This generates four fea-
tures per relation.
2.3.3 Subjectivity keyword proximity counts
We augment the proximity counts feature with
a third requirement: that the source and target ap-
pear within one of the ranges with a ?subjectivity
keyword.? The keywords are taken from Univer-
sity of Pittsburgh subjectivity lexicon; the utility
of this lexicon is supported in recent work (Soma-
sundaran and Wiebe, 2009).
For performance reasons, we did not use all of
the entries in the subjectivity lexicon. Instead,
we used a TFIDF-based measure to rank the key-
words by their prevalence in the ComputerWorld
corpus where the term frequency is defined over
the entire corpus. Then we selected 200 keywords
with the highest score.
For each keyword, we use the same proximity
ranges (5, 25, 100, and 500) in queries to Indri
where we obtain counts of each keyword-source-
target triple for each range. There are threfore 800
subjectivity keyword features.
1098
Positive class Negative class System Prec / Rec / F Accuracy
Y N Random baseline 0.60 / 0.53 / 0.56 0.52
Y N Maj.-class (Y) baseline 0.59 / 1.00 / 0.74 0.59
Y N Linear kernel 0.70 / 0.73 / 0.72 0.66
Y N RBF kernel 0.72 / 0.76 / 0.75 0.69
Y N/X Random baseline 0.44 / 0.50 / 0.47 0.50
Y N/X RBF kernel 0.65 / 0.55 / 0.59 0.67
Table 1: Results with all features against majority class and random baselines. All values are mean
averages under 10-fold cross validation.
2.3.4 Word context (unigram) features
For each relation, we take term frequency
counts of the paragraph to which the relation be-
longs. We multiply them by the IDF of the term
across the ComputerWorld corpus. This yields
6644 features over all paragraphs.
2.4 Machine learning
On these feature vectors, we trained SVM models
using Joachims? (1999) svmlight tool. We use a
radial basis function kernel with an error cost pa-
rameter of 100 and a ? of 0.25. We also use a lin-
ear kernel with an error cost parameter of 100 be-
cause it is straightforwardly possible with a linear
kernel to extract the top features from the model
generated by svmlight.
3 Experiments
We conducted most of our experiments with only
the Y and N classes, discarding all X; this re-
stricted most of our results to those assuming cor-
rect named entity recognition. Y was the posi-
tive class for training the svmlight models, and
N was the negative class. We also performed ex-
periments with N and X together being the nega-
tive class; this represents the condition that we are
seeking ?expresses-an-opinion-about? even with a
higher named-entity error rate.
We use two baselines. One is a random base-
line with uniform probability for the positive and
negative classes. The other is a majority-class as-
signer (Y is the majority class).
The best system for the Y vs. N experiment was
subjected to feature ablation. We first systemati-
cally removed each of the four feature types indi-
vidually. The feature type whose removal had the
largest effect on performance was removed per-
manently, and the rest of the features were tested
without it. This was done once more, at which
point only one feature type was present in the
models tested.
3.1 Evaluation
All evaluation was performed under 10-fold cross
validation, and we report the mean average of all
performance metrics (precision, recall, harmonic
mean F-measure, and accuracy) across folds.
We define these measures in the standard infor-
mation retrieval form. If tp represents true pos-
itives, tn true negatives, fp false positives, and
fn false negatives, then precision is tp/(tp+fp),
recall tp/(tp + fn), F-measure (harmonic mean)
is 2(prec ? rec)/(prec + rec), and accuracy is
(tp+ tn)/(tp+ fp+ fn+ tn).
4 Results and discussion
The results of the experiments with all features are
listed in table 1.
4.1 ?Perfect? named entity recognition
We achieve best results in the Y versus N case us-
ing the radial basis function kernel. We find im-
provement in F-measure and accuracy at 19% and
17% respectively. Simply assigning the majority
class to all test examples yields a very high re-
call, by definition, but poor precision and accu-
racy; hence its relatively high F-measure does not
reflect high applicability to further processing, as
the false positives would amplify errors in our so-
cial science application.
The linear kernel has results that are below the
RBF kernel for all measures, but are relatively
close to the RBF results.
1099
Subjectivity Proximity Frequency Unigram Prec / Rec / F Accuracy
X X X X 0.72 / 0.76 / 0.75 0.69
X X X 0.67 / 0.89 / 0.76 0.67
X X X 0.71 / 0.77 / 0.73 0.68
X X X 0.70 / 0.78 / 0.74 0.67
X X X 0.69 / 0.77 / 0.73 0.67
X X 0.63 / 0.91 / 0.75 0.64
X X 0.66 / 0.89 / 0.76 0.67
X X 0.65 / 0.90 / 0.76 0.66
X 0.61 / 0.92 / 0.73 0.60
X 0.61 / 0.94 / 0.74 0.60
Table 2: Feature ablation results for RBF kernel on Y vs. N case. The first line is the RBF result with
all features from table 1.
4.2 Introducing erroneous named entities
The case of Y versus N and X together unsurpris-
ingly performed worse than the case where named
entity errors were eliminated. However, relative to
its own random baseline, it performed well, with
a 12% and 17% improvement in F-measure and
accuracy using the RBF kernel. This suggests that
the errors do not introduce enough noise into the
system to produce a large decline in performance.
As X instances are about 26% of the total and
we see a considerable drop in recall, we can say
that some of the X instances are likely to be similar
to valid Y ones; indeed, examination of the named
entity recognizer?s errors suggests that some in-
correct organizations (e.g. product names) occur
in contexts where valid organizations occur. How-
ever, precision and accuracy have not fallen nearly
as far, so that the quality of the output for further
processing is not hurt in proportion to the intro-
duction of X class noise.
4.3 Feature ablation
Table 2 contains the result of our feature abla-
tion experiments. Overall, the removal of features
causes the SVM models to behave increasingly
like a majority class assigner. As we mentioned
earlier, higher recall at the expense of precision
and accuracy is not an optimal outcome for us
even if the F-measure is preserved. In our results,
the F-measure values are remarkably stable.
In the first round of feature removal, the sub-
jectivity keyword features have the biggest ef-
fect with the largest drop in precision and the
largest increase in recall; high-TFIDF words from
a general-purpose subjectivity lexicon allow the
model to assign more items to the negative class.
The next round of feature removal shows
that the proximity features have the next largest
amount of influence on the classifier, as precision
drops by 4%. The proximity features are very sim-
ilar to the subjectivity features in that they too in-
volve queries over windows of limited word sizes;
the subjectivity keyword features only differ in
that a subjectivity keyword must be within the
window as well. That the proximity features are
not more important than the subjectivity features,
implies that the subjectivity keywords matter to
the classifier, even though they are not specific to
the IT domain. However, the proximity of sources
and targets also matters, even in the absence of the
subjectivity keywords.
Finally, we are left with the frequency features
and the unigram context features. Either set of
features supports a level of performance greater
than the random baseline in table 1. However,
the unigram features allow for slightly better re-
call than the frequency features without loss of
precision, but this may not be very surprising, as
there are many more unigram features than fre-
quency features. More importantly, however, ei-
ther of these feature types is sufficient to prevent
the classifier from assigning the majority class all
of the time, although they come close.
1100
Feature type Range Keyword
Subjectivity 500 agreement
Subjectivity 500 critical
Subjectivity 500 want
Subjectivity 100 will
Subjectivity 100 able
Subjectivity 500 worth
Subjectivity 500 benefit
Subjectivity 100 trying
Subjectivity 500 large
Subjectivity 500 competitive
Table 3: The 10 most positive features via a linear
kernel in descending order.
Feature type Range Keyword
Subjectivity 500 low
Subjectivity 500 ensure
Subjectivity 25 want
Subjectivity 100 vice
Subjectivity 500 slow
Subjectivity 100 large
Subjectivity 500 ready
Subjectivity 100 actually
Subjectivity 100 ready
Subjectivity 100 against
Table 4: The 10 most negative features via a linear
kernel in descending order.
4.4 Most discriminative features
The models generated by svmlight under a lin-
ear kernel allow for the extraction of feature
weights by a script written by svmlight?s creator.
We divided the instances into a single 70%/30%
train/test split and trained a classifier with a linear
kernel and an error cost parameter of 100, with re-
sults similar to those reported under 10-fold cross-
validation in table 1. We used all features.
Then we were able to extract the 10 most pos-
itive (table 3) and 10 most negative (table 4) fea-
tures from the model.
Interestingly, all of these are subjectivity key-
word features, even the negatively weighted fea-
tures. The top positive features are often evocative
of business language, such as ?agreement?, ?crit-
ical?, and ?competitive?. Most of them emerge
from queries at the 500-word range, suggesting
that their presence in the document itself is evi-
dence that a source is expressing an opinion about
a target. That most of them are subjectivity fea-
tures is reflected in the feature ablation results in
the previous section.
It is less clear why ?ensure? and ?against?
should be evidence that a source-target pair is not
an instance of ?expresses-an-opinion-about?. On
the other hand, words like ?ready? (which appears
twice) and ?actually? can conceivably reflect sit-
uations in the IT domain that are not matters of
opinion. In either case, this demonstrates one of
the advantages of our technique, as these are fea-
tures that actively assist in classifying some rela-
tion instances as not expressing sentiment. For ex-
ample, contrary to what we would expect, ?want?
in a 25-word window with a source and a tar-
get is actually evidence against an ?expresses-an-
opinion-about? relation in text about IT innova-
tions (ComputerWorld, July 2007):
But Klein, who is director of infor-
mation services and technology, didn?t
want IT to become the blog police.
In this example, Klein is expressing a desire,
but not about the innovation (blogs) in question.
5 Conclusions and future work
5.1 Summary
We constructed and evaluated a system that de-
tects at paragraph level whether entities relevant
to the IT domain have expressed an opinion about
a list of IT innovations of interest to a larger social
science research program. To that end, we used
a combination of co-occurrence statistics gleaned
from a document indexing tool and TFIDF val-
ues from the local term context. Under these
novel conditions, we successfully exceeded sim-
ple baselines by large margins.
Despite only moderate annotator agreement, we
were able to produce results coherent enough to
successfully train classifiers and conduct experi-
ments.
Our feature ablation study suggests that all of
the feature types played a role in improving the
performance of the system over the random and
1101
majority-class baselines. However, the subjec-
tivity keyword features from an existing lexicon
played the largest role, followed by the proxim-
ity and unigram features. Subjectivity keyword
features dominated the ranks of feature weights
under a linear kernel, and the features most pre-
dictive of membership in ?expresses-an-opinion-
about? are words with semantic significance in the
context of the IT business press.
5.2 Application to other domains
We used somewhat na??ve statistics in a simple
machine learning system in order to implement a
form of opinion mining for a particular domain.
The most direct linguistic guidance we provided
our system were the query ranges and the sub-
jectivity lexicon. The generality of this approach
yields the advantage that it can be applied to other
domains where there are ways of expressing senti-
ment unique to those domains outside of newswire
text and product reviews.
5.3 Improving the features
Our use of an existing sentiment lexicon opens the
door in future work for the use of techniques to
bootstrap a larger sentiment lexicon that empha-
sizes domain-specific language in the expression
of opinion, including opinionated acts. In fact,
our results suggest that terminology in the exist-
ing lexicon that is most prominently weighted in
our classifier also tends to be domain-relevant. In
a further iteration, we might also improve perfor-
mance by using terms outside the lexicon that tend
to co-occur with terms from the lexicon.
5.4 Data generation
Our annotation exercise was a very simple one in-
volving a short reading exercise and the selection
of one of three choices per relation instance. This
type of exercise is ideally suited to the ?crowd-
sourcing? technique of paying many individuals
small amounts of money to perform these simple
annotations over the Internet. Previous research
(Snow et al, 2008) suggests that we can generate
very large datasets very quickly in this way; this
is a requirement for expanding to other domains.
5.5 Scalability
In order to classify on the order of 1000 instances,
it took nearly a million queries to the Indri index,
which took a little over an hour to do in parallel
on 25 processors by calling the Indri query engine
afresh at each query. While each query is nec-
essary to generate each feature value, there are a
number of optimizations we could implement to
accelerate the process. Various types of dynamic
programming and caching could be used to han-
dle related queries. One way of scaling up to
larger datasets would be to use the MapReduce
and cloud computing paradigms on which text
processing tools have already been implemented
(Moreira et al, 2007).
The application for this research is a social sci-
ence exercise in exploring trends in IT adoption
by analysing the IT business press. In the end, the
perfect discovery of all instances of ?expresses-
an-opinion-about? is not as important as finding
enough reliable data over a large number of docu-
ments. This work brings us several steps closer in
finding the right combination of features in order
to acquire trend-representative data.
Acknowledgements
This paper is based upon work supported by the
National Science Foundation under Grant IIS-
0729459.
References
Bikel, Daniel M., Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3).
Choi, Yejin, Eric Breck, and Claire Cardie. 2006.
Joint extraction of entities and relations for opinion
recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP).
Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: classification of semantic re-
lations between nominals. In SemEval ?07: Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 13?18, Morristown, NJ,
USA. Association for Computational Linguistics.
Joachims, T. 1999. Making large-scale SVM learn-
ing practical. In Scho?lkopf, B., C. Burges, and
1102
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
Kim, Soo-Min and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In SST ?06: Proceedings of
the Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Metzler, Donald and W. Bruce Croft. 2004. Combin-
ing the language model and inference network ap-
proaches to retrieval. Information Processing and
Management, 40(5):735 ? 750.
Mooney, Raymond J. and Razvan Bunescu. 2005.
Mining knowledge from text using information ex-
traction. SIGKDD Explor. Newsl., 7(1):3?10.
Moreira, Jose? E., Maged M. Michael, Dilma Da Silva,
Doron Shiloach, Parijat Dube, and Li Zhang. 2007.
Scalability of the nutch search engine. In Smith,
Burton J., editor, ICS, pages 3?12. ACM.
Rogers, Everett M. 2003. Diffusion of Innovations,
5th Edition. Free Press.
Snow, Rion, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In EMNLP 2008, Morristown,
NJ, USA.
Somasundaran, Swapna and Janyce Wiebe. 2009.
Recognizing stances in online debates. In ACL-
IJCNLP ?09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 1. Association
for Computational Linguistics.
Wilson, Theresa and Janyce Wiebe. 2005. Annotating
attributions and private states. In ACL 2005 Work-
shop: Frontiers in Corpus Annotation II: Pie in the
Sky, pages 53?60.
Wilson, Theresa, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. OpinionFinder: A system for subjec-
tivity analysis. In HLT/EMNLP.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP.
1103
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 356?367, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Syntactic surprisal affects spoken word duration in conversational contexts
Vera Demberg, Asad B. Sayeed, Philip J. Gorinski, and Nikolaos Engonopoulos
M2CI Cluster of Excellence and
Department of Computational Linguistics and Phonetics
Saarland University
66143 Saarbru?cken, Germany
{vera,asayeed,philipg,nikolaos}@coli.uni-saarland.de
Abstract
We present results of a novel experiment to in-
vestigate speech production in conversational
data that links speech rate to information den-
sity. We provide the first evidence for an asso-
ciation between syntactic surprisal and word
duration in recorded speech. Using the AMI
corpus which contains transcriptions of focus
group meetings with precise word durations,
we show that word durations correlate with
syntactic surprisal estimated from the incre-
mental Roark parser over and above simpler
measures, such as word duration estimated
from a state-of-the-art text-to-speech system
and word frequencies, and that the syntac-
tic surprisal estimates are better predictors of
word durations than a simpler version of sur-
prisal based on trigram probabilities. This re-
sult supports the uniform information density
(UID) hypothesis and points a way to more re-
alistic artificial speech generation.
1 Introduction
The uniform information density (UID) hypothesis
suggests that speakers try to distribute information
uniformly across their utterances (Frank and Jaeger,
2008). Information density can be measured in
terms of the surprisal incurred at each word, where
surprisal is defined as the negative log-probability
of an event. This paper sets out to test whether UID
holds across different linguistic levels, i.e. whether
speakers adapt word duration during production to
syntactic surprisal, such that words with higher sur-
prisal have longer durations than words with lower
surprisal. We investigate this question in a corpus
of transcribed speech from a mix of native and non-
native English speakers, a population that is a non-
trivial component of the user base for language tech-
nologies developed for English. This data reflects a
casual, uncontrolled conversational environment.
Using linear mixed-effects modeling, we found
that syntactic surprisal as calculated from a top-
down incremental PCFG parser accounts for a sig-
nificant amount of variation in spoken word dura-
tion, using an HMM-trained text-to-speech system
as a baseline. The findings of this paper provide ad-
ditional support the uniform information density hy-
pothesis and furthermore have implications for the
design of text-to-speech systems, which currently
do not take into account higher-level linguistic in-
formation such as syntactic surprisal (or even word
frequencies) for their word duration models.
1.1 Related work
The use of word-level surprisal as a predictor of pro-
cessing difficulty is based on the notion that pro-
cessing difficulty results when a word is encountered
that is unexpected given its preceding context. The
amount of surprisal on a word wi can be formal-
ized as the log of the inverse conditional probabil-
ity of wi given the preceding words in the sentence
w1 . . . wi?1, or ? logP (wi|w1...i?1). If this proba-
bility is low, then the word is unexpected, and sur-
prisal is high. Surprisal can be estimated in different
ways, e.g. from word sequences (n-grams) or with
respect to the possible syntactic structures covering
a sentence prefix (see Section 4).
Hale (2001) showed that surprisal calculated from
a probabilistic Earley parser correctly predicts well-
356
known processing phenomena that were believed
to emerge from structural ambiguities (e.g., garden
paths) and Levy (2008) further demonstrated the rel-
evance of surprisal to human sentence processing
difficulty on a range of syntactic processing diffi-
culty phenomena.
There is existing work in correlating information-
theoretic measures of linguistic redundancy to the
observed duration of speech units. Aylett and Turk
(2006) demonstrate that the contextual predictability
of a syllable (n-gram log probability) has an inverse
relationship to syllable duration in speech. Their ex-
periments were performed using a carefully articu-
lated speech synthesis training corpus.
This type of work fits into a larger programme of
understanding how speakers schedule utterances to
avoid high variation in the transmission of linguis-
tic information over time, also known as the Uni-
form Information Density (UID) hypothesis (Flo-
rian Jaeger, 2010). Levy and Jaeger (2007) show
that the reduction of optional that-complementizers
in English is related to trigram surprisal; low sur-
prisal predicts a high likelihood of reduction. Flo-
rian Jaeger (2010) shows the same result of in-
creased reduction when the complementizer is more
predictable according to information density calcu-
lated in terms of the main verb?s subcategorization
frequency.
Frank and Jaeger (2008) provide evidence that a
UID account can predict the use of reduced forms
of ?be?, ?have?, and ?not? in English. They use the
surprisal of the candidate word itself as well as sur-
prisals of the word before and after, computing bi-
gram and trigram estimates directly from the corpus
without smoothing or backoff.
Jurafsky et al2001) report a corpus study sim-
ilar to ours, showing that words that are more pre-
dictable from context are reduced. As measures
of word predictability, they use bigram and trigram
models, as well as joint probabilities, but not syntac-
tic surprisal.
Within the same theme of utterance duration
vs. information content, Piantadosi et al2011)
performed a study using Google-derived n-gram
datasets on the lexica of multiple languages, includ-
ing English, Portuguese, and Czech. For every word
in a given language?s lexicon, they calculated 2-, 3-,
and 4-gram surprisal values using the Google dataset
for every occurrence of the word, and then they
took the mean surprisal for that word over all oc-
currences. The 3-gram surprisal values in particular
were a better predictor of orthographic length than
unigram frequency, providing evidence for the use
of information content and contextual predictability
as improvement over a Zipf?s Law view of commu-
nicative efficiency. This is an n-gram approach to
supporting the UID hypothesis.
However, there is some counter-evidence for the
UID-based view. Kuperman et al2007) analyzed
the relationship between linguistic unit predictabil-
ity and syllable duration in read-aloud speech in
Dutch. Dutch makes use of interfix morphemes
-s- and -e(n)- in certain contexts to make com-
pound nouns, preferring a null interfix in most
cases. For example, the Dutch noun kandidaatsex-
amen (?Bachelor?s examination?) is composed of
kandidaat-, -s-, and -examen.
Kuperman et alind that the greater the pre-
dictability of the interfix from the morphological
context (i.e., the surrounding members of the com-
pound), the longer the duration of the pronuncia-
tion of the interfix. To illustrate, if -s- is more ex-
pected after kandidaat or if kandidaatsexamen is a
frequent compound, we would therefore expect the
-s- to be pronounced longer, given the correlations
they found. Their finding runs counter to a strong
view of UID?s fine-grained control over speech rate,
but it is focused on the morphological level. They
hypothesize that this counter-intuitive result may be
driven by complex paradigmatic constraints in the
choice of morpheme.
Our work, however, focuses on the syntactic level
rather than the paradigmatic. What we seek to an-
swer in our work is the extent to which an infor-
mation density-based analysis can not only be ap-
plied to real speech data in context but also be de-
rived from higher-level syntactic analyses, a com-
bination hitherto little explored. Existing broad-
coverage work on syntactic surprisal has largely fo-
cused on comprehension phenomena, such as Dem-
berg and Keller (2008), Roark et al2009), and
Frank (2010). We provide a production study in a
vein similar to that of Kuperman et albut show that
frequency effects work in the expected direction at
the syntactic level. This in turn expands upon the
view supported by n-gram-based work such as that
357
of Piantadosi et al2011); Levy and Jaeger (2007);
Jurafsky et al2001), showing that information con-
tent above the n-gram level is important in guiding
spoken language production in humans.
1.2 Implications for Potential Applications
Spoken dialogue systems are of increasing eco-
nomic and technological importance in recent times,
particularly as it is now feasible to include this tech-
nology in everything from small consumer devices
to industrial equipment. With this increase in impor-
tance, there is also unsurprisingly growing scientific
emphasis in understanding its usability and safety
characteristics. Recent work (Fang et al2009;
Taube-Schiff and Segalowitz, 2005) has shown that
linguistic information presentation has an effect on
user behaviour, but the overall granularity of this be-
haviour is still not well-understood.
Other potential applications exist in any place
where text-to-speech technologies can be applied,
such as in real-time spoken machine translation and
communications systems for the disabled.
In demonstrating that we can observe speakers be-
having in the manner predicted by the UID hypoth-
esis in conversational contexts, we provide evidence
for a finer-level of granularity necessary for control-
ling the rate of information presentation in artificial
systems.
1.3 AMI corpus
The Augmented Multi-Party Interaction (AMI) cor-
pus is a collection of recorded, transcribed con-
versations spanning 100 hours of simulated meet-
ings. The corpus contains a number of data streams
including speech, video, and whiteboard writing.
Transcription of the meetings was performed man-
ually, and the transcripts contain word-level time
bounds that were produced by an automatic speech
recognition system.
The freely-available AMI corpus is one of a very
small number of efforts that contain orthographic
transcriptions that are time-aligned at a word level.
We chose it for the realism of the setting in which
it was recorded; the physical presence of multiple
speakers in an unstructured discussion reflects a po-
tentially high level of noise in which we would be
looking for surprisal correspondences, potentially
increasing the application value of the correspon-
dences we find.
1.4 Organization
The remainder of this paper proceeds as follows. In
section 2, we describe at a high level the procedure
we used to test our hypothesis that parser-derived
surprisal values can partly account for utterance-
duration variation. Then (section 3.2) we discuss the
MARY text-to-speech system, from which we derive
?canonical? word utterance durations. We describe
the way we process and filter the AMI meeting cor-
pus in section 3.1. In section 4, we describe in detail
our predictors, frequency counts, trigram surprisal,
and Roark parser surprisal. Sections 5 and 6 de-
scribe how we use linear mixed effects modeling to
find significant correlations between our predictors
and the response variable, and we finally make some
concluding remarks in section 7.
2 Design
The overall design of our experiment is schemati-
cally depicted in Figure 1. We extract the words
and the word-by-word timings from the AMI corpus,
keeping track of each word?s position in the corpus
by conversation ID, speaker turn, and chronological
order. As we describe in the next section, we filter
the words for anomalies.
After pre-processing, for each word in the cor-
pus, we extract the following predictors: canoni-
cal speech durations from the MARY text-to-speech
system, logarithmic word frequencies, n-gram sur-
prisal, and surprisal values produced by the Roark
(2001a); Roark et al2009) parser (see Section 4).
The next sections describe how and from where
these values are obtained1.
Finally, we run mixed effects regression model
analyses (Baayen et al2008) with the observed
durations as a response variable and the predictors
mentioned above in order to detect whether syntac-
tic surprisal is a significant positive predictor of spo-
ken word durations above and beyond the more ba-
sic effects of canonical word duration and word fre-
quency.
1We will make this data widely available upon publication.
358
AMIcorpus Wordfiltrationandselection
Gigaword CMUtoolkit
AMIwordfreq. AMI n-gramsurprisal
Roarkparser
Roarksyntacticsurprisal
Observations
MARY
ComputedtimingsObservedtimings
Regressionanalysis
Relativesignificance
PennTreebank
Gigawordfreq. PTBn-gramsurprisal Gigawordn-gramsurprisal
Figure 1: Schematic overview of experiment.
3 Experimental materials
3.1 Corpus preparation
The AMI corpus is provided in the NITE XML
Toolkit (NXT) format. We developed a custom inter-
preter to assemble the relevant data streams: words,
meeting IDs, speaker IDs, speaker turns, and ob-
served word durations.
In addition to grouping and re-ordering the infor-
mation found in the original XML corpus, two more
steps were taken to eliminate confounding noise
from the data. Non-words (e.g. ?uhm?, ?uh-hmm?,
etc.) were filtered out, as were incomplete words or
incorrectly transcribed words (e.g. ?recogn?, ?some-
thi?, etc); the criterion for rejection was presence in
the English Gigaword corpus with subsequent mi-
nor corrections by hand, e.g., mapping unseen verbs
back into the corpus and correcting obvious com-
mon misspellings.2
Finally, turns that did not make for complete sen-
tences, e.g., utterances that were interrupted in mid-
2A reviewer asks about the extent to which our Gigaword fil-
tering process may remove words we might want to keep but ad-
mit words we want to reject. As Gigaword is mostly newswire
text, we do not expect the latter case to hold often. AMI is
hand-transcribed and uses consistent spellings for non-word in-
terjections (easy to remove), and any spelling mistakes would
have to coincide exactly with a Gigaword mistake.
The other way around (rejecting what should be allowed) is
easier to check, and we find that of 13K word types in AMI,
about 7.2% are rejected for non-appearance in Gigaword, after
filtering for interjections like ?mm-hmm?. However, we man-
ually checked them and returned all but 2.9% of word types to
the corpus. These tend to be very low-frequency types. The
manual check suggests that ultimately there would be few false
rejections.
359
sentence, were filtered out in order to maximize the
proportion of complete parses in surprisal calcula-
tion.
3.2 Word duration model
In order to investigate whether there is an association
between high/low surprisal and increased/decreased
word duration, one needs to have a baseline mea-
sure of what constitutes the ?canonical? duration of
each word?in other words, to account for the fact
that some words have longer pronunciations than
others. As one reviewer notes, one way of estimat-
ing word durations would be to calculate the aver-
age duration of each word in the corpus. However,
this approach would be insensitive to the phonolog-
ical, syllabic and phrasal context that a word oc-
curs in, which can have a large effect on word du-
ration. Therefore, we use word duration estimates
from the state-of-the-art open-source text-to-speech
system MARY (Schro?der et al2008, version 4.3.1),
with the default voice package included in this ver-
sion (cmu-slt-hsmm).
The cmu-slt-hsmm voice package uses
a Hidden Markov model, trained on the fe-
male US English section of the CMU ARCTIC
database (Kominek and Black, 2003), to predict
prosodic attributes of each individual synthesized
phone, including duration. Training was carried
out using a version of the HTS system (Zen et al
2007), modified for using the MARY context
features (Schro?der et al2008) for estimating the
parameters of the model and for decoding. Those
features include3:
? phonological features of the current and neigh-
boring phonemes
? syllabic and lexical features (e.g. syllable
stress, (estimated) part-of-speech, position of
syllable in word)
? phrasal / sentential features (e.g. sen-
tence/phrase boundaries, neighboring pauses
and punctuation)
For each word in the AMI corpus, we ob-
tained two alternative estimates of word duration:
3For further information about how HMM-based voices for
MARY TTS are trained, see http://mary.opendfki.
de/wiki/HMMVoiceCreation
one version which is independent of a word?s
sentential context, and a second version which
does take into account the sentential context (such
as phrasal/sentential and across-word-boundaries
phonological features) the word occurs in. In other
words, we obtain MARY word duration estimates
in the second version by running individual whole
sentences through MARY, segmented by standard
punctuation marks used in the AMI corpus transcrip-
tions. For each version, we obtained phone dura-
tions using MARY and calculate the total duration of
a word as the sum of the estimated phone durations
for that word. These durations serve as the ?canoni-
cal? baselines to which the observed durations of the
words in the AMI corpus are compared.
3.3 Word frequency baselines
In order to account for the effects of simple word
frequency on utterance duration, we extracted two
types of frequency counts. One was taken di-
rectly from the AMI corpus alone. The other was
taken from a 151 million-word (4.3 million full-
paragraph) sample of the English Gigaword cor-
pus. These came from the following newswire
sources: Agence France Press, Associated Press
Worldstream, New York Times Newswire, and the
Xinhua News Agency English Service. These
sources are organized by month-of-year. We se-
lected the subset of Gigaword by randomly select-
ing month-of-year files from those sources with uni-
form probability. Punctuation was stripped from the
beginnings and ends of words before taking the fre-
quency counts.
4 Surprisal models
For predicting the surprisal of utterances in context,
two different types of models were used? n-gram
probabilities models, as well as Roark?s 2001 incre-
mental top-down parser capable of calculating pre-
fix probabilities. We also estimated word frequen-
cies to account for words being spoken more quickly
due to their higher frequency which is independent
of structural surprisal.
The n-gram probabilities models, while being fast
in both training and application, inherently capture
very limited contextual influences on surprisal. The
full-fledged parser, on the other hand, quantifies sur-
360
prisal based in the prefix probability of the complete
sentence prefix and captures long-distance effects
by conditioning on c-commanding lexical items as
well as non-local node labels such as parents, grand-
parents and siblings from the left context.
CMU n-grams We used the CMU Statistical Nat-
ural Language Modeling Toolkit to provide a con-
venient way to calculate n-grams probabilities. For
the prediction of surprisal, we calculated 3-gram
models, 4-gram models and 5-gram models with
Witten-Bell smoothing. Different n-gram models
were trained on the full Gigaword corpus, as well
as the AMI corpus.
To avoid overfitting, the AMI text corpus was split
into 10 sub-corpora of equal word counts, preserv-
ing coherence of meetings. N-gram probabilities
were then calculated for each of the sub-corpora us-
ing models trained on the 9 others.
We also produced a trigram model using the text
of chapter 2?21 of the Penn Treebank?s (PTB) un-
derlying Wall Street Journal corpus. This consists
of approximately one million tokens. We generated
this model because it is the underlying training data
for the Roark parser, described below.
Syntactic Surprisal from Roark parser In order
to capture the effect of syntactically expected vs. un-
expected events, we can calculate the syntactic sur-
prisal of each word in a sentence. The syntactic sur-
prisal at word Swi is defined as the difference be-
tween the prefix probability at word wi and the pre-
fix probability at word wi?1. The prefix probability
at word wi is the sum of the probabilities of all trees
T spanning words w1 . . . wi; see also (Levy, 2008;
Demberg and Keller, 2008).
Swi = log
?
T
P (T,w1..wi?1)? log
?
T
P (T,w1..wi)
The top-down incremental Roark parser (Roark,
2001a) has the characteristic that all partial left-to-
right parses are rooted: they form a single tree with
one root. A set of heuristics ensures that rule appli-
cation occurs only through node expansion within
the connected structure.4 The grammar-derived pre-
fix probabilities of a given sentence prefix can there-
4The formulae for the calculation of the prefix probabilities
from the PCFG rules can be found in Roark et al2009).
S
NP
DT
A
3.989
NN
puppy
4.570
VP
AUX
is
3.089
S
VP
TO
to
3.873
PP
TO
to
3.873
NP
DT
a
5.973
Figure 2: Top-ranked partial parse of A puppy is to a dog
what a kitten is to a cat., stopping at the second a and
providing the Roark parser surprisal values by word. The
branch with dashed lines and struck-out symbols repre-
sents an analysis abandoned at the appearance of the a.
fore be calculated directly by multiplying the prob-
abilities of all rules used to generate the prefix tree.
The Roark parser shares this characteristic of gener-
ating fully connected structures with Earley parsers
(Earley, 1970) and left corner parsers (Rosenkrantz
and II, 1970).
The Roark parser uses a beam search. As the
amount of probability mass lost has been shown
to be small (Roark, 2001b), the surprisal estimates
can be assumed to be a good approximation. The
beam width of the parser search is controlled by a
?base parsing threshold?, which defines the distance
in terms of natural log-probability between the most
probable parse and the least probable parse within
the beam. For the experiments reported here, the
parsing beam was set to 21 (default setting is 12). A
wider beam also reduces the effects of pruning.
The parser was trained on Wall Street Journal sec-
tions 2?21 and applied to parse the full sentences
of the AMI corpus, collecting predicted surprisal at
each word (see Figure 2 for an example).
The syntactic surprisal can be furthermore be de-
composed into a structural and a lexical part: some-
times, high surprisal might be due to a word be-
ing incompatible with the high-probability syntactic
structures, other times high surprisal might just be
due to a lexical item being unexpected. It is inter-
361
esting to evaluate these two aspects of syntactic sur-
prisal separately, and the Roark parser conveniently
outputs both surprisal estimates. Structural surprisal
is estimated from the occurrence counts of the appli-
cation of syntactic rules during the parse discount-
ing the effect of lexical probabilities, while lexical
surprisal is calculated from the probabilities of the
derivational step from the POS-tag to lexical item.
5 Linear mixed effects modelling
In order to test whether surprisal estimates correlate
with speech durations, we use linear mixed effects
models (LME, Pinheiro and Bates (2000)). This
type of model can be thought of as a generalization
of linear regression that allows the inclusion of ran-
dom factors as well as fixed factors.We treat speak-
ers as a random factor, which means that our mod-
els contain an intercept term for each speaker, rep-
resenting the individual differences in speech rates.
Furthermore, we include a random slope for the
predictors (e.g. frequency, canonical duration, sur-
prisal), essentially accounting for idiosyncrasies of
a participant with respect to the predictor, such that
only the part of the variance that is common to all
participants and is attributed to that predictor.
In a first step, we fit a baseline model with all pre-
dictors related to a word?s canonical duration and its
frequency as well as their random slopes to the ob-
served word durations. Models with more than two
random slopes generally did not converge. We there-
fore included in the baseline model only the two best
random slopes (in terms of model fit). We then cal-
culated the residuals of that model, the part of the
observed word durations that cannot be accounted
for through canonical word durations or word fre-
quency.
For each of our predictors of interest (n-gram sur-
prisal, syntactic surprisal), we then fit another lin-
ear mixed-effects model with random slopes to the
residuals of the baseline model. This two-step pro-
cedure allows us to make sure to avoid problems
of collinearity between e.g. surprisal and word fre-
quency or canonical duration. A simpler (but less
conservative) method is to directly add the predic-
tors of interest to the baseline model. Results for
both modelling variants lead to the same conclusions
for our model, so we here report the more conserva-
tive two-step model. We compare models based on
the Akaike Information Criterion (AIC).
6 Results
Our baseline model uses speech durations from the
AMI corpus as the response variable and canoni-
cal duration estimates from the MARY TTS system
and log word frequencies as predictors. We exclude
from the analysis all data points with zero duration
(effectively, punctuation) or a real duration longer
than 2 seconds. Furthermore, we exclude all words
which were never seen in Gigaword and any words
for which syntactic surprisal couldn?t be estimated.
This leaves us with 771,234 out of the 799,997 data
points with positive duration.
MARY duration models As mentioned in the
earlier sections, we have calculated different ver-
sions of the MARY estimated word durations: one
model without the sentential context and one model
with the sentential context. In our regression analy-
ses, we find, as expected, that the model which in-
cludes sentential context achieves a much better fit
with the actually measured word durations from the
AMI corpus (AIC = 32167) than the model without
context (AIC = 70917).
Word frequency estimates We estimated word
frequencies from several different resources, from
the AMI corpus to have a spoken domain frequency
and from Gigaword as a very large resource. We
find that both frequency estimates significantly im-
prove model fit over a model that does not contain
frequency estimates. Including both frequency esti-
mates improves model fit with respect to a model
that includes just one of the predictors (all p <
0.0001).
Furthermore, including into the regression an in-
teraction of estimated word duration and word fre-
quency also significantly increases model fit (p <
0.0001). This means that words which are short and
frequent have longer duration than would be esti-
mated by adding up their length and frequency ef-
fects.
Baseline model Fixed effects of the fitted model
are shown in Table 2. We see a highly significant ef-
fect in the expected direction for both the canonical
duration estimate and word frequency. The positive
362
coefficient for MARY CONTEXT means that TTS
duration estimates are positively correlated with the
measured word durations. The negative coefficient
for WORDFREQUENCY means that more frequent
words are spoken faster than less frequent words.
Finally, the negative coefficient for the interaction
between word durations and frequencies means that
the duration estimate for short frequent and long in-
frequent words is less extreme than otherwise pre-
dicted by the main effects of duration and frequency.
Ami Mary Mary Giga PTB AMI AMI Giga
Dur Word Cntxt Freq Freq Freq 3grm 4grm
Mary Word .36 1
Mary Cntxt .42 .72 1
GigaFreq -.35 -.52 -.65 1
PTBFreq -.33 -.48 -.62 .98 1
AMIFreq -.33 -.61 -.57 .65 .62 1
AMI3gram .21 .40 .41 -.41 -.39 -.68 1
Giga4gram .24 .33 .44 -.59 -.59 -.44 .61 1
Srprsl .29 .40 .48 -.71 -.73 -.50 .50 .73
Table 1: Correlations (pearson) of model predictors.
Note though that the predictors are also correlated
(for correlations of the main predictors used in these
analyses, see Table 1), so there is some collinearity
in the below model. Since we are less interested in
the exact coefficients and significance sizes for these
baseline predictors, this does not have to bother us
too much. What is more important, is that we re-
move any collinearity between the baseline predic-
tors and our predictors of interest, i.e. the surprisal
estimates from the ngram models and parser. There-
fore, we run separate regression models for these
predictors on the residuals of the baseline model.
N-gram estimates We estimated 3-gram, 4-gram
and 5-gram models on the AMI corpus (9-fold-
Predictor Coef t-value Sig
INTERCEPT 0.3098 212.11 ***
MARY CONTEXT 0.4987 95.48 ***
AMIWORDFREQUENCY -0.0282 -32.28 ***
GIGAWORDFREQUENCY -0.0275 -62.44 ***
MARY CNTXT:GIGAFREQ -0.0922 -45.41 ***
Table 2: Baseline linear mixed effects model of speech
durations on the AMI corpus data for MARY CONTEXT
(including the sentential context), WORDFREQUENCY
under speaker with random intercept for speaker and ran-
dom slopes under speaker. Predictors are centered.
Predictor Coef t-value Sig
INTERCEPT 0.3099 212.94 ***
MARY CONTEXT 0.4970 94.60 ***
AMIWORDFREQUENCY -0.0279 -31.98 ***
GIGAWORDFREQUENCY -0.0254 -53.68 ***
GIGA4GRAMSURPRISAL 0.0027 11.81 ***
MARY CNTXT:GIGAFREQ -0.0912 -44.87 ***
Table 3: Linear mixed effects model of speech durations
including 4-gram surprisal trained on gigaword as a pre-
dictor.
cross), the Penn Treebank and the Gigaword Cor-
pus. We found that coefficient estimates and signif-
icance levels of the resulting models were compara-
ble. This is not surprising, given that 4-gram and 5-
gram models were backing of to 3-grams or smaller
contexts for more than 95% of cases on the AMI and
PTB corpora (both ca. 1m words), and thus were
correlated at p > .98. On the Gigaword Corpus,
the larger contexts were seen more often (5-grams:
11%, 4-grams: 36%), but still correlation with 3-
grams were high at (p > .96).
N-gram model surprisal estimated on newspaper
texts from PTB or Gigaword were statistically sig-
nificant positive predictors of spoken word durations
beyond simple word frequencies (but PTB ngram
surprisal did not improve fit over models containing
Gigaword frequency estimates). Counter-intuitively
however, ngram models estimated based on the AMI
corpus have a small negative coefficient in models
that already include word frequency as a predictor
? residuals of an AMI-estimated ngram model with
respect to word frequency are very noisy and do not
show a clear correlation anymore with word dura-
tions.
Surprisal Surprisal effects were found to have a
robust significant positive coefficient, meaning that
words with higher surprisal are spoken more slowly /
clearly than expected when taking into account only
canonical word duration and word frequency. Sur-
prisal achieves a better model fit than any of the
n-gram models, based on a comparsion of AICs,
and Surprisal significantly improved model fit over
a model including frequencies and ngram models
based on AMI and Gigaword. Table 4 shows the es-
timate for SURPRISAL on the residuals of the model
in Table 2.
363
Predictor Coef t-value Sig
INTERCEPT -0.0154 -23.45 ***
SURPRISAL 0.0024 26.09 ***
Table 4: Linear mixed effects model of surprisal (based
on Roark parser) with random intercept for speaker and
random slope. The response variable is residual word du-
rations from the model shown in Table 3.
Surprisal estimated from the Roark parser also
remains a significant positive predictor when re-
gressed against the residuals of a baseline model in-
cluding both 3-gram surprisal from the AMI corpus
and 4-gram surprisal from the Gigaword corpus. In
order to make really sure that the observed surprisal
effect has indeed to do with syntax and can not be
explained away as a frequency effect, we also cal-
culated frequency estimates for the corpus based on
the Penn Treebank. The significant positive surprisal
effect remains stable, also when run on the residuals
of a model which includes PTB trigrams and PTB
frequencies.
It is difficult from these regression models to in-
tuitively grasp the size of the effect of a particular
predictor on reading times, since one would have to
know the exact range and distribution of each pre-
dictor. To provide some intuition, we calculate the
estimated effect size of Roark surprisal on speech
durations. Per Roark surprisal ?unit?, the model es-
timates a 7 msec difference5. The range of Roark
surprisal in our data set is roughly from 0 to 25,
with most values between 2 and 15. For a word
like ?thing? which in one instance in the AMI cor-
pus was estimated with a surprisal of 2.179 and in
another instance as 16.277, the estimated difference
in duration between these instances would thus be
104msec, which is certainly an audible difference.
(Full range for Roark surprisal: 174msec, whereas
full range for gigaword 4gram surprisal is 35 msec.)
When analysing the surprisal effect in more detail,
we find that both the syntactic component of sur-
prisal and its lexical component are significant pos-
itive predictors of word durations, as well as the in-
teraction between them, which has a negative slope.
A model with the separate components and their in-
52.4msec for a unit of residualized Roark surprisal, but it
is even less intuitive what that means, hence we calculate with
non-residualized surprisal here.
Predictor Coef t-value Sig
INTERCEPT -0.0219 -18.77 ***
STRUCTSURPRISAL 0.0009 2.71 **
LEXICALSURPRISAL 0.0044 24.00 ***
STRUCT:LEXICAL -0.0004 - 6.83 ***
Table 5: Linear mixed effects model of residual speech
durations wrt. baseline model from Table 3, with random
intercept for speaker and random slope for structural and
lexical component of surprisal, estimated using the Roark
parser.
teraction achieves a better model fit (in AIC and BIC
scores) than a model with only the full surprisal ef-
fect. The detailed model is shown in Table 5.
To summarize, the positive coefficient of surprisal
means that words which carry a lot of information
from a structural point of view are spoken more
slowly than words that carry less such information.
These results thus provide good evidence for our
hypothesis that the predictability of syntactic struc-
ture affects phonetic realization and that speakers
use speech rate to achieve more uniform information
density.
Native vs. non-native speakers Finally, we also
compared effects in our native vs. non-native
speaker populations, see Table 6. Both populations
show the same effects and tell the same story (note
that significance values can?t be compared as the
sample sizes are different). It might be possible to
interpret the findings in the sense that native speak-
ers are more proficient at adapting their speech rate
to (syntactic) complexity to achieve more uniform
information density, given the slightly higher coeffi-
cient and significance for Surprisal for native speak-
ers. Since the effects are statistically significant
for both groups, we don?t want to make too strong
claims about differences between the groups.
7 Conclusions and future work
We have shown evidence in this work that syntac-
tic surprisal effects in transcribed speech data can
be detected through word utterance duration in both
native and non-native speech, and we did so using
a meeting corpus not specifically designed to iso-
late these effects. This result is the potential foun-
dation for futher work in applied, experimental, and
364
Native English Non-native
Predictor Coef t-value Sig Coef t-value Sig
INTERCEPT 0.2947 149.74 *** 0.3221 175.38 ***
MARY CONTEXT 0.5304 69.27 *** 0.4699 67.77 ***
AMIWORDFREQUENCY -0.0226 -18.10 *** -0.0321 -28.00 ***
GIGAWORDFREQUENCY -0.0264 -41.19 *** -0.0248 -39.58 ***
GIGAWORD4-GRAMS 0.0018 5.36 *** 0.0033 10.85 ***
MARY CONTEXT:GIGAFREQ -0.0810 -27.20 *** -0.0993 -35.71 ***
SURPRISAL 0.0033 24.21 *** 0.0018 15.09 ***
no of data points 320,592 391,106
*p < 0.05, **p < 0.01, ***p < 0.001
Table 6: Native speakers are possibly slightly better at adapting their speech rate to syntactic surprisal than non-native
speakers. Surprisal value is for model with residuals of other predictors as dependent variable.
theoretical psycholinguistics. It provides additional
direct support for approaches based on the UID hy-
pothesis.
From an applied perspective, the fact that fre-
quency and syntactic surprisal have a significant ef-
fect beyond what a HMM-trained TTS model would
predict for individual words is a case for further
research into incorporating syntactic models into
speech production systems. Our methodology im-
mediately provides a framework for estimating the
word-by-word effect on duration for increased nat-
uralness in TTS output. This is relevant to spo-
ken dialogue systems because it appears that syn-
thesized speech requires a greater level of attention
from the dialogue system users when compared to
the same words delivered in natural speech (Delogu
et al1998). Some of this effect may be attributable
to peaks in information density which are caused by
current generation systems not compensating for ar-
eas of high information density through speech rate,
lexical and structural choice.
Furthermore, syntax and semantics have been ob-
served to interact with the mode of speech deliv-
ery. Eye-tracking experiments by Swift et al2002)
showed that there was a synthetic vs. natural speech
difference in the time required to pay attention to
an object referred to using definite articles, but not
indefinite articles. Our result points a way towards
a direction for explaining of this phenomenon by
demonstrating that the differences between current-
technology artificial speech and natural speech can
be partially explained through higher-level syntactic
features.
However, further experimentation is required on
other measures of syntactic complexity (e.g. DLT,
Gibson (2000)) as well as other levels of representa-
tion such as the semantic level. From a theoretical
and neuroanatomical perspective, the finding that a
measure of syntactic ambiguity reduction has an ef-
fect on the phonological layer of production has ad-
ditional implications for the organization of the hu-
man language production system.
References
Aylett, M. and Turk, A. (2006). Language redun-
dancy predicts syllabic duration and the spec-
tral characteristics of vocalic syllable nuclei.
Journal of the acoustical society of America,
119(5):3048?3059.
Baayen, R., Davidson, D., and Bates, D. (2008).
Mixed-effects modeling with crossed random ef-
fects for subjects and items. Journal of memory
and language, 59(4):390?412.
Delogu, C., Conte, S., and Sementina, C. (1998).
Cognitive factors in the evaluation of synthetic
speech. Speech Communication, 24(2):153?168.
Demberg, V. and Keller, F. (2008). Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
109:193?210.
Earley, J. (1970). An efficient context-free parsing
algorithm. Commun. ACM, 13(2):94?102.
Fang, R., Chai, J. Y., and Ferreira, F. (2009). Be-
365
tween linguistic attention and gaze fixations in-
multimodal conversational interfaces. In Inter-
national Conference on Multimodal Interfaces,
pages 143?150.
Florian Jaeger, T. (2010). Redundancy and reduc-
tion: Speakers manage syntactic information den-
sity. Cognitive Psychology, 61(1):23?62.
Frank, A. and Jaeger, T. F. (2008). Speaking ra-
tionally: uniform information density as an opti-
mal strategy for language production. In The 30th
annual meeting of the Cognitive Science Society,
pages 939?944.
Frank, S. (2010). Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceed-
ings of the 2010 Workshop on Cognitive Model-
ing and Computational Linguistics, pages 81?89,
Uppsala, Sweden.
Gibson, E. (2000). Dependency locality theory: A
distance-dased theory of linguistic complexity. In
Marantz, A., Miyashita, Y., and O?Neil, W., ed-
itors, Image, Language, Brain: Papers from the
First Mind Articulation Project Symposium, pages
95?126. MIT Press, Cambridge, MA.
Hale, J. (2001). A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter
of the Association for Computational Linguistics,
volume 2, pages 159?166, Pittsburgh, PA.
Jurafsky, D., Bell, A., Gregory, M., and Raymond,
W. (2001). Evidence from reduction in lexical
production. Frequency and the emergence of lin-
guistic structure, 45:229.
Kominek, J. and Black, A. (2003). The cmu
arctic speech databases for speech synthesis
research. Language Technologies Institute,
Carnegie Mellon University, Pittsburgh, PA, Tech.
Rep. CMULTI-03-177 http://festvox. org/cmu arc-
tic.
Kuperman, V., Pluymaekers, M., Ernestus, M., and
Baayen, H. (2007). Morphological predictability
and acoustic duration of interfixes in dutch com-
pounds. The Journal of the Acoustical Society of
America, 121(4):2261?2271.
Levy, R. (2008). Expectation-based syntactic com-
prehension. Cognition, 106(3):1126?1177.
Levy, R. and Jaeger, T. F. (2007). Speakers opti-
mize information density through syntactic reduc-
tion. In Advances in Neural Information Process-
ing Systems.
Piantadosi, S., Tily, H., and Gibson, E. (2011). Word
lengths are optimized for efficient communica-
tion. Proceedings of the National Academy of Sci-
ences, 108(9).
Pinheiro, J. C. and Bates, D. M. (2000). Mixed-
effects models in S and S-PLUS. Statistics and
computing series. Springer-Verlag.
Roark, B. (2001a). Probabilistic top-down parsing
and language modeling. Computational linguis-
tics, 27(2):249?276.
Roark, B. (2001b). Robust probabilistic predictive
syntactic processing: motivations, models, and
applications. PhD thesis, Brown University.
Roark, B., Bachrach, A., Cardenas, C., and Pal-
lier, C. (2009). Deriving lexical and syntactic
expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
324?333, Singapore. Association for Computa-
tional Linguistics.
Rosenkrantz, D. J. and II, P. M. L. (1970). Deter-
ministic left corner parsing (extended abstract). In
SWAT (FOCS), pages 139?152.
Schro?der, M., Charfuelan, M., Pammi, S., and Tu?rk,
O. (2008). The MARY TTS entry in the Bliz-
zard Challenge 2008. In Proc. Blizzard Chal-
lenge. Citeseer.
Swift, M. D., Campana, E., Allen, J. F., and Tanen-
haus, M. K. (2002). Monitoring eye movements
as an evaluation of synthesized speech. In Pro-
ceedings of the IEEE 2002 Workshop on Speech
Synthesis.
Taube-Schiff, M. and Segalowitz, N. (2005). Lin-
guistic attention control: attention shifting gov-
erned by grammaticized elements of language.
Journal of experimental psychology Learning
memory and cognition, 31(3):508?519.
Zen, H., Nose, T., Yamagishi, J., Sako, S., Masuko,
T., Black, A., and Tokuda, K. (2007). The HMM-
based speech synthesis system (HTS) version 2.0.
366
In Proc. of Sixth ISCA Workshop on Speech Syn-
thesis, pages 294?299.
367
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 345?348,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Crowdsourcing the evaluation of a domain-adapted named entity
recognition system
Asad B. Sayeed, Timothy J. Meyer,
Hieu C. Nguyen, Olivia Buzek
Department of Computer Science
University of Maryland
College Park, MD 20742
asayeed@cs.umd.edu,
tmeyer1@umd.edu,
{hcnguyen88,olivia.buzek}
@gmail.com
Amy Weinberg
Department of Linguistics
University of Maryland
College Park, MD 20742
weinberg@umiacs.umd.edu
Abstract
Named entity recognition systems sometimes
have difficulty when applied to data from do-
mains that do not closely match the training
data. We first use a simple rule-based tech-
nique for domain adaptation. Data for robust
validation of the technique is then generated,
and we use crowdsourcing techniques to show
that this strategy produces reliable results even
on data not seen by the rule designers. We
show that it is possible to extract large im-
provements on the target data rapidly at low
cost using these techniques.
1 Introduction
1.1 Named entities and errors
In this work, we use crowdsourcing to generate eval-
uation data to validate simple techniques designed to
adapt a widely-used high-performing named entity
recognition system to new domains. Specifically, we
achieve a roughly 10% improvement in precision on
text from the information technology (IT) business
press via post hoc rule-based error reduction. We
first tested the system on a small set of data that we
annotated ourselves. Then we collected data from
Amazon Mechanical Turk in order to demonstrate
that the gain is stable. To our knowledge, there is no
previous work on crowdsourcing as a rapid means
of evaluating error mitigation in named entity rec-
ognizer development.
Named entity recognition (NER) is a well-known
problem in NLP which feeds into many other re-
lated tasks such as information retrieval (IR) and
machine translation (MT) and more recently social
network discovery and opinion mining. Generally,
errors in the underlying NER technology correlate
with a steep price in performance in the NLP sys-
tems further along a processing pipeline, as incor-
rect entities propagate into incorrect translations or
erroneous graphs of social networks.
Not all errors carry the same price. In some ap-
plications, omitting a named entity has the conse-
quence of reducing the availability of training data,
but including an incorrectly identified piece of text
as as a named entity has the consequence of pro-
ducing misleading results. Our application would
be opinion mining; an omitted entity may prevent
the system from attributing an opinion to a source,
but an incorrect entity reveals non-existent opinion
sources.
Machine learning is currently used extensively in
building NER systems. One such system is BBN?s
Identifinder (Bikel et al, 1999). The IdentiFinder al-
gorithm, based on Hidden Markov Models, has been
shown to achieve F-measure scores above 90% when
the training and testing data happen to be derived
from Wall Street Journal text produced in the 1990s.
We use IdentiFinder 3.3 as a starting point for per-
formance improvement in this paper.
The use of machine learning in existing systems
requires us to produce new and costly training data
if we want to adapt these systems directly to other
domains. Our post hoc error reduction strategy is
therefore profoundly different: it relieves us of the
burden of generating complete training examples.
The data we generate are strictly corrections of the
existing system?s output. Our thus cheaper evalua-
tion is therefore primarily on improvements to pre-
345
cision, while minimizing damage to recall, unlike
an evaluation based on retraining with new, fully-
annotated text.
1.2 Crowdsourcing
Crowdsourcing is the use of the mass collabora-
tion of Internet passers-by for large enterprises on
the World Wide Web such as Wikipedia and survey
companies. However, a generalized way to mon-
etize the many small tasks that make up a larger
task is relatively new. Crowdsourcing platforms
like Amazon Mechanical Turk have allowed some
NLP researchers to acquire data for small amounts
of money from large, unspecified groups of Internet
users (Snow et al, 2008; Callison-Burch, 2009).
The use of crowdsourcing for an NLP annotation
task required careful definition of the specifics of
the task. The individuals who perform these tasks
have no specific training, and they are trying to get
through as many tasks as they can, so each task must
be specified very simply and clearly.
Part of our work was to define a named entity
error detection task simply enough that the results
would be consistent across anonymous annotators.
2 Methodology
2.1 Process overview
The overall process for running this experiment was
as follows (figure 1).
Figure 1: Diagram of data pipeline.
First, we performed an initial performance assess-
ment of IdentiFinder on our domain. We selected
200 articles from an IT trade journal. IdentiFinder
was used to tag persons and organizations in these
documents. Domain experts (in this case, the au-
thors of this paper) analyzed the entity tags pro-
duced by the NER system and annotated the erro-
neous tags. We built an error reduction system based
on our error analysis. We then ran the IdentiFinder
output through the error reduction system and eval-
uated its performance against our annotations.
Next, we constructed an Amazon Mechanical
Turk-based interface for na??ve web users or ?Turk-
ers? to annotate the IdentiFinder entities for errors.
We measured the interannotator agreement between
the Turkers and the domain experts, and we evalu-
ated the IdentiFinder output and the repaired output
against the expert-generated and Turker gold stan-
dards.
We selected a new batch of 800 articles and ran
IdentiFinder and the filters on them, and we again
ran our Mechanical Turk application on the Iden-
tiFinder output. We measured the performance of
IdentiFinder and filtered output against the Turker
annotations.
2.2 Performance evaluation
Performance is evaluated in terms of standard pre-
cision and recall of entities. If the system output
contains a person or organization labelled correctly
as such, it considers this to be a hit. If it contains a
person or organization that is mislabelled or other-
wise incorrect in the gold standard annotation, it is
a miss. We compute the F-measure as the harmonic
mean of precision and recall.
As the IdentiFinder output is the baseline, and we
ignore missed entities, by definition the baseline re-
call is 100%.
3 Experiments and results
Here we delve into further detail about the tech-
niques we used and the results that they yielded. The
results are summarized in table 1.
3.1 Baseline performance assessment
We randomly selected 200 documents from Infor-
mationWeek, a major weekly magazine in the IT
business press. Running them through IdentiFinder
produces NIST ACE-standard XML entity markup.
We focused on the ENAMEX tags of person and or-
ganization type that IdentiFinder produces.
After we annotated the ENAMEX tags for errors,
we found that closer inspection of the errors in the
IdentiFinder output allowed us to classify the major-
ity of them into three major categories:
346
Annotator Collection System Precision Recall F-measure
Authors 200 document IdentiFinder only 0.74 1 0.85
Authors 200 document Filtered 0.86 0.98 0.92
MTurk 200 document IdentiFinder only 0.69 1 0.82
MTurk 200 document Filtered 0.79 0.97 0.87
MTurk 800 document IdentiFinder only 0.67 1 0.80
MTurk 800 document Filtered 0.77 0.95 0.85
Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.
? IdentiFinder tags words that are simply not
named entities.
? IdentiFinder assigns the wrong category (per-
son or organization) to an entity.
? IdentiFinder includes extraneous words in an
otherwise correct entity.
The second and third types of error are particu-
larly challenging. An example of the second type is
the following:
Yahoo is a reasonably strong competitor
to Google. It gets about half as much on-
line revenue and search traffic as Google,
. . .
Google is marked twice incorrectly as being a person
rather than an organization.
Finally, here is an example of the third error type:
A San Diego bartender reported that Bill
Gates danced the night away in his bar on
Nov. 11.
IdentiFinder incorrectly marks ?danced? as part of a
person tag.
We were able to find the precision of IdentiFinder
against our annotations: 0.74. This is poorer than the
reported performance of IdentiFinder on Wall Street
Journal text (Bikel et al, 1999).
3.2 Domain-specific error reduction
We wrote a series of rule-based filters to remove
instances of the error types?of which there were
many subtypes?described in the previous sec-
tion. For instance, the third example above was
eliminated via the use of a part-of-speech tagger;
?danced? was labelled as a verb, and entities with
tagged verbs were removed. In the second case,
the mislabelling of Google as a person rather than
an organization is identified by looking at Identi-
Finder?s majority labelling of Google throughout the
corpus?as an organization. Simple rules about cap-
italization allow instances like the first example to
be identified as errors.
This step increases the precision of the system
output to 86%, while only sacrificing a tiny amount
of recall. We see that this 10% increase is main-
tained even on the Mechanical Turk-generated an-
notations.
3.3 Mechanical Turk tasks
The basic unit of Mechanical Turk is the Human In-
telligence Task (HIT). Turkers select HITs presented
as web pages and perform the described task. Data-
collectors create HITs and pay Amazon to disburse
small amounts of money to Turkers who complete
them.
We designed our Mechanical Turk process so that
every HIT we create corresponds to an IdentiFinder-
marked document. Within its corresponding HIT,
each document is broken up into paragraphs. Fol-
lowing every paragraph is a table whose rows con-
sist of every person/organization ENAMEX discov-
ered by IdentiFinder and whose columns consist of
one of the four categories: ?Person,? ?Organization,?
?Neither,? and ?Don?t Know.? Then for each entity,
the user selects exactly one of the four options.
Each HIT is assigned to three different Turkers.
Every entity in that HIT is assigned a person or or-
ganization ENAMEX tag if two of the three Turkers
agreed it was one of those (majority vote); other-
wise, it is marked as an invalid entity.
We calculated the agreement between our annota-
tions and those developed from the Turker majority
347
vote scheme. This yields a Cohen?s ? of 0.68. We
considered this to be substantial agreement.
After processing the same 200 document set from
our own annotation, we found that the precision
of IdentiFinder was 69%, but after error reduction,
it increased to 79% with only a miniscule loss of
known valid entities (recall).
We then took another 800 documents from Infor-
mationWeek and ran them through IdentiFinder. We
did not annotate these documents ourselves, but in-
stead turned them over to Turkers. IdentiFinder out-
put alone has a 67% precision, but after error reduc-
tion, it rises to 77%, and recall is still minimally af-
fected.
4 Discussion
4.1 Benefits
It appears that high-performing NER systems ex-
hibit rather severe domain adaption problems. The
performance of IdentiFinder is quite low on the IT
business press. However, a simple rule-based sys-
tem was able to gain 10% improvement in precision
with little recall sacrificed. This is a particularly im-
portant improvement in applications with low toler-
ance for erroneous entities.
However, rule-based systems built by experts are
known to be vulnerable to new data unseen by the
experts. In order to apply this domain-specific error
reduction reliably, it has to be tested on data gathered
elsewhere. We used crowdsourced data to show that
the rule-based system was robust when confronted
with data that the designers did not see.
One danger in crowdsourcing is a potential lack
of commitment on the part of the annotators, as they
attempt to get through tasks as quickly as possible.
It turns out that in an NER context, we can design a
crowdsourced task that yields relatively reliable re-
sults across data sets by ensuring that for every data
point, there were multiple annotators making only
simple decisions about entity classification.
This method also provides us with a source of eas-
ily acquired supervised training data for testing more
advanced techniques, if required.
4.2 Costs
It took not more than an estimated two person weeks
to complete this work. This includes doing the
expert annotations, designing the Mechanical Turk
tasks, and building the domain-specific error reduc-
tion rules.
For each HIT, each annotator was paid 0.05 USD.
For three annotators for 1000 documents, that is
150.00 USD (plus additional small Amazon sur-
charges and any taxes that apply).
5 Conclusions and Future Work
This work was done on a single publication in a sin-
gle domain. One future experiment would be to see
whether these results are reliable across other pub-
lications in the domain. Another set of experiments
would be to determine the optimum number of an-
notators; we assumed three, but cross-domain results
may be more stable with more annotators.
Retraining an NER system for a particular domain
can be expensive if new annotations must be gen-
erated from scratch. While there is work on using
advanced machine learning techniques for domain
transfer (Guo et al, 2009), simply repairing the the
errors post hoc via a rule-based system can have a
low cost for high gains. This work shows a case
where the results are reliable and the verification
simple, in a context where reducing false positives
is a high priority.
Acknowledgements
This paper is based upon work supported by the Na-
tional Science Foundation under Grant IIS-0729459.
This research was also supported in part by NSF
award IIS-0838801.
References
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP 2009, Singapore, August.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL 2009, Morristown, NJ, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008, Morristown, NJ, USA.
348
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 667?676,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Grammatical structures for word-level sentiment detection
Asad B. Sayeed
MMCI Cluster of Excellence
Saarland University
66123 Saarbru?cken, Germany
asayeed@coli.uni-sb.de
Jordan Boyd-Graber,
Bryan Rusk, Amy Weinberg
{iSchool / UMIACS, Dept. of CS, CASL}
University of Maryland
College Park, MD 20742 USA
{jbg@umiacs,brusk@,
aweinberg@casl}.umd.edu
Abstract
Existing work in fine-grained sentiment anal-
ysis focuses on sentences and phrases but ig-
nores the contribution of individual words and
their grammatical connections. This is because
of a lack of both (1) annotated data at the word
level and (2) algorithms that can leverage syn-
tactic information in a principled way. We ad-
dress the first need by annotating articles from
the information technology business press via
crowdsourcing to provide training and testing
data. To address the second need, we propose
a suffix-tree data structure to represent syntac-
tic relationships between opinion targets and
words in a sentence that are opinion-bearing.
We show that a factor graph derived from this
data structure acquires these relationships with
a small number of word-level features. We
demonstrate that our supervised model per-
forms better than baselines that ignore syntac-
tic features and constraints.
1 Introduction
The terms ?sentiment analysis? and ?opinion mining?
cover a wide body of research on and development of
systems that can automatically infer emotional states
from text (after Pang and Lee (2008) we use the two
names interchangeably). Sentiment analysis plays a
large role in business, politics, and is itself a vibrant
research area (Bollen et al, 2010).
Effective sentiment analysis for texts such as
newswire depends on the ability to extract who
(source) is saying what (target). Fine-grained sen-
timent analysis requires identifying the sources and
targets directly relevant to sentiment bearing expres-
sions (Ruppenhofer et al, 2008). For example, con-
sider the following sentence from a major informa-
tion technology (IT) business journal:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtualiza-
tion also opens up a slew of potential net-
work access control issues.
There are three entities in the sentence that have the
capacity to express an opinion: Lloyd Hession, BT
Radianz, and New York. These are potential opinion
sources. There are also a number of mentioned con-
cepts that could serve as the topic of an opinion in
the sentence, or target. These include all the sources,
but also ?virtualization?, ?network access control?,
?network?, and so on.
The challenging task is to discriminate between
these mentions and choose the ones that are rele-
vant to the user. Furthermore, such a system must
also indicate the content of the opinion itself. This
means that we are actually searching for all triples
{source, target, opinion} in this sentence (Kim and
Hovy, 2006) and throughout each document in the
corpus. In this case, we want to identify that Lloyd
Hession is the source of an opinion, ?slew of network
issues,? about a target, virtualization. Providing such
fine-grained annotations would enrich information
extraction, question answering, and corpus explo-
ration applications by letting users see who is saying
what with what opinion (Wilson et al, 2005; Stoy-
anov and Cardie, 2006).
We motivate the need for a grammatically-focused
approach to fine-grained opinion mining and situate it
667
within the context of existing work in Section 2. We
propose a supervised technique for learning opinion-
target relations from dependency graphs in a way that
preserves syntactic coherence and semantic compo-
sitionality. In addition to being theoretically sound
? a lacuna identified in many sentiment systems1
? such approaches improve downstream sentiment
tasks (Moilanen and Pulman, 2007).
There are multiple types of downstream tasks that
potentially require the retrieval of {source, target,
opinion} relations on a sentence-by-sentence basis.
An increasingly significant application area is in the
use of large corpora in social science. This area of
research requires the exploration and aggregation of
data about the relationships between discourses, orga-
nizations, and people. For example, the IT business
press data that we use in this work belongs to a larger
research program (Tsui et al, 2009; Sayeed et al,
2010) of exploring industry opinion leadership. IT
business press text is one type of text in which many
entities and opinions can appear intermingled with
one another in a small amount of text.
Another application for fine-grained sentiment re-
lation retrieval of this type is paraphrasing, where
attribution of which opinion belongs to which entities
may be important for producing useful and accurate
output, since source and target identification errors
can change the entire meaning of an output text.
Unlike previous approaches that ignore syntax, we
use a sentence?s syntactic structure to build a proba-
bilistic model that encodes whether a word is opinion
bearing as a latent variable. We build a data structure
we call a ?syntactic relatedness trie? (Section 3) that
serves as the skeleton for a graphical model over the
sentiment relevance of words (Section 4). This ap-
proach allows us to learn features that predict opinion
bearing constructions from grammatical structures.
Because of a dearth of resources for this fine-grained
task, we also develop new crowdsourcing techniques
for labeling word-level, syntactically informed sen-
1Alm (2011) recently argued that work on sentiment anal-
ysis needs to de-emphasize the goal of building systems that
are ?high-performing? by traditional measures, because the field
risks sacrificing ?opportunities that may lead to a more thorough
understanding of language uses and users? in relation to subjec-
tive phenomena. The work we present in this paper therefore
focuses on extracting meaningful features as an investment in
future work that directly improves retrieval performance.
timent (Section 5). We use inference techniques to
uncover grammatical patterns that connect opinion-
expressing words and target entities (Section 6) per-
forming better than using syntactically uninformed
methods.
2 Background and existing work
We call opinion mining ?fine-grained? when it re-
trieves many different {source, target, opinion}
triples per document. This is particularly challenging
when there are multiple triples even within a sen-
tence. There is considerable work on identifying the
source of an opinion. However, it is much harder
to find obvious features that tell us whether ?virtual-
ization? is the target of an opinion. The most recent
target identification techniques use machine learning
to determine the presence of a target from known
opinionated language (Jakob and Gurevych, 2010).
Even when targets are identified we must decide if
an opinion is expressed, since not all target mentions
will necessarily be accompanied by opinion expres-
sions. Returning to the first example sentence, we
could say that the negative opinion about virtualiza-
tion is expressed by the words ?slew? and ?issues?.
A system that could automatically make this dis-
covery must draw on grammatical relationships be-
tween targets and the opinion bearing words. Parsers
reveal these relationships, but the relationships are
often indirect. The variability of language prevents
a complete enumeration of all intervening items that
make the relationships indirect, but examples include
negation and intensifiers, which change opinion, and
sentiment-neutral words, which fill syntactic or stylis-
tic needs. In this paper, we cope with the variability
of expression by using supervised machine learning
to generalize across observations and learn which fea-
tures best enable us to identify opinionated language.
Existing work in this area often uses semantic
frames and role labeling (Kim and Hovy, 2006; Choi
et al, 2006), but resources typically used in these
tasks (e.g. FrameNet) are not exhaustive. More gen-
eral approaches (Ruppenhofer et al, 2008) describe
semantic and discourse contexts of opinion sources
and targets cannot recognize them.
When techniques do identify targets via syntax,
they often only use grammar as a feature in an oth-
erwise syntax-agnostic model. Some work of this
668
nature merely identifies targets without providing the
syntactic evidence necessary to find domain-relevant
opinionated language (Jakob and Gurevych, 2010),
relying on lists of opinion keywords. There is also
work (Qiu et al, 2011) that uses predefined heuristics
over dependency parses to identify both targets and
opinion keywords but does not acquire new syntactic
heuristics. Other work (Nakagawa et al, 2010) is sim-
ilar to ours in that it uses factor graph modeling over
a dependency parse formalism, but it assumes that
opinionated language is known a priori and focuses
on polarity classification, while our work tackles the
more fundamental problem of identifying the opin-
ionated language itself.
Little work has been done to perform target and
opinion-expression extraction jointly, especially in a
way that extracts features for downstream processing.
This dearth persists despite evidence that such infor-
mation improves sentiment analysis (Moilanen and
Pulman, 2007).
An advantage of our proposed approach is that we
can use dependency paths in order to capture situa-
tions where the relations are non-compositional or
semantically motivated. In Section 5, we describe a
data set that has the additional property that opinion
is expressed in ways that require external pragmatic
knowledge of the domain. An advantage of arbi-
trary, non-local dependencies is that we can treat this
knowledge as part of the model we learn via long-
distance chains, which can capture pragmatics.
3 Syntactic relatedness tries
We now describe how we build the syntactic related-
ness trie (SRT) that forms the scaffolding for the prob-
abilistic models needed to identify sentiment-bearing
words via syntactic constraints extracted from a de-
pendency parse (Ku?bler et al, 2009).
We use the Stanford Parser (de Marneffe and Man-
ning, 2008) to produce a dependency graph and con-
sider the resulting undirected graph structure over
words. We construct a trie for each possible target
word in a sentence (it is possible for a sentence to
induce multiple tries if the sentence contains multi-
ple potential targets). Each trie encodes paths from
the possible target word to other words, and each
path represents a sequence of words connected by
undirected edges in the parse.
3.1 Encoding Dependencies in an SRT
SRTs enable us to encode the connections between
a single linguistic object of interest?in this appli-
cation, a possible target word?and a set of related
objects. SRTs are data structures consisting of nodes
and edges.
This description is very similar to the definition
of a dependency parse. The key difference is that
while a token only appears once as a node in a de-
pendency parse, an SRT can contain multiple nodes
that originate from the same token. This encodes the
possible connections between an opinion target and
opinion-conveying words.
The object of interest is the opinion target, defined
as the SRT root node (e.g. in Figure 1 ?policy? is a
known target, so it becomes the root of an SRT). Each
SRT edge corresponds to a grammatical relationship
between words and is labeled with that relationship.
We use the notation a
R
?? b to signify that node a has
the relationship (?role?) R with b. We say in this case
that node b is a descendent of node a with the role
R. The directed edges constitute a trie or suffix tree
that represents the fact that multiple paths may share
elements that all provide evidence for the relevance
of multiple leaves. 2
In the remainder of this section we describe the
necessary steps to create a training corpus for fine-
grained sentiment analysis. We provide an example
of how to create an SRT from a dependency parse and
then to attach latent variable assignments to an SRT
based on human annotations in a way that respects
syntactic constraints.
3.2 Using sentiment flow to label an SRT
Our goal is to discriminate between parts of the struc-
ture that are relevant to target-opinion word relations
and those that are not. We use the term sentiment
flow (shortened to ?flow? when space is an issue)
for relevant sentiment-bearing words in the SRT and
inert for the remainder of the sentence. We use the
term ?flow? because our invariant (section 3.3) con-
strains a sentiment flow in a SRT to be a contiguous
subgraph; this corresponds to linguistic intuitions
that, for example, in the sentence ?Linux with Wine
2The SRT will be used to create an undirected graphical
model; the notion of directedness refers to the traversal of paths
used to construct the SRT.
669
the dominant
role
the european climate protection
policy
has
benefits
our
economy
policy
policy
policy
protection
role
role
has
dominant
benefits
Dependency Parse
Paths for "policy" SRT
Figure 1: Dependency parse example. A dependency
parse (top) is used to generate a syntactic relatedness
trie for all possible targets of a sentiment-bearing
expression. For the target word ?policy?, there are a
number of paths (colors are consistent in paths to be
added to the SRT and in the dependency parse) that
connect it to other words; once extracted, these paths
will be inserted into a target-specific SRT.
is very usable?, {?Linux?, ?is?, ?very?} could not
be part of a sentiment flow without also including
{?usable?}.
Now that we have the structure of the model, we
need training data: sentences where sentiment bear-
ing words have been labeled. We describe how to go
from sentiment-labeled words to valid flows using
this sentence from the MPQA:
The dominant role of the European climate
protection policy has benefits for our econ-
omy.
In this sentence, the target word ?policy? is con-
nected to multiple sentiment-bearing words via paths
in the dependency parse (Figure 1). We can represent
these relationships using paths through the graph as
in Figure 2(a). (For clarity, we do not show some
paths.)
Suppose that an annotator decides that ?protec-
tion? and ?benefits? are directly expressing an opin-
ion about the policy, but ?dominant? is ambiguous (it
has some negative connotations). The nodes ?protec-
tion? and ?benefits? are a flow, and the ?dominant?
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
policy
protection
role
has benefits
dominant
(a)
(b)
(c)
(d)
Figure 2: Labeled SRTs rooted on the target word
?policy?; green-filled nodes represent words that are
part of a sentiment flow and nodes with a red outline
represent inert nodes. (a) Initial labels for SRT (e.g.
as provided by annotators) (b) propagating labels to
yield a valid sentiment flow (c) a change of ?role? to
inert also renders its children inert (d) a change of
?dominant? to be part of a sentiment flow also causes
its parents to be part of a flow.
node is inert. However, there is considerable overlap
between the ?dominant? path and the ?benefits? path.
That is the motivation for combining them into a trie
structure and labeling them in such a way that the
path remains a flow until there is no path element that
leads to a flow leaf (Figure 2).
In other words, we want the path elements com-
mon to a flow path and an inert path to reinforce
sentiment flow. The transition from flow to inert is
learned by the classifier.
We enforce this requirement through the procedure
shown in Figure 2, which is equivalent to finding the
depth first search tree of the dependency graph and
applying the node-labeling scheme as above.
3.3 Invariant
Anything that follows a node with an inert label is
by definition not reachable from the root of the tree.
670
Consequently, any node that is part of a sentiment
flow that follows an inert node is not reachable along
a path and is actually inert itself. We specify this
directly as an invariant on the data structure:
Invariant: no node descending from a
node labeled inert can be labeled as a part
of a sentiment flow.
This specifies that flow labels spread out from the
root of the SRT. Our inference algorithm requires
that we be able to change the labels of nodes for
test data, thus we need to define invariant-respecting
operations for switching labels from flow to flow and
vice-versa. A flow label switched to inert will require
all the descendents of that particular node to switch
to inert as well as in figure 2(c). Similarly, an inert
label switched to flow will require all of the ancestors
of that node to switch to flow as in 2(d).
4 Encoding SRTs as a factor graph
In this section, we develop supervised machine learn-
ing tools to produce a labeled SRT from unlabeled,
held-out data in a single, unified model, without per-
mitting the sorts of inconsistencies that may be ad-
mitted by using a local classifier at each node.
4.1 Sampling labels
A factor graph (Kschischang et al, 1998) is a rep-
resentation of a joint probability distribution in the
form of a graph with two types of vertices: vari-
able vertices and factor vertices. Given a set of vari-
ables Z = {z1 . . . zn}, we connect them via factors
F = {f1 . . . fm}. Factors are functions that repre-
sent relationships, i.e. probabilistic dependencies,
among the variables; the product of all factors gives
the complete joint distribution p. Each factor fi can
take as input some corresponding subset of variables
Yi from Z. We can then write the relationship as
follows:
p(Z) ?
?m
k=1 fk(Yk)
Our goal is to discover the values for the variables
that best explain a dataset. While there are many
approaches for inference in statistical models, we
turn to MCMC methods (Neal, 1993) to discover the
underlying structure of the model. More specifically,
we seek a posterior distribution over latent variables
parent
node
child
1
child
2
child
3
h
g
f
Figure 3: Graphical model of SRT factors
that partition words in a sentence into flow and in-
ert groups; we estimate this posterior using Gibbs
sampling (Finkel et al, 2005).
The sampler requires an initial state that respects
the invariant. Our initial setting is produced by iterat-
ing through all labels in the SRT forest and randomly
setting them as either flow or inert with uniform
probability.
A Gibbs sampler samples new variable assign-
ments from the conditional distribution, treating the
variable assignments for all other variables fixed.
However, the assignment of a single node is highly
coupled with its neighbors, so a block sampler is used
to propose changes to groups nodes that respect the
flow labeling of the overall assignments. This was
implemented by changing the proposal distribution
used by the FACTORIE framework (McCallum et al,
2009).
We can thus represent a node and its contribution
to the overall score using the graph in Figure 3. This
graph contains the given node, its parent, and a vari-
able number of children. The factors that go into the
labeling decision for each node are thus constrained
to a small, computationally tractable space around
the given node. This graph contains three factors:
? g represents a function over features of the given
node itself, or ?node features.?
? f represents a function over a bigram of features
taken from the parent node and the given node,
or ?parent-node? features.
? h represents a function over a combination fea-
tures on the node and features of all its children,
or ?node-child? features.
We provide further details about these factors in the
next section.
671
In addition to the latent value associated with each
word, we associate each node with features derived
from the dependency parse: the word from the sen-
tence itself, the part-of-speech (POS) tag assigned
by the Stanford parser, and the label of the incoming
dependency edge. We treat the edge labels from the
original dependency parse as a feature of the node.
We can represent the set of possible observed lin-
guistic feature classes as the set of features ?. Fig-
ure 3 induces a scoring function with contributions
of each node to the score(label|node) =
?
???
(
f(parent?, node?|label)g(node?|label)
h(node?, child1?, . . . , childn?|label)
)
.
After assignments for the latent variables are sampled,
the weights for the factors (which when combined
create individual factors f that define the joint) must
be learned. This is accomplished via the sample-rank
algorithm (Wick et al, 2009).
5 Data source
Our goal is to identify opinion-bearing words and tar-
gets using supervised machine learning techniques.
Sentiment corpora with sub-sentential annotations,
such as the Multi-Perspective Question-Answering
(MPQA) corpus (Wilson and Wiebe, 2005) and the
J. D. Power and Associates (JDPA) blog post cor-
pus (Kessler et al, 2010), exist, but most of these
annotations are at a phrase level. Within a phrase,
however, some words may contribute more than oth-
ers to the statement of an opinion. We developed our
own annotations to discover such distinctions3. We
describe these briefly here; more information about
the development of the data source can be found in
Sayeed et al (2011).
5.1 Information technology business press
Our work is part of a larger collaboration with so-
cial scientists to study the diffusion of information
technology (IT) innovations through society by iden-
tifying opinion leaders and IT-relevant opinionated
language Rogers (2003). Thus, we focus on a col-
lection of articles from the IT professional maga-
zine, Information Week, from the years 1991 to 2008.
3To download the corpus, visit http://www.umiacs.
umd.edu/?asayeed/naacl12data/.
This consists of 33K articles including news bulletins
and opinion columns. Our IT concept target list (59
terms) comes from our application. Thus, we con-
struct a trie for each appearance of any of these possi-
ble target terms. We consider this list of target terms
to be complete, which allows us to focus on discover-
ing opinion-bearing text associated with these targets.
5.2 Crowdsourced annotation process
Our process for obtaining gold standard data involves
multiple levels of human annotation including on
crowdsourcing platforms Hsueh et al (2009).
There are 75K sentences with IT concept mentions,
only a minority of which express relevant opinions.
Hired undergraduate students searched a random se-
lection of these sentences and found 219 that contain
these opinions. We used cosine-similarity to rank the
remaining sentences against the 219.
We then needed to identify which of the words
contained an opinion. We excluded all words that
were common function words (e.g.,?the?, ?in?) but
left negations. We engineered tasks so that only
a randomly-selected five or six words appear high-
lighted for classification in order to limit annotator
boredom. We called this group a ?highlight group?.
The virtualization example would look like this:
Lloyd Hession, chief security officer at BT
Radianz in New York, said that virtual-
ization also opens up a slew of potential
network access control issues.
In the virtualization example, the worker would see
that virtualization is highlighted as the IT concept
target. Other words are highlighted as candidates that
the worker must classify as being opinion-relevant to
?virtualization?. Each highlight group corresponds to
a syntactic relatedness trie (Section 3).
A task was presented to a worker in the form of
a highlight group and some list boxes that represent
classes for the highlighted words: ?positive?, ?nega-
tive?, ?not opinion-relevant?, and ?ambiguous?. The
worker was required to drag each highlighted can-
didate word to exactly one of the boxes. As we are
not doing opinion polarity classification, the ?posi-
tive? and ?negative? boxes were intended as a form
of misdirection intended to avoid having the worker
consider what an opinion is; we treated this input as
a single ?opinion-relevant? category.
672
Three or more users annotated each highlight
group, and an aggregation scheme was applied af-
terwards: ?ambiguous? answers were rolled into ?not
opinion-relevant? and ties were dropped. Our qual-
ity control process involved filtering out workers
who performed poorly on a small subset of gold-
standard answers We annotated 30 evaluation units to
determine that our process retrieved opinion-relevant
words at 85% precision and 74% recall.
Annotators labeled 700 highlight groups for the
results in this paper. The total cost of this exercise
was approximately 250 USD, which includes the fees
charged by Amazon and CrowdFlower. These last
highlight groups were converted to SRTs and divided
into training and testing groups, 465 and 196 SRTs
respectively, with a small number lost to fatal errors
in the Stanford parser.
6 Experiments and discussion
During the training phase, we evaluate the quality
of a candidate labeling based on label accuracy. We
need to identify both flow nodes and inert nodes in
order to distinguish between relevant and irrelevant
subcomponents. We thus also employ precision and
recall as performance metrics.
An example of how this works can be seen by com-
paring figure 2(b) to figure 2(d), viewing the former
as the gold standard and the latter as a hypothetical
system output. If we run the evaluation over that
single SRT and treat flow as the positive class, we
find that 3 true positives, 1 false positive, 2 false neg-
atives, and no true negatives. There are 6 labels in
total. That yields 0.50 accuracy, 0.75 precision, 0.60
recall, and 0.67 F-measure.
We run every experiment (training a model and
testing on held-out data) 10 times and take the mean
average and range of all measures. F-measure is
calculated for each run and averaged post hoc.
6.1 Experiments
Our baseline system is the initial setting of the labels
for the sampler: uniform random assignment of flow
labels, respecting the invariant. This leads to a large
class imbalance in favor of inert as any switch to
inert converts all nodes downstream from the root to
convert to inert, while a switch to flow causes only
one ancestor branch to convert to flow.
Our next systems involve combinations of our SRT
factors with the observed linguistic features. All our
experiments include the factor g that pertains only to
the features of the node. Then we add factor f?the
parent-node ?bigram? features?and finally factor h,
the variable-length node-child features. We also ex-
periment with including and excluding combinations
of POS, role, and word features. We also explored
models that only made local decisions, ignoring the
consistency constraints over sentiment flows. Al-
though such models cannot be used in techniques
such as Nakagawa et al?s polarity classifier, they
function as a baseline and inform whether syntactic
constraints help performance.
We ran the inferencer for 200 iterations to train a
model with a particular factor-feature combination.
We use the learned model to predict the labels on
the held-out testing data by running the inference
algorithm (sampling labels only) for 50 iterations.
6.2 Discussion
We present a sampling of possible feature-factor com-
binations in table 1 in order to show trends in the
performance of the system.
Unsurprisingly, the invariant-respecting baseline
had very high precision but low recall. Simply includ-
ing the node-only g factor with all features increases
the recall while hurting precision. On removing word
features, recall increases without changing precision.
This suggests that some words in some SRTs are as-
sociated with flow labels in the training data, but not
as much in the testing data.
Including parent-node f features with the g fea-
tures yields higher precision and lower recall, sug-
gesting that parent-node word features support preci-
sion. Including all features on all factors (f , g, and h)
preserves most of the precision but improves recall.
Excluding h features increases recall slightly more
than it hurts precision. Excluding both word features
for all factors and role h features hurts all measures.
The accuracy measure, however, does show over-
all improvement with the inclusion of more feature-
factor combinations. In particular, the node-child h
factor does appear to have an effect on the perfor-
mance. The presence of some combinations of child
word, POS tags, and roles appear to provide some
indication of the flow labeling of some of the nodes.
The best models in terms of accuracy include all or
673
Experiment Features Invariant? Precision Recall F Accuracy
Baseline N/A
Yes 0.78 ? 0.05 0.06 ? 0.01 0.11 ? 0.02 0.51 ? 0.01
No 0.50 ? 0.00 0.49 ? 0.00 0.50 ? 0.00 0.50 ? 0.00
Node only
All
Yes 0.63 ? 0.10 0.34 ? 0.10 0.42 ? 0.07 0.54 ? 0.03
No 0.51 ? 0.00 0.88 ? 0.03 0.65 ? 0.01 0.51 ? 0.01
All but word
Yes 0.63 ? 0.16 0.40 ? 0.22 0.42 ? 0.19 0.53 ? 0.03
No 0.57 ? 0.04 0.56 ? 0.17 0.55 ? 0.07 0.55 ? 0.03
Parent, node
Parent: all but word
Yes 0.71 ? 0.06 0.21 ? 0.04 0.31 ? 0.05 0.55 ? 0.01
Node: all
All Yes 0.84 ? 0.07 0.11 ? 0.04 0.19 ? 0.06 0.53 ? 0.01
Full graph
Parent: all but word
Yes 0.59 ? 0.06 0.39 ? 0.11 0.46 ? 0.07 0.54 ? 0.03Node: all but word
Children: POS only
Parent: all
Yes 0.67 ? 0.05 0.39 ? 0.08 0.47 ? 0.06 0.59 ? 0.02Node: all
Children: all but word
All
Yes 0.70 ? 0.05 0.35 ? 0.08 0.46 ? 0.07 0.59 ? 0.02
No 0.70 ? 0.03 0.20 ? 0.05 0.36 ? 0.06 0.56 ? 0.01
Table 1: Performance using different feature combinations, including some without enforcing the invariant.
Mean averages and standard deviation for 10 runs.
almost all of the features.
Our non-invariant-respecting baseline unsurpris-
ingly was nearly 50% on all measures. Including the
node-only features dramatically increases recall, less
if we exclude word features. The word features ap-
pear to have an effect on recall just as in the invariant-
respecting case with node-only features. With all
features, precision is dramatically improved, but with
a large cost to recall. However, it underperforms
the equivalent invariant-respecting model in recall,
F-measure, and accuracy.
Though these invariant-violating models are un-
constrained in the way they label the graph, our
invariant-respecting models still outperform them.
A coherent path contains more information than an
incoherent one; it is important to find negating and
intensifying elements in context. Our SRT invariant
allows us to achieve better performance and will be
more useful to downstream tasks.
Finally, it appears that using more factors and lin-
guistic features promotes stability in performance
and decreases sensitivity to the initial setting.
6.3 Manual inspection
One pattern that prominently stood out in the testing
data with the full-graph model was the misclassifica-
tion of flow labels as inert in the vicinity of Stanford
dependency labels such as conj and. These kinds
of labels have high ?fertility?; the labels immediately
following them in the SRT could be a variety of types,
creating potential data sparsity issues.
This problem could be resolved by making some
features transparent to the learner. For example, if
node q has an incoming conj and dependency edge
label, then q?s parent could also be directly connected
to q?s children, as a conjunction should be linguisti-
cally transparent to the status of the children in the
sentiment flow.
There are many fewer incidents of inert labels be-
ing classified as flow. There are paths through an
SRT where a flow candidate word is the ancestor of
an inert candidate word from the set of crowdsourced
candidates. The model sometimes appears to ?over-
shoot? the flow candidate. Considering that recall is
already fairly low, attempts to address this problem
risks making the model too conservative. One poten-
tial solution is to prune or separate paths that contain
multiple flow candidates.
6.3.1 Paths found
We examined the labeling on the held-out testing
data of the best-performing model of the full graph
system with all linguistic features. For example, con-
sider the following highlight group:
But Microsoft?s informal approach may not be
enough as the number of blogs at the company
grows, especially since the line between ?personal?
Weblogs and those done as part of the job can be
hard to distinguish.
In this case, the Turkers decided that ?distinguish?
expressed a negative opinion about blogs, in the sense
674
that something that was difficult to distinguish was
a problem: the modifier ?hard? is what makes it
negative. The system found an entirely flow path that
connected these attributes into a single unit:
Blog:flow prepof????? number:flow nsubj????
grows:flow ccomp????? hard:flow xcomp?????
distinguish:flow
In this path, ?blog? and ?distinguish? are both con-
nected to one another by ?hard?, giving ?distinguish?
its negative spin. There are two non-local dependen-
cies in this example: xcomp, ccomp. Very often,
more than one unique path connects the concept to
the opinion candidate word.
7 Conclusions and future work
In this work, we have applied machine learning to
produce a robust modeling of syntactic structure for
an information extraction application. A solution to
the problem of modeling these structures requires the
development of new techniques that model complex
linguistic relationships in an application-dependent
way. We have shown that we can mine these relation-
ships without being overcome by the data-sparsity
issues that typically stymie learning over complex
linguistic structure.
The limitations on these techniques ultimately find
their root in the difficulty in modeling complex syn-
tactic structures that simultaneously exclude irrel-
evant portions of the structure while maintaining
connected relations. Our technique uses a structure-
labelling scheme that enforces connectedness. En-
forcing connected structure is not only necessary to
produce useful results but also to improve accuracy.
Further performance gains might be possible by en-
riching the feature set. For example, the POS tagset
used by the Stanford parser contains multiple verb
tags that represent different English tenses and num-
bers. For the purpose of sentiment relations, it is
possible that the differences between verb tags are
too small to matter and are causing data sparsity is-
sues. Thus, we could additional features that ?back
off? to general verb tags.
Acknowledgements
This paper is based upon work supported by the
US National Science Foundation under Grant IIS-
0729459. Additional support came from the Cluster
of Excellence ?Multimodal Computing and Innova-
tion?, Germany. Jordan Boyd-Graber is also sup-
ported by US National Science Foundation Grant
NSF grant #1018625 and the Army Research Labora-
tory through ARL Cooperative Agreement W911NF-
09-2-0072. Any opinions, findings, conclusions, or
recommendations expressed are the authors? and do
not necessarily reflect those of the sponsors.
References
Alm, C. O. (2011). Subjective natural language prob-
lems: Motivations, applications, characterizations,
and implications. In ACL (Short Papers).
Bollen, J., Mao, H., and Zeng, X.-J. (2010). Twit-
ter mood predicts the stock market. CoRR,
abs/1010.3003.
Choi, Y., Breck, E., and Cardie, C. (2006). Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
de Marneffe, M.-C. and Manning, C. D. (2008). The
stanford typed dependencies representation. In
CrossParser ?08: Coling 2008: Proceedings of
the workshop on Cross-Framework and Cross-
Domain Parser Evaluation, Morristown, NJ, USA.
Association for Computational Linguistics.
Finkel, J. R., Grenager, T., and Manning, C. (2005).
Incorporating non-local information into informa-
tion extraction systems by gibbs sampling. In
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Hsueh, P.-Y., Melville, P., and Sindhwani, V. (2009).
Data quality from crowdsourcing: a study of anno-
tation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learn-
ing for Natural Language Processing, HLT ?09,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jakob, N. and Gurevych, I. (2010). Extracting opin-
ion targets in a single and cross-domain setting
with conditional random fields. In EMNLP.
Kessler, J. S., Eckert, M., Clark, L., and Nicolov,
N. (2010). The 2010 ICWSM JDPA sentment
675
corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Kim, S.-M. and Hovy, E. (2006). Extracting opinions,
opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the
Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Kschischang, F. R., Frey, B. J., and andrea Loeliger,
H. (1998). Factor graphs and the sum-product algo-
rithm. IEEE Transactions on Information Theory,
47:498?519.
Ku?bler, S., McDonald, R., and Nivre, J. (2009). De-
pendency parsing. Synthesis Lectures on Human
Language Technologies, 2(1).
McCallum, A., Schultz, K., and Singh, S. (2009).
Factorie: Probabilistic programming via impera-
tively defined factor graphs. In Neural Information
Processing Systems (NIPS).
Moilanen, K. and Pulman, S. (2007). Sentiment com-
position. In Proceedings of the Recent Advances in
Natural Language Processing International Con-
ference (RANLP-2007), Borovets, Bulgaria.
Nakagawa, T., Inui, K., and Kurohashi, S. (2010). De-
pendency tree-based sentiment classification using
crfs with hidden variables. In HLT-NAACL.
Neal, R. M. (1993). Probabilistic inference using
Markov chain Monte Carlo methods. Technical
Report CRG-TR-93-1, University of Toronto.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2).
Qiu, G., Liu, B., Bu, J., and Chen, C. (2011). Opin-
ion word expansion and target extraction through
double propagation. Computational linguistics,
37(1):9?27.
Rogers, E. M. (2003). Diffusion of Innovations, 5th
Edition. Free Press.
Ruppenhofer, J., Somasundaran, S., and Wiebe, J.
(2008). Finding the sources and targets of sub-
jective expressions. In Calzolari, N., Choukri, K.,
Maegaard, B., Mariani, J., Odjik, J., Piperidis, S.,
and Tapias, D., editors, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco. European Lan-
guage Resources Association (ELRA).
Sayeed, A. B., Nguyen, H. C., Meyer, T. J., and
Weinberg, A. (2010). Expresses-an-opinion-about:
using corpus statistics in an information extraction
approach to opinion mining. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10.
Sayeed, A. B., Rusk, B., Petrov, M., Nguyen, H. C.,
Meyer, T. J., and Weinberg, A. (2011). Crowd-
sourcing syntactic relatedness judgements for opin-
ion mining in the study of information technology
adoption. In Proceedings of the Association for
Computational Linguistics 2011 workshop on Lan-
guage Technology for Cultural Heritage, Social
Sciences, and the Humanities (LaTeCH). Associa-
tion for Computational Linguistics.
Stoyanov, V. and Cardie, C. (2006). Partially su-
pervised coreference resolution for opinion sum-
marization through structured rule learning. In
EMNLP ?06: Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 336?344, Morristown, NJ, USA.
Association for Computational Linguistics.
Tsui, C.-J., Wang, P., Fleischmann, K., Oard, D.,
and Sayeed, A. (2009). Understanding IT innova-
tions by computational analysis of discourse. In
International conference on information systems.
Wick, M., Rohanimanesh, K., Culotta, A., and Mccal-
lum, A. (2009). SampleRank: Learning preference
from atomic gradients. In NIPS WS on Advances
in Ranking.
Wilson, T. and Wiebe, J. (2005). Annotating attribu-
tions and private states. In CorpusAnno ?05: Pro-
ceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association
for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In HLT/EMNLP.
676
Proceedings of NAACL-HLT 2013, pages 691?696,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
An opinion about opinions about opinions: subjectivity and the aggregate
reader
Asad Sayeed
Computational Linguistics and Phonetics / M2CI Cluster of Excellence
Saarland University
66123 Saarbru?cken, Germany
asayeed@coli.uni-saarland.de
Abstract
This opinion piece proposes that recent ad-
vances in opinion detection are limited in the
extent to which they can detect important cat-
egories of opinion because they are not de-
signed to capture some of the pragmatic as-
pects of opinion. A component of these is the
perspective of the user of an opinion-mining
system as to what an opinion really is, which is
in itself a matter of opinion (metasubjectivity).
We propose a way to define this component of
opinion and describe the challenges it poses
for corpus development and sentence-level de-
tection technologies. Finally, we suggest that
investment in techniques to handle metasub-
jectivity will likely bear costs but bring bene-
fits in the longer term.
1 Introduction
Opinion mining, also known as sentiment analysis
(Pang and Lee, 2008), is a relatively recent area
of research in natural language processing. It has
grown very quickly as a research area, developing
around a small number of basic approaches. How-
ever, these approaches are based on particular def-
initions of opinion, assumptions about opinion ex-
pressions, and evaluation practices that we believe
need to be expanded in order for sentiment analysis
to reach new domains and applications.
We are not the first to express concern over the
direction of sentiment analysis as a field. This paper
seeks to further expand upon the views expressed
in Alm (2011) that prevailing evaluation concepts in
sentiment analysis limit the kinds of models we can
build, particularly through the encouragement of a
focus on ?high-performing? systems.
The central thread that connects our view of the
field is the idea that the basis of standard techniques
and evaluation in information retrieval and extrac-
tion that underlie existing approaches needs to be
rethought for applications that are inherently subjec-
tive and that the field needs to return to more theoret-
ical groundwork. This will entail sacrificing some of
the performance gains made in recent times, as well
as potentially reducing the capacity for easily com-
parable research that has been gained by the rapid
adoption of corpora that are very easily produced,
shared, and used.
This problem is particularly relevant in the expan-
sion of sentiment analysis techniques to areas such
as market prediction (Bollen et al, 2010) and social
science. In these areas, it is not enough to detect
opinions in predefined areas of text or even to mine
for the locations of opinions in large corpora, but it is
necessary to be able to connect opinions across doc-
uments and to reconstruct the social networks that
underlie social trends. Furthermore, it must be pos-
sible to do this in text that can have an arbitrary num-
ber of opinions intertwined in ways that go beyond
the base case of product review text. This requires
both additional consideration of the perspective of
the user and attention to the finer-grained details of
sentiment expression.
Do existing resources and techniques really re-
flect the ultimate goals and end-uses of fine-grained
opinion-mining, particularly focusing on the senten-
tial and sub-sentential levels? Consider an ?ideal
case? of a marketing director or a political campaign
manager requesting a forecast of how a product or
concept will unfold in the media and market. How
do the present conceptions of opinion mining relate
to this among other real-world problems of affect?
691
In the remainder of this position paper, we briefly
describe three closely related issues in sentiment
analysis that pertain to expanding beyond the cur-
rent limits of the field.
2 Challenges
2.1 Metasubjectivity and pragmatic opinion
Recent efforts in opinion mining (Ruppenhofer
et al, 2008) technology have often tended to take
the position that opinion is an internal characteristic
of the speaker, a ?private state?, and that the overall
aim of the opinion mining field is to discover tech-
niques that allow us to infer the that latent state from
the evidence presented in text. But this may not al-
ways be appropriate to all circumstances.
A very simple boundary example comes from So-
masundaran and Wiebe (2009): The blackberry is
something like $150 and the iPhone is $500. This
comes from a corpus of opinions on cell phone pref-
erence, and this sentence is intended to be a negative
opinion about the iPhone. According to Somasun-
daran and Wiebe, this kind of opinion-expression
requires a model of world-knowledge that is either
not practical under current technologies, or it re-
quires the development of techniques that can re-
cruit a larger context in the text in order to make the
correct inference. They refer to this phenomenon as
?pragmatic opinion?.
One crucial piece of world-knowledge that pro-
vides an opinion its polarity is that of the perspective
of the reader or listener to the opinion; we can min-
imally represent this as the ?application? to which
the opinion will be put. We refer to variation in the
application-specific interpretation of the concept of
opinion as ?metasubjectivity.? Metasubjectivity is
a serious problem in extending sentiment analysis
work to other domains, particularly for reasons that
we describe in the next section.
Metasubjectivity is closely related to the underly-
ing relative nature of veridicality assessment. The
veridicality of an utterance is the level to which the
listener may judge it as a factual statement about the
world. de Marneffe et al (2012) note that this re-
quires, in some cases, extensive pragmatic knowl-
edge. They present this sentence as an example:
FBI agents alleged in court documents today that
Zazi had admitted receiving weapons and explosives
training from al-Qaeda operatives in Pakistan last
year. There is an interplay between the trustworthi-
ness of the source of the sentence, the mentioned en-
tities, and the veridicality of words alleged and ad-
mitted, all of which are mediated by the perspective
of the reader. For example, if the reader is strongly
inclined to trust the FBI, then there may be a high
level of veridicality in ?alleged? than otherwise. But
it could also be the case that the reader believes that
Zazi is misleading the FBI.
These distinctions operate directly in the context
of determining polarity in opinion mining. Consider
the following example sentence from a major in-
formation technology (IT) business journal: Lloyd
Hession, chief security officer at BT Radianz in New
York, said that virtualization also opens up a slew of
potential network access control issues.
This sentence can be taken to represent an opinion
or merely a factual statement. A casual reader with-
out experience in the domain of IT might be con-
vinced that this sentence is simply a neutral state-
ment of fact. But from the perspective of an inter-
ested reader such as an investor, this may actually
represent a mildly negative statement about virtu-
alization, or it may represent a negative statement
about network access control. From the perspective
of the manager of an IT support department, it may
well be very negative. But from the perspective of
Lloyd Hession, we have no idea outside of the prag-
matic context. Mr. Hession could be a developer of
IT solutions, in which case he would view this as a
positive development for the market in new network
access control technologies, or, for that matter, he
may be invested in a set of technological approaches
that compete with virtualization.
This extends to the vocabulary used to express
opinions. The use of the word ?slew?, in this case,
has negative connotations, but only if the whole
statement is construed by the perspective of the
reader to represent an opinion. However, if Lloyd
Hession is a provider of new network access control
solutions, then the use of ?open? may convert this
negative context into a positive context.
This is not merely a matter of the perspectives
of individual users and participants. It is a matter
of how providers of sentiment analysis applications
choose to represent these choices to the user, which
is in turn reflected in the way in which they create
692
resources, models, and algorithms. If, for example,
our goal is to provide sentiment analysis for domain-
specific market prediction or social science, then we
need to model the reactions not of the private state
of Mr. Hession or of the writer of the article, but
of an ?aggregate reader? with a presumed interest in
the text. Here is a definition of this external state
aggregate reader model that might apply to the IT
business domain:
Opinion source A expresses opinion about opinion
target B if an interested third party C?s actions to-
wards B may be affected by A?s textually recorded
actions, in a context where actions have positive or
negative weight.
This accounts for the cases in which the opinion of
interest in the IT example happens to be held by an
investor or a IT support manager or other interested
readers, and it can be generalized to apply to other
domains in which the world?s opinion matters.
It is once again within the area of veridicality as-
sessment that we suggest that a possible form of
solution exists. de Marneffe et al (2012) present
a model in which the uncertainty in veridicality is
represented as a distribution rather than a discrete
labelling problem.
In the case of veridicality, there is generally an
ultimate ground truth in verifiable facts about the
world, apart from the relative veridical nature of a
statement. For sentiment, however, there is no such
foundation: opinion presence and opinion polarity
exist entirely relative to the perspective of the ag-
gregate reader. This requires a different process of
annotation, the challenges of which we describe in
the next section.
2.2 Corpus development and evaluation
Considering the prevalence of machine learning
techniques in opinion mining research, addressing
the issue of metasubjectivity must mean addressing
the matter of the corpus development.
Existing evaluation techniques depend on a no-
tion of ?gold standard data? that are produced by
expert judges or crowdsourced annotators (Wilson,
2007; Kessler et al, 2010; Hsueh et al, 2009). There
are NLP areas in which popular notions of objec-
tivity may partly apply, such as query relevance;
due, among other things, to metasubjectivity, opin-
ion mining is not entirely one of these. However,
gold standard data for opinion mining is typically
produced using procedures that are standard for in-
formation retrieval research, and the quality mea-
sures that are generally used happen to assume the
presence of an underlying objective truth.
This assumption can be coerced to fit particular
cases. For example, a large proportion of opinion
mining research is invested in predicting the rat-
ings of product reviews and then aggregating results
into a single ratings summary, sometimes based on
a lower-level breakdown of product features (de Al-
bornoz et al, 2011). Implicit in this type of work
is the assumption of the existence of an ideal rater
who uses language in a roughly predictable way to
express his or her feelings about the text.
The users of these types of systems can be as-
sumed, to some degree of safety, to share some of
the expectations of the builders of these systems,
particularly since groups of users as product raters
are often the source of the information itself.
But in environments where the users of the sys-
tem may have various different perspectives on the
nature of sentiment, it does not make sense to as-
sume that there would ever be significant agree-
ment among annotators, particularly for market-
relevant applications where prediction of reader re-
action is central to the task. We attempted to an-
notate IT business press articles for sentence-level
reader-perspective opinion occurrences and found
that multiple trained annotators had very low inter-
rater agreement by Cohen?s ?. Multiple attempts
at further annotator training and error analysis re-
vealed that the annotators simply found it very dif-
ficult to agree on what the definition of an opinion
was. Originally, we had two trained student anno-
tators for this task, with repeated training and joint
practice annotations in order to achieve consensus as
to what counts as an opinion mention instance and
what does not. Other groups of annotators and an-
notation designs had no better success.
However, we observed that this appears to be pri-
marily a problem of conservativity where annotators
differed in the quantity of sentences that they con-
sidered to be opinionated, and had a large amount of
overlap in those that they did consider to be opinion-
ated. Further discussion with the annotators found
that some simply had a much lower threshold at
which they would consider a sentence to contain an
693
opinion. In other words, this form of annotation is
more affected by metasubjectivity than opinion an-
notation focused on opinion source perspective. It
should be noted that this is a different task from
finding opinion sources and labelling the textual ev-
idence of their private states; we were attempting to
model the ?ideal case? we identified in section 1.
We suggest that the answer to this problem is
to deploy the concept of the aggregate reader men-
tioned in the previous section and to pose the anno-
tation question indirectly. The former requires the
collection of data from a larger number of people
and can be provided by existing crowdsourcing tech-
niques (Snow et al, 2008). The latter, however, re-
quires designing the annotation in such a way that
it avoids letting the annotator consider the question:
?What is an opinion?? This is most likely done by
a user interface that simulates the behaviour of the
intended aggregate reader (Sayeed et al, 2011).
2.3 Grammatical expression
There are a number of types of features with which
one can construct and train supervised sentence-
level sentiment detection models. Most recent tech-
niques (Kim and Hovy, 2006; Choi et al, 2006;
Jakob and Gurevych, 2010) take into account the
syntactic context of the sentence but limit the
amount of syntactic context thus used. These re-
strictions reduce the presence or absence of partic-
ular structures to binary features in the model. We
argue that we need techniques that take into account
more syntactic context, particularly without making
use of predefined structures.
The latest techniques make use of larger syntac-
tic contexts with potentially unlimited scope. One
example is Nakagawa et al (2010), who use fac-
tor graphs (McCallum et al, 2009) to learn a model
that traces paths through the dependency trees of
opinion-relevant sentences (de Marneffe and Man-
ning, 2008). However, this is in the service of polar-
ity classification, as it assumes that the appropriate
sentences have already been identified; then it is a
matter of correctly processing negations and other
polarity-changing items. The challenge of metasub-
jectivity is a barrier to opinion sentence detection it-
self, well before polarity classification.
Another example is Qiu et al (2011). They are
more directly focused on detecting opinion-relevant
language. However, they make use of a system of
hard-coded heuristics to find opinion words in de-
pendency parses. While these types of heuristics
support longer-distance syntactic relations, they tend
to focus on cases where some form of semantic com-
positionality holds. However, consider this sentence
from the IT business press: The contract is consis-
tent with the desktop computing Outsourcing deals
Citibank awarded EDS and Digital Equipment in
1996. . . In this case, an interested aggregate reader
might note that ?awarded? is a word that puts ?out-
sourcing? in a positive light. However, the syntac-
tic relationship between these two words does not
directly imply or permit any semantic composition-
ality, In order to find these relationships, we would
need to invest in techniques that can learn from ar-
bitrary non-compositional structure, thereby poten-
tially capturing patterns in grammar that actually re-
flect some aspects of external pragmatic knowledge.
3 Conclusions
This paper has proposed a challenge for opinion
mining, the challenge of metasubjectivity: where the
answer to the question ?What is an opinion?? is in
itself an opinion and an intrinsic part of the task. We
first established the context of metasubjectivity rela-
tive to existing characterizations of the opinion min-
ing task, establishing the notion of an external aggre-
gate reader as a way to extend from existing notions
of sentiment as an internal state. Then we described
how this affects the annotation process, given the
as-yet-continuing dependence on supervised corpus-
based detection techniques. Finally, we described
how this affects sentence-level fine-grained opinion
detection at the level of syntactic analysis.
One of the risks for the field in proceeding to
investigations of how to deal with the question
of metasubjectivity is one familiar in natural lan-
guage processing as a whole: there is a strong risk
that these techniques will?initially and for a non-
trivial quantity of time?cause the incremental per-
formance gains in existing research to be lost or
damaged. It will also require the creation of new
training corpora and related resources, temporarily
threatening comparability. Nevertheless, we believe
that these risks need to be accepted in order to make
progress in sentiment analysis.
694
References
Alm, C. O. (2011). Subjective natural language
problems: Motivations, applications, characteri-
zations, and implications. In ACL (Short Papers).
Bollen, J., Mao, H., and Zeng, X.-J. (2010). Twit-
ter mood predicts the stock market. CoRR,
abs/1010.3003.
Choi, Y., Breck, E., and Cardie, C. (2006). Joint ex-
traction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
de Albornoz, J., Plaza, L., Gerva?s, P., and D??az,
A. (2011). A joint model of feature mining
and sentiment analysis for product review rating.
In Clough, P., Foley, C., Gurrin, C., Jones, G.,
Kraaij, W., Lee, H., and Mudoch, V., editors, Ad-
vances in information retrieval, volume 6611 of
Lecture Notes in Computer Science, pages 55?66.
Springer Berlin / Heidelberg.
de Marneffe, M.-C. and Manning, C. D. (2008).
The stanford typed dependencies representation.
In CrossParser ?08: Coling 2008: Proceed-
ings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
de Marneffe, M.-C., Manning, C. D., and Potts, C.
(2012). Did it happen? the pragmatic complex-
ity of veridicality assessment. Computational lin-
guistics, 35(1).
Hsueh, P.-Y., Melville, P., and Sindhwani, V. (2009).
Data quality from crowdsourcing: a study of an-
notation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learn-
ing for Natural Language Processing, HLT ?09,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jakob, N. and Gurevych, I. (2010). Extracting opin-
ion targets in a single and cross-domain setting
with conditional random fields. In EMNLP.
Kessler, J. S., Eckert, M., Clark, L., and Nicolov, N.
(2010). The 2010 ICWSM JDPA sentment cor-
pus for the automotive domain. In 4th Int?l AAAI
Conference on Weblogs and Social Media Data
Workshop Challenge (ICWSM-DWC 2010).
Kim, S.-M. and Hovy, E. (2006). Extracting opin-
ions, opinion holders, and topics expressed in on-
line news media text. In SST ?06: Proceedings
of the Workshop on Sentiment and Subjectivity in
Text, pages 1?8, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
McCallum, A., Schultz, K., and Singh, S. (2009).
Factorie: Probabilistic programming via impera-
tively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Nakagawa, T., Inui, K., and Kurohashi, S. (2010).
Dependency tree-based sentiment classification
using crfs with hidden variables. In HLT-NAACL.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2).
Qiu, G., Liu, B., Bu, J., and Chen, C. (2011). Opin-
ion word expansion and target extraction through
double propagation. Computational linguistics,
37(1):9?27.
Ruppenhofer, J., Somasundaran, S., and Wiebe, J.
(2008). Finding the sources and targets of sub-
jective expressions. In Calzolari, N., Choukri, K.,
Maegaard, B., Mariani, J., Odjik, J., Piperidis, S.,
and Tapias, D., editors, Proceedings of the Sixth
International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco. European
Language Resources Association (ELRA).
Sayeed, A. B., Rusk, B., Petrov, M., Nguyen,
H. C., Meyer, T. J., and Weinberg, A. (2011).
Crowdsourcing syntactic relatedness judgements
for opinion mining in the study of information
technology adoption. In Proceedings of the Asso-
ciation for Computational Linguistics 2011 work-
shop on Language Technology for Cultural Her-
itage, Social Sciences, and the Humanities (LaT-
eCH). Association for Computational Linguistics.
Snow, R., O?Connor, B., Jurafsky, D., and Ng, A. Y.
(2008). Cheap and fast?but is it good?: evalu-
ating non-expert annotations for natural language
tasks. In EMNLP 2008.
Somasundaran, S. and Wiebe, J. (2009). Recogniz-
ing stances in online debates. In Proceedings of
695
the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the
AFNLP: Volume 1, ACL ?09.
Wilson, T. (2007). Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity,
Polarity, and Attitudes of private states. PhD the-
sis, Intelligent Systems Program, University of
Pittsburgh.
696
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 69?77,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Crowdsourcing syntactic relatedness judgements for opinion mining in the
study of information technology adoption
Asad B. Sayeed, Bryan Rusk, Martin Petrov,
Hieu C. Nguyen, Timothy J. Meyer
Department of Computer Science
University of Maryland
College Park, MD 20742 USA
asayeed@cs.umd.edu,brusk@umd.edu,
martin@martinpetrov.com,
{hcnguyen88,tmeyer88}@gmail.com
Amy Weinberg
Center for the Advanced
Study of Language
and Department of Linguistics
University of Maryland
College Park, MD 20742 USA
aweinberg@casl.umd.edu
Abstract
We present an end-to-end pipeline including
a user interface for the production of word-
level annotations for an opinion-mining task
in the information technology (IT) domain.
Our pre-annotation pipeline selects candidate
sentences for annotation using results from a
small amount of trained annotation to bias the
random selection over a large corpus. Our
user interface reduces the need for the user to
understand the ?meaning? of opinion in our
domain context, which is related to commu-
nity reaction. It acts as a preliminary buffer
against low-quality annotators. Finally, our
post-annotation pipeline aggregates responses
and applies a more aggressive quality filter.
We present positive results using two differ-
ent evaluation philosophies and discuss how
our design decisions enabled the collection of
high-quality annotations under subjective and
fine-grained conditions.
1 Introduction
Crowdsourcing permits us to use a bank of anony-
mous workers with unknown skill levels to perform
complex tasks given a simple breakdown of these
tasks with user interface design that hides the full
task complexity. Use of these techniques is growing
in the areas of computational linguistics and infor-
mation retrieval, particularly since these fields now
rely on the collection of large datasets for use in ma-
chine learning. Considering the variety of applica-
tions, a variety of datasets is needed, but trained,
known workers are an expense in principle that must
be furnished for each one. Consequently, crowd-
sourcing offers a way to collect this data cheaply and
quickly (Snow et al, 2008; Sayeed et al, 2010a).
We applied crowdsourcing to perform the fine-
grained annotation of a domain-specific corpus. Our
user interface design and our annotator quality con-
trol process allows these anonymous workers to per-
form a highly subjective task in a manner that cor-
relates their collective understanding of the task to
our own expert judgements about it. The path to
success provides some illustration of the pitfalls in-
herent in opinion annotation. Our task is: domain
and application-specific sentiment classification at
the sub-sentence level?at the word level.
1.1 Opinions
For our purposes, we define opinion mining (some-
times known as sentiment analysis) to be the re-
trieval of a triple {source, target, opinion} (Sayeed
et al, 2010b; Pang and Lee, 2008; Kim and Hovy,
2006) in which the source is the entity that origi-
nated the opinionated language, the target is a men-
tion of the entity or concept that is the opinion?s
topic, and the opinion is a value (possibly a struc-
ture) that reflects some kind of emotional orientation
expressed by the source towards the target.
In much of the recent literature on automatic
opinion mining, opinion is at best a gradient be-
tween positive and negative or a binary classifica-
tion thereof; further complexity affects the reliability
of machine-learning techniques (Koppel and Schler,
2006).
We call opinion mining ?fine-grained? when we
are attempting to retrieve potentially many different
69
{source, target, opinion} triples per document. This
is particularly challenging when there are multiple
triples even at a sentence level.
1.2 Corpus-based social science
Our work is part of a larger collaboration with social
scientists to study the diffusion of information tech-
nology (IT) innovations through society by identify-
ing opinion leaders and IT-relevant opinionated lan-
guage (Rogers, 2003). A key hypothesis is that the
language used by opinion leaders causes groups of
others to encourage the spread of the given IT con-
cept in the market.
Since the goal of our exercise is to ascertain the
correlation between the source?s behaviour and that
of others, then it may be more appropriate to look
at opinion analysis with the view that what we are
attempting to discover are the views of an aggregate
reader who may otherwise have an interest in the IT
concept in question. We thus define an expression of
opinion in the following manner:
A expresses opinion about B if an in-
terested third party C?s actions towards B
may be affected by A?s textually recorded
actions, in a context where actions have
positive or negative weight.
This perspective runs counter to a widespread view
(Ruppenhofer et al, 2008) which has assumed a
treatment of opinionated language as an observation
of a latent ?private state? held by the source. This
definition reflects the relationship of sentiment and
opinion with the study of social impact and market
prediction. We return to the question of how to de-
fine opinion in section 6.2.
1.3 Crowdsourcing in sentiment analysis
Paid crowdsourcing is a relatively new trend in com-
putational linguistics. Work exists at the paragraph
and document level, and it exists for the Twitter and
blog genres (Hsueh et al, 2009).
A key problem in crowdsourcing sentiment analy-
sis is the matter of quality control. A crowdsourced
opinion mining task is an attempt to use untrained
annotators over a task that is inherently very subjec-
tive. It is doubly difficult for specialized domains,
since crowdsourcing platforms have no way of di-
rectly recruiting domain experts.
Hsueh et al (2009) present results in quality con-
trol over snippets of political blog posts in a task
classifying them by sentiment and political align-
ment. They find that they can use a measurement of
annotator noise to eliminate low-quality annotations
at this coarse level by reweighting snippet ambigu-
ity scores with noise scores. We demonstrate that we
can use a similar annotator quality measure alone to
eliminate low-quality annotations on a much finer-
grained task.
1.4 Syntactic relatedness
We have a downstream application for this annota-
tion task which involves acquiring patterns in the
distribution of opinion-bearing words and targets us-
ing machine learning (ML) techniques. In partic-
ular, we want to acquire the syntactic relationships
between opinion-bearing words and within-sentence
targets. Supervised ML techniques require gold
standard data annotated in advance.
The Multi-Perspective Question-Answering
(MPQA) newswire corpus (Wilson and Wiebe,
2005) and the J. D. Power & Associates (JDPA)
automotive review blog post (Kessler et al, 2010)
corpus are appropriate because both contain sub-
sentence annotations of sentiment-bearing language
as text spans. In some cases, they also include links
to within-sentence targets. This is an example of an
MPQA annotation:
That was the moment at which the fabric
of compassion tore, and worlds cracked
apart; when the contrast and conflict of
civilisational values became so great as
to remove any sense of common ground -
even on which to do battle.
The italicized portion is intended to reflect a negative
sentiment about the bolded portion. However, while
it is the case that the whole italicized phrase repre-
sents a negative sentiment, ?remove? appears to rep-
resent far more of the negativity than ?common? and
?ground?. While there are techniques that depend
on access to entire phrases, our project is to identify
sentiment spans at the length of a single word.
2 Data source
Our corpus for this task is a collection of arti-
cles from the IT professional magazine, Information
70
Week, from the years 1991 to 2008. This consists
of 33K articles of varying lengths including news
bulletins, full-length magazine features, and opin-
ion columns. We obtained the articles via an institu-
tional subscription, and reformatted them in XML1.
Certain IT concepts are particularly significant in
the context of the social science application. Our tar-
get list consists of 59 IT innovations and concepts.
The list includes plurals, common variations, and
abbreviations. Examples of IT concepts include ?en-
terprise resource planning? and ?customer relation-
ship management?. To avoid introducing confound-
ing factors into our results, we only include explicit
mentions and omit pronominal coreference.
3 User interface
Our user interface (figure 1) uses a drag-and-drop
process through which workers make decisions
about whether particular highlighted words within
a given sentence reflect an opinion about a particu-
lar mentioned IT concept or innovation. The user
is presented with a sentence from the corpus sur-
rounded by some before and after context. Under-
neath the text are four boxes: ?No effect on opin-
ion? (none), ?Affects opinion positively? (postive),
?Affects opinion negatively? (negative), and ?Can?t
tell? (ambiguous).
The worker must drag each highlighted word in
the sentence into one of the boxes, as appropriate. If
the worker cannot determine the appropriate box for
a particular word, she is expected to drag this to the
ambiguous box. The worker is presented with de-
tailed instructions which also remind her that most
of words in the sentence are not actually likely to be
involved in the expression of an opinion about the
relevant IT concept2. The worker is not permitted
to submit the task without dragging all of the high-
lighted words to one of the boxes. When a word
is dragged to a box, the word in context changes
colour; the worker can change her mind by clicking
an X next to the word in the box.
1We will likely be able to provide a sample of sentence data
annotated by our process as a resource once we work out docu-
mentation and distribution issues.
2We discovered when testing the interface that workers can
feel obliged to find a opinion about the selected IT concept. We
reduced it by explicitly reminding them that most words do not
express a relevant opinion and by placing the none box first.
We used CrowdFlower to manage the task with
Amazon Mechanical Turk as its distribution chan-
nel. We set CrowdFlower to present three sentences
at a time to users. Only users with USA-based IP
addresses were permitted to perform the final task.
4 Procedure
In this section, we discuss the data processing
pipeline (figure 3) through which we select candi-
dates for annotations and the crowdsourcing inter-
face we present to the end user for classifying indi-
vidual words into categories that reflect the effect of
the word on the worker.
4.1 Data preparation
4.1.1 Initial annotation
Two social science undergraduate students were
hired to do annotations on Information Week with
the original intention of doing all the annotations
this way. There was a training period where they an-
notated about 60 documents in sets of 20 in iterative
consultation with one of the authors. Then they were
given 142 documents to annotate simultaneously in
order to assess their agreement after training.
Annotation was performed in Atlas.ti, an anno-
tation tool popular with social science researchers.
It was chosen for its familiarity to the social sci-
entists involved in our project and because of their
stated preference for using tools that would allow
them to share annotations with colleagues. Atlas.ti
has limitations, including the inability to create hier-
archical annotations. We overcame these limitations
using a special notation to connect related annota-
tions. An annotator highlights a sentence that she
believes contains an opinion about a mentioned tar-
get on one of the lists. She then highlights the men-
tion of the target and, furthermore, highlights the in-
dividual words that express the opinion about the tar-
get, using the notation to connect related highlights.
4.1.2 Candidate selection
While the use of trained annotators did not pro-
duce reliable results (section 6.2) in acceptable time
frames, we decided to use the annotations in a pro-
cess for selecting candidate sentences for crowd-
sourcing. All 219 sentences that the annotators se-
lected as having opinions about within-sentence IT
71
Figure 1: A work unit presented in grayscale. ?E-business? is the IT concept and would be highlighted in blue. The
words in question are highlighted in gray background and turn red after they are dragged to the boxes.
concepts were concatenated into a single string and
converted into a TFIDF unit vector.
We then selected all the sentences that contain
IT concept mentions from the entire Information
Week corpus using an OpenNLP 1.4.3 model as
our sentence-splitter. This produced approximately
77K sentences. Every sentence was converted into a
TFIDF unit vector, and we took the cosine similar-
ity of each sentence with the TFIDF vector. We then
ranked the sentences by cosine similarity.
4.1.3 Selecting highlighted words
We ran every sentence through the Stanford
part-of-speech tagger. Words that belonged to
open classes such as adjectives and verbs were se-
lected along with certain closed-class words such as
modals and negation words. These candidate words
were highlighted in the worker interface.
We did not want to force workers to classify every
single word in a sentence, because this would be too
tedious. So we instead randomly grouped the high-
lighted words into non-overlapping sets of six. (Re-
mainders less than five were dropped from the task.)
We call these combinations of sentence, six words,
and target IT concept a ?highlight group? (figure 2).
Each highlight group represents a task unit which
we present to the worker in our crowdsourcing ap-
plication. We generated 1000 highlight groups from
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
Figure 2: Two highlight groups consisting of the
same sentence and concept (ERP) but different non-
overlapping sets of candidate words.
the top-ranked sentences.
4.2 Crowdsourced annotation
4.2.1 Training gold
We used CrowdFlower partly because of its au-
tomated quality control process. The bedrock of
this process is the annotation of a small amount of
gold standard data by the task designers. Crowd-
Flower randomly selects gold-annotated tasks and
presents them to workers amidst other unannotated
tasks. Workers are evaluated by the percentage of
gold-annotated tasks they perform correctly. The re-
sult of a worker performing a task unit is called a
?judgement.?
Workers are initially presented their gold-
annotated tasks without knowing that they are an-
swering a test question. If they get the question
wrong, CrowdFlower presents the correct answer to
72
them along with a reason why their answer was an
error. They are permitted to write back to the task
designer if they disagree with the gold judgement.
This process functions in a manner analogous to
the training of a machine-learning system. Further-
more, it permits CrowdFlower to exclude or reject
low-quality results. Judgements from a worker who
slips below 65% correctness are rated as untrustwor-
thy and not included in the CrowdFlower?s results.
We created training gold in the manner recom-
mended by CrowdFlower. We randomly selected
50 highlight groups from the 1000 mentioned in the
previous section. We ran these examples through
CrowdFlower using the interface we discuss in the
next section. Then we used the CrowdFlower gold
editor to select 30 highlight groups that contained
clear classification decisions where it appeared that
the workers were in relative consensus and where we
agreed with their decision. Of these, we designated
only the clearest-cut classifications as gold, leav-
ing more ambiguous-seeming ones up to the users.
For example, in the second highlight group in 2, we
would designate software and systems as none and
extend as positive in the training gold and the re-
mainder as up to the workers. That would be a ?min-
imum effort? to indicate that the worker understands
the task the way we do.
Unfortunately, CrowdFlower has some limita-
tions in the way it processes the responses to gold?
it is not possible to define a minimum effort pre-
cisely. CrowdFlower?s setting either allow us to pass
workers based on getting at least one item in each
class correct or by placing all items in their correct
classes. The latter is too strict a criterion for an in-
herently subjective task. So we accepted the former.
We instead applied our minimum effort criterion in
some of our experiments as described in section 4.3.
4.2.2 Full run
We randomly selected another 200 highlight
groups and posted them at 12 US cents for each set
of three highlight groups, with at least three Me-
chanical Turk workers seeing each highlight group.
The 30 training gold highlight groups were posted
along with them. Including CrowdFlower and Ama-
zon fees, the total cost was approximately 60 USD.
We permitted only USA-based workers to access the
task. Once initiated, the entire task took approxi-
Figure 3: Schematic view of pipeline.
mately 24 hours to complete.
4.3 Post-processing
4.3.1 Aggregation
Each individual worker?s ambiguous annotations
are converted to none annotations, as the ambigu-
ous box is intended as an outlet for a worker?s un-
certainty, but we choose to interpret anything that
a worker considers too uncertain to be classified
as positive or negative as something that is not
strongly opinionated under our definitions.
Aggregation is performed by majority vote of the
annotators on each word in each highlight group. If
no classification obtains more than 50% for a given
word, the word is dropped as too ambiguous to be
accepted either way as a result. This aggregation
has the effect of smoothing out individual annotator
differences.
4.3.2 Extended quality control
While CrowdFlower provides a first-pass quality
control system for selecting annotators who are do-
ing the task in good faith and with some understand-
ing of the instructions, we wanted particularly to
select annotators who would be more likely to be
consistent on the most obvious cases without overly
constraining them. Even with the same general idea
of our intentions, some amount of variation among
the annotators is unavoidable; how do we then reject
annotations from those workers who pass Crowd-
Flower?s liberal criteria but still do not have an idea
of annotation close enough to ours?
73
Our solution was to score the annotators post hoc
by their accuracy on our minimum-effort training
gold data. Then we progressively dropped the worst
n annotators starting from n = 0 and measured the
quality of the aggregated annotations as per the fol-
lowing section.
5 Results
This task can be interpreted in two different ways:
as an annotation task and as a retrieval system. An-
notator reliability is an issue insofar as it is impor-
tant that the annotations themselves conform to a
predetermined standard. However, for the machine
learning task that is downstream in our processing
pipeline, obtaining a consistent pattern is more im-
portant than conformance to an explicit definition.
We can thus interpret the results as being the out-
put of a system whose computational hardware hap-
pens to be a crowd of humans rather than silicon,
considering that the time of the ?run? is compara-
ble to many automated systems; Amazon Mechani-
cal Turk?s slogan is ?artificial artificial intelligence?
for a reason.
Nevertheless, we evaluated our procedure under
both interpretations by comparing against our own
annotations in order to assess the quality of our col-
lection, aggregation, and filtering process:
1. As an annotation task: we use Cohen?s ?
between the aggregated and filtered data vs.
our annotations in the belief that higher above-
chance agreement would imply that the aggre-
gate annotation reflected collective understand-
ing of our definition of sentiment. Consider-
ing the inherently subjective nature of this task
and the interdependencies inherent in within-
sentence judgements, Cohen?s ? is not a defini-
tive proof of success or failure.
2. As a retrieval task: Relative to our own an-
notations, we use the standard information re-
trieval measures of precision, recall, and F-
measure (harmonic mean) as well as accuracy.
We merge positive and negative annotations
into a single opinion-bearing class and measure
whether we can retrieve opinion-bearing words
while minimizing words that are, in context,
not opinion-bearing relative to the given target.
(We do not merge the classes for agreement-
based evaluation as there was not much over-
lap between positive and negative classifica-
tions.) The particular relative difference be-
tween precision and recall will suggest whether
the workers had a consistent collective under-
standing of the task.
It should be noted that the MPQA and the JDPA do
not report Cohen?s ? for subjective text spans partly
for the reason we suggest above: the difficulty of as-
sessing objective agreement on a task in which sub-
jectivity is inherent and desirable. There is also a
large class imbalance problem. Both these efforts
substitute retrieval-based measures into their assess-
ment of agreement.
We annotated a randomly-selected 30 of the 200
highlight groups on our own. Those 30 had 169
annotated words of which 117 were annotated as
none, 35 as positive, and 17 as negative. The re-
sults of our process are summarized in table 1.
In the 30 highlight groups, there were 155 total
words for which a majority consensus (>50%) was
reached. 48 words were determined by us in our
own annotation to have opinion weight (positive or
negative). There are only 22 annotators who passed
CrowdFlower?s quality control.
The stringent filter on workers based on their ac-
curacy on our minimum-effort gold annotations has
a remarkable effect on the results. As we exclude
workers, the F-measure and the Cohen?s ? appear
to rise, up to a point. By definition, each exclu-
sion raises the threshold score for acceptance. As
we cross the 80% threshold, the performance of the
system drops noticeably, as the smoothing effect of
voting is lost. Opinion-bearing words also reduce
in number as the threshold rises as some highlight
groups simply have no one voting for them. We
achieve our best result in terms of Cohen?s ? on
dropping the 7 lowest workers. We achieve our high-
est precision and accuracy after dropping the 10 low-
est workers.
Between the 7th and 10th underperforming an-
notator, we find that precision starts to exceed re-
call, possibly due to the loss of retrievable words as
some highlight groups lose all their annotators. Lost
words can be recovered in another round of annota-
tion.
74
Workers excluded No. of words lost (of 48) Prec/Rec/F Acc Cohen?s ? Score threshold
(prior polarity) N/A 0.87 / 0.38 / 0.53 0.79 -0.26 N/A
0 0 0.64 / 0.71 / 0.67 0.79 0.48 0.333
1 0 0.64 / 0.71 / 0.67 0.79 0.48 0.476
3 0 0.66 / 0.73 / 0.69 0.80 0.51 0.560
5 0 0.69 / 0.73 / 0.71 0.81 0.53 0.674
7 2 0.81 / 0.76 / 0.79 0.86 0.65 0.714
10 9 0.85 / 0.74 / 0.79 0.88 0.54 0.776
12 11 0.68 / 0.68 / 0.68 0.82 0.20 0.820
Table 1: Results by number of workers excluded from the task. The prior polarity baseline comes from a lexicon by
Wilson et al (2005) that is not specific to the IT domain.
6 Discussion
We have been able to show that crowdsourcing a
very fine-grained, domain-specific sentiment analy-
sis task with a nonstandard, application-specific def-
inition of sentiment is possible with careful user in-
terface design and mutliple layers of quality control.
Our techniques succeed on two different interpreta-
tions of the evaluation measure, and we can reclaim
any lost words by re-running the task. We used an
elaborate processing pipeline before and after anno-
tation in order to accomplish this. In this section, we
discuss some aspects of the pipeline that led to the
success of this technique.
6.1 Quality
There are three major aspects of our procedure that
directly affect the quality of our results: the first-
pass quality control in CrowdFlower, the majority-
vote aggregation, and the stringent post hoc filtering
of workers. These interact in particular ways.
The first-pass quality control interacts with the
stringent filter in that even if it were possible to
have run the stringent filter on CrowdFlower itself,
it would probably not have been a good idea. Al-
though we intended the stringent filter to be a min-
imum effort, it would have rejected workers too
quickly. It is technically possible to implement the
stringent filtering directly without the CrowdFlower
built-in control, but that would have entailed spend-
ing an unpredictable amount more money paying for
additional unwanted annotations from workers.
Furthermore, the majority-vote aggregation re-
quires that there not be too few annotators; our re-
sults show that filtering the workers too aggressively
harms the aggregation?s smoothing effect. The les-
son we take from this is that it can be beneficial to
accept some amount of ?bad? with the ?good? in im-
plementing a very subjective crowdsourcing task.
6.2 Design decisions
Our successful technique for identifying opinionated
words was developed after multiple iterations using
other approaches which did not succeed in them-
selves but produced outputs that were amenable to
refinement, and so these techniques became part of
a larger pipeline. However, the reasons why they did
not succeed on their own are illustrative of some of
the challenges in both fine-grained domain-specific
opinion annotation and in annotation via crowd-
sourcing under highly subjective conditions.
6.2.1 Direct annotation
We originally intended to stop with the trained an-
notation we described in 4.1.1, but collecting opin-
ionated sentences in this corpus turned out to be very
slow. Despite repeated training rounds, the annota-
tors had a tendency to miss a large number of sen-
tences that the authors found to be relevant. On dis-
cussion with the annotators, it turned out that the
variable length of the articles made it easy to miss
relevant sentences, particularly in the long feature
articles likely to contain opinionated language?a
kind of ?needle-in-a-haystack? problem.
Even worse, however, the annotators were vari-
ably conservative about what constituted an opinion.
One annotator produced far fewer annotations than
the other one?but the majority of her annotations
were also annotated by the other one. Discussion
with the annotators revealed that one of them simply
had a tighter definition of what constituted an opin-
ion. Attempts to define opinion explicitly for them
still led to a situations in which one was far more
conservative than the other.
75
6.2.2 Cascaded crowdsourcing technique
Insofar as we were looking for training data for
use in downstream machine learning techniques,
getting uniform sentence-by-sentence coverage of
the corpus was not necessary. There are 77K sen-
tences in this corpus which mention the relevant IT
concepts; even if only a fraction of them mention the
IT concepts with opinionated language, we would
still have a potentially rich source of training data.
Nevertheless the direct annotation with trained
annotators provided data for selecting candidate sen-
tences for a more rapid annotation. We used the
process in section 4.1.2 and chose the top-ranked
sentences. Then we constructed a task design that
divided the annotation into two phases. In the first
phase, for each candidate sentence, we ask the anno-
tator whether or not the sentence contains opinion-
ated language about the mentioned IT concept. (We
permit ?unsure? answers.)
In the second phase, for each candidate sentence
for which a majority vote of annotators decided that
the sentence contained a relevant opinion, we run
a second task asking whether particular words (se-
lected as per section 4.1.3) were words directly in-
volved in the expression of the opinion.
We tested this process with the 90 top-ranked
sentences. Four individuals in our laboratory an-
swered the ?yes/no/unsure? question of the first
phase. However, when we took their pairwise Co-
hen?s ? score, no two got more than approximately
0.4. We also took majority votes of each subset of
three annotators and found the Cohen?s ? between
them and the fourth. The highest score was 0.7, but
the score was not stable, and we could not trust the
results enough to move onto the second phase.
We also ran this first phase through Amazon Me-
chanical Turk. It turned out that it was far too easy
to cheat on this yes/no question, and some workers
simply answered ?yes? or ?no? all the time. Agree-
ment scores of a Turker majority vote vs. one of the
authors turned out to yield a Cohen?s ? of 0.05?
completely unacceptable.
Discussion with the in-laboratory annotators sug-
gested the roots of the problem: it was the same
problem as with the direct Atlas.ti annotation we re-
ported in the previous section. It was very difficult
for them to agree on what it meant for a sentence to
contain an opinion expressed about a particular con-
cept. Opinions about the nature of opinion ranged
from very ?conservative? to very ?liberal.? Even
explicit definition with examples led annotators to
reach very different conclusions. Furthermore, the
longer the annotators thought about it, the more con-
fused and uncertain they were about the criterion.
What is an opinion can itself be a matter of opin-
ion. It became clear that without very tight review
of annotation and careful task design, asking users
an explicit yes/no question about whether a particu-
lar concept has a particular opinion mentioned in a
particular sentence has the potential to induce over-
thinking by annotators, despite our variations on the
task. The difficulty may also lead to a tendency to
cheat. Crowdsourcing allows us to make use of non-
expert labour on difficult tasks if we can break the
tasks down into simple questions and aggregate non-
expert responses, but we needed a somewhat more
complex task design in order to eliminate the diffi-
culty of the task and the tendency to cheat.
7 Future work
Foremost among the avenues for future work is ex-
perimentation with other vote aggregration and post
hoc filtering schemes. For example, one type of ex-
periment could be the reweighting of votes by an-
notator quality rather than the wholesale dropping
of annotators. Another could involve the use of
general-purpose sentiment analysis lexica to bias the
vote aggregation in the manner of work in sentiment
domain transfer (Tan et al, 2007).
This work also points to the potential for crowd-
sourcing in computational linguistics applications
beyond opinion mining. Our task is a sentiment-
specific instance of a large class of syntactic relat-
edness problems that may suitable for crowdsourc-
ing. One practical application would be in obtaining
training data for coreference detection. Another one
may be in the establishment of empirical support for
theories about syntactic structure.
Acknowledgements
This paper is based on work supported by the Na-
tional Science Foundation under grant IIS-0729459.
76
References
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, HLT ?09, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA sent-
ment corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media Data
Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Moshe Koppel and Jonathan Schler. 2006. The im-
portance of neutral examples for learning sentiment.
Computational Intelligence, 22(2).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2).
Everett M. Rogers. 2003. Diffusion of Innovations, 5th
Edition. Free Press.
Josef Ruppenhofer, Swapna Somasundaran, and Janyce
Wiebe. 2008. Finding the sources and targets of
subjective expressions. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08), Marrakech, Morocco. Eu-
ropean Language Resources Association (ELRA).
Asad B. Sayeed, Timothy J. Meyer, Hieu C. Nguyen,
Olivia Buzek, and Amy Weinberg. 2010a. Crowd-
sourcing the evaluation of a domain-adapted named
entity recognition system. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Asad B. Sayeed, Hieu C. Nguyen, Timothy J. Meyer, and
Amy Weinberg. 2010b. Expresses-an-opinion-about:
using corpus statistics in an information extraction ap-
proach to opinion mining. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, New York, NY, USA.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In CorpusAnno ?05:
Proceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association for
Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
77
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 57?65,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
The semantic augmentation of a psycholinguistically-motivated syntactic
formalism
Asad Sayeed and Vera Demberg
Computational Linguistics and Phonetics / M2CI Cluster of Excellence
Saarland University
66123 Saarbru?cken, Germany
{asayeed,vera}@coli.uni-saarland.de
Abstract
We augment an existing TAG-based in-
cremental syntactic formalism, PLTAG,
with a semantic component designed to
support the simultaneous modeling ef-
fects of thematic fit as well as syntactic
and semantic predictions. PLTAG is a
psycholinguistically-motivated formalism
which extends the standard TAG opera-
tions with a prediction and verification
mechanism and has experimental support
as a model of syntactic processing diffi-
culty. We focus on the problem of for-
mally modelling semantic role prediction
in the context of an incremental parse
and describe a flexible neo-Davidsonian
formalism and composition procedure to
accompany a PLTAG parse. To this
end, we also provide a means of aug-
menting the PLTAG lexicon with seman-
tic annotation. To illustrate this, we run
through an experimentally-relevant model
case, wherein the resolution of semantic
role ambiguities influences the resolution
of syntactic ambiguities and vice versa.
1 Introduction
PLTAG (PsychoLinguistically-motivated TAG,
Demberg and Keller, 2008; Demberg et al, 2014)
is a variant of Tree-Adjoining Grammar (TAG)
which is designed to allow the construction of
TAG parsers that enforce strict incrementality and
full connectedness through (1) constraints on the
order of operations, (2) a new type of unlexical-
ized tree, so-called prediction trees, and (3) a veri-
fication mechanism that matches up and extends
predicted structures with later evidence. Psy-
cholinguistic evaluation has shown that PLTAG
operations can be used to predict data from eye-
tracking experiments, lending this syntactic for-
malism greater psycholinguistic support.
Syntax, however, may not just be the skeleton of
a linguistic construction that bears semantic con-
tent: there is some evidence that syntactic struc-
ture and semantic plausibility interact with each
other. In a strongly interactive view, we would ex-
pect that semantic plausibility could directly affect
the syntactic expectations. Consider the sentences:
(1) a. The woman slid the butter to the man.
b. The woman slid the man the butter.
The ditransitive verb ?to slide? provides three
roles for participants in the predicate: agent, pa-
tient, and recipient. In both cases, ?the woman?
fills the agent role, ?the butter? the patient, and
?the man? the recipient. However, they do not gen-
erally fill all roles equally well. English-speakers
have the intuition that ?the butter? should neither
be an agent nor a recipient under normal circum-
stances. Likewise, ?the man? is not a typical pa-
tient in this situation. If there is a psycholinguis-
tic effect of semantic plausibility, we would ex-
pect that an incomplete sentence like ?The woman
slid the butter? would generate an expectation in
the listener of a PO construction (rather than DO)
with preposition ?to?, as well as an expectation of
a noun phrase and an expectation that that noun
phrase would belong to the class of entities that
are plausible recipients for entities that are slid.
If this is the case, then there is not only a syntac-
tic expectation at this point but a semantic expecta-
tion that is in turn informed by the syntactic struc-
ture and semantic content up to that point. Con-
structing a model that is formally rich, psycholin-
guistically plausible, and empirically robust re-
quires making design decisions about the specific
relationship between syntax and semantics and the
overall level of formal articulation on which the
statistical model rests. For PLTAG, we are inter-
ested in preserving as many of its syntactic charac-
teristics as are necessary to model the phenomena
that it already does (Demberg and Keller, 2009).
57
In the rest of this paper, we therefore present a
semantic augmentation of PLTAG that is based on
neo-Davidsonian event semantics and is capable
of supporting incrementality and prediction.
2 Psycholinguistic background
Does thematic fit dynamically influence the choice
of preferred syntactic structures, does it shape pre-
dictions of upcoming semantic sorts, and can we
measure this experimentally?
A classic study (Altmann and Kamide, 1999)
about the influence of thematic fit on predictions
showed that listeners can predict the complement
of a verb based on its selectional restrictions. Par-
ticipants heard sentences such as:
(2) a. The boy will eat the cake.
b. The boy will move the cake.
while viewing images that depicted sets of rele-
vant objects, in this example, a cake, a train set,
a ball, and a model car. Altmann and Kamide
(1999) monitored participants? eye-movements
while they heard the sentences and found an in-
creased number of looks to the cake during the
word eat compared the control condition, i.e., dur-
ing the word move (only the cake is edible, but
all depicted objects are movable). This indicates
that selectional preference information provided
by the verb is not only used as soon as it is avail-
able (i.e., incremental processing takes place), but
this information also triggers the prediction of up-
coming arguments of the verb. Subsequent work
has demonstrated that this is not a simple associ-
ation effect of eat and the edible item cake, but
that people assign syntactic roles rapidly based on
case marking and that missing obligatory thematic
role fillers are predicted; in a German visual world
study, Kamide et al (2003a) presented participants
with a scene containing a cabbage, a hare, a fox
and a distractor object while they heard sentences
like
(3) a. Der Hase frisst gleich den Kohl.
(The harenom will eat soon the cabbageacc.)
b. Den Hasen frisst gleich der Fuchs.?
(The hareacc will eat soon the foxnom.)
They found that, during the verb-adverb region,
people looked more to the cabbage in the first con-
dition and correctly anticipated the fox in the sec-
ond condition. This means that they were able to
correctly anticipate the filler of the missing the-
matic role. Kamide et al (2003b) furthermore
showed that role prediction is not only restricted
to the immediately-following grammatical object,
but that goals as in The woman slid the butter to
the man are also anticipated.
Thematic fit furthermore seems to interact with
syntactic structure. Consider the sentences in (4),
which are locally ambiguous with respect to a
main clause interpretation or a reduced relative
clause.
(4) a. The doctor sent for the patient arrived.
b. The flowers sent for the patient arrived.
Comprehenders incur decreased processing diffi-
culty in sentences like (4-b) compared to (4-a),
due to flowers not being a good thematic fit for
the agent role of sending (Steedman, 2000).
Taken together, the experimental evidence sug-
gests that semantic information in the form of
thematic fit can influence the syntactic structures
maintained by the comprehender and that peo-
ple do generate anticipations not only based on
the syntactic requirements of a sentence, but also
in terms of thematic roles. While there is evi-
dence that both syntactic and semantic process-
ing is rapid and incremental, there remain, how-
ever, some open questions on how closely syn-
tactic and semantic processing are integrated with
each other. The architecture suggested here mod-
els the parallel, highly incremental construction of
syntactic and semantic structure, but leaves open
to exploration the question of how quickly and
strongly they interact with each other. Note that
with the present architecture, thematic fit would
only be calculated for word pairs which stand in
a possible syntactic relation. The syntax thus ex-
erts strong constraints on which plausibilities are
considered. Our example in section 6.2 illustrates
how even a tight form of direct interaction between
syntax and semantics can be modelled.
3 Relation to previous work on joint
syntactic-semantic models
Previous attempts have been made to combine
the likelihood of syntactic structure and seman-
tic plausibility estimates into one model for pre-
dicting human processing difficulty (Pado? et al,
2009; Jurafsky, 2002). Pado? et al (2009) pre-
dict increased difficulty when the preferred syn-
tactic analysis is incompatible with the analysis
that would have the best thematic fit. They inte-
grate syntactic and semantic models as a weighted
combination of plausibility scores. The syntactic
58
and semantic models are computed to some extent
independently of one another, and then the result
is adjusted by a set of functions that take into ac-
count conflicts between the models. In relation to
the approach proposed here, it is also important
to note that the semantic components in (Pado? et
al., 2009; Jurafsky, 2002) are limited to semantic
role information, while the architecture proposed
in this paper can build complete semantic expres-
sions for a sentence. Furthermore, these models
do not model the prediction and verification pro-
cess (in particular, they do not make any seman-
tic role predictions of upcoming input) which has
been observed in human language processing.
Mitchell et al (2010) propose an integrated
measure of syntactic and semantic surprisal as a
model of processing difficulty, and show that the
semantic component improves modelling results
over a syntax-only model. However, the syntactic
and semantic surprisal components are only very
loosely integrated with one another, as the seman-
tic model is a distributional bag-of-words model
which does not take syntax into account.
Finally, the syntactic model underlying (Pado? et
al., 2009; Mitchell et al, 2010) is an incremental
top-down PCFG parser (Roark, 2001), which due
to its parsing strategy fails to predict human pro-
cessing difficulty that arises in certain cases, such
as for center embedding (Thompson et al, 1991;
Resnik, 1992). Using the PLTAG parsing model is
thus more psycholinguistically adequate.
3.1 Towards a broad-coverage integration of
syntax and semantics
The current paper does not propose a new model
of sentence processing difficulty, but rather ex-
plores the formal architecture and mechanism nec-
essary to enable the future implementation of an
integrated syntactic-semantic model. A syntax-
informed semantic surprisal component imple-
mented using distributional semantics could use
the semantic expressions generated during the
PLTAG semantics construction to determine what
words (in which relationships to the current word)
from the previous context to condition on for cal-
culating semantic surprisal.
4 PLTAG syntax
PLTAG uses the standard operations of TAG: sub-
stitution and adjunction. The order in which they
are applied during a parse is constrained by in-
crementality. This also implies that, in addition
to the standard operations, there are reverse Up
versions of these operations where the prefix tree
is substituted or adjoined into a new elementary
tree (see figure 4). In order to achieve strict incre-
mentality and full connectedness at the same time
while still using linguistically motivated elemen-
tary trees, PLTAG has an additional type of (usu-
ally) unlexicalized elementary tree called predic-
tion trees. Each node in a prediction tree is marked
with upper and/or lower indices kk to indicate its
predictive status. Examples for prediction trees
are given at the right hand side of figure 5b. The
availability of prediction trees enable a sentence
starting with ?The thief quickly? to integrate both
the NP (?The thief?) and the ADVP (?quickly?)
into the derivation even though neither type of el-
ementary tree can be substituted or adjoined to the
other?the system predicts an S tree to which both
can be attached, but no specific verb head. Pre-
diction markers can be removed from nodes via
the verification operation, which makes sure that
predicted structure is matched against actually ob-
served evidence from the input string. For the ex-
ample above, the verb ran in ?The thief quickly
ran? verifies the predicted verb structure. In fig-
ures 5c through 5e, we also provide an example of
prediction and verification as part of the demon-
stration of our semantic framework. Other foun-
dational work on PLTAG (Demberg-Winterfors,
2010) contains more detailed description.
5 Neo-Davidsonian semantics
Davidsonian semantics organizes the representa-
tion of predicates around existentially-quantified
event variables (e). Sentences are therefore treated
as descriptions of these events, leading to a less
recursive representation where predicates are not
deeply embedded inside one another. Highly
recursive representations can be incrementality-
unfriendly, potentially requiring complex infer-
ence rules to ?undo? recursive structures if rele-
vant information arrives later in the sentence.
Neo-Davidsonian semantics (Parsons, 1990;
Hunter, 2009) is an extension of Davidsonian
semantics wherein the semantic roles are also
separated out into their own first-order predi-
cates, rather than being fixed arguments of the
main predicate of the verb. This enables a sin-
gle verb predicate to correspond to multiple pos-
sible arrangements of role predicates, also an
59
incrementality-friendly characteristic1. The Neo-
Davidsonian representation allows us separate the
semantic prediction of a role from its syntactic ful-
fillment, permitting the type of flexible framework
we are proposing in this paper.
We adopt a neo-Davidsonian approach to se-
mantics by a formalism that bears similarity to ex-
isting frameworks such as (R)MRS (Robust Min-
imal Recursion Semantics) (Copestake, 2007).
However, this paper is intended to explore what
architecture is minimally required to augment the
PLTAG syntactic framework, so we do not adopt
these existing frameworks wholesale. Our ex-
amples such as figures 4, 5d, and several others
demonstrate how this looks in practice.
6 Semantics for PLTAG
6.1 Semantic augmentation for the lexicon
Constructing the lexicon for a semantically aug-
mented PLTAG uses a process based on the one
for ?purely syntactic? PLTAG. The PLTAG lex-
icon is extracted automatically from the PLTAG
treebank, which has been derived from the Penn
Treebank using heuristics for binarizing flat struc-
tures as well as additional noun phrase annotations
(Vadas and Curran, 2007), PropBank (Palmer et
al., 2003), and a slightly modified version of
the head percolation table of Magerman (1994).
PLTAG trees in the treebank are annotated with
syntactic headedness information as well as infor-
mation that allows one to distinguish arguments
and modifiers.
Given the PLTAG treebank, we extract the
canonical lexicon using well-established ap-
proaches from the LTAG literature (in particular
(Xia et al, 2000): we traverse the converted tree
from each leaf up towards the root, as long as the
parental node is the head child of its parent. If a
subtree is not the head child of its parent, we ex-
tract it as an elementary tree and proceed in this
way for each word of the converted tree. Given the
argument/modifier distinction, we then create sub-
stitution nodes in the parent tree for arguments or
a root and foot node in the child tree for modifiers.
Prediction trees are extracted automatically by cal-
culating the minimal amount of structure needed
to connect each word into a structure including all
previous words of the sentence2. The parts of this
1Consider the optionality of the agent role in passive sen-
tences, where the ?by-phrase? may or may not appear.
2The reader is referred to (Demberg-Winterfors, 2010;
S
{?e&? = e}
NP?
{Q1x1
ARG0(e, x1)}
VP
{e}
V
likes
{Like(e)}
NP?
{Q2x2
ARG1(e, x2)}
NP
{?e}
NP*
{Q1x1
ARG0(e, x1)
&? = x1
&? = Q1}
VP
{e}
V
including
{Include(e)}
NP?
{Q2x2
ARG1(e, x2)}
Figure 1: Verbal elementary trees extracted from
example sentence Pete likes sugary drinks includ-
ing alcoholic ones.
minimally-needed connecting syntactic structure
which belong to heads to the right of the current
word are stored in the lexicon as prediction trees,
c.f. right hand side of figure 5b.
Since Propbank is used in the construction pro-
cess of the PLTAG treebank, we can straightfor-
wardly display the semantic role annotation on the
tree and the extracted lexicon, with the exception
that we display role annotations for PPs on their
NP child. For arguments, annotations are retained
on the substitution node in the parental tree, while
for modifiers, the role annotation is displayed on
the foot node of the auxiliary tree, as shown for the
verbal trees extracted from the sentence Pete likes
sugary drinks including alcoholic ones in Figure
1. PropBank assigns two roles to the NP node
above sugary drinks (it is the ARG1 of likes and
the ARG0 of including), but we can correctly tease
apart these annotations in the lexical extraction
process using the syntactic annotation and argu-
ment/modifier distinction.
Using the same procedure, prediction trees are
annotated with semantic roles. It can then happen
that one form of a prediction tree is annotated with
different syntactic roles, hence introducing some
additional ambiguity into the lexicon. For exam-
ple, the NP substitution node in subject position of
the prediction tree rooted in Sk in figure 5b could
be an ARG0 for some verbs which can verify this
tree and an ARG1 for others.
PLTAG elementary trees can contain one or
more lexemes, where the first lexeme is the el-
ementary tree?s main anchor, and all further lex-
emes are predicted. In earlier PLTAG extractions,
elementary trees with several lexemes were used
for particle verbs like show up and some hand-
coded constructions in which the first part is pre-
dictive of the second part, such as either . . . or or
both . . . and. Here we extend this set of trees with
Demberg et al, 2014) for full details of the PLTAG conver-
sion and syntactic part of the lexicon extraction process.
60
more than one lexeme to verbs with subcatego-
rized PPs, as shown, for example, in the second
lexicon entry of slid in figure 5a. Note the differ-
ence to the lexicon entry of optional PPs in figure
5b as in on Sunday. Furthermore,
? All elementary trees which have a role anno-
tation in PropBank also have a correspond-
ing annotation ?e on their root node that
represents the existentially-quantified neo-
Davidsonian event variable for that predicate,
see fig. 1.
? The event variables and entity variables on an
elementary tree are available for binding on
the path from the anchor3 of the elementary
tree to the root node.
? Every role annotation on a node is in the form
of a predicate ARGn(e, x), where e is the
event variable, and x is an entity variable to
which the role is conferred.
? Every role annotation is prefixed with a vari-
able binding Qx, where Q is a higher-order
variable that represents an unknown quanti-
fier. This ensures that all variables are bound
if a role appears before its filler.
? Every elementary tree for an open-class word
has a head with corresponding predicate. For
example, ?butter? has a predicate Butter(x).
? Prediction trees for open lexical classes (such
as NPs) have a head with a (x) predicate.
? Every nominal elementary tree has a Qx at
the root node so that the entity variable that
is the argument to the predicate on the head
is bound. The Qx is on the root node so that
our semantic processing procedure for substi-
tutions and adjunctions (described in the next
section) can unify the entity variable x with
variables on higher trees.
For PPs, we obtain role annotations from Prop-
Bank and NomBank. Other closed-class syntactic
types such as pronouns have appropriately-
selected quantifier constants and predicates
(e.g. ?someone? would be represented as
?xPerson(x)&? = ?&? = x, see next paragraph
for the use of question marks). Determiners are
merely annotated with a quantifier ?constant?
symbol and no variables or predicates.
Then we require a type of additional annota-
tion to which we refer as a ?variable assignment
statement?, which we use in our syntactic com-
3Lowest node on the path to where the anchor would be
in a prediction tree which does not have a lexical anchor.
bination process. These statements are written
? = v, where v is either a quantifier variable
(Q) or constant (e.g. ?) or an entity variable (x).
These statements represent the possibility that an
incoming tree might have a variable v that could
have the same binding as one already in the pre-
fix tree. Variable assignment statements occur on
root nodes or foot nodes, except where there is a
descendent DT subsitution node, which receives
an additional ? = Q statement. The type of vari-
able assignment statement (event, entity or quan-
tifier) depends on the root node type (entity type
like NP or N vs. event type like S or VP), as shown
in figure 1. The next section describes the use of
these statements in semantic parsing. Note that
variable assignment statements need not be rep-
resented explicitly in an implementation, as reas-
signing variables can be done via references or
other data structures. We use them as a represen-
tational and illustrative convenience here.
6.2 Semantic parsing procedure
We integrate semantics into the overall process of
PLTAG parsing by the rules in figures 2 and 3. In
addition, we provide a more procedural descrip-
tion here. At the highest level, a step in an incre-
mental parse follows this pattern:
1. On scanning a new word or doing a predic-
tion step, the PLTAG statistical model selects
a tree from the lexicon, an operation (substi-
tion, adjunction, verification), and a position
in the prefix tree at which to insert the tree (or
none, if this is the first word).
2. All the nodes of the incoming tree are vis-
ited by the visit operation, and their semantic
content is appended as conjuncts to the out-
put semantic expression.
3. The operation of attaching the new tree into
the derived tree is performed (pltagOp):
(a) Variable assignment statements are
emitted and appended to the semantic
output expression according to the
rules in figure 3, as well as to the
semantic expression at the syntactic
node at which the integration occurs.
For verification, the Verify rule has to
be applied to all nodes that are verified.
(b) The syntactic integration of merging the
nodes at the substitution or adjunction
site is performed. The rules in 3 also
make sure that the semantic expressions
61
D : {?} T
PltagStep
pltagOp(D,T ) : {?&visit(T )}
D : {?}
Resolve
D : resolveEqns(?)
Figure 2: Overall rules for trees (T ) and derivations (D) and overall semantic expressions (?). PltagStep
applies when a new tree is chosen to be integrated with the prefix tree.
N1 ?: {?1, Q1,?2} N2 ?: {?3, ? = Q2,?4} D : {?}
QuantEquate
D[N1 7? nodeMerge(N1 : {?1, Q1,?2}, N2 : {?3,?4})] : {?&Q1 = Q2}
N1 ?: {?1, x1,?2} N2 ?: {?3, ? = x2,?4} D : {?}
VarEquate
D[N1 7? nodeMerge(N1 : {?1, x1,?2}, N2 : {?3,?4})] : {?&x1 = x2}
N1
p
p : {?1, 1,?2} N2 : {?3} anchor(N2):{?4, Pred(x),?5} D : {?}
Verify
D[N1 7? nodeMerge(N1 : {?1, 1,?2}, N2 : {?3})] : {?& 1 = Pred}
Figure 3: Rules for combining nodes. The nodes are attached during the derivation via the nodeMerge
operation, with N1 being the node above (?), and N2 being the node below (?). These hold for substi-
tution and adjunction (for both canonical and prediction trees). The underlying intuition is that the (?)
node will contain the variable equation, and the (?) node will contain the mention of a variable to be
equated. The Verify rule equates the variable with the predicate of the verification tree. The equation
is appended to the output expression ?. Q2 can also be ? or another quantifier. VarEquate also applies to
event variables. The ?n notation represents the prefixes and suffixes of the semantic expressions relative
to the mentioned variable or statement. The rules delete the variable assignment statement from the node
by concatenating ?3 and ?4.
from both nodes involved in the integra-
tion are included in the semantic expres-
sion of the merged node.
4. Optionally, a Resolve step is applied, which
eliminates variable assignment statements by
replacing variable mentions with their most
concrete realization.
Regarding variable assignments at the integra-
tion of two trees, the value for quantifier vari-
ables can be a constant in the form of a quanti-
fier. Entity variables can be equated with other
entity variables, and entity constants (e.g., proper
names) are a relatively simple extension to the
rules4. Verification variables can only be equated
with a constant?a predicate name.
We present an example of the processing of
a substitution step in figure 4. The S tree for
sleeps with an open NP substitution node is in
the process of having the NP ?someone? substi-
tuted into it using the substUp operation. So we
have already done step 1 of our parsing procedure.
Step 2 is visit, such that the semantic expression
of the NP is appended to the output expression
4A noun phrase like ?Peter? will have the associated se-
mantic expression peter&? = peter and will require an ad-
ditional inference rule to remove the quantifier when it is
adjoined or substituted to a node carrying a role. In other
words, substituting peter into QxARG1(e, x) should result
in ARG1(e, peter). An analogous rule for constant verifi-
cation that allows Qx (x) to be verified as peter is also
required.
?. For step 3, the variable assignment statements
are then processed by application of QuantEquate
and VarEquate. Finally in step 4, the expression is
simplified with Resolve.
The Resolve operation. From an implementa-
tion perspective, resolving variable assignment
statements does not really need a separate oper-
ation, as references can be maintained such that
the assignment is automatically performed with-
out any explicit substitution in the manner of a
Prolog inference engine?s resolution procedure.
The same holds for the variable assignment state-
ments. However, we include explicit mention of
this mechanism for ease of expression of the se-
mantic operations as well as to illustrate some de-
gree of convergence with existing formalisms such
as (R)MRS, which also has a mechanism to assert
relationships between variables post hoc.
There is only one condition under which ap-
plication of Resolve can fail, which is if there is
more than one assignment statement connecting
the same variable to different constants.
The Resolve rule is defined to be able to apply
to the entire output expression. When should it
apply? It is defined such that it can be applied at
any time; its actual execution will be controlled
by the parsing algorithm, e.g., after each parsing
operation or at the end of the parse.
There are remaining matters of quantifier scope
62
NP
{?x1Person(x1)
&? = x1&? = ?}
PRO
someone
substUp
?????? S
{Ee}
NP?
{Q0x0ARG0(e, x0)}
VP
sleeps
(Syntactic view)
? = ?eQ0x0ARG0(e, x0)
(Before substitution starts)
? = ?eQ0x0ARG0(e, x0)
&?x1Person(x1)&? = x1&? = ?
(Result of visit)
? = ?eQ0x0ARG0(e, x0)
&?x1Person(x1)&? = x1&? = ?
&Q0 = ?&x0 = x1
(Result of QuantEquate and VarEquate)
? = ?e?x0ARG0(e, x0)&Person(x0)
(Result of Resolve)
Figure 4: An example incremental step from the
semantic perspective.
and semantic well-formedness that must be han-
dled post hoc at every step. For example, univer-
sal quantifiers require a distinction to be made be-
tween the restrictor of the quantified variable and
the nuclear scope. It is possible within a neo-
Davidsonian representation to perform such rep-
resentational adjustments easily, as shown by Say-
eed and Demberg (2012).
Example Now that we have described the pro-
cedure, we provide an example of how this se-
mantic augmentation of PLTAG can represent role
labeling and prediction inside the syntactic pars-
ing system. We perform a relevant segment of the
parse of example (1-a), ?The woman slid the but-
ter to the man.? In this sentence, we expect that the
parser will already know the expected role of the
NP ?the man? before it actually receives it. That is,
it will know in advance that there is an upcoming
NP to be predicted such that it is compatible with a
recipient (ARG2) role, and this knowledge will be
represented in the incremental output expression.
The minimum lexicon required for our example
is contained in figures 5a and 5b. For our illustra-
tion, we only include the ditransitive alternation of
?slide?. Both versions of slide contain all the roles
on NP nodes. This parse involves only the predic-
tion of noun phrases, so we only have an NP pre-
diction tree. We presume for the sake of simplicity
that the determiner ?the? represents the existential
quantifier ?.
Our parse begins in figure 5c with ?The woman
slid?, since these are the same in both cases, and
it proceeds up to figure 5e with the sentence ?The
woman slid the butter to the man?. We Resolve
the assignments at every step for brevity in the ex-
amples, and we also apply it to the nodes. By fig-
ure 5d, the parser already knows that the ARG2 of
?slide? is what is sought. Finally, by figure 5e, the
appropriate NP is expected by prediction.
7 Discussion and conclusions
We demonstrated how syntactic prediction and
thematic roles can interact in our framework, but
we did so with a simple example of prediction:
a single noun phrase. Our framework is, how-
ever, able to accomodate more complex interac-
tions. In particular, we want to draw attention
to an example which can not be modelled by
other formalisms which are not fully connected
like PLTAG. Consider sentences beginning with
?The victim/criminal was violently. . . ?. Does the
semantic association between ?victim? vs. ?crimi-
nal? and ?violently? change the likelihoods of the
semantic roles that can be assigned to the subject
NP? Does it make an active or a passive voice
verb more likely after ?violently?? These are the
kinds of possible syntactic-semantic interactions
for which one will need a flexible but robust for-
malism such as we have described in this paper:
the prediction mechanism allows dependents to
jointly affect the expectation of a head even before
the head has been encountered. Note that these
interactions can also go beyond thematic roles.
In this paper, we have presented a procedure
to augment a treebank-extracted PLTAG lexicon
with semantic annotations based in a flexible neo-
Davidsonian theory of events. Then we have
provided the way to combine these representa-
tions during incremental parsing in a manner fully
synchronized with the existing PLTAG syntactic
operations. We demonstrated that we can rep-
resent thematic role prediction in a case that is
known to be relevant to an on-going stream of psy-
cholinguistic research. Ongoing and future work
includes the development of a joint syntactic-
semantic statistical model for PLTAG and experi-
mental validation of predictions made by our se-
mantic augmentation. We are also considering
higher-order semantic issues such as quantifier
scope underspecification in the context of our for-
malism (Koller et al, 2003).
63
S
{?e? = e}
NP?
{Q0x0ARG0(e, x0)}
VP
{e}
V
slid
{Slid(e)}
NP?
{Q2x2ARG2(e, x2)}
NP?
{Q1x1ARG1(e, x1)}
S
{?e? = e}
NP?
{Q0x0ARG0(e, x0)}
VP
{e}
V
slid
{Slid(e)}
NP?
{Q1x1ARG1(e, x1)}
PP
TOk
tokk
NP?
{Q2x2ARG2(e, x2)}
(a) Lexicon: ditransitive alternation of slid.
NP
{Qx? = Q&? = x}
DT?
{? = Q}
N
woman | man | butter
{Woman(x)
|Man(x)
|Butter(x)}
DT
{?}
the
TO
to
VP
VP*
{?e? = e}
PP
P
on
NP?
{ARGM-TEMP(e, x)}
Sk
{?e? = e}
NPk ?
{Q1x1ARG0(x1)}
VPkk
{ (e)}
NPk
{Qx? = Q&? = x}
DTk ?
{? = Q}
Nkk
{ (x)}
(b) Lexicalized trees and prediction trees.
S
{?e? = e}
NP?
{?x0ARG0(e, x0)}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP1
{Q2x2ARG2(e, x2)}
DT1 ?
{? = Q2}
N11
{ (x2)}
NP?
{Q1x1ARG1(e, x1)}
S
{?e? = e}
NP?
{?x0ARG0(e, x0)}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP1
{Q1x1ARG1(e, x1)}
DT1 ?
{? = Q1}
N11
{ (x1)}
PP
TOk
tokk
NP?
{Q2x2ARG2(e, x2)}
?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e) ?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)
&Q2x2ARG2(e, x2)&? = Q2& (x2)&Q1x1ARG1(e, x1) &Q1x1ARG1(e, x1)&? = Q1& (x1)&Q2x2ARG2(e, x2)
(c) Parse of ?The woman slid? with respect to the ditransitive alternation, with the syntactic prediction of an NP. Two possibilities
still remain. The semantics are identical except for the role of the predicted nominal predicate. The ? = e variable assignment
statement persists through the derivation, representing the possibility that this sentence is embedded in another.
S
{?e? = e}
NP?
{?x0ARG0(e, x0)}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP
{?x1ARG1(e, x1)}
DT
{?}
the
N
butter
{Butter(x1)}
PP
TOk
tokk
NP?
{Q2x2ARG2(e, x2)}
?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)
&?x1ARG1(e, x1)&Butter(x1)&Qx2ARG2(e, x2)
(d) Parse of ?The woman slid the butter. . . ?. The arrival of
?the butter? greatly reduces the likelihood of the recipient
role (ARG2) being the one filled at this point, effectively
abolishing the first parse.
S
{?e? = e}
NP?
{?x0 . . .}
DT
{?}
the
N
woman
{Woman(x0)}
VP
{e}
V
slid
{Slid(e)}
NP
{?x1 . . .}
DT
{?}
the
N
butter
{Butter(x1)}
PP
TO
to
NP2
{Q2x2 . . .}
DT2 ?
{? = Q2}
N22
{ (x2)}
?e? = e&?x0ARG0(e, x0)&Woman(x0)&Slid(e)
&?x1ARG1(e, x1)&Butter(x1)&Qx2ARG2(e, x2)
&? = Q2& (x2)
(e) Parse of ?The woman slid the butter to. . . ?. to is verified
and the last NP is expanded via prediction. This gives us
the last predicted predicate in the semantic expression. It
shares its variable with the ARG2 role, thus thematically
restricting its possible verifications.
Figure 5: Excerpt of our example parse.
64
References
Gerry Altmann and Yuki Kamide. 1999. Incremen-
tal interpretation at verbs: Restricting the domain of
subsequent reference. Cognition, 73(3):247?264.
Ann Copestake. 2007. Semantic composition with (ro-
bust) minimal recursion semantics. In Proc. of the
Workshop on Deep Linguistic Processing.
Vera Demberg and Frank Keller. 2008. A psycholin-
guistically motivated version of tag. In Proceedings
of the 9th International Workshop on Tree Adjoin-
ing Grammars and Related Formalisms. Tu?bingen,
pages 25?32.
Vera Demberg and Frank Keller. 2009. A computa-
tional model of prediction in human parsing: Uni-
fying locality and surprisal effects. In Proceedings
of the 29th meeting of the Cognitive Science Society
(CogSci-09).
Vera Demberg, Frank Keller, and Alexander Koller.
2014. Parsing with psycholinguistically motivated
tree-adjoining grammar. Computational Linguistics,
40(1).
Vera Demberg-Winterfors. 2010. A Broad-Coverage
Model of Prediction in Human Sentence Processing.
Ph.D. thesis, University of Edinburgh.
Tim Hunter. 2009. Deriving syntactic properties of ar-
guments and adjuncts from neo-davidsonian seman-
tics. In Proc. of MOL 2009, Los Angeles, CA, USA.
Srini Narayanan Daniel Jurafsky. 2002. A bayesian
model predicts human parse preference and reading
times in sentence processing. In Advances in Neu-
ral Information Processing Systems 14: Proceed-
ings of the 2001 Neural Information Processing Sys-
tems (NIPS) Conference, volume 1, page 59. The
MIT Press.
Yuki Kamide, Gerry Altmann, and Sarah L Haywood.
2003a. The time-course of prediction in incremen-
tal sentence processing: Evidence from anticipatory
eye movements. Journal of Memory and Language,
49(1):133?156.
Yuki Kamide, Christoph Scheepers, and Gerry TM
Altmann. 2003b. Integration of syntactic and se-
mantic information in predictive processing: Cross-
linguistic evidence from german and english. Jour-
nal of Psycholinguistic Research, 32(1):37?55.
Alexander Koller, Joachim Niehren, and Stefan Thater.
2003. Bridging the gap between underspecifica-
tion formalisms: Hole semantics as dominance con-
straints. In Proc. of EACL 2003, pages 367?374.
David M Magerman. 1994. Natural language pars-
ing as statistical pattern recognition. Ph.D. thesis,
Stanford University.
Jeff Mitchell, Mirella Lapata, Vera Demberg, and
Frank Keller. 2010. Syntactic and semantic factors
in processing difficulty: An integrated measure. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 196?
206. Association for Computational Linguistics.
Ulrike Pado?, Matthew W Crocker, and Frank Keller.
2009. A probabilistic model of semantic plausi-
bility in sentence processing. Cognitive Science,
33(5):794?838.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2003.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):71?
106.
T. Parsons. 1990. Events in the semantics of English.
MIT Press, Cambridge, MA, USA.
Philip Resnik. 1992. Left-corner parsing and psycho-
logical plausibility. In In The Proceedings of the fif-
teenth International Conference on Computational
Linguistics, COLING-92, pages 191?197.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational linguistics,
27(2):249?276.
Asad Sayeed and Vera Demberg. 2012. Incremen-
tal neo-davidsonian semantic construction for tag.
In 11th Workshop on Tree-Adjoining Grammars and
Related Formalisms (TAG+11).
Mark Steedman. 2000. The syntactic process. MIT
Press.
Henry S. Thompson, Mike Dixon, and John Lamping.
1991. Compose-reduce parsing. In Proceedings of
the 29th annual meeting on Association for Compu-
tational Linguistics, pages 87?97, Berkeley, Califor-
nia.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 240?247,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Fei Xia, Martha Palmer, and Aravind Joshi. 2000. A
uniform method of grammar extraction and its appli-
cations. In Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 53?62.
65
