Lex ica l i zed  H idden Markov  Mode ls  for  Par t -o f -Speech  Tagg ing  
Sang-Zoo  Lee  and Jun - i ch i  Tsu j i i  
Del)artInent of Infor lnation Science 
Graduate  School of Scien(:e 
University of Tokyo, Hongo 7-3-1 
Bunkyo-ku, Tokyo 113, Ja, l)3,iI 
{lee,tsujii} ((~is.s.u-tokyo.ac.jp 
Hae-Chang R im 
Det)artment of Computer  Science 
Korea Ulfiversity 
i 5-Ca Anam-Dong,  Seongbuk-C~u 
Seoul 136-701, Korea 
rim~)nll).korea.ac.kr 
Abst rac t  
Since most previous works tbr HMM-1)ased tag- 
ging consider only part-ofsl)eech intbrmation in 
contexts, their models (:minor utilize lexical in- 
forlnatiol~ which is crucial tbr resolving some 
morphological tmfl)iguity. In this paper we in- 
troduce mliformly lexicalized HMMs fin: i)art -
ofst)eech tagging in 1)oth English and \](ore, an. 
The lexicalized models use a simplified back-off 
smoothing technique to overcome data Sl)arse- 
hess. In experiment;s, lexi(:alized models a(:hieve 
higher accuracy than non-lexicifliz(~d models 
and the l)ack-off smoothing metho(l mitigates 
data sparseness 1)etter (;ban simple smoothing 
methods. 
1 I n t roduct ion  
1)arl;-Ofsl)e(:('h(POS) tagging is a l)ro(:ess ill 
which a l)rOl)('.r \])()S l;ag is assigned to ea(:h wor(l 
in raw tex(;s. Ev('n though morl)h()logi(:ally am- 
l)iguous words have more thnn one P()S tag, 
they l)elong to just one tag in a colll;ex(;. 'J~o 
resolve such ambiguity, taggers lmve to consult 
various som'ces of inibrmation such as lexica\] 
i)retbrences (e.g. without consulting context, 
table is more probably a n(mn than a. ver}) or 
an adje(:t;ive), tag n-gram context;s (e.g. after a 
non-1)ossessiv(: pronoun, table is more l)robal)ly 
a verb than a. nmm or an adjective., as in th, ey ta- 
ble an amendment), word n-grain conl;e.xl;s (e.g. 
betbre lamp, table is more probal)ly an adjective 
than ~ noun or ~ verb, as in I need a table lamp), 
and so on(Lee et al, 1.999). 
However, most previous HMM-1)ased tag- 
gers consider only POS intbrmation in con- 
texts, and so they C~I~IlII()t capture lexical infi)r- 
nmtion which is necessary for resolving some 
mort)hological alnbiguity. Some recent, works 
lmve rel)orted thai; tagging a('curacy could 
l)e iml)roved 1)y using lexicM intbrnml;ion in 
their models such as the transtbrmation-based 
patch rules(Brill, 1994), the ln~txinnun entropy 
model(lIatn~q)arkhi, 1996), the statistical ex- 
ical ruh:s(Lee et al, 1999), the IIMM consid- 
ering multi-words(Kim, 1996), the selectively 
lexicalized HMM(Kim et al, 1999), and so on. 
In the l)revious works(Kim, 1996)(Kim et al, 
1999), however, their ItMMs were lexicalized se- 
h:ctively and resl;rictively. 
\]n this l>al)er w('. prol)ose a method of uni- 
formly lcxicalizing the standard IIMM for part- 
o f  speech tagging in both English and Korean. 
Because the slmrse-da.ta problem is more seri- 
ous in lexicMized models ttl~ll ill the standard 
model, a simplified version of the well-known 
back-oil' smoothing nml;hod is used to overcome 
the. 1)rol)lem. For experiments, the Brown cor- 
pus(Francis, 1982) is used lbr English tagging 
and the KUNLP (:orlms(Lee ('t al., 1999) is 
used for Kore, an tagging. Tim eXl)criln(;nl;~t\] re- 
sults show that lexicalized models l)erform bet- 
ter than non-lexicalized models and the simpli- 
fied back-off smoothing technique can mitigate 
data sparseness betl;er than silnple smoothing 
techniques. 
2 Ti le "s tandard"  HMM 
We basically follow the not~ti(m of (Charniak 
et al, 1993) to describe Bayesian models. In 
this paper, we assume that {w I , 'w~,..., w ~0 } is 
a set of words, {t t , t '2 , . . . , t ;}  is a set of POS 
tags, a sequence of random variables l'lq,,~ = 
l~q lazy... I'E~ is a sentence of n words, and a 
sequence of random w~riables T1,,, = 7~T,2... TT~ 
is a sequence of n POS tags. Because each of 
random wtrbflfles W can take as its value any 
of the words in the vocabulary, we denote the 
value of l'l(i by wi mM a lmrticular sequence of 
wflues tbr H~,j (i < j) by wi, j. In a similar wl.ty, 
we denote the value of Ti by l,i and a particular 
481 
sequence of values for T/,j (i _< j) t)y ti,j. For 
generality, terms wi,j and ti,j (i > j) are defined 
as being empty. 
Tile purpose of Bayesian models for POS tag- 
ging is to find the most likely sequence of POS 
tags for a given sequence of' words, as follows: 
= arg lnaxPr (T , ,n  =- I W,,,, = w,, ,d 
tl,n 
Because l'efhrence to the random variables 
thelnselves can 1)e oulitted, the above equation 
b eco lnes :  
T('wl,n) = argmax Pr(tl,n \[ wl,,z) (1) 
~'l,~t 
Now, Eqn. 1 is transtbrnled into Eqn. 2 since 
Pr(wl,n) is constant for all tq,~, 
Pr (l.j ,n, wl,n) 
T(*/q,n) -- argmax 
t ,  .... Pr('wl,n) 
= arDnaxP,'(tj,,~,w,,,,) (2) 
tl ,n 
Then, tile prolmbility Pr(tL,z, wl,n ) is broken 
down into Eqn. 3 by using tile chain rule. 
fl(Pr(ti,t\],i-l,Wl,i-1) ) 
Pr(tl,n,~q,r,,) = x Pr(/~i \[tl,i,~Vl,i-l) (3) 
i= l  
Because it is difficult to compute Eqn. 3, the 
standard ItMM simplified it t)3; making a strict 
Markov assumption to get a more tract~d)le 
tbrm. 
Pr(tl,,,, Wl,n) ~ x Pr(wi I td (4) 
i= l  
I51 the standard HMM, the probability of the 
current tag ti depends oi5 only the previous K 
tags ti-K,i-1 and the t)robability of' the cur- 
rent word wi depends on only the current ag 1. 
Thereibre, this model cannot consider lexical in- 
formation in contexts. 
3 Lex ica l i zed  HMMs 
In English POS tagging, the tagging unit is a 
word. On the contrary, Korean POS tagging 
prefers a morpheme 2. 
1Usually, K is determined as1 (bigram as in (Char- 
niak et al, 1993)) or 2 (trigram as in (Merialdo, 1991)). 
2The main reason is that the mtmber of word-unit 
tags is not finite because I(orean words can be ti'eely 
and newly formed l)y agglutinating morphemes(Lee t 
al., 1999). 
, / ,  
Flies/NNS Flies/VBZ 
like/CS like/IN like/JJ like/VB 
a/A~ a/IN a/NN 
ttower/NN flower/VB 
. / .  
$/$ 
Figure 1: A word-unit lattice ot' "Flies like a 
\ [ l ower  ." 
Figure 1 shows a word-unit lattice of an Eil- 
glish sentence, "Flies like a flowc'r.", where each 
node has a word and its word-unit tag. Fig- 
ure 2 shows a morpheme-unit lattice of a Ko- 
rean sentence, "NcoNeun tIal Su issDa.", where 
each node has a morphenm and its morI)heme- 
unit tag. In case of Korean, transitions across 
a word boundary, which are depicted by a solid 
line, are distinguished fl'om transitions within a 
word, which are depicted by a dotted line. ill 
both cases, sequences connected by bold lines 
indicate the most likely sequences. 
3.1 Word-un i t  mode ls  
Lexicalized HMMs fbr word-unit agging are de- 
fined 1)y making a less strict Markov assmnp- 
tion, as tbllows: 
A(T(K,j), W( I ; j ) )~  Pr(tl,,~,wl,n) 
i=\] x Pr(wi I ti-L,i, wi-I , i -1) 
Ill models A(T(K,j), 14/(L j)) ,  the probability of 
the current tag ti depends on both tile previ- 
ous I f  tags t i -K, i - i  and the previous d words 
wi- j , i - i  and the probability of the current word 
'wi depends on the current ag and the previous 
L tags ti_L, i and the previous I words wi-l , i -~. 
So, they can consider lexieal inforination. In ex- 
periments, we set I f  as 1 or 2, J as 0 or K, L as 
1 or 2, and 1 as 0 or L. If J and I are zero, the 
above models are non-lexicalized models. Oth- 
erwise, they are lexicalized models. 
482 
$/, 
Neo/N NI" Ncol/VV 
?. 4 
No'an~ P X Ncun/EFD 
H~d/NNCC Hd/NNBU H~(VV \ ] Ia /VX 
S'a/NNCG Su/NNBG 
iss/\zJ iss/VX 
Da/EFF Da/EFC 
?"'OOoo,,,j~g_._.--"- 
./ss. 
$/$ 
Figure 2: A morl)heme-unit latti(:(; of "N,oN,'un 
llal S'u i.ssl)a." (= You (:an do it.) 
r l  f in a lexicalized model A(~/(9,2), lI ('J,2)), fin" ex- 
mnl)lc , the t)robal)ility of a node "a/AT" of tlm 
most likely sequen(:e in Figure 1 is calculate(t as 
tbllows: 
l'r(AT' I NM& vIL Fli(:,~, lit,:c) 
? tq  ? x Pr(a t :'1~, NNS,  VH, 1 l'~,c.s, lil,:c) 
3.2  Morphe lne-un i t  mode ls  
l);~yesian models for lnOrl)heme-unit tagging 
tin(t the most likely se(lueame of mor\])h(mms 
and corresponding tags fi)r ;~ given sequence of 
words, as follows: 
~'(11) ,1,,) = al'glll;XX Pr (c  l,v,, ?/~,,,u I '1,,,,~) (6) 
Cl~u flltl,,t 
, ra-ax Pr(c,,,,, m,,. ' ,,,,, ,,,) (7) 
Cl,~tllt~l,u 
In the above equations, u(_> 'n) denotes the 
llllIlll)cr of morph(mms in a Se(ltlell(;e ('orre- 
spending the given word sequ('ncc, c denotes 
a morl)heme-mfit tag, 'm. denotes a morl)heme , 
aim p denotes a type of transition froln the pre- 
v ious  tag to the current ag. p can have one of 
two values, "#" denoting a transition across a 
word bomldary and "+" denoting a transition 
within a word. Be(-ause it is difficult to calculate 
Eqn. 6, the word sequence term 'w~,,, is usually 
ignored as ill Eqn. 7. Instead, we introduce p in 
Eqn. 7 to consider word-spacing 3. 
Tile probability Pr(cj ,~L, P2,u, 'm,~ ,u) is also bro- 
ken down into Eqn. 8 t)3r using the chain rule. 
Pr(c~ ,,,, P2,,, , 'm, , ,,,,) 
f l  ( \])r(ci,Pi \[ cl,i-l,P2,i-l,'lnl,i-l) ) 
~- X P1"(1~'1,i \[('d,i,I,2,i,17tl,i_\]) (8) i=1 
\]3('caus(' Eqn. 8 is not easy to (;omlmte ~it is 
sinll)lified by making a Marker assmnt)tion to 
get; a more tractal)le forlll. 
In a similar way to the case of word-unit; tag- 
ging, lexicalize(t HMMs for morl)heme-mfit tag- 
ging are defined by making a less strict Markov 
assunq)tion, as tblh)ws: 
A(C\[,q(K,.\]), AJ\[sI(L,1)) 1= Pr(c\],,,,p2,,,, 'mq,~,) 
I'r(c \[,pd I ,,I,i-,Uc/--lC/-' (!)) 
~=~, x l ' r (mi l c i  l,,i\[,>-L+l,,i\],'mi-l,i--I) 
In models A(C\[.q(tc,,I),M\[q(L,Q), the 1)robal)il- 
ity of the (:urrent mori)heme tag ci depends 
on l)oth the 1)revious K |:ags Ci_K,i_ 1 (oi)tion- 
ally, th(' tyl)eS of their transition Pi-K~ 1,i-~) 
a.n(l the 1)revious ,\] morl)hemes H~,i_.l,i_ 1 all(1 
the probability of the current mort)heine 'm,i (t(> 
1)en(ls on the current, tag and I:he previous L 
tags % l,,i (optional\]y, the typ('~s of their tran- 
sition Pi -L-t-I,i) and the 1)revious I morl)hemes 
?lti--l,i-1. ~()~ t\]l(ly ('&ll &lSO (-onsid(,r h;xi(-al in- 
formation. 
In a lexicalized model A(C,.(~#), M(~,2)) whea:e 
word-spa(:ing is considered only in the tag prob- 
al)ilities, for example, the 1)rol)al)ility of a nod(; 
"S'u/NNBG" of the most likely sequence in Fig- 
urc 2 is calculated as follows: 
Pr(NNBG, # \[ Vl4 EFD, +, Ha, l) 
x Pr(gu \[ VV, EFD, NNBG,  Ha, l) 
3.3  Parameter  es t imat ion  
In supervised lcarning~ the simpliest parameter 
estimation is the maximum likelihood(ML) cs- 
t imation(Duda et al, 1973) which lnaximizes 
the i)robal)ility ot! a training set. The ML esti- 
mate of tag (K+l ) -gram i)robal)ility, PrML (f;i \[ 
t,i-K,i-i), is calculated as follows: 
P Pr(ti l ti_ir,i_j) __ \]: q ( t i - i ( , i )  (10) 
ML Fq(ti-lGi-l) 
aMost 1)rcvious HMM-bascd Korean taggcrs except 
(Kim et al, 1998) did not consider word-spacing. 
483 
where the flmction Fq(x) returns the fl:equency 
of x in the training set. When using the max- 
imum likelihood estimation, data sparseness i
more serious in lexicalized models than in non- 
lexicalized models because the former has even 
more parameters than the latter. 
In (Chen, 1996), where various smoothing 
techniques was tested for a language model 
by using the perplexity measure, a back-off 
smoothing(Katz, 1987) is said to perform bet- 
ter on a small traning set than other methods. 
In the back-off smoothing, the smoothed prob- 
ability of tag (K+l ) -gram PrsBo(ti \[ ti-l~,i-l) 
is calculated as tbllows: 
Pr (ti \[ ti-I(,i-~) = 
,5'1~20 
drPrML(ti \[ti-I(,i-1) " if r>0 (11) 
c~(ti-K,i-1) Prsso(ti \[ ti-K+l,i-l)if r = 0 
where r = Fq(ti_t(,i), r* = ( r+ 1)'nr+l 
7~, r
r* (r+l.) x~%.+l 
dr  ~ F l t l  
1-  (r+l)xm.+l 
n l  
n,. denotes the nmnber of (K+l ) -gram whose 
frequency is r, and the coefficient dr is called 
the discount ratio, which reflects the Good- 
~lhtring estimate(Good, 1953) 4. Eqn. 11 means 
that Prxgo(ti \[ ti-K,i-l) is under-etimated by 
dr than its maximum likelihood estimate, if 
r > 0, or is backed off by its smoothing term 
Prsuo(ti \[ ti-K+j,i-l) in proportion to the 
value of the flmction (~(ti-K,i-t) of its condi- 
tional term ti-K,i-1, if r = 0. 
However, because Eqn. 11 requires compli- 
cated computation in ~(ti-l(,i-1), we simI)lify 
it to get a flmction of the frequency of a condi- 
tional term, as tbllows: 
ct(Fq(ti-K,i-1) = f) = 
E\[Fq(ti-I(,i-1) = f\] Ax E7-o E\[Fq(ti-K,i-1) -= f\] 
where A = 1 - ~ Pr (tglti-/c,i-,), 
SBO ti--K,i~r>O 
E\[Fq(ti-g,i-1) = f\] = 
SP\]to ( ti \[ti-K + l,i-1) 
t i -  K + L i,r=O,F q( t i -  K, i -1)= f ' 
(12) 
In Eqn. 12, the range of .f is bucketed into 7 
4Katz  said that  d,. = i if r > 5. 
regions such as f = 0, 1, 2, 3, 4, 5 and f > 6 since 
it is also difficult to compute this equation tbr 
all possible values of f .  
Using the formalism of our simplified back-off 
smoothing, each of probabilities whose ML es- 
timate is zero is backed off by its corresponding 
smoothing term. In experiments, the smooth- 
ing terms of Prsl~o(ti \[ ti-K,i-l,~t)i-,l,i-l) are  
determined as follows: 
PI'sBo(ti\[ ti-Ii+l,i-h )if K> 1,d> 1 
wi_j+~,i_~ 
Prsuo(ti i fK  >_ 1, d = 1 
Prs13o(ti \[ ti-K+Li-l) if K > 1, J = 0 
PrAD(ti) if K = 0, J = 0 
Also, the snloothing terms of' Pl's\]~o(wi 
ti_L,i, Wi_l,i_ 1 ) are determined as follows: 
\[ Prst~o(wi 
Prsuo  (wi 
Prs,o (wi 
PrsBO(Wi) 
PrA.O i) 
ti-L+~,i, ) if L _> 1, I>  1 
i l ) i - I+ l  , i - I  
ti-L,i) if L _> 1, I = 1 
ti-L+Li) if L >_ 1, I = 0 
i f L  = 0, I --= 0 
i l L  = -1 ,  I = 0 
In Eqn. 13 and 14, the smoothing term of a 
unigram probability is calculated by using an 
additive smoothing with 5 = 10 .2 which is cho- 
sen through experiments. The equation for the 
additive smoothing(Chen, 1996) is as tbllows: 
Fq(ti-t(,i) + 5 
AD ~tl (Fq(ti-lf,i) + 5) 
In a similar way, the smoothing terms of param- 
eters in Eqn. 9 ~re determined. 
3.4 Model  decoding 
h'om the viewpoint of the lattice structure, the 
t)roblem of POS tagging can be regarded as the 
problem of finding the most likely path ti'om the 
start node ($/$) to the end node ($/$). The 
Viterbi search algorithm(Forney, 1973), which 
has been used for HMM decoding, can be effec- 
tively applied to this task just with slight mod- 
ification 5. 
4 Exper iments  
4.1 Environment 
In experiments, the Brown corpus is used tbr 
English POS tagging and the KUNLP corpus 
'%uch modification is explained in detail in (Lee, 
1999). 
(13) 
(14) 
484 
NW 1,113,189 
NS 
NT  
DA 
RUA 
Brown KUNLP 
167,115 
53,885 15,211 
82 65 
1.64 3.4:1 
61.54% 26.72% 
NW Number of words. NS Number of sen- 
tcnccs. NT Numl){'.r of tags (nlorpheme-unit 
tag for KUNLP). DA Degree of mnbiguity 
(i.e. the number of tags per word). RUA 
1\].atio f mlanlbiguous words. 
Table 1: Intbrmat ion al)out the Brown eortms 
and the KUNLP tort}us 
Inside-test ()utside-|;(;st 
ML 95.57 94 .97  
= 1) 
-AD(a - \](}- \]) 
AD(~ = 1{}2T)  - 
ADO; - -  =a) 
A\]) ( ( ;  = 
AD(5  = 
AD(5 = 
AD(5 = \]\]}-~7)- 
AD(5 = 
93.92 93.02 
95.02 94.79 
95.42 95.08 
95.55 95.05 
95.57 94.98 
95.57 94.94 :  
95.57 94.91 
95.57 94.89 
95.57 94.87 
SBO 95.55 95.25 
ML Maximum likelihood estimate (with sim- 
ple smoothing). A\]) Additiv(~ smoothing. 
SBO Sinll}liticd 1)ack-off smootlfing. 
lal)l(, 2: lagging accura(:y (}f A(C(\]:o), M0}:0 )) 
for Kore~m POS tagging. Table 1 shows some 
intbrmation M)out 1}oth (:ori)ora {~. Each of them 
was segmented into two parts, the training set 
of 90% and the test; set of 10%, ill. the way that  
each sentence in the test set was extra{'tc, d \]i'()ln 
every 1(} senl;ellce. A(:cording to Tabl(! 1, Ko- 
reml is said to 1)e lllOre (litli(:ult to disambiguat(; 
tl\]ml English. 
We assmne "closed" wmabulary for English 
and "open" vocabulary for Korean since we do 
not h~ve any Engl ish morphological  mmlyzer 
consistent with the Brown corlms. Therefore, 
for morphological mmlysis of English, we just 
aNote that some sentcnc('.s, which have coml}osite 
tags(such as "HV+TO" in "hafta") ,  "ILLEGAL" tag, 
or "NIL" tag~ were remov(M fronl the Brown corl)us and 
tags with "*" (not) such as "BEZ*" were r(',l)la(:(~(t 1)y (:of 
r{~st}o\]ttling ta s without "*" such as "BEZ". 
2M 
1.5M 
IM 
(}.5M 
I I I I I 
- ML  
AD .x .  - 
SBO 
1,02 ,01 ,02 ,01 ,02 ,0  
{},(} 0 ,01  ,(} 1 ,02 , (}  2 ,0  
\ ] '  - - I  I \[ I I 
.99 
.98 
.97 _ I~L~ ~_? 
1,02 ,01 ,023} 13} 2,{1 
(},0 {},{} 1 ,01 ,1}  2 ,02 ,0  
.98 
.97 
2)6 
.(,):, ?vii, -r J--  
AD '?- - 
.:)4 SBO -~--- 
1,02,01,02,(11,02,0 
o,00,01,01,02,02,0 
1. 
.99 
.98 
\[ 
( 
.97 
.96 
I I I I I i t ~  
I I I I I t t } I I I I I 
1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,01,01,12,0 1,1 1,1 0,01,0 1,12,0 2,2 2,2 2,2 2,2 
(a) # of paraln{;ters 
M\], -D- 
AD -?- - - 
SB( )  
I I I I I I I I I I I I I 
1 , 1 1, l 1,0 1,1 2,01,1 2,22,22,22,2 1 ,(} 2,01,12,2 
0,01 0 1,12,01,11,10,01,01 l 2,02,22,22,22,2 
(1)} Inside-test 
1,11,I 1,01,12,01,I 2,22,22,22,21,02,01,12,2 
0 01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(c) Ouiside-test 
1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,0 1,02,02,0 0,01,01,l  2,01,11,10,(11,{11,I 2,02,22,22,22,2 
(d) inside vs. outside-test in SBO 
Figure 3: Results of English tagging 
485 
looked up the dictionary tailored to the Brown 
corpus. In case of Korean, we have used a Ko- 
rean morphological analyzer(Lee, 1999) which 
is consistent with the KUNLP corpus. 
4.2  Resu l t s  and  eva luat ion  
Table 2 shows the tagging accuracy of the sim- 
plest HMM, A(C(l:0),M(0:0)), for Korean tag- 
ging, according to various smoothing meth- 
ods 7. Note that ML denotes a simple smooth- 
ing method where ML estimates with prob- 
ability less than 10 -9  a re  smoothed and re- 
placed by 10-9? Because, in the outside-test, 
AD(d = 10 -2) performs better than ML and 
kD(a ? 10-2), we use 5 = 10 -2 in our ad- 
ditive smoothing. According to Table 2, SBO 
I)ertbrms well even in the simplest HMM. 
Figure 3 illustrates 4 graphs'about the results 
of English tagging: (a) the number of param- 
eters in each model, (b) the accuracy of each 
model tbr the training set, (c) the accuracy of 
each model for the test set, and (d) the accuracy 
of each model with SBO tbr both training and 
test set. Here, labels in x-axis sI)ecify models 
K,  ,1 in the way that ~ denotes A(T(\];,j) , W(Lj)). 
Therefore, the first 6 models are non-lexicalized 
models and tile others are lexicalized models. 
Actually, SBO uses more parameters than 
others. The three smoothing methods, ML, 
AD, SBO, perform well for the training set; 
since the inside-tests usually have little data 
sparseness. On the other hand, tbr the un- 
seen test set, the simple methods, ML and 
AD, cannot mitigate the data sparseness prob- 
lem, especially in sophisticated models. How- 
ever, our method SBO can overcome the prob- 
lem, as shown in Figure 3(c). Also, we can 
see in Figure 3(d) that some lexicalized mod- 
els achieve higher accuracy than non-lexicalized 
models. We can say that the best lexicalized 
model, A(T(1,~),W(1,1)) using SBO, improved 
the simple bigram model, A(T(L0),W(0,0)) us- 
? ~ 0 mg SBO, from 97.19>/o to 97.87~ (the error re- 
duction ratio of 24.20%). Interestingly, some 
lexicalized models (such as A(T(1,1), W-(0,0)) and 
A(T(1,1), W(1,o))), which have a relatively small 
number of paranmters, perform better than 
non-lexicalized models in the case of outside- 
tests using SBO. Untbrtunately, we cannot ex- 
r Ins ide - tes t  means  an  exper iment  on  the  t ra in ing  set  
i t se l f  and  outs ide - tes t  an  exper iment  on  the  tes t  se t .  
.96 
.94 ~ ?  . .~  uu . X ? "" " ~ '  " .~1%~ ~ 
.92 
.90 
ML ~ k 
.88 AD .x. - 
SBO 
.86 I I I I I I I I I I f I I I I I I I 
1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,01,02,02~0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(a) Outside-test 
? 97 I I I I I I ~ I I I d~ I I I t I I -~  
C,M + ? 
.966 C~,/l~/ + 
.9(;2 ~.~, -~I~ X \[\] 
+ 
1,02,01,02,01102,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,2 
0,00,01,01,02,02,0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2 
(b) Considering word-spacing 
+ 
x 
? 
x 
\ [ \ ]  ? 
? 
I l l l l l l l  
Figure 4: Results of Korean tagging 
pect the result of outside-tests from that of 
inside-tests because there is no direct relation 
t)etween them 
Figm:e 4 includes 2 graphs about the re- 
sults of Korean tagging: (a) the outside ac- 
curacy of each model A(C(K,j),MiL,I)) and 
(b) the outside accnracy of each model 
A(C\[s\](~-g),M\[s\](L,0) with/without considering 
word-spacing when using SBO. Here, labels in 
K,J de-  x-axis specify models in the way that ,7,, 
notes A(C\[s\](K,j),i~/I\[.~\](Lj)) and, tbr example, 
C , ,M in (b) denotes k(C~(,r,j), M(L,r)). 
As shown in Figure 4, the simple meth- 
ods, ML and AD, cannot mitigate that sparse- 
data problem, t)ut our method SBO can over- 
come it. Also, some lexicalized models per- 
tbrm better than non-lexicalized models. On 
the other hand, considering word-spacing ives 
good clues to the models sometimes, but yet 
we cannot sw what is the best ww. From 
the experimental results, we can say that the 
best model, A(C(9,2),M(2,2)) using SBO, im- 
proved the previous models, A(C(1,0), M(o,o)) us- 
486 
ing ML(Lee, 1995), and A(G(,,0), M(0,0))using 
ML(Kim et al, 1998), t'ronl 94.97% and 95.05% 
to 96.98% (the error reduction ratio of 39.95% 
mid 38.99%) respectively. 
5 Conc lus ion  
We have 1)resented unitbrmly lexicalized HMMs 
for POS tagging of English and Korean. In 
the models, data sparseness was etlix:tively mit- 
igated by using our simplified ba(-k-ofl" smooth- 
ing. From the ext)eriments, we have ol)served 
that lexical intbrmation is usefifl fi)r POS tag- 
ging in HMMs, as is in other models, and 
ore" lexicalized models improved non-lexicalized 
models by the error reduction ratio of 24.20% 
(in English tagging) and 39.95% (in Korean tag- 
ging). 
G('.nerally, the mfiform extension of models 
requires ral)id increase of parameters, and hence 
suffers fl'om large storage a.nd sparse data. l~.e- 
cently in many areas where HMMs are used, 
many eflbrts to extend models non-mfitbrmly 
have been made, sometimes resulting in notice- 
able improvement. For this reason~ we are try- 
ing to transfbnn our uniform models into non- 
mliform models, which may 1)e more effective 
in terms of both st)ace (:omt)h'~xity and relial)le 
estimation of I)areme|;ers, without loss of accu- 
racy. 
Re ferences  
12. Brill. 1994. Some Advances in 
~l?ansformation-B ased Part of St)eech 
~Dtgging. In P~ve. of the 12th, Nat'l Cm?. on 
Art'tficial hdelligencc(AAAI-.9~), 722-727. 
E. Charniak, C. Hendrickson, N. Jacobson, and 
M. Perkowitz. 1993. l~3quations for Part- 
o f  Speech %~gging. In Proc, of the 11th, 
Nat'l CoT~:f. on Artificial Intclligence(AAAL 
93), 784-789. 
S. F. Chen. 1996. Building Probabilistic Models 
for Natural Language. Doctoral Dissert~tion, 
Harvard University, USA. 
R. O. Duda and R. E. Hart. 1973. Pattern CIas- 
s'~fication and Scene Analysis. John Wiley. 
G. D. Forney. 1973. The Viterbi Algorithm. Ill 
Proc. of the IEEE, 61:268-278. 
W. N. Francis and H. Ku~era. 1982. Fre- 
quency Analysis of English Usage: Lczicon 
and GTnmmar. Houghton Mitltin Coral)any , 
Boston, Massachusetts. 
I. J. Good. 1953. "The Population Frequen- 
cies of Species and the Estimation of Pop- 
ulation Parameters," Ill Biometrika, 40(3- 
4):237-264. 
S. M. Katz. 1987. Estimation of Probabilities 
fronl Sparse Data for the Language Model 
Component of a Speech Recognizer. In IEEE 
Transactions on Acoustics, Speech, and Signal 
i'rocessing(ASSl'), 35(3):400-401. 
J.-\]). Kim, S.-Z. Lee, and H.-C. Rim. 1998. 
A Morpheme-Unit POS Tagging Model Con- 
sidering Word-Spacing. Ill Pwc. of th.e I0 th 
National CoT~:fercnce on Korean h~:formation 
PTveessing, 3-8. 
J.-D. Kim, S.-Z. Lee, and H.-C. Rim. 1999. 
HMM Specialization with Selective Lexi- 
calization. In Pwe. of the joint SIGDAT 
Co~l:h':rence on Empirical Methods in Nat- 
'aral Language Processing and Very La'qtc 
Co'rpora(EMNLP- VL C-99), ld4-148. 
J.-H. Kim. 1996. Lcxieal Disambig'aation with 
Error-Driven Learning. Doctoral Disserta- 
tion, Korea Advanced Institute of Science and 
Te.clmology(KAIST), Korea. 
S.-H. Lee. 1995. Korean POS Tagging System 
Considering Unknown Words. Master The- 
sis, Korea Advanced Institute of Science and 
Teclmology(KAIST), Korea. 
S.-Z. Lee, .I.-D. Kim, W.-H. Ryu, and H.- 
C. Rim. 1999. A Part-of Speech Tagging 
Model Using Lexical l/.ules Based on Corlms 
Statistics. In Pwc. of the International Con- 
ference on Computer \])'lvcessin 9 of Oriental 
Languages(lCCPOL-99), 385-390. 
S.-Z. Lee. 1999. New Statistical Models for Au- 
tomatic POS Tagging. Doctoral Dissertation, 
l(orea University, Korea. 
B. Merialdo. 1991. Tagging Text with a Prol)- 
abilisl;ic Model. In P~vc. of the International 
Conference on Acoustic, Spccch and Signal 
Processing(ICASSP-91), 809-812. 
A. Ratnap~rkhi. 1996. A Maximum Entrol)y 
Model tbr Part-of-Speech Tagging. In Proe. 
of the Empirical Methods in Natural Lan- 
guage P~vcessi'ng Co'a:fercnce(EMNLP-9b'), 
133-142. 
487 
KCAT : A Korean Corpus Annotating Tool Minimizing Human 
Intervention 
Won-He Ryu, Jin-Dong Kim, l ine-Chang Rim 
Dept. of Computer Science & Engineering, 
Natural Language Processing Lab, 
Korea University 
Anam-dong 5-ga, Seongbuk-gu, Seoul, Korea 
whryu, jin, rim @nlp.korea.ac.kr 
Abstract 
While large POS(part-of-speech) annotated 
corpora play an important role in natural 
language processing, the annotated corpus 
requires very high accuracy and consistency. 
To build such an accurate and consistent 
corpus, we often use a manual tagging 
method. But the manual tagging is very 
labor intensive and expensive. Furthernaore, 
it is not easy to get consistent results from 
the humari experts. In this paper, we present 
an efficient tool lbr building large accurate 
and consistent corpora with minimal human 
labor. The proposed tool supports semi- 
automatic tagging. Using disambiguation 
rules acquired from human experts, it 
minimizes the human intervention in both 
the manual tagging and post-editing steps. 
1. Introduction 
The POS annotated corpora are very 
important as a resource of usefiil information tbr 
natural language processing. A problem for 
corpus annotation is tile trade-off between 
efficiency and accuracy. 
Although manual POS ta,,<,in,,==  is very 
reliable, it is labor intcnsive and hard to make a 
consistent POS tagged corpus. On the other hand, 
automatic ta,-,in,,>~  is prone to erroi-s Ibr 
infrequently occurring words duo to tile lack el" 
overall linguistic information. At present, it is 
ahnost impossible to construct a highly accurate 
corptls by usin<,~ an automatic taggcr~ alone. 
/ks a consequence, a semi-autonmtic ta,,,,in,~== 
method is proposed IBi corpus annotation. In 
Heui-.Seok Lira 
Information Communications Department, 
Natural Language Processing Lab, 
Chonan University 
85-1, Anseo-Dong, Chonan City, 
ChungChong-NamDo Province, Korea 
timhs@inli~com.chonan.ac.kr 
ordiriary semi-automatic tagging, an automatic 
tagger tags each word and human experts correct 
the rots-tagged words in the post-editing step. 
But, in the post-editing step, as the human expert 
cannot know which word has been annotated 
incorrectly, he must check every word in the 
whole corpus. And he lnust do the same work 
again and again for the same words in the same 
context. This situation causes as Inuch 
labor-intensive work as in manual ta<+<qlw 
In this paper, we propose a semi-automatic 
tagging method that can reduce the human labor 
and guarantee the consistent tagging. 
2o System Requivemer~ts 
To develop ari efficient tool that attempts to 
build a large accurately armotated corpus with 
minimal human labor~ we must consider the 
following requirements: 
? In order to minimize human labor, the same 
human intervention to tag and to correct the 
same word in tile same context should not be 
repeated. 
* There may be a word which was tagged 
inconsistently in the same context becatlse it 
was tagged by different human experts or at a 
different ask time. As an elticient tool, it can 
prevent tile inconsistency of tile annotated 
( I  results and ~uarantec the consistency of the 
annotated results. 
* It must provide an effective annotating 
capability lbr many unknown words in the 
whole corpus. 
1096 
3. Proposed POS Tagging ToohKCAT 
The proposed POG tagging tool is used to 
combine the manual tagging method and the 
automatic tagging method. They are integrated 
to increase the accuracy o\[" the automatic tagging 
method and to minimize the amount of tile 
human labor of thc manual tagging method. 
Figure 1 shows the overall architecture of the 
proposed tagging tool :KCAT. 
I . . . . . . . . .  I I I ~ I P I Raw (..rpus ILI 
Pos t -Fn Jcess  ~t  I re - l rocess  
( ' c J r rec t  an  ~ . . . . . . . . . . .  :~  ; . . . .  ,R  s " " . . . . . . . . . .  
- -7 - -~ i - -g  
~____~ " : . 
i ~'f::: 2aa' :ii,:n~ ...... 
Figure 1. System Architecture of KCAT 
As shown in figm'e 1, KCAT consists of 
three modules: the pre-processing module, the 
automatic tagging module, and the 
post-processing module. In the prcoprocessing 
module, the disambiguation rules are acquired 
I%m human experts. The candidate words are 
Ihe target words whose disambiguation rules are 
acquired. The candidate words can be unknown 
words and also very frequent words. In addition, 
the words with problematic ambiguity for tlle 
automatic tagger can become candidates. 
l)lsamblguation rules are acquired with minimal 
human labor using tile tool t:n'oposed in 
(Lee, 1996). In the automatic tagging naodule, the 
disambiguation rules resolve the ambiguity of 
{,'very word to which they can be applied. 
I lowever, tile rules are certainly not sufficient o 
resolve all the ambiguity of the whole words in 
file corpus. The proper tags are assigned to the 
remaining ambiguous words by a stochastic 
< t~"  c, hL l l l l an  lagger. After the automatic t, m~, a 
expert corrects tile onors o\[ the stochastic ta,me, 
The system presents the expert with the results 
of the stochastic tagger. If the result is incorrect, 
tile hulllan expel1 corrects the error and 
generates a disambiguation rule ~br the word. 
The rule is also saved in the role base in order to 
bc used later. 
3. I. l.exical Rules for Disambiguation 
There are many ambiguous words that are 
extremely difficult to resolve alnbiguities by 
using a stochastic tagger. Due to the problematic 
words, manual tagging and manual correction 
must be done to build a correct coqms. Such 
human intervention may be repeated again and 
again to tag or to correct tile same word in the 
same context. 
For example, a human expert should assign 
'Nal(flying)/Verb+Neun/Ending' to every 
'NaNemf repeatedly in the following sentences: 
" Keu-Nyeo-Neun Ha-Neul-Eul Na-Neun 
Pi-Haeng-Ki-Reul Port Ceok-i Iss-Ta." (she has 
seen a flying plane) 
"Keu-Netm lht-Nc'ul-Eul NaoNeun 
t'i-Itaeng--Ki-Reul Port Ceok-i Eops-Ta." (he has 
never seen a flying phme) 
"Keu-Netm tta-Ne,tl-Eul Na-Neun 
Pi--ttaeng--Ki-Reul Pal-Myeong-tlaess- Ta." (he 
invented a flying plane) 
In the above sentences, human experts can 
resolve the word, 'Na-Nemf with only the 
previous and ttle next lexical information: 
'fla-Neul-Eul' and 'Pi-tlaeng- Ki-Reul'. In other 
words, tile human expert has to waste time on 
tagging the same word in tile same context 
repeatedly. This inefficiency can also be 
happened in the manual correction of the 
ntis-tagged words. So, if the human expert can 
make a rule with his disambiguation knowledge 
and use it for tile same words in tile same 
context, such inefficiency can be minimized. We 
define the disambiguation rule as a lexical rule. 
Its template is as follows. 
\[P:N\] \[Current Word\] \[Context\] = \[Tagging 
P, esuh\] 
Context ? Previous words?p * Next Words?,, 
Ill tile above template, p and n mean tile 
previous and the next context size respectively. 
For the present, p and n are limited to 3. '*' 
1097 
represents the separating mark between the 
previous and next context. For example, tile rule 
\[1:1\] \[Na-,'\:lten\] \[Ha-Neul-Eld * Pi-Haeng-Ki- 
Reul\] = \[Na/(flying)/Verb i- Neun/Ending \]says 
the tag 'Nal(flying)/Verb +Neun/Ending' should 
be assigned to the word 'Na-Neun' when the 
previous word and the next word is 
'Ha-Neul-Eul' and 'Pi-Haeng-Ki-Reul'. 
Although these lexical rules cannot always 
correctly disambiguate all Korean words, they 
are enough to cover many problematic 
ambignous words. We can gain some advantages 
of using the lexical rule. First, it is very accurate 
because it refers to the very specific lexical 
information. Second, the possibility of rule 
conflict is very little even though the number of 
the rules is increased. Third, it can resolve 
problematic ambiguity that cannot be resolved 
without semantic inf'onnation(Lim, 1996). 
3.2. Lexicai Rule Acquisition 
Lexical rules are acquired for the unknown 
words and the problematic words that are likely 
to be tagged erroneously by an automatic tagger. 
Lexical rule acquisition is perlbrmed by 
following steps: 
1. The system builds a candidate list of 
words li)r which the lexical rules would be 
acquired. The candidate list is the collection 
of all examples of unknown words and 
problematic words for an automatic tagger. 
2. A human expert selects a word from the 
list and makes a lexical rule for the word. 
3. The system applies tile lexical rule to all 
examples of the selected word with same 
context and also saves the lexical rule in the 
rule base. 
4. P, epeat tile steps 2 and 3 until all 
examples of the candidate words can be 
tagged by the acquired lexical rules. 
3.3. Automatic Ta,,, in,,  
In the automatic ta,,~dn-oo ~ phase, words are 
disambiguated by using the lexical rules and a 
stochastic tagger. To armotate a word in a raw 
corpus, the rule-based tagger first searches the 
lexical rule base to find a lexical rule that can be 
nlatched with tile given context. If a matching 
rnle is found, the system assigns the result of the 
rule to the word. According to the corresponding 
rule, a proper tag is assigned to a word. With tile 
lexical rules~ a very precise tag can be assigned 
to a word. However, because the lexical rules do 
not  resolve all the ambiguity of the whole corpus, 
we must make use of a stochastic tagger. We 
employ an HMM--based POS tagger for this 
purpose(Kim,1998). The stochastic tagger 
assigns the proper tags to the ambiguous words 
afier the rule application. 
Alter disambiguating the raw corpus using 
the lexical rules and the atttomatic tagger, we 
arrive at the frilly disambiguated result. But the 
word tagged by the stochastic tagger may have a 
chance to be mis-tagged. Therefore, the 
post-processing for error correction is required 
for the words tagged by the stochastic tagger. 
3.4. Error Correction 
The human expert carries out the error 
correction task for the words tagged by a 
stochastic tagger. This error correction also 
requires tile repeatecl human labor as in the 
manual tagging. We employ the similar way of 
the rule acquisition to reduce the human labor 
needed for manual error cmTection. The results 
of the automatic tagger are marked to be 
distinguished from tile results of the rule-based 
tagger. The human expert checks the marked 
words only. If an error is found, the ht/man 
expert assigns a correct tag to the word. When 
tile expert corrects the erroneous word, tile 
system automatically generates a lexicat rule and 
stores it in tile rnle base. File newly acquired 
rule is autoinatically applied to the rest of tile 
corpus. Thus, the expert does not need to correct 
the repeated errors. 
1098 
B ........ 
A . . . J  
:~ ,?  "~; ~ ~'J ~'Y,I .:'l~ll,k! G'~(~ ~:)':'fl,q ! ! !~l))L" , l ' ) l  ,q';'.%ll.q !ll~ ~. "?1 )~:d~ 
:':} 'k L '  ~i~ tl ? r31 ,31 ?2 :~ '2'.' :~ ,:,i\[ .~ YZ. "?! :'J q l :'112j X,"~ ?t ) I -@ ~! ? I 
".'.20 t~'~tJ 2: .c Ul I '3t3b!:! I ~ :~ '~ (IM{3tl *,1 N ~ :31 ,'q ~?i ::i ; '  ?,,3 ~ :~ ~J g~ "JH G 
r.NwO}.lx* I '?v?et~a5 : !~31 W~'gf f l l  Y '~a!  t l t~0 l  adlTll ' , lLr'9~. 
r ,~ wU,t.l:<t E : '?t:S eft ~1:" 
E(';'.hTF ~ I '~ t 
,iH,'.t CH.q',~'~ Ncrs icTc~ 
, : ' I~L . IOF ' , I~qEA ~x~.'qlOL }~M~?-"-- '?I I~ "~. 
=i~I  'q 
:" Gt~i!~} 5"3~d~/t,\]HP*~:}IJ'2 
J, kt21 ~t X}el/tlt'l!3 ? N,,'JF O 
:" h!T;UIHOII !,~T?~I'I/f'g'dG.OII/JC 
:. *.;9 E ,~;.,V~.,*?.L.'EP* /EF -  GF  
~.~n ~ 7;/I, iN p .  ~,,d X 
:, ~la~ At ~I"~INhII\]-.Lt/JFB 
> @~(~,~ .. @~iNNP- ( /SS ,~ ' I /SH.  
g~ ~ 11 t,'(I ~J L~ .~,!/N N P ? Ilt Xl ~I/ JK G 
> ZI~01 M XI~/NN,-~*OtlM/Ji:B 
?H ;~?I  01 ~/NN,5 .  ~I,/.JK6 
G' Xtl ~ 7? .~tl / r,~ N 6 ? ~ ,/o K o " - . _ "  
> ~'\[?8101 gd,l> ~t' *~alDN G-0~/J  
> ~.~.  $t lVV*92/EP-~niEF - zSP 
i'~ 92XI g, ol  S'!gMtaNG.XI%VNN,3.?I/. 
> ~Xl2 J~ ~XI? J /NN? '~ I JF .O  
~ )I a~ IJ}:?_k ~ ) I /NNG -8 }/XSV-OHjLnt 
; ~.E}. 8t/VV*?)\[EP.E~/EF*/SF 
\] ~,J '_'6&}; .3} .~4j~q <?Jr 't ~r~? j  ~.II?41~.L ~> k12I > 3~t?ItG'?3 )ldlt ~"/q'41'lG.?l ', KG 
. . . . . . . . .  21 x oH,~t'~011:,11 ~,,ki~_~ ~HYj9 : ; ' .3o  x ~,-a4 ~,i1,,'i~bJ 3* /SP  ? 
. . . .  2\[_ '. f~ l _  ___ -~-~A Xll, ) lge.  131Ct1~21 2N.~gJ ~,{~0"IIM)~- . . . . . . . . . . . . .  I . . . . . . . . . .  ;111~ J 
::,~2!;,7~,-~ -~. ,~, ,~.~.  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ,~_  . . . . . .  
Figure 2. Bu i ld ing  Annotated  Corpus Usiug KCAT 
4. Application to Build Large Corpora 
Based on the proposed method~ we have 
imrdemented, a corpus--annotating tool for 
Koreart which is named as KCAT(Korean 
Corpus Annotating 'Fool). The process of 
building large corpora with KCAT is as lbllows: 
1. The lexical roles in the rule base are 
applied to a raw corpu::,. If the rule base i!; 
empty, nothing will be done. 
2. The sy,~;tem akes a candidate li';t. 
3. Ilunmn expert produces the lexical 1.ules 
for the words in the candidate list. 
4. The .~;ystem tags the corpus by using the 
lexical rHles and a stochastic t,l~,~.c~. 
5. Hunmn manually con?cots errors caused by 
the stochastic tagger, and lexical rules for 
those errors are also stored in the 
role--base. 
6. For other corpus, repeat the steps 1 
through 5. 
Figure 2 shows a screenshot f KCAT. In this 
figure, "A' window represents the list of raw 
corpus arm a "B' window contains the contcnt of 
the selected raw corpus in the window A. The 
tagging result is displayed in the window 'C'. 
Words beginning with ">' are tagged by a 
stocha,,;tic la-<,e, and the other words are ta~Eed 
by lexical rules. 
We can -et the more lexical rules as the 
ta,,,,itw process is prom-esscd. Therefore, we can 
expect that the aecunu-y and the reduction rate 
C 
of human htbor are increased a~ long as the 
tagging process is corltilmed. 
5. Experimental Results 
In order to estimate tim experimental results 
of our system, we collected the highly 
ambiguous words and frequently occurring 
words in our test corpus with 50,004 words. 
\]able I shows reductions in human intervention 
required to armotate the raw coums when we use 
lexical rules lbr the highly ambiguous words and 
the frequently occurring words respectively. The 
second colurnn shows that we examined the 
4,081 OCCLirrences of 2,088 words with tag 
choices above 7 and produced 4,081 lexical 
rules covering 4,832 occurrences of the corpl_lS. 
In this case, the reduction rate of human 
intervention is 1.5%. ~ The third column shows 
that we exalnined thc 6,845 occurrences of 511 
words with ficqucncy above 10 and produced 
6,845 lexical rules covering 15,4 l 8 occurrences 
of the corpus. In tiffs case, the reduction rate of 
human intervention is 17%. 2 
The last row in the table shows how 
intbrnmtive the rules are. We measured it by the 
inq-~iovement rate of stochastic tagging ;_!.l'l.el- the 
rules arc applied. From these experimental 
result.~;, wc can judge that rule-acquisition from 
flcquelatly occurring words is preferable. 
i (4,~., _4,(),v; l ) / 50,004 
~. ( 15,41 x-6,g~b ) / 50,004 
1099 
Table 1. Reduction in human Intervention 
I Type of word 
lbr rule 
acquisition 
Number of 
words 
Ambiguous 
words (_>7) 
Frequently 
occurring 
words (_>10) 
4832(9.6?/,,) 15418(30%) 
Number of 408 l 6845 
lexical rules 
Decrement of 1.5% 17% 
h u lll a 11 
intervention 
hnprovement 1.6% 3.7% 
of tagging 
accttracy (94.1-92.5%) (95.2-92.5%) 
Table 2 shows the results of our experiments on 
tile applicability of lexical rules. We measure it 
by the improyement rate of stochastic tagging 
alter the rules acquired from other corpus are 
applied. 
The third row shows that we annotate a training 
corpus with 10,032 words and produce 631 
lexieal rules, which can be applied to another 
test corpus to reduce tile number of the 
stochastic ta-,,in,, errors frorn 697 to 623. 3 
The ~brth and fifth row show that as the number 
of lexical rules is increased, the number of the 
errors of the tagger is decreased on the test 
corpus. 
These experilnental results demonstrate tile 
promise of gradual decrement of human 
intervention and improvement of tagging 
accuracy in annotating corpora. 
Table 2. Applicability of Lexical Rules 
Size of tile The nunaber The number of 
corpus of lexical stochastic 
roles errors 
0 0 697 
10,032 631 62.3 
20,047 136l 565 
_~( ,049 2091 538 
6. Conclusion 
The main goal of our work is to dcvelop an 
efficiclat tool which supports to build a very 
3 Our test corpus includes 10,015 words 
accurately and consistently POS annotated 
corpus with nlinilnal hunmn labor. To achieve 
the goal, we have proposed a POS ta,,-in- tool 
named KCAT which can use human linguistic 
knowledge as a lexical rule form. Once a lexical 
role is acquired, the hutnan expert doesn't need 
to spend titne in tagging the same word in the 
same context. By using the lexical roles, we 
could have very accurate and consistent results 
as well its reducing the amount of the hurnan 
labor. 
It is obvious that the more lexical roles the 
tool acquires the higher accuracy and 
consistency it achieves. But it still requires a lot 
of human labor and cost to acquire many lexical 
rules. And, as the number of the lexical rules is 
increased, the speed of rule application is 
decreased. To overcome the barriers, we try to 
find a way of rule generalization and a more 
efficient way of rule encoding scheme like the 
finite-state atttomata(Roche, 1995). 
Furthermore, we will use the distance of the 
best and second tag's probabilities to classify 
reliable automatic tagging result and unreliable 
ta,,,,in,, result(Brants, 1999). 
Refere\[Ices 
Brants~ T. Skut, W. and Uszkoreit, H. (1999) 
A),ntac'tic /hmotatio/1 of  a German N~.-*lri'Spal)e\]" 
Coums. In "Jourrlees ATALA", pp.69?76. 
Kim, J. D. Lira, H. S. and Rim, H. C. (1998) 
Morl)henle-Unit POS Tagging Mode/ 
Considering Eojeol-Spacing. In "Proc. of the 
10th ttangul and Korean Information 
Processing Conference", pp.3-8. 
Lee, J. K. (1996) Eojeol-tmit rule Based POS 
tag~in?~ with minimal human intervention. M. 
S dissertation, Dept. of Computer Science and 
Engineering, Korea Univ. 
Lira, H. S. Kim, J. D. and Rim, H. C. (1996) .4 
Korean 1)'an.@)rmation-I~axed POS Tagger 
with Lexical h!fi)rmation ojmi.vtag,4ed Eojeo\[. 
In "Proc. of the 2nd Korea-China Joint 
Symposium on Oriental Language 
Computing", pp. 119-124. 
Roche, E. and Schabes, Y. (1995)Determini.s'tic 
Part-o.f-St)eect~ Taggi/Ig with Fi//te-State 
7?aHsduc'er. Computational Linguistics, 21/2, 
pp. 227-253. 
1100 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 410?418,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Bridging Lexical Gaps between Queries and Questions on
Large Online Q&A Collections with Compact Translation Models
Jung-Tae Lee? and Sang-Bum Kim? and Young-In Song? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Search Business Team, SK Telecom, Seoul, Korea
?Dept. of Computer Science & Engineering, Korea University, Seoul, Korea
{jtlee,sbkim,song,rim}@nlp.korea.ac.kr
Abstract
Lexical gaps between queries and questions
(documents) have been a major issue in ques-
tion retrieval on large online question and
answer (Q&A) collections. Previous stud-
ies address the issue by implicitly expanding
queries with the help of translation models
pre-constructed using statistical techniques.
However, since it is possible for unimpor-
tant words (e.g., non-topical words, common
words) to be included in the translation mod-
els, a lack of noise control on the models can
cause degradation of retrieval performance.
This paper investigates a number of empirical
methods for eliminating unimportant words in
order to construct compact translation mod-
els for retrieval purposes. Experiments con-
ducted on a real world Q&A collection show
that substantial improvements in retrieval per-
formance can be achieved by using compact
translation models.
1 Introduction
Community-driven question answering services,
such as Yahoo! Answers1 and Live Search QnA2,
have been rapidly gaining popularity among Web
users interested in sharing information online. By
inducing users to collaboratively submit questions
and answer questions posed by other users, large
amounts of information have been collected in the
form of question and answer (Q&A) pairs in recent
years. This user-generated information is a valu-
able resource for many information seekers, because
1http://answers.yahoo.com/
2http://qna.live.com/
users can acquire information straightforwardly by
searching through answered questions that satisfy
their information need.
Retrieval models for such Q&A collections
should manage to handle the lexical gaps or word
mismatches between user questions (queries) and
answered questions in the collection. Consider the
two following examples of questions that are seman-
tically similar to each other:
? ?Where can I get cheap airplane tickets??
? ?Any travel website for low airfares??
Conventional word-based retrieval models would
fail to capture the similarity between the two, be-
cause they have no words in common. To bridge the
query-question gap, prior work on Q&A retrieval by
Jeon et al (2005) implicitly expands queries with the
use of pre-constructed translation models, which lets
you generate query words not in a question by trans-
lation to alternate words that are related. In prac-
tice, these translation models are often constructed
using statistical machine translation techniques that
primarily rely on word co-occurrence statistics ob-
tained from parallel strings (e.g., question-answer
pairs).
A critical issue of the translation-based ap-
proaches is the quality of translation models con-
structed in advance. If no noise control is conducted
during the construction, it is possible for translation
models to contain ?unnecessary? translations (i.e.,
translating a word into an unimportant word, such as
a non-topical or common word). In the query expan-
sion viewpoint, an attempt to identify and decrease
410
the proportion of unnecessary translations in a trans-
lation model may produce an effect of ?selective?
implicit query expansion and result in improved re-
trieval. However, prior work on translation-based
Q&A retrieval does not recognize this issue and uses
the translation model as it is; essentially no attention
seems to have been paid to improving the perfor-
mance of the translation-based approach by enhanc-
ing the quality of translation models.
In this paper, we explore a number of empiri-
cal methods for selecting and eliminating unimpor-
tant words from parallel strings to avoid unnecessary
translations from being learned in translation models
built for retrieval purposes. We use the term compact
translation models to refer to the resulting models,
since the total number of parameters for modeling
translations would be minimized naturally. We also
present experiments in which compact translation
models are used in Q&A retrieval. The main goal of
our study is to investigate if and how compact trans-
lation models can improve the performance of Q&A
retrieval.
The rest of this paper is organized as follows.
The next section introduces a translation-based re-
trieval model and accompanying techniques used to
retrieve query-relevant questions. Section 3 presents
a number of empirical ways to select and eliminate
unimportant words from parallel strings for training
compact translation models. Section 4 summarizes
the compact translation models we built for retrieval
experiments. Section 5 presents and discusses the
results of retrieval experiments. Section 6 presents
related works. Finally, the last section concludes the
paper and discusses future directions.
2 Translation-based Retrieval Model
This section introduces the translation-based lan-
guage modeling approach to retrieval that has been
used to bridge the lexical gap between queries and
already-answered questions in this paper.
In the basic language modeling framework for re-
trieval (Ponte and Croft, 1998), the similarity be-
tween a query Q and a document D for ranking may
be modeled as the probability of the document lan-
guage model MD built from D generating Q:
sim(Q,D) ? P (Q|MD) (1)
Assuming that query words occur independently
given a particular document language model, the
query-likelihood P (Q|MD) is calculated as:
P (Q|MD) =
?
q?Q
P (q|MD) (2)
where q represents a query word.
To avoid zero probabilities in document language
models, a mixture between a document-specific
multinomial distribution and a multinomial distribu-
tion estimated from the entire document collection
is widely used in practice:
P (Q|MD) =
?
q?Q
[
(1? ?) ? P (q|MD)
+? ? P (q|MC)
]
(3)
where 0 < ? < 1 and MC represents a language
model built from the entire collection. The probabil-
ities P (w|MD) and P (w|MC) are calculated using
maximum likelihood estimation.
The basic language modeling framework does not
address the issue of lexical gaps between queries
and question. Berger and Lafferty (1999) viewed
information retrieval as statistical document-query
translation and introduced translation models to map
query words to document words. Assuming that
a translation model can be represented by a condi-
tional probability distribution of translation T (?|?)
between words, we can model P (q|MD) in Equa-
tion 3 as:
P (q|MD) =
?
w?D
T (q|w)P (w|MD) (4)
where w represents a document word.3
The translation probability T (q|w) virtually rep-
resents the degree of relationship between query
word q and document word w captured in a differ-
ent, machine translation setting. Then, in the tra-
ditional information retrieval viewpoint, the use of
translation models produce an implicit query expan-
sion effect, since query words not in a document are
mapped to related words in the document. This im-
plies that translation-based retrieval models would
make positive contributions to retrieval performance
only when the pre-constructed translation models
have reliable translation probability distributions.
3The formulation of our retrieval model is basically equiva-
lent to the approach of Jeon et al (2005).
411
2.1 IBM Translation Model 1
Obviously, we need to build a translation model in
advance. Usually the IBM Model 1, developed in
the statistical machine translation field (Brown et al,
1993), is used to construct translation models for
retrieval purposes in practice. Specifically, given a
number of parallel strings, the IBM Model 1 learns
the translation probability from a source word s to a
target word t as:
T (t|s) = ??1s
N?
i
c(t|s;Ji) (5)
where ?s is a normalization factor to make the sum
of translation probabilities for the word s equal to 1,
N is the number of parallel string pairs, and Ji is the
ith parallel string pair. c(t|s; Ji) is calculated as:
c(t|s; Ji) =
( P (t|s)
P (t|s1) + ? ? ?+ P (t|sn)
)
?freqt,Ji ? freqs,Ji (6)
where {s1, . . . , sn} are words in the source text in
J i. freqt,Ji and freqs,Ji are the number of times
that t and s occur in Ji, respectively.
Given the initial values of T (t|s), Equations (5)
and (6) are used to update T (t|s) repeatedly until
the probabilities converge, in an EM-based manner.
Note that the IBM Model 1 solely relies on
word co-occurrence statistics obtained from paral-
lel strings in order to learn translation probabilities.
This implies that if parallel strings have unimportant
words, a resulted translation model based on IBM
Model 1 may contain unimportant words with non-
zero translation probabilities.
We alleviate this drawback by eliminating unim-
portant words from parallel strings, avoiding them
from being included in the conditional translation
probability distribution. This naturally induces the
construction of compact translation models.
2.2 Gathering Parallel Strings from Q&A
Collections
The construction of statistical translation models
previously discussed requires a corpus consisting of
parallel strings. Since monolingual parallel texts are
generally not available in real world, one must arti-
ficially generate a ?synthetic? parallel corpus.
Question and answer as parallel pairs: The
simplest approach is to directly employ questions
and their answers in the collections by setting ei-
ther as source strings and the other as target strings,
with the assumption that a question and its cor-
responding answer are naturally parallel to each
other. Formally, if we have a Q&A collection as
C = {D1, D2, . . . , Dn}, where Di refers to an ith
Q&A data consisting of a question qi and its an-
swer ai, we can construct a parallel corpus C ? as
{(q1, a1), . . . , (qn, an)}?{(a1, q1), . . . , (an, qn)} =
C ? where each element (s, t) refers to a parallel pair
consisting of source string s and target string t. The
number of parallel string samples would eventually
be twice the size of the collections.
Similar questions as parallel pairs: Jeon et
al. (2005) proposed an alternative way of auto-
matically collecting a relatively larger set of par-
allel strings from Q&A collections. Motivated
by the observation that many semantically identi-
cal questions can be found in typical Q&A collec-
tions, they used similarities between answers cal-
culated by conventional word-based retrieval mod-
els to automatically group questions in a Q&A col-
lection as pairs. Formally, two question strings qi
and qj would be included in a parallel corpus C ?
as {(qi, qj), (qj , qi)} ? C ? only if their answer
strings ai and aj have a similarity higher than a
pre-defined threshold value. The similarity is cal-
culated as the reverse of the harmonic mean of ranks
as sim(ai, aj) = 12( 1rj + 1ri ), where rj and ri refer to
the rank of the aj and ai when ai and aj are given as
queries, respectively. This approach may artificially
produce much more parallel string pairs for training
the IBM Model 1 than the former approach, depend-
ing on the threshold value.4
To our knowledge, there has not been any study
comparing the effectiveness of the two approaches
yet. In this paper, we try both approaches and com-
pare the effectiveness in retrieval performance.
3 Eliminating Unimportant Words
We adopt a term weight ranking approach to iden-
tify and eliminate unimportant words from parallel
strings, assuming that a word in a string is unim-
4We have empirically set the threshold (0.05) for our exper-
iments.
412
Figure 1: Term weighting results of tf-idf and TextRank (window=3). Weighting is done on underlined words only.
portant if it holds a relatively low significance in the
document (Q&A pair) of which the string is origi-
nally taken from. Some issues may arise:
? How to assign a weight to each word in a doc-
ument for term ranking?
? How much to remove as unimportant words
from the ranked list?
The following subsections discuss strategies we use
to handle each of the issues above.
3.1 Assigning Term Weights
In this section, the two different term weighting
strategies are introduced.
tf-idf: The use of tf-idf weighting on evaluating
how unimportant a word is to a document seems to
be a good idea to begin with. We have used the fol-
lowing formulas to calculate the weight of word w
in document D:
tf -idfw,D = tfw,D ? idfw (7)
tfw,D = freqw,D|D| , idfw = log
|C|
dfw
where freqw,D refers to the number of times w oc-
curs in D, |D| refers to the size of D (in words), |C|
refers to the size of the document collection, and dfw
refers to the number of documents where w appears.
Eventually, words with low tf-idf weights may be
considered as unimportant.
TextRank: The task of term weighting, in fact,
has been often applied to the keyword extraction
task in natural language processing studies. As
an alternative term weighting approach, we have
used a variant of Mihalcea and Tarau (2004)?s Tex-
tRank, a graph-based ranking model for keyword
extraction which achieves state-of-the-art accuracy
without the need of deep linguistic knowledge or
domain-specific corpora.
Specifically, the ranking algorithm proceeds as
follows. First, words in a given document are added
as vertices in a graph G. Then, edges are added be-
tween words (vertices) if the words co-occur in a
fixed-sized window. The number of co-occurrences
becomes the weight of an edge. When the graph is
constructed, the score of each vertex is initialized
as 1, and the PageRank-based ranking algorithm is
run on the graph iteratively until convergence. The
TextRank score of a word w in document D at kth
iteration is defined as follows:
Rkw,D = (1? d)+ d ?
?
?j:(i,j)?G
ei,j?
?l:(j,l)?G ej,l
Rk?1w,D
(8)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
The assumption behind the use of the variant of
TextRank is that a word is likely to be an important
word in a document if it co-occurs frequently with
other important words in the document. Eventually,
words with low TextRank scores may be considered
as unimportant. The main differences of TextRank
compared to tf-idf is that it utilizes the context infor-
mation of words to assign term weights.
Figure 1 demonstrates that term weighting results
of TextRank and tf-idf are greatly different. Notice
that TextRank assigns low scores to words that co-
413
Corpus: (Q?A) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 90,441 73
25%Removal 90,326 (?0.1%) 73,021 (?19.3%) 73 (?0.0%) 44 (?39.7%)
50%Removal 90,230 (?0.2%) 72,225 (?20.1%) 72 (?1.4%) 43 (?41.1%)
75%Removal 88,763 (?1.9%) 65,268 (?27.8%) 53 (?27.4%) 38 (?47.9%)
Avg.Score 66,412 (?26.6%) 31,849 (?64.8%) 14 (?80.8%) 18 (?75.3%)
Table 1: Impact of various word elimination strategies on translation model construction using (Q?A) corpus.
Corpus: (Q?Q) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 34,485 442
25%Removal 34,374 (?0.3%) 26,900 (?22.0%) 437 (?1.1%) 282 (?36.2%)
50%Removal 34,262 (?0.6%) 26,421 (?23.4%) 423 (?4.3%) 274 (?38.0%)
75%Removal 32,813 (?4.8%) 23,354 (?32.3%) 288 (?34.8%) 213 (?51.8%)
Avg.Score 28,613 (?17.0%) 16,492 (?52.2%) 163 (?63.1%) 164 (?62.9%)
Table 2: Impact of various word elimination strategies on translation model construction using (Q?Q) corpus.
occur only with stopwords. This implies that Tex-
tRank weighs terms more ?strictly? than the tf-idf
approach, with use of contexts of words.
3.2 Deciding the Quantity to be Removed from
Ranked List
Once a final score (either tf-idf or TextRank score)
is obtained for each word, we create a list of words
ranked in decreasing order of their scores and elim-
inate the ones at lower ranks as unimportant words.
The question here is how to decide the proportion or
quantity to be removed from the ranked list.
Removing a fixed proportion: The first ap-
proach we have used is to decide the number of
unimportant words based on the size of the original
string. For our experiments, we manually vary the
proportion to be removed as 25%, 50%, and 75%.
For instance, if the proportion is set to 50% and an
original string consists of ten words, at most five
words would be remained as important words.
Using average score as threshold: We also have
used an alternate approach to deciding the quantity.
Instead of eliminating a fixed proportion, words are
removed if their score is lower than the average score
of all words in a document. This approach decides
the proportion to be removed more flexibly than the
former approach.
4 Building Compact Translation Models
We have initially built two parallel corpora from
a Q&A collection5, denoted as (Q?A) corpus and
(Q?Q) corpus henceforth, by varying the methods
in which parallel strings are gathered (described in
Section 2.2). The (Q?A) corpus consists of 85,938
parallel string pairs, and the (Q?Q) corpus contains
575,649 parallel string pairs.
In order to build compact translation models, we
have preprocessed the parallel corpus using differ-
ent word elimination strategies so that unimpor-
tant words would be removed from parallel strings.
We have also used a stoplist6 consisting of 429
words to remove stopwords. The out-of-the-box
GIZA++7 (Och and Ney, 2004) has been used to
learn translation models using the pre-processed par-
allel corpus for our retrieval experiments. We have
also trained initial translation models, using a par-
allel corpus from which only the stopwords are re-
moved, to compare with the compact translation
models.
Eventually, the number of parameters needed
for modeling translations would be minimized if
unimportant words are eliminated with different ap-
5Details on this data will be introduced in the next section.
6http://truereader.com/manuals/onix/stopwords1.html
7http://www.fjoch.com/GIZA++.html
414
proaches. Table 1 and 2 shows the impact of various
word elimination strategies on the construction of
compact translation models using the (Q?A) corpus
and the (Q?Q) corpus, respectively. The two tables
report the size of the vocabulary contained and the
average number of translations per word in the re-
sulting compact translation models, along with per-
centage decreases with respect to the initial transla-
tion models in which only stopwords are removed.
We make these observations:
? The translation models learned from the (Q?Q)
corpus have less vocabularies but more aver-
age translations per word than the ones learned
from the (Q?A) corpus. This result implies that
a large amount of noise may have been cre-
ated inevitably when a large number of parallel
strings (pairs of similar questions) were artifi-
cially gathered from the Q&A collection.
? The TextRank strategy tends to eliminate larger
sets of words as unimportant words than the
tf-idf strategy when a fixed proportion is re-
moved, regardless of the corpus type. Recall
that the TextRank approach assigns weights to
words more strictly by using contexts of words.
? The approach to remove words according to
the average weight of a document (denoted as
Avg.Score) tends to eliminate relatively larger
portions of words as unimportant words than
any of the fixed-proportion strategies, regard-
less of either the corpus type or the ranking
strategy.
5 Retrieval Experiments
Experiments have been conducted on a real world
Q&A collection to demonstrate the effectiveness of
compact translation models on Q&A retrieval.
5.1 Experimental Settings
In this section, four experimental settings for the
Q&A retrieval experiments are described in detail.
Data: For the experiments, Q&A data have been
collected from the Science domain of Yahoo! An-
swers, one of the most popular community-based
question answering service on the Web. We have
obtained a total of 43,001 questions with a best an-
swer (selected either by the questioner or by votes of
other users) by recursively traversing subcategories
of the Science domain, with up to 1,000 question
pages retrieved.8
Among the obtained Q&A pairs, 32 Q&A pairs
have been randomly selected as the test set, and the
remaining 42,969 questions have been the reference
set to be retrieved. Each Q&A pair has three text
fields: question title, question content, and answer.9
The fields of each Q&A pair in the test set are con-
sidered as various test queries; the question title,
the question content, and the answer are regarded
as a short query, a long query, and a supplementary
query, respectively. We have used long queries and
supplementary queries only in the relevance judg-
ment procedure. All retrieval experiments have been
conducted using short queries only.
Relevance judgments: To find relevant Q&A
pairs given a short query, we have employed a pool-
ing technique used in the TREC conference series.
We have pooled the top 40 Q&A pairs from each
retrieval results generated by varying the retrieval
algorithms, the search field, and the query type.
Popular word-based models, including the Okapi
BM25, query-likelihood language model, and pre-
vious translation-based models (Jeon et al, 2005),
have been used.10
Relevance judgments have been done by two stu-
dent volunteers (both fluent in English). Since
many community-based question answering ser-
vices present their search results in a hierarchical
fashion (i.e. a list of relevant questions is shown
first, and then the user chooses a specific question
from the list to see its answers), a Q&A pair has been
judged as relevant if its question is semantically sim-
ilar to the query; neither quality nor rightness of the
answer has not been considered. When a disagree-
ment has been made between two volunteers, one of
the authors has made the final judgment. As a result,
177 relevant Q&A pairs have been found in total for
the 32 short queries.
Baseline retrieval models: The proposed ap-
8Yahoo! Answers did not expose additional question pages
to external requests at the time of collecting the data.
9When collecting parallel strings from the Q&A collection,
we have put together the question title and the question content
as one question string.
10The retrieval model using compact translation models has
not been used in the pooling procedure.
415
proach to Q&A retrieval using compact translation
models (denoted as CTLM henceforth) is compared
to three baselines:
QLM: Query-likelihood language model for re-
trieval (equivalent to Equation 3, without use of
translation models). This model represents word-
based retrieval models widely used in practice.
TLM(Q?Q): Translation-based language model
for question retrieval (Jeon et al, 2005). This model
uses IBM Model 1 learned from the (Q?Q) corpus
of which stopwords are removed.
TLM(Q?A): A variant of the translation-based ap-
proach. This model uses IBM model 1 learned from
the (Q?A) corpus.
Evaluation metrics: We have reported the re-
trieval performance in terms of Mean Average Pre-
cision (MAP) and Mean R-Precision (R-Prec).
Average Precision can be computed based on the
precision at each relevant document in the ranking.
Mean Average Precision is defined as the mean of
the Average Precision values across the set of all
queries:
MAP (Q) = 1|Q|
?
q?Q
1
mq
mq?
k=1
Precision(Rk) (9)
where Q is the set of test queries, mq is the number
of relevant documents for a query q, Rk is the set of
ranked retrieval results from the top until rank posi-
tion k, and Precision(Rk) is the fraction of relevant
documents in Rk (Manning et al, 2008).
R-Precision is defined as the precision after
R documents have been retrieved where R is
the number of relevant documents for the current
query (Buckley and Voorhees, 2000). Mean R-
Precision is the mean of the R-Precisions across the
set of all queries.
We take MAP as our primary evaluation metric.
5.2 Experimental Results
Preliminary retrieval experiments have been con-
ducted using the baseline QLM and different fields
of Q&A data as retrieval unit. Table 3 shows the
effectiveness of each field.
The results imply that the question title field is the
most important field in our Yahoo! Answers collec-
tion; this also supports the observation presented by
Retrieval unit MAP R-Prec
Question title 0.1031 0.2396
Question content 0.0422 0.0999
Answer 0.0566 0.1062
Table 3: Preliminary retrieval results.
Model MAP R-Prec
(%chg) (%chg)
QLM 0.1031 0.2396
TLM(Q?Q)* 0.1121 0.2251
(49%) (?6%)
CTLM(Q?Q) 0.1415 0.2425
(437%) (41%)
TLM(Q?A) 0.1935 0.3135
(488%) (431%)
CTLM(Q?A) 0.2095 0.3585
(4103%) (450%)
Table 4: Comparisons with three baseline retrieval mod-
els. * indicates that it is equivalent to Jeon et al (2005)?s
approach. MAP improvements of CTLMs have been
tested to be statistically significant using paired t-test.
Jeon et al (2005). Based on the preliminary obser-
vations, all retrieval models tested in this paper have
ranked Q&A pairs according to the similarity scores
between queries and question titles.
Table 4 presents the comparison results of three
baseline retrieval models and the proposed CTLMs.
For each method, the best performance after empir-
ical ? parameter tuning according to MAP is pre-
sented.
Notice that both the TLMs and CTLMs have out-
performed the word-based QLM. This implies that
word-based models that do not address the issue of
lexical gaps between queries and questions often fail
to retrieve relevant Q&A data that have little word
overlap with queries, as noted by Jeon et al (2005).
Moreover, notice that the proposed CTLMs have
achieved significantly better performances in all
evaluation metrics than both QLM and TLMs, regard-
less of the parallel corpus in which the incorporated
translation models are trained from. This is a clear
indication that the use of compact translation models
built with appropriate word elimination strategies is
effective in closing the query-question lexical gaps
416
(Q?Q) MAP (%chg)
tf-idf TextRank
Initial 0.1121
25%Rmv 0.1141 (41.8) 0.1308 (416.7)
50%Rmv 0.1261 (412.5) 0.1334 (419.00)
75%Rmv 0.1115 (?0.5) 0.1160 (43.5)
Avg.Score 0.1056 (?5.8) 0.1415 (426.2)
Table 5: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?Q).
(Q?A) MAP (%chg)
tf-idf TextRank
Initial 0.1935
25%Rmv 0.2095 (48.3) 0.1733 (?10.4)
50%Rmv 0.2085 (47.8) 0.1623 (?16.1)
75%Rmv 0.1449 (?25.1) 0.1515 (?21.7)
Avg.Score 0.1168 (?39.6) 0.1124 (?41.9)
Table 6: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?A).
for improving the performance of question retrieval
in the context of language modeling framework.
Note that the retrieval performance varies by the
type of training corpus; CTLM(Q?A) has outper-
formed CTLM(Q?Q) significantly. This proves the
statement we made earlier that the (Q?Q) corpus
would contain much noise since the translation mod-
els learned from the (Q?Q) corpus tend to have
smaller vocabulary sizes but significantly more aver-
age translations per word than the ones learned from
the (Q?A) corpus.
Table 5 and 6 show the effect of various word
elimination strategies on the retrieval performance
of CTLMs in which the incorporated compact trans-
lation models are trained from the (Q?Q) corpus and
the (Q?A) corpus, respectively. It is interesting to
note that the importance of modifications in word
elimination strategies also varies by the type of train-
ing corpus.
The retrieval results indicate that when the trans-
lation model is trained from the ?less noisy? (Q?A)
corpus, eliminating a relatively large proportions of
words may hurt the retrieval performance of CTLM.
In the case when the translation model is trained
from the ?noisy? (Q?Q) corpus, a better retrieval
performance may be achieved if words are elimi-
nated appropriately to a certain extent.
In terms of weighting scheme, the TextRank ap-
proach, which is more ?strict? than tf-idf in elim-
inating unimportant words, has led comparatively
higher retrieval performances on all levels of re-
moval quantity when the translation model has been
trained from the ?noisy? (Q?Q) corpus. On the con-
trary, the ?less strict? tf-idf approach has led better
performances when the translation model has been
trained from the ?less noisy? (Q?A) corpus.
In summary, the results imply that the perfor-
mance of translation-based retrieval models can be
significantly improved when strategies for building
of compact translation models are chosen properly,
regarding the expected noise level of the parallel cor-
pus for training the translation models. In a case
where a noisy parallel corpus is given for training
of translation models, it is better to get rid of noise
as much as possible by using ?strict? term weight-
ing algorithms; when a less noisy parallel corpus is
given for building the translation models, a tolerant
approach would yield better retrieval performance.
6 Related Works
Our work is most closely related to Jeon et
al. (2005)?s work, which addresses the issue of
word mismatch between queries and questions in
large online Q&A collections by using translation-
based methods. Apart from their work, there have
been some related works on applying translation-
based methods for retrieving FAQ data. Berger et
al. (2000) report some of the earliest work on FAQ
retrieval using statistical retrieval models, includ-
ing translation-based approaches, with a small set
of FAQ data. Soricut and Brill (2004) present an an-
swer passage retrieval system that is trained from 1
million FAQs collected from the Web using trans-
lation methods. Riezler et al (2007) demonstrate
the advantages of translation-based approach to an-
swer retrieval by utilizing a more complex trans-
lation model also trained from a large amount of
data extracted from FAQs on the Web. Although all
of these translation-based approaches are based on
the statistical translation models, including the IBM
Model 1, none of them focus on addressing the noise
issues in translation models.
417
7 Conclusion and Future Work
Bridging the query-question gap has been a major is-
sue in retrieval models for large online Q&A collec-
tions. In this paper, we have shown that the perfor-
mance of translation-based retrieval on real online
Q&A collections can be significantly improved by
using compact translation models of which the noise
(unimportant word translations) is properly reduced.
We have also observed that the performance en-
hancement may be achieved by choosing the appro-
priate strategies regarding the strictness of various
term weighting algorithms and the expected noise
level of the parallel data for learning such transla-
tion models.
Future work will focus on testing the effective-
ness of the proposed method on a larger set of Q&A
collections with broader domains. Since the pro-
posed approach cannot handle many-to-one or one-
to-many word transformations, we also plan to in-
vestigate the effectiveness of phrase-based transla-
tion models in closing gaps between queries and
questions for further enhancement of Q&A retrieval.
Acknowledgments
This work was supported by Microsoft Research
Asia. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the sponsor.
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the Lexical
Chasm: Statistical Approaches to Answer-Finding. In
Proceedings of the 23rd Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval, pages 192?199.
Adam Berger and John Lafferty. 1999. Information Re-
trieval as Statistical Translation. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 222?229.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Buckley and Ellen M. Voorhees. 2000. Evaluating
Evaluation Measure Stability. In Proceedings of the
23rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 33?40.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding Similar Questions in Large Question and An-
swer Archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 84?90.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Text. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 404?411.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Jay M. Ponte and W. Bruce Croft. 1998. A Language
Modeling Approach to Information Retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer
Retrieval. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
pages 464?471.
Radu Soricut and Eric Brill. 2004. Automatic Question
Answering: Beyond the Factoid. In Proceedings of
the 2004 Human Language Technology and Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 57?64.
418
Two-Phase Biomedical Named Entity
Recognition Using A Hybrid Method
Seonho Kim1, Juntae Yoon2, Kyung-Mi Park1, and Hae-Chang Rim1
1 Dept. of Computer Science and Engineering,
Korea University, Seoul, Korea
2 NLP Lab. Daumsoft Inc. Seoul, Korea
Abstract. Biomedical named entity recognition (NER) is a difficult
problem in biomedical information processing due to the widespread am-
biguity of terms out of context and extensive lexical variations. This pa-
per presents a two-phase biomedical NER consisting of term boundary
detection and semantic labeling. By dividing the problem, we can adopt
an effective model for each process. In our study, we use two exponential
models, conditional random fields and maximum entropy, at each phase.
Moreover, results by this machine learning based model are refined by
rule-based postprocessing implemented using a finite state method. Ex-
periments show it achieves the performance of F-score 71.19% on the
JNLPBA 2004 shared task of identifying 5 classes of biomedical NEs.
1 Introduction
Due to dynamic progress in biomedical literature, a vast amount of new infor-
mation and research results have been published and many of them are available
in the electronic form - for example, like the PubMed MedLine database. Thus,
automatic knowledge discovery and efficient information access are strongly de-
manded to curate domain databases, to find out relevant information, and to
integrate/update new information across an increasingly large body of scien-
tific articles. In particular, since most biomedical texts introduce specific no-
tations, acronyms, and innovative names to represent new concepts, relations,
processes, functions, locations, and events, automatic extraction of biomedical
terminologies and mining of their diverse usage are major challenges in biomed-
ical information processing system. In these processes, biomedical named entity
recognition (NER) is the core step to access the higher level of information.
In fact, there has been a wide range of research on NER like the NER task on
the standard newswire domain in the Message Understanding Conference (MUC-
6). In this task, the best system reported 95% accuracy in identifying seven types
of named entities (person, organization, location, time, date, money, and per-
cent). While the performance in the standard domain turned out to be quite good
as shown in the papers, that in the biomedical domain is not still satisfactory,
which is mainly due to the following characteristics of biomedical terminologies:
First, NEs have various naming conventions. For instance, some entities have
descriptive and expanded forms such as ?activated B cell lines, 47 kDa sterol
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 646?657, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Two-Phase Biomedical Named Entity Recognition 647
regulatory element binding factor?, whereas some entities appear in shortened
or abbreviated forms like ?EGFR? and ?EGF receptor? representing epidermal
growth factor receptor. Second, biomedical NEs have the widespread ambiguity
out of context. For instance, ?IL-2? can be doubly classified as ?protein? and
?DNA? according to its context. Third, biomedical NEs often comprise a nested
structure, for example ??DNA??protein?TNF alpha?/protein?gene?/DNA??. Ac-
cording to [13], 16.57% of biomedical terms in GENIA have cascaded construc-
tions. In the case, recognition of the longest terms is the main target in general.
However, in our evaluation task, when the embedded part of a term is regarded
as the meaningful or important class in the context, the term is labeled only with
the class of embedded one. Thus, identification of internal structures of NEs is
helpful to recognize correct NEs. In addition, more than one NE often share the
same head noun with a conjunction/disjunction or enumeration structure, for
instance, ?IFN-gamma and GM-CSF mRNA?, ?CD33+, CD56+, CD16- acute
leukemia?or ?antigen- or cAMP-activated Th2 cell?. Last, there is a lot of inter-
annotator disagreement. [7] reported that the inter-annotator agreement rate of
human experts was just 77.6% when performing gene/protein/mRNA classifica-
tion task manually.
Thus, a lot of term occurrences in real text would not be identified with sim-
ple dictionary look-up, despite the availability of many terminological databases,
as claimed in [12]. That is one of the reasons why machine learning approaches
are more dominant in biomedical NER than rule-based or dictionary-based ap-
proaches [5], even though existence of reliable training resources is very critical.
Accordingly, much work has been done on biomedical NER, based on ma-
chine learning techniques. [3] and [13] have used hidden Markov Model (HMM)
for biomedical NER where state transitions are made by semantic trigger fea-
tures. [4] and [11] have applied maximum entropy plus Markovian sequence based
models such as maximum entropy markov model (MEMM) and conditional ran-
dom fields (CRFs), which present a way for integrating different features such
as internal word spellings and morphological clues within an NE string and con-
textual clues surrounding the string in the sentence.
These works took an one-phase based approach where boundary detection
of named entities and semantic labeling come together. On the other hand, [9]
proposed a two-phase model in which the biomedical named entity recognition
process is divided into two processes of distinguishing biomedical named entities
from general terms and labeling the named entities with semantic classes that
they belong to. They use support vector machines (SVM) for each phase. How-
ever, the SVM does not provide an easy way for labeling Markov sequence data
like B following O and I following B in named entities. Furthermore, since this
system is tested on the GENIA corpus rather than JNLPBA 2004 shared task, we
cannot confirm the effectiveness of this approach on the ground of experiments
for common resources.
In this paper, we present a two-phase named entity recognition model: (1)
boundary detection for NEs and (2) term classification by semantic labeling.
The advantage of dividing the recognition process into two phase is that we can
648 S. Kim et al
select separately a discriminative feature set for each subtask, and moreover can
measure effectiveness of models at each phase. We use two exponential models
for this work, namely conditional random fields for boundary detection having
Markov sequence, and the maximum entropy model for semantic labeling. In ad-
dition, results from the machine learning based model are refined by a rule-based
postprocessing, which is implemented using a finite state transducer (FST). The
FST is constructed with the GENIA 3.02 corpus. We here focus on identification
of five classes of NEs, i.e. ?protein?, ?RNA?, ?DNA?, ?cell line?, and ?cell type?
and experiments are conducted on the training and evaluation set provided by
the shared task in COLING 2004 JNLPBA.
2 Training
2.1 Maximum Entropy and Conditional Random Fields
Before we describe the features used in our model, we briefly introduce the ME
and CRF model which we make use of. In the ME framework, the conditional
probability of predicting an outcome o given a history h is defined as follows:
p?(o|h) =
1
Z?(h)
exp
(
k
?
i=1
?ifi(h, o)
)
(1)
where fi(h, o) is a binary-valued feature function, ?i is the weighting parameter
of fi(h, o), k is the number of features, and Z?(h) is a normalization factor
for ?op?(o|h)=1. That is, the probability p?(o|h) is calculated by the weighted
sum of active features. Given an exponential model with k features and a set
of training data, empirical distribution, weights of the k features are trained to
maximize the model?s log-likelihood:
L(p) =
?
o,h
p?(h, o)log(o|h) (2)
Although the maximum entropy model above provides a powerful tool for
classification by integrating different features, it is not easy to model the Markov
sequence data. In this case, the CRF is used for a task of assigning label sequences
to a set of observation sequences. Based on the principle of maximum entropy,
a CRF has a single exponential model for the joint probability of the entire
sequence of labels given the observation sequence. The CRF is a special case of
the linear chain that corresponds to conditionally trained finite-state machine
and define conditional probability distributions of a particular label sequence s
given observation sequence o
p?(s|o) = 1Z(o)exp(
?k
j=1 ?jFj(s,o))
Fj(s,o) =
?n
i=1 fj(si?1, si,o, i)
(3)
Two-Phase Biomedical Named Entity Recognition 649
where s = s1 . . . sn, and o = o1 . . . on, Z(o) is a normalization factor, and each
feature is a transition function [8]. For example, we can think of the following
feature function.
fj(si?1, si,o, i) =
?
?
?
1 if si?1=B and si=I,
and the observation word at position i is ?gene??
0 otherwise
(4)
Our CRFs for term boundary detection have a first-order Markov dependency
between output tags. The label at position i, si is one of B, I and O. In contrast
to the ME model, since B is the beginning of a term, the transition from O to I
is not possible. CRFs constrain results to consider only reasonable paths. Thus,
total 8 combinations are possible for (si?1,si) and the most likely s can be found
with the Viterbi algorithm. The weights are set to maximize the conditional log
likelihood of labeled sequences in the training set using a quasi-Newton method
called L-BFGS [2].
2.2 Features for Term Boundary Detection
Table 1 shows features for the step of finding the boundary of biomedical terms.
Here, we give a supplementary description of a part of the features.
Table 1. Feature set for boundary detection (+:conjunction)
Model Feature Description
CRF, MEmarkov Word wi?1, wi?2, wi, wi+1, wi+2
CRF, MEmarkov Word Normalization normalization forms of the 5 words
CRF, MEmarkov POS POSwi?1 , POSwi , POSwi+1
CRF, MEmarkov Word Construction form WFwi
CRF, MEmarkov Word Characteristics WCwi?1 , WCwi , WCwi+1
CRF, MEmarkov Contextual Bigrams wi?1 + wi
wi + wi+1
wi+1 + wi+2
CRF, MEmarkov Contextual Trigrams wi?1 + wi + wi+1
CRF, MEmarkov Bigram POS POSwi?1 + POSwi
POSwi + POSwi+1
CRF, MEmarkov Trigram POS POSwi?1 + POSwi + POSwi+1
CRF, MEmarkov Modifier MODI(wi)
CRF, MEmarkov Header HEAD(wi)
CRF, MEmarkov SUFFIX SUFFIX(wi)
CRF, MEmarkov Chunk Type CTypewi
CRF, MEmarkov Chunk Type + Pre POS CTypewi + POSwi?1
MEmarkov Pre label labelwi?1
MEmarkov Pre label + Cur Word labelwi?1 + wi
? word and POS: 5 words(target word(wi), left two words, and right two
words) and three POS(POSwi?1 , POSwi , POSwi+1) are considered.
650 S. Kim et al
? word normalization: This feature contributes to word normalization. We
attempt to reduce a word to its stem or root form with a simple algorithm
which has rules for words containing plural, hyphen, and alphanumeric let-
ters. Specifically, the following patterns are considered.
(1) ?lymphocytes?, ?cells? ? ?lymphocyte?, ?cell?
(2) ?il-2?, ?il-2a?, ?il2a? ? ?il?
(3) ?5-lipoxygenase?, ?v-Abl? ? ?lipoxygenase?, ?abl?
(4) ?peri-kappa?or ?t-cell? has two normalization forms of ?peri?and?kappa?
and ?t? and ?cell? respectively.
(5) ?Ca2+-independent? has two roots of ?ca? and ?independent?.
(6) The root of digits is ?D?.
? informative suffix: This feature appears if a target word has a salient suffix
for boundary detection. The list of salient suffixes is obtained by relative
entropy [10].
? word construction form: This feature indicates how a target word is or-
thographically constructed. Word shapes refer to a mapping of each word
on equivalence classes that encodes with dashes, numerals, capitalizations,
lower letters, symbols, and so on. All spellings are represented with combina-
tions of the attributes1. For instance, the word construction form of ?IL-2?
would become ?IDASH-ALPNUM?.
? word characteristics: This feature appears if a word represents a DNA
sequence of ?A?,?C?,?G?,?T? or Greek letter such as beta or alpha, ordinal
index such as I, II or unit such as BU/ml, micron/mL. It is encoded with
?ACGT?, ?GREEK?, ?INDEX?, ?UNIT?.
? head/modifying information: If a word prefers the rightmost position
of terminologies, we regard it has the property of a head noun. On the
other hand, if a word frequently occurs in other positions, we regard it has
the property of a modifying noun. It can help to establish the beginning
and ending point of multi-word entities. We automatically extract 4,382
head nouns and 7,072 modifying nouns from the training data as shown in
Table 2.
? chunk-type information: This feature is also effective in determining the
position of a word in NEs, ?B?, ?I?, ?O? which means ?begin chunk?, ?in
chunk? and ?others?, respectively. We consider the chunk type of a target
word and the conjunction of the current chunk type and the POS of the
previous word to represent the structure of an NE.
We also tested an ME-based model for boundary detection. For this, we add
two special features : previous state (label) and conjunction of previous label
1 ?IDASH? (inter dash), ?EDASH? (end dash), ?SDASH? (start dash),
?CAP?(capitalization), ?LOW?(lowercase), ?MIX?(lowercase and capitaliza-
tion letters), ?NUM?(digit), ?ALPNUM?(alpha-numeric), ?SYM?(symbol),
?PUNC?(punctuation),and ?COMMA?(comma)
Two-Phase Biomedical Named Entity Recognition 651
Table 2. Examples of Head/Modifying Nouns
Modifying Nouns Head Nouns
nf-kappa cytokines
nuclear elements
activated assays
normal complexes
phorbol macrophages
viral molecules
inflammatory pathways
murine extracts
electrophoretic glucocorticoids
acute levels
intracellular responses
epstein-barr clones
cytoplasmic motifs
and current word to consider state transition. That is, a previous label can be
represented as a feature function in our model as follows:
fi(h, o) =
{
1 if pre label+tw=B+gene,o=I
0 otherwise
(5)
It means that the target word is likely to be inside a term (I), when the word
is ?gene? and the previous label is ?B?. In our model, the current label is de-
terministically assigned to the target word with considering the previous state
with the highest probability.
2.3 Features for Semantic Labeling
Table 3 shows features for semantic labeling with respect to recognized NEs.
? word contextual feature: We make use of three kinds of internal and ex-
ternal contextual features: words within identified NEs, their word normal-
ization forms, and words surrounding the NEs. In Table 3, NEw0 denotes
the rightmost word in an identified NE region. Moreover, the presence of
specific head nouns acting as functional words takes precedence when de-
termining the term class, even though many terms do not contain explicit
term category information. For example, functional words, such as ?factor?,
?receptor?, and ?protein? are very useful in determining protein class, and
?gene?, ?promoter?, and ?motif ? are clues for classifying DNA [5]. In gen-
eral, such functional words are often the last word of an entity. This is the
reason we consider the position where a word occurs in NEs along with the
word. For inside context features, we use non-positional word features as
well. As non-positional features, all words inside NEs are used.
? internal bigrams and trigrams: We consider the rightmost bigrams/
trigrams inside identified NEs and the normalized bigrams/trigrams.
652 S. Kim et al
Table 3. Feature Set for Semantic Classification
Feature description
Word Features (positional) NEwothers , NEw?3 , NEw?2 , NEw?1 , NEw0
Word Features (non-positional) AllNEw
Word Normalization (positional) WFNEw?3 , WFNEw?2 , WFNEw?1 , WFNEw0
Left Context(Words Surrounding an NE) LCW?2, LCW?1
Right Context RCW+1, RCW+2
Internal Bigrams NEw?1 + NEw0
Internal Trigrams NEw?2 + NEw?1 + NEw0
Normalized Internal Bigrams WFNEw?1 + WFNEw0
Normalized Internal Trigrams NEw?2 + NEw?1 + NEw0
IDASH-word related Bigrams/Trigrams
Keyword KEYWORD(NEi)
? IDASH-word related bigrams/trigrams: This feature appears if NEw0
or NEw?1 contains dash characters. In this case, the bigram/trigram are
additionally formed by removing all dashes from the spelling. It is useful to
deal with lexical variants.
? keywords: This feature appears if the identified NE is informative key-
word with respect to a specific class. The keywords set comprises terms
obtained by the relative entropy between general and biomedical domain
corpora.
3 Rule-Based Postprocessing
A rule-based method can be used to correct errors by NER based on machine
learning. For example, the CRFs tag ?IL-2 receptor expression? as ?B I I?,
since the NEs ended with ?receptor expression? in training data almost belong
to ?other name? class even if the NEs ended with ?receptor? belong to ?pro-
tein? class. It should be actually tagged as ?B I O?. That kind of errors is
caused mainly by the cascaded phenomenon in biomedical names. Since our sys-
tem considers all NEs belonging to other classes in the recognition phase, it
tends to recognize the longest ones. That is, in the term classification phase,
such NEs are classified as ?other? class and are ignored. Thus, the system
losts embedded NEs although the training and evaluation set in fact tends to
consider only the embedded NE when the embedded one is more meaningful
or important.
This error correction is conducted by the rule-based method, i.e. If condi-
tion THEN action. For example, the rule ?IF wi?2=IL-2, wi?1=receptor and
wi=expression THEN replace the tag of wi with O? can be applied for the above
case. We use a finite state transducer for this rule-based transformation, which
is easy to understand with given lexical rules, and very efficient. Rules used for
the FST are acquired from the GENIA corpus. We first retrieved all NEs in-
cluding embedded NEs and longest NEs from GENIA 3.02 corpus and change
Two-Phase Biomedical Named Entity Recognition 653
IL-2/B gene/I
IL-2/O gene/O expression/O
Fig. 1. Non-Deterministic FST
IL-2/? gene/ ?
expression/OOO
? /BI
Fig. 2. Deterministic FST
the outputs of all other classes except the target 5 classes to O. That is, the
input of FST is a sequence of words in a sentence and the output is categories
corresponding to the words.
Then, we removed the rules in conflict with NE information from the training
corpus. These rules are non-deterministic (Figure 1), and we can change it to
the deterministic FST (Figure 2) since the lengths of NEs are finite. The deter-
ministic FST is made by defining the final output function for the deterministic
behavior of the transducer, delaying the output. The deterministic FST is de-
fined as follows: (?1, ?2, Q, i, F, ?, ?, ?), where ?1 is a finite input alphabet; ?2
is a finite output alphabet; Q is a finite set of states or vertices; i ? Q is the
initial state; F ? Q is the set of final states; ? is the deterministic state transi-
tion function that maps Q ? ?1 on Q; ? is the deterministic emission function
that maps Q ? ?1 on ??2 and ? : F ? ??2 is the final output function for the
deterministic behavior of the transducer.
4 Evaluation
4.1 Experimental Environments
In the shared task, only biomedical named entities which belong to 5 specific
classes are annotated in the given training data. That is, terms belonging to
other classes in GENIA are excluded from the recognition target. However, we
consider all NEs in the boundary detection step since we separate the NER
task into two phases. Thus, in order to utilize other class terms, we additionally
annotated ?O? class words in the training data where they corresponds to other
classes such as other organic compound, lipid, and multi cell in GENIA 3.02p
version corpus. During the annotation, we only consider the longest NEs on
654 S. Kim et al
Table 4. Number of training examples
RNA DNA cell line cell type protein other
472 5,370 2,236 2,084 16,042 11,475
GENIA. As a consequence, we find all biomedical named entities in text at the
term detection phase. Then, biomedical NEs classified as other class are changed
to O at the semantic labeling phase. The total words that belong to other class
turned out to be 25,987. Table 4 shows the number of NEs with respect to each
class on the training data. In our experiments, a quasi-Newton method called the
L-BFGS with Gaussian Prior smoothing is applied for parameter estimation [2].
4.2 Experimental Results
Table 5 shows the overall performance on the evaluation data. Our system
achieves an F-score of 71.19%. As shown in the table, the performance of NER
for cell line class was not good, because its boundary recognition is not so good
as other classes. Also, Table 6 shows the results of semantic classification. In par-
ticular, the system often confuses protein with DNA, and cell line with cell type.
Among the correctly identified 7,093 terms, 790 terms were misclassified.
Table 7 shows the performance of each phase. Our system obtains 76.88%
F-score in the boundary detection task and, using 100% correctly recognized
terms from annotated test data, 90.54% F-score in the semantic classification
task. Currently, since we cannot directly assess the accuracy of the term detection
process on the evaluation set because of other class words, the 75% of the training
data were used for training and the rest for testing.
Table 5. Overall performance on the evaluation data
Fully Correct Left Correct Right Correct
Class Recall Precision F-score F-score F-score
protein 76.30 69.71 72.85 77.60 79.15
DNA 67.80 64.91 66.33 68.36 74.57
RNA 73.73 63.04 67.97 71.09 74.22
cell line 57.40 54.88 56.11 59.04 65.69
cell type 70.12 77.64 73.69 74.89 81.51
overall 72.77 69.68 71.19 74.75 78.23
Table 6. Confusion matrix over evaluation data
gold/sys protein DNA RNA cell line cell type other
protein 0 72 3 1 4 267
DNA 97 0 0 0 0 49
RNA 11 0 0 0 0 0
cell line 10 1 0 0 63 37
cell type 21 0 0 92 0 57
Two-Phase Biomedical Named Entity Recognition 655
Table 7. Performance of term detection and semantic classification
Recall Precision F-score
term detection (MEMarkov) 74.03 75.31 74.67
term detection (CRF) 76.14 77.64 76.88
semantic classification 87.50 93.81 90.54
overall NER 72.77 69.68 71.19
Table 8. Performance of NE recognition methods (one-phase vs. two-phase)
method Recall Precision F-score
one-phase 64.23 63.13 63.68
two-phase(baseline2) 66.24 64.54 65.38
(only 5 classes)
two-phase(baseline2) 68.51 67.58 68.04
(5 classes+other class)
Also, we compared our model with the one-phase model. The detailed results
are presented in Table 8. Both of them have pros and cons. The best-reported
system presented by [13] uses one-phase strategy. In our evaluation, the two-
phase method shows a better result than the one-phase method, although direct
comparison is not possible since we tested with a maximum entropy based expo-
nential models in all cases. The features for one-phase method are identical with
the recognition features except that the local context of a word is extended as
previous 4 words and next 4 words. In addition, we investigate whether the con-
sideration of ?other? class words is helpful in the recognition performance. Table
8 shows explicit annotations of other NE classes much improve the performance
of existing entity types.
In the next experiment, we test how individual methods have an effect on the
performance in the term detection step. Table 9 shows the results obtained by com-
bining different methods in the NER process. At the semantic labeling phase, all
methods employed the ME model using the features described in 2.3. Baseline1
is the two-phase ME model which restrict the inspection of NE candidates to the
NPs which include at least one biomedical salient word. Baseline2 is the two-phase
ME model considering all words. In order to retrieve domain salient words, we
utilized a relative frequency ratio of word distribution in the domain corpus and
that in the general corpus [10]. We used the Penn II raw corpus as out-of-domain
corpus. Both models do not use the features related to previous labels. As a re-
sult, usage of salient words decrease the performance and it only speeds up the
training process. Baseline2+FST indicates boundary extension/contraction using
FST are applied as postprocessing step in baseline2 recognition. In addition, we
compared use of CRFs and ME with Markov process features. For this, we added
features of previous labels to the feature set for ME. Baseline2+MEMarkov is the
two-phase ME model considering all features including previous label related fea-
tures. Baseline2+CRF is a model exploiting CRFs and baseline2+CRF+FST is a
model using CRFand FST as postprocessing.As shown in Table 9, the CRFs based
656 S. Kim et al
Table 9. F-score for different methods
Method Recall Precision F-score
baseline1(salientNP ) 66.21 66.34 66.27
baseline2(all) 68.51 67.58 68.04
baseline2 + FST 68.89 68.53 68.71
baseline2 + MEMarkov 70.30 67.65 68.95
baseline2 + MEMarkov + FST 70.61 68.40 69.49
baseline2 + CRF 72.44 68.77 70.56
baseline2 + CRF + FST 72.77 69.68 71.19
Table 10. Comparisons with other systems
System Precision Recall F-score
Zhou et. al (2004) 69.42 75.99 72.55
Our system 72.77 69.68 71.19
Finkel et. al (2004) 71.62 68.56 70.06
Settles (2004) 70.0 69.0 69.5
model outperforms the ME based model. Our system reached F-score 71.19% on
the baseline2 + CRF + FST model.
Table 10 shows the comparison with top-ranked systems in JNLPBA 2004
shared task. The top-ranked systems made use of external knowledge from
gazetteers and abbreviation handling routines, which were reported to be ef-
fective. Zhou et. al reported the usage of gazetteers and abbreviation handling
improves the performance of the NER system by 4.8% in F-score [13]. Finkel
et. al made use of a number of external resources, including gazetteers, web-
querying, use of the surrounding abstract, abbreviation handling, and frequency
counts from BNC corpus [4]. Settles utilized semantic domain knowledge of 17
kinds of lexicons [11]. Although the performance of our system is a bit lower than
the best system, the results are very promising since most systems use external
gazetteers, and abbreviation and conjunction/disjunction handling scheme. This
suggests areas for further work.
5 Conclusion and Discussion
We presented a two-phase biomedical NE recognition model, term boundary
detection and semantic labeling. We proposed two exponential models for each
phase. That is, CRFs are used for term detection phase including Markov process
and ME is used for semantic labeling. The benefit of dividing the whole process
into two processes is that, by separating the processes with different characteris-
tics, we can select separately the discriminative feature set for each subtask, and
moreover measure effectiveness of models at each phase. Furthermore, we use
the rule-based method as postprocessing to refine the result. The rules are ex-
tracted from the GENIA corpus, which is represented by the deterministic FST.
The rule-based approach is effective to correct errors by cascading structures
Two-Phase Biomedical Named Entity Recognition 657
of biomedical NEs. The experimental results are quite promising. The system
achieved 71.19% F-score without Gazetteers or abbreviation handling process.
The performance could be improved by utilizing lexical database and testing
various classification models.
Acknowledgements
This work was supported by Korea Research Foundation Grant, KRF-2004-037-
D00017.
References
1. Thorten Brants. TnT A Statistical Part-of-Speech Tagger. In Proceedings of the
6th Applied Natural Language Processing.; 2000.
2. Stanley F. Chen and Ronald Rosenfeld. A Gaussian prior for smoothing maximum
entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.
3. Nigel Collier, Chikashi Nobata and Jun-ichi Tsujii. Extracting the Names of Genes
and Gene Products with a Hidden Markov Model. In Proceedings of COLING 2000;
201-207.
4. Jenny Finkel, Shipra Dingare, and Huy Nguyen. Exploiting Context for Biomedical
Entity Recognition From Syntax to thw Web. In Proceedings of JNLPBA/BioNLP
2004; 88-91.
5. K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. Toward information extrac-
tion: identifying protein names from biological papers. In Proceedins of the Pacific
Symposium on Biocomputing 98; 707-718.
6. Junichi Kazama, Takaki Makino, Yoshihiro Ohta and Junichi Tsujii. Tuning Sup-
port Vector Machines for Biomedical Named Entity Recognition, Proceedings of the
ACL Workshop on Natural Language Processing in the Biomedical Domain 2002;
1-8.
7. Michael Krauthammer and Goran Nenadic. Term Identification in the Biomedical
literature. Journal of Biomedical Informatics. 2004; 37(6):512-526.
8. John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional Random
Fields: probabilistic models for segmenting and labeling sequence data. In Proceed-
ings of ICML-01; 282-289.
9. Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, Hae-Chang Rim. Biomedical
named entity recognition using two-phase model based on SVMs. Journal of
Biomedical Informatics 2004; 37(6):436-447.
10. Kyung-Mi Park, Seonho Kim, Ki-Joong Lee, Do-Gil Lee, and Hae-Chang Rim.
Incorportating Lexical Knowledge into Biomedical NE Recognition. In Proceedings
of Natural Language Processing in Biomedicine and its Applications Post-COLING
Workshop 2004; 76-79.
11. Burr Settles. Biomedical Named Entity Recognition Using Conditional Random
Fields and Rich Feature Sets. In Proceedings of JNLPBA/BioNLP 2004; 104-107.
12. Olivia Tuason, Lifeng Chen, Hongfang Liu, Judith A. Blake, Carol Friedman. Bi-
ological Nomenclatures: A Source of Lexical Knowledge and Ambiguity. In Pacific
Symposium on Biocomputing 2004; 238-249.
13. GuoDong Zhou, Jie Zhang, Jian Su, Chew-Lim Tan. Exploring Deep Knowledge
Resources in Biomedical Name Recognition. In Proceedings of JNLPBA/BioNLP
2004; 99-102.
Word Sense Disambiguation
by Relative Selection
Hee-Cheol Seo1, Hae-Chang Rim2, and Myung-Gil Jang1
1 Knowledge Mining Research Team,
Electronics and Telecommunications Research Institute (ETRI),
Daejeon, Korea
{hcseo, mgjang}@etri.re.kr
2 Dept. of Computer Science and Engineering, Korea University,
1, 5-ka, Anam-dong, Seongbuk-Gu, Seoul, 136-701, Korea
rim@nlp.korea.ac.kr
Abstract. This paper describes a novel method for a word sense disam-
biguation that utilizes relatives (i.e. synonyms, hypernyms, meronyms,
etc in WordNet) of a target word and raw corpora. The method disam-
biguates senses of a target word by selecting a relative that most prob-
ably occurs in a new sentence including the target word. Only one co-
occurrence frequency matrix is utilized to efficiently disambiguate senses
of many target words. Experiments on several English datum present
that our proposed method achieves a good performance.
1 Introduction
With its importance, a word sense disambiguation (WSD) has been known as
a very important field of a natural language processing (NLP) and has been
studied steadily since the advent of NLP in the 1950s. In spite of the long study,
few WSD systems are used for practical NLP applications unlike part-of-speech
(POS) taggers and syntactic parsers. The reason is because most of WSD studies
have focused on only a small number of ambiguous words based on sense tagged
corpus. In other words, the previous WSD systems disambiguate senses of just
a few words, and hence are not helpful for other NLP applications because of its
low coverage.
Why have the studies about WSD stayed on the small number of ambiguous
words? The answer is on sense tagged corpus where a few words are assigned to
correct senses. Since the construction of the sense tagged corpus needs a great
amount of times and cost, most of current sense tagged corpora contain a small
number of words less than 100 and the corresponding senses to the words. The
corpora, which have sense information of all words, have been built recently,
but are not large enough to provide sufficient disambiguation information of
the all words. Therefore, the methods based on the sense tagged corpora have
difficulties in disambiguating senses of all words.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 920?932, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Word Sense Disambiguation by Relative Selection 921
In this paper, we proposed a novel WSD method that requires no sense tagged
corpus1 and that identifies senses of all words in sentences or documents, not
a small number of words. Our proposed method depends on raw corpus, which
is relatively very large, and on WordNet [1], which is a lexical database in a
hierarchical structure.
2 Related Works
There are several works for WSD that do not depend on a sense tagged corpus,
and they can be classified into three approaches according to main resources
used: raw corpus based approach [2], dictionary based approach [3,4] and hier-
archical lexical database approach. The hierarchical lexical database approach
can be reclassified into three groups according to usages of the database: gloss
based method [5], conceptual density based method [6,7] and relative based
method [8,9,10]. Since our method is a kind of the relative based method, this
section describes the related works of the relative based method.
[8] introduced the relative based method using International Roget?s The-
saurus as a hierarchical lexical database. His method is conducted as follows: 1)
Get relatives of each sense of a target word from the Roget?s Thesaurus. 2) Col-
lect example sentences of the relatives, which are representative of each sense. 3)
Identify salient words in the collective context and determine weights for each
word. 4) Use the resulting weights to predict the appropriate sense for the target
word occurring in a novel text. He evaluated the method on 12 English nouns,
and showed over than 90% precision. However, the evaluation was conducted on
just a small part of senses of the words, not on all senses of them.
He indicated that a drawback of his method is on the ambiguous relative: just
one sense of the ambiguous relative is usually related to a target word but the
other senses of the ambiguous relatives are not. Hence, a collection of example
sentences of the ambiguous relative includes the example sentences irrelevant
to the target word, which prevent WSD systems from collecting correct WSD
information. For example, an ambiguous word rail is a relative of a meaning bird
of a target word crane at WordNet, but the word rail means railway for the most
part, not the meaning related to bird. Therefore, most of the example sentences
of rail are not helpful for WSD of crane. His method has another problem in
disambiguating senses of a large number of target words because it requires a
great amount of time and storage space to collect example sentences of relatives
of the target words.
[9] followed the method of [8], but tried to resolve the ambiguous relative
problem by using just unambiguous relatives. That is, the ambiguous relative
rail is not utilized to build a training data of the word crane because the word
rail is ambiguous. Another difference from [8] is on a lexical database: they
utilized WordNet as a lexical database for acquiring relatives of target words
1 Strictly speaking, our method utilizes bias of word senses at WordNet, which is
acquired a sense tagged corpus. However, our method does not access a sense tagged
corpus directly. Hence, our method is a kind of a weakly supervised approach.
922 H.-C. Seo, H.-C. Rim, and M.-G. Jang
instead of International Roget?s Thesaurus. Since WordNet is freely available
for research, various kinds of WSD studies based on WordNet can be compared
with the method of [9]. They evaluated their method on 14 ambiguous nouns
and achieved a good performance comparable to the methods based on the sense
tagged corpus. However, the evaluation was conducted on a small part of senses
of the target words like [8].
However, many senses in WordNet do not have unambiguous relatives
through relationships such as synonyms, direct hypernyms, and direct hy-
ponyms.2 A possible alternative is to use the unambiguous relatives in the long
distance from a target word, but the way is still problematic because the longer
the distance of two senses is, the weaker the relationship between them is. In
other words, the unambiguous relatives in the long distance may provide ir-
relevant examples for WSD like ambiguous relatives. Hence, the method has
difficulties in disambiguating senses of words that do not have unambiguous rel-
atives near the target words in the WordNet. The problem becomes more serious
when verbs, which most of the relatives are ambiguous, are disambiguated. Like
[8], the method also has a difficulty in disambiguating senses of many words
because the method collects the example sentences of relatives of many words.
[10] reimplemented the method of [9] using a web, which may be a very
large corpus, in order to collect example sentences. They built training datum
of all noun words in WordNet whose size is larger than 7GB, but evaluated their
method on a small number of nouns of lexical sample task of SENSEVAL-2 as
[8] and [9].
3 Word Sense Disambiguation by Relative Selection
Our method disambiguates senses of a target word in a sentence by selecting
only a relative among the relatives of the target word that most probably occurs
in the sentence. A flowchart of our method is presented in Figure 1 with an
example3: 1) Given a new sentence including a target word, a set of relatives of
the target word is created by looking up in WordNet. 2) Next, the relative that
most probably occurs in the sentence is chosen from the set. In this step, co-
occurrence frequencies between relatives and words in the sentence are used in
order to calculate the probabilities of relatives. Our method does not depend on
the training data, but on co-occurrence frequency matrix. Hence in our method,
it is not necessary to build the training data, which requires too much time and
space. 3) Finally, a sense of the target word is determined as the sense that is
related to the selected relative. In this example, the relative stork is selected with
the highest probability and the proper sense is determined as crane#1, which is
related to the selected relative stork.
2 In this paper, direct hypernyms and direct hyponyms mean parents and children at
a lexical database, respectively.
3 In WordNet 1.7.1, a word crane contains four senses, but in this paper only two
senses (i.e. bird and device) are described in the convenience of description.
Word Sense Disambiguation by Relative Selection 923
A mother crane soon laid an egg.
stork, ibis, flamingo, 
bird, beak, feather, ...
lifting device, elevator,
davit, derrick, ...
Pr(stork | Context ),  Pr(ibis | Context ) , 
...
Pr(davit | Context), Pr(derrick | Context ) 
stork
crane#1 crane#2
crane#1
Sentence
Collect 
Relatives
Calculate 
Probability
Select a 
Relative
Determine 
Sense
Fig. 1. Flowchart of our proposed method
Our method makes use of ambiguous relatives as well as unambiguous rela-
tives unlike [9] and hence overcomes the shortage problem of relatives and also
reduces the problem of ambiguous relatives in [8] by handling relatives separately
instead of putting example sentences of the relatives together into a pool.
3.1 Relative Selection
The selected relative of the i-th target word twi in a sentence C is defined to be
the relative of twi that has the largest co-occurrence probability with the words
in the sentence:
SR(twi, C)
def
= argmax
rij
P (rij |C)P (Srij )W (rij , twi) (1)
where SR is the selected relative, rij is the j-th relative of twi, Srij is a sense of
twi that is related to the relative rij , and W is a weight of rij . The right hand
side of Eq. 1 is logarithmically calculated by Bayesian rule:
argmax
rij
P (rij |C)P (Srij )W (rij , twi)
= argmax
rij
P (C|rij)P (rij)
P (C)
P (Srij )W (rij , twi)
= argmax
rij
P (C|rij)P (rij)P (Srij )W (rij , twi)
= argmax
rij
{logP (C|rij) + logP (rij)
+logP (Srij) + logW (rij , twi)} (2)
924 H.-C. Seo, H.-C. Rim, and M.-G. Jang
The first probability in Eq. 2 is computed under the assumption that words
in C occur independently as follows:
logP (C|rij) ?
n
?
k=1
logP (wk|rij) (3)
where wk is the k-th word in C and n is the number of words in C. The proba-
bility of wk given rij is calculated:
P (wk|rij) =
P (rij , wk)
P (rij)
(4)
where P (rij , wk) is a joint probability of rij and wk, and P (rij) is a probability
of rij .
Other probabilities in Eq. 2 and 4 are computed as follows:
P (rij , wk) =
freq(rij , wk)
CS
(5)
P (rij) =
freq(rij)
CS
(6)
Pr(Srij ) =
0.5 + WNf(Srij)
n ? 0.5 + WNf(twi)
(7)
where freq(rij , wk) is the frequency that rij and wk co-occur in a raw corpus,
freq(rij) is the frequency of rij in the corpus, and CS is a corpus size, which is
the sum of frequencies of all words in the raw corpus. WNf(Srij ) and WNf(twi)
is the frequency of a sense related to rij and twi in WordNet.4 In Eq. 7, 0.5 is
a smoothing factor and n is the number of senses of twi. Finally, in Eq. 2, the
weights of relatives, W (rij , twi), are described in following Section 3.1.
Relative Weight. WordNet provides relatives of words, but all of them are not
useful for WSD. That is to say, it is clear that most of ambiguous relatives may
bring about a problem by providing example sentences irrelevant to the target
word to WSD system as described in the previous section.
However, WordNet as a lexical database is classified as a fine-grained dictio-
nary, and consequently some words are classified into ambiguous words though
the words represent just one sense in the most occurrences. Such ambiguous rela-
tives may be useful for WSD of target words that are related to the most frequent
senses of the ambiguous relatives. For example, a relative bird of a word crane is
an ambiguous word, but it usually represents one meaning, ?warm-blooded egg-
laying vertebrates characterized by feathers and forelimbs modified as wings?,
4 WordNet provides the frequencies of words and senses in a sense tagged corpus (i.e.
SemCor), and WNf is calculated with the frequencies in WordNet. That represents
bias of word senses in WordNet.
Word Sense Disambiguation by Relative Selection 925
which is closely related to crane. Hence, the word bird can be a useful relative of
the word crane though the word bird is ambiguous. But the ambiguous relative
is not useful for other target words that are related to the least frequent senses
of the relatives: that is, a relative bird is never helpful to disambiguate the senses
of a word birdie, which is related to the least frequent sense of the relative bird.
We employ a weighting scheme for relatives in order to identify useful rel-
atives for WSD. In terms of weights of relatives, our intent is to provide the
useful relative with high weights, but the useless relatives with low weights. For
instance, a relative bird of a word crane has a high weight whereas a relative
bird of a word birdie get a low weight.
For the sake of the weights, we calculate similarities between a target word
and its relatives and determine the weight of each relative based on the degree of
the similarity. Among similarity measures between words, the total divergence
to the mean (TDM) is adopted, which is known as one of the best similarity
measures for word similarity [11].
Since TDM estimates a divergence between vectors, not between words,
words have to be represented by vectors in order to calculate the similarity
between the words based on the TDM. We define vector elements as words that
occur more than 10 in a raw corpus, and build vectors of words by counting
co-occurrence frequencies of the words and vector elements.
TDM does measure the divergence between words, and hence a reciprocal of
the TDM measure is utilized as the similarity measure:
Sim(
?
wi,
?
wj) =
1
TDM(
?
wi,
?
wj)
where Sim(
?
wi,
?
wj) represents a similarity between two word vectors,
?
wi and
?
wj .
A weight of a relative is determined by the similarity of a target word and
its relative as follows:
W (rij , twi) = Sim(
?
rij ,
?
twi)
3.2 Co-occurrence Frequency Matrix
In order to select a relative for a target word in a given sentence, we must
calculate probabilities of relatives given the sentence, as described in previous
section. These probabilities as Eq. 5 and 6 can be estimated based on frequencies
of relatives and co-occurrence frequencies between each relative and each word
in the sentence.
In order to acquire the frequency information for calculating the probabilities,
the previous relative based methods constructed a training data by collecting
example sentences of relatives. However, to construct the training data requires
a great amount of time and storage space. What is worse, it is an awful work
to construct training datum of all ambiguous words, whose number is over than
20,000 in WordNet.
Instead, we build a co-occurrence frequency matrix (CFM) from a raw corpus
that contains frequencies of words and word pairs. A value in the i-th row and
926 H.-C. Seo, H.-C. Rim, and M.-G. Jang
j-th column in the CFM represents the co-occurrence frequency of the i-th word
and j-th word in a vocabulary, and a value in the i-th row and the i-th column
represents the frequency of the i-th word.
The CFM is easily built by counting words and word pairs in a raw corpus.
Furthermore, it is not necessary to make a CFM per each ambiguous word since
a CFM contains frequencies of all words including relatives and word pairs.
Therefore, our proposed method disambiguates senses of all ambiguous words
efficiently by referring to only one CFM.
The frequencies in Eq. 5 and 6 can be obtained through a CFM as follows:
freq(wi) = cfm(i, i) (8)
freq(wi, wj) = cfm(i, j) (9)
where wi is a word, and cfm(i, j) represents the value in the i-th row and j-th
column of the CFM, in other word, the frequency that the i-th word and j-th
word co-occur in a raw corpus.
4 Experiments
4.1 Experimental Environment
Experiments were carried out on several English sense tagged corpora: SemCor
and corpora for both lexical sample task and all words task of both SENSEVAL-
2 & -3.5 SemCor [12]6 is a semantic concordance, where all content words (i.e.
noun, verb, adjective, and adverb) are assigned to WordNet senses. SemCor
consists of three parts: brown1, brown2 and brownv. We used all of the three
parts of the SemCor for evaluation.
In our method, raw corpora are utilized in order to build a CFM and to
calculate similarities between words for the sake of the weights of relatives. We
adopted Wall Street Journal corpus in Penn Treebank II [13] and LATIMES cor-
pus in TREC as raw corpora, which contain about 37 million word occurrences.
Our CFM contains frequencies of content words and content word pairs. In
order to identify the content words from the raw corpus, Tree-Tagger [14], which
is a kind of automatic POS taggers, is employed.
WordNet provides various kinds of relationships between words or synsets.
In our experiments, the relatives in Table 1 are utilized according to POSs of
target words. In the table, hyper3 means 1 to 3 hypernyms (i.e. parents, grand-
parents and great-grandparent) and hypo3 is 1 to 3 hyponyms (i.e. children,
grandchildren and great-grandchildren).
5 We did not evaluate on verbs of lexical sample task of SENSEVAL-3 because the
verbs are assigned to senses of WordSmyth, not WordNet.
6 In this paper, SemCor 1.7.1 is adopted.
Word Sense Disambiguation by Relative Selection 927
Table 1. Used Relative types
POS relatives
noun synonym, hyper3, hypo3, antonym, attribute, holonym, meronym, sibling
adjective synonym, antonym, similar to, alsosee, attribute, particle, pertain
verb synonym, hyper2, tropo2, alsosee, antonym, causal, entail, verbgroup
adverb synonyms, antonyms, derived
4.2 Experimental Results
ComparisonwithOtherRelative Based Methods. We tried to compare our
proposed method with the previous relative based methods. However, both of [8]
and [9] didnot evaluate theirmethods onapublicly available data.We implemented
their methods and compared our method with them on the same evaluation data.
When both of the methods are implemented, it is practically difficult to col-
lect example sentences of all target words in the evaluation data. Instead, we
implemented the previous methods to work with our CFM. WordNet was uti-
lized as a lexical database to acquire relatives of target words and the sense
disambiguation modules were implemented by using on Na??ve Bayesian classi-
fier, which [9] adopted though [8] utilized International Roget?s Thesaurus and
other classifier similar to decision lists. Also the bias of word senses, which is
presented at WordNet, is reflected on the implementation in order to be in a
same condition with our method. Hence, the reimplemented methods in this pa-
per are not exactly same with the previous methods, but the main ideas of the
methods are not corrupted. A correct sense of a target word twi in a sentence
C is determined as follows:
Sense(twi, C)
def
= arg max
sij
P (sij |C)Pwn(sij) (10)
where Sense(twi, C) is a sense of twi in C, sij is the j-th sense of twi. Pwn(sij)
is the WordNet probability of sij . The right hand side of Eq. 10 is calculated
logarithmically under the assumption that words in C occur independently:
arg max
sij
P (sij |C)Pwn(sij)
= argmax
sij
P (C|sij)P (sij)
P (C)
Pwn(sij)
= argmax
sij
P (C|sij)P (sij)Pwn(sij)
= argmax
sij
{logP (C|sij) + logP (sij))
+logPwn(sij)}
? argmax
sij
{
n
?
k=1
logP (wk|sij) + logP (sij))
+logPwn(sij)} (11)
928 H.-C. Seo, H.-C. Rim, and M.-G. Jang
where wk is the k-th word in C and n is the number of words in C. In Eq. 11,
we assume independence among the words in C.
Probabilities in Eq. 11 are calculated as follows:
P (wk|sij) =
P (sij , wk)
P (sij)
=
freq(sij , wk)
freq(sij)
(12)
P (sij) =
freq(sij)
CS
(13)
Pwn(sij) =
0.5 + WNf(sij)
n ? 0.5 + WNf(twi)
(14)
where freq(sij , wk) is the frequency that sij and wk co-occur in a corpus,
freq(sij) is the frequency of sij in a corpus, which is the sum of frequencies
of all relatives related to sij . CS means corpus size, which is the sum of frequen-
cies of all words in a corpus. WNf(sij) and WNf(twi) are the frequencies of a
sij and twi in WordNet, respectively, which represent bias of word senses. Eq.
14 is the same with Eq. 7 in Section 3.
Since the training data are built by collecting example sentences of relatives
in the previous works, the frequencies in Eq. 12 and 13 are calculated with our
matrix as follows:
freq(sij , wk) =
?
rl related to sij
freq(rl, wk)
freq(sij) =
?
rl related to sij
freq(rl)
where rl is a relative related to the sense sij . freq(rl, wk) and freq(rl) are the
co-occurrence frequency between rl and wk and the frequency of rl, respectively,
and both frequencies can be obtained by looking up the matrix since the matrix
contains the frequencies of words and word pairs.
The main difference between [8] and [9] is whether ambiguous relatives are
utilized or not. Considering the difference, we implemented the method of [8] to
include the ambiguous relatives into relatives, but the method of [9] to exclude
the ambiguous relatives.
Word Sense Disambiguation by Relative Selection 929
Table 2. Comparison results with previous relative-based methods
S2 LS S3 LS S2 ALL S3 ALL SemCor
All Relatives 38.86% 42.98% 45.57% 51.20% 53.68%
Unambiguous Relatives 27.40% 24.47% 30.73% 33.61% 30.63%
our method 40.94% 45.12% 45.90% 51.35% 55.58%
Table 3. Comparison results with top 3 systems at SENSEVAL
S2 LS S2 ALL S3 ALL
[15] 40.2% 56.9% .
[16] 29.3% 45.1% .
[5] 24.4% 32.8% .
[17] . . 58.3%
[18] . . 54.8%
[19] . . 48.1%
Our method 40.94% 45.12% 51.35%
Table 2 shows the comparison results.7 In the table, All Relatives and Unam-
biguous Relatives represent the results of the reimplemented methods of [8] and
[9], respectively. It is observed in the table that our proposed method achieves
better performance on all evaluation data than the previous methods though the
improvement is not large. Hence, we may have an idea that our method handles
relatives and in particular ambiguous relatives more effectively than [8] and [9].
Compared with [9], [8] obtains a better performance, and the difference be-
tween the performance of them are totally more than 15 % on all of the evaluation
data. From the comparison results, it is desirable to utilize ambiguous relatives
as well as unambiguous relatives.
[10] evaluated their method on nouns of lexical sample task of SENSEVAL-2.
Their method achieved 49.8% recall. When evaluated on the same nouns of the
lexical sample task, our proposed method achieved 47.26%, and the method of
[8] 45.61%, and the method of [9] 38.03%. Compared with our implementations,
[10] utilized a web as a raw corpus that is much larger than our raw corpus, and
employed various kinds of features such as bigram, trigram, part-of-speeches,
etc.8 Therefore, it can be conjectured that a size of a raw corpus and features
play an important role in the performance. We can observe that in our imple-
mentation of the method of [9], the data sparseness problem is very serious since
unambiguous relatives are usually not frequent in the raw corpus. In the web,
the problem seems to be alleviated. Further studies are required for the effects
of various features.
7 Evaluation measure is a recall, which is utilized for evaluating systems at SENSE-
VAL. In the table, S2 means SENSEVAL-2, LS means lexical sample task, and ALL
represents all words task.
8 [10] also utilized the bias information of word senses at WordNet.
930 H.-C. Seo, H.-C. Rim, and M.-G. Jang
Comparison with Systems Participated in SENSEVAL. We also com-
pared our method with the top systems at SENSEVAL that did not use sense
tagged corpora.9 Table 3 shows the official results of the top 3 participating
systems at SENSEVAL-2 & 3 and experimental performance of our method. In
the table, it is observed that our method is ranked in top 3 systems.
5 Conclusions
We have proposed a simple and novel method that determines senses of all
contents words in sentences by selecting a relative of the target words in Word-
Net. The relative is selected by using a co-occurrence frequency between the
relative and the words surrounding the target word in a given sentence. The co-
occurrence frequencies are obtained from a raw corpus, not from a sense tagged
corpus that is often required by other approaches.
We tested the proposed method on SemCor data and SENSEVAL data, which
are publicly available. The experimental results show that the proposed method
effectively disambiguates many ambiguous words in SemCor and in test data
for SENSEVAL all words task, as well as a small number of ambiguous words
in test data for SENSEVAL lexical sample task. Also our method more cor-
rectly disambiguates senses than [8] and [9]. Furthermore, the proposed method
achieved comparable performance with the top 3 ranked systems at SENSEVAL-
2 & 3.
In consequence, our method has two advantages over the previous methods
([8] and [9]): our method 1) handles the ambiguous relatives and unambiguous
relatives more effectively, and 2) utilizes only one co-occurrence matrix for dis-
ambiguating all contents words instead of collecting training data of the content
words.
However, our method did not achieve good performances. One reason of
the low performance is on the relatives irrelevant to the target words. That is,
investigation of several instances which assign to incorrect senses shows that
relatives irrelevant to the target words are often selected as the most probable
relatives. Hence, we will try to devise a filtering method that filters out the useless
relatives before the relative selection phase. Also we will plan to investigate a
large number of tagged instances in order to find out why our method did not
achieve much better performance than the previous works and to detect how
our method selects the correct relatives more precisely. Finally, we will conduct
experiments with various features such as bigrams, trigrams, POSs, etc, which
[10] considered and examine a relationship of a size of a raw corpus and a system
performance.
9 At SENSEVAL, unsupervised systems include the weakly supervised systems though
there are some debates. In this paper, our methods are compared with the systems
that are classified into the unsupervised approach at SENSEVAL.
Word Sense Disambiguation by Relative Selection 931
References
1. Fellbaum, C.: An WordNet Electronic Lexical Database. The MIT Press (1998)
2. Schu?tze, H.: Automatic word sense discrimination. Computational Linguistics 24
(1998) 97?123
3. Lesk, M.: Automatic sense disambiguation using machine readable dictionaries:
How to tell a pine cone from an ice cream cone. In: Proceedings of the 5th annual
international conference on Systems documentation, Toronto, Ontario, Canada
(1986) 24?26
4. Karov, Y., Edelman, S.: Similarity-based word sense disambiguation. Computa-
tional Linguistics 24 (1998) 41?59
5. Haynes, S.: Semantic tagging using WordNet examples. In: Proceedings of
SENSEVAL-2 Second International Workshop on Evaluating Word Sense Disam-
biguation Systems, Toulouse, France (2001) 79?82
6. Agirre, E., Rigau, G.: Word sense disambiguation using conceptual density. In:
Proceedings of COLING?96, Copenhagen Denmark (1996) 16?22
7. Fernandez-Amoros, D., Gonzalo, J., Verdejo, F.: The role of conceptual relations in
word sense disambiguation. In: Proceedings of the 6th International Workshop on
Applications of Natural Language for Information Systems, Madrid, Spain (2001)
87?98
8. Yarowsky, D.: Word-sense disambiguation using statistical models of Roget?s cat-
egories trained on large corpora. In: Proceedings of COLING-92, Nantes, France
(1992) 454?460
9. Leacock, C., Chodorow, M., Miller, G.A.: Using corpus statistics and WordNet
relations for sense identification. Computational Linguistics 24 (1998) 147?165
10. Agirre, E., Martinez, D.: Unsupervised wsd based on automatically retrieved
examples: The importance of bias. In: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain
(2004)
11. Lee, L.: Similarity-Based Approaches to Natural Language Processing. PhD thesis,
Harvard University, Cambridge, MA (1997)
12. Miller, G.A., Leacock, C., Tengi, R., Bunker, R.: A semantic concordance. In:
Proceedings of the 3 DARPA Workshop on Human Language Technology. (1993)
303?308
13. Marcus, M.P., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated
corpus of english: The penn treebank. Computational Linguistics 19 (1994) 313?
330
14. Schmid, H.: Probabilistic part-of-speech tagging using decision trees. In: Proceed-
ings of the Conference on New Methods in Language Processing, Manchester, UK
(1994)
15. Fernandez-Amoros, D., Gonzalo, J., Verdejo, F.: The UNED systems at
SENSEVAL-2. In: Proceedings of SENSEVAL-2 Second International Work-
shop on Evaluating Word Sense Disambiguation Systems, Toulouse, France (2001)
75?78
16. Litkowski, K.: SENSEVAL-2:overview. In: Proceedings of SENSEVAL-2 Sec-
ond International Workshop on Evaluating Word Sense Disambiguation Systems,
Toulouse, France (2001) 107?110
932 H.-C. Seo, H.-C. Rim, and M.-G. Jang
17. Strapparava, C., Gliozzo, A., Giuliano, C.: Pattern abstraction and term similarity
for word sense disambiguation: Irst at senseval-3. In: Proceedings of SENSEVAL-
3: Third International Workshop on the Evaluation of Systems for the Semantic
Analysis of Text, Barcelona, Spain (2004) 229?234
18. Fernandez-Amoros, D.: Wsd based on mutual information and syntactic patterns.
In: Proceedings of SENSEVAL-3: Third International Workshop on the Evalu-
ation of Systems for the Semantic Analysis of Text, Barcelona, Spain (2004)
117?120
19. Buscaldi, D., Rosso, P., Masulli, F.: The upv-unige-ciaosenso wsd system.
In: Proceedings of SENSEVAL-3: Third International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text, Barcelona, Spain (2004)
77?82
Probabilistic Models for Korean Morphological Analysis
Do-Gil Lee and Hae-Chang Rim
Dept. of Computer Science & Engineering
Korea University
1, 5-ka, Anam-dong, Seongbuk-gu
Seoul 136-701, Korea
 dglee, rim@nlp.korea.ac.kr
Abstract
This paper discusses Korean morpho-
logical analysis and presents three
probabilistic models for morphological
analysis. Each model exploits a distinct
linguistic unit as a processing unit. The
three models can compensate for each
other?s weaknesses. Contrary to the
previous systems that depend on man-
ually constructed linguistic knowledge,
the proposed system can fully automat-
ically acquire the linguistic knowledge
from annotated corpora (e.g. part-of-
speech tagged corpora). Besides, with-
out any modification of the system, it
can be applied to other corpora having
different tagsets and annotation guide-
lines. We describe the models and
present evaluation results on three cor-
pora with a wide range of conditions.
1 Introduction
This paper discusses Korean morphological anal-
ysis. Morphological analysis is to break down
an Eojeol1 into morphemes, which is the smallest
meaningful unit. The jobs to do in morphological
analysis are as follows:
  Separating an Eojeol into morphemes
  Assigning the morpho-syntactic category to
each morpheme
1Eojeol is the surface level form of Korean and is the
spacing unit delimited by a whitespace.
  Restoring the morphological changes to the
original form
We have to consider some difficult points in Ko-
rean morphology: there are two kinds of ambigu-
ities (segmentation ambiguity and part-of-speech
ambiguity). Moreover, morphological changes to
be restored are very frequent. In contrast to part-
of-speech (POS) tagging, morphological analysis
is characterized by producing all the (grammati-
cally) regal interpretations. Table 1 gives exam-
ples of morphological analysis for Eojeols ?na-
neun? and ?gam-gi-neun?.
Previous works on morphological analysis de-
pends on manually constructed linguistic knowl-
edge such as morpheme dictionary, morphosyn-
tactic rules, and morphological rules. There are
two major disadvantages in this approach:
  Construction of the knowledge base is time-
consuming and labor-intensive. In addition,
storing every word in a lexicon is impossi-
ble so the previous approch suffers from the
unknown word problem.
  There is a lack of portability. Because the re-
sults produced by a morphological analyzer
are limited to the given tagset and the anno-
tation guidelines, it is very difficult to apply
the system to other tagsets and guidelines.
The proposed morphological analyzer,
ProKOMA, tries to overcome these limitations:
Firstly, it uses only POS tagged corpora as an
information source and can automatically acquire
a knowledge base from these corpora. Hence,
there is no necessity for the manual labor in con-
structing and maintaining such a knowledge base.
197
Table 1: Examples of morphological analysis
na-neun gam-gi-neun
na/np+neun/jx ?I am? gam-gi/pv+neun/etm ?be wound?
na/pv+neun/etm ?to sprout? gam-gi/nc+neun/jx ?a cold is?
nal/pv+neun/etm ?to fly? gam/pv+gi/etn+neun/jx ?to wash is?
Although constructing such corpora also requires
a lot of efforts, the amount of annotated corpora
is increasing every year. Secondly, regardless
of tagsets and annotation guidelines, it can be
applied to any training data without modification.
Finally, it can provide not only analyzed results
but also their probabilities by the probabilistic
models. In Korean, no attempt has been made at
probabilistic approach to morphological analysis.
Probabilities enable the system to rank the results
and to provide the probabilities to the next
module such as POS tagger.
2 Related works
Over the past few decades, a considerable number
of studies have been made on Korean morpho-
logical analysis. The early studies concentrated
on the algorithmic research. The following ap-
proaches belong to this group: longest matching
algorithm, tabular parsing method using CYK al-
gorithm (Kim, 1986), dictionary based approach
(Kwon, 1991), two-level morphology(Lee, 1992),
and syllable-based approach (Kang and Kim,
1994).
Next, many studies have been made on im-
proving the efficiency of the morphological an-
alyzers. There have been studies to reduce the
search space and implausible interpretations by
using characteristics of Korean syllables (Kang,
1995; Lim et al, 1995).
There have been no standard tagset and anno-
tation guideline, so researchers have developed
methods with their own tagsets and guidelines.
The Morphological Analysis and Tagger Evalua-
tion Contest (MATEC) took place in 1999. This is
the first trial about the objective and relative eval-
uation of morphological analysis. Among the par-
ticipants, some newly implemented the systems
and others converted the existing systems? results
through postprocessing steps. In both cases, they
reported that they spent much effort and argued
the necessity of tuning the linguistic knowledge.
All the systems described so far can be con-
sidered as the so called dictionary and rule based
approach. In this approach, the quality of the dic-
tionary and the rules govern the system?s perfor-
mance.
The proposed approach is the first attempt to
probabilistic morphological analysis. The aim of
the paper is to show that this approach can achieve
comparable performances with the previous ap-
proaches.
3 Probabilistic morphological analysis
model
Probabilistic morphological analysis generates all
the possible interpretations and their probabilities
for a given Eojeol  . The probability that a given
Eojeol   is analyzed to a certain interpretation 
is represented as     . The interpretation 
is made up of a morpheme sequence  and its
corresponding POS sequence  as given in Equa-
tion 1.
           (1)
In the following subsections, we describe the
three morphological analysis models based on
three different linguistic units (Eojeol, mor-
pheme, and syllable2).
3.1 Eojeol-unit model
For the Eojeol-unit model, it is sufficient to store
the frequencies of each Eojeol (surface level
form) and its interpretation acquired from the
POS tagged corpus3.
The probabilities of Equation 1 are estimated
by the maximum likelihood estimator (MLE) us-
ing relative frequencies in the training data.
2In Korean written text, each character has one syllable.
We do not distinguish between character and syllable in this
paper.
3ProKOMA extracts only Eojeols occurred five times or
more in training data.
198
The most prominent advantage of the Eojeol-
unit analysis is its simplicity. As mentioned be-
fore, morphological analysis of Korean is very
complex. The Eojeol-unit analysis can avoid such
complex process so that it is very efficient and
fast. Besides, it can reduce unnecessary results
by only producing the interpretations that really
appeared in the corpus. So, we also expect an im-
provement in accuracy.
Due to the high productivity of Korean Eojeol,
the number of possible Eojeols is very large so
storing all kinds of Eojeols is impossible. There-
fore, using the Eojeol-unit analysis alone is unde-
sirable, but a small number of Eojeols with high
frequency can cover a significant portion of the
entire ones, thus this model will be helpful.
3.2 Morpheme-unit model
As discussed, not all Eojeols can be covered by
the Eojeol-unit analysis. The ultimate goal of
morphological analysis is to recognize every mor-
pheme within an Eojeol. For these reasons, most
previous systems have used morpheme as a pro-
cessing unit for morphological analysis.
The morpheme-unit morphological analysis
model is derived as follows by introducing lexi-
cal form :
                 (2)
where  should satisfy the following condition:
  
 
      
where 
 
is a set of lexical forms that can be de-
rived from the surface form  . This condition
means that among all possible lexical forms for
a given   (
 
), the only lexical form  is deter-
ministically derived from the interpretation .
                     (3)
        (4)
         (5)
Equation 3 assumes the interpretation  and the
surface form   are conditionally independent
given the lexical form . Since the lexical form
 is underlying in the morpheme sequence 4,
4A lexical form is just the concatenation of morphemes.
the lexical form  can be omitted as in equation 4.
In Equation 5, the left term      denotes ?the
morphological restoration model?, and the right
    ?the morpheme segmentation and POS
assignment model?.
We describe the morphological restoration
model first. The model is the probability of the
lexical form given a surface form and is to encode
the probability that the  substrings between the
surface form and its lexical form correspond to
each other. The equation of the model is as fol-
lows:
     

 
 
  	

	

 (6)
where, 	

and 	

denote the 
th substrings of the
surface form and the lexical form, respectively.
We call such pairs of substrings ?morpholog-
ical information?. This information can be ac-
quired by the following steps: If a surface form
(Eojeol) and its lexical form are the same, each
syllable pair of them is mapped one-to-one and
extracted. Otherwise, it means that a morpholog-
ical change occurs. In this case, the pair of two
substrings from the beginning to the end of the
mismatch is extracted. The morphological infor-
mation is also automatically extracted from train-
ing data. Table 2 shows some examples of apply-
ing the morphological restoration model.
Now we turn to the morpheme segmentation
and POS assignment model. It is the joint prob-
ability of the morpheme sequence and the tag se-
quence.
       

 




 
 
  

 

  

 
 

  

 

 (7)
In equation 7, 

and 

are pseudo tags to
indicate the beginning and the end of Eojeol, re-
spectively. We introduce the 

symbol to
reflect the preference for well-formed structure
of a given Eojeol. The model is represented
as the well-known bigram hidden Markov model
(HMM), which is widely used in POS tagging.
The morpheme dictionary and the morphosyn-
tactic rules that have been used in the previous
199
Table 2: Examples of applying the morphological restoration model
Surface form Lexical form Probability Description
na-neun na-neun   na na  neun neun No phonological change
na-neun nal-neun   nal na  neun neun ?l? irregular conjugation
go-ma-wo go-mab-eo   go go  mab-eo ma-wo ?b? irregular conjugation
beo-lyeo beo-li-eo   beo beo  li-eo  lyeo Contraction
ga-seo ga-a-seo   ga ga  a-seo seo Ellipsis
approaches are included in the lexical probabil-
ity   

 

 and the transition probability   



 
.
3.3 Syllable-unit model
One of the most difficult problems in morphologi-
cal analysis is the unknown word problem, which
is caused by the fact that we cannot register every
possible morpheme in the dictionary. In English,
contextual information and suffix information is
helpful to estimate the POS tag of an unknown
word. In Korean, the syllable characteristics can
be utilized. For instance, a syllable ?eoss? can
only be a pre-final ending.
The syllable-unit model is derived from Equa-
tion 4 as follows:
               (8)
where   
	
is the syllable sequence of the
lexical form, and   
	
is its corresponding
syllable tag sequence.
In the above equation,       is the same
as that of the morpheme-unit model (Equation 6),
we use the morpheme-unit model?s result as it is.
The right term    is referred to as ?the POS
assignment model?.
The POS assignment model is to assign the 
syllables to the  syllable tags:
      
	
 
	
 (9)

	
 
 

  


  
 
  

  


 
 
  


  


	 	
 
	 	

  


	
 

 
	 	
(10)
In Equation 10, when  is less than or equal to
zero, 

s and 

s denote the pseudo syllables and
the pseudo tags, respectively. They indicate the
beginning of Eojeol. Analogously, 

and


denote the pseudo syllables and the pseudo
tags to indicate the end of Eojeol, respectively.
Two Markov assumptions are applied in Equa-
tion 10. One is that the probability of the current
syllable 

conditionally depends only on the pre-
vious two syllables and two syllable tags. The
other is that the probability of the current syllable
tag 

conditionally depends only on the previous
syllable, the current syllable, and the previous two
syllable tags. This model can consider broader
context by introducing the less strict independent
assumption than the HMM.
In order to convert the syllable sequence  and
the syllable tag sequence  to the morpheme se-
quence  and the morpheme tag sequence  , we
can use two additional symbols (?B? and ?I?) to
indicate the boundary of morphemes: a ?B? de-
notes the first syllable of a morpheme and an ?I?
any non-initial syllable. Examples of syllable-
unit tagging with BI symbols are given in Table
3.
4 Experiments
4.1 Experimental environment
For evaluation, three data sets having different tag
sets and annotation guidelines are used: ETRI
POS tagged corpus, KAIST POS tagged corpus,
and Sejong POS tagged corpus. All experiments
were performed by the 10-fold cross-validation.
Table 4 shows the summary of the corpora.
Table 4: Summary of the data
Corpus ETRI KAIST Sejong
# of Eojeols 288,291 175,468 2,015,860
# of tags 27 54 41
In this paper, we use the following measures in
order to evaluate the system:
200
Table 3: Examples of syllable tagging with BI symbols
Eojeol na-neun ?I? hag-gyo-e ?to school? gan-da ?go?
Tagged Eojeol na/np+neun/jx hag-gyo/nc+e/jc ga/pv+n-da/ef
Morpheme na neun hag-gyo e ga n-da
Morpheme tag np jx nc jc pv ef
Syllable na neun hag gyo e ga n da
Syllable tag B-np B-jx B-nc I-nc B-jc B-pv B-ef I-ef
Answer inclusion rate (AIR) is defined as the
number of Eojeols among whose results con-
tain the gold standard over the entire Eojeols
in the test data.
Average ambiguity (AA) is defined as the aver-
age number of returned results per Eojeol by
the system.
Failure rate (FR) is defined as the number of
Eojeols whose outputs are not produced over
the number of Eojeols in the test data.
1-best tagging accuracy (1A) is defined as the
number of Eojeols of which only one inter-
pretation with highest probability per Eojeol
is matched to the gold standard over the en-
tire Eojeols in the test data.
There is a trade-off between AIR and AA. If
a system outputs many results, it is likely to in-
clude the correct answer in them, but this leads
to an increase of the ambiguity, and vice versa.
The higher AIR is, the better the system. The
AIR can be an upper bound on the accuracy of
POS taggers. On the contrary to AIR, the lower
AA is, the better the system. A low AA can re-
duce the burden of the disambiguation process of
the POS tagger. Although the 1A is not used as
a common evaluation measure for morphological
analysis because previous systems do not rank the
results, ProKOMA can be evaluated by this mea-
sure because it provides the probabilities for the
results. This measure can also be served as a base-
line for POS tagging.
4.2 Experimental results
To investigate the performance and the effective-
ness of the three models, we conducted several
tests according to the combinations of the mod-
els. For each test, we also performed the exper-
iments on the three corpora. The results of the
experiments are listed in Table 5. In the table,
?E?, ?M?, and ?S? mean the Eojeol-unit analysis,
the morpheme-unit analysis, and the syllable-unit
analysis, respectively. The columns having more
than one symbol mean that each model performs
sequentially.
According to the results, when applying a sin-
gle model, each model shows the significant dif-
ferences, especially between ?E? and ?S?. Be-
cause of low coverage of the Eojeol-unit analy-
sis, ?E? shows the lowest AIR and the highest FR.
However, it shows the lowest AA because it pro-
duces the small number of results. On the con-
trary, ?S? shows the highest AA but the best per-
formances on AIR and FR, which is caused by
producing many results.
Most previous systems use morpheme as a
processing unit for morphological analysis. We
would like to examine the effectiveness of the
proposed models based on Eojeol and syllable.
First, compare the models that use the Eojeol-
unit analysis with others (?M? vs. ?EM?, ?S? vs.
?ES?, and ?MS? vs. ?EMS?). When applying the
Eojeol-unit analysis, AA is decreased, and AIS
and 1A are increased. Then, compare the mod-
els that use the syllable-unit analysis with others
(?E? vs. ?ES?, ?M? vs. ?MS?, and ?EM? vs.
?EMS?). When applying the syllable-unit anal-
ysis, AIR and 1A are increased, and FR is de-
creased. Therefore, both models are very useful
when compared the morpheme-unit model only.
Compared with the performances of two sys-
tems that participated in MATEC 99, we listed
the results in Table 6. In this evaluation, the
ETRI corpus was used and the number of Eo-
jeols included in the test data is 33,855. The
evaluation data used in MATEC 99 and ours
are not the same, but are close. As can be
201
Table 5: Experimental results according to the combination of the processing units
Data Measure E M S EM ES MS EMS
ETRI
Answer inclusion rate (%) 54.65 93.87 98.91 94.16 98.81 97.09 97.22
Average ambiguity 1.23 2.63 6.95 2.10 4.46 2.95 2.41
Failure rate (%) 45.21 3.81 0.06 3.67 0.06 0.06 0.06
1-best accuracy (%) 51.66 83.49 89.98 86.41 91.22 86.15 88.92
KAIST
Answer inclusion rate (%) 57.43 94.29 98.36 94.41 98.25 96.97 97.02
Average ambiguity 1.26 1.84 6.05 1.57 3.80 2.16 1.89
Failure rate (%) 42.40 3.73 0.06 3.67 0.06 0.06 0.06
1-best accuracy (%) 54.22 87.51 90.02 89.18 91.02 89.50 91.12
Sejong
Answer inclusion rate (%) 67.79 90.96 99.38 92.17 99.33 96.60 97.09
Average ambiguity 1.29 2.35 6.60 1.82 3.52 2.72 2.17
Failure rate (%) 32.13 5.94 0.02 5.21 0.02 0.02 0.02
1-best accuracy (%) 64.64 83.86 91.56 87.00 92.96 88.72 91.16
Table 6: Performances of two systems participated in MATEC 99
(Lee et al, 1999)?s system (Song et al, 1999)?s system
Answer inclusion rate (%) 98 92
Average ambiguity 4.13 1.75
seen, the Lee et al (1999)?s system is better than
ProKOMA in terms of AIS, but it generates too
many results (with higher AA).
5 Conclusion
We have presented and described the new prob-
abilistic models used in our Korean morpholog-
ical analyzer ProKOMA. The previous systems
depend on manually constructed linguistic knowl-
edge such as morpheme dictionary, morphosyn-
tactic rules, and morphological rules. The system,
however, requires no manual labor because all the
information can be automatically acquired by the
POS tagged corpora. We also showed that the sys-
tem is portable and flexible by the experiments on
three different corpora.
The previous systems take morpheme as a pro-
cessing unit, but we take three kinds of processing
units (e.g. Eojeol, morpheme, and syllable). Ac-
cording to the experiments, we can know that the
Eojeol-unit analysis contributes efficiency and ac-
curacy, and the syllable-unit analysis is robust in
the unknown word problem and also contributes
accuracy. Finally, the system achieved compara-
ble performances with the previous systems.
References
S.-S. Kang and Y.-T. Kim. 1994. Syllable-based
model for the Korean morphology. In Proceedings
of the 15th International Conference on Computa-
tional Linguistics, pages 221?226.
S.-S. Kang. 1995. Morphological analysis of Ko-
rean irregular verbs using syllable characteristics.
Journal of the Korea Information Science Society,
22(10):1480?1487.
S.-Y. Kim. 1986. A morphological analyzer for
Korean language with tabular parsing method and
connectivity information. Master?s thesis, Dept.
of Computer Science, Korea Advanced Institute of
Science and Technology.
H.-C. Kwon. 1991. Dictionary-based morphological
analysis. In Proceedings of the Natural Language
Processing Pacific Rim Symposium, pages 87?91.
S.-Z. Lee, B.-R. Park, J.-D. Kim, W.-H. Ryu, D.-G.
Lee, and H.-C. Rim. 1999. A predictive morpho-
logical analyzer, a part-of-speech tagger based on
joint independence model, and a fast noun extractor.
In Proceedings of the MATEC 99, pages 145?150.
S.-J. Lee. 1992. A two-level morphological analy-
sis of Korean. Master?s thesis, Dept. of Computer
Science, Korea Advanced Institute of Science and
Technology.
H.-S. Lim, S.-Z. Lee, and H.-C. Rim. 1995. An ef-
ficient Korean mophological analysis using exclu-
sive information. In Proceedings of the 1995 In-
ternational Conference on Computer Processing of
Oriental Languages, pages 255?258.
T.-J. Song, G.-Y. Lee, and Y.-S. Lee. 1999. Mor-
phological analyzer using longest match method for
syntactic analysis. In Proceedings of the MATEC
99, pages 157?166.
202
Tree Annotation Tool using Two-phase Parsing
to Reduce Manual Effort for BuildingaTreebank
So-Young Park, Yongjoo Cho, Sunghoon Son,
College of Computer Software & Media Technology
SangMyung University
7 Hongji-dong, Jongno-gu
SEOUL, KOREA
{ssoya,ycho,shson}@smu.ac.kr
Ui-Sung Song and Hae-Chang Rim
Dept. of CSE
Korea University,
5-ka 1, Anam-dong, Seongbuk-ku
SEOUL, KOREA
{ussong,rim}@nlp.korea.ac.kr
Abstract
In this paper, we propose a tree annota-
tion tool using a parser in order to build
a treebank. For the purpose of mini-
mizing manual effort without any mod-
ification of the parser, it performs two-
phase parsing for the intra-structure of
each segment and the inter-structure af-
ter segmenting a sentence. Experimen-
tal results show that it can reduce man-
ual effort about 24.5% as compared
with a tree annotation tool without seg-
mentation because an annotation?s in-
tervention related to cancellation and
reconstruction remarkably decrease al-
though it requires the annotator to seg-
ment some long sentence.
1 Introduction
A treebank is a corpus annotated with syntactic
information, and the structural analysis of each
sentence is represented as a bracketed tree struc-
ture. This kind of corpus has served as an ex-
tremely valuable resource for computational lin-
guistics applications such as machine translation
and question answering (Lee et al, 1997; Choi,
2001), and has also proved useful in theoretical
linguistics research (Marcus et al, 1993).
However, for the purpose of building the tree-
bank, an annotator spends a lot of time and man-
ual effort. Furthermore, it is too difficult to main-
tain the consistency of the treebank based on only
the annotator (Hindle, 1989; Chang et al, 1997).
Therefore, we require a tree annotation tool to re-
duce manual effort by decreasing the frequency
of the human annotators? intervention. Moreover,
the tool can improve the annotating efficiency,
and help maintain the consistency of the treebank
(Kwak et al, 2001; Lim et al, 2004).
In this paper, we propose a tree annotation tool
using a parser in order to reduce manual effort for
building a treebank. Fundamentally, it generates a
candidate syntactic structure by utilizing a parser.
And then, the annotator cancels the incorrect con-
stituents in the candidate syntactic structure, and
reconstructs the correct constituents.
2 Previous Works
Up to data, several approaches have been devel-
oped in order to reduce manual effort for build-
ing a treebank. They can be classified into the
approaches using the heuristics (Hindle, 1989;
Chang et al, 1997) and the approaches using
the rules extracted from an already built treebank
(Kwak et al, 2001; Lim et al, 2004).
The first approaches are used for Penn Tree-
bank (Marcus et al, 1993) and the KAIST lan-
guage resource (Lee et al, 1997; Choi, 2001).
Given a sentence, the approaches try to assign an
unambiguous partial syntactic structure to a seg-
ment of each sentence based on the heuristics.
The heuristics are written by the grammarians so
that they are so reliable (Hindle, 1989; Chang et
al., 1997). However, it is too difficult to modify
the heuristics, and to change the features used for
constructing the heuristics (Lim et al, 2004).
The second approaches are used for SEJONG
treebank (Kim and Kang, 2002). Like the first
238
approaches, they also try to attach the partial syn-
tactic structure to each sentence according to the
rules. The rules are automatically extracted from
an already built treebank. Therefore, the ex-
tracted rules can be updated whenever the anno-
tator wants (Kwak et al, 2001; Lim et al, 2004).
Nevertheless, they place a limit on the manual ef-
fort reduction and the annotating efficiency im-
provement because the extracted rules are less
credible than the heuristics.
In this paper, we propose a tree annotation tool
using a parser for the purpose of shifting the re-
sponsibility of extracting the reliable syntactic
rules to the parser. It is always ready to change the
parser into another parser. However, most parsers
still tend to show low performance on the long
sentences (Li et al, 1990; Doi et al, 1993; Kim et
al., 2000). Besides, one of the reasons to decrease
the parsing performance is that the initial syntac-
tic errors of a word or a phrase propagates to the
whole syntactic structure.
In order to prevent the initial errors from propa-
gating without any modification of the parser, the
proposed tool requires the annotator to segment a
sentence. And then, it performs two-phase pars-
ing for the intra-structure of each segment and
the inter-structure. The parsing methods using
clause-based segmentation have been studied to
improve the parsing performance and the parsing
complexity (Kim et al, 2000; Lyon and Dicker-
son, 1997; Sang and Dejean, 2001). Nevertheless,
the clause-based segmentation can permit a short
sentence to be splitted into shorter segments un-
necessarily although too short segments increase
manual effort to build a treebank.
For the sake of minimizing manual effort, the
proposed tree annotation tool induces the annota-
tor to segment a sentence according to few heuris-
tics verified by experimentally analyzing the al-
ready built treebank. Therefore, the heuristics can
prefer the specific length unit rather than the lin-
guistic units such as phrases and clauses.
3 Tree Annotation Tool
The tree annotation tool is composed of segmen-
tation, tree annotation for intra-structure, and tree
annotation for inter-structure as shown in Figure1.
Figure 1: tree annotation tool
3.1 Sentence Segmentation
The sentence segmentation consists of three steps:
segmentation step, examination step, and cancel-
lation step. In the segmentation step, the annota-
tor segments a long sentence. In the examination
step, the tree annotation tool checks the length
of each segment. In the cancellation step, it in-
duces the annotator to merge the adjacent short
segments by cancelling some brackets. Given a
short sentence, the tree annotation tool skips over
the sentence segmentation.
As shown in the top of Figure 2, the annota-
tor segments a sentence by clicking the button
?)(? between each pair of words. Since the tool
regards segments as too short given the segment
length 9, it provides the cancel buttons. And then,
the annotator merges some segments by clicking
the third button, the fifth button, and the sixth but-
ton of the middle figure. The bottom figure does
not include the fourth button of the middle figure
because a new segment will be longer than the
segment length 9. When every segment is suit-
able, the annotator exclusively clicks the confirm
button ignoring the cancel buttons.
Segmentation Step:
Cancellation Step (1):
Cancellation Step (2):
Figure 2: Sentence Segmentation
239
Generation Step:
Cancellation Step:
Reconstruction Step:
Figure 3: Tree Annotation for Intra-Structure
3.2 Tree Annotation for Intra-Structure
The tree annotation for intra-structure consists of
three steps: generation step, cancellation step,
and reconstruction step. In the generation step,
the parser generates a candidate intra-structure for
each segment of a sentence. And then, the tree
annotation tool shows the annotator the candidate
intra-structure. In the cancellation step, the an-
notator can cancel some incorrect constituents in
the candidate intra-structure. In the reconstruc-
tion step, the annotator reconstructs the correct
constituents to complete a correct intra-structure.
For example, the tree annotation tool shows the
candidate intra-structure of a segment as shown
in the top of Figure 3. Assuming that the
candidate intra-structure includes two incorrect
constituents, the annotator cancels the incorrect
constituents after checking the candidate intra-
structure as represented in the middle figure. And
then, the annotator reconstructs the correct con-
stituent, and the intra-structure is completed as
described in the bottom of Figure 3.
3.3 Tree Annotation for Inter-Structure
The tree annotation for inter-structure also con-
sists of three steps: generation step, cancella-
tion step, and reconstruction step. In the gen-
eration step, the parser generates a candidate
inter-structure based on the given correct intra-
structures. And then, the tree annotation tool
shows an annotator the candidate syntactic struc-
ture which includes both the intra-structures and
the inter-structure. In the cancellation step, an
Generation Step:
Cancellation Step:
Reconstruction Step:
Figure 4: Tree Annotation for Inter-Structure
annotator can cancel incorrect constituents. In
the reconstruction step, the annotator reconstructs
correct constituents to complete a correct syntac-
tic structure.
For example, the tree annotation tool represents
the candidate syntactic structure of a sentence as
illustrated in the top of Figure 4. Assuming that
the candidate syntactic structure includes two in-
correct constituents, the annotator cancels the in-
correct constituents, and reconstructs the correct
constituent. Finally, the intra-structure is com-
pleted as described in the bottom of Figure 3.
4 Experiments
In order to examine how much the proposed tree
annotation tool reduces manual effort for build-
ing a Korean treebank, it is integrated with a
parser (Park et al, 2004), and then it is evaluated
on the test sentences according to the following
criteria. The segment length (Length) indicates
that a segment of a sentence is splitted when it
240
is longer than the given segment length. There-
fore, the annotator can skip the sentence segmen-
tation when a sentence is shorter than the seg-
ment length. The number of segments (#Seg-
ments) indicates the number of segments splitted
by the annotator. The number of cancellations
(#Cancellations) indicates the number of incor-
rect constituents cancelled by the annotator where
the incorrect constituents are generated by the
parser. The number of reconstructions (#Recon-
structions) indicates the number of constituents
reconstructed by the annotator. Assume that the
annotators are so skillful that they do not can-
cel their decision unnecessarily. On the other
hand, the test set includes 3,000 Korean sentences
which never have been used for training the parser
in a Korean treebank, and the sentences are the
part of the treebank converted from the KAIST
language resource (Choi, 2001). In this section,
we analyze the parsing performance and the re-
duction effect of manual effort according to seg-
ment length for the purpose of finding the best
segment length to minimize manual effort.
4.1 Parsing Performance According to
Segment Length
For the purpose of evaluating the parsing per-
formance given the correct segments, we clas-
sify the constituents in the syntactic structures
into the constituents in the intra-structures of seg-
ments and the constituents in the inter-structures.
Besides, we evaluate each classified constituents
based on labeled precision, labeled recall, and dis-
tribution ratio. The labeled precision (LP) indi-
cates the ratio of correct candidate constituents
from candidate constituents generated by the
parser, and the labeled recall (LR) indicates the
ratio of correct candidate constituents from con-
stituents in the treebank (Goodman, 1996). Also,
the distribution ratio (Ratio) indicates the distri-
bution ratio of constituents in the intra-structures
from all of constituents in the original structure.
Table 1 shows that the distribution ratio of the
constituents in the intra-structures increases ac-
cording to the longer segment length while the
distribution ratio of the constituents in the inter-
structures decreases. Given the segment length 1,
the constituents in the inter-structures of a sen-
tence are the same as the constituents of the sen-
Table 1: Parsing Performance
Intra-Structure Inter-Structure
Length LP LR Ratio LP LR Ratio
1 0.00 0.00 0.00 87.62 86.06 100.00
2 100.00 93.42 52.25 74.45 74.08 47.75
3 100.00 97.27 66.61 60.63 58.55 33.39
4 98.93 96.47 74.27 62.11 59.71 25.73
5 97.68 96.05 79.47 65.30 63.65 20.53
6 96.50 95.47 83.24 67.88 66.68 16.76
7 95.45 94.45 86.12 70.84 69.81 13.88
8 94.34 93.10 88.47 74.24 73.23 11.53
9 93.41 92.36 90.40 76.85 76.25 9.60
10 92.65 91.47 92.01 78.99 78.31 7.99
11 91.91 90.65 93.43 81.53 80.72 6.57
12 91.19 89.86 94.59 84.39 83.60 5.41
13 90.54 89.23 95.62 86.92 85.97 4.38
14 89.87 88.61 96.54 88.82 88.19 3.46
15 89.34 87.99 97.30 90.39 89.41 2.70
16 88.98 87.65 97.97 90.50 89.86 2.03
17 88.64 87.27 98.51 91.64 89.61 1.49
18 88.37 86.99 98.98 92.92 90.84 1.02
19 88.15 86.76 99.34 92.97 91.30 0.66
20 87.98 86.57 99.58 92.00 91.62 0.42
21 87.83 86.42 99.76 92.73 92.36 0.24
22 87.76 86.36 99.83 93.60 93.20 0.17
23 87.74 86.36 99.89 94.46 93.81 0.11
24 87.69 86.27 99.94 97.67 94.74 0.06
25 87.65 86.26 99.97 100.00 95.45 0.03
26 87.63 86.24 99.99 100.00 100.00 0.01
27 87.63 86.16 99.99 100.00 100.00 0.01
28 87.62 86.06 100.00 0.00 0.00 0.00
tence because there is no evaluated constituent on
intra-structure of one word. In the same way, the
constituents in the intra-structures of a sentence
are the same as the constituents of the sentence
given the segment length 28 because all test sen-
tences are shorter than 28 words.
As described in Table 1, the labeled preci-
sion and the labeled recall decrease on the intra-
structures according to the longer segment length
because both the parsing complexity and the pars-
ing ambiguity increase more and more. On the
other hand, the labeled precision and the labeled
recall tend to increase on the inter-structures since
the number of constituents in the inter-structures
decrease, and it makes the parsing problem of
the inter-structure easy. It is remarkable that the
labeled precision and the labeled recall get rel-
atively high performance on the inter-structures
given the segment length less than 2 because it
is so easy that the parser generates the syntactic
structure for 3 or 4 words.
4.2 Reduction Effect of Manual Effort
In order to examine the reduction effect of manual
effort according to segment length, we measure
the frequency of the annotator?s intervention on
the proposed tool, and also classify the frequency
into the frequency related to the intra-structure
and the frequency related to the inter-structure.
241
Figure 5: Reduction of Manual Effort
As shown in Figure 5, the total manual effort in-
cludes the number of segments splitted by the an-
notator, the number of constituents cancelled by
the annotator, and the number of constituents re-
constructed by the annotator.
Figure 5 shows that too short segments can ag-
gravate the total manual effort since the short seg-
ments require too excessive segmentation work.
According to longer length, the manual effort on
the intra-structures increase because the number
of the constituents increase and the labeled preci-
sion and the labeled recall of the parser decrease.
On the other hand, the manual effort on the inter-
structures decreases according to longer length on
account of the opposite reasons. As presented in
Figure 5, the total manual effort is reduced best at
the segment length 9. It describes that the annota-
tion?s intervention related to cancellation and re-
construction remarkably decrease although it re-
quires the annotator to segment some sentences.
Also, Figure 5 describes that we hardly expect the
effect of two-phase parsing based on the long seg-
ments while the short segments require too exces-
sive segmentation work.
4.3 Comparison with Other Methods
In this experiment, we compare the manual effort
of the four methods: the manual tree annotation
tool (only human), the tree annotation tool using
the parser (no segmentation), the tree annotation
tool using the parser with the clause-based sen-
tence segmentation (clause-based segmentation),
and the tree annotation tool using the parser with
the length-based sentence segmentation (length-
based segmentation) where the segment length is
9 words.
As shown in Figure 6, the first method (only
Figure 6: Comparison with Other Models
human) does not need any manual effort related
to segmentation and cancellation but it requires
too expensive reconstruction cost. The second
method (no segmentation) requires manual effort
related to cancellation and reconstruction without
segmentation. As compared with the first method,
the rest three methods using the parser can re-
duce manual effort by roughly 50% although the
parser generates some incorrect constituents. Fur-
thermore, the experimental results of the third and
fourth methods shows that the two-phase parsing
methods with sentence segmentation can reduce
manual effort more about 9.4% and 24.5% each
as compared with the second method.
Now, we compare the third method (clause-
based segmentation) and the fourth method
(length-based segmentation) in more detail. As
represented in Figure 6, the third method is some-
what better than the fourth method on manual ef-
fort related to intra-structure. It show that the
parser generates more correct constituents given
the clause-based segments because the intra-
structure of the clause-based segments is more
formalized than the intra-structure of the length-
based segments. However, the third method
is remarkably worse than the fourth method on
manual effort related to inter-structure. It de-
scribes that the third method can split a short sen-
tence into shorter segments unnecessarily since
the third method allows the segment length covers
a wide range. As already described in Figure 5,
too short segments can aggravate the manual ef-
fort. Finally, the experimental results shows that
the length-based segments help the tree annota-
tion tool to reduce manual effort rather than the
clause-based segments.
242
5 Conclusion
In this paper, we propose the tree annotation tool
which performs two-phase parsing for the intra-
structure of each segment and the inter-structure
after segmenting a sentence. The proposed tree
annotation tool has the following characteristics.
First, it can reduce manual effort to build a tree-
bank. Experimental results show that it can im-
prove approximately 60.0% and 24.5% as com-
pared with the manual tree annotation tool and
the tree annotation tool using one phase parsing.
Second, it can prevent the initial syntactic errors
of a word or a phrase from propagating to the
whole syntactic structure without any modifica-
tion of the parser because it takes sentence seg-
mentation. Third, it can shift the responsibility of
extracting the reliable syntactic rules to the parser.
For future works, we will try to develop an auto-
matic segmentation method to minimize manual
effort.
Acknowledgements
This work was supported by Ministry of Educa-
tion and Human Resources Development through
Embedded Software Open Education Resource
Center(ESC) at Sangmyung University.
References
Byung-Gyu Chang, Kong Joo Lee, Gil Chang Kim.
1997. Design and Implementation of Tree Tagging
Workbench to Build a Large Tree Tagged Corpus
of Korean. In Proceedings of Hangul and Korean
Information Processing Conference, 421-429.
Ki-Sun Choi. 2001. ?KAIST Language Resources
ver. 2001.? The Result of Core Software
Project from Ministry of Science and Technology,
http://kibs.kaist.ac.kr. (written in Korean)
Shinchi Doi, Kazunori Muraki, Shinichiro Kamei and
Kiyoshi Yamabana. 1993. Long sentence analysis
by domain-specific pattern grammar. In Proceed-
ings of the 6th Conference on the European Chap-
ter of the Association of Computational Linguistics,
466.
Joshua Goodman. 1996. Parsing Algorithms and Met-
rics. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, pp.177?
183.
Donald Hindle. 1989. Acquiring disambiguation rules
from text. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, 118-
125.
Sungdong Kim, Byungtak Zhang and Yungtaek Kim.
2000. Reducing Parsing Complexity by Intra-
Sentence Segmentation based on Maximum En-
tropy Model. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, 164-
171.
Ui-Su Kim, and Beom-Mo Kang. 2002. Principles,
Methods and Some Problems in Compiling a Ko-
rean Treebank. In Proceedings of Hangul and
Korean Information Processing Conference 1997.
155-162.
Yong-Jae Kwak, Young-Sook Hwang, Hoo-Jung
Chung, So-Young Park, Hae-Chang Rim. 2001.
FIDELITY: A Framework for Context-Sensitive
Grammar Development. In Proceedings of Interna-
tional Conference on Computer Processing of Ori-
ental Languages, 305-308.
Kong Joo Lee, Byung-Gyu Chang, Gil Chang Kim.
1997. Bracketing Guidlines for Korean Syntac-
tic Tree Tagged Corpus Version 1. Technical Re-
port CS/TR-976-112, KAIST, Dept. of Computer
Science.
Wei-Chuan Li, Tzusheng Pei, Bing-Huang Lee and
Chuei-Feng Chiou. 1990. Parsing long English sen-
tences with pattern rules. In Proceedings of the
13th International Conference on Computational
Linguistics, 410-412.
Joon-Ho Lim, So-Young Park, Yong-Jae Kwak, and
Hae-Chang Rim. 2004. A Semi-automatic Tree
Annotating Workbench for Building a Korean
Treebank. Lecture Note in Computer Science,
2945:253-257.
Caroline Lyon and Bob Dickerson. 1997. Reducing
the Complexity of Parsing by a Method of Decom-
position. In International Workshop on Parsing
Technology.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. Building a Large Annotated Cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313-330.
So-Young Park, Yong-Jae Kwak, Joon-Ho Lim, Hae-
Chang Rim, and Soo-Hong Kim. 2004. Partially
Lexicalized Parsing Model Utilizing Rich Features.
In Proceedings of the 8th International Conference
on Spoken Language Processing, 3:2201-2204.
Erik F. Tjong Kim Sang and Herve Dejean. 2001. In-
troduction to the CoNLL-2001 Shared Task: Clause
Identi-fication. In Proceedings of the Conference
on Natural Language Learning, 53-57.
243
 
	 
	 	 Self-Organizing Markov Models and
Their Application to Part-of-Speech Tagging
Jin-Dong Kim
Dept. of Computer Science
University of Tokyo
jdkim@is.s.u-tokyo.ac.jp
Hae-Chang Rim
Dept. of Computer Science
Korea University
rim@nlp.korea.ac.kr
Jun?ich Tsujii
Dept. of Computer Science
University of Tokyo, and
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper presents a method to de-
velop a class of variable memory Markov
models that have higher memory capac-
ity than traditional (uniform memory)
Markov models. The structure of the vari-
able memory models is induced from a
manually annotated corpus through a de-
cision tree learning algorithm. A series of
comparative experiments show the result-
ing models outperform uniform memory
Markov models in a part-of-speech tag-
ging task.
1 Introduction
Many major NLP tasks can be regarded as prob-
lems of finding an optimal valuation for random
processes. For example, for a given word se-
quence, part-of-speech (POS) tagging involves find-
ing an optimal sequence of syntactic classes, and NP
chunking involves finding IOB tag sequences (each
of which represents the inside, outside and begin-
ning of noun phrases respectively).
Many machine learning techniques have been de-
veloped to tackle such random process tasks, which
include Hidden Markov Models (HMMs) (Rabiner,
1989), Maximum Entropy Models (MEs) (Rat-
naparkhi, 1996), Support Vector Machines
(SVMs) (Vapnik, 1998), etc. Among them,
SVMs have high memory capacity and show high
performance, especially when the target classifica-
tion requires the consideration of various features.
On the other hand, HMMs have low memory
capacity but they work very well, especially when
the target task involves a series of classifications that
are tightly related to each other and requires global
optimization of them. As for POS tagging, recent
comparisons (Brants, 2000; Schro?der, 2001) show
that HMMs work better than other models when
they are combined with good smoothing techniques
and with handling of unknown words.
While global optimization is the strong point of
HMMs, developers often complain that it is difficult
to make HMMs incorporate various features and to
improve them beyond given performances. For ex-
ample, we often find that in some cases a certain
lexical context can improve the performance of an
HMM-based POS tagger, but incorporating such ad-
ditional features is not easy and it may even degrade
the overall performance. Because Markov models
have the structure of tightly coupled states, an ar-
bitrary change without elaborate consideration can
spoil the overall structure.
This paper presents a way of utilizing statistical
decision trees to systematically raise the memory
capacity of Markov models and effectively to make
Markov models be able to accommodate various fea-
tures.
2 Underlying Model
The tagging model is probabilistically defined as
finding the most probable tag sequence when a word
sequence is given (equation (1)).
T (w1,k) = arg maxt1,k P (t1,k|w1,k) (1)
= arg max
t1,k
P (t1,k)P (w1,k|t1,k) (2)
? arg max
t1,k
k
?
i=1
P (ti|ti?1)P (wi|ti) (3)
By applying Bayes? formula and eliminating a re-
dundant term not affecting the argument maximiza-
tion, we can obtain equation (2) which is a combi-
nation of two separate models: the tag language
model, P (t1,k) and the tag-to-word translation
model, P (w1,k|t1,k). Because the number of word
sequences, w1,k and tag sequences, t1,k is infinite,
the model of equation (2) is not computationally
tractable. Introduction of Markov assumption re-
duces the complexity of the tag language model and
independent assumption between words makes the
tag-to-word translation model simple, which result
in equation (3) representing the well-known Hidden
Markov Model.
3 Effect of Context Classification
Let?s focus on the Markov assumption which is
made to reduce the complexity of the original tag-
ging problem and to make the tagging problem
tractable. We can imagine the following process
through which the Markov assumption can be intro-
duced in terms of context classification:
P (T = t1,k) =
k
?
i=1
P (ti|t1,i?1) (4)
?
k
?
i=1
P (ti|?(t1,i?1)) (5)
?
k
?
i=1
P (ti|ti?1) (6)
In equation (5), a classification function ?(t1,i?1) is
introduced, which is a mapping of infinite contextual
patterns into a set of finite equivalence classes. By
defining the function as follows we can get equation
(6) which represents a widely-used bi-gram model:
?(t1,i?1) ? ti?1 (7)
Equation (7) classifies all the contextual patterns
ending in same tags into the same classes, and is
equivalent to the Markov assumption.
The assumption or the definition of the above
classification function is based on human intuition.
( )conjP |?
( )conjfwP ,|?
( )conjvbP ,|?
( )conjvbpP ,|?
vbvb
vbpvbp
Figure 1: Effect of 1?st and 2?nd order context
atat
prepprep
nnnn( )prepP |?
( )in'',| prepP ?
( )with'',| prepP ?
( )out'',| prepP ?
Figure 2: Effect of context with and without lexical
information
Although this simple definition works well mostly,
because it is not based on any intensive analysis of
real data, there is room for improvement. Figure 1
and 2 illustrate the effect of context classification on
the compiled distribution of syntactic classes, which
we believe provides the clue to the improvement.
Among the four distributions showed in Figure 1,
the top one illustrates the distribution of syntactic
classes in the Brown corpus that appear after all the
conjunctions. In this case, we can say that we are
considering the first order context (the immediately
preceding words in terms of part-of-speech). The
following three ones illustrates the distributions col-
lected after taking the second order context into con-
sideration. In these cases, we can say that we have
extended the context into second order or we have
classified the first order context classes again into
second order context classes. It shows that distri-
butions like P (?|vb, conj) and P (?|vbp, conj) are
very different from the first order ones, while distri-
butions like P (?|fw, conj) are not.
Figure 2 shows another way of context extension,
so called lexicalization. Here, the initial first order
context class (the top one) is classified again by re-
ferring the lexical information (the following three
ones). We see that the distribution after the prepo-
sition, out is quite different from distribution after
other prepositions.
From the above observations, we can see that by
applying Markov assumptions we may miss much
useful contextual information, or by getting a better
context classification we can build a better context
model.
4 Related Works
One of the straightforward ways of context exten-
sion is extending context uniformly. Tri-gram tag-
ging models can be thought of as a result of the
uniform extension of context from bi-gram tagging
models. TnT (Brants, 2000) based on a second or-
der HMM, is an example of this class of models and
is accepted as one of the best part-of-speech taggers
used around.
The uniform extension can be achieved (rela-
tively) easily, but due to the exponential growth of
the model size, it can only be performed in restric-
tive a way.
Another way of context extension is the selective
extension of context. In the case of context exten-
sion from lower context to higher like the examples
in figure 1, the extension involves taking more infor-
mation about the same type of contextual features.
We call this kind of extension homogeneous con-
text extension. (Brants, 1998) presents this type of
context extension method through model merging
and splitting, and also prediction suffix tree learn-
ing (Schu?tze and Singer, 1994; D. Ron et. al, 1996)
is another well-known method that can perform ho-
mogeneous context extension.
On the other hand, figure 2 illustrates heteroge-
neous context extension, in other words, this type
of extension involves taking more information about
other types of contextual features. (Kim et. al, 1999)
and (Pla and Molina, 2001) present this type of con-
text extension method, so called selective lexicaliza-
tion.
The selective extension can be a good alternative
to the uniform extension, because the growth rate
of the model size is much smaller, and thus various
contextual features can be exploited. In the follow-
VP
N C
$
$ C N P V
P-1-
$ C N P V
Figure 3: a Markov model and its equivalent deci-
sion tree
ing sections, we describe a novel method of selective
extension of context which performs both homoge-
neous and heterogeneous extension simultaneously.
5 Self-Organizing Markov Models
Our approach to the selective context extension is
making use of the statistical decision tree frame-
work. The states of Markov models are represented
in statistical decision trees, and by growing the trees
the context can be extended (or the states can be
split).
We have named the resulting models Self-
Organizing Markov Models to reflect their ability to
automatically organize the structure.
5.1 Statistical Decision Tree Representation of
Markov Models
The decision tree is a well known structure that is
widely used for classification tasks. When there are
several contextual features relating to the classifi-
cation of a target feature, a decision tree organizes
the features as the internal nodes in a manner where
more informative features will take higher levels, so
the most informative feature will be the root node.
Each path from the root node to a leaf node repre-
sents a context class and the classification informa-
tion for the target feature in the context class will be
contained in the leaf node1 .
In the case of part-of-speech tagging, a classifi-
cation will be made at each position (or time) of a
word sequence, where the target feature is the syn-
tactic class of the word at current position (or time)
and the contextual features may include the syntactic
1While ordinary decision trees store deterministic classifi-
cation information in their leaves, statistical decision trees store
probabilistic distribution of possible decisions.
V
P,*,
N C
$
$ C N W-1- V
P-1-
$ C N P V
P,out, t
P,*,P,out, t
Figure 4: a selectively lexicalized Markov model
and its equivalent decision tree
V
P,*,
N
(N)C( )$
$ P-2- N W-1- V
P-1-
$ C N P V
P,out, t
P,*,P,out, t
(V)C( )
(*)C( )
(*)C( )(N)C( ) (V)C( )
Figure 5: a selectively extended Markov model and
its equivalent decision tree
classes or the lexical form of preceding words. Fig-
ure 3 shows an example of Markov model for a sim-
ple language having nouns (N), conjunctions (C),
prepositions (P) and verbs (V). The dollar sign ($)
represents sentence initialization. On the left hand
side is the graph representation of the Markov model
and on the right hand side is the decision tree repre-
sentation, where the test for the immediately preced-
ing syntactic class (represented by P-1) is placed on
the root, each branch represents a result of the test
(which is labeled on the arc), and the correspond-
ing leaf node contains the probabilistic distribution
of the syntactic classes for the current position2 .
The example shown in figure 4 involves a further
classification of context. On the left hand side, it is
represented in terms of state splitting, while on the
right hand side in terms of context extension (lexi-
calization), where a context class representing con-
textual patterns ending in P (a preposition) is ex-
tended by referring the lexical form and is classi-
fied again into the preposition, out and other prepo-
sitions.
Figure 5 shows another further classification of
2The distribution doesn?t appear in the figure explicitly. Just
imagine each leaf node has the distribution for the target feature
in the corresponding context.
context. It involves a homogeneous extension of
context while the previous one involves a hetero-
geneous extension. Unlike prediction suffix trees
which grow along an implicitly fixed order, decision
trees don?t presume any implicit order between con-
textual features and thus naturally can accommodate
various features having no underlying order.
In order for a statistical decision tree to be a
Markov model, it must meet the following restric-
tions:
? There must exist at least one contextual feature
that is homogeneous with the target feature.
? When the target feature at a certain time is clas-
sified, all the requiring context features must be
visible
The first restriction states that in order to be a
Markov model, there must be inter-relations be-
tween the target features at different time. The sec-
ond restriction explicitly states that in order for the
decision tree to be able to classify contextual pat-
terns, all the context features must be visible, and
implicitly states that homogeneous context features
that appear later than the current target feature can-
not be contextual features. Due to the second re-
striction, the Viterbi algorithm can be used with the
self-organizing Markov models to find an optimal
sequence of tags for a given word sequence.
5.2 Learning Self-Organizing Markov Models
Self-organizing Markov models can be induced
from manually annotated corpora through the SDTL
algorithm (algorithm 1) we have designed. It is a
variation of ID3 algorithm (Quinlan, 1986). SDTL
is a greedy algorithm where at each time of the node
making phase the most informative feature is se-
lected (line 2), and it is a recursive algorithm in the
sense that the algorithm is called recursively to make
child nodes (line 3),
Though theoretically any statistical decision tree
growing algorithms can be used to train self-
organizing Markov models, there are practical prob-
lems we face when we try to apply the algorithms to
language learning problems. One of the main obsta-
cles is the fact that features used for language learn-
ing often have huge sets of values, which cause in-
tensive fragmentation of the training corpus along
with the growing process and eventually raise the
sparse data problem.
To deal with this problem, the algorithm incor-
porates a value selection mechanism (line 1) where
only meaningful values are selected into a reduced
value set. The meaningful values are statistically
defined as follows: if the distribution of the target
feature varies significantly by referring to the value
v, v is accepted as a meaningful value. We adopted
the ?2-test to determine the difference between the
distributions of the target feature before and after re-
ferring to the value v. The use of ?2-test enables
us to make a principled decision about the threshold
based on a certain confidence level3.
To evaluate the contribution of contextual features
to the target classification (line 2), we adopted Lopez
distance (Lo?pez, 1991). While other measures in-
cluding Information Gain or Gain Ratio (Quinlan,
1986) also can be used for this purpose, the Lopez
distance has been reported to yield slightly better re-
sults (Lo?pez, 1998).
The probabilistic distribution of the target fea-
ture estimated on a node making phase (line 4) is
smoothed by using Jelinek and Mercer?s interpola-
tion method (Jelinek and Mercer, 1980) along the
ancestor nodes. The interpolation parameters are
estimated by deleted interpolation algorithm intro-
duced in (Brants, 2000).
6 Experiments
We performed a series of experiments to compare
the performance of self-organizing Markov models
with traditional Markov models. Wall Street Jour-
nal as contained in Penn Treebank II is used as the
reference material. As the experimental task is part-
of-speech tagging, all other annotations like syntac-
tic bracketing have been removed from the corpus.
Every figure (digit) in the corpus has been changed
into a special symbol.
From the whole corpus, every 10?th sentence from
the first is selected into the test corpus, and the re-
maining ones constitute the training corpus. Table 6
shows some basic statistics of the corpora.
We implemented several tagging models based on
equation (3). For the tag language model, we used
3We used 95% of confidence level to extend context. In
other words, only when there are enough evidences for improve-
ment at 95% of confidence level, a context is extended.
Algorithm 1: SDTL(E, t, F )
Data : E: set of examples,
t: target feature,
F : set of contextual features
Result : Statistical Decision Tree predicting t
initialize a null node;
for each element f in the set F do
1 sort meaningful value set V for f ;
if |V | > 1 then
2 measure the contribution of f to t;
if f contributes the most then
select f as the best feature b;
end
end
end
if there is b selected then
set the current node to an internal node;
set b as the test feature of the current node;
3 for each v in |V | for b do
make SDTL(Eb=v, t, F ? {b}) as the
subtree for the branch corresponding to
v;
end
end
else
set the current node to a leaf node;
4 store the probability distribution of t over
E ;
end
return current node;
1,289,20168,590Total
129,1006,859Test
1,160,10161,731Training
 	 
  	 
   	 
  	 
  	 
  	 
  	 
  	 
       	 
  	 
  	 
  	 
  	 
  	 
  	 
  	 
     
Figure 6: Basic statistics of corpora
the following 6 approximations:
P (t1,k) ?
k
?
i=1
P (ti|ti?1) (8)
?
k
?
i=1
P (ti|ti?2,i?1) (9)
?
k
?
i=1
P (ti|?(ti?2,i?1)) (10)
?
k
?
i=1
P (ti|?(ti?1, wi?1)) (11)
?
k
?
i=1
P (ti|?(ti?2,i?1, wi?1)) (12)
?
k
?
i=1
P (ti|?(ti?2,i?1, wi?2,i?1))(13)
Equation (8) and (9) represent first- and second-
order Markov models respectively. Equation (10)
? (13) represent self-organizing Markov models at
various settings where the classification functions
?(?) are intended to be induced from the training
corpus.
For the estimation of the tag-to-word translation
model we used the following model:
P (wi|ti)
= ki ? P (ki|ti) ? P? (wi|ti)
+(1 ? ki) ? P (?ki|ti) ? P? (ei|ti) (14)
Equation (14) uses two different models to estimate
the translation model. If the word, wi is a known
word, ki is set to 1 so the second model is ig-
nored. P? means the maximum likelihood probabil-
ity. P (ki|ti) is the probability of knownness gener-
ated from ti and is estimated by using Good-Turing
estimation (Gale and Samson, 1995). If the word, wi
is an unknown word, ki is set to 0 and the first term
is ignored. ei represents suffix of wi and we used the
last two letters for it.
With the 6 tag language models and the 1 tag-to-
word translation model, we construct 6 HMM mod-
els, among them 2 are traditional first- and second-
hidden Markov models, and 4 are self-organizing
hidden Markov models. Additionally, we used T3,
a tri-gram-based POS tagger in ICOPOST release
1.8.3 for comparison.
The overall performances of the resulting models
estimated from the test corpus are listed in figure 7.
From the leftmost column, it shows the model name,
the contextual features, the target features, the per-
formance and the model size of our 6 implementa-
tions of Markov models and additionally the perfor-
mance of T3 is shown.
Our implementation of the second-order hid-
den Markov model (HMM-P2) achieved a slightly
worse performance than T3, which, we are in-
terpreting, is due to the relatively simple imple-
mentation of our unknown word guessing module4.
While HMM-P2 is a uniformly extended model
from HMM-P1, SOHMM-P2 has been selectively
extended using the same contextual feature. It is
encouraging that the self-organizing model suppress
the increase of the model size in half (2,099Kbyte vs
5,630Kbyte) without loss of performance (96.5%).
In a sense, the results of incorporating word
features (SOHMM-P1W1, SOHMM-P2W1 and
SOHMM-P2W2) are disappointing. The improve-
ments of performances are very small compared to
the increase of the model size. Our interpretation
for the results is that because the distribution of
words is huge, no matter how many words the mod-
els incorporate into context modeling, only a few of
them may actually contribute during test phase. We
are planning to use more general features like word
class, suffix, etc.
Another positive observation is that a homo-
geneous context extension (SOHMM-P2) and a
heterogeneous context extension (SOHMM-P1W1)
yielded significant improvements respectively, and
the combination (SOHMM-P2W1) yielded even
more improvement. This is a strong point of using
decision trees rather than prediction suffix trees.
7 Conclusion
Through this paper, we have presented a framework
of self-organizing Markov model learning. The
experimental results showed some encouraging as-
pects of the framework and at the same time showed
the direction towards further improvements. Be-
cause all the Markov models are represented as de-
cision trees in the framework, the models are hu-
4T3 uses a suffix trie for unknown word guessing, while our
implementations use just last two letters.
?96.6??T3
96.9
96.8
96.3
96.5
96.5
95.6
 	
                 	

24,628KT0P-2, W-1, P-1SOHMM-P2W1
W-2, P-2, W-1, P-1
W-1, P-1
P-2, P-1
P-2, P-1
P-1
T0
T0
T0
T0
T0
14,247KSOHMM-P1W1
35,494K
2,099K
5,630K
123K
SOHMM-P2
SOHMM-P2W2 
HMM-P2
HMM-P1
                
Figure 7: Estimated Performance of Various Models
man readable and we are planning to develop editing
tools for self-organizing Markov models that help
experts to put human knowledge about language into
the models. By adopting ?2-test as the criterion for
potential improvement, we can control the degree of
context extension based on the confidence level.
Acknowledgement
The research is partially supported by Information
Mobility Project (CREST, JST, Japan) and Genome
Information Science Project (MEXT, Japan).
References
L. Rabiner. 1989. A tutorial on Hidden Markov Mod-
els and selected applications in speech recognition. in
Proceedings of the IEEE, 77(2):257?285
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, UK.
I. Schr o?der. 2001. ICOPOST - Ingo?s Collection
Of POS Taggers. In http://nats-www.informatik.uni-
hamburg.de/?ingo/icopost/.
T. Brants. 1998 Estimating HMM Topologies. In The
Tbilisi Symposium on Logic, Language and Computa-
tion: Selected Papers.
T. Brants. 2000 TnT - A Statistical Part-of-Speech Tag-
ger. In 6?th Applied Natural Language Processing.
H. Sch u?tze and Y. Singer. 1994. Part-of-speech tagging
using a variable memory Markov model. In Proceed-
ings of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
D. Ron, Y. Singer and N. Tishby. 1996 The Power of
Amnesia: Learning Probabilistic Automata with Vari-
able Memory Length. In Machine Learning, 25(2-
3):117?149.
J.-D. Kim, S.-Z. Lee and H.-C. Rim. 1999 HMM
Specialization with Selective Lexicalization. In
Proceedings of the Joint SIGDAT Conference on
Empirical Methods in NLP and Very Large Cor-
pora(EMNLP/VLC99).
F. Pla and A. Molina. 2001 Part-of-Speech Tagging
with Lexicalized HMM. In Proceedings of the Inter-
national Conference on Recent Advances in Natural
Language Processing(RANLP2001).
R. Quinlan. 1986 Induction of decision trees. In Ma-
chine Learning, 1(1):81?106.
R. L o?pez de M a?ntaras. 1991. A Distance-Based At-
tribute Selection Measure for Decision Tree Induction.
In Machine Learning, 6(1):81?92.
R. L o?pez de M a?ntaras, J. Cerquides and P. Garcia. 1998.
Comparing Information-theoretic Attribute Selection
Measures: A statistical approach. In Artificial Intel-
ligence Communications, 11(2):91?100.
F. Jelinek and R. Mercer. 1980. Interpolated estimation
of Markov source parameters from sparse data. In Pro-
ceedings of the Workshop on Pattern Recognition in
Practice.
W. Gale and G. Sampson. 1995. Good-Turing frequency
estimatin without tears. In Jounal of Quantitative Lin-
guistics, 2:217?237
A Syllable Based Word Recognition Model
for Korean Noun Extraction
Do-Gil Lee and Hae-Chang Rim
Dept. of Computer Science & Engineering
Korea University
1, 5-ka, Anam-dong, Seongbuk-ku
Seoul 136-701, Korea
dglee, rim@nlp.korea.ac.kr
Heui-Seok Lim
Dept. of Information & Communications
Chonan University
115 AnSeo-dong
CheonAn 330-704, Korea
limhs@infocom.chonan.ac.kr
Abstract
Noun extraction is very important for
many NLP applications such as informa-
tion retrieval, automatic text classification,
and information extraction. Most of the
previous Korean noun extraction systems
use a morphological analyzer or a Part-
of-Speech (POS) tagger. Therefore, they
require much of the linguistic knowledge
such as morpheme dictionaries and rules
(e.g. morphosyntactic rules and morpho-
logical rules).
This paper proposes a new noun extrac-
tion method that uses the syllable based
word recognition model. It finds the
most probable syllable-tag sequence of
the input sentence by using automatically
acquired statistical information from the
POS tagged corpus and extracts nouns by
detecting word boundaries. Furthermore,
it does not require any labor for construct-
ing and maintaining linguistic knowledge.
We have performed various experiments
with a wide range of variables influenc-
ing the performance. The experimental
results show that without morphological
analysis or POS tagging, the proposed
method achieves comparable performance
with the previous methods.
1 Introduction
Noun extraction is a process to find every noun in
a document (Lee et al, 2001). In Korean, Nouns
are used as the most important terms (features) that
express the document in NLP applications such as
information retrieval, document categorization, text
summarization, information extraction, and etc.
Korean is a highly agglutinative language and
nouns are included in Eojeols. An Eojeol is a sur-
face level form consisting of more than one com-
bined morpheme. Therefore, morphological anal-
ysis or POS tagging is required to extract Korean
nouns.
The previous Korean noun extraction methods are
classified into two categories: morphological analy-
sis based method (Kim and Seo, 1999; Lee et al,
1999a; An, 1999) and POS tagging based method
(Shim et al, 1999; Kwon et al, 1999). The mor-
phological analysis based method tries to generate
all possible interpretations for a given Eojeol by
implementing a morphological analyzer or a sim-
pler method using lexical dictionaries. It may over-
generate or extract inaccurate nouns due to lexical
ambiguity and shows a low precision rate. Although
several studies have been proposed to reduce the
over-generated results of the morphological analy-
sis by using exclusive information (Lim et al, 1995;
Lee et al, 2001), they cannot completely resolve the
ambiguity.
The POS tagging based method chooses the most
probable analysis among the results produced by the
morphological analyzer. Due to the resolution of the
ambiguities, it can obtain relatively accurate results.
But it also suffers from errors not only produced by a
POS tagger but also triggered by the preceding mor-
phological analyzer.
Furthermore, both methods have serious deficien-
???(Cheol-Su-neun) ????(sa-lam-deul-eul) ??(bwass-da)
??(Cheol-Su) ?(neun) ???(sa-lam-deul) ?(eul) ??(bwass-da)
??(Cheol-Su) ??(sa-lam) ?(deul) ?(eul) ?(bo) ?(ass) ?(da)
eojeol
word
morpheme
proper
 noun
 :
 
person
 name
postposition
noun
 :
 person
noun
 suffix:
 plural
postposition
verb
 :
 see
prefinal
 ending
ending
?(neun)
Figure 1: Constitution of the sentence ?(Cheol-Su saw the persons)?
cies in that they require considerable manual la-
bor to construct and maintain linguistic knowledge
and suffer from the unknown word problem. If
a morphological analyzer fails to recognize an un-
known noun in an unknown Eojeol, the POS tagger
would never extract the unknown noun. Although
the morphological analyzer properly recognizes the
unknown noun, it would not be extracted due to the
sparse data problem.
This paper proposes a new noun extraction
method that uses a syllable based word recognition
model. The proposed method does not require labor
for constructing and maintaining linguistic knowl-
edge and it can also alleviate the unknown word
problem or the sparse data problem. It finds the most
probable syllable-tag sequence of the input sentence
by using statistical information and extracts nouns
by detecting the word boundaries. The statistical in-
formation is automatically acquired from a POS an-
notated corpus and the word boundary can be de-
tected by using an additional tag to represent the
boundary of a word.
This paper is organized as follows. In Section 2,
the notion of word is defined. Section 3 presents
the syllable based word recognition model. Section
4 describes the method of constructing the training
data from existing POS tagged corpora. Section 5
discusses experimental results. Finally, Section 6
concludes the paper.
2 A new definition of word
Korean spacing unit is an Eojeol, which is delimited
by whitespace, as with word in English. In Korean,
an Eojeol is made up of one or more words, and a
word is made up of one or more morphemes. Figure
1 represents the relationships among morphemes,
words, and Eojeols with an example sentence. Syl-
lables are delimited by a hyphen in the figure.
All of the previous noun extraction methods re-
gard a morpheme as a processing unit. In order to
extract nouns, nouns in a given Eojeol should be
segmented. To do this, the morphological analysis
has been used, but it requires complicated processes
because of the surface forms caused by various mor-
phological phenomena such as irregular conjugation
of verbs, contraction, and elision. Most of the mor-
phological phenomena occur at the inside of a mor-
pheme or the boundaries between morphemes, not a
word. We have also observed that a noun belongs to
a morpheme as well as a word. Thus, we do not have
to do morphological analysis in the noun extraction
point of view.
In Korean linguistics, a word is defined as a mor-
pheme or a sequence of morphemes that can be used
independently. Even though a postposition is not
used independently, it is regarded as a word because
it is easily segmented from the preceding word. This
definition is rather vague for computational process-
ing. If we follow the definition of the word in lin-
guistics, it would be difficult to analyze a word like
the morphological analysis. For this reason, we de-
fine a different notion of a word.
According to our definition of a word, each un-
inflected morpheme or a sequence of successive
inflected morphemes is regarded as an individual
word. 1 By virtue of the new definition of a word,
we need not consider mismatches between the sur-
face level form and the lexical level one in recogniz-
ing words.
The example sentence ?  
(Cheol-Su saw the persons)? represented in Fig-
ure 1 includes six words such as ?(Cheol-Su)?,
?(neun)?, ?(sa-lam)?, ?(deul)?, ?(eul)?,
and ?(bwass-da)?. Unlike the Korean linguis-
tics, a noun suffix such as ?(nim)?, ?(deul)?, or
?(jeog)? is also regarded as a word because it is
an uninflected morpheme.
3 Syllable based word recognition model
A Korean syllable consists of an obligatory onset
(initial-grapheme, consonant), an obligatory peak
(nuclear grapheme, vowel), and an optional coda
(final-grapheme, consonant). In theory, the number
of syllables that can be used in Korean is the same as
the number of every combination of the graphemes.
2 Fortunately, only a fixed number of syllables is
frequently used in practice. 3 The amount of in-
formation that a Korean syllable has is larger than
that of an alphabet in English. In addition, there are
particular characteristics in Korean syllables. The
fact that words do not start with certain syllables
is one of such examples. Several attempts have
been made to use characteristics of Korean sylla-
bles. Kang (1995) used syllable information to re-
duce the over-generated results in analyzing conju-
gated forms of verbs. Syllable statistics have been
also used for automatic word spacing (Shim, 1996;
Kang and Woo, 2001; Lee et al, 2002).
The syllable based word recognition model is rep-
resented as a function  like the following equations.
It is to find the most probable syllable-tag sequence


 

 

  

, for a given sentence  consist-
ing of a sequence of  syllables 

 

 

  

.
1Korean morphemes can be classified into two types: un-
inflected morphemes having fixed word forms (such as noun,
unconjugated adjective, postposition, adverb, interjection, etc.)
and inflected morphemes having conjugated word forms (such
as a morpheme with declined or conjugated endings, predicative
postposition, etc.)
2
 (   ) of pure Korean syllables are pos-
sible
3Actually,   of syllables are used in the training data,
including Korean characters and non-Korean characters (e.g. al-
phabets, digits, Chinese characters, symbols).





	


 

 

 (1)
 	





 

 

 

 

(2)
Two Markov assumptions are applied in Equation
2. One is that the probability of a current syllable tag


conditionally depends on only the previous sylla-
ble tag. The other is that the probability of a cur-
rent syllable 

conditionally depends on the current
tag. In order to reflect word spacing information in
Equation 2, which is very useful in Korean POS tag-
ging, Equation 2 is changed to Equation 3 which can
consider the word spacing information by calculat-
ing the transition probabilities like the equation used
in Kim et al (1998).


  	





 

 

 	 

 

 (3)
In the equation, 	 becomes zero if the transition oc-
curs in the inside of an Eojeol; otherwise 	 is one.
Word boundaries can be detected by an additional
tag. This method has been used in some tasks such
as text chunking and named entity recognition to
represent a boundary of an element (e.g. individual
phrase or named entity). There are several possi-
ble representation schemes to do this. The simplest
one is the BIO representation scheme (Ramshaw and
Marcus, 1995), where a ?B? denotes the first item of
an element and an ?I? any non-initial item, and a
syllable with tag ?O? is not a part of any element.
Because every syllable corresponds to one syllable
tag, ?O? is not used in our task. The representation
schemes used in this paper are described in detail in
Section 4.
The probabilities in Equation 3 are estimated by
the maximum likelihood estimator (MLE) using rel-
ative frequencies in the training data. 4
The most probable sequence of syllable tags in a
sentence (a sequence of syllables) can be efficiently
computed by using the Viterbi algorithm.
4Since the MLE suffers from zero probability, to avoid zero
probability, we just assign a very low value such as 		
for an unseen event in the training data.
Table 1: Examples of syllable tagging by BI, BIS, IE, and IES representation schemes
surface level lexical level BI BIS IE IES(syllable) (morpheme/POS tag)
(yak)
(yak-sok)/nc B-nc B-nc I-nc I-nc
(sok) I-nc I-nc E-nc E-nc
(jang)
(jang-so)/nc B-nc B-nc I-nc I-nc
(so) I-nc I-nc E-nc E-nc
(in) (i)/co+ (n)/etm B-co etm S-co etm E-co etm S-co etm
(Sin)
(Sin-la-ho-tel)/nc
B-nc B-nc I-nc I-nc
(la) I-nc I-nc I-nc I-nc
(ho) I-nc I-nc I-nc I-nc
(tel) I-nc I-nc E-nc E-nc
	
(keo)
	
(keo-pi-syob)/nc
B-nc B-nc I-nc I-nc
(pi) I-nc I-nc I-nc I-nc
(syob) I-nc I-nc E-nc E-nc
(e) (e)/jc B-jc S-jc E-jc S-jc
(Jai)
(Jai-Ok)/nc B-nc B-nc I-nc I-nc
(Ok) I-nc I-nc E-nc E-nc
(i) (i)/jc B-jc S-jc E-jc S-jc
	(meon)
	(meon-jeo)/mag B-mag B-mag I-mag I-mag
(jeo) I-mag I-mag E-mag E-mag
(wa) (o)/pv+(a)/ec B-pv ec S-pv ec E-pv ec S-pv ec
(gi)
(gi-da-li)/pv+Part-of-Speech Tagging Considering Surface Form
for an Agglutinative Language
Do-Gil Lee and Hae-Chang Rim
Dept. of Computer Science & Engineering
Korea University
1, 5-ka, Anam-dong, Seongbuk-ku
Seoul 136-701, Korea
dglee, rim@nlp.korea.ac.kr
Abstract
The previous probabilistic part-of-speech tagging
models for agglutinative languages have consid-
ered only lexical forms of morphemes, not surface
forms of words. This causes an inaccurate cal-
culation of the probability. The proposed model
is based on the observation that when there exist
words (surface forms) that share the same lexical
forms, the probabilities to appear are different from
each other. Also, it is designed to consider lexi-
cal form of word. By experiments, we show that
the proposed model outperforms the bigram Hidden
Markov model (HMM)-based tagging model.
1 Introduction
Part-of-speech (POS) tagging is a job to assign a
proper POS tag to each linguistic unit such as word
for a given sentence. In English POS tagging, word
is used as a linguistic unit. However, the num-
ber of possible words in agglutinative languages
such as Korean is almost infinite because words can
be freely formed by gluing morphemes together.
Therefore, morpheme-unit tagging is preferred and
more suitable in such languages than word-unit tag-
ging. Figure 1 shows an example of morpheme
structure of a sentence, where the bold lines indi-
cate the most likely morpheme-POS sequence. A
solid line represents a transition between two mor-
phemes across a word boundary and a dotted line
represents a transition between two morphemes in a
word.
The previous probabilistic POS models for ag-
glutinative languages have considered only lexical
forms of morphemes, not surface forms of words.
This causes an inaccurate calculation of the proba-
bility. The proposed model is based on the obser-
vation that when there exist words (surface forms)
that share the same lexical forms, the probabilities
to appear are different from each other. Also, it is
designed to consider lexical form of word. By ex-
periments, we show that the proposed model outper-
forms the bigram Hidden Markov model (HMM)-
based tagging model.
2 Korean POS tagging model
In this section, we first describe the standard
morpheme-unit tagging model and point out a mis-
take of this model. Then, we describe the proposed
model.
2.1 Standard morpheme-unit model
This section describes the HMM-based morpheme-
unit model. The morpheme-unit POS tagging model
is to find the most likely sequence of morphemes 
and corresponding POS tags  for a given sentence
 , as follows (Kim et al, 1998; Lee et al, 2000):
 




   
	 




 
	
 
	



 (1)
 




 
	
 
	
 (2)
In the equation, 	
	  denotes the number of
morphemes in the sentence. A sequence of  	



	 



  


is a sentence of  words, and a
sequence of  	 
	
	 



  
	
and a se-
quence of  	 
	
	 



   
	
denote a sequence
of 	 lexical forms of morphemes and a sequence of
	 morpheme categories (POS tags), respectively.
To simplify Equation 2, a Markov assumption is
usually used as follows:
   




	


 

 

  



 (3)
where, 

is a pseudo tag which denotes the begin-
ning of word and is also written as  .  de-
notes a type of transition from the previous tag to
the current tag. It has a binary value according to
the type of the transition (either intra-word or inter-
word transition).
As can be seen, the word1 sequence 


is dis-
carded in Equation 2. This leads to an inaccurate
1A word is a surface form.
na/NNP
na/VV
na/VX
nal/VV
neun/PX
neun/EFD
hag-gyo/NNC e/PA
ga/VV
ga/VX
gal/VV
n-da/EFF
n-da/EFC
BOS
EOS
Figure 1: Morpheme structure of the sentence ?na-neun hag-gyo-e gan-da? (I go to school)
calculation of the probability. A lexical form of a
word can be mapped to more than one surface word.
In this case, although the different surface forms are
given, if they have the same lexical form, then the
probabilities will be the same. For example, a lexi-
cal form mong-go/nc+leul/jc2 , can be mapped from
two surface forms mong-gol and mong-go-leul. By
applying Equation 1 and Equation 2 to both words,
the following equations can be derived:
 mong-go  leul  mong-gol
  mong-go  leul  (4)
 mong-go  leul  mong-go-leul
  mong-go  leul  (5)
As a result, we can acquire the following equation
from Equation 4 and Equation 5:
  mong-go  leul  mong-gol
	  mong-go  leul  mong-go-leul (6)
That is, they assume that probabilities of
the results that have the same lexical form
are the same. However, we can easily
show that Equation 6 is mistaken: Actually,
 mong-go  leul   mong-go-leul 	 

and  mong-gol   mong-gol 	 .
Hence,  mong-go  leul   mong-gol 
 mong-go  leul  mong-go-leul.
To overcome the disadvantage, we propose a new
tagging model that can consider the surface form.
2.2 The proposed model
This section describes the proposed model. To sim-
plify the notation, we introduce a variable R, which
means a tagging result of a given sentence and con-
sists of  and  .
 




    (7)
	 

    (8)
2mong-go means Mongolia, nc is a common noun, and jc is
a objective case postposition.
The probability     is given as follows:
    	  





 (9)
	




 




 

 (10)





 



 

 (11)
where, 

denotes the tagging result of th word
(

), and 

denotes a pseudo variable to indicate
the beginning of word. Equation 9 becomes Equa-
tion 10 by the chain rule. To be a more tractable
form, Equation 10 is simplified by a Markov as-
sumption as Equation 11.
The probability  



 

 cannot be calcu-
lated directly, so it is derived as follows:
 



 

 
 

 

 


 

 


(12)

 

 



 




 

 


(13)

 



 




 


(14)
  




 

 


 

 


(15)
Equation 12 is derived by Bayes rule, Equation
13 by a chain rule and an independence assumption,
and Equation 15 by Bayes rule. In Equation 15, we
call the left term ?morphological analysis model?
and right one ?transition model?.
The morphological analysis model  



 can
be implemented in a morphological analyzer. If a
morphological analyzer can provide the probability,
then the tagger can use the values as they are. Ac-
tually, we use the probability that a morphological
analyzer, ProKOMA (Lee and Rim, 2004) produces.
Although it is not necessary to discuss the morpho-
logical analysis model in detail, we should note that
surface forms are considered here.
The transition model is a form of point-wise mu-
tual information.
 

 


 

 



 

 



 


 

 

 

 


(16)

 


 





 



 


 


 


 



(17)
where, a superscript  in 

and 

denotes the
position of the word in a sentence.
The denominator means a joint probability that
the morphemes and the tags in a word appear to-
gether, and the numerator means a joint probability
that all the morphemes and the tags between two
words appear together. Due to the sparse data prob-
lem, they cannot also be calculated directly from the
test data. By aMarkov assumption, the denominator
and the numerator can be broken down into Equa-
tion 18 and Equation 19, respectively.
 

 

 	



 

 

 

 

 (18)
 


 





 


 




 






 







 
	






   












 








 









(19)
where, 





 


 means a transition probabil-
ity between the last morpheme of the 
th word
and the first morpheme of the th word.
By applying Equation 18 and Equation 19 to
Equation 17, we obtain the following equation:
 

 


 

 


	






 



 


 
(20)
For a given sentence, Figure 2 shows the bigram
HMM-based tagging model, and Figure 3 the pro-
posed model. The main difference between the
two models is the proposed model considers surface
forms but the HMM does not.
3 Experiments
For evaluation, two data sets are used: ETRI POS
tagged corpus and KAIST POS tagged corpus. We
divided the test data into ten parts. The perfor-
mances of the model are measured by averaging
over the ten test sets in the 10-fold cross-validation
experiment. Table 1 shows the summary of the cor-
pora.
Table 1: Summary of the data
Corpus ETRI KAIST
Total # of words 288,291 175,468
Total # of sentences 27,855 16,193
# of tags 27 54
Generally, POS tagging goes through the fol-
lowing steps: First, run a morphological analyzer,
where it generates all the possible interpretations
for a given input text. Then, a POS tagger takes
the results as input and chooses the most likely one
among them. Therefore, the performance of the tag-
ger depends on that of the preceding morphological
analyzer.
If the morphological analyzer does not generate
the exact result, the tagger has no chance to se-
lect the correct one, thus an answer inclusion rate
of the morphological analyzer becomes the upper
bound of the tagger. The previous works prepro-
cessed the dictionary to include all the exact an-
swers in the morphological analyzer?s results. How-
ever, this evaluation method is inappropriate to the
real application in the strict sense. In this experi-
ment, we present the accuracy of the morphologi-
cal analyzer instead of preprocessing the dictionary.
ProKOMA?s results with the test data are listed in
Table 2.
Table 2: Morphological analyzer?s results with the
test data
Corpus ETRI KAIST
Answer inclusion rate (%) 95.82 95.95
Average # of results per word 2.16 1.81
1-best accuracy (%) 88.31 90.12
In the table, 1-best accuracy is defined as the
number of words whose result with the highest
probability is matched to the gold standard over the
entire words in the test data. This can also be a tag-
ging model that does not consider any outer context.
To compare the proposed model with the standard
model, the results of the two models are given in
Table 3. As can be seen, our model outperforms the
HMM model. Moreover, the HMM model is even
worse than the ProKOMA?s 1-best accuracy. This
tells that the standard HMM by itself is not a good
model for agglutinative languages.
4 Conclusion
We have presented a new POS tagging model that
can consider the surface form for Korean, which
BOS EOSNNP
na
PX
neun
NNC
hag-gyo
PA
e
VV
ga
EFF
n-da
Figure 2: Lattice of the bigram HMM-based model
na/NNP+neun/PX hag-gyo/NNC+e/PA ga/VV+n-da/EFFBOS EOS
na-neun hag-gyo-e gan-da
Figure 3: Lattice of the proposed model
Table 3: Tagging accuracies (%) of the standard
HMM and the proposed model
Corpus ETRI KAIST
The standard HMM 87.47 89.83
The proposed model 90.66 92.01
is an agglutinative language. Although the model
leaves much room for improvement, it outperforms
the HMM based model according to the experimen-
tal results.
Acknowledgement
This work was supported by Korea Research Foun-
dation Grant (KRF-2003-041-D20485)
References
J.-D. Kim, S.-Z. Lee, and H.-C. Rim. 1998. A
morpheme-unit POS tagging model considering
word-spacing. In Proceedings of the 1998 Con-
ference on Hangul and Korean Information Pro-
cessing, pages 3?8.
D.-G. Lee and H.-C. Rim. 2004. ProKOMA:
A probabilistic Korean morphological analyzer.
Technical Report KU-NLP-04-01, Department of
Computer Science and Engineering, Korea Uni-
versity.
S.-Z. Lee, Jun?ichi Tsujii, and H.-C. Rim. 2000.
Hidden markov model-based Korean part-of-
speech tagging considering high agglutinativity,
word-spacing, and lexical correlativity. In Pro-
ceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 29?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
K-QARD: A Practical Korean Question Answering Framework for
Restricted Domain
Young-In Song, HooJung Chung,
Kyoung-Soo Han, JooYoung Lee,
Hae-Chang Rim
Dept. of Computer Science & Engineering
Korea University
Seongbuk-gu, Seoul 136-701, Korea
 song, hjchung, kshan, jylee
rim@nlp.korea.ac.kr
Jae-Won Lee
Computing Lab.
Samsung Advanced Institute of Technology
Nongseo-ri, Giheung-eup,
Yongin-si, Gyeonggi-do 449-712, Korea
jwonlee@samsung.com
Abstract
We present a Korean question answer-
ing framework for restricted domains,
called K-QARD. K-QARD is developed to
achieve domain portability and robustness,
and the framework is successfully applied
to build question answering systems for
several domains.
1 Introduction
K-QARD is a framework for implementing a fully
automated question answering system including
the Web information extraction (IE). The goal of
the framework is to provide a practical environ-
ment for the restricted domain question answering
(QA) system with the following requirements:
  Domain portability: Domain adaptation of
QA systems based on the framework should
be possible with minimum human efforts.
  Robustness: The framework has to provide
methodologies to ensure robust performance
for various expressions of a question.
For the domain portability, K-QARD is de-
signed as a domain-independent architecture and
it keeps all domain-dependent elements in exter-
nal resources. In addition, the framework tries to
employ various techniques for reducing the human
effort, such as simplifying rules based on linguis-
tic information and machine learning approaches.
Our effort for the robustness is focused the
question analysis. Instead of using a technique
for deep understanding of the question, the ques-
tion analysis component of K-QARD tries to ex-
tract only essential information for answering us-
ing the information extraction technique with lin-
guistic information. Such approach is helpful for
NL Answer
Question Analysis
Web Information 
Extraction
Answer Finding
Answer 
Generation
Database
Web Page
NL Question
Web Page
Semantic frames
TE/TR rules
Domain ontology
Training examples
Answer frames
Domain-dependent
External Resources
Domain-independent
Framework
Figure 1: Architecture of K-QARD
not only the robustness but also the domain porta-
bility because it generally requires smaller size of
hand-crafted rules than a complex semantic gram-
mar.
K-QARD uses the structural information auto-
matically extracted from Web pages which include
domain-specific information for question answer-
ing. It has the disavantage that the coverage of QA
system is limited, but it can simplify the question
answering process with robust performance.
2 Architecture of K-QARD
As shown in Figure 1, K-QARD has four major
components: Web information extraction, ques-
tion analysis, answer finding, and answer gener-
ation.
The Web information extraction (IE) compo-
nent extracts the domain-specific information for
question answering from Web pages and stores
the information into the relational database. For
the domain portability, the Web IE component
is based on the automatic wrapper induction ap-
proach which can be learned from small size of
training examples.
The question analysis component analyzes an
29
input question, extracts important information us-
ing the IE approach, and matches the question with
pre-defined semantic frames. The component out-
puts the best-matched frame whose slots are filled
with the information extracted from the question.
In the answer finding component, K-QARD re-
trieves the answers from the database using the
SQL generation script defined in each semantic
frame. The SQL script dynamically generates
SQL using the values of the frame slots.
The answer generation component provides the
answer to the user as a natural language sentence
or a table by using the generation rules and the
answer frames which consist of canned texts.
3 Question Analysis
The key component for ensuring the robustness
and domain portability is the question analy-
sis because it naturally requires many domain-
dependent resources and has responsibility to
solve the problem caused by various ways of ex-
pressing a question. In K-QARD, a question is an-
alyzed using the methods devised by the informa-
tion extraction approach. This IE-based question
analysis method consists of several steps:
1. Natural language analysis: Analyzing the
syntactic structure of the user?s question and
also identifiying named-entities and some im-
portant words, such as domain-specific pred-
icate or terms.
2. Question focus recognition: Finding the
intention of the user?s question using the
question focus classifier. It is learned from
the training examples based on decision
tree(C4.5)(Quinlan, 1993).
3. Template Element(TE) recognition: Find-
ing important concept for filling the slots
of the semantic frame, namely template el-
ements, using the rules, NE information, and
ontology, etc.
4. Template Relation(TR) recognition: Find-
ing the relation between TEs and a question
focus based on TR rules, and syntactic infor-
mation, etc.
Finally, the question analysis component selects
the proper frame for the question and fills proper
values of each slot of the selected frame.
Compared to other question analysis methods
such as the complex semantic grammar(Martin et
al., 1996), our approach has several advantages.
First, it shows robust performance for the variation
of a question because IE-based approach does not
require the understanding of the entire sentence. It
is sufficient to identify and process only the impor-
tant concepts. Second, it also enhances the porta-
bility of the QA systems. This method is based on
the divide-and-conquer strategy and uses only lim-
ited context information. By virture of these char-
acteristics, the question analysis can be processed
by using a small number of simple rules.
In the following subsections, we will describe
each component of our question analyzer in K-
QARD.
3.1 Natural language analysis
The natural language analyzer in K-QARD iden-
tifies morphemes, tags part-of-speeches to them,
and analyzes dependency relations between the
morphemes. A stochastic part-of-speech tagger
and dependency parser(Chung and Rim, 2004) for
the Korean language are trained on a general do-
main corpus and are used for the analyzer. Then,
several domain-specific named entities, such as a
TV program name, and general named entities,
such as a date, in the question are recognized us-
ing our dictionary and pattern-based named entity
tagger(Lee et al, 2004). Finally some important
words, such as domain-specific predicates, ter-
minologies or interrogatives, are replaced by the
proper concept names in the ontology. The man-
ually constructed ontology includes two different
types of information: domain-specific and general
domain words.
The role of this analyzer is to analyze user?s
question and transform it to the more generalized
representation form. So, the task of the question
focus recognition and the TE/TR recognition can
be simplified because of the generalized linguistic
information without decreasing the performance
of the question analyzer.
One of possible defects of using such linguis-
tic information is the loss of the robustness caused
by the error of the NLP components. However,
our IE-based approach for question analysis uses
the very restricted and essential contextual infor-
mation in each step and can avoid such a risk suc-
cessfully.
The example of the analysis process of this
30
Question :   ??? NBC?? ??? ?? ?? ????
(today) (on NBC)
(at night)
(program)
(play)
(what)
(?What movie will be played on NBC tonight?? in English)
(1) : 
Natural Language Analysis
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
????/C_what
(what)
????/C_prog
(program)
????/C_play
(play)
(2) : 
Question Focus Recognition
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
????/C_what
(what)
????/C_prog
(program)
????/C_play
(play)
Question focus region
Question focus : QF_program
a 
(3) : 
TE Recognition
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
Question focus : QF_program
TE_BEGINDATE
TE_BEGINTime
TE_CHANNEL
(4) : 
TR Recognition
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
TE_BEGINDATE
TE_BEGINTime
TE_CHANNEL
REL_OK
REL_OK
REL_OK
Translation of Semantic Frame
FRM : PROGRAM_QUESTION
Question focus : QF_program
Begin Date : ?Today?
Begin Time : ?Night?
Channel : ?NBC?
Question focus : QF_program
?NE_*? denotes that the corresponding word is named entity of *.
?C_*? denotes that the corresponding word is belong to the concept C_* in the ontology.
?TE_*? denotes that the corresponding word is template element whose type is *.
?REL_OK? indicates that the corresponding TE and question focus are related.
Figure 2: Example of Question Analysis Process in K-QARD
component is shown in Figure 2-(1).
3.2 Question focus recognition
We define a question focus as a type of informa-
tion that a user wants to know. For example, in
the question  What movies will be shown on TV
tonight?, the question focus is a program title, or
titles. For another example, the question focus is
a current rainfall in a question  San Francisco is
raining now, is it raining in Los Angeles too?.
To find the question focus, we define question
focus region, a part of a question that may contain
clues for deciding the question focus. The ques-
tion focus region is identified with a set of simple
rules which consider the characteristic of the Ko-
rean interrogatives. Generally, the question focus
region has a fixed pattern that is typically used in
interrogative questions(Akiba et al, 2002). Thus
a small number of simple rules is enough to cover
the most of question focus region pattern. Figure
2-(2) shows the part recognized as a question fo-
cus region in the sample question.
After recognizing the region, the actual focus of
the question is determined with features extracted
from the question focus region. For the detection,
we build the question focus classifier using deci-
sion tree (C4.5) and several linguistic or domain-
specific features such as the kind of the interroga-
tive and the concept name of the predicate.
Dividing the focus recognition process into two
parts helps to increase domain portability. While
the second part of deciding the actual question fo-
cus is domain-dependent because every domain-
application has its own set of question foci, the
first part that recognizes the question focus region
is domain-independent.
3.3 TE recognition
In the TE identification phase, pre-defined words,
phrases, and named entities are identified as slot-
filler candidates for appropriate slots, according to
TE tagging rules. For instance, movie and NBC
are tagged as Genre and Channel in the sample
question  Tell me the movie on NBC tonight. (i.e.
movie will be used to fill Genre slot and NBC
will be used to fill Channel slot in a semantic
frame). The hand-crafted TE tagging rules basi-
cally consider the surface form and the concept
name (derived from domain ontologies) of a target
word. The context surrounding the target word or
word dependency information is also considered
in some cases. In the example question of Figure
2, the date expression ?  (today)?, time expres-
sion ? (night)? and the channel name ?MBC?
are selected as TE candidates.
In K-QARD, such identification is accom-
plished by a set of simple rules, which only ex-
amines the semantic type of each constituent word
in the question, except the words in the question
region. It is mainly because of our divide-and-
conquer strategy motivated by IE. The result of
this component may include some wrong template
elements, which do not have any relation to the
user?s intention or the question focus. However,
they are expected to be removed in the next com-
ponent, the TR recognizer which examines the re-
lation between the recognized TE and the question
focus.
31
(1) Broadcast-domain QA system
(2) Answer for sample question, 
?What soap opera will be played on MBC tonight??
Figure 3: Broadcast-domain QA System using K-QARD
3.4 TR recognition
In the TR recognition phase, all entities identified
in the TE recognition phase are examined whether
they have any relationships with the question fo-
cus region of the question. For example, in the
question  Is it raining in Los Angeles like in San
Francisco?, both Los Angeles and San Francisco
are identified as a TE. However, by the TR recog-
nition, only Los Angeles is identified as a related
entity with the question focus region.
Selectional restriction and dependency relations
between TEs are mainly considered in TR tagging
rules. Thus, the TR rules can be quite simplified.
For example, many relations between the TEs and
the question region can be simply identified by ex-
amining whether there is a syntactic dependency
between them as shown in Figure 2-(4). Moreover,
to make up for the errors in dependency parsing,
lexico-semantic patterns are also encoded in the
TR tagging rules.
4 Application of K-QARD
To evaluate the K-QARD framework, we built re-
stricted domain question answering systems for
the several domains: weather, broadcast, and traf-
fic. For the adaptation of QA system to each do-
main, we rewrote the domain ontology consisting
of about 150 concepts, about 30 TE/TR rules, and
7-23 semantic frames and answer templates. In
addition, we learned the question focus classifier
from training examples of about 100 questions for
the each domain. All information for the ques-
tion answering was automatically extracted using
the Web IE module of K-QARD, which was also
learned from training examples consisting of sev-
eral annotated Web pages of the target Web site. It
took about a half of week for two graduate stu-
dents who clearly understood the framework to
build each QA system. Figure 3 shows an example
of QA system applied to the broadcast domain.
5 Conclusion
In this paper, we described the Korean question
answering framework, namely K-QARD, for re-
stricted domains. Specifically, this framework is
designed to enhance the robustness and domain
portability. To achieve this goal, we use the IE-
based question analyzer using the generalized in-
formation acquired by several NLP components.
We also showed the usability of K-QARD by suc-
cessfully applying the framework to several do-
mains.
References
T. Akiba, K. Itou, A. Fujii, and T Ishikawa. 2002.
Towards speech-driven question answering: Exper-
iments using the NTCIR-3 question answering col-
lection. In Proceedings of the Third NTCIR Work-
shop.
H. Chung and H. Rim. 2004. Unlexicalized de-
pendency parser for variable word order languages
based on local contextual pattern. Lecture Note in
Computer Science, (2945):112?123.
J. Lee, Y. Song, S. Kim, H. Chung, and H. Rim. 2004.
Title recognition using lexical pattern and entity dic-
tionary. In Proceedings of the 1st Asia Information
Retrieval Symposium (AIRS2004), pages 345?348.
P. Martin, F. Crabbe, S. Adams, E. Baatz, and
N. Yankelovich. 1996. Speechacts: a spoken lan-
guage framework. IEEE Computer, 7(29):33?40.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
32
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1048?1056,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Word or Phrase?
Learning Which Unit to Stress for Information Retrieval?
Young-In Song? and Jung-Tae Lee? and Hae-Chang Rim?
?Microsoft Research Asia, Beijing, China
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
yosong@microsoft.com?, {jtlee,rim}@nlp.korea.ac.kr?
Abstract
The use of phrases in retrieval models has
been proven to be helpful in the literature,
but no particular research addresses the
problem of discriminating phrases that are
likely to degrade the retrieval performance
from the ones that do not. In this paper, we
present a retrieval framework that utilizes
both words and phrases flexibly, followed
by a general learning-to-rank method for
learning the potential contribution of a
phrase in retrieval. We also present use-
ful features that reflect the compositional-
ity and discriminative power of a phrase
and its constituent words for optimizing
the weights of phrase use in phrase-based
retrieval models. Experimental results on
the TREC collections show that our pro-
posed method is effective.
1 Introduction
Various researches have improved the quality
of information retrieval by relaxing the tradi-
tional ?bag-of-words? assumption with the use of
phrases. (Miller et al, 1999; Song and Croft,
1999) explore the use n-grams in retrieval mod-
els. (Fagan, 1987; Gao et al, 2004; Met-
zler and Croft, 2005; Tao and Zhai, 2007) use
statistically-captured term dependencies within a
query. (Strzalkowski et al, 1994; Kraaij and
Pohlmann, 1998; Arampatzis et al, 2000) study
the utility of various kinds of syntactic phrases.
Although use of phrases clearly helps, there still
exists a fundamental but unsolved question: Do all
phrases contribute an equal amount of increase in
the performance of information retrieval models?
Let us consider a search query ?World Bank Crit-
icism?, which has the following phrases: ?world
?This work was done while Young-In Song was with the
Dept. of Computer & Radio Communications Engineering,
Korea University.
bank? and ?bank criticism?. Intuitively, the for-
mer should be given more importance than its con-
stituents ?world? and ?bank?, since the meaning
of the original phrase cannot be predicted from
the meaning of either constituent. In contrast, a
relatively less attention could be paid to the lat-
ter ?bank criticism?, because there may be alter-
nate expressions, of which the meaning is still pre-
served, that could possibly occur in relevant docu-
ments. However, virtually all the researches ig-
nore the relation between a phrase and its con-
stituent words when combining both words and
phrases in a retrieval model.
Our approach to phrase-based retrieval is moti-
vated from the following linguistic intuitions: a)
phrases have relatively different degrees of signif-
icance, and b) the influence of a phrase should be
differentiated based on the phrase?s constituents in
retrieval models. In this paper, we start out by
presenting a simple language modeling-based re-
trieval model that utilizes both words and phrases
in ranking with use of parameters that differenti-
ate the relative contributions of phrases and words.
Moreover, we propose a general learning-to-rank
based framework to optimize the parameters of
phrases against their constituent words for re-
trieval models that utilize both words and phrases.
In order to estimate such parameters, we adapt the
use of a cost function together with a gradient de-
scent method that has been proven to be effective
for optimizing information retrieval models with
multiple parameters (Taylor et al, 2006; Metzler,
2007). We also propose a number of potentially
useful features that reflect not only the characteris-
tics of a phrase but also the information of its con-
stituent words for minimizing the cost function.
Our experimental results demonstrate that 1) dif-
ferentiating the weights of each phrase over words
yields statistically significant improvement in re-
trieval performance, 2) the gradient descent-based
parameter optimization is reasonably appropriate
1048
to our task, and 3) the proposed features can dis-
tinguish good phrases that make contributions to
the retrieval performance.
The rest of this paper is organized as follows.
The next section discusses previous work. Section
3 presents our learning-based retrieval framework
and features. Section 4 reports the evaluations of
our techniques. Section 5 finally concludes the pa-
per and discusses future work.
2 Previous Work
To date, there have been numerous researches to
utilize phrases in retrieval models. One of the
most earliest work on phrase-based retrieval was
done by (Fagan, 1987). In (Fagan, 1987), the ef-
fectiveness of proximity-based phrases (i.e. words
occurring within a certain distance) in retrieval
was investigated with varying criteria to extract
phrases from text. Subsequently, various types
of phrases, such as sequential n-grams (Mitra et
al., 1997), head-modifier pairs extracted from syn-
tactic structures (Lewis and Croft, 1990; Zhai,
1997; Dillon and Gray, 1983; Strzalkowski et al,
1994), proximity-based phrases (Turpin and Mof-
fat, 1999), were examined with conventional re-
trieval models (e.g. vector space model). The ben-
efit of using phrases for improving the retrieval
performance over simple ?bag-of-words? models
was far less than expected; the overall perfor-
mance improvement was only marginal and some-
times even inconsistent, specifically when a rea-
sonably good weighting scheme was used (Mitra
et al, 1997). Many researchers argued that this
was due to the use of improper retrieval models
in the experiments. In many cases, the early re-
searches on phrase-based retrieval have only fo-
cused on extracting phrases, not concerning about
how to devise a retrieval model that effectively
considers both words and phrases in ranking. For
example, the direct use of traditional vector space
model combining a phrase weight and a word
weight virtually yields the result assuming inde-
pendence between a phrase and its constituent
words (Srikanth and Srihari, 2003).
In order to complement the weakness, a number
of research efforts were devoted to the modeling
of dependencies between words directly within re-
trieval models instead of using phrases over the
years (van Rijsbergen, 1977; Wong et al, 1985;
Croft et al, 1991; Losee, 1994). Most stud-
ies were conducted on the probabilistic retrieval
framework, such as the BIM model, and aimed on
producing a better retrieval model by relaxing the
word independence assumption based on the co-
occurrence information of words in text. Although
those approaches theoretically explain the relation
between words and phrases in the retrieval con-
text, they also showed little or no improvements
in retrieval effectiveness, mainly because of their
statistical nature. While a phrase-based approach
selectively incorporated potentially-useful relation
between words, the probabilistic approaches force
to estimate parameters for all possible combina-
tions of words in text. This not only brings
parameter estimation problems but causes a re-
trieval system to fail by considering semantically-
meaningless dependency of words in matching.
Recently, a number of retrieval approaches have
been attempted to utilize a phrase in retrieval mod-
els. These approaches have focused to model sta-
tistical or syntactic phrasal relations under the lan-
guage modeling method for information retrieval.
(Srikanth and Srihari, 2003; Maisonnasse et al,
2005) examined the effectiveness of syntactic re-
lations in a query by using language modeling
framework. (Song and Croft, 1999; Miller et al,
1999; Gao et al, 2004; Metzler and Croft, 2005)
investigated the effectiveness of language model-
ing approach in modeling statistical phrases such
as n-grams or proximity-based phrases. Some of
them showed promising results in their experi-
ments by taking advantages of phrases soundly in
a retrieval model.
Although such approaches have made clear dis-
tinctions by integrating phrases and their con-
stituents effectively in retrieval models, they did
not concern the different contributions of phrases
over their constituents in retrieval performances.
Usually a phrase score (or probability) is simply
combined with scores of its constituent words by
using a uniform interpolation parameter, which
implies that a uniform contribution of phrases
over constituent words is assumed. Our study is
clearly distinguished from previous phrase-based
approaches; we differentiate the influence of each
phrase according to its constituent words, instead
of allowing equal influence for all phrases.
3 Proposed Method
In this section, we present a phrase-based retrieval
framework that utilizes both words and phrases ef-
fectively in ranking.
1049
3.1 Basic Phrase-based Retrieval Model
We start out by presenting a simple phrase-based
language modeling retrieval model that assumes
uniform contribution of words and phrases. For-
mally, the model ranks a document D according to
the probability of D generating phrases in a given
query Q, assuming that the phrases occur indepen-
dently:
s(Q;D) = P (Q|D) ?
|Q|?
i=1
P (qi|qhi , D) (1)
where qi is the ith query word, qhi is the head word
of qi, and |Q| is the query size. To simplify the
mathematical derivations, we modify Eq. 1 using
logarithm as follows:
s(Q;D) ?
|Q|?
i=1
log[P (qi|qhi , D)] (2)
In practice, the phrase probability is mixed with
the word probability (i.e. deleted interpolation) as:
P (qi|qhi ,D)??P (qi|qhi ,D)+(1??)P (qi|D) (3)
where ? is a parameter that controls the impact of
the phrase probability against the word probability
in the retrieval model.
3.2 Adding Multiple Parameters
Given a phrase-based retrieval model that uti-
lizes both words and phrases, one would definitely
raise a fundamental question on how much weight
should be given to the phrase information com-
pared to the word information. In this paper, we
propose to differentiate the value of ? in Eq. 3
according to the importance of each phrase by
adding multiple free parameters to the retrieval
model. Specifically, we replace ? with well-
known logistic function, which allows both nu-
merical and categorical variables as input, whereas
the output is bounded to values between 0 and 1.
Formally, the input of a logistic function is a
set of evidences (i.e. feature vector) X generated
from a given phrase and its constituents, whereas
the output is the probability predicted by fitting X
to a logistic curve. Therefore, ? is replaced as fol-
lows:
?(X) = 11 + e?f(X) ? ? (4)
where ? is a scaling factor to confine the output to
values between 0 and ?.
f(X) = ?0 +
|X|?
i=1
?ixi (5)
where xi is the ith feature, ?i is the coefficient pa-
rameter of xi, and ?0 is the ?intercept?, which is
the value of f(X) when all feature values are zero.
3.3 RankNet-based Parameter Optimization
The ? parameters in Eq. 5 are the ones we wish
to learn for resulting retrieval performance via pa-
rameter optimization methods. In many cases, pa-
rameters in a retrieval model are empirically de-
termined through a series of experiments or auto-
matically tuned via machine learning to maximize
a retrieval metric of choice (e.g. mean average
precision). The most simple but guaranteed way
would be to directly perform brute force search
for the global optimum over the entire parame-
ter space. However, not only the computational
cost of this so-called direct search would become
undoubtfully expensive as the number of parame-
ters increase, but most retrieval metrics are non-
smooth with respect to model parameters (Met-
zler, 2007). For these reasons, we propose to adapt
a learning-to-rank framework that optimizes mul-
tiple parameters of phrase-based retrieval models
effectively with less computation cost and without
any specific retrieval metric.
Specifically, we use a gradient descent method
with the RankNet cost function (Burges et al,
2005) to perform effective parameter optimiza-
tions, as in (Taylor et al, 2006; Metzler, 2007).
The basic idea is to find a local minimum of a cost
function defined over pairwise document prefer-
ence. Assume that, given a query Q, there is
a set of document pairs RQ based on relevance
judgements, such that (D1, D2) ? RQ implies
document D1 should be ranked higher than D2.
Given a defined set of pairwise preferences R, the
RankNet cost function is computed as:
C(Q,R) =
?
?Q?Q
?
?(D1,D2)?RQ
log(1 + eY ) (6)
whereQ is the set of queries, and Y = s(Q;D2)?
s(Q;D1) using the current parameter setting.
In order to minimize the cost function, we com-
pute gradients of Eq. 6 with respect to each pa-
rameter ?i by applying the chain rule:
?C
??i =
?
?Q?Q
?
?(D1,D2)?RQ
?C
?Y
?Y
??i (7)
where ?C?Y and ?Y??i are computed as:
?C
?Y =
exp[s(Q;D2)? s(Q;D1)]
1 + exp[s(Q;D2)? s(Q;D1)] (8)
1050
?Y
??i =
?s(Q;D2)
??i ?
?s(Q;D1)
??i (9)
With the retrieval model in Eq. 2 and ?(X),
f(X) in Eq. 4 and 5, the partial derivate of
s(Q;D) with respect to ?i is computed as follows:
?s(Q;D)
??i
=
|Q|?
i=1
xi?(X)(1? ?(X)? )?(P (qi|qhi,D)?P (qi|D))
?(X)P (qi|qhi , D) + (1? ?(X))P (qi|D)
(10)
3.4 Features
We experimented with various features that are
potentially useful for not only discriminating a
phrase itself but characterizing its constituents. In
this section, we report only the ones that have
made positive contributions to the overall retrieval
performance. The two main criteria considered
in the selection of the features are the followings:
compositionality and discriminative power.
Compositionality Features
Features on phrase compositionality are designed
to measure how likely a phrase can be represented
as its constituent words without forming a phrase;
if a phrase in a query has very high composition-
ality, there is a high probability that its relevant
documents do not contain the phrase. In this case,
emphasizing the phrase unit could be very risky in
retrieval. In the opposite case that a phrase is un-
compositional, it is obvious that occurrence of a
phrase in a document can be a stronger evidence
of relevance than its constituent words.
Compositionality of a phrase can be roughly
measured by using corpus statistics or its linguis-
tic characteristics; we have observed that, in many
times, an extremely-uncompositional phrase ap-
pears as a noun phrase, and the distance between
its constituent words is generally fixed within a
short distance. In addition, it has a tendency to be
used repeatedly in a document because its seman-
tics cannot be represented with individual con-
stituent words. Based on these intuitions, we de-
vise the following features:
Ratio of multiple occurrences (RMO): This is a
real-valued feature that measures the ratio of the
phrase repeatedly used in a document. The value
of this feature is calculated as follows:
x =
?
?D;count(wi?whi ,D)>1
count(wi?whi , D)
count(wi ? whi , C) + ?
(11)
where wi ? whi is a phrase in a given query,
count(x, y) is the count of x in y, and ? is a small-
valued constant to prevent unreliable estimation
by very rarely-occurred phrases.
Ratio of single-occurrences (RSO): This is a bi-
nary feature that indicates whether or not a phrase
occurs once in most documents containing it. This
can be regarded as a supplementary feature of
RMO.
Preferred phrasal type (PPT): This feature indi-
cates the phrasal type that the phrase prefers in a
collection. We consider only two cases (whether
the phrase prefers verb phrase or adjective-noun
phrase types) as features in the experiments1.
Preferred distance (PD): This is a binary feature
indicating whether or not the phrase prefers long
distance (> 1) between constituents in the docu-
ment collection.
Uncertainty of preferred distance (UPD): We also
use the entropy (H) of the modification distance
(d) of the given phrase in the collection to measure
the compositionality; if the distance is not fixed
and is highly uncertain, the phrase may be very
compositional. The entropy is computed as:
x = H(p(d = x|wi ? whi)) (12)
where d ? 1, 2, 3, long and all probabilities are
estimated with discount smoothing. We simply
use two binary features regarding the uncertainty
of distance; one indicates whether the uncertainty
of a phrase is very high (> 0.85), and the other
indicates whether the uncertainty is very low (<
0.05)2.
Uncertainty of preferred phrasal type (UPPT): As
similar to the uncertainty of preferred distance, the
uncertainty of the preferred phrasal type of the
phrase can be also used as a feature. We consider
this factor as a form of a binary feature indicating
whether the uncertainty is very high or not.
Discriminative Power Features
In some cases, the occurrence of a phrase can be a
valuable evidence even if the phrase is very likely
to be compositional. For example, it is well known
that the use of a phrase can be effective in retrieval
when its constituent words appear very frequently
in the collection, because each word would have a
very low discriminative power for relevance. On
the contrary, if a constituent word occurs very
1For other phrasal types, significant differences were not
observed in the experiments.
2Although it may be more natural to use a real-valued fea-
ture, we use these binary features because of the two practical
reasons; firstly, it could be very difficult to find an adequate
transformation function with real values, and secondly, the
two intervals at tails were observed to be more important than
the rest.
1051
rarely in the collection, it could not be effective
to use the phrase even if the phrase is highly un-
compositional. Similarly, if the probability that a
phrase occurs in a document where its constituent
words co-occur is very high, we might not need to
place more emphasis on the phrase than on words,
because co-occurrence information naturally in-
corporated in retrieval models may have enough
power to distinguish relevant documents. Based
on these intuitions, we define the following fea-
tures:
Document frequency of constituents (DF): We
use the document frequency of a constituent as
two binary features: one indicating whether the
word has very high document frequency (>10%
of documents in a collection) and the other one
indicating whether it has very low document fre-
quency (<0.2% of documents, which is approxi-
mately 1,000 in our experiments).
Probability of constituents as phrase (CPP): This
feature is computed as a relative frequency of doc-
uments containing a phrase over documents where
two constituent words appear together.
One interesting fact that we observe is that doc-
ument frequency of the modifier is generally a
stronger evidence on the utility of a phrase in re-
trieval than of the headword. In the case of the
headword, we could not find an evidence that it
has to be considered in phrase weighting. It seems
to be a natural conclusion, because the importance
of the modifier word in retrieval is subordinate to
the relation to its headword, but the headword is
not in many phrases. For example, in the case of
the query ?tropical storms?, retrieving a document
only containing tropical can be meaningless, but a
document about storm can be meaningful. Based
on this observation, we only incorporate document
frequency features of syntactic modifiers in the ex-
periments.
4 Experiments
In this section, we report the retrieval perfor-
mances of the proposed method with appropriate
baselines over a range of training sets.
4.1 Experimental Setup
Retrieval models: We have set two retrieval mod-
els, namely the word model and the (phrase-based)
one-parameter model, as baselines. The ranking
function of the word model is equivalent to Eq. 2,
with ? in Eq. 3 being set to zero (i.e. the phrase
probability makes no effect on the ranking). The
ranking function of the one-parameter model is
also equivalent to Eq. 2, with ? in Eq. 3 used ?as
is? (i.e. as a constant parameter value optimized
using gradient descent method, without being re-
placed to a logistic function). Both baseline mod-
els cannot differentiate the importance of phrases
in a query. To make a distinction from the base-
line models, we will name our proposed method
as a multi-parameter model.
In our experiments, all the probabilities in all
retrieval models are smoothed with the collection
statistics by using dirichlet priors (Zhai and Laf-
ferty, 2001).
Corpus (Training/Test): We have conducted
large-scale experiments on three sets of TREC?s
Ad Hoc Test Collections, namely TREC-6, TREC-
7, and TREC-8. Three query sets, TREC-6 top-
ics 301-350, TREC-7 topics 351-400, and TREC-
8 topics 401-450, along with their relevance judg-
ments have been used. We only used the title field
as query.
When performing experiments on each query
set with the one-parameter and the multi-
parameter models, the other two query sets have
been used for learning the optimal parameters. For
each query in the training set, we have generated
document pairs for training by the following strat-
egy: first, we have gathered top m ranked doc-
uments from retrieval results by using the word
model and the one-parameter model (by manually
setting ? in Eq. 3 to the fixed constants, 0 and 0.1
respectively). Then, we have sampled at most r
relevant documents and n non-relevant documents
from each one and generated document pairs from
them. In our experiments, m, r, and n is set to
100, 10, and 40, respectively.
Phrase extraction and indexing: We evaluate
our proposed method on two different types of
phrases: syntactic head-modifier pairs (syntac-
tic phrases) and simple bigram phrases (statisti-
cal phrases). To index the syntactic phrases, we
use the method proposed in (Strzalkowski et al,
1994) with Connexor FDG parser3, the syntactic
parser based on the functional dependency gram-
mar (Tapanainen and Jarvinen, 1997). All neces-
sary information for feature values were indexed
together for both syntactic and statistical phrases.
To maintain indexes in a manageable size, phrases
3Connexor FDG parser is a commercial parser; the demo
is available at: http://www.connexor.com/demo
1052
Test set ? Training set
6 ? 7+8 7 ? 6+8 8 ? 6+7
Model Metric \ Query all partial all partial all partial
Word MAP 0.2135 0.1433 0.1883 0.1876 0.2380 0.2576
(Baseline 1) R-Prec 0.2575 0.1894 0.2351 0.2319 0.2828 0.2990
P@10 0.3660 0.3333 0.4100 0.4324 0.4520 0.4517
One-parameter MAP 0.2254 0.1633? 0.1988 0.2031 0.2352 0.2528
(Baseline 2) R-Prec 0.2738 0.2165 0.2503 0.2543 0.2833 0.2998
P@10 0.3820 0.3600 0.4540 0.4971 0.4580 0.4621
Multi-parameter MAP 0.2293? 0.1697? 0.2038? 0.2105? 0.2452 0.2701
(Proposed) R-Prec 0.2773 0.2225 0.2534 0.2589 0.2891 0.3099
P@10 0.4020 0.3933 0.4540 0.4971 0.4700 0.4828
Table 1: Retrieval performance of different models on syntactic phrases. Italicized MAP values with
symbols ? and ? indicate statistically significant improvements over the word model according to Stu-
dent?s t-test at p < 0.05 level and p < 0.01 level, respectively. Bold figures indicate the best performed
case for each metric.
that occurred less than 10 times in the document
collections were not indexed.
4.2 Experimental Results
Table 1 shows the experimental results of the three
retrieval models on the syntactic phrase (head-
modifier pair). In the table, partial denotes the
performance evaluated on queries containing more
than one phrase that appeared in the document col-
lection4; this shows the actual performance differ-
ence between models. Note that the ranking re-
sults of all retrieval models would be the same as
the result of the word model if a query does not
contain any phrases in the document collection,
because P (qi|qhi , D) would be calculated as zero
eventually. As evaluation measures, we used the
mean average precision (MAP), R-precision (R-
Prec), and precisions at top 10 ranks (P@10).
As shown in Table 1, when a syntactic phrase is
used for retrieval, one-parameter model trained by
gradient-descent method generally performs bet-
ter than the word model, but the benefits are in-
consistent; it achieves approximately 15% and 8%
improvements on the partial query set of TREC-
6 and 7 over the word model, but it fails to show
any improvement on TREC-8 queries. This may
be a natural result since the one-parameter model
is very sensitive to the averaged contribution of
phrases used for training. Compared to the queries
in TREC-6 and 7, the TREC-8 queries contain
more phrases that are not effective for retrieval
4The number of queries containing a phrase in TREC-6,
7, and 8 query set is 31, 34, and 29, respectively.
(i.e. ones that hurt the retrieval performance when
used). This indicates that without distinguishing
effective phrases from ineffective phrases for re-
trieval, the model trained from one training set for
phrase would not work consistently on other un-
seen query sets.
Note that the proposed model outperforms all
the baselines over all query sets; this shows that
differentiating relative contributions of phrases
can improve the retrieval performance of the one-
parameter model considerably and consistently.
As shown in the table, the multi-parameter model
improves by approximately 18% and 12% on the
TREC-6 and 7 partial query sets, and it also
significantly outperforms both the word model
and the one-parameter model on the TREC-8
query set. Specifically, the improvement on the
TREC-8 query set shows one advantage of using
our proposed method; by separating potentially-
ineffective phrases and effective phrases based on
the features, it not only improves the retrieval
performance for each query but makes parameter
learning less sensitive to the training set.
Figure 1 shows some examples demonstrating
the different behaviors of the one-parameter model
and the multi-parameters model. On the figure, the
un-dotted lines indicate the variation of average
precision scores when ? value in Eq. 3 is manu-
ally set. As ? gets closer to 0, the ranking formula
becomes equivalent to the word model.
As shown in the figure, the optimal point of ? is
quiet different from query to query. For example,
in cases of the query ?ferry sinking? and industrial
1053
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?ferry sinking?
varing lambdaone-parametermultiple-parameter
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?industrial espionage?
varing lambdaone-parametermultiple-parameter
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
A
v
g
P
r
lambda
Performance variation for the query ? declining birth rates?
varing lambdaone-parametermultiple-parameter 0.2
0.25
0.3
0.35
0.4
0.45
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?amazon rain forest?
varing lambdaone-parametermultiple-parameter
Figure 1: Performance variations for the queries ?ferry sinking?, ?industrial espionage?, ?declining birth
rate? and ?Amazon rain forest? according to ? in Eq. 3.
espionage? on the upper side, the optimal point is
the value close to 0 and 1 respectively. This means
that the occurrences of the phrase ?ferry sinking?
in a document is better to be less-weighted in
retrieval while ?industrial espionage? should be
treated as a much more important evidence than its
constituent words. Obviously, such differences are
not good for one-parameter model assuming rela-
tive contributions of phrases uniformly. For both
opposite cases, the multi-parameter model signifi-
cantly outperforms one-parameter model.
The two examples at the bottom of Figure 1
show the difficulty of optimizing phrase-based re-
trieval using one uniform parameter. For example,
the query ?declining birth rate? contains two dif-
ferent phrases, ?declining rate? and ?birth rate?,
which have potentially-different effectiveness in
retrieval; the phrase ?declining rate? would not
be helpful for retrieval because it is highly com-
positional, but the phrase ?birth rate? could be a
very strong evidence for relevance since it is con-
ventionally used as a phrase. In this case, we
can get only small benefit from the one-parameter
model even if we find optimal ? from gradient
descent, because it will be just a compromised
value between two different, optimized ?s. For
such query, the multi-parameter model could be
more effective than the one-parameter model by
enabling to set different ?s on phrases accord-
ing to their predicted contributions. Note that the
multi-parameter model significantly outperforms
the one-parameter model and all manually-set ?s
for the queries ?declining birth rate? and ?Amazon
rain forest?, which also has one effective phrase,
?rain forest?, and one non-effective phrase, ?Ama-
zon forest?.
Since our method is not limited to a particular
type of phrases, we have also conducted experi-
ments on statistical phrases (bigrams) with a re-
duced set of features directed applicable; RMO,
RSO, PD5, DF, and CPP; the features requiring
linguistic preprocessing (e.g. PPT) are not used,
because it is unrealistic to use them under bigram-
based retrieval setting. Moreover, the feature UPD
is not used in the experiments because the uncer-
5In most cases, the distance between words in a bigram
is 1, but sometimes, it could be more than 1 because of the
effect of stopword removal.
1054
Test ? Training
Model Metric 6 ? 7+8 7 ? 6+8 8 ? 6+7
Word MAP 0.2135 0.1883 0.2380
(Baseline 1) R-Prec 0.2575 0.2351 0.2828
P@10 0.3660 0.4100 0.4520
One-parameter MAP 0.2229 0.1979 0.2492?
(Baseline 2) R-Prec 0.2716 0.2456 0.2959
P@10 0.3720 0.4500 0.4620
Multi-parameter MAP 0.2224 0.2025? 0.2499?
(Proposed) R-Prec 0.2707 0.2457 0.2952
P@10 0.3780 0.4520 0.4600
Table 2: Retrieval performance of different models, using statistical phrases.
tainty of preferred distance does not vary much for
bigram phrases. The results are shown in Table 2.
The results of experiments using statistical
phrases show that multi-parameter model yields
additional performance improvement against
baselines in many cases, but the benefit is in-
significant and inconsistent. As shown in Table 2,
according to the MAP score, the multi-parameter
model outperforms the one-parameter model on
the TREC-7 and 8 query sets, but it performs
slightly worse on the TREC-6 query set.
We suspect that this is because of the lack
of features to distinguish an effective statistical
phrases from ineffective statistical phrase. In our
observation, the bigram phrases also show a very
similar behavior in retrieval; some of them are
very effective while others can deteriorate the per-
formance of retrieval models. However, in case
of using statistical phrases, the ? computed by our
multi-parameter model would be often similar to
the one computed by the one-parameter model,
when there is no sufficient evidence to differen-
tiate a phrase. Moreover, the insufficient amount
of features may have caused the multi-parameter
model to overfit to the training set easily.
The small size of training corpus could be an an-
other reason. The number of queries we used for
training is less than 80 when removing a query not
containing a phrase, which is definitely not a suf-
ficient amount to learn optimal parameters. How-
ever, if we recall that the multi-parameter model
worked reasonably in the experiments using syn-
tactic phrases with the same training sets, the lack
of features would be a more important reason.
Although we have not mainly focused on fea-
tures in this paper, it would be strongly necessary
to find other useful features, not only for statistical
phrases, but also for syntactic phrases. For exam-
ple, statistics from query logs and the probability
of snippet containing a same phrase in a query is
clicked by user could be considered as useful fea-
tures. Also, the size of the training data (queries)
and the document collection may not be sufficient
enough to conclude the effectiveness of our pro-
posed method; our method should be examined in
a larger collection with more queries. Those will
be one of our future works.
5 Conclusion
In this paper, we present a novel method to differ-
entiate impacts of phrases in retrieval according
to their relative contribution over the constituent
words. The contributions of this paper can be sum-
marized in three-fold: a) we proposed a general
framework to learn the potential contribution of
phrases in retrieval by ?parameterizing? the fac-
tor interpolating the phrase weight and the word
weight on features and optimizing the parameters
using RankNet-based gradient descent algorithm,
b) we devised a set of potentially useful features
to distinguish effective and non-effective phrases,
and c) we showed that the proposed method can be
effective in terms of retrieval by conducting a se-
ries of experiments on the TREC test collections.
As mentioned earlier, the finding of additional
features, specifically for statistical phrases, would
be necessary. Moreover, for a thorough analysis
on the effect of our framework, additional experi-
ments on larger and more realistic collections (e.g.
the Web environment) would be required. These
will be our future work.
1055
References
Avi Arampatzis, Theo P. van der Weide, Cornelis H. A.
Koster, and P. van Bommel. 2000. Linguistically-
motivated information retrieval. In Encyclopedia of
Library and Information Science.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of ICML ?05, pages 89?96.
W. Bruce Croft, Howard R. Turtle, and David D. Lewis.
1991. The use of phrases and structured queries in
information retrieval. In Proceedings of SIGIR ?91,
pages 32?45.
Martin Dillon and Ann S. Gray. 1983. Fasit: A
fully automatic syntactically based indexing system.
Journal of the American Society for Information Sci-
ence, 34(2):99?108.
Joel L. Fagan. 1987. Automatic phrase indexing for
document retrieval. In Proceedings of SIGIR ?87,
pages 91?101.
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Gui-
hong Cao. 2004. Dependence language model for
information retrieval. In Proceedings of SIGIR ?04,
pages 170?177.
Wessel Kraaij and Rene?e Pohlmann. 1998. Comparing
the effect of syntactic vs. statistical phrase indexing
strategies for dutch. In Proceedings of ECDL ?98,
pages 605?617.
David D. Lewis and W. Bruce Croft. 1990. Term clus-
tering of syntactic phrases. In Proceedings of SIGIR
?90, pages 385?404.
Robert M. Losee, Jr. 1994. Term dependence: truncat-
ing the bahadur lazarsfeld expansion. Information
Processing and Management, 30(2):293?303.
Loic Maisonnasse, Gilles Serasset, and Jean-Pierre
Chevallet. 2005. Using syntactic dependency and
language model x-iota ir system for clips mono and
bilingual experiments in clef 2005. In Working
Notes for the CLEF 2005 Workshop.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of SIGIR ?05, pages 472?479.
Donald Metzler. 2007. Using gradient descent to opti-
mize language modeling smoothing parameters. In
Proceedings of SIGIR ?07, pages 687?688.
David R. H. Miller, Tim Leek, and Richard M.
Schwartz. 1999. A hidden markov model informa-
tion retrieval system. In Proceedings of SIGIR ?99,
pages 214?221.
Mandar Mitra, Chris Buckley, Amit Singhal, and Claire
Cardie. 1997. An analysis of statistical and syn-
tactic phrases. In Proceedings of RIAO ?97, pages
200?214.
Fei Song and W. Bruce Croft. 1999. A general lan-
guage model for information retrieval. In Proceed-
ings of CIKM ?99, pages 316?321.
Munirathnam Srikanth and Rohini Srihari. 2003. Ex-
ploiting syntactic structure of queries in a language
modeling approach to ir. In Proceedings of CIKM
?03, pages 476?483.
Tomek Strzalkowski, Jose Perez-Carballo, and Mihnea
Marinescu. 1994. Natural language information re-
trieval: Trec-3 report. In Proceedings of TREC-3,
pages 39?54.
Tao Tao and ChengXiang Zhai. 2007. An exploration
of proximity measures in information retrieval. In
Proceedings of SIGIR ?07, pages 295?302.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of
ANLP ?97, pages 64?71.
Michael Taylor, Hugo Zaragoza, Nick Craswell,
Stephen Robertson, and Chris Burges. 2006. Opti-
misation methods for ranking functions with multi-
ple parameters. In Proceedings of CIKM ?06, pages
585?593.
Andrew Turpin and Alistair Moffat. 1999. Statisti-
cal phrases for vector-space information retrieval. In
Proceedings of SIGIR ?99, pages 309?310.
C. J. van Rijsbergen. 1977. A theoretical basis for the
use of co-occurrence data in information retrieval.
Journal of Documentation, 33(2):106?119.
S. K. M. Wong, Wojciech Ziarko, and Patrick C. N.
Wong. 1985. Generalized vector spaces model in
information retrieval. In Proceedings of SIGIR ?85,
pages 18?25.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
SIGIR ?01, pages 334?342.
Chengxiang Zhai. 1997. Fast statistical parsing of
noun phrases for document indexing. In Proceed-
ings of ANLP ?97, pages 312?319.
1056
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 29?32,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Novel Word Segmentation Approach for
Written Languages with Word Boundary Markers
Han-Cheol Cho
?
, Do-Gil Lee
?
, Jung-Tae Lee
?
, Pontus Stenetorp
?
, Jun?ichi Tsujii
?
and Hae-Chang Rim
?
?
Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan
?
Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
{hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr
Abstract
Most NLP applications work under the as-
sumption that a user input is error-free;
thus, word segmentation (WS) for written
languages that use word boundary mark-
ers (WBMs), such as spaces, has been re-
garded as a trivial issue. However, noisy
real-world texts, such as blogs, e-mails,
and SMS, may contain spacing errors that
require correction before further process-
ing may take place. For the Korean lan-
guage, many researchers have adopted a
traditional WS approach, which eliminates
all spaces in the user input and re-inserts
proper word boundaries. Unfortunately,
such an approach often exacerbates the
word spacing quality for user input, which
has few or no spacing errors; such is the
case, because a perfect WS model does
not exist. In this paper, we propose a
novel WS method that takes into consider-
ation the initial word spacing information
of the user input. Our method generates
a better output than the original user in-
put, even if the user input has few spacing
errors. Moreover, the proposed method
significantly outperforms a state-of-the-art
Korean WS model when the user input ini-
tially contains less than 10% spacing er-
rors, and performs comparably for cases
containing more spacing errors. We be-
lieve that the proposed method will be a
very practical pre-processing module.
1 Introduction
Word segmentation (WS) has been a fundamen-
tal research issue for languages that do not have
word boundary markers (WBMs); on the con-
trary, other languages that do have WBMs have re-
garded the issue as a trivial task. Texts segmented
with such WBMs, however, could contain a hu-
man writer?s intentional or un-intentional spacing
errors; and even a few spacing errors can cause
error-propagation for further NLP stages.
For written languages that have WBMs, such as
for the Korean language, the majority of recent
research has been based on a traditional WS ap-
proach (Nakagawa, 2004). The first step of the
traditional approach is to eliminate all spaces in
the user input, and then re-locate the proper places
to insert WBMs. One state-of-the-art Korean WS
model (Lee et al, 2007) is known to achieve a per-
formance of 90.31% word-unit precision, which is
comparable with other WS models for the Chinese
or Japanese language.
Still, there is a downside to the evaluation
method. If the user input has a few or no spac-
ing errors, traditional WS models may cause more
spacing errors than it correct because they produce
the same output regardless the word spacing states
of the user input.
In this paper, we propose a new WS method that
takes into account the word spacing information
from the user input. Our proposed method first
generates the best word spacing states for the user
input by using a traditional WS model; however
the method does not immediately apply the out-
put. Secondly, the method estimates a threshold
based on the word spacing quality of the user in-
put. Finally, the method uses the new word spac-
ing states that have probabilities that are higher
than the threshold.
The most important contribution of the pro-
posed method is that, for most cases, the method
generates an output that is better than the user in-
put. The experimental results show that the pro-
posed method produces a better output than the
user input even if the user input has less than 1%
spacing errors in terms of the character-unit pre-
cision. Moreover, the proposed method outper-
forms (Lee et al, 2007) significantly, when the
29
user input initially contains less than 10% spacing
errors, and even performs comparably, when the
input contains more than 10% errors. Based on
these results, we believe that the proposed method
would be a very practical pre-processing module
for other NLP applications.
The paper is organized as follows: Section 2 ex-
plains the proposed method. Section 3 shows the
experimental results. Finally, the last section de-
scribes the contributions of the proposed method.
2 The Proposed Method
The proposed method consists of three steps: a
baseline WS model, confidence and threshold es-
timation, and output optimization. The following
sections will explain the steps in detail.
2.1 Baseline Word Segmentation Model
We use the tri-gram Hidden Markov Model
(HMM) of (Lee et al, 2007) as the baseline WS
model; however, we adopt the Maximum Like-
lihood (ML) decoding strategy to independently
find the best word spacing states. ML-decoding
allows us to directly compare each output to the
threshold. There is little discrepancy in accuracy
when using ML-decoding, as compared to Viterbi-
decoding, as mentioned in (Merialdo, 1994).
1
Let o
1,n
be a sequence of n-character user input
without WBMs, x
t
be the best word spacing state
for o
t
where 1 ? t ? n. Assume that x
t
is either 1
(space after o
t
) or 0 (no space after o
t
). Then each
best word spacing state x?
t
for all t can be found by
using Equation 1.
x?
t
= argmax
i?(0,1)
P (x
t
= i|o
1,n
) (1)
= argmax
i?(0,1)
P (o
1,n
, x
t
= i) (2)
= argmax
i?(0,1)
?
x
t?2
,x
t?1
P (x
t
= i|x
t?2
, o
t?1
, x
t?1
, o
t
)
?
?
x
t?1
P (o
t+1
|o
t?1
, x
t?1
, o
t
, x
t
= i)
?
?
x
t+1
P (o
t+2
|o
t
, x
t
= i, o
t+1
, x
t+1
) (3)
Equation 2 is derived by applying the Bayes?
rule and by eliminating the constant denominator.
Moreover, the equation is simplified, as is Equa-
tion 3, by using the Markov assumption, and by
1
In the preliminary experiment, Viterbi-decoding showed
a 0.5% higher word-unit precision.
eliminating the constant parts. Every part of Equa-
tion 3 can be calculated by adding the probabilities
of all possible combinations of x
t?2
, x
t?1
, x
t+1
and x
t+2
values.
The model is trained by using the relative fre-
quency information of the training data, and a
smoothing technique is applied to relieve the data-
sparseness problem which is the linear interpola-
tion of n-grams that are used in (Lee et al, 2007).
2.2 Confidence and Threshold Estimation
We set a variable threshold that is proportional to
the word spacing quality of the user input, Confi-
dence. Formally, we can define the threshold T as
a function of a confidence C, as in Equation 4.
T = f(C) (4)
Then, we define the confidence as is done in
Equation 5. Because calculating such a variable
is impossible, we estimate the value by substi-
tuting the word spacing states produced by the
baseline WS model, x
WS
1,n
, with the correct word
spacing states, x
correct
1,n
, as is done in Equation 6.
This estimation is based on the assumption that
the word spacing states of the WS model is suf-
ficiently similar to the correct word spacing states
in the character-unit precision.
2
C =
# of x
input
t
same to x
correct
t
# of x
input
t
(5)
?
# of x
input
t
same to x
WS
t
# of x
input
t
(6)
?
n
?
?
?
?
n
?
k=1
P (x
input
k
|o
1,n
) (7)
To handle the estimation error for short sen-
tences, we use the probability generating word
spacing states of the user input with the length nor-
malization as shown in Equation 7.
Figure 1 shows that the estimated confidence of
Equation 7 is almost linearly proportional to the
true confidence of Equation 5, thus suggesting that
the threshold T can be defined as a function of the
estimated confidence of Equation 7.
3
2
In the experiment with the development data, the base-
line WS model shows about 97% character-unit precision.
3
The development data is generated by randomly intro-
ducing spacing errors into correctly spaced sentences. We
think that this reflects various intentional and un-intentional
error patterns of individuals.
30
20%30%
40%50%
60%70%
80%90%
100%
100% 96% 92% 88% 84% 80%
Estim
ated C
onfid
ence
True Confidence
Figure 1: The relationship between estimated con-
fidence and true confidence
To keep the focus on the research subject of this
paper, we simply assume f(x) = x as in Equation
8, for the threshold function f .
T ? f(C) = C (8)
In the experimental results, we confirm that
even this simple threshold function can be help-
ful in improving the performance of the proposed
method against traditional WS models.
2.3 Output Optimization
After completing the two steps described in Sec-
tion 2.1 and 2.2, we have acquired the new spacing
states for the user input generated by the baseline
WS model, and the threshold measuring the word
spacing quality of the user input.
The proposed method only applies a part of the
new word spacing states to the user input, which
have probabilities that are higher than the thresh-
old; further the method discards the other new
word spacing states that have probabilities that are
lower than the threshold. By rejecting the unreli-
able output of the baseline WS model in this way,
the proposed method can effectively improve the
performance when the user input contains a rela-
tively small number of spacing errors.
3 Experimental Results
Two types of experiments have been performed.
In the first experiment, we investigate the level of
performance improvement based on different set-
tings of the user input?s word spacing error rate.
Because it is nearly impossible to obtain enough
test data for any error rate, we generate pseudo test
data in the same way that we generate develop-
ment data.
4
In the second experiment, we attempt
4
See Footnote 3.
figuring out whether the proposed method really
improves the word spacing quality of the user in-
put in a real-world setting.
3.1 Performance Improvement according to
the Word Spacing Error Rate of User
Input
For the first experiment, we use the Sejong corpus
5
from 1998-1999 (1,000,000 Korean sentences) for
the training data, and ETRI corpus (30,000 sen-
tences) for the test data (ETRI, 1999). To gener-
ate the test data that have spacing errors, we make
twenty one copies of the test data and randomly
insert spacing errors from 0% to 20% in the same
way in which we made the development data. We
feel that this strategy can model both the inten-
tional and un-intentional human error patterns.
In Figure 2, the x-axis indicates the word spac-
ing error rate of the user input in terms of the
character-unit precision, and the y-axis shows the
word-unit precision of the output. Each graph de-
picts the word-unit precision of the test corpus,
a state-of-the-art Korean WS model (Lee et al,
2007), the baseline WS model, and the proposed
method.
Although Lee?s model is known to perform
comparably with state-of-the-art Chinese and
Japanese WS models, it does not necessarily sug-
gest that the word spacing quality of the model?s
output is better than the user input. In Figure 2,
Lee?s model exacerbates the user input when it has
spacing errors that are lower than 3%.
The proposed method, however, produces a bet-
ter output, even if the user input has 1% spacing er-
rors. Moreover, the proposed method shows a con-
siderably better performance within the 10% spac-
ing error range, as compared to Lee?s model, al-
though the baseline WS model itself does not out-
performs Lee?s model. The performance improve-
ment in this error range is fairly significant be-
cause we found that the spacing error rate of texts
collected for the second experiment was about
9.1%.
3.2 Performance Comparison with Web Text
having Usual Error Rate
In the second experiment, we attempt finding out
whether the proposed method can be beneficial un-
der real-world circumstances. Web texts, which
consist of 1,000 erroneous sentences from famous
5
Details available at: http://www.sejong.or.kr/eindex.php
31
84%
86%
88%
90%
92%
94%
96%
98%
100%
0% 2% 4% 6% 8% 10% 12% 14% 16% 18% 20%
w
or
d-u
nit
 
 
pr
ec
isio
n
word spacing error rate of user input (in character-unit precision)
Test corpus Lee's model Baseline WS model Proposed method
Figure 2: Performance improvement according to the word spacing error rate of user input
Method Web Text
Test Corpus 70.89%
Lee?s Model 70.45%
Baseline WS Model 69.13%
Proposed Method 73.74%
Table 1: Performance comparison with Web text
Web portals and personal blogs, were collected
and used as the test data. Since the test data tend
to have a similar error rate to the narrow standard
deviation, we computed the overall performance
over the average word spacing error rate, which is
9.1%. The baseline WS model is trained on the
Sejong corpus, described in Section 3.1.
The test result is shown in Table 1. The
overall performance of Lee?s model, the baseline
WS model and the proposed method decreased
by roughly 18%. We hypothesize that the per-
formance degradation probably results from the
spelling errors of the test data, and the inconsis-
tencies that exist between the training data and the
test data. However, the proposed method still im-
proves the word spacing quality of the user input
by 3%, while the two traditional WS models de-
grades the quality. Such a result indicates that
the proposed method is effective for real-world
environments, as we had intended. Furthermore,
we also believe that the performance can be im-
proved if a proper training corpus is provided, or
if a spelling correction method is integrated.
4 Conclusion
In this paper, we proposed a new WS method that
uses the word spacing information of the user in-
put, for languages with WBMs. By utilizing the
user input, the proposed method effectively refines
the output of the baseline WS model and improves
the overall performance.
The most important contribution of this work is
that it produces an output that is better than the
user input even if it contains few spacing errors.
Therefore, the proposed method can be applied as
a pre-processing module for practical NLP appli-
cations without introducing a risk that would gen-
erate a worse output than the user input. Moreover,
the performance is notably better than a state-of-
the-art Korean WS model (Lee et al, 2007) within
the 10% spacing error range, which human writers
seldom exceed. It also performs comparably, even
if the user input contains more than 10% spacing
errors.
5 Acknowledgment
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Special Coordination Funds for Promoting
Science and Technology (MEXT, Japan).
References
ETRI. 1999. Pos-tag guidelines. Technical report.
Electronics and Telecomminications Research Insti-
tute.
Do-Gil Lee, Hae-Chang Rim, and Dongsuk Yook.
2007. Automatic Word Spacing Using Probabilistic
Models Based on Character n-grams. IEEE Intelli-
gent Systems, 22(1):28?35.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level
information. In COLING ?04, page 466, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
32
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 233?236,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Bridging Morpho-Syntactic Gap between Source and Target Sentences for
English-Korean Statistical Machine Translation
Gumwon Hong, Seung-Wook Lee and Hae-Chang Rim
Department of Computer Science & Engineering
Korea University
Seoul 136-713, Korea
{gwhong,swlee,rim}@nlp.korea.ac.kr
Abstract
Often, Statistical Machine Translation
(SMT) between English and Korean suf-
fers from null alignment. Previous studies
have attempted to resolve this problem by
removing unnecessary function words, or
by reordering source sentences. However,
the removal of function words can cause
a serious loss in information. In this pa-
per, we present a possible method of bridg-
ing the morpho-syntactic gap for English-
Korean SMT. In particular, the proposed
method tries to transform a source sen-
tence by inserting pseudo words, and by
reordering the sentence in such a way
that both sentences have a similar length
and word order. The proposed method
achieves 2.4 increase in BLEU score over
baseline phrase-based system.
1 Introduction
Phrase-based SMT models have performed rea-
sonably well on languages where the syntactic
structures are very similar, including languages
such as French and English. However, Collins et
al. (2005) demonstrated that phrase-based models
have limited potential when applied to languages
that have a relatively different word order; such is
the case between German and English. They pro-
posed a clause restructuring method for reordering
German sentences in order to resemble the order
of English sentences. By modifying the source
sentence structure into the target sentence struc-
ture, they argued that they could solve the de-
coding problem by use of completely monotonic
translation.
The translation from English to Korean can be
more difficult than the translation of other lan-
guage pairs for the following reasons: First, Ko-
rean is language isolate: that is, it has little ge-
nealogical relations with other natural languages.1
Second, the word order in Korean is relatively
free because the functional morphemes, case par-
ticles and word endings, play the role as a gram-
matical information marker. Thus, the functional
morphemes, rather than the word order, determine
whether a word is a subject or an object. Third,
Korean is an agglutinative language, in which a
word is generally composed of at least one con-
tent morpheme and zero or more functional mor-
phemes. Some Korean words are highly synthetic
with complex inflections, and this phenomenon
produces a very large vocabulary and causes data-
sparseness in performing word-based alignment.
To mitigate this problem, many systems tokenize
Korean sentences by the morpheme unit before
training and decoding the sentences.
When analyzing English-Korean translation
with MOSES (Koehn et al, 2007), we found
high ratio of null alignment. In figure 1,
???r(eun)?, ?_(eui)?, ?(ha)?, ?(n)?, ?t(ji)? and
??H(neunda)? are not linked to any word in the
English sentence. In many cases, these words are
function words that are attached to preceding con-
tent words. Sometimes they can be linked (in-
correctly) to their head?s corresponding words, or
they can be linked to totally different words with
respect to their meaning.
In the preliminary experiment using GIZA++
(Och and Ney, 2003) with grow-diag-final heuris-
tic, we found that about 25% of words in Ko-
rean sentences and 21% of English sentences fail
to align. This null alignment ratio is relatively
high in comparison to the French-English align-
ment, in which about 9% of French sentences and
6% of English sentences are not aligned. Due to
this null alignment, the estimation of translation
probabilities for Korean function words may be in-
complete; a system would perform mainly based
1Some may consider it an Altaic language family.
233
eotteon daneodeul eun hana eui teukjeong ha   n  daneo eui hyeongtae wa yeongyol doi ji an     neunda .
some      words      X           one    of    particular      X      X word  of  form     with      connect    become X         not    X            .
Figure 1: An example of null alignment
eotteon daneodeul eun hana eui teukjeong ha       n       daneo eui hyeongtae wa yeongyol doi ji an    neunda .
Figure 2: An example of ideal alignment
on content-words, which can deteriorate the per-
formance of candidate generation during decod-
ing. Also, without generating appropriate function
words, the quality of the translation will undoubt-
edly degrade.
In this paper, we present a preprocessing
method for both training and decoding in English-
Korean SMT. In particular, we transform a source
language sentence by inserting pseudo words and
syntactically reordering it to form a target sen-
tence structure in hopes of reducing the morpho-
syntactic discrepancies between two languages.
Ultimately, we expect an ideal alignment, as
shown in Figure 2. Our results show that the
combined pseudo word insertion and syntactic re-
ordering method reduces null alignment ratio and
makes both sentences have similar length. We re-
port results showing that the proposed method can
improve the translation quality.
2 Pseudo Word Insertion
Lee et al (2006) find that function words in Ko-
rean sentences are not aligned to any English
words, and can simply and easily be removed by
referring to their POS information. The unaligned
words are case particles, final endings, and auxil-
iary particles, and they call these words ?untrans-
latable words?.
The method can be effective for Korean-English
SMT where target language does not have corre-
sponding function words, but it has a limitation
in application to the English-Korean SMT because
removing functional morphemes can cause a seri-
ous loss in information. Technically, the function
words they ignored are not ?untranslatable? but are
?unalignable?. Therefore, instead of removing the
function words, we decide to insert some pseudo
words into an English sentence in order to align
them with potential Korean function words and
make the length of both sentences similar.
To insert the pseudo words, we need to decide:
(1) the kinds of words to insert, and (2) the loca-
tion to insert the words. Because we expect that a
pseudo word corresponds to any Korean function
word which decides a syntactic role of its head,
it is reasonable to utilize a dependency relation of
English. Thus, given an English sentence, the can-
didate pseudo words are generated by the follow-
ing methods: First, we parse the English sentence
using Stanford dependency parser (de Marneffe et
al., 2006). Then, we select appropriate typed de-
pendency relations between pairs of words which
are able to generate Korean function words. We
found that 21 out of 48 dependency relations can
be directly used as pseudo words. Among them,
some relations provide very strong cue of case par-
ticles when inserted as pseudo words.
For example, from the following sentence, we
can select as pseudo words a subjective particle
<NS> and an objective particle <DO>, and in-
sert them after the corresponding dependents Eu-
gene and guitar respectively.
nominal subject(play, Eugene)
direct object(play, guitar)
Eugene <NS> can ?t play the guitar <DO> well .
In a preliminary experiment on word alignment,
234
nominal subject ?H(neun), null,s(i)
direct object `?(eul), null,\?(reul)
clausal subject ?H(neun), null,s(i)
temporal modifier \?(neun), null, ??Z?t(oneul)
adj complement null,(ah),(ha)
agent null,\?(e), (ga)
numeric modifier null,_(eui),>h(gae)
adj modifier null,\?(e), (ga)
particle modifier null,(n),?&(doe)
Figure 3: Selected dependency relations and their
aligned function words in training data (shown
the top 3 results in descending order of alignment
probability)
we observe that inserting too many pseudo words
can, on the contrary, increase null alignment of
English sentence. Thus we filtered some pseudo
words according to their respective null alignment
probabilities. Figure 3 shows the top 9 selected
dependency relations (actually used in the experi-
ment) and the aligned Korean function words.
3 Syntactic Reordering
Many approaches use syntactic reordering in the
preprocessing step for SMT systems (Collins et
al., 2005; Xia and McCord, 2004; Zwarts and
Dras, 2007). Some reordering approaches have
given significant improvements in performance for
translation from French to English (Xia and Mc-
Cord, 2004) and from German to English (Collins
et al, 2005). However, on the contrary, Lee et al
(2006) reported that the reordering of Korean for
Korean-English translation degraded the perfor-
mance. They presumed that the performance de-
crease might come from low parsing performance
for conversational domain.
We believe that it is very important to consider
the structural properties of Korean for reordering
English sentences. Though the word order of a
Korean sentence is relatively free, Korean gener-
ally observes the SOV word order, and it is a head-
final language. Consequently, an object precedes a
predicate, and all dependents precede their heads.
We use both a structured parse tree and de-
pendency relations to extract following reordering
rules.
? Verb final: In any verb phrase, move verbal
head to the end of the phrase. Infinitive verbs or
verb particles are moved together.
He (likes ((to play) (the piano))) (1)
He (likes ((the piano) (to play))) (2)
He (((the piano) (to play)) likes) (3)
? Adjective final: In adjective phrase, move ad-
jective head to the end of the phrase especially if
followed by PP or S/SBAR.
It is ((difficult) to reorder) (1)
It is (to reorder (difficult)) (2)
? Antecedent final: In noun phrase containing
relative clause, move preceding NP to the end of a
relative clause.
((rules) that are used for reordering) (1)
(that are used for reordering (rules)) (2)
? Negation final: Move negative markers to di-
rectly follow verbal head.
(can ?t) ((play) the guitar) (1)
(can ?t) (the guitar (play)) (2)
(the guitar (play)) (can ?t) (3)
4 Experiments
4.1 Experimental Setup
The baseline of our approach is a statisti-
cal phrase-based system which is trained using
MOSES (Koehn et al, 2007). We collect bilin-
gual texts from the Web and combine them with
the Sejong parallel corpora 2. About 300K pair of
sentences are collected from the major bilingual
news broadcasting sites. We also collect around
1M monolingual sentences from the sites to train
Korean language models. The best performing
language model is 5-gram order with Kneser-Ney
smoothing.
For sentence level alignment, we modified the
Champollion toolkit for English-Korean pair (Ma,
2006). We randomly selected 5,000 sentence pairs
from Sejong corpora, of which 1,500 were used
for a tuning set for minimum error rate training,
and another 1,500 for development set for analy-
sis experiment. We report testing results on the
remaining 2,000 sentence pairs for the evaluation.
Korean sentences are tokenized by the morpho-
logical analyzer (Lee and Rim, 2004). For English
sentence preprocessing, we use the Stanford parser
with output of typed dependency relations. We
then applied the pseudo word insertion and four
reordering rules described in the previous section
to the parse tree of each sentence.
2The English-Korean parallel corpora open for research
purpose which contain about 60,000 sentence pairs. See
http://www.sejong.or.kr/english.php for more information
235
BLEU(gain) Length Ratio
Baseline 18.03(+0.00) 0.78
+PWI only 18.62(+0.59) 0.91
+Reorder only 19.92(+1.89) 0.78
+PWI&Reorder 20.42(+2.39) 0.91
Table 1: BLEU score and sentence length ratio for
each method
Baseline +PWI +Reorder +P&R
src-null 20.5 21.4 19.1 20.9
tgt-null 25.4 22.3 23.4 20.8
all-null 23.3 21.9 21.5 20.8
Table 2: Null alignment ratio (%) for each method
(all-null is calculated on the whole training data)
4.2 Experimental Results
The BLEU scores are reported in Table 1. Length
ratio indicates the average sentence length ratio
between source sentences and target sentences.
The largest gain (+2.39) is achieved when the
combined pseudo word insertion (PWI) and word
reordering is performed.
There could be reasons why the proposed ap-
proach is effective over baseline approach. Pre-
sumably, transforming to similar length and word
order contributes to lower the distortion and fertil-
ity parameter values. Table 2 analyzes the effect
of individual techniques in terms of the null align-
ment ratio. We discover that the alignment ratio
can be a good way to measure the relation between
the quality of word alignment and the quality of
translation. As shown in Table 2, the BLEU score
tends to increase as the all-null ratio decreases. In-
terestingly, reordering achieves the smallest null
alignment ratio for source language.
5 Conclusions
In this paper, we presented a novel approach to
preprocessing English-Korean SMT. The morpho-
syntactic discrepancy between English and Korean
causes a serious null alignment problem.
The main contributions of this paper are the fol-
lowing: 1) we devise a new preprocessing method
for English-Korean SMT by transforming a source
sentence to be much closer to a target sentence in
terms of sentence length and word order. 2) we
discover that the proposed method can reduce the
null alignment problem, and consequently the null
word alignment ratio between two languages can
be a good way to measure the quality of transla-
tion.
When evaluating the proposed approach using
within MOSES, the combined pseudo word inser-
tion and syntactic reordering method outperforms
the other methods. The result proves that the pro-
posed method can be used as a useful technique
for English-Korean machine translation.
Acknowledgments
This work was supported by Microsoft Research
Asia. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the sponsor.
References
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of LREC.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demonstration session.
Do-Gil Lee and Hae-Chang Rim. 2004. Part-of-speech
tagging considering surface form for an agglutina-
tive language. In Proc. of ACL.
Jonghoon Lee, Donghyeon Lee, and Gary Geun-
bae Lee. 2006. Improving phrase-based korean-
english statistical machine translation. In Proc. of
Interspeech-ICSLP.
Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proc. of LREC.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proc. of COLING.
Simon Zwarts and Mark Dras. 2007. Syntax-based
word reordering in phrase-based statistical machine
translation: Why does it work? In Proc. of MT-
Summit XI.
236
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 321?324,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
The Contribution of Stylistic Information to
Content-based Mobile Spam Filtering
Dae-Neung Sohn and Jung-Tae Lee and Hae-Chang Rim
Department of Computer and Radio Communications Engineering
Korea University
Seoul, 136-713, South Korea
{danny,jtlee,rim}@nlp.korea.ac.kr
Abstract
Content-based approaches to detecting
mobile spam to date have focused mainly
on analyzing the topical aspect of a SMS
message (what it is about) but not on the
stylistic aspect (how it is written). In this
paper, as a preliminary step, we investigate
the utility of commonly used stylistic fea-
tures based on shallow linguistic analysis
for learning mobile spam filters. Experi-
mental results show that the use of stylis-
tic information is potentially effective for
enhancing the performance of the mobile
spam filters.
1 Introduction
Mobile spam, also known as SMS spam, is a sub-
set of spam that involves unsolicited advertising
text messages sent to mobile phones through the
Short Message Service (SMS) and has increas-
ingly become a major issue from the early 2000s
with the popularity of mobile phones. Govern-
ments and many service providers have taken var-
ious countermeasures in order to reduce the num-
ber of mobile spam (e.g. by imposing substantial
fines on spammers, blocking specific phone num-
bers, creating an alias address, etc.). Nevertheless,
the rate of mobile spam continues to rise.
Recently, a more technical approach to mobile
spam filtering based on the content of a SMS mes-
sage has started gaining attention in the spam re-
search community. G?omez Hidalgo et al (2006)
previously explored the use of statistical learning-
based classifiers trained with lexical features, such
as character and word n-grams, for mobile spam
filtering. However, content-based spam filtering
directed at SMS messages are very challenging,
due to the fact that such messages consist of only
a few words. More recent studies focused on ex-
panding the feature set for learning-based mobile
spam classifiers with additional features, such as
orthogonal sparse word bi-grams (Cormack et al,
2007a; Cormack et al, 2007b).
Collectively, the features exploited in earlier
content-based approach to mobile spam filtering
are topical terms or phrases that statistically in-
dicate the spamness of a SMS message, such as
?loan? or ?70% off sale?. However, there is
no guarantee that legitimate (non-spam) messages
would not contain such expressions. Any of us
may send a SMS message such as ?need ur ad-
vise on private loans, plz call me? or ?mary,
abc.com is having 70% off sale today?. For cur-
rent content-based mobile spam filters, there is a
chance that they would classify such legitimate
messages as spam. This motivated us to not only
rely on the message content itself but incorporate
new features that reflect its ?style,? the manner in
which the content is expressed, in mobile spam fil-
tering.
The main goal of this paper is to investigate the
potential of stylistic features in improving the per-
formance of learning-based mobile spam filters. In
particular, we adopt stylistic features previously
suggested in authorship attribution studies based
on stylometry, the statistical analysis of linguistic
style.
1
Our assumption behind adopting the fea-
tures from authorship attribution are as follows:
? There are two types of SMS message senders,
namely spammers and non-spammers.
? Spammers have distinctive linguistic styles
and writing behaviors (as opposed to non-
spammers) and use them consistently.
? The SMS message as an end product carries
the author?s ?fingerprints?.
1
Authorship attribution involves identifying the author of
a text given some stylistic characteristics of authors? writing.
See Holmes (1998) for overview.
321
Although there are many types of stylistic fea-
tures suggested in the literature, we make use of
the ones that are readily computable and countable
from SMS message texts without any complex lin-
guistic analysis as a preliminary step, including
word and sentence lengths (Mendenhall, 1887),
frequencies of function words (Mosteller and Wal-
lace, 1964), and part-of-speech tags and tag n-
grams (Argamon-Engelson et al, 1998; Koppel et
al., 2003; Santini, 2004).
Our experimental result on a large-scale, real
world SMS dataset demonstrates that the newly
added stylistic features effectively contributes to
statistically significant improvement on the perfor-
mance of learning-based mobile spam filters.
2 Stylistic Feature Set
All stylistic features listed below have been auto-
matically extracted using shallow linguistic analy-
sis. Note that most of them have been motivated
from previous stylometry studies.
2.1 Length features: LEN
Mendenhall (1887) first created the idea of count-
ing word lengths to judge the authorship of texts,
followed by Yule (1939) and Morton (1965) with
the use of sentence lengths. In this paper, we mea-
sure the overall byte length of SMS messages and
the average byte length of words in the message as
features.
2.2 Function word frequencies: FW
Motivated from a number of stylometry studies
based on function words including Mosteller and
Wallace (1964), Tweedie et al (1996) and Arg-
amon and Levitan (2005), we measure the fre-
quencies of function words in SMS messages as
features. The intuition behind function words is
that due to their high frequency in languages and
highly grammaticalized roles, such words are un-
likely to be subject to conscious control by the au-
thor and that the frequencies of different function
words would vary greatly across different authors
(Argamon and Levitan, 2005).
2.3 Part-of-speech n-grams: POS
Following the work of Argamon-Engelson et al
(1998), Koppel et al (2003), Santini (2004) and
Gamon (2004), we extract part-of-speech n-grams
(up to trigrams) from the SMS messages and use
their frequencies as features. The idea behind their
utility is that spammers would favor certain syn-
tactic constructions in their messages.
2.4 Special characters: SC
We have observed that many SMS messages con-
tain special characters and that their usage varies
between spam and non-spam messages. For in-
stance, non-spammers often use special characters
to create emoticons to express their mood, such as
?:-)? (smiling) or ?T T? (crying), whereas spam-
mers tend to use special character or patterns re-
lated to monetary matters, such as ?$$$? or ?%?.
Therefore, we also measured the ratio of special
characters, the number of emoticons, and the num-
ber of special character patterns in SMS messages
as features.
2
3 Learning a Mobile Spam Filter
In this paper, we use maximum entropy model,
which have shown robust performance in various
text classification tasks in the literature, for learn-
ing the mobile spam filter. Simply put, given a
number of training samples (in our case, SMS
messages), each with a label Y (where Y = 1 if
spam and 0 otherwise) and a feature vector x, the
filter learns a vector of feature weight parameters
w. Given a test sample X with its feature vector x,
the filer outputs the conditional probability of pre-
dicting the data as spam, P (Y = 1|X = x). We
use the L-BFGS algorithm (Malouf, 2002) and the
Information Gain (IG) measure for parameter esti-
mation and feature selection, respectively.
4 Experiments
4.1 SMS test collections
We use a collection of mobile SMS messages in
Korean, with 18,000 (60%) legitimate messages
and 12,000 (40%) spam messages. This collec-
tion is based on one used in our previous work
(Sohn et al, 2008) augmented with 10,000 new
messages. Note that the size is approximately 30
times larger than the most previous work by Cor-
mack et al (2007a) on mobile spam filtering.
4.2 Feature setting
We compare three types of feature sets, as follows:
2
For emoticon and special pattern counts, we used man-
ually constructed lexicons consisting of 439 emoticons and
229 special patterns.
322
? Baseline: This set consists of lexical features
in SMS messages, including words, charac-
ter n-grams, and orthogonal sparse word bi-
grams (OSB)
3
. This feature set represents
the content-based approaches previously pro-
posed by G?omez Hidalgo et al (2006), Cor-
mack et al (2007a) and Cormack et al
(2007b).
? Proposed: This feature set consists of all the
stylistic features mentioned in Section 2.
? Combined: This set is a combination of both
the baseline and proposed feature sets.
For all three sets, we make use of 100 features with
the highest IG values.
4.3 Evaluation measures
Since spam filtering task is very sensitive to false-
positives (i.e. legitimate classified as spam) and
false-negatives (i.e. spam classified as legitimate),
special care must be taken when choosing an ap-
propriate evaluation criterion.
Following the TREC Spam Track, we evalu-
ate the filters using ROC curves that plot false-
positive rate against false-negative rate. As a sum-
mary measure, we report one minus area under
the ROC curve (1?AUC) as a percentage with
confidence intervals, which is the TREC?s official
evaluation measure.
4
Note that lower 1?AUC(%)
value means better performance. We used the
TREC Spam Filter Evaluation Toolkit
5
in order to
perform the ROC analysis.
4.4 Results
All experiments were performed using 10-fold
cross validation. Statistical significance of differ-
ences between results were computed with a two-
tailed paired t-test. The symbol ? indicates statis-
tical significance over an appropriate baseline at
p < 0.01 level.
Table 1 reports the 1?AUC(%) summary for
each feature settings listed in Section 4.2. Notice
that Proposed achieves significantly better perfor-
mance than Baseline. (Recall that the smaller, the
3
OSB refers to words separated by 3 or fewer words,
along with an indicator of the difference in word positions;
for example, the expression ?the quick brown fox? would
induce following OSB features: ?the (0) quick?, ?the (1)
brown?, ?the (2) fox?, ?quick (0) brown?, ?quick (1) fox?,
and ?brown (0) fox? (Cormack et al, 2007a).
4
For detail on ROC analysis, see Cormack et al (2007a).
5
Available at http://plg.uwaterloo.ca/.trlynam/spamjig/
Feature set 1?AUC (%)
Baseline 10.7227 [9.4476 - 12.1176]
Proposed 4.8644
?
[4.2726 - 5.5886]
Combined 3.7538
?
[3.1186 - 4.4802]
Table 1: Performance of different feature settings.
50.00
10.00
1.00
50.0010.001.000.100.01
Fal
se 
Ne
gat
ive
 Ra
te(lo
git s
cale
)
False Positve Rate (logit scale)
CombinedProposedBaseline
Figure 1: ROC curves of different feature settings.
better.) An even greater performance gain is ob-
tained by combining both Proposed and Baseline.
This clearly indicates that stylistic aspects of SMS
messages are potentially effective for mobile spam
filtering.
Figure 1 shows the ROC curves of each fea-
ture settings. Notice the tradeoff when Proposed
is used solely with comparison to Baseline; false-
positive rate is worsened in return for gaining bet-
ter false-negative rate. Fortunately, when both fea-
ture sets are combined, false-positive rate is re-
mained unchanged while the lowest false-negative
rate is achieved. This suggests that the addition of
stylistic features contributes to the enhancement of
false-negative rate while not hurting false-positive
rate (i.e. the cases where spam is classified as le-
gitimate are significantly lessened).
In order to evaluate the contribution of different
types of stylistic features, we conducted a series
of experiments by removing features of a specific
type at a time from Combined. Table 2 shows the
detailed result. Notice that LEN and SC features
are the most helpful, since the performance drops
significantly after removing either of them. Inter-
estingly, FW and POS features show similar con-
tributions; we suggest that these two feature types
have similar effects in this filtering task.
We also conducted another series of experi-
ments, by adding one feature type at a time to
Baseline. Table 3 reports the results. Notice that
LEN features are consistently the most helpful.
The most interesting result is that POS features
continuously contributes the least. We carefully
323
Feature set 1?AUC (%)
Combined 3.7538 [3.1186 - 4.4802]
? LEN 4.7351
?
[4.0457 - 5.6405]
? FW 3.9823
?
[3.3048 - 4.5930]
? POS 4.0712
?
[3.4057 - 4.8630]
? SC 4.7644
?
[4.1012 - 5.4350]
Table 2: Performance by removing one stylistic
feature set from the Combined set.
Feature set 1?AUC (%)
Baseline 10.7227 [9.4476 - 12.1176]
+ LEN 5.5275
?
[4.0457 - 6.6281]
+ FW 6.0828
?
[5.1783 - 6.9249]
+ POS 9.6103
?
[8.7190 - 11.0579]
+ SC 7.5288
?
[6.6049 - 8.4466]
Table 3: Performance by adding one stylistic fea-
ture set to the Baseline set.
hypothesize that the result is due to high depen-
dencies between POS and lexical features.
5 Discussion
In this paper, we have introduced new features that
indicate the written style of texts for content-based
mobile spam filtering. We have also shown that the
stylistic features are potentially useful in improv-
ing the performance of mobile spam filters.
This is definitely a work in progress, and much
more experimentation is required. Deep linguis-
tic analysis-based stylistic features, such as con-
text free grammar production frequencies (Ga-
mon, 2004) and syntactic rewrite rules in an au-
tomatic parse (Baayen et al, 1996), that have al-
ready been successfully used in the stylometry lit-
erature may be considered. Perhaps most impor-
tantly, the method must be tested on various mo-
bile spam data sets written in languages other than
Korean. These would be our future work.
References
Shlomo Argamon and Shlomo Levitan. 2005. Measur-
ing the usefulness of function words for authorship
attribution. In Proceedings of ACH/ALLC ?05.
Shlomo Argamon-Engelson, Moshe Koppel, and Galit
Avneri. 1998. Style-based text categorization:
What newspaper am i reading? In Proceedings of
AAAI ?98 Workshop on Text Categorization, pages
1?4.
H. Baayen, H. van Halteren, and F. Tweedie. 1996.
Outside the cave of shadows: using syntactic annota-
tion to enhance authorship attribution. Literary and
Linguistic Computing, 11(3):121?132.
Gordon V. Cormack, Jos?e Mar??a G?omez Hidalgo, and
Enrique Puertas S?anz. 2007a. Spam filtering for
short messages. In Proceedings of CIKM ?07, pages
313?320.
Gordon V. Cormack, Jos?e Mar??a G?omez Hidalgo, and
Enrique Puertas S?anz. 2007b. Feature engineering
for mobile (sms) spam filtering. In Proceedings of
SIGIR ?07, pages 871?872.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analy-
sis features. In Proceedings of COLING ?04, page
611.
Jos?e Mar??a G?omez Hidalgo, Guillermo Cajigas
Bringas, Enrique Puertas S?anz, and Francisco Car-
rero Garc??a. 2006. Content based sms spam filter-
ing. In Proceedings of DocEng ?06, pages 107?114.
David I. Holmes. 1998. The evolution of stylometry
in humanities scholarship. Literary and Linguistic
Computing, 13(3):111?117.
Moshe Koppel, Shlomo Argamon, and Anat R. Shi-
moni. 2003. Automatically categorizing written
texts by author gender. Literary and Linguistic
Computing, 17(4):401?412.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of COLING ?02, pages 1?7.
T. C. Mendenhall. 1887. The characteristic curves of
composition. Science, 9(214):237?246.
A. Q. Morton. 1965. The authorship of greek prose.
Journal of the Royal Statistical Society Series A
(General), 128(2):169?233.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and Disputed Authorship: The Federalist.
Addison-Wesley.
Marina Santini. 2004. A shallow approach to syntactic
feature extraction for genre classification. In Pro-
ceedings of CLUK Colloquium ?04.
Dae-Neung Sohn, Joong-Hwi Shin, Jung-Tae Lee,
Seung-Wook Lee, and Hae-Chang Rim. 2008.
Contents-based korean sms spam filtering using
morpheme unit features. In Proceedings of HCLT
?08, pages 194?199.
E. J. Tweedie, S. Singh, and D. I. Holmes. 1996. Neu-
ral network applications in stylometry: The federal-
ist papers. Computers and the Humanities, 30:1?10.
G. Udny Yule. 1939. On sentence-length as a statisti-
cal characteristic of style in prose, with application
to two cases of disputed authorship. Biometrika,
30(3-4):363?390.
324
Automatic Word Spacing Using Hidden Markov Model
for Rening Korean Text Corpora
Do-Gil Lee and Sang-Zoo Lee and Hae-Chang Rim
NLP Lab., Dept. of Computer Science and Engineering, Korea University
1, 5-ka, Anam-dong, Seongbuk-ku, Seoul 136-701, Korea
Heui-Seok Lim
Dept. of Information and Communications, Chonan University
115 AnSeo-dong, CheonAn 330-704, Korea
Abstract
This paper proposes a word spacing model using
a hidden Markov model (HMM) for rening Ko-
rean raw text corpora. Previous statistical ap-
proaches for automatic word spacing have used
models that make use of inaccurate probabilities
because they do not consider the previous spac-
ing state. We consider word spacing problem as
a classication problem such as Part-of-Speech
(POS) tagging and have experimented with var-
ious models considering extended context. Ex-
perimental result shows that the performance
of the model becomes better as the more con-
text considered. In case of the same number
of parameters are used with other method, it
is proved that our model is more eective by
showing the better results.
1 Introduction
Automatic word spacing is a process to de-
cide correct boundaries between words in a sen-
tence containing spacing errors. In Korean,
word spacing is very important to increase the
readability and to communicate the accurate
meaning of a text. For example, if a sentence
\!Qt  ~??\? [?t#Q $4(Father entered the
room)" is written as \!Qt  ~??\? [?t#Q $4
(Father entered the bag)", then its meaning
is changed a lot.
There are many word spacing errors in doc-
uments on the Internet, which is the principal
source of information. To deal with these docu-
ments properly, an automatic word spacing sys-
tem is absolutely necessary. Besides, it plays
an important role as a preprocessor of a mor-
phological analyzer that is a fundamental tool
for natural language processing applications, a
postprocessor to restore line boundaries from
an OCR, a postprocessor for continuous-syllable
sentence from a speech recognition system, and
one module for an orthographic error revision
system.
In Korean, spacing unit is Eojeol. Each Eo-
jeol consists of one or more words and a word
consists of one or more morphemes. Figure
1 represents their relationships for a sentence
\o^=??  sl???`? {9%3". According to the
rules of Korean spelling, the main principle for
word spacing is to split every word in a sen-
tence. Because one morpheme may form a word
and several morphemes too, there are confusing
cases to distinguish among words. Even though
postpositions belong to words, they should be
concatenated with the preceding word. Besides,
there are many conicting (but can be permit-
ted) cases with the principles. For example,
spacing or concatenating individual nouns in-
cluding a compound noun are both considered
as right. As mentioned, word spacing is impor-
tant for some reasons, but it is di?cult for even
man to space words correctly by spelling rules
because of the characteristics of Korean and the
inconsistent rules. Especially, it is much more
confused in the case of having no inuence on
understanding the meaning of a sentence.
In this paper, we propose a word spacing
model
1
using an HMM. HMM is a widely used
statistical model to solve various NLP prob-
lems such as POS tagging(Charniak et al, 1993;
Merialdo, 1994; Kim et al, 1998a; Lee, 1999).
We regard the word spacing problem as a classi-
cation problem such as the POS tagging prob-
lem. When using an HMM for automatic word
spacing task, raw texts can be used as training
1
Strictly speaking, our model described here is an Eo-
jeol spacing model rather than a word spacing model
because spacing unit of Korean is Eojeol. But we in
this paper do not distinguish between Eojeol and word
for convenience. Therefore, we use the term \word" as
word, spacing unit in English.
  	

    	

     	 
 
Eojeol
word
morpheme
proper noun :
person name
postposition
noun : story
noun : book
postposition
verb : read
prefinal ending
ending
Figure 1: Constitution of the sentence \o^=??  sl???`? {9%3"
data. Therefore, we expect that HMM can be
applied to the task eectively without bothering
to construct training data.
2 Related Works
Previous approaches for automatic word spac-
ing can be classied into two groups: rule based
approach and statistical approach. The rule-
based approach uses lexical information and
heuristic rules(Choi, 1997; Kim et al, 1998b;
Kang, 1998; Kang, 2000). Lexical information
consists of postposition and Eomi
2
information,
a list of spaced word examples, etc. Heuristic
rules are composed of longest match or short-
est match rule, morphological rules, and error
patterns. This approach has disadvantage re-
quiring higher computational complexity than
the statistical approach. It also costs too much
in constructing and maintaining lexical informa-
tion. Most of rule-based systems use a morpho-
logical analyzer to recognize word boundaries.
Another disadvantages of rule-based approach
are resulted from using morphological analyzer.
First, if ambiguous analyses are possible, fre-
quent backtracking may be caused and many
errors are propagated by an erroneous analy-
sis. Second, results of automatic word spacing
are highly dependent on the morphological an-
alyzer; false word boundary recognition occurs
if morphological analysis fails due to unknown
words. In addition, if an erroneous word is suc-
cessfully analyzed through overgeneration, the
error cannot even be detected. Finally, if a word
2
Eomi is a grammatical morpheme of Korean which
is attached to verbal root
spacing system is used as a preprocessor of a
morphological analyzer, the same morphologi-
cal analyzing process should be repeated twice.
The statistical approach uses syllable statis-
tics extracted from large amount of corpora to
decide whether two adjacent syllables should be
spaced or not(Shim, 1996; Shin and Park, 1997;
Chung and Lee, 1999; Jeon and Park, 2000;
Kang and Woo, 2001). In contrast to the rule-
based approach, it does not require many costs
to construct and to maintain statistics because
they can be acquired automatically. It is more
robust against unknown words than rule-based
approach that uses a morphological analyzer.
A statistical method proposed in Kang and
Woo (2001) has shown the best performance so
far. In this method, word spacing probability
P (x
i
; x
i+1
), between two adjacent syllables x
i
and x
i+1
, is in Equation 1. If the probability is
greater than 0:375, a space is inserted between
x
i
and x
i+1
.
P (x
i
; x
i+1
) = 0:25  P
R
(x
i 1
; x
i
) +
0:5  P
M
(x
i
; x
i+1
) +
0:25  P
L
(x
i+1
; x
i+2
) (1)
In Equation 1, P
R
, P
M
, and P
L
denote the
probability of a space being inserted in the right,
middle, and left of the two syllables, respec-
tively. They are calculated as follows:
P
R
(x
i 1
; x
i
) =
freq(x
i 1
; x
i
; SPACE)
freq(x
i 1
; x
i
)
P
M
(x
i
; x
i+1
) =
freq(x
i
; SPACE; x
i+1
)
freq(x
i
; x
i+1
)
PL
(x
i+1
; x
i+2
) =
freq(SPACE; x
i+1
; x
i+2
)
freq(x
i+1
; x
i+2
)
In the above equations, freq(x) denotes a fre-
quency of a string x from training data, and
SPACE denotes a white space.
Similar to this method, other statistical sys-
tems usually use the word spacing probability
estimated from every syllable bigram
3
in the
corpora. They calculate the probability by com-
bining P
R
, P
M
, and P
L
and compare it with a
certain threshold. If the probability is higher
than the threshold, then a space is inserted be-
tween two syllables.
It is reported that the performance is so sensi-
tive to training data: it shows somewhat dier-
ent performance according to similarity between
input document and training data. And there is
a crucial problem in the statistical method re-
sulted from not considering the previous spacing
state. For example, consider a sentence \/BN??
?+???e?" of which correctly word spaced sen-
tence is \/BN???+? ?? e?". According to Equa-
tion 1, the word spacing probability of \??" and
\e?" will be calculated as follows:
P (??;e?) = 0:25  P
R
(?+?;??) + 0:5  P
M
(??;e?)
+ 0:25  P
L
(e?;)
The probability P
R
(?+?;??) as follows:
P
R
(?+?;??) =
freq(?+?;??; SPACE)
freq(?+?;??)
But a space should have been inserted be-
tween \?+?" and \??" in the correct sentence,
we should use freq(SPACE;??; SPACE) in-
stead of freq(?+?;??; SPACE) in order to get
the correct word spacing probability. This phe-
nomenon comes from not considering the previ-
ous spacing state. To alleviate this problem, we
can consider the previous spacing state that the
system has decided before. But errors can be
propagated from the previous false word spac-
ing result. Eventually, to avoid such propagated
errors, the system has to generate all possible in-
terpretations from a given sentence and choose
the best one. To choose the best state from all
possible states, we use an HMM in this paper.
3
syllable bigram is dened to be any combination of
two syllables with or without a space.
3 Word Spacing Model based on
Hidden Markov Model
POS tagging is the most representative area
for HMM. Before explaining our word spacing
model using HMM, let's consider the POS tag-
ging model using an HMM. POS tagging func-
tion  (W ) is to nd the most likely sequence
of POS tags T = (t
1
; t
2
; : : : ; t
n
) for a given sen-
tence of words W = (w
1
; w
2
; : : : ; w
n
) and is de-
ned in Equation 2:
 (W )
def
=
argmax
T
P (T j W ) (2)
= argmax
T
P (T )P (W j T )
P (W )
(3)
= argmax
T
P (T )P (W j T ) (4)
= argmax
T
P (T;W ) (5)
Using Bayes' rule, Equation 2 becomes Equa-
tion 3. Since P (W ) is a constant for T , Equa-
tion 3 is transformed into Equation 4.
The probability P (T;W ) is broken down into
the following equations by using the chain rule:
P (T;W ) = P (t
1;n
; w
1;n
) (6)
=
n
Y
i=1
 
P (t
i
j t
1;i 1
; w
1;i 1
)
P (w
i
j t
1;i
; w
1;i 1
)
!
(7)

n
Y
i=1
P (t
i
j t
i K;i 1
)P (w
i
j t
i
) (8)
Markov assumptions (conditional indepen-
dence) used in Equation 8 are that the prob-
ability of a current tag t
i
conditionally depends
on only the previous K tags and that the prob-
ability of a current word w
i
conditionally de-
pends on only the current tag. In Equation 8,
P (t
i
j t
i K;i 1
) is called transition probability
and P (w
i
j t
i
) is called lexical probability. Mod-
els are classied in terms of K. The larger K
is, the more context can be considered. Because
of the data sparseness problem, bigram model
(K is 1) and trigram model (K is 2) are used in
general.
The word spacing problem can be consid-
ered similar to POS tagging. We dene a
word spacing task as a task to nd the most
likely sequence of word spacing tags T =
(t
1
; t
2
; : : : ; t
n
) for a given sentence of syllables
S = (s
1
; s
2
; : : : ; s
n
). Our word spacing model is
dened as in Equation 9:
argmax
T
P (T j S) (9)
Word spacing tag is a tag to indicate whether
the current syllable and the next one should
be spaced or not. Tag, 1 means that a space
should be put after the current syllable. Tag,
0 means that the current and the next syllable
should not be spaced. For example, if we at-
tach the word spacing tags to a sentence \/BN??
?+? ?? e?. (I can study)", then it is tagged as
\/BN/0+??/0+?+?/1+??/1+e?/0+/0+./1".
Our proposed word spacing model is to nd
the tag sequence T for maximizing the proba-
bility P (T; S).
P (T; S )
= P (t
1;n
; s
1;n
) (10)
=

P (t
1
)  p(s
1
j t
1
)



P (t
2
j t
1
; s
1
)  P (s
2
j t
1;2
; s
1
)


 
P (t
3
j t
1;2
; s
1;2
)
P (s
3
j t
1;3
; s
1;2
)
!
   

 
P (t
n
j t
1;n 1
; s
1;n 1
)
P (s
n
j t
1;n
; s
1;n 1
)
!
(11)
=
n
Y
i=1
 
P (t
i
j t
1;i 1
; s
1;i 1
)
P (s
i
j t
1;i
; s
1;i 1
)
!
(12)

n
Y
i=1
 
P (t
i
j t
i K;i 1
; s
i J;i 1
)
P (s
i
j t
i L;i
; s
i I;i 1
)
!
(13)
There are two Markov assumptions in Equa-
tion 13. One is that the probability of a current
tag t
i
conditionally depends on only the previ-
ous K (word spacing) tags and the previous J
syllables. The other is that the probability of
a current syllable s
i
conditionally depends on
only the previous L tags, the current tag t
i
, and
the previous I tags. This model is denoted by
(T
(K:J)
; S
(L:I)
). Similar to the POS tagging
model, P (t
i
j t
i K;i 1
; s
i J;i 1
) is called tran-
sition probability, and P (s
i
j t
i L;i
; s
i I;i 1
) is
called syllable probability in Equation 13. On
the other hand, our word spacing model uses
less strict Markov assumptions to consider a
larger context. The larger the values of K, J ,
L, and I are, the more context can be consid-
ered. In order to avoid the data sparseness and
excessively increasing parameters of a model, it
is important to select proper values. In our cur-
rent work, they are restricted as follows:
0  K;J; L; I  2
Thus, 3333 = 81 models are possible. But
we do not use the case of (K;J) = (0; 0) in the
trasition probabilities. As a result, we actually
use 72 models. It has not yet been known that
which model is the best. We can verify this only
by means of experiments. Some possible models
and their equations are listed in Table 1.
Probabilities can be estimated simply by the
maximum likelihood estimator (MLE) from raw
texts. The syllable probabilities and the tran-
sition probabilities of the model (T
(1:2)
; S
(1:2)
)
are estimated as follows:
P
MLE
(t
i
j t
i 1
; s
i 2;i 1
)
=
freq(s
i 2
; t
i 1
; s
i 1
; t
i
)
freq(s
i 2
; t
i 1
; s
i 1
)
P
MLE
(s
i
j t
i 1;i
; s
i 2;i 1
)
=
freq(s
i 2
; t
i 1
; s
i 1
; t
i
; s
i
)
freq(s
i 2
; t
i 1
; s
i 1
; t
i
)
To avoid zero probability, we just set very low
value such as 0:00001 if an estimated probability
is 0.
The probability that the model
(T
(1:1)
; S
(0:1)
) outputs \/BN/0+??/0+?+?/1+
??/1+e?/0+/0+./1" from a sentence \/BN??
?+???e?." is calculated as follows:
P (T; S) = P (t
0
= 0 j s
 1
= $; t
 1
= 1)
 P (s
0
=/BN j s
 1
= $; t
0
= 0)
 P (t
1
= 0 j s
0
=/BN; t
0
= 0)
 P (s
1
=?? j s
0
=/BN; t
1
= 0)
 P (1 j??0)  P (?+? j??1)
 P (1 j?+?1)  P (?? j?+?1)
 P (0 j??1)  P (e? j??0)
 P (0 je?0)  P ( je?0)
 P (1 j0)  P (. j1)
\$" is a pseudo syllable which denotes the start
of a sentence, and its tag is always 1.
4
The
4
Because any two adjacent sentences should always
be spaced.
Table 1: Some models and their equations
Model Equation
(T
(1:0)
; S
(0:0)
)
Q
n
i=1
P (t
i
j t
i 1
)  P (s
i
j t
i
)
(T
(1:1)
; S
(0:1)
)
Q
n
i=1
P (t
i
j t
i 1
; s
i 1
)  P (s
i
j t
i
; s
i 1
)
(T
(1:1)
; S
(1:1)
)
Q
n
i=1
P (t
i
j t
i 1
; s
i 1
)  P (s
i
j t
i 1;i
; s
i 1
)
(T
(1:2)
; S
(1:2)
)
Q
n
i=1
P (t
i
j t
i 1
; s
i 2;i 1
)  P (s
i
j t
i 1;i
; s
i 2;i 1
)
(T
(2:2)
; S
(2:2)
)
Q
n
i=1
P (t
i
j t
i 2;i 1
; s
i 2;i 1
)  P (s
i
j t
i 2;i
; s
i 2;i 1
)
most probable sequence of word spacing tags is
e?ciently computed by using the Viterbi algo-
rithm.
4 Experimental Results
We used balanced 21st Century Sejong Project's
raw corpus of 26 million word size. As the bal-
anced corpus is used as training data, we ex-
pect that the performance would not be sensi-
tive too much to a certain document genre. The
ETRI POS tagged corpus of 288,269 word size
was used for evaluation. We modied the cor-
pus with no word boundary form for automatic
word spacing evaluation.
We used three kinds of evaluation measures:
syllable-unit accuracy (P
syl
), word-unit recall
(R
word
), and word-unit precision (P
word
). The
word-unit recall is the rate of the number of cor-
rectly spaced words compared to the number of
total words in a test document. The word-unit
precision measures how accurate the system's
results are. The reason why we do not divide the
syllable-unit accuracy as recall and precision is
that the number of syllables in a document and
that of the system created are the same. Each
measure is dened as follows:
P
syl
=
S
correct
S
total
 100(%)
R
word
=
W
correct
W
Dtotal
 100(%)
P
word
=
W
correct
W
Stotal
 100(%)
Where, S
correct
is the number of correctly
spaced syllables, S
total
is the total number of
syllables in a document, W
correct
is the number
of correctly spaced words, W
Dtotal
is the total
number of words in a document, and W
Stotal
is
the total number of words created by a system.
To investigate every model, we calculated the
two accuracies for dierent K, J , L, and I. Ac-
curacies for each model are listed in Table 2.
According to the experimental results, we
are sure that models considering more contexts
show better results. The model (T
(2:2)
; S
(1:2)
)
is the best for all measures.
Note that some models show the better ac-
curacies than the model (T
(2:2)
; S
(2:2)
), which
uses the largest context. It seems that this is
caused by sparseness of data. After evaluat-
ing the method of Kang and Woo (2001) for
our training and test data, it shows 93:06%
syllable-unit accuracy, 76:71% word-unit recall,
and 67:80% word-unit precision. Compared
with these results, our model shows much better
performance. If I is two in (S
(K:J)
; T
(L:I)
), syl-
lable trigrams are used. Although I is less than
two (such as the model (T
(2:1)
; S
(1:1)
, which
uses syllable bigrams), our model is better than
Kang and Woo (2001)'s. This fact tells us that
our model is also more eective even when used
the same number of parameters of the model.
There are two questions that we want to
know about the word spacing models: First,
how much training data is required to get the
best performance of a given model. Second,
which model best ts a given training cor-
pus. To answer these questions, we compare
the performance of various models according to
the size of training corpus in Figure 2. The
left plot shows the syllable-unit precision and
the right plot shows the word-unit precision.
In the gure, \HMM" denotes the proposed
model, and its number decides the model's
type. \Kang" denotes Kang and Woo (2001)'s
model. \HMM2110" uses syllable unigrams,
\HMM2111" and \Kang" use syllable bigrams,
and \HMM2212" uses syllable trigrams. The
models used here are the models that show the
best accuracies among the models that use same
Table 2: Experimental results according to (K, J , L, I)
Model P
syl
R
word
P
word
Model P
syl
R
word
P
word
Model P
syl
R
word
P
word
(0,1,0,0) 84.26 41.28 44.06 (0,1,0,1) 88.93 55.38 57.10 (0,1,0,2) 88.45 53.83 55.88
(0,1,1,0) 89.44 56.91 61.34 (0,1,1,1) 95.58 79.31 82.58 (0,1,1,2) 95.74 79.76 83.68
(0,1,2,0) 84.44 42.15 47.02 (0,1,2,1) 92.86 70.26 71.63 (0,1,2,2) 94.97 76.90 79.45
(0,2,0,0) 85.48 45.65 47.52 (0,2,0,1) 88.93 56.24 57.21 (0,2,0,2) 89.59 58.23 59.88
(0,2,1,0) 90.22 59.12 63.74 (0,2,1,1) 95.60 79.26 82.94 (0,2,1,2) 95.92 80.41 84.56
(0,2,2,0) 86.46 47.62 52.15 (0,2,2,1) 93.44 72.06 73.90 (0,2,2,2) 95.22 77.84 80.59
(1,0,0,0) 85.75 47.05 48.96 (1,0,0,1) 90.24 60.73 62.20 (1,0,0,2) 89.74 58.68 61.09
(1,0,1,0) 89.28 59.80 59.98 (1,0,1,1) 95.64 81.17 81.81 (1,0,1,2) 95.90 81.50 83.56
(1,0,2,0) 82.85 45.10 45.38 (1,0,2,1) 93.30 73.04 73.39 (1,0,2,2) 94.94 77.52 78.88
(1,1,0,0) 85.83 49.95 50.43 (1,1,0,1) 90.96 63.18 64.89 (1,1,0,2) 90.21 62.99 62.58
(1,1,1,0) 89.85 61.47 62.80 (1,1,1,1) 96.15 82.88 84.10 (1,1,1,2) 96.17 82.67 84.86
(1,1,2,0) 84.21 49.44 49.29 (1,1,2,1) 94.07 75.54 76.87 (1,1,2,2) 95.62 80.32 82.13
(1,2,0,0) 87.21 54.25 54.85 (1,2,0,1) 90.83 63.34 64.59 (1,2,0,2) 91.54 66.39 67.00
(1,2,1,0) 90.74 64.14 65.63 (1,2,1,1) 96.07 82.44 84.09 (1,2,1,2) 96.39 83.51 85.91
(1,2,2,0) 86.96 55.50 55.95 (1,2,2,1) 94.67 77.53 79.28 (1,2,2,2) 95.90 81.39 83.42
(2,0,0,0) 86.18 50.25 51.42 (2,0,0,1) 90.44 61.97 63.61 (2,0,0,2) 89.77 61.52 62.17
(2,0,1,0) 89.49 61.07 61.32 (2,0,1,1) 95.83 82.11 82.73 (2,0,1,2) 95.91 82.09 83.39
(2,0,2,0) 83.37 46.52 47.15 (2,0,2,1) 93.55 73.91 74.63 (2,0,2,2) 95.03 78.36 78.96
(2,1,0,0) 86.51 52.60 53.46 (2,1,0,1) 91.10 64.81 65.85 (2,1,0,2) 90.69 65.11 65.10
(2,1,1,0) 90.34 64.04 64.90 (2,1,1,1) 96.29 83.73 84.74 (2,1,1,2) 96.28 83.43 85.21
(2,1,2,0) 85.07 52.32 52.63 (2,1,2,1) 94.31 76.69 77.82 (2,1,2,2) 95.91 81.51 83.45
(2,2,0,0) 88.58 58.94 59.84 (2,2,0,1) 91.78 67.07 68.32 (2,2,0,2) 92.44 69.88 70.54
(2,2,1,0) 91.65 67.82 69.14 (2,2,1,1) 96.26 83.46 84.88 (2,2,1,2) 96.69 84.93 86.82
(2,2,2,0) 88.97 61.20 62.28 (2,2,2,1) 95.01 78.99 80.60 (2,2,2,2) 96.04 82.05 83.96
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
10000 100000 1e+06 1e+07
sy
lla
bl
e-
un
it 
pr
ec
isi
on
 (%
)
size of training corpus (# of words)
HMM2110
HMM2111
HMM2212
Kang
20
25
30
35
40
45
50
55
60
65
70
75
80
85
10000 100000 1e+06 1e+07
w
o
rd
-u
ni
t p
re
cis
io
n 
(%
)
size of training corpus (# of words)
HMM2110
HMM2111
HMM2212
Kang
Figure 2: Accuracies according to the size of training corpus
syllable ngrams.
We can observe the changes of the accura-
cies according to the size of the training data.
\HMM2110" using syllable unigrams converges
quickly on small training data. \HMM2111"
and \Kang" using syllable bigrams converge
on much more training data. Note that
\HMM2212" does not converge in these plots.
Therefore, there is a possibility of improve-
ment of this model's performance on more large
training data. \HMM2212" shows lower per-
formance than other models on small training
data. The reason is that the data sparseness
problem occurs.
5 Conclusion
Recently, text resources available from the In-
ternet have been rapidly increased. However,
there are many word spacing errors in those re-
sources, which cannot be used before correct-
ing errors. Therefore, the need for automatic
word spacing system to rene text corpora has
been raised. In this paper, we have proposed an
automatic word spacing model using an HMM.
Our method is a statistical approach and does
not require complex processes and costs in con-
structing and maintaining lexical information
as in the rule-based approach. The proposed
model can eectively solve the word spacing
problem by using only syllable statistics auto-
matically extracted from raw corpora. Accord-
ing to the experimental results, our model shows
higher performance than the previous method
even when using the same number of parame-
ters. We used just MLE to estimate probability,
but the more a model extends the context; the
more the data sparseness problem may arise.
In future work, we plan to adopt a smoothing
technique to increase the performance. Further
research on an eective evaluation method for
conicting cases is also necessary.
References
E. Charniak, C. Hendrickson, N. Jacobson, and
M. Perkowitz. 1993. Equations for part-of-
speech tagging. In National Conference on
Articial Intelligence, pages 784{789.
J.-H. Choi. 1997. Automatic Korean spacing
words correction system with bidirectional
longest match strategy. In Proceedings of the
9th Conference on Hangul and Korean Infor-
mation Processing, pages 145{151.
Y.-M. Chung and J.-Y. Lee. 1999. Automatic
word-segmentation at line-breaks for Korean
text processing. In Proceedings of the 6th
Conference of Korea Society for Information
Mangement, pages 21{24.
N.-Y. Jeon and H.-R. Park. 2000. Automatic
word-spacing of syllable bi-gram information
for Korean OCR postprocessing. In Proceed-
ings of the 12th Conference on Hangul and
Korean Information Processing, pages 95{
100.
S.-S. Kang and C.-W. Woo. 2001. Automatic
segmentation of words using syllable bigram
statistics. In Proceedings of the 6th Natural
Language Processing Pacic Rim Symposium,
pages 729{732.
S.-S. Kang. 1998. Automatic word-
segmentation for Hangul sentences. In
Proceedings of the 10th Conference on
Hangul and Korean Information Processing,
pages 137{142.
S.-S. Kang. 2000. Eojeol-block bidirectional
algorithm for automatic word spacing of
Hangul sentences. Journal of the Korea In-
formation Science Society, 27(4):441{447.
J.-D. Kim, H.-S. Lim, S.-Z. Lee, and H.-C. Rim.
1998a. Twoply hidden markov model: A Ko-
rean pos tagging model based on morpheme-
unit with word-unit context. Computer Pro-
cessing of Oriental Languages, 11(3):277{290.
K.-S. Kim, H.-J. Lee, and S.-J. Lee. 1998b.
Three-stage spacing system for Korean in
sentence with no word boundaries. Journal
of the Korea Information Science Society,
25(12):1838{1844.
S.-Z. Lee. 1999. New statistical models for au-
tomatic part-of-speech tagging. Ph.D. thesis,
Korea University.
B. Merialdo. 1994. Tagging english text with a
probabilistic model. Computational Linguis-
tics, 20(2):155{172.
Kwangseob Shim. 1996. Automated word-
segmentation for Korean using mutual infor-
mation of syllables. Journal of the Korea In-
formation Science Society, 23(9):991{1000.
J.-H. Shin and H.-R. Park. 1997. A statisti-
cal model for Korean text segmentation using
syllable-level bigrams. In Proceedings of the
9th Conference on Hangul and Korean Infor-
mation Processing, pages 255{260.
Poisson Naive Bayes for Text Classification with Feature Weighting
Sang-Bum Kim, Hee-Cheol Seo and Hae-Chang Rim
Dept. of CSE., Korea University
5-ka Anamdong, SungPuk-ku, SEOUL 136-701, KOREA
sbkim,hcseo,rim@nlp.korea.ac.kr
Abstract
In this paper, we investigate the use of
multivariate Poisson model and feature
weighting to learn naive Bayes text clas-
sifier. Our new naive Bayes text classifi-
cation model assumes that a document is
generated by a multivariate Poisson model
while the previous works consider a doc-
ument as a vector of binary term features
based on the presence or absence of each
term. We also explore the use of feature
weighting for the naive Bayes text classifi-
cation rather than feature selection, which
is a quite costly process when a small
number of the new training documents are
continuously provided.
Experimental results on the two test col-
lections indicate that our new model with
the proposed parameter estimation and the
feature weighting technique leads to sub-
stantial improvements compared to the
unigram language model classifiers that
are known to outperform the original pure
naive Bayes text classifiers.
1 Introduction
The naive Bayes classifier has been one of the core
frameworks in the information retrieval research for
many years. Recently, naive Bayes is emerged as a
research topic itself because it sometimes achieves
good performances on various tasks, compared to
more complex learning algorithms, in spite of the
wrong independence assumptions on naive Bayes.
Similarly, naive Bayes is also an attractive ap-
proach in the text classification task because it is
simple enough to be practically implemented even
with a great number of features. This simplicity en-
ables us to integrate the text classification and filter-
ing modules with the existing information retrieval
systems easily. It is because that the frequency re-
lated information stored in the general text retrieval
systems is all the required information in naive
Bayes learning. No further complex generaliza-
tion processes are required unlike the other machine
learning methods such as SVM or boosting. More-
over, incremental adaptation using a small number
of new training documents can be performed by just
adding or updating frequencies.
Several earlier works have extensively studied the
naive Bayes text classification (Lewis, 1992; Lewis,
1998; McCallum and Nigam, 1998). However,
their pure naive Bayes classifiers considered a doc-
ument as a binary feature vector, and so they can?t
utilize the term frequencies in a document, result-
ing the poor performances. For that reason, the
unigram language model classifier (or multinomial
naive Bayes text classifier) has been referred as an
alternative and promising naive Bayes by a num-
ber of researchers(McCallum and Nigam, 1998; Du-
mais et al, 1998; Yang and Liu, 1999; Nigam et
al., 2000). Although the unigram language model
classifiers usually outperform the pure naive Bayes,
they also have given the disappointing results com-
pared to many other statistical learning methods
such as nearest neighbor classifiers(Yang and Chute,
1994), support vector machines(Joachims, 1998),
and boosting(Schapire and Singer, 2000), etc.
In the real world, an operational text classifica-
tion system is usually placed in the environment
where the amount of human-annotated training doc-
uments is small in spite of the hundreds of thousands
classes. Moreover, re-training of the text classifiers
is required since a small number of new training
documents are continuously provided. In this envi-
ronment, naive Bayes is probably the most appropri-
ate model for the practical systems rather than other
complex learning models. Therefore, more inten-
sive studies about the naive Bayes text classification
model are required.
In this paper, we revisit the naive Bayes frame-
work, and propose a Poisson naive Bayes model for
text classification with a statistical feature weight-
ing method. Feature weighting has many advan-
tages compared to the previous feature selection ap-
proaches, especially when the new training exam-
ples are continuously provided. Our new model as-
sumes that a document is generated by a multivari-
ate Poisson model, and their parameters are esti-
mated by weighted averaging of the normalized and
smoothed term frequencies over all the training doc-
uments. Under the assumption, we have tested the
feature weighting approach with three measures: in-
formation gain, -statistic, and newly introduced
probability ratio. With the proposed model and fea-
ture weighting techniques, we can get much better
performance without losing the simplicity and effi-
ciency of the naive Bayes model.
The remainder of this paper is organized as fol-
lows. The next section presents a naive Bayes frame-
work for the text classification briefly. Section 3
describes our new naive Bayes model and the pro-
posed technique, and the experimental results are
presented in Section 4. Finally, we conclude the pa-
per by suggesting possible directions for future work
in Section 5.
2 Naive Bayes Text Classification
A naive Bayes classifier is a well-known and highly
practical probabilistic classifier, and has been em-
ployed in many applications. It assumes that all
attributes of the examples are independent of each
other given the context of the class, that is, an in-
dependent assumption. Several studies show that
naive Bayes performs surprisingly well in many do-
mains(Domingos and Pazzani, 1997) in spite of its
wrong independent assumption.
In the context of text classification, the probabil-
ity of class  given a document 

is calculated by
Bayes? theorem as follows:


 












  









 






   
(1)
Now, if we define a new function 

,


 






(2)
then, Equation (1) can be rewritten as


 



 



   
(3)
Using Equation (3), we can get the posterior prob-
ability 

 by obtaining 

, which is a form of
log ratio similar to the BIM retrieval model(Jones et
al., 2000). It means that the linked independence as-
sumption(Cooper et al, 1992), which explains that
the strong independent assumption can be relaxed
in the BIM model, is sufficient for the use of naive
Bayes text classification model.
With this framework, two representative naive
Bayes text classification approaches are well intro-
duced in (McCallum and Nigam, 1998). They desig-
nated the pure naive Bayes as multivariate Bernoulli
model, and the unigram language model classifier as
multinomial model. Instead, we introduce multivari-
ate Poisson model to improve the pure naive Bayes
text classification in the next section.
3 Poisson Naive Bayes Text Classification
3.1 Overview
The Poisson distribution is most commonly used to
model the number of random occurrences of some
phenomenon in a specified unit of space or time,
for example, the number of phone calls received by
a telephone operator in a 10-minute period. If we
think that the occurrence of each term is a random
occurrence in a fixed unit of space (i.e. a length
of document) the Poisson distribution is intuitively
suitable to model the term frequencies in a given
document. For that reason, the use of Poisson model
is widely investigated in the IR literature, but it is
rarely used for the text classification task(Lewis,
1998). It motivates us to adopt the Poisson model
for learning the naive Bayes text classification.
Our model assumes that 

is generated by multi-
variate Poisson model. In other words, a document


is a random vector which consists of the Poisson
random variables 

, and 

has the value of within-
term-frequency 

for the 	-th term 


. Thus, if we
assume the independence among the terms in 

, a
probability of 

is represented by,


 
 

 

 

 (4)
where,   is a vocabulary size, and each  




 is given by,
 

 

 



	



	
(5)
where,  is the Poisson mean.
As a result, the 

function of Equation (2) is
rewritten using Equations (4) and (5) as follows:



 



 

 


 

 



 








	







	

(6)
where, 

and 

is the Poisson mean for 


in class
 and class , respectively.
The most important issues of this work are as fol-
lows:
 How to decide the frequency 

representing
the characteristic of document 

?
 How to estimate the model parameter 

and 

representing the characteristic of each class?
We propose the possible answers in the next subsec-
tion.
3.2 Parameter Estimation
Since 

is a frequency of a term 	 in a document


with a fixed length according to the definition of
Poisson distribution, we should normalize the actual
term frequencies in the documents with the different
length. In addition, many earlier works in NLP and
IR fields recommend that smoothing term frequen-
cies is necessary in order to build a more accurate
model.
Thus, we estimate 

as the normalized and
smoothed frequency of actual term frequency 

,
represented by,







 


    
  (7)
where  is a laplace smoothing parameter,  is any
huge value which makes all the 


in our model an
integer value1, and 

is the length of 

.
Conceptually, 


can be regarded as the value es-
timated by the following steps : 1) Add  of all  
terms to the document 

, 2) Scale 

up to 

whose
total length is  without changing the proportion of
frequency for each term 


, 3) Count 


in 

.
Then, Poisson mean 

, which represents an aver-
age number of occurrence of 


in the documents be-
longing to class , is estimated using the normalized
and smoothed 


values over the training documents
as follows:












 




(8)
where 

is the set of training documents belonging
to class , and 


2 is the interpolation of the
uniform probability and the probability proportional
to the length of the document, and the interpolation
is calculated as follows:


  




   









(9)
Simple averaging of 


, the case of =1, seems to
be correct to estimate 

. However, the statistics
1Since 

is a value of random variable 

representing
the frequency in our Poisson distribution, we multiply the nor-
malized frequency with some unnatural constant  to make 

integer value. However,  is dropped in the final induced func-
tion.
2We use the notation 

 for the distribution defined
only in the training documents, to distinguish it from the no-
tation 

 used in the Section 2.
from the long documents can be more reliable than
those in the short documents, hence we try to inter-
polate between the two different probabilities with
the parameter  ranging from 0 to 1. Consequently,


is a weighted average over all training documents
belonging to the class , and 

for the class  can be
estimated in the same manner.
3.3 Feature Weighting
Feature selection is often performed as a preprocess-
ing step for the purpose of both reducing the fea-
ture space and improving the classification perfor-
mance. Text classifiers are then trained with various
machine learning algorithms in the resulting feature
space. (Yang and Pedersen, 1997) investigated some
measures to select useful term features including
mutual information(MI), information gain(IG), and


-statistics(CHI), etc. On the contrary, (Joachims,
1998) claimed that there is no useless term features,
and it is preferable to use all term features. It is
clear that learning and classification become very
efficient when the feature space is considerably re-
duced. However, there is no definite conclusion
about the contribution of feature selection to im-
prove overall performances of the text classification
systems. It may considerably depend on the em-
ployed learning algorithm. We believe that proper
external feature selection or weighting is required to
improve the performances of naive Bayes since the
naive Bayes has no framework of the discriminative
optimizing process in itself. Of the two possible ap-
proaches, feature selection is very inefficient in case
that the additional training documents are provided
continuously. It is because the feature set should
be redefined according to the modified term statis-
tics in the new training document set, and classifiers
should be trained again with this new feature set. For
that reason, we prefer to use feature weighting to
improve naive Bayes rather than feature selection.
With the feature weighting method, our 

is rede-
fined as follows:



 




W

 










	


 






	

(10)
where, 

is the weight of feature for the class ,
and W

is the normalization factor, that is,





.
In our work, three measures are used to weight
Table 1: Two-way contingency table
presence of 


absence of 


labeled as  a b
not labeled as  c d
each term feature: information gain, -statistics
and probability ratio. Information gain (or aver-
age mutual information) is an information-theoretic
measure defined by the amount of reduced uncer-
tainty given a piece of information. We use the in-
formation gain value as the weight of each term for
the class , and the value is calculated using a docu-
ment event model as follows:


   Two-Phase Biomedical NE Recognition based on SVMs
Ki-Joong Lee Young-Sook Hwang and Hae-Chang Rim
Department of Computer Science & Engineering
Korea University
1, 5-ka, Anam-dong, SEOUL, 136-701, KOREA
{kjlee, yshwang, rim}@nlp.korea.ac.kr
Abstract
Using SVMs for named entity recogni-
tion, we are often confronted with the
multi-class problem. Larger as the num-
ber of classes is, more severe the multi-
class problem is. Especially, one-vs-rest
method is apt to drop the performance by
generating severe unbalanced class distri-
bution. In this study, to tackle the prob-
lem, we take a two-phase named entity
recognition method based on SVMs and
dictionary; at the first phase, we try to
identify each entity by a SVM classifier
and post-process the identified entities by
a simple dictionary look-up; at the sec-
ond phase, we try to classify the seman-
tic class of the identified entity by SVMs.
By dividing the task into two subtasks, i.e.
the entity identification and the semantic
classification, the unbalanced class distri-
bution problem can be alleviated. Further-
more, we can select the features relevant
to each task and take an alternative classi-
fication method according to the task. The
experimental results on the GENIA cor-
pus show that the proposed method is ef-
fective not only in the reduction of train-
ing cost but also in performance improve-
ment: the identification performance is
about 79.9(F? = 1), the semantic clas-
sification accuracy is about 66.5(F? = 1).
1 Introduction
Knowledge discovery in the rapidly growing area of
biomedicine is very important. While most knowl-
edge are provided in a vast amount of texts, it is im-
possible to grasp all of the huge amount of knowl-
edge provided in the form of natural language. Re-
cently, computational text analysis techniques based
on NLP have received a spotlight in bioinformat-
ics. Recognizing the named entities such as proteins,
DNAs, RNAs, cells etc. has become one of the most
fundamental tasks in the biomedical knowledge dis-
covery.
Conceptually, named entity recognition consists
of two tasks: identification, which finds the bound-
aries of a named entity in a text, and classifi-
cation, which determines the semantic class of
that named entity. Many machine learning ap-
proaches have been applied to biomedical named
entity recognition(Nobata, 1999)(Hatzivalssiloglou,
2001)(Kazama, 2002). However, no work has
achieved sufficient recognition accuracy. One rea-
son is the lack of annotated corpora. This is some-
what appeased with announcement of the GENIA
corpus v3.0(GENIA, 2003). Another reason is that
it is difficult to recognize biomedical named entities
by using general features compared with the named
entities in newswire articles. In addition, since non-
entity words are much more than entity words in
biomedical documents, class distribution in the class
representation combining a B/I/O tag with a seman-
tic class C is so severely unbalanced that it costs too
much time and huge resources, especially in SVMs
training(Hsu, 2001).
Therefore, Kazama and his colleagues tackled the
problems by tuning SVMs(Kazama, 2002). They
splitted the class with unbalanced class distribution
into several subclasses to reduce the training cost.
In order to solve the data sparseness problem, they
explored various features such as word cache fea-
tures and HMM state features. According to their re-
port, the word cache and HMM state features made
a positive effect on the performance improvement.
But, not separating the identification task from the
semantic classification, they tried to classify the
named entities in the integrated process.
By the way, the features for identifying the
biomedical entity are different from those for se-
mantically classifying the entity. For example, while
orthographical characteristics and a part-of-speech
tag sequence of an entity are strongly related to the
identification, those are weakly related to the seman-
tic classification. On the other hand, context words
seem to provide useful clues to the semantic classifi-
cation of a given entity. Therefore, we will separate
the identification task from the semantic classifica-
tion task. We try to select different features accord-
ing to the task. This approach enables us to solve the
unbalanced class distribution problem which often
occurs in a single complicated approach. Besides, to
improve the performance, we will post-process the
results of SVM classifiers by utilizing the dictionary.
That is, we adopt a simple dictionary lookup method
to correct the errors by SVMs in the identification
phase.
Through some experiments, we will show how
separating the entity recognition task into two sub-
tasks contributes to improving the performance of
biomedical named entity recognition. And we will
show the effect the hybrid approach of the SVMs
and the dictionary-lookup.
2 Definition of Named Entity
Classification Problem
We divide the named entity recognition into two
subtasks, the identification task which finds the re-
gions of the named entities in a text and the semantic
classification which determines the semantic classes
of them. Figure 1 illustrates the proposed method,
which is called two-phase named entity recognition
method.
Figure 1: Examples of Biomedical Named Entity
Recognition
The identification task is formulated as classifica-
tion of each word into one of two classes, T or O
that represent region information. The region infor-
mation is encoded by using simple T/O representa-
tion: T means that current word is a part of a named
entity, and O means that the word is not in a named
entity. With the representation, we need only one
binary SVM classifier of two classes, T, O.
The semantic classification task is to assign one
of semantic classes to the identified entity. At the
semantic classification phase, we need to classify
only the identified entities into one of the N seman-
tic classes because the entities were already identi-
fied. Non-entity words are ignored at this phase. The
classes needed to be classified are just only the N se-
mantic classes. Note that the number of total classes,
N + 1 is remarkably small compared with the num-
ber, 2N + 1 required in the complicated recognition
approaches in which a class is represented by com-
bining a region information B/I/O with a semantic
class C. It can considerably reduce workload in the
named entity recognition.
Especially when using SVMs, the number of
classes is very critical to the training in the as-
pect of training time and required resources. Let
L be the number of training samples and let N be
the number of classes. Then one-vs-rest method
takes N ? O(L) in the training step. The com-
plicated approach with the B/I/O notation requires
(2N + 1)?O(Lwords) (L is number of total words
in a training corpus). In contrast, the proposed ap-
proach requires (N ? O(Lentities)) + O(Lwords).
Here, O(Lwords) stands for the number of words in
a training corpus and O(Lentities) for the number of
entities. It is a considerable reduction in the training
cost. Ultimately, it affects the performance of the
entity recognizer.
To achieve a high performance of the defined
tasks, we use SVM(Joachims, 2002) as a machine
learning approach which has showed the best perfor-
mance in various NLP tasks. And we post-process
the classification results of SVMs by utilizing a dic-
tionary. Figure 2 outlines the proposed two-phase
named entity recognition system. At each phase,
each classifier with SVMs outputs the class of the
best score. For classifying multi-classes based on a
binary classifier SVM, we use the one-vs-rest clas-
sification method and the linear kernel in both tasks.
Furthermore, for correcting the errors by SVMs,
the entity-word dictionary constructed from a train-
ing corpus is utilized in the identification phase. The
dictionary is searched to check whether the bound-
ary words of an identified entity were excluded or
not because the boundary words of an entity might
be excluded during the entity identification. If a
boundary word was excluded, then we concatenate
the left or the right side word adjacent to the iden-
tified entity. This post-processing may enhance the
capability of the entity identifier.
3 Biomedical Named Entity Identification
The named entity identification is defined as the
classification of each word to one of the classes that
represent the region information. The region infor-
mation is encoded by using simple T/O representa-
tion: T means that the current word is a part of a
named entity, and O means that the current word is
not in a named entity.
The above representation yields two classes of the
task and we build just one binary SVM classifiers for
them. By accepting the results of the SVM classifier,
we determine the boundaries of an entity. To correct
boundary errors, we post-process the identified enti-
ties with the entity-word dictionary.
3.1 Features for Entity Identification
An input x to a SVM classifier is a feature represen-
tation of a target word to be classified and its context.
We use a bit-vector representation. The features of
the designated word are composed of orthographi-
cal characteristic features, prefix, suffix, and lexical
of the word.
Table 1 shows all of the 24 orthographical fea-
tures. Each feature may be a discriminative fea-
ture appeared in biomedical named entites such as
protein, DNA and RNA etc. Actually, the name of
protein, DNA or RNA is composed by combining
alpha-numeric string with several characters such as
Greek or special symbols and so on.
Table 1: Orthographical characteristic features of
the designated word
Orthographic Feature examples
DIGITS 1 , 39
SINGLE CAP A , M
COMMA ,
PERIOD .
HYPHON -
SLASH /
QUESTION MARK ?
OPEN SQUARE [
CLOSE SQUARE ]
OPEN PAREN (
CLOSE PAREN )
COLON :
SEMICOLON ;
PERCENT %
APOSTROPHE ?
ETC SYMBOL +, *, etc.
TWO CAPS alphaCD28
ALL UPPER AIDS
INCLUDE CAPS c-Jun
GREEK LETTER NF-kappa
ALPHA NUMERIC p65
ALL LOWER motif
CAPS DIGIT CD40
INIT CAP Rel
And the suffix/prefix, the designated word and the
context word features are as follows:
wi =
?
??
??
1 if the word is the ith word
in the vocabulary V
0 otherwise
Figure 2: System Configuration of Two Phase Biomedical NE Recognition System
posi =
?
??
??
1 if the word is assigned the ith
POS tag in the POS tag list
0 otherwise
sufi =
?
??
??
1 if the word contains the
ith suffix in the suffix list
0 otherwise
prei =
?
??
??
1 if the word contains the
ith prefix in the prefix list
0 otherwise
wki =
?
??
??
1 if a word at k is the ith word
in the vocabulary V
0 otherwise
poski =
?
??
??
1 if a word at k is assigned the
ith POS tag in the POS tag list
0 otherwise
In the definition, k is the relative word position
from the target word. A negative value represents
a preceeding word and a positive value represents
a following word. Among them, the part-of-speech
tag sequence of the word and the context words is a
kind of a syntactic rule to compose an entity. And
lexical information is a sort of filter to identify an
entity which is as possible as semantically cohesive.
3.2 Post-Processing by Dictionary Look-Up
After classifying the given instances, we do post-
processing of the identified entities. During the post-
processing, we scan the identified entities and exam-
ine the adjacent words to those. If the part-of-speech
of an adjacent word belongs to one of the group, ad-
jective, noun, or cardinal, then we look up the dic-
tionary to check whether the word is in it or not. If it
exists in the dictionary, we include the word into the
entity region. The dictionary is constructed of words
consisting of the named entities in a training corpora
and stopwords are ignored.
Figure 3 illustrates the post-processing algorithm.
In Figure 3, the word cell adjacent to the left of the
identified entity cycle-dependent transcription, has
the part-of-speech NN and exists in the dictionary.
The word factor adjacent to the right of the entity
has the part-of-speech NN. It exists in the dictionary,
too. Therefore, we include the words cell and factor
into the entity region and change the position tags of
the words in the entity.
By taking the post-processing method, we can
correct the errors by a SVM classifier. It also gives
us a great effect of overcoming the low coverage
problem of the small-sized entity dictionary.
4 Semantic Classification of Biomedical
Named Entity
The objects of the semantic tagging are the entities
identified in the identification phase. Each entity is
assigned to a proper semantic class by voting the
SVM classifiers.
Figure 3: An example of the post-processing of an entity identification
4.1 Features for Semantic Classification
For semantically tagging an entity, an input x to a
SVM classifier is represented by a feature vector.
The vector is composed of following features:
fwi =
?
??
??
1 if a given entity contains one
of the functional words
0 otherwise
inwi =
?
??
??
1 if one of the words in the
entity is in the inside word list
0 otherwise
lcwi =
?
????
????
1 if noun or verb word in the
left context is the ith word
in the left context word list
0 otherwise
rcwi =
?
????
????
1 if noun or verb word in the
right context is the ith word
in the right context word list
0 otherwise
Of the above features, fwi checks whether the
entity contains one of functional words. The func-
tional words are similar to the feature terms used by
(Fukuda, 1998). For example, the functional words
such as factor, receptor and protein are very help-
ful to classifying named entities into protein and the
functional words such as gene, promoter and motif
are very useful for classifying DNA.
In case of the context features of a given entity, we
divide them into two kinds of context features, inside
context features and outside context features. As in-
side context features, we take at most three words
from the backend of the entity 1. We make a list of
the inside context words by collecting words in the
1The average length of entities is about 2.2 in GENIA cor-
pus.
range of the inside context. If one of the three words
is the ith word in the inside context word list, we set
the inwi bit to 1. The outside context features are
grouped in the left ones and the right ones. For the
left and the right context features, we restrict them
to noun or verb words in a sentence, whose position
is not specified. This grouping make an effect of al-
leviating the data sparseness problem when using a
word as a feature.
For example, given a sentence with the entity,
RNA polymerase II as follows:
General transcription factor are required
for accurate initiation of transcription by
RNA polymerase II PROTEIN .
The nouns transcription, factor, initiation and the
verbs are, required are selected as left context fea-
tures, and the words RNA, polymerase, II are se-
lected as inside context features. The bit field cor-
responding to each of the selected word is set to 1.
In this case, there is no right context features. And
since the entity contains the functional word RNA,
the bit field of RNA is set to 1.
For classifying a given entity, we build SVM clas-
sifiers as many as the number of semantic classes.
We take linear kernel and one-vs-rest classification
method.
5 Experiments
5.1 Experimental Environments
Experiments have been conducted on the GENIA
corpus(v3.0p)(GENIA, 2003), which consists of
2000 MEDLINE abstracts annotated with Penn
Treebank (PTB) POS tags. There exist 36 distinct
semantic classes in the corpus. However, we used
22 semantic classes which are all but protein, DNA
and RNA?s subclasses on the GENIA ontology 2.
The corpus was transformed into a B/I/O annotated
corpus to represent entity boundaries and a semantic
class.
We divided 2000 abstracts into 10 collections for
10-fold cross validation. Each collection contains
not only abstracts but also paper titles. The vo-
cabularies for lexical features and prefix/suffix lists
were constructed by taking the most frequent 10,000
words from the training part only.
Also, we made another experimental environ-
ment to compare with the previous work by
(Kazama, 2002). From the GENIA corpus, 590
abstracts(4,808 sentences; 20,203 entities; 128,463
words) were taken as a training part and 80 ab-
stracts(761 sentences; 3,327 entities; 19,622 words)
were selected as a test part. Because we couldn?t
make the experimental environment such as the
same as that of Kazama?s, we tried to make a com-
parable environment.
We implemented our method using the SVM-light
package(Joachims, 2002). Though various learning
parameters can significantly affect the performance
of the resulting classifiers, we used the SVM system
with linear kernel and default options.
The performance was evaluated by precision, re-
call and F?=1. The overall F?=1 for two models and
ten collections, were calculated using 10-fold cross
validation on total test collection.
5.2 Effect of Training Data Size
In this experiment, varying the size of training set,
we observed the change of F?=1 in the entity identi-
fication and the semantic classification. We fixed the
test data with 200 abstracts(1,921 sentences; 50,568
words). Figure 4 shows that the performance was
improved by increasing the training set size. As the
performance of the identification increases, the gap
between the performance of the identification and
that of the semantic classification is gradually de-
creased.
5.3 Computational Efficiency
When using one-vs-rest method, the number of
negative samples is very critical to the training in
2That is, All of the protein?s subclass such as pro-
tein molecule, protein family or group were regarded as pro-
tein.
Figure 4: Perfomance shift according to the increase
of training data size w/o post-processing
the aspect of training time and required resources.
The SVM classifier for entity identifiation deter-
mines whether each word is included in an entity
or not. Figure 5 shows there are much more nega-
tive samples than positive samples in the identifica-
tion phase. Once entities are identified, non-entity
words are not considered in next semantic classifi-
cation phase. Therefore, the proposed method can
effectively remove the unnecessary samples. It en-
ables us effectively save the training costs.
Furthermore, the proposed method could effec-
tively decrease the degree of the unbalance among
classes by simplifying the classes. Figure 6 shows
how much the proposed method can alleviate the un-
balanced class distribution problem compared with
1-phase complicated classification model. However,
even though the unbalanced class distribution prob-
lem could be alleviated in the identification phase,
we are still suffering from the problem in the seman-
tic classification as long as we take the one-vs-rest
method. It indicates that we need to take another
classification method such as a pairwise method in
the semantic classification(Krebel, 1999).
5.4 Discriminative Feature Selection
We subsequently examined several alternatives for
the feature sets described in section 3.1 and section
4.1.
The column (A) in Table 2 shows the identifica-
tion cases. The base feature set consisted of only the
designated word and the context words in the range
from the left 2 to the right 2. Several alternatives for
feature sets were constructed by adding a different
combination of features to the base feature set. From
Figure 5: training size vs. positive and negative sam-
ple size in identification phase and semantic classi-
fication phase
Figure 6: 2-phase model vs. 1-phase model : change
of the negative and the positive sample size accord-
ing to the training data size
( A ) ( B )
FeatSet F-score FeatSet F-score
base 74.6 base(inw) 65.8
pos 77.4 (+2.8) fw 67.9 (+2.1)
pre 75.0 (+0.4) lcw 67.9 (+2.1)
suf 75.2 (+0.6) rcw 67.0 (+1.2)
pre+suf 75.6 (+1.0) lcw+rcw 66.4 (+0.6)
pos+pre 77.9 (+3.3) fw+lcw 68.1(+2.3)
pos+suf 77.9 (+3.3) fw+rcw 67.1 (+1.3)
all 77.9 (+3.3) all 66.9 (+1.1)
Table 2: Effect of each feature set(training with 900
abstracts, test with 100 abstracts): (A) identification
phase, (B) semantic classification phase
Table 2, we can see that part-of-speech information
certainly improves the identification accuracy(about
+2.8). Prefix and suffix features made a positive ef-
fect, but only modestly(about +1.2 on average).
The column (B) in Table 2 shows semantic clas-
sification cases with the identification phase of the
best performance. We took the feature set composed
of the inside words of an entity as a base feature set.
And we made several alternatives by adding another
features. The experimental results show that func-
tional words and left context features are useful, but
right context features are not. Furthermore, part-of-
speech information was not effective in the seman-
tic classification while it was useful for the entity
identification. That is, when we took the part-of-
speech tags of inside context words instead of the
inside context words, the performance of the seman-
tic classification was very low(F?=1.0 was 25.1).
5.5 Effect of PostProcessing by Dictionary
Lookup
Our two-phase model has the problem that identifi-
cation errors are propagated to the semantic classi-
fication. For this reason, it is necessary to ensure
a high accuracy of the boundary identification by
adopting a method such as post processing of the
identified entities. Table 3 shows that the post pro-
cessing by dictionary lookup is effective to improv-
ing the performance of not only the boundary identi-
fication accurary(79.2 vs. 79.9) but also the seman-
tic classification accuracy(66.1 vs. 66.5).
When comparing with the (Kazama, 2002) even
though the environments is not the same, the pro-
posed two-phase model showed much better per-
formance in both the entity identification (73.6 vs.
81.4) and the entity classification (54.4 vs. 68.0).
One of the reason of the performance improvement
is that we could take discriminative features for each
subtask by separating the task into two subtasks.
6 Conclusion
In this paper, we proposed a new method of two-
phase biomedical named entity recognition based on
SVMs and dictionary-lookup. At the first phase, we
tried to identify each entity with one SVM classifier
and to post-process with a simple dictionary look-up
for correcting the errors by the SVM. At the second
Table 3: Performance comparison with or w/o post-processing(F?=1): (A)10-fold cross validation(1800
abstracts, test with 200 abstracts), (B)training with 590 abstracts, test with 80 abstracts
A B (Kazama, 2002)
No. of W/O PostProc with PostProc No. of W/O PostProc with PostProc No. of
Inst Inst Inst
Identification 76.2/82.4/79.2 76.8/83.1/79.9 78.4/80.8/79.6 80.2/82.6/81.4 75.9/71.4/73.6
Classification 63.6/68.8/66.1 64.0/69.2/66.5 65.8/67.9/66.8 67.0/69.0/68.0 56.2/52.8/54.4
protein 25,276 60.9/79.8/69.1 61.7/78.8/69.2 1,056 61.3/81.3/69.9 62.8/80.7/70.6 709 49.2/66.4/56.5
DNA 8,858 65.1/63.9/64.5 65.0/63.8/64.4 474 71.4/61.0/65.8 72.1/61.6/66.4 460 49.6/37.0/42.3
RNA 683 72.2/71.7/72.0 73.8/72.5/73.1 36 74.4/88.9/81.0 75.6/86.1/80.5
cell line 3,783 71.6/54.2/61.7 72.3/72.3/72.3 201 73.2/44.8/55.6 73.2/44.8/55.6 121 60.2/46.3/52.3
cell type 6,423 67.2/77.5/72.0 67.5/67.5/67.5 252 64.9/82.1/72.5 65.4/81.7/72.7 199 70.0/75.4/72.6
phase, we tried to classify the identified entity into
its semantic class by voting the SVMs. By dividing
the task into two subtasks, the identification and the
semantic classification task, we could select more
relevant features for each task and take an alternative
classification method according to the task. This is
resulted into the mitigation effect of the unbalanced
class distribution problem but also improvement of
the performance of the overall tasks.
References
N. Collier, C. Nobata, and J. Tsujii 2000. Extracting
the Names of Genes and Gene Products with a Hidden
Markov Model. In Proc. of Coling2000, pages 201-
207.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. 1998.
Information extraction: identifying protein nmes from
biological papers. In Proc. of the Pacific Symposium
on Biocomputing ?98(PSB?98).
GENIA Corpus 3.0p. 2003. available
at http://www-tsujii.is.s.u-tokyo.ac.jp/ ge-
nia/topics/Corpus/3.0/GENIA3.0p.intro.html
V. Hatzivalssiloglou, P. A. Duboue, and A. Rzhetsky.
2001. Disambiguating proteins, genes, and RNA in
text: a machine learning approach. Bioinformatics. 17
Supple 1.
C. Hsu and C. Lin. 2001. A comparison on methods for
multi-class support vector machines. Technical report,
National Taiwan University, Taiwan.
T. Joachims. 1998. Making Large-Scale SVM Learning
Practical. LS8-Report, 24, Universitat Dortmund, LS
VIII-Report.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proc. of the Seven-
teenth International Conference on Machine Learning.
Morgan Kaufmann, pages 431-438.
SVM Light. 2002. available at
http://svmlight.joachims.org/
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta and
Jun?ichi Tsujii. 2002. Tuning support vector machines
for biomedical named entity recognition. In Proc. of
ACL-02 Workshop on Natural Language Processing in
the Biomedical Domain, pages 1-8.
U. H.-G Krebel 1999. Pairwise Classification and Sup-
port Vector machines. In B. Scholkopf, C.J.C. Burges,
Advances in Kernel Methods: Support Vector Learn-
ing, pp. 255-268, The MIT Press, Cambridge, MA.
C. Nobata, N. Collier, and J. Tsujii. 1999. Automatic
term identification and classification in biology texts.
In Proc. of the 5th NLPRS, pages 369-374.
B.J. Stapley, L.A. Kelley, and M.J.E. Sternberg. 2002.
Predicting the Sub-Cellular Location of Proteins from
Text Using Support Vector Machines. In Proc. of Pa-
cific Symposium on Biocomputing 7, pages 374-385.
Vladimir Vapnik. 1998. Statistical Learning Theory Wi-
ley, New York.
A Practical QA System in Restricted Domains
Hoojung Chung, Young-In Song, Kyoung-Soo Han,
Do-Sang Yoon, Joo-Young Lee, Hae-Chang Rim
Dept. of Comp. Science and Engineering
Korea University
Seoul 136-701 Korea
{hjchung,sprabbit,kshan,yds5004,jylee,rim}@nlp.korea.ac.kr
Soo-Hong Kim
Dept. of Comp. Software Engineering
Sangmyung University
Chonan 330-720 Korea
soohkim@smuc.ac.kr
Abstract
This paper describes an on-going research for a
practical question answering system for a home
agent robot. Because the main concern of the QA
system for the home robot is the precision, rather
than coverage (No answer is better than wrong an-
swers), our approach is try to achieve high accuracy
in QA. We restrict the question domains and extract
answers from the pre-selected, semi-structured doc-
uments on the Internet. A named entity tagger and a
dependency parser are used to analyze the question
accurately. User profiling and inference rules are
used to infer hidden information that is required for
finding a precise answer. Testing with a small set of
queries on weather domain, the QA system showed
90.9% of precision and 75.0% of recall.
1 Introduction
During the last decade, automatic question-
answering has become an interesting research field
and resulted in a significant improvement in its
performance, which has been largely driven by
the TREC (Text REtrieval Conference) QA Track
(Voorhees, 2004). The best of the systems in the QA
Track is able to answer questions correctly 70% of
the time (Light et al, 2003). The 70% of accuracy
is, of course, high enough to surprise the researchers
of this field, but, on the other hand, the accuracy is
not enough to satisfy the normal users in the real
world, who expect more precise answers.
The difficulty of constructing open-domain
knowledge base is one reason for the difficulties of
open-domain question answering. Since question
answering requires understanding of natural lan-
guage text, the QA system requires much linguis-
tic and common knowledge for answering correctly.
The simplest approach to improve the accuracy of a
question answering system might be restricting the
domain it covers. By restricting the question do-
main, the size of knowledge base to build becomes
smaller.
This paper describes our restricted domain ques-
tion answering system for an agent robot in home
environment. One of the roles of the home agent
robot is to be able to answer the practical ques-
tions such as weather information, stock quote, TV
broadcasting schedule, traffic information etc. via a
speech interface. The agent should provide high-
precision answers, otherwise the users will not trust
the entire functions of the home agent robot, which
includes not only the ability of question answering
but also the speech interface for controlling home
appliances. That means no answer is preferred to
a wrong answer and the primary concern in our re-
search is improving the precision of the question an-
swering system.
In this paper, we present a question answering
system which is restricted to answer only to the
questions on weather forecasts 1, and provide some
experimental results of the restricted QA system.
To achieve the high accuracy, the QA system pro-
cesses the semi-structured text data on the Inter-
net and store it in the form of relational database.
The domain specific hand-coded ontology contain-
ing weather terms and cities is manually built for the
question analysis and the inference process.
The remainder of the paper is organized as fol-
lows. Section 2 describes the overall architecture
of the QA system. Section 3 describes the prac-
tical QA system. Section 4 evaluates the system
and reports the limitation of the QA system. Sec-
tion 5 compares our system with other QA systems.
Section 6 concludes with some directions for future
work.
2 Overall Architecture
The overall framework of the QA system is pre-
sented in Figure 1. The QA system consists of two
major parts; the IE (Information Extractor) engine
and the QA engine.
1We?ve developed the QA system for a TV broadcast sched-
ule domain as well, which is more complex to process than the
weather forecasts QA, but have not evaluated it yet. So, in this
paper, we present the system for weather forecasts only.
QA
Engine
DBMS
IE
Engine
Web Browser
Internet
(WWW)
Web Interface
Speech Processor
QA SYSTEM
Figure 1: Overall architecture of the Question Answering System
The IE engine consists of two parts; a web
crawler and a wrapper. The web crawler down-
loads the selected webpages from the website of the
Korea Meteorological Administration (KMA) every
hour. The website provides current weather con-
ditions and 7 day-forecasts for dozens of Korean
cities. The wrapper, which is a set of extraction
rules, is used to extract weather information from
the webpages . The extracted information is stored
in the database.
TheQA engine performs three-phase processing:
First, it analyzes natural language questions and
translates the questions into Structured Query Lan-
guage (SQL) statements. Second, the SQL queries
are directed to a DBMS to retrieve the answers in
the database. Finally, the result from the DBMS
is converted to natural language sentences for out-
put. Figure 2 depicts overall processes for the QA
engine. A DBMS (Currently, Oracle Database) is
used for managing extracted data. A speech proces-
sor can be merged with the system when it is used
in the home agent robot, which provides speech in-
terface. A web interface is used for providing web-
based QA service with the QA system.
3 A Practical QA System
The question answering starts from extracting
weather information from the web site. The user
request is analyzed with the question analyzer and
the appropriate query frame is selected, and then the
request is translated into the SQL expression. The
SQL query is used to retrieve the correct answer
from the database, which stores weather informa-
tion from the webpages. Finally, natural language
Question
Analyzer
Named Entity
Tagger
Temporal Data
Normalizer
Query Frame
Classifier
Keywords
Natural Language
Question
SQL
Generator
Query
Frame
User Profile
SQL
Query
Inference
Rules
NL Answer
Generator
Query
Result
Natural Language
Answer
QA Engine
DBMS
Figure 2: The QA Engine
answer is generated based on the every result ex-
tracted from the DBMS.
3.1 Information Extraction
The weather information in the webpages is semi-
structured. Semi-structured resources generally do
not employ unrestricted natural language text, but
rather exhibit a fair degree of structure (Kushmer-
ick, 1997). Therefore, information can be accu-
rately and easily extracted from the webpage, com-
pared to IE from unstructured data.
On the other hand, semi-structured resources
are usually formatted for use by people, and con-
tain irrelevant elements that must be ignored, such
as images, advertisements, and HTML formatting
tags (Figure 3). Thus information extraction from
the semi-structured documents is not entirely triv-
ial. Currently, the QA system is using hand-coded
wrappers. However, we are developing an auto-
matic process of constructing wrappers (wrapper in-
duction) for semi-structured resources and that can
detect the modification of the web page design and
adapt the wrapper according to the modification, au-
tomatically, like (Sigletos et al, 2003).
Presently, the IE engine extracts following infor-
mation :
? Current observation: weather summary, visi-
bility, temperature, wind, relative humidity
? 7 days-forecasts : weather summary, forecast
temperature (highest/lowest).
3.2 Question Analysis
First, user?s request is analyzed with the query an-
alyzer as represented in Figure 2. The analyzer ex-
tracts several keywords that describing the question,
such as event word, date, time, and location, by us-
ing a dependency parser, and the user question is
represented only by these extracted keywords.
The named entity tagger is used to identify tem-
poral expressions, place names, and weather events.
The tagger consults the domain-dependent ontology
for recognizing weather events, and the domain-
independent ontology for place names. The ontol-
ogy for the weather events consists of event con-
cepts, which are similar to Synset in WORDNET
(Fellbaum, 1998). For example, rain and umbrella
are in same event concept in the domain ontology
for weather events, because the questions about us-
ing umbrella are usually asking about raining (e.g.
Will I need to bring umbrella tomorrow? and Will it
be raining tomorrow?)
The temporal data normalizer converts temporal
expressions such as today, this weekend and now
into absolute values that can be used in querying to
the database.
Seoul, March. 11., wide spread dust, (-/-)
Seoul, March. 12., cloudy, (0/11)
Seoul, March, 13., Sunny, (1/11)
...
Figure 3: Wrappers extracts weather information
from the semi-structured documents
If the information on date, time, or location is
not expressed in the user?s request, the question an-
alyzer infers the missing information. The infer-
ence rules, which are built based on our observation
on various user questions, are domain-independent,
because the omission of temporal or spatial infor-
mation is common not only in weather information
question, but also in questions for other domains.
The user profile is used for the inference in
query analysis. We observed many people omit the
place name in the weather-domain question. Unlike
the temporal information, it is impossible to guess
the current location without any user information.
Thus, we store some user-related information in the
user profile. Portfolio of stocks or favorite TV pro-
grams can be stored in the user profile if the QA sys-
tem processes queries on stock quote or TV sched-
ule domain.
Let?s take an example of the query analysis. The
following keywords are extracted from the question
?Is it raining??
EVENT : rain
DATE : 03/12/04
TIME : 02:20
CITY : Seoul
Even though the time, date, and city is not explic-
itly mentioned in the question, the question analyzer
infers the information with the user profile and the
inference rules.
3.3 Query Frame Decision
Restricting the question domain and information re-
source, we could restrict the scope of user request.
That is, there is a finite number of expected ques-
tion topics. Each expected question topic is defined
as a single query frame. The following are query
frame examples. They are used for processing the
query for the precipitation forecast for the next day,
diurnal range of today, current wind direction, and
current temperature, respectively.
[PRECIPITATION_TOMORROW]
[DIURNALRANGE_TODAY]
[WINDDIRECTION_CURRENT]
[TEMPERATURE_CURRENT]
Each frame has a rule for SQL generation. PRE-
CIPITATION TOMORROW has the following
SQL generation rule.
[PRECIPITATION_TOMORROW]
SELECT date, amprecpr, pmprecpr FROM
forecast tbl WHERE $date $city
date, amprecpr and pmprecpr are field names in the
database table forecast tbl, which mean date, the
precipitation probability of morning and afternoon
of the day. The rule generates the SQL statement
that means: retrieve the precipitation probability of
tomorrow morning and afternoon from the DB table
which stores forecast information.
Here is another example, which is the SQL gen-
eration rule for [DIURNALRANGE TODAY].
[DIURNALRANGE_TODAY]
SELECT city, max(temp)-main(temp) FROM
current tbl WHERE $date $city group by city
Analyzing a question means selecting a query
frame in this system. Thus, it is important to se-
lect the appropriate query frame for the user request.
The selection process is a great influence on the pre-
cision of the system, while there is not much likeli-
hood of errors in other processes, such as generating
SQL query from the selected query frame, retriev-
ing DB records, and generating an answer.
As represented in Figure 2, the extracted event,
temporal and spatial keywords are used for selecting
an appropriate query frame. Currently, we are us-
ing a hand-coded decision tree-like classifier for se-
lecting an appropriate query frame for the extracted
keywords. If a question isn?t proper for the handling
Is it raining?
?
EVENT : rain
DATE : 03/12/04
TIME : 02:20
CITY : Seoul
?
The frame [RAIN_CURRENT] is selected.
?
SELECT time, city, weather FROM current tbl
WHERE time=?03/12/04/0200?, city=?Seoul?
Figure 4: Interpreting the natural language question
to the SQL query
SELECT time, city, weather FROM current tbl
WHERE time=?03/12/04/0200?, city=?Seoul?
?
DBMS returns ?03/12/04/0200 Seoul Sunny?
?
On 2:00 p.m., Seoul is sunny.
Figure 5: Answer generation from the result of
query
domain, the classifier rejects it. Machine learned
classifier is being developed in order to substitute
for the hand-coded classifier.
3.4 SQL Generation
If a query frame is selected for a question, an SQL
query statement is generated from the SQL pro-
duction rule of the frame. The query is sent to
the DBMS to acquire the records that match to the
query. Figure 4 depicts the conversion from a natu-
ral language question to its SQL expression.
3.5 Answer Generation
Based on the result of the DBMS, a natural lan-
guage answer is generated. We use a rule based
answer generation method. Each query frame has
an answer generation pattern for the frame. For
example, DIURNALRANGE TODAY has the
following generation pattern.
[DIURNALRANGE_TODAY]
The diurnal temperature range of $date($1) is $2?C
$1 and $2 are the the first and second field value of
the queried result. $date() is the function that con-
verts a normalized date expression to a natural lan-
guage expression. Figure 5 shows the answer gener-
ated from the SQL query shown in Figure 4 (More
sample outputs from the QA System are presented
on the Appendix) .
4 Evaluation and Limitation
We have evaluated our domain restricted QA sys-
tem based on precision and recall, and investigated
the limitation of the our approach to the restricted-
domain QA system.
For evaluation, we?ve collected 50 weather ques-
tions from 10 graduate students. Precision and re-
call rates are 90.9 % and 75.0% respectively.
The low recall rate is due to some questions re-
lated to invalid date and topic. The system provides
weather forecasts for 7 days from the querying day.
But some of queries are asking for a future weather
outlook which is out of range ( e.g. Will it be very
hot summer this year? or Will it be snow on this
Christmas?). Some questions asked the information
that the database doesn?t contain, such as UVI (ul-
traviolet index).
The primary reason for the wrong answer is the
failure of invalid topic rejection. It is due to the
insufficient of weather-domain ontology data. For
example, from the question What is the discom-
fort index calculated from the today?s weather?,
the keyword discomfort index was not extracted
while weather was extracted, because the former
was not in the ontology. So the query frame
WEATHER TODAY was misselected and the sys-
tem generated the wrong answer Seoul will be sunny
on March 9th 2004.
An error was caused by the flaw of our keyword-
based query frame decision approach. For the ques-
tion Can the flight for Jeju Island take off today?,
the extracted keywords are
EVENT : flight take_off
DATE : 03/12/04
CITY : Jeju
In order to know whether the flight can take off
or not, the weather information of the departure city
instead of the destination city (i.e. Jeju) should be
returned, but our keyword based approach failed to
make an appropriate query. To solve this problem,
more sophisticated semantic representation, rather
than the sequence of keywords, is required for the
question.
5 Related Works
In this section, we compare our system with other
QA-related approaches and briefly describe the dis-
tinctive characteristics of our system. Open-domain
QA systems in QA track mostly extract answers
from unstructrued documents. In the contrast, our
system extracts answers from semi-structured web
pages, which are pre-selected by us, because our
system aims to achieve high precision with the sac-
rifice of the coverage of questions.
Natural language front ends for databases
(Copestake and Jones, 1990) and our system handle
user questions similarly. However, our system has
information extraction part that makes the database
be updated regularly and automatically. Moreover,
our system returns natural language responses to
users.
The START system (Katz, 1997) is a web-based
QA system. It uses World Wide Web as knowledge
resource. Unstructured natural language sentences
are indexed in the form of ternary expressions and
stored in RDB. The START system covers much
wider domain of questions than ours, however, it
seems that the system returns more wrong answers
than ours, because we extract the answer only from
semi-structured documents.
The Jupiter system (Zue et al, 2000) is a con-
versational system that provides weather informa-
tion over the phone. Based on the Galaxy architec-
ture (Goddeau et al, 1994), Jupiter recognizes user
question over the phone, parses the question with
the TINA language understanding system (Seneff,
1992) and generates SQL and natural language an-
swer with the GENESIS system (Baptist and Sen-
eff, 2000). The generated answer is synthesized
with the ENVOICE system. Even the Jupiter system
deals with the same domain, ours can process a bit
wider-range of weather topics. Our QA system can
cover the question which requires inferences such
as When is the best day for washing my car in this
week? Moreover, our system has an ability of infer-
ring missing information from the user profile and
the inferring algorithm.
6 Conclusion
This paper describes the practical QA system for re-
stricted domains. To be practically used, our sys-
tem tries to achieve high precision at the sacrifice of
question coverage.
To achieve high accuracy, we pre-designate semi-
structured information resource webpages and ex-
tracted domain-specific information from them. We
also prepare a domain-specific ontology and query
frames for the question analysis. The user?s request
in natural language is converted into SQL expres-
sion to generate an answer for the question. Testing
with a small set of queries on weather domain, the
QA system showed 90.9% of precision and 75.0%
of recall. By restricting the coverage of questions,
our system could achieve relatively high precision.
However, the figures are not enough for a real prac-
tical system.
Question 
Analyzer
for Domain 2
Question 
Analyzer
for Domain 1
Question 
Analyzer
for Domain n
Domain
Classifier
SQL Generator
for Domain 1
SQL Generator
for Domain 2
SQL Generator
for Domain n
.
.
.
.
.
.
.
.
.
Natural Language
Question
Query Frame
Classifier for
Domain 1
Query Frame
Classifier for
Domain 2
Query Frame
Classifier for
Domain n
QA Engine
for Domain 1
QA Engine
for Domain 2
QA Engine
for Domain n
Figure 6: A domain classifier selects a proper re-
stricted domain QA engine
Much work is left for our future work. First,
we are expanding the domain for the system. A
domain classifier will be added to the QA sys-
tem to process multiple-domain questions, as rep-
resented in Figure 6. We will separate domain de-
pendent resources (query frames, ontology contain-
ing domain-dependent information, and etc.) and
domain independent resources (linguistic resources,
and ontology for domain-independent information)
to allow easier domain expansion.
Second, the information extractor has to be up-
graded. Currently, the QA system is using hand-
coded wrappers, and the wrappers cannot extract
necessary information robustly when the webpages
are modified. We are developing an information ex-
tractor that can recognize the modification of the
webpages and modify the wrappers automatically.
The upgraded information extractor will improve
the robustness of our system.
Finally, we will increase the size of ontology to
cover more question types. From the experimenta-
tion, we realize that a larger ontology for weather
terms is necessary to classify a question correctly.
It seems more query frames are necessary for more
proper answers to the users? requests.
References
L. Baptist and S. Seneff. 2000. Genesis-II: A ver-
stile system for language generation in conversa-
tional system applications. In Proceedings of In-
ternational Conference on Spoken Language Pro-
cessing, October.
A. Copestake and K. Sparck Jones. 1990. Natural
language interfaces to databases. The Knowledge
Engineering Review, 5(4):225?249.
C. Fellbaum. 1998. WordNet: an Electronic Lexi-
cal Database. The MIT Press.
D. Goddeau, E. Brill, J. Glass., C. Pao, M. Phillips,
J. Polifrini, S. Seneff, and V. Zue. 1994. Galaxy:
A human language interface to on-line travel in-
formation. In Proceedings of International Con-
ference on Spoken Language Processing, pages
707?710.
B. Katz. 1997. Annotating the World Wide Web
using natural language. In Proceedings of the 5th
RIAO conference on Computer Assisted Informa-
tion Searching on the Internet.
N. Kushmerick. 1997. Wrapper Inductin for In-
formation Extraction. Ph.D. thesis, University of
Washington.
M. Light, A. Ittycheriah, A. Latto, and N. Mac-
Cracken. 2003. Reuse in question answering: A
preliminary study. In New Directions in Question
Answering: Papers from the 2003 AAAI Sympo-
sium, pages 78?86.
S. Seneff. 1992. Tina: A natural language system
for spoken language applications. Computational
Linguistics, 18(1):pp. 61?86.
G. Sigletos, G. Paliouras, C. D. Spyropoulos, and
M. Hatzopoulos. 2003. Mining web sites us-
ing wrapper induction, named entities and post-
processing. In 1st European Web Mining Forum
Workshop at ECML/PKDD-2003.
Ellen .M. Voorhees. 2004. Overview of the TREC
2003 question answering track. In Proceedings
of the 12th Text Retrieval Conference.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao,
T. J. Hazen, and L. Hetherington. 2000. Jupiter:
A telephone-based conversational interface for
weather information. IEEE Transactions on
Speech and Audio Processing, pages 100?112.
Appendix: Example of QA
Q: Is it cold outside?
A: The temperature of Seoul at 1 p.m. is 11? C.
Q: Will it be cold today?
A: The highest temperature of Seoul on March 9th
2004, will be 12? C.
Q: What?s the diurnal temperature range of today?
A: The diurnal temperature range of Seoul is 1?C.
Q: I am planing a picnic to Yongin on these week-
ends, and which day would be better for the
picnic?
A: Yongin will be sunny on March 3rd 2004. Yon-
gin will be partly cloudy on March 4th 2004.
Q: What is the ultraviolet index?
A: The system cannot answer to that topic.
Q: Can I have car wash today?
A: The precipitation probability of Seoul this after-
noon is 10%.
KUNLP System in SENSEVAL-3
Hee-Cheol Seo, Hae-Chang Rim
Dept. of Computer Science
and Engineering,
Korea University
1, 5-ka, Anam-dong, Seongbuk-Gu,
Seoul, 136-701, Korea
 hcseo, rim@nlp.korea.ac.kr
Soo-Hong Kim
Dept. of Computer Software Engineering,
College of Engineering,
Sangmyung University,
San 98-20, Anso-Dong,
Chonan, Chungnam, Korea
soohkim@smuc.ac.kr
Abstract
We have participated in both English all words
task and English lexical sample task of SENSEVAL-
3. Our system disambiguates senses of a target
word in a context by selecting a substituent among
WordNet relatives of the target word, such as syn-
onyms, hypernyms, meronyms and so on. The deci-
sion is made based on co-occurrence frequency be-
tween candidate relatives and each of the context
words. Since the co-occurrence frequency is obtain-
able from raw corpus, our method is considered to
be an unsupervised learning algorithm that does not
require a sense-tagged corpus.
1 Introduction
At SENSEVAL-3, we adopted an unsupervised ap-
proach based on WordNet and raw corpus, which
does not require any sense tagged corpus. Word-
Net specifies relationships among the meanings of
words.
Relatives of a word in WordNet are defined as
words that have a relationship with it, e.g. they
are synonyms, antonyms, superordinates (hyper-
nyms), or subordinates (hyponyms). Relatives, es-
pecially those in a synonym class, usually have
related meanings and tend to share similar con-
texts. Hence, some WordNet-based approaches ex-
tract relatives of each sense of a polysemous word
from WordNet, collect example sentences of the rel-
atives from a raw corpus, and learn the senses from
the example sentences for WSD. Yarowsky (1992)
first proposed this approach, but used International
Roget?s Thesaurus as a hierarchical lexical database
instead of WordNet. However, the approach seems
to suffer from examples irrelevant to the senses of
a polysemous word since many of the relatives are
polysemous. Leacock et al (1998) attempted to ex-
clude irrelevant or spurious examples by using only
monosemous relatives in WordNet. However, some
senses do not have short distance monosemous rel-
atives through a relation such as synonym, child,
and parent. A possible alternative of using only
monosemous relatives in the long distance, how-
ever, is problematic because the longer the distance
of two synsets in WordNet, the weaker the relation-
ship between them. In other words, the monose-
mous relatives in the long distance may provide ir-
relevant examples for WSD.
Our approach is somewhat similar to the Word-
Net based approach of Leacock et al (1998) in that
it acquires relatives of a target word from WordNet
and extracts co-occurrence frequencies of the rela-
tives from a raw corpus, but our system uses poly-
semous as well as monosemous relatives. To avoid
a negative effect of polysemous relatives on the co-
occurrence frequency calculation, our system han-
dles the example sentences of each relative sepa-
rately instead of putting together the example sen-
tences of all relatives into a pool. Also we devised
our system to efficiently disambiguate senses of all
words using only co-occurrence frequency between
words.
2 KUNLP system
2.1 Word Sense Disambiguation
We disambiguate senses of a word in a context1
by selecting a substituent word from the WordNet2
relatives of the target word. Figure 1 represents a
flowchart of the proposed approach. Given a target
word and its context, a set of relatives of the target
word is created by searches in WordNet. Next, the
most appropriate relative that can be substituted for
the word in the context is chosen. In this step, co-
occurrence frequency is used. Finally, the sense of
the target word that is related to the selected relative
is determined.
The example in Figure 2 illustrates how the pro-
posed approach disambiguates senses of the tar-
get word chair given the context. The set of rel-
atives  president, professorship, ... of chair is
built by WordNet searches, and the probability,
1In this paper, a context indicates a target word and six
words surrounding the target word in an instance.
2The WordNet version is 1.7.1.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Context Target Word
Context
Words
Surrounding
Target Word
Acquire
Set of Relatives
Select
a Relative
Determine
a Sense
WordNet
Co-occurrence
Information
Matrix
Sense
Figure 1: Flowchart of KUNLP System
?  	
,? that a relative can
be substituted for the target word in the given con-
text is estimated by the co-occurrence frequency be-
tween the relative and each of the context words. In
this example, the relative, seat, is selected with the
highest probability and the proper sense, ?a seat for
one person, with a support for the back,? is chosen.
Thus, the second step of our system (i.e. selecting
a relative) has to be carefully implemented to select
the proper relative that can substitute for the target
word in the context, while the first step (i.e. acquir-
ing the set of relatives) and the third step (i.e. deter-
mining a sense) are done simply through searches in
WordNet.
The substituent word of the -th target word 
 
in a context 	 is defined to be the relative of 
 
which has the largest co-occurrence probability with
the words in the context:
  
 
 	
 
 

 
   

 
	 (1)
where  is the substituent word, 
 
is the -th
relative of 
 
, and 
 
is the -th sense related to

 
3
. If  is 2, the 2-nd sense of 
 
is related to

 
. The right hand side of Equation 1 is calculated
with logarithm as follows:


 
   

 
	
 

 
   	

 
   

 

   	
 

 
   	

 
   

 

 

 
    	

 
     

 
 (2)
3
  is a function with two parameters 
 
and 
 
, but it can
be written in brief without parameters.
Instance :
    He should sit in the chair beside the desk.
Target Word :
    'chair'
Context :
    sit in the chair beside the desk
Set of Relatives :
    {professorship, president, chairman,
     electronic chair, death chair, seat,
     office, presiding officer, ...}
Probability of Relative given the Context :
    P( professorship | Context )
    P( president | Context )
    ...
    P( seat | Context )
    ...
Selected Relative :
    'seat' - it is the most likely word occurred
                from the above context among
                the relatives of 'chair'
Determined Sense :
    chair%1:06:00 - "a seat for one person,
                                with a support for the back."
    'seat' - the hypernym of chair%1:06:00.
Figure 2: Example of sense disambiguation proce-
dure for chair
Then Equation 2 may be calculated under the as-
sumption that words in 	 occur independently:


 
    	

 
     

 

 

 
 

 

   



 
     

 
 (3)
where 

is the -th word in 	 and 
 is the number
of words in 	 . In Equation 3, we assume indepen-
dence among words in 	 .
The first probability in Equation 3 is calculated as
follows:
   



 

    


 


   
 


   


   
 

(4)
The second probability in Equation 3 is computed
as follows:
   

 
   

 
   
 
 (5)
where  
 
 is the ratio of the frequency of 
 
to
that of 
 
:
 

 
 
 

 
  	


  	
   
 

where  
 
 is the frequency of 
 
in Word-
Net,  
 
 is the frequency of 
 
in WordNet,
0.5 is a smoothing factor, and 
 is the number of
senses of 
 
.
Applying Equations 4 and 5 to Equation 3, we
have the following equation for acquiring the rela-
tive with the largest co-occurrence probability:


 
   

 
	
 

 

 


   
 


   


   
 

  

 
   
 

 

 

 


   
 



   
 

  

 
   
 

In the case that several relatives have the largest
co-occurrence probability, all senses related to the
relatives are determined as proper senses.
2.2 Co-occurrence Frequency Matrix
In order to select a substituent word for a target
word in a given context, we must calculate the
probabilities of finding relatives, given the con-
text. These probabilities can be estimated based on
the co-occurrence frequency between a relative and
context words as follows:
   
 
 
 
 

	
(6)
   
 


 
   
 
 


   



 
 
 


 


(7)
where  
 
 is the frequency of 
 
, 	 is the
corpus size,    
 
 

 is the probability that 
 
and 

co-occur, and  
 
 

 is the frequency
that 
 
and 

co-occur.
In order to calculate these probabilities, frequen-
cies of words and word pairs are required. For this,
we build a co-occurrence frequency matrix that con-
tains co-occurrence frequencies of words pairs. In
this matrix, an element 76
77
78
79
Semantic Role Labeling using Maximum Entropy Model
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, Hae-Chang Rim
Department of Computer Science & Engineering Korea University
5-ka, Anam-dong, SEOUL, 136-701, KOREA
{jhlim, yshwang, ssoya, rim}@nlp.korea.ac.kr
Abstract
In this paper, we propose a semantic role label-
ing method using a maximum entropy model,
which enables not only to exploit rich features
but also to alleviate the data sparseness prob-
lem in a well-founded model. For applying the
maximum entropy model to semantic role la-
beling, we take a incremental approach as fol-
lows: firstly, the semantic roles are assigned to
the arguments in the immediate clause includ-
ing a predicate, and then, the semantic roles are
assigned to the arguments in the upper clauses
by using previously assigned labels. The exper-
imental result shows that the proposed method
has about 64.76% (F1-measure) on the test set.
1 Introduction
The semantic role represents the relationship between a
predicate and an argument. It provides a general semantic
interpretation of the sentence, and it can play a key role
in NLP. The shared task of CoNLL-2004 concerns the
automatic semantic role labeling (Carreras, 2004). The
challenge for this task is to come forward with machine
learning approaches which based on only partial syntactic
information such as words, POS tags, chunks, clauses,
and named entities.
Some machine learning approaches for semantic role
labeling have been previously developed (Gildea, 2002;
Pradhan, 2003; Thompson, 2003). Gildea (2002) pro-
posed a probabilistic discriminative model to assign a
semantic roles to the constituent. However, it needs a
complex interpolation for smoothing because of the data
sparseness problem. Pradhan (2003) applied a support
vector machine to semantic role labeling, but if it use
a polynomial kernel function for the dependencies be-
tween features, it requires high computational complex-
ity. Futhermore, becuase the SVM is a binary classifier,
one-vs-rest or pairwise method is required for multi-class
classification. Thompson (2003) proposed a probabilis-
tic generative model which the constituents is generated
by the semantic roles. In this model, because a con-
stituent depends only on the role that generated it, and
constituents are independent of each other, so this model
can not utilize contextual information or a relational in-
formation between the constituent and the predicate.
In this paper, we propose a semantic role labeling
method using a maximum entropy model. It is moti-
vated by the thought of that for building a successful
model, some knowledge of the task are reflected into
the model based on the machine learning technique. In
this method, we try to combine the structural linguistic
knowledge linking syntax to semantics into the machine
learning technique. It is realized in terms of two aspects:
one is the model framework, the other is the design of
feature sets. First of all, for the model framework, we uti-
lize the syntactic knowledge of representing the semantic
roles in a clause: the arguments of a predicate are located
in the immediate clause or the upper clauses. Secondly,
for the feature sets, we consider the relation between syn-
tactic and semantic characteristics of a given context. For
implementing the method with a machine learning algo-
rithm, we take a maximum entropy model, which enables
not only to exploit rich features but also to alleviate the
data sparseness problem in a well-founded model.
The remaining of the paper is organized as follows:
section 2 describes the proposed semantic role labeling
method using a maximum entropy model. Section 3
presents feature sets for semantic role labeling. Section 4
shows some experimental results of the proposed method.
Finally, section 5 concludes with some directions of fu-
ture works.
2 Semantic Role Labeling using ME
In the maximum entropy framework (Berger, 1996), the
conditional probability of predicting an outcome y given
Figure 1: An example of the semantic role labels and an incremental approach.
a history x is defined as follows :
P (y|x) =
1
Z(x)
exp
(
k
?
i=1
?
i
f
i
(x, y)
)
where f
i
(x, y) is the feature function, ?
i
is the weighting
parameter of f
i
(x, u), k is the number of features, and
Z(x) is the normalization factor for
?
y
p(y|x) = 1.
Given a predicate and its partial parse tree represented
by constituents such as chunks and clauses, the proba-
bilistic model for semantic role labeling assigns the se-
mantic role labels to the constituents as described in the
equation (1).
R
best
= argmax
R
P (R|c
1n
, pred)
= argmax
R
?
n
i=1
P (r
i
|c
1n
, pred, r
1...i?1
) (1)
where R is a sequence of the semantic roles, c
1n
is a se-
quence of constituents, pred is the given predicate, r
i
is
the i-th semantic role, n is the number of constituents.
In order to apply the equation (1) to an incremental
approach, we classify clauses into the immediate clause
and the upper clause. The immediate clause is the clause
which contains the target predicate, and the upper clause
is the clause which includes the immediate clause. Gen-
erally, most of the arguments of the predicate are located
in the immediate clause while some of them are located
in the upper clauses, especially the first or second up-
per clauses. Since it is much easier and more reliable to
identify the arguments in the immediate clause, the pro-
posed method first assigns the semantic role labels to the
constituents 1 in the immediate clause. Then, it assigns
the semantic role labels to the constituents in the upper
clauses by using previously assigned labels. This incre-
mental approach is described in the equation (2) derived
from the equation (1).
R
best
= argmax
R
?
n
i=1
P (r
i
|c
1n
, pred, r
1...i?1
)
? argmax
R
?
m
i=1
P (r
i
|?
1
(c
1n
, pred, r
1...i?1
))
?
?
n
i=m+1
P (r
i
|?
2
(c
1n
, pred, r
1...i?1
)) (2)
1Here, we regard a chunk or a clause as a constituent.
where m is the number of constituents covered by the im-
mediate clause, ?
1
is a feature set for immediate clause,
and ?
2
is a feature set for upper clauses.
A semantic role label(r
i
) is represented by using a BIO
notation such as B-A*, I-A*, etc. However, O is too fre-
quently occurred than other semantic role labels, it can
have a somewhat high probability than others. Therefore,
to degrade its probability, we divide the single O into O-,
O+, O0with respect to the position of a constituent which
is relative to the predicate. Therefore, B-A*, I-A*, O-,
O+, and O0 are used as semantic roles as shown in Fig-
ure 1.
After processing the equation (2), we use some heuris-
tic to attach the some semantic roles and to adjust the
boundary of semantic arguments in the post-processing
step. More specifically, we use some rules to attach the
V, AM-MOD, and AM-NEG, and extend the boundary
of core roles to include to infinitive of the VP chunk like
?expect/B-VP (A1 to/I-VP take/I-VP dive/B-NP)?.
3 Feature Sets for Semantic Role Labeling
For accurate semantic role labeling, we regard that the
following information is important: the contextual infor-
mation of the constituent, the syntactic information of the
predicate, and the relation between the constituent and
the predicate. Therefore, we use the features presented in
Table 1 for semantic role labeling. For example, Figure
previous-label(pl)
predicate-POS(predpos), predicate-lex(predlex)
predicate-type(predtype)
tag(ctag), voice(v), position(p), path(path)
head-lex(hl), head-POS(hp), content-head(chl)
prev-tag(ptag), prev-head-lex(phl)
next-tag(ntag), next-head-lex(nhl)
path-immediate-clause(path-im-cl)
path-begin-end(path-beg-end)
level-of-clause(l-cl), is-clause-boundary(cl-bn)
immediate-clause-roles(im-cl-roles)
Table 1: Features for semantic role labeling.
Figure 2: Some instances extracted from example of Figure 1.
feature set ?
1
for immediate clause feature set ?
2
for upper clause
pl, ctag, ctag+v+p, ctag+v+p+pl pl, ctag, ctag+v+p
ptag+ctag, ctag+ntag ptag+ctag, ctag+ntag
hp+p, hp+p+ntag hp+p, hl+ctag,
predtype+ctag predtype+ctag
predlex+hl, predlex+ctag+v+p, predlex+ctag+pl predlex+hl, predlex+ctag+v+p, predlex+ctag+pl
predpos+p, predpos+hp+pl, predpos+ctag
path, path+hp+v, path+nhl, path+predlex path-im-cl, path-im-cl+ctag+v, path-beg-end
hl+p, hl+ctag, hl+ctag+predlex ctag+l-cl, ptag+ctag+l-cl, ctag+ntag+l-cl
chl+pl, chl+pl+predlex ctag+cl-bn, ptag+ctag+cl-bn, ctag+ntag+cl-bn
chl+phl, chl+phl+predlex im-cl-roles
Table 2: Conjoined Feature Sets
2 shows how the features in Table 1 are used for labeling
semantic roles to the proposition in Figure 1.
Because the maximum entropy model assumes the in-
dependence of features, we should conjoin the coherent
features. As presented in Table 2, we use the conjoined
feature sets to assign semantic roles to the constituents of
the immediate clause and the upper clauses.
The predicate-type feature represents the predicate us-
age such as to-infinitive form (TO), the beginning of the
immediate clause (BEG), and otherwise (SEN). The tag
feature represents the tag of the current constituent. If it
is a clause, it is subdivided into a relative pronoun, a in-
finitival relative clause, etc according to its represented
form.
The path feature indicates the sequence of constituent
tags between the current constituent and the predicate.
The voice feature is determined to be an active or pas-
sive voice of the predicate, and the position feature is as-
signed by the constituent position with respect to pred-
icate. These features implicitly represent the predicate-
argument relation such as predicate-subject or predicate-
object.
For the headword feature, we use the Collins? head-
word rules, and as a complementary feature to the head
word feature, a content word feature2 is used to represent
the content of the PP, VP, or CONJP chunk.
The path-immediate-clause feature is the sequence of
constituent tags between the current constituent and the
immediate clause, and the path-begin-end feature is the
sequence between current constituent and beginning/end
of clause. The level-of-clause feature indicates whether
the current constituent is located in the first upper clause
or in the second upper clause, and the is-clause-boundary
feature is the binary value which indicates the existence
of the starting clause. The immediate-clause-roles fea-
tures are the binary indicators to represent whether the
core arguments exist in the immediate clause or not.
The path-immediate-clause, path-begin-end, level-of-
clause, is-clause-boundary, and immediate-clause-roles
features are used only in the second phase, and the others
except the path feature and the content-head feature are
used in common.
2For example, if the PP-chunk is because of, the headword
feature is of, and the content word feature is because.
Precision Recall F
?=1
Overall 68.42% 61.47% 64.76
A0 79.20% 75.73% 77.42
A1 67.41% 64.65% 66.00
A2 52.65% 45.94% 49.07
A3 52.53% 34.67% 41.77
A4 63.16% 48.00% 54.55
A5 0.00% 0.00% 0.00
AM-ADV 46.75% 35.18% 40.15
AM-CAU 57.69% 30.61% 40.00
AM-DIR 48.28% 28.00% 35.44
AM-DIS 60.11% 50.23% 54.73
AM-EXT 58.33% 50.00% 53.85
AM-LOC 35.56% 35.09% 35.32
AM-MNR 51.26% 23.92% 32.62
AM-MOD 89.77% 91.10% 90.43
AM-NEG 86.15% 88.19% 87.16
AM-PNC 48.98% 28.24% 35.82
AM-PRD 100.00% 33.33% 50.00
AM-TMP 59.51% 42.70% 49.73
R-A0 86.96% 75.47% 80.81
R-A1 57.89% 62.86% 60.27
R-A2 50.00% 33.33% 40.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 33.33% 25.00% 28.57
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 42.86% 21.43% 28.57
V 97.99% 97.99% 97.99
Table 3: Experimental results on the test set.
4 Experiments
To test the proposed method, we have experimented on
CoNLL-2004 datasets. For our experiments, we use the
Zhang le?s MaxEnt toolkit 3, and the L-BFGS parame-
ter estimation algorithm with Gaussian Prior smoothing
(Chen, 1999). The results on the test set are shown in
Table 3, and Table 4 shows the overall results when the
model is tested on the training set, the development set,
and the test set.
From these experimental results, we can find that the
proposed model has relatively high performance on the
labels related to A0 and A1, while it has relatively low
performance on the other labels. This may be caused by
following two reasons. Firstly, the instances of A0 or A1
are provided enough for accurate semantic role labeling.
Secondly, the thematic roles of A0 and A1 are more clear
than other core semantic roles. For example, agent is la-
beled as mainly A0 while benefactive can be labeled as
A2 or A3. Therefore, the maximum entropy model can
3http://www.nlplab.cn/zhangle/maxent toolkit.html
Precision Recall F
?=1
Overall(training) 96.40% 92.28% 94.29
Overall(dev) 69.78% 62.56% 65.97
Overall(test) 68.42% 61.47% 64.76
Table 4: The results when the model is tested on the train-
ing set, the development set, and the test set
get a good generalize performance in case of A0 or A1,
but can?t generalize well in other cases.
5 Conclusion
In this paper, we propose a semantic role labeling method
using a maximum entropy model. Because the maximum
entropy model enables not only to exploit rich features
but also to alleviate the data sparseness problem, we use
it to model the probability of a semantic role label se-
quence. The proposed method has following characteris-
tics: firstly, it assigns the semantic role labels to the con-
stituents in the immediate clause, and then assigns role
labels to the constituents in the upper clauses, and it uti-
lizes the relation between syntactic and semantic charac-
teristics of a given context.
For the future work, we will device a method of clus-
tering for the path and predicate features, and include the
clustering results as additional features.
References
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to natu-
ral language processing . Computational Linguistics,
22(1):39?71.
Xavier Carreras, and Lluis Marquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling . Proceedings of CoNLL-2004.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models . Technical Re-
port CMUCS-99-108, Carnegie Mellon University.
Daniel Gildea, and Daniel Jurafsky. 2002. Automatic La-
beling of Semantic Roles . Computational Linguistics,
28(3):245-288.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin and Daniel Jurafsky.
Oct 2003. Support Vector Learning for Semantic Ar-
gument Classification . Technical Report, TR-CSLR-
2003-03.
Cynthia A. Thompson, Roger Levy and Christopher D.
Manning. A Generative Model for Semantic Role La-
beling . ECML 2003, 397-408.
Two-Phase Semantic Role Labeling based on Support Vector Machines
Kyung-Mi Park and Young-Sook Hwang and Hae-Chang Rim
Department of Computer Science & Engineering, Korea University
5-ka, Anam-dong, SEOUL, 136-701, KOREA
{kmpark, yshwang, rim}@nlp.korea.ac.kr
Abstract
In this study, we try to apply SVMs to the se-
mantic role labeling task, which is one of the
multiclass problems. As a result, we propose a
two-phase semantic role labeling model which
consists of the identification phase and the clas-
sification phase. We first identify semantic ar-
guments, and then assign semantic roles to the
identified semantic arguments. By taking the
two-phase approach, we can alleviate the un-
balanced class distribution problem, and select
the features appropriate for each task.
1 Introduction
A semantic role in a language is a semantic relation-
ship between a syntactic constituent and a predicate. The
shared task of CoNLL-2004 relates to recognize seman-
tic roles in English (X. Carreras, 2004). Given a sentence,
the task is to analyze a proposition expressed by a target
verb of a sentence. Especially, for each target verb, all
constituents in a sentence which fill semantic roles of the
verb have to be recognized. This task is based only on
partial parsing information, avoiding use of a full parser
and external lexico-semantic knowledge base. According
to previous results of the CoNLL shared task, the POS
tagged, chunked, clause identified, and named-entity rec-
ognized sentences are given as an input (Figure 1).
SVM is a well-known machine learning algorithm with
high generalization performance in high dimensional fea-
ture spaces (H. Yamada, 2003). Also, learning with com-
bination of multiple features is possible by virtue of poly-
nomial kernel functions. However, since it is a binary
classifier, we are often confronted with the unbalanced
class distribution problem in a multiclass classification
task. The larger the number of classes, the more severe
the problem is. The semantic role labeling can be formu-
lated as a multiclass classification problem. If we try to
apply SVMs in the semantic role labeling problem, we
have to find a method of resolving the unbalanced class
distribution problem.
Conceptually, semantic role labeling can be divided
into two subtasks: the identification task which finds
the boundary of semantic arguments in a given sentence,
and the classificiation task which determines the seman-
tic role of the argument. This provides us a hint of using
SVMs with less severe unbalanced class distribution. In
this paper, we present a two-phase semantic role label-
ing method which consists of an identification phase and
a classification phase. By taking two phase model based
on SVMs, we can alleviate the unbalanced class distri-
bution problem. That is, since we find only the bound-
ary of an argument in the identification phase, the num-
ber of classes is decreased into two (ARG, NON-ARG)
or three (B-ARG, I-ARG, O). Therefore, we have to build
only one or three SVM classifiers. We can alleviate the
unbalanced class distribution problem by decreasing the
number of negative examples, which is much larger than
the number of positive exampels without two-phase mod-
eling. In the classification phase, we classify only the
identified argument into a proper semantic role. This en-
ables us to reduce the computational cost by ignoring the
non-argument constitutents.
Since features for identifying arguments are different
from features for classifying a role, we need to determine
different feature sets appropriate for the tasks. For iden-
tification, we focus on the features to detect the depen-
dency between a constituent and a predicate because the
arguments are dependent on the predicate. For seman-
tic role labeling, we consider both the syntactic and the
semantic information such as the sentential form of the
target predicate, the head of a constituent, and so on. In
the following sections, we will explain the two phase se-
mantic role labeling method in detail and show some ex-
perimental results.
Figure 1: An example of semantic role labeling. The columns contain: the word, its POS, its chunk type, clause
boundary, its named-entity tag, the target predicate, the result of semantic role labeling in the target predicate exist,
and deliver.
2 Two Phase Semantic Role Labeling
based on SVMs
We regard the semantic role labeling as a classification
problem of a syntactic constituent. However, a syntac-
tic constituent can be a chunk, or a clause. Therefore, we
have to identify the boundaries of semantic arguments be-
fore we assign roles to the arguments.
2.1 Semantic Argument Identification
This phase is the step of finding the boundary of seman-
tic arguments. A sequence of chunks or a subclause in
the immediate clause of a predicate can be a semantic ar-
gument of the predicate. A chunk or a subclause of the
predicate becomes a unit of the constituent of an argu-
ment. The chunks within the subclause are ignored.
For identifying the semantic arguments of a target
predicate, it is necessary to find the dependency rela-
tion between each constituent and a predicate. Identify-
ing a dependency relation is important for identifying a
subject/object relation (S. Buchholz, 2002) and also for
identifying the semantic arguments of a target predicate.
Therefore, the features for finding dependency relations
are implicitly represented in the feature set for the identi-
fication task.
For implementing the method based on the SVMs, we
represent a constituent of an argument with B/I/O nota-
tion, and assign one of the following classes to each con-
stituent: B-ARG class representing the beginning of se-
mantic argument, I-ARG class representing a part of a
semantic argument, or O class indicating that the con-
stituent does not belong to the semantic arguments.
Because we decide the unit of a constituent as a chunk
or a subclause, words except the predicate in the target
phrase 1 do not belong to constituent. Therefore, these
words have to be handled independently. In the training
data, we often observed that the beginning of semantic
arguments starts from the word right after the predicate.
For the agreement with the chunk boundary, we regard
the word following a predicate as the beginning word of
a new chunk. Namely, when the beginning of chunk tag
is I, we change I to B. Also, the words located in front of
the predicate in the target phrase are post-processed by 4
hand-crafted rules 2 and 211 automated rules 3 based on
frequency in the training data.
In order to restrict the search space in terms of the con-
stituents, we use the clause boundaries. The left search
boundary for identifying the semantic argument is set to
the left boundary of the second upper clause, and the right
search boundary is set to the right boundary of the imme-
diate clause.
2.1.1 Features for Identifying Semantic Argument
For this phase, we use 29 features for representing syn-
tactic and semantic information related to constituent and
predicate. Table1 shows a set of features employed. The
features can be described as follows:
? position: This is a binary feature identifying
whether the constituent is before (-1) or after (1) the
predicate in the immediate clause. The feature value
1The chunk containing a predicate is referred to as target
phrase.
2For example, if a word in target phrase is n?t, not or Not,
and POS tag of the word is RB and the distance between the
word and the predicate is less than 4, then the semantic role is
AM-NEG.
3For example, if a word in target phrase is already, and POS
tag of the word is RB, then the semantic role is AM-TMP.
Features examples
position -2, -1, 1
distance 0, 1, 2, . . .
predicate-candidate # of VP, NP, SBAR 0, 1, 2, . . .
(intervening features) # of POS [CC], [,], [:] 0, 1, 2, . . .
POS [?] & POS [?] -1, 0, 1
path VP-PP-NP, . . .
headword, headword?s POS, chunk type
predicate itself & context beginning word?s POS MD, TO, VBZ, . . .
context-1: headword, headword?s POS, chunk type
headword, headword?s POS, chunk type
candidate itself & context context-2: headword, headword?s POS, chunk type
context-1: headword, headword?s POS, chunk type
context+1: headword, headword?s POS, chunk type
Table 1: Features for Identifying a semantic argument
Figure 2: Two-phase semantic role labeling procedure using the example sentence presented in Figure 1. (P means the
target phrase containing the predicate deliver, and C means the constituent such as a chunk (e.g. Under) or a subclause
(e.g. Rockwell said))
(-2) means that the constituent is out of the immedi-
ate clause.
? distance: The distance is measured by the number
of chunks between the predicate and the constituent.
? # of VP, NP, SBAR: These are numeric features rep-
resenting the number of the specific chunk types be-
tween the predicate and the constituent.
? # of POS [CC], [,], [:]: These are numeric features
representing the number of the specific POS types
between the predicate and the constituent.
? POS [?] & POS [?]: This is used as a feature rep-
resenting the difference between # of POS[?] and #
of POS[?] counted in the range from the predicate
to the constituent. In Table 1, the feature value (-1)
means that # of POS[?] is larger than # of POS[?].
The feature value (1) conversly means that # of
POS[?] is larger than # of POS[?]. The featue value
(0) means that # of POS[?] is equal to # of POS[?].
? path: This is the syntactic path from the predicate to
the constituent, and is a symbolic feature comprising
all the elements (chunk or subclause) between the
predicate and the constituent.
? beginning word?s POS: In the target phrase, these
values appear only with VPs and represent the POS
of the syntactic head (MD, TO, VB, VBD, VBG,
VBN, VBP, VBZ). This represents the property of the
target phrase, for example, the feature value TO in-
dicates that the target phrase is to-infinitive.
? context: These are information for the predicate it-
self, the left context of the predicate, the constituent
itself, and the left and right context of the con-
stituent. In Table 1, - means the left context, and +
means the right context. In case that the constituent
is the subclause, the chunk type of the constituent is
set to the first chunk type of the subclause.
2.2 Semantic Role Assignment
In this phase, we assign appropriate semantic roles to the
identified semantic arguments. For learning SVM classi-
fiers, we consider not all semantic roles, but only 18 se-
mantic roles based on frequency in the training data (Ta-
ble 2). The (AM-MOD, AM-NEG) are post-processed by
hand-crafted rules. As we decrease the number of SVM
classifiers to be learned in the training data, the training
cost of classifiers can be reduced. Furthermore, we can
alleviate the unbalanced class distribution problem by ex-
semantic role
A0, A1, A2, A3, A4, R-A0, R-A1, R-A2, C-A1
AM-TMP, AM-ADV, AM-MNR, AM-LOC, AM-DIS
AM-PNC, AM-CAU, AM-DIR, AM-EXT
Table 2: 18 semantic roles
cluding the infrequent classes.
2.2.1 Features for Assigning Semantic Role
This phase also uses all features applied in the seman-
tic argument identification phase, except for # of POS [:]
and POS[?] & POS[?]. In addition, we use the following
feature.
? voice: This is a binary feature identifying whether
the target phrase is active or passive.
In Figure 2, we show two-phase semantic role labeling
procedure using the example sentence in Figure 1.
3 Experiments
For experiments, we utilized the SVM light package (T.
Joachims, 2002). In both the semantic argument identifi-
cation and the semantic role assignment phase, we used a
polynomial kernel (degree 2) with the one-vs-rest classi-
fication method. Table 3 shows the experimental results
on the test set and Table 4 shows the experimental results
on the development set. Table 4 also shows the perfor-
mance of each phase.
For improving the performance, we try to select the
discrminative features for each subtask. Especially, since
the performance of the identification phase is critical
to the total performance, we concentrate on improving
the identification performance. Our system obtains a F-
measure of 74.08 in the identification phase, as present-
eded in Table 4. For the argument classification task, the
our system obtains a classification accuracy (A) of 85.45.
4 Conclusion
In this paper, we present a method of two phase seman-
tic role labeling based on the support vector machines.
We found that SVM is useful to incorporate the hetero-
geneous features for the semantic role labeling. Also, by
applying the two phase model, we can alleviate the unbal-
anced class distribution problem caused by the the nega-
tive examples. Experimental results show that our system
obtains a F-measure of 63.99 on the test set and 65.78 on
the development set.
Precision Recall F?=1
Overall 65.63% 62.43% 63.99
A0 78.24% 74.60% 76.38
A1 65.83% 66.46% 66.14
A2 49.84% 43.70% 46.57
A3 56.04% 34.00% 42.32
A4 62.86% 44.00% 51.76
A5 0.00% 0.00% 0.00
AM-ADV 45.18% 44.30% 44.74
AM-CAU 36.67% 22.45% 27.85
AM-DIR 20.00% 20.00% 20.00
AM-DIS 56.62% 58.22% 57.41
AM-EXT 61.54% 57.14% 59.26
AM-LOC 26.01% 31.14% 28.34
AM-MNR 43.54% 35.69% 39.22
AM-MOD 97.46% 91.10% 94.17
AM-NEG 94.92% 88.19% 91.43
AM-PNC 40.00% 28.24% 33.10
AM-PRD 0.00% 0.00% 0.00
AM-TMP 51.83% 45.38% 48.39
R-A0 80.49% 83.02% 81.73
R-A1 75.00% 51.43% 61.02
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 96.66% 96.66% 96.66
Table 3: Results on the test set: closed challenge
Precision Recall F?=1 A
Overall 67.27% 64.36% 65.78 -
identification 75.96% 72.30% 74.08 -
classification - - - 85.45
Table 4: Results on the development set: closed chal-
lenge. (A means accuracy.)
References
X. Carreras and L. Marquez. 2004. Introduction to the
CoNLL-2004 Shared Task: Semantic Role Labeling.
CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical
Dependency Analysis with Support Vector Machines.
IWPT03.
S. Buchholz 2002. Memory-Based Grammatical Rela-
tion Finding. PhD. thesis, Tilburg University.
T. Joachims 2002. SVM Light available at http:// svm-
light.joachims.org/.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 209?212, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Maximum Entropy based Semantic Role Labeling
Kyung-Mi Park and Hae-Chang Rim
Department of Computer Science & Engineering, Korea University
5-ka, Anam-dong, SeongBuk-gu, SEOUL, 136-701, KOREA
{kmpark, rim}@nlp.korea.ac.kr
1 Introduction
The semantic role labeling (SRL) refers to finding
the semantic relation (e.g. Agent, Patient, etc.) be-
tween a predicate and syntactic constituents in the
sentences. Especially, with the argument informa-
tion of the predicate, we can derive the predicate-
argument structures, which are useful for the appli-
cations such as automatic information extraction. As
previous work on the SRL, there have been many
machine learning approaches. (Gildea and Jurafsky,
2002; Pradhan et al, 2003; Lim et al, 2004).
In this paper, we present a two-phase SRL method
based on a maximum entropy (ME) model. We first
identify parse constituents that represent valid se-
mantic arguments of a given predicate, and then as-
sign appropriate semantic roles to the the identified
parse constituents. In the two-phase SRL method,
the performance of the argument identification phase
is very important, because the argument classifica-
tion is performed on the region identified at the iden-
tification phase. In this study, in order to improve the
performance of identification, we try to incorporate
clause boundary restriction and tree distance restric-
tion into pre-processing of the identification phase.
Since features for identifying arguments are dif-
ferent from features for classifying a role, we need
to determine different feature sets appropriate for the
tasks. We determine final feature sets for each phase
with experiments. We participate in the closed chal-
lenge of the CoNLL-2005 shared task and report re-
sults on both development and test sets. A detailed
description of the task, data and related work can be
found in Carreras and Ma`rquez (2005).
2 System Description
In this section, we describe our system that iden-
tifies and classifies semantic arguments. First, we
explain pre-processing of the identification phase.
Next, we describe features employed. Finally, we
explain classifiers used in each phase.
2.1 Pre-processing
We thought that the occurrence of most semantic
arguments are sensitive to the boundary of the im-
mediate clause or the upper clauses of a predicate.
Also, we assumed that they exist in the uniform dis-
tance on the parse tree from the predicate?s parent
node (called Pp) to the parse constituent?s parent
node (called Pc). Therefore, for identifying seman-
tic arguments, we do not need to examine all parse
constituents in a parse tree. In this study, we use
the clause boundary restriction and the tree distance
restriction, and they can provide useful information
for spotting the probable search space which include
semantic arguments.
In Figure 1 and Table 1, we show an example of
applying the tree distance restriction. We show the
distance between Pp=VP and the nonterminals of a
parse tree in Figure 1. For example, NP2:d=3 means
3 times downward movement through the parse tree
from Pp=VP to Pc=NP2. NP4 does not have the dis-
tance from Pp because we allow to move only up-
ward or only downward through the tree from Pp to
Pc. In Table 1, we indicate all 14 argument can-
didates that correspond to tree distance restriction
(d?3). Only 2 of the 14 argument candidates are
actually served to semantic arguments (NP4, PP).
209
Figure 1: Distance between Pp=VP and Pc.
distance direction Pc argument candidates
d=1 UP S NP4
d=0 - VP PP
d=1 DOWN PP IN, NP3
d=2 DOWN NP3 NP1, CONJP, NP2
d=3 DOWN NP1 JJ, NNS, NN
d=3 DOWN CONJP RB, IN
d=3 DOWN NP2 NNS, NN
Table 1: Probable argument candidates (d?3).
2.2 Features
The following features describe properties of the
verb predicate. These featues are shared by all the
parse constituents in the tree.
? pred lex: this is the predicate itself.
? pred POS: this is POS of the predicate.
? pred phr: this is the syntactic category of Pp.
? pred type: this represents the predicate usage
such as to-infinitive form, the verb predicate of
a main clause, and otherwise.
? voice: this is a binary feature identifying
whether the predicate is active or passive.
? sub cat: this is the phrase structure rule ex-
panding the predicate?s parent node in the tree.
? pt+pl: this is a conjoined feature of pred type
and pred lex. Because the maximum entropy
model assumes the independence of features,
we need to conjoin the coherent features.
The following features characterize the internal
structure of a argument candidate. These features
change with the constituent under consideration.
? head lex: this is the headword of the argument
candidate. We extracts the headword by using
the Collins?s headword rules.
? head POS: this is POS of the headword.
? head phr: this is the syntactic category of Pc.
? cont lex: this is the content word of the argu-
ment candidate. We extracts the content word
by using the head table of the chunklink.pl 1.
? cont POS: this is POS of the content word.
? gov: this is the governing category introduced
by Gildea and Jurafsky (2002).
The following features capture the relations be-
tween the verb predicate and the constituent.
? path: this is the syntactic path through the parse
tree from the parse constituent to the predicate.
? pos: this is a binary feature identifying whether
the constituent is before or after the predicate.
? pos+clau: this, conjoined with pos, indicates
whether the constituent is located in the imme-
diate clause, in the first upper clause, in the sec-
ond upper clause, or in the third upper clause.
? pos+VP, pos+NP, pos+SBAR: these are nu-
meric features representing the number of the
specific chunk types between the constituent
and the predicate.
? pos+CC, pos+comma, pos+colon, pos+quote:
these are numeric features representing the
number of the specific POS types between the
constituent and the predicate .
? pl+hl (pred lex + head lex), pl+cl (pred lex +
cont lex), v+gov (voice + gov).
2.3 Classifier
The ME classifier for the identification phase clas-
sifies each parse constituent into one of the follow-
ing classes: ARG class or NON-ARG class. The ME
classifier for the classification phase classifies the
identified argument into one of the pre-defined se-
mantic roles (e.g. A0, A1, AM-ADV, AM-CAU, etc.).
1http://pi0657.kub.nl/s?abine/chunklink/chunklink 2-2-
2000 for conll.pl
210
#exa. %can. #can. %arg. F?=1
no restriction
All1 3,709,080 - 233,394 96.06 79.37
All2 2,579,278 - 233,004 95.90 79.52
All3 1,598,726 100.00 231,120 95.13 79.92
restriction on clause boundary
1/0 1,303,596 81.54 222,238 91.47 78.97
1/1 1,370,760 85.74 223,571 92.02 79.14
2/0 1,403,630 87.80 228,891 94.21 79.66
2/1 1,470,794 92.00 230,224 94.76 79.89
3/0 1,439,755 90.06 229,548 94.48 79.63
3/1 1,506,919 94.26 230,881 95.03 79.79
restriction on tree distance
6/1 804,413 50.32 226,875 93.38 80.17
6/2 936,021 58.55 227,637 93.69 79.94
7/1 842,453 52.70 228,129 93.90 80.44
7/2 974,061 60.93 228,891 94.21 80.03
8/1 871,541 54.51 228,795 94.17 80.24
8/2 1,003,149 62.75 229,557 94.48 80.04
restriction on clause boundary & tree distance
2/1,7/1 786,951 49.22 227,523 93.65 80.12
2/1,8/1 803,040 50.23 228,081 93.88 80.11
3/1,7/1 800,740 50.09 227,947 93.82 80.28
3/1,8/1 822,225 51.43 228,599 94.09 80.06
Table 2: Different ways of reducing candidates.
3 Experiments
To test the proposed method, we have experimented
with CoNLL-2005 datasets (Wall Street sections 02-
21 as training set, Charniak? trees). The results have
been evaluated by using the srl-eval.pl script pro-
vided by the shared task organizers. For building
classifiers, we utilized the Zhang le?s MaxEnt toolkit
2
, and the L-BFGS parameter estimation algorithm
with Gaussian Prior smoothing.
Table 2 shows the different ways of reducing the
number of argument candidates. The 2nd and 3rd
columns (#can., %can.) indicate the number of ar-
gument candidates and the percentage of argument
candidates that satisfy each restriction on the train-
ing set. The 4th and 5th columns (#arg., %arg.)
indicate the number of correct arguments and the
percentage of correct arguments that satisfy each re-
striction on the training set. The last column (F?=1)
indicates the performance of the identification task
on the development set by applying each restriction.
In no restriction, All1 extracts candidates from all
the nonterminals?s child nodes of a tree. All2 fil-
ter the nonterminals which include at least one non-
2http://www.nlplab.cn/zhangle/maxent toolkit.html
Prec. Recall F?=1 Accu.
All 82.57 78.41 80.44 86.00
All-(pred lex) 82.80 77.78 80.21 84.93
All-(pred POS) 83.40 76.72 79.92 85.95
All-(pred phr) 83.11 77.57 80.24 85.87
All-(pred type) 82.76 77.91 80.26 85.99
All-(voice) 82.87 77.88 80.30 85.88
All-(sub cat) 82.48 77.68 80.00 84.88
All-(pt+pl) 83.20 77.40 80.20 85.62
All-(head lex) 82.58 77.87 80.16 85.61
All-(head POS) 82.66 77.88 80.20 85.89
All-(head phr) 83.52 76.82 80.03 85.81
All-(cont lex) 82.57 77.87 80.15 85.64
All-(cont POS) 82.65 77.92 80.22 86.09
All-(gov) 82.69 78.34 80.46 85.91
All-(path) 78.39 67.96 72.80 85.69
All-(pos) 82.70 77.74 80.14 85.85
All-(pos+clau) 82.94 78.34 80.57 86.19
All-(pos+VP) 82.69 77.87 80.20 85.87
All-(pos+NP) 82.78 77.69 80.15 85.77
All-(pos+SBAR) 82.51 78.00 80.19 85.83
All-(pos+CC) 82.84 78.10 80.40 85.70
All-(pos+comma) 82.78 77.69 80.15 85.70
All-(pos+colon) 82.67 77.96 80.25 85.72
All-(pos+quote) 82.63 77.98 80.24 85.66
All-(pl+hl) 82.62 77.71 80.09 84.98
All-(pl+cl) 82.72 77.79 80.18 85.24
All-(v+gov) 82.93 77.81 80.29 85.85
Prec. Recall F?=1 Accu.
Iden. 82.56 78.72 80.59 -
clas. - - - 87.16
Iden.+Clas. 72.68 69.16 70.87 -
Table 3: Performance of various feature combina-
tions (top) and performance of each phase (bottom).
terminal child 3. All3 filter the nonterminals which
include at least one nonterminal child and have dis-
tance from Pp. We use All3 as a baseline.
In restriction on clause boundary, for example,
2/0 means that the left search boundary for identi-
fying the argument is set to the left boundary of the
second upper clause, and the right search boundary
is set to the right boundary of the immediate clause.
In restriction on tree distance, for example, 7/1
means that it is possible to move up to 7 times up-
ward (d?7) through the parse tree from Pp to Pc, and
it is possible to move up to once downward (d?1)
through the parse tree from Pp to Pc.
In clause boundary & tree distance, for example,
3/1,7/1 means the case when we use both the clause
boundary (3/1) and the tree distance (7/1).
3We ignore the nonterminals that have only pre-terminal
children (e.g. in Figure 1, NP1, CONJP, NP2).
211
Precision Recall F?=1
Development 72.68% 69.16% 70.87
Test WSJ 74.69% 70.78% 72.68
Test Brown 64.58% 60.31% 62.38
Test WSJ+Brown 73.35% 69.37% 71.31
Test WSJ Precision Recall F?=1
Overall 74.69% 70.78% 72.68
A0 85.02% 81.53% 83.24
A1 73.98% 72.25% 73.11
A2 63.20% 57.57% 60.25
A3 62.96% 49.13% 55.19
A4 73.40% 67.65% 70.41
A5 100.00% 40.00% 57.14
AM-ADV 56.73% 50.00% 53.15
AM-CAU 70.21% 45.21% 55.00
AM-DIR 46.48% 38.82% 42.31
AM-DIS 70.95% 65.62% 68.18
AM-EXT 87.50% 43.75% 58.33
AM-LOC 44.09% 46.28% 45.16
AM-MNR 55.56% 52.33% 53.89
AM-MOD 97.59% 95.64% 96.61
AM-NEG 96.05% 95.22% 95.63
AM-PNC 40.68% 41.74% 41.20
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 70.11% 61.73% 65.66
R-A0 84.68% 83.93% 84.30
R-A1 73.33% 70.51% 71.90
R-A2 50.00% 31.25% 38.46
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 85.71% 57.14% 68.57
R-AM-MNR 16.67% 16.67% 16.67
R-AM-TMP 72.50% 55.77% 63.04
V 97.32% 97.32% 97.32
Table 4: Overall results (top) and detailed results on
the WSJ test (bottom).
Precision Recall F?=1
one-phase 71.94 68.70 70.29
two-phase 72.68 69.16 70.87
Table 5: Performance of one-phase vs. two-phase.
According to the experimental results, we use
7/1 tree distance restriction for all following ex-
periments. By applying the restriction, we can re-
move about 47.3% (%can.=52.70%) of total argu-
ment candidates as compared with All3. 93.90%
(%arg.) corresponds to the upper bound on recall.
In order to estimate the relative contribution of
each feature, we measure performance of each phase
on the development set by leaving out one feature at
a time, as shown in the top of Table 3. Precision,
Recall, and F?=1 represent the performance of the
identification task, and Accuracy represent the per-
formance of the classification task only with 100%
correct argument identification respectively. All rep-
resents the performance of the experiment when all
26 features introduced by section 2.2 are considered.
Finally, for identification, we use 24 features except
gov and pos+clau, and obtain an F?=1 of 80.59%, as
shown in the bottom of Table 3. Also, for classifica-
tion, we use 23 features except pred type, cont POS,
and pos+clau, and obtain an Accuracy of 87.16%.
Table 4 presents our best system performance on
the development set, and the performance of the
same system on the test set. Table 5 shows the
performance on the development set using the one-
phase method and the two-phase method respec-
tively. The one-phase method is implemented by in-
corporating the identification into the classification.
one-phase shows the performance of the experiment
when 25 features except pos+clau are used. Exper-
imental results show that the two-phase method is
better than the one-phase method in our evaluation.
4 Conclusion
We have presented a two-phase SRL method based
on a ME model. In the two-phase method, in order to
improve the performance of identification that dom-
inate the overall performance, we have performed
pre-processing. Experimental results show that our
system obtains an F?=1 of 72.68% on the WSJ test
and that the introduction of pre-processing improves
the performance, as compared with the case when
all parse constituents are considered.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. Pro-
ceedings of CoNLL-2005.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistics.
Joon-Ho Lim, Young-Sook Hwang, So-Young Park and Hae-
Chang Rim. 2004. Semantic Role Labeling using Maximum
Entropy Model. Proceedings of CoNLL.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James H. Martin and Daniel Jurafsky. 2003. Shallow
Semantic Parsing Using Support Vector Machines. Techni-
cal Report, TR-CSLR-2003-03.
212
Proceedings of the Workshop on BioNLP: Shared Task, pages 107?110,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Multi-Phase Approach to Biomedical Event Extraction
Hyoung-Gyu Lee, Han-Cheol Cho, Min-Jeong Kim
Joo-Young Lee, Gumwon Hong, Hae-Chang Rim
Department of Computer and Radio Communications Engineering
Korea University
Seoul, South Korea
{hglee,hccho,mjkim,jylee,gwhong,rim}@nlp.korea.ac.kr
Abstract
In this paper, we propose a system for biomed-
ical event extraction using multi-phase ap-
proach. It consists of event trigger detector,
event type classifier, and relation recognizer
and event compositor. The system firstly iden-
tifies triggers in a given sentence. Then, it
classifies the triggers into one of nine pre-
defined classes. Lastly, the system examines
each trigger whether it has a relation with
participant candidates, and composites events
with the extracted relations. The official score
of the proposed system recorded 61.65 preci-
sion, 9.40 recall and 16.31 f-score in approxi-
mate span matching. However, we found that
the threshold tuning for the third phase had
negative effect. Without the threshold tuning,
the system showed 55.32 precision, 16.18 re-
call and 25.04 f-score.
1 Introduction
As the volume of biomedical literature grows expo-
nentially, new biomedical terms and their relations
are also generated. However, it is still not easy for
researchers to access necessary information quickly
since it is lost within large volumes of text. This is
the reason that the study of information extraction
is receiving the attention of biomedical and natural
language processing (NLP) researchers today.
In the shared task, the organizers provide partic-
ipants with raw biomedical text, tagged biomedical
terms (proteins), and the analyzed data with various
NLP techniques such as tokenization, POS-tagging,
phrase structure and dependency parsing and so on.
The expected results are the events, which exist in
the given text, consisting of a trigger and its partici-
pant(s) (Kim et al, 2009).
The proposed system consists of three phases;
event trigger detection phase(TD phase), event type
classification phase(TC phase), relation recognition
and event composition phase(RE phase). It works in
the following manner. Firstly, it identifies triggers of
a given biomedical sentence. Then, it classifies trig-
gers into nine pre-defined classes. Lastly, the sys-
tem finds the relations between triggers and partic-
ipant candidates by examining each trigger whether
it has relations with participant candidates, and com-
posites events with the extracted relations. In the
last phase, multiple relations of the same trigger
can be combined into an event for Binding event
type. In addition, multiple relations can be com-
bined and their participant types can be classified
into not only theme but also cause for three Regu-
lation event types.
In this paper, we mainly use dependency pars-
ing information of the analyzed data because sev-
eral previous studies for SRL have improved their
performance by using features extracted from this
information (Hacioglu, 2004; Tsai et al, 2006).
In the experimental results, the proposed system
showed 68.46 f-score in TD phase, 85.20 accuracy
in TC phase, 89.91 f-score in the initial step of RE
phase and 81.24 f-score in the iterative step of RE
phase, but officially achieved 61.65 precision, 9.40
recall and 16.31 f-score in approximate span match-
ing. These figures were the lowest among twenty-
four shared-task participants. However, we found
that the threshold tuning for RE phase had caused
a negative effect. It deteriorates the f-score of the
107
Event Trigger Detector
Event Type Classifier
Relation Recognizer &
Event Compositor
Initial Step
Iterative Step
Source Data
Analyzed Data
Result of Event Extraction
Figure 1: System Architecture
proposed system by enlarging the gap between pre-
cision and recall. With the default threshold, the sys-
tem showed better result in the final test data, 55.32
precision, 16.18 recall and 25.04 f-score with the
rank 17th among 24 teams.
2 System Description
Figure 1 shows our bio-event extraction system
which consists of Event Trigger Detector, Event
Type Classifier and Relation Recognizer & Event
Compositor. Each component includes single or
multiple Maximum Entropy models trained by gold
annotation data. The inputs of the system are source
data and analyzed data. The former is raw text with
entity annotation, and the latter is tokenized, POS
tagged and parsed data of the raw text.1
Because the event type is useful to recognize the
relation, we perform TC phase before RE phase.
One of important characteristics of bio-event is
that one event as well as a protein may participate
in another event. Considering this, we designed the
system in which the Relation Recognizer be per-
formed through two steps. In the initial step, the sys-
tems examines each trigger whether it has the rela-
tions with only proteins, and composites events with
recognized relations. In the iterative step, it repeat-
edly examines remained triggers in the same man-
1We used the GDep result provided by organizers of the
shared task as analyzed data.
ner. This step allows the system to extract chain-
style events, which means that one event participates
in another one and the other participates in the for-
mer.
To increase the f-score, we tuned a threshold for
RE phase which is a binary classification task; de-
ciding whether a given relation candidate is correct
one or not. When the output probability of a maxi-
mum entropy model is lower than the threshold, we
discard a relation candidate.
2.1 Event Trigger Detection
We assume that an event trigger is a single word.
In other words, we do not consider the multi-word
trigger detection. Because the trigger statistic in
the training data showed that about 93% of triggers
are single word, we concentrated on the single word
trigger detection.
This phase is simply defined as the task that clas-
sify whether each token is a trigger or not in a doc-
ument. It is necessary to select targets to classify
among all tokens, because a set of all tokens includes
too many negative examples. For this, the follow-
ing filtering rules are applied to each token. Though
these rules filtered out 69.5% of tokens, the trigger
recall was 94.8%.
? Filter out tokens whose POS tag is not matched
to anything among NN, NNS, VB, VBD, VBG,
VBN, VBP, VBZ, JJ and JJR.
? Filter out tokens that are a biomedical named
entity.
? Filter out sentences that do not have any pro-
teins.
Proposed features for the binary classification of
tokens include both features similar to those used in
(Hacioglu, 2004; Tsai et al, 2006; Ahn, 2006) and
novel ones. The selected feature set is showed in
Table 1.
2.2 Event Type Classification
In TC phase, tokens recognized as trigger are clas-
sified into nine pre-defined classes. Although more
than a dozen features had been tested, the features
except word and lemma features hardly contributed
to the performance improvement. The tuned feature
set is showed in Table 2.
108
Word level features
- Token word
- Token lemma
- Token POS
- POSs of previous two tokens
- Distance, word and POS of the nearest protein
- Positional independence: Whether a noun or a
verb is adjacent to the current token
Dependency level features
- Dependency label path of the nearest protein
- The existence of protein in family: This feature is
motivated by the study in (Hacioglu, 2004)
- A boolean feature which is true if token?s child is
a proposition and the chunk of the child include a
protein
- A boolean feature which is true if token?s child is
a protein and its dependency label is OBJ
Table 1: Features for event trigger detection
Features for the event type classification
- Trigger word
- Trigger lemma
- A boolean feature which is true if a protein exists
within left and right two words
Table 2: Features for event type classification
We found that TC phase showed relatively high
precision and recall with simple lexical features in
the experiment. However, it was quite difficult to
find additional features that could improve the per-
formance.
2.3 Relation Recognition and Event
Composition
In the last phase, the system examines each trigger
whether it has relations with participant candidates,
and composites events with the extracted relations.
(A relation consists of one trigger and one partici-
pant)
We devised a two-step process, consisting of ini-
tial and iterative steps, because a participant candi-
date can be a protein or an event. In the initial step,
the system finds relations between triggers and pro-
tein participant candidates. Features are explained
in Table 3. Then, it generates one event with one
relation for event types that have only one partici-
pant. For Binding event type, the system combines
at most three relations of the same trigger into one
Word level features
- Trigger word
- Trigger lemma
- Trigger type (I-1)
- Entity word
- Entity type (I-2)
- Word sequence between T&P (I-1)
- Word distance
- Existence of another trigger between T&P
- The number of triggers of above feature
- Existence of another participant candidate
- The number of participants of above feature
Dependency level features
- Trigger dependency label (I-1)
- Entity dependency label
- Lemma of trigger?s head word (I-1)
- POS of trigger?s head word
- Lemma of entity?s head word (I-1)
- POS of entity?s head word
- Lemma of trigger?s head word + Lemma of en-
tity?s head word
- Right lemma of trigger?s head word
- 2nd right lemma of trigger?s head word (I-1)
- Right lemma of entity?s head word
- 2nd right lemma of entity?s head word (I-1)
- Dependency path between T&P
- Dependency distance between T&P
- Direct descendant: a participant candidate is a di-
rect descendant of a given trigger
Table 3: Features for relation recognition between a trig-
ger and a participant (T&P)
event. For Regulation event types, we trained a bi-
nary classifier to classify participants of a Regulation
event into theme or cause. Features for participant
type classification is explained in Table 4. Among
multiple participants of a Regulation event, only two
participants having highest probabilities for theme
and cause constitute one event.
In the iterative step, the system finds relations be-
tween triggers and event participant candidates that
were extracted in the previous step, and generates
events in the same manner. The system performs it-
erative steps three times to find chain events.
Features are basically common in the initial (I-1)
step and the iterative (I-2) step, but some features
improve the performance only in one step. In order
to represent the difference in Table 3, we indicate (I-
1) when a feature is used in the initial step only, and
indicate (I-2) when it used in the iterative step only.
109
Word level features
- Trigger word
- Trigger lemma
- Participant words - event?s trigger words if a par-
ticipant is an event
- Left lemma of a participant
- Right lemma of a participant
- Trigger word + Participant words
- Trigger lemma + Participant lemmas
- Participant lemmas
- Right lemma of a trigger
- 2nd right lemma of a trigger
- Right lemma of a participant
- 2nd left lemma of a participant
Dependency level features
- Dependency path
- Dependency relation to trigger?s head
- Dependency relation to participant?s head
- POS pattern of common head chunk of a trigger
and a participant
- POS pattern of common head chunk of a trigger
and a participant + The presence of an object word
in dependency path
Table 4: Features of the participant type classifier for
Regulation events
3 Experimental Result
Table 5 shows the official results of the final test
data. After the feature selection, we have performed
the experiments with the development data to tune
the threshold to be used in RE phase. The work im-
proved the performance slightly. The new thresh-
old discovered by the work was 0.65 rather than
the default value, 0.5. However, we found that the
tuned threshold was over-fitted to development data.
When we tested without any threshold change, the
proposed system showed better f-score by reducing
the gap between precision and recall. Table 6 shows
the performance in this case.
Nevertheless, recall is still quite lower than preci-
sion in Table 6. The reason is that many triggers are
not detected in TD phase. The recall of the trigger
detector was 63% with the development data. An-
alyzing errors of TD phase, we found that the sys-
tem missed terms such as role, prevent while it easily
detected bio-terms such as phosphorylation, regula-
tion. It implies that the word feature causes not only
high precision but also low recall in TD phase.
Event equality recall precision f-score
Strict 8.99 58.97 15.60
Approximate Span 9.40 61.65 16.31
Table 5: The official results with threshold tuning
Event equality recall precision f-score
Strict 15.46 52.85 23.92
Approximate Span 16.18 55.32 25.04
Table 6: The results without threshold tuning
4 Conclusion
In this paper, we have presented a biomedical
event extraction system consisting of trigger detec-
tor, event type classifier and two-step participant rec-
ognizer. The system uses dependency parsing and
predicate argument information as main sources for
feature extraction.
For future work, we would like to increase the
performance of TD phase by adopting two-step
method similar to RE phase. We also will exploit
more analyzed data such as phrase structure parsing
information to improve the performance.
References
Kadri Hacioglu. 2004. Semantic Role Labeling Using
Dependency Trees. In Proceedings of COLING-2004,
Geneva, Switzerland.
Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun Lin,
Cheng-Lung Sung, Wei Ku, Ying-shan Su, Ting-Yi
Sung and Wen-Lian Hsu. 2006. BIOSMILE: Adapt-
ing Semantic Role Labeling for Biomedical Verbs: An
Exponential Model Coupled with Automatically Gen-
erated Template Features. In Proceedings of BioNLP-
2006.
Mihai Surdeanu, Sanda Harabagiu, John Williams and
Paul Aarseth. 2003. Using Predicate-Argument Struc-
tures for Information Extraction. In Proceedings of
ACL-2003, Sapporo, Japan.
David Ahn. 2006. The stages of event extraction. In Pro-
ceedings of Workshop On Annotating And Reasoning
About Time And Events.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop.
110
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 108?111,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
A Hybrid Approach to English-Korean Name Transliteration
Gumwon Hong?, Min-Jeong Kim?, Do-Gil Lee+ and Hae-Chang Rim?
?Department of Computer Science & Engineering, Korea University, Seoul 136-713, Korea
{gwhong,mjkim,rim}@nlp.korea.ac.kr
+Institute of Korean Culture, Korea University, Seoul 136-701, Korea
motdg@korea.ac.kr
Abstract
This paper presents a hybrid approach to
English-Korean name transliteration. The
base system is built on MOSES with en-
abled factored translation features. We
expand the base system by combining
with various transliteration methods in-
cluding a Web-based n-best re-ranking, a
dictionary-based method, and a rule-based
method. Our standard run and best non-
standard run achieve 45.1 and 78.5, re-
spectively, in top-1 accuracy. Experimen-
tal results show that expanding training
data size significantly contributes to the
performance. Also we discover that the
Web-based re-ranking method can be suc-
cessfully applied to the English-Korean
transliteration.
1 Introduction
Often, named entities such as person names or
place names from foreign origin do not appear in
the dictionary, and such out of vocabulary words
are a common source of errors in processing nat-
ural languages. For example, in statistical ma-
chine translation (SMT), if a new word occurs
in the input source sentence, the decoder will at
best drop the unknown word or directly copy the
source word to the target sentence. Transliteration,
a method of mapping phonemes or graphemes of
source language into those of target language, can
be used in this case in order to identify a possible
translation of the word.
The approaches to automatic transliteration be-
tween English and Korean can be performed
through the following ways: First, in learning how
to write the names of foreign origin, we can re-
fer to a transliteration standard which is estab-
lished by the government or some official linguis-
tic organizations. No matter where the standard
comes from, the basic principle of the standard
is based on the correct pronunciation of foreign
words. Second, since constructing such rules are
very costly in terms of time and money, we can
rely on a statistical method such as SMT. We be-
lieve that the rule-based method can guarantee to
increase accuracy for known cases, and the statis-
tical method can be robust to handle various ex-
ceptions.
In this paper, we present a variety of tech-
niques for English-Korean name transliteration.
First, we use a phrase-base SMT model with some
factored translation features for the transliteration
task. Second, we expand the base system by ap-
plying Web-based n-best re-ranking of the results.
Third, we apply a pronouncing dictionary-based
method to the base system which utilizes the pro-
nunciation symbols which is motivated by linguis-
tic knowledge. Finally, we introduce a phonics-
based method which is originally designed for
teaching speakers of English to read and write that
language.
2 Proposed Approach
In order to build our base system, we use MOSES
(Koehn et al, 2007), a well-known phrase-based
system designed for SMT. MOSES offers a con-
venient framework which can be directly applied
to machine transliteration experiments. In this
framework, the transliteration can be performed
in a very similar process of SMT task except the
following changes. First, the unit of translation
is changed from words to characters. Second, a
phrase in transliteration refers to any contiguous
block of character sequence which can be directly
matched from a source word to a target word.
Also, we do not have to worry about any distortion
parameters because decoding can be performed in
a totally monotonic way.
The process of the general transliteration ap-
proach begins by matching the unit of a source
108
LetterAlignment
Bilingual Corpus
Factored Phrase-based Training
Trained Model
EumjeolDecomposition MOSESDecoder
Input Word
EumjeolRe-composition
Target Word
EumjeolDecomposition
N-bestRe-ranking
Web
Dictionary
Phonics
Figure 1: System Architecture
word to the unit of a target word. The unit can be
based on graphemes or phonemes, depending on
language pairs or approaches. In English-Korean
transliteration, both grapheme-to-grapheme and
grapheme-to-phoneme approaches are possible. In
our method, we select grapheme-to-grapheme ap-
proach as a base system, and we apply grapheme-
to-phoneme functions in pronouncing dictionary-
based approach.
The transliteration between Korean and other
languages requires some special preprocessing
techniques. First of all, Korean alphabet is or-
ganized into syllabic blocks called Eumjeol. Ko-
rean transliteration standard allows each Eumjeol
to consist of either two or three of the 24 Korean
letters, with (1) leading 14 consonants, (2) inter-
mediate 10 vowels, and (3) optionally, trailing 7
consonants (out of the possible 14). Therefore,
Korean Eumjeol should be decomposed into letters
before performing training or decoding any input.
Consequently, after the letter-unit transliteration is
finished, all the letters should be re-composed to
form a correct sequence of Eumjeols.
Figure 1 shows the overall architecture of our
system. The alignment between English letter and
Korean letter is performed using GIZA++ (Och
and Ney, 2003). We use MOSES decoder in or-
der to search the best sequence of transliteration.
In this paper we focus on describing factored
phrase-based training and n-best re-ranking tech-
niques including a Web-based method, a pro-
nouncing dictionary-based method, and a phonics-
based method.
Figure 2: Alignment example between ?Knight?
and ?s?? [naiteu]?
2.1 Factored Phrase-based Training
Koehn and Hoang (2007) introduces an integration
of different information for phrase-based SMT
model. We report on experiments with three fac-
tors: surface form, positional information, and
the type of a letter. Surface form indicates a
letter itself. For positional information, we add
a BIO label to each input character in both the
source words and the target words. The intuition is
that certain character is differently pronounced de-
pending on its position in a word. For example, ?k?
in ?Knight? or ?h? in ?Sarah? are not pronounced.
The type of a letter is used to classify whether a
given letter is a vowel or a consonant. We assume
that a consonant in source word would more likely
be linked to a consonant in a target word. Figure 2
shows an example of alignment with factored fea-
tures.
2.2 Web-based Re-ranking
We re-ranked the top n results of the decoder by
referring to how many times both source word and
target word co-occur on the Web. In news articles
on the Web, a translation of a foreign name is of-
ten provided near the foreign name to describe its
pronunciation or description. To reflect this obser-
vation, we use Google?s proximity search by re-
stricting two terms should occur within four-word
distance. The frequency is adjusted as relative fre-
quency form by dividing each frequency by total
frequency of all n-best results.
Also, we linearly interpolate the n-best score
with the relative frequency of candidate output. To
make fair interpolation, we adjust both scores to be
between 0 and 1. Also, in this method, we decide
to remove all the candidates whose frequencies are
zero.
2.3 Pronouncing Dictionary-based Method
According to ?Oeraeeo pyogibeop1? (Korean or-
thography and writing method of borrowed for-
1http://www.korean.go.kr/08 new/data/rule03.jsp
109
Methods Acc.1 Mean F1 Mean Fdec MRR MAPref MAP10 MAPsys
BS 0.451 0.720 0.852 0.576 0.451 0.181 0.181
ER 0.740 0.868 0.930 0.806 0.740 0.243 0.243
WR 0.784 0.889 0.944 0.840 0.784 0.252 0.484
PD 0.781 0.885 0.941 0.839 0.781 0.252 0.460
PB 0.785 0.887 0.943 0.840 0.785 0.252 0.441
Table 1: Experimental Results (EnKo)
eign words), the primary principle of English-to-
Korean transliteration is to spell according to the
mapping table between the international phonetic
alphabets and the Korean alphabets. Therefore,
we can say that a pronouncing dictionary-based
method is very suitable for this principle.
We use the following two resources for build-
ing a pronouncing dictionary: one is an English-
Korean dictionary that contains 130,000 words.
The other is the CMU pronouncing dictionary2
created by Carnegie Mellon University that con-
tains over 125,000 words and their transcriptions.
Phonetic symbols for English words in the
dictionaries are transformed to their pronuncia-
tion information by using an internal code table.
The internal code table represents mappings from
each phonetic symbol to a single character within
ASCII code table. Our pronouncing dictionary in-
cludes a list of words and their pronunciation in-
formation.
For a given English word, if the word exists
in the pronouncing dictionary, then its pronunci-
ations are translated to Korean graphemes by a
mapping table and transformation rules, which are
defined by ?Oeraeeo pyogibeop?.
2.4 Phonics-based Method
Phonics is a pronunciation-based linguistic teach-
ing method, especially for children (Strickland,
1998). Originally, it was designed to connect the
sounds of spoken English with group of English
letters. In this research, we modify the phonics
in order to connect English sounds to Korean let-
ter because in Korean there is nearly a one-to-one
correspondence between sounds and the letter pat-
terns that represent them. For example, alpha-
bet ?b? can be pronounced to ??(bieup) in Ko-
rean. Consequently, we construct about 150 rules
which map English alphabet into one or more sev-
eral Korean graphemes, by referring to the phon-
ics. Though phonics cannot reveal all of the pro-
2http://www.speech.cs.cmu.edu/cgi-bin/cmudict
nunciation of English words, the conversion from
English alphabet into Korean letter is performed
simply and efficiently. We apply the phonics in
serial order from left to right of each input word.
If multiple rules are applicable, the most specific
rules are fist applied.
3 Experiments
3.1 Experimental Setup
We participate in both standard and non-standard
tracks for English-Korean name transliteration in
NEWS 2009 Machine Transliteration Shared Task
(Li et al, 2009). Experimenting on the develop-
ment data, we determine the best performing pa-
rameters for MOSES as follows.
? Maximum Phrase Length: 3
? Language Model N-gram Order: 3
? Language Model Smoothing: Kneser-Ney
? Phrase Alignment Heuristic: grow-diag-final
? Reordering: Monotone
? Maximum Distortion Length: 0
With above parameter setup, the results are pro-
duced from the following five different systems.
? Baseline System (BS): For the standard task,
we use only given official training data 3 to con-
struct translation model and language model for
our base system.
? Expanded Resource (ER): For all four non-
standard tasks, we use the examples of writing for-
eign names as additional training data. The ex-
amples are provided from the National Institute of
the Korean Language4. The data originally con-
sists of around 27,000 person names and around
7,000 place names including non-Ascii characters
for English side words as well as duplicate entries.
We preprocess the data in order to use 13,194 dis-
3Refer to Website http://www.cjk.org for more informa-
tion
4The resource is open to public. See
http://www.korean.go.kr/eng for more information.
110
tinct pairs of English names and Korean transliter-
ation.
? Web-based Re-ranking (WR): We re-rank the
result of ER by applying the method described in
section 2.2.
? Pronouncing Dictionary-based Method (PD):
The re-ranking of WR by combining with the
method described in section 2.3.
? Phonics-based Method (PB): The re-ranking
of WR by combining with the method described in
section 2.4.
The last two methods re-rank the WR method
by applying pronouncing dictionary-based method
and Phonics-based method. We restrict that
the pronouncing dictionary-based method and
Phonics-based method can produce only one out-
put, and use the outputs of the two methods to re-
rank (again) the result of Web-based re-ranking.
When re-ranking the results, we heuristically com-
bined the outputs of PD or PB with the n-best re-
sult of WR. If the outputs of the two methods exist
in the result of WR, we add some positive scores to
the original scores of WR. Otherwise, we inserted
the result into fixed position of the rank. The fixed
position of rank is empirically decided using de-
velopment set. We inserted the output of PD and
PB at second rank and at sixth rank, respectively.
3.2 Experimental Results
Table 1 shows our experimental results of the five
systems on the test data. We found that the use
of additional training data (ER) and web-based re-
ranking (WR) have a strong effect on translitera-
tion performance. However, the integration of the
PD or PB with WB proves not to significantly con-
tribute the performance. To find more elaborate
integration of those results will be one of our fu-
ture work.
The MAPsys value of the three re-ranking
methods WR, PD, and PB are relatively higher
than other methods because we filter out some
candidates in n-best by their Web frequencies. In
addition to the standard evaluation measures, we
include the Mean Fdec to measure the Levenshtein
distance between reference and the output of the
decoder (decomposed result).
4 Conclusions
In this paper, we proposed a hybrid approach to
English-Korean name transliteration. The system
is built on MOSES with factored translation fea-
tures. When evaluating the proposed methods,
we found that the use of additional training data
can significantly outperforms the baseline system.
Also, the experimental result of using three n-best
re-ranking techniques shows that the Web-based
re-ranking is proved to be a useful method. How-
ever, our two integration methods with dictionary-
based or rule-based method does not show the sig-
nificant gain over the Web-based re-ranking.
For future work, we plan to devise more elab-
orate way to integrate statistical method and dic-
tionary or rule-based method to further improve
the transliteration performance. Also, we will ap-
ply the proposed techniques to possible applica-
tions such as SMT or Cross Lingual Information
Retrieval.
References
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Demo and Poster Sessions, June.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of news 2009
machine transliteration shared task. In Proceed-
ings of ACL-IJCNLP 2009 Named Entities Work-
shop (NEWS 2009), Singapore.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29.
D.S. Strickland. 1998. Teaching phonics today: A
primer for educators. International Reading Asso-
ciation.
111
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 474?482,
Beijing, August 2010
An Empirical Study on Web Mining of Parallel Data 
Gumwon Hong1, Chi-Ho Li2, Ming Zhou2 and Hae-Chang Rim1 
1Department of Computer Science & En-
gineering, Korea University 
{gwhong,rim}@nlp.korea.ac.kr 
2Natural Language Computing Group, 
Microsoft Research Asia 
{chl,mingzhou}@microsoft.com 
 
Abstract 
This paper 1  presents an empirical ap-
proach to mining parallel corpora. Con-
ventional approaches use a readily 
available collection of comparable, non-
parallel corpora to extract parallel sen-
tences. This paper attempts the much 
more challenging task of directly search-
ing for high-quality sentence pairs from 
the Web. We tackle the problem by 
formulating good search query using 
?Learning to Rank? and by filtering 
noisy document pairs using IBM Model 
1 alignment. End-to-end evaluation 
shows that the proposed approach sig-
nificantly improves the performance of 
statistical machine translation. 
1 Introduction 
Bilingual corpora are very valuable resources in 
NLP. They can be used in statistical machine 
translation (SMT), cross language information 
retrieval, and paraphrasing. Thus the acquisition 
of bilingual corpora has received much attention. 
Hansards, or parliamentary proceedings in 
more than one language, are obvious source of 
bilingual corpora, yet they are about a particular 
domain and therefore of limited use. Many re-
searchers then explore the Web. Some approach 
attempts to locate bilingual text within a web 
page (Jiang et al, 2009); some others attempt to 
collect web pages in different languages and 
decide the parallel relationship between the web 
pages by means of structural cues, like exist-
ence of a common ancestor web page, similarity 
between URLs, and similarity between the 
HTML structures (Chen and Nie, 2000; Resnik 
                                                 
1 This work has been done while the first author was visit-
ing Microsoft Research Asia. 
and Smith, 2003; Yang and Li, 2003; Shi et al, 
2006). The corpora thus obtained are generally 
of high quality and wide variety in domain, but 
the amount is still limited, as web pages that 
exhibit those structural cues are not abundant. 
Some other effort is to mine bilingual corpora 
by textual means only. That is, two pieces of 
text are decided to be parallel merely from the 
linguistic perspective, without considering any 
hint from HTML markup or website structure. 
These approaches (Zhao and Vogel, 2002; 
Utiyama and Isahara 2003; Fung and Cheung, 
2004; Munteanu and Marcu, 2005; Abdul-Rauf 
and Schwenk, 2009) share roughly the same 
framework: 
Phase 1: Document Pair Retrieval 
1) documents in some target language (TL) are 
stored in some database; 
2) each document in some source language (SL) 
is represented by some TL keywords; 
3) the TL keywords in (2) are used to assign 
some TL documents to a particular SL doc-
ument, using some information retrieval (IR) 
technique. For example, Munteanu and Mar-
cu (2005) apply the Lemur IR toolkit, 
Utiyama and Isahara (2003) use the BM25 
similarity measure, and Fung and Cheung 
(2004) use cosine similarity. Each TL docu-
ment pairs up with the SL document to form 
a candidate parallel document pair. 
Phase 2: Sentence Pair Extraction 
1) sentence pairs can be obtained by running 
sentence alignment over all candidate docu-
ment pairs (or a selection of them) (Zhao and 
Vogel, 2002; Utiyama and Isahara, 2003); 
2) sentence pairs can also be selected, by some 
classifier or reliability measure, from the 
candidate sentence pairs enumerated from 
the candidate document pairs (Munteanu and 
Marcu, 2005). 
Note that the primary interest of these ap-
proaches is sentence pairs rather than document 
474
pairs, partially because document pair retrieval 
is not accurate, and partially because the ulti-
mate purpose of these corpora is SMT training, 
which is based on sentence pairs. It is found that 
most of the sentence pairs thus obtained are not 
truly parallel; rather they are loose translations 
of each other or they carry partially similar mes-
sages. Such bilingual corpora are thus known as 
comparable corpora, while genuinely mutual 
translations constitute parallel corpora.  
Note also that all these comparable corpus 
mining approaches are tested on closed docu-
ment collections only. For example, Zhao and 
Vogel (2002), Utiyama and Isahara (2003), and 
Munteanu and Marcu (2005) all acquire their 
comparable corpora from a collection of news 
articles which are either downloaded from the 
Web or archived by LDC. The search of candi-
date document pairs in such a closed collection 
is easy in three ways:  
1) all the TL documents come from the same 
news agency and they are not mixed up with 
similar documents from other news agencies;  
2) all the TL documents are news text and they 
are not mixed up with text of other domains;  
3) in fact, the search in these approaches is 
made easier by applying tricks like date win-
dow. 
There is no evidence that these methods apply 
to corpus mining from an open document col-
lection (e.g. the entire Web) without search con-
straint. The possibility of open-ended text min-
ing is a crucial problem. 
This paper focuses on bilingual corpus min-
ing using only textual means. It attempts to an-
swer two questions: 
1) Can comparable corpus mining be applied to 
an open document collection, i.e., the Web? 
2) Can comparable corpus mining be adapted to 
parallel corpus mining? 
We give affirmation to both questions. For the 
first problem, we modify document pair 
retrieval so that there is no longer a closed set of 
TL documents. Instead we search for candidate 
TL documents for a particular SL document 
from the Web by means of some Web search 
engine. For the second problem, in Phase 2 we 
replace the sentence pair classifier by a 
document pair filter and a sentence alignment 
module. Based on end-to-end SMT experiments, 
we will show that 1) high quality bilingual 
corpora can be mined from the Web; 2) the very 
first key to Web-mining of bilingual corpus is 
the formulation of good TL keywords to 
represent a SL document; 3) a simple document 
pair filter using IBM Model 1 probabilities is 
able to identify parallel corpus out of noisy 
comparable text; and 4) Web-mined parallel 
corpus, despite its smaller size, improves SMT 
much more than Web-mined comparable corpus. 
2 Problem Setting 
Our ultimate goal is to mine from the Web 
training data for translation from Chinese (SL) 
to English (TL). As the first step, about 11,000 
Chinese web pages of news articles are crawled 
from some Chinese News sites. Then the task is 
to search for the English sentences correspond-
ing to those in the selected SL articles. These 
selected SL news articles all contain cue phrases 
like ???????? (according to foreign me-
dia), as these cue phrases suggest that the Chi-
nese articles are likely to have English counter-
parts. Moreover, each selected SL article has at 
least 500 words (empirically determined) since 
we assume that it is much easier to formulate 
reliable keywords from a long document than a 
short one. 
3 Document Pair Retrieval 
Conventional approaches to comparable corpus 
mining usually start with document pair retriev-
al, which assigns to each SL document a set of 
candidate TL documents. This step is essentially 
a preliminary search for candidate sentence 
pairs for further scrutiny in Phase 2. The target 
is to find document pairs which may contain 
many good sentence pairs, rather than to discard 
document pairs which may not contain good 
sentence pairs. Therefore, recall is much more 
emphasized than precision. 
Document pair retrieval in conventional ap-
proaches presumes a closed set of TL docu-
ments which some IR system can handle easily. 
In this paper we override this presumption and 
attempt a much more challenging retrieval task, 
viz. to search for TL documents among the Web, 
using the search engines of Google and Yahoo. 
Therefore we are subject to a much noisier data 
domain. The correct TL documents may not be 
indexed by the search engines at all, and even 
when the target documents are indexed, it re-
475
quires a more sophisticated formulation of que-
ries to retrieve them. 
In response to these challenges, we propose 
various kinds of queries (elaborated in the fol-
lowing subsections). Moreover, we merge the 
TL documents found by each query into a big 
collection, so as to boost up the recall. In case a 
query fails to retrieve any document, we itera-
tively drop a keyword in the query until some 
documents are found. On the other hand, alt-
hough the document pairs in question are of 
news domain, we use the general Google/Yahoo 
web search engines instead of the specific news 
search engines, because 1) the news search en-
gines keep only a few web pages for all pages 
about the same news event, and 2) we leave 
open possibility for correct TL documents to be 
found in non-news web pages.  
3.1 Simple Queries 
There are three baseline formulations of queries: 
1) Query of translations of SL TF-IDF-ranked 
keywords (QSL-TFIDF). This is the method 
proposed by Munteanu and Marcu (2005). 
All the words in a SL document are ranked 
by TF-IDF and the top-N words are selected. 
Each keyword is then translated into a few 
TL words by a statistically learned diction-
ary. In our experiments the dictionary is 
learned from NIST SMT training data.  
2) Query of TF-IDF-ranked machine translated 
keywords (QTL-TFIDF). It is assumed that a 
machine translation (MT) system is better at 
handling lexical ambiguity than simple dic-
tionary translation. Thus we propose to first 
translate the SL document into TL and ex-
tract the top-N TF-IDF-ranked words as 
query. In our experiments the MT system 
used is hierarchical phrase-based system 
(Chiang, 2007).2 
3) Query of named entities (QNE). Another 
way to tackle the drawback of QSL-TFIDF is to 
focus on named entities (NEs) only, since 
NEs often provide strong clue for identify-
ing correspondence between two languages. 
All NEs in a SL document are ranked by 
TF-IDF, and the top-N NEs are then trans-
lated (word by word) by dictionary. In our 
experiments we identify SL (Chinese) NEs 
                                                 
2 We also try online Google translation service, and the 
performance was roughly the same. 
implicitly found by the word segmentation 
algorithm stated in Gao et al (2003), and 
the dictionaries for translating NEs include 
the same one used for QSL-TFIDF, and the 
LDC  Chinese/English NE dictionary. For 
the NEs not covered by our dictionary, we 
use Google translation service as a back-up. 
A small-scale experiment is run to evaluate 
the merits of these queries. 300 Chinese news 
web pages in three different periods (each 100) 
are collected. For each Chinese text, each query 
(containing 10 keywords) is constructed and 
submitted to both Google and Yahoo Search, 
and top-40 returned English web pages for each 
search are kept. Note that the Chinese news ar-
ticles are not part of 11,000 pages in section 2. 
In fact, they do not only satisfy the requirement 
of length and cue phrases (described in section 
2), but they also have another property that they 
are translated from some English news articles 
(henceforth target pages) on the Web. Thus they 
are ideal data for studying the performance of 
document pair retrieval. 
To test the influence of translation quality in 
document pair retrieval, we also try ?oracle que-
ries?, i.e. queries formulated directly from the 
target pages:  
1) OQTFIDF. This is the query of the top-N TF-
IDF-ranked words from the target page. 
2) OQNE. This is the query of the top-N TF-
IDF-ranked NEs from the target web page. 
We define recall as the proportion of SL docu-
ments whose true target pages are found. The 
comparison between a retrieved page and the 
target page is done by Longest Common Subse-
quence (LCS) ratio, defined as the length of the 
longest common word sequence of two docu-
ments divided by the length of the longer of two 
documents. The threshold 0.7 is adopted as it is 
strict enough to distinguish parallel document 
pairs from non-parallel ones. 
Table 1 shows the recalls for various queries. 
It can be seen from Tests 6 and 7 that the largest 
recall, 85% (within top 40 search results), is 
achieved when the word distributions in the tar-
get web pages are known. In the real scenario 
where the true English word distribution is not 
known, the recalls achieved by the simple que-
ries are very unsatisfactory, as shown by Tests 1 
to 3. This clearly shows how challenging Web-
based mining of bilingual corpora is. Another 
challenge can be observed in comparing across 
476
columns, viz. it is much more difficult to re-
trieve outdated news document pairs. This im-
plies that bilingual news mining must be incre-
mentally carried out.  
Comparing Test 1 to Tests 2 and 3, it is obvi-
ous that QSL-TFIDF is not very useful in document 
pair retrieval. This confirms our hypothesis that 
suitable TL keywords are not likely to be ob-
tained by simple dictionary lookup. While the 
recalls by QTL-TFIDF are similar to those by QNE, 
the two queries contribute in different ways. 
Test 4 simply merges the Web search results in 
Tests 2 and 3. The significantly higher recalls in 
Test 4 imply that each of the two queries finds 
substantially different targets than each other. 
The comparison of Test 5 to Test 4 further con-
firms the weakness of QSL-TFIDF. 
The huge gap between the three simple que-
ries and the oracle queries shows that the quality 
of translation of keywords from SL to TL is a 
major obstacle. There are two problems in trans-
lation quality: 1) the MT system or dictionary 
cannot produce any translation for a SL word 
(let us refer to such TL keywords as ?Utopian 
translations?); 2) the MT system or dictionary 
produces an incorrect translation for a SL word. 
We can do very little for the Utopian transla-
tions, as the only solution is simply to use a bet-
ter MT system or a larger dictionary. On the 
contrary, it seems that the second problem can 
somewhat be alleviated, if we have a way to 
distinguish those terms that are likely to be cor-
rect translations from those terms that are not. 
In other words, it may be worthwhile to reorder 
candidate TL keywords by our confidence in its 
translation quality.  
Tests 8 and 9 in Table 1 show that this hy-
pothesis is promising. In both tests the TF-IDF-
based (Test 8) or the NE-based (Test 9) key-
words are selected from only those TL words 
that appear both in the target page and the ma-
chine translated text of the source page. In other 
words, we ensure that the keywords in the query 
must be correct translations. The recalls (espe-
cially the recalls by NE-based query in Test 9) 
are very close to the recalls by oracle queries. 
The conclusion is, even though we cannot pro-
duce the Utopian translations, document pair 
retrieval can be improved to a large extent by 
removing incorrect translations. Even an imper-
fect MT system or NE dictionary can help us 
achieve as good document pair retrieval recall 
as oracle queries.  
In the next subsection we will take this in-
sight into our bilingual data mining system, by 
selecting keywords which are likely to be cor-
rect translation.  
3.2 Re-ranked Queries 
Machine learning is applied to re-rank key-
words for a particular document. The re-ranking 
of keywords is based on two principles. The 
first one is, of course, the confidence on the 
translation quality. The more likely a keyword 
is a correct translation, the higher this keyword 
should be ranked. The second principle is the 
representativeness of document. The more rep-
resentative of the topic of the document where a 
keyword comes from, the higher this keyword 
should be ranked. The design of features should 
incorporate both principles.  
The representativeness of document is mani-
fested in the following features for each key-
word per each document: 
? TF: the term frequency. 
? IDF: the inverted document frequency. 
? TF-IDF: the product of TF and IDF. 
? Title word: it indicates whether a key-
word appears in the title of the document. 
? Bracketed word: it indicates whether a 
word is enclosed in a bracket in the 
source document. 
? Position of first appearance: the position 
where a keyword first appears in a doc-
ument, normalized by number of words 
in the document. 
ID Query Remote Near Recent 
1 QSL-TFIDF 7 6 8 
2 QTL-TFIDF 16 19 32 
3 QNE 16 21 38 
4 union(2,3) 27 31 48 
5 union(1,2,3) 28 31 48 
6 OQTFIDF 56 66 82 
7 OQNE 62 68 85 
8 OverlapTFIDF 52 51 74 
9 OverlapNE 55 62 83 
Table 1: Recall (%age) of simple queries. ?Remote? 
refers to news documents more than a year ago; 
?Near? refers to documents about 3 months ago; ?Re-
cent? refers to documents in the last two weeks. 
477
? NE types: it indicates whether a keyword 
is a person, organization, location, nu-
merical expression, or non NE. 
The confidence on translation quality is man-
ifested in the following features: 
? Translation source: it indicates whether 
the keyword (in TL) is produced by MT 
system, dictionary, or by both. 
? Original word: it indicates whether the 
keyword is originally written in English 
in the source document. Note that this 
feature also manifests the representative-
ness of a document. 
? Dictionary rank: if the keyword is a NE 
produced by dictionary, this feature indi-
cates the rank of the NE keyword among 
all translation options registered in the 
dictionary.  
It is difficult to definitely classify a TL key-
word into good or bad translation in absolute 
sense, and therefore we take the alternative of 
ranking TL keywords with respect to the two 
principles. The learning algorithm used is Rank-
ing SVM (Herbrich et al, 2000; Joachims, 
2006), which is a state-of-the-art method of the 
?Learning to rank? framework. 
The training dataset of the keyword re-ranker 
comprises 1,900 Chinese/English news docu-
ment pairs crawled from the Web3. This set is 
not part of 11,000 pages in section 2. These 
document pairs share the same properties as 
those 300 pairs used in Section 3.1. For each 
English/target document, we build a set TALL, 
which contains all words in the English docu-
ment, and also a set TNE, which is a subset of 
TALL such that all words in TNE are NEs in TALL. 
The words in both sets are ranked by TFIDF. 
On the other hand, for each Chinese/source 
document, we machine-translate it and then 
store the translated words into a set S, and we 
also add the dictionary translations of the source 
NEs into S. Note that S is composed of both 
good translations (appearing in the target docu-
ment) and bad translations (not appearing in the 
target document).  
Then there are two ways to assign labels to 
the words in S. In the first way of labeling 
(LALL), the label 3 is assigned to those words in 
S which are ranked among top 5 in TALL, label 2 
                                                 
3 We also attempt to add more training data for re-ranking 
but the performance remain the same. 
to those ranked among top 10 but not top 5 in 
TALL, 1 to those beyond top 10 but still in TALL, 
and 0 to those words which do not appear in 
TALL at all. The second way of labeling, LNE, is 
done in similar way with respect to TNE. Col-
lecting all training samples over all document 
pairs, we can train a model, MALL, based on la-
beling LALL, and another model MNE, based on 
labeling LNE. 
The trained models can then be applied to re-
rank the keywords of simple queries. In this 
case, a set STEST is constructed from the 300 
Chinese documents in similar way of construct-
ing S. We repeat the experiment in Section 3.1 
with two new queries: 
1) QRANK-TFIDF: the top N keywords from re-
ranking STEST by MALL; 
2) QRANK-NE: the top N keywords from rerank-
ing STEST by MNE. 
Again N is chosen as 10. 
The results shown in Table 2 indicate that, 
while the re-ranked queries still perform much 
poorer than oracle queries (Tests 6 and 7 in Ta-
ble 1), they show great improvement over the 
simple queries (Tests 1 to 5 in Table 1). The 
results also show that re-ranked queries based 
on NEs are more reliable than those based on 
common words. 
4 Sentence pair Extraction 
The document pairs obtained by the various 
queries described in Section 3 are used to pro-
duce sentence pairs as SMT training data. There 
are two different methods of extraction for cor-
pora of different nature. 
4.1 For Comparable Corpora 
Sentence pair extraction for comparable corpus 
is the same as that elaborated in Munteanu and 
Marcu (2005). All possible sentence pairs are 
enumerated from all candidate document pairs 
produced in Phase 1. These huge number of 
candidate sentence pairs are first passed to a 
coarse sentence pair filter, which discards very 
unlikely candidates by heuristics like sentence 
ID Query Remote Near Recent 
10 QRANK-TFIDF 18 20 29 
11 QRANK-NE 35 43 54 
12 union(10,11) 39 49 63 
Table 2: Recall (%age) of re-ranked queries. 
 
478
length ratio and percentage of word pairs regis-
tered in some dictionary. 
The remaining candidates are then given to a 
Maximum Entropy based classifier (Zhang, 
2004), which uses features based on alignment 
patterns produced by some word alignment 
model. In our experiment we use the HMM 
alignment model with the NIST SMT training 
dataset. The sentence pairs which are assigned 
as positive by the classifier are collected as the 
mined comparable corpus.  
4.2 For Parallel Corpora 
The sentence pairs obtained in Section 4.1 are 
found to be mostly not genuine mutual transla-
tions. Often one of the sentences contains some 
extra phrase or clause, or even conveys different 
meaning than the other. It is doubtful if the doc-
ument pairs from Phase 1 are too noisy to be 
processed by the sentence pair classifier. An 
alternative way for sentence pair extraction is to 
further filter the document pairs and discard any 
pairs that do not look like parallel.  
It is hypothesized that the parallel relation-
ship between two documents can be assimilated 
by the word alignment between them. The doc-
ument pair filter produces the Viterbi alignment, 
with the associated probability, of each docu-
ment pair based on IBM Model 1 (Brown et al, 
1993). The word alignment model (i.e. the sta-
tistical dictionary used by IBM Model 1) is 
trained on the NIST SMT training dataset. The 
probability of the Viterbi alignment of a docu-
ment pair is the sole basis on which we decide 
whether the pair is genuinely parallel. That is, 
an empirically determined threshold is used to 
distinguish parallel pairs from non-parallel ones. 
In our experiment, a very strict threshold is se-
lected so as to boost up the precision at the ex-
pense of recall. 
There are a few important details that enable 
the document pair filter succeed in identifying 
parallel text: 
1) Function words and other common words 
occur frequently and so any pair of common 
word occupies certain probability mass in 
an alignment model. These common words 
enable even non-parallel documents achieve 
high alignment probability. In fact, it is well 
known that the correct alignment of com-
mon words must take into account position-
al and/or structural factors, and it is benefi-
cial to a simple alignment model like IBM 
Model 1 to work on data without common 
words. Therefore, all words on a compre-
hensive stopword list must be removed 
from a document pair before word align-
ment. 
2) The alignment probability must be normal-
ized with respect to sentence length, so that 
the threshold applies to all documents re-
gardless of document length.  
Subjective evaluation on selected samples 
shows that most of the document pairs kept by 
the filter are genuinely parallel. Thus the docu-
ment pairs can be broken down into sentence 
pairs simply by a sentence alignment method. 
For the sentence alignment, our experiments use 
the algorithm in Moore (2002). 
5 Experiments 
It is a difficult task to evaluate the quality of 
automatically acquired bilingual corpora. As our 
ultimate purpose of mining bilingual corpora is 
to provide more and better training data for 
SMT, we evaluate the parallel and comparable 
corpora with respect to improvement in Bleu 
score (Papineni et al, 2002). 
5.1 Experiment Setup 
Our experiment starts with the 11,000 Chinese 
documents as described in Section 2. We use 
various combinations of queries in document 
pair retrieval (Section 3). Based on the candi-
date document pairs, we produce both compara-
ble corpora and parallel corpora using sentence 
pair extraction (Section 4). The corpora are then 
given to our SMT systems as training data. 
The SMT systems are our implementations of 
phrase-based SMT (Koehn et al, 2003) and hi-
erarchical phrase-based SMT (Chiang, 2007). 
The two systems employ a 5-gram language 
model trained from the Xinhua section of the 
Gigaword corpus. There are many variations of 
the bilingual training dataset. The B1 section of 
the NIST SMT training set is selected as the 
baseline bilingual dataset; its size is of the same 
order of magnitude as most of the mined corpo-
ra so that the comparison is fair. Each of the 
mined bilingual corpora is compared to that 
baseline dataset, and we also evaluate the per-
formance of the combination of each mined bi-
lingual corpus with the baseline set. 
479
The SMT systems learn translation knowledge 
(phrase table and rule table) in standard way. 
The parameters in the underlying log-linear 
model are trained by Minimum Error Rate 
Training (Och, 2003) on the development set of 
NIST 2003 test set. The quality of translation 
output is evaluated by case-insensitive BLEU4 
on NIST 2005 and NIST 2008 test sets4. 
5.2 Experimental result 
Table 3 lists the size of various mined parallel 
and comparable corpora against the baseline B1 
bilingual dataset. It is obvious that for a specific 
type of query in document pair retrieval, the 
parallel corpus is significantly smaller than the 
corresponding comparable corpus. 
The apparent explanation is that a lot of doc-
ument pairs are discarded due to the document 
                                                 
4 It is checked that there is no sentence in the test sets 
overlapping with any sentences in the mined corpus. 
pair filter. Note that the big difference in size of 
the two comparable corpora by single queries, 
i.e., QRANK-NE and M&M, verifies again that re-
ranked queries based on NEs are more reliable 
in sentence pair extraction. 
Table 4 lists the Bleu scores obtained by 
augmenting the baseline bilingual training set 
with the mined corpora. The most important 
observation is that, despite their smaller size, 
parallel corpora lead to no less, and often better, 
improvement in translation quality than compa-
rable corpora. That is especially true for the 
case where document pair retrieval is based on 
all five types of query5. The superiority of paral-
lel corpora confirms that, in Phase 2 (sentence 
pair extraction), quality is more important than 
quantity and thus the filtering of document 
pair/sentence pair must not be generous. 
On the other hand, sentence pair extraction 
for parallel corpora generally achieves the best 
result when all queries are applied in document 
pair retrieval. It is not sufficient to use the more 
sophisticated re-ranked queries. That means in 
Phase 1 quantity is more important and we must 
seek more ways to retrieve as many document 
pairs as possible. That also confirms the empha-
sis on recall in document pair retrieval.  
Looking into the performance of comparable 
corpora, it is observed that the M&M query 
does not effectively apply to Web mining of 
comparable corpora but the proposed queries do. 
Any of the proposed query leads to better result 
than the conventional method, i.e. M&M. 
Moreover, it can be seen that all four combina-
tions of proposed queries achieve similar per-
                                                 
5 QSL-TFIDF, QTL-TFIDF, QNE, QRANK-TFIDF, and QRANK-NE 
Queries SP 
extraction 
#SP #SL 
words 
#TL 
words 
Baseline: B1 in NIST 68K 1.7M 1.9M 
M&M comparable 43K 1.1M 1.2M 
QRANK-NE comparable 98K 2.7M 2.8M 
all simple comparable 98K 2.6M 2.9M 
all ranked comparable 115K 3.1M 3.3M 
all query comparable 135K 3.6M 4.0M 
QRANK-NE 
all simple 
parallel 
parallel 
66K 
52K 
1.9M 
1.5M 
1.8M 
1.4M 
all ranked parallel 73K 2.1M 2.0M 
all query parallel 90K 2.5M 2.4M 
Table 3: Statistics on corpus size. SP means sentence 
pair. ?all simple?, ?all ranked?, and ?all query? refer to 
the merge of the retrieval results of all simple queries, 
all re-ranked queries, and all simple and re-ranked que-
ries, respectively; M&M (after Munteanu and Marcu 
(2005)) refers to QSL-TFIDF.  
Bilingual Training Corpus 
Phrase-based SMT (PSMT) Hierarchical PSMT 
NIST 2005 NIST 2008 NIST 2005 NIST 2008 
B1 (baseline) 33.08 21.66 32.85 21.18 
B1+comparable(M&M) 33.51(+0.43) 22.71(+1.05) 32.99(+0.14) 22.11(+0.93) 
B1+comparable(QRANK-NE) 34.81(+1.73) 23.30(+1.64) 34.43(+1.58) 22.85(+1.67) 
B1+comparable(all simple) 34.74(+1.66) 23.48(+1.82) 34.28(+1.43) 23.18(+2.00) 
B1+comparable(all ranked) 34.79(+1.71) 23.48(+1.82) 34.37(+1.52) 23.06(+1.88) 
B1+comparable(all query) 34.74(+1.66) 23.19(+1.53) 34.46(+1.61) 23.12(+1.94) 
B1+parallel(QRANK-NE) 34.75(+1.67) 23.37(+1.71) 34.24(+1.39) 23.45(+2.27) 
B1+parallel(all simple) 34.99(+1.91) 23.96(+2.30) 34.94(+2.09) 23.35(+2.17) 
B1+parallel(all ranked) 34.76(+1.68) 23.41(+1.75) 34.54(+1.69) 23.59(+2.41) 
B1+parallel(all query) 35.40(+2.32) 23.47(+1.81) 35.27(+2.42) 23.61(+2.43) 
Table 4: Evaluation of translation quality improvement by mined corpora. The figures inside brackets refer 
to the improvement over baseline. The bold figures indicate the highest Bleu score in each column for 
comparable corpora and parallel corpora, respectively. 
480
formance. This illustrates a particular advantage 
of using a single re-ranked query, viz. QRANK-NE, 
because it significantly reduces the retrieval 
time and downloading space required for docu-
ment pair retrieval as it is the main bottleneck of 
whole process. 
Table 5 lists the Bleu scores obtained by re-
placing the baseline bilingual training set with 
the mined corpora. It is easy to note that transla-
tion quality drops radically by using mined bi-
lingual corpus alone. That is a natural conse-
quence of the noisy nature of Web mined data. 
We should not be too pessimistic about Web 
mined data, however. Comparing the Bleu 
scores for NIST 2005 test set to those for NIST 
2008 test set, it can be seen that the reduction of 
translation quality for the NIST 2008 set is 
much smaller than that for the NIST 2005 set. It 
is not difficult to explain the difference. Both 
the baseline B1 training set and the NIST 2005 
comprise news wire (in-domain) text only. Alt-
hough the acquisition of bilingual data also tar-
gets news text, the noisy mined corpus can nev-
er compete with the well prepared B1 dataset. 
On the contrary, the NIST 2008 test set contains 
a large portion of out-of-domain text, and so the 
B1 set does not gain any advantage over Web 
mined corpora. It might be that better and/or 
larger Web mined corpus achieves the same 
performance as manually prepared corpus.  
Note also that the reduction in Bleu score by 
each mined corpus is roughly the same as that 
by each other, while in general parallel corpora 
are slightly better than comparable corpora. 
6 Conclusion and Future Work 
In this paper, we tackle the problem of mining 
parallel sentences directly from the Web as 
training data for SMT. The proposed method 
essentially follows the corpus mining frame-
work by pioneer work like Munteanu and Mar-
cu (2005). However, unlike those conventional 
approaches, which work on closed document 
collection only, we propose different ways of 
formulating queries for discovering parallel 
documents over Web search engines. Using 
learning to rank algorithm, we re-rank keywords 
based on representativeness and translation 
quality. This new type of query significantly 
outperforms existing query formulation in re-
trieving document pairs. We also devise a doc-
ument pair filter based on IBM model 1 for 
handling the noisy result from document pair 
retrieval. Experimental results show that the 
proposed approach achieves substantial im-
provement in SMT performance. 
For mining news text, in future we plan to 
apply the proposed approach to other language 
pairs. Also, we will attempt to use meta-
information implied in SL document, such as 
?publishing date? or ?news agency name?, as 
further clue to the document pair retrieval. Such 
meta-information may likely to increase the 
precision of retrieval, which is important to the 
efficiency of the retrieval process. 
An important contribution of this work is to 
show the possibility of mining text other than 
news domain from the Web, which is another 
piece of future work. The difficulty of this task 
should not be undermined, however. Our suc-
cess in mining news text from the Web depends 
on the cue phrases available in news articles. 
These cue phrases more or less indicate the ex-
istence of corresponding articles in another lan-
guage. Therefore, to mine non-news corpus, we 
should carefully identify and select cue phrases.  
Bilingual Training Corpus 
Phrase-based SMT Hierarchical PSMT 
NIST 2005 NIST 2008 NIST 2005 NIST 2008 
B1 (baseline) 33.08 21.66 32.85 21.18 
comparable(M&M) 20.84(-12.24) 14.33(-7.33) 20.65(-12.20) 13.73(-7.45) 
comparable(QRANK-NE) 26.78(-6.30) 18.54(-3.12) 27.10(-5.75) 18.02(-3.16) 
comparable(all simple) 26.39(-6.69) 18.52(-3.14) 26.40(-6.45) 18.22(-2.96) 
comparable(all ranked) 27.36(-5.72) 18.89(-2.77) 27.40(-5.45) 18.72(-2.46) 
comparable(all query) 27.96(-5.12) 19.27(-2.39) 27.83(-5.02) 19.46(-1.72) 
parallel(QRANK-NE) 26.37(-6.71) 18.70(-2.96) 26.47(-6.38) 18.51(-2.67) 
parallel(all simple) 25.65(-7.43) 18.69(-2.97) 25.28(-7.57) 18.55(-2.63) 
parallel(all ranked) 26.86(-6.22) 18.94(-2.72) 27.10(-5.75) 18.78(-2.40) 
parallel(all query) 27.58(-5.50) 19.73(-1.93) 28.10(-4.75) 19.52(-1.66) 
Table 5: Evaluation of translation quality by mined corpora. 
481
References 
Abdul-Rauf, Sadaf and Holger Schwenk. 2009. Ex-
ploiting Comparable Corpora with TER and 
TERp. In Proceedings of ACL-IJCNLP 2009 
workshop on Building and Using Comparable 
Corpora, pages 46?54. 
Brown, Peter F., Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Chen, Jiang and Jian-Yun Nie. 2000. Automatic 
Construction of Parallel Chinese-English Corpus 
for Cross-Language Information Retrieval. In 
Proceedings of NAACL-ANLP, pages 21-28. 
Chiang, David. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics, 33(2): 
202-228. 
Fung, Pascale, and Percy Cheung. 2004. Mining very 
non-parallel corpora: Parallel sentence and lexi-
con extraction via bootstrapping and EM. In Pro-
ceedings of 2004 Conference on Empirical Meth-
ods in Natural Language Processing, pages 57-63. 
Gao, Jianfeng, Mu Li, and Changning Huang. 2003. 
Improved Source-Channel Models for Chinese 
Word Segmentation. In Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, pages 272-279. 
Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. 
2000. Large margin rank boundaries for ordinal 
regression. In Advances in Large Margin Classifi-
ers, pages 115?132. MIT Press, Cambridge, MA. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua 
Liu, and Qingsheng Zhu. 2009. Mining Bilingual 
Data from the Web with Adaptively Learnt Pat-
terns. In Proceedings of the 47th Annual Meeting 
of the Association for Computational Linguistics 
and 4th International Joint Conference on Natural 
Language Processing, pages 870-878. 
Joachims, Thorsten. 2006. Training Linear SVMs in 
Linear Time. In Proceedings of the 12th ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining, pages 217-226.  
Koehn, Philipp, Franz Och, and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In Proceed-
ings of conference combining Human Language 
Technology conference series and the North 
American Chapter of the Association for Compu-
tational Linguistics conference series, pages 48-
54. 
Moore, Robert. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Proceedings 
of the 5th conference of the Association for Ma-
chine Translation in the Americas, pages 135?144. 
Munteanu, Dragos, and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
Och, Franz J. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 311-
318. 
Resnik, Philip, and Noah Smith. 2003. The Web as a 
Parallel Corpus. Computational Linguistics, 29(3): 
349-380. 
Shi, Lei, Cheng Niu, Ming Zhou, and Jianfeng Gao. 
2006. A DOM Tree Alignment Model for Mining 
Parallel Data from the Web. In Proceedings of the 
21st International Conference on Computational 
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 
489-496. 
Utiyama, Masao, and Hitoshi Isahara. 2003. Reliable 
Measures for Aligning Japanese-English News 
Articles and Sentences. In Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, pages 72-79. 
Vogel, Stephan. 2003. Using noisy bilingual data for 
statistical machine translation. In Proceedings of 
the 10th Conference of the European Chapter of 
the Association for Computational Linguistics, 
pages 175-178. 
Yang, Christopher C., and Kar Wing Li. 2003. Au-
tomatic construction of English/Chinese parallel 
corpora. Journal of the American Society for In-
formation Science and Technology, 54(8):730?
742. 
Zhang, Le. 2004. Maximum Entropy Modeling 
Toolkit for Python and C++. 
http://homepages.inf.ed.ac.uk/s0450736/maxent_t
oolkit.html 
Zhao, Bing, and Stephan Vogel. 2002. Adaptive Par-
allel Sentences Mining from Web Bilingual News 
Collection. In Proceedings of IEEE international 
conference on data mining, pages 745-750. 
482
Coling 2010: Poster Volume, pages 623?629,
Beijing, August 2010
A Post-processing Approach to Statistical Word Alignment  
Reflecting Alignment Tendency between Part-of-speeches 
Jae-Hee Lee1, Seung-Wook Lee1, Gumwon Hong1, 
Young-Sook Hwang2, Sang-Bum Kim2,  Hae-Chang Rim1 
1Dept. of Computer and Radio Communications Engineering, Korea University 
2Institute of Future Technology, SK Telecom 
1{jlee,swlee,gwhong,rim}@nlp.korea.ac.kr, 
2{yshwang,sangbum.kim}@sktelecom.com 
 
Abstract 
Statistical word alignment often suffers 
from data sparseness. Part-of-speeches 
are often incorporated in NLP tasks to 
reduce data sparseness. In this paper, 
we attempt to mitigate such problem by 
reflecting alignment tendency between 
part-of-speeches to statistical word 
alignment. Because our approach does 
not rely on any language-dependent 
knowledge, it is very simple and purely 
statistic to be applied to any language 
pairs. End-to-end evaluation shows that 
the proposed method can improve not 
only the quality of statistical word 
alignment but the performance of sta-
tistical machine translation. 
1 Introduction 
Word alignment is defined as mapping corre-
sponding words in parallel text. A word 
aligned parallel corpora are very valuable re-
sources in NLP. They can be used in various 
applications such as word sense disambigua-
tion, automatic construction of bilingual lexi-
con, and statistical machine translation (SMT). 
In particular, the initial quality of statistical 
word alignment dominates the quality of SMT 
(Och and Ney 2000; Ganchev et al, 2008); 
almost all current SMT systems basically refer 
to the information inferred from word align-
ment result. 
One of the widely used approaches to statis-
tical word alignment is based on the IBM 
models (Brown et al, 1993). IBM models are 
constructed based on words? co-occurrence 
and positional information. If sufficient train-
ing data are given, IBM models can be suc-
cessfully applied to any language pairs. How-
ever, for minority language pairs such as Eng-
lish-Korean and Swedish-Japanese, it is very 
difficult to obtain large amounts of parallel 
corpora. Without sufficient amount of parallel 
corpus, it is very difficult to learn the correct 
correspondences between words that infre-
quently occur in the training data. 
Part-of-speeches (POS), which represent 
morphological classes of words, can give valu-
able information about individual words and 
their neighbors. Identifying whether a word is 
a noun or a verb can let us predict which words 
are likely to be mapped in word alignment and 
which words are likely to occur in its vicinity 
in target sentence generation. 
Many studies incorporate POS information 
in SMT. Some researchers perform POS tag-
ging on their bilingual training data (Lee et al, 
2006; Sanchis and S?nchez, 2008). Some of 
them replace individual words as new words, 
such as in ?word/POS? form, producing new, 
extended vocabulary. The advantage of this 
approach is that POS information can help to 
resolve lexical ambiguity and thus improve 
translation quality. 
On the other hand, Koehn et al (2007) pro-
pose a factored translation model that can in-
corporate any linguistic factors including POS 
information in phrase-based SMT. The model 
provides a generalized representation of a 
translation model, because it can map multiple 
source and target factors. 
Although all of these approaches are shown 
to improve SMT performance by utilizing POS 
information, we observe that the influence is 
virtually marginal in two ways: 
623
 
 
 
 
1) The POS information tagged to each word 
may help to disambiguate in selecting 
word correspondences, but the increased 
vocabulary can also make the training data 
more sparse. 
2) The factored translation model may help to 
effectively handle out-of-vocabulary 
(OOV) by incorporating many linguistic 
factors, but it still crucially relies on the in-
itial quality of word alignment that will 
dominate the translation probabilities. 
This paper focuses on devising a better 
method for incorporating POS information in 
word alignment. It attempts to answer the fol-
lowing questions: 
1) Can the information regarding POS align-
ment tendency affect the post-processing 
of word alignment? 
2) Can the result of word alignment affected 
by such information help improving the 
quality of SMT? 
2 POS Alignment Tendency 
Despite the language pairs, words with similar 
POSs often correspond to each other in statisti-
cal word alignment. Similarly, words with dif-
ferent POSs are seldom aligned. For example, 
Korean proper nouns very often align with 
English proper nouns very often but seldom 
align with English adverbs. We believe that 
this phenomenon occurs not only on English-
Korean pairs but also on most of other lan-
guage pairs.  
Thus, in this study we hypothesize that all 
source language (SL) POSs have some rela-
tionship with target language (TL) POSs. Fig-
ure 1 exemplifies some results of using the 
IBM Models in English-Korean word align-
ment. As can be seen in the figure, the English 
word ?visiting? is incorrectly and excessively 
aligned to four Korean morphemes ?maejang?, 
?chat?, ?yeoseong?, and ?gogaek?. One reason 
for this is the sparseness of the training data; 
the only correct Korean morpheme ?chat? does 
not sufficiently co-occur with ?visiting? in the 
training data. However, it is generally believed 
that an English verb is more likely aligned to a 
Korean verb rather than a Korean noun. Like-
wise, we suppose that among many POSs, 
there are strong relationships between similar 
POSs and relatively weak relationships be-
tween different POSs. We hypothesize that the 
discovery of such relationships in advance can 
lead to better word alignment results. 
 In this paper, we propose a new method to 
obtain the relationship from word alignment 
results. The relationships among POSs, hence-
forth the POS alignment tendency, can be 
identified by the probability of the given POS 
pairs? alignment result where the source lan-
guage POS and the target language POS co-
occur in bilingual sentences. We formulate this 
idea using the maximum likelihood estimation 
as follows: 
     (          |   ( )    ( ))   
 
    
     (              ( )    ( ))
?      (           ( )    ( ))  *          +
 
 
where f and e denote source word and target 
word respectively. count() is a function that 
returns the number of co-occurrence of f and e 
when they are aligned (or not aligned). Then, 
we adjust the formula with the existing align-
ment score between f and e. 
     (   )        (   )   
             (   ) (          |   ( )    ( )) 
 
where )|( efPIBM  indicates the alignment prob-
ability estimated by the IBM models.   is a 
weighting parameter to interpolate the reliabili-
ties of both alignment factors. In the expe-
 
Figure 1. An example of inaccurate word alignment 
624
 
 
 
 
riment,   is empirically set to improve the 
word alignment performance ( =0.5).  
3 Modifying Alignment 
Based on the new scoring scheme as intro-
duced in the previous section, we modify the 
result of the initial word alignment. The modi-
fication is performed in the following proce-
dure: 
1. For each source word f that has out-bound 
alignment link other than null, 
2. Find the target word e that has the maxi-
mum alignment score according to the 
proposed alignment adjustment measure, 
and change the alignment result by map-
ping f to e. 
This modification guarantees that the number 
of alignment does not change; the algorithm is 
designed to minimize the risk by maintaining 
the fertility of a word estimated by the IBM 
Model. Figure 2 illustrates the result before 
and after the alignment modification. Incor-
rectly links from e1 and e3 are deleted and 
missing links from e2 and e4 are generated dur-
ing this alignment modification. 
The alignment modification through the re-
flection of POS alignment tendency is per-
formed on both e-to-f and f-to-e bidirectional 
word alignments. The bidirectional word 
alignment results are then symmetrized. 
4 Experiments 
In this paper, we attempt to reflect the POS 
alignment tendency in improving the word 
alignment performance. This section provides 
the experimental setup and the results that 
demonstrate whether the proposed approach 
can improve the statistical word alignment per-
formance. 
We collected bilingual texts from major bi-
lingual news broadcasting sites. 500K sentence 
pairs are collected and refined manually to 
construct correct parallel sentences pairs. The 
same number of monolingual sentences is also 
used from the same sites to train Korean lan-
guage. We also prepared a subset of the bilin-
gual text with the size of 50K to show that the 
proposed model is very effective when the 
training set is small. 
In order to evaluate the performance of 
word alignment, we additionally constructed a 
reference set with 400 sentence pairs. The 
evaluation is performed using precision, recall, 
and F-score. We use the GIZA++ toolkit for 
word alignment as well as four heuristic sym-
metrizations: intersection, union, grow-diag-
final, and grow-diag (Och, 2000).  
4.1 Word Alignment 
We now evaluate the effectiveness of the pro-
posed word alignment method. Table 1 and 2 
report the experimental results by adding POS 
information to the parallel corpus. ?Lexical? 
denotes the result of conventional word align-
ment produced by GIZA++. No pre-processing 
or post-processing is applied in this result. 
?Lemma/POS? is the result of word alignment 
with the pre-processing introduced Lee et al 
(2006). Compared to the result, lemmatized 
lexical and POS tags are proven to be useful 
information for word alignment. ?Lemma/POS? 
consistently outperforms ?Lexical? despite the 
symmetrization heuristics in terms of precision, 
recall and F-score. We expect this improve-
ment is benefited from the alleviated data 
sparseness by using lemmatized lexical and 
POS tags rather than using the lexical itself. 
 
 
Figure 2. An example of word alignment modification 
625
 
 
 
 
  
 Alignment heuristic Precision Recall F-score 
Lexical 
Intersection 94.0% 50.8% 66.0% 
Union 53.2% 81.2% 64.3% 
Grow-diag-final 54.6% 80.9% 65.2% 
Grow-diag 60.9% 67.2% 63.9% 
Lemma/POS 
Intersection 95.8% 55.3% 70.1% 
Union 58.1% 83.3% 68.4% 
Grow-diag-final 59.7% 83.0% 69.5% 
Grow-diag 67.0% 71.6% 69.2% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 96.1% 63.5% 76.5% 
Union 67.4% 85.1% 75.2% 
Grow-diag-final 69.8% 84.9% 76.6% 
Grow-diag 80.0% 77.0% 78.5% 
Table 1. The performance of word alignment using small training set (50k pairs) 
 
Experimental Setup Alignment heuristic Precision Recall F-score 
Lexical 
Intersection 96.8% 64.9% 77.7% 
Union 66.6% 87.4% 75.6% 
Grow-diag-final 67.8% 87.1% 76.2% 
Grow-diag 74.4% 79.2% 76.7% 
Lemma/POS 
Intersection 97.3% 66.2% 78.8% 
Union 70.7% 89.0% 78.8% 
Grow-diag-final 72.1% 88.8% 79.6% 
Grow-diag 78.8% 80.5% 79.7% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 97.2% 69.3% 80.9% 
Union 73.9% 86.7% 79.8% 
Grow-diag-final 75.6% 86.4% 80.7% 
Grow-diag 85.2% 81.5% 83.4% 
Table 2. The performance of word alignment using a large training set (500k pairs) 
 
Experimental Setup Symmetrization Heuristic BLEU(50k) BLEU (500k) 
Lexical 
Intersection 20.1% 29.2% 
Union 18.6% 27.2% 
Grow-diag-final 19.9% 27.7% 
Grow-diag 20.2% 29.4% 
Lemma/POS 
Intersection 20.3% 26.4% 
Union 18.5% 27.8% 
Grow-diag-final 20.1% 29.2% 
Grow-diag 20.4% 30.8% 
Factored Model 
(Lemma, POS) 
Intersection 20.5% 30.0% 
Union 18.1% 27.5% 
Grow-diag-final 20.3% 28.2% 
Grow-diag 20.9% 31.1% 
Lemma/POS 
+ POS alignment 
tendency 
Intersection 21.8% 29.3% 
Union 19.5% 27.2% 
Grow-diag-final 21.3% 28.4% 
Grow-diag 20.8% 29.1% 
Table  3. The performance of translation 
626
 
 
 
 
 
Since lemmatized lexical and POS tags are 
shown to be useful, our post-processing meth-
od is applied to ?Lemma/POS?. 
The experimental results show that the pro-
posed method consistently improves word 
alignment in terms of F-score. It is interesting 
that the proposed method improves the recall 
of the intersection result and the precision of 
the union result. Thus, the proposed method 
achieves the best alignment performance. 
As can be seen in Table 1 and 2, our method 
consistently improves the performance of word 
alignment despite the size of training data. In a 
small data set, the improvement of our method 
is much higher than that in a large set. This 
implies that our method is more helpful when 
the training data set is insufficient.  
We investigate whether the proposed meth-
od actually alleviates the data sparseness prob-
lem by analyzing the aligned word pairs of low 
co-occurrence frequency. There are multiple 
word pairs that share the same number of co-
occurrence in the corpus. For example, let us 
assume that ?report-bogoha?, ?newspaper-
sinmun? and ?China-jungguk? pairs are co-
occurred 1,000 times. We can calculate the 
mean of their individual recalls. We refer to 
this new measurement as average recall. The 
average recalls of these pairs are relatively 
higher than those of pairs with low co-
occurrence frequency such as ?food-jinji? and 
?center-chojeom? pairs. These pairs are diffi-
cult to be linked, because the word alignment 
model suffers from data sparseness when esti-
mating their translation probability.  
Figure 3 shows the average recall according 
to the number of co-occurrence. We can ob-
serve that the word alignment model tends to 
link word pairs more correctly if they are more 
frequently co-occurred. Both ?Lemma/POS? 
and our method consistently show higher aver-
age recall throughout all frequencies, and the 
proposed method shows the best performance. 
It is also notable that the both ?Lemma/POS? 
and our method achieve much more improve-
ment for low co-occurrence frequencies (e.g., 
11~40). This implies that the proposed method 
incorporates POS information more effectively 
than the previous method, since the proposed 
method achieves much higher average recall. 
4.2 Statistical Machine Translation 
Next, we examine the effect of the improve-
ment of the word alignment on the translation 
quality. For this, we built some SMT systems 
with the word alignment results. We use the 
Moses toolkit for translation (Koehn et al, 
2007). Moses is an implementation of phrase-
based statistical machine translation model that 
has shown a state-of-the-art performance in 
various evaluation sets. We also perform the 
evaluation of the Factored model (Koehn et al, 
2007) using Moses.  
To investigate how the improved word 
alignment affect the quality of machine trans-
lation, we calculate the BLEU score for trans-
lation results with different word alignment 
settings as shown in Table 3. First of all, we 
can easily conclude that the quality of the 
translation is strongly dominated by the size of 
the training data. We can also find that the 
quality of the translation is correlated to the 
performance of the word alignment. 
For a small test set, the proposed method 
achieved the best performance in terms of 
BLEU (21.8%). For a larger test set, however, 
the proposed method could not improve the 
performance of the translation with better word 
alignment. It is not feasible to investigate the 
factors that affect this deterioration, since Mo-
ses is a black box module to our system. The 
training of the phrase-based SMT model in-
volves the extraction of phrases, and the result 
of word alignment is reflected within this pro-
cess. When the training data is small, the num-
ber of extracted phrases is also apparently 
small. However, abundant phrases are extract-
ed from a large amount of training data. In this 
case, we hypothesize that the most plausible 
 
Figure 3. Average recall of word alignment pairs 
according to the number of their co-occurrence 
627
 
 
 
 
phrases are already obtained, and the effect of 
more accurate word alignment seems insignifi-
cant. More thorough analysis of this is re-
mained as future work. 
4.3 Acquisition of Bilingual Dictionary 
One of the most applications of word align-
ment is the construction of bilingual dictionar-
ies. By using word alignment, we can collect a 
(ranked) list of bilingual word pairs. Table 4 
reports the top 10 translations (the most ac-
ceptable target words to align) for Korean 
word ?bap? (food). The table contains the 
probabilities estimated by the IBM Models, the 
adjusted scores, and the number of co-
occurrence, respectively. Italicized translations 
are in fact incorrect translations. Highlighted 
ones are new translation candidates that are 
correct. As can be seen in the table, the pro-
posed approach shows a positive effect of rais-
ing new and better candidates for translation. 
For example, ?bread? and ?breakfast? have 
come up to the top 10 translations. This 
demonstrates that the low co-occurrences of 
?bap? with ?bread? and ?breakfast? are not 
suitably handled by alignments solely based on 
lexicals. However, the proposed approach 
ranks them at higher positions by reflecting the 
alignment tendency of POSs. 
5 Conclusion 
In this paper, we propose a new method for 
incorporating the POS alignment tendency to 
improve traditional word alignment model in 
post processing step. Experimental results 
show that the proposed method helps to allevi-
ate the data sparseness problem especially 
when the training data is insufficient. 
It is still difficult to conclude that better 
word alignment always leads to better transla-
tion. We plan on investigating the effective-
ness of the proposed method using other trans-
lation system, such as Hiero (Chiang et al, 
2005). We also plan to incorporate our method 
into other effective models, such as Factored 
translation model. 
References 
David Chiang et al, 2005. The Hiero machine 
translation system: Extensions, evaluation, and 
analysis. In Proc. of HLT-EMLP:779?786, Oct. 
 
Franz Josef Och. 2000. Giza++: Training of statis-
tical translation models. Available at http://www-
i6.informatik.rwthaachen.de/?och/software/GIZA+
+.html. 
 
Franz Josef Och & Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment 
Models. Computational Linguistics 29 (1):19-51. 
 
G. Sanchis and J.A. S?nchez. Vocabulary exten- 
sion via POS information for SMT. In Mixing 
Approaches to Machine Translation, 2008. 
 
Jonghoon Lee, Donghyeon Lee and Gary Geunbae 
Lee. Improving Phrase-based Korean-English Sta-
tistical Machine Translation. INTERSPEECH 2006. 
 
Kuzman Ganchev, Joao V. Graca and Ben Taskar. 
2008. Better Alignments = Better Translations? 
Proceedings of ACL-08: HLT: 986?993. 
 
Peter F. Brown et al,1993. The Mathematics of 
Statistical Machine Translation: Parameter Esti-
mation. Computational Linguistics 9(2): 263-311 
 
Rank 
IBM Model POS Alignment Tendency 
translation     (   ) #co-occur translation      (   ) #co-occur 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
bob/NNP 
rice/NN 
eat/VB 
meal/NN 
food/NN 
bob/NN 
feed/VB 
cook/VB 
living/NN 
dinner/NN 
0.348 
0.192 
0.107 
0.075 
0.043 
0.038 
0.010 
0.010 
0.008 
0.008 
83 
73 
57 
43 
29 
10 
7 
9 
4 
10 
bob/NNP 
rice/NN 
meal/NN 
food/NN 
eat/VB 
bob/NN 
living/NN 
dinner/NN 
bread/NN 
breakfast/NN 
0.214 
0.136 
0.078 
0.062 
0.061 
0.059 
0.045 
0.044 
0.044 
0.043 
83 
73 
43 
29 
57 
10 
4 
10 
9 
6 
Table 4. Top 10 translations for Korean word ?bap? (food). 
628
 
 
 
 
Philipp Koehn and Hieu Hoang. Factored Transla-
tion Models. EMNLP 2007. 
 
Phillipp Koehn et al, 2007. Moses: Open source 
toolkit for statistical machine translation.In Pro-
ceedings of the Annual Meeting of the Association 
for Computational Linguistics, demonstation ses-
sion. 
629
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 645?650,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Joint Relational Embeddings for Knowledge-based Question Answering
Min-Chul Yang
?
Nan Duan
?
Ming Zhou
?
Hae-Chang Rim
?
?
Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea
?
Microsoft Research Asia, Beijing, China
mcyang@nlp.korea.ac.kr
{nanduan, mingzhou}@microsoft.com
rim@nlp.korea.ac.kr
Abstract
Transforming a natural language (NL)
question into a corresponding logical form
(LF) is central to the knowledge-based
question answering (KB-QA) task. Un-
like most previous methods that achieve
this goal based on mappings between lex-
icalized phrases and logical predicates,
this paper goes one step further and pro-
poses a novel embedding-based approach
that maps NL-questions into LFs for KB-
QA by leveraging semantic associations
between lexical representations and KB-
properties in the latent space. Experimen-
tal results demonstrate that our proposed
method outperforms three KB-QA base-
line methods on two publicly released QA
data sets.
1 Introduction
Knowledge-based question answering (KB-QA)
involves answering questions posed in natural
language (NL) using existing knowledge bases
(KBs). As most KBs are structured databases,
how to transform the input question into its corre-
sponding structured query for KB (KB-query) as
a logical form (LF), also known as semantic pars-
ing, is the central task for KB-QA systems. Pre-
vious works (Mooney, 2007; Liang et al., 2011;
Cai and Yates, 2013; Fader et al., 2013; Berant et
al., 2013; Bao et al., 2014) usually leveraged map-
pings between NL phrases and logical predicates
as lexical triggers to perform transformation tasks
in semantic parsing, but they had to deal with two
limitations: (i) as the meaning of a logical pred-
icate often has different natural language expres-
sion (NLE) forms, the lexical triggers extracted for
a predicate may at times are limited in size; (ii)
entities detected by the named entity recognition
(NER) component will be used to compose the
logical forms together with the logical predicates,
so their types should be consistent with the pred-
icates as well. However, most NER components
used in existing KB-QA systems are independent
from the NLE-to-predicate mapping procedure.
We present a novel embedding-based KB-QA
method that takes all the aforementioned lim-
itations into account, and maps NLE-to-entity
and NLE-to-predicate simultaneously using sim-
ple vector operations for structured query con-
struction. First, low-dimensional embeddings of
n-grams, entity types, and predicates are jointly
learned from an existing knowledge base and from
entries <entity
subj
, NL relation phrase, entity
obj
>
that are mined from NL texts labeled as KB-
properties with weak supervision. Each such en-
try corresponds to an NL expression of a triple
<entity
subj
, predicate, entity
obj
> in the KB. These
embeddings are used to measure the semantic as-
sociations between lexical phrases and two prop-
erties of the KB, entity type and logical predicate.
Next, given an NL-question, all possible struc-
tured queries as candidate LFs are generated and
then they are ranked by the similarity between the
embeddings of observed features (n-grams) in the
NL-question and the embeddings of logical fea-
tures in the structured queries. Last, answers are
retrieved from the KB using the selected LFs.
The contributions of this work are two-fold: (1)
as a smoothing technique, the low-dimensional
embeddings can alleviate the coverage issues of
lexical triggers; (2) our joint approach integrates
entity span selection and predicate mapping tasks
for KB-QA. For this we built independent entity
embeddings as the additional component, solving
the entity disambiguation problem.
2 Related Work
Supervised semantic parsers (Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005; Mooney,
2007) heavily rely on the <sentence, semantic an-
645
notation> pairs for lexical trigger extraction and
model training. Due to the data annotation re-
quirement, such methods are usually restricted to
specific domains, and struggle with the coverage
issue caused by the limited size of lexical triggers.
Studies on weakly supervised semantic parsers
have tried to reduce the amount of human supervi-
sion by using question-answer pairs (Liang et al.,
2011) or distant supervision (Krishnamurthy and
Mitchell, 2012) instead of full semantic annota-
tions. Still, for KB-QA, the question of how to
leverage KB-properties and analyze the question
structures remains.
Bordes et al. (2012) and Weston et al. (2013) de-
signed embedding models that connect free texts
with KBs using the relational learning method
(Weston et al., 2010). Their inputs are often
statement sentences which include subject and ob-
ject entities for a given predicate, whereas NL-
questions lack either a subject or object entity that
is the potential answer. Hence, we can only use
the information of a subject or object entity, which
leads to a different training instance generation
procedure and a different training criterion.
Recently, researchers have developed open do-
main systems based on large scale KBs such as
FREEBASE
1
(Cai and Yates, 2013; Fader et al.,
2013; Berant et al., 2013; Kwiatkowski et al.,
2013; Bao et al., 2014; Berant and Liang, 2014;
Yao and Van Durme, 2014). Their semantic
parsers for Open QA are unified formal and scal-
able: they enable the NL-question to be mapped
into the appropriate logical form. Our method ob-
tains similar logical forms, but using only low-
dimensional embeddings of n-grams, entity types,
and predicates learned from texts and KB.
3 Setup
3.1 Relational Components for KB-QA
Our method learns semantic mappings between
NLEs and the KB
2
based on the paired relation-
ships of the following three components: C de-
notes a set of bag-of-words (or n-grams) as context
features (c) for NLEs that are the lexical represen-
tations of a logical predicate (p) in KB; T denotes
a set of entity types (t) in KB and each type can be
used as the abstract expression of a subject entity
1
http://www.freebase.com
2
For this paper, we used a large scale knowledge base that
contains 2.3B entities, 5.5K predicates, and 18B assertions.
A 16-machine cluster was used to host and serve the whole
data.
(s) that occurs in the input question; P denotes a
set of logical predicates (p) in KB, each of which
is the canonical form of different NLEs sharing an
identical meaning (bag-of-words; c).
Based on the components defined above, the
paired relationships are described as follows: T -
P can investigate the relationship between sub-
ject entity and logical predicate, as object entity
is always missing in KB-QA; C-T can scruti-
nize subject entity?s attributes for the entity span
selection such as its positional information and
relevant entity types to the given context, which
may solve the entity disambiguation problem in
KB-QA; C-P can leverage the semantic overlap
between question contexts (n-gram features) and
logical predicates, which is important for mapping
NL-questions to their corresponding predicates.
3.2 NLE-KB Pair Extraction
This section describes how we extract the semantic
associated pairs of NLE-entries and KB-triples to
learn the relational embeddings (Section 4.1).
<Relation Mention, Predicate> Pair (MP)
Each relation mention denotes a lexical phrase
of an existing KB-predicate. Following informa-
tion extraction methods, such as PATTY (Nakas-
hole et al., 2012), we extracted the <relation
mention, logical predicate> pairs from English
WIKIPEDIA
3
, which is closely connected to our
KB, as follows: Given a KB-triple <entity
subj
,
logical predicate, entity
obj
>, we extracted NLE-
entries <entity
subj
, relation mention, entity
obj
>
where relation mention is the shortest path be-
tween entity
subj
and entity
obj
in the dependency
tree of sentences. The assumption is that any re-
lation mention (m) in the NLE-entry containing
such entity pairs that occurred in the KB-triple is
likely to express the predicate (p) of that triple.
With obtaining high-qualityMP pairs, we kept
only relation mentions that were highly associated
with a predicate measured by the scoring function:
S(m, p) = PMI(e
m
; e
p
) + PMI(u
m
;u
p
) (1)
where e
x
is the set of total pairs of both-side
entities of entry x (m or p) and u
x
is the set
of unique (distinct) pairs of both-side entities of
entry x. In this case, the both-side entities in-
dicate entity
subj
and entity
obj
. For a frequency-
based probability, PMI(x; y) = log
P (x,y)
P (x)P (y)
3
http://en.wikipedia.org/
646
(Church and Hanks, 1990) can be re-written as
PMI(x; y) = log
|x
?
y|?C
|x|?|y|
, where C denotes the
total number of items shown in the corpus. The
function is partially derived from the support score
(Gerber and Ngonga Ngomo, 2011), but we fo-
cus on the correlation of shared entity pairs be-
tween relation mentions and predicates using the
PMI computation.
<Question Pattern, Predicate> Pair (QP)
Since WIKIPEDIA articles have no information to
leverage interrogative features which highly de-
pend on the object entity (answer), it is difficult to
distinguish some questions that are composed of
only different 5W1H words, e.g., {When|Where}
was Barack Obama born? Hence, we used the
method of collecting question patterns with human
labeled predicates that are restricted by the set of
predicates used inMP (Bao et al., 2014).
4 Embedding-based KB-QA
Our task is as follows. First, our model learns the
semantic associations of C-T , C-P , and T -P (Sec-
tion 3.1) based on NLE-KB pairs (Section 3.2),
and then predicts the semantic-related KB-query
which can directly find the answer to a given NL-
question.
For our feature space, given an NLE-KB pair,
the NLE (relation mention in MP or question
pattern in QP) is decomposed into n-gram fea-
tures: C = {c | c is a segment of NLE}, and
the KB-properties are represented by entity type
t of entity
subj
and predicate p. Then we can ob-
tain a training triplet w = [C, t, p]. Each feature
(c ? C, t ? T , p ? P) is encoded in the distributed
representation which is n-dimensional embedding
vectors (E
n
): ?x, x
encode
? E(x) ? E
n
.
All n-gram features (C) for an NLE are merged
into one embedding vector to help speed up the
learning process: E(C) =
?
c?C
E(c)/|C|. This
feature representation is inspired by previous work
in embedding-based relation extraction (Weston et
al., 2013), but differs in the following ways: (1)
entity information is represented on a separate em-
bedding, but its positional information remains as
symbol ?entity?; (2) when the vectors are com-
bined, we use the average of each index to normal-
ize features.
For our joint relational approach, we focus on
the set of paired relationships R = {C-t, C-p, t-
p} that can be semantically leveraged. Formally,
these features are embedded into the same latent
space (E
n
) and their semantic similarities can be
computed by a dot product operation:
Sim(a, b) = Sim(r
ab
) = E(a)
?
E(b) (2)
where r
ab
denotes a paired relationship a-b (or (a,
b)) in the above set R. We believe that our joint re-
lational learning can smooth the surface (lexical)
features for semantic parsing using the aligned en-
tity and predicate.
4.1 Joint Relational Embedding Learning
Our ranking-based relational learning is based on
a ranking loss (Weston et al., 2010) that supports
the idea that the similarity scores of observed pairs
in the training set (positive instances) should be
larger than those of any other pairs (negative in-
stances):
?i, ?y
?
6= y
i
, Sim(x
i
, y
i
) > 1+Sim(x
i
, y
?
) (3)
More precisely, for each triplet w
i
= [C
i
, t
i
, p
i
]
obtained from an NLE-KB pair, the relationships
R
i
= {C
i
-t
i
, C
i
-p
i
, t
i
-p
i
} are trained under the
soft ranking criterion, which conducts Stochastic
Gradient Descent (SGD). We thus aim to minimize
the following:
?i,?y
?
6= y
i
,max(0, 1?Sim(x
i
, y
i
)+Sim(x
i
, y
?
))
(4)
Our learning strategy is as follows. First, we ini-
tialize embedding space E
n
by randomly giving
mean 0 and standard deviation 1/n to each vec-
tor. Then for each training triplet w
i
, we select the
negative pairs against positive pairs (C
i
-t
i
, C
i
-p
i
,
and t
i
-p
i
) in the triplet. Last, we make a stochastic
gradient step to minimize Equation 4 and update
E
n
at each step.
4.2 KB-QA using Embedding Models
Our goal for KB-QA is to translate a given NL-
question to a KB-query with the form <subject
entity, predicate, ?>, where ? denotes the an-
swer entity we are looking for. The decoding pro-
cess consists of two stages. The first stage in-
volves generating all possible KB-queries (K
q
) for
an NL-question q. We first extract n-gram fea-
tures (C
q
) from the NL-question q. Then for a
KB-query k
q
, we find all available entity types
(t
q
) of the identified subject entities (s
q
) using
the dictionary-based entity detection on the NL-
question q (all of spans can be candidate entities),
and assign all items of predicate set (P) as the can-
didate predicates (p
q
). Like the training triplets,
647
q where is the city of david?
?
k(q) [The City of David, contained by, ?]
C
q
n-grams of ?where is ?entity? ??
t
q
location
p
q
contained by
Table 1: The corresponding KB-query
?
k(q) for a
NL-question q and its decoding triplet w
q
.
we also represent the above features as the triplet
form w
q
i
= [C
q
i
, t
q
i
, p
q
i
] which is directly linked to
a KB-query k
q
i
= [s
q
i
, p
q
i
, ?]. The second stage
involves ranking candidate KB-queries based on
the similarity scores between the following paired
relationships from the triplet w
q
i
: R
q
i
= {C
q
i
-t
q
i
,
C
q
i
-p
q
i
, t
q
i
-p
q
i
}. Unlike in the training step, the sim-
ilarities of C
q
i
-t
q
i
and C
q
i
-p
q
i
are computed by sum-
mation of all pairwise elements (each context em-
bedding E(c), not E(C), with each paired E(t) or
E(p)) for a more precise measurement. Since sim-
ilarites of R
q
are calculated on different scales, we
normalize each value using Z-score (Z(x) =
x??
?
)
(Kreyszig, 1979). The final score is measured by:
Sim
q2k
(q, k
q
) =
?
r?R
q
Z(Sim(r)) (5)
Then, given any NL-question q, we can predict the
corresponding KB-query
?
k(q):
?
k(q) = argmax
k?K
q
Sim
q2k
(q, k) (6)
Last, we can retrieve an answer from the KB using
a structured query
?
k(q). Table 1 shows an example
of our decoding process.
Multi-related Question Some questions in-
clude two-subject entities, both of which are cru-
cial to understanding the question. For the ques-
tion who plays gandalf in the lord of the rings?
Gandalf (character) and The Lord Of The
Rings (film) are explicit entities that should be
joined to a pair of the two entities (implicit entity).
More precisely, the two entities can be combined
into one concatenated entity (character-in-film)
using our manual rule, which compares the possi-
ble pairs of entity types in the question with the
list of pre-defined entity type pairs that can be
merged into a concatenated entity. Our solution
enables a multi-related question to be transformed
to a single-related question which can be directly
translated to a KB-query. Then, the two entity
# Entries Accuracy
MP pairs 291,585 89%
QP pairs 4,764 98%
Table 2: Statistics of NLE-KB pairs
mentions are replaced with the symbol ?entity?
(who play ?entity? in ?entity? ?). We re-
gard the result of this transformation as one of the
candidate KB-queries in the decoding step.
5 Experiments
Experimental Setting We first performed pre-
processing, including lowercase transformation,
lemmatization and tokenization, on NLE-KB pairs
and evaluation data. We used 71,310 n-grams
(uni-, bi-, tri-), 990 entity types, and 660 predi-
cates as relational components shown in Section
3.1. The sum of these three numbers (72,960)
equals the size of the embeddings we are going
to learn. In Table 2, we evaluated the quality of
NLE-KB pairs (MP and QP) described in Sec-
tion 3.2. We can see that the quality ofQP pairs is
good, mainly due to human efforts. Also, we ob-
tained MP pairs that have an acceptable quality
using threshold 3.0 for Equation 1, which lever-
ages the redundancy information in the large-scale
data (WIKIPEDIA). For our embedding learning,
we set the embedding dimension n to 100, the
learning rate (?) for SGD to 0.0001, and the it-
eration number to 30. To make the decoding
procedure computable, we kept only the popular
KB-entity in the dictionary to map different entity
mentions into a KB-entity.
We used two publicly released data sets for QA
evaluations: Free917 (Cai and Yates, 2013) in-
cludes the annotated lambda calculus forms for
each question, and covers 81 domains and 635
Freebase relations; WebQ. (Berant et al., 2013)
provides 5,810 question-answer pairs that are built
by collecting common questions from Web-query
logs and by manually labeling answers. We used
the previous three approaches (Cai and Yates,
2013; Berant et al., 2013; Bao et al., 2014) as our
baselines.
Experimental Results Table 3 reports the over-
all performances of our proposed KB-QA method
on the two evaluation data sets and compares them
with those of the three baselines. Note that we
did not re-implement the baseline systems, but just
borrowed the evaluation results reported in their
648
Methods Free917 WebQ.
Cai and Yates (2013) 59.00% N/A
Berant et al. (2013) 62.00% 31.40%
Bao et al. (2014) N/A 37.50%
Our method 71.38% 41.34%
Table 3: Accuracy on the evaluation data
Methods Free917 WebQ.
Our method 71.38% 41.34%
w/o T -P 70.65% 40.55%
w/o C-T 67.03% 38.44%
w/o C-P 31.16% 19.24%
Table 4: Ablation of the relationship types
papers. Although the KB used by our system is
much larger than FREEBASE, we still think that
the experimental results are directly comparable
because we disallow all the entities that are not in-
cluded in FREEBASE.
Table 3 shows that our method outperforms the
baselines on both Free917 and WebQ. data sets.
We think that using the low-dimensional embed-
dings of n-grams rather than the lexical triggers
greatly improves the coverage issue. Unlike the
previous methods which perform entity disam-
biguation and predicate prediction separately, our
method jointly performs these two tasks. More
precisely, we consider the relationships C-T and
C-P simultaneously to rank candidate KB-queries.
In Table 1, the most independent NER in KB-QA
systems may detect David as the subject entity,
but our joint approach can predict the appropriate
subject entity The City of David by leveraging
not only the relationships with other components
but also other relationships at once. The syntax-
based (grammar formalism) approaches such as
Combinatory Categorial Grammar (CCG) may ex-
perience errors if a question has grammatical er-
rors. However, our bag-of-words model-based ap-
proach can handle any question as long as the
question contains keywords that can help in un-
derstanding it.
Table 4 shows the contributions of the relation-
ships (R) between relational components C, T ,
and P . For each row, we remove the similarity
from each of the relationship types described in
Section 3.1. We can see that the C-P relationship
plays a crucial role in translating NL-questions to
KB-queries, while the other two relationships are
slightly helpful.
Result Analysis Since the majority of questions
in WebQ. tend to be more natural and diverse, our
method cannot find the correct answers to many
questions. The errors can be caused by any of
the following reasons. First, some NLEs cannot
be easily linked to existing KB-predicates, mak-
ing it difficult to find the answer entity. Second,
some entities can be mentioned in several different
ways, e.g., nickname (shaq?Shaquille O?neal)
and family name (hitler?Adolf Hitler). Third, in
terms of KB coverage issues, we cannot detect the
entities that are unpopular. Last, feature represen-
tation for a question can fail when the question
consists of rare n-grams.
The two training sets shown in Section 3.2 are
complementary: QP pairs provide more oppor-
tunities for us to learn the semantic associations
between interrogative words and predicates. Such
resources are especially important for understand-
ing NL-questions, as most of them start with such
5W1H words; on the other hand, MP pairs en-
rich the semantic associations between context in-
formation (n-gram features) and predicates.
6 Conclusion
In this paper, we propose a novel method that
transforms NL-questions into their corresponding
logical forms using joint relational embeddings.
We also built a simple and robust KB-QA system
based on only the learned embeddings. Such em-
beddings learn the semantic associations between
natural language statements and KB-properties
from NLE-KB pairs that are automatically ex-
tracted from English WIKIPEDIA using KB-triples
with weak supervision. Then, we generate all pos-
sible structured queries derived from latent logical
features of the given NL-question, and rank them
based on the similarity scores between those re-
lational attributes. The experimental results show
that our method outperforms the latest three KB-
QA baseline systems. For our future work, we will
build concept-level context embeddings by lever-
aging latent meanings of NLEs rather than their
surface n-grams with the aligned logical features
on KB.
Acknowledgement This research was sup-
ported by the Next-Generation Information
Computing Development Program through the
National Research Foundation of Korea (NRF)
funded by the Ministry of Science, ICT & Future
Planning (NRF-2012M3C4A7033344).
649
References
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 967?976. Association for Computa-
tional Linguistics.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, pages 1415?1425. Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533?1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In In Proceedings of 15th International
Conference on Artificial Intelligence and Statistics.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Association for Computational Lin-
guistics (ACL), pages 423?433. The Association for
Computer Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22?29, March.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In Association for Computational Lin-
guistics (ACL), pages 1608?1618. The Association
for Computer Linguistics.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In 1st Workshop
on Web Scale Knowledge Extraction @ ISWC 2011.
E. Kreyszig. 1979. Advanced Engineering Mathemat-
ics. Wiley.
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 754?765, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 590?599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
RaymondJ. Mooney. 2007. Learning for semantic
parsing. In Alexander Gelbukh, editor, Computa-
tional Linguistics and Intelligent Text Processing,
volume 4394 of Lecture Notes in Computer Science,
pages 311?324. Springer Berlin Heidelberg.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1135?1145, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2010. Large scale image annotation: Learning to
rank with joint word-image embeddings. Machine
Learning, 81(1):21?35, October.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366?1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 956?966, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence - Volume
2, AAAI?96, pages 1050?1055. AAAI Press.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666. AUAI Press.
650
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 291?295,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Size Reduction for
Hierarchical Phrase-based Statistical Machine Translation
Seung-Wook Lee? Dongdong Zhang? Mu Li? Ming Zhou? Hae-Chang Rim?
? Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea
{swlee,rim}@nlp.korea.ac.kr
? Microsoft Research Asia, Beijing, China
{dozhang,muli,mingzhou}@microsoft.com
Abstract
In this paper, we propose a novel method of
reducing the size of translation model for hier-
archical phrase-based machine translation sys-
tems. Previous approaches try to prune in-
frequent entries or unreliable entries based on
statistics, but cause a problem of reducing the
translation coverage. On the contrary, the pro-
posed method try to prune only ineffective
entries based on the estimation of the infor-
mation redundancy encoded in phrase pairs
and hierarchical rules, and thus preserve the
search space of SMT decoders as much as
possible. Experimental results on Chinese-to-
English machine translation tasks show that
our method is able to reduce almost the half
size of the translation model with very tiny
degradation of translation performance.
1 Introduction
Statistical Machine Translation (SMT) has gained
considerable attention during last decades. From a
bilingual corpus, all translation knowledge can be
acquired automatically in SMT framework. Phrase-
based model (Koehn et al, 2003) and hierarchical
phrase-based model (Chiang, 2005; Chiang, 2007)
show state-of-the-art performance in various lan-
guage pairs. This achievement is mainly benefit
from huge size of translational knowledge extracted
from sufficient parallel corpus. However, the errors
of automatic word alignment and non-parallelized
bilingual sentence pairs sometimes have caused the
unreliable and unnecessary translation rule acquisi-
tion. According to Bloodgood and Callison-Burch
(2010) and our own preliminary experiments, the
size of phrase table and hierarchical rule table con-
sistently increases linearly with the growth of train-
ing size, while the translation performance tends to
gain minor improvement after a certain point. Con-
sequently, the model size reduction is necessary and
meaningful for SMT systems if it can be performed
without significant performance degradation. The
smaller the model size is, the faster the SMT de-
coding speed is, because there are fewer hypotheses
to be investigated during decoding. Especially, in a
limited environment, such as mobile device, and for
a time-urgent task, such as speech-to-speech transla-
tion, the compact size of translation rules is required.
In this case, the model reduction would be the one
of the main techniques we have to consider.
Previous methods of reducing the size of SMT
model try to identify infrequent entries (Zollmann
et al, 2008; Huang and Xiang, 2010). Several sta-
tistical significance testing methods are also exam-
ined to detect unreliable noisy entries (Tomeh et al,
2009; Johnson et al, 2007; Yang and Zheng, 2009).
These methods could harm the translation perfor-
mance due to their side effect of algorithms; simi-
lar multiple entries can be pruned at the same time
deteriorating potential coverage of translation. The
proposed method, on the other hand, tries to mea-
sure the redundancy of phrase pairs and hierarchi-
cal rules. In this work, redundancy of an entry is
defined as its translational ineffectiveness, and esti-
mated by comparing scores of entries and scores of
their substituents. Suppose that the source phrase
s1s2 is always translated into t1t2 with phrase en-
try <s1s2?t1t2> where si and ti are correspond-
291
ing translations. Similarly, source phrases s1 and
s2 are always translated into t1 and t2, with phrase
entries, <s1?t1> and <s2?t2>, respectively. In
this case, it is intuitive that <s1s2?t1t2> could be
unnecessary and redundant since its substituent al-
ways produces the same result. This paper presents
statistical analysis of this redundancy measurement.
The redundancy-based reduction can be performed
to prune the phrase table, the hierarchical rule table,
and both. Since the similar translation knowledge
is accumulated at both of tables during the train-
ing stage, our reduction method performs effectively
and safely. Unlike previous studies solely focus on
either phrase table or hierarchical rule table, this
work is the first attempt to reduce phrases and hi-
erarchical rules simultaneously.
2 Proposed Model
Given an original translation model, TM , our goal
is to find the optimally reduced translation model,
TM?, which minimizes the degradation of trans-
lation performance. To measure the performance
degradation, we introduce a new metric named con-
sistency:
C(TM,TM?) =
BLEU(D(s;TM),D(s;TM?)) (1)
where the function D produces the target sentence
of the source sentence s, given the translation model
TM . Consistency measures the similarity between
the two groups of decoded target sentences produced
by two different translation models. There are num-
ber of similarity metrics such as Dices coefficient
(Kondrak et al, 2003), and Jaccard similarity coef-
ficient. Instead, we use BLEU scores (Papineni et
al., 2002) since it is one of the primary metrics for
machine translation evaluation. Note that our con-
sistency does not require the reference set while the
original BLEU does. This means that only (abun-
dant) source-side monolingual corpus is needed to
predict performance degradation. Now, our goal can
be rewritten with this metric; among all the possible
reduced models, we want to find the set which can
maximize the consistency:
TM? = argmax
TM ??TM
C(TM,TM ?) (2)
In minimum error rate training (MERT) stages,
a development set, which consists of bilingual sen-
tences, is used to find out the best weights of fea-
tures (Och, 2003). One characteristic of our method
is that it isolates feature weights of the transla-
tion model from SMT log-linear model, trying to
minimize the impact of search path during decod-
ing. The reduction procedure consists of three
stages: translation scoring, redundancy estimation,
and redundancy-based reduction.
Our reduction method starts with measuring the
translation scores of the individual phrase and the
hierarchical rule. Similar to the decoder, the scoring
scheme is based on the log-linear framework:
PS(p) =
?
i
?ihi(p) (3)
where h is a feature function and ? is its weight.
As the conventional hierarchical phrase-based SMT
model, our features are composed of P (e|f ), P (f |e),
Plex(e|f ), Plex(f |e), and the number of phrases,
where e and f denote a source phrase and a target
phrase, respectively. Plex is the lexicalized proba-
bility. In a similar manner, the translation scores of
hierarchical rules are calculated as follows:
HS(r) =
?
i
?ihi(r) (4)
The features are as same as those that are used for
phrase scoring, except the last feature. Instead of the
phrase number penalty, the hierarchical rule num-
ber penalty is used. The weight for each feature is
shared from the results of MERT. With this scoring
scheme, our model is able to measure how important
the individual entry is during decoding.
Once translation scores for all entries are es-
timated, our method retrieves substituent candi-
dates with their combination scores. The combina-
tion score is calculated by accumulating translation
scores of every member as follows:
CS(p1...n) =
n
?
i=1
PS(pi) (5)
This scoring scheme follows the same manner
what the conventional decoder does, finding the best
phrase combination during translation. By compar-
ing the original translation score with combination
292
scores of its substituents, the redundancy scores are
estimated, as follows:
Red(p) = min
p1...n?Sub(p)
PS(p)?CS(p1...n) (6)
where Sub is the function that retrieves all possi-
ble substituents (the combinations of sub-phrases,
and/or sub-rules that exactly produce the same tar-
get phrase, given the source phrase p). If the com-
bination score of the best substituent is same as the
translation score of p, the redundancy score becomes
zero. In this case, the decoder always produces the
same translation results without p. When the redun-
dancy score is negative, the best substituent is more
likely to be chosen instead of p. This implies that
there is no risk to prune p; the search space is not
changed, and the search path is not changed as well.
Our method can be varied according to the desig-
nation of Sub function. If both of the phrase table
and the hierarchical rule table are allowed, cross re-
duction can be possible; the phrase table is reduced
based on the hierarchical rule table and vice versa.
With extensions of combination scoring and redun-
dancy scoring schemes like following equations, our
model is able to perform cross reduction.
CS(p1...n, h1...m) =
n
?
i=1
PS(pi) +
m
?
i=1
HS(hi) (7)
Red(p) = min
<p1...n,h1...m>?Sub(p)
PS(p)? CS(p1...n, h1...m) (8)
The proposed method has some restrictions for
reduction. First of all, it does not try to prune the
phrase that has no substituents, such as unigram
phrases; the phrase whose source part is composed
of a single word. This restriction guarantees that
the translational coverage of the reduced model is
as high as those of the original translation model.
In addition, our model does not prune the phrases
and the hierarchical rules that have reordering within
it to prevent information loss of reordering. For
instance, if we prune phrase, <s1s2s3?t3t1t2>,
phrases, <s1s2?t1t2> and <s3?t3> are not able
to produce the same target words without appropri-
ate reordering.
Once the redundancy scores for all entries have
been estimated, the next step is to select the best
N entries to prune to satisfy a desired model size.
We can simply prune the first N from the list of en-
tries sorted by increasing order of redundancy score.
However, this method may not result in the opti-
mal reduction, since each redundancy scores are es-
timated based on the assumption of the existence of
all the other entries. In other words, there are depen-
dency relationships among entries. We examine two
methods to deal with this problem. The first is to
ignore dependency, which is the more efficient man-
ner. The other is to prune independent entries first.
After all independent entries are pruned, the depen-
dent entries are started to be pruned. We present the
effectiveness of each method in the next section.
Since our goal is to reduce the size of all transla-
tion models, the reduction is needed to be performed
for both the phrase table and the hierarchical rule
table simultaneously, namely joint reduction. Sim-
ilar to phrase reduction and hierarchical rule reduc-
tion, it selects the best N entries of the mixture of
phrase and hierarchical rules. This method results
in safer pruning; once a phrase is determined to be
pruned, the hierarchical rules, which are related to
this phrase, are likely to be kept, and vice versa.
3 Experiment
We investigate the effectiveness of our reduction
method by conducting Chinese-to-English transla-
tion task. The training data, as same as Cui et
al. (2010), consists of about 500K parallel sentence
pairs which is a mixture of several datasets pub-
lished by LDC. NIST 2003 set is used as a devel-
opment set. NIST 2004, 2005, 2006, and 2008 sets
are used for evaluation purpose. For word align-
ment, we use GIZA++1, an implementation of IBM
models (Brown et al, 1993). We have implemented
a hierarchical phrase-based SMT model similar to
Chiang (2005). The trigram target language model
is trained from the Xinhua portion of English Gi-
gaword corpus (Graff and Cieri, 2003). Sampled
10,000 sentences from Chinese Gigaword corpus
(Graff, 2007) was used for source-side development
dataset to measure consistency. Our main met-
ric for translation performance evaluation is case-
1http://www.statmt.org/moses/giza/GIZA++.html
293
 0.60
 0.70
 0.80
 0.90
 1.00
Co
ns
ist
en
cy
Freq-Cutoff
NoDep
Dep
CrossNoDep
CrossDep
0.286
0.290
0.294
0.298
0% 10% 20% 30% 40% 50% 60%
BL
EU
Phrase Reduction Ratio
0% 10% 20% 30% 40% 50% 60%
Hierarchical Rule Reduction Ratio
0% 10% 20% 30% 40% 50% 60%
Joint Reduction Ratio
Figure 1: Performance comparison. BLEU scores and consistency scores are averaged over four evaluation sets.
insensitive BLEU-4 scores (Papineni et al, 2002).
As a baseline system, we chose the frequency-
based cutoff method, which is one of the most
widely used filtering methods. As shown in Fig-
ure 1, almost half of the phrases and hierarchical
rules are pruned when cutoff=2, while the BLEU
score is also deteriorated significantly. We intro-
duced two methods for selecting the N pruning
entries considering dependency relationships. The
non-dependency method does not consider depen-
dency relationships, while the dependency method
prunes independent entries first. Each method can be
combined with cross reduction. The performance is
measured in three different reduction tasks: phrase
reduction, hierarchical rule reduction, and joint re-
duction. As the reduction ratio becomes higher,
the model size, i.e., the number of entries, is re-
duced while BLEU scores and coverage are de-
creased. The results show that the translation per-
formance is highly co-related with the consistency.
The co-relation scores measured between them on
the phrase reduction and the hierarchical rule reduc-
tion tasks are 0.99 and 0.95, respectively, which in-
dicates very strong positive relationship.
For the phrase reduction task, the dependency
method outperforms the non-dependency method in
terms of BLEU score. When the cross reduction
technique was used for the phrase reduction task,
BLEU score is not deteriorated even when more than
half of phrase entries are pruned. This result implies
that there is much redundant information stored in
the hierarchical rule table. On the other hand, for the
hierarchical rule reduction task, the non-dependency
method shows the better performance. The depen-
dency method sometimes performs worse than the
baseline method. We expect that this is caused by
the unreliable estimation of dependency among hi-
erarchical rules since the most of them are automat-
ically generated from the phrases. The excessive de-
pendency of these rules would cause overestimation
of hierarchical rule redundancy score.
4 Conclusion
We present a novel method of reducing the size of
translation model for SMT. The contributions of the
proposed method are as follows: 1) our method is
the first attempt to reduce the phrase table and the hi-
erarchical rule table simultaneously. 2) our method
is a safe reduction method since it considers the re-
dundancy, which is the practical ineffectiveness of
individual entry. 3) our method shows that almost
the half size of the translation model can be reduced
without significant performance degradation. It may
be appropriate for the applications running on lim-
ited environment, e.g., mobile devices.
294
Acknowledgement
The first author performed this research during an
internship at Microsoft Research Asia. This research
was supported by the MKE(The Ministry of Knowl-
edge Economy), Korea and Microsoft Research, un-
der IT/SW Creative research program supervised by
the NIPA(National IT Industry Promotion Agency).
(NIPA-2010-C1810-1002-0025)
References
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the Trend: Large-Scale Cost-Focused Active
Learning for Statistical Machine Translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 854?864.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19:263?311, June.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43th Annual Meeting on Association
for Computational Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. Computational Linguistics, 33:201?228, June.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. Hybrid Decoding: Decoding with
Partial Hypotheses Combination Over Multiple SMT
Systems. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 214?222, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Graff and Christopher Cieri. 2003. English Giga-
word. In Linguistic Data Consortium, Philadelphia.
David Graff. 2007. Chinese Gigaword Third Edition. In
Linguistic Data Consortium, Philadelphia.
Fei Huang and Bing Xiang. 2010. Feature-Rich Discrim-
inative Phrase Rescoring for SMT. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 492?500, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving Translation Quality by Dis-
carding Most of the Phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can Improve Statistical Translation
Models. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy: companion volume of the Proceedings of HLT-
NAACL 2003?short papers - Volume 2, NAACL-Short
?03, pages 46?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based Phrase-Table Filtering for
Statistical Machine Translation.
Mei Yang and Jing Zheng. 2009. Toward Smaller, Faster,
and Better Hierarchical Phrase-based SMT. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, ACLShort ?09, pages 237?240, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A Systematic Comparison of Phrase-
based, Hierarchical and Syntax-Augmented Statistical
MT. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 1145?1152, troudsburg, PA, USA. Association
for Computational Linguistics.
295
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 233?237
Manchester, August 2008
Semantic Dependency Parsing using N-best Semantic Role Sequences and
Roleset Information
Joo-Young Lee, Han-Cheol Cho, and Hae-Chang Rim
Natural Language Processing Lab.
Korea University
Seoul, South Korea
{jylee,hccho,rim}@nlp.korea.ac.kr
Abstract
In this paper, we describe a syntactic and
semantic dependency parsing system sub-
mitted to the shared task of CoNLL 2008.
The proposed system consists of five mod-
ules: syntactic dependency parser, predi-
cate identifier, local semantic role labeler,
global role sequence candidate generator,
and role sequence selector. The syntac-
tic dependency parser is based on Malt
Parser and the sequence candidate gen-
erator is based on CKY style algorithm.
The remaining three modules are imple-
mented by using maximum entropy classi-
fiers. The proposed system achieves 76.90
of labeled F1 for the overall task, 84.82 of
labeled attachment, and 68.71 of labeled
F1 on the WSJ+Brown test set.
1 Introduction
In the framework of the CoNLL08 shared task
(Surdeanu et al, 2008), a system takes POS tagged
sentences as input and produces sentences parsed
for syntactic and semantic dependencies as output.
A syntactic dependency is represented by an ID
of head word and a dependency relation between
the head word and its modifier in a sentence. A
Semantic dependency is represented by predicate
rolesets and semantic arguments for each predi-
cate.
The task combines two sub-tasks: syntactic
dependency parsing and semantic role labeling.
Among the sub-tasks, we mainly focus on the se-
mantic role labeling task. Compared to previous
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
CoNLL 2004 and 2005 shared tasks (Carreras and
Ma`rquez, 2004; Carreras and Ma`rquez, 2005) and
other semantic role labeling research, major dif-
ferences of our semantic role labeling task are 1)
considering nominal predicates and 2) identify-
ing roleset of predicates. Based on our observa-
tion that verbal predicate and nominal predicate
have have different characteristics, we decide to
build diffent classification modeles for each pred-
icate types. The modeles use same features but,
their statistical parameters are different. In this
paper, maximum entropy1 is used as the classifi-
cation model, but any other classification models
such as Naive Bayse, SVM, etc. also can be used.
To identify roleset, we investigate a roleset match
scoring method which evaluate how likely a roleset
is matched with the given predicate.
2 System Description
The proposed system sequentially performs syn-
tactic dependency parsing, predicate identification,
local semantic role classification, global sequence
generation, and roleset information based selec-
tion.
2.1 Syntactic Dependency Parsing
In the proposed system, Malt Parser (Nivre et
al., 2007) is adopted as the syntactic dependency
parser. Although the training and test set of
CoNLL08 use non-projective dependency gram-
mar, we decide to use projective parsing algorithm,
Nivre arc-standard, and projective/non-
projective conversion functions that Malt Parser
provides. The reason is that non-projective parsing
shows worse performance than projective parsing
with conversion in our preliminary experiment.
1We use Zhang Le?s MaxEnt toolkit, http://homepages.
inf.ed.ac.uk/s0450736/maxent toolkit.html
233
We projectize the non-projective training sen-
tences in the training set to generate projective sen-
tences. And then, the parser is trained with the
transformed sentences. Finally, the parsing result
is converted into non-projective structure by using
a function of Malt Parser.
2.2 Predicate Identification
Unlike previous semantic role labeling task (Car-
reras and Ma`rquez, 2004; Carreras and Ma`rquez,
2005), predicates of sentences are not provided
with input in the CoNLL08. It means that a sys-
tem needs to identify which words in a sentence
are predicates.
We limit predicate candidates to the words that
exist in the frameset list of Propbank and Nom-
bank. Propbank and Nombank provide lists of
about 3,100 verbal predicates and about 4,400
nominal predicates. After dependency parsing,
words which are located in the frameset list are se-
lected as predicate candidates. The predicate iden-
tifier determines if a candidate is a predicate or not.
The identifier is implemented by using two maxi-
mum entropy models, the one is for verbal predi-
cates and the other is for nominal predicates. The
following features are used for predicate identifi-
cation:
Common Features
For Predicate Identification
- Lemma of Previous Word
- Lemma of Current Word
- Lemma of Next Word
- POS of Previous Word
- POS of Current Word
- POS of Next Word
- Dependency Label of Previous Word
- Dependency Label of Current Word
- Dependency Label of Next Word
Additional Features for Verbal Predicate
- Lemma + POS of Current Word
- Trigram Lemma of Previous, Current,
and Next Word
Additional Features for Nominal Predicate
- Lemma of Head of Current Word
- POS of Head of Current Word
- Dependency Label of Head of Current Word
Verbal predicate identifier shows 87.91 of F1 and
nominal predicate identifier shows 81.58 of F1.
Through a brief error analysis, we found that main
bottle neck for verbal predicate is auxiliary verb
be and have.
2.3 Local Semantic Role Labeling
Prediate identification is followed by argument la-
beling. For the given predicate, the system first
eliminates inappropriate argument candidates. The
argument identification uses different strategies for
verbs, nouns, and other predicates.
The argument classifier extracts features and la-
bels semantic roles. None is used to indicate that
a word is not a semantic argument. The classifier
also uses different maximum entropy models for
verbs, nouns, and other predicates
2.3.1 Argument Candidate Identification
As mentioned by Pradhan et al (2004), ar-
gument identification poses a significant bottle-
neck to improving performance of Semantic Role
Labeling system. We tried an algorithm moti-
vated from Hacioglu (2004) which defined a tree-
structured family membership of a predicate to
identify more probable argument candidates and
prune the others. However, we find that it works
for verb and other predicate type, but does not
work properly for noun predicate type. The main
reason is due to the characteristics of arguments
of noun predicates. First of all, a noun predicate
can be an argument for itself, whereas a verb pred-
icate cannot be. Secondly, dependency relation
paths from a noun predicate to its arguments are
usually shorter than a verb predicate. Although
some dependency relation paths are long, they ac-
tually involve non-informative relations like IN,
MD, or TO. Finally, major long distance relation
paths could be identified by several path patterns
acquired from the corpus.
Based on the above analysis, we specify a new
argument identification strategy for nominal pred-
icate type. The argument identifier regards a pred-
icate and its nearest neighbors - its parent and chil-
dren - as argument candidates. However, if the
POS tag of a nearest neighbor is IN, MD, or TO, it
will be ignored and the next nearest candidates will
be used. Moreover, several patterns (three consec-
utive nouns, adjective and two consecutive nouns,
two nouns combined with conjunction, and etc.)
are applied to find long distance argument candi-
dates.
234
2.3.2 Argument Classification
For argument classification, various features
have been used. Primarily, we tested a set of fea-
tures suggested by Hacioglu (2004). The voice of
the predicate, left and right words, its POS tag for
a predicate, and lexical clues for adjunctive argu-
ments also have been tested. Based on the type
of predicate (i.e. verb predicate, noun predicate,
and other predicate) three classification models are
trained by using maximum entropy with the fol-
lowing same features:
Features for Argument Classification
- Dependen Relation Type
- Family Membership
- Position
- Lemma of Head Word
- POS of Head Word
- Path
- POS Pattern of Predicate?s Children
- Relation Pattern of Predicate?s Children
- POS Pattern of Predicate?s Siblings
- Relation Pattern of Predicate?s Siblings
- POS of candidate
- Lemma of Left Word of Candidate
- POS of Left Word of Candidate
- Lemma of Right Word of Candidate
- POS of Right Word of Candidate
The classifier produces a list of possible seman-
tic roles and its probabilities for each word in the
given sentence.
2.4 Global Semantic Role Sequence
Generation
For local semantic role labeling, we assume that
semantic roles of words are independent of each
other. Toutanova et al (2005) and Surdeanu et
al. (2007) show that global constraint and opti-
mization are important in semantic role labeling.
We use CKY-based dynamic programming strat-
egy, similar to Surdeanu et al (2007), to verify
whether role sequences satisfy global constraint
and generate candidates of global semantic role se-
quences.
In this paper, we just use one constraint: no
duplicate arguments are allowed for verbal pred-
icates. For verbal predicates, CKY module builds
a list of all kinds of combinations of semantic roles
augmented with their probabilities. While building
the list of semantic role sequences, it removes the
sequences that violate the global constraint. The
output of CKY module is the list of semantic role
sequences satisfying the global constraint.
2.5 Global Sequence Selection using Roleset
Information
Finally, we need to select the most likely semantic
role sequence. In addition, we need to identify a
roleset for a predicate. We perform these tasks by
finding a role sequence and roleset maximizing a
score on the following formula:
? ? c+ ? ? rf + ? ? mc (1)
where, c, rf , mc are role sequence score, relative
frequence of roleset, and matching score with role-
set respectively. ?, ?, ? are tuning parameters of
each factor and decided empirically by using de-
velopment set. In this paper, we set ?, ?, ? to 0.5,
0.3, 0.2, respectively.
The role sequence score is calculated in the
global semantic role sequence generation ex-
plained in Section 2.4. The relative frequency of a
roleset means how many times the roleset occurred
in the training set compared to the total occurrence
of the predicate. It can be easily estimated by
MLE.
The remaining problem is how to calculate the
matching score. We use maximum entropy models
as binary classifiers which output match and not-
match and use probability of match as matching
score. The features used for the roleset matching
classifiers are based on following intuitions:
? If core roles (e.g., A0, A2, etc) defined in
a roleset occur in a given role sequence, it
seems to be the right roleset for the role se-
quence.
? If matched core roles are close to or have de-
pendency relations with a predicate, it seems
to be the right roleset.
? If a roleset has a particle and the predicate of
a sentence also has that particle, it seems to
be the right roleset. For example, the lemma
of predicate node for the roleset cut.05
in frameset file ?cut.xml.gz? is cut back, so
the particle of cut.05 is back. If the predicate
of a sentence also has particle ?back?, it seems
to be the right roleset.
? If example node of a roleset in frameset file
has a functional word for certain core role that
235
also exists in a given sentence, it seems to be
the right roleset. For example, example node
is defined as follows2:
<roleset id="cut.09" ...>
<example>
<text>
As the building?s new owner,
Chase will have its work cut
out for it.
</text>
<arg n="1">its work</arg>
<rel>cut out</rel>
<arg n="2" f="for">it</arg>
</example>
</roleset>
Here, semantic role A2 has functional word
for. If a given role sequence has A2 and its
word is ?for?, than this role sequence probably
matches that roleset.
Based on these intuitions, we use following fea-
tures for roleset matching:
? Core Role Matching Count The number of
core roles exist in both roleset definition and
given role sequence
? Distance of Matched Core Role Distance
between predicate and core role which ex-
ists in both roleset and given role sequence.
We use number of word and dependency path
length as a distance
? Indication for Same Particle It becomes
yes if given predicate and roleset have same
particle. (otherwise no)
? Indication for Same Functional Word It be-
comes yes if one of core argument is same to
the functional word of roleset. (otherwise no)
To train the roleset match classifiers, we extract
semantic role sequence and its roleset from train-
ing data as a positive example. And then, we gen-
erate negative examples by changing its roleset to
other roleset of that predicate. For example, the
above sentence in <text> node3 becomes a pos-
itive example for cut.09 and negative examples
for other roleset such as cut.01, cut.02, etc.
2Some nodes are omitted to simplify the definition of ex-
ample.
3Of cause, we assume that this sentence exist in training
corpus. So, we will extract it from corpus, not from frameset
file.
WSJ+Brown WSJ Brown
LM 76.90 77.96 68.34
LA 84.82 85.69 77.83
LF 68.71 69.95 58.63
Table 1: System performance. LM, LA, LF means
macro labeled F1 for the overall task, labeled at-
tachment for syntactic dependencies, and labeled
F1 for semantic dependencies, respectively
Labeled Prec. Labeled Rec. Labeled F1
88.68 73.89 80.28
Table 2: Performance of Local Semantic Role La-
beler n WSJ test set. Gold parsing result, correct
predicates, and correct rolesets are used.
3 Experimental Result
We have tested our system with the test set and
obtained official results as shown in Table 1. We
have also experimented on each module and ob-
tained promising results.
We have tried to find the upper bound of the
local semantic role labeling module. Table 2
shows the performance when gold syntactic pars-
ing result, correct predicates, and correct rolesets
are given. Comparing to phrase structure parser
based semantic role labelings such as Pradhan et
al. (2005) and Toutanova et al (2005), our local
semantic role labeler needs to enhance the perfor-
mance. We will try to add some lexical features or
chunk features in future works.
Next, we have analyzed the effect of roleset
based selector. Table 3 shows the effect of match-
ing score and relative frequency which are the
weighted factor of selection described in section
2.5. Here, baseline means that it selects a role se-
quence which has the highest score in CKY mod-
ule and roleset is chosen randomly. The results
show that roleset matching score and relative fre-
quency of roleset are effective to choose the correct
role sequence and identify roleset.
4 Conclusion
In this paper, we have described a syntactic and
semantic dependency parsing system with five dif-
ferent modules. Each module is developed with
maximum entropy classifiers based on different
predicate types. In particular, dependency relation
compression method and extracted path patterns
are used to improve the performance in the argu-
236
Prec. Rec. F1
Baseline (c) 69.34 58.42 63.41
+ mc 71.40 60.20 65.32
+ rf 75.94 63.98 69.45
+ mc, rf 76.46 64.45 69.95
Table 3: Semantic scores of global sequence selec-
tion in WSJ test set. mc, rf means matching score
and relative frequency, respectively
ment candidate identification. The roleset match-
ing method is devised to select the most appropri-
ate role sequence and to identify the correct role-
set.
However, the current features for roleset match-
ing seem to be not enough and other useful features
are expected to be found in the future work. There
is also a room for improving the method to inte-
grate the role sequence score, matching score, and
the relative frequency.
References
Joakim Nivre, Jens Nilsson, Johan Hall, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, Erwin Marsi. 2007. MaltParser: A
Language-Independent System for Data-Driven De-
pendency Parsing. Natural Language Engineering,
13(2):95?135.
Kadri Hacioglu. 2008. Semantic role labeling using
dependency trees. In COLING ?04: Proceedings of
Proceedings of the 20th international conference on
Computational Linguistics. Morristown, NJ, USA.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics. Morristown, NJ, USA.
Mihai Surdeanu and Richard Johansson and Adam
Meyers and Llu??s Ma`rquez and Joakim Nivre. 2008.
The CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th Conference on Computational Natu-
ral Language Learning (CoNLL-2008).
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere Comas. 2007. Combination Strategies for Se-
mantic Role Labeling. The Journal of Artificial In-
telligence Research, 29:105?151.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. Machine Learning. 60:11?39.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow Se-
mantic Parsing Using Support Vector Machines. In
Proceedings of the Human Language Technology
Conference/North American chapter of the Associ-
ation of Computational Linguistics (HLT/NAACL).
Boston, MA, USA.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic Role
Labeling. In Proceedings of CoNLL-2005.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling. In Proceedings of CoNLL-2004.
237
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 251?256,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Korea University System in the HOO 2012 Shared Task
Jieun Lee? and Jung-Tae Lee? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Research Institute of Computer Information & Communication, Korea University, Seoul, Korea
{jelee,jtlee,rim}@nlp.korea.ac.kr
Abstract
In this paper, we describe the Korea Univer-
sity system that participated in the HOO 2012
Shared Task on the correction of preposition
and determiner errors in non-native speaker
texts. We focus our work on training the sys-
tem on a large collection of error-tagged texts
provided by the HOO 2012 Shared Task or-
ganizers and incrementally applying several
methods to achieve better performance.
1 Introduction
In the literature, there have been efforts aimed at de-
veloping grammar correction systems designed es-
pecially for non-native English speakers. A typi-
cal approach is to train statistical models on well-
formed texts written by native English speakers and
apply the learned models to non-native speaker texts
to correct textual errors based on given context. This
approach, however, fails to model the types of errors
that non-native speakers usually make. Recent stud-
ies demonstrate that it is possible to improve the per-
formance of error correction systems by training the
models on error-annotated non-native speaker texts
(Han et al, 2010; Dahlmeier and Ng, 2011; Gamon,
2010). Most recently, a large collection of training
data consisting of preposition and determiner errors
made by non-native English speakers has been re-
leased in the HOO (Helping Our Own) 2012 Shared
Task, which aims at promoting the research and de-
velopment of automated tools for assisting authors
in writing (Dale et al, 2012).
In this paper, we introduce our error correction
system that participated in the HOO 2012 Shared
Task, where the goal is to correct errors in the use of
prepositions and determiners by non-native speakers
of English. We mainly focus our efforts on training
the system using the non-native speaker texts pro-
vided in the HOO 2012 Shared Task. We also share
our experience in handling some issues that emerged
while exclusively using the non-native speaker texts
for training our system. In the following sections,
we will describe the system in detail.
2 System Architecture
The goal of our system is to detect and correct prepo-
sition and determiner errors in a given text. Our sys-
tem consists of two types of classifiers, namely edit
and insertion classifiers. Inputs for the two types
of classifiers are noun phrases (NP), verb phrases
(VP), and prepositional phrases (PP); we initially
pre-process the text given for training/testing by us-
ing the Illinois Chunker1 and the Stanford Part-of-
Speech Tagger (Toutanova et al, 2003). For learn-
ing the classifiers, we use maximum entropy models,
which have been successfully applied to many tasks
in natural language processing. We particularly use
Le Zhang?s Maximum Entropy Modeling Toolkit2
for implementation.
2.1 Edit Classifiers
The role of an edit classifier is to check the source
preposition/determiner word originally chosen by
the author in a given text. If the source word
is incorrect, the classifier replaces it with a bet-
ter choice. For every preposition/determiner word,
1Available at http://cogcomp.cs.illinois.edu
2Available at http://homepages.inf.ed.ac.uk/lzhang10/
251
we train a classifier using examples that are ob-
served in training data. The choice for preposi-
tions is limited to eleven prepositions (about, at,
as, by, for, from, in, of, on , to, with) that most
frequently occur in the training data, and the can-
didates for determiner choice are the and a/an. In
summary, we train a total of thirteen edit classifiers,
one for each source preposition or determiner. For
each edit classifier, the set of candidate outputs con-
sists of the source preposition/determiner word it-
self, other confusable preposition/determiner words,
and no preposition/determiner in case the source
word should be deleted. Note that the number of
confusable words for each source preposition is de-
cided flexibly, depending on examples observed in
the training data; a similar approach has been pro-
posed earlier by Rozovskaya and Roth (2010a). For
a particular source preposition/determiner word in
the test data, the system decides whether to correct
it or not based on the output of the classifier for that
source word.
2.2 Insertion Classifier
Although the edit classifiers described above are
capable of deciding whether a source preposi-
tion/determiner word that appears in the test data
should be replaced or removed, a large proportion
of common mistakes for non-native English writers
consists of missing prepositions/determiners (i.e.,
leaving them out by mistake). To deal with those
types of errors, we train a special classifier for inser-
tions. A training or testing event for this particular
classifier is any whitespace before or after a word
in a noun or verb phrase that is a potential loca-
tion for a preposition or determiner. Table 1 shows
the five simple heuristic patterns based on part-of-
speech tags that the system uses in order to locate
potential sites for prepositions/determiners. Note
that s is a whitespace to be examined, an asterisk (*)
means wildcard, and NN includes the tags that start
with NN, such as NNS, NNP, and NNPS. VB is also
treated in the same manner as NN. The set of can-
didate outputs consists of the eleven prepositions,
the two determiners, and no preposition/determiner
class. Once a candidate position for insertion is de-
tected in the test data, the system decides whether to
make an insertion or not based on the output of the
insertion classifier.
Pattern Example
s+NN I?ll give you all information
s+*+NN I need few days
s+VB It may seem relaxing at beginning
s+*+VB Buy new colored clothes
VB+s I?m looking forward your reply
Table 1: Patterns of candidates for insertion
2.3 Features
Both edit and insertion classifiers can be trained us-
ing three types of features described below.
? LEX/POS/HEAD This feature set refers to the
contextual features from a window of n tokens
to the right and left that are practically used in
error correction studies (Rozovskaya and Roth,
2010b; Han et al, 2010; Gamon, 2010). Such
features include lexical features, part-of-speech
tags, and head words of the preceding and the
following chunks of the source word. In this
work, we set n to be 3.
? HAN This represents the set of features specifi-
cally used in the work of Han et al (2010); they
demonstrate that a model trained on non-native
speaker texts can outperform one trained solely
on well-formed texts.
? L13 L1 refers to the first language of the au-
thor. There have been some efforts to leverage
L1 information for improving error correction
performance. For example, Rozovskaya and
Roth (2011) propose an algorithm for adapting
a learned model to the L1 of the author. There
have been many studies leveraging writers? L1.
In this work, we propose to directly utilize L1
information of the authors as features. We also
leverage additional features by combining L1
and individual head words that govern or are
governed by VP or NP.
3 Additional Methods for Improvement
The training data provided in the HOO 2012 Shared
Task consists of exam scripts drawn from the pub-
licly available FCE dataset (Yannakoudakis et al,
3L1 information was provided in the training data but not in
the test data. Therefore, the benefits of using L1 remain incon-
clusive in this paper.
252
a/an the NULL
6028 114 203
Table 2: Training data distribution for a/an classifier
about as at by for from
0 3 2510 1 2 3
in of on to with NULL
75 7 20 30 3 41
Table 3: Training data distribution for at classifier
2011) with textual errors annotated in HOO data
format. From this data, we extract examples for
training our classifiers. For example, let w be a
source word that we specifically want our classifier
to learn. Every use of w that appears in the train-
ing data may be an example that the classifier can
learn from. However, it is revealed that for all w,
there are always many more examples where w is
used correctly than examples where w is replaced or
removed. Table 2 and Table 3 respectively show the
class distributions of all examples for source words
a/an and at that are observable from the whole train-
ing data for training a/an- and at-specific classifiers.
We can see that various classes among the training
data are unevenly represented. When training data is
highly skewed as shown in the two tables, construct-
ing a useful classifier becomes a challenging task.
We observed from our preliminary experiments that
classifiers learned on highly unbalanced data hardly
tend to correct the incorrect choices made by non-
native speakers. Therefore, we investigate two sim-
ple ways to alleviate this problem.
3.1 Filtering Examples Less Likely to be
Incorrect
As mentioned above, there are many more exam-
ples where the source preposition/determiner is used
without any error. One straightforward way to ad-
just the training data distribution is to reduce the
number of examples where the source word is less
likely to be replaced or removed by using language
model probabilities. If a language model learned on
a very large collection of well-formed texts returns
a very high language model probability for a source
word surrounded by its context, it may be reason-
Class Initial After After
Distribution Filtering Adding
about 0 0 528
as 3 3 275
at 2510 2367 2367
by 1 1 207
for 2 2 1159
from 3 3 550
in 75 75 1521
of 7 7 1454
on 20 20 541
to 30 30 2309
with 3 3 727
NULL 41 41 41
Table 4: Refined data distribution for at classifier
able to assume that the source word is used correctly.
Therefore we build a language model trained on the
English Gigaword corpus by utilizing trigrams. Be-
fore providing examples to the classifiers for train-
ing or testing, we filter out those that have very high
language model probabilities above a pre-defined
threshold value.
3.2 Adding Artificial Errors
Our second approach is to introduce more artificial
examples to the training data, so that the class dis-
tribution of all training examples becomes more bal-
anced. For example, if we aim at adding more train-
ing examples for a/an classifier, we would extract
correct phrases such as ?the different actor? from the
training data and artificially convert it into ?a differ-
ent actor? so that an example of a/an being corrected
to the is also provided to a/an classifier for training.
When adding aritificial examples into the training
data, we avoid the number of examples belonging
to each class exceeding the number of cases where
the source word is not replaced or removed. Table
4 demonstrates the results of both the filtering and
adding approaches for training the a/an classifier.
4 Experiments
4.1 Runs
This section describes individual runs that we sub-
mitted to the HOO 2012 Shared Task organizers. Ta-
ble 5 represents the setting of each run.
253
Runs Models Features Filtering Adding
Threshold
Run0 LM n/a n/a
Run1 ME LEX/POS/HEAD X X
Run2 ME HAN X X
Run3 ME LEX/POS/HEAD -2 X
Run4 ME LEX/POS/HEAD -2 O
Run5 ME LEX/POS/HEAD, L1 -2 O
Run6 ME LEX/POS/HEAD, L1, age -2 O
Run7 ME Insertion: POS/HEAD X X
Other: LEX/POS/HEAD
Run8 ME LEX/POS/HEAD -3 X
Table 5: The explanation of each runs
? Run0 This is a baseline run that represents the
language model approach proposed by Gamon
(2010). We train our language model on Giga-
word corpus, utilizing trigrams with interpola-
tion and Kneser-Ney discount smoothing.
? Run1, 2 Run1 and 2 represent our system us-
ing the LEX/POS/HEAD feature sets and HAN
feature sets respectively. Neither additional
method described in Section 3 is applied.
? Run3, 8 These runs represent our system us-
ing LEX/POS/HEAD features (Run1), where
examples that are less likely to be incorrect are
filtered out by consulting our language model.
The threshold value is set to ?2 and ?3 for
Runs 3 and 8 respectively.
? Run4 This particular run is one where we intro-
duce additional errors in order to make the class
distribution of the training data for the classi-
fiers more balanced. This step is incrementally
applied in the setting of Run3.
? Run5, 6 Run5 and 6 are when we consider L1
information and age respectively as additional
features for training the classifiers. The basic
setup is same as Run4.
? Run7 This run represents our system with
its insertion classifier trained using POS and
HEAD features only. No LEX features are
used.
Runs Precision Recall F-score
Run0 1.45 15.45 2.65
Run1 1.35 10.82 2.39
Run2 1.23 11.48 2.22
Run3 1.33 10.6 2.36
Run4 1.19 11.26 2.15
Run5 1.02 10.38 1.87
Run6 0.99 9.93 1.79
Run7 1.16 11.26 2.1
Run8 1.46 11.04 2.58
Table 6: Correction before test data revisions
5 Results
Table 6 shows the correction scores of the individual
runs that we originally submitted. Unfortunately, we
should confess that we made a vital mistake while
generating the runs from 1-8; the modules imple-
mented for learning the insertion classifier had some
bugs that we could not notice during the submission
time. Because of this, our system was unable to han-
dle MD and MT type errors properly. This is the
reason why the performance figures of our runs are
very low. For reference, we include Tables 7-10 that
illustrate the performance of our individual runs that
we calculated by ourselves using the test data and
the evaluation tool provided by the organizers.
We can observe that Run3 outperforms Run1 and
Run4 performs better than Run3, which demon-
strates that our attempts to improve the system per-
formance by adjusting training data for classifiers
254
Runs Precision Recall F-score
Run1 42.67 7.06 12.12
Run2 49.28 7.51 13.03
Run3 47.62 6.62 11.63
Run4 45.45 7.73 13.21
Run5 33.82 10.15 15.62
Run6 8.68 18.54 11.82
Run7 33.33 10.82 16.33
Run8 50.0 7.28 12.72
Table 7: Recognition before test data revisions (system
revised)
Runs Precision Recall F-score
Run1 32.0 5.3 9.09
Run2 42.03 6.4 11.11
Run3 34.92 4.86 8.53
Run4 37.66 6.4 10.94
Run5 26.47 7.94 12.22
Run6 5.68 12.14 7.74
Run7 24.49 7.95 12.0
Run8 42.42 7.28 10.79
Table 8: Correction before test data revisions (system re-
vised)
help. Moreover, we can also see that L1 informa-
tion helps when directly used for training features.
6 Conclusion
This was our first attempt to participate in a shared
task that involves the automatic correction of gram-
matical errors made by non-native speakers of En-
glish. In this work, we tried to focus on investigating
simple ways to improve the error correction system
learned on non-native speaker texts. While we had
made some critical mistakes on the submitted runs,
we were able to observe that our method can poten-
tially improve error correction systems.
Acknowledgments
We would like to thank Hyoung-Gyu Lee for his
technical assistance.
References
Daniel Dahlmeier and Hwee Tou Ng. 2011. Gram-
matical error correction with alternating structure op-
Runs Precision Recall F-score
Run1 49.33 7.82 13.50
Run2 52.17 7.61 13.28
Run3 52.38 6.98 12.31
Run4 51.95 8.46 14.55
Run5 37.5 10.78 16.75
Run6 9.29 19.03 12.5
Run7 36.73 11.42 17.42
Run8 51.52 7.18 12.62
Table 9: Recognition after test data revisions (system re-
vised)
Runs Precision Recall F-score
Run1 34.67 5.5 9.49
Run2 42.02 6.13 10.7
Run3 36.51 4.86 8.58
Run4 38.96 6.34 10.91
Run5 29.42 8.45 13.13
Run6 6.40 13.11 8.61
Run7 25.85 8.03 12.26
Run8 42.42 5.92 10.39
Table 10: Correction after test data revisions (system re-
vised)
timization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, ACL-HLT
?11, pages 915?923, Portland, Oregon.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the 7th Workshop on Innovative Use of NLP for
Building Educational Applications, HOO ?12, Mon-
treal, Canada.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing: a meta-classifier
approach. In Human Language Technologies: Pro-
ceedings of the 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, NAACL-HLT ?10, pages 163?171,
Los Angeles, California.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner
corpus to develop an esl/efl error correction system.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation, LREC ?10,
pages 763?770, Malta.
Alla Rozovskaya and Dan Roth. 2010a. Generating
confusion sets for context-sensitive error correction.
255
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 961?970, Cambridge, Massachusetts.
Alla Rozovskaya and Dan Roth. 2010b. Training
paradigms for correcting errors in grammar and usage.
In Human Language Technologies: Proceedings of the
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL-HLT ?10, pages 154?162, Los Angeles, Cali-
fornia.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for esl correction tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, ACL-HLT ?11, pages
924?933, Portland, Oregon.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Edmonton, Canada.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, ACL-
HLT ?11, pages 180?189, Portland, Oregon.
256
Proceedings of the TextGraphs-7 Workshop at ACL, pages 15?19,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Link Analysis to Discover Interesting Messages Spread Across Twitter
Min-Chul Yang? and Jung-Tae Lee? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Research Institute of Computer Information & Communication, Korea University, Seoul, Korea
{mcyang,jtlee,rim}@nlp.korea.ac.kr
Abstract
Twitter, a popular social networking service,
enables its users to not only send messages
but re-broadcast or retweet a message from an-
other Twitter user to their own followers. Con-
sidering the number of times that a message is
retweeted across Twitter is a straightforward
way to estimate how interesting it is. How-
ever, a considerable number of messages in
Twitter with high retweet counts are actually
mundane posts by celebrities that are of inter-
est to themselves and possibly their followers.
In this paper, we leverage retweets as implicit
relationships between Twitter users and mes-
sages and address the problem of automati-
cally finding messages in Twitter that may be
of potential interest to a wide audience by us-
ing link analysis methods that look at more
than just the sheer number of retweets. Exper-
imental results on real world data demonstrate
that the proposed method can achieve better
performance than several baseline methods.
1 Introduction
Twitter (http://twitter.com) is a popular so-
cial networking and microblogging service that en-
ables its users to share their status updates, news,
observations, and findings in real-time by posting
text-based messages of up to 140 characters, called
tweets. The service rapidly gained worldwide pop-
ularity as a communication tool, with millions of
users generating millions of tweets per day. Al-
though many of those tweets contain valuable in-
formation that is of interest to many people, many
others are mundane tweets, such as ?Thanks guys
for the birthday wishes!!? that are of interest only to
the authors and users who subscribed to their tweets,
known as followers. Finding tweets that are of po-
tential interest to a wide audience from large volume
of tweets being accumulated in real-time is a crucial
but challenging task. One straightforward way is to
rely on the numbers of times each tweet has been
propagated or retweeted by readers of the tweet.
Hong et al (2011) propose to regard retweet count
as a measure of popularity and present classifiers for
predicting whether and how often new tweets will be
retweeted in the future. However, mundane tweets
by highly popular users, such as celebrities with
huge numbers of followers, can record high retweet
counts. Alonso et al (2010) use crowdsourcing to
categorize a set of tweets as ?only interesting to au-
thor and friends? and ?possibly interesting to others?
and report that the presence of a URL link is a single,
highly effective feature for distinguishing interest-
ing tweets with more than 80% accuracy. This sim-
ple rule, however, may incorrectly recognize many
interesting tweets as not interesting, simply because
they do not contain links. Lauw et al (2010) suggest
several features for identifying interesting tweets but
do not experimentally validate them.
In this study, we follow the definition of inter-
esting tweets provided by Alonso et al (2010) and
focus on automatic methods for finding tweets that
may be of potential interest to not only the authors
and their followers but a wider audience. Since
retweets are intended to spread tweets to new audi-
ences, they are often a recommendation or, accord-
ing to Boyd et al (2010), productive communica-
tion tool. Thus, we model Twitter as a graph con-
15
sisting of user and tweet nodes implicitly connected
by retweet links, each of which is formed when one
user retweets what another user tweeted. We present
a variant of the popular HITS algorithm (Kleinberg,
1999) that exploits the retweet link structure as an
indicator of how interesting an individual tweet is.
Specifically, we draw attention on the fact that not all
retweets are meaningful. Some users retweet a mes-
sage, not because of its content, but only because
they were asked to, or because they regard retweet-
ing as an act of friendship, loyalty, or homage to-
wards the person who originally tweeted (Boyd et
al., 2010). The algorithm proposed in this paper is
designed upon the premise that not all retweet links
are created equal, assuming that some retweets may
carry more importance or weight than others. Welch
et al (2011) and Romero et al (2011) similarly ex-
tend link analysis to Twitter, but address essentially
different problems. We conduct experiments on real
world tweet data and demonstrate that our method
achieves better performance than the simple retweet
count approach and a similar recent work on Twitter
messages (Castillo et al, 2011) that uses supervised
learning with a broad spectrum of features.
2 Proposed Method
We treat the problem of finding interesting tweets as
a ranking problem where the goal is to derive a scor-
ing function which gives higher scores to interesting
tweets than to uninteresting ones in a given set of
tweets. To derive the scoring function, we adopt a
variant of HITS, a popular link analysis method that
emphasizes mutual reinforcement between authority
and hub nodes (Kleinberg, 1999).
Formally, we model the Twitter structure as di-
rected graph G = (N,E) with nodes N and di-
rectional edges E. We consider both users U =
{u1, . . . , unu} and tweets T = {t1, . . . , tnt} as
nodes and the retweet relations between these nodes
as directional edges. For instance, if tweet ta, cre-
ated by user ua, retweets tb, written by user ub, we
create a retweet edge eta,tb from ta to tb and another
retweet edge eua,ub from ua to ub.
1 Strictly speak-
ing,G has two subgraphs, one based only on the user
nodes and another based on the tweet nodes. Instead
of running HITS on the tweet subgraph right away,
1Note that two user nodes can have multiple edges.
we first run it on the user subgraph and let tweets in-
herit the scores from their publishers. Our premise
is that the scores of a user is an important prior in-
formation to infer the scores of the tweets that the
user published.
User-level procedure: We first run the algorithm
on the user subgraph. ?ui, we update the authority
scores A(ui) as:
?
?j:euj,ui?E
|{uk ? U : euj ,uk ? E}|
|{k : euj ,uk ? E}|
?H(uj) (1)
Then, ?ui, we update the hub scores H(ui) to be:
?
?j:eui,uj?E
|{uk ? U : euk,uj ? E}|
|{k : euk,uj ? E}|
?A(uj) (2)
A series of iterations is performed until the scores
are converged. After each iteration, the author-
ity/hub scores are normalized by dividing each of
them by the square root of the sum of the squares
of all authority/hub values. When this user-level
stage ends, the algorithm outputs a function SUA :
U ? [0, 1], which represents the user?s final au-
thority score, and another function SUH : U ?
[0, 1], which outputs the user?s final hub score. Note
that, unlike the standard HITS, the authority/hub
scores are influenced by edge weights that reflect the
retweet behaviors of individual users. The idea here
is to dampen the influence of users who devote most
of their retweet activities toward a very few other
users, such as celebrities, and increase the weight
of users who retweet many different users? tweets.
To demonstrate the effectiveness of the parameter,
we have done some preliminary experiments. The
column Userfrom in Table 1 shows the retweet be-
havior of users who retweeted tweets belonging to
?uninteresting? and ?interesting? classes observed
in our Twitter dataset. The values are calculated
by the ratio of all other users that a user retweeted
to all retweet outlinks from the user; a value closer
to 1 means that outlinks are pointed to many dif-
ferent users.2 We observe that the value for users
who retweeted interesting tweets is shown to be
higher, which means that they tend to retweet mes-
sages from many different users, more than users
who retweeted uninteresting ones.
2For calculating the ratios, we limit the target to users who
retweeted two or more times in our dataset.
16
Class Userfrom F = ? #
Not Interesting 0.591 0.252 1985
Possibly Interesting 0.711 0.515 1115
Both 0.634 0.346 3100
Table 1: Dataset analysis.
Tweet-level procedure: After the user-level
stage, we start computing the scores of the tweet
nodes. In each iteration, we start out with each tweet
node initially inheriting the scores of its publisher.
Let P : T ? U be a function that returns the pub-
lisher of a given tweet. ?ti, we update A(ti) to be:
SUA(P (ti)) +
?
?j:etj ,ti?E
F (etj ,ti)?H(tj) (3)
Then, ?ti, we update H(ti) to be:
SUH (P (ti)) +
?
?j:eti,tj?E
F (eti,tj )?A(tj) (4)
where F (eta,tb) is a parameter function that returns
? > 1 if P (ta) is not a follower of P (tb) and 1 other-
wise. It is intuitive that if users retweet other users?
tweets even if they are not friends, then it is more
likely that those tweets are interesting. The column
F = ? in Table 1 shows the ratio of all unfollow-
ers who retweeted messages in a particular class to
all users who retweeted messages in that class, ob-
served in our dataset. We observe that users retweet
interesting messages more, even when they do not
follow the publishers. Similar observation has also
been made by Recuero et al (2011). After each it-
eration, the authority/hub scores are normalized as
done in the user-level. After performing several it-
erations until convergence, the algorithm finally out-
puts a scoring function STA : T ? [0, 1], which rep-
resents the tweet node?s final authority score. We use
this function to produce the final ranking of tweets.
Text pattern rules: We observe that in some
cases users retweet messages from their friends, not
because of the contents, but via retweet requests to
simply evoke attention. To prevent useless tweets
containing such requests from receiving high author-
ity scores, we collect 20 simple text pattern match-
ing rules that frequently appear in those tweets.
Specifically, we let the rules make influence while
updating the scores of tweets by modifying the sum-
mations in Eq. (3) and (4) respectively as:
?
?j:etj ,ti?E
F (etj ,ti)?R(ti)?H(tj) (5)
?
?j:eti,tj?E
F (eti,tj )?R(tj)?A(tj) (6)
where R(t) is a rule-based function that returns 0 if
tweet t contains one of the pre-defined text patterns
and 1 otherwise. Such patterns include ?RT this if?
and ?If this tweet gets RT * times I will?.
3 Experiment and Discussion
Our Twitter dataset is collected during 31 days of
October 2011, containing 64,107,169 tweets and
2,824,365 users. For evaluation, we generated 31
immediate Twitter graphs composed of 1.5 million
retweet links in average and 31 initially ranked lists
of tweets, each consisting of top 100 tweets created
on a specific date of the month with highest retweet
counts accumulated during the next 7 days. Two an-
notators were instructed to categorize each tweet as
interesting or not, by inspecting its content as done
in the work of Alonso et al (2010). In case of dis-
agreement (about 15% of all cases), a final judgment
was made by consensus between the two annotators.
We observe that the ratio of tweets judged to be in-
teresting is about 36%; the column ?#? in Table 1
shows the actual counts of each class. The goal of
this evaluation is to demonstrate that our method is
able to produce better ranked lists of tweets by re-
ranking interesting tweets highly.
Table 2 reports the ranking performance of vari-
ous methods in terms of Precisions @10 and @20,
R-Precision, and MAP. We compare our approach
to four baselines. The first baseline, #RT, is obvi-
ously based on retweet counts; tweets with higher
retweet counts are ranked higher. The second base-
line, #URL+#RT, favors tweets that contain URL
links (Alonso et al, 2010). Since it is less likely for
a tweet to contain more than one link, we addition-
ally use #RT to break ties in tweet ranking. Thirdly,
HITSoriginal, is the standard HITS algorithm run on
both user and tweet subgraphs that calculates author-
ity/hub scores of a node purely by the sum of hub
values that point to it and the sum of authority val-
ues that it points to, respectively, during iterations;
17
Method P@10 P@20 R-Prec MAP
#RT 0.294 0.313 0.311 0.355
#URL+#RT 0.245 0.334 0.362 0.361
HITSoriginal 0.203 0.387 0.478 0.465
MLmessage 0.671 0.645 0.610 0.642
MLall 0.819 0.795 0.698 0.763
HITSproposed 0.881 0.829 0.744 0.807
Table 2: Performance of individual methods
no other influential factors are considered in the cal-
culations. Lastly, we choose one recent work by
Castillo et al (2011) that addresses a related prob-
lem to ours, which aims at learning to classify tweets
as credible or not credible. Although interestingness
and credibility are two distinct concepts, the work
presents a wide range of features that may be ap-
plied for assessing interestingness of tweets using
machine learning. For re-implementation, we train
a binary SVM classifier using features proposed by
Castillo et al (2011), which include features from
users? tweet and retweet behavior, the text of the
tweets, and citations to external sources; we use
the probability estimates of the learned classifier for
re-ranking.3 We use leave-one-out cross validation
in order to evaluate this last approach, denoted as
MLall. MLmessage is a variant that relies only on
message-based features of tweets. Our method, with
? empirically set to 7, is denoted as HITSproposed.
We observe that #RT alone is not sufficient mea-
sure for discovering interesting tweets. Additionally
leveraging #URL helps, but the improvements are
only marginal. By manually inspecting tweets with
both high retweet counts and links, it is revealed
that many of them were tweets from celebrities with
links to their self-portraits photographed in their
daily lives, which may be of interest to their own
followers only. HITSoriginal performs better than
both #RT and #URL across most evaluation met-
rics but generally does not demonstrate good per-
formance. MLmessage always outperform the first
three significantly; we observe that tweet lengths
in characters and in words are the two most effec-
tive message-based features for finding interesting
tweets. The results of MLall demonstrates that more
3We do not use some topic-based features in (Castillo et al,
2011) since such information is not available in our case.
Method P@10 P@20 R-Prec MAP
HITSproposed 0.881 0.829 0.744 0.807
w/o User 0.677 0.677 0.559 0.591
w/o Tweet 0.861 0.779 0.702 0.772
w/o Rule 0.858 0.81 0.733 0.781
Table 3: Contributions of individual stages.
reasonable performance can be achieved when user-
and propagation-based features are combined with
message-based features. The proposed method sig-
nificantly outperforms all the baselines. This is a
significant result in that our method is an unsuper-
vised approach that relies on a few number of tweet
features and does not require complex training.
We lastly report the contribution of individual
procedures in our algorithm in Table 3 by ablat-
ing each of the stages at a time. ?w/o User? is
when tweet nodes do not initially inherit the scores
of their publishers. ?w/o Tweet? is when tweets
are re-ranked according to the authority scores of
their publishers. ?w/o Rule? is when we use Eq.
(3) and (4) instead of Eq. (5) and (6) for updating
tweet scores. We observe that the user-level proce-
dure plays the most crucial role. We believe this is
because of the ability of HITS to distinguish good
?hub-users?. Since authoritative users can post ordi-
nary status updates occasionally in Twitter, we can-
not always expect them to create interesting content
every time they tweet. However, good hub-users4
tend to continuously spot and retweet interesting
messages; thus, we can expect the tweets they share
to be interesting steadily. The role of hubs is not as
revealed on the tweet side of the Twitter graph, since
each tweet node can only have at most one retweet
outlink. The exclusion of text pattern rules does not
harm the overall performance much. We suspect this
is because of the small number of rules and expect
more improvement if we add more effective rules.
Acknowledgments
This work was supported by the Ministry of Knowl-
edge Economy of Korea, under the title ?Develop-
ment of social web issue detection-monitoring &
prediction technology for big data analytic listening
platform of web intelligence (10039158)?.
4Often referred to as content curators (Bhargava, 2009).
18
References
Omar Alonso, Chad Carson, David Gerster, Xiang Ji, and
Shubha U. Nabar. 2010. Detecting uninteresting con-
tent in text streams. In Proceedings of the SIGIR 2010
Workshop on Crowdsourcing for Search Evaluation,
CSE ?10, pages 39?42.
Rohit Bhargava. 2009. Manifesto for the content curator:
The next big social media job of the future? http:
//rohitbhargava.typepad.com/.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of the 2010 43rd
Hawaii International Conference on System Sciences,
HICSS ?10, pages 1?10.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete.
2011. Information credibility on twitter. In Proceed-
ings of the 20th international conference on World
wide web, WWW ?11, pages 675?684.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison. 2011.
Predicting popular messages in twitter. In Proceed-
ings of the 20th international conference companion
on World wide web, WWW ?11, pages 57?58.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. J. ACM, 46(5):604?632.
Hady W. Lauw, Alexandros Ntoulas, and Krishnaram
Kenthapadi. 2010. Estimating the quality of postings
in the real-time web. In Proceedings of the WSDM
2010 Workshop on Search in Social Media, SSM ?10.
Raquel Recuero, Ricardo Araujo, and Gabriela Zago.
2011. How does social capital affect retweets? In Pro-
ceedings of the 5th International AAAI Conference on
Weblogs and Social Media, ICWSM ?11, pages 305?
312.
Daniel M. Romero, Wojciech Galuba, Sitaram Asur, and
Bernardo A. Huberman. 2011. Influence and passiv-
ity in social media. In Proceedings of the 2011 Euro-
pean Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases,
ECML-PKDD ?11, pages 18?33.
Michael J. Welch, Uri Schonfeld, Dan He, and Junghoo
Cho. 2011. Topical semantics of twitter links. In Pro-
ceedings of the 4th International Conference on Web
Search and Web Data Mining, WSDM ?11, pages 327?
336.
19
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 123?127,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
KUNLP Grammatical Error Correction System
For CoNLL-2013 Shared Task
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim
Department of Computer and Radio Communications Engineering
Korea University
Anam-dong 5-ga, Seongbuk-gu, Seoul, South Korea
{bjyi,hclee,rim}@nlp.korea.ac.kr
Abstract
This paper describes an English grammat-
ical error correction system for CoNLL-
2013 shared task. Error types covered by
our system are article/determiner, prepo-
sition, and noun number agreement. This
work is our first attempt on grammatical
error correction research. In this work,
we only focus on reimplementing the tech-
niques presented before and optimizing
the performance. As a result of the imple-
mentation, our system?s final F1-score by
m2 scorer is 0.1282 in our internal test set.
1 Introduction
As the number of English learners is increas-
ing world widely, the research topic of auto-
mated grammar error correction is lively dis-
cussed. However, automated grammar error cor-
rection is a very difficult field and the result is not
satisfactory. Therefore, the shared task about En-
glish error correction has been annually held and
many researchers are trying to solve this problem.
Helping Our Own (HOO) 2011 is a pilot shared
task for automated correction of errors in non-
native English speakers? papers. The shared task
evaluates the performance of detection, recogni-
tion, correction on thirteen types of English gram-
matical errors by using F1-score. Because each
error type has different characteristics, they have
to use different approaches to correct appropriate
error types.
In HOO 2012, only two types of errors, prepo-
sition and determiner were handled. This shared
task also evaluated the performance of detection,
recognition, correction by using F1-score. The
best result of the preposition error correction is
0.2371 in F1-score and the determiner error cor-
rection is 0.3460 in F1-score. These are remark-
able achievement.
This year CoNLL 2013 shared task covers five
types of errors based on the result of HOO 2012.
These error types are determiner, preposition,
noun number, verb form, and subject-verb agree-
ment. Because of the limited amount of time and
manpower, we only focus on preposition, deter-
miner and noun number.
2 Previous Works
Most methods for grammar error correction have
tried to correct one type of errors. Researchers
have never attempted to correct different types of
errors at the same time.
In this work, we try to solve the error correction
problem based on the previous research presenting
good performance.
First, the preposition error correction is based
on (Han et al, 2010). They tried to correct the
most commonly used 10 preposition errors based
on the classification approach. 10 prepositions are
about, at, by, for, from, in, of, on, to, with. They
have implemented 11-way classifier to output 11
types of proper word(10 prepositions + ?NULL?)
for 11 types of source words. This work assumes
that three kinds of corrections exist. If ?NULL? is
taken as input and some preposition is produced, it
is omission. If some preposition is taken as input
and another preposition is produced, it is replace-
ment. If some preposition is taken as input and
?NULL? is produced, it is commission. In the case
of replacement, correction precision is 0.817 and
recall is 0.132. Furthermore, they reported that
the performance is much better when they train the
model with well edited error tagged corpus.
(Felice and Pulman, 2008) also used a method
based on classification. It is, nevertheless, unusual
that they did not use error tagged learner?s corpus
but error free British National Corpus. Without
using an error tagged corpus, they have achieved
51.5% accuracy for error correction.
To improve low recall of Han?s method, to con-
123
struct large training data is the best way. However,
it is very costly and hard work to obtain well edited
error tagged corpus. By the way, error free corpus
like news articles is relatively easy to acquire. We
plan to utilize large error free corpus as the train-
ing data to overcome the problem of low recall.
That plan motivated by Felice?s work has not been
tried on the proposed system. We will attempt to
reimplement the system by utilizing the error free
corpus in the near future.
3 System Description
Our system is composed of three components,
preposition error corrector, article error corrector,
and noun number error corrector. In this work,
we do not consider complex cases of grammar er-
rors, thus we assume that the order of correction
does not influence the result of correction. And
all components are based on the machine learning
method.
3.1 Preposition Error Correction
In the training corpus, there are more article errors
than preposition errors in number. However, the
preposition error correction is much more difficult
and the performance of correction is worse than
the article error correction.
We select preposition error candidates for re-
placement or commission or omission as follows.
? Replacement or Commission
? Preposition : tagged as ?IN? or ?TO?
and dependency relation with its par-
ent(DPREL) is identified as a ?prep?
? Omission
? In front of a noun phrase : the preceding
word of the noun phrase is not preposi-
tion
? In front of a verb phrase : the preced-
ing word of the verb phrase including
?VBG?(verb, gerund/present participle)
is not preposition
As described above, we use all
words(preposition and ?NULL? when omis-
sion) in that place as source word for preposition
correction.
We have implemented only one classifier that
takes a source word as input and produces cor-
rected preposition or ?NULL? as output. We use
No. Feature Name Description
1 s the source word
2 wd-1 the word preceding s
3 wd+1 the word following s
4 wd-1,2 s the two words preceding s
and s
5 s wd+1,2 s and the two words fol-
lowing s
6 3GRAM the trigram, wd-1, s,
wd+1
7 5GRAM the five-gram, wd-2, wd-
1, s, wd+1, wd+2
8 MOD the lexical head which the
preposition modifies
9 ARG the lexical head of the
preposition argument
10 MOD ARG MOD and ARG
11 MOD s MOD and s
12 s ARG s and ARG
13 MOD s ARG MOD, s, ARG
14 MODt ARGt POS tags of MOD and
ARG
15 MODt s the POS tag of MOD and
s
16 s ARGt s and the POS tag of ARG
17 MODt s ARGt MODt and s and ARGt
18 TRIGRAMt POS trigram
19 wd L 3 words preceding s
20 wd R 3 words following s
Table 1: Set of features proposed by (Han et al,
2010)
the part of feature set(Table 1) proposed by (Han
et al, 2010) for learning. They are presented in
the experiment part.
Each feature represents the word itself in the
Han?s work. However, the same word can be ex-
tracted again as a different kind of features. In or-
der to distinguish the same word used for the dif-
ferent features, we attach the feature name to the
word as postfix. This naming convention can make
the feature sparse, but increase the discrimination
power and improve the performance of the classi-
fier. In our experiment, we have tested the system
with two different sets of features(i.e. raw word
and with feature names).
124
3.2 Article Error Correction
We have implemented the article error corrector
just like the preposition error corrector. When we
experiment the pilot article correction system just
like the preposition correction system, it shows a
good performance unexpectedly. There is a little
difference in presenting set of features. In prepo-
sition error corrector, we add postfix to the set of
feature to keep sort of features(e.g. word ?in? as
source word feature, postfix is ?S?, final feature
is ?in S?). This method gives more discrimination
ability to the classifier. But in case of article, using
raw word lead to a better result.
3.3 Noun Number Error Correction
Noun number error indicates improper use of
singular or plural form of nouns. For example,
the singular form ?problem? should be corrected
to the plural form ?problems? in the following
sentence.
?They are educational and resource problem.?
As far as we know, there have been few at-
tempts to correct noun number agreement errors.
In this shared task, we propose a novel noun num-
ber agreement correction system based on a ma-
chine learning method trained with basic features.
In order to extract nouns from the input sen-
tences, we parse the sentence and extract the last
noun in every noun phrase for the error correction
candidates. If there is a coordinating conjunction
in the noun phrase, we split the noun phrase into
two parts and extract two candidates.
[S [NP Relevant information] [VP are [ADJP
readily ROOT available [PP on [NP [NP the
internet] and [NP article] [PP in [NP maga-
zines and newspapers] .]]]
Figure 1: Extracting candidates for error correc-
tion of noun number(candidates are indicated by
bold)
Figure 1 shows the example of selecting can-
didates for the error correction of noun number.
We classify a noun into four classes using fea-
tures of Table 2 based on the machine learn-
ing method. Four classes are NN(plural noun),
NNP(plural proper noun), NNS(singular noun),
and NNPS(singular proper noun). It is based on
the observation that the common noun and the
No. Feature Name Description
1 p POS tag for the source word
2 s the source word
3 DET the determiner of the noun ar-
gument
4 CD boolean value whether the
cardinal number exists
5 UCNT boolean value whether the
noun is uncountable
6 MOD the lexical head which the
noun modifies
7 MMOD the lexical head which MOD
modifies
8 ARG the lexical head of the noun
argument
9 AARG the lexical head of the ARG
argument
10 MOD s ARG MOD, s, and ARG
11 MODt POS tag for MOD
12 MMODt POS tag for MMOD
13 ARGt POS tag for ARG
14 AARGt POS tag for AARG
15 MODt p ARGt MODt, p, and ARGt
16 MMOD MOD s MMOD, MOD, and s
17 s ARG AARG s, ARG, and AARG
18 MOD s ARG MOD, s, and ARG
19 MM M s A AA MMOD, MOD, s, ARG, and
AARG
20 MMODt MODt p MMODt, MODt, and p
21 p ARGt AARGt p, ARGt, and AARGt
22 MODt p ARGt MODt, p, and ARGt
23 MMt Mt s At AAt MMODt, MODt, s, ARGt,
and AARGt
Table 2: Set of features for learning noun number
error correction
proper noun have many different characteristic.
The set of features used for learning is shown in
Table 2.
4 Experiments
4.1 Corpus
We use only NUS Corpus of Leaner En-
glish(NUCLE)((Dahlmeier, 2013)) provided from
CoNLL 2013 shared task. We construct the devel-
opment set with first sentences for every 10 sen-
tence and the test set with second sentences and
the training set with the rest of sentences. The
system is trained to learn error correction with the
training set and optimized with the development
set and finally evaluated with the test set.
4.2 Preposition Correction Experiment
Table 1 shows 20 types of features used by (Han
et al, 2010). We have found that the features con-
sist of various types and the learning world be dis-
turbed by too many features. In our experiment,
125
Number of feature 20 18 9
Raw
Word
Precision 0.0571 0.1194 0.0196
Recall 0.0402 0.0402 0.1256
F1-score 0.0472 0.0602 0.0339
With
Feature
Name
Precision 0.1034 0.1750 0.0208
Recall 0.0302 0.0352 0.1307
F1-score 0.0467 0.0586 0.0359
Table 3: The result of preposition error correction
we exclude wd L(19), wd R(20) and employ 18
kinds of features.
We will try to train the correction model by us-
ing large amount of error free corpus in order to
overcome the problem of low recall. To parse large
corpus is very time consuming task. So, in this
experiment, we select 9 features which can be ex-
tracted without parsing, and test the possibility of
using 9 features by training and testing the correc-
tion model.
We have performed two different experiments.
In the first experiment, we have used the word it-
self as a feature. In the tables 3?5, ?Raw Word?
represents the case when we use just the word
itself. In the second experiment, we have used
the feature name as the postfix of the feature. In
the tables 3?5, ?With Feature Name? represents
the case when we attach the feature name to the
feature and use it as a feature. For all experi-
ments, we have tried to differentiate the number
of features. 20 features are same as Han?s work.
18 features are the case when we exclude 2 fea-
tures(i.e. wd L(19), wd R(20)). 9 features are the
case when we use only features which do not re-
quire parsing.
We have experimented with Maximum Entropy
learning method, and fixed the iteration number to
200. Table 3 shows that the precision has highly
increased although the recall has decreased when
we add the feature name to the set of features used
for learning.
When we use 18 features except wd L(3 words
preceding s) and wd R(3 words following s), the
error correction system achieves the best perfor-
mance. According to the experimental result, we
can achieve the better result when we use 18 fea-
tures and the raw word. But we select final option
using 18 features and the word with feature name
because of optimization strategies that improve the
precision.
Number of feature 20 18 9
Raw
Word
Precision 0.1827 0.3176 0.0914
Recall 0.1123 0.1264 0.1139
F1-score 0.1391 0.1808 0.1014
With
Feature
Name
Precision 0.1942 0.3174 0.1059
Recall 0.1154 0.1139 0.1061
F1-score 0.1448 0.1676 0.1060
Table 4: The result of article error correction
Kinds of feature Basic
Basic&
Indep-
endent
Basic&
Com-
plex
Raw
Word
Precision 0.2462 0.1435 0.2469
Recall 0.0379 0.0811 0.0540
F1-score 0.0662 0.1036 0.0887
With
Feature
Name
Precision 0.2413 0.1676 0.2875
Recall 0.0378 0.0838 0.0621
F1-score 0.0654 0.1117 0.1022
Table 5: The result of noun number error correc-
tion
4.3 Article Correction Experiment
Table 4 shows that the feature name addition does
not improve the precision in the case of article cor-
rection, and the set of 18 features achieves the best
performance for article correction. Therefore, we
just use raw words for features and select 18 fea-
tures for article correction.
4.4 Noun Number Correction Experiment
In Table 2, features of number 1?5 belong to the
basic feature set and features of number 6?15 be-
long to the independent feature set and features
of number 16?23 belong to the complex feature
set. The experimental result with various combi-
nations of feature sets shows that the set of basic
and complex features achieves the best precision
in spite of low recall as shown in Table 5. We use
this option and experimentally select the iteration
number 700.
5 Conclusions
We develop a grammatical error correction system
which can recognize and correct preposition, arti-
cle, and noun number errors. In this experiment,
we have found out the set of good features for
preposition and article error correction, and pro-
posed a novel noun number error correction tech-
nique based on the machine learning method. For
126
the future work, we will try to utilize large amount
of external resources such as well written error
free corpus.
References
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu 2013.
Building a Large Annotated Corpus of Learner En-
glish: The NUS Corpus of Learner English, Pro-
ceedings of the 8th Workshop on Innovative Use of
NLP for Building Educational Applications (BEA
2013), Atlanta, Georgia, USA.
Rachele De Felice and Stephen G. Pulman 2008. A
classifier-based approach to preposition and deter-
miner error correction in L2 English, Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (Coling 2008), 169?176, Manch-
ester, UK
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, Jin-Young
Ha 2010. Using an Error-Annotated Learner
Corpus to Develop an ESL/EFL Error Correction
System, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC 2010), 763?770, Malta
127
