Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168?1179,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning Sentential Paraphrases from Bilingual Parallel Corpora
for Text-to-Text Generation
Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
Previous work has shown that high quality
phrasal paraphrases can be extracted from
bilingual parallel corpora. However, it is not
clear whether bitexts are an appropriate re-
source for extracting more sophisticated sen-
tential paraphrases, which are more obviously
learnable from monolingual parallel corpora.
We extend bilingual paraphrase extraction to
syntactic paraphrases and demonstrate its abil-
ity to learn a variety of general paraphrastic
transformations, including passivization, da-
tive shift, and topicalization. We discuss how
our model can be adapted to many text gener-
ation tasks by augmenting its feature set, de-
velopment data, and parameter estimation rou-
tine. We illustrate this adaptation by using
our paraphrase model for the task of sentence
compression and achieve results competitive
with state-of-the-art compression systems.
1 Introduction
Paraphrases are alternative ways of expressing the
same information (Culicover, 1968). Automatically
generating and detecting paraphrases is a crucial as-
pect of many NLP tasks. In multi-document sum-
marization, paraphrase detection is used to collapse
redundancies (Barzilay et al, 1999; Barzilay, 2003).
Paraphrase generation can be used for query expan-
sion in information retrieval and question answer-
ing systems (McKeown, 1979; Anick and Tipirneni,
1999; Ravichandran and Hovy, 2002; Riezler et al,
2007). Paraphrases allow for more flexible matching
of system output against human references for tasks
like machine translation and automatic summariza-
tion (Zhou et al, 2006; Kauchak and Barzilay, 2006;
Madnani et al, 2007; Snover et al, 2010).
Broadly, we can distinguish two forms of para-
phrases: phrasal paraphrases denote a set of surface
text forms with the same meaning:
the committee?s second proposal
the second proposal of the committee
while syntactic paraphrases augment the surface
forms by introducing nonterminals (or slots) that are
annotated with syntactic constraints:
the NP1?s NP2
the NP2 of the NP1
It is evident that the latter have a much higher poten-
tial for generalization and for capturing interesting
paraphrastic transformations.
A variety of different types of corpora (and se-
mantic equivalence cues) have been used to auto-
matically induce paraphrase collections for English
(Madnani and Dorr, 2010). Perhaps the most nat-
ural type of corpus for this task is a monolingual
parallel text, which allows sentential paraphrases to
be extracted since the sentence pairs in such corpora
are perfect paraphrases of each other (Barzilay and
McKeown, 2001; Pang et al, 2003). While rich syn-
tactic paraphrases have been learned from monolin-
gual parallel corpora, they suffer from very limited
data availability and thus have poor coverage.
Other methods obtain paraphrases from raw
monolingual text by relying on distributional simi-
larity (Lin and Pantel, 2001; Bhagat and Ravichan-
dran, 2008). While vast amounts of data are
readily available for these approaches, the distri-
butional similarity signal they use is noisier than
the sentence-level correspondency in parallel cor-
pora and additionally suffers from problems such as
mistaking cousin expressions or antonyms (such as
{boy , girl} or {rise, fall}) for paraphrases.
1168
Abundantly available bilingual parallel corpora
have been shown to address both these issues, ob-
taining paraphrases via a pivoting step over foreign
language phrases (Bannard and Callison-Burch,
2005). The coverage of paraphrase lexica extracted
from bitexts has been shown to outperform that
obtained from other sources (Zhao et al, 2008a).
While there have been efforts pursuing the extrac-
tion of more powerful paraphrases (Madnani et
al., 2007; Callison-Burch, 2008; Cohn and Lapata,
2008; Zhao et al, 2008b), it is not yet clear to what
extent sentential paraphrases can be induced from
bitexts. In this paper we:
? Extend the bilingual pivoting approach to para-
phrase induction to produce rich syntactic para-
phrases.
? Perform a thorough analysis of the types of
paraphrases we obtain and discuss the para-
phrastic transformations we are capable of cap-
turing.
? Describe how training paradigms for syntac-
tic/sentential paraphrase models should be tai-
lored to different text-to-text generation tasks.
? Demonstrate our framework?s suitability for a
variety of text-to-text generation tasks by ob-
taining state-of-the-art results on the example
task of sentence compression.
2 Related Work
Madnani and Dorr (2010) survey a variety of data-
driven paraphrasing techniques, categorizing them
based on the type of data that they use. These
include large monolingual texts (Lin and Pantel,
2001; Szpektor et al, 2004; Bhagat and Ravichan-
dran, 2008), comparable corpora (Barzilay and Lee,
2003; Dolan et al, 2004), monolingual parallel cor-
pora (Barzilay and McKeown, 2001; Pang et al,
2003), and bilingual parallel corpora (Bannard and
Callison-Burch, 2005; Madnani et al, 2007; Zhao et
al., 2008b). We focus on the latter type of data.
Paraphrase extraction using bilingual parallel cor-
pora was proposed by Bannard and Callison-Burch
(2005) who induced paraphrases using techniques
from phrase-based statistical machine translation
(Koehn et al, 2003). After extracting a bilingual
phrase table, English paraphrases are obtained by
pivoting through foreign language phrases. Since
many paraphrases can be extracted for a phrase,
Bannard and Callison-Burch rank them using a para-
phrase probability defined in terms of the translation
model probabilities p(f |e) and p(e|f):
p(e2|e1) =
?
f
p(e2, f |e1) (1)
=
?
f
p(e2|f, e1)p(f |e1) (2)
?
?
f
p(e2|f)p(f |e1). (3)
Several subsequent efforts extended the bilin-
gual pivoting technique, many of which introduced
elements of more contemporary syntax-based ap-
proaches to statistical machine translation. Mad-
nani et al (2007) extended the technique to hier-
archical phrase-based machine translation (Chiang,
2005), which is formally a synchronous context-free
grammar (SCFG) and thus can be thought of as a
paraphrase grammar. The paraphrase grammar can
paraphrase (or ?decode?) input sentences using an
SCFG decoder, like the Hiero, Joshua or cdec MT
systems (Chiang, 2007; Li et al, 2009; Dyer et al,
2010). Like Hiero, Madnani?s model uses just one
nonterminal X instead of linguistic nonterminals.
Three additional efforts incorporated linguistic
syntax. Callison-Burch (2008) introduced syntac-
tic constraints by labeling all phrases and para-
phrases (even non-constituent phrases) with CCG-
inspired slash categories (Steedman and Baldridge,
2011), an approach similar to Zollmann and Venu-
gopal (2006)?s syntax-augmented machine transla-
tion (SAMT). Callison-Burch did not formally de-
fine a synchronous grammar, nor discuss decoding,
since his presentation did not include hierarchical
rules. Cohn and Lapata (2008) used the GHKM
extraction method (Galley et al, 2004), which is
limited to constituent phrases and thus produces
a reasonably small set of syntactic rules. Zhao
et al (2008b) added slots to bilingually extracted
paraphrase patterns that were labeled with part-of-
speech tags, but not larger syntactic constituents.
Before the shift to statistical natural language pro-
cessing, paraphrasing was often treated as syntactic
transformations or by parsing and then generating
1169
from a semantic representation (McKeown, 1979;
Muraki, 1982; Meteer and Shaked, 1988; Shem-
tov, 1996; Yamamoto, 2002). Indeed, some work
generated paraphrases using (non-probabilistic) syn-
chronous grammars (Shieber and Schabes, 1990;
Dras, 1997; Dras, 1999; Kozlowski et al, 2003).
After the rise of statistical machine translation, a
number of its techniques were repurposed for para-
phrasing. These include sentence alignment (Gale
and Church, 1993; Barzilay and Elhadad, 2003),
word alignment and noisy channel decoding (Brown
et al, 1990; Quirk et al, 2004), phrase-based models
(Koehn et al, 2003; Bannard and Callison-Burch,
2005), hierarchical phrase-based models (Chiang,
2005; Madnani et al, 2007), log-linear models and
minimum error rate training (Och, 2003a; Madnani
et al, 2007; Zhao et al, 2008a), and here syntax-
based machine translation (Wu, 1997; Yamada and
Knight, 2001; Melamed, 2004; Quirk et al, 2005).
Beyond cementing the ties between paraphrasing
and syntax-based statistical machine translation, the
novel contributions of our paper are (1) an in-depth
analysis of the types of structural and sentential
paraphrases that can be extracted with bilingual piv-
oting, (2) a discussion of how our English?English
paraphrase grammar should be adapted to specific
text-to-text generation tasks (Zhao et al, 2009) with
(3) a concrete example of the adaptation procedure
for the task of paraphrase-based sentence compres-
sion (Knight and Marcu, 2002; Cohn and Lapata,
2008; Cohn and Lapata, 2009).
3 SCFGs in Translation
The model we use in our paraphrasing approach is
a syntactically informed synchronous context-free
grammar (SCFG). The SCFG formalism (Aho and
Ullman, 1972) was repopularized for statistical ma-
chine translation by Chiang (2005). Formally, a
probabilistic SCFG G is defined by specifying
G = ?N , TS , TT ,R, S?,
whereN is a set of nonterminal symbols, TS and TT
are the source and target language vocabularies, R
is a set of rules and S ? N is the root symbol. The
rules inR take the form:
C ? ??, ?,?, w?,
PP/NN ? mit einer  |  with a
NP ? das leck  |  the leak
VP ?  NP PP/NN detonation zu schliessen  |  closing NP PP/NN blast 
they
VP
VP
PRP VBD NNDTNN
NP NPNP
closing tried the   
S
sie versuchten das zu schliessen
leak
leck
with a   blast
DT IN
PP
VBG
einermit detonation
Figure 1: Synchronous grammar rules for translation are
extracted from sentence pairs in a bixtext which have
been automatically parsed and word-aligned. Extraction
methods vary on whether they extract only minimal rules
for phrases dominated by nodes in the parse tree, or more
complex rules that include non-constituent phrases.
where the rule?s left-hand side C ? N is a nonter-
minal, ? ? (N?TS)? and ? ? (N?TT )? are strings
of terminal and nonterminal symbols with an equal
number of nonterminals cNT (?) = cNT (?) and
?: {1 . . . cNT (?)} ? {1 . . . cNT (?)}
constitutes a one-to-one correspondency function
between the nonterminals in ? and ?. A non-
negative weight w ? 0 is assigned to each rule, re-
flecting the likelihood of the rule.
Rule Extraction Phrase-based approaches to sta-
tistical machine translation (and their successors)
extract pairs of (e, f) phrases from automatically
word-aligned parallel sentences. Och (2003b)
described various heuristics for extracting phrase
alignments from the Viterbi word-level alignments
that are estimated using Brown et al (1993) word-
alignment models.
These phrase extraction heuristics have been ex-
tended so that they extract synchronous grammar
rules (Galley et al, 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al, 2006). Most of
these extraction methods require that one side of the
parallel corpus be parsed. This is typically done au-
tomatically with a statistical parser.
Figure 1 shows examples of rules obtained from
a sentence pair. To extract a rule, we first choose a
source side span f like das leck. Then we use phrase
extraction techniques to find target spans e that are
consistent with the word alignment (in this case the
1170
leak is consistent with our f ). The nonterminal sym-
bol that is the left-hand side of the SCFG rule is then
determined by the syntactic constituent that domi-
nates e (in this case NP). To introduce nonterminals
into the right-hand side of the rule, we can apply
rules extracted over sub-phrases of f , synchronously
substituting the corresponding nonterminal symbol
for the sub-phrases on both sides. The synchronous
substitution applied to f and e then yields the corre-
spondency ?.
One significant differentiating factor between the
competing ways of extracting SCFG rules is whether
the extraction method generates rules only for con-
stituent phrases that are dominated by a node in
the parse tree (Galley et al, 2004; Cohn and
Lapata, 2008) or whether they include arbitrary
phrases, including non-constituent phrases (Zoll-
mann and Venugopal, 2006; Callison-Burch, 2008).
We adopt the extraction for all phrases, including
non-constituents, since it allows us to cover a much
greater set of phrases, both in translation and para-
phrasing.
Feature Functions Rather than assigning a single
weight w, we define a set of feature functions ~? =
{?1...?N} that are combined in a log-linear model:
w = ?
N?
i=1
?i log?i. (4)
The weights ~? of these feature functions are set to
maximize some objective function like BLEU (Pap-
ineni et al, 2002) using a procedure called minimum
error rate training (MERT), owing to Och (2003a).
MERT iteratively adjusts the weights until the de-
coder produces output that best matches reference
translations in a development set, according to the
objective function. We will examine appropriate ob-
jective functions for text-to-text generation tasks in
Section 6.2.
Typical features used in statistical machine trans-
lation include phrase translation probabilities (cal-
culated using maximum likelihood estimation over
all phrase pairs enumerable in the parallel cor-
pus), word-for-word lexical translation probabili-
ties (which help to smooth sparser phrase transla-
tion estimates), a ?rule application penalty? (which
governs whether the system prefers fewer longer
they can not be dangerous to the rest of the village
VP/PP
VB+JJ
S
NP
NP/NN
sie k?nnengef?hrlich werdennichtdem rest des dorfes
VP/PP
VB+JJ
S
NP
NP/NN
NP/NN ? dem rest des  |   the rest of the
NP ? NP/NN dorfes  |  NP/NN village
VP/PP ? nicht VB+JJ k?nnen  |  can not VB+JJ
VB+JJ ? gef?hrlich werden  |  be dangerous
S ? sie NP VP/PP  |  they VP/PP to NP
Figure 2: An example derivation produced by a syntactic
machine translation system. Although the synchronous
trees are unlike the derivations found in the Penn Tree-
bank, their yield is a good translation of the German.
phrases or a greater number of shorter phrases), and
a language model probability.
Decoding Given an SCFG and an input source
sentence, the decoder performs a search for the sin-
gle most probable derivation via the CKY algorithm.
In principle the best translation should be the En-
glish sentence e that is the most probable after sum-
ming over all d ? D derivations, since many deriva-
tions yield the same e. In practice, we use a Viterbi
approximation and return the translation that is the
yield of the single best derivation:
e? = arg max
e?Trans(f)
?
d?D(e,f)
p(d, e|f)
? yield(arg max
d?D(e,f)
p(d, e|f)). (5)
Derivations are simply successive applications of the
SCFG rules such as those given in Figure 2.
4 SCFGs in Paraphrasing
Rule Extraction To create a paraphrase grammar
from a translation grammar, we extend the syntac-
tically informed pivot approach of Callison-Burch
(2008) to the SCFG model. For this purpose, we
assume a grammar that translates from a given for-
eign language to English. For each pair of trans-
lation rules where the left-hand side C and foreign
1171
string ? match:
C ? ??, ?1,?1, ~?1?
C ? ??, ?2,?2, ~?2?,
we create a paraphrase rule:
C ? ??1, ?2,?, ~??,
where the nonterminal correspondency relation ?
has been set to reflect the combined nonterminal
alignment:
? = ??11 ? ?2 .
Feature Functions In the computation of the fea-
tures ~? from ~?1 and ~?2 we follow the approximation
in Equation 3, which yields lexical and phrasal para-
phrase probability features. Additionally, we add a
boolean indicator for whether the rule is an iden-
tity paraphrase, ?identity . Another indicator feature,
?reorder , fires if the rule swaps the order of two non-
terminals, which enables us to promote more com-
plex paraphrases that require structural reordering.
Decoding With this, paraphrasing becomes an
English-to-English translation problem which can
be formulated similarly to Equation 5 as:
e?2 ? yield(arg max
d?D(e2,e1)
p(d, e2|e1)).
Figure 3 shows an example derivation produced as a
result of applying our paraphrase rules in the decod-
ing process. Another advantage of using the decoder
from statistical machine translation is that n-gram
language models, which have been shown to be use-
ful in natural language generation (Langkilde and
Knight, 1998), are already well integrated (Huang
and Chiang, 2007).
5 Analysis
A key motivation for the use of syntactic paraphrases
over their phrasal counterparts is their potential to
capture meaning-preserving linguistic transforma-
tions in a more general fashion. A phrasal system is
limited to memorizing fully lexicalized transforma-
tions in its paraphrase table, resulting in poor gener-
alization capabilities. By contrast, a syntactic para-
phrasing system intuitively should be able to address
this issue and learn well-formed and generic patterns
that can be easily applied to unseen data.
twelve cartoons insulting the
prophet
mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
12 the prophet mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
cartoons offensive
Foreign Pivot PhraseParaphrase Rule
JJ ? offensive  |   insulting
Lexical paraphrase:
NP ? NP that VP  |  NP VP
Reduced relative clause:
NP ? CD of the NNS  |  CD NNS
Partitive construction: 
VP ? are JJ to NP  |  JJ NP
Pred. adjective copula deletion:
JJ -> beleidigend  |  offensive
JJ -> beleidigend  |  insulting
NP -> NP die VP  |  NP VP
NP -> NP die VP  |  NP that VP
NP -> CD der NNS  |  CD of the NNS
NP -> CD der NNS  |  CD NNS
VP ? sind JJ f?r NP  |  are JJ to NP
VP ? sind JJ f?r NP  |  JJ NP
of the that are to
Figure 3: An example of a synchronous paraphrastic
derivation. A few of the rules applied in the parse are
show in the left column, with the pivot phrases that gave
rise to them on the right.
To put this expectation to the test, we investigate
how our grammar captures a number of well-known
paraphrastic transformations.1 Table 1 shows the
transformations along with examples of the generic
grammar rules our system learns to represent them.
When given a transformation to extract a syntactic
paraphrase for, we want to find rules that neither
under- nor over-generalize. This means that, while
replacing the maximum number of syntactic argu-
ments with nonterminals, the rules ideally will both
retain enough lexicalization to serve as sufficient ev-
idence for the applicability of the transformation and
impose constraints on the nonterminals to ensure the
arguments? well-formedness.
The paraphrases implementing the possessive rule
and the dative shift shown in Table 1 are a good
examples of this: the two noun-phrase arguments
to the expressions are abstracted to nonterminals
while each rule?s lexicalization provides an appro-
priate frame of evidence for the transform. This is
important for a good representation of dative shift,
which is a reordering transformation that fully ap-
plies to certain ditransitive verbs while other verbs
are uncommon in one of the forms:
1The data and software used to extract the grammar we draw
these examples from is described in Section 6.5.
1172
Possessive rule NP ? the NN of the NNP | the NNP ?s NNNP ? the NNS 1 made by NNS 2 | the NNS 2?s NNS 1
Dative shift VP ? give NN to NP | give NP the NNVP ? provide NP1 to NP2 | give NP2 NP1
Adv./adj. phrase move S/VP ? ADVP they VBP | they VPB ADVPS ? it is ADJP VP | VP is ADJP
Verb particle shift VP ? VB NP up | VB up NP
Reduced relative clause SBAR/S ? although PRP VBP that | although PRP VBPADJP ? very JJ that S | JJ S
Partitive constructions NP ? CD of the NN | CD NNNP ? all DT\NP | all of the DT\NP
Topicalization S ? NP , VP . | VP , NP .
Passivization SBAR? that NP had VBN | which was VBN by NP
Light verbs VP ? take action ADVP | to act ADVPVP ? TO take a decision PP | TO decide PP
Table 1: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that
our system extracts capturing these.
give decontamination equipment to Japan
give Japan decontamination equipment
provide decontamination equipment to Japan
? provide Japan decontamination equipment
Note how our system extracts a dative shift rule for
to give and a rule that both shifts and substitutes a
more appropriate verb for to provide.
The use of syntactic nonterminals in our para-
phrase rules to capture complex transforms also
makes it possible to impose constraints on their ap-
plication. For comparison, as Madnani et al (2007)
do not impose any constraints on how the nontermi-
nal X can be realized, their equivalent of the topi-
calization rule would massively overgeneralize:
S ? X1, X2 . | X2, X1 .
Additional examples of transforms our use of syn-
tax allows us to capture are the adverbial phrase
shift and the reduction of a relative clause, as well
as other phenomena listed in Table 1.
Unsurprisingly, syntactic information alone is not
sufficient to capture all transformations. For in-
stance it is hard to extract generic paraphrases for all
instances of passivization, since our syntactic model
currently has no means of representing the morpho-
logical changes that the verb undergoes:
the reactor leaks radiation
radiation is leaking from the reactor .
Still, for cases where the verb?s morphology does
not change, we manage to learn a rule:
the radiation that the reactor had leaked
the radiation which leaked from the reactor .
Another example of a deficiency in our synchronous
grammar models are light verb constructs such as:
to take a walk
to walk .
Here, a noun is transformed into the corresponding
verb ? something our synchronous syntactic CFGs
are not able to capture except through memorization.
Our survey shows that we are able to extract ap-
propriately generic representations for a wide range
of paraphrastic transformations. This is a surpris-
ing result which shows that bilingual parallel cor-
pora can be used to learn sentential paraphrases, and
that they are a viable alternative to other data sources
like monolingual parallel corpora, which more obvi-
ously contain sentential paraphrases, but are scarce.
6 Text-to-Text Applications
The core of many text-to-text generation tasks is
sentential paraphrasing, augmented with specific
constraints or goals. Since our model borrows much
of its machinery from statistical machine translation
? a sentential rewriting problem itself ? it is straight-
forward to use our paraphrase grammars to generate
new sentences using SMT?s decoding and param-
eter optimization techniques. Our framework can
be adapted to many different text-to-text generation
tasks. These could include text simplification, sen-
1173
tence compression, poetry generation, query expan-
sion, transforming declarative sentences into ques-
tions, and deriving hypotheses for textual entail-
ment. Each individual text-to-text application re-
quires that our framework be adapted in several
ways, by specifying:
? A mechanism for extracting synchronous
grammar rules (in this paper we argue that
pivot-based paraphrasing is widely applicable).
? An appropriate set of rule-level features that
capture information pertinent to the task (e.g.
whether a rule simplifies a phrase).
? An appropriate ?objective function? that scores
the output of the model, i.e. a task-specific
equivalent to the BLEU metric in SMT.
? A development set with examples of the sen-
tential transformations that we are modeling.
? Optionally, a way of injecting task-specific
rules that were not extracted automatically.
In the remainder of this section, we illustrate how
our bilingually extracted paraphrases can be adapted
to perform sentence compression, which is the task
of reducing the length of sentence while preserving
its core meaning. Most previous approaches to sen-
tence compression focused only on the deletion of
a subset of words from the sentence (Knight and
Marcu, 2002). Our approach follows Cohn and La-
pata (2008), who expand the task to include substi-
tutions, insertions and reorderings that are automat-
ically learned from parallel texts.
6.1 Feature Design
In Section 4 we discussed phrasal probabilities.
While these help quantify how good a paraphrase
is in general, they do not make any statement on
task-specific things such as the change in language
complexity or text length. To make this information
available to the decoder, we enhance our paraphrases
with four compression-targeted features. We add the
count features csrc and ctgt , indicating the number of
words on either side of the rule as well as two differ-
ence features: cdcount = ctgt ? csrc and the anal-
ogously computed difference in the average word
length in characters, cdavg .
6.2 Objective Function
Given our paraphrasing system?s connection to
SMT, the naive/obvious choice for parameter op-
timization would be to optimize for BLEU over a
set of paraphrases, for instance parallel English ref-
erence translations for a machine translation task
(Madnani et al, 2007). For a candidate C and a ref-
erence R, (with lengths c and r) BLEU is defined as:
BLEUN (C,R)
=
{
e(1?c/r) ? e
?N
n=1 logwnpn if c/r ? 1
e
?N
n=1 logwnpn otherwise ,
where pn is the modified n-gram precision of C
against R, with typically N = 4 and wn = 1N . The
?brevity penalty? term e(1?c/r) is added to prevent
short candidates from achieving perfect scores.
Naively optimizing for BLEU, however, will re-
sult in a trivial paraphrasing system heavily biased
towards producing identity ?paraphrases?. This is
obviously not what we are looking for. Moreover,
BLEU does not provide a mechanism for directly
specifying a per-sentence compression rate, which
is desirable for the compression task.
Instead, we propose PRE?CIS, an objective func-
tion tailored to the text compression task:
PRE?CIS?,?(I, C,R)
=
{
e?(??c/i) ? BLEU(C,R) if c/i ? ?
BLEU(C,R) otherwise
For an input sentence I , an output C and ref-
erence compression R (with lengths i, c and r),
PRE?CIS combines the precision estimate of BLEU
with an additional ?verbosity penalty? that is ap-
plied to compressions that fail to meet a given target
compression rate ?. We rely on the BLEU brevity
penalty to prevent the system from producing overly
aggressive compressions. The scaling term ? deter-
mines how severely we penalize deviations from ?.
In our experiments we use ? = 10.
It is straightforward to find similar adaptations for
other tasks. For text simplification, for instance, the
penalty term can include a readability metric. For
poetry generation we can analogously penalize out-
puts that break the meter (Greene et al, 2010).
6.3 Development Data
To tune the parameters of our paraphrase system for
sentence compression, we need an appropriate cor-
1174
pus of reference compressions. Since our model is
designed to compress by paraphrasing rather than
deletion, the commonly used deletion-based com-
pression data sets like the Ziff-Davis corpus are not
suitable. We have thus created a corpus of com-
pression paraphrases. Beginning with 9570 tuples
of parallel English?English sentences obtained from
multiple reference translations for machine transla-
tion evaluation, we construct a parallel compression
corpus by selecting the longest reference in each tu-
ple as the source sentence and the shortest reference
as the target sentence. We further retain only those
sentence pairs where the compression rate cr falls in
the range 0.5 < cr ? 0.8. From these, we randomly
select 936 sentences for the development set, as well
as 560 sentences for a test set that we use to gauge
the performance of our system.
6.4 Grammar Augmentations
As we discussed in Section 5, the paraphrase gram-
mar we induce is capable of representing a wide va-
riety of transformations. However, the formalism
and extraction method are not explicitly geared to-
wards a compression application. For instance, the
synchronous nature of our grammar does not allow
us to perform deletions of constituents as done by
Cohn and Lapata (2007)?s tree transducers. One way
to extend the grammar?s capabilities towards the re-
quirements of a given task is by injecting additional
rules designed to capture appropriate operations.
For the compression task, this could include
adding rules to delete target-side nonterminals:
JJ ? JJ | ?
This would render the grammar asynchronous and
require adjustments to the decoding process. Al-
ternatively, we can generate rules that specifically
delete particular adjectives from the corpus:
JJ ? superfluous | ? .
In our experiments we evaluate the latter approach
by generating optional deletion rules for all adjec-
tives, adverbs and determiners.
6.5 Experimental Setup
We extracted a paraphrase grammar from the
French?English Europarl corpus (v5). The bitext
was aligned using the Berkeley aligner and the En-
glish side was parsed with the Berkeley parser. We
Grammar # Rules
total 42,353,318
w/o identity 23,641,016
w/o complex constituents 6,439,923
w/o complex const. & identity 5,097,250
Table 2: Number and distribution of rules in our para-
phrase grammar. Note the significant number of identity
paraphrases and rules with complex nonterminal labels.
obtained the initial translation grammar using the
SAMT toolkit (Venugopal and Zollmann, 2009).
The grammars we extract tend to be extremely
large. To keep their size manageable, we only con-
sider translation rules that have been seen more than
3 times and whose translation probability exceeds
10?4 for pivot recombination. Additionally, we only
retain the top 25 most likely paraphrases of each
phrase, ranked by a uniformly weighted combina-
tion of phrasal and lexical paraphrase probabilities.
We tuned the model parameters to our PRE?CIS
objective function, implemented in the Z-MERT
toolkit (Zaidan, 2009). For decoding we used the
Joshua decoder (Li et al, 2010). The language
model used in our paraphraser and the Clarke and
Lapata (2008) baseline system is a Kneser-Ney dis-
counted 5-gram model estimated on the Gigaword
corpus using the SRILM toolkit (Stolcke, 2002).
6.6 Evaluation
To assess the output quality of the resulting sentence
compression system, we compare it to two state-of-
the-art sentence compression systems. Specifically,
we compare against our implementation of Clarke
and Lapata (2008)?s compression model which uses
a series of constraints in an integer linear program-
ming (ILP) solver, and Cohn and Lapata (2007)?s
tree transducer toolkit (T3) which learns a syn-
chronous tree substitution grammar (STSG) from
paired monolingual sentences. Unlike SCFGs, the
STSG formalism allows changes to the tree topol-
ogy. Cohn and Lapata argue that this is a natural
fit for sentence compression, since deletions intro-
duce structural mismatches. We trained the T3 soft-
ware2 on the 936 ?full, compressed? sentence pairs
that comprise our development set. This is equiva-
lent in size to the training corpora that Cohn and La-
pata (2007) used (their training corpora ranged from
2www.dcs.shef.ac.uk/people/T.Cohn/t3/
1175
882?1020 sentence pairs), and has the advantage of
being in-domain with respect to our test set. Both
these systems reported results outperforming previ-
ous systems such as McDonald (2006). To showcase
the value of the adaptations discussed above, we also
compare variants of our paraphrase-based compres-
sion systems: using Hiero instead of syntax, using
syntax with or without compression features, using
an augmented grammar with optional deletion rules.
We solicit human judgments of the compres-
sions along two five-point scales: grammaticality
and meaning. Judges are instructed to decide how
much the meaning from a reference translation is
retained in the compressed sentence, with a score
of 5 indicating that all of the important information
is present, and 1 being that the compression does
not retain any of the original meaning. Similarly, a
grammar score of 5 indicates perfect grammaticality,
and a grammar score of 1 is assigned to sentences
that are entirely ungrammatical. To ensure fairness,
we perform pairwise system comparisons with com-
pression rates strictly tied on the sentence-level. For
any comparison, a sentence is only included in the
computation of average scores if the difference be-
tween both systems? compression rates is < 0.05.3
Table 4 shows a set of pairwise comparisons for
compression rates ? 0.5. We see that going from
a Hiero-based to a syntactic paraphrase grammar
yields a significant improvement in grammatical-
ity. Adding compression-specific features improves
grammaticality even further. Further augmenting the
grammar with deletion rules significantly helps re-
tain the core meaning at compression rates this high,
however compared to the un-augmented syntactic
system grammaticality scores drop. While our ap-
proach significantly outperforms the T3 system, we
are not able to match ILP?s results in grammaticality.
In Table 3 we compare our system to the ILP ap-
proach at a modest compression rate of? 0.8. Here,
we significantly outperform ILP in meaning reten-
tion while achieving comparable results in gram-
maticality. This improvement is significant at p <
0.0001, using the sign test, while the better gram-
maticality score of the ILP system is not statisti-
3Because evaluation quality correlates linearly with com-
pression rate, the community-accepted practice of not compar-
ing based on a closely tied compression rate is potentially sub-
ject to erroneous interpretation (Napoles et al, 2011).
CR Meaning Grammar
Reference 0.73 4.26 4.35
Syntax+Feat. 0.80 3.67 3.38
ILP 0.80 3.50 3.49
Random Deletions 0.50 1.94 1.57
Table 3: Results of the human evaluation on longer com-
pressions: pairwise compression rates (CR), meaning and
grammaticality scores. Bold indicates a statistically sig-
nificance difference at p < 0.05.
CR Meaning Grammar
Hiero 0.56 2.57 2.35
Syntax 0.56 2.76 2.67
Syntax 0.53 2.70 2.49
Syntax+Feat. 0.53 2.71 2.54
Syntax+Feat. 0.54 2.79 2.71
Syntax+Aug. 0.54 2.96 2.52
Syntax+Aug. 0.52 2.87 2.40
ILP 0.52 2.83 3.09
Syntax+Aug. 0.50 2.41 2.20
T3 0.50 2.01 1.93
Table 4: Human evaluation for shorter compressions and
for variations of our paraphrase system. +Feat. includes
the compression features from Section 6.1, +Aug. in-
cludes optional deletion rules from Section 6.4.
cally significant (p < 0.088). These results indi-
cate that, over a variety of compression rates, our
framework for text-to-text generation is performing
as well as or better than specifically tailored state-
of-the-art methods.
Table 5 shows an example sentence drawn from
our test set and the compressions produced by the
different systems. We see that both the paraphrase
and ILP systems produce good quality results, with
the paraphrase system retaining the meaning of the
source sentence more accurately.
7 Conclusion
In this work we introduced a method to learn syntac-
tically informed paraphrases from bilingual parallel
texts. We discussed the expressive power and limita-
tions of our formalism and outlined straightforward
adaptation strategies for applications in text-to-text
generation. We demonstrated when our paraphras-
ing system was adapted to do sentence compression,
it achieved results competitive with state-of-the-art
compression systems with only minimal effort.
1176
Source he also expected that he would have a role in the future at the level of the islamic movementacross the palestinian territories , even if he was not lucky enough to win in the elections .
Reference he expects to have a future role in the islamic movement in the palestinian territories if he isnot successful in the elections .
Syntax+Feat. he also expected that he would have a role in the future of the islamic movement in thepalestinian territories , although he was not lucky enough to win elections .
ILP he also expected that he would have a role at the level of the islamic movement , even if hewas not lucky enough to win in the elections .
Source in this war which has carried on for the last 12 days , around 700 palestinians , which includea large number of women and children , have died .
Reference about 700 palestinians , mostly women and children , have been killed in the israeli offensiveover the last 12 days .
Syntax+Feat. in this war has done for the last 12 days , around 700 palestinians , including women andchildren , died .
ILP in this war which has carried for the days palestinians , which include a number of womenand children died .
Source hala speaks arabic most of the time with her son , taking into consideration that he can speakenglish with others .
Reference hala speaks to her son mostly in arabic , as he can speak english to others .
Syntax+Feat. hala speaks arabic most of the time with her son , considering that he can speak english withothers .
ILP hala speaks arabic most of the time , taking into consideration that he can speak english withothers .
Table 5: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.
Acknowledgments
We would like to thank Trevor Cohn for kindly pro-
viding us with the T3 compression system. This re-
search was supported by the NSF under grant IIS-
0713448. Opinions, interpretations, and conclusions
are the authors? alone.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Peter G. Anick and Suresh Tipirneni. 1999. The para-
phrase search assistant: terminological feedback for
iterative information seeking. In Proceedings of SI-
GIR.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings of
ACL.
Regina Barzilay. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Genera-
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Peter Brown, John Cocke, Stephen Della Pietra, Vincent
Della Pietra, Frederick Jelinek, Robert Mercer, and
1177
Paul Poossin. 1990. A statistical approach to language
translation. Computational Linguistics, 16(2), June.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoLing.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research (JAIR), 34:637?674.
Peter W. Culicover. 1968. Paraphrase generation and
information retrieval from stored text. Mechani-
cal Translation and Computational Linguistics, 11(1-
2):78?88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Mark Dras. 1997. Representing paraphrases using syn-
chronous tree adjoining grammars. In Proceedings of
ACL.
Mark Dras. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text. Ph.D. thesis, Macquarie
University, Australia.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL.
William Gale and Kenneth Church. 1993. A program
for aligning sentences in bilingual corpora. Compu-
atational Linguistics, 19(1):75?90.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010.
Automatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceedings of
EMNLP.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
EMNLP.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Raymond Kozlowski, Kathleen McCoy, and K. Vijay-
Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure using
lexico-grammatical resources. In Workshop On Para-
phrasing.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Workshop On
Natural Language Generation, Ontario, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of WMT09.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of WMT10.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of WMT07.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
1178
Kathleen R. McKeown. 1979. Paraphrasing using given
and new information in a question-answer system. In
Proceedings of ACL.
Dan Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
Marie W. Meteer and Varda Shaked. 1988. Strategies for
effective paraphrasing. In Proceedings of COLING.
Kazunori Muraki. 1982. On a semantic model for multi-
lingual paraphrasing. In Proceedings of COLING.
Courtney Napoles, Chris Callison-Burch, and Ben-
jamin Van Durme. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Work-
shop on Monolingual Text-To-Text Generation.
Franz Josef Och. 2003a. Minimum error rate training for
statistical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003b. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual bitext-derived
paraphrases in automatic MT evaluation. In Proceed-
ings of WMT06.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
sufrace text patterns for a question answering system.
In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Hadar Shemtov. 1996. Generation of paraphrases from
ambiguous logical forms. In Proceedings of COLING.
Stuart Shieber and Yves Schabes. 1990. Generation and
synchronous tree-adjoining grammars. In Workshop
On Natural Language Generation.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Non-Transformational
Syntax: Formal and Explicit Models of Grammar.
Wiley-Blackwell.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceeding of the International
Conference on Spoken Language Processing.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP, Pro-
ceedings of EMNLP.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. Prague Bul-
letin of Mathematical Linguistics, 91.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL.
Kazuhide Yamamoto. 2002. Machine translation by in-
teraction between paraphraser and transfer. In Pro-
ceedings of COLING.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of WMT06.
1179
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 42?50,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Simple Wikipedia:
A Cogitation in Ascertaining Abecedarian Language
Courtney Napoles and Mark Dredze
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
courtneyn@jhu.edu, mdredze@cs.jhu.edu
Abstract
Text simplification is the process of changing
vocabulary and grammatical structure to cre-
ate a more accessible version of the text while
maintaining the underlying information and
content. Automated tools for text simplifica-
tion are a practical way to make large corpora
of text accessible to a wider audience lacking
high levels of fluency in the corpus language.
In this work, we investigate the potential of
Simple Wikipedia to assist automatic text sim-
plification by building a statistical classifica-
tion system that discriminates simple English
from ordinary English. Most text simplifica-
tion systems are based on hand-written rules
(e.g., PEST (Carroll et al, 1999) and its mod-
ule SYSTAR (Canning et al, 2000)), and
therefore face limitations scaling and trans-
ferring across domains. The potential for us-
ing Simple Wikipedia for text simplification
is significant; it contains nearly 60,000 ar-
ticles with revision histories and aligned ar-
ticles to ordinary English Wikipedia. Us-
ing articles from Simple Wikipedia and ordi-
nary Wikipedia, we evaluated different classi-
fiers and feature sets to identify the most dis-
criminative features of simple English for use
across domains. These findings help further
understanding of what makes text simple and
can be applied as a tool to help writers craft
simple text.
1 Introduction
The availability of large collections of electronic
texts is a boon to information seekers, however, ad-
vanced texts often require fluency in the language.
Text simplification (TS) is an emerging area of text-
to-text generation that focuses on increasing the
readability of a given text. Potential applications
can increase the accessibility of text, which has great
value in education, public health, and safety, and can
aid natural language processing tasks such as ma-
chine translation and text generation.
Corresponding to these applications, TS can be
broken down into two rough categories depending
on the target ?reader.? The first type of TS aims to
increase human readability for people lacking high-
level language skills, either because of age, educa-
tion level, unfamiliarity with the language, or dis-
ability. Historically, generating this text has been
done by hand, which is time consuming and expen-
sive, especially when dealing with material that re-
quires expertise, such as legal documents. Most cur-
rent automatic TS systems rely on handwritten rules,
e.g., PEST (Carroll et al, 1999), its SYSTAR mod-
ule (Canning et al, 2000), and the method described
by Siddharthan (2006). Systems using handwritten
rules can be susceptible to changes in domains and
need to be modified for each new domain or lan-
guage. There has been some research into automat-
ically learning the rules for simplifying text using
aligned corpora (Daelemans et al, 2004; Yatskar et
al., 2010), but these have yet to match the perfor-
mance hand-crafted rule systems. An example of
a manually simplified sentence can be found in ta-
ble 1.
The second type of TS has the goal of increas-
ing the machine readability of text to aid tasks such
as information extraction, machine translation, gen-
erative summarization, and other text generation
42
tasks for selecting and evaluating the best candi-
date output text. In machine translation, the eval-
uation tool most commonly used for evaluating out-
put, the BLEU score (Papineni et al, 2001), rates the
?goodness? of output based on n-gram overlap with
human-generated text. However this metric has been
criticized for not accurately measuring the fluency
of text and there is active research into other met-
rics (Callison-Burch et al, 2006; Ye et al, 2007).
Previous studies suggest that text simplified for ma-
chine and human comprehension are categorically
different (Chae and Nenkova, 2009). Our research
considers text simplified for human readers, but the
findings can be used to identify features that dis-
criminate simple text for both applications.
The process of TS can be divided into three as-
pects: removing extraneous or superfluous text, sub-
stituting more complex lexical and syntactic forms,
and inserting information to offer further clarifica-
tion where needed (Alu??sio et al, 2008). In this re-
gard, TS is related to several different natural lan-
guage processing tasks such as text summarization,
compression, machine translation, and paraphras-
ing.
While none of these tasks alone directly provide
a solution to text simplification, techniques can be
drawn from each. Summarization techniques can
be used to identify the crucial, most informative
parts of a text and compression can be used to re-
move superfluous words and phrases. In fact, in the
Wikipedia documents analyzed for this research, the
average length of a ?simple? document is only 21%
the length of an ?ordinary? English document (al-
though this may be an unintentional byproduct of
how articles were simplified, as discussed in section
6.1).
In this paper we study the properties of language
that differentiate simple from ordinary text for hu-
man readers. Specifically, we use statistical learn-
ing techniques to identify the most discriminative
features of simple English and ?ordinary? English
using articles from Simple Wikipedia and English
Wikipedia. We use cognitively motivated features
as well as statistical measurements of a document?s
lexical, syntactic, and surface features. Our study
demonstrates the validity and potential benefits of
using Simple Wikipedia as a resource for TS re-
search.
Ordinary text
Every person has the right to a name, in which is
included a first name and surname. . . . The alias
chosen for legal activities has the same protection
as given to the name.
Same text in simple language
Every person has the right to have a name, and
the law protects people?s names. Also, the law
protects a person?s alias. . . . The name is made
up of a first name and a surname (name = first
name + surname).
Table 1: A text in ordinary and simple language from
Alu??sio et al (2008).
2 Wikipedia as a Corpus
Wikipedia is a unique resource for natural lan-
guage processing tasks due to its sheer size, acces-
sibility, language diversity, article structure, inter-
document links, and inter-language document align-
ments. Denoyer and Gallinari (2006) introduced
the Wikipedia XML Corpus, with 1.5 million doc-
uments in eight languages from Wikipedia, that
stored the rich structural information of Wikipedia
with XML. This corpus was designed specifically
for XML retrieval but has uses in natural language
processing, categorization, machine translation, en-
tity ranking, etc. YAWN (Schenkel et al, 2007), a
Wikipedia XML corpus with semantic tags, is an-
other example of exploiting Wikipedia?s structural
information. Wikipedia provides XML site dumps
every few weeks in all languages as well as static
HTML dumps.
A diverse array of NLP research in the past
few years has used Wikipedia, such as for word
sense disambiguation (Mihalcea, 2007), classifica-
tion (Gantner and Schmidt-Thieme, 2009), machine
translation (Smith et al, 2010), coreference resolu-
tion (Versley et al, 2008; Yang and Su, 2007), sen-
tence extraction for summarization (Biadsy et al,
2008), information retrieval (Mu?ller and Gurevych,
2008), and semantic role labeling (Ponzetto and
Strube, 2006), to name a few. However, except for
very recent work by Yatskar et al (2010), to our
knowledge there has not been comparable research
in using Wikipedia for text simplification.
43
Ordinary Wikipedia
Hawking was the Lucasian Professor of Mathe-
matics at the University of Cambridge for thirty
years, taking up the post in 1979 and retiring on 1
October 2009.
Simple Wikipedia
Hawking was a professor of mathematics at the
University of Cambridge (a position that Isaac
Newton once had). He retired on October 1st
2009.
Table 2: Comparable sentences from the ordinary
Wikipedia and Simple Wikipedia entry for ?Stephen
Hawking.?
What makes Wikipedia an excellent resource for
text simplification is the new Simple Wikipedia
project1, a collection of 58,000 English Wikipedia
articles that have been rewritten in Simple English,
which uses basic vocabulary and less complex gram-
mar to make the content of Wikipedia accessible to
students, children, adults with learning difficulties,
and non-native English speakers. In addition to be-
ing a large corpus, these articles are linked to their
ordinary Wikipedia counterparts, so for each article
both a simple and an ordinary version are available.
Furthermore, on inspection many articles in Simple
Wikipedia appear to be copied and edited from the
corresponding ordinary Wikipedia article. This in-
formation, together with revision history and flags
signifying unsimplified text, can provide a scale of
information on the text-simplification process previ-
ously unavailable. Example sentences from Simple
Wikipedia and ordinary Wikipedia are shown in ta-
ble 2.
We used articles from Simple Wikipedia and or-
dinary English Wikipedia to create a large cor-
pus of simple and ordinary articles for our exper-
iments. In order to experiment with models that
work across domains, the corpus includes articles
from nine of the primary categories identified in
Simple Wikipedia: Everyday Life, Geography, His-
tory, Knowledge, Literature, Media, People, Reli-
gion, and Science. A total of 55,433 ordinary and
42,973 simple articles were extracted and processed
from English Wikipedia and Simple Wikipedia, re-
1http://simple.wikipedia.org/
Coarse Tag Penn Treebank Tags
DET DT, PDT
ADJ JJ, JJR, JJS
N NN, NNS, NP, NPS, PRP, FW
ADV RB, RBR, RBS
V VB, VBN, VBG, VBP, VBZ, MD
WH WDT, WP, WP$, WRB
Table 3: A mapping of the Penn Treebank tags to a coarse
tagset used to generate features.
spectively. Each document contains at least two sen-
tences. Additionally, the corpus contains only the
main text body of each article and does not con-
sider info boxes, tables, lists, external and cross-
references, and other structural features. The exper-
iments that follow randomly extract documents and
sentences from this collection.
Before extracting features, we ran a series of nat-
ural language processing tools to preprocess the col-
lection. First, all of the XML and ?wiki markup?
was removed. Each document was split into sen-
tences using the Punkt sentence tokenizer (Kiss and
Strunk, 2006) in NLTK (Bird and Loper, 2004). We
then parsed each sentence using the PCFG parser
of Huang and Harper (2009), a modified version
of the Berkeley parser (Petrov et al, 2006; Petrov
and Klein, 2007), for the tree structure and part-of-
speech tags.
3 Task Setup
To evaluate the feasibility of learning simple and or-
dinary texts, we sought to identify text properties
that differentiated between these classes. Using the
two document collections, we constructed a simple
binary classification task: label a piece of text as ei-
ther simple or ordinary. The text was labeled ac-
cording to its source: simple or ordinary Wikipedia.
From each piece of text, we extracted a set of fea-
tures designed to capture differences between the
texts, using cognitively motivated features based on
a document?s lexical, syntactic, and surface features.
We first describe our features and then our experi-
mental setup.
44
4 Features
We began by examining the guidelines for writing
Simple Wikipedia pages.2 These guidelines suggest
that articles use only the 1000 most common and ba-
sic English words and contain simple grammar and
short sentences. Articles should be short but can be
longer if they need to explain vocabulary words nec-
essary to understand the topic. Additionally, words
should appear on lists of basic English words, such
as the Voice of America Special English words list
(Voice Of America, 2009) or the Ogden Basic En-
glish list (Ogden, 1930). Idioms should be avoided
as well as compounds and the passive voice as op-
posed to a single simple verb.
To capture these properties in the text, we created
four classes of features: lexical, part-of-speech, sur-
face, and parse. Several of our features have previ-
ously been used for measuring text fluency (Alu??sio
et al, 2008; Chae and Nenkova, 2009; Feng et al,
2009; Petersen and Ostendorf, 2007).
Lexical. Previous work by Feng et al (2009) sug-
gests that the document vocabulary is a good predic-
tor of document readability. Simple texts are more
likely to use basic words more often as opposed to
more complicated, domain-specific words used in
ordinary texts. To capture these features we used a
unigram bag-of-words representation. We note that
lexical features are unlikely to be useful unless we
have access to a large training corpus that allowed
the estimation of the relative frequency of words
(Chae and Nenkova, 2009). Additionally, we can
expect lexical features to be very fragile for cross-
domain experiments as they are especially suscepti-
ble to changes in domain vocabulary. Nevertheless,
we include these features as a baseline in our exper-
iments.
Parts of speech. A clear focus of the simple text
guidelines is grammar and word type. One way
of representing this information is by measuring
the relative frequency of different types of parts
of speech. We consider simple unigram part-of-
speech tag information. We measured the nor-
malized counts and relative frequency of part-of-
speech tags and counts of bigram part-of-speech tags
2http://simple.wikipedia.org/wiki/Wikipedia:
Simple_English_Wikipedia
Feature Simple Ordinary
Tokens 158 4332
Types 100 1446
Sentences 10 172
Average sentence length 15.80 25.19
Type-token ratio 0.63 0.33
Percent simple words 0.31 0.08
Not BE850 type-token ratio 0.65 0.30
BE850 type-token ratio 0.59 0.67
Table 4: A comparison of the article ?Stephen Hawking?
from Simple and ordinary Wikipedia.
in each piece of text. Since Devlin and Unthank
(2006) has shown that word order (subject verb ob-
ject (SVO), object verb subject (OVS), etc.) is cor-
related with readability, we also included a reduced
tagset to capture grammatical patterns (table 3). We
also included normalized counts of these reduced
tags in the model.
Surface features. While lexical items may be im-
portant, more general properties can be extracted
from the lexical forms. We can also include fea-
tures that correspond to surface information in the
text. These features include document length, sen-
tence length, word length, numbers of lexical types
and tokens, and the ratio of types to tokens. All
words are labeled as basic or not basic according
to Ogden?s Basic English 850 (BE850) list (Ogden,
1930).3 In order to measure the lexical complexity
of a document, we include features for the number
of BE850 words, the ratio of BE850 words to total
words, and the type-token ratio of BE850 and non-
BE850 words. Investigating the frequency and pro-
ductivity of words not in the BE850 list will hope-
fully improve the flexibility of our model to work
across domains and not learn any particular jargon.
We also hope that the relative frequency and pro-
ductivity measures of simple and non-simple words
will codify the lexical choices of a sentence while
avoiding the aforementioned problems with includ-
ing specific lexical items.
3Wikipedia advocates using words that appear on the BE850
list. Ogden also provides extended Basic English vocabulary
lists, totaling 2000 Basic English words, but these words tend
to be more specialized or domain specific. For the purposes of
this study only words in BE850 were used.
45
Table 4 shows the difference in some surface
statistics in an aligned document from Simple and
ordinary Wikipedia. In this example, nearly one-
third of the words in the simple document are from
the BE850 while less than a tenth of the words in the
ordinary document are. Additionally, the productiv-
ity of words, particularly non-BE850 words, is much
higher in the ordinary document. There are also
clear differences in the length of the documents, and
on average documents from ordinary Wikipedia are
more than four times longer than documents from
Simple Wikipedia.
Syntactic parse. As previously mentioned, a
number of Wikipedia?s writing guidelines focus on
general grammatical rules of sentence structure. Ev-
idence of these rules may be captured in the syn-
tactic parse of the sentences in the text. Chae and
Nenkova (2009) studied text fluency in the context
of machine translation and found strong correlations
between parse tree structures and sentence fluency.
In order to represent the structural complexity of
the text, we collected extracted features from the
parse trees. Our features included the frequency and
length of noun phrases, verb phrases, prepositional
phrases, and relative clauses (including embedded
structures). We also considered relative ratios, such
as the ratio of noun to verb phrases, prepositional to
noun phrases, and relative clauses to noun phrases.
We used the length of the longest noun phrase as
a signal of complexity, and we also sought features
that measured how typical the sentences were of En-
glish text. We included some of the features from
the parser reranking work of Charniak and Johnson
(2005): the height of the parse tree and the number
of right branches from the root of the tree to the fur-
thest right leaf that is not punctuation.
5 Experiments
Using the feature sets described above, we evalu-
ated a simple/ordinary text classifier in several set-
tings on each category. First, we considered the task
of document classification, where a classifier deter-
mines whether a full Wikipedia article was from
ordinary English Wikipedia or Simple Wikipedia.
For each category of articles, we measured accu-
racy on this binary classification task using 10-fold
cross-validation. In the second setting, we consid-
Category Documents Sentences
Everyday Life 15,124 7,392
Geography 10,470 5,852
History 5,174 1,644
Literature 992 438
Media 502 429
People 4,326 1,562
Religion 1,863 1,581
Science 25,787 21,054
All 64,238 39,952
Table 5: The number of examples available in each cate-
gory. To compare experiments in each category we used
at most 2000 instances in each experiment.
Feature class Features
Lexical 522,153
Part of speech 2478
tags 45
tag pairs 1972
tags (reduced) 22
tag pairs (reduced) 484
Parse 11
Surface 9
Table 6: The number of features in each feature class.
ered the performance of a sentence-level classifier.
The classifier labeled each sentence as either ordi-
nary or simple and we report results using 10-fold
cross-validation on a random split of the sentences.
For both settings we also evaluated a single classifier
trained on all categories.
We next considered cross-category performance:
how would a classifier trained to detect differences
between simple and ordinary examples from one
category do when tested on another category. In
this experiment, we trained a single classifier on data
from a single category and used the classifier to label
examples from each of the other categories. We re-
port the accuracy on each category in these transfer
experiments.
For learning we require a binary classifier train-
ing algorithm. We evaluated several learning algo-
rithms for classification and report results for each
one: a) MIRA?a large margin online learning al-
gorithm (Crammer et al, 2006). Online learning
algorithms observe examples sequentially and up-
46
date the current hypothesis after each observation; b)
Confidence Weighted (CW) learning?a probabilis-
tic large margin online learning algorithm (Dredze et
al., 2008); c) Maximum Entropy?a log-linear dis-
criminative classifier (Berger et al, 1996); and d)
Support Vector Machines (SVM)?a large margin
discriminator (Joachims, 1998).
For each experiment, we used default settings of
the parameters and 10 online iterations for the online
methods (MIRA, CW). To create a fair comparison
for each category, we limited the number of exam-
ples to a maximum of 2000.
6 Results
For the first task of document classification, we saw
at least 90% mean accuracy with each of the clas-
sifiers. Using all features, SVM and Maximum En-
tropy performed almost perfectly. The online clas-
sifiers, CW and MIRA, displayed similar preference
to the larger feature sets, lexical and part-of-speech
counts. When using just lexical counts, both CW
and MIRA were more accurate than the SVM and
Maximum Entropy (reporting 92.95% and 86.55%
versus 75.00% and 78.75%, respectively). For all
classifiers, the models using the counts of part-of-
speech tags did better than classifiers trained on the
surface features and on the parse features. This is
surprising, since we expected the surface features to
be robust predictors of the document class, mainly
because the average ordinary Wikipedia article in
our corpus is about four times longer than the av-
erage Simple Wikipedia article. We also expected
the syntactic features to be a strong predictor of the
document class since more complicated parse trees
correspond to more complex sentences.
For each classifier, we looked at its performance
without its less predictive feature categories, and
for CW the inclusion of the surface features de-
creased performance noticeably. The best CW
classifiers used either part-of-speech and lexical
features (95.95%) or just part-of-speech features
(95.80%). The parse features, which by themselves
only yielded 64.60% accuracy, when combined with
part-of-speech and lexical features showed high ac-
curacy as well (95.60%). MIRA also showed higher
accuracy when surface features were not included
(from 97.50% mean accuracy with all features to
97.75% with all but surface features).
The best SVM classifier used all four feature
classes, but had nearly as good accuracy with just
part-of-speech counts and surface features (99.85%
mean accuracy) and with surface and parse features
(also 99.85% accuracy). Maximum Entropy, on
the other hand, improved slightly when the lexical
and parse features were not included (from 99.45%
mean accuracy with all feature classes to 99.55%).
We examined the weights learned by the classi-
fiers to determine the features that were effective for
learning. We selected the features with the highest
absolute weight for a MIRA classifier trained on all
categories. The most predictive features for docu-
ment classification were the sentence length (shorter
favors Simple), the length of the longest NP (longer
favors ordinary), the number of sentences (more fa-
vors ordinary), the average number of prepositional
phrases and noun phrases per sentence, the height
of the parse tree, and the number of adjectives. The
most predictive features for sentence classification
were the ratio of different tree non-terminals (VP, S,
NP, S-Bar) to the number of words in the sentence,
the ratio of the total height of the productions in a
tree to the height of the tree, and the extent to which
the tree was right branching. These features are con-
sistent with the rules described above for simple text.
Next we looked at a pairwise comparison of how
the classifiers perform when trained on one category
and tested on another. Surprisingly, the results were
robust across categories, across classifiers. Using
the best feature class as determined in the first task,
the average drop in accuracy when trained on each
domain was very low across all classifiers (the mean
accuracy rate of each cross-category classification
was at least 90%). Table 6 shows the mean change in
accuracy from CW models trained and tested on the
same category to the models trained and tested on
different categories. When trained on the Everyday
Life category, the model actually showed a mean in-
crease in accuracy when predicting other categories.
In the final task, we trained binary classifiers to
identify simple sentences in isolation. The mean
accuracy was lower for this task than for the doc-
ument classification task, and we anticipated indi-
vidual sentences to be more difficult to classify be-
cause each sentence only carries a fraction of the
47
Classifier All features Lexical POS Surface Parse
CW 86.40% 92.95% 95.80% 69.80% 64.60%
MIRA 97.50% 86.55% 94.55% 79.65% 66.90%
MaxEnt 99.45% 78.75% 96.25% 86.90% 80.70%
SVM 99.90% 75.00% 96.60% 89.75% 82.70%
Table 7: Mean accuracy of all classifiers on the document classification task.
Classifier All features POS Surface Parse
CW 73.20% 74.45% 57.40% 62.25%
MIRA 71.15% 72.65% 56.50% 56.45%
MaxEnt 80.80% 77.65% 71.30% 69.00%
SVM 77.00% 76.40% 72.55% 73.00%
Table 8: Mean accuracy of all classifiers on the sentence classification task.
Category Mean accuracy change
Everyday life +1.42%
Geography ?4.29%
History ?1.01%
Literature ?1.84%
Media ?0.56%
People ?0.20%
Religion ?0.56%
Science ?2.50%
Table 9: Mean accuracy drop for a CW model trained on
one category and tested on all other categories. Negative
numbers indicate a decrease in performance.
information held in an entire document. It is com-
mon to have short, simple sentences as part of ordi-
nary English text, although they will not make up the
whole. However results were still promising, with
between 72% and 80% mean accuracy. With CW
and MIRA, the classifiers benefited from training on
all categories, while MaxEnt and SVM in-category
and all-category models achieved similar accuracy
levels, but the results on cross-category tests were
more variable than in the document classification.
There was also no consistency across features and
classifiers with regard to category-to-category clas-
sification. Overall the results of the sentence classi-
fication task are encouraging and show promise for
detecting individual simple sentences taken out of
context.
6.1 Discussion
The classifiers performed robustly for the document-
level classification task, although the corpus itself
may have biased the model due to the longer aver-
age length of ordinary documents, which we tried
to address by filtering out articles with only one
or two sentences. Cursory inspection suggests that
there is overlap between many Simple Wikipedia ar-
ticles and their corresponding ordinary English arti-
cles, since a large number of Simple Wikipedia doc-
uments appear to be generated directly from the En-
glish Wikipedia articles with more complicated sub-
sections of the documents omitted from the Simple
article.
The sentence classification task could be im-
proved by better labeling of sentences. In these ex-
periments, we assumed that every sentence in an or-
dinary document would be ordinary (i.e., not simple)
and vice versa for simple documents. However it is
not the case that ordinary English text contains only
complicated sentences. In future research we can
use human annotated sentences for building the clas-
sifiers. The features we used in this research suggest
that simple text is created from categorical lexical
and syntactic replacement, but more complicated,
technical, or detailed oriented text may require more
rewriting, and would be of more interest in future
research.
7 Conclusion and Future Work
We have demonstrated the ability to automatically
identify texts as either simple or ordinary at both
48
the document and sentence levels using a variety of
features based on the word usage and grammatical
structures in text. Our statistical analysis has identi-
fied relevant features for this task accessible to com-
putational systems. Immediate applications of the
classifiers created in this research for text simplifi-
cation include editing tools that can identify parts of
a text that may be difficult to understand or for word
processors, in order to notify writers of complicated
sentences in real time.
Using this initial exploration of Simple
Wikipedia, we plan to continue working in a
number of directions. First, we will explore ad-
ditional robust indications of text difficulty. For
example, Alu??sio et al (2008) claim that sentences
that are easier to read are also easier to parse, so
the entropy of the parser or confidence in the output
may be indicative of a text?s difficulty. Additionally,
language models trained on large corpora can assign
probability scores to texts, which may indicate
text difficulty. Of particular interest are syntactic
language models that incorporate some of the
syntactic observations in this paper (Filimonov and
Harper, 2009).
Our next goal will be to look at parallel sentences
to learn rules for simplifying text. One of the ad-
vantages of the Wikipedia collection is the parallel
articles in ordinary English Wikipedia and Simple
Wikipedia. While the content of the articles can dif-
fer, these are excellent examples of comparable texts
that can be useful for learning simplification rules.
Such learning can draw from machine translation,
which learns rules that translate between languages.
The related task of paraphrase extraction could also
provide comparable phrases, one of which can be
identified as a simplified version of the other (Ban-
nard and Callison-Burch, 2005). An additional re-
source available in Simple Wikipedia is the flagging
of articles as not simple. By examining the revision
history of articles whose flags have been changed,
we can discover changes that simplified texts. Initial
work on this topic has automatically learned which
edits correspond to text simplifications (Yatskar et
al., 2010).
Text simplification may necessitate the removal of
whole phrases, sentences, or even paragraphs, as, ac-
cording to the writing guidelines for Wikipedia Sim-
ple (Wikipedia, 2009), the articles should not exceed
a specified length, and some concepts may not be
explainable using the lexicon of Basic English. In
some situations, adding new text to explain confus-
ing but crucial points may serve to aid the reader,
and text generation needs to be further investigated
to make text simplification an automatic process.
Acknowledgements
The authors would like to thank Mary Harper for her
help in parsing our corpus.
References
S.M. Alu??sio, L. Specia, T.A.S. Pardo, E.G. Maziero, and
R.P.M. Fortes. 2008. Brazilian portuguese automatic
text simplification systems. In DocEng.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Associa-
tion for Computational Linguistics (ACL).
A.L. Berger, V.J.D. Pietra, and S.A.D. Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational linguistics, 22(1):39?71.
F. Biadsy, J. Hirschberg, E. Filatova, and LLC InforS-
ense. 2008. An unsupervised approach to biography
production using Wikipedia. In Association for Com-
putational Linguistics (ACL).
S. Bird and E. Loper. 2004. NLTK: The natural lan-
guage toolkit. Proceedings of the ACL demonstration
session, pages 214?217.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In European Conference for Computational
Linguistics (EACL), volume 2006, pages 249?256.
Y. Canning, J. Tait, J. Archibald, and R. Crawley. 2000.
Cohesive generation of syntactically simplified news-
paper text. Lecture notes in computer science, pages
145?150.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin,
and J. Tait. 1999. Simplifying text for language-
impaired readers. In European Conference for Com-
putational Linguistics (EACL), pages 269?270.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features. In European
Conference for Computational Linguistics (EACL),
pages 139?147.
E. Charniak andM. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In As-
sociation for Computational Linguistics (ACL), page
180. Association for Computational Linguistics.
49
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research (JMLR).
W. Daelemans, A. Ho?thker, and E Tjong Kim Sang.
2004. Automatic sentence simplification for subtitling
in Dutch and English. In Conference on Language Re-
sources and Evaluation (LREC), pages 1045?1048.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
S. Devlin and G. Unthank. 2006. Helping aphasic peo-
ple process online information. In SIGACCESS Con-
ference on Computers and Accessibility.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification.
In International Conference on Machine Learning
(ICML).
L. Feng, N. Elhadad, and M. Huenerfauth. 2009. Cog-
nitively motivated features for readability assessment.
In European Conference for Computational Linguis-
tics (EACL).
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
Empirical Methods in Natural Language Processing
(EMNLP).
Z. Gantner and L. Schmidt-Thieme. 2009. Automatic
content-based categorization of Wikipedia articles. In
Association for Computational Linguistics (ACL).
Z. Huang and M. Harper. 2009. Self-training pcfg
grammars with latent annotations across languages. In
Empirical Methods in Natural Language Processing
(EMNLP).
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learning
(ECML).
T. Kiss and J. Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguis-
tics, 32(4):485?525.
R. Mihalcea. 2007. Using Wikipedia for automatic
word sense disambiguation. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
C. Mu?ller and I. Gurevych. 2008. Using Wikipedia
and Wiktionary in domain-specific information re-
trieval. In Working Notes of the Annual CLEF Meet-
ing. Springer.
C.K. Ogden. 1930. Basic English: A General Introduc-
tion with Rules and Grammar. Paul Treber & Co., Ltd.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Association for Computational Linguis-
tics (ACL).
S.E. Petersen and M. Ostendorf. 2007. Text simplifi-
cation for language learners: A corpus analysis. In
The Speech and Language Technology for Education
Workshop, pages 69?72.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Association for Computa-
tional Linguistics (ACL).
S.P. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, WordNet and Wikipedia for coreference
resolution. In North American Chapter of the Associ-
ation for Computational Linguistics (NAACL).
R. Schenkel, F. Suchanek, and G. Kasneci. 2007.
YAWN: A semantically annotated Wikipedia XML
corpus. In Proceedings of GI-Fachtagung fu?r
Datenbanksysteme in Business, Technologie und Web
(BTW2007).
A. Siddharthan. 2006. Syntactic simplification and text
cohesion. Research on Language & Computation,
4(1):77?109.
Jason Smith, Chris Quirk, and Kristina Toutanova. 2010.
Extracting parallel sentences from comparable corpora
using document level alignment. In North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Association for Computational Linguistics (ACL)
Demo Session.
Voice Of America. 2009. Word book, 2009 edition.
www.voaspecialenglish.com, February.
Wikipedia. 2009. Simple Wikipedia English.
http://en.wikipedia.org/wiki/Citing Wikipedia, Octo-
ber.
X. Yang and J. Su. 2007. Coreference resolution using
semantic relatedness information from automatically
discovered patterns. In Association for Computational
Linguistics (ACL).
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Experiments with unsupervised extraction of lexi-
cal simplifications. In North American Chapter of the
Association for Computational Linguistics (NAACL).
Y. Ye, M. Zhou, and C.Y. Lin. 2007. Sentence level ma-
chine translation evaluation as a ranking problem: one
step aside from BLEU. In ACL Workshop on statisti-
cal machine translation.
50
Workshop on Monolingual Text-To-Text Generation, pages 84?90,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 84?90,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Paraphrastic Sentence Compression with a Character-based Metric:
Tightening without Deletion
Courtney Napoles1 and Chris Callison-Burch1 and Juri Ganitkevitch1 and
Benjamin Van Durme1,2
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present a substitution-only approach to
sentence compression which ?tightens? a sen-
tence by reducing its character length. Replac-
ing phrases with shorter paraphrases yields
paraphrastic compressions as short as 60% of
the original length. In support of this task,
we introduce a novel technique for re-ranking
paraphrases extracted from bilingual corpora.
At high compression rates1 paraphrastic com-
pressions outperform a state-of-the-art dele-
tion model in an oracle experiment. For fur-
ther compression, deleting from oracle para-
phrastic compressions preserves more mean-
ing than deletion alone. In either setting, para-
phrastic compression shows promise for sur-
passing deletion-only methods.
1 Introduction
Sentence compression is the process of shortening a
sentence while preserving the most important infor-
mation. Because it was developed in support of ex-
tractive summarization (Knight and Marcu, 2000),
much of the previous work considers deletion-based
models, which extract a subset of words from a long
sentence to create a shorter sentence such that mean-
ing and grammar are maximally preserved. This
framework imposes strict constraints on the task and
does not accurately model human-written compres-
sions, which tend to be abstractive rather than ex-
tractive (Marsi et al, 2010). This is one sense in
which paraphrastic compression can improve exist-
ing compression methodologies.
1Compression rate is defined as the compression length over
original length, so lower values indicate shorter sentences.
We distinguish two non-identical notions of sen-
tence compression: making a sentence substantially
shorter versus ?tightening? a sentence by remov-
ing unnecessary verbiage. We propose a method to
tighten sentences with just substitution and no dele-
tion operations. Using paraphrases extracted from
bilingual text and re-ranked on monolingual data,
our system selects the set of paraphrases that min-
imizes the character length of a sentence.
While not currently the standard, character-based
lengths have been considered before in compres-
sion, and we believe that it is relevant for current
and future applications. Character lengths have been
used for document summarization (DUC 2004, Over
and Yen (2004)), summarizing for mobile devices
(Corston-Oliver, 2001), and subtitling (Glickman et
al., 2006). Although in the past strict word limits
have been imposed for various documents, informa-
tion transmitted electronically is often limited by the
number of bytes, which directly relates to the num-
ber of characters. Mobile devices, SMS messages,
and microblogging sites such as Twitter are increas-
ingly important for quickly spreading information.
In this context, it is important to consider character-
based constraints.
We examine whether paraphrastic compression
allows more information to be conveyed in the same
number of characters as deletion-only compressions.
For example, the length constraint of Twitter posts or
tweets is 140 characters, and many article lead sen-
tences exceed this limit. A paraphrase substitution
oracle compresses the sentence in the table below to
76% of its original length (162 to 123 characters; the
first is the original). The compressed tweet is 140
84
characters, including spaces 17-character shortened
link to the original article.2
Congressional leaders reached a last-gasp agreement
Friday to avert a shutdown of the federal government,
after days of haggling and tense hours of brinksman-
ship.
Congress made a final agreement Fri. to avoid govern-
ment shutdown, after days of haggling and tense hours
of brinkmanship. on.wsj.com/h8N7n1
In contrast, using deletion to compress to the same
length may not be as expressive:
Congressional leaders reached agreement Friday to
avert a shutdown of federal government, after haggling
and tense hours. on.wsj.com/h8N7n1
This work presents a model that makes paraphrase
choices to minimize the character length of a sen-
tence. An oracle paraphrase-substitution experiment
shows that human judges rate paraphrastic compres-
sions higher than deletion-based compressions. To
achieve further compression, we shortened the or-
acle compressions using a deletion model to yield
compressions 80% of the original sentence length
and compared these to compressions generated us-
ing just deletions. Manual evaluation found that
the oracle-then-deletion compressions to preserve
more meaning than deletion-only compressions at
uniform compression rates.
2 Related work
Most of the previous research on sentence compres-
sion focuses on deletion using syntactic informa-
tion, (e.g., Galley and McKeown (2007), Knight
and Marcu (2002), Nomoto (2009), Galanis and An-
droutsopoulos (2010), Filippova and Strube (2008),
McDonald (2006), Yamangil and Shieber (2010),
Cohn and Lapata (2008), Cohn and Lapata (2009),
Turner and Charniak (2005)). Woodsend et al
(2010) incorporate paraphrase rules into a deletion
model. Previous work in subtitling has made one-
word substitutions to decrease character length at
high compression rates (Glickman et al, 2006).
More recent approaches in steganography have used
paraphrase substitution to encode information in text
but focus on grammaticality, not meaning preserva-
tion (Chang and Clark, 2010). Zhao et al (2009) ap-
plied an adaptable paraphrasing pipeline to sentence
2Taken from the main page of http://wsj.com, April 9, 2011.
compression, optimizing for F-measure over a man-
ually annotated set of gold standard paraphrases.
Sentence compression has been considered be-
fore in contexts outside of summarization, such as
headline, title, and subtitle generation (Dorr et al,
2003; Vandeghinste and Pan, 2004; Marsi et al,
2009). Corston-Oliver (2001) deleted characters
from words to shorten the character length of sen-
tences. To our knowledge character-based compres-
sion has not been examined before with the surging
popularity and utility of Twitter.
3 Sentence Tightening
The distinction between tightening and compression
can be illustrated by considering how much space
needs to be preserved. In the case of microblogging,
often a sentence has just a few too many characters
and needs to be ?tightened?. On the other hand, if a
sentence is much longer than a desired length, more
drastic compression is necessary. The first subtask
is relevant in any context with strict word or charac-
ter limits. Some sentences may not be compressible
beyond a certain limit. For example, we found that
near 10% of the compressions generated by Clarke
and Lapata (2008) were identical to the original sen-
tence. In situations where the sentence must meet
a minimum length, tightening can be used to meet
these requirements.
Multi-reference translations provide an instance
of the natural length variation of human-generated
sentences. These translations represent different
ways to express the foreign same sentence, so there
should be no meaning lost between the different ref-
erence translations. The character-based length of
different translations of a given sentence varies on
average by 80% when compared to the shortest sen-
tence in a set.3 This provides evidence that sen-
tences can be tightened to some extent without los-
ing any meaning.
Through the lens of sentence tightening, we con-
sider whether paraphrase substitutions alone can
yield compressions competitive with a deletion at
the same length. A character-based compression
rate is crucial in this framework, as two compres-
3This value will vary by collection and with the number of
references: for example, the NIST05 Arabic reference set has a
mean compression rate of 0.92 with 4 references per set.
85
sions having the same character-based compres-
sion rate may have different word-based compres-
sion rates. The advantage of a character-based sub-
stitution model is in choosing shorter words when
possible, freeing space for more content words. Go-
ing by word length alone would exclude the many
paraphrases with fewer characters than the original
phrase and the same number of words (or more).
3.1 Paraphrase Acquisition
To generate paraphrases for use in our experiments,
we took the approach described by Bannard and
Callison-Burch (2005), which extracts paraphrases
from bilingual parallel corpora. Figure 1 illustrates
the process. A phrase to be paraphrased, like thrown
into jail, is found in a German-English parallel cor-
pus. The corresponding foreign phrase (festgenom-
men) is identified using word alignment and phrase
extraction techniques from phrase-based statistical
machine translation (Koehn et al, 2003). Other oc-
currences of the foreign phrase in the parallel corpus
may align to another English phrase like jailed. Fol-
lowing Bannard and Callison-Burch, we treated any
English phrases that share a common foreign phrase
as potential paraphrases of each other.
As the original phrase occurs several times and
aligns with many different foreign phrases, each of
these may align to a variety of other English para-
phrases. Thus, thrown into jail not only paraphrases
as jailed, but also as arrested, detained, impris-
oned, incarcerated, locked up, taken into custody,
and thrown into prison . Moreover, because the
method relies on noisy and potentially inaccurate
word alignments, it is prone to generate many bad
paraphrases, such as maltreated, thrown, cases, cus-
tody, arrest, owners, and protection.
To rank candidates, Bannard and Callison-Burch
defined the paraphrase probability p(e2|e1) based
on the translation model probabilities p(e|f) and
p(f |e) from statistical machine translation. Follow-
ing Callison-Burch (2008), we refine selection by re-
quiring both the original phrase and paraphrase to
be of the same syntactic type, which leads to more
grammatical paraphrases.
Although many excellent paraphrases are ex-
tracted from parallel corpora, many others are un-
suitable and the translation score does not always
accurately distinguish the two. Therefore, we re-
Paraphrase Monlingual Bilingual
study in detail 1.00 0.70
scrutinise 0.94 0.08
consider 0.90 0.20
keep 0.83 0.03
learn 0.57 0.10
study 0.42 0.07
studied 0.28 0.01
studying it in detail 0.16 0.05
undertook 0.06 0.06
Table 1: Candidate paraphrases for study in detail with
corresponding approximate cosine similarity (Monolin-
gual) and translation model (Bilingual) scores.
ranked our candidates based on monolingual distri-
butional similarity, employing the method described
by Van Durme and Lall (2010) to derive approxi-
mate cosine similarity scores over feature counts us-
ing single token, independent left and right contexts.
Features were computed from the web-scale n-gram
collection of Lin et al (2010). As 5-grams are the
highest order of n-gram in this collection, the al-
lowable set of paraphrases have at most four words
(which allows at least one word of context).
To our knowledge this is the first time such tech-
niques have been used in combination in order to
derive higher quality paraphrase candidates. See Ta-
ble 1 for an example.
The monolingual-filtering technique we describe
is by no means limited to paraphrases extracted from
bilingual corpora. It could be applied to other data-
driven paraphrasing techniques (see Madnani and
Dorr (2010) for a survey). Although it is particularly
well suited to the bilingual extracted corpora, since
the information that it adds is orthogonal to that
model, it would presumably add less to paraphras-
ing techniques that already take advantage of mono-
lingual distributional similarity (Pereira et al, 1993;
Lin and Pantel, 2001; Barzilay and Lee, 2003).
In order to evaluate the paraphrase candidates
and scoring techniques, we randomly selected 1,000
paraphrase sets where the source phrase was present
in the corpus described in Clarke and Lapata (2008).
For each phrase and set of candidate paraphrases, we
extracted all of the contexts from the corpus in which
the source phrase appeared. Human judges were
presented each sentence with the original phrase and
the same sentences with each paraphrase candidate
86
... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten
... last week five farmers were thrown into jail in Ireland because they resisted ...
...
Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .
Quite a few journalists have disappeared or have been imprisoned , tortured and killed .
Figure 1: Using a bilingual parallel corpus to extract paraphrases.
substituted in. Each paraphrase substitution was
graded based on the extent to which it preserved
the meaning and affected the grammaticality of the
sentence. While both the bilingual translation score
and monolingual cosine similarity positively corre-
lated with human judgments, the monolingual score
proved a stronger predictor of quality in both dimen-
sions. Using Kendall?s tau correlation coefficient,
the agreement between the ranking imposed by the
monolingual score and human ratings surpassed that
of the original ranking as derived during the bilin-
gual extraction, for both meaning and grammar.4 In
our substitution framework, we ignore the transla-
tion probabilities and use only the approximate co-
sine similarity in the paraphrase decision task.
4 Framework for Sentence Tightening
Our sentence tightening approach uses a dynamic
programming strategy to find the combination of
non-overlapping paraphrases that minimizes a sen-
tence?s character length. The threshold of the mono-
lingual score for paraphrases can be varied to widen
or narrow the search space, which may be further in-
creased by considering any lexical paraphrases not
subject to syntactic constraints. Sentences with a
compression rate as low as 0.6 can be generated
without thresholding the paraphrase scores. Because
the system can generate multiple paraphrased sen-
tences of equal length, we apply two layers of filter-
ing to generate a single output. First we calculate a
word-overlap score between the original and candi-
date sentences to favor compressions similar to the
original sentence; then, from among the sentences
4For meaning and grammar respectively, ? = 0.28 and 0.31
for monolingual scores and 0.19 and 0.15 for bilingual scores.
with the highest word overlap, we select the com-
pression with the best language model score.
Higher paraphrase thresholds guarantee more ap-
propriate paraphrases but yield longer compressions.
Using a cosine-similarity threshold of 0.95, the av-
erage compression rate is 0.968, which is consider-
ably longer than the compressions using no thresh-
old (0.60). In these experiments we did not syntac-
tically constrain paraphrases. However, we believe
that our monolingual refining of paraphrase sets im-
proves paraphrase selection and is a reasonable al-
ternative to using syntactic constraints.
In case judges favor compressions that have high
word overlap with the original sentence, we com-
pressed the longest sentence from each set of ref-
erence translations (Huang et al, 2002) and ran-
domly chose a sentence from the set of reference
translations to use as the standard for comparison.
Paraphrastic compressions were generated at cosine-
similarity thresholds ranging from 0.60 to 0.95.
We implemented a state-of-the-art deletion model
(Clarke and Lapata, 2008) to generate deletion-only
compressions. We fixed the compression length
to ?5 characters of the length of each paraphras-
tic compression, in order to isolate the compression
quality from the effect of compression rate (Napoles
et al, 2011). Manual evaluation used Amazon?s
Mechanical Turk with three-way redundancy and
positive and negative controls to filter bad workers.
Meaning and grammar judgments were collected us-
ing two 5-point scales (5 being the highest score).
5 Evaluation
The initial results of our substitution system show
room for improvement in future work (Table 2). We
believe this is due to erroneous paraphrase substi-
87
System Grammar Meaning CompR Cos.
Substitution 3.8 3.7 0.97 0.95
Deletion 4.1 4.0 0.97 -
Substitution 3.4 3.2 0.89 0.85
Deletion 4.0 3.8 0.89 -
Substitution 3.1 3.0 0.85 0.75
Deletion 3.9 3.7 0.85 -
Substitution 2.9 2.9 0.82 0.65
Deletion 3.8 3.5 0.82 -
Table 2: Mean ratings of compressions using just deletion
or substitution at different paraphrase thresholds (Cos.).
Deletion performed better in all settings.
tutions, since phrases with the same syntactic cate-
gory and distributional similarity are not necessarily
semantically identical. Illustrative examples include
WTO for United Nations and east or west for south.
Because the quality of the multi-reference transla-
tions is not uniformly high, for the following exper-
iment we used a dataset of English newspaper arti-
cles.
To control against these errors and test the viabil-
ity of a substitution-only approach, we generated all
possible paraphrase substitutions above a threshold
of 0.80 within a set of 20 randomly chosen sentences
from the written corpus of Clarke and Lapata (2008).
We solicited humans to make a ternary decision of
whether a paraphrase was acceptable in the context
(good, bad, or not sure). We applied our model to
generate compressions using only paraphrase substi-
tutions on which all three annotators agreed that the
paraphrase was good. The oracle generated com-
pressions with an average compression rate of 0.90.
On the same set of original sentences, we used
the deletion model to generate compressions con-
strained to ?5 characters of the length of the ora-
cle compression. Next, we examined whether apply-
ing the deletion model to paraphrastic compressions
would improve compression quality. In manual eval-
uation along the dimensions of grammar and mean-
ing, both the oracle compressions and oracle-plus-
deletion compressions outperformed the deletion-
only compressions at uniform lengths (Table 3)5.
These results suggest that improvements in para-
phrase acquisition will make our system competitive
with deletion-only models.
5Paraphrastic compressions were rated significantly higher
for meaning, p < 0.05
Model Grammar Meaning CompR
Oracle 4.1 4.3 0.90
Deletion 4.0 4.1 0.90
Gold 4.3 3.8 0.75
Oracle+deletion 3.4 3.7 0.80
Deletion 3.2 3.4 0.80
Table 3: Mean ratings of compressions generated by a
substitution oracle, deletion only, deletion on the oracle
compression, and the gold standard. Being able to choose
the best paraphrases would enable our substitution model
to outperform the deletion model.
6 Conclusion
This work shows promise for the use of only sub-
stitution in the task of sentence tightening. There
are myriad possible extensions and improvements
to this method, most notably richer features be-
yond paraphrase length. We do not currently use
syntactic information in our paraphrastic compres-
sion model because it places limits on the number
of paraphrases available for a sentence and thereby
limits the possible compression rate. The current
method for paraphrase extraction does not include
certain types of rewriting, such as passivization, and
should be extended to incorporate even more short-
ening paraphrases. Future work can directly apply
these methods to Twitter and extract additional para-
phrases and abbreviations from Twitter and/or SMS
data. Our substitution approach can be improved by
applying more sophisticated techniques to choosing
the best candidate compression, or by framing it as
an optimization problem over more than just mini-
mal length. Overall, we find these results to be en-
couraging for the possibility of sentence compres-
sion without deletion.
Acknowledgments
We are grateful to John Carroll for helping us obtain
the RASP parser. This research was partially funded
by the JHU Human Language Technology Center of
Excellence. This research was funded in part by the
NSF under grant IIS-0713448. The views and find-
ings are the authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
88
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Ching-Yun Chang and Stephen Clark. 2010. Linguis-
tic steganography using automatically generated para-
phrases. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
591?599. Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Simon Corston-Oliver. 2001. Text compaction for dis-
play on very small screens. In Proceedings of the
NAACL Workshop on Automatic Summarization.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference. Association for Computational Linguis-
tics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-
gio, and Walter Daelemans. 2006. Investigating lexi-
cal substitution scoring for subtitle generation. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning, pages 45?52. Associa-
tion for Computational Linguistics.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? Step one: Sentence compression. In
Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2009. Is sentence compression an NLG
task? In Proceedings of the 12th European Workshop
on Natural Language Generation.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2010. On the limits of sentence com-
pression by deletion. Empirical Methods in Natural
Language Generation, pages 45?66.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of ACL, Workshop on Monolingual Text-To-Text Gen-
eration.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
Paul Over and James Yen. 2004. An introduction to
DUC 2004: Intrinsic evaluation of generic news text
summarization systems. In Proceedings of DUC 2004
Document Understanding Workshop, Boston.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In ACL-93.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach.
In Proceedings of the ACL workshop on Text Summa-
rization.
89
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.
Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
synchronous tree-substitution grammar induction and
its application to sentence compression. In Proceed-
ings of ACL.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
90
Workshop on Monolingual Text-To-Text Generation, pages 91?97,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 91?97,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Evaluating sentence compression: Pitfalls and suggested remedies
Courtney Napoles1 and Benjamin Van Durme1,2 and Chris Callison-Burch1
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
This work surveys existing evaluation
methodologies for the task of sentence
compression, identifies their shortcomings,
and proposes alternatives. In particular,
we examine the problems of evaluating
paraphrastic compression and comparing the
output of different models. We demonstrate
that compression rate is a strong predictor
of compression quality and that perceived
improvement over other models is often a side
effect of producing longer output.
1 Introduction
Sentence compression is the natural language gen-
eration (NLG) task of automatically shortening sen-
tences. Because good compressions should be gram-
matical and retain important meaning, they must be
evaluated along these two dimensions. Evaluation is
a difficult problem for NLG, and many of the prob-
lems identified in this work are relevant for other
generation tasks. Shared tasks are popular in many
areas as a way to compare system performance in an
unbiased manner. Unlike other tasks, such as ma-
chine translation, there is no shared-task evaluation
for compression, even though some compression
systems are indirectly evaluated as a part of DUC.
The benefits of shared-task evaluation have been dis-
cussed before (e.g., Belz and Kilgarriff (2006) and
Reiter and Belz (2006)), and they include compar-
ing systems fairly under the same conditions.
One difficulty in evaluating compression systems
fairly is that an unbiased automatic metric is hard
to define. Automatic evaluation relies on a com-
parison to a single gold standard at a predetermined
length, which greatly limits the types of compres-
sions that can be fairly judged. As we will discuss
in Section 2.1.1, automatic evaluation assumes that
deletions are independent, considers only a single
gold standard, and cannot handle compressions with
paraphrasing. Like for most areas in NLG, human
evaluation is preferable. However, as we discuss in
Section 2.2, there are some subtleties to appropri-
ate experiment design, which can give misleading
results if not handled properly.
This work identifies the shortcomings of widely
practiced evaluation methodologies and proposes al-
ternatives. We report on the effect of compression
rate on perceived quality and suggest ways to control
for this dependency when evaluating across different
systems. In this work we:
? highlight the importance of comparing systems
with similar compression rates,
? argue that comparisons in many previous pub-
lications are invalid,
? provide suggestions for unbiased evaluation.
While many may find this discussion intuitive, these
points are not addressed in much of the existing re-
search, and therefore it is crucial to enumerate them
in order to improve the scientific validity of the task.
2 Current Practices
Because it was developed in support of extractive
summarization (Knight and Marcu, 2000), com-
pression has mostly been framed as a deletion task
(e.g., McDonald (2006), Galanis and Androutsopou-
los (2010), Clarke and Lapata (2008), and Galley
91
Words Sentence
31 Kaczynski faces charges contained in a 10-count federal indictment naming him as the person responsible for trans-
porting bombs and bomb parts from Montana to California and mailing them to victims .
17 Kaczynski faces charges naming him responsible for transporting bombs to California and mailing them to victims .
18 Kaczynski faces charges naming him responsible for transporting bombs and bomb parts and mailing them to victims .
18 Kaczynski faces a 10-count federal indictment for transporting bombs and bomb parts and mailing them to victims .
Table 1: Three acceptable compressions of a sentence created by different annotators (the first is the original).
and McKeown (2007)). In this context, a compres-
sion is an extracted subset of words from a long
sentence. There are limited compression corpora
because, even when an aligned corpus exists, the
number of extractive sentence pairs will be few and
therefore gold-standard compressions must be man-
ually annotated. The most popular corpora are the
Ziff-Davis corpus (Knight and Marcu, 2000), which
contains a small set of 1067 extracted sentences
from article/abstract pairs, and the manually anno-
tated Clarke and Lapata (2008) corpus, consisting of
nearly 3000 sentences from news articles and broad-
cast news transcripts. These corpora contain one
gold standard for each sentence.
2.1 Automatic Techniques
One of the most widely used automatic metrics is the
F1 measure over grammatical relations of the gold-
standard compressions (Riezler et al, 2003). This
metric correlates significantly with human judg-
ments and is better than Simple String Accuracy
(Bangalore et al, 2000) for judging compression
quality (Clarke and Lapata, 2006). F1 has also been
used over unigrams (Martins and Smith, 2009) and
bigrams (Unno et al, 2006). Unno et al (2006)
compared the F1 measures to BLEU scores (using
the gold standard as a single reference) over vary-
ing compression rates, and found that BLEU be-
haves similarly to both F1 measures. A syntactic
approach considers the alignment over parse trees
(Jing, 2000), and a similar technique has been used
with dependency trees to evaluate the quality of sen-
tence fusions (Marsi and Krahmer, 2005).
The only metric that has been shown to correlate
with human judgments is F1 (Clarke and Lapata,
2006), but even this is not entirely reliable. F1 over
grammatical relations also depends on parser accu-
racy and the type of dependency relations used.1
1For example, the RASP parser uses 16 grammatical depen-
2.1.1 Pitfalls of Automatic Evaluation
Automatic evaluation operates under three often
incorrect assumptions:
Deletions are independent. The dependency
structure of a sentence may be unaltered when de-
pendent words are not deleted as a unit. Examples
of words that should be treated as a single unit in-
clude negations and negative polarity items or cer-
tain multi-word phrases (such as deleting Latin and
leaving America). F1 treats all deletions equally,
when in fact errors of this type may dramatically al-
ter the meaning or the grammaticality of a sentence
and should be penalized more than less serious er-
rors, such as deleting an article.
The gold standard is the single best compres-
sion. Automatic evaluation considers a single
gold-standard compression. This ignores the pos-
sibility of different length compressions and equally
good compressions of the same length, where mul-
tiple non-overlapping deletions are acceptable. For
an example, see Table 1.
Having multiple gold standards would provide
references at different compression lengths and re-
flect different deletion choices (see Section 3). Since
no large corpus with multiple gold standards exists
to our knowledge, systems could instead report the
quality of compressions at several different com-
pression rates, as Nomoto (2008) did. Alternatively,
systems could evaluate compressions that are of a
similar length as the gold standard compression, to
fix a length for the purpose of evaluation. Output
length is controlled for evaluation in some other ar-
eas, notably DUC.
Systems compress by deletion and not substitu-
tion. More recent approaches to compression in-
troduce reordering and paraphrase operations (e.g.,
dencies (Briscoe, 2006) while there are over 50 Stanford De-
pendencies (de Marneffe and Manning, 2008).
92
Cohn and Lapata (2008), Woodsend et al (2010),
and Napoles et al (2011)). For paraphrastic com-
pressions, manual evaluation alone reliably deter-
mines the compression quality. Because automatic
evaluation metrics compare shortened sentences to
extractive gold standards, they cannot be applied to
paraphrastic compression.
To apply automatic techniques to substitution-
based compression, one would need a gold-standard
set of paraphrastic compressions. These are rare.
Cohn and Lapata (2008) created an abstractive cor-
pus, which contains word reordering and paraphras-
ing in addition to deletion. Unfortunately, this cor-
pus is small (575 sentences) and only includes one
possible compression for each sentence.
Other alternatives include deriving such corpora
from existing corpora of multi-reference transla-
tions. The longest reference translation can be
paired with the shortest reference to represent a
long sentence and corresponding paraphrased gold-
standard compression.
Similar to machine translation or summarization,
automatic translation of paraphrastic compressions
would require multiple references to capture allow-
able variation, since there are often many equally
valid ways of compressing an input. ROUGE
or BLEU could be applied to a set of multiple-
reference compressions, although BLEU is not with-
out its own shortcomings (Callison-Burch et al,
2006). One benefit of both ROUGE and BLEU is
that they are based on n-gram recall and precision
(respectively) instead of word-error rate, so reorder-
ing and word substitutions can be evaluated. Dorr et
al. (2003) used BLEU for evaluation in the context
of headline generation, which uses rewording and
is related to sentence compression. Alternatively,
manual evalation can be adapted from other NLG
domains, such as the techniques described in the fol-
lowing section.
2.2 Manual Evaluation
In order to determine semantic and syntactic suit-
ability, manual evaluation is preferable over au-
tomatic techniques whenever possible. The most
widely practiced manual evaluation methodology
was first used by Knight and Marcu (2002). Judges
grade each compressed sentence against the original
and make two separate decisions: how grammatical
is the compression and how much of the meaning
from the original sentence is preserved. Decisions
are rated along a 5-point scale (LDC, 2005).
Most compression systems consider sentences out
of context (a few exceptions exist, e.g., Daume? III
and Marcu (2002), Martins and Smith (2009), and
Lin (2003)). Contextual cues and discourse struc-
ture may not be a factor to consider if the sentences
are generated for use out of context. An example
of a context-aware approach considered the sum-
maries formed by shortened sentences and evalu-
ated the compression systems based on how well
people could answer questions about the original
document from the summaries (Clarke and Lapata,
2007). This technique has been used before for
evaluating summarization and text comprehension
(Mani et al, 2002; Morris et al, 1992).
2.2.1 Pitfalls of Manual Evaluation
Grammar judgments decrease when the compres-
sion is presented alongside the original sentence.
Figure 1 shows that the mean grammar rating for the
same compressions is on average about 0.3 points
higher when the compression is judged in isolation.
Researchers should be careful to state when gram-
mar is judged on compressions lacking reference
sentences.
Another factor is the group of judges. Obvi-
ously different studies will rely on different judges,
so whenever possible the sentences from an exist-
ing model should be re-evaluated alongside the new
model. The ?McD? entries in Table 2 represent a set
of sentences generated from the exact same model
evaluated by two different sets of judges. The mean
grammar and meaning ratings in each evaluation
setup differ by 0.5?0.7 points.
3 Compression Rate Predicts Performance
The dominant assumption in compression research
is that the system makes the determination about the
optimal compression length. For this reason, com-
pression rates can vary drastically across systems. In
order to get unbiased evaluations, systems should be
compared only when they are compressing at similar
rates.
Compression rate is defined as:
# of tokens in compressed sentence
# of tokens in original sentence
? 100 (1)
93
CR
Mean
ing
1
2
3
4
5
l l
l l
l l
l l
0 20 40 60 80 100
Modell DeletionGold
CR
Gram
mar
1
2
3
4
5
l l
l l
l
l l
l
0 20 40 60 80 100
Modell DeletionGold.1Gold.2
Figure 1: Compression rate strongly correlates with human judgments of meaning and grammaticality. Gold represents
gold-standard compression and Deletion the results of a leading deletion model. Gold.1 grammar judgments were
made alongside the original sentence and Gold.2 were made in isolation.
It seems intuitive that sentence quality diminishes
in relation to the compression rate. Each word
deleted increases the probability that errors are intro-
duced. To verify this notion, we generated compres-
sions at decreasing compression rates of 250 sen-
tences randomly chosen from the written corpus of
Clarke and Lapata (2008), generated by our imple-
mentation of a leading extractive compression sys-
tem (Clarke and Lapata, 2008). We collected hu-
man judgments using the 5-point scales of meaning
and grammar described above. Both quality judg-
ments decreased linearly with the compression rate
(see ?Deletion? in Figure 1).
As this behavior could have been an artifact of
the particular model employed, we next developed
a unique gold-standard corpus for 50 sentences se-
lected at random from the same corpus described
above. The authors manually compressed each sen-
tence at compression rates ranging from less than
10 to 100. Using the same setup as before, we
collected human judgments of these gold standards
to determine an upper bound of perceived quality
at a wide range of compression rates. Figure 1
demonstrates that meaning and grammar ratings de-
cay more drastically at compression rates below 40
(see ?Gold?). Analysis suggests that humans are of-
ten able to practice ?creative deletion? to tighten a
sentence up to a certain point, before hitting a com-
pression barrier, shortening beyond which leads to
significant meaning and grammatically loss.
4 Mismatched Comparisons
We have observed that a difference in compression
rates as small as 5 percentage points can influence
the quality ratings by as much as 0.1 points and
conclude: systems must be compared using simi-
lar levels of compression. In particular, if system
A?s output is higher quality, but longer than system
B?s, then it is not necessarily the case that A is better
than B. Conversely, if B has results at least as good
as system A, one can claim that B is better, since B?s
output is shorter.
Here are some examples in the literature of mis-
matched comparisons:
? Nomoto (2009) concluded their system signif-
icantly outperformed that of Cohn and Lapata
(2008). However, the compression rate of their
system ranged from 45 to 74, while the com-
pression rate of Cohn and Lapata (2008) was
35. This claim is unverifiable without further
comparison.
? Clarke and Lapata (2007), when comparing
against McDonald (2006), reported signifi-
cantly better results at a 5-point higher com-
pression rate. At first glance, this does not
seem like a remarkable difference. However,
94
Model Meaning Grammar CompR
C&L 3.83 3.66 64.1
McD 3.94 3.87 64.2
C&L 3.76? 3.53? 78.4?
McD 3.50? 3.17? 68.5?
Table 2: Mean quality ratings of two competing mod-
els once the compression rates have been standardized,
and as reported in the original work (denoted ?). There
is no significant improvement, but the numerically better
model changes.
the study evaluated the quality of summaries
containing automatically shortened sentences.
The average document length in the test set was
20 sentences, and with approximately 24 words
per sentence, a typical 65.4% compressed doc-
ument would have 80 more words than a typical
60.1% McDonald compression. The aggregate
loss from 80 words can be considerable, which
suggests that this comparison is inconclusive.
We re-evaluated the model described in Clarke
and Lapata (2008) (henceforth C&L) against the
McDonald (2006) model with global constraints, but
fixed the compression rates to be equal. We ran-
domly selected 100 sentences from that same cor-
pus and generated compressions with the same com-
pression rate as the sentences generated by the Mc-
Donald model (McD), using our implementation of
C&L. Although not statistically significant, this new
evaluation reversed the polarity of the results re-
ported by Clarke and Lapata (Table 2). This again
stresses the importance of using similar compression
rates to draw accurate conclusions about different
models.
An example of unbiased evaluation is found in
Cohn and Lapata (2009). In this work, their model
achieved results significantly better than a compet-
ing system (McDonald, 2006). Recognizing that
their compression rate was about 15 percentage
points higher than the competing system, they fixed
the target compression rate to one similar to McDon-
ald?s output, and still found significantly better per-
formance using automatic measures. This work is
one of the few that controls their output length in
order to make an objective comparison (another ex-
ample is found in McDonald (2006)), and this type
of analysis should be emulated in the future.
5 Suggestions
Models should be tested on the same corpus, be-
cause different corpora will likely have different fea-
tures that make them easier or harder to compress. In
order to make non-vacuous comparisons of different
models, a system also needs to be constrained to pro-
duce the same length output as another system, or
report results at least as good for shorter compres-
sions. Using the multi-reference gold-standard col-
lection described in Section 3, relative performance
could be estimated through comparison to the gold-
standard curve. The reference set we have annotated
is yet small, but this is an area for future work based
on feedback from the community.2
Other methods for limiting quality disparities in-
troduced by the compression rate include fixing the
target length to that of the gold standard (e.g., Unno
et al (2006)). Alternately, results for a system at
varying compression levels can be reported,3 allow-
ing for comparisons at similar lengths. This is a
practice to be emulated, if possible, because systems
that cannot control output length can make compar-
isons against the appropriate compression rate.
In conclusion, we have provided justification for
the following practices in evaluating compressions:
? Compare systems at similar compression rates.
? Provide results across multiple compression
rates when possible.
? Report that system A surpasses B iff: A and
B have the same compression rate and A does
better than B, or A produces shorter output than
B and A does at least as well B.
? New corpora for compression should have mul-
tiple gold standards for each sentence.
Acknowledgments
We are very grateful to James Clarke for helping us
obtain the results of existing systems and to the re-
viewers for their helpful comments and recommen-
dations. The first author was supported by the JHU
Human Language Technology Center of Excellence.
This research was funded in part by the NSF under
grant IIS-0713448. The views and findings are the
authors? alone.
2This data is available on request.
3For example, Nomoto (2008) reported results ranging over
compression rates: 0.50?0.70.
95
References
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceed-
ings of the first international conference on Natural
language generation-Volume 14, pages 1?8. Associa-
tion for Computational Linguistics.
A. Belz and A. Kilgarriff. 2006. Shared-task eval-
uations in HLT: Lessons for NLG. In Proceedings
of the Fourth International Natural Language Gen-
eration Conference, pages 133?135. Association for
Computational Linguistics.
Ted Briscoe. 2006. An introduction to tag sequence
grammars and the RASP system parser. Computer
Laboratory Technical Report, 662.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL,
Trento, Italy.
James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across domains,
training requirements and evaluation measures. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 377?384. Association for Computational Lin-
guistics.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1?11.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 449?456. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.
Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the sixth con-
ference on Applied natural language processing, pages
310?315. Association for Computational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? Step one: Sentence compression. In
Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression: a pilot study. In
Proceedings of the sixth international workshop on In-
formation retrieval with Asian languages-Volume 11,
pages 1?8. Association for Computational Linguistics.
Inderjeet Mani, Gary Klein, David House, Lynette
Hirschman, Therese Firmin, and Beth Sundheim.
2002. SUMMAC: a text summarization evaluation.
Natural Language Engineering, 8(01):43?68.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European
Workshop on Natural Language Generation, pages 8?
10.
Andre? F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Langauge Pro-
cessing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.
Andrew H. Morris, George M. Kasper, and Dennis A.
Adams. 1992. The effects and limitations of auto-
mated text condensing on reading comprehension per-
formance. INFORMATION SYSTEMS RESEARCH,
3(1):17?35.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphrastic
sentence compression with a character-based metric:
Tightening without deletion. In Proceedings of ACL,
Workshop on Monolingual Text-To-Text Generation.
96
Tadashi Nomoto. 2008. A generic sentence trimmer with
CRFs. Proceedings of ACL-08: HLT, pages 299?307.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
E. Reiter and A. Belz. 2006. GENEVAL: A proposal
for shared-task evaluation in NLG. In Proceedings
of the Fourth International Natural Language Gen-
eration Conference, pages 136?138. Association for
Computational Linguistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 118?125. Association for Computational Lin-
guistics.
Yuya Unno, Takashi Ninomiya, Yusuke Miyao, and
Jun?ichi Tsujii. 2006. Trimming CFG parse trees
for sentence compression using machine learning ap-
proaches. In Proceedings of the COLING/ACL on
Main conference poster sessions, pages 850?857. As-
sociation for Computational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.
97
