Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 361?364,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Query Ambiguity Revisited: Clickthrough Measures for Distinguishing
Informational and Ambiguous Queries
Yu Wang
Math & Computer Science Department
Emory University
yu.wang@emory.edu
Eugene Agichtein
Math & Computer Science Department
Emory University
eugene@mathcs.emory.edu
Abstract
Understanding query ambiguity in web search
remains an important open problem. In this
paper we reexamine query ambiguity by ana-
lyzing the result clickthrough data. Previously
proposed clickthrough-based metrics of query
ambiguity tend to conflate informational and
ambiguous queries. To distinguish between
these query classes, we introduce novel met-
rics based on the entropy of the click distri-
butions of individual searchers. Our exper-
iments over a clickthrough log of commer-
cial search engine demonstrate the benefits of
our approach for distinguishing informational
from truly ambiguous queries.
1 Introduction
Since query interpretation is the first crucial step in
the operation of the web search engines, more re-
liable query intent classification, such as detecting
whether a query is ambiguous, could allow a search
engine to provide more diverse results, better query
suggestions, or otherwise improve user experience.
In this paper we re-examine query ambiguity
in connection with searcher clickthrough behavior.
That is, we posit that clickthrough information could
provide important evidence for classifying query
ambiguity. However, we find that previously pro-
posed clickthrough-based measures tend to conflate
informational and ambiguous queries. We propose a
novel clickthrough measure for query classification,
user click entropy, and show that it helps distinguish
between informational and truly ambiguous queries.
Previous research on this topic focused on binary
classification of query ambiguity. Notably, (Tee-
van et al, 2008) used click entropy as a proxy for
query ambiguity to estimate the potential for search
personalization. (Mei and Church, 2008) considered
click entropy as measure of search difficulty. More
broadly, clickthrough information has been used for
many other tasks such as improving search rank-
ing (Zhu and Mishne, 2009), learning semantic cat-
egories (Komachi et al, 2009), and for topical query
classification (Li et al, 2008). However, our work
sheds new light on distinguishing between informa-
tional and ambiguous queries, by using clickthrough
data. Our contributions include:
? More precise definition of query ambiguity in
terms of clickthrough behavior (Section 2).
? Entropy-based formalization of resulting click be-
haviors (Section 3).
? Empirical validation of our methods on a large
real query and clickthrough log (Section 4).
2 Defining Query Ambiguity
In this study we focus on two orthogonal query in-
tent dimensions, adapted from the top level of user
goal taxonomies such as (Rose and Levinson, 2004).
Specifically, a query could be ambiguous or unam-
biguous; as well as informational or navigational.
Consider the example queries of each type below:
Ambiguous Unambiguous
Informational ?al pacino? ?lyrics?
Navigational ?people? ?google?
The query ?al pacino?, the name of a famous ac-
tor, is a typical ambiguous and informational query.
In the clickthrough logs that we examined, the most
popular searcher destinations include sites with pic-
tures of Al Pacino, movie sites, and biography sites ?
corresponding to different informational intents. In
contrast, the query ?lyrics? has an unambiguous in-
formational intent, which is to explore websites with
song lyrics. For the ambiguous navigational query
?people?, popular destinations include people.com,
Yahoo People or People?s United Bank. Finally, the
361
query ?google? is unambiguous and navigational,
with over 94% of the clicks on the Google?s home-
page.
Definitions of query classes: we now more for-
mally define the query classes we consider:
? Clear: Unambiguous navigational query, such as
?google?.
? Informational: Unambiguous informational
query, such as ?lyrics?
? Ambiguous: Ambiguous informational or navi-
gational query, such as ?people? or ?al pacino?.
The key challenge in distinguishing the last two
classes, Informational and Ambiguous, is that the
overall clickthrough patterns for these classes are
similar: in both cases, there are clicks on many re-
sults, without a single dominant result for the query.
3 Clickthrough Measures for
Distinguishing Ambiguous and
Informational Queries
In this section we describe the features used to rep-
resent a query for intent classification, listed in Ta-
ble 1. In addition to popular features such as click-
through frequency and query length, we introduce
novel features related to user click entropy, to cap-
ture the distinction between informational and am-
biguous queries.
Overall Entropy: Previous methods for query classi-
fication utilize entropy of all result clicks for a query,
or overall entropy (the uncertainty associated with
obtaining a click on any specific result), defined as:
H(Rq) = ?
?
r?Rq
p(r) log p(r)
Rq is the set of results r, clicked by all users after
submitting the query q. For example, a clear query
?target? has the overall entropy of 0.36, and most
results corresponding to this query point to Target?s
company website. The click log data shows that
85% of the users click the Target website for this
query. In contrast, an unclear query ?lyrics? has the
overall entropy of 2.26. However, overall entropy
is insufficient for distinguishing between informa-
tional and ambiguous queries. To fill this gap, we
introduce new clickthrough metrics to detect such
ambiguous queries.
User Entropy: Recall, that both informational
queries and ambiguous queries could have high
1 2 30
5
10
15
(a) Overall Entropy
Fr
eq
ue
nc
y
Ambiguous
Informational
0.15 0.3 0.450
5
10
15
(b) User Entropy
Fr
eq
ue
nc
y InformationalAmbiguous
Figure 1: Frequency of ambiguous and informational
queries by Overall Entropy (a) and User Entropy (b).
overall entropy, making it difficult to distinguish
them. Thus, we introduce a new metric, user en-
tropy of a query q H(Uq), as the average entropy of
a distribution of clicks for each searcher:
H(Uq) =
?
?
u?Uq
?
r?Ru
p(r) log p(r)
|Uq|
where Uq is the set of users who have submitted the
query q, and Ru is the set of results r, clicked by
the user u. For the example informational query
?lyrics?, a single user may click many different
URLs, thereby increasing user entropy of this query
to 0.317. While for an ambiguous query, which has
multiple meanings, a user typically searches for only
one meaning of this query at a time, so the results
clicked by each user will concentrate on one topic.
For example, the query ?people? is ambiguous, and
has the overall entropy of 1.73 due to the variety
of URLs clicked. However, a particular user usu-
ally clicks only one of the websites, resulting in low
user entropy of 0.007. Figure 1 illustrates the dif-
ference in the distributions of informational and am-
biguous queries according to their overall and user
entropy values: more informational queries tend to
have medium to high User Entropy values, com-
pared to the truly ambiguous queries.
Domain Entropy: One problem with the above mea-
sures is that clickthrough data for individual URLs
is sparse. A common approach is to backoff to
the URLs domain, with the assumption that URLs
within the same domain usually relate to the same
topic or concept. Therefore, domain entropy H(Dq)
of a query may be more robust, and is defined as:
H(Dq) = ?
?
d?Dq
p(d) log p(d)
where Dq are the domains of all URL clicked for
q. For example, the query ?excite? is a navigational
and clear query, as all the different clicked URLs for
this query are within the same domain, excite.com.
362
Query Feature Description
QueryLength Number of tokens (words) in the query
ClickFrequency Number of total clicks for this query
OverallEntropy Entropy of all URLs for this query
UserEntropy* Average entropy of the URLs clicked by one user for this query
OverallDomainEntropy Entropy of all URL domains for this query
UserDomainEntropy* Average entropy of URL domains clicked by one user for this query
RelativeUserEntropy* Fraction of UserEntropy divided by OverallEntropy
RelativeOverallEntropy* Fraction of OverallEntropy divided by UserEntropy
RelativeUserDomainEntropy* Fraction of UserDomainEntropy divided by OverallDomainEntropy
RelativeOverallDomainEntropy* Fraction of OverallDomainEntropy divided by UserDomainEntropy
Table 1: Features used to represent a query (* indicates features derived from User Entropy).
While this query has high Overall and User Entropy
values, the Domain Entropy is low, as all the clicked
URLs for this query are within the same domain.
The features described here can then be used as
input to many available classifiers. In particular, we
use the Weka toolkit1, as described below.
4 Experimental Results
We describe the dataset and annotation process, and
then present and analyze the experimental results.
Dataset: We use an MSN Search query log
(from 2006 Microsoft data release) with 15 million
queries, from US users, sampled over one month.
Queries with click frequency under 10 are discarded.
As a result, 84,703 unique queries remained, which
form our universe of queries. To separately analyze
queries with different frequencies, we divide the
queries into three groups: low frequency group (10-
100 clicks), medium frequency group (100-1000
clicks) and high frequency group (over 1000 clicks).
From each group, we draw a random sample of 50
queries for manual labeling, for the total of 150
queries. Each query was labeled by three members
of our lab. The inter-annotator agreeement was 85%,
and Cohen?s Kappa value was 0.77.
Table 2 reports the distribution of query classes in
our dataset. Note that low frequency queries dom-
inate, but are equally represented in the data sam-
ples used for classification training and prediction
(we will separately analyze performance on differ-
ent query frequency groups).
Results: Table 3 shows that best classification re-
quired User Entropy features. The Weka classifiers
were Naive Bayes (NB), Logistic Regression (Lo-
gistic), and Support Vector Machines (SVM).
1http://www.cs.waikato.ac.nz/ml/weka/
Clear Informational Ambiguous Frequency (%)
High 76% 8% 16% 255 (0.3%)
Medium 52% 20% 28% 3802 (4.5%)
Low 32% 46% 22% 80646 (95.2%)
Table 2: Frequency distribution of different query types
All Clear Informational Ambiguous
Ac. Pre. Rec. Pre. Rec. Pre. Rec.
All features
NB 0.72 0.90 0.85 0.77 0.54 0.42 0.61
Logistic 0.77 0.84 0.98 0.68 0.73 0.59 0.30
SVM 0.76 0.79 1.00 0.69 0.78 0.71 0.15
Without user entropy
NB 0.73 0.85 0.95 0.63 0.73 0.39 0.21
Logistic 0.73 0.84 0.95 0.63 0.68 0.47 0.27
SVM 0.74 0.79 1.00 0.65 0.76 0.50 0.09
Table 3: Classification performance by query type
High Mid Low
Ac. Ac. Ac. Pre. Rec.
All features
NB 0.76 0.76 0.74 0.80 0.74
Logistic 0.78 0.76 0.70 0.68 0.7
SVM 0.78 0.72 0.79 0.69 0.72
Without user entropy
NB 0.80 0.76 0.70 0.66 0.70
Logistic 0.80 0.82 0.66 0.63 0.66
SVM 0.80 0.78 0.68 0.62 0.68
Table 4: Classification performance by query frequency
Recall, that low frequency queries dominate our
dataset, so we focus on performance of low fre-
quency queries, as reported in Table 4. The respec-
tive ?2 values are reported in (Table 5). The features
UserDomainEntropy and UserEntropy correlate the
most with manual query intent labels.
As an alternative to direct multiclass classification
described above, we first classify clear vs. unclear
queries, and only then attempt to distinguish am-
biguous and informational queries (within the un-
363
Feature ?2 (multiclass) ?2 (binary)
UserDomainEntropy 132.9618 23.3629
UserEntropy 128.0111 21.6112
RelativeOverallEntropy 96.6842 20.0255
RelativeUserEntropy 98.6842 20.0255
OverallEntropy 96.1205 0
Table 5: ?2 values of top five features for multiclass clas-
sification (clear vs. informational vs. ambiguous) and for
and for binary classification (informational vs. ambigu-
ous), given the manual unclear label.
Overall Informational Ambiguous
Ac. Pre. Rec. Pre. Rec.
With User Entropy features
NB 0.72 0.82 0.60 0.65 0.85
Logistic 0.71 0.74 0.70 0.69 0.73
SVM 0.65 0.64 0.73 0.64 0.55
Without User Entropy features
NB 0.66 0.65 0.76 0.67 0.55
Logistic 0.68 0.69 0.73 0.68 0.64
SVM 0.68 0.67 0.81 0.72 0.55
Table 6: Binary classification performance for queries
manually labeled as unclear.
clear category). For classification between clear
and unclear queries, the accuracy was 90%, preci-
sion was 91%, and recall was 90%. The results for
subsequently classifying ambiguous vs. information
queries are reported in Table 6. For this task, User
Entropy features are beneficial, while the ?2 value or
Overall Entropy is 0, supporting our claim that User
Entropy is more useful for distinguishing informa-
tional from ambiguous queries.
Discussion: Interestingly, User Entropy does not
show a large effect on classification of High and
Medium frequency queries. However, as Table 2
indicates, High and Medium frequency queries are
largely clear (76% and 52%, respectively). As dis-
cussed above, User Entropy helps classify unclear
queries, but there are fewer such queries among
the High frequency group, which also tend to have
larger click entropy in general.
An ambiguous query is difficult to detect when
most users interpret it only one way. For instance,
query ?ako? was annotated as ambiguous, as it could
refer to different popular websites, such as the site
for Army Knowledge Online and the company site
for A.K.O., Inc. However, most users select the re-
sult for the Army Knowledge Online site, making
the overall entropy low, resulting in prediction as
a clear query. On the positive side, we find that
User Entropy helps detect ambiguous queries, such
as ?laguna beach?, which was labeled ambiguous as
it could refer to both a geographical location and a
popular MTV show. As a result, while the Overall
Entropy value of the clickthrough is high, the low
User Entropy value identifies the query as truly am-
biguous and not informational.
In summary, our techniques are of most help
for Low frequency queries and moderately helpful
for Medium frequency queries. These results are
promising, as Low frequency queries make up the
majority of queries processed by search engines, and
also contain the highest proportion of informational
queries, which our techniques can identify.
5 Conclusions
We explored clickthrough-based metrics for dis-
tinguishing between ambiguous and informational
queries - which, while exhibiting similar overall
clickthrough distributions, can be more accurately
identified by using our User Entropy-based features.
We demonstrated substantial improvements for low-
frequency queries, which are the most frequent in
query logs. Hence, our results are likely to have no-
ticeable impact in a real search setting.
Acknowledgments: This work was partially sup-
ported by grants from Yahoo! and Microsoft.
References
M. Komachi, S. Makimoto, K. Uchiumi, and M. Sassano.
2009. Learning semantic categories from clickthrough
logs. In Proc. of ACL-IJCNLP.
X. Li, Y.Y. Wang, and A.Acero. 2008. Learning query
intent from regularized click graphs. In SIGIR, pages
339?346.
Q. Mei and K. Church. 2008. Entropy of search logs:
how hard is search? with personalization? with back-
off? In Proc. of WSDM, pages 45?54.
D. E. Rose and D. Levinson. 2004. Understanding user
goals in web search. In Proc. of WWW, pages 13?19.
J. Teevan, S. T. Dumais, and D. J. Liebling. 2008. To per-
sonalize or not to personalize: modeling queries with
variation in user intent. In Proc. of SIGIR, pages 163?
170.
G. Zhu and G. Mishne. 2009. Mining rich session con-
text to improve web search. In Proc. of KDD, pages
1037?1046.
364
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 88?93,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
Towards Tracking Political Sentiment through Microblog Data
Yu Wang
Emory University
yu.wang@emory.edu
Tom Clark
Emory University
tclark7@emory.edu
Eugene Agichtein
Emory University
eugene@mathcs.emory.edu
Jeffrey Staton
Emory University
jkstato@emory.edu
Abstract
People express and amplify political opin-
ions in Microblogs such as Twitter, espe-
cially when major political decisions are
made. Twitter provides a useful vehicle for
capturing and tracking popular opinion on
burning issues of the day. In this paper,
we focus on tracking the changes in polit-
ical sentiment related to the U.S. Supreme
Court (SCOTUS) and its decisions, fo-
cusing on the key dimensions on support,
emotional intensity, and polarity. Mea-
suring changes in these sentiment dimen-
sions could be useful for social and politi-
cal scientists, policy makers, and the pub-
lic. This preliminary work adapts existing
sentiment analysis techniques to these new
dimensions and the specifics of the cor-
pus (Twitter). We illustrate the promise
of our work with an important case study
of tracking sentiment change building up
to, and immediately following one recent
landmark Supreme Court decision. This
example illustrates how our work could
help answer fundamental research ques-
tions in political science about the nature
of Supreme Court power and its capacity
to influence public discourse.
1 Background and Motivation
Political opinions are a popular topic in Mi-
croblogs. On June 26th, 2013, when the U.S.
Supreme Court announced the decision on the un-
constitutionality of the ?Defense of Marriage Act?
(DOMA), there were millions of Tweets about the
users? opinions of the decision. In their Tweets,
people not only voice their opinions about the is-
sues at stake, expressing different dimensions of
sentiment, such as support or opposition to the de-
cision, or anger or happiness. Thus, simply ap-
plying traditional sentiment analysis scales such
as ?positive? vs. ?negative? classification would
not be sufficient to understand the public reaction
to political decisions.
Research on mass opinion and the Supreme
Court is valuable as it could shed light on the fun-
damental and related normative concerns about the
role of constitutional review in American gover-
nance, which emerge in a political system possess-
ing democratic institutions at cross-purposes. One
line of thought, beginning with Dahl (Dahl, 1957),
suggests that the Supreme Court of the United
States has a unique capacity among major institu-
tions of American government to leverage its legit-
imacy in order to change mass opinion regarding
salient policies. If the Dahl?s hypothesis is correct,
then the Supreme Court?s same-sex marriage deci-
sions should have resulted in a measurable change
in opinion. A primary finding about implication of
Dahl?s hypothesis is that the Court is polarizing,
creating more supportive opinions of the policies
it reviews among those who supported the pol-
icy before the decision and more negative opin-
ions among those who opposed the policy prior to
the decision (Franklin and Kosaki, 1989) (Johnson
and Martin, 1998).
We consider Twitter as important example of
social expression of opinion. Recent studies of
content on Twitter have revealed that 85% of Twit-
ter content is related to spreading and commenting
on headline news (Kwak et al., 2010); when users
talk about commercial brands in their Tweets,
about 20% of them have personal sentiment in-
volved (Jansen et al., 2009). These statistical evi-
dences imply that Twitter has became a portal for
public to express opinions. In the context of pol-
itics, Twitter content, together with Twitter users?
88
information, such as user?s profile and social net-
work, have shown reasonable power of detecting
user?s political leaning (Conover et al., 2011) and
predicting elections (Tumasjan et al., 2010). Al-
though promising, the effectiveness of using Twit-
ter content to measure public political opinions re-
mains unclear. Several studies show limited corre-
lation between sentiment on Twitter and political
polls in elections (Mejova et al., 2013) (O?Connor
et al., 2010). Our study mainly focuses on inves-
tigating sentiment on Twitter about U.S. Supreme
Court decisions.
We propose more fine-grained dimensions for
political sentiment analysis, such as supportive-
ness, emotional intensity and polarity, allowing
political science researchers, policy makers, and
the public to better comprehend the public reaction
to major political issues of the day. As we describe
below, these different dimensions of discourse on
Twitter allows examination of the multiple ways in
which discourse changes when the Supreme Court
makes a decision on a given issue of public policy.
Our dimensions also open the door to new avenues
of theorizing about the nature of public discourse
on policy debates.
Although general sentiment analysis has made
significant advances over the last decade (Pang et
al., 2002) (Pang and Lee, 2008) (Liu, 2012) (Wil-
son et al., 2009), and with the focus on certain
aspects, such as intensity (Wilson et al., 2004),
irony detection (Carvalho et al., 2009) and sar-
casm detection (Davidov et al., 2010), analyzing
Microblog content such as Twitter remains a chal-
lenging research topic (Reyes et al., 2012) (Vanin
et al., 2013) (Agarwal et al., 2011). Unlike previ-
ous work, we introduce and focus on sentiment di-
mensions particularly important for political anal-
ysis of Microblog text, and extend and adapt clas-
sification techniques accordingly. To make the
data and sentiment analysis results accessible for
researchers in other domain, we build a website to
visualize the sentiment dynamics over time and let
users download the data. Users could also define
their own topics of interest and perform deeper
analysis with keyword filtering and geolocation
filtering.
We present a case study in which our results
might be used to answer core questions in polit-
ical science about the nature of Supreme Court
influence on public opinion. Political scientists
have long been concerned with whether and how
Supreme Court decisions affect public opinion and
discourse about political topics (Hoekstra, 2003)
(Johnson and Martin, 1998) (Gibson et al., 2003).
Survey research on the subject has been limited in
two ways. Survey analysis, including panel de-
signs, rely on estimates near but never on the date
of particular decisions. In addition, all survey-
based research relies on estimates derived from an
instrument designed to elicit sentiment ? survey
responses, useful as they are, do not reflect well
how public opinion is naturally expressed. Our
analysis allows for the examination of public opin-
ion as it is naturally expressed and in a way that is
precisely connected to the timing of decisions.
Next, we state the problem more formally, and
outline our approach and implementation.
2 Problem Statement and Approach
2.1 Political Sentiment Classification
We propose three refinements to sentiment analy-
sis to quantify political opinions. Specifically, we
pose the following dimensions as particularly im-
portant for politics:
? Support: Whether a Tweet is Opposed, Neu-
tral, or Supportive regarding the topic.
? Emotional Intensity: Whether a Tweet is
emotionally Intense or Dispassionate.
? Sentiment Polarity: Whether a Tweet?s tone
is Angry, Neutral, or Pleased.
2.2 Approach
In this work, each of the proposed measures is
treated as a supervised classification problem. We
use multi-class classification algorithms to model
Support and Sentiment Polarity, and binary classi-
fication for Emotional Intensity and Sarcasm. Sec-
tion 3.2 describes the labels used to train the super-
vised classification models. Notice some classes
are more interesting than the others. For exam-
ple, the trends or ratio of opposed vs. supportive
Microblogs are more informative than the factual
ones. Particularly, we pay more attention to the
classes of opposed, supportive, intense, angry, and
pleased.
2.3 Classifier Feature Groups
To classify the Microblog message into the classes
of interest, we develop 6 groups of features:
Popularity: Number of times the message has been
89
posted or favored by users. As for a Tweet, this
feature means number of Retweets and favorites.
Capitalization and Punctuation.
N-gram of text: Unigram, bigram, and trigram of
the message text.
Sentiment score: The maximum, minimum, aver-
age and sum of sentiment score of terms and each
Part-of-Speech tags in the message text.
Counter factuality and temporal compression dic-
tionary: This feature counts the number of times
such words appear in the message text.
Political dictionary: Number of times a political-
related word appears in the message text.
We compute sentiment scores based on Senti-
WordNet
1
, a sentiment dictionary constructed on
WordNet.
2
Political dictionary is built upon
political-related words in WordNet. As in this pa-
per, we construct a political dictionary with 56
words and phrases, such as ?liberal?, ?conserva-
tive?, and ?freedom? etc.
3 Case Study: DOMA
Our goal is to build and test classifiers that can dis-
tinguish political content between classes of inter-
est. Particularly, we focus on classifying Tweets
related to one of the most popular political topics,
?Defence of Marriage Act? or DOMA, as the tar-
get. The techniques can be easily generalized to
other political issues in Twitter.
3.1 Dataset
In order to obtain relevant Tweets, we use Twit-
ter streaming API to track representative key-
words which include ?DOMA?, ?gay marriage?,
?Prop8?, etc. We track all matched Tweets gen-
erated from June 16th to June 29th, immedi-
ately prior and subsequent to the DOMA decision,
which results in more than 40 thousand Tweets per
day on average.
3.2 Human Judgments
With more than 0.5 million potential DOMA rele-
vant Tweets collected, we randomly sampled 100
Tweets per day from June 16th to June 29th, and
1,400 Tweets were selected in total. Three re-
search assistants were trained and they showed
high agreement on assigning labels of relevance,
support, emotional intensity, and sentiment polar-
ity after training. Each Tweet in our samples was
1
http://sentiwordnet.isti.cnr.it/
2
http://wordnet.princeton.edu/
labeled by all three annotators. After the label-
ing, we first removed ?irrelevant? Tweets (if the
Tweet was assigned ?irrelevant? label by at least
one annotator), and then the tweets with no major
agreement among annotators on any of the senti-
ment dimensions were removed. As a result, 1,151
tweets with what we consider to be reliable labels
remained in our dataset (which we expect to share
with the research community).
3.2.1 Annotator Agreement
The Fleiss? Kappa agreement for each scale is re-
ported in Table 1 and shows that labelers have an
almost perfect agreement on relevance. Support,
emotional intensity, and sentiment polarity, show
either moderate or almost perfect agreement.
Measure Fleiss? Kappa
Relevance 0.93
Support 0.84
Intensity 0.54
Polarity 0.49
Table 1: Agreement (Fleiss? Kappa) of Human Labels.
3.3 Classification Performance Results
We reproduce the same feature types as previous
work and develop the political dictionary feature
for this particular task. We experimented with a
variety of automated classification algorithms, and
for this preliminary experiment report the perfor-
mance of Na??ve Bayes algorithm (simple, fast, and
shown to be surprisingly robust to classification
tasks with sparse and noisy training data). 10-fold
cross validation are performed to test the general-
izability of the classifiers. Table 2 reports the aver-
age precision, recall and accuracy for all measures.
Sarcasm is challenging to detect in part due to the
lack of positive instances. One goal in this study
is to build a model that captures trends among the
different classes. In Section 3.4, we will show that
the trends of different measures estimated by the
trained classifier align with the human annotated
ones over time.
3.4 Visualizing Sentiment Before and After
DOMA
One natural application of the automated politi-
cal sentiment analysis proposed in this paper is
tracking public sentiment around landmark U.S.
Supreme Court decisions. To provide a more re-
liable estimate, we apply our trained classifier on
all relevant Tweets in our collection. More than
90
Value Prec. (%) Rec. (%) Accuracy(%)
Supportive (48%) 73 74
Neutral (45%) 76 67 68
Opposed (7%) 17 30
Intense (31%) 56 60
73
Dispassionate (69%) 81 79
Pleased (10%) 48 31
Neutral (79%) 84 78 69
Angry (11%) 24 45
Table 2: Performance of Classifiers on Each Class.
2.5 million Tweets are estimated in four proposed
measures. Figure 1 shows the distribution of on-
topic Tweet count over time. The Supreme Court
decision triggered a huge wave of Tweets, and the
volume went down quickly since then.
0
100,000
200,000
300,000
400,000
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
Nu
mb
er o
f Tw
eet
s 
Date 
Figure 1: Number of ?Gay Marriage? Tweets Over Time.
Figures 2 and 3 visualize both the human la-
beled trends and the ones obtained by the classi-
fier for the classes ?Supportive? and ?Intense?. In
both figures, the peaks in the predicted labels gen-
erally align with the human-judged ones. We can
see the supportiveness and intensity are both rela-
tively high before the decision, and then they de-
cline gradually after the Supreme Court decision.
Figure 3 shows the volume of intensive Tweets
detected by our trained model has a burst on June
22rd, which is not captured by human labeled
data. To investigate this, we manually checked all
Tweets estimated as ?intensive? on June 22rd. It
turns out most of the Tweets are indeed intensive.
The reason of the burst is that one Tweet was heav-
ily retweeted on that day. We do not disclose the
actual tweet due to its offensive content.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
Human Labeled
Estimated
Figure 2: Percentage of ?Supportive? Tweets Over Time.
Figure 4 plots the trends of ?supportive? and
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
Human Labeled
Estimated
Figure 3: Percentage of ?Intense? Tweets Over Time.
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
16-Jun 19-Jun 22-Jun 25-Jun 28-Jun
% of 
Oppose
d Tw
eets
 
% o
f Suppo
rtive 
Twe
ets
 Supportive Opposed
Figure 4: Comparison between ?Supportive? and ?Op-
posed? Trends.
?opposed? Tweets in different scales. According
to the Supreme Court decision, the ?supportive?
group wins the debate. Interestingly, instead of
responding immediately, the ?loser? group react
and start Tweeting 2 days after the decision. These
trends indicate that ?winner? and ?loser? in the de-
bate react differently in time and intensity dimen-
sions.
We believe that our estimates of sentiment can
be used in various ways by political scientists.
The ?positivity bias? (Gibson and Caldeira, 2009)
model of Supreme Court opinion suggests that
the Court can move public opinion in the direc-
tion of its decisions. Our results possibly indicate
the opposite, the ?polarizing? model suggested by
(Franklin and Kosaki, 1989) and (Johnson and
Martin, 1998), where more negative opinions are
observed after the decision (in Figure 4), at least
for a short period. By learning and visualize polit-
ical sentiments, we could crystalize the nature of
the decision that influences the degree to which the
Supreme Court can move opinion in the direction
of its decisions.
4 An Open Platform for Sharing and
Analyzing Political Sentiments
Figure 5 shows a website
3
that visualizes politi-
cal sentiments over time. The website shows sev-
eral popular U.S. Supreme Court cases, such as
?gay marriage?, ?voting right act?, ?tax cases?,
3
http://www.courtometer.com
91
etc., and general topics, such as ?Supreme Court?
and ?Justices?. Each of the topics is represented
by a list of keywords developed by political sci-
ence experts. The keywords are also used to track
relevant Tweets through Twitter streaming API. To
let users go deeper in analyzing public opinions,
the website provides two types of real-time filter-
ing: keywords and location of Tweet authors. Af-
ter applying filters, a subset of matched Tweets are
generated as subtopics and their sentiments are vi-
sualized. The example filtering in Figure 5 shows
the process of creating subtopic ?voting right act?
out of a general topic ?Supreme Court? by using
keyword ?VRA?. We can see that the volume of
negative Tweets of ?voting right act? is higher than
the positive ones, compared to the overall senti-
ment of the general Supreme Court topic. Once an
interesting subtopic is found, users can download
the corresponding data and share with other users.
Topic ?Supreme Court? 
Nu
mb
er 
of 
Tw
eet
s 
Nu
mb
er 
of 
Tw
eet
s 
Subtopic ?Voting Right Act? 
Filtered by keyword: ?VRA? 
Figure 5: We build a website that visualizes political sen-
timents over time and let users create ?subtopics? by using
keyword and location filters.
5 Conclusions
In this paper we considered the problem of polit-
ical sentiment analysis. We refined the notion of
sentiment, as applicable to the political domain,
and explored the features needed to perform auto-
mated classification to these dimensions, on a real
corpus of tweets about one U.S. Supreme Court
case. We showed that our existing classifier can
already be useful for exploratory political analy-
sis, by comparing the predicted sentiment trends to
those derived from manual human judgments, and
then applying the classifier on a large sample of
tweets ? with the results providing additional ev-
idence for an important model of Supreme Court
opinion formation from political science.
This work provides an important step towards
robust sentiment analysis in the political domain,
and the data collected in our study is expected to
serve as a stepping stone for subsequent explo-
ration. In the future, we plan to refine and im-
prove the classification performance by exploring
additional features, in particular in the latent topic
space, and experimenting with other political sci-
ence topics.
ACKNOWLEDGMENTS The work of Yu Wang
and Eugene Agichtein was supported in part by
DARPA grants N11AP20012 and D11AP00269,
and by the Google Research Award.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
Analysis of Twitter Data. In Proceedings of the
Workshop on Language in Social Media (LSM).
Paula Carvalho, Lu??s Sarmento, M?ario J. Silva, and
Eug?enio de Oliveira. 2009. Clues for detecting
irony in user-generated contents: oh...!! it?s ?so
easy? ;-). In Proceedings of the 1st international
CIKM workshop on Topic-sentiment analysis for
mass opinion.
M.D. Conover, B. Goncalves, J. Ratkiewicz, A. Flam-
mini, and F. Menczer. 2011. Predicting the Political
Alignment of Twitter Users In Proceedings of IEEE
third international conference on social computing
Robert Dahl. 1957. Decision-Making in a Democracy:
The Supreme Court as National Policy-Maker. Jour-
nal of Public Law.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised Recognition of Sarcastic Sentences
in Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL).
Charles H. Franklin, and Liane C. Kosaki. 1989. Re-
publican Schoolmaster: The U.S. Supreme Court,
Public Opinion, and Abortion. The American Po-
litical Science Review.
James L Gibson, and Gregory A Caldeira. 2009. Cit-
izens, courts, and confirmations: Positivity theory
and the judgments of the American people. Prince-
ton University Press.
James L Gibson, Gregory A Caldeira, and Lester Keny-
atta Spence. 2003. Measuring Attitudes toward the
92
United States Supreme Court. American Journal of
Political Science.
Valerie Hoekstra. 2003. Public Reaction to Supreme
Court Decisions. Cambridge University Press.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Micro-blogging As Online Word
of Mouth Branding. in CHI ?09 Extended Abstracts
on Human Factors in Computing Systems.
Timothy R. Johnson, and Andrew D. Martin. 1998.
The Public?s Conditional Response to Supreme
Court Decisions. American Political Science Re-
view 92(2):299-309.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a Social Network
or a News Media?. in Proceedings of the 19th Inter-
national Conference on World Wide Web (WWW).
Yu-Ru Lin, Drew Margolin, Brian Keegan, and David
Lazer. 2013. Voices of Victory: A Computational
Focus Group Framework for Tracking Opinion Shift
in Real Time. In Proceedings of International World
Wide Web Conference (WWW).
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies.
Yelena Mejova, Padmini Srinivasan, and Bob Boynton.
2013. GOP Primary Season on Twitter: ?Popular?
Political Sentiment in Social Media. In Proceedings
of the Sixth ACM International Conference on Web
Search and Data Mining (WSDM).
B. O?Connor, R. Balasubramanyan, B. R. Routledge,
and N. A. Smith. 2010. From tweets to polls: Link-
ing text sentiment to public opinion time series. In
Proceedings of International AAAI Conference on
Weblogs and Social Media (ICWSM).
Bo Pang, and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Antonio Reyes, Paolo Rosso, and Tony Veale. 2012.
A multidimensional approach for detecting irony in
Twitter. Language Resources and Evaluation.
Swapna Somasundaran, Galileo Namata, Lise Getoor,
and Janyce Wiebe. 2009. Opinion Graphs for Po-
larity and Discourse Classification. TextGraphs-
4: Graph-based Methods for Natural Language Pro-
cessing.
Aline A. Vanin, Larissa A. Freitas, Re-nata Vieira, and
Marco Bochernitsan. 2013. Some clues on irony
detection in tweets. In Proceedings of International
World Wide Web Conference (WWW).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just how mad are you? Finding strong and
weak opinion clauses. In Proceedings of Conference
on Artificial Intelligence (AAAI).
Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
Elections with Twitter: What 140 Characters Re-
veal about Political Sentiment. In Proceedings of
the Fourth International AAAI Conference on We-
blogs and Social Media (ICWSM).
93
