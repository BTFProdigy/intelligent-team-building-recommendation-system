Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 74?78,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
The Effect of Cognitive Load on a Statistical Dialogue System
M. Gas?ic??, P. Tsiakoulis?, M. Henderson?, B. Thomson?, K. Yu?, E. Tzirkel?? and S. Young?
?Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{mg436, pt344, mh521, brmt2, ky219, sjy}@eng.cam.ac.uk
??General Motors Advanced Technical Centre, Israel
eli.tzirkel@gm.com
Abstract
In recent years statistical dialogue systems
have gained significant attention due to their
potential to be more robust to speech recogni-
tion errors. However, these systems must also
be robust to changes in user behaviour caused
by cognitive loading. In this paper, a statistical
dialogue system providing restaurant informa-
tion is evaluated in a set-up where the sub-
jects used a driving simulator whilst talking to
the system. The influences of cognitive load-
ing were investigated and some clear differ-
ences in behaviour were discovered. In partic-
ular, it was found that users chose to respond
to different system questions and use different
speaking styles, which indicate the need for an
incremental dialogue approach.
1 Introduction
A spoken dialogue system enables a user to obtain
information while using their hands to perform some
other task, which in many cases is the user?s primary
task. A typical example is an in-car spoken dialogue
system where the spoken interaction is secondary to
the main task of driving the car (Weng et al, 2004).
This domain is particularly challenging since it in-
volves dealing with the errors caused by the varying
noise levels and changes in user behaviour caused
by the cognitive load.
A statistical approach to dialogue modelling has
been proposed as a means of automatically optimis-
ing dialogue policies. In particular, the partially ob-
servable Markov decision process (POMDP) model
for dialogue provides a representation of varying
levels of uncertainty of the user input, yielding more
robust dialogue policies (Williams and Young, 2007;
Thomson and Young, 2010; Young et al, 2010).
Another thread of research deals with speech
interfaces for in-car applications, see (Baron and
Green, 2006) for a review. Past research has inves-
tigated the extent to which speaking is cognitively
less demanding than typing (Gartner et al, 2001;
Tsimhoni et al, 2004; Kun et al, 2007). In addi-
tion, considerable research has examined how driv-
ing safety is influenced by a dialogue system (Lai
et al, 2001; Lee et al, 2001; Nielsen et al, 2008).
However, to the best of our knowledge, little work
has been done to investigate the effect of the cog-
nitive load when interacting with a real conversa-
tional spoken dialogue system. The work presented
in (Mishra et al, 2004) suggests that the user speech
is more disfluent when the user is performing an-
other task. However, this work is based on a Wiz-
ard of Oz framework, where a human provides the
system?s responses. Also, a push-to-talk button was
used for every utterance which will have affected the
natural flow of the dialogue. It is important to know
if the change of cognitive load has an effect on the
speaking style and whether the system can alter its
behaviour to accommodate for this.
In this paper we try to answer these questions by
examining dialogues where users drove a car simu-
lator and talked to an open-microphone fully auto-
mated spoken dialogue system at the same time.
The rest of the paper is organised as follows. Sec-
tion 2 provides an overview of the dialogue system
used and section 3 describes the evaluation set-up.
The analysis of the results is given in Section 4. Sec-
tion 5 concludes the paper.
74
Table 1: Example dialogue task
You are looking for a cheap restaurant and it
should be in the east part of town. Make sure you
get the address of the venue.
2 System overview
The user speaks to the system, and the acoustic sig-
nal is converted by the speech recogniser into a set
of sentence hypotheses, which represents a proba-
bility distribution over all possible things that the
user might have said. The sentence hypotheses are
converted into an N-best list of dialogue acts by a
semantic decoder. Since the dialogue state cannot
be directly observed it maintains a probability dis-
tribution over all states, which is called the belief
state. The belief state is updated at every user turn
using Bayesian inference treating the input dialogue
acts as evidence. Based on belief state, the optimal
system act is selected using a policy and which is
trained automatically using reinforcement learning.
The abstract system dialogue act is converted to an
appropriate utterance by a natural language genera-
tor and then converted to speech by an HMM-based
speech synthesiser. To enable in-car speech inter-
action via mobile phone, a VoIP interface is imple-
mented. The domain is Cambridge restaurant infor-
mation with a database of about 150 venues and 7
slots that users can query.
3 Evaluation set-up
Our goal is to understand system performance
when driving. However, due to the safety restric-
tions, performance was tested using a driving simu-
lator. The following sections explain the set-up.
3.1 Car simulator
The car simulator used in the evaluation was the
same as in (Davies and Robinson, 2011). It con-
sists of a seat, a steering wheel and pedals, which
give a realistic cab-like environment for the par-
ticipants. There is also a projection screen which
largely fills the visual field of the driver. The sim-
ulation software is a modified version of Rockstar
Games? Grand Theft Auto: San Andreas, with over
500 km of roads. For the purpose of the evaluation,
the subjects were asked to drive on the main motor-
way, to keep the lane and not to drive over 70mph.
3.2 Subjects
For the study 28 subjects were recruited, 22 where
native speakers. Each subject had to complete three
scenarios: (1) to drive the car simulator for 10 min-
utes, (2) to talk to the system for 7 dialogues and (3)
to talk to the system for 7 dialogues while driving.
The scenarios were in counter-balanced order.
While they were driving, the speed and the road
position were recorded. If the scenario involved
talking to the system, the instructor read out the di-
alogue task (see an example in Table 1) and dialled
the phone number. In addition, the subject had the
dialogue task displayed on a small screen next to the
driving wheel. The subject talked to the system us-
ing loud speaker mode on the mobile phone.
4 Results
To examine the influence of cognitive load, the
following examinations were performed. First, we
investigate if the subjects felt any change in the cog-
nitive load (Section 4.1). Then, in Section 4.2, we
examine how the driving was influenced by the sub-
jects talking to the system. Finally, we investigate
how successfully the subjects were able to complete
the dialogue tasks while driving (Section 4.3). This
is followed with an examination of the conversa-
tional patterns that occurred when the subjects were
driving whilst talking to the system (Section 4.4).
4.1 Cognitive load
After each scenario the subjects were asked to an-
swer five questions based on the NASA-TLX self-
reporting scheme for workload measurement. They
answered by providing a rating from 1 (very easy)
to 5 (very hard). The averaged results are given
in Table 2. We performed a Kruskal test, followed
by pairwise comparisons for every scenario for each
answer and all differences are statistically signifi-
cant (p < 0.03) apart from the differences in the
frustration, the stress and the pace between talking
and talking and driving. This means that they were
clearly able to feel the change in cognitive load.
75
Table 2: Subjective evaluation of the cognitive load
Driving Talking Talking&Driving
How mentally demanding was the scenario?
1.61 2.21 2.89
How hurried was the pace of the scenario?
1.21 1.71 1.89
How hard did you have to work?
1.5 2.32 2.96
How frustrated did you feel during the task?
1.29 2.61 2.61
How stressed did you feel during the task?
1.29 2.0 2.32
Table 3: Analysis of driving speed to determine which
measures are larger for Talking&Driving than Driving
Measure Percentage of
users
Confidence in-
terval
Higher speed 8% [1%, 25%]
Larger std.dev 77% [56%, 91%]
Larger entropy 85% [65%, 95%]
4.2 Driving performance
For 26 subjects we recorded position on the road
and the speed. Since these measurements vary sig-
nificantly across the subjects, for each subject we
calculated the average speed, the standard deviation
and the entropy and similarly for the average posi-
tion in the lane. For the speed, we computed how
many subjects had a higher average speed when they
were talking and driving versus when they were just
talking and similarly for the standard deviation and
the entropy. The results are given in Table 3. It
can be seen that the user?s speed is lower when they
are driving and talking, however, the increase in the
standard deviation and the entropy suggest that their
driving is more erratic. No significant differences
were observed for the road position.
4.3 Dialogue task completion
Each participant performed 14 dialogues, 7 for each
scenario. In total, there were 196 dialogues per sce-
nario. After each dialogue they told the instruc-
tor if they thought the dialogue was successful, and
this information was used to compute the subjective
Table 4: Subjective and Objective Task completion (196
Dialogues per scenario)
Talking Talking&Driving
Subjective 78.6% 74.0%
Objective 68.4% 64.8%
Table 5: Percentage of turns that are in line with the pre-
defined task
Talking Talking&Driving
Percentage of turns
that follow the task
98.3% 96.79%
Number of turns 1354 1388
completion rate. In addition, all dialogues were tran-
scribed and analysed to see if the system provided
information the user asked for and hence calculate
an objective completion rate. The results are given
in Table 4. These differences are not statistically sig-
nificant due to the small sample size. However, it
can be seen that the trend is that the dialogues where
the subject was not performing another task at the
same time were more successful. Also, it is inter-
esting that the subjective scores are higher than the
objective ones. This can be explained by the fact that
the dialogue tasks were predefined and the subjects
do not always pay sufficient attention to their task
descriptions.
4.4 Conversational patterns
Given that the subjects felt the change of cognitive
load when they were talking to the system and op-
erating the car simulator at the same time, we were
interested to see if there are any changes in the dia-
logues which might suggest this.
First, we examine how well they follow the given
task on a turn-to-turn basis. For example, if the task
is to find a cheap restaurant and if at some point
in the dialogue the user says I?d like an expensive
restaurant that turn is not consistent with the task.
The results are given in Table 5 and they are statisti-
cally significant (p < 0.01).
We then examine the number of contradictions on
a turn-to-turn basis. For example, if the user says I?d
like a cheap restaurant and later on they say I?d like
76
Table 6: User obedience to system questions
1. system requests or confirms and requests
Samples Obedience
Talking 392 67.6%
Talking&Driving 390 63.9%
2. system confirms
Samples Obedience
Talking 91 73.6%
Talking&Driving 92 81.5%
an expensive restaurant the latter turn is clearly a
contradiction. The percentage of contradicting turns
is less than 1% and the difference between the sce-
narios is not statistically significant. This suggests
that while users tend to forget the task they are given
when they are driving, they still act rationally despite
the increase in the cognitive load.
The next analysis concerns the user obedience,
i.e. the extent to which subjects answer the sys-
tem questions. We grouped the system questions in
two classes. The first class represents the questions
where the system requests a value for a particular
slot, for instance What part of town are you looking
for? and the questions where the system confirms
and requests at the same time, for instance You are
looking for a cheap restaurant. What part of town
are you looking for? The second class correspond to
system confirmations, for example Did you say you
are looking for a cheap restaurant? The percent-
age of the obedient user turns per class is given in
Table 6. Due to the small sample size these results
are not statistically significant. Still, it is interest-
ing to see that when driving the subjects appear to
be more obedient to the system confirmations than
when they are just talking. When the system makes
a confirmation, the user can answer with simple yes
or no, whereas when the system requests the value
of a particular slot, the user needs to think more to
provide an answer.
The number of barge-ins, the number of filler
words and the average speech intensity vary con-
siderably among the subjects. Therefore, we aver-
age these statistics per user and examine the number
of users for which the particular measure is greater
for the scenario where they talked to the system and
drove the simulator at the same time. The results
Table 7: Analysis of measures related to the speaking
style which values are larger for Talking&Driving than
Talking
Measure % of users Conf. interval
More barge-ins 87% [69%, 96%]
More fillers 73% [54%, 88%]
Higher intensity 67% [47%, 83%]
(Table 7) show that the number of barge-ins and the
number of fillers is significantly greater for the sce-
nario when they are talking and driving and the in-
tensity on average tend to be greater.
5 Conclusion and Future work
There are several important observations arising
from this study. Firstly, dialogues with cognitively
loaded users tend to be less successful. This sug-
gests that the system should alter its behaviour to
match user behaviour and alleviate the cognitive
load in order to maintain the level of performance.
This necessitates rapid on-line adaptation of dia-
logue policies.
The second observation is that cognitively loaded
users tend to respond to some types of system ques-
tions more than others. This indicates that the user
model within a POMDP dialogue system should be
conditioned on a measure of cognitive load.
Finally, this study has found that users barge-in
and use filler words significantly more often when
they are cognitively loaded. This suggests the need
for a much richer turn-taking model which allows
the system to use back-channels and barge-in when
the user hesitates. An obvious candidate is the in-
cremental approach (Schlangen and Skantze, 2009;
DeVault et al, 2009) which allows the system to pro-
cess partial user inputs, back-channels, predict short
term user input and interrupt the user during hesita-
tions. While incremental dialogue is a growing area
of study, it has not so far been examined in the con-
text of dialogue for secondary tasks. We signpost
this as an important area for future work.
Acknowledgments
We would like to thank to Peter Robinson and Ian
Davies for their help with the experiments.
77
References
A Baron and P Green. 2006. Safety and Usability of
Speech Interfaces for In-Vehicle Tasks while Driving:
A Brief Literature Review. Technical Report UMTRI-
2006-5.
I Davies and P Robinson. 2011. Emotional investment
in naturalistic data collection. In International Con-
ference on Affective Computing and Intelligent Inter-
action.
D DeVault, K Sagae, and DR Traum. 2009. Can I fin-
ish? Learning when to respond to incremental inter-
pretation results in interactive dialogue. In 10th An-
nual SIGDIAL meeting on Discourse and Dialogue.
U Gartner, W Konig, and T Wittig. 2001. Evaluation of
Manual vs. Speech Input When Using a Driver Infor-
mation System in Real Traffic. In International Driv-
ing Symposium on Human Factors in Driving Assess-
ment, Training and Vehicle Design.
A Kun, T Paek, and Z? Medenica. 2007. The effect of
speech interface accuracy on driving performance. In
Interspeech.
J Lai, K Cheng, P Green, and O Tsimhoni. 2001. On the
Road and on the Web? Comprehension of synthetic
and human speech while driving. In SIGCHI.
JD Lee, B Caven, S Haake, and TL Brown. 2001.
Speech-based Interaction with In-vehicle Computers:
The Effect of Speech-based E-mail on Drivers? Atten-
tion to the Roadway. Human Factors, 43:631?640.
R Mishra, E Shriberg, S Upson, J Chen, F Weng, S Pe-
ters, L Cavedon, J Niekrasz, H Cheng, and H Bratt.
2004. A wizard of Oz framework for collecting spo-
ken human-computer dialogs. In Interspeech.
BS Nielsen, B Harsham, B Raj, and C Forlines. 2008.
Speech-Based UI Design for the Automobile. Hand-
book of Research on User Interface Design and Eval-
uation for Mobile Technology, pages 237?252.
David Schlangen and Gabriel Skantze. 2009. A general,
abstract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL ?09, pages 710?718.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken di-
alogue systems. Computer Speech and Language,
24(4):562?588.
O Tsimhoni, D Smith, and P Green. 2004. Address Entry
While Driving: Speech Recognition Versus a Touch-
Screen Keyboard. Human Factors, 46:600?610.
F Weng, L Cavedon, B Raghunathan, D Mirkovic,
H Cheng, H Schmidt, H Bratt, R Mishra, S Peters,
L Zhao, S Upson, E Shriberg, and C Bergmann. 2004.
Developing a conversational dialogue system for cog-
nitively overloaded users. In Proceedings of the Inter-
national Congress on Intelligent Transportation Sys-
tems.
JD Williams and SJ Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2010. The Hidden Information
State Model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language, 24(2):150?174.
78
Proceedings of the SIGDIAL 2013 Conference, pages 214?222,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
POMDP-based dialogue manager adaptation to extended domains
M. Gas?ic?, C. Breslin, M. Henderson, D. Kim, M. Szummer, B. Thomson, P. Tsiakoulis and S. Young
Cambridge University Engineering Department
{mg436,cb404,mh521,dk449,mos25,brmt2,pt344,sjy}@eng.cam.ac.uk
Abstract
Existing spoken dialogue systems are typ-
ically designed to operate in a static and
well-defined domain, and are not well
suited to tasks in which the concepts and
values change dynamically. To handle dy-
namically changing domains, techniques
will be needed to transfer and reuse ex-
isting dialogue policies and rapidly adapt
them using a small number of dialogues in
the new domain. As a first step in this di-
rection, this paper addresses the problem
of automatically extending a dialogue sys-
tem to include a new previously unseen
concept (or slot) which can be then used
as a search constraint in an information
query. The paper shows that in the con-
text of Gaussian process POMDP optimi-
sation, a domain can be extended through
a simple expansion of the kernel and then
rapidly adapted. As well as being much
quicker, adaptation rather than retraining
from scratch is shown to avoid subjecting
users to unacceptably poor performance
during the learning stage.
1 Introduction
Existing spoken dialogue systems are typically de-
signed to operate in a static and well-defined do-
main, and are not well suited to tasks in which
the concepts and values change dynamically. For
example, consider a spoken dialogue system in-
stalled in a car, which is designed to provide in-
formation about nearby hotels and restaurants. In
this case, not only will the data change as the
car moves around, but the concepts (or slots) that
a user might wish to use to frame a query will
also change. For example, a restaurant system de-
signed to be used within cities might not have the
concept of ?al fresco? dining and could not there-
fore handle a query such as ?Find me a French
restaurant where I can eat outside?. In order to
make this possible, techniques will be needed to
extend and adapt existing dialogue policies.
Adaptation can be viewed as a process of im-
proving action selection in a different condition to
the one in which the policy was originally trained.
While adaptation has been extensively studied in
speech recognition (see an overview in (Gales and
Young, 2007)), in spoken dialogue systems it is
still relatively novel and covers a wide range of
possible research topics (Litman and Pan, 1999;
Litman and Pan, 2002; Georgila and Lemon, 2004;
Janarthanam and Lemon, 2010).
A recent trend in statistical dialogue modelling
has been to model dialogue as a partially ob-
servable Markov decision process (POMDP). This
provides increased robustness to errors in speech
understanding and automatic dialogue policy op-
timisation via reinforcement learning (Roy et al,
2000; Zhang et al, 2001; Williams and Young,
2007; Young et al, 2010; Thomson and Young,
2010). A POMDP-based dialogue manager main-
tains a distribution over every possible dialogue
state at every dialogue turn. This is called the
belief state. Based on that distribution the sys-
tem chooses the action that gives the highest ex-
pected reward, measured by the Q-function. The
Q-function for a belief state and an action is the
expected cumulative reward that can be obtained
if that action is taken in that belief state. The opti-
misation typically requires O(105) to O(106) di-
alogues, so is normally done in interaction with a
simulated user (Jurc???c?ek et al, 2011b).
In reinforcement learning, policy adaptation has
been addressed in the context of transfer learn-
ing (Taylor and Stone, 2009). The core idea is to
exploit expertise gained in one domain (source do-
main) to improve learning in another domain (tar-
get domain). A number of techniques have been
developed but they have not been previously ap-
plied to dialogue management.
214
Gaussian process (GP) based reinforcement
learning (Engel, 2005) has been recently applied
to POMDP dialogue policy optimisation in or-
der to exploit the correlations between different
belief states and thus reduce the number of dia-
logues needed for the learning process (Gas?ic? et
al., 2010).
An important feature of a Gaussian process is
that it can incorporate a prior mean and variance
for the function it estimates, in this case the Q-
function. Setting these appropriately can signif-
icantly speed up the process of learning. If the
mean or the variance are estimated in one envi-
ronment, for example a particular user type or a
particular domain, they can be used as a prior for
adaptation in a different environment, i.e. another
user type or another domain. A Gaussian process
does not depend on the belief state but on the cor-
relation between two belief states encoded by the
kernel function. Therefore, if one defines a kernel
function for two belief states in one domain, the
policy can be used in a different domain, provided
that the correlations between belief states follow a
similar pattern.
This paper explores the problem of extending an
existing domain by introducing a previously un-
seen slot. Specifically, a simple restaurant system
is considered which allows a user to search for
restaurants based on food-type and area. This do-
main is then extended by introducing an additional
price-range slot. The policy is trained for the basic
two-slot domain and then reused in the extended
domain by defining a modified kernel function and
using adaptation. This strategy not only allows for
the knowledge of a previously trained policy to be
reused but it also guards against poor performance
in the early stages of learning. This is particularly
useful in a real-world situation where the adapta-
tion is performed in direct interaction with users.
In addition, a potential application of this tech-
nique to reduce the number of training dialogues
is examined. The domain is decomposed into a
series of simple domains and the policy is grad-
ually adapted to the final domain with a smaller
number of dialogues than are normally needed for
training.
The rest of the paper is organised as follows. In
Section 2 the background on Gaussian processes
in POMDP optimisation is given. Then Section 3
gives a description of the Bayesian Update of Di-
alogue State dialogue manager, which is used as
a test-bed for the experiments. In Section 4, a
simple method of kernel modification is described
which allows a policy trained in the basic domain
to be used in an extended domain. Methods of
fast adaptation are investigated in Section 5 and
this adaptation strategy is then tested via interac-
tion with humans using the Amazon Mechanical
Turk service in Section 6. Finally, the use of re-
peated adaptation to speed up the process of policy
optimisation by learning gradually from simple to
more complex domains is explored in Section 7,
before presenting conclusions in Section 8.
2 Gaussian processes in POMDPs
The role of a dialogue policy pi is to map each be-
lief state b ? B into an action a ? A so as to
maximise the expected cumulative reward, a mea-
sure of how good the dialogue is.
The expected cumulative reward is defined by
the Q-function as:
Q(b, a) = Epi
( T?
?=t+1
???t?1r? |bt = b, at = a
)
,
(1)
where r? is the reward obtained at time ? , T is
the dialogue length and ? is the discount factor,
0 < ? ? 1. Optimising the Q-function is then
equivalent to optimising the policy pi.
A Gaussian process (GP) is a non-parametric
Bayesian probabilistic model that can be used
for function regression (Rasmussen and Williams,
2005). It is fully defined by a mean and a kernel
function which defines prior function correlations.
GP-Sarsa is an on-line RL algorithm that mod-
els the Q-function as a Gaussian process (Engel
et al, 2005), Q(b, a) ? GP (0, k((b, a), (b, a)))
where the kernel k(?, ?) is factored into separate
kernels over the belief state and action spaces
kC(b,b?)kA(a, a?). For a sequence of belief state-
action pairs Bt = [(b0, a0), . . . , (bt, at)]T visited
in a dialogue and the corresponding observed im-
mediate rewards rt = [r1, . . . , rt]T, the posterior
of the Q-function for any belief state-action pair
(b, a) is defined by the following:
215
Q(b, a)|rt,Bt ? N (Q(b, a), cov((b, a), (b, a))),
Q(b, a) = kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1rt,
cov((b, a), (b, a)) = k((b, a), (b, a))? kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1Htkt(b, a)
Ht =
?
????
1 ?? ? ? ? 0 0
0 1 ? ? ? 0 0
... . . . . . . ... ...
0 ? ? ? 0 1 ??
?
???? ,
kt(b, a) = [k((b0, a0), (b, a)), . . . , k((bt, at), (b, a))]T,
Kt = [kt((b0, a0)), . . . ,kt((bt, at))]
(2)
where Kt is the Gram matrix ? the matrix of the
kernel function values for visited points Bt, Ht is
a linear operator that captures the reward looka-
head from the Q-function (see Eq. 1) and ?2 is
an additive noise parameter which controls how
much variability in theQ-function estimate we ex-
pect during the process of learning.
If we assume that the Gaussian process
places a prior mean on the Q-function,
Q(b, a) ? GP (m(b, a), k((b, a), (b, a)))
then the posterior mean Q(b, a) is given by (Ras-
mussen and Williams, 2005):
Q(b, a) = m(b, a) + kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1(rt ?mt), (3)
where mt = [m(b0, a0), . . . ,m(bt, at)]T. The
estimate of the variance is same as in Eq. 2.
The Q-function posterior in Eqs. 2 and 3
defines a Gaussian distribution for every be-
lief state-action pair. Thus, when a new be-
lief state b is encountered, for each action a ?
A, there is a Gaussian distribution Q(b, a) ?
N (Q(b, a), cov((b, a), (b, a)))). Sampling from
these Gaussian distributions gives a set of Q-
values for each action {Q(b, a) : a ? A} from
which the action with the highest sampledQ-value
can be selected:
pi(b) = argmax
a
{Q(b, a) : a ? A} . (4)
In this way, the stochastic model of theQ-function
is effectively transformed into a stochastic policy
model, which can be optimised to maximise the re-
ward (Geist and Pietquin, 2011; Gas?ic? et al, 2011;
Gas?ic? et al, 2012).
Due to the matrix inversion in Eq. 2, the compu-
tational complexity of calculating the Q-function
posterior is O(t3), where t is the number of data
points in Bt, and this poses a serious computa-
tional problem. The algorithm used here to ap-
proximate the Gaussian process is the kernel span
sparsification method described in (Engel, 2005).
In this case, only a set of representative data points
is retained ? called the dictionary of visited points.
3 BUDS dialogue manager
The Bayesian Update of Dialogue State (BUDS)
dialogue manager is a POMDP-based dialogue
manager (Thomson and Young, 2010) which fac-
torises the dialogue state into conditionally de-
pendent elements. These elements are arranged
into a dynamic Bayesian network, which allows
for their marginal probability distributions to be
updated during the dialogue. Thus, the belief
state of the BUDS dialogue manager consists of
the marginal posterior probability distribution over
hidden nodes in the Bayesian network. The hidden
nodes in the BUDS system consist of the history
nodes and the goal nodes for each concept in the
dialogue. For instance in a restaurant information
domain these include area, food-type, address.
The history nodes define possible dialogue histo-
ries for a particular concept, eg. system-informed,
user-requested. The goal nodes define possible
values for a particular concept, eg. Chinese, In-
dian. The role of the policy pi is then to map each
216
belief state into a summary action a from the sum-
mary action space A. Once a summary action is
found it is heuristically mapped into the master
action that the system finally takes (Gas?ic? et al,
2012). The master actions are composed of dia-
logue act type and list of slot value pairs. There are
15 dialogue act types in the BUDS system that fa-
cilitate not only simple information providing sce-
narios but also more complex dialogues where the
user can change their mind and ask for alterna-
tives.
To apply GP policy optimisation, a kernel func-
tion must be defined on both the belief state space
B and the action space A. The kernel function
over the belief state b is constructed from the sum
of individual kernels over the hidden node distri-
butions, such that the kernel function of two cor-
responding nodes is based on the expected likeli-
hood kernel (Jebara et al, 2004), which is also a
simple linear inner product:
kB(b,b?) =
?
h
?bh,b?h?, (5)
where bh is the probability distribution encoded
in the hth hidden node. This kernel gives the ex-
pectation of one belief state distribution under the
other.
For history nodes, the kernel is a simple inner
product between the corresponding node distribu-
tions. While it is possible to calculate the kernel
function for the goal nodes in the same way as for
the history nodes, in this case, the choice of sys-
tem action, such as confirm or inform, does not
depend on the actual values. It rather depends on
the shape of the distribution and, in particular, it
depends on the probability of the most likely value
compared to the rest. Therefore, to exploit the cor-
relations further, the kernel over two goal nodes
is calculated as the dot product of vectors, where
each vector represents the corresponding distribu-
tion sorted into order of probability. The only ex-
ceptions are the goal for the method node and the
discourse act node. The former defines whether
the user is searching for a venue by name or by
constraints and the latter defines which discourse
act the user used, eg. acknowledgement, thank you.
Their kernels are calculated in the same way as for
the history nodes.
For the action space kernel, the ?-kernel is used
defined by:
kA(a, a?) = ?a(a?). (6)
where ?a(a?) = 1 iff a = a?.
3.1 TopTable domain
The TopTable domain consists of restaurants in
Cambridge, UK automatically extracted from the
TopTable web service (TopTable, 2012). There are
about 150 restaurants and each restaurant has 7 at-
tributes ? slots. This results in a belief space that
consists of 25 concepts where each concept takes
from 3 to 150 values and each value has a proba-
bility in [0, 1]. The summary action space consists
of 16 summary actions.
3.2 The agenda-based simulated user
In training and testing a simulated user was used.
The agenda-based user simulator (Schatzmann,
2008; Keizer et al, 2010) factorises the user state
into an agenda and a goal. The goal ensures
that the user simulator exhibits consistent, goal-
directed behaviour. The role of the agenda is to
elicit the dialogue acts that are needed for the user
simulator to fulfil the goal. In addition, an er-
ror model adds confusions to the simulated user
input such that it resembles those found in real
data (Thomson et al, 2012). The length of the N-
best list was set to 10 and the confusion rate was
set to 15% during training and testing.1 This error
rate means that 15% of time the true hypothesis is
not in the N-best list. Intermediate experimenta-
tion showed that these confusion rates are typical
of real data.
The reward function was set to give a reward
of 20 for successful dialogues, zero otherwise. In
addition, 1 is deducted for each dialogue turn to
encourage shorter dialogues. The discount factor
? is set to 1 and the dialogue length is limited to
30 turns.
4 Extended domains
Transfer learning is a reinforcement learning tech-
nique which address three problems:
? given a target domain, how to select the
most appropriate source domain from a set of
source domains,
? given a target and a source domain how to
find the relationship between them, and
? given a target and a source domain and the
relationship between them, how to effectively
transfer knowledge between them.
1Except of course where the system is explicitly tested on
varying noise levels.
217
Here we assume that we are given a source and
a target domain and that the relationship between
them is defined by mapping the kernel function.
Knowledge transfer is then effected by adapting
the source domain policy for use in the target do-
main. For the latter, two forms of adaptation are
investigated: one simply continues to update the
set of source data dictionary points with new dic-
tionary points, the second uses the source domain
posterior as a prior for the new target domain.
In this case, the source is a basic restaurant do-
main with slots name, area, food-type, phone, ad-
dress, and postcode. The extended target domain
has an additional price-range slot. We are inter-
ested primarily in training the policy on the ba-
sic domain and testing it on the extended domain.
However, since real applications may also require
a slot to be forgotten, we also investigate the re-
verse where the policy is trained in the extended
domain and tested on the basic domain.
In order to enable the required cross domain
portability, a kernel function defining the correla-
tion between belief states from differing domains
is needed. Since the extended domain has an ex-
tra slot and thus extra hidden nodes, we need to
define the correlations between the extra hidden
nodes and the hidden nodes in the belief state of
the basic domain. This can be performed in vari-
ous ways, but the simplest approach is to specify
which slot from the basic domain is most similar
to the new slot in the extended domain and then
match their corresponding hidden nodes. In that
way the belief state kernel function between two
belief states bB, bE for the basic B and the ex-
tended E domain becomes:
kB(bB,bE) =
?
h?B
?bBh ,bEh?+
?
e/?B
?bBl(e),bEe ?, (7)
where h are the hidden nodes in the basic domain,
e are the hidden nodes in the extended domain and
function l : E? B for each hidden node that does
not exist in the basic domain finds its appropriate
replacement. In the particular case studied here,
the slot area is most similar to the new price-range
slot since they both have a relatively small number
of values, about 5. Hence, l(price-range)? area.
If the cardinality of the mapped slots differ, the
shorter is padded with zeros though other forms of
normalisation are clearly possible.
The (summary) action space for the extended
domain has more actions than the basic domain.
For example, one action that exists in the extended
domain and does not exist in the basic domain is
request(price-range). To define the kernel func-
tion between these sets of actions, one can specify
for each extra action in the extended domain its
most similar action in the basic domain:
kA(aB, aE) =
{
?aB(aE) aE ? AB,
?aB(L(aE)) aE /? AB,
(8)
where function L : AE ? AB for each action
that does not exist in the basic domain finds its
replacement action.
Functions L and l are here defined manually.
However, a simple but effective heuristic would be
to find for each new slot in the extended domain, a
slot in the basic domain with similar cardinality.
Porting in the reverse direction from the ex-
tended to the basic domain is easier since one can
simply disregard the extra hidden nodes and ac-
tions in the kernel calculation.
To experimentally examine the extent to which
this method supports cross domain portability, we
trained policies for both domains until conver-
gence, using 105 dialogues on the simulated user.
We then cross tested them on the mismatching do-
mains at varying user input error rates. The results
are given in Fig. 1.
0 10 20 30 40 50ErrorRate2
0
2
4
6
8
10
12
Rewa
rd
bsc-trn&tstextd-trn&tstextd-trn&bsc-tstbsc-trn&extd-tst
Figure 1: Cross testing policies trained on differ-
ent domains. bsc refers to the basic domain, extd is
the extended domain, trn is training and tst is test-
ing.
From the results it can be seen that the policy
trained for the basic domain has a better perfor-
mance than the policy trained on the extended do-
main, when tested on the matching domain (com-
218
pare bsc-trn&tst with extd-trn&tst). The extended do-
main has more slots so it is more difficult for the
system to fulfil the user request, especially in noisy
conditions. Secondly, the performance of the pol-
icy trained on the extended domain and tested on
the basic domain is close to optimal (compare bsc-
trn&tst with extd-trn&bsc-tst). However, the pol-
icy trained on the basic domain and tested on the
extended domain has much worse performance
(compare bsc-trn&extd-tst with extd-trn&tst). It is
hard for the policy to adequately extrapolate from
the basic to the extended domain. This difference
in performance, however, motivates the need for
adaptation and this is investigated in the next sec-
tion.
5 Adaptation
Adaptation of a policy trained on one domain to
another can be performed in several ways. Here
we examine two adaptation strategies similar to
the method described in (Taylor et al, 2007),
where every action-value for each state in the tar-
get domain is initialised with learned source do-
main values.
The first strategy is to take the policy trained in
the source domain and simply continue training it
in the target domain until convergence. In Gaus-
sian process reinforcement learning, this means
that we assume a zero-mean prior on the Gaussian
process for theQ-function and let the dictionary of
visited points Bt from Eq. 2 consist of both points
visited in the source domain and the extended tar-
get domain, making sure that the Gram matrix
Kt uses extended domain kernel function where
necessary. However, the estimate of the variance
decreases with the number of visited points (see
Eq. 2). The danger therefore when performing
adaptation in this way is that the estimate of vari-
ances obtained in the source domain will be very
small since the policy has already been trained un-
til convergence with a large number of dialogues.
As a consequence, the rate of exploration defined
by sampling in Eq. 4 will be reduced and thus lead
to the subsequent optimisation in the new target
domain falling prematurely into a local optimum.
As an alternative, we propose another adapta-
tion strategy. The estimate of the posterior of the
mean for the Q-function, Q in Eq. 2, from the pol-
icy trained on the basic domain can be taken to be
the prior of the mean when the policy is trained on
the extended domain as in Eq. 3. More precisely, if
Qbsc is the posterior mean of the policy trained on
the basic domain then mextd = Qbsc. In this case
it is also important to make sure that the kernel
function used to calculateQbsc is redefined for the
extended domain where necessary. The prior on
the variance is the original kernel function renor-
malised:
k((b, a), (b?, a?))? k((b,a),(b?,a?))?
k((b,a),(b,a))k((b?,a?),(b?,a?))
.
(9)
Given that the estimate of the mean provides rea-
sonable performance, it is not necessary to place
a flat prior on the variance of the Q-function and
therefore the kernel is normalised as in Eq. 9.
When comparing adaptation strategies, we are
interested in two aspects of performance. The first
is the performance of the policy during training.
The second is how quickly the policy reaches the
optimal performance. For that reason we adopt
the following evaluation scheme. After every 100
adaptation dialogues we test the partially opti-
mised policy with 1000 simulated dialogues, dif-
ferent to the ones used in adaptation. These 1000
dialogues are the same for every test point on the
graph. The results are given in Fig. 2.
0 200 400 600 800 1000 1200 1400 1600Training dialogues20
15
10
5
0
5
10
Rewa
rd
PRIORADAPTTRAINbsc-trn&extd-tstextd-trn&tst
Figure 2: Different adaptation strategies
The lower horizontal line represents the perfor-
mance of the policy trained on the basic source
domain and tested on the extended target domain.
This is the baseline. The upper horizontal line
represents the policy trained until convergence on
the extended domain and also tested on the ex-
tended domain. This provides the gold standard.
The adaptation strategy that takes both the mean
and variance of the policy trained on the basic do-
main and retrains the policy on the extended do-
219
main is denoted as ADAPT in Fig. 2. The adap-
tation strategy that uses the posterior mean of the
policy trained on the source domain as the prior
mean for adaptation is denoted as PRIOR in Fig. 2.
Finally, for comparison purposes we show the per-
formance of the policy that is trained from scratch
on the extended domain. This is denoted as TRAIN
on the graph. It can be seen that both adapta-
tion strategies significantly reduce the number of
training dialogues and, more importantly, main-
tain the level of performance during adaptation.
The adaptation strategy that places the prior on the
mean has slightly worse performance in the begin-
ning but provides the best performance after 1500
dialogues. As already noted, this could be due
to overly confident variances in the ADAPT case
leading to a local optimum.
6 Human experiments
In order to adapt and evaluate policies with hu-
mans, we used crowd-sourcing via the Ama-
zon Mechanical Turk service in a set-up similar
to (Jurc???c?ek et al, 2011a; Gas?ic? et al, 2013).
The BUDS dialogue manager was incorporated
in a live telephone-based spoken dialogue system.
The Mechanical Turk users were assigned spe-
cific tasks in the extended TopTable domain. They
were asked to find restaurants that have particu-
lar features as defined by the given task. To elicit
more complex dialogues, the users were some-
times asked to find more than one restaurant, and
in cases where such a restaurant did not exist they
were required to seek an alternative, for example
find a Chinese restaurant instead of a Vietnamese
one. After each dialogue the users filled in a feed-
back form indicating whether they judged the di-
alogue to be successful or not. Based on that bi-
nary rating, the subjective success was calculated
as well as the average reward. An objective rat-
ing can also be obtained by comparing the system
outputs with the predefined task.
During policy adaptation, at the end of each
call, users were asked to press 1 if they were satis-
fied (i.e. believed that they had been successful in
fulfilling the assigned task) and 0 otherwise. The
objective success was also calculated. The dia-
logue was then only used for adaptation if the user
rating agreed with the objective measure of suc-
cess as in (Gas?ic? et al, 2013). The performance
based on user ratings during adaptation for both
adaptation strategies is given in Table 1.
Table 1: Policy performance during adaptation
#Diags Reward Success (%)
ADAPT 251 11.7? 0.5 92.0? 1.7
PRIOR 329 12.1? 0.4 96.7? 1.0
We then evaluated four policies with real users:
the policy trained on the basic domain, the pol-
icy trained on the extended domain and the pol-
icy adapted to the extended domain using the prior
and the policy adapted to the extended domain via
interaction with real users using retraining. The
results are given in Table 2.
Table 2: Human evaluation of four systems in the
extended domain: trained in the basic domain,
trained in the extended domain, trained in the ba-
sic and adapted in the extended domain using both
ADAPT and PRIOR methods.
Training #Diags Reward Success(%)
Basic 246 11.0? 0.5 91.9? 1.7
Extended 250 12.1? 0.4 94.4? 1.5
ADAPT 268 12.6? 0.4 94.4? 1.4
PRIOR 252 12.4? 0.4 95.6? 1.3
The results show two important features of
these adaptation strategies. The first is that it is
possible to adapt the policy from one domain to
another with a small number of dialogues. Both
adaptation techniques achieve results statistically
indistinguishable from the matched case where the
policy was trained directly in the extended do-
main. The second important feature is that both
adaptation strategies guarantee a minimum level
of performance during training, which is better
than the performance of the basic policy tested on
the extended domain. This is particularly impor-
tant when training with real users so that they are
not exposed to poor performance at any time dur-
ing training.
7 Application to fast learning
The above results show that transfer learning
through policy adaptation can be relatively fast.
Since complex domains can be decomposed into a
series of domains with gradually increasing com-
plexity, an alternative to training a system to con-
vergence starting from an uninformative prior is
220
to train a system in stages iteratively adapting to
successively more complex domains (Taylor and
Stone, 2009).
We explored this idea by training the extended
system in three stages. The first has only one slot
that the user can specify: food-type and additional
slots phone, address and postcode that can be re-
quested (initial in Fig. 3). The second has an ad-
ditional area slot (intermediate in Fig. 3) and the
final domain has a the price-range slot added (final
on the graph).
A policy for each of these domains was trained
until convergence and the average rewards of these
policies are the horizontal lines on Fig. 3. In addi-
tion, the following adaptation schedule was imple-
mented. An initial policy was trained from scratch
for the one-slot initial system using only 1500 dia-
logues. The resulting policy was then retrained for
the intermediate two-slot system using again just
1500 dialogues. Finally, the required three-slot
system was trained using 1500 dialogues. At each
stage the policy was tested every 100 training dia-
logues, and the resulting performances are shown
by the three graphs initial-train, intermediate-adapt
and final-adapt in Fig. 3. The policies were tested
on the domains they are trained on or adapted to.
It can be seen that after just 500 dialogues of
the third stage (i.e. after just 3500 dialogues in to-
tal) the policy reaches optimal performance. It has
been shown previously that Gaussian process re-
inforcement learning for this task normally takes
104 dialogues (Gas?ic? et al, 2012) so this schedule
halves the number of dialogues needed for train-
ing. Also it is important to note that when training
from scratch the average reward is less than 5 for
300 dialogues (see TRAIN in Fig. 2), in this case
that only happens for about 100 dialogues (see
initial-train in Fig. 3).
8 Conclusions
This paper has investigated the problem of ex-
tending a dialogue system to handle new previ-
ously unseen concepts (i.e. slots) using adapta-
tion based transfer learning. It has been shown that
a GP kernel can be mapped to establish a relation-
ship between a basic and an extended domain and
that GP-based adaptation can restore a system to
optimal performance within 200 to 300 adaptation
dialogues. A major advantage of this technique is
that it allows a minimum level of performance to
be guaranteed and hence guards against subject-
0 200 400 600 800 1000 1200 1400 1600Training dialogues15
10
5
0
5
10
15
Rewa
rd
initial-trainintermediate-adaptfinal-adaptintermediateinitialfinal
Figure 3: Application of transfer learning to fast
training. The target is to achieve the performance
of the fully trained 3 slot system as shown by the
lower horizontal line final. This is achieved in three
stages, with the target being achieved part way
through the 3rd stage using just 3500 dialogues in
total.
ing the user to poor performance during the early
stages of adaptation.
Two methods of adaptation have been studied ?
one based on augmenting the training points from
the source domain with new points from the tar-
get domain, and a second which treats the source
policy as a prior for the target policy. Results us-
ing the prior method were consistently better. In a
further experiment, it was also shown that starting
with a simple system and successively extending
and adapting it slot by slot, can achieve optimal
performance faster than one trained directly from
scratch.
These results suggest that it should be feasi-
ble to construct dialogue systems which can dy-
namically update and extend their domains of dis-
course automatically during direct conversations
with users. However, further investigation of
methods for learning the relationship between the
new and the old domains is needed. Also, the
scalability of these results to large-scale domain
expansion remains a topic for future work.
Acknowledgments
This work was partly supported by PAR-
LANCE (www.parlance-project.eu), an EU Sev-
enth Framework Programme project (grant num-
ber 287615).
221
References
Y Engel, S Mannor, and R Meir. 2005. Reinforcement
learning with Gaussian processes. In Proceedings of
ICML.
Y Engel. 2005. Algorithms and Representations for
Reinforcement Learning. PhD thesis, Hebrew Uni-
versity.
M Gales and S Young. 2007. The application of hid-
den Markov models in speech recognition. Found.
Trends Signal Process., 1:195?304.
M Gas?ic?, F Jurc???c?ek, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, K Yu, and S Young. 2010.
Gaussian Processes for Fast Policy Optimisation of
POMDP-based Dialogue Managers. In Proceedings
of SIGDIAL.
M Gas?ic?, F Jurc???c?ek, B Thomson, K Yu, and S Young.
2011. On-line policy optimisation of spoken dia-
logue systems via live interaction with human sub-
jects. In Proceedings of ASRU.
M Gas?ic?, M Henderson, B Thomson, P Tsiakoulis, and
S Young. 2012. Policy optimisation of POMDP-
based dialogue systems without state space com-
pression. In Proceedings of SLT.
M Gas?ic?, C. Breslin, M. Henderson, Szummer M.,
B Thomson, P. Tsiakoulis, and S Young. 2013.
On-line policy optimisation of Bayesian Dialogue
Systems by human interaction. In Proceedings of
ICASSP.
M Geist and O Pietquin. 2011. Managing Uncertainty
within the KTD Framework. In Proceedings of the
Workshop on Active Learning and Experimental De-
sign, Sardinia (Italy).
K Georgila and O Lemon. 2004. Adaptive multimodal
dialogue management based on the information state
update approach. In W3C Workshop on Multimodal
Interaction.
S Janarthanam and O Lemon. 2010. Adaptive Re-
ferring Expression Generation in Spoken Dialogue
Systems: Evaluation with Real Users. In Proceed-
ings of SIGDIAL.
T Jebara, R Kondor, and A Howard. 2004. Probability
product kernels. J. Mach. Learn. Res., 5:819?844,
December.
F Jurc???c?ek, S Keizer, M Gas?ic?, F Mairesse, B Thomson,
K Yu, and S Young. 2011a. Real user evaluation of
spoken dialogue systems using Amazon Mechanical
Turk. In Proceedings of Interspeech.
F Jurc???c?ek, B Thomson, and S Young. 2011b. Natural
actor and belief critic: Reinforcement algorithm for
learning parameters of dialogue systems modelled as
POMDPs. ACM Transactions on Speech and Lan-
guage Processing.
S Keizer, M Gas?ic?, F Jurc???c?ek, F Mairesse, B Thomson,
K Yu, and S Young. 2010. Parameter estimation
for agenda-based user simulation. In Proceedings of
SIGDIAL.
DJ Litman and S Pan. 1999. Empirically evaluating
an adaptable spoken dialogue system. In Proceed-
ings of the seventh international conference on User
modelling.
DJ Litman and S Pan. 2002. Designing and evaluat-
ing an adaptive spoken dialogue system. User Mod-
elling and User-Adapted Interaction, 12:111?137.
CE Rasmussen and CKI Williams. 2005. Gaussian
Processes for Machine Learning. MIT Press, Cam-
bridge, Massachusetts.
N Roy, J Pineau, and S Thrun. 2000. Spoken dialogue
management using probabilistic reasoning. In Pro-
ceedings of ACL.
J Schatzmann. 2008. Statistical User and Error Mod-
elling for Spoken Dialogue Systems. Ph.D. thesis,
University of Cambridge.
ME Taylor and P Stone. 2009. Transfer learning for
reinforcement learning domains: A survey. J. Mach.
Learn. Res., 10:1633?1685, December.
ME Taylor, P Stone, and Y Liu. 2007. Transfer learn-
ing via inter-task mappings for temporal difference
learning. J. Mach. Learn. Res., 8:2125?2167, De-
cember.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken di-
alogue systems. Computer Speech and Language,
24(4):562?588.
B Thomson, M Gas?ic?, M Henderson, P Tsiakoulis, and
S Young. 2012. N-Best error simulation for training
spoken dialogue systems. In Proceedings of SLT.
TopTable. 2012. TopTable. https://www.
toptable.com.
JD Williams and SJ Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
S Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The Hid-
den Information State model: A practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
B Zhang, Q Cai, J Mao, E Chang, and B Guo.
2001. Spoken Dialogue Management as Planning
and Acting under Uncertainty. In Proceedings of
Eurospeech.
222
Proceedings of the SIGDIAL 2013 Conference, pages 467?471,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Deep Neural Network Approach for the
Dialog State Tracking Challenge
Matthew Henderson, Blaise Thomson and Steve Young
Department of Engineering,
University of Cambridge, U.K.
{mh521, brmt2, sjy}@eng.cam.ac.uk
Abstract
While belief tracking is known to be im-
portant in allowing statistical dialog sys-
tems to manage dialogs in a highly robust
manner, until recently little attention has
been given to analysing the behaviour of
belief tracking techniques. The Dialogue
State Tracking Challenge has allowed for
such an analysis, comparing multiple be-
lief tracking approaches on a shared task.
Recent success in using deep learning for
speech research motivates the Deep Neu-
ral Network approach presented here. The
model parameters can be learnt by directly
maximising the likelihood of the training
data. The paper explores some aspects of
the training, and the resulting tracker is
found to perform competitively, particu-
larly on a corpus of dialogs from a system
not found in the training.
1 Introduction
Statistical dialog systems, in maintaining a distri-
bution over multiple hypotheses of the true dialog
state, are able to behave in a robust manner when
faced with noisy conditions and ambiguity. Such
systems rely on probabilistic tracking of dialog
state, with improvements in the tracking quality
being important in the system-wide performance
in a dialog system (see e.g. Young et al (2009)).
This paper presents a Deep Neural Network
(DNN) approach for dialog state tracking which
has been evaluated in the context of the Dia-
log State Tracking Challenge (DSTC) (Williams,
2012a; Williams et al, 2013)1.
Using Deep Neural Networks allows for the
modelling of complex interactions between arbi-
trary features of the dialog. This paper shows im-
provements in using deep networks over networks
1More information on the DSTC is available at
http://research.microsoft.com/en-us/events/dstc/
with fewer hidden layers. Recent developments in
speech research have shown promising results us-
ing deep learning, motivating its use in the context
of dialog (Hinton et al, 2012; Li et al, 2013).
This paper presents a technique which solves
the task of outputting a sequence of probability
distributions over an arbitrary number of possible
values using a single neural network, by learning
tied weights and using a form of sliding window.
As the classification task is not split into multiple
sub-tasks for a given slot, the log-likelihood of the
tracker on training data can be directly maximised
using gradient ascent techniques.
The domain of the DSTC is bus route informa-
tion in the city of Pittsburgh, but the presented
technique is easily transferable to new domains,
with the learned models in fact being domain in-
dependent. No domain specific knowledge is used,
and the classifier learned does not require knowl-
edge of the set of possible values. The tracker per-
formed highly competitively in the ?test4? dataset,
which consists of data from a dialog system not
seen in training. This suggests the model is ca-
pable of capturing the important aspects of dia-
log in a robust manner without overtuning to the
specifics of a particular system.
Most attention in the dialog state belief tracking
literature has been given to generative Bayesian
network models (Paek and Horvitz, 2000; Thom-
son and Young, 2010). Few trackers have been
published using discriminative classifiers, a no-
table exception being Bohus and Rudnicky (2006).
An analysis by Williams (2012b) demonstrates
how such generative models can in fact degrade
belief tracking performance relative to a simple
baseline. The successful use of discriminative
models for belief tracking has recently been al-
luded to by Williams (2012a) and Li et al (2013),
and was a prominent theme in the results of the
DSTC.
467
2 The Dialog State Tracking Challenge
This section describes the domain and method-
ology of the Dialog State Tracking Challenge
(DSTC). The Challenge uses data collected during
the course of the Spoken Dialog Challenge (Black
et al, 2011), in which participants implemented
dialog systems to provide bus route information in
the city of Pittsburgh. This provides a large cor-
pus of real phonecalls from members of the public
with real information needs.
Set Number of calls Notes
train1a 1013 Labelled training data
train1b&c 10619 Same dialog system astrain1a, but unlabelled
train2 678 Similar to train1*
train3 779 Different participant toother train sets
test1 765 Very similar to train1*and train2
test2 983 Somewhat similar totrain1* and train2
test3 1037 Very similar to train3
test4 451 System not found inany training set
Table 1: Summary of datasets in the DSTC
Table 1 summarises the data provided in the
challenge. Labelled training sets provide labels
for the caller?s true goal in each dialog for 5 slots;
route, from, to, date and time.
Participants in the DSTC were asked to report
the results of their tracker on the four test sets in
the form of a probability distribution over each
slot for each turn. Performance was determined
using a basket of metrics designed to capture dif-
ferent aspects of tracker behaviour Williams et al
(2013). These are discussed further in Section 4.
The DNN approach described here is referred to
in the results of the DSTC as ?team1/entry1?.
3 Model
For a given slot s at turn t in a dialog, let St, s de-
note the set of possible values for s which have oc-
curred as hypotheses in the SLU for turns ? t. A
tracker must report a probability distribution over
St, s ? {other} representing its belief of the user?s
true goal for the slot s. The probability of ?other?
represents the probability that the user?s true goal
is yet to appear as an SLU hypothesis.
A neural network structure is defined which
gives a discrete distribution over the |St, s|+1 val-
ues, taking the turns ? t as input.
Figure 1 illustrates the structure used in this ap-
proach. Feature functions fi (t, v) for i = 1 . . .M
f1 (t, v) f1 (t? T + 1, v) ?t?Tt?=0 f1 (t?, v)
f2 (t, v) f2 (t? T + 1, v) ?t?Tt?=0 f2 (t?, v)
fM (t, v) fM (t? T + 1, v) ?t?Tt?=0 fM (t?, v)
t t? T + 1 (0 . . . t? T )
f1
f2
fM
. . .
...
h1 [= tanh(W0fT + b0)]
h2 [= tanh(W1hT1 + b1)]
h3 [= tanh(W2hT2 + b2)]
E(t, v) [= W3hT3 ]
Figure 1: The Neural Network structure for computing
E (t, v) ? R for each possible value v in the set St, s. The
vector f is a concatenation of all the input nodes.
are defined which extract information about the
value v from the SLU hypotheses and machine
actions at turn t. A simple example would be
fSLU (t, v), the SLU score that s=v was informed
at turn t. A list of the feature functions actually
used in the trial is given in Section 3.1. For nota-
tional convenience, feature functions at negative t
are defined to be zero:
?i ?v, t? < 0? fi (t?, v) = 0.
The input layer for a given value v is fixed in
size by choosing a window size T , such that the
feature functions are summed for turns ? t ? T .
The input layer therefore consists of (T ?M) in-
put nodes set to fi (t?, v) for t? = t?T+1 . . . t and
i = 1 . . .M , and M nodes set to ?t?Tt?=0 fi (t?, v)
for i = 1 . . .M .
A feed-forward structure of hidden layers is
chosen, which reduces to a single node denoted
E (t, v). Each hidden layer introduces a weight
matrix Wi and a bias vector bi as parameters,
which are independent of v but possibly trained
separately for each s. The equations for each layer
in the network are given in Figure 1.
The final distribution from the tracker is:
P (s = v) = eE(t, v)/Z
P (s /? St, s) = eB/Z
Z = eB +
?
v??St, s
eE(t, v
?)
where B is a new parameter of the network, in-
dependent of v and possibly trained separately for
each slot s.
468
3.1 Feature Functions
As explained above, a feature function is a func-
tion f (t, v) which (for a given dialog) returns a
real number representing some aspect of the turn
t with respect to a possible value v. A turn con-
sists of a machine action and the subsequent Spo-
ken Language Understanding (SLU) results. The
functions explored in this paper are listed below:
1. SLU score; the score assigned by the SLU to
the user asserting s=v.
2. Rank score; 1/r where r is the rank of s=v in
the SLU n-best list, or 0 if it is not on the list.
3. Affirm score; SLU score for an affirm action
if the system just confirmed s=v.
4. Negate score; as previous but with negate.
5. Go back score; the score assigned by the SLU
to a goback action matching s=v.
6. Implicit score; 1? the score given in the SLU
to a contradictory action if the system just im-
plicitly confirmed s=v, otherwise 0.
7. User act type; a feature function for each pos-
sible user act type, giving the total score of the
user act type in the SLU. Independent of s & v.
8. Machine act type; a feature function for each
possible machine act type, giving the total num-
ber of machine acts with the type in the turn.
Independent of s & v.
9. Cant help; 1 if the system just said that it can-
not provide information on s=v, otherwise 0.
10. Slot confirmed; 1 if s=v? was just confirmed
by the system for some v?, otherwise 0.
11. Slot requested; 1 if the value of s was just re-
quested by the system, otherwise 0.
12. Slot informed; 1 if the system just gave infor-
mation on a set of bus routes which included a
specific value of s, otherwise 0.
4 Training
The derivatives of the training data likelihood with
respect to all the parameters of the model can
be computed using back propagation, i.e. the
chain rule. Stochastic Gradient Descent with mini-
batches is used to optimise the parameters by de-
scending the negative log-likelihood in the direc-
tion of the derivatives (Bottou, 1991). Termina-
tion is triggered when performance on a held-out
development set stops improving.
Each turn t and slot s in a dialog for which
|St, s| > 0 provides a non-zero summand to the
total log-likelihood of the training data. These in-
stances may be split up by slot to train a separate
network for each slot. Alternatively the data can
be combined to learn a slot independent model.
The best approach found was to train a slot inde-
pendent model for a few epochs, and then switch
to training one model per slot (see Section 4.4).
This section presents experiments varying the
training of the model. In each case the parameters
are trained using all of the labelled training sets.
The results are reported for test4 since this system
is not found in the training data. They are therefore
unbiassed and avoid overtuning problems.
The ROC curves, accuracy, Mean Reciprocal
Rank (MRR) and l2 norm of the tracker across all
slots are reported here. (A full definition of the
metrics is found in Williams et al (2013).) These
are computed throughout using statistics at every
turn t where |St, s| > 0 (referred to as ?schedule
2? in the terminology of the challenge.) Table 2
and Figure 3 in Appendix A show these metrics.
The ?Baseline? system (?team0/entry1? in the chal-
lenge), considers only the top SLU hypothesis so
far, and assigns the SLU confidence score as the
tracker probability. It does not therefore incorpo-
rate any belief tracking.
4.1 Window Size
The window size, T , was varied from 2 to 20. T
must be selected so that it is large enough to cap-
ture enough of the sequence of the dialog, whilst
ensuring sufficient data to train the weights con-
necting the inputs from the earlier turns. The re-
sults suggest that T = 10 is a good compromise.
4.2 Feature Set
The features enumerated in Section 3.1 were split
into 4 sets. F1 = {1} includes only the SLU
scores; F2 = {1, ..., 6} includes feature func-
tions which depend on the user act and the value;
F3 = {1, ..., 8} also includes the user act and ma-
chine act types; and finally F4 = {1, ..., 12} in-
cludes functions which depend on the system act
and the value. The results clearly show that adding
more and more features in this manner monotoni-
cally increases the performance of the tracker.
4.3 Structure
Some candidate structures of the hidden layers
(h1, h2, ...) were evaluated, including having no
hidden layers at all, which gives a logistic regres-
sion model. In Table 2 the structure is represented
as a list giving the size of each hidden layer in turn.
Three layers in a funnelling [20, 10, 2] configu-
ration is found to outperform the other structures.
The l2 norm is highly affected by the use of deeper
network structure, suggesting it is most useful in
tweaking the calibration of the confidence scores.
469
ROC Acc. MRR l2
Baseline
0.5841 0.7574 0.5728
Window Size
T =2 0.6679 0.8044 0.5405
5 0.6875 0.8191 0.5164
10 0.6922 0.8207 0.5331
15 0.6718 0.8107 0.5352
20 0.6817 0.8190 0.5174
Feature Set
F1 0.5495 0.7364 0.6838
F2 0.6585 0.7954 0.6631
F3 0.6823 0.8134 0.5525
F4 0.6922 0.8207 0.5331
Structure
[] 0.6751 0.8074 0.5658
[50] 0.6679 0.8046 0.5450
[20] 0.6656 0.8060 0.5394
[50, 10] 0.6645 0.8045 0.5404
[20, 2] 0.6543 0.7952 0.5514
[20, 10, 2] 0.6922 0.8207 0.5331
Initialisation
Separate 0.6907 0.8206 0.5472
Single Model 0.6779 0.8111 0.5570
Shared Init. 0.6922 0.8207 0.5331
Table 2: Results for variant trackers described in Section
4. By default, we train using the shared initialisation training
method with T = 10, all the features enumerated in Section
3.1, and 3 hidden layers of size 20, 10 and 2.
4.4 Initialisation
The three methods of training alluded to in Sec-
tion 4 were evaluated; training a model for each
slot without sharing data between slots (Separate);
training a single slot independent model (Single
Model); and training for a few epochs a slot in-
dependent model, then using this to initialise the
training of separate models (Shared Initialisation).
The method of shared initialisation appears to
be the most effective, scoring the best on accu-
racy, MRR and l2. Training in this manner is par-
ticularly beneficial for slots which are under rep-
resented in the training data, as it initiates the pa-
rameters to sensible values before going on to spe-
cialise to that particular slot.
5 Performance in the DSTC
A DNN tracker was trained for entry in the
DSTC. Training used T=10, the full feature set,
a [20, 10, 2] hidden structure and the shared ini-
tialisation training method. Other parameters such
as the learning rate and regularisation coefficient
were tweaked by analysing performance on a held
out subset of the training data. All the labelled
test1
Acc.MRR
test2
Acc.MRR
test3
Acc.MRR
test4
Acc.MRR
all
Acc.MRR
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 2: Accuracy and MRR of the 28 entries in the DSTC
for all slots. Boxplots show minimum, maximum, quartiles
and the median. Dark dot is location of the entry presented in
this paper (DNN system).
training data available was used. The tracker is
labelled as ?team1/entry1? in the DSTC.
The DNN approach performed competitively in
the challenge. Figure 2 summarises the perfor-
mance of the approach relative to all 28 entries in
the DSTC. The results are less competitive in test2
and test3 but very strong in test1 and test4.
The performance in test4, dialogs with an un-
seen system, was probably the best because the
chosen feature functions forced the learning of a
general model which was not able to exploit the
specifics of particular ASR+SLU configurations.
Features which depend on the identity of the slot-
values would have allowed better performance in
test2 and test3, allowing the model to learn dif-
ferent behaviours for each value and learn typical
confusions. It would also have been possible to ex-
ploit the system-specific data available in the chal-
lenge, such as more detailed confidence metrics
from the ASR.
For a full comparison across the entries in the
DSTC, see Williams et al (2013). In making com-
parisons it should be noted that this team did not
alter the training for different test sets, and submit-
ted only one entry.
6 Conclusion
This paper has presented a discriminative ap-
proach for tracking the state of a dialog which
takes advantage of deep learning. While sim-
ple Gradient Ascent training was tweaked in this
paper using the ?Shared Initialisation? scheme, a
possible promising future direction would be to
further experiment with more recent methods for
training deep structures e.g. initialising the net-
works layer by layer (Hinton et al, 2006).
Richer feature representations of the dialog con-
tribute strongly to the performance of the model.
The feature set presented is applicable across a
broad range of slot-filling dialog domains, sug-
gesting the possibility of using the models across
domains without domain-specific training data.
470
A ROC Curves
Window Size
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Feature Set
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Structure
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
Initialisation
0.1 0.2 0.3 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Figure 3: ROC (Receiver Operating Characteristic) Curves
x-axis and y-axis are false acceptance and true acceptance
respectively. Lines are annotated as per Table 2.
Acknowledgments
The authors would like to thank the organisers of
the DSTC. The principal author was funded by a
studentship from the EPSRC.
References
Alan W. Black, Susanne Burger, Alistair Conkie, He-
len Wright Hastie, Simon Keizer, Oliver Lemon,
Nicolas Merigaud, Gabriel Parent, Gabriel Schu-
biner, Blaise Thomson, Jason D. Williams, Kai Yu,
Steve Young, and Maxine Eskenazi. 2011. Spoken
dialog challenge 2010: Comparison of live and con-
trol test results. In SigDIAL.
Dan Bohus and Alex Rudnicky. 2006. A K-
hypotheses+ Other Belief Updating Model. Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
Le?on Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, N??mes, France. EC2.
Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.
2006. A Fast Learning Algorithm for Deep Belief
Nets. Neural computation.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. Signal Processing Magazine.
Deng Li, Jinyu Li, Jui-Ting Huang, Kaisheng Yao,
Dong Yu, Frank Seide, Michael L Seltzer, Geoff
Zweig, Xiaodong He, Jason D. Williams, Yifan
Gong, and Alex Acero. 2013. Recent Advances in
Deep Learning for Speech Research at Microsoft. In
ICASSP.
Tim Paek and Eric Horvitz. 2000. Conversation as
action under uncertainty. In The Sixteenth Confer-
ence on Uncertainty in Artificial Intelligence. Mor-
gan Kaufmann.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech & Lan-
guage.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan W. Black. 2013. The Dialogue State
Tracking Challenge. In SigDIAL.
Jason D. Williams. 2012a. A belief tracking chal-
lenge task for spoken dialog systems. In NAACL
HLT 2012 Workshop on Future directions and needs
in the Spoken Dialog Community: Tools and Data.
Association for Computational Linguistics.
Jason D. Williams. 2012b. Challenges and opportu-
nities for state tracking in statistical spoken dialog
systems: Results from two public deployments. J.
Sel. Topics Signal Processing, 6(8):959?970.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2009. The Hidden Information State model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech & Lan-
guage.
471
Proceedings of the SIGDIAL 2014 Conference, pages 263?272,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
The Second Dialog State Tracking Challenge
Matthew Henderson
1
, Blaise Thomson
1
and Jason Williams
2
1
Department of Engineering, University of Cambridge, U.K.
2
Microsoft Research, Redmond, WA, USA
mh521@eng.cam.ac.uk brmt2@eng.cam.ac.uk jason.williams@microsoft.com
Abstract
A spoken dialog system, while commu-
nicating with a user, must keep track of
what the user wants from the system at
each step. This process, termed dialog
state tracking, is essential for a success-
ful dialog system as it directly informs the
system?s actions. The first Dialog State
Tracking Challenge allowed for evalua-
tion of different dialog state tracking tech-
niques, providing common testbeds and
evaluation suites. This paper presents a
second challenge, which continues this
tradition and introduces some additional
features ? a new domain, changing user
goals and a richer dialog state. The chal-
lenge received 31 entries from 9 research
groups. The results suggest that while
large improvements on a competitive base-
line are possible, trackers are still prone
to degradation in mismatched conditions.
An investigation into ensemble learning
demonstrates the most accurate tracking
can be achieved by combining multiple
trackers.
1 Introduction
Spoken language provides a medium of communi-
cation that is natural to users as well as hands- and
eyes-free. Voice-based computer systems, called
spoken dialog systems, allow users to interact us-
ing speech to achieve a goal. Efficient operation of
a spoken dialog system requires a component that
can track what has happened in a dialog, incor-
porating system outputs, user speech and context
from previous turns. The building and evaluation
of these trackers is an important field of research
since the performance of dialog state tracking is
important for the final performance of a complete
system.
Until recently, it was difficult to compare ap-
proaches to state tracking because of the wide va-
riety of metrics and corpora used for evaluation.
The first dialog state tracking challenge (DSTC1)
attempted to overcome this by defining a challenge
task with standard test conditions, freely available
corpora and open access (Williams et al., 2013).
This paper presents the results of a second chal-
lenge, which continues in this tradition with the
inclusion of additional features relevant to the re-
search community.
Some key differences to the first challenge in-
clude:
? The domain is restaurant search instead of
bus timetable information. This provides par-
ticipants with a different category of interac-
tion where there is a database of matching en-
tities.
? Users? goals are permitted to change. In the
first challenge, the user was assumed to al-
ways want a specific bus journey. In this chal-
lenge the user?s goal can change. For exam-
ple, they may want a ?Chinese? restaurant at
the start of the dialog but change to wanting
?Italian? food by the end.
? The dialog state uses a richer representa-
tion than in DSTC1, including not only the
slot/value attributes of the user goal, but also
their search method, and what information
they wanted the system to read out.
As well as presenting the results of the different
state trackers, this paper attempts to obtain some
insights into research progress by analysing their
performance. This includes analyses of the predic-
tive power of performance on the development set,
the effects of tracking the dialog state using joint
distributions, and the correlation between 1-best
accuracy and overall quality of probability distri-
butions output by trackers. An evaluation of the
effects of ensemble learning is also performed.
The paper begins with an overview of the chal-
263
lenge in section 2. The labelling scheme and met-
rics used for evaluation are discussed in section 3
followed by a summary of the results of the chal-
lenge in section 4. An analysis of ensemble learn-
ing is presented in section 5. Section 6 concludes
the paper.
2 Challenge overview
2.1 Problem statement
This section defines the problem of dialog state
tracking as it is presented in the challenge. The
challenge evaluates state tracking for dialogs
where users search for restaurants by specifying
constraints, and may ask for information such as
the phone number. The dialog state is formu-
lated in a manner which is general to information
browsing tasks such as this.
Included with the data is an ontology
1
, which
gives details of all possible dialog states. The
ontology includes a list of attributes termed re-
questable slots which the user may request, such
as the food type or phone number. It also provides
a list of informable slots which are attributes that
may be provided as constraints. Each informable
slot has a set of possible values. Table 1 gives de-
tails on the ontology used in DSTC2.
The dialog state at each turn consists of three
components:
? The goal constraint for each informable slot.
This is either an assignment of a value from
the ontology which the user has specified as
a constraint, or is a special value ? either
Dontcare which means the user has no pref-
erence, or None which means the user is yet
to specify a valid goal for this slot.
? A set of requested slots, i.e. those slots
whose values have been requested by the
user, and should be informed by the system.
? An assignment of the current dialog search
method. This is one of
? by constraints, if the user is attempting
to issue a constraint,
? by alternatives, if the user is requesting
alternative suitable venues,
? by name, if the user is attempting to ask
about a specific venue by its name,
? finished, if the user wants to end the call
? or none otherwise.
Note that in DSTC1, the set of dialog states
1
Note that this ontology includes only the schema for di-
alog states and not the database entries
was dependent on the hypotheses given by a Spo-
ken Language Understanding component (SLU)
(Williams et al., 2013), whereas here the state is
labelled independently of any SLU (see section 3).
Appendix B gives an example dialog with the state
labelled at each turn.
A tracker must use information up to a given
turn in the dialog, and output a probability distri-
bution over dialog states for the turn. Trackers
output separately the distributions for goal con-
straints, requested slots and the method. They may
either report a joint distribution over the goal con-
straints, or supply marginal distributions and let
the joint goal constraint distribution be calculated
as a product of the marginals.
2.2 Challenge design
DSTC2 studies the problem of dialog state track-
ing as a corpus-based task, similar to DSTC1. The
challenge task is to re-run dialog state tracking
over a test corpus of dialogs.
A corpus-based challenge means all trackers
are evaluated on the same dialogs, allowing di-
rect comparison between trackers. There is also
no need for teams to expend time and money in
building an end-to-end system and getting users,
meaning a low barrier to entry.
When a tracker is deployed, it will inevitably al-
ter the performance of the dialog system it is part
of, relative to any previously collected dialogs. In
order to simulate this, and to penalise overfitting to
known conditions, evaluation dialogs in the chal-
lenge are drawn from dialogs with a dialog man-
ager which is not found in the training data.
2.3 Data
A large corpus of dialogs with various telephone-
based dialog systems was collected using Ama-
zon Mechanical Turk. The dialogs used in the
challenge come from 6 conditions; all combina-
tions of 3 dialog managers and 2 speech recognis-
ers. There are roughly 500 dialogs in each condi-
tion, of average length 7.88 turns from 184 unique
callers.
The 3 dialog managers are:
? DM-HC, a simple tracker maintaining a sin-
gle top dialog state, and a hand-crafted policy
? DM-POMDPHC, a dynamic Bayesian net-
work for tracking a distribution of dialog
states, and a hand-crafted policy
? DM-POMDP, the same tracking method as
DM-POMDPHC, with a policy learnt using
264
Slot Requestable
Informable
area yes
yes. 5 values; north,
south, east, west, centre
food yes
yes, 91 possible values
name yes
yes, 113 possible values
pricerange yes
yes, 3 possible values
addr yes
no
phone yes
no
postcode yes
no
signature yes
no
Table 1: Ontology used in DSTC2 for restaurant informa-
tion. Counts do not include the special Dontcare value.
POMDP reinforcement learning
The 2 speech recognisers are:
? ASR-degraded, speech recogniser with arti-
ficially degraded statistical acoustic models
? ASR-good, full speech recogniser optimised
for the domain
These give two acoustic conditions, the de-
graded model producing dialogs at higher error
rates. The degraded models simulate in-car con-
ditions and are described in Young et al. (2013).
The set of all calls with DM-POMDP, with both
speech recognition configurations, constitutes the
test set. All calls with the other two dialog man-
agers are used for the training and development
set. Specifically, the datasets are arranged as so:
? dstc2 train. Labelled dataset released in Oc-
tober 2013, with 1612 calls from DM-HC and
DM-POMDPHC, and both ASR conditions.
? dstc2 dev. Labelled dataset released at the
same time as dstc2 train, with 506 calls under
the same conditions as dstc2 train. No caller
in this set appears in dstc2 train.
? dstc2 test. Set used for evaluation. Released
unlabelled at the beginning of the evaluation
week. This consists of all 1117 dialogs with
DM-POMDP.
Paid Amazon Mechanical Turkers were as-
signed tasks and asked to call the dialog systems.
Callers were asked to find restaurants that matched
particular constraints on the slots area, pricerange
and food. To elicit more complex dialogs, includ-
ing changing goals (goals in DSTC1 were always
constant), the users were sometimes asked to find
more than one restaurant. In cases where a match-
ing restaurant did not exist they were required to
seek an alternative, for example finding an Indian
instead of an Italian restaurant.
A breakdown of the frequency of goal con-
straint changes is given in table 2, showing around
40% of all dialogs involved a change in goal con-
straint. The distribution of the goal constraints in
50
100
150
200
Figure 1: Histogram of values for the food constraint (ex-
cluding dontcare) in all data. The most frequent values are
Indian, Chinese, Italian and European.
Dataset
train dev test
area 2.9% 1.4% 3.8%
food 37.3% 34.0% 40.9%
name 0.0% 0.0% 0.0%
pricerange 1.7% 1.6% 3.1%
any 40.1% 37.0% 44.5%
Table 2: Percentage of dialogs which included a change in
the goal constraint for each informable (and any slot). Barely
any users asked for restaurants by name.
the data was reasonably uniform across the area
and pricerange slots, but was skewed for food as
shown in figure 1. The skew arises from the distri-
bution of the restaurants in the system?s database;
many food types have very few matching venues.
Recently, researchers have started using word
confusion networks for spoken language under-
standing (Henderson et al., 2012; T?ur et al., 2013).
Unfortunately, word confusion networks were not
logged at the time of collecting the dialog data. In
order to provide word confusion networks, ASR
was run offline in batch mode on each dialog us-
ing similar models as the live system. This gives
a second set of ASR results, labelled batch, which
not only includes ASR N -best lists (as in live re-
sults), but also word confusion networks.
For each dataset and speech recogniser, table 3
gives the Word Error Rate on the top ASR hypoth-
esis, and F-score for the top SLU hypothesis (cal-
culated as in Henderson et al. (2012)). Note the
batch ASR was always less accurate than the live.
Live Batch
Dataset ASR WER F-score WER
train
degraded 30.7% 72.4% 37.7%
good 22.4% 78.7% 25.5%
all 26.4% 75.7% 31.3%
dev
degraded 40.4% 67.3% 47.3%
good 25.2% 75.2% 30.0%
all 31.9% 71.6% 37.6%
test
degraded 33.6% 70.0% 41.1%
good 23.5% 77.8% 27.1%
all 28.7% 73.8% 34.3%
Table 3: Word Error Rate on the top hypothesis, and F-score
on top SLU hypothesis.
265
3 Labelling and evaluation
The output of each tracker is a distribution over
dialog states for each turn, as explained in section
2.1. To allow evaluation of the tracker output, the
single correct dialog state at each turn is labelled.
Labelling of the dialog state is facilitated by first
labelling each user utterance with its semantic rep-
resentation, in the dialog act format described in
Henderson et al. (2013) (some example seman-
tic representations are given in appendix B). The
semantic labelling was achieved by first crowd-
sourcing the transcription of the audio to text.
Next a semantic decoder was run over the tran-
scriptions, and the authors corrected the decoder?s
results by hand. Given the sequence of machine
actions and user actions, both represented seman-
tically, the true dialog state is computed determin-
istically using a simple set of rules.
Recall the dialog state is composed of multiple
components; the goal constraint for each slot, the
requested slots, and the method. Each of these
is evaluated separately, by comparing the tracker
output to the correct label. The joint over the goal
constraints is evaluated in the same way, where the
tracker may either explicitly enumerate and score
its joint hypotheses, or let the joint be computed as
the product of the distributions over the slots.
A bank of metrics which look at the tracker out-
put and the correct labels are calculated in the eval-
uation. These metrics are a slightly expanded set
of those calculated in DSTC1.
Denote an example probability distribution
given by a tracker as p and the correct label to be
i, so we have that the probability reported to the
correct hypothesis is p
i
, and
?
j
p
j
= 1.
Accuracy measures the fraction of turns where
the top hypothesis is correct, i.e. where i =
argmax
j
p
j
. AvgP, average probability, mea-
sures the mean score of the correct hypothesis, p
i
.
This gives some idea of the quality of the score
given to the correct hypothesis, ignoring the rest
of the distribution. Neglogp is the mean nega-
tive logarithm of the score given to the correct hy-
pothesis, ? logp
i
. Sometimes called the negative
log likelihood, this is a standard score in machine
learning tasks. MRR is the mean reciprocal rank
of the top hypothesis, i.e.
1
1+k
where j
k
= i and
p
j
0
? p
j
1
? . . .. This metric measures the qual-
ity of the ranking, without necessarily treating the
scores as probabilities. L2 measures the square
of the l
2
norm between the distribution and the
correct label, indicating quality of the whole re-
ported distribution. It is calculated for one turn
as (1 ? p
i
)
2
+
?
j 6=i
p
2
j
. Two metrics, Update
precision and Update accuracy measure the ac-
curacy and precision of updates to the top scoring
hypothesis from one turn to the next. For more
details, see Higashinaka et al. (2004), which finds
these metrics to be highly correlated with dialog
success in their data.
Finally there is a set of measures relating to
the receiver operating characteristic (ROC) curves,
which measure the discrimination of the scores for
the highest-ranked hypotheses. Two versions of
ROC are computed, V1 and V2. V1 computes
correct-accepts (CA), false accepts (FA) and false-
rejects (FR) as fractions of all utterances. The
V2 metrics consider fractions of correctly classi-
fied utterances, meaning the values always reach
100% regardless of the accuracy. V2 metrics mea-
sure discrimination independently of the accuracy,
and are therefore only comparable between track-
ers with similar accuracies.
Several metrics are computed from the ROC
statistics. ROC V1 EER computes the false ac-
ceptance rate at the point where false-accepts are
equal to false-rejects. ROC V1 CA05, ROC V1
CA10, ROCV1 CA20 and ROCV2 CA05, ROC
V2 CA10, ROC V2 CA20, compute the correct
acceptance rates for both versions of ROC at false-
acceptance rates 0.05, 0.10, and 0.20.
Two schedules are used to decide which turns to
include when computing each metric. Schedule 1
includes every turn. Schedule 2 only includes a
turn if any SLU hypothesis up to and including the
turn contains some information about the compo-
nent of the dialog state in question, or if the correct
label is not None. E.g. for a goal constraint, this is
whether the slot has appeared with a value in any
SLU hypothesis, an affirm/negate act has appeared
after a system confirmation of the slot, or the user
has in fact informed the slot regardless of the SLU.
The data is labelled using two schemes. The
first, scheme A, is considered the standard la-
belling of the dialog state. Under this scheme,
each component of the state is defined as the most
recently asserted value given by the user. The
None value is used to indicate that a value is yet
to be given. Appendix B demonstrates labelling
under scheme A.
A second labelling scheme, scheme B, is in-
cluded in the evaluation, where labels are prop-
266
agated backwards through the dialog. This la-
belling scheme is designed to assess whether a
tracker is able to predict a user?s intention be-
fore it has been stated. Under scheme B, the la-
bel at a current turn for a particular component of
the dialog state is considered to be the next value
which the user settles on, and is reset in the case
of goal constraints if the slot value pair is given in
a canthelp act by the system (i.e. the system has
informed that this constraint is not satisfiable).
3.1 Featured metrics
All combinations of metrics, state components,
schedules and labelling schemes give rise to 815
total metrics calculated per tracker in evaluation.
Although each may have its particular motiva-
tion, many of the metrics will be highly corre-
lated. From the results of DSTC1 it was found
the metrics could be roughly split into 3 indepen-
dent groups; one measuring 1-best quality (e.g.
Acc), another measuring probability calibration
(e.g. L2), and the last measuring discrimination
(e.g. ROC metrics) (Williams et al., 2013).
By selecting a representative from each of these
groups, the following were chosen as featured
metrics:
? Accuracy, schedule 2, scheme A
? L2 norm, schedule 2, scheme A
? ROC V2 CA 5, schedule 2, scheme A
Accuracy is a particularly important measure
for dialog management techniques which only
consider the top dialog state hypothesis at each
turn, while L2 is of more importance when mul-
tiple dialog states are considered in action selec-
tion. Note that the ROC metric is only compara-
ble among systems operating at similar accuracies,
and while L2 should be minimised, Accuracy and
the ROC metric should be maximised.
Each of these, calculated for joint goal con-
straints, search method and combined re-
quested slots, gives 9 metrics altogether which
participants were advised to focus on optimizing.
3.2 Baseline trackers
Three baseline trackers were entered in the chal-
lenge, under the ID ?team0?. Source code for
all the baseline systems is available on the DSTC
website
2
. The first, ?team0.entry0?, follows sim-
ple rules commonly used in spoken dialog sys-
tems. It gives a single hypothesis for each slot,
2
http://camdial.org/
?
mh521/dstc/
whose value is the top scoring suggestion so far in
the dialog. Note that this tracker does not account
well for goal constraint changes; the hypothesised
value for a slot will only change if a new value
occurs with a higher confidence.
The focus baseline, ?team0.entry1?, includes a
simple model of changing goal constraints. Be-
liefs are updated for the goal constraint s = v, at
turn t, P (s = v), using the rule:
P (s = v)
t
= q
t
P (s = v)
t?1
+ SLU (s = v)
t
where 0 ? SLU(s = v)
t
? 1 is the evidence
for s = v given by the SLU in turn t, and q
t
=
?
v
?
SLU(s = v
?
)
t
? 1.
Another baseline tracker, based on the tracker
presented in Wang and Lemon (2013) is included
in the evaluation, labelled ?team0.entry2?. This
tracker uses a selection of domain independent
rules to update the beliefs, similar to the focus
baseline. One rule uses a learnt parameter called
the noise adjustment, to adjust the SLU scores.
Full details of this and all baseline trackers are pro-
vided on the DSTC website.
Finally, an oracle tracker is included under the
label ?team0.entry3?. This reports the correct la-
bel with score 1 for each component of the dialog
state, but only if it has been suggested in the dialog
so far by the SLU. This gives an upper-bound for
the performance of a tracker which uses only the
SLU and its suggested hypotheses.
4 Results
Altogether 9 research teams participated in the
challenge. Each team could submit a maximum of
5 trackers, and 31 trackers were submitted in total.
Teams are identified by anonymous team numbers
team1-9, and baseline systems are grouped under
team0. Appendix A gives the results on the fea-
tured metrics for each entry submitted to the chal-
lenge. The full results, including tracker output,
details of each tracker and scripts to run the evalu-
ation are available on the DSTC2 website.
The table in appendix A specifies which of the
inputs available were used for each tracker- from
live ASR, live SLU and batch ASR. This facil-
itates comparisons between systems which used
the same information.
A variety of techniques were used in the sub-
mitted trackers. Some participants provided short
synopses, which are available in the download
from the DSTC2 website. Full details on the track-
ers themselves are published at SIGdial 2014.
267
For the ?requested slot? task, some trackers out-
performed the oracle tracker. This was possible
because trackers could guess a slot was requested
using dialog context, even if there was no mention
of it in the SLU output.
Participants were asked to report the results of
their trackers on the dstcs2 dev development set.
Figure 2 gives some insight into how well perfor-
mance on the development set predicted perfor-
mance on the test set. Metrics are reported as per-
centage improvement relative to the focus base-
line to normalise for the difficulty of the datasets;
in general trackers achieved higher accuracies on
the test set than on development. Figure 2 shows
that the development set provided reasonable pre-
dictions, though in all cases improvement rel-
ative to the baseline was overestimated, some-
times drastically. This suggests that approaches to
tracking have trouble with generalisation, under-
performing in the mismatched conditions of the
test set which used an unseen dialog manager.
Joint Goal Constraint Accuracy
0.3 0.2 0.1 0.1
team1entry0team2entry1team3entry0team4entry0team5entry4team6entry2team7entry0team8entry1team9entry0
Joint Goal Constraint L2team1entry0team2entry1team3entry0team4entry0team5entry4team6entry2team7entry0team8entry1team9entry0
0.2 0.2 0.4 0.6
Figure 2: Performance relative to the focus baseline (per-
centage increase) for dev set (white) and test set (grey). Top
entry for each team chosen based on joint goal constraint ac-
curacy. A lower L2 score is better.
Recall from section 2, trackers could output
joint distributions for goal constraints, or simply
output one distribution for each slot and allow the
joint to be calculated as the product. Two teams,
team2 and team8, opted to output a joint distribu-
tion for some of their entries. Figure 3 compares
performance on the test set for these trackers be-
tween the joint distributions they reported, and the
joint calculated as the product. The entries from
team2 were able to show an increase in the accu-
racy of the top joint goal constraint hypotheses,
but seemingly at a cost in terms of the L2 score.
Conversely the entries from team8, though oper-
ating at lower performance than the focus base-
line, were able to show an improvement in L2 at a
slight loss in accuracy. These results suggest that a
tracking method is yet to be proposed which can,
at least on this data, improve both accuracy and
the L2 score of tracker output by reporting joint
predictions of goal constraints.
Accuracy
team0entry2team2entry0team2entry1team2entry2team2entry3team2entry4team8entry0team8entry1team8entry2team8entry3
0.70 0.72 0.74 0.76 0.78
0.03% 1.34%0.44%-0.11%0.20%-0.30%-0.04%
-0.09%-0.04%-0.09%
L2team0entry2
team2entry0
team2entry1
team2entry3
team2entry4
team8entry0
team8entry1
team8entry2
team8entry3
team8entry4
0.4 0.5 0.6 0.7
team0entry2team2entry0team2entry1team2entry2team2entry3team2entry4team8entry0team8entry1team8entry2team8entry3
0.03% 38.21%52.17%0.22% 23.05% -2.00%-1.73%-1.69%-1.79%-1.75%
Figure 3: Influence of reporting a full joint distribution.
White bar shows test set performance computing the goal
constraints as a product of independent marginals; dark bar is
performance with a full joint distribution. All entries which
reported a full joint are shown. A lower L2 score is better.
It is of interest to investigate the correlation be-
tween accuracy and L2. Figure 4 plots these met-
rics for each tracker on joint goal constraints. We
see that in general a lower L2 score correlates with
a higher accuracy, but there are examples of high
accuracy trackers which do poorly in terms of L2.
This further justifies the reporting of these as two
separate featured metrics.
0.50 0.55 0.60 0.65 0.70 0.75 0.800.3
0.4
0.5
0.6
0.7
0.8 team2entry0
team2entry1
team4entry0team2entry3
focus baseline, team0entry2
Accuracy
L2
Figure 4: Scatterplot of joint goal constraint accuracy and
joint goal constraint L2 for each entry. Plotted line is least-
squares linear regression, L2 = 1.53? 1.43Accuracy
268
Joint goal Method Requested
Tracker Acc. L2 Acc. L2 Acc. L2
Single best entry 0.784 0.346 0.950 0.082 0.978 0.035
Score averaging: top 2 entries 0.787 0.364- 0.945- 0.083 0.976 0.039-
Score averaging: top 5 entries 0.777 0.347 0.945 0.089- 0.976 0.038
Score averaging: top 10 entries 0.760- 0.364- 0.934- 0.108- 0.967- 0.056-
Score averaging: all entries 0.765- 0.362- 0.934- 0.103- 0.971- 0.052-
Stacking: top 2 entries 0.789 0.322+ 0.949 0.085- 0.977 0.040-
Stacking: top 5 entries 0.795+ 0.315+ 0.949 0.084 0.978 0.037
Stacking: top 10 entries 0.796+ 0.312+ 0.949 0.083 0.979 0.035
Stacking: all entries 0.798+ 0.308+ 0.950 0.083 0.980 0.034
Table 4: Accuracy and L2 for Joint goal constraint, Method, and Requested slots for the single best tracker (by accuracy) in
DSTC2, and various ensemble methods. ?Top N entries? means the N entries with highest accuracies from distinct teams, where
the baselines are included as a team. +/- indicates statistically significantly better/worse than the single best entry (p < 0.01),
computed with McNemar?s test for accuracy and the paired t-test for L2, both with Bonferroni correction for repeated tests.
5 Ensemble learning
The dialog state tracking challenge provides an
opportunity to study ensemble learning ? i.e. syn-
thesizing the output of many trackers to improve
performance beyond any single tracker. Here we
consider two forms of ensemble learning: score
averaging and stacking.
In score averaging, the final score of a class is
computed as the mean of the scores output by all
trackers for that class. One of score averaging?s
strengths is that it requires no additional training
data beyond that used to train the constituent track-
ers. If each tracker?s output is correct more than
half the time, and if the errors made by trackers are
not correlated, then score averaging is guaranteed
to improve performance (since the majority vote
will be correct in the limit). In (Lee and Eskenazi,
2013), score averaging (there called ?system com-
bination?) has been applied to combine the output
of four dialog state trackers. To help decorrelate
errors, constituent trackers were trained on differ-
ent subsets of data, and used different machine
learning methods. The relative error rate reduction
was 5.1% on the test set.
The second approach to ensemble learning is
stacking (Wolpert, 1992). In stacking, the scores
output by the constituent classifiers are fed to a
new classifier that makes a final prediction. In
other words, the output of each constituent classi-
fier is viewed as a feature, and the new final classi-
fier can learn the correlations and error patterns of
each. For this reason, stacking often outperforms
score averaging, particularly when errors are cor-
related. However, stacking requires a validation
set for training the final classifier. In DSTC2, we
only have access to trackers? output on the test set.
Therefore, to estimate the performance of stack-
ing, we perform cross-validation on the test set:
the test set is divided into two folds. First, fold 1
is used for training the final classifier, and fold 2
is used for testing. Then the process is reversed.
The two test outputs are then concatenated. Note
that models are never trained and tested on the
same data. A maximum entropy model (maxent) is
used (details in (Metallinou et al., 2013)), which is
common practice for stacking classifiers. In addi-
tion, maxent was found to yield best performance
in DSTC1 (Lee and Eskenazi, 2013).
Table 4 reports accuracy and L2 for goal con-
straints, search method, and requested slots. For
each ensemble method and each quantity (column)
the table gives results for combining the top track-
ers from 2 or 5 distinct teams, for combining the
top tracker from each team, and combining all
trackers (including the baselines as a team). For
example, the joint goal constraint ensemble with
the top 2 entries was built from team2.entry1 &
team4.entry0, and the method ensemble with the
top 2 entries from team2.entry4 & team4.entry0.
Table 4 shows two interesting trends. The first
is that score averaging does not improve perfor-
mance, and performance declines as more track-
ers are combined, yielding a statistically signifi-
cant decrease across all metrics. This suggests that
the errors of the different trackers are correlated,
which is unsurprising since they were trained on
the same data. On the other hand, stacking yields
a statistically significant improvement in accuracy
for goal constraints, and doesn?t degrade accuracy
for the search method and requested slots. For
stacking, the trend is that adding more trackers in-
creases performance ? for example, combining the
best tracker from every team improves goal con-
straint accuracy from 78.4% to 79.8%.
For completeness, we note that the additional
data could alternatively be used to improve the ac-
curacy of a constituent classifier; given the con-
straints of the challenge, we can?t assess the mag-
269
nitude of that improvement, so it is an open ques-
tion whether stacking is the best use of additional
data. Also, the training and test conditions of
the final stacking classifier are not mis-matched,
whereas in practice they would be. Nonethe-
less, this result does suggest that, if additional
data is available, stacking can be used to success-
fully combine multiple trackers and achieve per-
formance better than the single best tracker.
6 Conclusions
DSTC2 continues the tradition of DSTC1 by pro-
viding a common testbed for dialog state track-
ing, introducing some additional features relevant
to the research community? specifically a new
domain, changing user goals and a richer dialog
state. The data, evaluation scripts, and baseline
trackers will remain available and open to the re-
search community online.
Results from the previous challenge motivated
the selection of a few metrics as featured met-
rics, which facilitate comparisons between track-
ers. Analysis of the performance on the matched
development set and the mismatched test set sug-
gests that there still appears to be limitations on
generalisation, as found in DSTC1. The results
also suggest there are limitations in exploiting cor-
relations between slots, with few teams exploiting
joint distributions and the effects of doing so being
mixed. Investigating ensemble learning demon-
strates the effectiveness of combining tracker out-
puts. Ensemble learning exploits the strengths of
individual trackers to provide better quality output
than any constituent tracker in the group.
A follow up challenge, DSTC3, will present
the problem of adapting to a new domain with
very few example dialogs. Future work should
also verify that improvements in dialog state track-
ing translate to improvements in end-to-end dia-
log system performance. In this challenge, paid
subjects were used as users with real information
needs were not available. However, differences
between these two user groups have been shown
(Raux et al., 2005), so future studies should also
test on real users.
Acknowledgements
The authors thank the advisory committee for
their valuable input: Paul Crook, Maxine Eske-
nazi, Milica Ga?si?c, Helen Hastie, Kee-Eung Kim,
Sungjin Lee, Oliver Lemon, Olivier Pietquin,
Joelle Pineau, Deepak Ramachandran, Brian
Strope and Steve Young. The authors also thank
Zhuoran Wang for providing a baseline tracker,
and DJ Kim, Sungjin Lee & David Traum for com-
ments on evaluation metrics. Finally, thanks to
SIGdial for their endorsement, and to the partic-
ipants for making the challenge a success.
References
Matthew Henderson, Milica Ga?si?c, Blaise Thom-
son, Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012. Discriminative Spoken Language Under-
standing Using Word Confusion Networks. In Spo-
ken Language Technology Workshop, 2012. IEEE.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2013. Dialog State Tracking Challenge
2 & 3 Handbook. camdial.org/?mh521/dstc/.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2004. Evaluat-
ing discourse understanding in spoken dialogue sys-
tems. ACM Trans. Speech Lang. Process., Novem-
ber.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference.
Angeliki Metallinou, Dan Bohus, and Jason D.
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proc Association for
Computational Linguistics, Sofia.
Antoine Raux, Brian Langner, Dan Bohus, Alan W
Black, and Maxine Eskenazi. 2005. Let?s go public!
Taking a spoken dialog system to the real world.
G?okhan T?ur, Anoop Deoras, and Dilek Hakkani-T?ur.
2013. Semantic parsing using word confusion net-
works with conditional random fields. In INTER-
SPEECH.
Zhuoran Wang and Oliver Lemon. 2013. A simple
and generic belief tracking mechanism for the dia-
log state tracking challenge: On the believability of
observed information. In Proceedings of the SIG-
DIAL 2013 Conference.
Jason Williams, Antoine Raux, Deepak Ramachadran,
and Alan Black. 2013. The Dialog State Track-
ing Challenge. In Proceedings of the SIGDIAL 2013
Conference, Metz, France, August.
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5:241?259.
Steve Young, Catherine Breslin, Milica Ga?si?c,
Matthew Henderson, Dongho Kim, Martin Szum-
mer, Blaise Thomson, Pirros Tsiakoulis, and Eli
Tzirkel Hancock. 2013. Evaluation of Statistical
POMDP-based Dialogue Systems in Noisy Environ-
ment. In Proceedings of IWSDS, Napa, USA, Jan-
uary.
270
Appendix A: Featured results of evaluation
Tracker Inputs Joint Goal Constraints Search Method Requested Slots
team entry
Live
ASR
Live
SLU
Batch
ASR
Acc L2 ROC Acc L2 ROC Acc L2 ROC
0* 0 X 0.619 0.738 0.000 0.879 0.209 0.000 0.884 0.196 0.000
1 X 0.719 0.464 0.000 0.867 0.210 0.349 0.879 0.206 0.000
2 X 0.711 0.466 0.000 0.897 0.158 0.000 0.884 0.201 0.000
3 X
?
0.850 0.300 0.000 0.986 0.028 0.000 0.957 0.086 0.000
1 0 X 0.601 0.649 0.064 0.904 0.155 0.187 0.960 0.073 0.000
1 X 0.596 0.671 0.036 0.877 0.204 0.397 0.957 0.081 0.000
2 0 X X 0.775 0.758 0.063 0.944 0.092 0.306 0.954 0.073 0.383
1 X X X 0.784 0.735 0.065 0.947 0.087 0.355 0.957 0.068 0.446
2 X 0.668 0.505 0.249 0.944 0.095 0.499 0.972 0.043 0.300
3 X X X 0.771 0.354 0.313 0.947 0.093 0.294 0.941 0.090 0.262
4 X X X 0.773 0.467 0.140 0.950 0.082 0.351 0.968 0.050 0.497
3 0 X 0.729 0.452 0.000 0.878 0.210 0.000 0.889 0.188 0.000
4 0 X 0.768 0.346 0.365 0.940 0.095 0.452 0.978 0.035 0.525
1 X 0.746 0.381 0.383 0.939 0.097 0.423 0.977 0.038 0.490
2 X 0.742 0.387 0.345 0.922 0.124 0.447 0.957 0.069 0.340
3 X 0.737 0.406 0.321 0.922 0.125 0.406 0.957 0.073 0.385
5 0 X X 0.686 0.628 0.000 0.889 0.221 0.000 0.868 0.264 0.000
1 X X 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.000
2 X X 0.637 0.726 0.000 0.927 0.147 0.000 0.974 0.053 0.000
3 X X 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.000
4 X X 0.695 0.610 0.000 0.927 0.147 0.000 0.974 0.053 0.000
6 0 X 0.713 0.461 0.100 0.865 0.228 0.199 0.932 0.118 0.057
1 X 0.707 0.447 0.223 0.871 0.211 0.290 0.947 0.093 0.218
2 X 0.718 0.437 0.207 0.871 0.210 0.287 0.951 0.085 0.225
7 0 X 0.750 0.416 0.081 0.936 0.105 0.237 0.970 0.056 0.000
1 X 0.739 0.428 0.159 0.921 0.161 0.554 0.970 0.056 0.000
2 X 0.750 0.416 0.081 0.929 0.117 0.379 0.971 0.054 0.000
3 X 0.725 0.432 0.105 0.936 0.105 0.237 0.972 0.047 0.000
4 X 0.735 0.433 0.086 0.910 0.140 0.280 0.946 0.089 0.190
8 0 X 0.692 0.505 0.071 0.899 0.153 0.000 0.935 0.106 0.000
1 X 0.699 0.498 0.067 0.899 0.153 0.000 0.939 0.101 0.000
2 X 0.698 0.504 0.067 0.899 0.153 0.000 0.939 0.101 0.000
3 X 0.697 0.501 0.068 0.899 0.153 0.000 0.939 0.101 0.000
4 X 0.697 0.508 0.068 0.899 0.153 0.000 0.939 0.101 0.000
9 0 X 0.499 0.760 0.000 0.857 0.229 0.000 0.905 0.149 0.000
* The entries under team0 are the baseline systems mentioned in section 3.2.
?
team0.entry3 is the
oracle tracker, which uses the labels on the test set and limits itself to hypotheses suggested by the live
SLU.
The top score in each column is indicated by bold-type. The ROC metric is only comparable for trackers
operating at a similar accuracy, and so the highest values are not indicated.
271
Appendix B: Sample dialog, labels, and tracker output
S:
U:
Which part of town?
The north uh area
0.2 inform(food=north_african) area=north
method=byconstraints
requested=()
0.1 inform(area=north)
0.2 food=north_african
0.1 area=north
request(area)
inform(area=north)
0.9 byconstraints
0.1 none
0.0 phone
0.0 address
Actual input and output SLU hypotheses and scores Labels Example tracker output Correct?
S:
U:
Which part of town?
A cheap place in 
the north
inform(area=north, 
pricerange=cheap)
0.8 inform(area=north),
inform(pricerange=cheap)
area=north
pricerange=cheap
method=byconstraints
requested=()
0.1 inform(area=north)
0.7 area=north
pricerange=cheap
0.1 area=north
food=north_african
request(area)
0.9 byconstraints
0.1 none
0.0 phone
0.0 address
S:
U:
Clown caf? is a cheap 
restaurant in the 
north part of town.
Do you have any 
others l ike that, 
maybe in the south 
part of town?
reqalts(area=south)
0.7 reqalts(area=south) area=south
pricerange=cheap
method=byalternatives
requested=()
0.2 reqmore()
0.8 area=south
pricerange=cheap
0.1 area=north
pricerange=cheap
0.6 byalternatives
0.2 byconstraints
0.0 phone
0.0 address
S:
U:
Galleria is a cheap 
restaurant in the 
south.
What is their phone 
number and 
address?
request(phone), 
request(address)
0.6 request(phone) area=south
pricerange=cheap
method=byalternatives
requested= (phone, 
address)
0.2 request(phone),
request(address)
0.9 area=south
pricerange=cheap
0.1 area=north
pricerange=cheap
0.5 byconstraints 
0.4 byalternatives
0.8 phone
0.3 address
0.1 request(address)
0.7 ()
0.2 ()
0.1 ()
0.0 ()
Example dialog illustrating DSTC2 data, labels, and evaluation procedure. The left column shows the
actual system output and user input. The second column shows two SLU N-Best hypothesis and their
scores. In practice, up to 10 SLU N-Best hypotheses are output. In the right 3 columns, the three shaded
regions correspond to the three components of the dialog state output by a tracker at each turn. The blue
region corresponds to the user?s joint goal constraint; the red region to the user?s search method; and
the yellow region to the slots requested by the user. For space, only 2 of the 5 methods and 2 of the
8 requestable slots are shown. The third column shows the label (correct output) for each component.
The fourth column shows example tracker output for each of these three quantities, and the fifth column
indicates correctness. A goal constraint is correct if it exactly matches the label. Therefore, 0 or 1 of
the output goal constraints is correct, and all the others are incorrect. Accuracy is determined by the
correctness of the goal constraint with the highest tracker score. For search method, exactly one method
is correct at each turn, so correctness is determined by comparing the maximum scoring method to the
label. For requested slots, each slot can be requested (or not) in the same turn, so each requestable slot
is separately marked as correct or incorrect. The quantity requested.all averages the correctness of all
requested slots.
272
Proceedings of the SIGDIAL 2014 Conference, pages 292?299,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Word-Based Dialog State Tracking
with Recurrent Neural Networks
Matthew Henderson, Blaise Thomson and Steve Young
Department of Engineering,
University of Cambridge, U.K.
{mh521, brmt2, sjy}@eng.cam.ac.uk
Abstract
Recently discriminative methods for track-
ing the state of a spoken dialog have been
shown to outperform traditional generative
models. This paper presents a new word-
based tracking method which maps di-
rectly from the speech recognition results
to the dialog state without using an explicit
semantic decoder. The method is based on
a recurrent neural network structure which
is capable of generalising to unseen dialog
state hypotheses, and which requires very
little feature engineering. The method
is evaluated on the second Dialog State
Tracking Challenge (DSTC2) corpus and
the results demonstrate consistently high
performance across all of the metrics.
1 Introduction
While communicating with a user, statistical spo-
ken dialog systems must maintain a distribution
over possible dialog states in a process called di-
alog state tracking. This distribution, also called
the belief state, directly determines the system?s
decisions. In MDP-based systems, only the most
likely dialog state is considered and in this case
the primary metric is dialog state accuracy (Bo-
hus and Rudnicky, 2006). In POMDP-based sys-
tems, the full distribution is considered and then
the shape of the distribution as measured by an L2
norm is equally important (Young et al., 2009). In
both cases, good quality state tracking is essential
to maintaining good overall system performance.
Typically, state tracking has assumed the output
of a Spoken Language Understanding (SLU) com-
ponent in the form of a semantic decoder, which
maps the hypotheses from Automatic Speech
Recognition (ASR) to a list of semantic hypothe-
ses. This paper considers mapping directly from
ASR hypotheses to an updated belief state at each
turn in the dialog, omitting the intermediate SLU
processing step. This word-based state tracking
avoids the need for an explicit semantic represen-
tation and also avoids the possibility of informa-
tion loss at the SLU stage.
Recurrent neural networks (RNNs) provide a
natural model for state tracking in dialog, as
they are able to model and classify dynamic se-
quences with complex behaviours from step to
step. Whereas, most previous approaches to dis-
criminative state tracking have adapted station-
ary classifiers to the temporal process of dialog
(Bohus and Rudnicky, 2006; Lee and Eskenazi,
2013; Lee, 2013; Williams, 2013; Henderson et
al., 2013b). One notable exception is Ren et al.
(2013), which used conditional random fields to
model the sequence temporally.
Currently proposed methods of discriminative
state tracking require engineering of feature func-
tions to represent the turn in the dialog (Ren et
al., 2013; Lee and Eskenazi, 2013; Lee, 2013;
Williams, 2013; Henderson et al., 2013b). It is un-
clear whether differences in performance are due
to feature engineering or the underlying models.
This paper proposes a method of using simple n-
gram type features which avoid the need for fea-
ture engineering. Instead of using inputs with a se-
lect few very informative features, the approach is
to use high-dimensional inputs with all the infor-
mation to potentially reconstruct any such hand-
crafted feature. The impact of significantly in-
creasing the dimensionality of the inputs is man-
aged by careful initialisation of model parameters.
Accuracy on unseen or infrequent slot values
is an important concern, particularly for discrim-
inative classifiers which are prone to overfitting
training data. This is addressed by structuring
the recurrent neural network to include a compo-
nent which is independent of the actual slot value
in question. It thus learns general behaviours for
specifying slots enabling it to successfully decode
292
ASR output which includes previously unseen slot
values.
In summary, this paper presents a word-based
approach to dialog state tracking using recurrent
neural networks. The model is capable of gen-
eralising to unseen dialog state hypotheses, and
requires very little feature engineering. The ap-
proach is evaluated in the second Dialog State
Tracking Challenge (DSTC2) (Henderson et al.,
2014) where it is shown to be extremely competi-
tive, particularly in terms of the quality of its con-
fidence scores.
Following a brief outline of DSTC2 in section
2, the definition of the model is given in section
3. Section 4 then gives details on the initialisation
methods used for training. Finally results on the
DSTC2 evaluation are given in 5.
2 The Second Dialog State Tracking
Challenge
This section describes the domain and method-
ology of the second Dialog State Tracking Chal-
lenge (DSTC2). The challenge is based on a
large corpus collected using a variety of telephone-
based dialog systems in the domain of finding a
restaurant in Cambridge. In all cases, the subjects
were recruited using Amazon Mechanical Turk.
The data is split into a train, dev and test set.
The train and dev sets were supplied with labels,
and the test set was released unlabelled for a one
week period. At the end of the week, all partici-
pants were required to submit their trackers? out-
put on the test set, and the labels were revealed. A
mis-match was ensured between training and test-
ing conditions by choosing dialogs for the eval-
uation collected using a separate dialog manager.
This emulates the mis-match a new tracker would
encounter if it were actually deployed in an end-
to-end system.
In summary, the datasets used are:
? dstc2 train - Labelled training consisting of
1612 dialogs with two dialog managers and
two acoustic conditions.
? dstc2 dev - Labelled dataset consisting
of 506 calls in the same conditions as
dstc2 train, but with no caller in common.
? dstc2 test - Evaluation dataset consisting of
1117 dialogs collected using a dialog man-
ager not seen in the labelled data.
In contrast with DSTC1, DSTC2 introduces dy-
namic user goals, tracking of requested slots and
tracking the restaurant search method. A DSTC2
tracker must therefore report:
? Goals: A distribution over the user?s goal for
each slot. This is a distribution over the possi-
ble values for that slot, plus the special value
None, which means no valid value has been
mentioned yet.
? Requested slots: A reported probability for
each requestable slot that has been requested
by the user, and should be informed by the
system.
? Method: A distribution over methods, which
encodes how the user is trying to use the di-
alog system. E.g. ?by constraints?, when the
user is trying to constrain the search, and ?fin-
ished?, when the user wants to end the dialog.
A tracker may report the goals as a joint over
all slots, but in this paper the joint is reported as a
product of the marginal distributions per slot.
Full details of the challenge are given in Hen-
derson et al. (2013a), Henderson et al. (2014). The
trackers presented in this paper are identified un-
der ?team4? in the reported results.
3 Recurrent Neural Network Model
This section defines the RNN structure used for
dialog state tracking. One such RNN is used per
slot, taking the most recent dialog turn (user input
plus last machine dialog act) as input, updating its
internal memory and calculating an updated belief
over the values for the slot. In what follows, the
notation a?b is used to denote the concatenation
of two vectors, a and b. The i
th
component of the
vector a is written a|
i
.
3.1 Feature Representation
Extracting n-grams from utterances and dialog
acts provides the feature representations needed
for input into the RNN. This process is very sim-
ilar to the feature extraction described in Hender-
son et al. (2012), and is outlined in figure 1.
For n-gram features extracted from the ASR
N -best list, unigram, bigram and trigram features
are calculated for each hypothesis. These are
then weighted by the N -best list probabilities and
summed to give a single vector.
Dialog acts in this domain consist of
a list of component acts of the form
acttype(slot=value) where the slot=value
pair is optional. The n-gram type features
293
extracted from each such component act are
?acttype?, ?slot?, ?value?, ?acttype
slot?, ?slot value? and ?acttype slot
value?, or just ?acttype? for the act acttype().
Each feature is given weight 1, and the features
from individual component acts are summed.
To provide a contrast, trackers have also been
implemented using the user dialog acts output by
an SLU rather than directly from the ASR output.
In this case, the SLU N -best dialog act list is en-
coded in the same way except that the n-grams
from each hypothesis are weighted by the corre-
sponding probabilities, and summed to give a sin-
gle feature vector.
Consider a word-based tracker which takes an
ASR N -best list and the last machine act as input
for each turn, as shown in figure 1. A combined
feature representation of both the ASR N -best list
and the last machine act is obtained by concate-
nating the vectors. This means that in figure 1 the
food feature from the ASR and the food feature
from the machine act contribute to separate com-
ponents of the final vector f .
fv
ASR
foodjamaican
indian food
1.00.9
0.1
<value> food<value> 0.90.9
Machine Act
confirm food
confirm food jamaican
food jamaican
1.0
1.0
1.0
e.g. v = jamaican
confirm food <value>
food <value>
1.0
1.0
for each value, v
jamaican food 0.9
<slot>
<value> 
1.01.0<value> <slot> 1.0
confirm 1.0
<value> 1.0
confirm <slot> <value>
<slot> <value>
1.0
1.0
<value> 1.0
indian 0.1
<value> food 1.0jamaican <slot> 0.9indian <slot> 0.1
confirm food <value> 1.0
food <value> 1.0
confirm <slot> jamaican 1.0
<slot> jamaican 1.0
0.9jamaican food 0.1indian food confirm(food=jamaican)
food 1.0
<slot> 1.0
fs
f
5 non-zero elements
6 non-zero elements
2 non-zero elements
6 non-zero elements
8 non-zero elements
3 non-zero elements
11 non-zero elements
14 non-zero elements
5 non-zero elements
jamaican 1.0
Figure 1: Example of feature extraction for one
turn, giving f , f
s
and f
v
. Here s = food. For all
v /?{indian, jamaican}, f
v
= 0.
Note that all the methods for tracking reported
in DSTC1 required designing feature functions.
For example, suggested feature functions included
the SLU score in the current turn, the probabil-
ity of an ?affirm? act when the value has been
confirmed by the system, the output from base-
line trackers etc. (e.g. Lee and Eskenazi (2013),
Williams (2013), Henderson et al. (2013b)). In
contrast, the approach described here is to present
the model with all the information it would need
to reconstruct any feature function that might be
useful.
3.2 Generalisation to Unseen States
One key issue in applying machine learning to the
task of dialog state tracking is being able to deal
with states which have not been seen in training.
For example, the system should be able to recog-
nise any obscure food type which appears in the
set of possible food types. A na??ve neural net-
work structure mapping n-gram features to an up-
dated distribution for the food slot, with no tying
of weights, would require separate examples of
each of the food types to learn what n-grams are
associated with each. In reality however n-grams
like ?<value> food? and ?serving <value>? are likely
to correspond to the hypothesis food=?<value>? for
any food-type replacing ?<value>?.
The approach taken here is to embed a network
which learns a generic model of the updated belief
of a slot-value assignment as a function of ?tagged?
features, i.e. features which ignore the specific
identity of a value. This can be considered as re-
placing all occurrences of a particular value with
a tag like ?<value>?. Figure 1 shows the process of
creating the tagged feature vectors, f
s
and f
v
from
the untagged vector f .
3.3 Model Definition
In this section an RNN is described for tracking
the goal for a given slot, s, throughout the se-
quence of a dialog. The RNN holds an internal
memory, m ? R
N
mem
which is updated at each
step. If there are N possible values for slot s, then
the probability distribution output p is in R
N+1
,
with the last component p|
N
giving the probabil-
ity of the None hypothesis. Figure 2 provides an
overview of how p and m are updated in one turn
to give the new belief and memory, p
?
and m
?
.
One part of the neural network is used to learn
a mapping from the untagged inputs, full memory
and previous beliefs to a vector h ? R
N
which
goes directly into the calculation of p
?
:
h = NNet (f ? p?m) ? R
N
294
p m
hN. Net.
g v
p v
N. Net.for each value, v
h+g
p?softmax m?logisticfor each slot,  s
f
fs
fv
pN
Figure 2: Calculation of p
?
and m
?
for one turn
where NNet(?) denotes a neural network function
of the input. In this paper all such networks have
one hidden layer with a sigmoidal activation func-
tion.
The sub-network for h requires examples of ev-
ery value in training, and is prone to poor general-
isation as explained in section 3.2. By including a
second sub-network for g which takes tagged fea-
tures as input, it is possible to exploit the obser-
vation that the string corresponding to a value in
various contexts is likely to be good evidence for
or against that value. For each value v, a compo-
nent of g is calculated using the neural network:
g|
v
= NNet
(
f? f
s
? f
v
?
{p|
v
, p|
N
} ?m
)
? R
By using regularisation, the learning will pre-
fer where possible to use the sub-network for g
rather than learning the individual weights for
each value required in the sub-network for h. This
sub-network is able to deal with unseen or infre-
quently seen dialog states, so long as the state can
be tagged in the feature extraction. This model can
also be shared across slots since f
s
is included as
an input, see section 4.2.
The sub-networks applied to tagged and un-
tagged inputs are combined to give the new belief:
p
?
= softmax ([h + g]? {B}) ? R
N+1
where B is a parameter of the RNN, contributing
to the None hypothesis. The contribution from g
may be seen as accounting for general behaviour
of tagged hypotheses, while h makes corrections
due to correlations with untagged features and
value specific behaviour e.g. special ways of ex-
pressing specific goals and fitting to specific ASR
confusions.
Finally, the memory is updated according to the
logistic regression:
m
?
= ? (W
m
0
f +W
m
1
m) ? R
N
mem
where the W
m
i
are parameters of the RNN.
3.4 Requested Slots and Method
A similar RNN is used to track the requested slots.
Here the v runs over all the requestable slots, and
requestable slot names are tagged in the feature
vectors f
v
. This allows the neural network calcu-
lating g to learn general patterns across slots just
as in the case of goals. The equation for p
?
is
changed to:
p
?
= ? (h + g)
so each component of p
?
represents the probability
(between 0 and 1) of a slot being requested.
For method classification, the same RNN struc-
ture as for a goal is used. No tagging of the feature
vectors is used in the case of methods.
4 Training
The RNNs are trained using Stochastic Gradient
Descent (SGD), maximizing the log probability of
the sequences of observed beliefs in the training
data (Bottou, 1991). Gradient clipping is used to
avoid the problem of exploding gradients (Pascanu
et al., 2012). A regularisation term is included,
which penalises the l2 norm of all the parameters.
It is found empirically to be beneficial to give more
weight in the regularisation to the parameters used
in the network calculating h.
When using the ASR N -best list, f is typi-
cally of dimensionality around 3500. With so
many weights to learn, it is important to initialise
the parameters well before starting the SGD algo-
rithm. Two initialisation techniques have been in-
vestigated, the denoising autoencoder and shared
initialisation. These were evaluated by training
trackers on the dstc2 train set, and evaluating on
dstc2 dev (see table 1).
4.1 Denoising Autoencoder
The denoising autoencoder (dA), which provides
an unsupervised method for learning meaningful
295
Joint Goals Method Requested
Shared
init.
dA
init.
Acc L2 Acc L2 Acc L2
0.686 0.477 0.913 0.147 0.963 0.059
X 0.688 0.466 0.915 0.144 0.962 0.059
X 0.680 0.479 0.910 0.152 0.962 0.059
X X 0.696 0.463 0.915 0.144 0.965 0.057
Baseline: 0.612 0.632 0.830 0.266 0.894 0.174
Table 1: Performance on the dev set when varying initialisation techniques for word-based tracking. Acc
denotes the accuracy of the most likely belief at each turn, and L2 denotes the squared l2 norm between
the estimated belief distribution and correct (delta) distribution. For each row, 5 trackers are trained
and then combined using score averaging. The final row shows the results for the focus-based baseline
tracker (Henderson et al., 2014).
underlying representations of the input, has been
found effective as an initialisation technique in
deep learning (Vincent et al., 2008).
A dA is used to initialise the parameters of the
RNN which multiply the high-dimensional input
vector f . The dA learns a matrix W
dA
which re-
duces f to a lower dimensional vector such that
the original vector may be recovered with minimal
loss in the presence of noise.
For learning the dA, f is first mapped such that
feature values lie between 0 and 1. The dA takes as
input f
noisy
, a noisy copy of f where each compo-
nent is set to 0 with probability p. This is mapped
to a lower dimensional hidden representation h:
h = ? (W
dA
f
noisy
+ b
0
)
A reconstructed vector, f
rec
, is then calculated
as:
f
rec
= ?
(
W
T
dA
h+ b
1
)
The cross-entropy between f and f
rec
is used as
the objective function in gradient descent, with an
added l1 regularisation term to ensure the learning
of sparse weights. As the ASR features are likely
to be very noisy, dense weights would be prone to
overfitting the examples.
1
When using W
dA
to initialise weights in the
RNN, training is observed to converge faster. Ta-
ble 1 shows that dA initialisation leads to better
solutions, particularly for tracking the goals.
4.2 Shared Initialisation
It is possible to train a slot-independent RNN, us-
ing training data from all slots, by not including h
in the model (the dimensionality of h is dependent
1
The state-of-the-art in dialog act classification with very
similar data also uses sparse weights Chen et al. (2013).
on the slot). In shared initialisation, such an RNN
is trained for a few epochs, then the learnt param-
eters are used to initialise slot-dependent RNNs
for each slot. This follows the shared initialisation
procedure presented in Henderson et al. (2013b).
Table 1 suggests that shared initialisation when
combined with dA initialisation gives the best per-
formance.
4.3 Model Combination
In DSTC1, the most competitive results were
achieved with model combination whereby the
output of multiple trackers were combined to give
more accurate classifications (Lee and Eskenazi,
2013). The technique for model combination used
here is score averaging, where the final score for
each component of the dialog state is computed as
the mean of the scores output by all the trackers
being combined. This is one of the simplest meth-
ods for model combination, and requires no extra
training data. It is guaranteed to improve the accu-
racy if the outputs from the individual trackers are
not correlated, and the individual trackers operate
at an accuracy > 0.5.
Multiple runs of training the RNNs were found
to give results with high variability and model
combination provides a method to exploit this
variability. In order to demonstrate the effect,
10 trackers with varying regularisation parame-
ters were trained on dstc2 train and used to track
dstc2 dev. Figure 3 shows the effects of combin-
ing these trackers in larger groups. The mean ac-
curacy in the joint goals from combining m track-
ers is found to increase with m. The single output
from combining all 10 trackers outperforms any
single tracker in the group.
The approach taken for the DSTC2 challenge
was therefore to train multiple trackers with vary-
296
Accuracy
# trackers combined, m
1 2 3 4 5 6 7 8 9 10
0.64
0.65
0.66
0.67
0.68
0.69
0.70
0.71
0.72
Figure 3: Joint goal accuracy on dstc2 dev from system
combination. Ten total trackers are trained with varying reg-
ularisation parameters. For each m = 1 . . . 10, all subsets
of size m of the 10 trackers are used to generate
10
C
m
com-
bined results, which are plotted as a boxplot. Boxplots show
minimum, maximum, the interquartile range and the median.
The mean values are plotted as connected points.
ing model hyper-parameters (e.g. regularisation
parameters, memory size) and combine their out-
put using score averaging. Note that maintaining
around 10 RNNs for each dialog state components
is entirely feasible for a realtime system, as the
RNN operations are quick to compute. An un-
optimised Python implementation of the tracker
including an RNN for each dialog state compo-
nent is able to do state tracking at a rate of around
50 turns per second on an Intel? Core? i7-970
3.2GHz processor.
5 Results
The strict blind evaluation procedure defined for
the DSTC2 challenge was used to investigate the
effect on performance of two contrasts. The first
contrast compares word-based tracking and con-
ventional tracking based on SLU output. The sec-
ond contrast investigates the effect of including
and omitting the sub-network for h in the RNN.
Recall h is the part of the model that allows learn-
ing special behaviours for particular dialog state
hypotheses, and correlations with untagged fea-
tures. These two binary contrasts resulted in a to-
tal of 4 system variants being entered in the chal-
lenge.
Each system is the score-averaged combined
output of 12 trackers trained with varying hyper-
parameters (see section 4.3). The performance of
the 4 entries on the featured metrics of the chal-
lenge are shown in table 2.
It should be noted that the live SLU used the
word confusion network, not made available in the
challenge. The word confusion network is known
to provide stronger features than theN -best list for
language understanding (Henderson et al., 2012;
T?ur et al., 2013), so the word-based trackers us-
ing N -best ASR features were at a disadvantage
in that regard. Nevertheless, despite this hand-
icap, the best results were obtained from word-
based tracking directly on the ASR output, rather
than using the confusion network generated SLU
output. Including h always helps, though this is
far more pronounced for the word-based track-
ers. Note that trackers which do not include h are
value-independent and so are capable of handling
new values at runtime.
The RNN trackers performed very competi-
tively in the context of the challenge. Figure 4 vi-
sualises the performance of the four trackers rela-
tive to all the entries submitted to the challenge for
the featured metrics. For full details of the evalua-
tion metrics see Henderson et al. (2014). The box
in this figure gives the entry IDs under which the
results are reported in the DSTC (under the team
ID ?team4?). The word-based tracker including
h (h-ASR), was top for joint goals L2 as well as
requested slots accuracy and L2. It was close to
the top for the other featured metrics, following
closely entries from team 2. The RNN trackers
performed particularly well on measures assessing
the quality of the scores such as L2.
There are hundreds of numbers reported in the
DSTC2 evaluation, and it was found that the h-
ASR tracker ranked top on many of them. Consid-
ering L2, accuracy, average probability, equal er-
ror rate, log probability and mean reciprocal rank
across all components of the the dialog state, these
give a total of 318 metrics. The h-ASR tracker
ranked top of all trackers in the challenge in 89 of
these metrics, more than any other tracker. The
ASR tracker omitting h came second, ranking top
in 33 of these metrics.
The trackers using SLU features ranked top
in all of the featured metrics among the trackers
which used only the SLU output.
6 Conclusions
The RNN framework presented in this paper pro-
vides very good performance in terms of both ac-
curacy and the quality of reported probability dis-
tributions. Word-based tracking is shown to be one
of the most competitive approaches submitted to
DSTC2. By mapping straight from the ASR out-
put to a belief update, it avoids any information
297
Tracker
Inputs
Joint Goals Method Requested
entry
Include
h
Live
ASR
Live
SLU
Acc L2 ROC Acc L2 ROC Acc L2 ROC
0 X X 0.768 0.346 0.365 0.940 0.095 0.452 0.978 0.035 0.525
1 X 0.746 0.381 0.383 0.939 0.097 0.423 0.977 0.038 0.490
2 X X 0.742 0.387 0.345 0.922 0.124 0.447 0.957 0.069 0.340
3 X 0.737 0.406 0.321 0.922 0.125 0.406 0.957 0.073 0.385
Table 2: Featured metrics on the test set for the 4 RNN trackers entered to the challenge.
0.4
1.0
0.6
0.8
0.0
0.8
Accuracy
Joint Goals Method Requested All
0.2
0.4
0.6
L2
entry0
entry2
entry1
entry3
word-based
SLU input
full model no h
baseline
Figure 4: Relative performance of RNN trackers for fea-
tured metrics in DSTC2. Each dash is one of the 34 trackers
evaluated in the challenge. Note a lower L2 is better. ROC
metric is only comparable for systems of similar accuracies,
so is not plotted. The focus baseline system is shown as a
circle.
lost in the omitted SLU step.
In general, the RNN appears to be a promising
model, which deals naturally with sequential input
and outputs. High dimensional inputs are handled
well, with little feature engineering, particularly
when carefully initialised (e.g. as here using de-
noising autoencoders and shared initialisation).
Future work should include making joint pre-
dictions on components of the dialog state. In this
paper each component was tracked using its own
RNN. Though not presented in this paper, no im-
provement could be found by joining the RNNs.
However, this may not be the case for other do-
mains in which slot values are more highly cor-
related. The concept of tagging the feature func-
tions allows for generalisation to unseen values
and slots. This generalisation will be explored in
future work, particularly for dialogs in more open-
domains.
Acknowledgements
Matthew Henderson is a Google Doctoral Fellow.
References
Dan Bohus and Alex Rudnicky. 2006. A K-
hypotheses+ Other Belief Updating Model. Proc.
of the AAAI Workshop on Statistical and Empirical
Methods in Spoken Dialogue Systems.
L?eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, N??mes, France. EC2.
Yun-Nung Chen, William Yang Wang, and Alexan-
der I Rudnicky. 2013. An empirical investigation of
sparse log-linear models for improved dialogue act
classification. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2013 IEEE International Confer-
ence on.
Matthew Henderson, Milica Ga?si?c, Blaise Thom-
son, Pirros Tsiakoulis, Kai Yu, and Steve Young.
2012. Discriminative Spoken Language Under-
standing Using Word Confusion Networks. In Spo-
ken Language Technology Workshop, 2012. IEEE.
298
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2013a. Dialog State Tracking Challenge
2 & 3 Handbook. camdial.org/
?
mh521/dstc/.
Matthew Henderson, Blaise Thomson, and Steve
Young. 2013b. Deep Neural Network Approach for
the Dialog State Tracking Challenge. In Proceed-
ings of SIGdial, Metz, France, August.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe for
building robust spoken dialog state trackers: Dialog
state tracking challenge system description. In Pro-
ceedings of the SIGDIAL 2013 Conference, Metz,
France, August.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. In Proceedings of the SIG-
DIAL 2013 Conference, Metz, France, August.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. Understanding the exploding gradient prob-
lem. CoRR.
Hang Ren, Weiqun Xu, Yan Zhang, and Yonghong Yan.
2013. Dialog state tracking using conditional ran-
dom fields. In Proceedings of the SIGDIAL 2013
Conference, Metz, France, August.
G?okhan T?ur, Anoop Deoras, and Dilek Hakkani-T?ur.
2013. Semantic parsing using word confusion net-
works with conditional random fields. In INTER-
SPEECH.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th International
Conference on Machine Learning, Helsinki, Fin-
land.
Jason Williams. 2013. Multi-domain learning and gen-
eralization in dialog state tracking. In Proceedings
of the SIGDIAL 2013 Conference, Metz, France, Au-
gust.
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2009. The Hidden Information State model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech & Lan-
guage.
299
