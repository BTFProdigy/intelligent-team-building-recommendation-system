Proceedings of the ACL-HLT 2011 Student Session, pages 88?93,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Combining Indicators of Allophony
Luc Boruta
Univ. Paris Diderot, Sorbonne Paris Cite?, ALPAGE, UMR-I 001 INRIA, F-75205, Paris, France
LSCP, De?partement d?E?tudes Cognitives, E?cole Normale Supe?rieure, F-75005, Paris, France
luc.boruta@inria.fr
Abstract
Allophonic rules are responsible for the great
variety in phoneme realizations. Infants can
not reliably infer abstract word representa-
tions without knowledge of their native allo-
phonic grammar. We explore the hypothe-
sis that some properties of infants? input, re-
ferred to as indicators, are correlated with al-
lophony. First, we provide an extensive evalu-
ation of individual indicators that rely on dis-
tributional or lexical information. Then, we
present a first evaluation of the combination of
indicators of different types, considering both
logical and numerical combinations schemes.
Though distributional and lexical indicators
are not redundant, straightforward combina-
tions do not outperform individual indicators.
1 Introduction
Though the phonemic inventory of a language is typ-
ically small, phonetic and phonological processes
yield manifold variants1 for each phoneme. Words
too are affected by this variability, yielding different
realizations for a given underlying form. Allophonic
rules relate phonemes to their variants, expressing
the contexts in which the latter occur. We are in-
terested in describing procedures by which infants,
learning their native allophonic grammar, could re-
duce the variation and recover words. Combining in-
sights from both computational and behavioral stud-
ies, we endorse the hypothesis that infants are good
distributional learners (Maye et al, 2002; Saffran
et al, 1996) and that they may ?bootstrap? into lan-
guage tracking statistical regularities in the signal.
1We use allophony as an umbrella term for the continuum
ranging from typical allophones to mere coarticulatory variants.
We seek to identify which features of infants? in-
put are most reliable for learning allophonic rules. A
few indicators, based on distributional (Peperkamp
et al, 2006) and lexical (Martin et al, submitted) in-
formation, have been described and validated in sil-
ico.2 Yet, other aspects have barely been addressed,
e.g. the question of whether or not these indicators
capture different aspects of allophony and, if so,
which combination scheme yields better results.
We present an extensive evaluation of individual
indicators and, based on theoretical and empirical
desiderata, we outline a more comprehensive frame-
work to model the acquisition of allophonic rules.
2 Indicators of allophony
We build upon Peperkamp et al?s framework: the
task is to induce a two-class classifier deciding, for
every possible pair of segments, whether or not they
realize the same phoneme. Discrimination relies on
indicators, i.e. linguistic properties which are corre-
lated with allophony. As a model of language acqui-
sition, this classifier is induced without supervision.
In line with previous studies, we assume that in-
fants are able to segment the continuous stream of
acoustic input into a sequence of discrete segments,
and that they quantize each of these segments into
one of a finite number of phonetic categories. Quan-
tization is a necessary assumption for the framework
to apply. However, the larger the set of phonetic cat-
egories, the closer we get to recent ?single-stage? ap-
proaches (e.g. work by Dillon et al, in preparation)
where phonological categories are acquired directly
from raw infant-directed speech.
2See also the work of Dautriche (2009) on acoustic indica-
tors of allophony, albeit using adult-directed speech.
88
2.1 Distributional indicators
Complementary distribution is a ubiquitous criterion
for the discovery of phonemes. If two segments oc-
cur in mutually exclusive contexts, the two may be
realizations of the same phoneme.
Bearing in mind that the signal may be noisy,
Peperkamp et al (2006) looked for segments in
near-complementary distributions. Using the sym-
metrised Kullback?Leibler divergence (henceforth
KL), they compared the probability distributions of
how often the contexts of each segment occur. In a
follow-up study, Le Calvez (2007) compared KL to
other indicators, namely the Jensen?Shannon diver-
gence (JS) and the Bhattacharyya coefficient (BC).3
2.2 Lexical indicators
Adjacent segments can condition the realization of
a word?s initial and final phonemes. If two words
only differ by their initial or final segments, these
segments may be realizations of the same phoneme.
Instantiating the general concept of functional load
(Hockett, 1955), lexical indicators gauge the degree
of contrast in the lexicon between two segments.
Using the simplest expression of functional load,
Martin et al (submitted) defined a Boolean-valued
indicator, FL, satisfied by a single pair of minimally
different words. As a result, FL is sensitive to noise.
We define a finer-grained variant, FL*, which tallies
the number of such pairs. Moreover, as words get
longer, it becomes increasingly unlikely that such
word pairs occur by chance. Thus, for any such pair,
FL* is incremented by the length of those words.
We also propose an information-theoretic lexi-
cal indicator, HFL, based on Hockett?s definition of
functional load. HFL accounts for the fraction of
information content, represented by the language?s
word entropy, that is lost when the opposition be-
tween two segments is neutralized. The ?broken
typewriter? function used for neutralization guaran-
tees that values lie in [0, 1] (Coolen et al, 2005).
3 Corpora and experimental setup
In the absence of phonetic transcriptions of infant-
directed speech, and as the number of allophones in-
3As for the actual computations, we use the same definitions
as Le Calvez (2007) except that, as BC increases when distribu-
tions overlap and 0 ? BC ? 1, we actually use 1?BC.
fants must learn is unknown (if assessable at all), we
use Boruta et al?s (submitted) corpora. They created
a range of possible inputs, applying artificial allo-
phonic grammars4 of different sizes (Boruta, 2011)
to the now-standard CHILDES ?Brent/Ratner? cor-
pus of English (Brent and Cartwright, 1996). We
quantify the amount of variation in a corpus by its
allophonic complexity, i.e. the ratio of the number of
phones to the number of phonemes in the language.
Lexical indicators require an ancillary procedure
yielding a lexicon. Martin et al approximated a lex-
icon by a list of frequent n-grams. Here, the lexicon
is induced from the output of an explicit word seg-
mentation model, viz. Venkataraman?s incremental
(2001) model, using the unsegmented phonetic cor-
pora as the input. Though, obviously, infants can
not access it, we use the lexicon derived from the
CHILDES orthographic transcripts for reference.
4 Indicators? discriminant power
As the aforementioned indicators have been evalu-
ated using various languages, allophonic grammars
and measures, we present a unified evaluation, con-
ducted using Sing et al?s (2005) ROCR package.
4.1 Evaluation
Non-Boolean indicators require a threshold at and
above which pairs are classified as allophonic. We
evaluate indicators across all possible discrimination
thresholds, reporting the area under the ROC curve
(henceforth AUC). Equivalent to Martin et al?s ?,
values lie in [0, 1] and are equal to the probability
that a randomly drawn allophonic pair will score
higher than a randomly drawn non-allophonic pair;
.5 thus indicates random prediction.
Moreover, we evaluate indicators? misclassifica-
tions at the discrimination threshold maximizing
Matthews? (1975) correlation coefficient: let ?, ?, ?
and ? be, respectively, the number of false positives,
false negatives, true positives and true negatives,
MCC = (?????)/
?
(?+?)(?+?)(?+?)(?+?).
Values of 1, 0 and ?1 indicate perfect, random and
inverse prediction, respectively. This coefficient is
more appropriate than the accuracy or the F-measure
4Because all allophonic rules implemented in the corpora
are of the type p ? a / c, FL and FL* only look for words
minimally differing by their last segments.
89
when, as here, the true classes have very differ-
ent sizes.5 Using this optimal, MCC-maximizing
threshold, we report the maximal MCC and, as per-
centages, the accuracy (Acc), the false positive rate
(FPR) and the false negative rate (FNR).
4.2 Results and discussion
Indicators? AUC corroborate previous results for
distributional indicators: they perform almost iden-
tically and do not accommodate high allophonic
complexities at which they perform below chance
(Figure 1.a) because, as suggested by Martin et
al., every segment has an extremely narrow distri-
bution and complementary distribution is the rule
rather than the exception. By contrast, all three
lexical indicators are much more robust even if, as
predicted, FL?s coarseness impedes its discriminant
power (Figure 1.b).6 The reason why FL* outper-
forms HFL may be due to the very definition of
HFL?s broken typewriter function: as the segments,
e.g. {x, y}, are collapsed into a single symbol, the
indicator captures not only minimal alternations like
wx ? wy, but also word pairs such as xy ? yx.
AUC curves suggest that, for each type, indi-
cators converge at medium allophonic complexity.
Thus, misclassification scores are reported in Table 1
only at low (2 allophones/phoneme) and medium (9)
complexities. Previous observations are confirmed
by MCC and accuracy values: though all indicators
are positively correlated with the underlying allo-
phonic relation, correlation is stronger for lexical in-
dicators. Surprisingly, zero FPR values are observed
for some lexical indicators, meaning that they make
no false alarms and, as a consequence, that all errors
are caused by missed allophonic pairs.
5 Indicators? redundancy
None of the indicators we benchmarked in the previ-
ous section makes a perfect discrimination between
allophonic and non-allophonic pairs of segments.
5If p phonemes have on average a allophones, out of the
pa(pa?1)/2 possible pairs, only pa(a?1)/2 are allophonic,
and a dummy indicator that rejects all pairs achieves a constant
accuracy of 1? (a?1)/(pa?1), which is greater than 98%
for any of our corpora. Besides, the computation of precision,
recall and the F-measure do not take true negatives into account.
6These indicators perform similarly using the orthographic
lexicon: we only report AUC for FL* (referred to as oFL*), as
it gives the upper bound on lexical indicators? performance.
0 5 10 15 20 25
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9 a. Distributional
l
l
l
l l l l
l KL
JS
BC
0 5 10 15 20 25
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9 b. Lexical
ll
l
l l
l
l
l FL
FL*
HFL
oFL*
Figure 1: Indicators? AUC as a function of allophonic
complexity. The dashed line indicates random prediction.
2 allophones/phoneme 9 allophones/phoneme
MCC Acc FPR FNR MCC Acc FPR FNR
KL .095 88.2 11.3 58.5 .017 90.7 07.8 88.8
JS .097 86.4 13.1 53.7 .014 93.3 05.1 93.0
BC .097 86.8 12.8 54.4 .016 89.9 08.6 88.1
FL .048 37.3 63.2 13.6 .116 73.1 26.8 35.2
FL* .564 99.3 00.0 67.3 .563 98.6 00.4 53.0
HFL .301 99.1 00.0 87.8 .125 94.1 04.5 78.7
Table 1: Indicators? performance at low and medium
complexities, using the MCC-maximizing thresholds.
Boldface indicates the best value. Italics indicate accura-
cies below that of a dummy indicator rejecting all pairs.
Yet, if some segment pairs are misclassified by one
but not all (types of) indicators, a suitable combi-
nation should outperform individual indicators. In
other words, combining indicators may yield better
results only if, individually, indicators capture dif-
ferent subsets of the underlying allophonic relation.
5.1 Evaluation
To get a straightforward estimation of redundancy,
we compute the Jaccard index between each indica-
tor?s set of misclassified pairs: let D and L be sets
containing, respectively, a distributional and a lexi-
cal indicator?s errors, J(D,L) = |D ? L|/|D ? L|.
Values lie in [0, 1] and the lower the index, the more
promising the combination. To distinguish false pos-
itives from false negatives, we compute two Jaccard
indices for each possible combination.
5.2 Results and discussion
Jaccard indices, reported in Table 2, emphasize the
distinction between false positives and false nega-
tives. False negatives have rather high indices: most
90
allophonic pairs that are not captured by distribu-
tional indicators are not captured either by lexical
indicators, and vice versa. By contrast, there is little
or no redundancy in false positives, even at medium
allophonic complexity: though random pairs can be
incorrectly classified as allophonic, the error is un-
likely to recur across all types of indicators.
It is also worth noting that though JS performs
slightly better than KL and BC, the exact nature of
the distributional indicator seems to have little influ-
ence on the performance of the combination.
6 Combining indicators
As distributional and lexical indicators are not com-
pletely redundant, combining them is a natural ex-
tension. However, not all conceivable combination
schemes are appropriate for our task. We present our
choices in terms of Marr?s (1982) levels of analysis.
At the computational level, a combination scheme
can be either disjunctive or conjunctive, i.e. each in-
dicator can be either sufficient or (only) necessary.
Aforementioned indicators were designed as neces-
sary but not sufficient correlates of phonemehood.
For instance, while a phoneme?s allophones have
complementary distributions, not all segments that
have complementary distributions are allophones of
a single phoneme. Therefore, we favor a conjunctive
scheme,7 even if this conflicts with abovementioned
results: most errors are due to missed allophonic
pairs but a conjunctive scheme, where every indi-
cator must be satisfied, is likely to increase misses.
At the algorithmic level, a combination scheme
can be either logical or numerical. A logical scheme
uses a logical connective to join indicators? Boolean
decisions, typically by conjunction according to our
previous decision. By contrast, a numerical scheme
tries to approximate interactions between indicators?
values, merging them using any monotone increas-
ing function; discrimination then relies on a single
threshold. In practical terms, we use multiplication
as a numerical counterpart of conjunction.
6.1 Evaluation
Setting aside the following minor adjustments, we
use the same protocol as for individual indicators.
7This generalizes Martin et al?s attempt at combination:
they used FL as a high-pass lexical filter prior to the use of KL.
2 allo./phon. 9 allo./phon.
FP FN FP FN
KL FL .096 .071 .113 .359
JS FL .113 .076 .071 .355
BC FL .110 .075 .118 .358
KL FL* .000 .595 .008 .520
JS FL* .000 .548 .005 .525
BC FL* .000 .556 .007 .517
KL HFL .000 .667 .087 .788
JS HFL .000 .612 .033 .781
BC HFL .000 .620 .089 .787
Table 2: Indicators? redundancy at low and medium allo-
phonic complexities, estimated by the Jaccard indices be-
tween their false positives (FP) and false negatives (FN).
Boldface indicates the best value.
0 5 10 15 20 25
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
l
l
l
l l l
l
l JS . FL
JS . FL*
JS . HFL
JS . oFL*
Figure 2: Indicators? AUC as a function of allophonic
complexity, for the multiplicative combination scheme.
The dashed line indicates random prediction.
Logical combinations require one discrimination
threshold per combined indicator. As it facilitates
comparison with previous results, we report perfor-
mance at the thresholds maximizing the MCC of
individual indicators (rather than at the thresholds
maximizing the combined MCC) .
Numerical combinations are sensitive to differ-
ences in indicators? magnitudes. Equal contribution
of all indicators may or may not be a desirable prop-
erty, but in the absence of a priori knowledge of
indicators? relative weights, each indicator?s values
were standardized so that they lie in [0, 1], shifting
the minimum to zero and rescaling by the range.
6.2 Results and discussion
It is worth noting that, while the performance of
combined indicators is still good (Table 3), it is
less satisfactory than that of the best individual in-
dicators. Moreover, even if misclassification scores
91
Logical combination: conjunction Numerical combination: multiplication
2 allophones/phoneme 9 allophones/phoneme 2 allophones/phoneme 9 allophones/phoneme
MCC Acc FPR FNR MCC Acc FPR FNR MCC Acc FPR FNR MCC Acc FPR FNR
KL FL .104 92.9 06.5 67.3 .037 94.7 03.6 91.3 .104 92.9 06.5 67.3 .116 73.1 26.7 35.2
JS FL .109 91.7 07.8 62.6 .032 96.2 02.1 94.6 .110 91.5 07.9 61.9 .116 73.1 26.7 35.2
BC FL .109 91.9 07.5 63.3 .038 94.5 03.9 90.8 .109 92.8 06.6 66.0 .116 73.1 26.7 35.2
KL FL* .457 99.2 00.0 78.9 .207 98.2 00.1 93.3 .526 99.3 00.0 71.4 .371 98.4 00.1 81.6
JS FL* .465 99.2 00.0 78.2 .153 98.2 00.0 95.7 .548 99.3 00.0 66.0 .393 98.4 00.2 78.3
BC FL* .465 99.2 00.0 78.2 .211 98.2 00.1 93.0 .535 99.3 00.0 68.7 .388 98.4 00.1 79.0
KL HFL .348 99.1 00.0 87.8 .078 97.0 01.3 93.5 .363 99.1 00.0 84.4 .117 90.3 08.4 73.7
JS HFL .348 99.1 00.0 87.8 .068 97.9 00.3 96.5 .359 99.1 00.1 83.7 .119 90.4 08.4 73.9
BC HFL .348 99.1 00.0 87.8 .077 96.9 01.4 93.2 .361 99.1 00.0 85.7 .119 90.3 08.4 73.5
Table 3: Performance of combined distributional and lexical indicators, at low and medium allophonic complexity.
Boldface indicates the best value. Italics indicate accuracies below that of a dummy indicator rejecting all pairs.
show that conjoined and multiplied indicators per-
form similarly, disparities emerge at medium allo-
phonic complexity: while multiplication yields bet-
ter MCC and FNR, conjunction yields better accu-
racy and FPR. In that regard, observing FPR values
of zero is quite satisfactory from the point of view
of language acquisition, as processing two segments
as realizations of a single phoneme (while they are
not) may lead to the confusion of true minimal pairs
of words. Indeed, at a higher level, learning allo-
phonic rules allows the infant to reduce the size of
its emerging lexicon, factoring out allophonic real-
izations for each underlying word form.
Furthermore, AUC curves for the multiplicative
scheme (Figure 2),8 most notably FL?s, suggest that
distributional indicators? contribution to the combi-
nations appears to be rather negative, except at very
low allophonic complexities. One explanation (yet
to be tested experimentally) would be that they come
into play later in the learning process, once part of
allophony has been reduced using other indicators.
7 Conclusion
We presented an evaluation of distributional and lex-
ical indicators of allophony. Although they all per-
form well at low allophonic complexities, misclas-
sifications increase, more or less seriously, when
8We do not report a threshold-free evaluation for the logi-
cal scheme. As it requires the estimation of the volume under a
surface, comparison between schemes becomes difficult. More-
over, as the exact definition of the distributional indicator does
not affect the results, we only plot combinations with JS.
the average number of allophones per phoneme in-
creases. We also presented a first evaluation of the
combination of indicators, and found no significant
difference between the two combination schemes we
defined. Unfortunately, none of the combinations
we tested outperforms individual indicators.
For comparability with previous studies, we only
considered combination schemes requiring no mod-
ification in the definition of the task; however,
learning allophonic pairs becomes unnatural when
phonemes can have more than two realizations.
Embedding each indicator?s segment-to-segment
(dis)similarities in a multidimensional space, for ex-
ample, would enable the use of clustering techniques
where minimally distant points would be analyzed
as allophones of a single phoneme.
Thus far, segments have been nothing but abstract
symbols and, for example, the task at hand is as
hard for [a] ? [a
?
] as it is for [4] ? [k]. However,
not only do allophones of a given phoneme tend to
be acoustically similar, but acoustic differences may
be more salient and/or available earlier to the infant
than complementary distributions or minimally dif-
fering words. Therefore, the main extension towards
a comprehensive model of the acquisition of allo-
phonic rules would be to include acoustic indicators.
Acknowledgments
This work was supported by a graduate fellowship
from the French Ministery of Research. We thank
Beno??t Crabbe?, Emmanuel Dupoux and Sharon
Peperkamp for helpful comments and discussion.
92
References
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and Em-
manuel Dupoux. Submitted. Testing the robustness of
online word segmentation: effects of linguistic diver-
sity and phonetic variation.
Luc Boruta. 2011. A note on the generation of allo-
phonic rules. Technical Report 0401, INRIA.
Michael R. Brent and Timothy A. Cartwright. 1996. Dis-
tributional regularity and phonotactic constraints are
useful for segmentation. Cognition, 61:93?125.
Anthony C. C. Coolen, Reimer Ku?hn, and Peter Sollich.
2005. Theory of Neural Information Processing Sys-
tems. Oxford University Press.
Isabelle Dautriche. 2009. Mode?lisation des processus
d?acquisition du langage par des me?thodes statistiques.
Master?s thesis, INSA, Toulouse.
Brian Dillon, Ewan Dunbar, and William Idsardi. In
preparation. A single stage approach to learning
phonological categories: insights from inuktitut.
Charles Hockett. 1955. A manual of phonology. Inter-
national Journal of American Linguistics, 21(4).
Rozenn Le Calvez. 2007. Approche computationnelle
de l?acquisition pre?coce des phone`mes. Ph.D. thesis,
UPMC, Paris.
David Marr. 1982. Vision: a Computational Investiga-
tion into the Human Representation and Processing of
Visual Information. W. H. Freeman.
Andrew T. Martin, Sharon Peperkamp, and Emmanuel
Dupoux. Submitted. Learning phonemes with a
pseudo-lexicon.
Brian W. Matthews. 1975. Comparison of the pre-
dicted and observed secondary structure of T4 phage
lysozyme. Biochimica et Biophysica Acta, Protein
Structure, 405(2):442?451.
Jessica Maye, Janet F. Werker, and LouAnn Gerken.
2002. Infant sensitivity to distributional informa-
tion can affect phonetic discrimination. Cognition,
82(3):B101?B111.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre
Nadal, and Emmanuel Dupoux. 2006. The acquisition
of allophonic rules: statistical learning with linguistic
constraints. Cognition, 101(3):B31?B41.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-
port. 1996. Statistical learning by 8-month-old in-
fants. Science, 274(5294):1926?1928.
Tobias Sing, Oliver Sander, Niko Beerenwinkel, and
Thomas Lengauer. 2005. ROCR: visualizing classifier
performance in R. Bioinformatics, 21(20):3940?3941.
Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computational
Linguistics, 27(3):351?372.
93
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 1?9,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Testing the Robustness of Online Word Segmentation:
Effects of Linguistic Diversity and Phonetic Variation
Luc Boruta1,2, Sharon Peperkamp2, Beno??t Crabbe?1, and Emmanuel Dupoux2
1 Univ. Paris Diderot, Sorbonne Paris Cite?, ALPAGE, UMR-I 001 INRIA, F-75205, Paris, France
2 LSCP?DEC, E?cole des Hautes E?tudes en Sciences Sociales, E?cole Normale Supe?rieure,
Centre National de la Recherche Scientifique, F-75005, Paris, France
luc.boruta@inria.fr, peperkamp@ens.fr, benoit.crabbe@inria.fr, emmanuel.dupoux@gmail.com
Abstract
Models of the acquisition of word segmen-
tation are typically evaluated using phonem-
ically transcribed corpora. Accordingly, they
implicitly assume that children know how to
undo phonetic variation when they learn to ex-
tract words from speech. Moreover, whereas
models of language acquisition should per-
form similarly across languages, evaluation
is often limited to English samples. Us-
ing child-directed corpora of English, French
and Japanese, we evaluate the performance
of state-of-the-art statistical models given in-
puts where phonetic variation has not been re-
duced. To do so, we measure segmentation
robustness across different levels of segmen-
tal variation, simulating systematic allophonic
variation or errors in phoneme recognition.
We show that these models do not resist an in-
crease in such variations and do not generalize
to typologically different languages. From the
perspective of early language acquisition, the
results strengthen the hypothesis according to
which phonological knowledge is acquired in
large part before the construction of a lexicon.
1 Introduction
Speech contains very few explicit boundaries be-
tween linguistic units: silent pauses often mark ut-
terance boundaries, but boundaries between smaller
units (e.g. words) are absent most of the time. Pro-
cedures by which infants could develop word seg-
mentation strategies have been discussed at length,
from both a psycholinguistic and a computational
point of view. Many models relying on statistical
information have been proposed, and some of them
exhibit satisfactory performance: MBDP-1 (Brent,
1999), NGS-u (Venkataraman, 2001) and DP (Gold-
water, Griffiths and Johnson, 2009) can be consid-
ered state-of-the-art. Though there is evidence that
prosodic, phonotactic and coarticulation cues may
count more than statistics (Johnson and Jusczyk,
2001), it is still a matter of interest to know how
much can be learned without linguistic cues. To use
Venkataraman?s words, we are interested in ?the per-
formance of bare-bones statistical models.?
The aforementioned computational simulations
have two major downsides. First, all models of
language acquisition should generalize to typolog-
ically different languages; however, the word seg-
mentation experiments mentioned above have never
been carried out on phonemically transcribed, child-
directed speech in languages other than English.
Second, these experiments use phonemically tran-
scribed corpora as the input and, as such, make the
implicit simplifying assumption that, when children
learn to segment speech into words, they have al-
ready learned phonological rules and know how to
reduce the inherent variability in speech to a finite
(and rather small) number of abstract categories: the
phonemes. Rytting, Brew and Fosler-Lussier (2010)
addressed this issue and replaced the usual phone-
mic input with probability vectors over a finite set
of symbols. Still, this set of symbols is limited to
the phonemic inventory of the language: the reduc-
tion of phonetic variation is taken for granted. In
other words, previous simulations evaluated the per-
formance of the models given idealized input but of-
fered no guarantee as to the performance of the mod-
1
els on realistic input.
We present a comparative survey that evaluates
the extent to which state-of-the-art statistical models
of word segmentation resist segmental variation. To
do so, we designed a parametric benchmark where
more and more variation was gradually introduced
into phonemic corpora of child-directed speech.
Phonetic variation was simulated applying context-
dependent allophonic rules to phonemic corpora.
Other corpora in which noise was created by ran-
dom phoneme substitutions were used as controls.
Furthermore, to draw language-independent conclu-
sions, we used corpora from three typologically dif-
ferent languages: English, French and Japanese.
2 Robustness benchmark
2.1 Word segmentation models
The segmentation task can be summarized as fol-
lows: given a corpus of utterances in which word
boundaries have been deleted, the model has to put
them back. Though we did not challenge the usual
idealization that children are able to segment speech
into discrete, phoneme-sized units, modeling lan-
guage acquisition imposes significant constraints on
the models (Brent, 1999; Gambell and Yang, 2004):
they must generalize to different (if not all) lan-
guages, start without any knowledge specific to a
particular language, learn in an unsupervised man-
ner and, most importantly, operate incrementally.
Online learning is a sound desideratum for any
model of language acquisition: indeed, human
language-processors do not wait, in Brent?s words,
?until the corpus of all utterances they will ever
hear becomes available?. Therefore, we favored an
?infant-plausible? setting and only considered on-
line word segmentation models, namely MBDP-1
(Brent, 1999) and NGS-u (Venkataraman, 2001).
Even if DP (Goldwater et al, 2009) was shown to
be more flexible than both MBDP-1 and NGS-u,
we did not include Goldwater et al?s batch model,
nor recent online variants by Pearl et al (in press),
in the benchmark. All aforementioned models rely
on word n-grams statistics and have similar perfor-
mance, but MBDP-1 and NGS-u are minimally suf-
ficient in providing an quantitative evaluation of how
cross-linguistic and/or segmental variation impact
the models? performance. We added two random
segmentation models as baselines. The four models
are described below.
2.1.1 MBDP-1
The first model is Heinz?s implementation of
Brent?s MBDP-1 (Brent, 1999; Heinz, 2006). The
general idea is that the best segmentation of an ut-
terance can be inferred from the best segmentation
of the whole corpus. However, explicitly search-
ing the space of all possible segmentations of the
corpus dramatically increases the model?s computa-
tional complexity. The implementation thus uses an
incremental approach: when the ith utterance is pro-
cessed, the model computes the best segmentation of
the corpus up to the ith utterance included, assuming
the segmentation of the first i?1 utterances is fixed.
2.1.2 NGS-u
This unigram model was described and imple-
mented by Venkataraman (2001). MBDP-1?s prob-
lems of complexity were circumvented using an in-
trinsically incremental n-gram approach. The strat-
egy is to find the most probable word sequence for
each utterance, according to information acquired
while processing previous utterances. In the end,
the segmentation of the entire corpus is the con-
catenation of each utterance?s best segmentation. It
is worth noting that NGS-u satisfies all three con-
straints proposed by Brent: strict incrementality,
non-supervision and universality.
2.1.3 Random
This dummy model rewrites its input, uniformly
choosing after each segment whether to insert a
word boundary or not. It defines a chance line at
and below which models can be considered ineffi-
cient. The only constraint is that no empty word is
allowed, hence no consecutive boundaries.
2.1.4 Random+
The second baseline is weakly supervised: though
each utterance is segmented at uniformly-chosen
random locations, the correct number of word
boundaries is given. This differs from Brent?s base-
line, which was given the correct number of bound-
aries to insert in the entire corpus. As before, con-
secutive boundaries are forbidden.
2
English French Japanese
Tokens Types Tokens Types Tokens Types
U 9,790 5,921 10,000 7,660 10,000 6,315
W 33,399 1,321 51,069 1,893 26,609 4,112
P 95,809 50 121,486 35 102,997 49
Table 1: Elementary corpus statistics, including number
of utterances (U), words (W) and phonemes (P).
2.2 Corpora
The three corpora we used were derived from tran-
scribed adult-child verbal interactions collected in
the CHILDES database (MacWhinney, 2000). For
each sample, elementary textual statistics are pre-
sented in Table 1. The English corpus contains
9790 utterances from the Bernstein?Ratner corpus
that were automatically transcribed and manually
corrected by Brent and Cartwright (1996). It has
been used in many word segmentation experiments
(Brent, 1999; Venkataraman, 2001; Batchelder,
2002; Fleck, 2008; Goldwater et al, 2009; among
others) and can be considered a de facto standard.
The French and the Japanese corpora were both
made by Le Calvez (2007), the former by automati-
cally transcribing the Champaud, Leveille? and Ron-
dal corpora, the latter by automatically transcribing
the Ishii and Noji corpora from ro?maji to phonemes.
To get samples comparable in size to the English
corpus, 10,000 utterances were selected at random
in each of Le Calvez?s corpora. All transcription
choices made by the authors in terms of phonemic
inventory and word segmentation were respected.1
2.3 Variation sources
The main effect of the transformations we applied
to the phonemic corpora was the increase in the av-
erage number of word forms per word. We refer to
this quantity, similar to a type-token ratio, as the cor-
pora?s lexical complexity. As allophonic variation
is context-dependent, the increase in lexical com-
plexity is, in this condition, limited by the phono-
tactic constraints of the language: the fewer con-
texts a phoneme appears in, the fewer contextual al-
lophones it can have. By contrast, the upper limit
is much higher in the control condition, as phoneme
1Some transcription choices made by Brent and Cartwright
are questionable (Blanchard and Heinz, 2008). Yet, we used the
canonical version of the corpus for the sake of comparability.
substitutions are context-free.
From a computational point of view, the applica-
tion of allophonic rules increases both the number of
symbols in the alphabet and, as a byproduct, the lex-
ical complexity. Obviously, when any kind of noise
or variation is added, there is less information in the
data to learn from. We can therefore presume that
the probability mass will be scattered, and that as a
consequence, statistical models relying on word n-
grams statistics will do worse than with phonemic
inputs. Yet, we are interested in quantifying how
such interference impacts the models? performance.
2.3.1 Allophonic variation
In this experiment, we were interested in the per-
formance of online segmentation models given rich
phonetic transcriptions, i.e. the input children pro-
cess before the acquisition of allophonic rules. Con-
sider the following rule that applies in French:
/r/ ?
{
[X] before a voiceless consonant
[K] otherwise
The application of this rule creates two contextual
variants for /kanar/ (canard, ?duck?): [kanaK Zon]
(canard jaune, ?yellow duck?) and [kanaX flotA?] (ca-
nard flottant, ?floating duck?). Before learning the
rule, children have to store both [kanaK] and [kanaX]
in their emerging lexicon as they are not yet able to
undo allophonic variation and construct a single lex-
ical entry: /kanar/.
Daland and Pierrehumbert (2010) compared the
performance of a phonotactic segmentation model
using canonical phonemic transcripts and transcripts
implementing conversational reduction processes.
They found that incorporating pronunciation vari-
ation has a mild negative impact on performance.
However, they used adult-directed speech. Even if,
as they argue, reduced adult-directed speech may
present a worst-case scenario for infants (compared
to hyperarticulated child-direct speech), it offers no
quantitative evaluation of the models? performance
using child-directed speech.
Because of the lack of phonetically transcribed
child-directed speech data, we emulated rich tran-
scriptions applying allophonic rules to the phonemic
corpora. To do so, we represented the internal struc-
ture of the phonemes in terms of articulatory fea-
tures and used the algorithm described by Boruta
3
(2011) to create artificial allophonic grammars of
different sizes containing assimilatory rules whose
application contexts span phonologically similar
contexts of the target phoneme. Compared to Da-
land and Pierrehumbert?s manual inspection of the
transcripts, this automatic approach gives us a finer
control on the degree of pronunciation variation.
The rules were then applied to our phonemic cor-
pora, thus systematizing coarticulation between ad-
jacent segments. We made two simplifying assump-
tions about the nature of the rules. First, all al-
lophonic rules we generated are of the type p ?
a / c where a phoneme p is realized as its allo-
phone a before context c. Thus, we did not model
rules with left-hand or bilateral contexts. Second,
we ensured that no two allophonic rules introduced
the same allophone (as in English flapping, where
both /t/ and /d/ have an allophone [R]), using parent
annotation: each phone is marked by the phoneme
it is derived from (e.g. [R]/t/ and [R]/d/). This was
done to avoid probability mass derived from differ-
ent phonemes merging onto common symbols.
The amount of variation in the corpora is de-
termined by the average number of allophones per
phoneme. We refer to this quantity as the corpora?s
allophonic complexity. Thus, at minimal allophonic
complexity, each phoneme has only one possible re-
alization (i.e. phonemic transcription), whereas at
maximal allophonic complexity, each phoneme has
as many realizations as attested contexts. For each
language, the range of attested lexical and allo-
phonic complexities obtained using Boruta?s (2011)
algorithm are reported in Figure 1.
2.3.2 Phoneme substitutions
Allophonic variation is not the only type of varia-
tion that may interfere with word segmentation. In-
deed, the aforementioned simulations assumed that
all phonemes are recognized with 100% accuracy,
but ?due to factors such as noise or speech rate?
human processors may mishear words. In this con-
trol condition, we examined the models? perfor-
mance on corpora in which some phonemes were
replaced by others. Thus, substitutions increase the
corpus? lexical complexity without increasing the
number of symbols: phoneme misrecognitions give
a straightforward baseline against which to compare
the models? performance when allophonic variation
5 10 15 20
1.0
1.5
2.0
2.5
3.0
3.5
4.0
l
l
l
l
l
l
l
l
l English
French
Japanese
Figure 1: Lexical complexity (the average number of
word forms per word) as a function of allophonic com-
plexity (the average number of allophones per phoneme).
has not been reduced. Such corpora can be consid-
ered the output of a hypothetical imperfect speech-
to-phoneme system or a winner-take-all scalar re-
duction of Rytting et al?s (2010) probability vectors.
We used a straightforward model of phoneme
misrecognition: substitutions are based neither on
a confusion matrix (Nakadai et al, 2007) nor on
phoneme similarity. Starting from the phonemic
corpus, we generated 10 additional corpora con-
trolling the proportion of misrecognized phonemes,
ranging from 0 (perfect recognition) to 1 (constant
error) in increments of 0.1. A noise intensity of n
means that each phoneme has probability n of being
rewritten by another phoneme. The random choice
of the substitution phoneme is weighted by the rela-
tive frequencies of the phonemes in the corpus. The
probability P (p ? x) that a phoneme x rewrites a
phoneme p is defined as
P (p? x) =
?
?
?
1? n if p = x
n
(
f(x) +
f(p)
|P| ? 1
)
otherwise
where n is the noise intensity, f(x) the relative fre-
quency of phoneme x in the corpus andP the phone-
mic inventory of the language.
4
2.4 Evaluation
We used Venkataraman?s (2001) implementation of
the now-standard evaluation protocol proposed by
Brent (1999) and then extended by Goldwater et al
(2009). Obviously, orthographic words are not the
optimal target for a model of language acquisition.
Yet, in line with previously reported experiments,
we used the orthographic segmentation as the stan-
dard of correct segmentation.
2.4.1 Scoring
For each model, we report (as percentages) the
following scores as functions of the lexical complex-
ity of the corpus:
? Ps, Rs, Fs: precision, recall and F -score on
word segmentation as defined by Brent;
? Pl, Rl, Fl: precision, recall and F -score on the
induced lexicon of word types: let L be the
standard lexicon and L? the one discovered by
the algorithm, we define Pl = |L ? L?|/|L?|,
Rl = |L?L?|/|L| and Fl = 2?Pl ?Rl/(Pl+Rl).
The difference between scoring the segmenta-
tion and the lexicon can be exemplified consider-
ing the utterance [@wUd?2kwUd?2kwUd] (a wood-
chuck would chuck wood). If it is segmented as
[@ wUd?2k wUd ?2k wUd], both the segmentation
and the induced lexicon are correct. By contrast, if
it is segmented as [@ wUd ?2k wUd?2k wUd], the
lexicon is still accurate while the word segmentation
is incorrect. A good segmentation inevitably yields a
good lexicon, but the reverse is not necessarily true.
2.4.2 k-shuffle cross-validation
As the segmental variation procedures and the
segmentation baselines are non-deterministic pro-
cesses, all scores were averaged over multiple simu-
lations. Moreover, as MBDP-1 and NGS-u operate
incrementally, their output is conditioned by the or-
der in which utterances are processed. To lessen the
influence of the utterance order, we shuffled the cor-
pora for each simulation. Testing all permutations of
the corpora for each combination of parameter val-
ues is computationally intractable. Thus, scores re-
ported below were averaged over three distinct sim-
ulations with shuffled corpora.
JP
FR
EN
a. Segmentation F?score
0 10 20 30 40 50 60 70 80 90
JP
FR
EN
b. Lexicon F?score
0 10 20 30 40 50 60 70 80 90
MBDP?1
NGS?u
Random+
Random
Figure 2: Cross-linguistic performance of MBDP-1 and
NGS-u on child-directed phonemic corpora in English
(EN), French (FR) and Japanese (JP).
3 Results and discussion
3.1 Cross-linguistic evaluation
Performance of the segmentation models2 on phone-
mic corpora is presented in Figure 1 in terms of Fs-
and Fl-score (upper and lower panel, respectively).
We were able to replicate previous results on En-
glish by Brent and Venkataraman almost exactly; the
small difference, less than one percent, was probably
caused by the use of different implementations.
From a cross-linguistic point of view, the main
observation is that these models do not seem
to generalize to typologically different languages.
Whereas MBDP-1 and NGS-u?s Fs value is 69%
for English, it is only 54% for French and 41% for
Japanese. Similar observations can be made for Fl.
Purely statistical strategies seem to be particularly
ineffective on our Japanese sample: inserting word
boundaries at random yields a better lexicon than us-
ing probabilistic models.
A crude way to determine whether a word seg-
mentation model tends to break words apart (over-
segmentation) or to cluster various words in a single
chunk (under-segmentation) is to compare the aver-
age word length (AWL) in its output to the AWL in
the standard segmentation. If the output?s AWL is
greater than the standard?s, then the output is under-
segmented, and vice versa. Even if NGS-u produces
2The full table of scores for each language, variation source,
and segmentation model was not included due to space limita-
tions. It is available upon request from the first author.
5
shorter words than MBDP-1, both models exhibit,
once again, similar within-language behaviors. En-
glish was slightly under-segmented by MBDP-1 and
over-segmented by NGS-u: ouputs? AWL are re-
spectively 3.1 and 2.7, while the standard is 2.9.
Our results are consistent with what Goldwater et al
(2009) observed for DP: error analysis shows that
both MBDP-1 and NGS-u also break off frequent
English morphological affixes, namely /IN/ (-ing)
and /s,z/ (-s). As for French, AWL values suggest
the corpus was under-segmented: 3.1 for MBDP-1?s
output and 2.9 for NGS-u?s, while the standard is
2.4. On the contrary, Japanese was heavily over-
segmented: many monophonemic words emerged
and, whereas the standard AWL is 3.9, the ouputs?
AWL is 2.7 for both models.
Over-segmentation may be correlated to the num-
ber of syllable types in the language: English
and French phonotactics allow consonantal clusters,
bringing the number of syllable types to a few thou-
sands. By contrast, Japanese has a much simpler
syllabic structure and less syllable types which, as
a consequence, are often repeated and may (incor-
rectly) be considered as words by statistical mod-
els. The fact that the models do worse for French
and Japanese is not especially surprising: both lan-
guages have many more affixal morphemes than En-
glish. Consider French, where the lexical autonomy
of clitics is questionable: whereas /s/ (s? or c?) or
/k/ (qu?) are highly frequent words in our ortho-
graphic standard, many errors are due to the aggluti-
nation of these clitics to the following word. These
are counted as segmentation errors, but should they?
Furthermore, none of the segmentation models
we benchmarked exhibit similar performance across
languages: invariably, they perform better on En-
glish. There may be a correlation between the per-
formance of segmentation models and the percent-
age of word hapaxes, i.e. words which occur only
once in the corpus: the English, French and Japanese
corpora contain 31.7%, 37.1% and 60.7% of word
hapaxes, respectively. The more words tend to occur
only once, the less MBDP-1, NGS-u and DP per-
form on segmentation. This is consistent with the
usual assumption that infants use familiar words to
find new ones. It may also be the case that these
models are not implicitly tuned to English, but that
the contribution of statistical cues to word segmen-
tation differs across languages. In French, for exam-
ple, stress invariably marks the end of a word (al-
though the end of a word is not necessarily marked
by stress). By contrast, there are languages like
English or Spanish where stress is less predictable:
children cannot rely solely on this cue to extract
words and may thus have to give more weight to
statistics.
3.2 Robustness to segmental variation
The performance of MBDP-1, NGS-u and the two
baselines on inputs altered by segmental variation
is presented in Figure 2.3 The first general observa-
tion is that, as predicted, MBDP-1 and NGS-u do not
seem to resist an increase in lexical complexity. In
the case of allophonic variation, their performance
is inversely related to the corpora?s allophonic com-
plexity. However, as suggested by the change in
the graphs? slope, performance for English seems
to stabilize at 2 word forms per word. Similar ob-
servations can be made for French and Japanese on
which the performance of the models is even worse:
Fl values are below chance at 1.7 and 3 variants per
word for Japanese and French, respectively; like-
wise, Fs is below chance at 1.5 for Japanese and
2.5 for French. Phoneme substitutions also impede
the performance of MBDP-1 and NGS-u: the more
phonemes are substituted, the more difficult it be-
comes for the algorithms to learn how to insert word
boundaries. Furthemore, Fl is below chance for
complexities greater than 4 for French, and approx-
imately 2.5 for Japanese. It is worth noting that, in
both conditions, the models exhibit similar within-
language performance as the complexity increases.
The potential lexicon that can be built by com-
bining segments into words may account for the
discrepancy between the two conditions, as it is in
fact the models? search space. In the control con-
dition, substituting phonemes does not increase its
size. However, the likelihood of a given phoneme in
a given word being replaced by the same substitu-
tion phoneme decreases as words get longer. Thus,
the proportion of hapax increases, making statisti-
cal segmentation harder to achieve. By contrast, the
3For the control condition, we did not graph scores for noise
intensities greater than 0.2: 80% accuracy is comparable to the
error rates of state-of-the-art systems in speaker-independent,
continuous speech recognition (Makhoul and Schwartz, 1995).
6
1 2 3 4 5 6 7 8
10
20
30
40
50
60
70
a. English: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l l
l l l
l
l
1 2 3 4 5 6 7 8
10
20
30
40
50
60
70
b. English: lexicon F?score
Le
xic
on
 F?
sco
re
l MBDP?1
NGS?u
Random+
Random
Allophony
Substitutions
l
l
l l l l l l
l
l
1 2 3 4 5 6
10
20
30
40
50
60
c. French: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l l
l
l l l
l
l
1 2 3 4 5 6
10
20
30
40
50
60
d. French: lexicon F?score
Le
xic
on
 F?
sco
re
l
l
l
l l l l l
l
l
l
1.0 1.5 2.0 2.5 3.0
10
20
30
40
50
e. Japanese: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l
l
l
l
l
l
l
1.0 1.5 2.0 2.5 3.0
10
20
30
40
50
f. Japanese: lexicon F?score
Le
xic
on
 F?
sco
re
l
l l l
l
l
l
l
l
l
Figure 3: Fs-score (left column) and Fl-score (right column) as functions of the lexical complexity, i.e. the number of
word forms per word, in the English (top row), French (middle row) and Japanese (bottom row) corpora.
7
application of allophonic rules increases the number
of objects to build words with; as a consequence, the
size of the potential lexicon explodes.
As neither MBDP-1 nor NGS-u is designed to
handle noise, the results are unsurprising. Indeed,
any word form found by these models will be incor-
porated in the lexicon: if [l?NgwI?] and [l?NgwI?]
are both found in the corpus, these variants will be
included as is in the lexicon. There is no mechanism
for ?explaining away? data that appear to have been
generated by systematic variation or random noise.
It is an open issue for future research to create ro-
bust models of word segmentation that can handle
segmental variation.
4 Conclusions
We have shown, first, that online statistical mod-
els of word segmentation that rely on word n-gram
statistics do not generalize to typologically differ-
ent languages. As opposed to French and Japanese,
English seems to be easier to segment using only
statistical information. Such differences in perfor-
mance from one language to another emphasize the
relevance of cross-linguistic studies: any conclusion
drawn from the monolingual evaluation of a model
of language acquisition should be considered with
all proper reservations. Second, our results quan-
tify how imperfect, though realistic, inputs impact
MBDP-1?s and NGS-u?s performance. Indeed, both
models become less and less efficient in discover-
ing words in transcribed child-directed speech as
the number of variants per word increases: though
the performance drop we observed is not surpris-
ing, it is worth noting that both models are less ef-
ficient than random procedures at about twenty al-
lophones per phoneme. However, the number of
context-dependent allophones we introduced is far
less than what is used by state-of-the-art models of
speech recognition (Makhoul and Schwartz, 1995).
To our knowledge, there is no computational
model of word segmentation that both respects the
constraints imposed on a human learner and accom-
modates noise. This highlights the complexity of
early language acquisition: while no accurate lex-
icon can be learned without a good segmentation
strategy, state-of-the-art models fail to deliver good
segmentations in non-idealized settings. Our re-
sults also emphasize the importance of other cues
for word segmentation: statistical learning may be
helpful or necessary for word segmentation, but it is
unlikely that it is sufficient.
The mediocre performance of the models
strengthens the hypotheses that phonological
knowledge is acquired in large part before the
construction of a lexicon (Jusczyk, 1997), or that
allophonic rules and word segmentations could be
acquired jointly (so that neither is a prerequisite
for the other): children cannot extract words from
fluent speech without knowing how to undo at least
part of contextual variation. Thus, the knowledge
of allophonic rules seems to be a prerequisite for
accurate segmentation. Recent simulations of word
segmentation and lexical induction suggest that
using phonological knowledge (Venkataraman,
2001; Blanchard and Heinz, 2008), modeling
morphophonological structure (Johnson, 2008) or
preserving subsegmental variation (Rytting et al,
2010) invariably increases performance. Vice
versa, Martin et al (submitted) have shown that the
algorithm proposed by Peperkamp et al (2006) for
undoing allophonic variation crashes in the face of
realistic input (i.e. many allophones), and that it
can be saved if it has approximate knowledge of
word boundaries. Further research is needed, at
both an experimental and a computational level, to
explore the performance and suitability of an online
model that combines the acquisition of allophonic
variation with that of word segmentation.
References
E. Batchelder. 2002. Bootstrapping the lexicon: a com-
putational model of infant speech segmentation. Cog-
nition, 83:167?206.
D. Blanchard and J. Heinz. 2008. Improving word seg-
mentation by simultaneously learning phonotactics. In
Proceedings of the Conference on Natural Language
Learning, pages 65?72.
L. Boruta. 2011. A note on the generation of allophonic
rules. Technical Report 0401, INRIA.
M. R. Brent and T. A. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful for
segmentation. Cognition, 61:93?125.
M. R. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34(1?3):71?105.
8
R. Daland and J. B. Pierrehumbert. 2010. Learn-
ing diphone-based segmentation. Cognitive Science,
35(1):119?155.
M. Fleck. 2008. Lexicalized phonotactic word segmen-
tation. In Proceedings of ACL-2008, pages 130?138.
T. Gambell and C. Yang. 2004. Statistics learning and
universal grammar: Modeling word segmentation. In
Proceedings of the 20th International Conference on
Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2009. A
bayesian framework for word segmentation: exploring
the effects of context. Cognition, 112(1):21?54.
J. Heinz. 2006. MBDP-1, OCaml implementation. Re-
trieved from http://phonology.cogsci.udel.edu/?heinz/
on January 26, 2009.
E. K. Johnson and P. W. Jusczyk. 2001. Word segmenta-
tion by 8-month-olds: When speech cues count more
than statistics. Journal of Memory and Language,
44:548?567.
M. Johnson. 2008. Unsupervised word segmentation for
Sesotho using adaptor grammars. In Proceedings of
the 10th Meeting of ACL SIGMORPHON, pages 20?
27.
P. Jusczyk. 1997. The Discovery of Spoken Language.
MIT Press.
R. Le Calvez. 2007. Approche computationnelle de
l?acquisition pre?coce des phone`mes. Ph.D. thesis,
UPMC.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Elbraum Associates.
J. Makhoul and R. Schwartz. 1995. State of the art in
continuous speech recognition. PNAS, 92:9956?9963.
A. Martin, S. Peperkamp, and E. Dupoux. Submitted.
Learning phonemes with a pseudo-lexicon.
K. Nakadai, R. Sumiya, M. Nakano, K. Ichige, Y. Hi-
rose, and H. Tsujino. 2007. The design of phoneme
grouping for coarse phoneme recognition. In IEA/AIE,
pages 905?914.
L. Pearl, Sh. Goldwater, and M. Steyvers. In press. On-
line learning mechanisms for bayesian models of word
segmentation. Research on Language and Computa-
tion.
S. Peperkamp, R. Le Calvez, J. P. Nadal, and E. Dupoux.
2006. The acquisition of allophonic rules: statisti-
cal learning with linguistic constraints. Cognition,
101(3):B31?B41.
C. A. Rytting, C. Brew, and E. Fosler-Lussier. 2010.
Segmenting words from natural speech: subsegmen-
tal variation in segmental cues. Journal of Child Lan-
guage, 37:513?543.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
9
