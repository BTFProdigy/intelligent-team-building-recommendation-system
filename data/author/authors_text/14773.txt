Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?90,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
GenERRate: Generating Errors for Use in Grammatical Error Detection
Jennifer Foster
National Centre for Language Technology
School of Computing
Dublin City University, Ireland
jfoster@computing.dcu.ie
?istein E. Andersen
Computer Laboratory
University of Cambridge
United Kingdom
oa223@cam.ac.uk
Abstract
This paper explores the issue of automatically
generated ungrammatical data and its use in
error detection, with a focus on the task of
classifying a sentence as grammatical or un-
grammatical. We present an error generation
tool called GenERRate and show how Gen-
ERRate can be used to improve the perfor-
mance of a classifier on learner data. We de-
scribe initial attempts to replicate Cambridge
Learner Corpus errors using GenERRate.
1 Introduction
In recent years automatically generated ungrammat-
ical data has been used in the training and evalu-
ation of error detection systems, in evaluating the
robustness of NLP tools and as negative evidence
in unsupervised learning. The main advantage of
using such artificial data is that it is cheap to pro-
duce. However, it is of little use if it is not a real-
istic model of the naturally occurring and expensive
data that it is designed to replace. In this paper we
explore the issues involved in generating synthetic
data and present a tool called GenERRate which can
be used to produce many different kinds of syntacti-
cally noisy data. We use the tool in two experiments
in which we attempt to train classifiers to distinguish
between grammatical and ungrammatical sentences.
In the first experiment, we show how GenERRate
can be used to improve the performance of an ex-
isting classifier on sentences from a learner corpus
of transcribed spoken utterances. In the second ex-
periment we try to produce a synthetic error corpus
that is inspired by the Cambridge Learner Corpus
(CLC)1, and we evaluate the difference between a
classifier?s performance when trained on this data
and its performance when trained on original CLC
material. The results of both experiments provide
pointers on how to improve GenERRate, as well as
highlighting some of the challenges associated with
automatically generating negative evidence.
The paper is organised as follows: In Section 2,
we discuss the reasons why artificial ungrammatical
data has been used in NLP and we survey its use
in the field, focussing mainly on grammatical error
detection. Section 3 contains a description of the
GenERRate tool. The two classification experiments
which use GenERRate are described in Section 4.
Problematic issues are discussed in Section 5 and
avenues for future work in Section 6.
2 Background
2.1 Why artifical error data is useful
Before pointing out the benefit of using artificial
negative evidence in grammatical error detection, it
is worth reminding ourselves of the benefits of em-
ploying negative evidence, be it artificial or natu-
rally occurring. By grammatical error detection, we
mean either the task of distinguishing the grammat-
ical from the ungrammatical at the sentence level
or more local targeted error detection, involving the
identification, and possibly also correction, of par-
ticular types of errors. Distinguishing grammati-
cal utterances from ungrammatical ones involves the
use of a binary classifier or a grammaticality scor-
1http://www.cambridge.org/elt/corpus/
learner_corpus2.htm
82
ing model. Examples are Andersen (2006; 2007),
Okanohara and Tsujii (2007), Sun et al (2007) and
Wagner et al (2007). In targeted error detection,
the focus is on identifying the common errors made
either by language learners or native speakers (de-
pending on the application). For ESL applications,
this includes the detection of errors involving ar-
ticles (Han et al, 2006; De Felice and Pulman,
2008; Gamon et al, 2008), prepositions (De Felice
and Pulman, 2008; Gamon et al, 2008; Tetreault
and Chodorow, 2008), verb forms (Lee and Seneff,
2008b), mass/count noun confusions (Brockett et al,
2006) and word order (Metcalf and Meurers, 2006).
The presence of a pattern in a corpus of well-
formed language is positive evidence that the pat-
tern is well-formed. The presence of a pattern in an
corpus of ill-formed language is negative evidence
that the pattern is erroneous. Discriminative tech-
niques usually lead to more accurate systems than
those based on one class alone. The use of the two
types of evidence can be seen at work in the system
described by Lee and Seneff (2008b): Verb phrases
are parsed and their parse trees are examined. If
the parse trees resemble the ?disturbed? trees that
statistical parsers typically produce when an incor-
rect verb form is used, the verb phrase is consid-
ered a likely candidate for correction. However, to
avoid overcorrection, positive evidence in the form
of Google n-gram statistics is also employed: a cor-
rection is only applied if its n-gram frequency is
higher than that of the original uncorrected n-gram.
The ideal situation for a grammatical error de-
tection system is one where a large amount of la-
belled positive and negative evidence is available.
Depending on the aims of the system, this labelling
can range from simply marking a sentence as un-
grammatical to a detailed description of the error
along with a correction. If an error detection sys-
tem employs machine learning, the performance of
the system will improve as the training set size in-
creases (up to a certain point). For systems which
employ learning algorithms with large feature sets
(e.g. maximum entropy, support vector machines),
the size of the training set is particularly important
so that overfitting is avoided. The collection of a
large corpus of ungrammatical data requires a good
deal of manual effort. Even if the annotation only
involves marking the sentence as correct/incorrect,
it still requires that the sentence be read and a gram-
maticality judgement applied to it. If more detailed
annotation is applied, the process takes even longer.
Some substantially-sized annotated error corpora do
exist, e.g. the Cambridge Learner Corpus, but these
are not freely available.
One way around this problem of lack of availabil-
ity of suitably large error-annotated corpora is to in-
troduce errors into sentences automatically. In order
for the resulting error corpus to be useful in an error
detection system, the errors that are introduced need
to resemble those that the system aims to detect.
Thus, the process is not without some manual effort:
knowing what kind of errors to introduce requires
the inspection of real error data, a process similar
to error annotation. Once the error types have been
specified though, the process is fully automatic and
allows large error corpora to be compiled. If the set
of well-formed sentences into which the errors are
introduced is large and varied enough, it is possi-
ble that this will result in ungrammatical sentence
structures which learners produce but which have
not yet been recorded in the smaller naturally occur-
ring learner corpora. To put it another way, the same
type of error will appear in lexically and syntacti-
cally varied contexts, which is potentially advanta-
geous when training a classifier.
2.2 Where artificial error data has been used
Artificial errors have been employed previously in
targeted error detection. Sjo?bergh and Knutsson
(2005) introduce split compound errors and word or-
der errors into Swedish texts and use the resulting
artificial data to train their error detection system.
These two particular error types are chosen because
they are frequent errors amongst non-native Swedish
speakers whose first language does not contain com-
pounds or has a fixed word order. They compare the
resulting system to three Swedish grammar check-
ers, and find that their system has higher recall at the
expense of lower precision. Brockett et al (2006)
introduce errors involving mass/count noun confu-
sions into English newswire text and then use the re-
sulting parallel corpus to train a phrasal SMT system
to perform error correction. Lee and Seneff (2008b)
automatically introduce verb form errors (subject?
verb agreement errors, complementation errors and
errors in a main verb after an auxiliary) into well-
83
formed text, parse the resulting text and examine the
parse trees produced.
Both Okanohara and Tsujii (2007) and Wagner et
al. (2007) attempt to learn a model which discrimi-
nates between grammatical and ungrammatical sen-
tences, and both use synthetic negative data which
is obtained by distorting sentences from the British
National Corpus (BNC) (Burnard, 2000). The meth-
ods used to distort the BNC sentences are, however,
quite different. Okanohara and Tsujii (2007) gener-
ate ill-formed sentences by sampling a probabilistic
language model and end up with ?pseudo-negative?
examples which resemble machine translation out-
put more than they do learner texts. Indeed, ma-
chine translation is one of the applications of their
resulting discriminative language model. Wagner
et al (2007) introduce grammatical errors of the
following four types into BNC sentences: context-
sensitive spelling errors, agreement errors, errors in-
volving a missing word and errors involving an extra
word. All four types are considered equally likely
and the resulting synthetic corpus contains errors
that look like the kind of slips that would be made
by native speakers (e.g. repeated adjacent words) as
well as errors that resemble learner errors (e.g. miss-
ing articles). Wagner et al (2009) report a drop in
accuracy for their classification methods when ap-
plied to real learner texts as opposed to held-out syn-
thetic test data, reinforcing the earlier point that ar-
tificial errors need to be tailored for the task at hand
(we return to this in Section 4.1).
Artificial error data has also proven useful in
the automatic evaluation of error detection systems.
Bigert (2004) describes how a tool called Missplel
is used to generate context-sensitive spelling errors
which are then used to evaluate a context-sensitive
spelling error detection system. The performance
of general-purpose NLP tools such as part-of-speech
taggers and parsers in the face of noisy ungrammat-
ical data has been automatically evaluated using ar-
tificial error data. Since the features of machine-
learned error detectors are often part-of-speech n-
grams or word?word dependencies extracted from
parser output (De Felice and Pulman, 2008, for ex-
ample), it is important to understand how part-of-
speech taggers and parsers react to particular gram-
matical errors. Bigert et al (2005) introduce artifi-
cial context-sensitive spelling errors into error-free
Swedish text and then evaluate parsers and a part-
of-speech tagger on this text using their performance
on the error-free text as a reference. Similarly, Fos-
ter (2007) investigates the effect of common English
grammatical errors on two widely-used statistical
parsers using distorted treebank trees as references.
The procedure used by Wagner et al (2007; 2009) is
used to introduce errors into the treebank sentences.
Finally, negative evidence in the form of automat-
ically distorted sentences has been used in unsuper-
vised learning. Smith and Eisner (2005a; 2005b)
generate negative evidence for their contrastive es-
timation method by moving or removing a word in a
sentence. Since the aim of this work is not to detect
grammatical errors, there is no requirement to gener-
ate the kind of negative evidence that might actually
be produced by either native or non-native speakers
of a language. The negative examples are used to
guide the unsupervised learning of a part-of-speech
tagger and a dependency grammar.
We can conclude from this survey that synthetic
error data is useful in a variety of NLP applications,
including error detection and evaluation of error de-
tectors. In Section 3, we describe an automatic error
generation tool, which has a modular design and is
flexible enough to accommodate the generation of
the various types of synthetic data described above.
3 Error Generation Tool
GenERRate is an error generation tool which ac-
cepts as input a corpus and an error analysis file
consisting of a list of errors and produces an error-
tagged corpus of syntactically ill-formed sentences.
The sentences in the input corpus are assumed to be
grammatically well-formed. GenERRate is imple-
mented in Java and will be made available to down-
load for use by other researchers.2
3.1 Supported Error Types
Error types are defined in terms of their corrections,
that is, in terms of the operations (insert, delete, sub-
stitute and move) that are applied to a well-formed
sentence to make it ill-formed. As well as being
a popular classification scheme in the field of er-
ror analysis (James, 1998), it has the advantage of
2http://www.computing.dcu.ie/?jfoster/
resources/generrate.html
84
being theory-neutral. This is important in this con-
text since it is hoped that GenERRate will be used
to create negative evidence of various types, be it
L2-like grammatical errors, native speaker slips or
more random syntactic noise. It is hoped that Gen-
ERRate will be easy to use for anyone working in
linguistics, applied linguistics, language teaching or
computational linguistics.
The inheritance hierarchy in Fig. 1 shows the er-
ror types that are supported by GenERRate. We
briefly describe each error type.
Errors generated by removing a word
? DeletionError: Generated by selecting a word
at random from the sentence and removing it.
? DeletionPOSError: Extends DeletionError by
allowing a specific POS to be specified.
? DeletionPOSWhereError: Extends Deletion-
POSError by allowing left and/or right context
(POS tag or start/end) to be specified.
Errors generated by inserting a word
? InsertionError: Insert a random word at a ran-
dom position. The word is chosen either from
the sentence itself or from a word list, and this
choice is also random.
? InsertionFromFileOrSentenceError: This
differs from the InsertionError in that the de-
cision of whether to use the sentence itself or a
word list is not made at random but supplied in
the error type specification.
? InsertionPOSError: Extends InsertionFrom-
FileOrSentenceError by allowing the POS of
the new word to be specified.
? InsertionPOSWhereError: Analogous to the
DeletionPOSWhereError, this extends Inser-
tionPOSError by allowing left and/or right con-
text to be specified.
Errors generated by moving a word
? MoveError: Generated by randomly selecting
a word in the sentence and moving it to another
position, randomly chosen, in the sentence.
? MovePOSError: A word tagged with the
specified POS is randomly chosen and moved
to a randomly chosen position in the sentence.
? MovePOSWhereError: Extends Move-
POSError by allowing the change in position
subst,word,an,a,0.2
subst,NNS,NN,0.4
subst,VBG,TO,0.2
delete,DT,0.1
move,RB,left,1,0.1
Figure 2: GenERRate Toy Error Analysis File
to be specified in terms of direction and
number of words.
Errors generated by substituting a word
? SubstError: Replace a random word by a
word chosen at random from a word list.
? SubstWordConfusionError: Extends Sub-
stError by allowing the POS to be specified
(same POS for both words).
? SubstWordConfusionNewPOSError: Simi-
lar to SubstWordConfusionError, but allows
different POSs to be specified.
? SubstSpecificWordConfusionError:
Replace a specific word with another
(e.g. be/have).
? SubstWrongFormError: Replace a word
with a different form of the same word. The
following changes are currently supported:
noun number (e.g. word/words), verb number
(write/writes), verb form (writing/written), ad-
jective form (big/bigger) and adjective/adverb
(quick/quickly). Note that this is the only er-
ror type which is language-specific. At the mo-
ment, only English is supported.
3.2 Input Corpus
The corpus that is supplied as input to GenERRate
must be split into sentences. It does not have to be
part-of-speech tagged, but it will not be possible to
generate many of the errors if it is not. GenERRate
has been tested using two part-of-speech tagsets,
the Penn Treebank tagset (Santorini, 1991) and the
CLAWS tagset (Garside et al, 1987).
3.3 Error Analysis File
The error analysis file specifies the errors that Gen-
ERRate should attempt to insert into the sentences
in the input corpus. A toy example with the Penn
tagset is shown in Fig. 2. The first line is an instance
of a SubstSpecificWordConfusion error. The second
85
Error
Deletion
DeletionPOS
DeletionPOSWhere
Insertion
InsertionFromFileOrSentence
InsertionPOS
InsertionPOSWhere
Move
MovePOS
MovePOSWhere
Subst
SubstWordConfusion
SubstWordConfusionNewPOS SubstSpecificWordConfusion
SubstWrongForm
Figure 1: GenERRate Error Types
and third are instances of the SubstWrongFormEr-
ror type. The fourth is a DeletionPOSError, and the
fifth is a MovePOSWhereError. The number in the
final column specifies the desired proportion of the
particular error type in the output corpus and is op-
tional. However, if it is present for one error type, it
must be present for all. The overall size of the out-
put corpus is supplied as a parameter when running
GenERRate.
3.4 Error Generation
When frequency information is not supplied in the
error analysis file, GenERRate iterates through each
error in the error analysis file and each sentence in
the input corpus, tries to insert an error of this type
into the sentence and writes the resulting sentence
to the output file together with a description of the
error. GenERRate includes an option to write the
sentences into which an error could not be inserted
and the reason for the failure to a log file. When the
error analysis file does include frequency informa-
tion, a slightly different algorithm is used: for each
error, GenERRate selects sentences from the input
file and attempts to generate an instance of that error
until the desired number of errors has been produced
or all sentences have been tried.
4 Classification Experiments
We describe two experiments which involve the
use of GenERRate in a binary classification task
in which the classifiers attempt to distinguish be-
tween grammatically well-formed and ill-formed
sentences or, more precisely, to distinguish between
sentences in learner corpora which have been anno-
tated as erroneous and their corrected counterparts.
In the first experiment we use GenERRate to cre-
ate ungrammatical training data using information
about error types gleaned from a subset of a corpus
of transcribed spoken utterances produced by ESL
learners in a classroom environment. The classifier
is one of those described in Wagner et al (2007).
In the second experiment we try to generate a CLC-
inspired error corpus and we use one of the simplest
classifiers described in Andersen (2006). Our aim
is not to improve classification performance, but to
test the GenERRate tool, to demonstrate how it can
be used and to investigate differences between syn-
thetic and naturally occurring datasets.
4.1 Experiments with a Spoken Language
Learner Corpus
Wagner et al (2009) train various classifiers to
distinguish between BNC sentences and artificially
produced ungrammatical versions of BNC sentences
(see ?2). They report a significant drop in accuracy
when they apply these classifiers to real learner data,
including the sentences in a corpus of transcribed
spoken utterances. The aim of this experiment is to
investigate to what extent this loss in accuracy can
be reduced by using GenERRate to produce a more
realistic set of ungrammatical training examples.
The spoken language learner corpus contains over
4,000 transcribed spoken sentences which were pro-
duced by learners of English of all levels and with
a variety of L1s. The sentences were produced in
a classroom setting and transcribed by the teacher.
The transcriptions were verified by the students. All
86
of the utterances have been marked as erroneous.
4.1.1 Setup
A 200-sentence held-out section of the corpus is
analysed by hand and a GenERRate error analysis
file containing 89 errors is compiled. The most fre-
quent errors are those involving a change in noun or
verb number or an article deletion. GenERRate then
applies this error analysis file to 440,930 BNC sen-
tences resulting in the same size set of synthetic ex-
amples (?new-ungram-BNC?). Another set of syn-
thetic sentences (?old-ungram-BNC?) is produced
from the same input using the error generation pro-
cedure used by Wagner et al (2007; 2009). Table 1
shows examples from both sets.
Two classifiers are then trained, one on the orig-
inal BNC sentences and the old-ungram-BNC sen-
tences, and the other on the original BNC sentences
and the new-ungram-BNC sentences. Both classi-
fiers are tested on 4,095 sentences from the spo-
ken language corpus (excluding the held-out sec-
tion). 310 of these sentences are corrected, resulting
in a small set of grammatical test data. The classi-
fier used is the POS n-gram frequency classifier de-
scribed in Wagner et al (2007).3 The features are
the frequencies of the least frequent n-grams (2?7)
in the input sentence. The BNC (excluding those
sentences that are used as training data) is used as
reference data to compute the frequencies. Learning
is carried out using the Weka implementation of the
J48 decision tree algorithm.4
4.1.2 Results
The results of the experiment are displayed in Ta-
ble 2. The evaluation measures used are precision,
recall, total accuracy and accuracy on the grammat-
ical side of the test data. Recall is the same as accu-
racy on the ungrammatical side of the test data.
The results are encouraging. There is a signifi-
cant increase in accuracy when we train on the new-
ungram-BNC set instead of the old-ungram-BNC
set. This increase is on the ungrammatical side of
3Wagner et al (2009) report accuracy figures in the range
55?70% for their various classifiers (when tested on synthetic
test data), but the best performance is obtained by combining
parser-output and n-gram POS frequency features using deci-
sion trees in a voting scheme.
4http://www.cs.waikato.ac.nz/ml/weka/
the test data, i.e. an increase in recall, demonstrat-
ing that by analysing a small set of data from our
test domain, we can automatically create more effec-
tive training data. This is useful in a scenario where
a small-to-medium-sized learner corpus is available
but which is not large enough to be split into a train-
ing/development/test set. These results seem to indi-
cate that reasonably useful training data can be cre-
ated with minimum effort. Of course, the accuracy is
still rather low but we suspect that some of this dif-
ference can be explained by domain effects ? the
sentences in the training data are BNC written sen-
tences (or distorted versions of them) whereas the
sentences in the learner corpus are transcribed spo-
ken utterances. Re-running the experiments using
the spoken language section of the BNC as training
data might yield better results.
4.2 A CLC-Inspired Corpus
We investigate to what extent it is possible to cre-
ate a large error corpus inspired by the CLC using
the current version of GenERRate. The CLC is a
30-million-word corpus of learner English collected
from University of Cambridge ESOL exam papers
at different levels. Approximately 50% of the CLC
has been annotated for errors and corrected.
4.2.1 Setup
We attempt to use GenERRate to insert errors
into corrected CLC sentences. In order to do
this, we need to create a CLC-specific error anal-
ysis file. In contrast to the previous experiment,
we do this automatically by extracting erroneous
POS trigrams from the error-annotated CLC sen-
tences and encoding them as GenERRate errors.
This results in approximately 13,000 errors of the
following types: DeletionPOSWhereError, Inser-
tionPOSWhereError, MovePOSWhereError, Sub-
stWordConfusionError, SubstWordConfusionNew-
POSError, SubstSpecificWordConfusionError and
SubstWrongFormError. Frequencies are extracted,
and errors occurring only once are excluded.
Three classifiers are trained. The first is trained
on corrected CLC sentences (the grammatical sec-
tion of the training set) and original CLC sentences
(the ungrammatical section). The second classifier
is trained on corrected CLC sentences and the sen-
tences that are generated from the corrected CLC
87
Old-Ungram-BNC New-Ungram-BNC
Biogas production production is growing rapidly Biogas productions is growing rapidly
Emil as courteous and helpful Emil courteous and was helpful
I knows what makes you tick I know what make you tick
He did n?t bother to lift his eyes from the task hand He did n?t bother lift his eyes from the task at hand
Table 1: Examples from two synthetic BNC sets
Training Data Precision Recall Accuracy Accuracy on Grammatical
BNC/old-ungram-BNC 95.5 37.0 39.8 76.8
BNC/new-ungram-BNC 94.9 51.6 52.4 63.2
Table 2: Spoken Language Learner Corpus Classification Experiment
sentences using GenERRate (we call these ?faux-
CLC?). The third is trained on corrected CLC sen-
tences and a 50/50 combination of CLC and faux-
CLC sentences. In all experiments, the grammat-
ical section of the training data contains 438,150
sentences and the ungrammatical section 454,337.
The classifiers are tested on a held-out section of
the CLC containing 43,639 corrected CLC sen-
tences and 45,373 original CLC sentences. To train
the classifiers, the Mallet implementation of Naive
Bayes is used.5 The features are word unigrams
and bigrams, as well as part-of-speech unigrams, bi-
grams and trigrams. Andersen (2006) experimented
with various learning algorithms and, taking into ac-
count training time and performance, found Naive
Bayes to be optimal. The POS-tagging is carried out
by the RASP system (Briscoe and Carroll, 2002).
4.2.2 Results
The results of the CLC classification experiment
are presented in Table 3. There is a 6.2% drop in
accuracy when we move from training on original
CLC sentences to artificially generated sentences.
This is somewhat disappointing since it means that
we have not completely succeeded in replicating the
CLC errors using GenERRate. Most of the accu-
racy drop is on the ungrammatical side, i.e. the cor-
rect/faux model classifies more incorrect CLC sen-
tences as correct than the correct/incorrect model.
This drop in accuracy occurs because some fre-
quently occurring error types are not included in the
error analysis file. One reason for the gap in cover-
age is the failure of the part-of-speech tagset to make
some important distinctions. The corrected CLC
5http://mallet.cs.umass.edu/
sentences which were used to generate the faux-
CLC set were tagged with the CLAWS tagset, and
although more fine-grained than the Penn tagset, it
does not, for example, make a distinction between
mass and count nouns, a common source of error.
Another important reason for the drop in accuracy
are the recurrent spelling errors which occur in the
incorrect CLC test set but not in the faux-CLC test
set. It is promising, however, that much of the per-
formance degradation is recovered when a mixture
of the two types of ungrammatical training data is
used, suggesting that artificial data could be used to
augment naturally occurring training sets
5 Limitations of GenERRate
We present three issues that make the task of gener-
ating synthetic error data non-trivial.
5.1 Sophistication of Input Format
The experiments in ?4 highlight coverage issues
with GenERRate, some of which are due to the sim-
plicity of the supported error types. When linguis-
tic context is supplied for deletion or insertion er-
rors, it takes the form of the POS of the words im-
mediately to the left and/or right of the target word.
Lee and Seneff (2008a) analysed preposition errors
made by Japanese learners of English and found that
a greater proportion of errors in argument preposi-
tional phrases (look at him) involved a deletion than
those in adjunct PPs (came at night). The only way
for such a distinction to be encoded in a GenERRate
error analysis file is to allow parsed input to be ac-
cepted. This brings with it the problem, however,
that parsers are less accurate than POS-taggers. An-
other possible improvement would be to make use
88
Training Data Precision Recall Accuracy Accuracy on Grammatical
Held-Out Test Data
Correct/Incorrect CLC 69.7 42.6 61.3 80.8
Correct/Faux CLC 62.0 30.7 55.1 80.5
Correct/Incorrect+Faux CLC 69.7 38.2 60.0 82.7
Table 3: CLC Classification Experiment
of WordNet synsets in order to choose the new word
in substitution errors.
5.2 Covert Errors
A covert error is an error that results in a syntac-
tically well-formed sentence with an interpretation
different from the intended one. Covert errors are a
natural phenomenon, occurring in real corpora. Lee
and Seneff (2008b) give the example I am preparing
for the exam which has been annotated as erroneous
because, given its context, it is clear that the per-
son meant to write I am prepared for the exam. The
problems lie in deciding what covert errors should
be handled by an error detection system and how to
create synthetic data which gets the balance right.
When to avoid: Covert errors can be produced
by GenERRate as a result of the sparse linguistic
context provided for an error in the error analysis
file. An inspection of the new-ungram-BNC set
shows that some error types are more likely to re-
sult in covert errors. An example is the SubstWrong-
FormError when it is used to change a noun from
singular to plural. This results in the sentence But
there was no sign of Benny?s father being changed
to the well-formed but more implausible But there
was no sign of Benny?s fathers. The next version of
GenERRate should include the option to change the
form of a word in a certain context.
When not to avoid: In the design of GenERRate,
particularly in the design of the SubstWrongFormEr-
ror type, the decision was made to exclude tense er-
rors because they are likely to result in covert er-
rors, e.g. She walked home? She walks home. But
in doing so we also avoid generating examples like
this one from the spoken language learner corpus:
When I was a high school student, I go to bed at one
o?clock. These tense errors are common in L2 data
and their omission from the faux-CLC training set
is one of the reasons why the performance of this
model is inferior to the real-CLC model.
5.3 More complex errors
The learner corpora contain some errors that are
corrected by applying more than one transforma-
tion. Some are handled by the SubstWrongFormEr-
ror type (I spend a long time to fish?I spend a long
time fishing) but some are not (She is one of reason
I became interested in English ? She is one of the
reasons I became interested in English).
6 Conclusion
We have presented GenERRate, a tool for automati-
cally introducing syntactic errors into sentences and
shown how it can be useful for creating synthetic
training data to be used in grammatical error detec-
tion research. Although we have focussed on the
binary classification task, we also intend to test Gen-
ERRate in targeted error detection. Another avenue
for future work is to explore whether GenERRate
could be of use in the automatic generation of lan-
guage test items (Chen et al, 2006, for example).
Our immediate aim is to produce a new version of
GenERRate which tackles some of the coverage is-
sues highlighted by our experiments.
Acknowledgments
This paper reports on research supported by the Uni-
versity of Cambridge ESOL Examinations. We are
very grateful to Cambridge University Press for giv-
ing us access to the Cambridge Learner Corpus and
to James Hunter from Gonzaga College for sup-
plying us with the spoken language learner corpus.
We thank Ted Briscoe, Josef van Genabith, Joachim
Wagner and the reviewers for their very helpful sug-
gestions.
89
References
?istein E. Andersen. 2006. Grammatical error detection.
Master?s thesis, Cambridge University.
?istein E. Andersen. 2007. Grammatical error detection
using corpora and supervised learning. In Ville Nurmi
and Dmitry Sustretov, editors, Proceedings of the 12th
Student Session of the European Summer School for
Logic, Language and Information, Dublin.
Johnny Bigert, Jonas Sjo?bergh, Ola Knutsson, and Mag-
nus Sahlgren. 2005. Unsupervised evaluation of
parser robustness. In Proceedings of the 6th CICling,
Mexico City.
Johnny Bigert. 2004. Probabilistic detection of context-
sensitive spelling errors. In Proceedings of the 4th
LREC, Lisbon.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd LREC, Las Palmas.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL errors using phrasal SMT tech-
niques. In Proceedings of the 21st COLING and the
44th ACL, Sydney.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Chia-Yin Chen, Liou Hsien-Chin, and Jason S. Chang.
2006. Fast ? an automatic generation system for
grammar tests. In Proceedings of the COLING/ACL
2006 Interactive Presentation Sessions, Sydney.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of the 22nd COLING, Manchester.
Jennifer Foster. 2007. Treebanks gone bad: Parser evalu-
ation and retraining using a treebank of ungrammatical
sentences. International Journal on Document Analy-
sis and Recognition, 10(3-4):129?145.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitriy Belenko,
and Lucy Vanderwende. 2008. Using contextual
speller techniques and language modelling for ESL er-
ror correction. In Proceedings of the 3rd IJCNLP, Hy-
derabad.
Roger Garside, Geoffrey Leech, and Geoffrey Sampson,
editors. 1987. The Computational Analysis of En-
glish: a Corpus-Based Approach. Longman, London.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115?129.
Carl James. 1998. Errors in Language Learning and
Use: Exploring Error Analysis. Addison Wesley
Longman.
John Lee and Stephanie Seneff. 2008a. An analysis of
grammatical errors in non-native speech in English. In
Proceedings of the 2008 Spoken Language Technology
Workshop, Goa.
John Lee and Stephanie Seneff. 2008b. Correcting mis-
use of verb forms. In Proceedings of the 46th ACL,
Columbus.
Vanessa Metcalf and Detmar Meurers. 2006. Towards
a treatment of word order errors: When to use deep
processing ? and when not to. Presentation at the NLP
in CALL Workshop, CALICO 2006.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proceedings of the 45th ACL, Prague.
Beatrice Santorini. 1991. Part-of-speech tagging guide-
lines for the Penn Treebank project. Technical report,
University of Pennsylvania, Philadelphia, PA.
Jonas Sjo?bergh and Ola Knutsson. 2005. Faking errors to
avoid making errors. In Proceedings of RANLP 2005,
Borovets.
Noah A. Smith and Jason Eisner. 2005a. Contrastive
Estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd ACL, Ann Arbor.
Noah A. Smith and Jason Eisner. 2005b. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proceedings of the IJCAI Workshop on Gram-
matical Inference Applications, Edinburgh.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proceedings of the
45rd ACL, Prague.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proceedings of the 22nd COLING, Manchester.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A comparative evaluation of deep and
shallow approaches to the automatic detection of com-
mon grammatical errors. In Proceedings of the joint
EMNLP/CoNLL, Prague.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal. Special Issue
on the 2008 Automatic Analysis of Learner Language
CALICO Workshop. To Appear.
90
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 32?41,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Developing and testing
a self-assessment and tutoring system
?istein E. Andersen
iLexIR
Streets, 62 Hills Road
Cambridge, CB2 1LA
and@ilexir.co.uk
Helen Yannakoudakis
Cambridge English
1 Hills Road
Cambridge, CB1 2EU
yannakoudakis.h
@cambridgeenglish.org
Fiona Barker
Cambridge English
1 Hills Road
Cambridge, CB1 2EU
barker.f
Tim Parish
iLexIR
Streets, 62 Hills Road
Cambridge, CB2 1LA
tim@ilexir.co.uk
Abstract
Automated feedback on writing may be a use-
ful complement to teacher comments in the
process of learning a foreign language. This
paper presents a self-assessment and tutoring
system which combines an holistic score with
detection and correction of frequent errors and
furthermore provides a qualitative assessment
of each individual sentence, thus making the
language learner aware of potentially prob-
lematic areas rather than providing a panacea.
The system has been tested by learners in
a range of educational institutions, and their
feedback has guided its development.
1 Introduction
Learning to write a foreign language well requires
a considerable amount of practice and appropriate
feedback. Good teachers are essential, but their time
is limited. As recently shown in a study by Wang et
al. (in press) conducted amongst first-year students
of English at a Taiwanese university, automated
writing evaluation can lead to increased learner au-
tonomy and higher writing accuracy. In this pa-
per, we investigate the merits of a self-assessment
and tutoring (SAT) system specifically aimed at in-
termediate learners of English, at around B2 level
in the Common European Framework of Reference
for Languages (CEFR) (Council of Europe, 2001).
There are a large number of students at this level,
and they should have sufficient knowledge of the
language to benefit from the system whilst at the
same time committing errors which can be identified
reliably.
The system provides automated feedback on
learners? writing at three different levels of gran-
ularity: an overall assessment of their proficiency,
a score for each individual sentence, highlighting
well-written passages as well as ones requiring more
work, and specific comments on local issues includ-
ing spelling and word choice.
Computer-based writing tools have been around
for a long time, with Criterion (Burstein et al, 2003,
which also provides a number of features for teach-
ers) and ESL Assistant (Gamon et al, 2009, not
currently available) aimed specifically at second-
language learners, but the idea of indicating the rel-
ative quality of different parts of a text (sentences in
our case) has, to the best of our knowledge, not been
implemented previously. This kind of non-specific
feedback does not provide a precise diagnosis or im-
mediate cure, but might have the advantage of fos-
tering learning.
In addition to describing the SAT system itself, we
present a series of three trials in which learners of
English in a number of educational contexts used the
system as a tool to work on written responses to spe-
cific tasks and improve their writing skills.
2 System
The SAT system is made available to students learn-
ing English as a Web service to which they can
sign up with a code (?class key?) provided by their
teacher. Once they have filled in a short demo-
graphic questionnaire, the users can respond to one,
two, three or more writing tasks. The students can
save their work at any time and ask the system to
assess the current version of their text, which will
32
Figure 1: SAT system screen where students can see the automated feedback and revise their piece of writing. The
?score feedback? and ?error feedback? views are shown in Figures 2 and 3.
give feedback as shown in Figure 1 and described
in more detail in the following subsections. Assess-
ment times are currently around 15sec, which facil-
itates incremental and exploratory editing of a text
to improve it, giving the students the ability to try
out different ways of correcting a problematic turn
of phrase. The teacher can see which students have
signed up and look at the last saved version of their
responses. Finally, the students are asked to answer
a few questions about their experience with the sys-
tem.
2.1 Text assessment
The SAT system provides an overall assessment of
someone?s proficiency by automatically analysing
and scoring the text as a whole. There is a large
body of literature with regard to automated text scor-
ing systems (Page, 1968; Rudner and Liang, 2002;
Attali and Burstein, 2006; Briscoe et al, 2010). Ex-
isting systems, overviews of which have been pub-
lished in various studies (Dikli, 2006; Williamson,
2009; Shermis and Hamner, 2012), involve a large
range of techniques, such as discriminative and gen-
erative machine learning, clustering algorithms and
vectorial semantics, as well as syntactic parsers.
We approach automated text assessment as a su-
pervised machine learning problem, which enables
us to take advantage of existing annotated data. We
use the publically-available First Certificate in En-
glish (FCE) dataset of upper-intermediate learner En-
glish (Yannakoudakis et al, 2011) and focus on as-
sessing general linguistic competence. Systems that
measure English competence directly are easier and
faster to deploy, since they are more likely to be re-
usable and generalise better across different genres
than topic-specific ones, which are not immediately
33
usable when new tasks are added, since the model
cannot be applied until a substantial amount of man-
ually annotated responses have been collected for a
specific prompt.
Following previous research, we employ discrim-
inative ranking, which has been shown to achieve
state-of-the-art results on the task of assessing
free-text writing competence (Yannakoudakis et al,
2011). The underlying idea is that high-scoring texts
(or ?scripts?) should receive a higher rank than low-
scoring ones. We train a linear ranking perceptron
(Bo?s and Opper, 1998) on features derived from pre-
vious work (namely, lexical and grammatical prop-
erties of text) and compare it to our previous model
(Yannakoudakis et al, 2011), which is trained using
ranking Support Vector Machines (Joachims, 2002).
Our new perceptron model achieves 0.740 and 0.765
Pearson product-moment (r) and Spearman?s rank
correlation coefficient (?) respectively between the
gold and predicted scores; this is comparable to
our previous SVM model, which achieves 0.741 and
0.773, and the differences are not significant.
In order to provide scoring feedback1 based on
the predictions of our model, we use visual presen-
tations. Visualisation techniques allow us to go be-
yond the mere display of a number, can stimulate the
learners? visual perceptions, and, when used appro-
priately, information can be displayed in an intuitive
and easily interpretable way. Furthermore, aesthet-
ics in computer-based interfaces have been shown to
have an effect on the users. For example, Ben-Bassat
et al (2006) have found an interdependence between
perceived aesthetics and usability in questionnaire-
based assessments, and have shown that users? pref-
erences are not necessarily based only upon perfor-
mance; aesthetics also play a role.
More specifically, we assign an overall score on
a scale from red for a text that looks like it may be
at intermediate level or below to green for a text that
shows some evidence of being at upper-intermediate
level (the level assessed by the FCE exam) or above
(i.e., advanced). This is illustrated in Figure 1 below
the Overall score section, where an arrow is used to
indicate the level of text quality on a colour gradient
defined by the two extreme points, red and green.
1Note that ranks can be transformed to scores through linear
regression, while correlation remains unaltered as it is invariant
to linear transformations.
A text with the highest score possible would indi-
cate that the learner has potentially shown evidence
of being at a level higher than that assessed by FCE,
the latter, of course, being dependent on the extent
to which higher-order linguistic skills are elicited by
the prompts. On the contrary, a very low score in-
dicates poor linguistic abilities corresponding to a
lower level.
Although exams that encompass the full range of
language proficiency exhibited at different stages of
learning are hard to design, the FCE exam, bench-
marked at the B2 level and reserving some of its
score range for performances beneath and beyond,
allows us to roughly estimate someone?s proficiency
as being far below, just below, around or above an
upper intermediate level. The task of predicting at-
tainment levels has recently started to receive atten-
tion (Dickinson et al, 2012; Hawkins and Filipovic?,
2012).
2.2 Sentence evaluation
The second component of the SAT system automat-
ically assesses and scores the quality of individual
sentences, independently of their context. The chal-
lenge of assessing intra-sentential quality lies in the
limited linguistic evidence that can be extracted au-
tomatically from relatively short sentences for them
to be assessed reliably, in addition to the difficulty
in acquiring annotated data, since rating a response
sentence by sentence is not something examiners
typically do and would therefore require an addi-
tional and expensive manual annotation effort.
Previous work has primarily focused on automatic
content scoring of short answers, ranging from a few
words to a few sentences (Pulman and Sukkarieh,
2005; Attali et al, 2008; Mohler et al, 2011; Ziai
et al, 2012). On the other hand, scoring of individ-
ual sentences with respect to their linguistic quality,
specifically in learner texts, has received consider-
ably less attention. Higgins et al (2004) devised
guidelines for the manual annotation of sentences in
learner texts, and evaluated a rule-based approach
that classifies sentences with respect to clarity of ex-
pression based on grammar, mechanics and word us-
age errors; however, their system performs binary
classification, whereas we are focusing on scoring
sentences. Writing instruction tools, such as Crite-
rion (Burstein et al, 2003), give advice on stylistic
34
and organisational issues and automatically detect a
variety of errors in the text, though they do not ex-
plicitly allow for an overall evaluation of sentences
with respect to various writing aspects. The latter,
used in combination with an error feedback compo-
nent (see Section 2.3), can be a useful instrument
informing learners about the severity of their mis-
takes; for example, although sentences may contain
some errors, they may still maintain a certain level
of acceptability that does not impede communica-
tion. Moreover, indicating problematic regions may
be better from a pedagogic point of view than detect-
ing and correcting all errors identified in the text.
To date, there is no publically available annotated
dataset consisting of sentences marked with a score
representing their linguistic quality. Manual annota-
tion is typically expensive and time-consuming, and
a certain amount of annotator training is generally
required. Instead, we exploit already available an-
notated data ? scores and error annotation in the FCE
dataset ? and evaluate various approaches, two of
which are: a) to use the script-level model (see Sec-
tion 2.1) to predict sentence quality scores, and b) to
use the script-level score divided by the total num-
ber of (manually annotated) errors in a sentence as
pseudo-gold labels to train a sentence-level model.
As the models above are expected to contain a cer-
tain amount of noise, it is imperative that we iden-
tify evaluation measures that are indicative of our
application ? that is, assign higher scores to high-
quality sentences compared to low-quality ones ?
and not only depend on the labels they have been
trained on. More specifically, we use correlation
with pseudo-gold scores (rg and ?g; not applicable
to the script-level model), correlation with the script-
level scores by first averaging predicted sentence-
level scores (rs and ?s), correlation with error counts
(re and ?e), average precision (AP) and pairwise ac-
curacy. AP is a measure used in information retrieval
to evaluate systems that return a ranked list of doc-
uments. Herein, sentences are ranked by their pre-
dicted scores, precision is calculated at each correct
sentence (that is, containing no errors), and aver-
aged over all correct sentences (in other words, we
treat sentences with no errors as the ?relevant doc-
uments?). Pairwise accuracy is calculated based on
the number of times the corrected sentence (avail-
able through the error annotation in the FCE dataset)
is ranked higher than the original one written by the
candidate, ignoring sentences without errors. Corre-
lation with error counts, average precision and pair-
wise accuracy are particularly important as they re-
flect more directly the extent to which good and bad
sentences are discriminated. Again, in both cases,
we employ a linear ranking perceptron.
We conducted a series of experiments on a sep-
arate development set to evaluate the performance
of features beyond the ones used in the script-level
model. The final results, reported in Table 1, are
calculated on the FCE test set (Yannakoudakis et al,
2011).
Our best configuration is model b, which achieves
the highest results according to most evaluation
measures with a feature space consisting of 1) er-
ror counts identified through the absence of word
trigrams in a large background corpus, 2) phrase-
structure rules, 3) presence of frequent errors, as
well as the number of words defining an error, as
described in Section 2.3, 4) the presence of main
verbs, nouns, adjectives, subordinating conjuctions
and adverbs, 5) affixes and 6) the presence of clausal
subjects and modifiers. The texts were parsed using
RASP (Briscoe et al, 2006).
Model a, the script-level model, does not work as
well at the sentence level. However, it does perform
better when evaluated against script-level scores (rs
and ?s), and this is expected given that it is trained
directly on gold script-level scores. On the other
hand, this evaluation measure is not as indicative of
good performance in our application as the others,
as it does not take into account the varying quality
of individual sentences within a script.
Training the script-level model with different fea-
ture sets (including those utilised in the sentence-
level model) did not yield an improvement in per-
formance (the results are omitted due to space re-
strictions). Additional experiments were conducted
to investigate the effect of training the sentence-level
model with different pseudo-gold labels (e.g., addi-
tive/subtractive pseudo-gold scores rather than divi-
sive/multiplicative), but the results are not reported
here as the difference in performance was not sub-
stantial.
Table 1 shows that better performance can be
achieved with our pseudo-gold labels, used to train
a model at the sentence level, rather than gold la-
35
Model a Model b
rg ? 0.550
?g ? 0.646
rs 0.572 0.385
?s 0.578 0.301
re ?0.111 ?0.750
?e ?0.078 ?0.702
AP 0.393 0.747
Pairwise
Correct 0.608 0.703
Incorrect 0.359 0.204
Table 1: Results on the FCE test set for the script-level
model (a) and our model (b).
bels at the script level. To evaluate this further,
we trained a sentence-level model using the script-
level scores as labels (that is, sentences within the
same script are all assigned the same label/score).
However, this did not improve performance (again,
the results are omitted due to space restrictions).
We also point out that the best-performing feature
space (described above) is based on text properties
that are more likely to be present in relatively short
sentences (e.g., the presence of main verbs), com-
pared to those used for script-level models in previ-
ous work (Yannakoudakis et al, 2011), such as word
and part-of-speech bigrams and trigrams, which may
be too sparse for a sentence-level model.
Analogously to what we did to present the over-
all score, we developed a sentence score feedback
view to indicate the general quality of the sentences,
as given by our best model, by highlighting each of
them with a background colour ranging from green
for a well-written sentence, via yellow and orange
for a sentence which the system thinks is accept-
able, to dark orange and red for a sentence which
may have a few problems. Figure 2 shows how the
SAT system evaluates and colour-codes a few au-
thentic student-written sentences containing errors,
as well as their corrected counterparts based on the
error-coding in the FCE test set. Overall, the system
correctly identifies correct and incorrect versions of
each sentence, attributing a higher score (greener
colour) to the corrected sentence in each pair.
2.3 Word-level feedback
Basic spelling checkers have been around since the
1970s and grammar checkers since the 1980s (Ku-
kich, 1992), but misleading ?corrections? may be be-
wildering (Galletta et al, 2005), and the systems do
not always focus on the kinds of error frequently
committed, even less so in the case of learners as
was pointed out early on by Liou (1992), who tested
commercial grammar checkers on and developed a
system for detecting common errors in Taiwanese
learners? writing.
For word-level feedback within the SAT system,
we have implemented a method similar to one we
have used earlier in the context of pre-annotation of
learner corpora (Andersen, 2011). To ensure high
precision and good coverage of local errors typi-
cally committed by learners, error rules are gen-
erated from the Cambridge Learner Corpus (CLC)
(Nicholls, 2003) to detect word unigrams, bigrams
and trigrams which have been annotated as incorrect
at least five times and at least ninety per cent of the
times they occur. This way, rules can be extracted
from the existing error annotation in the corpus,
obviating the need for manually constructed mal-
rules, although the rules obtained by the two differ-
ent methods may to some extent be complementary.
In addition to corpus-derived rules, many classes of
incorrect but plausible derivational and inflectional
morphology are detected by means of rules derived
from a machine-readable dictionary. Many mistakes
are still not detected, but precision has been found to
be more important in terms of learning effect (Na-
gata and Nakatani, 2010), and errors missed by this
module will often give lower sentence scores.
Figure 3 illustrates some types of error detected
by the system. The feedback text is generated from
a small number of templates corresponding to differ-
ent categories of error marked up in the CLC.
We are currently working on extending this part
of the system with more general rules in addition to
word n-grams, e.g., part-of-speech tags and gram-
matical relations, in order to detect more errors with-
out loss in precision.
3 Trials
After the SAT system had been developed, a series
of trials were set up in order to test the online sys-
36
Figure 2: Examples of correct sentences (top) and incorrect ones (bottom) colour-coded by the SAT system.
Figure 3: The error feedback view identifies specific words that may have been used incorrectly. Explanations and
suggested corrections are provided in a separate column. The system actually proposes two different corrections for
and etc., namely etc. and and so on; the user will have to choose one or the other. The confusion between the verb see
and the noun sea is identified, but the the is not actually unnecessary; in this case, the system has been led astray by
the surrounding errors.
tem and to collect feedback from language learners
and their teachers in a variety of contexts. Three tri-
als were undertaken in November 2012, December
2012 and in March 2013, with changes made to the
system between each pair of trials.
English Profile Network member institutions
were contacted who had access to language learners
and who had previously participated in data collec-
tion for the English Profile Programme2. Teachers at
universities, secondary schools and private language
schools signed up for two or more trials so that their
learners could use and provide feedback on several
iterations of the SAT system. Certificates of partici-
2See www.englishprofile.org
pation were offered to encourage involvement in the
trials.
Ten institutions were involved from nine coun-
tries, namely Belgium, the Czech Republic, France,
Lithuania, Poland, Romania, Russia, Slovakia and
Spain. Eight universities, one secondary school and
one private language school were represented, in-
cluding specialist and generalist institutions of ed-
ucational sciences, agricultural science, veterinary
medicine and foreign languages. Each trial had be-
tween 4 and 8 institutions taking part, and each in-
stitution participated in two or three trials with many
students undertaking more than one trial.
All students who took part in the trials, over 450
37
in total, were expected to be at or above the upper-
intermediate (CEFR B2) level as this was the level at
which the SAT system was designed to function.
Three initial sets of tasks were developed for the
planned system trials, each set consisting of three
short written prompts which asked the users to write
on a specified topic for a particular purpose, for ex-
ample:
Daily life
Your English class is going to make a
short video about daily life in your town.
Write a report for your teacher, suggest-
ing which activities should be filmed, and
why.
Tasks were based on retired questions from an in-
ternational proficiency test at B2 level of the CEFR.
Each task was given a short name which was shown
in the SAT system in order for the users to select the
most interesting or relevant task for themselves.
A short set of instructions was produced for both
teachers and students which was emailed to the main
contact in each institution and passed on to their col-
leagues, teachers and students who were interested
in taking part in the trial.
The trials operated as follows:
? The main institutional contact receives an invi-
tation to participate in the trials.
? Interested institutions receive instructions and
confirm the number of class keys required
(sign-up codes for the system).
? Main contact and teachers at each institution
log in and work through the system as if they
are a language learner, by completing a demo-
graphic questionnaire, writing 1?3 tasks which
are assessed by the system, and finally complet-
ing a short user satisfaction questionnaire.
? Students work through the SAT system either
with the support of their teacher in class or re-
motely.
3.1 SAT system usage
During Trial 1, on the busiest day there were 155
submissions and the highest number of users on
a single day was 32. These figures indicate that
Revisions Count
1 292
2 272
3 142
4 78
5 50
6 28
7 15
8 25
9 11
10 14
11?15 21
16?20 6
20? 5
Table 2: Number of revisions per task response.
all users were submitting their work for assessment
more than once, which suggests that the system is
being used in an iterative fashion as envisaged. Dur-
ing Trial 2, the busiest day saw more than twice as
many submissions as during the first trial (442), and
the most people online on any one day almost dou-
bled to 62. Across both trials we collected around
3000 submissions in total, including revisions; the
average number of revisions for a submitted piece
of writing is 3.2 with the highest figure being 54
revisions (see Table 2 for details). This suggests
that some users write their first response, then make
changes to one word or phrase at a time, resulting in
such a large number of revisions. When more than
one revision has been submitted, the score given by
the system to the last revision is higher than that
given to the initial revision in over 80% of the cases.
Current changes to the system allowing system ad-
ministrators to check on intermediate versions of
submitted texts are underway.
3.2 Feedback
In addition to looking at the writing submitted by
users of the system, there was both numerical and
written feedback available to the system developers.
This was used to suggest changes to the system at
subsequent trials.
As can be seen from Table 3, user satisfaction
scores were generally high and increased from Trial
1 to Trial 2. In the first pilot, the written feed-
back from instructors was generally positive whilst
38
Trial 1 Trial 2
Using the SAT system helps me to write better in English. 3.80 3.92
I find the SAT system useful for understanding my mistakes. 3.74 3.96
I think the sentence colouring is useful. 3.74 4.15
I think the word-level information [error feedback] is useful. 3.86 4.12
The SAT system is easy to use. 4.45 4.49
The feedback on my writing is clear. 3.80 3.93
If you have used the SAT system before, has it improved since the last time? 3.86
Table 3: Average feedback scores on a scale from 1 (strongly disagree) to 5 (strongly agree).
the learner feedback was mixed, especially when it
comes to sentence evaluation:
In summary, I liked this system, because
the sentence colouring suggests me to
think about my writing style, mistakes,
what I should improve, change. This sys-
tem is not like a teacher, who checks all
our errors, but makes us develop our crit-
ical thinking, which is the most important
for writing especially. [...]
It?s okay the way of colouring system, the
problem is that it doesn?t tell you specifi-
cally what?s wrong with constructions so
you have think what you failed.
The fact that the system provides almost immediate
feedback has been appreciated:
I like that the paragraphs which I wrote
assesed so quickly. . . . Secondly, I really
like that student can correct his text till it
gets ideal.
Users have also made suggestions for improve-
ments, which have been essential for deciding which
parts of the system should be developed further.
3.3 System changes
As a result of feedback and the team?s extensive use
of the system, after each trial changes were made
both to the on-screen experience and behind the
scenes. After Trial 1, the system was amended to
enable users to see paragraph breaks in the corrected
version (which before had not been shown in the as-
sessed view of the text). There was also a new error
view with permanently visible explanations and ex-
amples and an additional question on the feedback
questionnaire which asked whether users felt the
Words Count
0? 99 540
100?199 1,294
200?299 928
300?399 201
400?499 67
500?999 26
1,000? 36
Table 4: Number of words per submission.
system had improved since the previous time they
used it. Behind the scenes, the server was upgraded
to cope with anticipated demand and code was writ-
ten so that administrators could review statistics on
usage.
At the time of writing the third SAT system trial
was underway. In the first two trials the total number
of words collected was over 600,000 with an average
response length of around 1100 characters or 200
words. Encouragingly, there were many longer re-
sponses including twelve over 1080 words in length
and the longest written to date is 1773 words. These
figures indicate that the system is not restrictive, but
encourages and inspires students to write. Table 4
gives an overview of the script length distribution.
Following two successful trials, the third trial
aimed to involve new and existing users and to pro-
vide more detailed teacher feedback.
4 Conclusions
In this paper, we described a tool that provides feed-
back to learners of English at three different levels
of granularity: an overall assessment of their profi-
ciency, assessment of individual sentences, and di-
agnostic feedback on local issues including spelling
and word choice. We argued that the use of visual-
39
isation techniques is important, as they allow us to
go beyond the mere display of a number, can stimu-
late the learners? visual perceptions, and can display
information in an intuitive and easily interpretable
way. The usefulness and usability of the tool as a
whole, as well as of its components, was confirmed
through questionnaire-based evaluations, where, for
example, the perceived usefulness of the sentence
colouring received an average of 4.15 on a 5-point
scale.
The first component of the SAT system, script-
level assessment, uses a machine learner to predict
a score for a text and roughly estimate someone?s
proficiency level based on lexical and grammatical
features. The second component allows for an auto-
matic evaluation of the linguistic quality of individ-
ual sentences. We proposed a method for generat-
ing sentence-level scores, which we use for training
our model. Using this method, we were able to learn
what features can be used to evaluate linguistic qual-
ity of (relatively short) sentences. Indicating prob-
lematic regions via highlighting of sentences may be
better from a pedagogic point of view than detecting
and correcting all errors identified in the text. The
third component automatically provides diagnostic
feedback on local errors with high precision on the
basis of a few templates, without relying on manu-
ally crafted rules.
The trials undertaken so far have improved the
functionality of the system in regard to what is on
offer to teachers and their students, but they have
also provided the basis for further research and de-
velopment to enhance the system?s functionality and
design and move towards wider deployment. We
plan to continue improving the methodologies used
for providing feedback to learners, as well as adding
further functionality, such as L1-specific feedback.
Another logical next step would be to continue to-
wards lower levels of granularity, moving from the
sentence as the unit of assessment to clauses and
phrases, which may be particularly beneficial for
more advanced language users who write longer and
more complex sentences.
Acknowledgements
Special thanks to Ted Briscoe and Marek Rei, as
well as to the anonymous reviewers, for their valu-
able contributions at various stages.
References
?istein E. Andersen. 2011. Semi-automatic ESOL error
annotation. English Profile Journal, 2.
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-Rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3):1?30.
Yigal Attali, Don Powers, Marshall Freedman, Marissa
Harrison, and Susan Obetz. 2008. Automated Scoring
of short-answer open-ended GRE subject test items.
Technical Report 04, ETS.
Tamar Ben-Bassat, Joachim Meyer, and Noam Tractin-
sky. 2006. Economic and subjective measures
of the perceived value of aesthetics and usability.
ACM Transactions on Computer-Human Interaction,
13(2):210?234.
Siegfried Bo?s and Manfred Opper. 1998. Dynamics of
batch training in a perceptron. Journal of Physics A:
Mathematical and General, 31(21):4835?4850.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In ACL-
Coling?06 Interactive Presentation Session, pages 77?
80.
Ted Briscoe, Ben Medlock, and ?istein E. Andersen.
2010. Automated assessment of ESOL free text exam-
inations. Technical Report UCAM-CL-TR-790, Uni-
versity of Cambridge, Computer Laboratory.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003. Criterion: Online essay evaluation: An appli-
cation for automated evaluation of student essays. In
Proceedings of the fifteenth annual conference on in-
novative applications of artificial intelligence, pages
3?10.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teaching,
Assessment. Cambridge University Press.
Markus Dickinson, Sandra Ku?bler, and Anthony Meyer.
2012. Predicting learner levels for online exercises of
Hebrew. In Proceedings of the Seventh Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 95?104. Association for Computa-
tional Linguistics.
Semire Dikli. 2006. An overview of automated scoring
of essays. Journal of Technology, Learning, and As-
sessment, 5(1).
Dennis F. Galletta, Alexandra Durcikova, Andrea Ever-
ard, and Brian M. Jones. 2005. Does spell-checking
software need a warning label? Communications of
the ACM, 48(7):82?86.
Michael Gamon, Claudia Leacock, Chris Brockett,
William B Dolan, Jianfeng Gao, Dmitriy Belenko, and
40
Alexandre Klementiev. 2009. Using statistical tech-
niques and web search to correct ESL errors. Calico
Journal, 26(3):491?511.
John A. Hawkins and Luna Filipovic?. 2012. Criterial
Features in L2 English: Specifying the Reference Lev-
els of the Common European Framework. English
Profile Studies. Cambridge University Press.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing, pages 133?142.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24(4):377?439.
Hsien-Chin Liou. 1992. An automatic text-analysis
project for EFL writing revision. System: The Inter-
national Journal of Educational Technology and Lan-
guage Learning Systems, 20(4):481?492.
Michael A.G. Mohler, Razvan Bunescu, and Rada Mi-
halcea. 2011. Learning to grade short answer ques-
tions using semantic similarity measures and depen-
dency graph alignments. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies.
Ryo Nagata and Kazuhide Nakatani. 2010. Evaluating
performance of grammatical error detection to maxi-
mize learning effect. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, COLING ?10, pages 894?900, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Dawn Archer, Paul Rayson, Andrew Wilson,
and Tony McEnery, editors, Proceedings of the Cor-
pus Linguistics conference, volume 16 of Technical
Papers, pages 572?581. University Centre For Com-
puter Corpus Research on Lanugage, Lancaster Uni-
versity, Lancaster.
Ellis B. Page. 1968. The use of the computer in analyz-
ing student essays. International Review of Education,
14(2):210?225.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
second workshop on Building Educational Applica-
tions Using natural language processing, pages 9?16.
Lawrence M. Rudner and Tahung Liang. 2002. Auto-
mated essay scoring using Bayes? theorem. The Jour-
nal of Technology, Learning and Assessment, 1(2):3?
21.
Mark D. Shermis and Ben Hamner. 2012. Contrasting
state-of-the-art automated scoring of essays: analysis.
Technical report, The University of Akron and Kaggle.
Ying-Jian Wang, Hui-Fang Shang, and Paul Briody. In
press. Exploring the impact of using automated writ-
ing evaluation in English as a foreign language univer-
sity students? writing. Computer Assisted Language
Learning.
David M. Williamson. 2009. A framework for imple-
menting automated scoring. In Proceedings of the An-
nual Meeting of the American Educational Research
Association and the National Council on Measurement
in Education, San Diego, CA.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012.
Short answer assessment: Establishing links between
research strands. In Proceedings of the workshop on
Building Educational Applications Using natural lan-
guage processing, pages 190?200.
41
