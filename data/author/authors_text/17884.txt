Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1447?1456, Dublin, Ireland, August 23-29 2014.
A Markovian approach to distributional semantics
with application to semantic compositionality
?
Edouard Grave
EECS Department
UC Berkeley
grave@berkeley.edu
Guillaume Obozinski
LIGM ? Universit?e Paris-Est
?
Ecole des Ponts ? ParisTech
guillaume.obozinski
@imagine.enpc.fr
Francis Bach
Inria ? Sierra project-team
?
Ecole Normale Sup?erieure
francis.bach@ens.fr
Abstract
In this article, we describe a new approach to distributional semantics. This approach relies
on a generative model of sentences with latent variables, which takes the syntax into account
by using syntactic dependency trees. Words are then represented as posterior distributions over
those latent classes, and the model allows to naturally obtain in-context and out-of-context word
representations, which are comparable. We train our model on a large corpus and demonstrate
the compositionality capabilities of our approach on different datasets.
1 Introduction
It is often considered that words appearing in similar contexts tend to have similar meaning (Harris,
1954). This idea, known as the distributional hypothesis was famously summarized by Firth (1957)
as follow: ?you shall know a word by the company it keeps.? The distributional hypothesis has been
applied in computational linguistics in order to automatically build word representations that capture
their meaning. For example, simple distributional information about words, such as co-occurence counts,
can be extracted from a large text corpus, and used to build a vectorial representation of words (Lund
and Burgess, 1996; Landauer and Dumais, 1997). According to the distributional hypothesis, two words
having similar vectorial representations must have similar meanings. It is thus possible and easy to
compare words using their vectorial representations.
In natural languages, sentences are formed by the composition of simpler elements: words. It is
thus reasonable to assume that the meaning of a sentence is determined by combining the meanings
of its parts and the syntactic relations between them. This principle, often attributed to the German
logician Frege, is known as semantic compositionality. Recently, researchers in computational linguistics
started to investigate how the principle of compositionality could be applied to distributional models of
semantics (Clark and Pulman, 2007; Mitchell and Lapata, 2008). Given the representations of individual
words, such as federal and agency, is it possible to combine them in order to obtain a representation
capturing the meaning of the noun phrase federal agency?
Most approaches to distributional semantics represent words as vectors in a high-dimensional space
and use linear algebra operations to combine individual word representations in order to obtain represen-
tations for complex units. In this article, we propose a probabilistic approach to distributional semantics.
This approach is based on the generative model of sentences with latent variables, which was introduced
by Grave et al. (2013). We make the following contributions:
? Given the model introduced by Grave et al. (2013), we describe how in-context and ouf-of-context
words can be represented by posterior distributions over latent variables (section 4).
? We evaluate out-of-context representations on human similarity judgements prediction tasks and
determine what kind of semantic relations are favored by our approach (section 5).
? Finally, we evaluate in-context representations on two similarity tasks for short phrases (section 6).
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1447
2 Related work
Most approaches to distributional semantics are based on vector space models (VSM), in which words
are represented as vectors in a high-dimensional space. These vectors are obtained from a large text
corpus, by extracting distributional information about words such as the contexts in which they appear.
A corpus is then represented as a word-by-context co-occurence matrix. Contexts can be defined as
documents in which the target word appear (Deerwester et al., 1990; Landauer and Dumais, 1997) or
as words that appear in the neighbourhood of the target word, for example in the same sentence or in a
fixed-size window around the target word (Schutze, 1992; Lund and Burgess, 1996).
Next to vector space models, other approaches to distributional semantics are based on probabilistic
models of documents, such as probabilistic latent semantic analysis (pLSA) introduced by Hofmann
(1999) and which is inspired by latent semantic analysis, or latent Dirichlet allocation (LDA), introduced
by Blei et al. (2003). In those models, each document is viewed as a mixture of k topics, where each
topic is a distribution over the words of the vocabulary.
The previous models do not take into account the linguistic structure of the sentences used to build
word representations. Several models have been proposed to address this limitation. In those models, the
contexts are defined by using the syntactic relations between words (Lin, 1998; Curran and Moens, 2002;
Turney, 2006; Pad?o and Lapata, 2007; Baroni and Lenci, 2010). For example, two words are considered
in the same context if there exists a syntactic relation between them, or if there is a path between them in
the dependency graph.
One of the first approaches to semantic compositionality using vector space models was proposed
by Mitchell and Lapata (2008). In this study, individual word representations are combined using linear
algebra operations such as addition, componentwise multiplication, tensor product or dilation. Those dif-
ferent composition operations are then used to disambiguate intransitive verbs given a subject (Mitchell
and Lapata, 2008) or to compute similarity scores between pairs of small phrases (Mitchell and Lapata,
2010).
Another approach to semantic compositionality is to learn the function used to compose individual
word representations. First, a semantic space containing representations for both individual words and
phrases is built. For example, the words federal, agency and the phrase federal agency all have a vectorial
representation. Then, a function mapping individual word representations to phrase representations can
be learnt in a supervised way. Guevara (2010) proposed to use partial least square regression to learn
this function. Similarly, Baroni and Zamparelli (2010) proposed to learn a matrix A for each adjective,
such that the vectorial representation p of the adjective-noun phrase can be obtained from the vectorial
representation b of the noun by the matrix-vector multiplication:
p = Ab.
Socher et al. (2012) later generalized this model by proposing to represent each node in a parse tree by a
vector capturing the meaning and a matrix capturing the compositional effects. A composition function,
inspired by artificial neural networks, is recursively applied in the tree to compute those representations.
Following the theoretical framework introduced by Coecke et al. (2010), Grefenstette and Sadrzadeh
(2011) proposed to represent relational words (such as verbs) by tensors and theirs arguments (such
as nouns) by vectors. Composing a relational word with its arguments is then performed by taking
the pointwise product between the tensor and the Kronecker product of the vectors representing the
arguments. Jenatton et al. (2012) and Van de Cruys et al. (2013) proposed two approaches to model
subject-verb-object triples based on tensor factorization.
Finally, research in computation of word meaning in context is closely related to distributional seman-
tic compositionality. Erk and Pad?o (2008) proposed a structured vector space model in which a word
is represented by multiple vectors, capturing its meaning but also the selectional restrictions it has for
the different arguments. Those different vectors can then be combined to compute a word representation
in context. This model was later generalized by Thater et al. (2010). Dinu and Lapata (2010) intro-
duced a probabilistic model for computing word representations in context. In their approach, words are
represented as probability distributions over latent senses.
1448
Computers can be designed to do anything with information
c
0
c
1
c
2
c
3
c
4
c
5
c
6
c
7
c
8
c
9
w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9
Figure 1: Example of a dependency tree and its corresponding graphical model.
3 Model of semantics
In this section we briefly review the generative model of sentences introduced by Grave et al. (2013), and
which serves as the basis of our approach to distributional semantics.
3.1 Generative model of sentences
We denote the tokens of a sentence of length K by the K-uple w = (w
1
, ..., w
K
) ? {1, ..., V }
K
, where
V is the size of the vocabulary and each integer represents a word. We suppose that each token w
k
is
associated to a corresponding semantic class c
k
? {1, ..., C}, where C is the number of semantic classes.
Finally, the syntactic dependency tree corresponding to the sentence is represented by the function pi :
{1, ...,K} 7? {0, ...,K}, where pi(k) represents the parent of word k and 0 is the root of the tree (which
is not associated to a word).
Given a tree pi, the semantic classes and the words of a sentence are generated as follows. The semantic
class of the root of the tree is set to a special start symbol, represented by the integer 0.
1
Then, the
semantic classes corresponding to words are recursively generated down the tree: each semantic class
c
k
is drawn from a multinomial distribution p
T
(c
k
| c
pi(k)
), conditioned on the semantic class c
pi(k)
of
its parent in the tree. Finally, each word w
k
is also drawn from a multinomial distribution p
O
(w
k
| c
k
),
conditioned on its corresponding semantic class c
k
. Thus, the joint probability distribution on words and
semantic classes can be factorized as
p(w, c) =
K
?
k=1
p
T
(c
k
| c
pi(k)
)p
O
(w
k
| c
k
),
where the variable c
0
= 0 represents the root of the tree. The initial class probability distribution
p
T
(c
k
| c
0
= 0) is parameterized by the probability vector q, while the transition probability distribution
between classes p
T
(c
k
| c
pi(k)
) and the emission probability distribution p
O
(w
k
| c
k
) are parameterized
by the stochastic matrices T and O (i.e., matrices with non-negative elements and unit-sum columns).
This model is a hidden Markov model on a tree (instead of a chain). See Fig. 1 for an example of a
sentence and its corresponding graphical model.
3.2 Corpus and learning
We train the generative model of sentences on the ukWac corpus (Baroni et al., 2009). This corpus, which
contains approximately 1.9 billions tokens, was POS-tagged and lemmatized using TreeTagger (Schmid,
1994) and parsed using MaltParser (Nivre et al., 2007). Each word of our vocabulary is a pair of lemma
and its part-of-speech. We perform smoothing by only keeping the V most frequent pairs, the infrequent
ones being replaced by a common token. The parameters ? = (q,T,O) of the model are learned
using the algorithm described by Grave et al. (2013). The number of latent states C and the number of
lemma/POS pairs V were set using the development set of Bruni et al. (2012).
1
We recall that the semantic classes corresponding to words are represented by integers between 1 and C.
1449
president
chief
chairman
director
executivemanagereyeface
shoulder
hand
leg foot
head head-2head-1
Figure 2: Comparison of out-of-context (black) and in-context (red) word representations. The two-
dimensional visualization is obtained by using multidimensional scaling (Borg, 2005). See text for de-
tails.
4 Word representations
Given a trained hidden Markov model, we now describe how to obtain word representations, for both in-
context and out-of-context words. In both cases, words will be represented as a probability distribution
over the latent semantic classes.
In-context word representation. Obtaining a representation of a word in the context of a sentence is
very natural using the model introduced in the previous section: we start by parsing the sentence in order
to obtain the syntactic dependency tree. We then compute the posterior distribution of semantic classes c
for that word, and use this probability distribution to represent the word. More formally, given a sentence
w = (w
1
, ..., w
K
), the kth word of the sentence is represented by the vector u
k
? R
C
defined by
u
k
i
= P(C
k
= i |W = w).
The vector u
k
is the posterior distribution of latent classes corresponding to the kth word of the sentence,
and thus, sums to one. It is efficiently computed using the message passing algorithm (a.k.a. forward-
backward algorithm for HMM).
Out-of-context representation. In order to obtain word representations that are independent of the
context, we compute the previously introduced in-context representations on a very large corpus, and for
each word type, we average all the in-context representations for all the occurrences of that word type
in the corpus. More formally, given a large set of pairs of tokens and their in-context representations
(w
k
,u
k
) ? N? R
C
, the representation of the word type a is the vector v
a
? R
C
, defined by
v
a
=
1
Z
a
?
k : w
k
=a
u
k
,
where Z
a
is the number of occurrences of the word type a. The vector v
a
is thus the posterior distribution
of semantic classes averaged over all the occurrences of word type a.
Comparing in-context and out-of-context representations. Since in-context and out-of-context
word representations are defined on the same space (the simplex of dimension C) it is possible to com-
pare in-context and out-of-context representations easily. As an example, we have plotted in Figure 2
the out-of-context representation for the words head, president, chief, chairman, director, executive, eye,
face, shoulder, hand, leg, etc. and the in-context representations for the word head in the context of the
two following sentences:
1. The nurse stuck her head in the room to announce that Dr. Reitz was on the phone.
2. A well-known Wall Street figure may join the Cabinet as head of the Treasury Department.
1450
Distance RG65 WS353
Cosine 0.68 0.50
Kullback-Leibler 0.69 0.47
Jensen-Shannon 0.72 0.50
Hellinger 0.73 0.51
Agirre et al. (BoW) 0.81 0.65
Distance SIM. REL.
Cosine 0.68 0.34
Kullback-Leibler 0.64 0.31
Jensen-Shannon 0.69 0.33
Hellinger 0.70 0.34
Agirre et al. (BoW) 0.70 0.62
Table 1: Left: Spearman?s rank correlation coefficient ? between human and distributional similarity, on
the RG65 and WORDSIM353 datasets. Right: Spearman?s rank correlation coefficient ? between human
and distributional similarity on two subsets (similarity v.s. relatedness) of the WORDSIM353 dataset.
The two-dimensional visualization is obtained by using multidimensional scaling (Borg, 2005). First of
all, we observe that the words are clustered in two groups, one containing words belonging to the body
part class, the other containing words belonging to the leader class, and the word head, appears between
those two groups. Second, we observe that the in-context representations are shifted toward the cluster
corresponding to the disambiguated sense of the ambiguous word head.
5 Out-of-context evaluation
In this section, we evaluate out-of-context word representations on a similarity prediction task and deter-
mine what kind of semantic relations are favored by our approach.
5.1 Similarity judgements prediction
In word similarity prediction tasks, pairs of words are presented to human subjects who are asked to
rate the relatedness between those two words. These human similarity scores are then compared to
distributional similarity scores induced by our models, by computing the correlation between them.
Methodology. We use the RG65 dataset, introduced by Rubenstein and Goodenough (1965) and the
WORDSIM353 dataset, collected by Finkelstein et al. (2001). These datasets comprise 65 and 353 word
pairs respectively. Human subjects rated the relatedness of those word pairs. We use the Spearman?s
rank correlation coefficient ? to compare human and distributional score distributions.
Comparison of similarity measures. Since words are represented by posterior distributions over la-
tent semantic classes, we have considered distances (or divergences) that are adapted to probability dis-
tributions to compute the similarity between word representations: the symmetrised Kullback-Leibler
divergence, the Jensen-Shannon divergence, and the Hellinger distance. We use the opposite of these
dissimilarity measures in order to obtain similarity scores. We also included the cosine similarity mea-
sure as a baseline, as it is widely used in the field of distributional semantics.
We report results on both datasets in Table 1. Unsurprisingly, we observe that the dissimilarity mea-
sures giving the best results are the one tailored for probability distribution, namely the Jensen-Shannon
divergence and the Hellinger distance. The Kullback-Leibler divergence is too sensitive to fluctuations
of small probabilities and thus does not perform as well as other similarity measures between probability
distributions. In the following, we will use the Hellinger distance. It should be noted that the results
reported by Agirre et al. (2009) were obtained using a corpus containing 1.6 terawords, making it 1,000
times larger than ours. They also report results for various corpus sizes, and when using a corpus whose
size is comparable to ours, their result on WORDSIM353 drops to 0.55.
Relatedness v.s. similarity. As noted by Agirre et al. (2009), words might be rated as related for
different reasons since different kinds of semantic relations exist between word senses. Some words,
such as telephone and communication might even be rated as related because they belong to the same
semantic field. Thus, they proposed to split the WORDSIM353 dataset into two subsets: the first one
comprising words that are similar, i.e., synonyms, antonyms and hyperonym-hyponym and the second
1451
cohyp hyper mero attri event randn randj randv2
1
0
1
2
3
cohyp hyper mero attri event randn randj randv
Figure 3: Similarity score distributions for various semantic relations on the BLESS dataset, without
using the transition matrix (left) and with using the transition matrix (right) for comparing adjectives and
verbs with nouns.
one comprising words that are related, i.e., meronym-holonym and topically related words. We report
results on these two subsets in Table 1. We observe that our model capture similarity (? = 0.70) much
better than relatedness (? = 0.34). This is not very surprising since our model takes the syntax into
account.
5.2 Semantic relations captured by our word representations
As we saw in the previous section, different semantic relations between words are not equally captured
by our word representations. In this section, we thus investigate which kind of semantic relations are
favored by our approach.
The BLESS dataset. The BLESS dataset (Baroni and Lenci, 2011) comprises 200 concrete concepts
and eight relations. For each pair of concept-relation, a list of related words, referred to as relatum, is
given. Five semantic relations are considered: co-hyponymy, hypernymy, meronymy, attribute and event.
The attribute relation means that the relatum is an adjective expressing an attribute of the concept, while
the event relation means that the relatum is a verb designing an activity or an event in which the concept
is involved. The dataset also contains three random relations (randn, randj ans randv), obtained by the
association of a random relatum, for different POS: noun, adjective and verb.
Methodology. We follow the evaluation proposed by the authors: for each pair of concept-relation, we
keep the score of the most similar relatum associated to that pair of concept-relation. Thus, for each
concept, we have eight scores, one for each relation. We normalize these eight scores (mean: 0, std: 1),
in order to reduce concept-specific effects. We then report the score distributions for each relation as box
plots in Figure 3 (left).
Results. We observe that the co-hyponymy relation is the best captured relation by a large margin.
It is followed by the hypernymy and meronymy relations. The random noun relation is prefered over
the attribute and the event relations. This happens because words with different part-of-speeches tend
to appear in different semantic classes. It is thus impossible to compare words with different parts-of-
speeches and thus to capture relation such as the event or the attribute relation as defined in the BLESS
dataset. It is however possible to make a more principled use of the model to overcome this issue.
Comparing adjectives with nouns and nouns with verbs. In syntactic relations between nouns and
adjectives, the noun is the head word and the adjective is the dependent. Similarly, in syntactic relations
between nouns and verbs, most often the verb is the head and the noun is the dependent. Given a vector
v
a
representing an adjective and a vector v
n
representing a noun, it is thus natural to left multiply them by
the transition matrix of the model to obtain a vector u
a
comparable to nouns and a vector u
n
comparable
to verbs:
u
a
= T
>
v
a
and u
n
= T
>
v
n
.
1452
small house
emphasise need
scholar write book
c
2
c
1
w
2
w
1
c
1
c
2
w
1
w
2
c
2
c
1
c
3
w
2
w
1
w
3
Figure 4: Graphical models used to compute in-context word representations for the compositional tasks.
We report in Figure 3 (right) the new score distributions obtained when adjective and noun representa-
tions are transformed before being compared to nouns and verbs. We observe that, when using these
transformations, the attribute and event relations are better captured than the random relations. This
demonstrates that the transition matrix T captures selectional preferences.
6 Compositional semantics
So far, we have only evaluated how well our representations are able to capture the meaning of words
taken as individual and independent units. However, natural languages are highly compositional, and it
is reasonable to assume that the meaning of a sentence or a phrase can be deduced from the meanings of
its parts and the syntactic relations between them. This assumption is known as the principle of semantic
compositionality.
In this section, we thus evaluate our representations on semantic composition tasks. More precisely, we
determine if using in-context word representations helps to compute the similarity between short phrases
such as adjective-noun, verb-object, compound-noun or subject-verb-object phrases. We use two datasets
of human similarity scores, introduced respectively by Mitchell and Lapata (2010) and Grefenstette and
Sadrzadeh (2011).
6.1 Methodology
We compare different ways to obtain a representation of a short phrase given our model. First, as a
baseline, we represent a phrase by the out-of-context representation of its head word. In that case, there
is no composition at all. Second, following Mitchell and Lapata (2008), we represent a phrase by the
sum of the out-of-context representations of the words forming that phrase. Third, we represent a phrase
by the in-context representation of its head word. Finally, we represent a phrase by the sum of the two
in-context representations of the words forming that phrase. The graphical models used to compute in-
context word representations are represented in Fig 4. The probability distribution p(c
1
) of the head?s
semantic class is set to the uniform distribution (and not to the initial class distribution p
T
(c
k
| c
0
= 0)).
6.2 Datasets
The first dataset we consider was introduced by Mitchell and Lapata (2010), and is composed of pairs of
adjective-noun, compound-noun and verb-object phrases, whose similarities were evaluated by human
subjects on a 1? 7 scale. We compare our results with the one reported by (Mitchell and Lapata, 2010).
The second dataset we consider was introduced by Grefenstette and Sadrzadeh (2011). Each example of
this dataset consists in a triple of subject-verb-object, forming a small transitive sentence, and a landmark
verb. Human subjects were asked to evaluate the similarity between the verb and its landmark in the
context of the small sentence. Following Van de Cruys et al. (2013), we compare the contextualized verb
with the non-contextualized landmark, meaning that the landmark is always represented by its out-of-
context representation. We do so because it is believed to better capture the compositional ability of our
model and it works better in practice. We compare our results with the one reported by Van de Cruys et
al. (2013).
1453
AN NN VN
head (out-of-context) 0.44 0.26 0.41
add (out-of-context) 0.50 0.45 0.42
head (in-context) 0.49 0.42 0.43
add (in-context) 0.51 0.46 0.41
M&L (vector space model) 0.46 0.49 0.38
Humans 0.52 0.49 0.55
SVO
head (out-of-context) 0.25
add (out-of-context) 0.25
head (in-context) 0.41
add (in-context) 0.40
Van de Cruys et al. 0.37
Humans 0.62
Table 2: Spearman?s rank correlation coefficients between human similarity judgements and similarity
computed by our models on the Mitchell and Lapata (2010) dataset (left) and on the Grefenstette and
Sadrzadeh (2011) dataset (right). AN stands for adjective-noun, NN stands for compoundnoun and VN
stands for verb-object.
6.3 Discussion
Before discussing the results, it is interesting to note that our approach provides a way to evaluate the
importance of disambiguation for compositional semantics. Indeed, the in-context representations pro-
posed in this paper are a way to disambiguate their out-of-context equivalents. It was previously noted by
Reddy et al. (2011) that disambiguating the vectorial representations of words improve the performance
on compositional tasks.
Mitchell and Lapata (2010) dataset. We report results on the Mitchell and Lapata (2010) dataset in
Table 2 (left). Overall, in-context representations achieves better performance than out-of-context ones.
For the adjective-noun pairs and the verb-noun pairs, using only the in-context representation of the head
word works almost as well (AN) or even better (VN) than adding the representations of the two words
forming a pair. This means that for those particular tasks, disambiguation plays an important role. On
the other hand, this is not the case for the noun-noun pairs. On that task, most improvement over the
baseline comes from the add operation.
Grefenstette and Sadrzadeh (2011) dataset. We report results in Table 2 (right). First, we observe
that in-context representations clearly outperform out-of-context ones. Second, we note that adding the
subject, object and verb representations does not improve the result over only using the representation of
the verb. These two conclusions are not really surprising since this task is mainly a disambiguation task,
and disambiguation is achieved by using the in-context representations. We also note that our approach
yields better results than those obtained by Van de Cruys et al. (2013), while their method was specifically
designed to model subject-verb-object triples.
7 Conclusion and future work
In this article, we introduced a new approach to distributional semantics, based on a generative model
of sentences. This model is somehow to latent Dirichlet allocation as structured vector space models are
to latent semantic analysis. Indeed, our approach is based on a probabilistic model of sentences, which
takes the syntax into account by using dependency trees. Similarly to LDA, our model can be viewed
as a topic model, the main difference being that the topics are generated using a Markov process on a
syntactic dependency tree instead of using a Dirichlet process.
The approach we propose seems quite competitive with other distributional models of semantics. In
particular, we match or outperform state-of-the-art methods on semantic compositionality tasks. Thanks
to its probabilistic nature, it is very easy to derive word representations for various tasks: the same model
can be used to compute in-context word representations for adjective-noun phrases, subject-verb-object
triples or even full sentences, which is not the case of the tensor based approach proposed by Van de
Cruys et al. (2013).
1454
Currently, the model of sentences does not use the dependency labels, which is the most significant
limitation that we would like to address in future work. We also plan to explore spectral methods (Anand-
kumar et al., 2012) to provide better initialization for learning the parameters of the model. Indeed, we
believe this could speed up learning and yields better results, since the expectation-maximization al-
gorithm is quite sensitive to bad initialization. Finally, the code corresponding to this article will be
available on the first author webpage.
Acknowledgments
Edouard Grave is supported by a grant from INRIA (Associated-team STATWEB). Francis Bach is
partially supported by the European Research Council (SIERRA Project)
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. 2012. Tensor decompositions for learning latent
variable models. arXiv preprint arXiv:1210.7559.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
M. Baroni and A. Lenci. 2011. How we blessed distributional semantic evaluation. In Proceedings of the GEMS
2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1?10. Association for Compu-
tational Linguistics.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun
constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. 2009. The WaCky wide web: a collection of very large
linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209?226.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning
Research.
I. Borg. 2005. Modern multidimensional scaling: Theory and applications. Springer.
E. Bruni, G. Boleda, M. Baroni, and N. K. Tran. 2012. Distributional semantics in technicolor. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages
136?145. Association for Computational Linguistics.
S. Clark and S. Pulman. 2007. Combining symbolic and distributional models of meaning. In AAAI Spring
Symposium: Quantum Interaction, pages 52?55.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Mathematical foundations for a compositional distributional model
of meaning. arXiv preprint arXiv:1003.4394.
J. R. Curran and M. Moens. 2002. Scaling context space. In Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American society for information science.
G. Dinu and M. Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Processing.
K. Erk and S. Pad?o. 2008. A structured vector space model for word meaning in context. In Proceedings of the
2008 Conference on Empirical Methods in Natural Language Processing.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search
in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web.
1455
J. R. Firth. 1957. A synopsis of linguistic theory, 1930-1955.
E. Grave, G. Obozinski, and F. Bach. 2013. Hidden Markov tree models for semantic class induction. In Proceed-
ings of the Seventeenth Conference on Computational Natural Language Learning.
E. Grefenstette and M. Sadrzadeh. 2011. Experimental support for a categorical compositional distributional
model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Pro-
cessing.
E. Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceed-
ings of the 2010 Workshop on GEometrical Models of Natural Language Semantics.
Z. S. Harris. 1954. Distributional structure. Springer.
T. Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on Uncer-
tainty in artificial intelligence.
R. Jenatton, N. Le Roux, A. Bordes, and G. Obozinski. 2012. A latent factor model for highly multi-relational
data. In Advances in Neural Information Processing Systems 25.
T. K Landauer and S. T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological review.
D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-volume 2.
K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behav-
ior Research Methods, Instruments, & Computers.
J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. In Proceedings of the 46th
Annual Meeting of the Association of Computational Linguistics.
J. Mitchell and M. Lapata. 2010. Composition in distributional models of semantics. Cognitive Science.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K?ubler, S. Marinov, and E. Marsi. 2007. Maltparser: A
language-independent system for data-driven dependency parsing. Natural Language Engineering.
S. Pad?o and M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguis-
tics.
S. Reddy, I. P. Klapaftis, D. McCarthy, and S. Manandhar. 2011. Dynamic and static prototype vectors for
semantic composition. In IJCNLP, pages 705?713.
H. Rubenstein and J. B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM,
8(10):627?633.
H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of international
conference on new methods in language processing.
H. Schutze. 1992. Dimensions of meaning. In Supercomputing?92. Proceedings. IEEE.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. 2012. Semantic compositionality through recursive matrix-
vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning.
S. Thater, H. F?urstenau, and M. Pinkal. 2010. Contextualizing semantic representations using syntactically en-
riched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Lin-
guistics.
P. D. Turney. 2006. Similarity of semantic relations. Computational Linguistics.
T. Van de Cruys, T. Poibeau, and A. Korhonen. 2013. A tensor-based factorization model of semantic composi-
tionality. In Proceedings of NAACL-HLT.
1456
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 233?243,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Structured Penalties for Log-linear Language Models
Anil Nelakanti,*? Ce?dric Archambeau,* Julien Mairal,? Francis Bach,? Guillaume Bouchard*
*Xerox Research Centre Europe, Grenoble, France
?INRIA-LEAR Project-Team, Grenoble, France
?INRIA-SIERRA Project-Team, Paris, France
firstname.lastname@xrce.xerox.com firstname.lastname@inria.fr
Abstract
Language models can be formalized as log-
linear regression models where the input fea-
tures represent previously observed contexts
up to a certain length m. The complexity
of existing algorithms to learn the parameters
by maximum likelihood scale linearly in nd,
where n is the length of the training corpus
and d is the number of observed features. We
present a model that grows logarithmically
in d, making it possible to efficiently leverage
longer contexts. We account for the sequen-
tial structure of natural language using tree-
structured penalized objectives to avoid over-
fitting and achieve better generalization.
1 Introduction
Language models are crucial parts of advanced nat-
ural language processing pipelines, such as speech
recognition (Burget et al, 2007), machine trans-
lation (Chang and Collins, 2011), or information
retrieval (Vargas et al, 2012). When a sequence
of symbols is observed, a language model pre-
dicts the probability of occurrence of the next sym-
bol in the sequence. Models based on so-called
back-off smoothing have shown good predictive
power (Goodman, 2001). In particular, Kneser-Ney
(KN) and its variants (Kneser and Ney, 1995) are
still achieving state-of-the-art results for more than a
decade after they were originally proposed. Smooth-
ing methods are in fact clever heuristics that require
tuning parameters in an ad-hoc fashion. Hence,
more principled ways of learning language mod-
els have been proposed based on maximum en-
tropy (Chen and Rosenfeld, 2000) or conditional
random fields (Roark et al, 2004), or by adopting
a Bayesian approach (Wood et al, 2009).
In this paper, we focus on penalized maxi-
mum likelihood estimation in log-linear models.
In contrast to language models based on unstruc-
tured norms such as `2 (quadratic penalties) or
`1 (absolute discounting), we use tree-structured
norms (Zhao et al, 2009; Jenatton et al, 2011).
Structured penalties have been successfully applied
to various NLP tasks, including chunking and named
entity recognition (Martins et al, 2011), but not lan-
guage modelling. Such penalties are particularly
well-suited to this problem as they mimic the nested
nature of word contexts. However, existing optimiz-
ing techniques are not scalable for large contexts m.
In this work, we show that structured tree norms
provide an efficient framework for language mod-
elling. For a special case of these tree norms, we
obtain an memory-efficient learning algorithm for
log-linear language models. Furthermore, we aslo
give the first efficient learning algorithm for struc-
tured `? tree norms with a complexity nearly lin-
ear in the number of training samples. This leads to
a memory-efficient and time-efficient learning algo-
rithm for generalized linear language models.
The paper is organized as follows. The model
and other preliminary material is introduced in Sec-
tion 2. In Section 3, we review unstructured penal-
ties that were proposed earlier. Next, we propose
structured penalties and compare their memory and
time requirements. We summarize the characteris-
tics of the proposed algorithms in Section 5 and ex-
perimentally validate our findings in Section 6.
233
34
6
6
5 7
7
7
(a) Trie-structured vector.
w = [ 3 4 6 6 4 5 7 7 ]>.
3
4
6 [2]
4 5
7 [2]
(b) Tree-structured vector.
w = [ 3 4 6 6 4 5 7 7 ]>.
2.8
3.5
4.8
4.3
2.3 3
5.6
4.9
(c) `T2 -proximal ?`T2
(w, 0.8) =
[ 2.8 3.5 4.8 4.3 2.3 3 5.6 4.9 ]>.
3
4
5.2 [2]
3.2 4.2
5.4 [2]
(d) `T?-proximal ?`T? (w, 0.8) =
[ 3 4 5.2 5.2 3.2 4.2 5.4 5.4 ]>.
Figure 1: Example of uncollapsed (trie) and corresponding collapsed (tree) structured vectors and proximal
operators applied to them. Weight values are written inside the node. Subfigure (a) shows the complete
trie S and Subfigure (b) shows the corresponding collapsed tree T . The number in the brackets shows the
number of nodes collapsed. Subfigure (c) shows vector after proximal projection for `T2 -norm (which cannot
be collapsed), and Subfigure (d) that of `T?-norm proximal projection which can be collapsed.
2 Log-linear language models
Multinomial logistic regression and Poisson regres-
sion are examples of log-linear models (McCullagh
and Nelder, 1989), where the likelihood belongs
to an exponential family and the predictor is lin-
ear. The application of log-linear models to lan-
guage modelling was proposed more than a decade
ago (Della Pietra et al, 1997) and it was shown to
be competitive with state-of-the-art language mod-
elling such as Knesser-Ney smoothing (Chen and
Rosenfeld, 2000).
2.1 Model definition
Let V be a set of words or more generally a set of
symbols, which we call vocabulary. Further, let xy
be a sequence of n+1 symbols of V , where x ? V n
and y ? V . We model the probability that symbol y
succeeds x as
P (y = v|x) =
ew
>
v ?m(x)
?
u?V e
w>u ?m(x)
, (1)
where W = {wv}v?V is the set of parameters, and
?m(x) is the vector of features extracted from x, the
sequence preceding y. We will describe the features
shortly.
Let x1:i denote the subsequence of x starting at
the first position up to the ith position and yi the next
symbol in the sequence. Parameters are estimated by
minimizing the penalized log-loss:
W ? ? argmin
W?K
f(W ) + ??(W ), (2)
where f(W ) := ?
?n
i=1 ln p(yi|x1:i;W ) and K is
a convex set representing the constraints applied on
the parameters. Overfitting is avoided by adjust-
ing the regularization parameter ?, e.g., by cross-
validation.
2.2 Suffix tree encoding
Suffix trees provide an efficient way to store and
manipulate discrete sequences and can be con-
structed in linear time when the vocabulary is
fixed (Giegerich and Kurtz, 1997). Recent examples
include language models based on a variable-length
Markovian assumption (Kennington et al, 2012)
and the sequence memoizer (Wood et al, 2011). The
suffix tree data structure encodes all the unique suf-
fixes observed in a sequence up to a maximum given
length. It exploits the fact that the set of observed
contexts is a small subset of all possible contexts.
When a series of suffixes of increasing lengths are
234
Algorithm 1 W ? := argmin {f(X,Y ;W )+
??(W )} Stochastic optimization algorithm (Hu et
al., 2009)
1 Input: ? regularization parameter , L Lipschitz constant of
?f , ? coefficient of strong-convexity of f + ??, X design
matrix, Y label set
2 Initialize: W = Z = 0, ? = ? = 1, ? = L+ ?
3 repeat until maximum iterations
4 #estimate point for gradient update
W = (1? ?)W + ?Z
5 #use mini-batch {X?, Y?} for update
W = ParamUpdate(X?, Y?, W , ?, ?)
6 #weighted combination of estimates
Z = 1??+?
(
(1? ?)Z + (?? ?)W + ?W
)
7 #update constants
? = L+ ?/?, ? =
?
4?+?2??
2 , ? = (1? ?)?
Procedure: W := ParamUpdate(X?, Y?, W , ?, ?)
1 W ? = W ? 1??f(X?, Y?,W ) #gradient step
2 W = [W ]+ #projection to non-negative orthant
3 W = ??(w, ?) #proximal step
always observed in the same context, the successive
suffixes are collapsed into a single node. The un-
collapsed version of the suffix tree T is called a suf-
fix trie, which we denote S. A suffix trie also has
a tree structure, but it potentially has much larger
number of nodes. An example of a suffix trie S and
the associated suffix tree T are shown in Figures 1(a)
and 1(b) respectively. We use |S| to denote the num-
ber of nodes in the trie S and |T | for the number of
nodes in the tree T .
Suffix tree encoding is particularly helpful in ap-
plications where the resulting hierarchical structures
are thin and tall with numerous non-branching paths.
In the case of text, it has been observed that the num-
ber of nodes in the tree grows slower than that of
the trie with the length of the sequence (Wood et
al., 2011; Kennington et al, 2012). This is a signif-
icant gain in the memory requirements and, as we
will show in Section 4, can also lead to important
computational gains when this structure is exploited.
The feature vector ?m(x) encodes suffixes (or
contexts) of increasing length up to a maximum
length m. Hence, the model defined in (1) is simi-
lar tom-gram language models. Naively, the feature
vector ?m(x) corresponds to one path of length m
starting at the root of the suffix trie S. The entries
in W correspond to weights for each suffix. We thus
have a trie structure S on W (see Figure 1(a)) con-
straining the number of free parameters. In other
words, there is one weight parameter per node in the
trie S and the matrix of parameters W is of size |S|.
In this work, however, we consider models where
the number of parameters is equal to the size of the
suffix tree T , which has much fewer nodes than S.
This is achieved by ensuring that all parameters cor-
responding to suffixes at a node share the same pa-
rameter value (see Figure 1(b)). These parameters
correspond to paths in the suffix trie that do not
branch i.e. sequence of words that always appear to-
gether in the same order.
2.3 Proximal gradient algorithm
The objective function (2) involves a smooth convex
loss f and a possibly non-smooth penalty ?. Sub-
gradient descent methods for non-smooth ? could
be used, but they are unfortunately very slow to con-
verge. Instead, we choose proximal methods (Nes-
terov, 2007), which have fast convergence rates
and can deal with a large number of penalties ?,
see (Bach et al, 2012).
Proximal methods iteratively update the current
estimate by making a generalized gradient update at
each iteration. Formally, they are based on a lin-
earization of the smooth function f around a param-
eter estimate W , adding a quadratic penalty term
to keep the updated estimate in the neighborhood
of W . At iteration t, the update of the parameter W
is given by
W t+1 = argmin
W?K
{
f(W ) + (W ?W )>?f(W )
+?(W ) +
L
2
?W ?W?22
}
, (3)
where L > 0 is an upper-bound on the Lipschitz
constant of the gradient ?f . The matrix W could
either be the current estimate W t or its weighted
combination with the previous estimate for accel-
erated convergence depending on the specific algo-
rithm used (Beck and Teboulle, 2009). Equation (3)
can be rewritten to be solved in two independent
steps: a gradient update from the smooth part fol-
lowed by a projection depending only on the non-
smooth penalty:
W ? = W ?
1
L
?f(W ), (4)
235
W t+1 = argmin
W?K
1
2
?
?W ?W ?
?
?2
2 +
??(W )
L
. (5)
Update (5) is called the proximal operator of W ?
with parameter ?L that we denote ??
(
W ?, ?L
)
. Ef-
ficiently computing the proximal step is crucial to
maintain the fast convergence rate of these methods.
2.4 Stochastic proximal gradient algorithm
In language modelling applications, the number of
training samples n is typically in the range of 105
or larger. Stochastic version of the proximal meth-
ods (Hu et al, 2009) have been known to be well
adapted when n is large. At every update, the
stochastic algorithm estimates the gradient on a
mini-batch, that is, a subset of the samples. The size
of the mini-batches controls the trade-off between
the variance in the estimate of gradient and the time
required for compute it. In our experiments we use
mini-batches of size 400. The training algorithm is
summarized in Algorithm 1. The acceleration is ob-
tained by making the gradient update at a specific
weighted combination of the current and the previ-
ous estimates of the parameters. The weighting is
shown in step 6 of the Algorithm 1.
2.5 Positivity constraints
Without constraining the parameters, the memory
required by a model scales linearly with the vocabu-
lary size |V |. Any symbol in V observed in a given
context is a positive example, while any symbols
in V that does not appear in this context is a neg-
ative example. When adopting a log-linear language
model, the negative examples are associated with a
small negative gradient step in (4), so that the solu-
tion is not sparse accross multiple categories in gen-
eral. By constraining the parameters to be positive
(i.e., the set of feasible solutions K is the positive
orthant), the projection step 2 in Algorithm 1 can be
done with the same complexity, while maintaining
sparse parameters accross multiple categories. More
precisely, the weights for the category k associated
to a given context x, is always zeros if the category k
never occured after context x. A significant gain in
memory (nearly |V |-fold for large context lengths)
was obtained without loss of accuracy in our exper-
iments.
3 Unstructured penalties
Standard choices for the penalty function ?(W ) in-
clude the `1-norm and the squared `2-norm. The
former typically leads to a solution that is sparse
and easily interpretable, while the latter leads to a
non-sparse, generally more stable one. In partic-
ular, the squared `2 and `1 penalties were used in
the context of log-linear language models (Chen and
Rosenfeld, 2000; Goodman, 2004), reporting perfor-
mances competitive with bi-gram and tri-gram inter-
polated Kneser-Ney smoothing.
3.1 Proximal step on the suffix trie
For squared `2 penalties, the proximal step
?`22(w
t, ?2 ) is the element-wise rescaling operation:
w(t+1)i ? w
(t)
i (1 + ?)
?1 (6)
For `1 penalties, the proximal step ?`1(w
t, ?)] is the
soft-thresholding operator:
w(t+1)i ? max(0, w
(t)
i ? ?). (7)
These projections have linear complexity in the
number of features.
3.2 Proximal step on the suffix tree
When feature values are identical, the corresponding
proximal (and gradient) steps are identical. This can
be seen from the proximal steps (7) and (6), which
apply to single weight entries. This property can be
used to group together parameters for which the fea-
ture values are equal. Hence, we can collapse suc-
cessive nodes that always have the same values in a
suffix trie (as in Figure 1(b)), that is to say we can
directly work on the suffix tree. This leads to a prox-
imal step with complexity that scales linearly with
the number of symbols seen in the corpus (Ukkonen,
1995) and logarithmically with context length.
4 Structured penalties
The `1 and squared `2 penalties do not account for
the sequential dependencies in the data, treating suf-
fixes of different lengths equally. This is inappro-
priate considering that longer suffixes are typically
observed less frequently than shorter ones. More-
over, the fact that suffixes might be nested is disre-
garded. Hence, we propose to use the tree-structured
236
Algorithm 2 w := ?`T2 (w, ?) Proximal projection
step for `T2 on grouping G.
1 Input: T suffix tree, w trie-structured vector, ? threshold
2 Initialize: {?i} = 0, {?i} = 1
3 ? = UpwardPass(?, ?, ?, w)
4 w = DownwardPass(?, w)
Procedure: ? := UpwardPass(?, ?, ?, w)
1 for x ? DepthFirstSuffixTraversal(T, PostOrder)
2 ?x = w2x +
?
h?children(x) ?h
3 ?x = [1? ?/
?
?x]+
4 ?x = ?2x?x
Procedure: w := DownwardPass(?, w)
1 for x ? DepthFirstSuffixTraversal(T, PreOrder)
2 wx = ?xwx
3 for h ? children(x)
4 ?h = ?x?h
a DepthFirstSuffixTraversal(T,Order) returns observed suf-
fixes from the suffix tree T by depth-first traversal in the order
prescribed by Order.
b wx is the weights corresponding to the suffix x from the
weight vector w and children(x) returns all the immediate
children to suffix x in the tree.
norms (Zhao et al, 2009; Jenatton et al, 2011),
which are based on the suffix trie or tree, where sub-
trees correspond to contexts of increasing lengths.
As will be shown in the experiments, this prevents
the model to overfit unlike the `1- or squared `2-
norm.
4.1 Definition of tree-structured `Tp norms
Definition 1. Let x be a training sequence. Group
g(w, j) is the subvector of w associated with the
subtree rooted at the node j of the suffix trie S(x).
Definition 2. Let G denote the ordered set of nodes
of the tree T (x) such that for r < s, g(w, r) ?
g(w, s) = ? or g(w, r) ? g(w, s). The tree-
structured `p-norm is defined as follows:
`Tp (w) =
?
j?G
?g(w, j)?p . (8)
We specifically consider the cases p = 2,? for
which efficient optimization algorithms are avail-
able. The `Tp -norms can be viewed as a group
sparsity-inducing norms, where the groups are or-
ganized in a tree. This means that when the weight
associated with a parent in the tree is driven to zero,
the weights associated to all its descendants should
also be driven to zero.
Algorithm 3 w := ?`T?(w, ?) Proximal projection
step for `T? on grouping G.
Input: T suffix tree, w=[v c] tree-structured vector v with
corresponding number of suffixes collapsed at each node in
c, ? threshold
1 for x ? DepthFirstNodeTraversal(T, PostOrder)
2 g(v, x) := pi`T?( g(v, x), cx? )
Procedure: q := pi`?(q, ?)
Input: q = [v c], qi = [vi ci], i = 1, ? ? ? , |q|
Initialize: U = {}, L = {}, I = {1, ? ? ? , |q|}
1 while I 6= ?
2 pick random ? ? I #choose pivot
3 U = {j|vj ? v?} #larger than v?
4 L = {j|vj < v?} #smaller than v?
5 ?S =
?
i?U vi ? ci, ?C =
?
i?U ci
6 if (S + ?S)? (C + ?C)? < ?
7 S := (S + ?S), C := (C + ?C), I := L
8 else I := U\{?}
9 r = S??C , vi := vi ?max(0, vi ? r) #take residuals
a DepthFirstNodeTraversal(T,Order) returns nodes x from the
suffix tree T by depth-first traversal in the order prescribed
by Order.
For structured `Tp -norm, the proximal step
amounts to residuals of recursive projections on the
`q-ball in the order defined by G (Jenatton et al,
2011), where `q-norm is the dual norm of `p-norm1.
In the case `T2 -norm this comes to a series of pro-
jections on the `2-ball. For `T?-norm it is instead
projections on the `1-ball. The order of projections
defined by G is generated by an upward pass of the
suffix trie. At each node through the upward pass,
the subtree below is projected on the dual norm ball
of size ?, the parameter of proximal step. We detail
the projections on the norm ball below.
4.2 Projections on `q-ball for q = 1, 2
Each of the above projections on the dual norm ball
takes one of the following forms depending on the
choice of the norm. Projection of vector w on the
`2-ball is equivalent to thresholding the magnitude
of w by ? units while retaining its direction:
w ? [||w||2 ? ?]+
w
||w||2
. (9)
This can be performed in time linear in size of w,
O(|w|). Projection of a non-negative vectorw on the
`1-ball is more involved and requires thresholding
1`p-norm and `q-norm are dual to each other if 1p +
1
q = 1.
`2-norm is self-dual while the dual of `?-norm is the `1-norm.
237
by a value such that the entries in the resulting vector
add up to ?, otherwise w remains the same:
w ? [w ? ? ]+ s.t. ||w||1 = ? or ? = 0. (10)
? = 0 is the case where w lies inside the `1-ball
of size ? with ||w||1 < ?, leaving w intact. In the
other case, the threshold ? is to be computed such
that after thresholding, the resulting vector has an
`1-norm of ?. The simplest way to achieve this is
to sort by descending order the entries w = sort(w)
and pick the k largest values such that the (k + 1)th
largest entry is smaller than ? :
k?
i=1
wi ? ? = ? and ? > wk+1. (11)
We refer to wk as the pivot and are only interested in
entries larger than the pivot. Given a sorted vector,
it requires looking up to exactly k entries, however,
sorting itself take O(|w| log |w|).
4.3 Proximal step
Naively employing the projection on the `2-ball de-
scribed above leads to an O(d2) algorithm for `T2
proximal step. This could be improved to a linear al-
gorithm by aggregating all necessary scaling factors
while making an upward pass of the trie S and ap-
plying them in a single downward pass as described
in (Jenatton et al, 2011). In Algorithm 2, we detail
this procedure for trie-structured vectors.
The complexity of `T?-norm proximal step de-
pends directly on that of the pivot finding algorithm
used within its `1-projection method. Naively sort-
ing vectors to find the pivot leads to an O(d2 log d)
algorithm. Pivot finding can be improved by ran-
domly choosing candidates for the pivot and the
best known algorithm due to (Bruckner, 1984) has
amortized linear time complexity in the size of the
vector. This leaves us with O(d2) complexity for
`T?-norm proximal step. (Duchi et al, 2008) pro-
poses a method that scales linearly with the num-
ber of non-zero entries in the gradient update (s)
but logarithmically in d. But recursive calls to
`1-projection over subtrees will fail the sparsity
assumption (with s ? d) making proximal step
quadratic. Procedure for ?`T? on trie-structured vec-
tors using randomized pivoting method is described
in Algorithm 3.
We next explain how the number of `1-projections
can be reduced by switching to the tree T instead of
trie S which is possible due to the good properties of
`T?-norm. Then we present a pivot finding method
that is logarithmic in the feature size for our appli-
cation.
4.4 `T?-norm with suffix trees
We consider the case where all parameters are ini-
tialized with the same value for the optimization pro-
cedure, typically with zeros. The condition that the
parameters at any given node continue to share the
same value requires that both the gradient update (4)
and proximal step (5) have this property. We mod-
ify the tree structure to ensure that after gradient up-
dates parameters at a given node continue to share a
single value. Nodes that do not share a value after
gradient update are split into multiple nodes where
each node has a single value. We formally define
this property as follows:
Definition 3. A constant value non-branching path
is a set of nodes P ? P(T,w) of a tree structure T
w.r.t. vector w if P has |P | nodes with |P |?1 edges
between them and each node has at most one child
and all nodes i, j ? P have the same value in vector
w as wi = wj .
The nodes of Figure 1(b) correspond to constant
value non-branching paths when the values for all
parameters at each of the nodes are the same. Next
we show that this tree structure is retained after
proximal steps of `T?-norm.
Proposition 1. Constant value non-branching paths
P(T,w) of T structured vector w are preserved un-
der the proximal projection step ?`T?(w, ?).
Figure 1(d) illustrates this idea showing `T? pro-
jection applied on the collapsed tree. This makes it
memory efficient but the time required for the prox-
imal step remains the same since we must project
each subtree of S on the `1-ball. The sequence of
projections at nodes of S in a non-branching path
can be rewritten into a single projection step using
the following technique bringing the number of pro-
jections from |S| to |T |.
Proposition 2. Successive projection steps for sub-
trees with root in a constant value non-branching
path P = {g1, ? ? ? , g|P |} ? P(T,w) for ?`T?(w, ?)
238
is pig|P | ?? ? ??pig1(w, ?) applied in bottom-up order
defined by G. The composition of projections can be
rewritten into a single projection step with ? scaled
by the number of projections |P | as,
pig|P |(w, ?|P |) ? pig|P | ? ? ? ? ? pig1(w, ?).
The above propositions show that `T?-norm can be
used with the suffix tree with fewer projection steps.
We now propose a method to further improve each
of these projection steps.
4.5 Fast proximal step for `T?-norm
Let k be the cardinality of the set of values larger
than the pivot in a vector to compute the thresh-
old for `1-projection as referred in (11). This value
varies from one application to another, but for lan-
guage applications, our experiments on 100K en-
glish words (APNews dataset) showed that k is gen-
erally small: its value is on average 2.5, and its
maximum is around 10 and 20, depending on the
regularization level. We propose using a max-heap
data structure (Cormen et al, 1990) to fetch the k-
largest values necessary to compute the threshold.
Given the heap of the entries the cost of finding the
pivot is O(k log(d)) if the pivot is the kth largest en-
try and there are d features. This operation is per-
formed d times for `T?-norm as we traverse the tree
bottom-up. The heap itself is built on the fly dur-
ing this upward pass. At each subtree, the heap is
built by merging those of their children in constant
time by using Fibonacci heaps. This leaves us with a
O(dk log(d)) complexity for the proximal step. This
procedure is detailed in Algorithm 4.
5 Summary of the algorithms
Table 1 summarizes the characteristics of the algo-
rithms associated to the different penalties:
1. The unstructured norms `p do not take into
account the varying sparsity level with con-
text length. For p=1, this leads to a sparse
solution and for p=2, we obtain the classical
quadratic penalty. The suffix tree representa-
tion leads to an efficient memory usage. Fur-
thermore, to make the training algorithm time
efficient, the parameters corresponding to con-
texts which always occur in the same larger
Algorithm 4 w := ?`T?(w, ?) Proximal projection
step for `T? on grouping G using heap data structure.
Input: T suffix tree, w=[v c] tree-structured vector v with
corresponding number of suffixes collapsed at each node in
c, ? threshold
InitializeH = {}# empty set of heaps
1 for x ? DepthFirstNodeTraversal(T, PostOrder)
g(v, x) := pi`T?(w, x, cx?,H )
Procedure: q := pi`?(w, x, ?,H )
1 Hx = NewHeap(vx, cx, vx)
2 for j ? children(x) # merge with child heaps
?x = ?x + ?j # update `1-norm
Hx = Merge(Hx,Hj),H = H\Hj
3 H = H ?Hx, S = 0, C = 0, J = {}
4 ifHx(?) < ?, setHx = 0 return
5 for j ? OrderedIterator(Hx) # get max values
if vj >
S+(vj?cj)??
C+cj
S = S + (vj ?cj), C = C + cj , J = J ? {j}
else break
6 r = S??C , ? = 0 # compute threshold
7 for j ? J # apply threshold
? = min(vj , r), ? = ? + (vj ? ?)
Hj(v) = ?
8 Hx(?) = Hj(?)? ? # update `1-norm
a. Heap structure on vector w holds three values (v, c, ?) at
each node. v, c being value and its count, ? is the `1-norm of
the sub-vector below. Tuples are ordered by decreasing value
of v and Hj refers to heap with values in sub-tree rooted at
j. Merge operation merges the heaps passed. OrderedIterator
returns values from the heap in decreasing order of v.
context are grouped. We will illustrate in the
experiments that these penalties do not lead to
good predictive performances.
2. The `T2 -norm nicely groups features by subtrees
which concurs with the sequential structure of
sequences. This leads to a powerful algorithm
in terms of generalization. But it can only be
applied on the uncollapsed tree since there is
no closure property of the constant value non-
branching path for its proximal step making it
less amenable for larger tree depths.
3. The `T?-norm groups features like the `
T
2 -norm
while additionally encouraging numerous fea-
ture groups to share a single value, leading to
a substantial reduction in memory usage. The
generalization properties of this algorithm is as
good as the generalization obtained with the `T2
penalty, if not better. However, it has the con-
stant value non-branching path property, which
239
Penalty good generalization memory efficient time efficient
unstructured `1 and `22 no yes O(|T |) yes O(|T |)
struct.
`T2 yes no O(|S|) no O(|S|)
`T? rand. pivot yes yes O(|T |) no O(|T |2)
`T? heap yes yes O(|T |) yes O(|T | log |T |)
Table 1: Properties of the algorithms proposed in this paper. Generalization properties are as compared by
their performance with increasing context length. Memory efficiency is measured by the number of free
parameters of W in the optimization. Note that the suffix tree is much smaller than the trie (uncollapsed
tree): |T | << |S|. Time complexities reported are that of one proximal projection step.
2 4 6 8 10 12210220
230240250
260
order of language model
perplexity
 
 KN`22`1`T2`T?
(a) Unweighted penalties.
2 4 6 8 10 12210220
230240250
260
order of language model
perplexity
 
 KNw`22w`1w`T2w`T?
(b) Weighted penalties.
2 4 6 8 10 1202
46
8x 105
order of language model#o
fparame
ters
 
 KNw`T2w`T?
(c) Model complexity for structured
penalties.
Figure 2: (a) compares average perplexity (lower is better) of different methods from 2-gram through 12-
gram on four different 100K-20K train-test splits. (b) plot compares the same with appropriate feature
weighting. (c) compares model complexity for weighted structured penalties w`T2 and w`
T
? measure by
then number of parameters.
means that the proximal step can be applied di-
rectly to the suffix tree. There is thus also a
significant gain of performances.
6 Experiments
In this section, we demonstrate empirically the prop-
erties of the algorithms summarized in Table 1. We
consider four distinct subsets of the Associated Press
News (AP-news) text corpus with train-test sizes of
100K-20K for our experiments. The corpus was
preprocessed as described in (Bengio et al, 2003)
by replacing proper nouns, numbers and rare words
with special symbols ??proper noun??, ?#n? and
??unknown?? respectively. Punctuation marks are
retained which are treated like other normal words.
Vocabulary size for each of the training subsets was
around 8,500 words. The model was reset at the start
of each sentence, meaning that a word in any given
sentence does not depend on any word in the previ-
ous sentence. The regularization parameter ? is cho-
sen for each model by cross-validation on a smaller
subset of data. Models are fitted to training sequence
of 30K words for different values of ? and validated
against a sequence of 10K words to choose ?.
We quantitatively evaluate the proposed model
using perplexity, which is computed as follows:
P ({xi, yi},W ) = 10
{
?1
nV
?n
i=1 I(yi?V ) log p(yi|x1:i;W )
}
,
where nV =
?
i I(yi ? V ). Performance is mea-
sured for varying depth of the suffix trie with dif-
ferent penalties. Interpolated Kneser-Ney results
were computed using the openly available SRILM
toolkit (Stolcke, 2002).
Figure 2(a) shows perplexity values averaged over
four data subsets as a function of the language model
order. It can be observed that performance of un-
structured `1 and squared `2 penalties improve until
a relatively low order and then degrade, while `T2
penalty does not show such degradation, indicating
240
2 4 6 8 10 1220
4060
tree depth
time(sec)
 
 rand-pivotrand-pivot-col
(a) Iteration time of random-pivoting on
the collapsed and uncollapsed trees.
1 2 3 4 5x 106
2040
60
train size
time(sec
)
 
 k-best heaprand-pivot-col
(b) Iteration time of random-pivoting and
k-best heap on the collapsed tree.
Figure 3: Comparison of different methods for performing `T? proximal projection. The rand-pivot
is the random pivoting method of (Bruckner, 1984) and rand-pivot-col is the same applied with the
nodes collapsed. The k-best heap is the method described in Algorithm 4.
that taking the tree-structure into account is benefi-
cial. Moreover, the log-linear language model with
`T2 penalty performs similar to interpolated Kneser-
Ney. The `T?-norm outperforms all other models
at order 5, but taking the structure into account
does not prevent a degradation of the performance
at higher orders, unlike `T2 . This means that a single
regularization for all model orders is still inappro-
priate.
To investigate this further, we adjust the penal-
ties by choosing an exponential decrease of weights
varying as ?m for a feature at depth m in the suffix
tree. Parameter ? was tuned on a smaller validation
set. The best performing values for these weighted
models w`22, w`1, w`
T
2 and w`
T
? are 0.5, 0.7, 1.1
and 0.85 respectively. The weighting scheme fur-
ther appropriates the regularization at various levels
to suit the problem?s structure. Perplexity plots for
weighted models are shown in Figure 2(b). While
w`1 improves at larger depths, it fails to compare
to others showing that the problem does not admit
sparse solutions. Weighted `22 improves consider-
ably and performs comparably to the unweighted
tree-structured norms. However, the introduction of
weighted features prevents us from using the suf-
fix tree representation, making these models inef-
ficient in terms of memory. Weighted `T? is cor-
rected for overfitting at larger depths and w`T2 gains
more than others. Optimal values for ? are frac-
tional for all norms except w`T2 -norm showing that
the unweighted model `T2 -norm was over-penalizing
features at larger depths, while that of others were
under-penalizing them. Interestingly, perplexity im-
proves up to about 9-grams with w`T2 penalty for
the data set we considered, indicating that there is
more to gain from longer dependencies in natural
language sentences than what is currently believed.
Figure 2(c) compares model complexity mea-
sured by the number of parameters for weighted
models using structured penalties. The `T2 penalty
is applied on trie-structured vectors, which grows
roughly at a linear rate with increasing model order.
This is similar to Kneser-Ney. However, the number
of parameters for the w`T? penalty grows logarith-
mically with the model order. This is due to the fact
that it operates on the suffix tree-structured vectors
instead of the suffix trie-structured vectors. These
results are valid for, both, weighted and unweighted
penalties.
Next, we compare the average time taken per iter-
ation for different implementations of the `T? prox-
imal step. Figure 3(a) shows this time against in-
creasing depth of the language model order for ran-
dom pivoting method with and without the collaps-
ing of parameters at different constant value non-
branching paths. The trend in this plot resembles
that of the number of parameters in Figure 2(c). This
shows that the complexity of the full proximal step
is sublinear when accounting for the suffix tree data
structure. Figure 3(b) plots time per iteration ran-
dom pivoting and k-best heap against the varying
size of training sequence. The two algorithms are
operating directly on the suffix tree. It can be ob-
served that the heap-based method are superior with
241
increasing size of training data.
7 Conclusion
In this paper, we proposed several log-linear lan-
guage models. We showed that with an efficient
data structure and structurally appropriate convex
regularization schemes, they were able to outper-
form standard Kneser-Ney smoothing. We also de-
veloped a proximal projection algorithm for the tree-
structured `T?-norm suitable for large trees.
Further, we showed that these models can be
trained online, that they accurately learn the m-gram
weights and that they are able to better take advan-
tage of long contexts. The time required to run the
optimization is still a concern. It takes 7583 min-
utes on a standard desktop computer for one pass of
the of the complete AP-news dataset with 13 mil-
lion words which is little more than time reported
for (Mnih and Hinton, 2007). The most time con-
suming part is computing the normalization factor
for the log-loss. A hierarchical model in the flavour
of (Mnih and Hinton, 2008) should lead to signifi-
cant improvements to this end. Currently, the com-
putational bottleneck is due to the normalization fac-
tor in (1) as it appears in every gradient step com-
putation. Significant savings would be obtained by
computing it as described in (Wu and Khundanpur,
2000).
Acknowledgements
The authors would like to thank anonymous review-
ers for their comments. This work was partially
supported by the CIFRE grant 1178/2010 from the
French ANRT.
References
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. 2012.
Optimization with sparsity-inducing penalties. Foun-
dations and Trends in Machine Learning, pages 1?
106.
A. Beck and M. Teboulle. 2009. A fast itera-
tive shrinkage-thresholding algorithm for linear in-
verse problems. SIAM Journal of Imaging Sciences,
2(1):183?202.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137?1155.
P. Bruckner. 1984. An o(n) algorithm for quadratic
knapsack problems. Operations Research Letters,
3:163?166.
L. Burget, P. Matejka, P. Schwarz, O. Glembek, and J.H.
Cernocky. 2007. Analysis of feature extraction and
channel compensation in a GMM speaker recognition
system. IEEE Transactions on Audio, Speech and
Language Processing, 15(7):1979?1986, September.
Y-W. Chang and M. Collins. 2011. Exact decoding
of phrase-based translation models through lagrangian
relaxation. In Proc. Conf. Empirical Methods for Nat-
ural Language Processing, pages 26?37.
S. F. Chen and R. Rosenfeld. 2000. A survey of
smoothing techniques for maximum entropy models.
IEEE Transactions on Speech and Audio Processing,
8(1):37?50.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990.
An Introduction to Algorithms. MIT Press.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380?393.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
2008. Efficient projections onto the `1-ball for learn-
ing in high dimensions. Proc. 25th Int. Conf. Machine
Learning.
R. Giegerich and S. Kurtz. 1997. From ukkonen to Mc-
Creight and weiner: A unifying view of linear-time
suffix tree construction. Algorithmica.
J. Goodman. 2001. A bit of progress in language mod-
elling. Computer Speech and Language, pages 403?
434, October.
J. Goodman. 2004. Exponential priors for maximum en-
tropy models. In Proc. North American Chapter of the
Association of Computational Linguistics.
C. Hu, J.T. Kwok, and W. Pan. 2009. Accelerated gra-
dient methods for stochastic optimization and online
learning. Advances in Neural Information Processing
Systems.
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
2011. Proximal methods for hierarchical sparse cod-
ing. Journal of Machine Learning Research, 12:2297?
2334.
C. R. Kennington, M. Kay, and A. Friedrich. 2012. Sufx
trees as language models. Language Resources and
Evaluation Conference.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. IEEE Int. Conf.
Acoustics, Speech and Signal Processing, volume 1.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Structured sparsity in
structured prediction. In Proc. Conf. Empirical Meth-
ods for Natural Language Processing, pages 1500?
1511.
242
P. McCullagh and J. Nelder. 1989. Generalized linear
models. Chapman and Hall. 2nd edition.
A. Mnih and G. Hinton. 2007. Three new graphical mod-
els for statistical language modelling. Proc. 24th Int.
Conference on Machine Learning.
A. Mnih and G. Hinton. 2008. A scalable hierarchical
distributed language model. Advances in Neural In-
formation Processing Systems.
Y. Nesterov. 2007. Gradient methods for minimizing
composite objective function. CORE Discussion Pa-
per.
B. Roark, M. Saraclar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with con-
ditional random fields and the perceptron algorithm.
Proc. Association for Computation Linguistics.
A. Stolcke. 2002. Srilm- an extensible language mod-
eling toolkit. Proc. Int. Conf. Spoken Language Pro-
cessing, 2:901?904.
E. Ukkonen. 1995. Online construction of suffix trees.
Algorithmica.
S. Vargas, P. Castells, and D. Vallet. 2012. Explicit rel-
evance models in intent-oriented information retrieval
diversification. In Proc. 35th Int. ACM SIGIR Conf.
Research and development in information retrieval,
SIGIR ?12, pages 75?84. ACM.
F. Wood, C. Archambeau, J. Gasthaus, J. Lancelot, and
Y.-W. Teh. 2009. A stochastic memoizer for sequence
data. In Proc. 26th Intl. Conf. on Machine Learning.
F. Wood, J. Gasthaus, C. Archambeau, L. James, and
Y. W. Teh. 2011. The sequence memoizer. In Com-
munications of the ACM, volume 54, pages 91?98.
J. Wu and S. Khundanpur. 2000. Efficient training meth-
ods for maximum entropy language modeling. Proc.
6th Inter. Conf. Spoken Language Technologies, pages
114?117.
P. Zhao, G. Rocha, and B. Yu. 2009. The compos-
ite absolute penalties family for grouped and hierar-
chical variable selection. The Annals of Statistics,
37(6A):3468?3497.
243
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 94?103,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Hidden Markov tree models for semantic class induction
E?douard Grave
Inria - Sierra Project-Team
E?cole Normale Supe?rieure
Paris, France
Edouard.Grave
@inria.fr
Guillaume Obozinski
Universite? Paris-Est, LIGM
E?cole des Ponts - ParisTech
Marne-la-Valle?e, France
Guillaume.Obozinski
@imagine.enpc.fr
Francis Bach
Inria - Sierra Project-Team
E?cole Normale Supe?rieure
Paris, France
Francis.Bach
@ens.fr
Abstract
In this paper, we propose a new method
for semantic class induction. First, we in-
troduce a generative model of sentences,
based on dependency trees and which
takes into account homonymy. Our model
can thus be seen as a generalization of
Brown clustering. Second, we describe
an efficient algorithm to perform inference
and learning in this model. Third, we
apply our proposed method on two large
datasets (108 tokens, 105 words types),
and demonstrate that classes induced by
our algorithm improve performance over
Brown clustering on the task of semi-
supervised supersense tagging and named
entity recognition.
1 Introduction
Most competitive learning methods for compu-
tational linguistics are supervised, and thus re-
quire labeled examples, which are expensive to
obtain. Moreover, those techniques suffer from
data scarcity: many words only appear a small
number of time, or even not at all, in the training
data. It thus helps a lot to first learn word clus-
ters on a large amount of unlabeled data, which
are cheap to obtain, and then to use this clusters
as features for the supervised task. This scheme
has proven to be effective for various tasks such
as named entity recognition (Freitag, 2004; Miller
et al, 2004; Liang, 2005; Faruqui et al, 2010),
syntactic chunking (Turian et al, 2010) or syntac-
tic dependency parsing (Koo et al, 2008; Haffari
et al, 2011; Tratz and Hovy, 2011). It was also
successfully applied for transfer learning of multi-
lingual structure by Ta?ckstro?m et al (2012).
The most commonly used clustering method for
semi-supervised learning is the one proposed by
Brown et al (1992), and known as Brown clus-
tering. While still being one of the most efficient
word representation method (Turian et al, 2010),
Brown clustering has two limitations we want to
address in this work. First, since it is a hard clus-
tering method, homonymy is ignored. Second, it
does not take into account syntactic relations be-
tween words, which seems crucial to induce se-
mantic classes. Our goal is thus to propose a
method for semantic class induction which takes
into account both syntax and homonymy, and then
to study their effects on semantic class learning.
In this paper, we start by introducing a new un-
supervised method for semantic classes induction.
This is achieved by defining a generative model
of sentences with latent variables, which aims at
capturing semantic roles of words. We require our
method to be scalable, in order to learn models on
large datasets containing tens of millions of sen-
tences. More precisely, we make the following
contributions:
? We introduce a generative model of sen-
tences, based on dependency trees, which can
be seen as a generalization of Brown cluster-
ing,
? We describe a fast approximate inference al-
gorithm, based on message passing and on-
line EM for scaling to large datasets. It al-
lowed us to learn models with 512 latent
states on a dataset with hundreds of millions
of tokens in less than two days on a single
core,
? We learn models on two datasets, Wikipedia
articles about musicians and the NYT corpus,
94
and evaluate them on two semi-supervised
tasks, namely supersense tagging and named
entity recognition.
1.1 Related work
Brown clustering (Brown et al, 1992) is the most
commonly used method for word cluster induc-
tion for semi-supervised learning. The goal of this
algorithm is to discover a clustering function C
from words to clusters which maximizes the like-
lihood of the data, assuming the following sequen-
tial model of sentences:
?
k
p(wk | C(wk))p(C(wk) | C(wk?1)).
It can be shown that the best clustering is actually
maximizing the mutual information between adja-
cent clusters. A greedy agglomerative algorithm
was proposed by Brown et al (1992) in order to
find the clustering C, while Clark (2003) proposed
to use the exchange clustering algorithm (Kneser
and Ney, 1993) to maximize the previous likeli-
hood. One of the limitations of this model is the
fact that it neither takes into account homonymy
or syntax.
Another limitation of this method is the com-
plexity of the algorithms proposed to find the best
clustering. This led Uszkoreit and Brants (2008)
to consider a slightly different model, where the
class-to-class transitions are replaced by word-to-
class transitions:
?
k
p(wk | C(wk))p(C(wk) | wk?1).
Thanks to that modification, Uszkoreit and Brants
(2008) designed an efficient variant of the ex-
change algorithm, allowing them to train models
on very large datasets. This model was then ex-
tended to the multilingual setting by Ta?ckstro?m et
al. (2012).
Semantic space models are another family of
methods, besides clustering, that can be used as
features for semi-supervised learning. In those
techniques, words are represented as vectors in
a high-dimensional space. These vectors are ob-
tained by representing the unlabeled corpus as a
word-document co-occurrence matrix in the case
of latent semantic analysis (LSA) (Deerwester et
al., 1990), or word-word co-occurrence matrix in
the case of the hyperspace analog to language
model (HAL) (Lund and Burgess, 1996). Dimen-
sion reduction is then performed, by taking the
singular value decomposition of the co-occurrence
matrix, in order to obtained the so-called seman-
tic space. Hofmann (1999) proposed a variant of
LSA, which corresponds to a generative model of
document. More recently, Dhillon et al (2011)
proposed a method based on canonical correlation
analysis to obtained a such word embeddings.
A last approach to word representation is la-
tent Dirichlet alocation (LDA), proposed by Blei
et al (2003). LDA is a generative model where
each document is viewed as a mixture of topics.
The major difference between LDA and our model
is the fact that LDA treats documents as bags of
words, while we introduce a model of sentences,
taking into account the syntax. Griffiths et al
(2005) defined a composite model, using LDA for
topic modeling and an HMM for syntax model-
ing. This model, HMM-LDA, was used by Li
and McCallum (2005) for semi-supervised learn-
ing and applied to part-of-speech tagging and Chi-
nese word segmentation. Se?aghdha (2010) pro-
posed to use topic models, such as LDA, to per-
form selectional preference induction.
Finally, Boyd-Graber and Blei (2009) proposed
a variant of LDA, using parse trees to include the
syntax. Given that we aim for our classes to cap-
ture as much of the word semantics reflected by
the syntax, such as the semantic roles of words,
we believe that it is not necessarily useful or even
desirable that the latent variables should be deter-
mined, even in part, by topic parameters that are
sharing information at the document level. More-
over, our model being significantly simpler, we
were able to design fast and efficient algorithms,
making it possible to use our model on much
larger datasets, and with many more latent classes.
2 Model
In this section, we introduce our probabilistic gen-
erative model of sentences. We start by setting
up some notations. A sentence is represented
by a K-tuple w = (w1, ..., wK) where each
wk ? {1, ..., V } is an integer representing a word
and V is the size of the vocabulary. Our goal will
be to infer a K-tuple c = (c1, ..., cK) of seman-
tic classes, where each ck ? {1, ..., C} is an in-
teger representing a semantic class, corresponding
to the word wk.
The generation of a sentence can be decom-
posed in two steps: first, we generate the seman-
tic classes according to a Markov process, and
95
Opposition political parties have harshly criticized the pact
c0 c1 c2 c3 c4 c5 c6 c7 c8
w1 w2 w3 w4 w5 w6 w7 w8
Figure 1: Example of a dependency tree and its corresponding graphical model.
then, given each class ck, we generate the corre-
sponding word wk independently of other words.
The Markov process used to generate the seman-
tic classes will take into account selectional pref-
erence. Since we want to model homonymy, each
word can be generated by multiple classes.
We now describe the Markov process we pro-
pose to generate the semantic classes. We assume
that we are given a directed tree defined by the
function pi : {1, ...,K} 7? {0, ...,K}, where pi(k)
represents the unique parent of the node k and 0
is the root of the tree. Each node, except the root,
corresponds to a word of the sentence. First, we
generate the semantic class corresponding to the
root of the tree and then generate recursively the
class for the other nodes. The classes are condi-
tionally independent given the classes of their par-
ents. Using the language of probabilistic graphical
models, this means that the distribution of the se-
mantic classes factorizes in the tree defined by pi
(See Fig. 1 for an example). We obtain the fol-
lowing distribution on pairs (w, c) of words and
semantic classes:
p(w, c) =
K?
k=1
p(ck | cpi(k))p(wk | ck),
with c0 being equal to a special symbol denoting
the root of the tree.
In order to fully define our model, we now
need to specify the observation probability distri-
bution p(wk | ck) of a word given the correspond-
ing class and the transition probability distribution
p(ck | cpi(k)) of a class given the class of the par-
ent. Both these distributions will be categorical
(and thus multinomial with one trial). The cor-
responding parameters will be represented by the
stochastic matrices O and T (i.e. matrices with
non-negative elements and unit-sum columns):
p(wk = i | ck = j) = Oij ,
p(ck = i | cpi(k) = j) = Tij .
Finally, we introduce the trees that we consider to
define the distribution on semantic classes. (We
recall that the trees are assumed given, and not a
part of the model.)
2.1 Markov chain model
The simplest structure we consider on the seman-
tic classes is a Markov chain. In this special case,
our model reduces to a hidden Markov model.
Each semantic class only depends on the class of
the previous word in the sentence, thus failing to
capture selectional preference of semantic class.
But because of its simplicity, it may be more ro-
bust, and does not rely on external tools. It can be
seen as a generalization of the Brown clustering
algorithm (Brown et al, 1992) taking into account
homonymy.
2.2 Dependency tree model
The second kind of structure we consider to model
interactions between semantic classes is a syntac-
tic dependency tree corresponding to the sentence.
A dependency tree is a labeled tree in which nodes
correspond to the words of a sentence, and edges
represent the grammatical relations between those
words, such as nominal subject, direct object or
determiner. We use the Stanford typed dependen-
cies basic representations, which always form a
tree (De Marneffe and Manning, 2008).
96
We believe that a dependency tree is a better
structure than a Markov chain to learn semantic
classes, with no additional cost for inference and
learning compared to a chain. First, syntactic de-
pendencies can capture long distance interactions
between words. See Fig. 1 and the dependency
between parties and criticized for an ex-
ample. Second, the syntax is important to model
selectional preference. Third, we believe that syn-
tactic trees could help much for languages which
do not have a strict word order, such as Czech,
Finnish, or Russian. One drawback of this model
is that all the children of a particular node share
the same transition probability distribution. While
this is not a big issue for nouns, it is a bigger con-
cern for verbs: subject and object should not share
the same transition probability distribution.
A potential solution would be to introduce a dif-
ferent transition probability distribution for each
type of dependency. This possibility will be ex-
plored in future work.
2.3 Brown clustering on dependency trees
As for Brown clustering, we can assume that
words are generated by a single class. In that case,
our model reduces to finding a deterministic clus-
tering function C which maximizes the following
likelihood:
?
k
p(wk | C(wk))p(C(wk) | C(wpi(k))).
In that case, we can use the algorithm proposed
by Brown et al (1992) to greedily maximize the
likelihood of the data. This model can be seen as
a generalization of Brown clustering taking into
account the syntactic relations between words.
3 Inference and learning
In this section, we present the approach used to
perform learning and inference in our model. Our
goal here is to have efficient algorithms, in order
to apply our model to large datasets (108 tokens,
105 words types). The parameters T and O of the
model will be estimated with the maximum likeli-
hood estimator:
T?, O? = argmax
T,O
N?
n=1
p(w(n) | T,O),
where (w(n))n?{1,...,N} represents our training set
of N sentences.
First, we present an online variant of the well-
known expectation-maximization (EM) algorithm,
proposed by Cappe? and Moulines (2009), allowing
our method to be scalable in term of numbers of
examples. Then, we present an approximate mes-
sage passing algorithm which has a linear com-
plexity in the number of classes, instead of the
quadratic complexity of the exact inference algo-
rithm. Finally, we describe a state-splitting strat-
egy to speed up the learning.
3.1 Online EM
In the batch EM algorithm, the E-step consists in
computing the expected sufficient statistics ? and
? of the model, sometimes referred as pseudo-
counts, corresponding respectively to T and O:
?ij =
N?
n=1
Kn?
k=1
E
[
?(c(n)k = i, c
(n)
pi(k) = j)
]
,
?ij =
N?
n=1
Kn?
k=1
E
[
?(w(n)k = i, c
(n)
k = j)
]
.
On large datasets, N which is the number of sen-
tences can be very large, and so, EM is inefficient
because it requires that inference is performed on
the entire dataset at each iteration. We therefore
consider the online variant proposed by Cappe?
and Moulines (2009): instead of recomputing the
pseudocounts on the whole dataset at each itera-
tion t, those pseudocounts are updated using only
a small subset Bt of the data, to get
? (t)ij = (1? ?t)?
(t?1)
ij +
?t
?
n?Bt
Kn?
k=1
E
[
?(c(n)k = i, c
(n)
pi(k) = j)
]
,
and
?(t)ij = (1? ?t)?
(t?1)
ij +
?t
?
n?Bt
Kn?
k=1
E
[
?(w(n)k = i, c
(n)
k = j)
]
,
where the scalars ?t are defined by ?t = 1/(a +
t)? with 0.5 < ? ? 1. In the experiments,
we used a = 4. We chose ? in the set
{0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.
3.2 Approximate inference
Inference is performed on trees using the sum-
product message passing algorithm, a.k.a. belief
97
0 2000 4000 6000 8000 10000Iteration5.95
5.905.85
5.80
Normali
zed log-
likelihoo
d
k = 128k = 64k = 32k = 16 0 2000 4000 6000 8000 10000Iteration5.95
5.905.85
5.80
Normali
zed log-
likelihoo
d
epsilon = 0.0epsilon = 0.001epsilon = 0.01epsilon = 0.1 0 100 200 300 400 500Iteration010
203040
506070
80
Support
 size epsilon = 0.0001epsilon = 0.001epsilon = 0.01epsilon = 0.1
Figure 2: Comparison of the two projection methods for approximating vectors, for a model with 128
latent classes. The first two plots are the log-likelihood on a held-out set as a function of the iterates of
online EM. Green curves (k = 128 and ? = 0) correspond to learning without approximation.
propagation, which extends the classical ??? re-
cursions used for chains, see e.g. Wainwright and
Jordan (2008). We denote by N (k) the set con-
taining the children and the father of node k. In
the exact message-passing algorithm, the message
?k?pi(k) from node k to node pi(k) takes the form:
?k?pi(k) = T>u,
where u is the vector obtained by taking the ele-
mentwise product of all the messages received by
node k except the one from node pi(k), i.e.,
ui =
?
k??N (k)\{pi(k)}
?k??k(i).
Similarly, the pseudocounts can be written as
E
[
?(c(n)k = i, c
(n)
pi(k) = j)
]
? uiTijvj ,
where v is the vector obtained by taking the ele-
mentwise product of all the messages received by
node pi(k), except the one from node k, i.e.,
vj =
?
k??N (pi(k))\{k}
?k??pi(k)(j).
Both these operations thus have quadratic com-
plexity in the number of semantic classes. In or-
der to reduce the complexity of those operations,
we propose to start by projecting the vectors u
and v on a set of sparse vectors, and then, per-
form the operations with the sparse approximate
vectors. We consider two kinds of projections:
? k-best projection, where the approximate
vector is obtained by keeping the k largest
coefficients,
? ?-best projection, where the approximate
vector is obtained by keeping the smallest set
of larger coefficients such that their sum is
greater than (1? ?) times the `1-norm of the
original vector.
This method is similar to the one proposed by Pal
et al (2006). The advantage of the k-best projec-
tion is that we control the complexity of the op-
erations, but not the error, while the advantage of
the ?-best projection is that we control the error
but not the complexity. As shown in Fig. 2, good
choices for ? and k are respectively 0.01 and 16.
We use these values in the experiments. We also
note, on the right plot of Fig. 2, that during the
first iterations of EM, the sparse vectors obtained
with the ?-best projection have a large number of
non-zero elements. Thus, this projection is not
adequate to directly learn large latent class mod-
els. This issue is addressed in the next section,
where we present a state splitting strategy in or-
der to learn models with a large number of latent
classes.
3.3 State splitting
A common strategy to speed up the learning of
large latent state space models, such as ours, is
to start with a small number of latent states, and
split them during learning (Petrov, 2009). As far
as we know, there are still no good heuristics to
choose which states to split, or how to initialize the
parameters corresponding to the new states. We
thus apply the simple, yet effective method, con-
sisting in splitting all states into two and in break-
ing the symmetry by adding a bit of randomness
to the emission probabilities of the new states. As
noted by Petrov (2009), state splitting could also
improve the quality of learnt models.
3.4 Initialization
Because the negative log-likelihood function is not
convex, initialization can greatly change the qual-
ity of the final model. Initialization for online EM
is done by setting the initial pseudocounts, and
then performing an M-step. We have considered
98
the following strategies to initialize our model:
? random initialization: the initial pseudo-
counts ?ij and ?ij are sampled from a uni-
form distribution on [0, 1],
? Brown initialization: the model is initial-
ized using the (normalized) pseudocounts ob-
tained by the Brown clustering algorithm.
Because a parameter equal to zero remains
equal to zero when using the EM algorithm,
we replace null pseudocounts by a small
smoothing value, e.g., for observation i, we
use 10?5 ?maxj ?ij ,
4 Experiments
In this section, we present the datasets used for the
experiments, and the two semi-supervised tasks
on which we evaluate our models: named entity
recognition and supersense tagging.
4.1 Datasets
We considered two datasets: the first one, which
we refer to as the music dataset, corresponds to
all the Wikipedia articles refering to a musical
artist. They were extracted using the Freebase
database1. This dataset comprises 2.22 millions
sentences and 56 millions tokens. We choose this
dataset because it corresponds to a restricted do-
main.
The second dataset are the articles of the NYT
corpus (Sandhaus, 2008) corresponding to the pe-
riod 1987-1997 and labeled as news. This dataset
comprises 14.7 millions sentences and 310 mil-
lions tokens.
We parsed both datasets using the Stanford
parser, and converted parse trees to dependency
trees (De Marneffe et al, 2006). We decided to
discard sentences longer than 50 tokens, for pars-
ing time reasons, and then lemmatized tokens us-
ing Wordnet. Each word of our vocabulary is then
a pair of lemma and its associated part-of-speech.
This means that the noun attack and the verb at-
tack are two different words. Finally, we intro-
duced a special token, -*-, for infrequent (lemma,
part-of-speech) pairs, in order to perform smooth-
ing. For the music dataset, we kept the 25 000
most frequent words, while for the NYT corpus,
we kept the 100 000 most frequent words. For the
music dataset we set the number of latent states to
256, while we set it to 512 for the NYT corpus.
1www.freebase.com
4.2 Qualitative results
Before moving on to the quantitative evaluation of
our model, we discuss qualitatively the induced se-
mantic classes. Examples of semantic classes are
presented in Tables 1, 2 and 3. Tree models with
random initialization were used to obtain those se-
mantic classes. First we observe that most classes
can be easily given natural semantic interpretation.
For example class 196 of Table 1 contains musical
instruments, while class 116 contains musical gen-
res.
Table 2 presents groups of classes that contain a
given homonymous word; it seems that the differ-
ent classes capture rather well the different senses
of each word. For example, the word head belongs
to the class 116, which contains body parts and to
the class 127, which contains words referring to
leaders.
4.3 Semi-supervised learning
We propose to evaluate and compare the different
models in the following semi-supervised learning
setting: we start by learning a model on the NYT
corpus in an unsupervised way, and then use it to
define features for a supervised classifier. We now
introduce the tasks we considered.
4.3.1 Named entity recognition
The first supervised task on which we evaluate the
different models, is named entity recognition. We
cast it as a sequence tagging problem, and thus, we
use a linear conditional random field (CRF) (Laf-
ferty et al, 2001) as our supervised classifier. For
each sentence, we apply the Viterbi algorithm in
order to obtain the most probable sequence of se-
mantic classes, and use this as features for the
CRF. The only other feature we use is a binary
feature indicating if the word is capitalized or not.
Results of experiments performed on the MUC7
dataset are reported in table 4. The baseline for
this task is assigning named entity classes to word
sequences that occur in the training data.
4.3.2 Supersense tagging
Supersense tagging consists in identifying, for
each word of a sentence, its corresponding su-
persense, a.k.a. lexicographer class, as defined by
Wordnet (Ciaramita and Altun, 2006). Because
each Wordnet synset belongs to one lexicogra-
pher class, supersense tagging can be seen as a
coarse disambiguation task for nouns and verbs.
We decided to evaluate our models on this task to
99
# 54 radio BBC television station tv stations channel 1 MTV program network fm music
# 52 chart billboard uk top top singles 100 Hot album country 40 10 R&B 200 US song u.s.
# 78 bach mozart liszt beethoven wagner chopin brahms stravinsky haydn debussy tchaikovsky
# 69 sound style instrument elements influence genre theme form lyric audience direction
#215 tour show concert performance appearance gig date tours event debut session set night party
#116 rock pop jazz classical folk punk metal roll hip country traditional -*- blues dance
#123 win receive sell gain earn award achieve garner give enjoy have get attract bring include
#238 reach peak hit chart go debut make top platinum fail enter gold become with certify
#203 piano concerto -*- for violin symphony in works sonata string of quartet orchestra no.
#196 guitar bass vocal drum keyboard piano saxophone percussion violin player trumpet organ
#243 leave join go move form return sign tour begin decide continue start attend meet disband
#149 school university college hall conservatory academy center church institute cathedral
Table 1: Selected semantic classes corresponding to the music dataset. Like LDA, our model is a proba-
bilistic model which generates words from latent classes. Unlike LDA though, rather than treating words
as exchangeable, it accounts for syntax and semantic relations between words. As a consequence, instead
of grouping words with same topic but various semantic roles or grammatical functions, our model tends
to group words that tend to be syntactically and semantically equivalent.
#116 head hand hands foot face shoulder way knee eyes back body finger car arms arm
#127 president member director chairman executive head editor professor manager secretary
#360 company corporation group industry fund bank association institute trust system
#480 street avenue side bank square precinct coast broadway district strip bridge station
#87 pay base sell use available buy depend make provide receive get lose spend charge offer
#316 charge arrest convict speak tell found accuse release die indict ask responsible suspend
#263 system computer machine technology plant product program equipment line network
#387 plan agreement contract effort program proposal deal offer bill bid order campaign request
#91 have be win score play lead hit make run -*- lose finish pitch start miss come go shoot take
#198 kill shoot die wound injure found arrest fire report take dead attack beat leave strike carry
Table 2: Semantic classes containing homonymous words. Different classes capture different senses of
each word.
demonstrate the effect of homonymy. We cast su-
persense tagging as a classification problem and
use posterior distribution of semantic classes as
features for a support vector machine with the
Hellinger kernel, defined by
K(p,q) =
C?
c=1
?pcqc,
where p and q are posterior distributions. We train
and test the SVM classifier on the section A, B and
C of the Brown corpus, tagged with Wordnet su-
persenses (SemCor). All the considered methods
predict among the possible supersenses according
to Wordnet, or among all the supersenses if the
word does not appear in Wordnet. We report re-
sults in Table 5. The baseline predicts the most
common supersense of the training set.
4.4 Discussion of results
First, we observe that hidden Markov models im-
prove performances over Brown clustering, on
both chains and trees. This seems to indicate
that taking into account homonymy leads to richer
models which is beneficial for both tasks. We also
note that Brown clustering on dependency trees al-
ways outperforms Brown clustering on chains for
the two tasks we consider, confirming that syntac-
tic dependencies are a better structure to induce
semantic classes than a linear chain.
Hidden Markov tree models also outperform
hidden Markov chain models, except for super-
sense tagging on verbs. We believe that this drop
in performance on verbs can be explained because
in English the word order (Subject-Verb-Object)
is strict, and thus, the chain model is able to dif-
100
#484 rise fell be close offer drop gain trade price jump slip end decline unchanged sell total lose
#352 it have would But be not nt will get may too make So see might can always still probably
#115 coach manager bill Joe george don pat Jim bob Lou al general mike Dan tom owner ray
#131 San St. santa Notre s Francisco calif. green tampa Diego louis class AP bay &aaa Fla. Jose
#350 strong short score good better hit second leave fast close impressive easy high quick enough
#274 A Another an new second single free -*- special fair national strong long major political big
#47 gogh rushdie pan guardia vega freud Prensa miserable picasso jesus Armani Monde Niro
#489 health public medical right care human civil community private social research housing
#238 building house home store apartment area space restaurant site neighborhood town park
#38 more very too as so much less enough But seem even because if particularly relatively pretty
Table 3: Randomly selected semantic classes corresponding to the news dataset.
F1 score
Baseline 71.66
Brown clustering 82.57
tree Brown clustering 82.93
chain HMM, random init 84.66
chain HMM, Brown init 84.47
tree HMM, random init 84.07
tree HMM, Brown init 85.49
Table 4: Results of semi-supervised named entity
recognition.
ferentiate between subject and object, while the
tree model treats subject and object in the same
way (both are children of the verb). Moreover, in
the tree model, verbs have a lot of children, such
as adverbial clauses and auxiliary verbs, which
share their transition probability distribution with
the subject and the object. These two effects make
the disambiguation of verbs more noisy for trees
than for chains. Another possible explanation of
this drop of performance is that it is due to errors
made by the syntactic parser.
4.5 On optimization parameters
We briefly discuss the different choices that can
influence learning efficiency in the proposed mod-
els. In practice, we have not observed noticeable
differences between ?-best projection and k-best
projection for the approximate inference, and we
thus advise to use the latter as its complexity is
controled. By contrast, as illustrated by results in
tables 4 and 5, initialization can greatly change the
performance in semi-supervised learning, in par-
ticular for tree models. We thus advise to initialize
with Brown clusters. Finally, as noted by Liang
and Klein (2009), the step size of online EM also
nouns verbs
Baseline 61.9 (0.2) 43.1 (0.2)
Brown clustering 73.9 (0.1) 63.7 (0.2)
tree Brown clustering 75.0 (0.2) 65.2 (0.2)
HMM (random) 76.1 (0.1) 63.0 (0.2)
HMM (Brown) 76.8 (0.1) 66.6 (0.3)
tree HMM (random) 76.7 (0.1) 61.5 (0.2)
tree HMM (Brown) 77.9 (0.1) 66.0 (0.2)
Table 5: Results of semi-supervised supersense
tagging: prediction accuracies with confidence in-
tervals, obtained on 50 random splits of the data.
has a significant impact on performance.
5 Conclusion
In this paper, we considered an arguably natural
generative model of sentences for semantic class
induction. It can be seen as a generalization of
Brown clustering, taking into account homonymy
and syntax, and thus allowed us to study their im-
pact on semantic class induction. We developed an
efficient algorithm to perform inference and learn-
ing, which makes it possible to learn in this model
on large datasets, such as the New York Times
corpus. We showed that this model induces rel-
evant semantic classes and that it improves perfor-
mance over Brown clustering on semi-supervised
named entity recognition and supersense tagging.
We plan to explore in future work better ways to
model verbs, and in particular how to take into ac-
count the type of dependencies between words.
Acknowledgments
Francis Bach is supported in part by the European
Research Council (SIERRA ERC-239993).
101
References
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learn-
ing Research.
J. L. Boyd-Graber and D. Blei. 2009. Syntactic topic
models. In Advances in Neural Information Pro-
cessing Systems 21.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-
gram models of natural language. Computational
linguistics.
O. Cappe? and E. Moulines. 2009. On-line
expectation?maximization algorithm for latent data
models. Journal of the Royal Statistical Society: Se-
ries B (Statistical Methodology).
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings
of the 2006 Conference on Empirical Methods in
Natural Language Processing.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth conference of Eu-
ropean chapter of the Association for Computational
Linguistics.
M. C. De Marneffe and C. D. Manning. 2008. The
Stanford typed dependencies representation. In Col-
ing 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
M. C. De Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of
LREC.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American society
for information science.
P. S. Dhillon, D. Foster, and L. Ungar. 2011. Multi-
view learning of word embeddings via CCA. Ad-
vances in Neural Information Processing Systems.
M. Faruqui, S. Pado?, and M. Sprachverarbeitung.
2010. Training and evaluating a German named en-
tity recognizer with semantic generalization. Se-
mantic Approaches in Natural Language Process-
ing.
D. Freitag. 2004. Trained named entity recognition
using distributional clusters. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. B.
Tenenbaum. 2005. Integrating topics and syn-
tax. Advances in Neural Information Processing
Systems.
G. Haffari, M. Razavi, and A. Sarkar. 2011. An en-
semble model that combines syntactic and semantic
clustering for discriminative dependency parsing. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics.
T. Hofmann. 1999. Probabilistic latent semantic anal-
ysis. In Proceedings of the Fifteenth conference on
Uncertainty in artificial intelligence.
R. Kneser and H. Ney. 1993. Improved clustering
techniques for class-based statistical language mod-
elling. In Third European Conference on Speech
Communication and Technology.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings
of the 18th International Conference on Machine
Learning.
W. Li and A. McCallum. 2005. Semi-supervised se-
quence modeling with syntactic topic models. In
Proceedings of the National Conference on Artificial
Intelligence.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute
of Technology.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative train-
ing. In Proceedings of HLT-NAACL.
C. Pal, C. Sutton, and A. McCallum. 2006.
Sparse forward-backward using minimum diver-
gence beams for fast training of conditional random
fields. In ICASSP 2006 Proceedings.
S. Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California
at Bekeley.
E. Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium, Philadelphia.
D. O. Se?aghdha. 2010. Latent variable models of se-
lectional preference. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics.
102
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
S. Tratz and E. Hovy. 2011. A fast, accurate, non-
projective, semantically-enriched parser. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics.
J. Uszkoreit and T. Brants. 2008. Distributed word
clustering for large scale class-based language mod-
eling in machine translation. Proceedings of ACL-
08: HLT.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
models, exponential families, and variational infer-
ence. Foundations and Trends R? in Machine Learn-
ing.
103
