Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 451?455,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing Partial Textual Entailment
Omer Levy? Torsten Zesch? Ido Dagan? Iryna Gurevych?
? Natural Language Processing Lab ? Ubiquitous Knowledge Processing Lab
Computer Science Department Computer Science Department
Bar-Ilan University Technische Universita?t Darmstadt
Abstract
Textual entailment is an asymmetric rela-
tion between two text fragments that de-
scribes whether one fragment can be in-
ferred from the other. It thus cannot cap-
ture the notion that the target fragment
is ?almost entailed? by the given text.
The recently suggested idea of partial tex-
tual entailment may remedy this problem.
We investigate partial entailment under the
faceted entailment model and the possibil-
ity of adapting existing textual entailment
methods to this setting. Indeed, our results
show that these methods are useful for rec-
ognizing partial entailment. We also pro-
vide a preliminary assessment of how par-
tial entailment may be used for recogniz-
ing (complete) textual entailment.
1 Introduction
Approaches for applied semantic inference over
texts gained growing attention in recent years,
largely triggered by the textual entailment frame-
work (Dagan et al, 2009). Textual entailment is
a generic paradigm for semantic inference, where
the objective is to recognize whether a textual hy-
pothesis (labeled H) can be inferred from another
given text (labeled T ). The definition of textual
entailment is in some sense strict, in that it requires
that H?s meaning be implied by T in its entirety.
This means that from an entailment perspective, a
text that contains the main ideas of a hypothesis,
but lacks a minor detail, is indiscernible from an
entirely unrelated text. For example, if T is ?mus-
cles move bones?, and H ?the main job of muscles
is to move bones?, then T does not entail H , and
we are left with no sense of how close (T,H) were
to entailment.
In the related problem of semantic text similar-
ity, gradual measures are already in use. The se-
mantic text similarity challenge in SemEval 2012
(Agirre et al, 2012) explicitly defined different
levels of similarity from 5 (semantic equivalence)
to 0 (no relation). For instance, 4 was defined
as ?the two sentences are mostly equivalent, but
some unimportant details differ?, and 3 meant that
?the two sentences are roughly equivalent, but
some important information differs?. Though this
modeling does indeed provide finer-grained no-
tions of similarity, it is not appropriate for seman-
tic inference for two reasons. First, the term ?im-
portant information? is vague; what makes one de-
tail more important than another? Secondly, simi-
larity is not sufficiently well-defined for sound se-
mantic inference; for example, ?snowdrops bloom
in summer? and ?snowdrops bloom in winter?
may be similar, but have contradictory meanings.
All in all, these measures of similarity do not quite
capture the gradual relation needed for semantic
inference.
An appealing approach to dealing with the
rigidity of textual entailment, while preserving the
more precise nature of the entailment definition, is
by breaking down the hypothesis into components,
and attempting to recognize whether each one is
individually entailed by T . It is called partial tex-
tual entailment, because we are only interested in
recognizing whether a single element of the hy-
pothesis is entailed. To differentiate the two tasks,
we will refer to the original textual entailment task
as complete textual entailment.
Partial textual entailment was first introduced
by Nielsen et al (2009), who presented a ma-
chine learning approach and showed significant
improvement over baseline methods. Recently, a
public benchmark has become available through
the Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment (RTE) Challenge in
SemEval 2013 (Dzikovska et al, 2013), on which
we focus in this paper.
Our goal in this paper is to investigate the idea
of partial textual entailment, and assess whether
451
existing complete textual entailment methods can
be used to recognize it. We assume the facet
model presented in SemEval 2013, and adapt ex-
isting technologies to the task of recognizing par-
tial entailment (Section 3). Our work further ex-
pands upon (Nielsen et al, 2009) by evaluating
these adapted methods on the new RTE-8 bench-
mark (Section 4). Partial entailment may also fa-
cilitate an alternative divide and conquer approach
to complete textual entailment. We provide an ini-
tial investigation of this approach (Section 5).
2 Task Definition
In order to tackle partial entailment, we need to
find a way to decompose a hypothesis. Nielsen et
al. (2009) defined a model of facets, where each
such facet is a pair of words in the hypothesis
and the direct semantic relation connecting those
two words. We assume the simplified model that
was used in RTE-8, where the relation between the
words is not explicitly stated. Instead, it remains
unstated, but its interpreted meaning should corre-
spond to the manner in which the words are related
in the hypothesis. For example, in the sentence
?the main job of muscles is to move bones?, the
pair (muscles, move) represents a facet. While it is
not explicitly stated, reading the original sentence
indicates that muscles is the agent of move.
Formally, the task of recognizing faceted entail-
ment is a binary classification task. Given a text T ,
a hypothesis H , and a facet within the hypothesis
(w1, w2), determine whether the facet is either ex-
pressed or unaddressed by the text. Nielsen et al
included additional classes such as contradicting,
but in the scope of this paper we will only tend to
the binary case, as was done in RTE-8.
Consider the following example:
T: Muscles generate movement in the body.
H: The main job of muscles is to move bones.
The facet (muscles, move) refers to the agent role
in H , and is expressed by T . However, the facet
(move, bones), which refers to a theme or direct
object relation in H , is unaddressed by T .
3 Recognizing Faceted Entailment
Our goal is to investigate whether existing entail-
ment recognition approaches can be adapted to
recognize faceted entailment. Hence, we speci-
fied relatively simple decision mechanisms over a
set of entailment detection modules. Given a text
and a facet, each module reports whether it rec-
ognizes entailment, and the decision mechanism
then determines the binary class (expressed or un-
addressed) accordingly.
3.1 Entailment Modules
Current textual entailment systems operate across
different linguistic levels, mainly on lexical infer-
ence and syntax. We examined three representa-
tive modules that reflect these levels: Exact Match,
Lexical Inference, and Syntactic Inference.
Exact Match We represent T as a bag-of-words
containing all tokens and lemmas appearing in the
text. We then check whether both facet lemmas
w1, w2 appear in the text?s bag-of-words. Exact
matching was used as a baseline in previous rec-
ognizing textual entailment challenges (Bentivogli
et al, 2011), and similar methods of lemma-
matching were used as a component in recogniz-
ing textual entailment systems (Clark and Harri-
son, 2010; Shnarch et al, 2011).
Lexical Inference This feature checks whether
both facet words, or semantically related words,
appear in T . We use WordNet (Fellbaum, 1998)
with the Resnik similarity measure (Resnik, 1995)
and count a facet term wi as matched if the sim-
ilarity score exceeds a certain threshold (0.9, em-
pirically determined on the training set). Both w1
and w2 must match for this module?s entailment
decision to be positive.
Syntactic Inference This module builds upon
the open source1 Bar-Ilan University Textual En-
tailment Engine (BIUTEE) (Stern and Dagan,
2011). BIUTEE operates on dependency trees by
applying a sequence of knowledge-based transfor-
mations that converts T into H . It determines en-
tailment according to the ?cost? of generating the
hypothesis from the text. The cost model can be
automatically tuned with a relatively small train-
ing set. BIUTEE has shown state-of-the-art per-
formance on previous recognizing textual entail-
ment challenges (Stern and Dagan, 2012).
Since BIUTEE processes dependency trees,
both T and the facet must be parsed. We therefore
extract a path in H?s dependency tree that repre-
sents the facet. This is done by first parsing H ,
and then locating the two nodes whose words com-
pose the facet. We then find their lowest common
ancestor (LCA), and extract the path P from w1 to
1cs.biu.ac.il/?nlp/downloads/biutee
452
w2 through the LCA. This path is in fact a depen-
dency tree. BIUTEE can now be given T and P
(as the hypothesis), and try to recognize whether
the former entails the latter.
3.2 Decision Mechanisms
We started our experimentation process by defin-
ing Exact Match as a baseline. Though very sim-
ple, this unsupervised baseline performed surpris-
ingly well, with 0.96 precision and 0.32 recall on
expressed facets of the training data. Given its
very high precision, we decided to use this mod-
ule as an initial filter, and employ the others for
classifying the ?harder? cases.
We present all the mechanisms that we tested:
Baseline Exact
BaseLex Exact ? Lexical
BaseSyn Exact ? Syntactic
Disjunction Exact ? Lexical ? Syntactic
Majority Exact ? (Lexical ? Syntactic)
Note that since every facet that Exact Match
classifies as expressed is also expressed by Lexi-
cal Inference, BaseLex is essentially Lexical Infer-
ence on its own, and Majority is equivalent to the
majority rule on all three modules.
4 Empirical Evaluation
4.1 Dataset: Student Response Analysis
We evaluated our methods as part of RTE-8. The
challenge focuses on the domain of scholastic
quizzes, and attempts to emulate the meticulous
marking process that teachers do on a daily basis.
Given a question, a student?s response, and a refer-
ence answer, the task of student response analysis
is to determine whether the student answered cor-
rectly. This task can be approximated as a special
case of textual entailment; by assigning the stu-
dent?s answer as T and the reference answer as H ,
we are basically asking whether one can infer the
correct (reference) answer from the student?s re-
sponse.
Recall the example from Section 2. In this case,
H is a reference answer to the question:
Q: What is the main job of muscles?
T is essentially the student answer, though it is
also possible to define T as the union of both the
question and the student answer. In this work, we
chose to exclude the question.
There were two tracks in the challenge: com-
plete textual entailment (the main task) and partial
Unseen Unseen Unseen
Answers Questions Domains
Baseline .670 .688 .731
BaseLex .756 .710 .760
BaseSyn .744 .733 .770
Disjunction .695 .655 .703
Majority .782 .765 .816
Table 1: Micro-averaged F1 on the faceted Sci-
EntsBank test set.
entailment (the pilot task). Both tasks made use of
the SciEntsBank corpus (Dzikovska et al, 2012),
which is annotated at facet-level, and provides a
convenient test-bed for evaluation of both partial
and complete entailment. This dataset was split
into train and test subsets. The test set has 16,263
facet-response pairs based on 5,106 student re-
sponses over 15 domains (learning modules). Per-
formance was measured using micro-averaged F1,
over three different scenarios:
Unseen Answers Classify new answers to ques-
tions seen in training. Contains 464 student re-
sponses.
Unseen Questions Classify new answers to
questions that were not seen in training, but other
questions from the same domain were. Contains
631 student responses.
Unseen Domains Classify new answers to un-
seen questions from unseen domains. Contains
4,011 student responses.
4.2 Results
Table 1 shows the F1-measure of each configu-
ration in each scenario. There is some variance
between the different scenarios; this may be at-
tributed to the fact that there are much fewer Un-
seen Answers and Unseen Questions instances. In
all cases, Majority significantly outperformed the
other configurations. While BaseLex and BaseSyn
improve upon the baseline, they seem to make dif-
ferent mistakes, in particular false positives. Their
conjunction is thus a more conservative indicator
of entailment, and proves helpful in terms of F1.
All improvements over the baseline were found
to be statistically significant using McNemar?s test
with p < 0.01 (excluding Disjunction). It is also
interesting to note that the systems? performance
does not degrade in ?harder? scenarios; this is a re-
sult of the mostly unsupervised nature of our mod-
ules.
453
Unfortunately, our system was the only submis-
sion in the partial entailment pilot track of RTE-
8, so we have no comparisons with other sys-
tems. However, the absolute improvement from
the exact-match baseline to the more sophisticated
Majority is in the same ballpark as that of the best
systems in previous recognizing textual entailment
challenges. For instance, in the previous recogniz-
ing textual entailment challenge (Bentivogli et al,
2011), the best system yielded an F1 score of 0.48,
while the baseline scored 0.374. We can therefore
conclude that existing approaches for recognizing
textual entailment can indeed be adapted for rec-
ognizing partial entailment.
5 Utilizing Partial Entailment for
Recognizing Complete Entailment
Encouraged by our results, we ask whether the
same algorithms that performed well on the
faceted entailment task can be used for recogniz-
ing complete textual entailment. We performed an
initial experiment that examines this concept and
sheds some light on the potential role of partial en-
tailment as a possible facilitator for complete en-
tailment.
We suggest the following 3-stage architecture:
1. Decompose the hypothesis into facets.
2. Determine whether each facet is entailed.
3. Aggregate the individual facet results and de-
cide on complete entailment accordingly.
Facet Decomposition For this initial investiga-
tion, we use the facets provided in SciEntsBank;
i.e. we assume that the step of facet decomposition
has already been carried out. When the dataset
was created for RTE-8, many facets were extracted
automatically, but only a subset was selected. The
facet selection process was done manually, as part
of the dataset?s annotation. For example, in ?the
main job of muscles is to move bones?, the facet
(job, muscles) was not selected, because it was not
critical for answering the question. We refer to the
issue of relying on manual input further below.
Recognizing Faceted Entailment This step was
carried out as explained in the previous sections.
We used the Baseline configuration and Majority,
which performed best in our experiments above.
In addition, we introduce GoldBased that uses the
gold annotation of faceted entailment, and thus
Unseen Unseen Unseen
Answers Questions Domains
Baseline .575 .582 .683
Majority .707 .673 .764
GoldBased .842 .897 .852
BestComplete .773 .745 .712
Table 2: Micro-averaged F1 on the 2-way com-
plete entailment SciEntsBank test set.
provides a certain upper bound on the perfor-
mance of determining complete entailment based
on facets.
Aggregation We chose the simplest sensible ag-
gregation rule to decide on overall entailment: a
student answer is classified as correct (i.e. it en-
tails the reference answer) if it expresses each
of the reference answer?s facets. Although this
heuristic is logical from a strict entailment per-
spective, it might yield false negatives on this par-
ticular dataset. This happens because tutors may
sometimes grade answers as valid even if they
omit some less important, or indirectly implied,
facets.
Table 2 shows the experiment?s results. The
first thing to notice is that GoldBased is not per-
fect. There are two reasons for this behavior.
First, the task of student response analysis is only
an approximation of textual entailment, albeit a
good one. This discrepancy was also observed
by the RTE-8 challenge organizers (Dzikovska et
al., 2013). The second reason is because some of
the original facets were filtered when creating the
dataset. This caused both false positives (when
important facets were filtered out) and false neg-
atives (when unimportant facets were retained).
Our Majority mechanism, which requires that
the two underlying methods for partial entailment
detection (Lexical Inference and Syntactic Infer-
ence) agree on a positive classification, bridges
about half the gap from the baseline to the gold
based method. As a rough point of comparison,
we also show the performance of BestComplete,
the winning entry in each setting of the RTE-8
main task. This measure is not directly compara-
ble to our facet-based systems, because it did not
rely on manually selected facets, and due to some
variations in the dataset size (about 20% of the stu-
dent responses were not included in the pilot task
dataset). However, these results may indicate the
454
prospects of using faceted entailment for complete
entailment recognition, suggesting it as an attrac-
tive research direction.
6 Conclusion and Future Work
In this paper, we presented an empirical attempt
to tackle the problem of partial textual entail-
ment. We demonstrated that existing methods for
recognizing (complete) textual entailment can be
successfully adapted to this setting. Our experi-
ments showed that boolean combinations of these
methods yield good results. Future research may
add additional features and more complex fea-
ture combination methods, such as weighted sums
tuned by machine learning. Furthermore, our
work focused on a specific decomposition model
? faceted entailment. Other flavors of partial en-
tailment should be investigated as well. Finally,
we examined the possibility of utilizing partial en-
tailment for recognizing complete entailment in a
semi-automatic setting, which relied on the man-
ual facet annotation in the RTE-8 dataset. Our
preliminary results suggest that this approach is
indeed feasible, and warrant further research on
facet-based entailment methods that rely on fully-
automatic facet extraction.
Acknowledgements
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 287923 (EXCITEMENT).
We would like to thank the Minerva Foundation
for facilitating this cooperation with a short term
research grant.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6:
A pilot on semantic textual similarity. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation, in conjunction with the 1st Joint Con-
ference on Lexical and Computational Semantics,
pages 385?393, Montreal, Canada.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. Proceed-
ings of TAC.
Peter Clark and Phil Harrison. 2010. Blue-lite: a
knowledge-based lexical entailment system for rte6.
Proc. of TAC.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nale, evaluation and approaches. Natural Language
Engineering, 15(4):i?xvii.
Myroslava O Dzikovska, Rodney D Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210. Association for Computational Lin-
guistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang
Dang. 2013. Semeval-2013 task 7: The joint stu-
dent response analysis and 8th recognizing textual
entailment challenge. In *SEM 2013: The First
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, Georgia, USA, 13-14 June. Asso-
ciation for Computational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Rodney D Nielsen, Wayne Ward, and James H Mar-
tin. 2009. Recognizing entailment in intelligent
tutoring systems. Natural Language Engineering,
15(4):479?501.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI 1995), pages 448?
453.
Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011.
A probabilistic modeling framework for lexical en-
tailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 558?
563, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Proceedings of the 8th International Conference
on Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462.
Asher Stern and Ido Dagan. 2012. Biutee: A mod-
ular open-source system for recognizing textual en-
tailment. In Proceedings of the ACL 2012 System
Demonstrations, pages 73?78, Jeju Island, Korea,
July. Association for Computational Linguistics.
455
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302?308,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Dependency-Based Word Embeddings
Omer Levy
?
and Yoav Goldberg
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
{omerlevy,yoav.goldberg}@gmail.com
Abstract
While continuous word embeddings are
gaining popularity, current models are
based solely on linear contexts. In this
work, we generalize the skip-gram model
with negative sampling introduced by
Mikolov et al to include arbitrary con-
texts. In particular, we perform exper-
iments with dependency-based contexts,
and show that they produce markedly
different embeddings. The dependency-
based embeddings are less topical and ex-
hibit more functional similarity than the
original skip-gram embeddings.
1 Introduction
Word representation is central to natural language
processing. The default approach of represent-
ing words as discrete and distinct symbols is in-
sufficient for many tasks, and suffers from poor
generalization. For example, the symbolic repre-
sentation of the words ?pizza? and ?hamburger?
are completely unrelated: even if we know that
the word ?pizza? is a good argument for the verb
?eat?, we cannot infer that ?hamburger? is also
a good argument. We thus seek a representation
that captures semantic and syntactic similarities
between words. A very common paradigm for ac-
quiring such representations is based on the distri-
butional hypothesis of Harris (1954), stating that
words in similar contexts have similar meanings.
Based on the distributional hypothesis, many
methods of deriving word representations were ex-
plored in the NLP community. On one end of the
spectrum, words are grouped into clusters based
on their contexts (Brown et al, 1992; Uszkor-
eit and Brants, 2008). On the other end, words
?
Supported by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
are represented as a very high dimensional but
sparse vectors in which each entry is a measure
of the association between the word and a particu-
lar context (see (Turney and Pantel, 2010; Baroni
and Lenci, 2010) for a comprehensive survey).
In some works, the dimensionality of the sparse
word-context vectors is reduced, using techniques
such as SVD (Bullinaria and Levy, 2007) or LDA
(Ritter et al, 2010; S?eaghdha, 2010; Cohen et
al., 2012). Most recently, it has been proposed
to represent words as dense vectors that are de-
rived by various training methods inspired from
neural-network language modeling (Bengio et al,
2003; Collobert and Weston, 2008; Mnih and
Hinton, 2008; Mikolov et al, 2011; Mikolov et
al., 2013b). These representations, referred to as
?neural embeddings? or ?word embeddings?, have
been shown to perform well across a variety of
tasks (Turian et al, 2010; Collobert et al, 2011;
Socher et al, 2011; Al-Rfou et al, 2013).
Word embeddings are easy to work with be-
cause they enable efficient computation of word
similarities through low-dimensional matrix op-
erations. Among the state-of-the-art word-
embedding methods is the skip-gram with nega-
tive sampling model (SKIPGRAM), introduced by
Mikolov et al (2013b) and implemented in the
word2vec software.
1
Not only does it produce
useful word representations, but it is also very ef-
ficient to train, works in an online fashion, and
scales well to huge copora (billions of words) as
well as very large word and context vocabularies.
Previous work on neural word embeddings take
the contexts of a word to be its linear context ?
words that precede and follow the target word, typ-
ically in a window of k tokens to each side. How-
ever, other types of contexts can be explored too.
In this work, we generalize the SKIP-
GRAM model, and move from linear bag-of-words
contexts to arbitrary word contexts. Specifically,
1
code.google.com/p/word2vec/
302
following work in sparse vector-space models
(Lin, 1998; Pad?o and Lapata, 2007; Baroni and
Lenci, 2010), we experiment with syntactic con-
texts that are derived from automatically produced
dependency parse-trees.
The different kinds of contexts produce no-
ticeably different embeddings, and induce differ-
ent word similarities. In particular, the bag-of-
words nature of the contexts in the ?original?
SKIPGRAM model yield broad topical similari-
ties, while the dependency-based contexts yield
more functional similarities of a cohyponym na-
ture. This effect is demonstrated using both quali-
tative and quantitative analysis (Section 4).
The neural word-embeddings are considered
opaque, in the sense that it is hard to assign mean-
ings to the dimensions of the induced represen-
tation. In Section 5 we show that the SKIP-
GRAM model does allow for some introspection
by querying it for contexts that are ?activated by? a
target word. This allows us to peek into the learned
representation and explore the contexts that are
found by the learning process to be most discrim-
inative of particular words (or groups of words).
To the best of our knowledge, this is the first work
to suggest such an analysis of discriminatively-
trained word-embedding models.
2 The Skip-Gram Model
Our departure point is the skip-gram neural em-
bedding model introduced in (Mikolov et al,
2013a) trained using the negative-sampling pro-
cedure presented in (Mikolov et al, 2013b). In
this section we summarize the model and train-
ing objective following the derivation presented by
Goldberg and Levy (2014), and highlight the ease
of incorporating arbitrary contexts in the model.
In the skip-gram model, each word w ? W is
associated with a vector v
w
? R
d
and similarly
each context c ? C is represented as a vector
v
c
? R
d
, where W is the words vocabulary, C
is the contexts vocabulary, and d is the embed-
ding dimensionality. The entries in the vectors
are latent, and treated as parameters to be learned.
Loosely speaking, we seek parameter values (that
is, vector representations for both words and con-
texts) such that the dot product v
w
? v
c
associated
with ?good? word-context pairs is maximized.
More specifically, the negative-sampling objec-
tive assumes a dataset D of observed (w, c) pairs
of words w and the contexts c, which appeared in
a large body of text. Consider a word-context pair
(w, c). Did this pair come from the data? We de-
note by p(D = 1|w, c) the probability that (w, c)
came from the data, and by p(D = 0|w, c) =
1 ? p(D = 1|w, c) the probability that (w, c) did
not. The distribution is modeled as:
p(D = 1|w, c) =
1
1+e
?v
w
?v
c
where v
w
and v
c
(each a d-dimensional vector) are
the model parameters to be learned. We seek to
maximize the log-probability of the observed pairs
belonging to the data, leading to the objective:
argmax
v
w
,v
c
?
(w,c)?D
log
1
1+e
?v
c
?v
w
This objective admits a trivial solution in which
p(D = 1|w, c) = 1 for every pair (w, c). This can
be easily achieved by setting v
c
= v
w
and v
c
?v
w
=
K for all c, w, where K is large enough number.
In order to prevent the trivial solution, the ob-
jective is extended with (w, c) pairs for which
p(D = 1|w, c) must be low, i.e. pairs which are
not in the data, by generating the set D
?
of ran-
dom (w, c) pairs (assuming they are all incorrect),
yielding the negative-sampling training objective:
argmax
v
w
,v
c
(
?
(w,c)?D
p(D = 1|c, w)
?
(w,c)?D
?
p(D = 0|c, w)
)
which can be rewritten as:
argmax
v
w
,v
c
(
?
(w,c)?D
log ?(v
c
? v
w
) +
?
(w,c)?D
?
log ?(?v
c
? v
w
)
)
where ?(x) = 1/(1+e
x
). The objective is trained
in an online fashion using stochastic-gradient up-
dates over the corpus D ?D
?
.
The negative samples D
?
can be constructed in
various ways. We follow the method proposed by
Mikolov et al: for each (w, c) ? D we construct
n samples (w, c
1
), . . . , (w, c
n
), where n is a hy-
perparameter and each c
j
is drawn according to its
unigram distribution raised to the 3/4 power.
Optimizing this objective makes observed
word-context pairs have similar embeddings,
while scattering unobserved pairs. Intuitively,
words that appear in similar contexts should have
similar embeddings, though we have not yet found
a formal proof that SKIPGRAM does indeed max-
imize the dot product of similar words.
3 Embedding with Arbitrary Contexts
In the SKIPGRAM embedding algorithm, the con-
texts of a word w are the words surrounding it
303
in the text. The context vocabulary C is thus
identical to the word vocabulary W . However,
this restriction is not required by the model; con-
texts need not correspond to words, and the num-
ber of context-types can be substantially larger
than the number of word-types. We generalize
SKIPGRAM by replacing the bag-of-words con-
texts with arbitrary contexts.
In this paper we experiment with dependency-
based syntactic contexts. Syntactic contexts cap-
ture different information than bag-of-word con-
texts, as we demonstrate using the sentence ?Aus-
tralian scientist discovers star with telescope?.
Linear Bag-of-Words Contexts This is the
context used by word2vec and many other neu-
ral embeddings. Using a window of size k around
the target word w, 2k contexts are produced: the
k words before and the k words after w. For
k = 2, the contexts of the target word w are
w
?2
, w
?1
, w
+1
, w
+2
. In our example, the contexts
of discovers are Australian, scientist, star, with.
2
Note that a context window of size 2 may miss
some important contexts (telescope is not a con-
text of discovers), while including some acciden-
tal ones (Australian is a context discovers). More-
over, the contexts are unmarked, resulting in dis-
covers being a context of both stars and scientist,
which may result in stars and scientists ending
up as neighbours in the embedded space. A win-
dow size of 5 is commonly used to capture broad
topical content, whereas smaller windows contain
more focused information about the target word.
Dependency-Based Contexts An alternative to
the bag-of-words approach is to derive contexts
based on the syntactic relations the word partic-
ipates in. This is facilitated by recent advances
in parsing technology (Goldberg and Nivre, 2012;
Goldberg and Nivre, 2013) that allow parsing to
syntactic dependencies with very high speed and
near state-of-the-art accuracy.
After parsing each sentence, we derive word
contexts as follows: for a target word w with
modifiers m
1
, . . . ,m
k
and a head h, we consider
the contexts (m
1
, lbl
1
), . . . , (m
k
, lbl
k
), (h, lbl
?1
h
),
2
word2vec?s implementation is slightly more compli-
cated. The software defaults to prune rare words based on
their frequency, and has an option for sub-sampling the fre-
quent words. These pruning and sub-sampling happen before
the context extraction, leading to a dynamic window size. In
addition, the window size is not fixed to k but is sampled
uniformly in the range [1, k] for each word.
Australian scientist discovers star with telescope
amod
nsubj
dobj
prep
pobj
Australian scientist discovers star telescope
amod
nsubj
dobj
prep with
WORD CONTEXTS
australian scientist/amod
?1
scientist australian/amod, discovers/nsubj
?1
discovers scientist/nsubj, star/dobj, telescope/prep with
star discovers/dobj
?1
telescope discovers/prep with
?1
Figure 1: Dependency-based context extraction example.
Top: preposition relations are collapsed into single arcs,
making telescope a direct modifier of discovers. Bottom: the
contexts extracted for each word in the sentence.
where lbl is the type of the dependency relation be-
tween the head and the modifier (e.g. nsubj, dobj,
prep with, amod) and lbl
?1
is used to mark the
inverse-relation. Relations that include a preposi-
tion are ?collapsed? prior to context extraction, by
directly connecting the head and the object of the
preposition, and subsuming the preposition itself
into the dependency label. An example of the de-
pendency context extraction is given in Figure 1.
Notice that syntactic dependencies are both
more inclusive and more focused than bag-of-
words. They capture relations to words that are
far apart and thus ?out-of-reach? with small win-
dow bag-of-words (e.g. the instrument of discover
is telescope/prep with), and also filter out ?coinci-
dental? contexts which are within the window but
not directly related to the target word (e.g. Aus-
tralian is not used as the context for discovers). In
addition, the contexts are typed, indicating, for ex-
ample, that stars are objects of discovery and sci-
entists are subjects. We thus expect the syntactic
contexts to yield more focused embeddings, cap-
turing more functional and less topical similarity.
4 Experiments and Evaluation
We experiment with 3 training conditions: BOW5
(bag-of-words contexts with k = 5), BOW2
(same, with k = 2) and DEPS (dependency-based
syntactic contexts). We modified word2vec to
support arbitrary contexts, and to output the con-
text embeddings in addition to the word embed-
dings. For bag-of-words contexts we used the
original word2vec implementation, and for syn-
tactic contexts, we used our modified version. The
negative-sampling parameter (how many negative
contexts to sample for every correct one) was 15.
304
All embeddings were trained on English
Wikipedia. For DEPS, the corpus was tagged
with parts-of-speech using the Stanford tagger
(Toutanova et al, 2003) and parsed into labeled
Stanford dependencies (de Marneffe and Man-
ning, 2008) using an implementation of the parser
described in (Goldberg and Nivre, 2012). All to-
kens were converted to lowercase, and words and
contexts that appeared less than 100 times were
filtered. This resulted in a vocabulary of about
175,000 words, with over 900,000 distinct syntac-
tic contexts. We report results for 300 dimension
embeddings, though similar trends were also ob-
served with 600 dimensions.
4.1 Qualitative Evaluation
Our first evaluation is qualitative: we manually in-
spect the 5 most similar words (by cosine similar-
ity) to a given set of target words (Table 1).
The first target word, Batman, results in similar
sets across the different setups. This is the case for
many target words. However, other target words
show clear differences between embeddings.
In Hogwarts - the school of magic from the
fictional Harry Potter series - it is evident that
BOW contexts reflect the domain aspect, whereas
DEPS yield a list of famous schools, capturing
the semantic type of the target word. This ob-
servation holds for Turing
3
and many other nouns
as well; BOW find words that associate with w,
while DEPS find words that behave like w. Turney
(2012) described this distinction as domain simi-
larity versus functional similarity.
The Florida example presents an ontologi-
cal difference; bag-of-words contexts generate
meronyms (counties or cities within Florida),
while dependency-based contexts provide cohy-
ponyms (other US states). We observed the same
behavior with other geographical locations, partic-
ularly with countries (though not all of them).
The next two examples demonstrate that simi-
larities induced from DEPS share a syntactic func-
tion (adjectives and gerunds), while similarities
based on BOW are more diverse. Finally, we ob-
serve that while both BOW5 and BOW2 yield top-
ical similarities, the larger window size result in
more topicality, as expected.
3
DEPS generated a list of scientists whose name ends with
?ing?. This is may be a result of occasional POS-tagging
errors. Still, the embedding does a remarkable job and re-
trieves scientists, despite the noisy POS. The list contains
more mathematicians without ?ing? further down.
Target Word BOW5 BOW2 DEPS
batman
nightwing superman superman
aquaman superboy superboy
catwoman aquaman supergirl
superman catwoman catwoman
manhunter batgirl aquaman
hogwarts
dumbledore evernight sunnydale
hallows sunnydale collinwood
half-blood garderobe calarts
malfoy blandings greendale
snape collinwood millfield
turing
nondeterministic non-deterministic pauling
non-deterministic finite-state hotelling
computability nondeterministic heting
deterministic buchi lessing
finite-state primality hamming
florida
gainesville fla texas
fla alabama louisiana
jacksonville gainesville georgia
tampa tallahassee california
lauderdale texas carolina
object-oriented
aspect-oriented aspect-oriented event-driven
smalltalk event-driven domain-specific
event-driven objective-c rule-based
prolog dataflow data-driven
domain-specific 4gl human-centered
dancing
singing singing singing
dance dance rapping
dances dances breakdancing
dancers breakdancing miming
tap-dancing clowning busking
Table 1: Target words and their 5 most similar words, as in-
duced by different embeddings.
We also tried using the subsampling option
(Mikolov et al, 2013b) with BOW contexts (not
shown). Since word2vec removes the subsam-
pled words from the corpus before creating the
window contexts, this option effectively increases
the window size, resulting in greater topicality.
4.2 Quantitative Evaluation
We supplement the examples in Table 1 with
quantitative evaluation to show that the qualita-
tive differences pointed out in the previous sec-
tion are indeed widespread. To that end, we use
the WordSim353 dataset (Finkelstein et al, 2002;
Agirre et al, 2009). This dataset contains pairs of
similar words that reflect either relatedness (top-
ical similarity) or similarity (functional similar-
ity) relations.
4
We use the embeddings in a re-
trieval/ranking setup, where the task is to rank the
similar pairs in the dataset above the related ones.
The pairs are ranked according to cosine sim-
ilarities between the embedded words. We then
draw a recall-precision curve that describes the
embedding?s affinity towards one subset (?sim-
ilarity?) over another (?relatedness?). We ex-
pect DEPS?s curve to be higher than BOW2?s
curve, which in turn is expected to be higher than
4
Some word pairs are judged to exhibit both types of sim-
ilarity, and were ignored in this experiment.
305
Figure 2: Recall-precision curve when attempting to rank the
similar words above the related ones. (a) is based on the
WordSim353 dataset, and (b) on the Chiarello et al dataset.
BOW5?s. The graph in Figure 2a shows this is in-
deed the case. We repeated the experiment with a
different dataset (Chiarello et al, 1990) that was
used by Turney (2012) to distinguish between do-
main and functional similarities. The results show
a similar trend (Figure 2b). When reversing the
task such that the goal is to rank the related terms
above the similar ones, the results are reversed, as
expected (not shown).
5
5 Model Introspection
Neural word embeddings are often considered
opaque and uninterpretable, unlike sparse vec-
tor space representations in which each dimen-
sion corresponds to a particular known context, or
LDA models where dimensions correspond to la-
tent topics. While this is true to a large extent, we
observe that SKIPGRAM does allow a non-trivial
amount of introspection. Although we cannot as-
sign a meaning to any particular dimension, we
can indeed get a glimpse at the kind of informa-
tion being captured by the model, by examining
which contexts are ?activated? by a target word.
Recall that the learning procedure is attempting
to maximize the dot product v
c
?v
w
for good (w, c)
pairs and minimize it for bad ones. If we keep the
context embeddings, we can query the model for
the contexts that are most activated by (have the
highest dot product with) a given target word. By
doing so, we can see what the model learned to be
a good discriminative context for the word.
To demonstrate, we list the 5 most activated
contexts for our example words with DEPS em-
beddings in Table 2. Interestingly, the most dis-
criminative syntactic contexts in these cases are
5
Additional experiments (not presented in this paper) re-
inforce our conclusion. In particular, we found that DEPS
perform dramatically worse than BOW contexts on analogy
tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014).
batman hogwarts turing
superman/conj
?1
students/prep at
?1
machine/nn
?1
spider-man/conj
?1
educated/prep at
?1
test/nn
?1
superman/conj student/prep at
?1
theorem/poss
?1
spider-man/conj stay/prep at
?1
machines/nn
?1
robin/conj learned/prep at
?1
tests/nn
?1
florida object-oriented dancing
marlins/nn
?1
programming/amod
?1
dancing/conj
beach/appos
?1
language/amod
?1
dancing/conj
?1
jacksonville/appos
?1
framework/amod
?1
singing/conj
?1
tampa/appos
?1
interface/amod
?1
singing/conj
florida/conj
?1
software/amod
?1
ballroom/nn
Table 2: Words and their top syntactic contexts.
not associated with subjects or objects of verbs
(or their inverse), but rather with conjunctions, ap-
positions, noun-compounds and adjectivial modi-
fiers. Additionally, the collapsed preposition rela-
tion is very useful (e.g. for capturing the school
aspect of hogwarts). The presence of many con-
junction contexts, such as superman/conj for
batman and singing/conj for dancing, may
explain the functional similarity observed in Sec-
tion 4; conjunctions in natural language tend to en-
force their conjuncts to share the same semantic
types and inflections.
In the future, we hope that insights from such
model introspection will allow us to develop better
contexts, by focusing on conjunctions and prepo-
sitions for example, or by trying to figure out why
the subject and object relations are absent and
finding ways of increasing their contributions.
6 Conclusions
We presented a generalization of the SKIP-
GRAM embedding model in which the linear bag-
of-words contexts are replaced with arbitrary ones,
and experimented with dependency-based con-
texts, showing that they produce markedly differ-
ent kinds of similarities. These results are ex-
pected, and follow similar findings in the distri-
butional semantics literature. We also demon-
strated how the resulting embedding model can be
queried for the discriminative contexts for a given
word, and observed that the learning procedure
seems to favor relatively local syntactic contexts,
as well as conjunctions and objects of preposition.
We hope these insights will facilitate further re-
search into improved context modeling and better,
possibly task-specific, embedded representations.
Our software, allowing for experimentation with
arbitrary contexts, together with the embeddings
described in this paper, are available for download
at the authors? websites.
306
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27, Boulder, Colorado, June. Association
for Computational Linguistics.
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proc. of CoNLL 2013.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F Brown, Robert L Mercer, Vincent J
Della Pietra, and Jenifer C Lai. 1992. Class-based
n-gram models of natural. Computational Linguis-
tics, 18(4).
John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510?526.
Christine Chiarello, Curt Burgess, Lorie Richards, and
Alma Pollock. 1990. Semantic and associative
priming in the cerebral hemispheres: Some words
do, some words don?t... sometimes, some places.
Brain and Language, 38(1):75?104.
Raphael Cohen, Yoav Goldberg, and Michael Elhadad.
2012. Domain adaptation of a dependency parser
with a class-class selectional preference model. In
Proceedings of ACL 2012 Student Research Work-
shop, pages 43?48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al?s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for the arc-eager system. In Proc. of COLING
2012.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics, 1.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Omer Levy and Yoav Goldberg. 2014. Linguistic
regularities in sparse and explicit word representa-
tions. In Proceedings of the Eighteenth Conference
on Computational Natural Language Learning, Bal-
timore, Maryland, USA, June. Association for Com-
putational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
768?774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States, pages 3111?
3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
307
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
pages 1081?1088.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional pref-
erences. In ACL, pages 424?434.
Diarmuid
?
O S?eaghdha. 2010. Latent variable models
of selectional preference. In ACL, pages 435?444.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of NAACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
P.D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37(1):141?
188.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Proc.
of ACL, pages 755?762.
308
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 43?48,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Excitement Open Platform for Textual Inferences
Bernardo Magnini
?
, Roberto Zanoli
?
, Ido Dagan
?
, Kathrin Eichler
?
, G?unter Neumann
?
,
Tae-Gil Noh
?
, Sebastian Pado
?
, Asher Stern
?
, Omer Levy
?
?
FBK (magnini|zanoli@fbk.eu)
?
Heidelberg, Stuttgart Univ. (pado|noh@cl.uni-heidelberg.de)
?
DFKI (neumann|eichler@dfki.de)
?
Bar Ilan University (dagan|sterna3|omerlevy@cs.biu.ac.il)
Abstract
This paper presents the Excitement Open
Platform (EOP), a generic architecture and
a comprehensive implementation for tex-
tual inference in multiple languages. The
platform includes state-of-art algorithms,
a large number of knowledge resources,
and facilities for experimenting and test-
ing innovative approaches. The EOP is
distributed as an open source software.
1 Introduction
In the last decade textual entailment (Dagan et al.,
2009) has been a very active topic in Computa-
tional Linguistics, providing a unifying framework
for textual inference. Several evaluation exercises
have been organized around Recognizing Textual
Entailment (RTE) challenges and many method-
ologies, algorithms and knowledge resources have
been proposed to address the task. However, re-
search in textual entailment is still fragmented and
there is no unifying algorithmic framework nor
software architecture.
In this paper, we present the Excitement Open
Platform (EOP), a generic architecture and a com-
prehensive implementation for multilingual textual
inference which we make available to the scien-
tific and technological communities. To a large
extent, the idea is to follow the successful experi-
ence of the Moses open source platform (Koehn et
al., 2007) in Machine Translation, which has made
a substantial impact on research in that field. The
EOP is the result of a two-year coordinated work
under the international project EXCITEMENT.
1
A
consortium of four academic partners has defined
the EOP architectural specifications, implemented
the functional interfaces of the EOP components,
imported existing entailment engines into the EOP
1
http://www.excitement-project.eu
and finally designed and implemented a rich envi-
ronment to support open source distribution.
The goal of the platform is to provide function-
ality for the automatic identification of entailment
relations among texts. The EOP is based on a modu-
lar architecture with a particular focus on language-
independent algorithms. It allows developers and
users to combine linguistic pipelines, entailment al-
gorithms and linguistic resources within and across
languages with as little effort as possible. For ex-
ample, different entailment decision approaches
can share the same resources and the same sub-
components in the platform. A classification-based
algorithm can use the distance component of an
edit-distance based entailment decision approach,
and two different approaches can use the same set
of knowledge resources. Moreover, the platform
has various multilingual components for languages
like English, German and Italian. The result is an
ideal software environment for experimenting and
testing innovative approaches for textual inferences.
The EOP is distributed as an open source software
2
and its use is open both to users interested in using
inference in applications and to developers willing
to extend the current functionalities.
The paper is structured as follows. Section 2
presents the platform architecture, highlighting
how the EOP component-based approach favors
interoperability. Section 3 provides a picture of
the current population of the EOP in terms of both
entailment algorithms and knowledge resources.
Section 4 introduces expected use cases of the plat-
form. Finally, Section 5 presents the main features
of the open source package.
2 Architecture
The EOP platform takes as input two text portions,
the first called the Text (abbreviated with T), the
second called the Hypothesis (abbreviated with H).
2
http://hltfbk.github.io/
Excitement-Open-Platform/
43
Linguis'c)Analysis)Pipeline)(LAP))
Entailment)Core)(EC))
Entailment)Decision))Algorithm)(EDA))
Dynamic)and)Sta'c)Components)(Algorithms)and)Knowledge))
Linguis'c)Analysis)Components)
Decision)
1)
Raw)Data)
Figure 1: EOP architecture
The output is an entailment judgement, either ?En-
tailment? if T entails H, or ?NonEntailment? if the
relation does not hold. A confidence score for the
decision is also returned in both cases.
The EOP architecture (Pad?o et al., 2014) is based
on the concept of modularization with pluggable
and replaceable components to enable extension
and customization. The overall structure is shown
in Figure 1 and consists of two main parts. The
Linguistic Analysis Pipeline (LAP) is a series of
linguistic annotation components. The Entailment
Core (EC) performs the actual entailment recog-
nition. This separation ensures that (a) the com-
ponents in the EC only rely on linguistic analysis
in well-defined ways and (b) the LAP and EC can
be run independently of each other. Configuration
files are the principal means of configuring the EOP.
In the rest of this section we first provide an intro-
duction to the LAP, then we move to the EC and
finally describe the configuration files.
2.1 Linguistic Analysis Pipeline (LAP)
The Linguistic Analysis Pipeline is a collection of
annotation components for Natural Language Pro-
cessing (NLP) based on the Apache UIMA frame-
work.
3
Annotations range from tokenization to
part of speech tagging, chunking, Named Entity
Recognition and parsing. The adoption of UIMA
enables interoperability among components (e.g.,
substitution of one parser by another one) while
ensuring language independence. Input and output
of the components are represented in an extended
version of the DKPro type system based on UIMA
3
http://uima.apache.org/
Common Analysis Structure (CAS) (Gurevych et
al., 2007; Noh and Pad?o, 2013).
2.2 Entailment Core (EC)
The Entailment Core performs the actual entail-
ment recognition based on the preprocessed text
made by the Linguistic Analysis Pipeline. It con-
sists of one or more Entailment Decision Algo-
rithms (EDAs) and zero or more subordinate com-
ponents. An EDA takes an entailment decision
(i.e., ?entailment? or ?no entailment?) while com-
ponents provide static and dynamic information for
the EDA.
Entailment Decision Algorithms are at the top
level in the EC. They compute an entailment deci-
sion for a given Text/Hypothesis (T/H) pair, and
can use components that provide standardized al-
gorithms or knowledge resources. The EOP ships
with several EDAs (cf. Section 3).
Scoring Components accept a Text/Hypothesis
pair as an input, and return a vector of scores.
Their output can be used directly to build minimal
classifier-based EDAs forming complete RTE sys-
tems. An extended version of these components are
the Distance Components that can produce normal-
ized and unnormalized distance/similarity values
in addition to the score vector.
Annotation Components can be used to add dif-
ferent annotations to the Text/Hypothesis pairs. An
example of such a type of component is one that
produces word or phrase alignments between the
Text and the Hypothesis.
Lexical Knowledge Components describe se-
mantic relationships between words. In the
EOP, this knowledge is represented as directed
rules made up of two word?POS pairs, where
the LHS (left-hand side) entails the RHS (right-
hand side), e.g., (shooting star,Noun) =?
(meteorite,Noun). Lexical Knowledge Compo-
nents provide an interface that allows for (a) listing
all RHS for a given LHS; (b) listing all LHS for
a given RHS; and (c) checking for an entailment
relation for a given LHS?RHS pair. The interface
also wraps all major lexical knowledge sources cur-
rently used in RTE research, including manually
constructed ontologies like WordNet, and encyclo-
pedic resources like Wikipedia.
Syntactic Knowledge Components capture en-
tailment relationships between syntactic and
44
lexical-syntactic expressions. We represent such
relationships by entailment rules that link (option-
ally lexicalized) dependency tree fragments that
can contain variables as nodes. For example, the
rule fall of X =? X falls, or X sells Y to Z =?
Z buys Y from X express general paraphrasing pat-
terns at the predicate-argument level that cannot be
captured by purely lexical rules. Formally, each
syntactic rule consists of two dependency tree frag-
ments plus a mapping from the variables of the
LHS tree to the variables of the RHS tree.
4
2.3 Configuration Files
The EC components can be combined into actual
inference engines through configuration files which
contain information to build a complete inference
engine. A configuration file completely describes
an experiment. For example, it specifies the re-
sources that the selected EDA has to use and the
data set to be analysed. The LAP needed for data
set preprocessing is another parameter that can be
configured too. The platform ships with a set of
predefined configuration files accompanied by sup-
porting documentation.
3 Entailment Algorithms and Resources
This section provides a description of the Entail-
ment Algorithms and Knowledge Resources that
are distributed with the EOP.
3.1 Entailment Algorithms
The current version of the EOP platform ships with
three EDAs corresponding to three different ap-
proaches to RTE: an EDA based on transformations
between T and H, an EDA based on edit distance
algorithms, and a classification based EDA using
features extracted from T and H.
Transformation-based EDA applies a sequence
of transformations on T with the goal of making
it identical to H. If each transformation preserves
(fully or partially) the meaning of the original text,
then it can be concluded that the modified text
(which is actually the Hypothesis) can be inferred
from the original one. Consider the following sim-
ple example where the text is ?The boy was located
by the police? and the Hypothesis is ?The child
was found by the police?. Two transformations for
?boy? ? ?child? and ?located? ? ?found? do the
job.
4
Variables of the LHS may also map to null, when material
of the LHS must be present but is deleted in the inference step.
In the EOP we include a transformation based
inference system that adopts the knowledge based
transformations of Bar-Haim et al. (2007), while in-
corporating a probabilistic model to estimate trans-
formation confidences. In addition, it includes a
search algorithm which finds an optimal sequence
of transformations for any given T/H pair (Stern et
al., 2012).
Edit distance EDA involves using algorithms
casting textual entailment as the problem of map-
ping the whole content of T into the content of H.
Mappings are performed as sequences of editing
operations (i.e., insertion, deletion and substitu-
tion) on text portions needed to transform T into H,
where each edit operation has a cost associated with
it. The underlying intuition is that the probability
of an entailment relation between T and H is related
to the distance between them; see Kouylekov and
Magnini (2005) for a comprehensive experimental
study.
Classification based EDA uses a Maximum En-
tropy classifier to combine the outcomes of sev-
eral scoring functions and to learn a classification
model for recognizing entailment. The scoring
functions extract a number of features at various
linguistic levels (bag-of-words, syntactic dependen-
cies, semantic dependencies, named entities). The
approach was thoroughly described in Wang and
Neumann (2007).
3.2 Knowledge Resources
As described in Section 2.2, knowledge resources
are crucial to recognize cases where T and H use
different textual expressions (words, phrases) while
preserving entailment. The EOP platform includes
a wide range of knowledge resources, including lex-
ical and syntactic resources, where some of them
are grabbed from manual resources, like dictionar-
ies, while others are learned automatically. Many
EOP resources are inherited from pre-existing RTE
systems migrated into the EOP platform, but now
use the same interfaces, which makes them acces-
sible in a uniform fashion.
There are about two dozen lexical (e.g. word-
nets) and syntactic resources for three languages
(i.e. English, Italian and German). However,
since there is still a clear predominance of En-
glish resources, the platform includes lexical and
syntactic knowledge mining tools to bootstrap re-
sources from corpora, both for other languages and
45
EDA Accuracy / F1
Transformation-based English RTE-3 67.13%
Transformation-based English RTE-6 49.55%
Edit-Distance English RTE-3 64.38%
Edit-Distance German RTE-3 59.88%
Edit-Distance Italian RTE-3 63.50%
Classification-based English RTE-3 65.25%
Classification-based German RTE-3 63.75%
Median of RTE-3 (English) submissions 61.75%
Median of RTE-6 (English) submissions 33.72%
Table 1: EDAs results
for specific domains. Particularly, the EOP plat-
form includes a language independent tool to build
Wikipedia resources (Shnarch et al., 2009), as well
as a language-independent framework for building
distributional similarity resources like DIRT (Lin
and Pantel, 2002) and Lin similarity(Lin, 1998).
3.3 EOP Evaluation
Results for the three EDAs included in the EOP
platform are reported in Table 1. Each line rep-
resents an EDA, the language and the dataset
on which the EDA was evaluated. For brevity,
we omit here the knowledge resources used for
each EDA, even though knowledge configuration
clearly affects performance. The evaluations were
performed on RTE-3 dataset (Giampiccolo et al.,
2007), where the goal is to maximize accuracy. We
(manually) translated it to German and Italian for
evaluations: in both cases the results fix a refer-
ence for the two languages. The two new datasets
for German and English are available both as part
of the EOP distribution and independently
5
. The
transformation-based EDA was also evaluated on
RTE-6 dataset (Bentivogli et al., 2010), in which
the goal is to maximize the F1 measure.
The results of the included EDAs are higher than
median values of participated systems in RTE-3,
and they are competing with state-of-the-arts in
RTE-6 results. To the best of our knowledge, the
results of the EDAs as provided in the platform are
the highest among those available as open source
systems for the community.
4 Use Cases
We see four primary use cases for the EOP. Their
requirements were reflected in our design choices.
Use Case 1: Applied Textual Entailment. This
category covers users who are not interested in the
5
http://www.excitement-project.eu/
index.php/results
details of RTE but who are interested in an NLP
task in which textual entailment can take over part
of or all of the semantic processing, such as Ques-
tion Answering or Intelligent Tutoring. Such users
require a system that is as easy to deploy as possi-
ble, which motivates our offer of the EOP platform
as a library. They also require a system that pro-
vides good quality at a reasonable efficiency as
well as guidance as to the best choice of parame-
ters. The latter point is realized through our results
archive in the official EOP Wiki on the EOP site.
Use Case 2: Textual Entailment Development.
This category covers researchers who are interested
in Recognizing Textual Entailment itself, for exam-
ple with the goal of developing novel algorithms
for detecting entailment. In contrast to the first
category, this group need to look ?under the hood?
of the EOP platform and access the source code of
the EOP. For this reason, we have spent substantial
effort to provide the code in a well-structured and
well-documented form.
A subclass of this group is formed by researchers
who want to set up a RTE infrastructure for lan-
guages in which it does not yet exist (that is, al-
most all languages). The requirements of this class
of users comprises clearly specified procedures to
replace the Linguistic Analysis Pipeline, which are
covered in our documentation, and simple methods
to acquire knowledge resources for these languages
(assuming that the EDAs themselves are largely
language-independent). These are provided by the
language-independent knowledge acquisition tools
which we offer alongside the platform (cf. Section
3.2).
Use Case 3: Lexical Semantics Evaluation. A
third category consists of researchers whose pri-
mary interest is in (lexical) semantics.
As long as their scientific results can be phrased
in terms of semantic similarities or inference rules,
the EOP platform can be used as a simple and stan-
dardized workbench for these results that indicates
the impact that the semantic knowledge under con-
sideration has on deciding textual entailment. The
main requirement for this user group is the simple
integration of new knowledge resources into the
EOP platform. This is catered for through the defi-
nition of the generic knowledge component inter-
faces (cf. Section 2.2) and detailed documentation
on how to implement these interfaces.
46
Use Case 4: Educational Use. The fourth and
final use case is as an educational tool to support
academic courses and projects on Recognizing Tex-
tual Entailment and inference more generally. This
use case calls, in common with the others, for easy
usability and flexibility. Specifically for this use
case, we have also developed a series of tutorials
aimed at acquainting new users with the EOP plat-
form through a series of increasingly complexity
exercises that cover all areas of the EOP. We are
also posting proposals for projects to extend the
EOP on the EOP Wiki.
5 EOP Distribution
The EOP infrastructure follows state-of-the-art soft-
ware engineering standards to support both users
and developers with a flexible, scalable and easy to
use software environment. In addition to communi-
cation channels, like the mailing list and the issue
tracking system, the EOP infrastructure comprises
the following set of facilities.
Version Control System: We use GitHub,
6
a
web-based hosting service for code and documen-
tation storage, development, and issue tracking.
Web Site: The GitHub Automatic Page Genera-
tor was used to build the EOP web site and Wiki,
containing a general introduction to the software
platform, the terms of its license, mailing lists to
contact the EOP members and links to the code
releases.
Documentation: Both user and developer docu-
mentation is available from Wiki pages; the pages
are written with the GitHub Wiki Editor and hosted
on the GitHub repository. The documentation in-
cludes a Quick Start guide to start using the EOP
platform right away, and a detailed step by step
tutorial.
Results Archive: As a new feature for commu-
nity building, EOP users can, and are encouraged
to, share their results: the platform configuration
files used to produce results as well as contact infor-
mation can be saved and archived into a dedicated
page on the EOP GitHub repository. That allows
other EOP users to replicate experiments under
the same condition and/or avoid doing experiments
that have already been done.
6
https://github.com/
Build Automation Tool: The EOP has been de-
veloped as a Maven
7
multi-modules project, with
all modules sharing the same Maven standard struc-
ture, making it easier to find files in the project once
one is used to Maven.
Maven Artifacts Repository: Using a Maven
repository has a twofold goal: (i) to serve as an
internal private repository of all software libraries
used within the project (libraries are binary files
and should not be stored under version control sys-
tems, which are intended to be used with text files);
(ii) to make the produced EOP Maven artifacts
available (i.e., for users who want to use the EOP
as a library in their own code). We use Artifactory
8
repository manager to store produced artifacts.
Continuous Integration: The EOP uses Jenk-
ins
9
for Continuous Integration, a software develop-
ment practice where developers of a team integrate
their work frequently (e.g., daily).
Code Quality Tool: Ensuring the quality of the
produced software is one of the most important
aspects of software engineering. The EOP uses
tools like PMD
10
that can automatically be run
during development to help the developers check
the quality of their software.
5.1 Project Repository
The EOP Java source code is hosted on the EOP
Github repository and managed using Git. The
repository consists of three main branches: the
release branch contains the code that is supposed to
be in a production-ready state, whereas the master
branch contains the code to be incorporated into the
next release. When the source code in the master
branch reaches a stable point and is ready to be
released, all of the changes are merged back into
release. Finally, the gh-pages branch contains the
web site pages.
5.2 Licensing
The software of the platform is released under the
terms of General Public License (GPL) version
3.
11
The platform contains both components and
resources designed by the EOP developers, as well
as others that are well known and freely available
7
http://maven.apache.org/
8
http://www.jfrog.com/
9
http://jenkins-ci.org/
10
http://pmd.sourceforge.net
11
http://www.gnu.org/licenses/gpl.html
47
in the NLP research community. Additional com-
ponents and resources whose license is not compat-
ible with the EOP license have to be downloaded
and installed separately by the user.
6 Conclusion
This paper has presented the main characteristics
of Excitement Open Platform platform, a rich envi-
ronment for experimenting and evaluating textual
entailment systems. On the software side, the EOP
is a complex endeavor to integrate tools and re-
sources in Computational Linguistics, including
pipelines for three languages, three pre-existing
entailment engines, and about two dozens of lex-
ical and syntactic resources. The EOP assumes a
clear and modular separation between linguistic
annotations, entailment algorithms and knowledge
resources which are used by the algorithms. A
relevant benefit of the architectural design is that
a high level of interoperability is reached, provid-
ing a stimulating environment for new research in
textual inferences.
The EOP platform has been already tested in sev-
eral pilot research projects and educational courses,
and it is currently distributed as open source soft-
ware under the GPL-3 license. To the best of our
knowledge, the entailment systems and their con-
figurations provided in the platform are the best
systems available as open source for the commu-
nity. As for the future, we are planning several
initiatives for the promotion of the platform in the
research community, as well as its active experi-
mentation in real application scenarios.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages 871?
876, Vancouver, BC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Chal-
lenge. In Proceedings of TAC, Gaithersburg, MD.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Journal of Natural
Language Engineering, 15(4):i?xvii.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, Prague, Czech Repub-
lic.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture (UIMA@GSCL 2007), T?ubingen, Ger-
many.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL demo session, pages 177?
180, Prague, Czech Republic.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the First PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 17?20, Southampton, UK.
Dekang Lin and Patrick Pantel. 2002. Discovery of
Inference Rules for Question Answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL/COLING,
pages 768?774, Montr?eal, Canada.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using
UIMA to structure an open platform for textual en-
tailment. In Proceedings of the 3rd Workshop on
Unstructured Information Management Architecture
(UIMA@GSCL 2013).
Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realiza-
tion of a modular architecture for textual entailment.
Journal of Natural Language Engineering. doi:
10.1017/S1351324913000351.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP, pages 450?458, Sin-
gapore.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL, pages 283?291,
Jeju Island, South Korea.
Rui Wang and G?unter Neumann. 2007. Recogniz-
ing textual entailment using a subsequence kernel
method. In Proceedings of AAAI, pages 937?945,
Vancouver, BC.
48
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 285?289, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis
Torsten Zesch? Omer Levy? Iryna Gurevych? Ido Dagan?
? Ubiquitous Knowledge Processing Lab ? Natural Language Processing Lab
Computer Science Department Computer Science Department
Technische Universita?t Darmstadt Bar-Ilan University
Abstract
Our system combines text similarity measures
with a textual entailment system. In the main
task, we focused on the influence of lexical-
ized versus unlexicalized features, and how
they affect performance on unseen questions
and domains. We also participated in the pi-
lot partial entailment task, where our system
significantly outperforms a strong baseline.
1 Introduction
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment Challenge (Dzikovska
et al, 2013) brings together two important dimen-
sions of Natural Language Processing: real-world
applications and semantic inference technologies.
The challenge focuses on the domain of middle-
school quizzes, and attempts to emulate the metic-
ulous marking process that teachers do on a daily
basis. Given a question, a reference answer, and a
student?s answer, the task is to determine whether
the student answered correctly. While this is not
a new task in itself, the challenge focuses on em-
ploying textual entailment technologies as the back-
bone of this educational application. As a conse-
quence, we formalize the question ?Did the student
answer correctly?? as ?Can the reference answer be
inferred from the student?s answer??. This question
can (hopefully) be answered by a textual entailment
system (Dagan et al, 2009).
The challenge contains two tasks: In the main
task, the system must analyze each answer as a
whole. There are three settings, where each one de-
fines ?correct? in a different resolution. The highest-
resolution setting defines five different classes or
?correctness values?: correct, partially correct, con-
tradictory, irrelevant, non-domain. In the pilot task,
critical elements of the answer need to be analyzed
separately. Each such element is called a facet, and
is defined as a pair of words that are critical in an-
swering the question. As there is a substantial dif-
ference between the two tasks, we designed sibling
architectures for each task, and divide the main part
of the paper accordingly.
Our goal is to provide a robust architecture for stu-
dent response analysis, that can generalize and per-
form well in multiple domains. Moreover, we are
interested in evaluating how well general-purpose
technologies will perform in this setting. We there-
fore approach the challenge by combining two such
technologies: DKPro Similarity ?an extensive suite
of text similarity measures? that has been success-
fully applied in other settings like the SemEval 2012
task on semantic textual similarity (Ba?r et al, 2012a)
or reuse detection (Ba?r et al, 2012b).
BIUTEE, the Bar-Ilan University Textual Entail-
ment Engine (Stern and Dagan, 2011), which has
shown state-of-the-art performance on recognizing
textual entailment challenges. Our systems use both
technologies to extract features, and combine them
in a supervised model. Indeed, this approach works
relatively well (with respect to other entries in the
challenge), especially in unseen domains.
2 Background
2.1 Text Similarity
Text similarity is a bidirectional, continuous func-
tion which operates on pairs of texts of any length
and returns a numeric score of how similar one text
is to the other. In previous work (Mihalcea et al,
285
2006; Gabrilovich and Markovitch, 2007; Landauer
et al, 1998), only a single text similarity measure
has typically been applied to text pairs. However,
as recent work (Ba?r et al, 2012a; Ba?r et al, 2012b)
has shown, text similarity computation can be much
improved when a variety of measures are combined.
In recent years, UKP lab at TU Darmstadt has de-
veloped DKPro Similarity1, an open source toolkit
for analyzing text similarity. It is part of the
DKPro framework for natural language processing
(Gurevych et al, 2007). DKPro Similarity excels
at the tasks of measuring semantic textual simi-
larity (STS) and detecting text reuse (DTR), hav-
ing achieved the best performance in previous chal-
lenges (Ba?r et al, 2012a; Ba?r et al, 2012b).
2.2 Textual Entailment
The textual entailment paradigm is a generic frame-
work for applied semantic inference (Dagan et al,
2009). The most prevalent task of textual entailment
is to recognize whether the meaning of a target nat-
ural language statement (H for hypothesis) can be
inferred from another piece of text (T for text). Ap-
parently, this core task underlies semantic inference
in many text applications. The task of analyzing stu-
dent responses is one such example. By assigning
the student?s answer as T and the reference answer
as H , we are basically asking whether one can in-
fer the correct (reference) answer from the student?s
response. In recent years, Bar-Ilan University has
developed BIUTEE (Stern and Dagan, 2011), an ex-
tensive textual entailment recognition engine. BI-
UTEE tries to convert T (represented as a depen-
dency tree) to H . It does so by applying a series of
knowledge-based transformations, such as synonym
substitution, active-passive conversion, and more.
BIUTEE is publicly available as open source.2
3 Main Task
In this section, we explain how we approached the
main task, in which the system needs to analyze each
answer as a whole. After describing our system?s ar-
chitecture, we explain how we selected training data
for the different scenarios in the main task. We then
1code.google.com/p/dkpro-similarity-asl
2cs.biu.ac.il/?nlp/downloads/biutee
provide the details for each submitted run, and fi-
nally, our empirical results.
3.1 System Description
We build a system based on the Apache UIMA
framework (Ferrucci and Lally, 2004) and DKPro
Lab (Eckart de Castilho and Gurevych, 2011). We
use DKPro Core3 for preprocessing. Specifically,
we used the default DKPro segmenter, TreeTagger
POS tagger and chunker, Jazzy Spell Checker, and
the Stanford parser.4 We trained a supervised model
(Naive Bayes) using Weka (Hall et al, 2009) with
feature extraction based on clearTK (Ogren et al,
2008). The following features have been used:
BOW features Bag-of-word features are based on
the assumption that certain words need to appear in
a correct answer. We used a mixture of token uni-
grams, bigrams, and trigrams, where each n-gram is
a binary feature that can either be true or false for a
document.5 Additionally, we also used the number
of tokens in the student answer as another feature in
this group.
Syntactic Features We extend BOW features
with syntactic functions by adding dependency and
phrase n-grams. Dependency n-grams are combina-
tions of two tokens and their dependency relation.
Phrase n-grams are combinations of the main verb
and the noun phrase left and right of the verb. In
both cases, we use the 10 most frequent n-grams.
Basic Similarity Features This group of features
computes the similarity between the reference an-
swer and the student answer. In case there is more
than one reference answer, we compute all pairwise
similarity scores and add the minimum, maximum,
average, and median similarity.6
Semantic Similarity Features are very similar to
the basic similarity features, except that we use se-
mantic similarity measures in order to bridge a pos-
sible vocabulary gap between the student and refer-
ence answer. We use the ESA measure (Gabrilovich
3code.google.com/p/dkpro-core-asl/
4DKPro Core v1.4.0, TreeTagger models v20130204.0,
Stanford parser PCFG model v20120709.0.
5Using the 750 most frequent n-grams gave good results on
the training set, so we also used this number for the test runs.
6As basic similarity measures, we use greedy string tiling
(Wise, 1996) with n = 3, longest common subsequence and
longest common substring (Allison and Dix, 1986), and word
n-gram containment(Lyon et al, 2001) with n = 2.
286
and Markovitch, 2007) based on concept vectors
build from WordNet, Wiktionary, and Wikipedia.
Spelling Features As spelling errors might be in-
dicative of the answer quality, we use the number of
spelling errors normalized by the text length as an
additional feature.
Entailment Features We run BIUTEE (Stern and
Dagan, 2011) on the test instance (as T ) with each
reference answer (as H), which results in an array
of numerical entailment confidence values. If there
is more than one reference answer, we compute all
pairwise confidence scores and add the minimum,
maximum, average, and median confidence.
3.2 Data Selection Regime
There are three scenarios under which our system
is expected to perform. For each one, we chose (a-
priori) a different set of examples for training.
Unseen Answers Classify new answers to famil-
iar questions. Train on instances that have the same
question as the test instance.
Unseen Questions Classify new answers to un-
seen (but related) questions. Partition the questions
according to their IDs, creating sets of related ques-
tions, and then train on all the instances that share
the same partition as the test instance.
Unseen Domains Classify new answers to unseen
questions from unseen domains. Use all available
training data from the same dataset.
3.3 Submitted Runs
The runs represent the different levels of lexicaliza-
tion of the model which we expect to have strong
influence in each scenario:
Run 1 uses all features as described above. We
expect the BOW features to be helpful for the Un-
seen Answers setting, but to be misleading for un-
seen questions or domains, as the same word indi-
cating a correct answer for one question might cor-
respond to a wrong answer for another question.
Run 2 uses only non-lexicalized features, i.e. all
features except the BOW and syntactic features, in
order to assess the impact of the lexicalization that
overfits on the topic of the questions. We expect this
run to be less sensitive to the topic changes in the
Unseen Questions and Unseen Domains settings.
Run 3 uses only the basic similarity and the en-
tailment features. It should indicate the baseline per-
Unseen Unseen Unseen
Task Run Answers Questions Domains
2-way
1 .734 .678 .671
2 .665 .644 .677
3 .662 .625 .677
3-way
1 .670 .573 .572
2 .595 .561 .577
3 .574 .540 .576
5-way
1 .590 .376 .407
2 .495 .397 .371
3 .461 .394 .376
Table 1: Main task performance for the SciEntsBank test
set. We show weighted averageF1 for the three scenarios.
Cor. Par Con. Irr. Non.
Correct 903 463 164 309 78
Partially Correct 219 261 93 333 80
Contradictory 61 126 91 103 36
Irrelevant 209 229 119 476 189
Non-Domain 0 0 0 2 18
Table 2: Confusion matrix of Run 1 in the 5-way Unseen
Domains scenario. The vertical axis is the real class, the
horizontal axis is the predicted class.
formance that can be expected without targeting the
system towards a certain topic.
3.4 Empirical Results
Table 1 shows the F1-measure (weighted average)
of the three runs. As was expected for the Unseen
Answers scenario, Run 1 using a lexicalized model
outperformed the other two runs. However, in the
other scenarios Run 1 is not significantly better, as
lexicalized features do not have the same impact if
the question or the domain changes.
Table 2 shows the confusion matrix of Run 1 in
the 5-way Unseen Domains scenario. The Correct
category was classified quite reliably, but the Irrele-
vant category was especially hard. While the refer-
ence answer provides some clues for what is correct
or incorrect, the range of things that are ?irrelevant?
for a given question is potentially very big and thus
cannot be easily learned. We also see that the system
ability to distinguish Correct and Partially Correct
answers need to be improved.
It is difficult to provide an exact assessment of our
system?s performance (with respect to other systems
in the challenge), since there are multiple tasks, sce-
287
narios, datasets, and even metrics. However, we can
safely say that our system performed above average
in most settings, and showed competitive results in
the Unseen Domains scenario.
4 Pilot Task
In the pilot task each facet needs to be analyzed sep-
arately, which requires some changes in the system
architecture.
4.1 System Description
We segmented and lemmatized the input data using
the default DKPro segmenter and the TreeTagger
lemmatizer. The partial entailment system is com-
posed of three methods: Exact, WordNet, and BI-
UTEE. These were combined in different combina-
tions to form the different runs.
Exact In this baseline method, we represent a
student answer as a bag-of-words containing all to-
kens and lemmas appearing in the text. Lemmas
are used to account for minor morphological dif-
ferences, such as tense or plurality. We then check
whether both facet words appear in the set.
WordNet checks whether both facet words, or
their semantically related words, appear in the stu-
dent?s answer. We use WordNet (Fellbaum, 1998)
with the Resnik similarity measure (Resnik, 1995)
and count a facet term as matched if the similarity
score exceeds a certain threshold (0.9, empirically
determined on the training set).
BIUTEE processes dependency trees instead of
bags of words. We therefore added a pre-processing
stage that extracts a path in the dependency parse
that represents the facet. This is done by first pars-
ing the entire reference answer, and then locating the
two nodes mentioned in the facet. We then find their
lowest common ancestor (LCA), and extract the path
from the facet?s first word to the second through the
LCA. BIUTEE can now be given the student an-
swer and the pre-processed facet, and try to recog-
nize whether the former entails the latter.
4.2 Submitted Runs
In preliminary experiments using the provided train-
ing data, we found that the very simple Exact Match
baseline performed surprisingly well, with 0.96 pre-
cision and 0.32 recall on positive class instances (ex-
pressed facets). We therefore decided to use this fea-
Unseen Unseen Unseen
Answers Questions Domains
Baseline .670 .688 .731
Run 1 .756 .710 .760
Run 2 .782 .765 .816
Run 3 .744 .733 .770
Table 3: Pilot task performance across different scenar-
ios. The scores are F1-measures (weighted average).
ture as an initial filter, and employ the others for
classifying the ?harder? cases. Training BIUTEE
only on these cases, while dismissing easy ones, im-
proved our system?s performance significantly.
Run 1: Exact OR WordNet This is essentially
just the WordNet feature on its own, because every
instance that Exact classifies as positive is also pos-
itive by WordNet.
Run 2: Exact OR (BIUTEE AND WordNet) If
the instance is non-trivial, this configuration requires
that both BIUTEE and WordNet Match agree on pos-
itive classification. Equivalent to the majority rule.
Run 3: Exact OR BIUTEE BIUTEE increases
recall of expressed facets at the expense of precision.
4.3 Empirical Results
Table 3 shows the F1-measure (weighted average) of
each run in each scenario, including Exact Match as
a quite strong baseline. In the majority of cases, Run
2 that combines entailment and WordNet-based lex-
ical matching, significantly outperformed the other
two. It is interesting to note that the systems? perfor-
mance does not degrade in ?harder? scenarios; this is
a result of the non-lexicalized nature of our methods.
Unfortunately, our system was the only submission
in this track, so we do not have any means to perform
relative comparison.
5 Conclusion
We combined semantic textual similarity with tex-
tual entailment to solve the problem of student re-
sponse analysis. Though our features were not tai-
lored for this task, they proved quite indicative, and
adapted well to unseen domains. We believe that ad-
ditional generic features and knowledge resources
are the best way to improve on our results, while
retaining the same robustness and generality as our
current architecture.
288
Acknowledgements
This work has been supported by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant No.
I/82806, and by the European Community?s Seventh Frame-
work Programme (FP7/2007-2013) under grant agreement no.
287923 (EXCITEMENT). We would like to thank the Minerva
Foundation for facilitating this cooperation with a short term
research grant.
References
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305?310.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012a. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation and the 1st Joint Confer-
ence on Lexical and Computational Semantics, pages
435?440, June.
Daniel Ba?r, Torsten Zesch, and Iryna Gurevych. 2012b.
Text reuse detection using a composition of text sim-
ilarity measures. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING 2012), pages 167?184, December.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rationale,
evaluation and approaches. Natural Language Engi-
neering, 15(4):i?xvii.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Richard Eckart de Castilho and Iryna Gurevych. 2011.
A lightweight framework for reproducible parame-
ter sweeping in information retrieval. In Proceed-
ings of the 2011 workshop on Data infrastructurEs for
supporting information retrieval evaluation (DESIRE
?11), New York, NY, USA. ACM.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
David Ferrucci and Adam Lally. 2004. UIMA: An ar-
chitectural approach to unstructured information pro-
cessing in the corporate research environment. Natu-
ral Language Engineering, 10(3-4):327?348.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence (IJCAI 2007), pages 1606?1611.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt Knowledge Processing Repository
based on UIMA. In Proceedings of the 1st Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the Society for
Computational Linguistics and Language Technology,
Tu?bingen, Germany, April.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2&3):259?284.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in
large document collections. In Proceedings of the
6th Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2001), pages 118?125,
Pittsburgh, PA USA.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775?780, Boston, MA.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA Toolkit for Statistical Nat-
ural Language Processing. In Towards Enhanced
Interoperability for Large HLT Systems: UIMA for
NLP workshop at Language Resources and Evaluation
Conference (LREC).
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence (IJCAI 1995), pages 448?453.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Proceedings of the 8th International Conference
on Recent Advances in Natural Language Processing
(RANLP 2011), pages 455?462.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE Technical Sympo-
sium on Computer Science Education (SIGCSE 1996),
pages 130?134, Philadelphia, PA.
289
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87?97,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Focused Entailment Graphs for Open IE Propositions
Omer Levy
?
Ido Dagan
?
Jacob Goldberger
?
? Computer Science Department ? Faculty of Engineering
Bar-Ilan University
Ramat-Gan, Israel
{omerlevy,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
Abstract
Open IE methods extract structured propo-
sitions from text. However, these propo-
sitions are neither consolidated nor gen-
eralized, and querying them may lead
to insufficient or redundant information.
This work suggests an approach to or-
ganize open IE propositions using entail-
ment graphs. The entailment relation uni-
fies equivalent propositions and induces a
specific-to-general structure. We create a
large dataset of gold-standard proposition
entailment graphs, and provide a novel
algorithm for automatically constructing
them. Our analysis shows that predicate
entailment is extremely context-sensitive,
and that current lexical-semantic resources
do not capture many of the lexical infer-
ences induced by proposition entailment.
1 Introduction
Open information extraction (open IE) extracts
natural language propositions from text without
pre-defined schemas as in supervised relation ex-
traction (Etzioni et al., 2008). These proposi-
tions represent predicate-argument structures as
tuples of natural language strings. Open IE en-
ables knowledge search by aggregating billions of
propositions from the web
1
. It may also be per-
ceived as capturing an unsupervised knowledge
representation schema, complementing supervised
knowledge bases such as Freebase (Bollacker et
al., 2008), as suggested by Riedel et al (2013).
However, language variability obstructs open IE
from becoming a viable knowledge representation
framework. As it does not consolidate natural lan-
guage expressions, querying a database of open IE
propositions may lead to either insufficient or re-
dundant information. As an illustrative example,
1
See demo: openie.cs.washington.edu
querying the demo (footnote 1) for the generally
equivalent relieves headache or treats headache
returns two different lists of entities; out of the top
few results, the only answers these queries seem
to agree on are caffeine and sex. This is a major
drawback relative to supervised knowledge rep-
resentations, which map natural language expres-
sions to structured formal representations, such as
treatments in Freebase.
In this work, we investigate an approach for or-
ganizing and consolidating open IE propositions
using the novel notion of proposition entailment
graphs (see Figure 1) ? graphs in which each
node represents a proposition and each directed
edge reflects an entailment relation, in the spirit
of textual entailment (Dagan et al., 2013). En-
tailment provides an effective structure for ag-
gregating natural-language based information; it
merges semantically equivalent propositions into
cliques, and induces specification-generalization
edges between them. For example, (aspirin, elim-
inate, headache) entails, and is more specific than,
(headache, respond to, painkiller).
We thus propose the task of constructing an
entailment graph over a set of open IE proposi-
tions (Section 3), which is closely related to Be-
rant et al?s work (2012) who introduced predicate
entailment graphs. In contrast, our work explores
propositions, which are essentially predicates in-
stantiated with arguments, and thus semantically
richer. We provide a dataset of 30 such graphs,
which represent 1.5 million pairwise entailment
decisions between propositions (Section 4).
To approach this task, we extend the state-of-
the-art method for building entailment graphs (Be-
rant et al., 2012) from predicates to complete
propositions. Both Snow et al (2006) and Berant et
al used WordNet as distant supervision when train-
ing a local pairwise model of lexical entailment.
However, analyzing our data revealed that the lex-
ical inferences captured in WordNet are quite dif-
87
Figure 1: An excerpt from a proposition entailment graph focused on the topic headache. The dashed boundaries in the figure
denote cliques, meaning that all propositions within them are equivalent.
ferent from the real lexical inferences induced by
proposition entailment, making WordNet a mis-
leading form of supervision. We therefore employ
direct proposition-level supervision, and design a
probabilistic model that captures the underlying
lexical-component inferences (Section 5). We ex-
plore a variety of natural extensions to prior art as
baselines (Section 6) and show that our model out-
performs them (Section 7).
While our model increases performance on this
task, there is still much room for improvement. A
deeper analysis (Section 8) shows that common
lexical-semantic resources, on which we rely as
well, are either too noisy or provide inadequate re-
call regarding lexical entailment. In particular, we
find that predicate inference within propositions
often goes beyond inference between the predi-
cates? linguistic meanings. While pneumonia re-
quires antibiotics and pneumonia is treated by an-
tibiotics mean the same, the inherent meanings of
require and treat are different. These inferences
pertain to specific world knowledge, and warrant
future research.
Our work also contributes to textual entailment
research. First, we extend entailment graphs to
complete propositions. Secondly, we investigate
an intermediate problem of recognizing entail-
ment between language-based predicate-argument
tuples. Though this problem is simpler than
sentence-level entailment, it does capture entail-
ment of complete statements, which proves to be
quite challenging indeed.
2 Background
Our work builds upon two major research threads:
open IE, and entailment graphs.
2.1 Open Information Extraction
Research in open IE (Etzioni et al., 2008) has fo-
cused on transforming text to predicate-argument
tuples (propositions). The general approach is to
learn proposition extraction patterns, and use them
to create tuples while denoting extraction confi-
dence. Various methods differ in the type of pat-
terns they acquire. For instance, (Banko et al.,
2007) and (Fader et al., 2011) used surface pat-
terns, while (Mausam et al., 2012) and (Xu et al.,
2013) used syntactic dependencies.
Yates and Etzioni (2009) tried to mitigate the
issue of language variability (as exemplified in
the introduction) by clustering synonymous predi-
cates and arguments. While these clusters do con-
tain semantically related items, they do not neces-
sarily reflect equivalence or implication. For ex-
ample, coffee, tea, and caffeine may all appear
in one cluster, but coffee does not imply tea; on
the other hand, separating any element from this
cluster removes a valid implication. Entailment,
however, can capture the fact that both beverages
imply caffeine, but not one another. Also related,
Riedel et al (2013) try to generalize over open IE
extractions by combining knowledge from Free-
base and globally predicting which unobserved
propositions are true. In contrast, our work identi-
fies inference relations between concrete pairs of
observed propositions.
2.2 Entailment Graphs of Words and Phrases
Previous work focused on entailment graphs or
similar structures at the sub-propositional level.
In these graphs, each node represents a natu-
ral language word or phrase, and each directed
edge an entailment (or generalization) relation.
Snow et al (2006) created a taxonomy of sense-
88
disambiguated nouns and their hyponymy rela-
tions. Berant et al (2012) constructed entailment
graphs of predicate templates. Recently, Mehdad
et al (2013) built an entailment graph of noun
phrases and partial sentences for topic labeling.
The notion of proposition entailment graphs, how-
ever, is novel. This distinction is critical, be-
cause apparently, entailment in the context of spe-
cific propositions does not behave like context-
oblivious lexical entailment (see Section 8).
Berant et al?s work was implemented in Adler
et al?s (2012) text exploration demo, which instan-
tiated manually-annotated predicate entailment
graphs with arguments, and used an additional
lexical resource to determine argument entail-
ment. The combined graphs of predicate and argu-
ment entailments induced a proposition entailment
graph, which could then be explored in a faceted-
search scheme. Our work goes beyond, and at-
tempts to build entailment graphs of propositions
automatically.
2.2.1 Berant et al?s Algorithm for Predicate
Entailment Graph Construction
We present Berant et al?s algorithm in detail, as we
rely on it later on. Given a set of predicates {i}
1..n
as input (constituting the graph nodes), it returns
a set of entailment decisions (i, j), which become
the directed edges of the entailment graph. The
method works in two phases: (1) local estimation,
and (2) global optimization.
The local estimation model considers every po-
tential edge (i, j) and estimates the probability p
ij
that this edge indeed exists, i.e. that i entails j.
Each predicate pair is represented with distribu-
tional similarity features, providing some indica-
tion of whether i entails j. The estimator then uses
logistic regression (or a linear SVM) over those
features to predict the probability of entailment. It
is trained with distant supervision from WordNet,
employing synonyms, hypernyms, and (WordNet)
entailments as positive examples, and antonyms,
hyponyms, and cohyponyms as negative.
The global optimization phase then searches
for the most probable transitive entailment graph,
given the local probability estimations. It does so
with an integer linear program (ILP), where each
pair of predicates is represented by a binary vari-
able x
ij
, denoting whether there is an entailment
edge from i to j. The objective function corre-
sponds to the log likelihood of the assignment:
?
i 6=j
x
ij
(
log
(
p
ij
1?p
ij
)
+ log
(
pi
1?pi
))
. The prior
term pi is the probability of a random pair of pred-
icates to be in an entailment relation, and can be
estimated in advance. The ILP solver searches
for the optimal assignment that maximizes the ob-
jective function under transitivity constraints, ex-
pressed as linear constraints ?
i,j,k
x
ij
+ x
jk
?
x
ik
? 1.
3 Task Definition
A proposition entailment graph is a directed graph
where each node is a proposition s
i
(s for sen-
tence) and each edge (s
i
, s
j
) represents an en-
tailment relation from s
i
to s
j
. A proposi-
tion s
i
is a predicate-argument structure s
i
=
(
p
i
, a
1
i
, a
2
i
, ..., a
m
i
i
)
with one predicate p
i
and its
arguments. A proposition-level entailment (s
i
, s
j
)
holds if the verbalization of s
i
implies s
j
, accord-
ing to the definition of textual entailment (Dagan
et al., 2013); i.e. if humans reading s
i
would typi-
cally infer that s
j
is most likely true. Given a set of
propositions (graph nodes), the task of construct-
ing a proposition entailment graph is to recognize
all the entailments among the propositions, i.e.
deciding which directional edges connect which
pairs of nodes.
In this paper, we consider the narrower task
of constructing focused proposition entailment
graphs, following Berant et al?s methodology
in creating focused predicate entailment graphs.
First, all predicates are binary (have two argu-
ments) and are denoted s
i
=
(
a
1
i
, p
i
, a
2
i
)
. Sec-
ondly, we assume that the propositions were re-
trieved by querying for a particular concept; out
of the two arguments, one argument t (topic) is
common to all the propositions in a single graph.
We denote the non-topic argument as a
i
. Figure 1
presents an example of an informative entailment
graph focused on the topic headache.
Though confined, this setting still challenges
the state-of-the-art in textual entailment (see Sec-
tion 7). Moreover, these restrictions facilitate
piece-wise investigation of the entailment problem
(see Section 8).
4 Dataset
To construct our dataset of open IE extractions, we
found Google?s syntactic ngrams (Goldberg and
Orwant, 2013) as a useful source of high-quality
propositions. Based on a corpus of 3.5 million En-
glish books, it aggregates every syntactic ngram
89
? subtree of a dependency parse ? with at most
4 dependency arcs. The resource contains only
tree fragments that appeared at least 10 times in
the corpus, filtering out many low-quality syntac-
tic ngrams.
We extracted the syntactic ngrams that reflect
propositions, i.e. subject-verb-object fragments
where object modifies the verb with either dobj
or pobj. Prepositions in pobj were concatenated
to the verb (e.g. use with). In addition, both sub-
ject and object must each be a noun phrase con-
taining two tokens at most, which are either nouns
or adjectives. Each token in the extracted frag-
ments was then lemmatized using WordNet. After
lemmatization, we grouped all identical proposi-
tions and aggregated their counts. Approximately
68 million propositions were collected.
We chose 30 topics from the healthcare domain
(such as influenza, hiv, and penicillin). For each
topic, we collected the set of propositions con-
taining it, and manually filtered noisy extractions.
This yielded 30 high-quality sets of 5,714 propo-
sitions in total, where each set becomes the set of
nodes in a separate focused entailment graph. The
graphs range from 55 propositions (scurvy) to 562
(headache), with an average of over 190 proposi-
tions per graph. Summing the number of propo-
sition pairs within each graph amounts to a total
of 1.5 million potential entailment edges, which
makes it by far the largest annotated textual entail-
ment dataset to date.
We used a semi-automatic annotation process,
which dramatically narrows down the number of
manual decisions, and hence, the required anno-
tation time. In short, the annotators are given a
series of small clustering tasks before annotating
entailment between those clusters.
2
The annotation process was carried out by two
native English speakers, with the aid of encyclope-
dic knowledge for unfamiliar medical terms. The
agreement on a subset of five randomly sampled
graphs was ? = 0.77. Annotating a single graph
took about an hour and a half on average.
Positive entailment judgements constituted only
8.4% of potential edges, and were found to be
100% transitive. We observe that in nearly all of
those cases, a natural alignment between entail-
ing components occurs: predicates align with each
other, the topic is shared, and the remaining non-
2
The annotated dataset is publicly available on the first
author?s website.
topic argument aligns with its counterpart. Con-
sider the topic arthritis and the entailing proposi-
tion pair (arthritis, cause, pain)?(symptom, as-
sociate with, arthritis); cause?associate with,
while pain?symptom. Rarely, some mis-
alignments do occur; for instance (vaccine,
protects, body)?(vaccine, provides, protection).
However, it is almost always the case that proposi-
tions entail if and only if their aligned lexical com-
ponents entail as well.
5 Algorithm
In this section, we extend Berant et al?s algorithm
(2012) to construct entailment graphs of proposi-
tions. As described in Section 2.2.1, their method
first performs local estimation of predicate entail-
ment and then global optimization. We modify the
local estimation phase to estimate proposition en-
tailment instead, and then apply the same global
optimization in the second phase.
In Section 4, we observed the alignment-based
relationship between proposition and lexical en-
tailment. We leverage this observation to predict
proposition entailment with lexical entailment fea-
tures (as Berant et al), using the Component En-
tailment Conjunction (CEC) model in Section 5.1.
Following Snow et al (2006) and Berant et
al, we could train CEC using distant supervision
from WordNet. In fact, we did try this approach
(presented as baseline methods, Section 6) and
found that it performed poorly. Furthermore, our
analysis (Section 8) suggests that WordNet rela-
tions do not adequately capture the lexical infer-
ences induced by proposition-level entailment. In-
stead, we use a more realistic signal to train CEC ?
direct supervision from the annotated dataset. Sec-
tion 5.2 describes how we propagate proposition-
level entailment annotations to the latent lexical
components.
5.1 Component Entailment Conjunction
CEC assumes that proposition-level entailment
is the result of entailment within each pair of
aligned components, i.e. a pair of propositions
entail if and only if both their predicate and ar-
gument pairs entail. This assumption stems from
our observation of alignment in Section 4. Fur-
thermore, CEC leverages this interdependence to
learn separate predicate-entailment and argument-
entailment features through proposition-level su-
pervision.
90
Formally, for every ordered pair of propositions
(i, j) we denote proposition entailment as a binary
random variable x
s
ij
and predicate and argument
entailments as x
p
ij
and x
a
ij
, respectively. In our
setting, proposition entailment (x
s
ij
) is observed,
but component entailments (x
p
ij
, x
a
ij
) are hidden.
We use logistic regression, with features ?
p
ij
and
parameter w
p
, as a probabilistic model of predi-
cate entailment (and so for arguments with ?
a
ij
and
w
a
):
p
ij
= P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
)
= ?
(
?
p
ij
? w
p
)
a
ij
= P
(
x
a
ij
= 1
?
?
?
?
a
ij
;w
a
)
= ?
(
?
a
ij
? w
a
)
(1)
where ? is the sigmoid ? (z) =
1
1+e
?z
. We then
define proposition entailment as the conjunction of
its binary components: x
s
ij
= x
p
ij
?x
a
ij
. Therefore,
the probability of proposition entailment given the
component features is:
s
ij
= P
(
x
s
ij
= 1
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
= P
(
x
p
ij
= 1, x
a
ij
= 1
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
= P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
)
? P
(
x
a
ij
= 1
?
?
?
?
a
ij
;w
a
)
= p
ij
? a
ij
The proposition entailment probability is thus the
product of component entailment probabilities.
Given the proposition-level information
{
x
s
ij
}
,
the log-likelihood is:
` (w
p
, w
a
)=
?
i 6=j
logP
(
x
s
ij
?
?
?
?
p
ij
, ?
a
ij
;w
p
, w
a
)
=
?
i 6=j
(
x
s
ij
log (p
ij
a
ij
) +
(
1? x
s
ij
)
log (1? p
ij
a
ij
)
)
5.2 Learning Component Models
We wish to learn the model?s parameters (w
p
, w
a
).
Our approach uses direct proposition-level super-
vision from our annotated dataset to train the com-
ponent logistic regression models. Since compo-
nent entailment (x
p
ij
, x
a
ij
) is not observed in the
data, we apply the iterative EM algorithm (Demp-
ster et al., 1977). In the E-step we estimate their
probabilities from proposition-level labels (x
s
ij
),
and in the M-step we use those estimates as ?soft?
labels to learn the component-level model param-
eters (w
p
, w
a
).
E-Step During the E-step in iteration t + 1,
we compute the probability of component entail-
ments given the proposition entailment informa-
tion, based on the parameters at iteration t (w
p
t
,
w
a
t
). The predicate probabilities are given by:
c
p
ij
= P
(
x
p
ij
= 1
?
?
?
x
s
ij
, ?
p
ij
, ?
a
ij
;w
p
t
, w
a
t
)
(2)
and are computed with Bayes? law:
c
p
ij
=
?
?
?
1 if x
s
ij
= 1
p
t
ij
(
1?a
t
ij
)
1?p
t
ij
a
t
ij
if x
s
ij
= 0
(3)
where p
t
ij
is computed as in Equations 1, with the
parameters at iteration t (w
p
t
). Argument entail-
ment probabilities (c
a
ij
) are computed analogously.
M-Step In the M-step, we compute new values
for the parameters (w
p
t+1
, w
a
t+1
). In our case, there
is no closed-form formula for updating the param-
eters. Instead, at each iteration, we solve a sepa-
rate logistic regression for each component. While
we have each component model?s features (?
p
ij
,
assuming predicates for notation), we do not ob-
serve the component-level entailment labels (x
p
ij
);
instead, we obtain their probabilities (c
p
ij
) from the
expectation step.
To learn the parameters (w
p
t+1
, w
a
t+1
) from the
component entailment probabilities (c
p
ij
), we em-
ploy a weighted variant of logistic regression, that
can utilize ?soft? class labels (i.e. a probability
distribution over {0, 1}). To solve such a logistic
regression (e.g. for w
p
t+1
), we maximize the log-
likelihood:
`
(
w
p
t+1
)
=
?
ij
(
c
p
ij
log
(
P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
t+1
))
+
(
1? c
p
ij
)
log
(
P
(
x
p
ij
= 0
?
?
?
?
p
ij
;w
p
t+1
)))
For optimization, we calculate the derivative, and
use gradient ascent to update w
p
t+1
:
?w
p
t+1
=
?`
(
w
p
t+1
)
?w
p
t+1
=
?
ij
(
c
p
ij
? P
(
x
p
ij
= 1
?
?
?
?
p
ij
;w
p
t+1
))
?
p
ij
This optimization is concave, and therefore the
unique global maximum can be efficiently ob-
tained.
5.3 Features
Similar to Berant et al, we used three types of fea-
tures to describe both predicate pairs (?
p
ij
) and ar-
gument pairs (?
a
ij
): distributional similarities, lex-
ical resources, and string distances.
91
We used the entire database of 68 million ex-
tracted propositions (see Section 4) to create a
word-context matrix; context was defined as other
words that appeared in the same proposition, and
each word was represented as (string, role), role
being the location within the proposition, either
a
1
, p, or a
2
. The matrix was then normalized with
pointwise mutual information (Church and Hanks,
1990). We used various metrics to measure dif-
ferent types of similarities between each compo-
nent pair, including: cosine similarity, Lin?s sim-
ilarity (1998), inclusion (Weeds and Weir, 2003),
average precision, and balanced average precision
(Kotlerman et al., 2010). Weed?s and Kotlerman?s
metrics are directional (asymmetric) and indicate
the direction of a potential entailment relation.
These features were used for both predicates and
arguments. In addition, we used Melamud et al?s
(2013) method to learn a context-sensitive model
of predicate entailment, which estimates predicate
similarity in the context of the given arguments.
We leveraged the Unified Medical Language
System (UMLS) to check argument entailment,
using the parent and synonym relations. A single
feature indicated whether such a connection ex-
ists. We also used WordNet relations as features,
specifically: synonyms, hypernyms, entailments,
hyponyms, cohyponyms, antonyms. Each Word-
Net relation constituted a different feature for both
predicates and arguments.
Finally, we added a string equality feature and a
Levenshtein distance feature (Levenshtein, 1966)
for different spellings of the same word to both
predicate and argument feature vectors.
6 Baseline Methods
We consider four algorithms that naturally ex-
tend the state-of-the-art to propositions, while us-
ing distant supervision (from WordNet). Since
CEC uses direct supervision, we also examined
another (simpler) directly-supervised algorithm.
As a naive unsupervised baseline, we use Argu-
ment Equality, which returns ?entailing? if the ar-
gument pair is identical. Predicate Equality is de-
fined similarly for predicates.
Component-Level Distant Supervision The
following methods use distant supervision from
WordNet (as in Berant et al?s work, Section 2.2.1)
to explicitly train component-level entailment esti-
mators. Specifically, we train a logistic regression
model for each component as specified in Equa-
tions 1 in Section 5.1. We present four methods,
which differ in the way they obtain global graph-
level entailment decisions for propositions, based
on the local component entailment estimates (p
ij
,
a
ij
in Section 5.1).
The first method, Opt(Arg ? Pred), uses the
product of both component models to estimate lo-
cal proposition-level entailment: s
ij
= p
ij
? a
ij
.
The global set of proposition entailments is then
determined using Berant et al?s global optimiza-
tion, according to the proposition-level scores s
ij
.
Note that this method is identical to CEC dur-
ing inference, but differs in the way the local es-
timators are learned (with component-level super-
vision from WordNet).
An alternative is Opt(Arg) ? Opt(Pred). It
first obtains local probabilities (p
ij
, a
ij
) for each
component as in Opt(Arg ? Pred), but then em-
ploys component-level global optimization (tran-
sitivity enforcement), yielding two sets of entail-
ment decisions, x
p
ij
and x
a
ij
. Proposition entail-
ment is then determined by the conjunction x
s
ij
=
x
p
ij
? x
a
ij
, as in (Adler et al., 2012).
Finally, Opt(Arg) ignores the predicate com-
ponent. Instead, it uses only the argument en-
tailment graph (as produced by Opt(Arg) ?
Opt(Pred)) to decide on proposition entailment;
i.e. a pair of propositions entail if and only if their
arguments entail. Opt(Pred) is defined analo-
gously.
Proposition-Level Direct Supervision A sim-
pler alternative to CEC that also employs
proposition-level supervision is Joint Features,
which concatenates the component level features
into a unified feature vector: ?
s
ij
= ?
p
ij
? ?
a
ij
. We
then couple them with the gold-standard annota-
tions x
s
ij
to create a training set for a single logistic
regression. We use the trained logistic regression
to estimate the local probability of proposition en-
tailment, and then perform global optimization to
construct the entailment graph.
7 Empirical Evaluation
We evaluate the models in Sections 5 & 6 on the
30 annotated entailment graphs presented in Sec-
tion 4. During testing, each graph was evaluated
separately. The results presented in this section
are all micro-averages, though macro-averages
were also computed and found to reflect the same
trends. Models trained with distant supervision
were evaluated on all graphs. For directly super-
92
vised methods, we used 2 ? 6-fold cross valida-
tion (25 training graphs per fold). In this scenario,
each graph induced a set of labeled examples ?
its edges being positive examples, and the miss-
ing potential edges being negative ones ? and the
union of these sets was used as the training set of
that cross-validation fold.
7.1 Results
Table 1 compares the performance of CEC with
that of the baseline methods.
While Joint Features and CEC share exactly the
same features, CEC exploits the inherent conjunc-
tion between predicate and argument entailments
(as observed in Section 4 and modeled in Sec-
tion 5.1), and forces both components to decide on
entailment separately. This differs from the sim-
pler log-linear model (Joint Features) where, for
example, a very strong predicate entailment fea-
ture might override the overall proposition-level
decision, even if there was no strong indication
of argument entailment. As a result, CEC dom-
inates Joint Features in both precision and recall.
The F
1
difference between these methods is sta-
tistically significant with McNemar?s test (1947)
with p  0.01. Specifically, CEC corrected Joint
Features 7621 times, while the opposite occurred
only 4048 times.
CEC also yields relatively high precision
and recall. While it has 2% less recall than
Opt(Arg) (the highest-recall baseline), it sur-
passes Opt(Arg)?s precision by 14%. Along with
a similar comparison to Argument Equality (the
highest precision baseline), CEC notably outper-
forms all baselines.
It is also evident that both directly super-
vised methods outperform the distantly super-
vised methods. Our analysis (Section 8.1) shows
that WordNet lacks significant coverage, and may
therefore be a problematic source of supervision.
Perhaps the most surprising result is the com-
plete failure of WordNet-supervised methods that
consider predicate information. A deeper analy-
sis (Section 8.2) shows that predicate inference is
highly context-sensitive, and deviates beyond the
lexical inferences provided by WordNet.
7.2 Learning Curve
We measure the supervision needed to train the di-
rectly supervised models by their learning curves
(Figure 2). Each point is the average F
1
score
Supervision Method Prec. Rec. F1
None
Argument
81.6% 42.2% 55.6%
Equality
Predicate
9.3% 1.5% 2.6%
Equality
Component
(WordNet)
Opt(Arg
73.8% 3.8% 7.2%
? Pred)
Opt(Arg) ?
72.3% 3.2% 6.0%
Opt(Pred)
Opt(Arg) 64.6% 55.4% 59.7%
Opt(Pred) 11.0% 6.2% 8.0%
Proposition
(Annotated)
Joint
76.3% 51.7% 61.6%
Features
CEC 78.7% 53.5% 63.7%
Table 1: Performance on gold-standard (micro averaged).
Figure 2: Learning curve of directly supervised methods.
across 12 cross-validation folds; e.g. for 10 train-
ing graphs, we used 4 ? 3-fold cross validation.
Even 5 training graphs (a day?s worth of annota-
tion) are enough for CEC to perform on-par with
the best distantly supervised method, and with 15
training graphs it outperforms every baseline, in-
cluding Joint Features trained with 25 graphs.
7.3 Effects of Global Optimization
We evaluate the effects of enforcing transitivity by
considering CEC with and without the global op-
timization phase. Table 2 shows how many entail-
ment edges were added (and removed) by enforc-
ing transitivity, and measures how many of those
modifications were correct. Apparently, transi-
tivity?s greatest effect is the removal of incorrect
entailment edges. The same phenomenon was
also observed in the work on predicate entailment
graphs (Berant et al., 2012). Overall, transitivity
made 4,848 correct modifications out of 6,734 in
total. A ?
2
test reveals that the positive contribu-
tion of enforcing transitivity is indeed statistically
significant (p 0.01).
93
Gold Global Opt Global Opt
Standard Added Edge Removed Edge
Edge Exists 1150 482
No Edge 1404 3698
Table 2: The modifications made by enforcing transitivity
w.r.t. the gold standard. 55% of the edges added by enforcing
transitivity are incorrect, but it removed even more incorrect
edges, improving the overall performance.
8 Analysis of Lexical Inference
Although CEC had a statistically-significant im-
provement upon the baselines, its absolute perfor-
mance leaves much room for improvement. We
hypothesize that the lexical entailment features we
used, following state-of-the-art lexical entailment
modeling, do not capture many of the actual lexi-
cal inferences induced by proposition entailment.
We demonstrate that this is indeed the case.
8.1 Argument Entailment
To isolate the effect of different features on pre-
dicting argument entailment, we collected all
proposition pairs that shared exactly the same
predicate and topic, and thus differed in only their
?free? argument. This yielded 20,336 aligned ar-
gument pairs, whose entailment annotations are
equal to the corresponding proposition-entailment
annotation in the dataset.
Using WordNet synonyms and hypernyms to
predict entailment yielded a precision of about
88%, at 40% recall. Though relatively precise,
WordNet?s coverage is limited, and misses many
inferences. We describe three typical types of in-
ferences that were absent from WordNet.
The first type constitutes of widely-
used paraphrases such as people?persons,
woman?female, and pain?ache. These may be
seen as weaker types of synonyms, which may
have nuances, but are typically interchangeable.
Another type is metonymy, in which a concept
is not referred to by its own name, but by that of
an associated concept. This is very common in
our healthcare dataset, where a disease is often re-
ferred to by its underlying pathogen and vice-versa
(e.g. pneumonia?pneumococcus).
The third type of missing inferences is causal-
ity. Many instances of metonymy (such as the
disease-pathogen example) may be seen as causal-
ity as well. Other examples can be drug and ef-
fect (laxative?diarrhea) or condition and symp-
tom (influenza?fever).
WordNet?s lack of such common-sense infer-
ences, which are abundant in our proposition en-
tailment dataset, might make WordNet a problem-
atic source of distant supervision. The fact that
60% of the entailing examples in our dataset are
labeled by WordNet as non-entailing, means that
for each truly positive training example, there is a
higher chance that it will have a negative label.
Distributional similarity is commonly used to
capture such missing inferences and complement
WordNet-like resources. On this dataset, how-
ever, it failed to do so. One of the more in-
dicative similarity measures, inclusion (Weeds and
Weir, 2003), yielded only 27% precision at 40%
recall when tuning a threshold to optimize F
1
. In-
creasing precision caused a dramatic drop in re-
call: 50% precision limited recall to 3.2%. Other
similarity measures performed similarly or worse.
It seems that current methods of distributional
word similarity also capture relations quite differ-
ent from inference, such as cohyponyms and do-
main relatedness, and might be less suitable for
modeling lexical entailment on their own.
8.2 Context-Sensitive Predicate Entailment
The proposition-level entailment annotation in-
duces an entailment relation between the predi-
cates, which holds in the particular context of the
proposition pair. We wish to understand the na-
ture of this predicate-level entailment, and how it
compares to classic lexical inference as portrayed
in the lexical semantics literature. To that end, we
collected all the entailing proposition pairs with
equal arguments, and extracted the corresponding
predicate pairs (which, assuming alignment, are
necessarily entailing in that context). This list con-
tains 52,560 predicate pairs.
In our first analysis, we explored which Word-
Net relations correlate with predicate entailment,
by checking how well each relation covers the set
of entailed predicate pairs. Synonyms and hyper-
nyms, which are considered positive entailment
indicators, covered only about 8% each. Sur-
prisingly, the hyponym and cohyponym relations
(which are considered negative entailment indica-
tors) covered over 9% and 14%, respectively. Ta-
ble 3 shows the exact details.
It seems that WordNet relations are hardly cor-
related with the context-sensitive predicate-level
entailments in our dataset, and that the classic in-
terpretation of WordNet relations with respect to
entailment does not hold in practice, where en-
94
Interpretation WordNet Relation Coverage
Positive
Synonyms 7.85%
Direct Hypernyms 5.62%
Indirect Hypernyms 3.14%
Entailment 0.33%
Negative
Antonyms 0.31%
Direct Hyponyms 5.74%
Indirect Hyponyms 3.51%
Cohyponyms 14.30%
Table 3: The portion of positive predicate entailments cov-
ered by each WordNet relation. WordNet relations are di-
vided according to their common interpretations with respect
to lexical entailment.
tailments are judged in the context of concrete
propositions. In fact, negative indicators in Word-
Net seem to cover more predicate entailments
than positive ones. This explains the failure of
WordNet-supervised methods with predicate en-
tailment features (Section 7.1).
Since we do not expect WordNet to cover all
shades of entailment, we conducted a manual anal-
ysis as well. 100 entailing predicate pairs were
randomly sampled, and manually annotated for
lexical-level entailment, without seeing their argu-
ments. To compensate for the lack of context, we
guided the annotators to assume a general health-
care scenario, and use a more lenient interpretation
of textual entailment (biased towards positive en-
tailment decisions). Nevertheless, only 56% of the
predicate pairs were labeled as entailing, indicat-
ing that the context-sensitive predicate inferences
captured in our dataset can be quite different from
generic predicate inferences.
We suggest that this phenomenon goes one step
beyond what the current literature considers as
context-sensitive entailment, and that it is more
specific than determining an appropriate lexical
sense. To demonstrate, we present four such
predicate-entailment phenomena.
First, there are cases in which an appropriate
lexical sense could exist in principle, but it is too
specific to be practically covered by a manual re-
source. For example, cures cancer?kills cancer,
but the appropriate sense for kill (cause to cease
existing) does not exist, and in turn, neither does
the hypernymy relation from cure to kill. It is hard
to expect these kinds of obscure senses or relation-
ships to comprehensively appear in a manually-
constructed resource.
In many cases, such a specific sense does not
exist. For example, (pneumonia, require, antibi-
otic)?(pneumonia, treated by, antibiotics), but re-
quire does not have a general sense which means
treat by. The inference in this example does not
stem from the linguistic meaning of each predi-
cate, but rather from the real-world situation their
encapsulating propositions describe.
Another aspect of predicate entailment that
may change when considering propositional con-
text is the direction of inference. For instance,
cause9trigger. While it may be the case that trig-
ger entails cause, the converse is not necessarily
true since cause is far more general. However,
when considering (caffeine, cause, headache) and
(caffeine, trigger, headache), both propositions de-
scribe the same real-world situation, and thus both
propositions are mutually entailing. In this con-
text, cause does indeed entail trigger as well.
Finally, figures of speech (such as metaphors)
are abundant and diverse. Though it may not be
so common to read about a drug that ?banishes?
headaches, most readers would understand the un-
derlying meaning. These phenomena exceed the
current scope of lexical-semantic resources such
as WordNet, and require world knowledge.
9 Conclusion
This paper proposes a novel approach, based on
entailment graphs, for consolidating information
extracted from large corpora. We define the prob-
lem of building proposition entailment graphs, and
provide a large annotated dataset. We also present
the CEC model, which models the connection be-
tween proposition entailment and lexical entail-
ment. Although it outperforms the state-of-the-
art, its performance is not ideal because it relies on
inadequate lexical-semantic resources that do not
capture the common-sense and context-sensitive
inferences which are inherent in proposition en-
tailment. In future work, we intend to further in-
vestigate lexical entailment as induced by proposi-
tion entailment, and hope to develop richer meth-
ods of lexical inference that address the phenom-
ena exhibited in this setting.
Acknowledgements
This work has been supported by the Israeli Min-
istry of Science and Technology grant 3-8705, the
Israel Science Foundation grant 880/12, and the
European Communitys Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT). We would like to
thank our reviewers for their insightful comments.
95
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of the
System Demonstrations of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2012), pages 79?84.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, volume 7, pages 2670?2676.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies, 6(4):1?220.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), pages 1?
38.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the Web. Communications of the ACM,
51(12):68?74.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535?1545, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 1: Proceedings of the Main Conference and
the Shared Task: Semantic Textual Similarity, pages
241?247, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet Physics Doklady, volume 10, page 707.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Volume 2, pages 768?774,
Montreal, Quebec, Canada, August. Association for
Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 523?534, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157.
Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng,
and Shafiq Joty. 2013. Towards topic labeling
with phrase entailment and aggregation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
179?189, Atlanta, Georgia, June. Association for
Computational Linguistics.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1331?1340, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 74?84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Com-
putational Linguistics (ACL-COLING 2006), pages
801?808.
Julie Weeds and David Weir. 2003. A general
framework for distributional similarity. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81?88.
96
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 868?877, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255.
97
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171?180,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Linguistic Regularities in Sparse and Explicit Word Representations
Omer Levy
?
and Yoav Goldberg
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
{omerlevy,yoav.goldberg}@gmail.com
Abstract
Recent work has shown that neural-
embedded word representations capture
many relational similarities, which can be
recovered by means of vector arithmetic
in the embedded space. We show that
Mikolov et al.?s method of first adding
and subtracting word vectors, and then
searching for a word similar to the re-
sult, is equivalent to searching for a word
that maximizes a linear combination of
three pairwise word similarities. Based on
this observation, we suggest an improved
method of recovering relational similar-
ities, improving the state-of-the-art re-
sults on two recent word-analogy datasets.
Moreover, we demonstrate that analogy
recovery is not restricted to neural word
embeddings, and that a similar amount
of relational similarities can be recovered
from traditional distributional word repre-
sentations.
1 Introduction
Deep learning methods for language processing
owe much of their success to neural network lan-
guage models, in which words are represented as
dense real-valued vectors in R
d
. Such representa-
tions are referred to as distributed word represen-
tations or word embeddings, as they embed an en-
tire vocabulary into a relatively low-dimensional
linear space, whose dimensions are latent contin-
uous features. The embedded word vectors are
trained over large collections of text using vari-
ants of neural networks (Bengio et al., 2003; Col-
lobert and Weston, 2008; Mnih and Hinton, 2008;
Mikolov et al., 2011; Mikolov et al., 2013b). The
?
Supported by the European Community?s Seventh
Framework Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
word embeddings are designed to capture what
Turney (2006) calls attributional similarities be-
tween vocabulary items: words that appear in sim-
ilar contexts will be close to each other in the
projected space. The effect is grouping of words
that share semantic (?dog cat cow?, ?eat devour?)
or syntactic (?cars hats days?, ?emptied carried
danced?) properties, and are shown to be effective
as features for various NLP tasks (Turian et al.,
2010; Collobert et al., 2011; Socher et al., 2011;
Al-Rfou et al., 2013). We refer to such word rep-
resentations as neural embeddings or just embed-
dings.
Recently, Mikolov et al. (2013c) demonstrated
that the embeddings created by a recursive neu-
ral network (RNN) encode not only attributional
similarities between words, but also similarities
between pairs of words. Such similarities are
referred to as linguistic regularities by Mikolov
et al. and as relational similarities by Turney
(2006). They capture, for example, the gen-
der relation exhibited by the pairs ?man:woman?,
?king:queen?, the language-spoken-in relation in
?france:french?, ?mexico:spanish? and the past-
tense relation in ?capture:captured?, ?go:went?.
Remarkably, Mikolov et al. showed that such rela-
tions are reflected in vector offsets between word
pairs (apples ? apple ? cars ? car), and
that by using simple vector arithmetic one could
apply the relation and solve analogy questions of
the form ?a is to a
?
as b is to ?? in which the
nature of the relation is hidden. Perhaps the most
famous example is that the embedded representa-
tion of the word queen can be roughly recovered
from the representations of king, man and woman:
queen ? king ?man+ woman
The recovery of relational similarities using vector
arithmetic on RNN-embedded vectors was evalu-
ated on many relations, achieving state-of-the-art
results in relational similarity identification tasks
171
(Mikolov et al., 2013c; Zhila et al., 2013). It was
later demonstrated that relational similarities can
be recovered in a similar fashion also from embed-
dings trained with different architectures (Mikolov
et al., 2013a; Mikolov et al., 2013b).
This fascinating result raises a question: to what
extent are the relational semantic properties a re-
sult of the embedding process? Experiments in
(Mikolov et al., 2013c) show that the RNN-based
embeddings are superior to other dense represen-
tations, but how crucial is it for a representation to
be dense and low-dimensional at all?
An alternative approach to representing words
as vectors is the distributional similarity repre-
sentation, or bag of contexts. In this representa-
tion, each word is associated with a very high-
dimensional but sparse vector capturing the con-
texts in which the word occurs. We call such vec-
tor representations explicit, as each dimension di-
rectly corresponds to a particular context. These
explicit vector-space representations have been
extensively studied in the NLP literature (see (Tur-
ney and Pantel, 2010; Baroni and Lenci, 2010) and
the references therein), and are known to exhibit
a large extent of attributional similarity (Pereira
et al., 1993; Lin, 1998; Lin and Pantel, 2001;
Sahlgren, 2006; Kotlerman et al., 2010).
In this study, we show that similarly to the
neural embedding space, the explicit vector space
also encodes a vast amount of relational similar-
ity which can be recovered in a similar fashion,
suggesting the explicit vector space representation
as a competitive baseline for further work on neu-
ral embeddings. Moreover, this result implies that
the neural embedding process is not discovering
novel patterns, but rather is doing a remarkable
job at preserving the patterns inherent in the word-
context co-occurrence matrix.
A key insight of this work is that the vector
arithmetic method can be decomposed into a linear
combination of three pairwise similarities (Section
3). While mathematically equivalent, we find that
thinking about the method in terms of the decom-
posed formulation is much less puzzling, and pro-
vides a better intuition on why we would expect
the method to perform well on the analogy re-
covery task. Furthermore, the decomposed form
leads us to suggest a modified optimization objec-
tive (Section 6), which outperforms the state-of-
the-art at recovering relational similarities under
both representations.
2 Explicit Vector Space Representation
We adopt the traditional word representation used
in the distributional similarity literature (Turney
and Pantel, 2010). Each word is associated with
a sparse vector capturing the contexts in which it
occurs. We call this representation explicit, as each
dimension corresponds to a particular context.
For a vocabulary V and a set of contexts C,
the result is a |V |?|C| sparse matrix S in which
S
ij
corresponds to the strength of the association
between word i and context j. The association
strength between a word w ? V and a context
c ? C can take many forms. We chose to use
the popular positive pointwise mutual information
(PPMI) metric:
S
ij
= PPMI(w
i
, c
j
)
PPMI(w, c) =
{
0 PMI(w, c) < 0
PMI(w, c) otherwise
PMI(w, c) = log
P (w,c)
P (w)P (c)
= log
freq(w,c)|corpus|
freq(w)freq(c)
where |corpus| is the number of items in the cor-
pus, freq(w, c) is the number of times word w
appeared in context c in the corpus, and freq(w),
freq(c) are the corpus frequencies of the word
and the context respectively.
The use of PMI in distributional similarity mod-
els was introduced by Church and Hanks (1990)
and widely adopted (Dagan et al., 1994; Turney,
2001). The PPMI variant dates back to at least
(Niwa and Nitta, 1994), and was demonstrated to
perform very well in Bullinaria and Levy (2007).
In this work, we take the linear contexts in
which words appear. We consider each word sur-
rounding the target word w in a window of 2 to
each side as a context, distinguishing between dif-
ferent sequential positions. For example, in the
sentence a b c d e the contexts of the word c
are a
?2
, b
?1
, d
+1
and e
+2
. Each vector?s dimen-
stion is thus |C| ? 4 |V |. Empirically, the num-
ber of non-zero dimensions for vocabulary items
in our corpus ranges between 3 (for some rare to-
kens) and 474,234 (for the word ?and?), with a
mean of 1595 and a median of 415.
Another popular choice of context is the syntac-
tic relations the word participates in (Lin, 1998;
Pad?o and Lapata, 2007; Levy and Goldberg,
2014). In this paper, we chose the sequential
context as it is compatible with the information
available to the state-of-the-art neural embedding
method we are comparing against.
172
3 Analogies and Vector Arithmetic
Mikolov et al. demonstrated that vector space rep-
resentations encode various relational similarities,
which can be recovered using vector arithmetic
and used to solve word-analogy tasks.
3.1 Analogy Questions
In a word-analogy task we are given two pairs of
words that share a relation (e.g. ?man:woman?,
?king:queen?). The identity of the fourth word
(?queen?) is hidden, and we need to infer it based
on the other three (e.g. answering the question:
?man is to woman as king is to ? ??). In the rest
of this paper, we will refer to the four words as
a:a
?
, b:b
?
. Note that the type of the relation is
not explicitly provided in the question, and solv-
ing the question correctly (by a human) involves
first inferring the relation, and then applying it to
the third word (b).
3.2 Vector Arithmetic
Mikolov et al. showed that relations between
words are reflected to a large extent in the
offsets between their vector embeddings
(queen ? king ? woman ? man),
and thus the vector of the hidden word b
?
will be
similar to the vector b ? a + a
?
, suggesting that
the analogy question can be solved by optimizing:
argmax
b
?
?V
(sim (b
?
, b? a+ a
?
))
where V is the vocabulary excluding the question
words b, a and a
?
, and sim is a similarity mea-
sure. Specifically, they used the cosine similarity
measure, defined as:
cos (u, v) =
u ? v
?u??v?
resulting in:
argmax
b
?
?V
(cos (b
?
, b? a+ a
?
)) (1)
Since cosine is inverse to the angle, high cosine
similarity (close to 1) means that the vectors share
a very similar direction. Note that this metric nor-
malizes (and thus ignores) the vectors? lengths,
unlike the Euclidean distance between them. For
reasons that will be clear later, we refer to (1) as
the 3COSADD method.
An alternative to 3COSADD is to require that
the direction of transformation be conserved:
argmax
b
?
?V
(cos (b
?
? b, a
?
? a)) (2)
This basically means that b
?
? b shares the same
direction with a
?
? a, ignoring the distances. We
refer to this method as PAIRDIRECTION. Though
it was not mentioned in the paper, Mikolov
et al. (2013c) used PAIRDIRECTION for solving
the semantic analogies of the SemEval task, and
3COSADD for solving the syntactic analogies.
1
3.3 Reinterpreting Vector Arithmetic
In Mikolov et al.?s experiments, all word-vectors
were normalized to unit length. Under such nor-
malization, the argmax in (1) is mathematically
equivalent to (derived using basic algebra):
argmax
b
?
?V
(cos (b
?
, b)? cos (b
?
, a) + cos (b
?
, a
?
))
(3)
This means that solving analogy questions with
vector arithmetic is mathematically equivalent to
seeking a word (b
?
) which is similar to b and a
?
but is different from a. Relational similarity is
thus expressed as a sum of attributional similari-
ties. While (1) and (3) are equal, we find the intu-
ition as to why (3) ought to find analogies clearer.
4 Empirical Setup
We derive explicit and neural-embedded vec-
tor representations, and compare their capacities
to recover relational similarities using objectives
3COSADD (eq. 3) and PAIRDIRECTION (eq. 2).
Underlying Corpus and Preprocessing Previ-
ous reported results on the word analogy tasks us-
ing vector arithmetics were obtained using propri-
etary corpora. To make our experiments repro-
ducible, we selected an open and widely accessi-
ble corpus ? the English Wikipedia. We extracted
all sentences from article bodies (excluding ti-
tles, infoboxes, captions, etc) and filtered non-
alphanumeric tokens, allowing mid-token symbols
as apostrophes, hyphens, commas, and periods.
All the text was lowercased. Duplicates and sen-
tences with less than 5 tokens were then removed.
Overall, we retained a corpus of about 1.5 billion
tokens, in 77.5 million sentences.
Word Representations To create contexts for
both embedding and sparse representation, we
used a window of two tokens to each side (5-
grams, in total), ignoring words that appeared less
1
This was confirmed both by our independent trials and
by corresponding with the authors.
173
than 100 times in the corpus. The filtered vocabu-
lary contained 189,533 terms.
2
The explicit vector representations were created
as described in Section 2. The neural embeddings
were created using the word2vec software
3
ac-
companying (Mikolov et al., 2013b). We embed-
ded the vocabulary into a 600 dimensional space,
using the state-of-the-art skip-gram architecture,
the negative-training approach with 15 negative
samples (NEG-15), and sub-sampling of frequent
words with a parameter of 10
?5
. The parameter
settings follow (Mikolov et al., 2013b).
4.1 Evaluation Conditions
We evaluate the different word representations us-
ing the three datasets used in previous work. Two
of them (MSR and GOOGLE) contain analogy
questions, while the third (SEMEVAL) requires
ranking of candidate word pairs according to their
relational similarity to a set of supplied word pairs.
Open Vocabulary The open vocabulary
datasets (MSR and GOOGLE) present questions
of the form ?a is to a
?
as b is to b
?
?, where b
?
is hidden, and must be guessed from the entire
vocabulary. Performance on these datasets is
measured by micro-averaged accuracy.
The MSR dataset
4
(Mikolov et al., 2013c) con-
tains 8000 analogy questions. The relations por-
trayed by these questions are morpho-syntactic,
and can be categorized according to parts of
speech ? adjectives, nouns and verbs. Adjec-
tive relations include comparative and superlative
(good is to best as smart is to smartest). Noun
relations include single and plural, possessive and
non-possessive (dog is to dog?s as cat is to cat?s).
Verb relations are tense modifications (work is to
worked as accept is to accepted).
The GOOGLE dataset
5
(Mikolov et al., 2013a)
contains 19544 questions. It covers 14 relation
types, 7 of which are semantic in nature and 7
are morpho-syntactic (enumerated in Section 8).
The dataset was created by manually constructing
example word-pairs of each relation, and provid-
ing all the pairs of word-pairs (within each relation
type) as analogy questions.
2
Initial experiments with different window-sizes and cut-
offs showed similar trends.
3
http://code.google.com/p/word2vec
4
research.microsoft.com/en-us/
projects/rnn/
5
code.google.com/p/word2vec/source/
browse/trunk/questions-words.txt
Out-of-vocabulary words
6
were removed from
both test sets.
Closed Vocabulary The SEMEVAL dataset con-
tains the collection of 79 semantic relations that
appeared in SemEval 2012 Task 2: Measuring Re-
lation Similarity (Jurgens et al., 2012). Each rela-
tion is exemplified by a few (usually 3) character-
istic word-pairs. Given a set of several dozen tar-
get word pairs, which supposedly have the same
relation, the task is to rank the target pairs ac-
cording to the degree in which this relation holds.
This can be cast as an analogy question in the
following manner: For example, take the Recipi-
ent:Instrument relation with the prototypical word
pairs king:crown and police:badge. To measure
the degree that a target word pair wife:ring has the
same relation, we form the two analogy questions
?king is to crown as wife is to ring? and ?police is
to badge as wife is to ring?. We calculate the score
of each analogy, and average the results. Note that
as opposed to the first two test sets, this one does
not require searching the entire vocabulary for the
most suitable word in the corpus, but rather to rank
a list of existing word pairs.
Following previous work, performance on SE-
MEVAL was measured using accuracy, macro-
averaged across all the relations.
5 Preliminary Results
Our first experiment uses 3COSADD (method (3)
in Section 3) to measure the prevalence of linguis-
tic regularities within each representation.
Representation MSR GOOGLE SEMEVAL
Embedding 53.98% 62.70% 38.49%
Explicit 29.04% 45.05% 38.54%
Table 1: Performance of 3COSADD on different tasks with
the explicit and neural embedding representations.
The results in Table 1 show that a large amount
of relational similarities can be recovered with
both representations. In fact, both representations
achieve the same accuracy on the SEMEVAL task.
However, there is a large performance gap in favor
of the neural embedding in the open-vocabulary
MSR and GOOGLE tasks.
Next, we run the same experiment with
PAIRDIRECTION (method (2) in Section 3).
6
i.e. words that appeared in English Wikipedia less
than 100 times. This removed 882 instances from the
MSR dataset and 286 instances from GOOGLE.
174
Representation MSR GOOGLE SEMEVAL
Embedding 9.26% 14.51% 44.77%
Explicit 0.66% 0.75% 45.19%
Table 2: Performance of PAIRDIRECTION on different tasks
with the explicit and neural embedding representations.
The results in Table 2 show that the PAIRDI-
RECTION method is better than 3COSADD on
the restricted-vocabulary SEMEVAL task (accu-
racy jumps from 38% to 45%), but fails at the
open-vocabulary questions in GOOGLE and MSR.
When the method does work, the numbers for the
explicit and embedded representations are again
comparable to one another.
Why is PAIRDIRECTION performing so well
on the SEMEVAL task, yet so poorly on the oth-
ers? Recall that the PAIRDIRECTION objective
focuses on the similarity of b
?
? b and a
?
? a,
but does not take into account the spatial distances
between the individual vectors. Relying on di-
rection alone, while ignoring spatial distance, is
problematic when considering the entire vocabu-
lary as candidates (as is required in the MSR and
GOOGLE tasks). We are likely to find candidates
b
?
that have the same relation to b as reflected by
a ? a
?
but are not necessarily similar to b. As a
concrete example, in man:woman, king:?, we are
likely to recover feminine entities, but not neces-
sarily royal ones. The SEMEVAL test set, on the
other hand, already provides related (and therefore
geometrically close) candidates, leaving mainly
the direction to reason about.
6 Refining the Objective Function
The 3COSADD objective, as expressed in (3), re-
veals a ?balancing act? between two attractors and
one repeller, i.e. two terms that we wish to maxi-
mize and one that needs to be minimized:
argmax
b
?
?V
(cos (b
?
, b)? cos (b
?
, a) + cos (b
?
, a
?
))
A known property of such linear objectives is that
they exhibit a ?soft-or? behavior and allow one
sufficiently large term to dominate the expression.
This behavior is problematic in our setup, because
each term reflects a different aspect of similarity,
and the different aspects have different scales. For
example, king is more royal than it is masculine,
and will therefore overshadow the gender aspect
of the analogy. It is especially true in the case of
explicit vector representations, as each aspect of
the similarity is manifested by a different set of
features with varying sizes and weights.
A case in point is the analogy question ?London
is to England as Baghdad is to ? ??, which we
answer using:
argmax
x?V
(cos (x, en)? cos (x, lo) + cos (x, ba))
We seek a word (Iraq) which is similar to Eng-
land (both are countries), is similar to Baghdad
(similar geography/culture) and is dissimilar to
London (different geography/culture). Maximiz-
ing the sum yields an incorrect answer (under both
representations): Mosul, a large Iraqi city. Look-
ing at the computed similarities in the explicit vec-
tor representation, we see that both Mosul and Iraq
are very close to Baghdad, and are quite far from
England and London:
(EXP) ? England ? London ? Baghdad Sum
Mosul 0.031 0.031 0.244 0.244
Iraq 0.049 0.038 0.206 0.217
The same trends appear in the neural embedding
vectors, though with different similarity scores:
(EMB) ? England ? London ? Baghdad Sum
Mosul 0.130 0.141 0.755 0.748
Iraq 0.153 0.130 0.631 0.655
While Iraq is much more similar to England than
Mosul is (both being countries), both similarities
(0.049 and 0.031 in explicit, 0.130 and 0.153 in
embedded) are small and the sums are dominated
by the geographic and cultural aspect of the anal-
ogy: Mosul and Iraq?s similarity to Baghdad (0.24
and 0.20 in explicit, 0.75 and 0.63 in embedded).
To achieve better balance among the different
aspects of similarity, we propose switching from
an additive to a multiplicative combination:
argmax
b
?
?V
cos (b
?
, b) cos (b
?
, a
?
)
cos (b
?
, a) + ?
(4)
(? = 0.001 is used to prevent division by zero)
This is equivalent to taking the logarithm of each
term before summation, thus amplifying the dif-
ferences between small quantities and reducing
the differences between larger ones. Using this ob-
jective, Iraq is scored higher than Mosul (0.259 vs
0.236, 0.736 vs 0.691). We refer to objective (4)
as 3COSMUL.
7
7
3COSMUL requires that all similarities be non-negative,
which trivially holds for explicit representations. With em-
beddings, we transform cosine similarities to [0, 1] using
(x+ 1)/2 before calculating (4).
175
7 Main Results
We repeated the experiments, this time using the
3COSMUL method. Table 3 presents the results,
showing that the multiplicative objective recov-
ers more relational similarities in both representa-
tions. The improvements achieved in the explicit
representation are especially dramatic, with an ab-
solute increase of over 20% correctly identified re-
lations in the MSR and GOOGLE datasets.
Objective Representation MSR GOOGLE
3COSADD
Embedding 53.98% 62.70%
Explicit 29.04% 45.05%
3COSMUL
Embedding 59.09% 66.72%
Explicit 56.83% 68.24%
Table 3: Comparison of 3COSADD and 3COSMUL.
3COSMUL outperforms the state-of-the-art
(3COSADD) on these two datasets. Moreover, the
results illustrate that a comparable amount of rela-
tional similarities can be recovered with both rep-
resentations. This suggests that the linguistic reg-
ularities apparent in neural embeddings are not a
consequence of the embedding process, but rather
are well preserved by it.
On SEMEVAL, 3COSMUL preformed on par
with 3COSADD , recovering a similar amount of
analogies with both explicit and neural representa-
tions (38.37% and 38.67%, respectively).
8 Error Analysis
With 3COSMUL, both the explicit vectors and
the neural embeddings recover similar amounts of
analogies, but are these the same patterns, or per-
haps different types of relational similarities?
8.1 Agreement between Representations
Considering the open-vocabulary tasks (MSR and
GOOGLE), we count the number of times both rep-
resentations guessed correctly, both guessed in-
correctly, and when one representations leads to
the right answer while the other does not (Ta-
ble 4). While there is a large amount of agreement
between the representations, there is also a non-
negligible amount of cases in which they comple-
ment each other. If we were to run in an ora-
cle setup, in which an answer is considered cor-
rect if it is correct in either representation, we
would have achieved an accuracy of 71.9% on the
MSR dataset and 77.8% on GOOGLE.
Both Both Embedding Explicit
Correct Wrong Correct Correct
MSR 43.97% 28.06% 15.12% 12.85%
GOOGLE 57.12% 22.17% 9.59% 11.12%
ALL 53.58% 23.76% 11.08% 11.59%
Table 4: Agreement between the representations on open-
vocabulary tasks.
Relation Embedding Explicit
G
O
O
G
L
E
capital-common-countries 90.51% 99.41%
capital-world 77.61% 92.73%
city-in-state 56.95% 64.69%
currency 14.55% 10.53%
family (gender inflections) 76.48% 60.08%
gram1-adjective-to-adverb 24.29% 14.01%
gram2-opposite 37.07% 28.94%
gram3-comparative 86.11% 77.85%
gram4-superlative 56.72% 63.45%
gram5-present-participle 63.35% 65.06%
gram6-nationality-adjective 89.37% 90.56%
gram7-past-tense 65.83% 48.85%
gram8-plural (nouns) 72.15% 76.05%
gram9-plural-verbs 71.15% 55.75%
M
S
R
adjectives 45.88% 56.46%
nouns 56.96% 63.07%
verbs 69.90% 52.97%
Table 5: Breakdown of relational similarities in each repre-
sentation by relation type, using 3COSMUL.
8.2 Breakdown by Relation Type
Table 5 presents the amount of analogies dis-
covered in each representation, broken down by
relation type. Some trends emerge: the ex-
plicit representation is superior in some of the
more semantic tasks, especially geography re-
lated ones, as well as the ones superlatives and
nouns. The neural embedding, however, has the
upper hand on most verb inflections, compara-
tives, and family (gender) relations. Some rela-
tions (currency, adjectives-to-adverbs, opposites)
pose a challenge to both representations, though
are somewhat better handled by the embedded
representations. Finally, the nationality-adjectives
and present-participles are equally handled by
both representations.
8.3 Default-Behavior Errors
The most common error pattern under both repre-
sentations is that of a ?default behavior?, in which
one central representative word is provided as an
answer to many questions of the same type. For
example, the word ?Fresno? is returned 82 times
as an incorrect answer in the city-in-state rela-
tion in the embedded representation, and the word
?daughter? is returned 47 times as an incorrect an-
swer in the family relation in the explicit represen-
176
RELATION WORD EMB EXP
gram7-past-tense who 0 138
city-in-state fresno 82 24
gram6-nationality-adjective slovak 39 39
gram6-nationality-adjective argentine 37 39
gram6-nationality-adjective belarusian 37 39
gram8-plural (nouns) colour 36 35
gram3-comparative higher 34 35
city-in-state smith 1 61
gram7-past-tense and 0 49
gram1-adjective-to-adverb be 0 47
family (gender inflections) daughter 8 47
city-in-state illinois 3 40
currency currency 5 40
gram1-adjective-to-adverb and 0 39
gram7-past-tense enhance 39 20
Table 6: Common default-behavior errors under both repre-
sentations. EMB / EXP: the number of time the word was
returned as an incorrect answer for the given relation under
the embedded or explicit representation.
tation. Loosely, ?Fresno? is identified by the em-
bedded representation as a prototypical location,
while ?daughter? is identified by the explicit rep-
resentation as a prototypical female. Under a def-
inition in which a default behavior error is one in
which the same incorrect answer is returned for a
particular relation 10 or more times, such errors
account for 49% of the errors in the explicit repre-
sentation, and for 39% of the errors in the embed-
ded representation.
Table 6 lists the 15 most common default er-
rors under both representations. In most default er-
rors the category of the default word is closely re-
lated to the analogy question, sharing the category
of either the correct answer, or (as in the case of
?Fresno?) the question word. Notable exceptions
are the words ?who?, ?and?, ?be? and ?smith? that
are returned as default answers in the explicit rep-
resentation, and which are very far from the in-
tended relation. It seems that in the explicit repre-
sentation, some very frequent function words act
as ?hubs? and confuse the model. In fact, the
performance gap between the representations in
the past-tense and plural-verb relations can be at-
tributed specifically to such function-word errors:
23.4% of the mistakes in past-tense relation are
due to the explicit representation?s default answer
of ?who? or ?and?, while 19% of the mistakes in
the plural-verb relations are due to default answers
of ?is/and/that/who?.
8.4 Verb-inflection Errors
A correct solution to the morphological anal-
ogy task requires recovering both the correct in-
flection (requiring syntactic similarity) and the
correct base word (requiring semantic similar-
ity). We observe that linguistically, the mor-
phological distinctions and similarities tend to
rely on a few common word forms (for exam-
ple, the ?walk:walking? relation is characterized
by modals such as ?will? appearing before ?walk?
and never before ?walking?, and be verbs ap-
pearing before walking and never before ?walk?),
while the support for the semantic relations is
spread out over many more items. We hypothe-
size that the morphological distinctions in verbs
are much harder to capture than the semantics. In-
deed, under both representations, errors in which
the selected word has a correct form with an incor-
rect inflection are over ten times more likely than
errors in which the selected word has the correct
inflection but an incorrect base form.
9 Interpreting Relational Similarities
The ability to capture relational similarities by
performing vector (or similarity) arithmetic is re-
markable. In this section, we try and provide intu-
ition as to why it works.
Consider the word ?king?; it has several aspects,
high-level properties that it implies, such as roy-
alty or (male) gender, and its attributional simi-
larity with another word is based on a mixture of
those aspects; e.g. king is related to queen on the
royalty and the human axes, and shares the gender
and the human aspect with man. Relational simi-
larities can be viewed as a composition of attribu-
tional similarities, each one reflecting a different
aspect. In ?man is to woman as king is to queen?,
the two main aspects are gender and royalty. Solv-
ing the analogy question involves identifying the
relevant aspects, and trying to change one of them
while preserving the other.
How are concepts such as gender, royalty, or
?cityness? represented in the vector space? While
the neural embeddings are mostly opaque, one of
the appealing properties of explicit vector repre-
sentations is our ability to read and understand the
vectors? features. For example, king is represented
in our explicit vector space by 51,409 contexts, of
which the top 3 are tut
+1
, jeongjo
+1
, adulyadej
+2
? all names of monarchs. The explicit representa-
tion allows us to glimpse at the way different as-
pects are represented. To do so, we choose a repre-
sentative pair of words that share an aspect, inter-
sect their vectors, and inspect the highest scoring
177
Aspect Examples Top Features
Female woman queen estrid
+1
ketevan
+1
adeliza
+1
nzinga
+1
gunnhild
+1
impregnate
?2
hippolyta
+1
Royalty queen king savang
+1
uncrowned
?1
pmare
+1
sisowath
+1
nzinga
+1
tupou
+1
uvea
+2
majesty
?1
Currency yen ruble devalue
?2
banknote
+1
denominated
+1
billion
?1
banknotes
+1
pegged
+2
coin
+1
Country germany  australia emigrates
?2
1943-45
+2
pentathletes
?2
emigrated
?2
emigrate
?2
hong-kong
?1
Capital berlin canberra hotshots
?1
embassy
?2
1925-26
+2
consulate-general
+2
meetups
?2
nunciature
?2
Superlative sweetest tallest freshest
+2
asia?s
?1
cleveland?s
?2
smartest
+1
world?s
?1
city?s
?1
america?s
?1
Height taller  tallest regnans
?2
skyscraper
+1
skyscrapers
+1
6?4
+2
windsor?s
?1
smokestacks
+1
burj
+2
Table 7: The top features of each aspect, recovered by pointwise multiplication of words that share that aspect. The result of
pointwise multiplication is an ?aspect vector? in which the features common to both words, characterizing the relation, receive
the highest scores. The feature scores (not shown) correspond to the weight the feature contributes to the cosine similarity
between the vectors. The superscript marks the position of the feature relative to the target word.
features in the intersection. Table 7 presents the
top (most influential) features of each aspect.
Many of these features are names of people or
places, which appear rarely in our corpus (e.g.
Adeliza, a historical queen, and Nzinga, a royal
family) but are nonetheless highly indicative of
the shared concept. The prevalence of rare words
stems from PMI, which gives them more weight,
and from the fact that words like woman and queen
are closely related (a queen is a woman), and thus
have many features in common. Ordering the fea-
tures of woman  queen by prevalence reveals
female pronouns (?she?, ?her?) and a long list of
common feminine names, reflecting the expected
aspect shared by woman and queen. Word pairs
that share more specific aspects, such as capital
cities or countries, show features that are charac-
teristic of their shared aspect (e.g. capital cities
have embassies and meetups, while immigration
is associated with countries). It is also interesting
to observe how the relatively syntactic ?superlativ-
ity? aspect is captured with many regional posses-
sives (?america?s?, ?asia?s?, ?world?s?).
10 Related Work
Relational similarity (and answering analogy
questions) was previously tackled using explicit
representations. Previous approaches use task-
specific information, by either relying on a
(word-pair, connectives) matrix rather than the
standard (word, context) matrix (Turney and
Littman, 2005; Turney, 2006), or by treating anal-
ogy detection as a supervised learning task (Ba-
roni and Lenci, 2009; Jurgens et al., 2012; Turney,
2013). In contrast, the vector arithmetic approach
followed here is unsupervised, and works on a
generic single-word representation. Even though
the training process is oblivious to the task of anal-
ogy detection, the resulting representation is able
to detect them quite accurately. Turney (2012) as-
sumes a similar setting but with two types of word
similarities, and combines them with products and
ratios (similar to 3COSMUL) to recover a variety
of semantic relations, including analogies.
Arithmetic combination of explicit word vec-
tors is extensively studied in the context of com-
positional semantics (Mitchell and Lapata, 2010),
where a phrase composed of two or more words
is represented by a single vector, computed by a
function of its component word vectors. Blacoe
and Lapata (2012) compare different arithmetic
functions across multiple representations (includ-
ing embeddings) on a range of compositionality
benchmarks. To the best of our knowledge such
methods of word vector arithmetic have not been
explored for recovering relational similarities in
explicit representations.
11 Discussion
Mikolov et al. showed how an unsupervised neural
network can represent words in a space that ?nat-
urally? encodes relational similarities in the form
of vector offsets. This study shows that finding
analogies through vector arithmetic is actually a
form of balancing word similarities, and that, con-
trary to the recent findings of Baroni et al. (2014),
under certain conditions traditional word similar-
ities induced by explicit representations can per-
form just as well as neural embeddings on this
task.
Learning to represent words is a fascinating and
important challenge with implications to most cur-
rent NLP efforts, and neural embeddings in par-
ticular are a promising research direction. We
believe that to improve these representations we
should understand how they work, and hope that
the methods and insights provided in this work
will help to deepen our grasp of current and future
investigations of word representations.
178
References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proc. of CoNLL 2013.
Marco Baroni and Alessandro Lenci. 2009. One dis-
tributional memory, many semantic spaces. In Pro-
ceedings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 1?8, Athens,
Greece, March. Association for Computational Lin-
guistics.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni, Georgiana Dinu, and Germ?an
Kruszewski. 2014. Dont count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Jeju Island, Korea,
July. Association for Computational Linguistics.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510?526.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of the 32nd annual
meeting on Association for Computational Linguis-
tics, pages 272?278. Association for Computational
Linguistics.
David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 356?364. As-
sociation for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), Balti-
more, Maryland, USA, June. Association for Com-
putational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Dirt: discovery
of inference rules from text. In KDD, pages 323?
328.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98, pages
768?774, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States, pages 3111?
3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1439.
179
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
pages 1081?1088.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vec-
tors from dictionaries. In Proceedings of the 15th
conference on Computational linguistics-Volume 1,
pages 304?309. Association for Computational Lin-
guistics.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st annual meeting on Associa-
tion for Computational Linguistics, pages 183?190.
Association for Computational Linguistics.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stock-
holm.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Peter D. Turney and Michael L. Littman. 2005.
Corpus-based learning of analogies and semantic re-
lations. Machine Learning, 60(1-3):251?278.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the 12th
European Conference on Machine Learning, pages
491?502. Springer-Verlag.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379?416.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
Peter D. Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. CoRR, abs/1310.5042.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geof-
frey Zweig, and Tomas Mikolov. 2013. Combining
heterogeneous models for measuring relational sim-
ilarity. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1000?1009, Atlanta, Georgia, June.
Association for Computational Linguistics.
180
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 19?24,
Dublin, Ireland, August 23 2014.
Proposition Knowledge Graphs
Gabriel Stanovsky Omer Levy Ido Dagan
Computer Science Department, Bar-Ilan University
{gabriel.satanovsky, omerlevy}@gmail.com
dagan@cs.biu.ac.il
Abstract
Open Information Extraction (Open IE) is a promising approach for unrestricted Information
Discovery (ID). While Open IE is a highly scalable approach, allowing unsupervised relation
extraction from open domains, it currently has some limitations. First, it lacks the expressiveness
needed to properly represent and extract complex assertions that are abundant in text. Second, it
does not consolidate the extracted propositions, which causes simple queries above Open IE as-
sertions to return insufficient or redundant information. To address these limitations, we propose
in this position paper a novel representation for ID ? Propositional Knowledge Graphs (PKG).
PKGs extend the Open IE paradigm by representing semantic inter-proposition relations in a
traversable graph. We outline an approach for constructing PKGs from single and multiple texts,
and highlight a variety of high-level applications that may leverage PKGs as their underlying
information discovery and representation framework.
1 Introduction
Information discovery from text (ID) aims to provide a consolidated and explorable data representation of
an input document or a collection of documents addressing a common topic. Ideally, this representation
would separate the input into logically discrete units, omit redundancies in the original text, and provide
semantic relations between the basic units of the representation. This representation can then be used
by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question
answering and multidocument summarization) as a structured input representation.
A common approach to ID is to extract propositions conveyed in the text by applying either supervised
Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a
predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information
Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009;
Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface
or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a
natural language string. While Open IE presents a promising direction for ID, thanks to its robustness
and scalability across domains, we argue that it currently lacks representation power in two major aspects:
representing complex propositions extracted from discourse, such as interdependent propositions or
implicitly conveyed propositions, and consolidating propositions extracted across multiple sources,
which leads to either insufficient or redundant information when exploring a set of Open IE extractions.
In this position paper we outline Propositional Knowledge Graphs (PKG), a representation which
addresses both of Open IE?s mentioned drawbacks. The graph?s nodes are discrete propositions extracted
from text, and edges are drawn where semantic relations between propositions exists. Such relations can
be inferred from a single discourse, or from multiple text fragments along with background knowledge ?
by applying methods such as textual entailment recognition (Dagan et al., 2013) ? which consolidates the
information within the graph. We discuss this representation as a useful input for semantic applications,
and describe work we have been doing towards implementing such a framework.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
19
Figure 1: An excerpt from a PKG, containing a few propositions extracted from news reports about Curiosity (the Mars rover)
and their relations. The dashed boundaries in the figure denote paraphrase cliques, meaning that all propositions within them
are mutually entailing. Some of these propositions are complex, and the bottom-right corner illustrates how one of them can be
represented by inter-connected sub-propositions.
2 Approach: Discover Inter-Proposition Relations
We propose a novel approach for textual information discovery and representation that enhances the
expressiveness of Open IE with structural power similar to traditional knowledge graphs. Our represen-
tation aims to extract all the information conveyed by text to a traversable graph format ? a Propositional
Knowledge Graph (PKG). The graph?s nodes are natural language propositions and its labeled edges are
semantic relations between these propositions. Figure 1 illustrates an excerpt of a PKG.
We separate the construction of such graphs into two phases, each of which addresses one of the afore-
mentioned limitations of current Open IE. The first phase (described in section 2.1) is the extraction of
complex propositions from a single discourse. This phase extends upon the definition of Open IE ex-
tractions to gain a more expressive paradigm and improve the recall of extracted propositions. In this
extension, a single assertion is represented by a set of interconnected propositions. An example can be
seen in the bottom right of Figure 1. The second phase (described in section 2.2) deals with the consolida-
tion of propositions extracted in the first phase. This is done by drawing relations such as entailment and
temporal succession between these propositions, which can be inferred utilizing background knowledge
applied on multiple text fragments.
2.1 Relations Implied by Discourse
Current Open IE representation schemes lack the expressibility to represent certain quite common propo-
sitions implied by syntax, hindering Open IE?s potential as an information discovery framework. We dis-
cuss several cases in which this limitation is evident, and describe possible solutions within our proposed
framework.
Embedded and Interrelated Propositions Common Open IE systems retrieve only propositions in
which both predicates and arguments are instantiated in succession in the surface form. For such propo-
sitions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of
a predicate and a list of its arguments, all expressed in natural language, in the same way they originally
appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are
inherently embedded, such as conditionals and propositional arguments (e.g. ?Senator Kennedy asked
congress to pass the bill?). Mausam et al. (2012) introduced a context analysis layer, extending this
20
representation with an additional field per tuple, which intends to represent the factuality of the extrac-
tion, accounting specifically for cases of conditionals and attribution. For instance, the assertion ?If he
wins five key states, Romney will be elected President? will be represented as ((Romney; will be elected;
President) ClausalModifier if; he wins five key states).
While these methods capture some of the propositions conveyed by text, they fail to retrieve other
propositions expressed by more sophisticated syntactic constructs. Consider the sentence from Figure 1
?Curiosity will look for evidence that Mars might have had conditions for supporting life?. It exhibits a
construction which the independent tuples format seems to fall short from representing. Our proposed
representation for this sentence is depicted in the bottom right of Figure 1. We represent the complexity
of the sentence through a nested structure of interlinked propositions, each composed of a single pred-
icate and its syntactic arguments and modifiers. In addition, we model certain syntactic variabilities as
features, such as tense, negation, passive voice, etc. Thus, a single assertion is represented through the
discrete propositions it conveys, along with their inter-relations. In addition to the expressibility that this
representation offers, an immediate gain is the often recurring case in which a part of a proposition (for
example, one of the arguments) immediately implies another proposition. For instance, ?The Mars rover
Curiosity is a mobile science lab? implies that ?Curiosity is a rover?, and does so syntactically.
Implicit propositions Certain propositions which are conveyed by the text are not explicitly expressed
in the surface form. Consider, for instance, the sentence ?Facebook?s acquisition of WhatsApp occurred
yesterday?. It introduces the proposition (Facebook, acquired, WhatsApp) through nominalization. Cur-
rent Open IE formalisms are unable to extract such triplets, since the necessary predicate (namely ?ac-
quired?) does not appear in the surface form. Implicit propositions might be introduced in many other
linguistic constructs, such as: appositions (?The company, Random House, doesn?t report its earnings.?
implies that Random House is a company), adjectives (?Tall John walked home? implies that John is tall),
and possessives (?John?s book is on the table? implies that John has a book). We intend to syntactically
identify these implicit propositions, and make them explicit in our representation.
For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al.,
2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic
parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe
and Manning, 2008) to interconnected propositions as described.
2.2 Consolidating Information across Propositions
While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natu-
ral language expressions, which leads to either insufficient or redundant information when accessing a
repository of Open IE extractions. As an illustrating example, querying the University of Washington?s
Open IE demo (openie.cs.washington.edu) for the generally equivalent relieves headache or
treats headache returns two different lists of entities; out of the top few results, the only answers these
queries seem to agree on are caffeine and sex. Desirably, an information discovery platform should re-
turn identical results (or at least very similar ones) to these queries. This is a major drawback relative
to supervised knowledge representations, such as Freebase (Bollacker et al., 2008), which map natural
language expressions to canonical formal representations (e.g. the treatments relation in Freebase).
While much relational information can be salvaged from the original text, many inter-propositional
relations stem from background knowledge and our understanding of language. Perhaps the most promi-
nent of these is the entailment relation, as demonstrated in Figure 1. We rely on the definition of textual
entailment as defined by Dagan et al. (2013): proposition T entails proposition H if humans reading T
would typically infer that H is most likely true. Entailment provides an effective structure for aggregat-
ing natural-language based information; it merges semantically equivalent propositions into cliques, and
induces specification-generalization edges between them (if T entails H , then H is more general).
Figure 1 demonstrates the usefulness of entailment in organizing the propositions within a PKG. For
example, the two statements describing Curiosity as a mobile science lab (middle right) originated from
two different texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both
entail an additional proposition from a third source: ?Curiosity is a lab?. If one were to query all the
21
propositions that entail ?Curiosity is a lab? ? e.g. in response to the query ?What is Curiosity?? ? all
three propositions would be retrieved, even though their surface forms may have ?functions as? instead
of ?is? or ?laboratory? instead of ?lab?.
We have recently taken some first steps in this direction, investigating algorithms for constructing
entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions,
recognizing entailment is challenging. We are currently working on new methods that will leverage
structured and unstructured data to recognize entailment for Open IE propositions. There are additional
relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples
are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for
recognizing and utilizing these relations is intended for future work.
3 Applications
An appealing application of knowledge graphs is question answering (QA). In this section we demon-
strate how our representation may facilitate more sophisticated information access scenarios.
Structured Queries Queries over structured data give the user the power to receive targeted answers
for her queries. Consider for example the query ?electric cars on sale in Canada?. PKGs can give the
power of queries over structured data to the domain of unstructured information. To answer our query,
we can search the PKG for all of the propositions that entail these two propositions: (1) ?X is an electric
car?, (2) ?X is on sale in Canada?, where X is a variable. The list of X instantiations is the answer
to our structured query. Our knowledge structure enables even more sophisticated queries that involve
more than one variable. For example, ?Japanese corporations that bought Australian start-ups? retrieves
a collection of pairs (X,Y ) where X is the Japanese corporation that bought Y , an Australian start-up.
Summarization Multi-document summarization gives the user the ability to compactly assimilate in-
formation from multiple documents on the same topic. PKGs can be a natural platform leveraged by
summarization because: (1) they would contain the information from those documents as fine-grained
propositions (2) they represent the semantic relations between those propositions. These semantic re-
lations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual
entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order
in which each proposition is presented. A recent method of summarizing text with entailment graphs
(Gupta et al., 2014) demonstrates the appeal and feasibility of this application.
Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012)
demonstrate this concept on a limited proposition graph. When searching for ?headache? in their demo,
the user can drill-down to find possible causes or remedies, and even focus on subcategories of those;
for example, finding the foods which relieve headaches. As opposed to the structured query application,
retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new
information they might not have considered a-priori.
4 Discussion
In this position paper we outlined a framework for information discovery that leverages and extends Open
IE, while addressing two of its current major drawbacks. The proposed framework enriches Open IE by
representing natural language in a traversable graph, composed of propositions and their semantic inter-
relations ? A Propositional Knowledge Graph (PKG). The resulting structure provides a representation
in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a
single sentence, and globally, at inter-proposition level, where relations are drawn between propositions
from discourse, or from various sources.
At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu
et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument
relations) - a ?meaning representation?. AMR uses Propbank (Kingsbury and Palmer, 2003) for pred-
icates? meaning representation, where possible, and ungrounded natural language, where no respective
22
Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level
representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic
interpretation.
At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (such as Freebase
and Google?s Knowledge Graph). These Knowledge Graphs, in contrast with PKGs, require manual
intervention and aim to cover a rich set of relations using formal language and a pre-specified schema,
thus many relations are inevitably left out (e.g. the relation cracked, as in (Alan Turing, cracked, the
Enigma) does not exist in Freebase).
We believe that PKGs are a promising extension of Open IE?s unsupervised traits, for combining as-
pects of information representation - on a local scale, providing a rich schema for representing sentences,
and on a global scale providing an automated and consolidated method for structuring knowledge.
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the
health-care domain. In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012), pages 79?84.
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpe-
dia: A nucleus for a web of open data. In The semantic web, pages 722?735. Springer.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp
Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively
created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD interna-
tional conference on Management of data, pages 1247?1250. ACM.
Jim Cowie and Wendy Lehnert. 1996. Information extraction. Communications of the ACM, 39(1):80?91.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment:
Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1?220.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,
pages 1?8, Manchester, UK, August. Coling 2008 Organizing Committee.
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from
the Web. Communications of the ACM, 51(12):68?74.
Anand Gupta, Manpreet Kathuria, Shachar Mirkin, Adarsh Singh, and Aseem Goyal. 2014. Text summarization
through entailment-based minimum vertex cover. In *SEM.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for open ie propositions. In
Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland,
USA, June. Association for Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pages 523?534, Jeju Island, Korea, July.
Association for Computational Linguistics.
Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav Goldberg. 2014. Intermediary semantic representation
through proposition structures. In Workshop on Semantic Parsing, Baltimore, Maryland, USA, June. Associa-
tion for Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and
wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203?217.
23
Fei Wu, Raphael Hoffmann, and Daniel S Weld. 2008. Information extraction from wikipedia: Moving down the
long tail. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 731?739. ACM.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to
extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages
101?110. ACM.
24
