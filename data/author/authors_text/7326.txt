Named Entity Extraction from Noisy Input: Speech and OCR 
David Miller, Scan Boisen, Richard Schwartz, Rebecca Stone, Ralph Weischedel 
BBN Technologies 
70 Fawcett Street 
Cambridge, MA 02138 
dmiller@bbn.com, boisen@bbn.com, schwartz@bbn.com, rwstone@bbn.com, weischedel@bbn.com 
Abstract 
In this paper, we analyze the performance 
of name finding in the context of a variety 
of automatic speech recognition (ASR) 
systems and in the context of one optical 
character recognition (OCR) system. We 
explore the effects of word error rate from 
ASR and OCR, performance as a function 
of the amount of training data, and for 
speech, the effect of out-of-vocabulary 
errors and the loss of punctuation and mixed 
case 
I Introduction 
Information extraction systems have 
traditionally been evaluated on online text with 
relatively few errors in the input. For example, 
this description of the Nominator system 
(Wacholder et al 1997) would apply to several 
other systems: "We chose The Wall Street 
Journal corpus because it follows standard 
stylistic conventions, especially capitalization, 
which is essential for Nominator to work." The 
real-world challenge, however, is pointed out in 
Palmer and Day (1997): "It is also unknown 
how the existing high-scoring systems would 
perform on less well-behaved texts, such as 
single-case texts, non-newswire texts, or text 
obtained via optical character recognition 
(OCR)." 
In this paper we explore how performance 
degrades on noisy input, in particular on 
broadcast news (speech) and on newspaper 
(printed matter). Error rates of automatic 
speech recognizers (ASR) on broadcast news 
are still very high, e.g., 14-28% word error. 
Though character error can be very low for laser 
printer output, word error rates of 20% are 
possible for OCR systems applied to newsprint 
or low-quality printed matter. 
In this paper, we evaluate a learning algorithm, 
a hidden Markov model (HM) ,  for named 
entity extraction applied to human transcripts of 
news, to transcripts without case or punctuation 
(perfect speech output), to errorful ASR output 
and to OCR output. Extracting information from 
noisy sources poses the following challenges, 
which are addressed in the paper. 
? Since speech recognizers do not generate 
mixed case nor punctuation, how much do 
case and punctuation contribute to 
recognizing names in English? (Section 3.) 
Note that these challenges also arise in 
languages without case to signal proper 
nouns (e.g., Chinese, German, Japanese), in 
mono-case English or informal English 
(e.g., emails). 
? How much will performance degrade with 
increasing error in the input? (Section 4.) 
? How does closed vocabulary recognition 
affect information extraction performance? 
(Section 5) 
? For the learning algorithm employed, how 
much training and effort are required? 
(Section 6) 
? How much do lists of names contribute to 
performance? (Section 7) 
316 
2 Algorithms and Data 
2.1 Task Definition and Data 
The named entity (NE) task used for this 
evaluation requires the system to identify all 
named locations, named persons, named 
organizations, dates, times, monetary amounts, 
and percentages. The task definition is given in 
Chinchor, et al (1998). 
For speech recognition, roughly 175 hours of 
news broadcasts (roughly 1.2m words of audio) 
were available from the National Institute for 
Science and Technology (NIST) for training. 
All of that data includes both the audio and a 
manual transcription. The test set consisted of 3 
hours of news (roughly 25k words). 
For the combined OCR/NE system, the OCR 
component was trained on the University of 
Washington English Image Database, which is 
comprised primarily of technical journal 
articles. The NE system was trained separately 
on 690K words of 1993 Wall Street Journal 
(WSJ) data (roughly 1250 articles), including 
development data from the Sixth Message 
Understanding Conference (MUC-6) Named 
Entity evaluation. The test set was 
approximately 20K words of separate WSJ data 
(roughly 45 articles), also taken from the MUC- 
6 data set. Both test and training texts were 
original text (no OCR errors) in mixed case with 
normal punctuation. Printing the on-line text, 
rather than using the original newsprint, 
produced the images for OCR, which were all 
scanned at 600 DPI. 
2.2 Algorithms 
The information extraction system tested is 
IdentiFinder(TM), which has previously been 
detailed in Bikel et al (1997, 1999). In that 
system, an HMM labels each word either with 
one of the desired classes (e.g., person, 
organization, etc.) or with the label NOT-A- 
NAME (to represent "none of the desired 
classes"). The states of the HMM fall into 
regions, one region for each desired class plus 
one for NOT-A-NAME. (See Figure 2-1.) The 
HMM thus has a model of each desired class 
and of the other text. Note that the 
implementation is not confined to the seven 
name classes used in the NE task; the particular 
classes to be recognized can be easily changed 
via a parameter. 
Within each of the regions, we use a statistical 
bigram language model, and emit exactly one 
word upon entering each state. Therefore, the 
number of states in each of the name-class 
regions is equal to the vocabulary size. 
Additionally, there are two special states, the 
START-OF-SENTENCE and END-OF-SENTENCE 
states. In addition to generating the word, states 
may also generate f atures of that word. 
START-OF-SENTENCE END-OF SENTENCE 
Figure 2-1: Pictorial representation of conceptual model 
317 
3 Effect of Textual Clues 
The output of each of the speech recognizers i  
in SNOR (speech normalized orthographic 
representation) format, a format which is largely 
unpunctuated and in all capital letters 
(apostrophes and periods after spoken letters are 
preserved). When a typical NE extraction 
system runs on ordinary English text, it uses 
punctuation and capitalization as features that 
contribute to its decisions. In order to learn how 
much degradation i performance is caused by 
the absence of these features from SNOR 
format, we performed the following experiment. 
We took a corpus that had full punctuation and 
mixed case and preprocessed it to make three 
new versions: one with all upper case letters but 
punctuation preserved, one with original case 
but punctuation marks removed, and one with 
both case and punctuation removed. We then 
partitioned all four versions of the corpus into a 
training set and a held-out est set, using the 
same partition in all four versions, and 
measured I entiFinder's performance. 
The corpus we used for this experiment was the 
transcriptions of the second 100 hours of the 
Broadcast News acoustic modelling data, 
comprising 114 episodes. We partitioned this 
data to form a training set of 98 episodes 
(640,000 words) and a test set of 16 episodes 
(130,000 words). Because the test transcriptions 
were created by humans, they have a 0% word 
error rate. The results are shown in Table 3-1. 
The removal of case information has the greater 
effect, reducing performance by 2.3 points, 
while the loss of punctuation reduces 
performance by 1.4 points. The loss from 
removing both features is 3.4 points, less than 
the sum of the individual degradations. This 
suggests that there are some events where both 
mixed case and punctuation are required to lead 
IdentiFinder to the correct answer. 
Mixed Upper 
Case Case 
With punctuation 92.4 90.1 
Without punctuation 91.0 89.0 
Table 3-1: Effect of case and punctuation on 
performance(F-measure) on Broadcast News 
data 
It should be noted that because the data are 
transcriptions of speech, no version of the 
corpus contains all the textual clues that would 
appear in newspaper text like the MUC-7 New 
York Times data. In particular, numbers are 
written out in words as they would be spoken, 
not represented using digits, and abbreviations 
such as "Dr.", "Jr." or "Sept." are expanded out 
to their full spoken word. We conclude that the 
degradation in performance going from 
newspaper text to SNOR recognizer output is at 
least 3.4 points in the 0% WER case, and 
probably more due to these other missing text 
clues. 
4 Effect of Word Errors 
4.1 Optical Character 
(OCR) 
Recognition 
The OCR experiments were performed using the 
system described in Makhoul et al (1998). 
Recognition was performed at the character 
level, rather than the word level, so the 
vocabulary is not closed (unlike the ASR results 
discussed in subsequent sections). Figure 4-1 
shows IdentiFinder's performance under 4 
conditions of varying word error ate (WER): 
1. Original text (no OCR, 0% WER) 
2. OCR from high-quality (laser-printed) text 
images (2.7% WER) 
3. OCR on degraded images (13.7% WER). 
4." OCR on degraded images, processed with a 
weak character language model (19.1% 
WER) 
For the second and third conditions, 1.3M 
characters of Wall Street Journal were used for 
318 
OCR language model training: the fourth 
condition used a much weaker character 
language model, which accounts for the poorer 
performance. 
The interpolated line has been fit to the 
performance of the OCR-based systems, with a 
slope indicating 0.6 points of F-measure lost for 
each percentage point increase in word error. 
The line has been extrapolated to 0% WER: the 
actual 0% WER condition is 95.4, which only 
slightly exceeds the projected value. 
100 
95 ~ 
75 
7O 
5 10 15 20 25 30 
Word Error Rate 
Figure 4-1: IdentiFinder Named Entity 
performance as a function of OCR word 
error rate 
4.2 Automatic Speech Recognition 
(ASR) 
Figure 5-1 shows IdentiFinder's performance on 
all speech systems in the 1998 Hub-4 
evaluations (Przybocki, et al, 1999). These 
experiments were run in co-operation with 
NIST. The interpolated line has been fit to the 
errorful transcripts, and then extrapolated out to 
0% WER speech. As can be seen, the line fits 
the data extremely well, and has a slope of 0.7 
points of F-measure lost for each additional 1% 
of word error rate. The fact that the extrapolated 
' These figures do not reflect the best possible 
performance of the OCR system: for example, when 
testing on degraded ata, it would be usual to include 
representative data in training. This was not a 
concern for this experiment, however, which focussed 
on name finding performance. 
line slightly overestimates the actual 
performance at 0% WER (given by a A) 
indicates that the degradation may be sub-linear 
in the range 0-15% WER. 
leO 
95 
9o 
i= ,E 
~- 80 
75 
B 
E i i 
0 5 10 15 20 25 
Wo~ error r~e (Hub4 E~ 9e) 
Figure 4-2: IdentiFinder named-entity 
performance as a function of word error rate 
(in cooperation with NIST) 
5 Out of Vocabulary Rates for Names 
It is generally agreed that out-of-vocabulary 
(OOV) words do not have a major impact on the 
word error rate achieved by large vocabulary 
speech recognizers doing transcription. The 
reason is that speech lexicons are designed to 
include the most frequent words, thus ensuring 
that OOV words will represent only a small 
fraction of the words in any test set. However, 
we have seen that the ,OOV rate for words that 
are part of named-entities can be as much as a 
factor of ten greater than the baseline OOV for 
non-name words. This could make OOV a 
major problem for NE extraction from speech. 
To explore this, we measured the percentage of
names in the Broadcast News data that contain 
at least one OOV word as a function of lexicon 
size. For this purpose, we built lexicons simply 
by ordering the words of the 1998 Hub-4 
Language Modeling data according to 
30 
319 
Name Category Lexicon Size 
5K 10K 20K 40K 60K 80K 100K 120K 
PERSON 
ORGANIZATION 
LOCATION 
TIME 
MONEY 
DATE 
PERCENT 
34.7 52.7 69.9 85.1 89.4 91.1 91.9 93.9 
73.2 90.2 94.2 97.5 98.2 98.5 98.7 98.8 
76.6 87.1 92.2 96.2 97.5 98.0 98.8 99.1 
97.0 97.0 99.0 100 100 100 100 100 
94.4 98.2 98.8 100 100 100 100 100 
96.1 99.3 99.8 100 100 100 100 100 
98.9 99.3 I00 100 100 100 100 100 
Table 5-1: Percentage of in-vocabulary events as a function of lexicon size. 
frequency, and truncating the list at various 
lengths. The percentage of in-vocabulary events 
of each type as a function of lexicon size is 
shown in Table 5-1. 
Most modem speech recognizers employ a 
vocabulary of roughly 60,000 words; using a 
larger lexicon introduces more errors from 
acoustic perplexity than it fixes through 
enlarged vocabulary. It is clear from the table 
that the only name category that might suffer a 
significant OOV problem with a 60K 
vocabulary is PERSONs. One might imagine 
that a more carefully constructed lexicon could 
reduce the OOV rate for PERSONs while still 
staying within the 60,000 word limit. However, 
even if a cleverly designed 60K lexicon 
succeeded in having the name coverage of the 
frequency-ordered 120K word lexicon (which 
contains roughly 40,000 more proper names 
than the 60K lexicon), it would reduce the 
PERSON OOV rate by only 4% absolute. 
6 Effect of training set size 
6.1 Automatic Speech Recognition 
We have measured NE performance in the 
context of speech as a function of training set 
size and found that the performance increases 
logarithmically with the amount of training data 
for 15% WER test data as well as for 0% WER 
input. However the growth rate is slower for 
15% WER test data. We constructed small 
training sets of various size by randomly 
selecting sets of 6, 12, 25, and 49 episodes from 
the second 100 hours of annotated Broadcast 
News training data. We also defined a training 
set of 98 episodes from the second 100 hours, as 
well as sets containing the full 98 episodes plus 
some or all of the first 100 hours of Broadcast 
News training. Our largest training set contained 
1.2 million words, and our smallest a mere 
30,000 words. All training data were converted 
to SNOR format. 
Given that PERSONs account for roughly 50% 
of the named-entities in broadcast news, the 
maximum gain in F measure available for 
doubling the lexicon size is 2 points. Moreover, 
this gain would require that every PERSON 
name added to the vocabulary be recognized 
properly -- an unlikely prospect, since most of 
these words will not appear in the acoustic 
training for the recognizer. For these reasons, 
we conclude that the OOV problem is not a 
major factor in determining NE performance 
from speech. 
For each training set, we trained a separate 
IdentiFinder model and evaluated it on two 
versions of the 1998 Hub4-IE data -- the 0% 
WER transcription created by a human, and an 
ASR transcript with 15%. The results are 
plotted in Figure 6-1. The slopes of the 
interpolated lines predict that IdentiFinder's 
performance on 15% WER speech will increase 
by 1.5 points for each additional doubling of the 
training data, while performance goes up 1.8 
points per doubling of the training for perfect 
speech input. 
320 
100 
95 
0 L .  
$ 
E 
90 
85 
80 
75 
70 
v . - -  " 
? e. 
? 15% WER 
? 0% WER 
i \[ 
10000 100000 1000000 10000000 
Number of training words (log scale) 
Figure 6-1: Performance as a 
Possibly, the difference in slope of the two lines 
is that the real value of increasing the training 
set lies in increasing the number of distinct rare 
names that appear. Once an example is in the 
training, IdentiFinder is able to extract it and use 
it in test. However, when the test data is 
recognizer output, the rare names are less likely 
to appear in the test, either because they don't 
appear in the speech lexicon or they are poorly 
trained in the speech model and misrecognized. 
If they don't appear in the test, IdentiFinder can't 
make full use of the additional training, and thus 
performance on errorful input increases more 
slowly than it does on error-free input text. 
6.2 Optical Character Recognition 
A similar relationship between training size and 
performance is seen for the OCR test condition. 
function of training data for speech. 
The training was partitioned by documents into 
equal sized sets: 
Partition size Training Size 
Eighth 77.5 K words 
Quarter 155 K words 
Half 310 K words 
Whole 620 K words 
Using the same test set, each partition was used 
to train a separate model, which was then 
evaluated on the different word error conditions: 
performance was then averaged across each 
partition size to produce the data points below. 
Input Word Error Rate (WER) Eighth Quarter Half Whole 
0% WER (Original text) 92.4 93.7 94.3 95.3 
2.7% WER 90.0 90.8 91.6 92.5 
13.7% WER 84.3 85.2 86.0 86.6 
19.1% WER 79.6 80.4 80.8 82.5 
321 
100 
95 
9O 
8O 
75 
70 
10000 
Performance as a function of training size 
n - - ' - ' '~ -~ ' '~  .....m ..--B--'-- 
A0% WER 
\[\] 2.7% WER 
X 13.7% WER 
O19.1% WER 
100000 1000000 
Number of training words (log scala) 
Figure 6-2: Performance as a function of training data for OCR. 
While this graph of this data in Figure 6-2 
shows a logarithmic improvement, as with the 
ASR experiments, the rate of improvement is
substantially less, roughly 0.9 increase in F- 
measure for doubling the training data. This 
may be explained by the difference in difficulty 
between the two tests: even with only 77.5k 
words of training, the 0% WER performance 
exceeds the ASR system trained on 1.2M words. 
full point, while on recognizer produced output, 
performance goes u~p by only 0.3 points. 
/ 0% WER 15% WER 
Without lists 89.5 81.9 
With lists 90.5 82.2 
Table 7-1: Effect of lists in the presence of 
speech errors. 
8 Related Work  and Future Work 
7 Effect of Lists 
Like most NE extraction systems, IdentiFinder 
can use lists of strings of known to be names to 
estimate the probability that a word will be a 
name, given that it appears on a particular list. 
We trained two models on 1.2 million words of 
SNOR data, one with lists and one without. We 
tested on the human transcription (0% WER) 
and the ASR (15% WER) versions of the 1998 
evaluation transcripts. Table 7-1 shows the 
results. We see that on human constructed 
transcripts, lists improve the performance by a 
To our knowledge, no other information 
extraction technology has been applied to OCR 
material. 
For audio materials, three related efforts were 
benchmarked on NE extraction from broadcast 
news. Palmer, et al (1999) employs an HMM 
very similar to that reported for IdentifFinder 
(Bikel et al, 1997,1999). Renals et al (1999) 
reports on a rule-based system and an HMM 
integrated with a speech recognizer. Appelt and 
Martin (1999) report on the TEXTPRO system, 
which recognises names using manually written 
finite state sales. 
322 
Of these, the Palmer system and TEXTPRO 
report results on five different word error rates. 
Both degrade linearly, about .7F, with each 1% 
increase in WER from ASR. None report the 
effect of training set size, capitalization, 
punctuation, or out-of-vocabulary items. 
Of the four systems, IdentiFinder epresents 
state-of-the-art performance. Of all the systems 
evaluated, those with the simple architecture of 
ASR followed by information extraction 
performed markedly better than the system 
where extraction was more integrated with 
ASR. 
In general, these results compare favorably with 
results reported in the Message Understanding 
Conference (Chinchor, et al, 1998). The 
highest NE score in MUC-7 was 93.39; for 0% 
WER, our best score was 90.5 without case and 
punctuation which costs about 3.4 points. 
9 Conclusions 
First and foremost, the hidden Markov model is 
quite robust in the face of errorful input. 
Performance on both speech and OCR input 
degrades linearly as a function of word error. 
Even, without case information or punctuation 
in the input, the performance on the broadcast 
news task is above 90%, with only a 3.4 point 
degradation in performance due to missing 
textual clues. Performance even with 15% word 
error degrades by only about 8 points of F for 
both OCR and ASR systems. 
Second, because annotation can be performed 
quickly and inexpensively by non-experts, 
training-based systems like IdentiFinder offer a 
powerful advantage in moving to new languages 
and new domains. In our experience, annotation 
of English typically proceeds at 5k words per 
hour or more. This means interesting 
performance an be achieved with as little as 20 
hours of student annotation (i.e., at least 100k 
words). Increasing training continually 
improves performance, generally as the 
logarithm of the training set size. On 
transcribed speech, performance is already good 
(89.3 on 0% WER) with only 100 hours or 
643K words of training data. 
Third, though errors due to words out of the 
vocabulary of the speech recognizer are a 
problem, they represent only about 15% of the 
errors made by the combined speech recognition 
and named entity system. 
Fourth, we used exactly the same training data, 
modeling, and search algorithm for errorful 
input as we do for error-free input. For OCR, 
we trained on correct newswire once only for 
both correct text input 0% (WER) and for a 
variety of errorful text input conditions. For 
speech, we simply transformed text training data 
into SNOR format and retrained. Using this 
approach, the only cost of handling errorful 
input from OCR or ASR was a small amount of 
computing time. There were no rules to rewrite, 
no lists to change, and no vocabulary 
adjustments. Even so, the degradation in 
performance on errorful input is no worse than 
the word error rate of the OCR/ASR system. 
Acknowledgments 
The work reported here was supported in part by 
the Defense Advanced Research Projects 
Agency. Technical agent for part of this work 
was NRaD under contract number N66001-97- 
D-8501. The views and conclusions contained 
in this document are those of the authors and 
should not be interpreted as necessarily 
representing the official policies, either 
expressed or implied, of the Defense Advanced 
Research Projects Agency or the United States 
Government. 
References 
D. E. Appelt, D. Martin, "Named Entity Extraction 
from Speech: Approach and Results Using the 
Text.Pro System," Proceedings Of The DARPA 
Broadcast News Workshop, February 28-March 3, 
Morgan Kaufmann Publishers, pp 51-54 (1999). 
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel, 
'Nymble: a High-Performance L arning Name- 
finder". In Fifth Conference on Applied Natural 
Language Processing, (published by ACL) pp 194- 
201 (1997). 
323 
D. Bikel, R. Schwartz, and R. Weischedel, "An 
Algorithm that Learns What's in a Name," 
Machine Learning 34, pp 211-231, (1999). 
N. Chinchor, "MUC-7 Named Entity Task Definition 
Version 3.5". Available by ftp from 
ftp.muc.saic.com/pub/MUC/MUC7-guidelines. 
(1997). 
N. Chincor, P. Robinson, E. Brown, "HUB-4 Named 
Entity Task Definition Version 4.8". Available by 
ftp from www.nist.gov/speech/hub4_98. (1998). 
J. Makhoul, R. Schwartz, C. Lapre, and I. Bazzi, "A 
Script-Independent Methodology for Optical 
Character Recognition,", Pattern Recognition, pp 
1285-1294 (1998). 
Z. Lu, R. Schwartz, P. Natarajan, I. Bazzi, J. 
Makhoul, "Advances in the BBN BYBLOS OCR 
System," Proceedings of the International 
Conference on Document Analysis and 
Recognition, (1999). 
D. D. Palmer, J. D. Burger, M. Ostendorf, 
"Information Extraction from Broadcast News 
Speech Data," Proceedings Of The DARPA 
Broadcast News Workshop, February 28-March 3, 
Morgan Kaufmann Publishers, pp 41-46 (1999). 
M. A. Przybocki, J. G. Fiscus, J. S. Garofolo, D. S. 
Pallett, "1998 Hub-4 Information Extraction 
Evaluation," Proceedings Of The DARPA 
Broadcast News Workshop, February 28-March 3, 
Morgan Kaufmann Publishers, pp 13-18 (1999). 
S. Renals, Y. Gotoh, R. Gaizauskas, M. Stevenson, 
"Baseline IE-NE Experiments Using the 
SPRACH/LASIE System," Proceedings Of The 
DARPA Broadcast News Workshop, February 28- 
March 3, Morgan Kaufmann Publishers, pp 47-50 
(1999). 
324 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 857?866,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Language and Translation Model Adaptation using Comparable Corpora
Matthew Snover and Bonnie Dorr
Laboratory for Computational Linguistics
and Information Processing
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, MD 20742
{snover, bonnie}@umiacs.umd.edu
Richard Schwartz
BBN Technologies
10 Moulton Street
Cambridge, MA 02138, USA
schwartz@bbn.com
Abstract
Traditionally, statistical machine translation
systems have relied on parallel bi-lingual data
to train a translation model. While bi-lingual
parallel data are expensive to generate, mono-
lingual data are relatively common. Yet mono-
lingual data have been under-utilized, having
been used primarily for training a language
model in the target language. This paper de-
scribes a novel method for utilizing monolin-
gual target data to improve the performance
of a statistical machine translation system on
news stories. The method exploits the exis-
tence of comparable text?multiple texts in
the target language that discuss the same or
similar stories as found in the source language
document. For every source document that is
to be translated, a large monolingual data set
in the target language is searched for docu-
ments that might be comparable to the source
documents. These documents are then used
to adapt the MT system to increase the prob-
ability of generating texts that resemble the
comparable document. Experimental results
obtained by adapting both the language and
translation models show substantial gains over
the baseline system.
1 Introduction
While the amount of parallel data available to train a
statistical machine translation system is sharply lim-
ited, vast amounts of monolingual data are generally
available, especially when translating to languages
such as English. Yet monolingual data are generally
only used to train the language model of the trans-
lation system. Previous work (Fung and Yee, 1998;
Rapp, 1999) has sought to learn new translations for
words by looking at comparable, but not parallel,
corpora in multiple languages and analyzing the co-
occurrence of words, resulting in the generation of
new word-to-word translations.
More recently, Resnik and Smith (2003)
and Munteanu and Marcu (2005) have exploited
monolingual data in both the source and target
languages to find document or sentence pairs that
appear to be parallel. This newly discovered bilin-
gual data can then be used as additional training data
for the translation system. Such methods generally
have a very low yield leaving vast amounts of data
that is only used for language modeling.
These methods rely upon comparable corpora,
that is, multiple corpora that are of the same gen-
eral genre. In addition to this, documents can be
comparable?two documents that are both on the
same event or topic. Comparable documents occur
because of the repetition of information across lan-
guages, and in the case of news data, on the fact that
stories reported in one language are often reported
in another language. In cases where no direct trans-
lation can be found for a source document, it is of-
ten possible to find documents in the target language
that are on the same story, or even on a related story,
either in subject matter or historically. Such docu-
ments can be classified as comparable to the origi-
nal source document. Phrases within this compara-
ble document are likely to be translations of phrases
in the source document, even if the documents them-
selves are not parallel.
Figure 1 shows an excerpt of the reference trans-
lation of an Arabic document, and figure 2 shows a
857
Cameras are flashing and reporters are following up, for
Hollywood star Angelina Jolie is finally talking to the pub-
lic after a one-month stay in India, but not as a movie star.
The Hollywood actress, goodwill ambassador of the United
Nations high commissioner for refugees, met with the In-
dian minister of state for external affairs, Anand Sharma,
here today, Sunday, to discuss issues of refugees and chil-
dren. ... Jolie, accompanied by her five-year-old son, Mad-
dox, visited the refugee camps that are run by the Khalsa
Diwan Society for social services and the high commis-
sioner for refugees Saturday afternoon after she arrived in
Delhi. Jolie has been in India since October 5th shooting
the movie ?A Mighty Heart,? which is based on the life of
Wall Street Journal correspondent Daniel Pearl, who was
kidnapped and killed in Pakistan. Jolie plays the role of
Pearl?s wife, Mariane.
Figure 1: Excerpt of Example Reference Translation of
an Arabic Source Document
comparable passage.1 In this case, the two new sto-
ries are not translations of each other and were not
reported at the same time?the comparable passage
being an older news story?but both discuss actress
Angelina Jolie?s visit to India. Many phrases and
words are shared between the two, including: the
name of the movie, the name and relationship of the
actress? character, the name and age of her son and
many others. Such a pairing is extremely compara-
ble, although even less related document pairs could
easily be considered comparable.
We seek to take advantage of these comparable
documents to inform the translation of the source
document. This can be done by augmenting the ma-
jor components of the statistical translation system:
the Language Model and the Translation Model.
This work is in the same tradition as Kim and
Khudanpur (2003), Zhao et al (2004), and Kim
(2005). Kim (2005) used large amounts of compa-
rable data to adapt language models on a document-
by-document basis, while Zhao et al (2004) used
comparable data to perform sentence level adapta-
tion of the language model. These adapted lan-
guage models were shown to improve performance
1This is an actual source document from the tuning set used
in our experiments, and the first of a number of similar passages
found by the comparable text selection system described in sec-
tion 2.
Actress Angelina Jolie hopped onto a crowded Mumbai
commuter train Monday to film a scene for a movie about
slain journalist Daniel Pearl, who lived and worked in In-
dia?s financial and entertainment capital. Hollywood actor
Dan Futterman portrays Pearl and Jolie plays his wife Mar-
iane in the ?A Mighty Heart? co-produced by Plan B, a pro-
duction company founded by Brad Pitt and his ex-wife, ac-
tress Jennifer Aniston. Jolie and Pitt, accompanied by their
three children ? Maddox, 5, 18-month-old Zahara and 5-
month-old Shiloh Nouvel ? arrived in Mumbai on Saturday
from the western Indian city Pune where they were shooting
the movie for nearly a month. ...
Figure 2: Excerpt of Example Comparable Document
for both automatic speech recognition as well as ma-
chine translation.
In addition to language model adaptation we
also modify the translation model, adding additional
translation rules that enable the translation of new
words and phrases in both the source and target lan-
guages, as well as increasing the probability of ex-
isting translation rules. Translation adaptation us-
ing the translation system?s own output, known as
Self-Training (Ueffing, 2006) has previously shown
gains by augmenting the translation model with ad-
ditional translation rules. In that approach however,
the translation model was augmented using parallel
data, rather than comparable data, by interpolating
a translation model trained using the system output
with the original translation model.
Translation model adaptation using comparable
out-of-domain parallel data, rather than monolingual
data was shown by Hildebrand et al (2005) to yield
significant gains over a baseline system. The trans-
lation model was adapted by selecting comparable
sentences from parallel corpora for each of the sen-
tences to be translated. In addition to selecting out-
of-domain data to adapt the translation model, com-
parable data selection techniques have been used to
select and weight portions of the existing training
data for the translation model to improve translation
performance (Lu et al, 2007).
The research presented in this paper utilizes a dif-
ferent approach to translation model adaptation us-
ing comparable monolingual text rather than parallel
text, exploiting data that would otherwise be unused
858
for estimating the translation model. In addition,
this data also informs the translation system by in-
terpolating the original language model with a new
language model trained from the same comparable
documents.
We discuss the selection of comparable text for
model adaptation in section 2. In sections 3.1
and 3.2, we describe the model adaptation for the
language model and translation model, respectively.
Experimental results describing the application of
model adaptation to a hierarchical Arabic-to-English
MT system are presented in section 4. Finally we
draw conclusions in sections 5.
2 Comparable Text Selection
Comparable text is selected for every source doc-
ument from a large monolingual corpus in the tar-
get language. In practice, one could search the
World Wide Web for documents that are compara-
ble to a set of source documents, but this approach
presents problems for ensuring the quality of the re-
trieved documents. The experiments in this paper
use comparable text selected from a collection of
English news texts. Because these texts are all flu-
ent English, and of comparable genre to the test set,
they are also used for training the standard language
model training.
The problem of selecting comparable text has
been widely studied in the information retrieval
community and cross-lingual information retrieval
(CLIR) (Oard and Dorr, 1998; Levow et al, 2005)
has been largely successful at the task of selecting
comparable or relevant documents in one language
given a query in another language. We use CLIR to
select a ranked list of documents in our target lan-
guage, English in the experiments described in this
paper, for each source document, designated as the
query in the CLIR framework, that we wish to trans-
late.
The CLIR problem can be framed probabilisti-
cally as: Given a query Q, find a document D that
maximizes the equation Pr(D is rel|Q). This equa-
tion can be expanded using Bayes? Law as shown
in equation 1. The prior probability of a document
being relevant can be viewed as uniform, and thus
in this work, we assume Pr(D is rel) is a constant.2
2In fact, it can be beneficial to use features of the document
The Pr(Q) is constant across all documents. There-
fore finding a document to maximize Pr(D is rel|Q)
is equivalent to finding a document that maximizes
Pr(Q|D is rel).
Pr(D is rel|Q) =
Pr(D is rel) Pr(Q|D is rel)
Pr(Q)
(1)
A method of calculating the probability of a query
given a document was proposed by (Xu et al, 2001)3
and is shown in Equation 2. In this formulation, each
foreign word, f , in the query is generated from the
foreign vocabulary with probability ? and from the
English document with probability 1 ? ?, where ?
is a constant.4 The probability of f being generated
by the general foreign vocabulary, F , is Pr(f |F ) =
freq(f, F )/|F |, the frequency of the word f in the
vocabulary divided by the size of the vocabulary.
The probability of the word being generated by the
English document is the sum of the probabilities of it
being generated by each English word, e, in the doc-
ument which is the frequency of the English word in
the document, (Pr(e|D) = freq(e,D)/|D|) multi-
plied by the probability of the translation of the En-
glish word to the foreign word, Pr(f |e).
Pr(Q|D) =
?
f?Q
(?Pr(f |F )+ (2)
(1? ?)
?
e
Pr(e|D) Pr(f |e))
This formulation favors longer English docu-
ments over shorter English documents. In addition,
many documents cover multiple stories and topics.
For the purposes of adaptation, shorter, fully com-
parable documents are preferred to longer, only par-
tially comparable documents. We modify the CLIR
system by taking the 1000 highest ranked target lan-
guage documents found by the CLIR system for
each source document, and dividing them into over-
lapping passages of approximately 300 words.5 Sen-
to estimate Pr(D is rel) (Miller and Schwartz, 1998) but we
have not explored that here.
3Xu et al (2001) formulated this for the selection of foreign
documents given an English query. We reverse this to select
English documents given a foreign query.
4As in Xu et al (2001), a value of 0.3 was used for ?.
5The length of 300 was chosen as this was approximately
the same length as the source documents.
859
tence boundaries are preserved when creating pas-
sages, insuring that the text is fluent within each pas-
sage. These passages are then scored again by the
CLIR system, resulting in a list of passages of about
300 words each for each source document. Finally,
we select the top N passages to be used for adapta-
tion.
The N passages selected by this method are not
guaranteed to be comparable and are often largely
unrelated to the story or topic in the source docu-
ment. We shall refer to the set of passages selected
by the CLIR system as the bias text to differentiate
it from comparable text, as the adaptation methods
will use this text to bias the MT system so that its
output will be more similar to the bias text.
While we have not conducted experiments using
other CLIR systems, the adaptation methods pre-
sented in this paper could be applied without modifi-
cation using another CLIR system, as the adaptation
method treats the CLIR system as a black box. With
the exception of running a second pass of CLIR, we
use the algorithm of Xu et al (2001) without any
significant modification, including the use of a stop
word list for both the English and foreign texts. The
parameters for Pr(f |F ) and Pr(f |e) were estimated
using the same parallel data that our translation sys-
tem was trained on.
The bias text selected for a source document is
used to adapt the language model (described in sec-
tion 3.1) and the translation model (described in sec-
tion 3.2) when translating that source document.
3 Model Adaptation
We use the same bias text to adapt both the lan-
guage model and the translation model. For lan-
guage model adaptation, we increase the probability
of the word sequences in the bias text, and for trans-
lation model adaptation we use additional phrasal
translation rules. The adaptations can be done in-
dependently and while they can augment each other
when used together, this is not required. It is not
necessary to use the same number of passages for
both forms of adaptation, although doing so makes
it more likely both that the English side of the new
translation rule will be assigned a high probability
by the adapted language model, and that the transla-
tion model produces the English text to which the
language model has been adapted. Bias text that
is used by one adaptation but not the other will re-
ceive no special treatment by the other model. This
could result in new translation rules that produce text
to which the language assigns low probability, or it
could result in the language model being able to as-
sign a high probability to a good English translation
that cannot be produced by the translation model due
to a lack of necessary translation rules.
While both adaptation methods are integrated into
a hierarchical translation model (Chiang, 2005),
they are largely implementation independent. Lan-
guage model adaptation could be integrated into any
statistical machine translation that uses a language
model over words, while translation model adapta-
tion could be added to any statistical machine trans-
lation that can utilize phrasal translation rules.
3.1 Language Model Adaptation
For every source document, we estimate a new lan-
guage model, the bias language model, from the cor-
responding bias text. Since this bias text is short, the
corresponding bias language model is small and spe-
cific, giving high probabilities to those phrases that
occur in the bias text. The bias language model is
interpolated with the generic language model that
would otherwise be used for translation if no LM
adaptation was used. The new bias language model
is of the same order as the generic language model,
so that if a trigram language model is used for the
MT decoding, then the biased language model will
also be a trigram language model. The bias lan-
guage model is created using the same settings as
the generic language model. In our particular im-
plementation however, the generic language model
uses Kneser-Ney smoothing, while the biased lan-
guage model uses Witten-Bell smoothing due to im-
plementation limitations. In principle the biased lan-
guage model can be smoothed in the same manner as
the generic language model.
We interpolate the bias language model and
the generic language model as shown in equa-
tion 3, where Prg and Prb are the probabilities
from the generic language model and the bias lan-
guage model, respectively. A constant interpolation
weight, ? is used to weight the two probabilities for
all documents. While a value for ? could be cho-
sen that minimizes perplexity on a tuning set, in a
860
similar fashion to Kim (2005), it is unclear that such
a weight would be ideal when the interpolated lan-
guage model is used as part of a statistical translation
system. In practice we have observed that weights
other than one that minimizes perplexity, typically a
lower weight, can yield better translation results on
the tuning set.
Pr(e) = (1? ?) Pr
g
(e) + ?Pr
b
(e) (3)
The resulting interpolated language model is then
used in place of the generic language model in the
translation process, increasing the probability that
the translation output will resemble the bias text. It
is important to note that, unlike the translation model
adaptation described in section 3.2, no new infor-
mation is added to the system with language model
adaptation. Because the bias text is extracted from
the same monolingual corpus that the generic lan-
guage model was estimated from, all of the word se-
quences used for training the bias language model
were also used for training the generic language
model. Language model adaptation only increases
the weight of the portion of the language model data
that was selected as comparable.
3.2 Translation Model Adaptation
It is frequently the case in machine translation that
unknown words or phrases are present in the source
document, or that the known translations of source
words are based on a very small number of oc-
currences in the training data. In other cases,
translations may be known for individual words in
the source document, but not for longer phrases.
Translation model adaptation seeks to generate new
phrasal translation rules for these source words and
phrases. The bias text for a source document may,
if comparable, contain a number of English words
and phrases that are the English side of these desired
rules.
Because the source data and the bias text are
not translations of each other and are not sen-
tence aligned, conventional alignment tools, such as
GIZA++ (Och and Ney, 2000), cannot be used to
align the source and bias text. Because the passages
in the bias text are not translations of the source doc-
ument, it will always be the case that portions of the
source document have no translation in the bias text,
and portions of the bias text have no translation in
the source document. In addition a phrase in one
of these texts might have multiple, differing transla-
tions in the other text.
Unlike language model adaptation, the entirety of
the bias text is not used for translation adaptation.
We extract those phrases that occur in at least M
of the passages in the bias texts. A phrase is only
counted once for every passage in which it occurs,
so that repeated use of a phrase within a passage
does not affect whether it used to generate new rules.
Typically, passages selected by the CLIR tend to be
very similar to each other if they are comparable
to the source document and are very different from
each other if they are not comparable to the source
document. Phrases that are identical across passages
are the ones that are most likely to be comparable,
whereas a phrase or word that occurs in only one
passage is likely to be present only by chance or if
the passage it is in is not comparable. Filtering the
target phrases to those that occur in multiple pas-
sages therefore serves not only to reduce the total
number of rules, but also to filter out phrases from
passages that are not comparable.
For each phrase in the source document we gener-
ate a new translation to each of the phrases selected
from the bias text, and assign it a low uniform prob-
ability.6 For each translation rule we also have a
lexical translation probability that we estimate cor-
rectly from the trained word model. These new rules
are then added to the phrase table of the existing
translation model when translating the source doc-
ument. Rather than adding probability to the ex-
isting generic rules, the new rules are marked as
bias rules by the system and given their own fea-
ture weight. While the vast majority of these rules
are incorrect translations, these incorrect rules will
be naturally biased against by the translation sys-
tem. If the source side of a translation already has a
number of observed translations, then the low prob-
ability of the new bias rule will cause it to not be
selected by the translation system. If the new trans-
lation rules would produce garbled English, then it
will be biased against by the language model. When
this is combined with the language model adapta-
6A probability of 1/700 is arbitrarily used for the bias rules
although it is then weighted by the bias translation rule weight.
861
tion, a natural pressure is exerted to use the bias rules
for source phrases primarily when it would cause the
output to look more like the bias text.
4 Experimental Results
We evaluated the performance of language and
translation model adaptation with our translation
system on two conditions, the details of which are
presented in section 4.1. One condition involved a
small amount of parallel training, such as one might
find when translating a less commonly taught lan-
guage (LCTL). The other condition involved the full
amount of training available for Arabic-to-English
translation. In the case of LCTLs we expect our
translation model to have the most deficiencies and
be most in need of additional translation rules. So,
it is under such a condition we would expect the
translation model adaptation to be the most bene-
ficial. We evaluate the system?s performance under
this condition in section 4.2. The effectiveness of
this technique on state-of-the-art systems, and its ef-
ficiency when used with a well trained generic trans-
lation model is presented in section 4.3.
4.1 Implementation Details
Both language-model and translation-model adap-
tation are implemented on top of a hierarchical
Arabic-to-English translation system with string-to-
dependency rules as described in Shen et al (2008).
While generalized rules are generated from the par-
allel data, rules generated by the translation model
adaptation are not generalized and are used only as
phrasal rules. A trigram language model was used
during decoding, and a 5-gram language model was
used to re-score the n-best list after decoding. In ad-
dition to the features described in Shen et al (2008),
a new feature is added to the model for the bias
rule weight, allowing the translation system to ef-
fectively tune the probability of the rules added by
translation model adaptation in order to improve per-
formance on the tuning set.
Bias texts were selected from three mono-
lingual corpora: the English Gigaword cor-
pus (2,793,350,201 words), the FBIS corpus
(28,465,936 words), and a collection of news archive
data collected from the websites of various on-
line, public news sites (828,435,409 words). All
three corpora were also part of the generic language
model training data. Language model adaptation
on both the trigram and 5-gram language models
used 10 comparable passages with an interpolation
weight of 0.1. Translation model adaptation used 10
comparable passages for the bias text and a value of
2 for M .
Each selected passage contains approximately
300 words, so in the case where 10 comparable pas-
sages are used to create a bias text, the resulting text
will be 3000 words long on average. The language
models created using these bias texts are very spe-
cific giving large probability to n-gram sequences
seen in those texts.
The construction of the bias texts increases the
overall run-time of the translation system, although
in practice this is a small expenditure. The most in-
tensive portion is the initial indexing of the monolin-
gual corpus, but this is only required once and can be
reused for any subsequent test set that is evaluated.
This index can then be quickly searched for com-
parable passages. When considering research envi-
ronments, test sets are used repeatedly and bias texts
only need to be built once per set, making the build-
ing cost negligible. Otherwise, the time required to
build the bias text is still small compared to the ac-
tual translation time.
All conditions were optimized using BLEU (Pap-
ineni et al, 2002) and evaluated using both BLEU
and Translation Edit Rate (TER) (Snover et al,
2006). BLEU is an accuracy measure, so higher
values indicate better performance, while TER is an
error metric, so lower values indicate better perfor-
mance. Optimization was performed on a tuning set
of newswire data, comprised of portions of MTEval
2004, MTEval 2005, and GALE 2007 newswire de-
velopment data, a total of 48921 words of English
in 1385 segments and 173 documents. Results were
measured on the NIST MTEval 2006 Arabic Evalu-
ation set, which was 55578 words of English in 1797
segments and 104 documents. Four reference trans-
lations were used for scoring each translation.
Parameter optimization method was done using n-
best optimization, although the adaptation process
is not tied to this method. The MT decoder is run
on the tuning set generating an n-best list (where
n = 300), on which all of the translation features
(including bias rule weights) are optimized using
862
Powell?s method. These new weights are then used
to decode again, repeating the whole process, using
a cumulative n-best list. This continues for several
iterations until performance on the tuning set stabi-
lizes. The resulting feature weights are used when
decoding the test set. A similar, but simpler, method
is used to determine the feature weights after 5-gram
rescoring. This n-best optimization method has sub-
tle implications for translation model adaptation. In
the first iteration, few bias rules are used in decoding
the 300-best, and those that are used frequently help,
although the overall gain is small due to the small
number of bias rules used. This causes the opti-
mizer to greatly increase the weight of the bias rules,
causing the decoder to overuse the bias rules in the
next iteration causing a sharp decrease in translation
quality. Several iterations are needed for the cumu-
lative n-best to achieve sufficient diversity and size
to assign a weight for the bias translation rules that
results in an increase in performance over the base-
line. Alternative optimization methods could likely
circumvent this process. Language model adapta-
tion does not suffer from this phenomenon.
4.2 Less Commonly Taught Language
Simulation
In order to better examine the nature of translation
model adaptation, we elected to work with a transla-
tion model that was trained on only 5 million words
of parallel Arabic-English text. Limiting the trans-
lation model training in this way simulates the prob-
lem of translating less commonly taught languages
(LCTL) where less parallel text is available, a situa-
tion that is not the case for Arabic. Since the model
is trained on less parallel data, it is lacking a large
number of translation rules, which is expected to be
addressed by the translation model adaptation. By
working in an environment with a more deprived
baseline translation model, we are giving the trans-
lation model adaptation more room to assist.
The experiments described below use a 5 million
word Arabic parallel text corpus constructed from
the LDC2004T18 and LDC2006E25 corpora. The
full monolingual English data were used for the lan-
guage model and for selection of comparable doc-
uments. Unless otherwise specified no language
model adaptation was used.
We first establish an upper limit on the gain us-
ing translation model adaptation, using the reference
data to adapt the translation system. These reference
data can be considered to be extremely comparable,
better than could ever be hoped to gain by compara-
ble document selection. We first aligned this data
using GIZA++ to the source data, simulating the
ideal case where we can perfectly determine which
source words translate to which comparable words.
Because our translation model adaptation system as-
signs uniform probability to all bias rules, we ignore
the correct rule probabilities that we could extract
from word alignment and assign uniform probabil-
ity to all of the bias translation rules. As expected,
this gives a large gain over the baseline.
We also examine limiting these new translation
rules to those rules whose target side occurs in the
top 100 passages selected by CLIR, thus minimiz-
ing the adaption to those rules that it theoretically
could learn from the bias text. On average, 50% of
the rules were removed by this filtering, resulting in
a corresponding 50% decrease in the gain over the
baseline. The results of these experiments and an
unadapted baseline are shown in table 1.
Test Set TM Adaptation TER BLEU
Tune None 0.4984 0.4080
Aligned Reference 0.3692 0.5841
Overlapping Only 0.4179 0.5138
MT06 None 0.5516 0.3468
Aligned Reference 0.4517 0.5216
Overlapping Only 0.4899 0.4335
Table 1: LCTL Aligned Reference Adaptation Results
The fair translation model adaptation system,
however, does not align source phrases to the cor-
rect bias text phrases in such a fashion, and instead
aligns all source words to all target words. To in-
vestigate the effect of this over production of rules,
we again used the reference translations as if they
were comparable data, but we ignored the align-
ments learned by GIZA++, and instead allowed all
source phrases to translate to all English phrases in
the reference text, with uniform probability. This
still shows large gains in translation quality over the
baseline, as measured by TER and BLEU. Again,
we also examined limiting the text used for transla-
tion model adaptation to those phrases that occur in
863
both the reference text and the top 100 comparable
passages selected the CLIR system. While this de-
creased performance, the system still performs sig-
nificantly better than the baseline, as shown in the
following table 2.
Test Set TM Adaptation TER BLEU
Tune None 0.4984 0.4080
Unaligned Ref. 0.4492 0.4566
Overlapping Only 0.4808 0.4313
MT06 None 0.5516 0.3468
Unaligned Ref. 0.5254 0.3990
Overlapping Only 0.5390 0.3695
Table 2: LCTL Unaligned Reference Adaptation Results
Applying translation model and language model
adaptation fairly, using only bias text from the com-
parable data selection, yields smaller gains on both
the tuning and MT06 sets, as shown in table 3.
The combination of language-model and translation-
model adaptation exceeds the gains that would be
achieved over the baseline by either method sepa-
rately.
Test Set Adaptation TER BLEU
Tune None 0.4984 0.4080
LM 0.4922 0.4140
TM 0.4916 0.4169
LM & TM 0.4888 0.4244
MT06 None 0.5516 0.3468
LM 0.5559 0.3490
TM 0.5545 0.3478
LM & TM 0.5509 0.3536
Table 3: LCTL Fair Adaptation Results
4.3 Full Parallel Training Results
While the simulation described in section 4.2 used
only 5 million words of parallel training, 230 mil-
lion words of parallel data from 18.5 million seg-
ments were used for training the full Arabic-to-
English translation system. This parallel data in-
cludes the LDC2007T08 ?ISI Arabic-English Auto-
matically Extracted Parallel Text? corpus (Munteanu
and Marcu, 2007), which was created from monolin-
gual corpora in English and Arabic using the algo-
rithm described in Munteanu and Marcu (2005), as
the techniques used in that work are separate and
independent from the adaptation methods we de-
scribe in this paper.7 Language model adaptation
and translation model adaptation were applied both
independently and jointly to the translation system,
and the results were evaluated against an unadapted
baseline, as shown in table 4.
While gains from language model adaptation
were substantial on the tuning set, on the MT06 test
set they are reduced to a 0.65% gain on BLEU and
a negligible improvement in TER. The translation
model adaptation performs better with 1.37% im-
provement in BLEU and a 0.26% improvement in
TER. This gain increases to a 2.07% improvement
in BLEU and a 0.64% improvement in TER when
language adaptation is used in conjunction with the
translation model adaptation, showing the impor-
tance of using both adaptation methods. While it
could be expected that a more heavily trained trans-
lation model might not require the benefit of lan-
guage and translation model adaptation, a more sub-
stantial gain over the baseline can be seen when both
forms of adaptation are used than in the case with
less parallel training?a difference of 2.07% BLEU
versus 0.68% BLEU.
Test Set Adaptation TER BLEU
Tune None 0.4339 0.4661
LM 0.4227 0.4857
TM 0.4351 0.4657
LM & TM 0.4245 0.4882
MT06 None 0.5146 0.3852
LM 0.5140 0.3917
TM 0.5120 0.3989
LM & TM 0.5082 0.4059
Table 4: Full Training Adaptation Results
Of the comparable passages selected by the CLIR
system for the MT06 test set in the full training
experiment, 16.3% were selected from the News
7The two methods are not directly comparable, and so we
do not make any attempt to do so. Munteanu and Marcu (2005)
creates new parallel corpora from two monolingual corpora.
This new parallel data is generally applicable for training a
translation model but does not target any particular test set. Our
adaptation method does not generate new parallel data, but cre-
ates a new specific translation model for a test document that is
being translated.
864
Archive corpus, 81.2% were selected from the En-
glish GigaWord corpus and 2.5% were selected from
the FBIS corpus. A slightly different distribution
was found for the Tuning set, where 17.8% of the
passages were selected from the News Archive cor-
pus, 77.1% were selected from the English Giga-
Word corpus, and 5.1% were selected from the FBIS
corpus.
5 Discussion
The reuse of a monolingual corpus that was already
used by a translation system for language model
training to perform both language and translation
model adaptation shows large gains over an un-
adapted baseline. By leveraging off of a CLIR sys-
tem, which itself contains no information not al-
ready given to the translation system,8 potentially
comparable passages can be found which allow im-
proved translation. Surprisingly, these gains are
largest when the baseline model is better trained, in-
dicating that a strong reliance of the adaptation on
the existing models.
One explantation for these counter-intuitive
results?larger gains in the full training scenario ver-
sus the LCTL scenario?is that the lexical probabili-
ties are better estimated in the former case. The bias
rules all have equal translation probability and only
vary in probability according to the lexical proba-
bility of the rules. Better estimates of these lexical
probabilities may enable the translation system to
more clearly distinguish between helpful and harm-
ful bias rules.
There are many clear directions for the improve-
ment of these methods. The current adaptation
method does not utilize the probabilities from the
CLIR system and treats the top-ranked passages all
as equally comparable regardless of the probabil-
ity assigned. Variable weighting of passages could
prove beneficial to both language model adaptation,
where the passages could be weighted proportion-
ally to the probability of the passage being relevant,
and translation model adaptation, where the require-
ment on repetition of phrases across passages could
be weighted, as could the probability of the new
8The probabilistic parameters of the CLIR system are esti-
mated from the same parallel corpora that is used to train the
generic translation model.
rules produced by the translation system. In ad-
dition, the CLIR score, among other possible fea-
tures such as phrase overlap, could be used to de-
termine those documents where no comparable pas-
sage could be detected and where it would be bene-
ficial to not adapt the models.
A clear limitation of using comparable documents
to adapt the language and translation model is that
comparable documents must be found. For many
source documents, none of the top passages found
by the CLIR system were comparable. We suspect
that while this will always occur to some extent, this
becomes more common as the monolingual data be-
comes less like the source data, such as when there is
a large time gap between the two. The full extent of
this and the effect of the level of document compa-
rability on translation remains an open question. In
addition, while newswire is an excellent source of
comparable text, it is unclear how well this method
can be used on newsgroups or spoken data, where
the fluency of the source text is diminished. When
translating news stories, this technique is not lim-
ited to major news events. While many of the events
discussed in the source data receive world-wide at-
tention, many are local events that are unreported
in the English comparable data used in our experi-
ments. Events of a similar nature or events involving
many of the same people often do occur in the En-
glish comparable data, allowing improvement even
when the stories are quite different.
The adaptation methods described in this paper
are not limited to a particular framework of statis-
tical machine translation, but have applicability to
any statistical machine translation system that uses
a language model or translation rules.
Acknowledgments
This work was supported, in part, by BBN Tech-
nologies under the GALE Program, DARPA/IPTO
Contract No. HR0011-06-C-0022. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the sponsor.
We are very grateful to all three reviewers for their
careful and thoughtful reviews.
865
References
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Pro-
ceedings of ACL, pages 263?270.
Pascale Fung and Lo Yuen Yee. 1998. An IR Approach
for Translating New Words from Nonparallel, Compa-
rable Texts. In Proceedings of COLING-ACL98, pages
414?420, August.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the Translation
Model for Statistical Machine Translation based on In-
formation Retrieval. In Proceedings of EAMT 2005,
Budapest, Hungary, May.
Woosung Kim and Sanjeev Khudanpur. 2003. Cross-
Lingual Lexical Triggers in Statistical Language Mod-
eling. In 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2003), pages
17?24, July.
Woosung Kim. 2005. Language Model Adaptation for
Automatic Speech Recognition and Statistical Machine
Translation. Ph.D. thesis, The Johns Hopkins Univer-
sity, Baltimore, MD.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2005. Dictionary-based cross-language retrieval. In-
formation Processing and Management, 41:523?547.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343?350.
T. Leek Miller and Richard Schwartz. 1998. BBN at
TREC7: Using Hidden Markov Models for Informa-
tion Retrieval. In TREC 1998, pages 80?89, Gaithers-
burg, MD.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Exploit-
ing Non-Parallel Corpora. Computational Linguistics,
31:477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2007. Isi
arabic-english automatically extracted parallel text.
Linguistic Data Consortium, Philadelphia.
Douglas W. Oard and Bonnie J. Dorr. 1998. Evaluat-
ing Cross-Language Text Retrieval Effectiveness. In
Gregory Grefenstette, editor, Cross-Language Infor-
mation Retrieval, pages 151?161. Kluwer Academic
Publishers, Boston, MA.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual Con-
ference of the Association for Computational Linguis-
tics, pages 440?447, Hongkong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Traslation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages
519?526.
Philip Resnik and Noah Smith. 2003. The Web as a
Parallel Corpus. Computational Linguistics, 29:349?
380.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
Nicola Ueffing. 2006. Using Monolingual Source-
Language to Improve MT Performance. In Proceed-
ings of IWSLT 2006.
Jinxi Xu, Ralpha Weischedel, and Chanh Nguyen. 2001.
Evaluating a Probabilistic Model for Cross-lingual In-
formation Retrieval. In Proceedings of SIGIR 2001
Conference, pages 105?110.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation via Structured Query Models. In Proceed-
ings of Coling 2004, pages 411?417, Geneva, Switzer-
land, Aug 23?Aug 27. COLING.
866
Using N-best Lists for Named Entity Recognition from Chinese Speech 
Lufeng ZHAI*, Pascale FUNG*, Richard SCHWARTZ?, Marine CARPUAT?, Dekai WU? 
 
* HKUST 
Human Language Technology Center 
Electrical & Electronic Engineering 
University of Science and Technology 
Clear Water Bay, Hong Kong 
{lfzhai,pascale}@ee.ust.hk 
? BBN Technologies 
9861 Broken Land Parkway  
Columbia, MD 21046 
                     U.S.A 
schwartz@bbn.com 
? HKUST 
Human Language Technology Center 
Department of Computer Science 
University of Science and Technology 
Clear Water Bay, Hong Kong 
{marine,dekai}@cs.ust.hk 
 
Abstract 
We present the first known result for named 
entity recognition (NER) in realistic large-
vocabulary spoken Chinese.  We establish this 
result by applying a maximum entropy model, 
currently the single best known approach for 
textual Chinese NER, to the recognition 
output of the BBN LVCSR system on Chinese 
Broadcast News utterances. Our results 
support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese 
than for English.  We propose re-segmenting 
the ASR hypotheses as well as applying post-
classification to improve the performance. 
Finally, we introduce a method of using n-best 
hypotheses that yields a small but nevertheless 
useful improvement NER accuracy.  We use 
acoustic, phonetic, language model, NER and 
other scores as confidence measure.  
Experimental results show an average of 6.7% 
relative improvement in precision and 1.7% 
relative improvement in F-measure. 
1. Introduction 
Named Entity Recognition (NER) is the first step for 
many tasks in the fields of natural language processing 
and information retrieval. It is a designated task in a 
number of conferences, including the Message 
Understanding Conference (MUC), the Information 
Retrieval and Extraction Conference (IREX), the 
Conferences on Natural Language Learning (CoNLL) 
and the recent Automatic Content Extraction 
Conference (ACE).   
 
There has been a considerable amount of work on 
English NER yielding good performance (Tjong Kim 
Sang et al 2002, 2003; Cucerzan & Yarowsky 1999; 
Wu et al 2003). However, Chinese NER is more 
difficult, especially on speech output, due to two 
reasons. First, Chinese has a large number of homonyms 
and the vocabulary used in Chinese person names is an 
open set so more characters/words are unseen in the 
training data. Second, there is no standard definition of 
Chinese words. Word segmentation errors made by 
recognizers may lead to NER errors. Previous work on 
Chinese textual NER includes Jing et al (2003) and Sun 
et al (2003) but there has been no published work on 
NER in spoken Chinese.  
 
Named Entity Recognition for speech is more difficult 
than for text, since the most reliable features for textual 
NER (punctuation, capitalization, and syntactic 
patterns) are often not available in speech output. NER 
on automatically recognized broadcast news was first 
conducted by MITRE in 1997, and was subsequently 
added to Hub-4 evaluation as a task. Palmer et al 
(1999) used error modeling, and Horlock & King (2003) 
proposed discriminative training to handle NER errors; 
both used a hidden Markov model (HMM). Miller et al 
(1999) also reported results in English speech NER 
using an HMM model. In a NIST 1999 evaluation, it 
was found that NER errors on speech arise from a 
combination of ASR errors and errors of the underlying 
NER system. 
 
In this work, we investigate whether the NIST finding 
holds for Chinese speech NER as well. We present the 
first known result for recognizing named entities in 
realistic large-vocabulary spoken Chinese. We propose 
to use the best-known model for Chinese textual NER?
a maximum entropy model?on Chinese speech NER. 
We also propose using re-segmentation and post-
classification to improve this model. Finally, we 
propose to integrate the ASR and NER components to 
optimize NER performance by making use of the n-best 
ASR output.  
2. A Spoken Chinese NER Model 
2.1 LVCSR output  
We use the ASR output from BBN?s Byblos system on  
broadcast news data from the Xinhua News Agency, 
which has 1046 sentences. This system has a character 
error rate of 7%. We had manually annotated them with 
named entities as an evaluation set according to the PFR 
corpus annotation guideline (PFR 2001). 
2.2 A maximum-entropy NER model with post- 
classification  
To establish a baseline spoken Chinese NER model, we 
selected a maximum entropy (MaxEnt) approach since 
this is currently the single most accurate approach 
known for recognizing named entities in text (Tjong 
Kim Sang et al, 2002, 2003, Jing et al, 2003)1. In the 
CoNLL 2003 NER evaluation, 5 out of 16 systems use 
MaxEnt models and the top 3 results for English and top 
2 results for German were obtained by systems that use 
MaxEnt.  
 
Natural language can be viewed as a stochastic process. 
We can use p(y|x) to denote the probability distribution 
of what we try to predict y (.e.g. part-of-speech tag, 
Named Entity tag) conditioned on what we observe x 
(e.g. previous POS or the actual word). The Maximum 
Entropy principle can be stated as follows: given some 
set of constrains from observations, find the most 
uniform probability distribution (Maximum Entropy) 
p(y|x) that satisfies these constrains:  
0
0 0
* arg max ( | )
1( | ) exp( ( , ))
( )
( ) exp( ( , ))
yi i i
m
i i j j i i
ji
l m
i j j i k
k j
y P y x
P y x f x y
Z x
Z x f x y
?
?
=
= =
=
= ?
= ?
?
? ?
 
In the above equations, fj(xi,yk) is a binary valued feature 
function, and ?j is a weight that indicates how important 
feature fj is for the model. Z(xi) is a normalization factor. 
We estimate the weights using the improved iterative 
scaling (IIS) algorithm.  
 
For our task, we first compare a character-based 
MaxEnt model to a word-based model. Since 
recognition errors also lead to segmentation errors 
which in turn have an adverse effect on the NER 
performance, we experiment with disregarding the word 
boundaries in the ASR hypothesis and instead re-
segment using a MaxEnt segmenter. We also compare 
an approach of one-pass identification/classification to a 
two-pass approach where the identified NE candidates 
are classified later. In addition, we propose a hybrid 
approach of using one-pass identification/classification 
results, discarding the extracted NE tags, and re-
classifying the extracted NE in a second pass.  
                                                 
1 We exclude from the present focus the slight improvements 
that are usually possible to obtain by combination of multiple 
models, usually through ad hoc methods such as voting. 
2.3 Experimental setup  
We use two annotated corpora for training. One is a 
corpus of People?s Daily newspaper from January 1998, 
annotated by the Institute of Computational Linguistics 
of Beijing University (the ?PFR? corpus). This corpus 
consists of about 20k sentences, annotated with word 
segmentation, part-of-speech tags and three named-
entity tags including person (PER), location (LOC) and 
organization (ORG) . We use the first 6k sentences to 
train our NER system. Our system is then evaluated on 
2k sentences from People?s Daily and 1k sentences from 
the BBN ASR output. The results are shown in Tables 1 
and 3. 
  
To compare our system to the IBM baseline described 
in (Jing et al 2003), we need to evaluate our system on 
the same corpus as they used. Among the data they used, 
the only publicly available corpus is a human-generated 
transcription of broadcast news, provided by NIST for 
the Information Extraction ? Entity Recognition 
evaluation (the ?IEER? corpus). This corpus consists of 
10 hours of training data and 1 hour of test data. Ten 
categories of NEs were annotated, including person 
names, location, organization, date, duration, and 
measure. A comparison of results is shown in Table 2. 
2.4 Results and discussion 
From text to speech 
Table 1 compares the NER performances of the same 
MaxEnt model on the Chinese textual PFR test data and 
the one-best BBN ASR hypotheses. We can see a 
significant drop in performance in the latter. These 
results support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese than for 
English.  We argue that this is due to the combination of 
different factors specific to spoken Chinese. First, 
Chinese has a large number of homonyms that leads to a 
degradation in speech recognition accuracy which in 
turn leads to low NER accuracy. Second, the vocabulary 
used in Chinese person names is an open set so many 
characters/words are unseen in the training data.  
 
Comparison to IBM baseline 
Table 2 compares results on IEER data from our 
baseline word-based MaxEnt model compared with that 
of IBM?s HMM word-based model. These two models 
achieved almost the same results, which show that our 
NER system based on MaxEnt is state-of-the-art.  
 
Re-segmentation effect 
Table 3 shows that by discarding word boundaries from 
the ASR hypothesis, and then re-segmenting using our 
MaxEnt segmenter, we obtained a better performance in 
most cases. We believe that some reduction in 
segmentation errors due to recognition errors is obtained 
this way; for example, in the ASR output, two words 
??  ? ? in ???  ?  ?  ???   ?  ? ? are 
misrecognized as one word ????, which can be 
corrected by re-segmentation. 
 
Post-classification effect 
Table 3 also shows that the one-pass 
identification/classification method yields better results 
than the two-pass method. However, there are still 
errors in the one-pass output where the bracketing is 
correct, but the NE classification is wrong. In particular, 
the type ORG is easily confusable with LOC in Chinese. 
Both types of NEs tend to be rather long. We propose a 
hybrid approach by first using the one-pass method to 
extract NEs, and then removing all type information, 
combining words of one NE to a whole NE-word and 
post-classifying all the NE-words again. Our results in 
Figure 1 show that the post-classification combined 
with the one-pass approach performs much better on all 
types. 
Table 1. NER results on Chinese speech data are worse 
than on Chinese text data. 
Table 2. Our NER baseline is comparable to the IBM 
baseline. 
Table 3. The character model is better than the word 
model, and one-pass NER is better than two-pass. 
3. Using N-Best Lists to Improve NER 
Miller et al (1999) performed NER on the one-best 
hypothesis of English Broadcast News data.  Palmer & 
Ostendorf (2001) and Horlock & King (2003) carried 
out English NER on word lattices. We are interested in 
investigating how to best utilize the n-best hypothesis 
from the ASR system to improve NER performances. 
From Figure 1, we can see that recall increases as the 
number of hypotheses increases. Thus it would appear 
possible to find a way to make use of the n-best ASR 
output, in order to improve the NER performance. 
However, we can expect it to be difficult to get 
significant improvement since the same figure (Figure 1) 
shows that precision drops much more quickly than 
recall. This is because the nth hypothesis tends to have 
more character errors than the (n-1)th hypothesis, which 
may lead to more NER errors. Therefore the question is, 
given n NE-tagged hypotheses, what is the best way to 
use them to obtain a better NER overall performance 
than by using the one-best hypothesis alone? 
                                                                                                                
One simple approach is to allow all the hypotheses to 
vote on a possible NE output. In simple voting, a 
recognized named-entity is considered correct only 
when it appears in more than 30 percent of the total 
number of all the hypotheses for one utterance. The 
result of this simple voting is shown in Table 4. Next, 
we propose a mechanism of weighted voting using 
confidence measure for each hypothesis. In one 
experiment, we use the MaxEnt NER score as 
confidence measure. In another experiment, we use all 
the six scores (acoustic, language model, number of 
words, number of phones, number of silence, or NER 
score) provided by the BBN ASR system as confidence 
measure. During implementation, an optimizer based on 
Powell?s algorithm is used to find the 6 weights (?k) for 
each score (Sk). For any given hypothesis, confidence 
measure is given by: 
PER LOC ORG  P R F P R F P R F 
Newspaper 
text .86 .76 .81 .87 .75 .81 .83 .83 .83 
1-best ASR 
hypothesis  .22 .18 .20 .75 .79 .77 .43 .35 .39 
Model Precision Recall F-measure
IBM HMM 77.51% 65.22% 70.83% 
MaxEnt 77.3% 65.4% 70.9% 
6
1
k k
k
W S ?
=
= ??  
The above confidence measure is then normalized into a 
final confidence measure for each hypothesis: 
1
exp( )?
exp( )
i
i N
l
l
WW
W
=
=
?
 
Finally, an NE output is considered valid if  
1
? ( ) 0.
N
i i
i
W NE?
=
? >? 3  
1,
( )
0,i
NE occurs in the i th hypothesis
NE
Otherwise
? ??= ??
 
PER LOC ORG  
P R F P R F P R F 
2-pass, 
word  .23  .18  .20  .75  .79  .77  .43 .35 .39  
1-pass, 
word .25  .20  .21  .76  .84  .80  .70 .25 .36  
2-pass, 
character  .53  .43  .48  .67  .70  .68  .75 .59 .66 
1-pass, 
character  .60  .45  .52  .56  .69  .62  .55 .35 .43 
3.1 Experimental setup 
We use the n-best hypothesis of 1,046 Broadcast News 
Chinese utterances from the BBN LVCSR system. n 
ranges from one to 300, averaging at 68. Each utterance 
has a reference transcription with no recognition error. 
3.2 Results and discussion 
Table 4 presents the NER results for the reference 
sentence, one best hypothesis, and different n-best 
voting methods. Results for the reference sentences 
show the upper bound performance (68% F-measure) of 
applying a MaxEnt NER system trained from the 
Chinese text corpus (e.g., PFR) to Chinese speech 
output (e.g., Broadcast News). From Table 4, we can 
conclude that it is possible to improve NER precision by 
using n-best hypothesis by finding the optimized 
combination of different acoustic, language model, 
NER, and other scores. In particular, since most errors 
in Chinese ASR seem to be for person names, using 
NER score on the n-best hypotheses can improve 
recognition results by a relative 6.7% in precision and 
1.7% in F-measure.   
 
PER LOC ORG Results 
F P F P F P 
Reference 
sentence 
0.71 0.75 0.78 0.77 0.56 0.72 
One best 0.46 0.50 0.75 0.74 0.54 0.69 
n-best  
simple vote 
0.45 0.59 0.76 0.75 0.56 0.71 
n-best 
weighted vote 
(NE score) 
0.46 0.59 0.77 0.76 0.55 0.71 
n-best 
weighted vote 
(all scores) 
0.48 0.53 0.75 0.73 0.55 0.69 
 
Table 4. n-best weighted voting with NE score gives the 
best performance.                                                                                                   
Recall 
 
Precision 
 
F-measure 
 
Figure 1. Post-classification improves NER performance. 
4. Conclusion 
We present the first known result for named entity 
recognition (NER) in realistic large-vocabulary spoken 
Chinese.  We apply a maximum entropy (MaxEnt) 
based system to the n-best output of the BBN LVCSR 
system on Chinese Broadcast News utterances. Our 
results support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese than for 
English.  We show that re-segmenting the ASR 
hypotheses improves the NER performance by 24%. We 
also show that applying post-classification improves the 
NER performance by 13%. Finally, we introduce a 
method of using n-best hypotheses that yields a useful 
6.7% relative improvement in NER precision, and 1.7% 
relative improvement in F-measure, by weighted voting.  
 
Institute of Computational Linguistics, Beijing University. 2001. The PFR 
corpus.  http://icl.pku.edu.cn/research/corpus/shengming.htm. 
Hongyan JING, Radu FLORIAN, Xiaoqiang LUO, Tong ZHANG and Abraham 
ITTYCHERIAH. 2003. HowtogetaChineseName(Entity): Segmentation and 
combination issues. Proceedings of EMNLP. Sapporo, Japan: July 2003. 
 
David MILLER, Richard SCHWARTZ, Ralph WEISCHEDEL and Rebecca STONE. 
1999. Named entity extraction from broadcast news. Proceedings of the 
DARPA Broadcast News Workshop. Herndon, Virginia: 1999. 37-40. 
 David D. PALMER, Mari OSTENDORF and John D. BURGER. 1999. Robust 
information extraction from spoken language data. Proceedings of 
Eurospeech 1999. Sep 1999. 
Acknowledgements. We would like to thank the Hong Kong 
Research Grants Council (RGC) for supporting this research 
in part via grants HKUST6206/03E, HKUST6256/00E, 
HKUST6083/99E, DAG03/04.EG30, and DAG03/04.EG09.  
Jian SUN, Ming ZHOU and Jianfeng GAO. 2003. A class-based language model 
approach to Chinese named entity identification. Computational Linguistics 
and Chinese Language Processing. 2003. 
References Erik F. TJONG KIM SANG. 2002. Introduction to the CoNLL-2002 Shared Task: Language-independent named entity recognition. Proceedings of CoNLL-
2002. Taipei, Taiwan: 2002. 155-158.  
Silviu CUCERZAN and David YAROWSKY. 1999. Language independent named 
entity recognition combining morphological and contextual evidence. 
Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC. 
University of Maryland, MD. 
Erik F. TJONG KIM SANG and Fien DE MEULDER. 2003. Introduction to the 
CoNLL-2003 Shared Task: Language-Independent Named Entity 
Recognition. Proceedings of CoNLL-2003. Edmonton, Canada. 142-147.  
James HORLOCK and Simon KING. 2003. Discriminative Methods for Improving 
Named Entity Extraction on Speech Data. Proceedings of Eurospeech 2003. 
Geneva. 
Dekai WU, Grace NGAI and Marine CARPUAT. 2003. A Stacked, Voted, Stacked 
Model for Named Entity Recognition. Proceedings of CoNLL-2003. 
Edmonton, Canada: 2003. 200-203.  
Proceedings of NAACL HLT 2007, pages 228?235,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Outputs from Multiple Machine Translation Systems
Antti-Veikko I. Rosti   and Necip Fazil Ayan

and Bing Xiang   and
Spyros Matsoukas   and Richard Schwartz   and Bonnie J. Dorr

  BBN Technologies, 10 Moulton Street, Cambridge, MA 02138

arosti,bxiang,smatsouk,schwartz  @bbn.com

Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742

nfa,bonnie  @umiacs.umd.edu
Abstract
Currently there are several approaches to
machine translation (MT) based on differ-
ent paradigms; e.g., phrasal, hierarchical
and syntax-based. These three approaches
yield similar translation accuracy despite
using fairly different levels of linguistic
knowledge. The availability of such a
variety of systems has led to a growing
interest toward finding better translations
by combining outputs from multiple sys-
tems. This paper describes three differ-
ent approaches to MT system combina-
tion. These combination methods oper-
ate on sentence, phrase and word level
exploiting information from  -best lists,
system scores and target-to-source phrase
alignments. The word-level combination
provides the most robust gains but the
best results on the development test sets
(NIST MT05 and the newsgroup portion
of GALE 2006 dry-run) were achieved by
combining all three methods.
1 Introduction
In recent years, machine translation systems based
on new paradigms have emerged. These systems
employ more than just the surface-level information
used by the state-of-the-art phrase-based translation
systems. For example, hierarchical (Chiang, 2005)
and syntax-based (Galley et al, 2006) systems have
recently improved in both accuracy and scalability.
Combined with the latest advances in phrase-based
translation systems, it has become more attractive
to take advantage of the various outputs in forming
consensus translations (Frederking and Nirenburg,
1994; Bangalore et al, 2001; Jayaraman and Lavie,
2005; Matusov et al, 2006).
System combination has been successfully ap-
plied in state-of-the-art speech recognition evalua-
tion systems for several years (Fiscus, 1997). Even
though the underlying modeling techniques are sim-
ilar, many systems produce very different outputs
with approximately the same accuracy. One of the
most successful approaches is consensus network
decoding (Mangu et al, 2000) which assumes that
the confidence of a word in a certain position is
based on the sum of confidences from each system
output having the word in that position. This re-
quires aligning the system outputs to form a con-
sensus network and ? during decoding ? simply
finding the highest scoring path through this net-
work. The alignment of speech recognition outputs
is fairly straightforward due to the strict constraint in
word order. However, machine translation outputs
do not have this constraint as the word order may be
different between the source and target languages.
MT systems employ various re-ordering (distortion)
models to take this into account.
Three MT system combination methods are pre-
sented in this paper. They operate on the sentence,
phrase and word level. The sentence-level combi-
nation is based on selecting the best hypothesis out
of the merged N-best lists. This method does not
generate new hypotheses ? unlike the phrase and
word-level methods. The phrase-level combination
228
is based on extracting sentence-specific phrase trans-
lation tables from system outputs with alignments
to source and running a phrasal decoder with this
new translation table. This approach is similar to
the multi-engine MT framework proposed in (Fred-
erking and Nirenburg, 1994) which is not capable of
re-ordering. The word-level combination is based
on consensus network decoding. Translation edit
rate (TER) (Snover et al, 2006) is used to align
the hypotheses and minimum Bayes risk decoding
under TER (Sim et al, 2007) is used to select the
alignment hypothesis. All combination methods use
weights which may be tuned using Powell?s method
(Brent, 1973) on  -best lists. Both sentence and
phrase-level combination methods can generate  -
best lists which may also be used as new system out-
puts in the word-level combination.
Experiments on combining six machine transla-
tion system outputs were performed. Three sys-
tems were phrasal, two hierarchical and one syntax-
based. The systems were evaluated on NIST MT05
and the newsgroup portion of the GALE 2006 dry-
run sets. The outputs were evaluated on both TER
and BLEU. As the target evaluation metric in the
GALE program was human-mediated TER (HTER)
(Snover et al, 2006), it was found important to im-
prove both of these automatic metrics.
This paper is organized as follows. Section 2
describes the evaluation metrics and a generic dis-
criminative optimization technique used in tuning of
the various system combination weights. Sentence,
phrase and word-level system combination methods
are presented in Sections 3, 4 and 5. Experimental
results on Arabic and Chinese to English newswire
and newsgroup test data are presented in Section 6.
2 Evaluation Metrics and Discriminative
Tuning
The official metric of the 2006 DARPA GALE
evaluation was human-mediated translation edit rate
(HTER). HTER is computed as the minimum trans-
lation edit rate (TER) between a system output and
a targeted reference which preserves the meaning
and fluency of the sentence (Snover et al, 2006).
The targeted reference is generated by human post-
editors who make edits to a reference translation so
as to minimize the TER between the reference and
the MT output without changing the meaning of the
reference. Computing the HTER is very time con-
suming due to the human post-editing. It is desir-
able to have an automatic evaluation metric that cor-
relates well with the HTER to allow fast evaluation
of the MT systems during development. Correla-
tions of different evaluation metrics have been stud-
ied (Snover et al, 2006) but according to various
internal HTER experiments it is not clear whether
TER or BLEU correlates better. Therefore it is prob-
ably safest to try and not degrade either.
The TER of a translation   is computed as

 	
 
ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Improved Word-Level System Combination for Machine Translation
Antti-Veikko I. Rosti and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street
Cambridge, MA 02138
 
arosti,smatsouk,schwartz  @bbn.com
Abstract
Recently, confusion network decoding has
been applied in machine translation system
combination. Due to errors in the hypoth-
esis alignment, decoding may result in un-
grammatical combination outputs. This pa-
per describes an improved confusion net-
work based method to combine outputs from
multiple MT systems. In this approach, ar-
bitrary features may be added log-linearly
into the objective function, thus allowing
language model expansion and re-scoring.
Also, a novel method to automatically se-
lect the hypothesis which other hypotheses
are aligned against is proposed. A generic
weight tuning algorithm may be used to op-
timize various automatic evaluation metrics
including TER, BLEU and METEOR. The
experiments using the 2005 Arabic to En-
glish and Chinese to English NIST MT eval-
uation tasks show significant improvements
in BLEU scores compared to earlier confu-
sion network decoding based methods.
1 Introduction
System combination has been shown to improve
classification performance in various tasks. There
are several approaches for combining classifiers. In
ensemble learning, a collection of simple classifiers
is used to yield better performance than any single
classifier; for example boosting (Schapire, 1990).
Another approach is to combine outputs from a few
highly specialized classifiers. The classifiers may
be based on the same basic modeling techniques
but differ by, for example, alternative feature repre-
sentations. Combination of speech recognition out-
puts is an example of this approach (Fiscus, 1997).
In speech recognition, confusion network decoding
(Mangu et al, 2000) has become widely used in sys-
tem combination.
Unlike speech recognition, current statistical ma-
chine translation (MT) systems are based on various
different paradigms; for example phrasal, hierarchi-
cal and syntax-based systems. The idea of combin-
ing outputs from different MT systems to produce
consensus translations in the hope of generating bet-
ter translations has been around for a while (Fred-
erking and Nirenburg, 1994). Recently, confusion
network decoding for MT system combination has
been proposed (Bangalore et al, 2001). To generate
confusion networks, hypotheses have to be aligned
against each other. In (Bangalore et al, 2001), Lev-
enshtein alignment was used to generate the net-
work. As opposed to speech recognition, the word
order between two correct MT outputs may be dif-
ferent and the Levenshtein alignment may not be
able to align shifted words in the hypotheses. In
(Matusov et al, 2006), different word orderings are
taken into account by training alignment models by
considering all hypothesis pairs as a parallel corpus
using GIZA++ (Och and Ney, 2003). The size of
the test set may influence the quality of these align-
ments. Thus, system outputs from development sets
may have to be added to improve the GIZA++ align-
ments. A modified Levenshtein alignment allowing
shifts as in computation of the translation edit rate
(TER) (Snover et al, 2006) was used to align hy-
312
potheses in (Sim et al, 2007). The alignments from
TER are consistent as they do not depend on the test
set size. Also, a more heuristic alignment method
has been proposed in a different system combina-
tion approach (Jayaraman and Lavie, 2005). A full
comparison of different alignment methods would
be difficult as many approaches require a significant
amount of engineering.
Confusion networks are generated by choosing
one hypothesis as the ?skeleton?, and other hypothe-
ses are aligned against it. The skeleton defines the
word order of the combination output. Minimum
Bayes risk (MBR) was used to choose the skeleton
in (Sim et al, 2007). The average TER score was
computed between each system?s   -best hypothesis
and all other hypotheses. The MBR hypothesis is
the one with the minimum average TER and thus,
may be viewed as the closest to all other hypothe-
ses in terms of TER. This work was extended in
(Rosti et al, 2007) by introducing system weights
for word confidences. However, the system weights
did not influence the skeleton selection, so a hypoth-
esis from a system with zero weight might have been
chosen as the skeleton. In this work, confusion net-
works are generated by using the   -best output from
each system as the skeleton, and prior probabili-
ties for each network are estimated from the average
TER scores between the skeleton and other hypothe-
ses. All resulting confusion networks are connected
in parallel into a joint lattice where the prior proba-
bilities are also multiplied by the system weights.
The combination outputs from confusion network
decoding may be ungrammatical due to alignment
errors. Also the word-level decoding may break
coherent phrases produced by the individual sys-
tems. In this work, log-posterior probabilities are
estimated for each confusion network arc instead of
using votes or simple word confidences. This allows
a log-linear addition of arbitrary features such as
language model (LM) scores. The LM scores should
increase the total log-posterior of more grammatical
hypotheses. Powell?s method (Brent, 1973) is used
to tune the system and feature weights simultane-
ously so as to optimize various automatic evaluation
metrics on a development set. Tuning is fully auto-
matic, as opposed to (Matusov et al, 2006) where
global system weights were set manually.
This paper is organized as follows. Three evalu-
ation metrics used in weights tuning and reporting
the test set results are reviewed in Section 2. Sec-
tion 3 describes confusion network decoding for MT
system combination. The extensions to add features
log-linearly and improve the skeleton selection are
presented in Sections 4 and 5, respectively. Section
6 details the weights optimization algorithm and the
experimental results are reported in Section 7. Con-
clusions and future work are discussed in Section 8.
2 Evaluation Metrics
Currently, the most widely used automatic MT eval-
uation metric is the NIST BLEU-4 (Papineni et al,
2002). It is computed as the geometric mean of  -
gram precisions up to  -grams between the hypoth-
esis  and reference  as follows

	
 (1)
ffHedge Trimmer: A Parse-and-Trim Approach to Headline Generation 
Bonnie Dorr, David Zajic 
University of Maryland 
bonnie, dmzajic@umiacs.umd.edu 
Richard Schwartz 
BBN 
schwartz@bbn.com 
 
 
Abstract 
This paper presents Hedge Trimmer, a HEaDline 
GEneration system that creates a headline for a newspa-
per story using linguistically-motivated heuristics to 
guide the choice of a potential headline.  We present 
feasibility tests used to establish the validity of an ap-
proach that constructs a headline by selecting words in 
order from a story.  In addition, we describe experimen-
tal results that demonstrate the effectiveness of our lin-
guistically-motivated approach over a HMM-based 
model, using both human evaluation and automatic met-
rics for comparing the two approaches. 
1 Introduction 
 In this paper we present Hedge Trimmer, a HEaD-
line GEneration system that creates a headline for a 
newspaper story by removing constituents from a parse 
tree of the first sentence until a length threshold has 
been reached.  Linguistically-motivated heuristics guide 
the choice of which constituents of a story should be 
preserved, and which ones should be deleted.  Our focus 
is on headline generation for English newspaper texts, 
with an eye toward the production of document surro-
gates?for cross-language information retrieval?and 
the eventual generation of readable headlines from 
speech broadcasts. 
In contrast to original newspaper headlines, which 
are often intended only to catch the eye, our approach 
produces informative abstracts describing the main 
theme or event of the newspaper article.    We claim that 
the construction of informative abstracts requires access 
to deeper linguistic knowledge, in order to make sub-
stantial improvements over purely statistical ap-
proaches. 
In this paper, we present our technique for produc-
ing headlines using a parse-and-trim approach based on 
the BBN Parser. As described in Miller et al (1998), the 
BBN parser builds augmented parse trees according to a 
process similar to that   described in Collins (1997).  
The BBN parser has been used successfully for the task 
of information extraction in the SIFT system (Miller et 
al., 2000). 
The next section presents previous work in the area 
of automatic generation of abstracts.  Following this, we 
present feasibility tests used to establish the validity of 
an approach that constructs headlines from words in a 
story, taken in order and focusing on the earlier part of 
the story.  Next, we describe the application of the 
parse-and-trim approach to the problem of headline 
generation.  We discuss the linguistically-motivated 
heuristics we use to produce results that are headline-
like.  Finally, we evaluate Hedge Trimmer by compar-
ing it to our earlier work on headline generation, a prob-
abilistic model for automatic headline generation (Zajic 
et al 2002).  In this paper we will refer to this statistical 
system as HMM Hedge  We demonstrate the effective-
ness of our linguistically-motivated approach, Hedge 
Trimmer, over the probabilistic model, HMM Hedge, 
using both human evaluation and automatic metrics. 
2 Previous Work 
 Other researchers have investigated the topic of 
automatic generation of abstracts, but the focus has been 
different, e.g., sentence extraction (Edmundson, 1969; 
Johnson et al 1993; Kupiec et al, 1995; Mann et al, 
1992; Teufel and Moens, 1997; Zechner, 1995), proc-
essing of structured templates (Paice and Jones, 1993), 
sentence compression (Hori et al, 2002; Knight and 
Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and gen-
eration of abstracts from multiple sources (Radev and 
McKeown, 1998).  We focus instead on the construction 
of headline-style abstracts from a single story. 
 Headline generation can be viewed as analogous to 
statistical machine translation, where a concise docu-
ment is generated from a verbose one using a Noisy 
Channel Model and the Viterbi search to select the most 
likely summarization.  This approach has been explored 
in (Zajic et al, 2002) and (Banko et al, 2000). 
 The approach we use in Hedge is most similar to 
that of (Knight and Marcu, 2001), where a single sen-
tence is shortened using statistical compression. As in 
this work, we select headline words from story words in 
the order that they appear in the story?in particular, the 
first sentence of the story.  However, we use linguisti-
cally motivated heuristics for shortening the sentence; 
there is no statistical model, which means we do not 
require any prior training on a large corpus of 
story/headline pairs. 
 Linguistically motivated heuristics have been used 
by (McKeown et al 2002) to distinguish constituents of 
parse trees which can be removed without affecting 
grammaticality or correctness.  GLEANS (Daum? et al 
2002) uses parsing and named entity tagging to fill val-
ues in headline templates. 
 Consider the following excerpt from a news story: 
 
(1) Story Words: Kurdish guerilla forces moving 
with lightning speed poured into Kirkuk today 
immediately after Iraqi troops, fleeing relent-
less U.S. airstrikes, abandoned the hub of Iraq?s 
rich northern oil fields. 
 
Generated Headline: Kurdish guerilla forces 
poured into Kirkuk after Iraqi troops abandoned 
oil fields. 
 
 In this case, the words in bold form a fluent and 
accurate headline for the story.  Italicized words are 
deleted based on information provided in a parse-tree 
representation of the sentence.   
3 Feasibility Testing 
 Our approach is based on the selection of words 
from the original story, in the order that they appear in 
the story, and allowing for morphological variation.  To 
determine the feasibility of our headline-generation ap-
proach, we first attempted to apply our ?select-words-
in-order? technique by hand.  We asked two subjects to 
write headline headlines for 73 AP stories from the 
TIPSTER corpus for January 1, 1989, by selecting 
words in order from the story.  Of the 146 headlines, 2 
did not meet the ?select-words-in-order? criteria be-
cause of accidental word reordering.  We found that at 
least one fluent and accurate headline meeting the crite-
ria was created for each of the stories.  The average 
length of the headlines was 10.76 words. 
 Later we examined the distribution of the headline 
words among the sentences of the stories, i.e. how many 
came from the first sentence of a story, how many from 
the second sentence, etc.  The results of this study are 
shown in Figure 1.  We observe that 86.8% of the head-
line words were chosen from the first sentence of their 
stories.  We performed a subsequent study in which two 
subjects created 100 headlines for 100 AP stories from 
August 6, 1990.  51.4% of the headline words in the 
second set were chosen from the first sentence.  The 
distribution of headline words for the second set shown 
in Figure 2. 
 Although humans do not always select headline 
words from the first sentence, we observe that a large 
percentage of headline words are often found in the first 
sentence. 
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
N=
1
N=
2
N=
3
N=
4
N=
5
N=
6
N=
7
N=
8
N=
9
N>
=
10
 
Figure 1: Percentage of words from human-generated 
headlines drawn from Nth sentence of story (Set 1) 
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
N=
1
N=
2
N=
3
N=
4
N=
5
N=
6
N=
7
N=
8
N=
9
N>
=
10
 
Figure 2:  Percentage of words from human-generated head-
lines drawn from Nth sentence of story (Set 2) 
 
4 Approach 
The input to Hedge is a story, whose first sentence is 
immediately passed through the BBN parser.  The 
parse-tree result serves as input to a linguistically-
motivated module that selects story words to form head-
lines based on key insights gained from our observa-
tions of human-constructed headlines.  That is, we 
conducted a human inspection of the 73 TIPSTER sto-
ries mentioned in Section 3 for the purpose of develop-
ing the Hedge Trimmer algorithm. 
 Based on our observations of human-produced 
headlines, we developed the following algorithm for 
parse-tree trimming: 
 
1. Choose lowest leftmost S with NP,VP 
2. Remove low content units 
o some determiners 
o time expressions 
3. Iterative shortening: 
o XP Reduction 
o Remove preposed adjuncts 
o Remove trailing PPs 
o Remove trailing SBARs 
 
 More recently, we conducted an automatic analysis 
of the human-generated headlines that supports several 
of the insights gleaned from this initial study. We parsed 
218 human-produced headlines using the BBN parser 
and analyzed the results. For this analysis, we used 72 
headlines produced by a third participant.1 The parsing 
results included 957 noun phrases (NP) and 315 clauses 
(S).  
 We calculated percentages based on headline-level, 
NP-level, and Sentence-level structures in the parsing 
results.  That is, we counted: 
 
? The percentage of the 957 NPs containing de-
terminers and relative clauses 
? The percentage of the 218 headlines containing 
preposed adjuncts and conjoined S or VPs 
? The percentage of the 315 S nodes containing 
trailing time expressions, SBARs, and PPs 
 
Figure 3 summarizes the results of this automatic analy-
sis.  In our initial human inspection, we considered each 
of these categories to be reasonable candidates for dele-
tion in our parse tree and this automatic analysis indi-
cates that we have made reasonable choices for deletion, 
with the possible exception of trailing PPs, which show 
up in over half of the human-generated headlines.  This 
suggests that we should proceed with caution with re-
spect to the deletion of trailing PPs; thus we consider 
this to be an option only if no other is available. 
 
HEADLINE-LEVEL PERCENTAGES 
preposed adjuncts = 0/218 (0%)  
conjoined S = 1/218 ( .5%) 
conjoined VP = 7/218 (3%) 
NP-LEVEL PERCENTAGES 
relative clauses = 3/957 (.3%)  
determiners = 31/957 (3%); of these, only 
16 were ?a? or ?the? (1.6% overall) 
S-LEVEL PERCENTAGES2 
time expressions = 5/315 (1.5%) 
trailing PPs = 165/315 (52%) 
trailing SBARs = 24/315 (8%) 
Figure 3: Percentages found in human-generated headlines 
                                                          
1
 No response was given for one of the 73 stories. 
2
 Trailing constituents (SBARs and PPs) are computed by 
counting the number of SBARs (or PPs) not designated as an 
argument of (contained in) a verb phrase. 
 For a comparison, we conducted a second analysis 
in which we used the same parser on just the first sen-
tence of each of the 73 stories.  In this second analysis, 
the parsing results included 817 noun phrases (NP) and 
316 clauses (S).  A summary of these results is shown in 
Figure 4.  Note that, across the board, the percentages 
are higher in this analysis than in the results shown in 
Figure 3 (ranging from 12% higher?in the case of trail-
ing PPs?to 1500% higher in the case of time expres-
sions), indicating that our choices of deletion in the 
Hedge Trimmer algorithm are well-grounded. 
 
HEADLINE-LEVEL PERCENTAGES 
preposed adjuncts = 2/73 (2.7%) 
conjoined S = 3/73 (4%) 
conjoined VP = 20/73 (27%) 
NP-LEVEL PERCENTAGES 
relative clauses = 29/817 (3.5%)  
determiners = 205/817 (25%); of these, 
only 171 were ?a? or ?the? (21% overall) 
S-LEVEL PERCENTAGES 
time expressions = 77/316 (24%) 
trailing PPs = 184/316 (58%) 
trailing SBARs =  49/316 (16%) 
Figure 4: Percentages found in first sentence of 
each story. 
4.1 Choose the Correct S Node 
 The first step relies on what is referred to as the 
Projection Principle in linguistic theory (Chomsky, 
1981): Predicates project a subject (both dominated by 
S) in the surface structure.  Our human-generated head-
lines always conformed to this rule; thus, we adopted it 
as a constraint in our algorithm. 
 An example of the application of step 1 above is 
the following, where boldfaced material from the parse 
tree representation is retained and italicized material is 
eliminated: 
 
(2) Input: Rebels agree to talks with government of-
ficials said Tuesday. 
 
Parse: [S [S [NP Rebels] [VP agree to talks 
with government]] officials said Tuesday.] 
 
Output of step 1: Rebels agree to talks with gov-
ernment. 
 
When the parser produces a correct tree, this step pro-
vides a grammatical headline.  However, the parser of-
ten produces an incorrect output.  Human inspection of 
our 624-sentence DUC-2003 evaluation set revealed 
that there were two such scenarios, illustrated by the 
following cases:  
 
(3) [S [SBAR What started as a local contro-
versy] [VP has evolved into an international 
scandal.]] 
 
(4) [NP [NP Bangladesh] [CC and] [NP [NP In-
dia] [VP signed a water sharing accord.]]] 
  
In the first case, an S exists, but it does not conform 
to the requirements of step 1.  This occurred in 2.6% of 
the sentences in the DUC-2003 evaluation data.  We 
resolve this by selecting the lowest leftmost S, i.e., the 
entire string ?What started as a local controversy has 
evolved into an international scandal? in the example 
above. 
In the second case, there is no S available.  This oc-
curred in 3.4% of the sentences in the evaluation data.  
We resolve this by selecting the root of the parse tree; 
this would be the entire string ?Bangladesh and India 
signed a water sharing accord? above.  No other parser 
errors were encountered in the DUC-2003 evaluation 
data. 
4.2 Removal of Low Content Nodes  
 Step 2 of our algorithm eliminates low-content 
units.  We start with the simplest low-content units: the 
determiners a and the.  Other determiners were not con-
sidered for deletion because our analysis of the human-
constructed headlines revealed that most of the other 
determiners provide important information, e.g., nega-
tion (not), quantifiers (each, many, several), and deictics 
(this, that). 
 Beyond these, we found that the human-generated 
headlines contained very few time expressions which, 
although certainly not content-free, do not contribute 
toward conveying the overall ?who/what content? of the 
story.  Since our goal is to provide an informative head-
line (i.e., the action and its participants), the identifica-
tion and elimination of time expressions provided a 
significant boost in the performance of our automatic 
headline generator. 
 We identified time expressions in the stories using 
BBN?s IdentiFinder? (Bikel et al 1999). We imple-
mented the elimination of time expressions as a two-
step process: 
 
? Use IdentiFinder to mark time expressions 
? Remove [PP ? [NP [X] ?] ?] and [NP [X]] 
where X is tagged as part of a time expression 
 
The following examples illustrate the application of 
this step: 
 
(5) Input: The State Department on Friday lifted the 
ban it had imposed on foreign fliers. 
 
Parse:  [Det The] State Department [PP [IN 
on] [NP [NNP Friday]]] lifted [Det the] ban it 
had imposed on foreign fliers.  
 
Output of step 2: State Department lifted ban it 
has imposed on foreign fliers. 
 
(6) Input: An international relief agency announced 
Wednesday that it is withdrawing from North 
Korea. 
 
Parse:  [Det An] international relief agency an-
nounced [NP [NNP Wednesday]] that it is with-
drawing from North Korea. 
 
Output of step 2: International relief agency an-
nounced that it is withdrawing from North Korea. 
 
 We found that 53.2% of the stories we examined 
contained at least one time expression which could be 
deleted.  Human inspection of the 50 deleted time ex-
pressions showed that 38 were desirable deletions, 10 
were locally undesirable because they introduced an 
ungrammatical fragment,3 and 2 were undesirable be-
cause they removed a potentially relevant constituent.  
However, even an undesirable deletion often pans out 
for two reasons: (1) the ungrammatical fragment is fre-
quently deleted later by some other rule; and (2) every 
time a constituent is removed it makes room under the 
threshold for some other, possibly more relevant con-
stituent.  Consider the following examples. 
 
(7) At least two people were killed Sunday. 
 
(8) At least two people were killed when single-
engine airplane crashed. 
 
Example (7) was produced by a system which did not 
remove time expressions.  Example (8) shows that if the 
time expression Sunday were removed, it would make 
room below the 10-word threshold for another impor-
tant piece of information. 
4.3 Iterative Shortening 
 The final step, iterative shortening, removes lin-
guistically peripheral material?through successive de-
letions?until the sentence is shorter than a given 
threshold.  We took the threshold to be 10 for the DUC 
task, but it is a configurable parameter.  Also, given that 
the human-generated headlines tended to retain earlier 
material more often than later material, much of our 
                                                          
3
 Two examples of genuinely undesirable time expression deletion 
are: 
? The attack came on the heels of [New Year?s Day]. 
? [New Year?s Day] brought a foot of snow to the region. 
iterative shortening is focused on deleting the rightmost 
phrasal categories until the length is below threshold. 
 There are four types of iterative shortening rules. 
The first type is a rule we call ?XP-over-XP,? which is 
implemented as follows: 
 
In constructions of the form [XP [XP ?] ?] re-
move the other children of the higher XP, where 
XP is NP, VP or S. 
 
This is a linguistic generalization that allowed us apply 
a single rule to capture three different phenomena (rela-
tive clauses, verb-phrase conjunction, and sentential 
conjunction).   The rule is applied iteratively, from the 
deepest rightmost applicable node backwards, until the 
length threshold is reached. 
The impact of XP-over-XP can be seen in these ex-
amples of NP-over-NP (relative clauses), VP-over-VP 
(verb-phrase conjunction), and S-over-S (sentential con-
junction), respectively: 
 
(9) Input: A fire killed a firefighter who was fatally 
injured as he searched the house. 
 
Parse:  [S [Det A] fire killed [Det a]  [NP [NP 
firefighter] [SBAR who was fatally injured as 
he searched the house] ]] 
 
Output of NP-over-NP: fire killed firefighter 
 
(10) Input: Illegal fireworks injured hundreds of peo-
ple and started six fires.  
 
Parse:  [S Illegal fireworks [VP [VP injured 
hundreds of people] [CC and] [VP started six 
fires] ]] 
 
Output of VP-over-VP: Illegal fireworks injured 
hundreds of people 
 
(11) Input: A company offering blood cholesterol 
tests in grocery stores says medical technology 
has outpaced state laws, but the state says the 
company doesn?t have the proper licenses. 
 
Parse:  [S [Det A] company offering blood cho-
lesterol tests in grocery stores says [S [S medi-
cal technology has outpaced state laws], [CC 
but] [S [Det the] state stays [Det the] company 
doesn?t have [Det the] proper licenses.]] ]  
 
Output of S-over-S: Company offering blood 
cholesterol tests in grocery store says medical 
technology has outpaced state laws 
 
 The second type of iterative shortening is the re-
moval of preposed adjuncts.  The motivation for this 
type of shortening is that all of the human-generated 
headlines ignored what we refer to as the preamble of 
the story.  Assuming the Projection principle has been 
satisfied, the preamble is viewed as the phrasal material 
occurring before the subject of the sentence. Thus, ad-
juncts are identified linguistically as any XP unit pre-
ceding the first NP (the subject) under the S chosen by 
step 1. This type of phrasal modifier is invisible to the 
XP-over-XP rule, which deletes material under a node 
only if it dominates another node of the same phrasal 
category. 
 The impact of this type of shortening can be seen in 
the following example:  
 
(12) Input: According to a now finalized blueprint de-
scribed by U.S. officials and other sources, the 
Bush administration plans to take complete, unilat-
eral control of a post-Saddam Hussein Iraq 
 
Parse:  [S [PP According to a now-finalized blue-
print described by U.S. officials and other sources] 
[Det the] Bush administration plans to take 
complete, unilateral control of [Det a] post-
Saddam Hussein Iraq ]  
 
Output of Preposed Adjunct Removal: Bush ad-
ministration plans to take complete unilateral con-
trol of post-Saddam Hussein Iraq 
 
 The third and fourth types of iterative shortening 
are the removal of trailing PPs and SBARs, respec-
tively: 
 
? Remove PPs from deepest rightmost node back-
ward until length is below threshold. 
? Remove SBARs from deepest rightmost node 
backward until length is below threshold. 
 
  These are the riskiest of the iterative shortening rules, 
as indicated in our analysis of the human-generated 
headlines.  Thus, we apply these conservatively, only 
when there are no other categories of rules to apply.  
Moreover, these rules are applied with a backoff option 
to avoid over-trimming the parse tree. First the PP 
shortening rule is applied.  If the threshold has been 
reached, no more shortening is done.  However, if the 
threshold has not been reached, the system reverts to the 
parse tree as it was before any PPs were removed, and 
applies the SBAR shortening rule.  If the threshold still 
has not been reached, the PP rule is applied to the result 
of the SBAR rule.   
 Other sequences of shortening rules are possible.  
The one above was observed to produce the best results 
on a 73-sentence development set of stories from the 
TIPSTER corpus.  The intuition is that, when removing 
constituents from a parse tree, it?s best to remove 
smaller portions during each iteration, to avoid produc-
ing trees with undesirably few words.  PPs tend to rep-
resent small parts of the tree while SBARs represent 
large parts of the tree.  Thus we try to reach the thresh-
old by removing small constituents, but if we can?t 
reach the threshold that way, we restore the small con-
stituents, remove a large constituent and resume the 
deletion of small constituents. 
The impact of these two types of shortening can be 
seen in the following examples:  
 
(13) Input: More oil-covered sea birds were found 
over the weekend. 
 
Parse:  [S More oil-covered sea birds were 
found [PP over the weekend]]     
 
Output of PP Removal: More oil-covered sea 
birds were found. 
 
(14) Input: Visiting China Interpol chief expressed 
confidence in Hong Kong?s smooth transition 
while assuring closer cooperation after Hong 
Kong returns.  
 
Parse:  [S Visiting China Interpol chief ex-
pressed confidence in Hong Kong?s smooth 
transition [SBAR while assuring closer coopera-
tion after Hong Kong returns]]   
 
Output of SBAR Removal: Visiting China Inter-
pol chief expressed confidence in Hong Kong?s 
smooth transition 
 
5 Evaluation 
We conducted two evaluations.  One was an informal 
human assessment and one was a formal automatic 
evaluation.  
5.1 HMM Hedge 
We compared our current system to a statistical 
headline generation system we presented at the 2001 
DUC Summarization Workshop (Zajic et al, 2002), 
which we will refer to as HMM Hedge.  HMM Hedge 
treats the summarization problem as analogous to statis-
tical machine translation.  The verbose language, arti-
cles, is treated as the result of a concise language, 
headlines, being transmitted through a noisy channel.  
The result of the transmission is that extra words are 
added and some morphological variations occur.  The 
Viterbi algorithm is used to calculate the most likely 
unseen headline to have generated the seen article.  The 
Viterbi algorithm is biased to favor headline-like char-
acteristics gleaned from observation of human perform-
ance of the headline-construction task.  Since the 2002 
Workshop, HMM Hedge has been enhanced by incorpo-
rating part of speech of information into the decoding 
process, rejecting headlines that do not contain a word 
that was used as a verb in the story, and allowing mor-
phological variation only on words that were used as 
verbs in the story.  HMM Hedge was trained on 700,000 
news articles and headlines from the TIPSTER corpus. 
5.2 Bleu: Automatic Evaluation 
 
BLEU (Papineni et al 2002) is a system for auto-
matic evaluation of machine translation.  BLEU uses a 
modified n-gram precision measure to compare machine 
translations to reference human translations.  We treat 
summarization as a type of translation from a verbose 
language to a concise one, and compare automatically 
generated headlines to human generated headlines. 
For this evaluation we used 100 headlines created 
for 100 AP stories from the TIPSTER collection for 
August 6, 1990 as reference summarizations for those 
stories.  These 100 stories had never been run through 
either system or evaluated by the authors prior to this 
evaluation.  We also used the 2496 manual abstracts for 
the DUC2003 10-word summarization task as reference 
translations for the 624 test documents of that task.  We 
used two variants of HMM Hedge, one which selects 
headline words from the first 60 words of the story, and 
one which selects words from the first sentence of the 
story.  Table 1 shows the BLEU score using trigrams, 
and the 95% confidence interval for the score. 
 
 AP900806 DUC2003 
HMM60 0.0997 ? 0.0322 
avg len: 8.62 
0.1050 ? 0.0154 
avg len: 8.54 
HMM1Sent 0.0998 ? 0.0354 
avg len: 8.78 
0.1115 ? 0.0173 
avg len: 8.95 
HedgeTr 0.1067 ? 0.0301 
avg len: 8.27 
0.1341 ? 0.0181 
avg len: 8.50 
Table 1 
These results show that although Hedge Trimmer 
scores slightly higher than HMM Hedge on both data 
sets, the results are not statistically significant.  How-
ever, we believe that the difference in the quality of the 
systems is not adequately reflected by this automatic 
evaluation. 
5.3 Human Evaluation 
Human evaluation indicates significantly higher 
scores than might be guessed from the automatic 
evaluation.  For the 100 AP stories from the TIPSTER 
corpus for August 6, 1990, the output of Hedge Trim-
mer and HMM Hedge was evaluated by one human.  
Each headline was given a subjective score from 1 to 5, 
with 1 being the worst and 5 being the best.  The aver-
age score of HMM Hedge was 3.01 with standard devia-
tion of 1.11.  The average score of Hedge Trimmer was 
3.72 with standard deviation of 1.26.  Using a t-score, 
the difference is significant with greater than 99.9% 
confidence. 
The types of problems exhibited by the two systems are 
qualitatively different.  The probabilistic system is more 
likely to produce an ungrammatical result or omit a nec-
essary argument, as in the examples below. 
 
(15) HMM60: Nearly drowns in satisfactory condi-
tion satisfactory condition. 
 
(16) HMM60: A county jail inmate who noticed. 
 
 In contrast, the parser-based system is more likely 
to fail by producing a grammatical but semantically 
useless headline. 
 
(17) HedgeTr:  It may not be everyone?s idea espe-
cially coming on heels. 
 
 Finally, even when both systems produce accept-
able output, Hedge Trimmer usually produces headlines 
which are more fluent or include more useful informa-
tion. 
 
(18)   a. HMM60:  New Year?s eve capsizing 
 b. HedgeTr:  Sightseeing cruise boat capsized 
and sank. 
 
(19)   a. HMM60:  hundreds of Tibetan students 
demonstrate in Lhasa. 
 b. HedgeTr:  Hundreds demonstrated in Lhasa 
demanding that Chinese authorities respect cul-
ture. 
6 Conclusions and Future Work 
 We have shown the effectiveness of constructing 
headlines by selecting words in order from a newspaper 
story.  The practice of selecting words from the early 
part of the document has been justified by analyzing the 
behavior of humans doing the task, and by automatic 
evaluation of a system operating on a similar principle. 
 We have compared two systems that use this basic 
technique, one taking a statistical approach and the 
other a linguistic approach.  The results of the linguisti-
cally motivated approach show that we can build a 
working system with minimal linguistic knowledge and 
circumvent the need for large amounts of training data.  
We should be able to quickly produce a comparable 
system for other languages, especially in light of current 
multi-lingual initiatives that include automatic parser 
induction for new languages, e.g. the TIDES initiative. 
 We plan to enhance Hedge Trimmer by using a 
language model of Headlinese, the language of newspa-
per headlines (M?rdh 1980) to guide the system in 
which constituents to remove.  We Also we plan to al-
low for morphological variation in verbs to produce the 
present tense headlines typical of Headlinese.   
 Hedge Trimmer will be installed in a translingual 
detection system for enhanced display of document sur-
rogates for cross-language question answering.  This 
system will be evaluated in upcoming iCLEF confer-
ences. 
7 Acknowledgements 
The University of Maryland authors are supported, 
in part, by BBNT Contract 020124-7157, DARPA/ITO 
Contract N66001-97-C-8540, and NSF CISE Research 
Infrastructure Award EIA0130422.  We would like to 
thank Naomi Chang and Jon Teske for generating refer-
ence headlines. 
References 
Banko, M., Mittal, V., Witbrock, M. (2000).  Headline 
Generation Based on Statistical Translation.  In Pro-
ceedings of 38th Meeting of Association for Computa-
tion Linguistics, Hong Kong, pp. 218-325. 
Bikel, D., Schwartz, R., and Weischedel, R. (1999). An 
algorithm that learns what?s in a name.  Machine Learn-
ing, 34(1/3), February 
Chomsky, Noam A. (1981). Lectures on Government 
and Binding, Foris Publications, Dordrecht, Holland. 
Collins, M. (1997). Three generative lexicalised models 
for statistical parsing. In Proceedings of the 35th ACL, 
1997. 
Daum?, H., Echihabi, A., Marcu, D., Munteanu, D., 
Soricut, R. (2002).  GLEANS: A Generator of Logical 
Extracts and Abstracts for Nice Summaries, In Work-
shop on Automatic Summarization, Philadelphia, PA, 
pp. 9-14. 
Edmundson, H. (1969). ?New methods in automatic 
extracting.? Journal of the ACM, 16(2).  
Grefenstett, G. (1998).  Producing intelligent tele-
graphic text reduction to provide an audio scanning ser-
vice for the blind.  In Working Notes of the AIII Spring 
Symposium on Intelligent Text Summarization, Stanford 
University, CA, pp. 111-118. 
Hori, C., Furui, S., Malkin, R., Yu, H., Waibel, A. 
(2002).  Automatic Speech Summarization Applied to 
English Broadcast News Speech.  In Proceedings of 
2002 International Conference on Acoustics, Speech 
and Signal Processing, Istanbul, pp. 9-12. 
Johnson, F. C., Paice, C. D., Black, W. J., and Neal, A. 
P. (1993). ?The application of linguistic processing to 
automatic abstract generation.? Journal of Document 
and Text Management, 1(3):215-42. 
Knight, K. and Marcu, D. (2001).  ?Statistics-Based 
Summarization Step One: Sentence Compression,? In 
Proceedings of AAAI-2001. 
Kupiec, J., Pedersen, J., and Chen, F. (1995).  ?A train-
able document summarizer.?  In Proceedings of the 18th 
ACM-SIGIR Conference. 
Luhn, H. P. (1958).  "The automatic creation of litera-
ture abstracts." IBM Journal of Research and Develop-
ment, 2(2). 
Mann, W.C., Matthiesen, C.M.I.M., and Thomspson, 
S.A. (1992).  Rhetorical structure theory and text analy-
sis.  In Mann, W.C. and Thompson, S.A., editors, Dis-
course Description.  J. Benjamins Pub. Co., Amsterdam. 
M?rdh, I. (1980).  Headlinese:  On the Grammar of 
English Front Page Headlines, Malmo. 
McKeown, K.,  Barzilay, R.,  Blair-Goldensohn, S., 
Evans, D.,  Hatzivassiloglou, V., Klavans, J., Nenkova, 
A., Schiffman, B.,  and Sigelman, S. (2002).  ?The Co-
lumbia Multi-Document Summarizer for DUC 2002,?  
In Workshop on Automatic Summarization, Philadel-
phia, PA, pp. 1-8. 
Miller, S., Crystal, M., Fox, H., Ramshaw, L., Schwartz, 
R., Stone, R., Weischedel, R. and Annotation Group, the 
(1998). Algorithms that Learn to Extract Information; 
BBN: Description of the SIFT System as Used for 
MUC-7. In Proceedings of the MUC-7. 
Miller, S., Ramshaw, L., Fox, H., and Weischedel, R. 
(2000). ?A Novel Use of Statistical Parsing to Extract 
Information from Text,? In Proceedings of 1st Meeting 
of the North American Chapter of the ACL, Seattle, 
WA, pp.226-233. 
Paice, C. D. and Jones, A. P. (1993).  ?The identifica-
tion of important concepts in highly structured technical 
papers.?  In Proceedings of the Sixteenth Annual Inter-
national ACM SIGIR conference on research and de-
velopment in IR. 
Papineni, K., Roukos, S., Ward, T., and Zhu, W. (2002).  
?BLEU: a Method for Automatic Evaluation of Ma-
chine Translation,? In Proceedings of 40th Annual 
Meeting of the Association for Computational Linguis-
tics, Philadelphia, PA, pp. 331-318 
Radev, Dragomir R. and Kathleen R. McKeown (1998). 
?Generating Natural Language Summaries from Multi-
ple On-Line Sources.? Computational Linguistics, 
24(3):469--500, September 1998. 
Teufel, Simone and Marc Moens (1997).  ?Sentence 
extraction as a classification task,? In Proceedings of 
the Workshop on Intelligent and scalable Text summari-
zation, ACL/EACL-1997, Madrid, Spain. 
Zajic, D., Dorr, B., Schwartz, R. (2002) ?Automatic 
Headline Generation for Newspaper Stories,? In Work-
shop on Automatic Summarization, Philadelphia, PA, 
pp. 78-85. 
Zechner, K. (1995). ?Automatic text abstracting by se-
lecting relevant passages.?  Master's thesis, Centre for 
Cognitive Science, University of Edinburgh. 
 
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 1?8, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Methodology for Extrinsic Evaluation of Text Summarization:
Does ROUGE Correlate?
Bonnie J. Dorr and Christof Monz and Stacy President and Richard Schwartz? and David Zajic
Department of Computer Science and UMIACS
University of Maryland
College Park, MD 20742
{bonnie,christof,stacypre,dmzajic}@umiacs.umd.edu
?BBN Technologies
9861 Broken Land Parkway
Columbia, Maryland 21046
schwartz@bbn.com
Abstract
This paper demonstrates the usefulness of sum-
maries in an extrinsic task of relevance judgment
based on a new method for measuring agree-
ment, Relevance-Prediction, which compares sub-
jects? judgments on summaries with their own judg-
ments on full text documents. We demonstrate that,
because this measure is more reliable than previ-
ous gold-standard measures, we are able to make
stronger statistical statements about the benefits of
summarization. We found positive correlations be-
tween ROUGE scores and two different summary
types, where only weak or negative correlations
were found using other agreement measures. How-
ever, we show that ROUGE may be sensitive to the
choice of summarization style. We discuss the im-
portance of these results and the implications for fu-
ture summarization evaluations.
1 Introduction
People often prefer to read a summary of a text document,
e.g., news headlines, scientific abstracts, movie previews
and reviews, and meeting minutes. Correspondingly, the
explosion of online textual material has prompted ad-
vanced research in document summarization. Although
researchers have demonstrated that users can read sum-
maries faster than full text (Mani et al, 2002) with some
loss of accuracy, researchers have found it difficult to
draw strong conclusions about the usefulness of summa-
rization due to the low level of interannotator agreement
in the gold standards that they have used. Definitive con-
clusions about the usefulness of summaries would pro-
vide justification for continued research and development
of new summarization methods.
To investigate the question of whether text summariza-
tion is useful in an extrinsic task, we examined human
performance in a relevance assessment task using a hu-
man text surrogate (i.e. text intended to stand in the place
of a document). We use single-document English sum-
maries as these are sufficient for investigating task-based
usefulness, although more elaborate surrogates are possi-
ble, e.g., those that span more than one document (Radev
and McKeown, 1998; Mani and Bloedorn, 1998).
The next section motivates the need for develop-
ing a new framework for measuring task-based useful-
ness. Section 3 presents a novel extrinsic measure called
Relevance-Prediction. Section 4 demonstrates that this is
a more reliable measure than that of previous gold stan-
dard methods, e.g., the LDC-Agreement method used for
SUMMAC-style evaluations, and that this reliability al-
lows us to make stronger statistical statements about the
benefits of summarization. We expect these findings to
be important for future summarization evaluations.
Section 5 presents the results of correlation between
task usefulness and the Recall Oriented Understudy for
Gisting Evaluation (ROUGE) metric (Lin and Hovy,
2003).1 While we show that ROUGE correlates with task
usefulness (using our Relevance-Prediction measure), we
detect a slight difference between informative, extractive
headlines (containing words from the full document) and
less informative, non-extractive ?eye-catchers? (contain-
ing words that might not appear in the full document, and
intended to entice a reader to read the entire document).
Section 6 further highlights the importance of this
point and discusses the implications for automatic eval-
uation of non-extractive summaries. To evaluate non-
extractive summaries reliably, an automatic measure may
require knowledge of sophisticated meaning units.2 It is
our hope that the conclusions drawn herein will prompt
investigation into more sophisticated automatic metrics
as researchers shift their focus to non-extractive sum-
maries.
1ROUGE has been previously used as the primary automatic
evaluation metric by NIST in the 2003 and 2004 DUC Evalua-
tions.
2The content units proposed in recent methods (Nenkova
and Passonneau, 2004) are a first step in this direction.
1
2 Background
In the past, assessments of usefulness involved a wide
range of both intrinsic and extrinsic (task-based) mea-
sures (Sparck-Jones and Gallier, 1996). Intrinsic evalu-
ations focus on coherence and informativeness (Jing et
al., 1998) and often involve quality comparisons between
automatic summaries and reference summaries that are
pre-determined to be of high quality. Human intrinsic
measures determine quality by assessing document accu-
racy, fluency, and clarity. Automatic intrinsic measures
such as ROUGE use n-gram scoring to produce rankings
of summarization methods.
Extrinsic evaluations concentrate on the use of sum-
maries in a specific task, e.g., executing instructions, in-
formation retrieval, question answering, and relevance
assessments (Mani, 2001). In relevance assessments, a
user reads a topic or event description and judges rele-
vance of a document to the topic/event based solely on its
summary.3 These have been used in many large-scale ex-
trinsic evaluations, e.g., SUMMAC (Mani et al, 2002)
and the Document Understanding Conference (DUC)
(Harman and Over, 2004). The task chosen for such eval-
uations must support a very high degree of interannota-
tor agreement, i.e., consistent relevance decisions across
subjects with respect to a predefined gold standard.
Unfortunately, a consistent gold standard has not yet
been reported. For example, in two previous studies
(Mani, 2001; Tombros and Sanderson, 1998), users?
judgments were compared to ?gold standard judgments?
produced by members of the University of Pennsylva-
nia?s Linguistic Data Consortium. Although these judg-
ments were supposed to represent the correct relevance
judgments for each of the documents associated with an
event, both studies reported that annotators? judgments
varied greatly and that this was a significant issue for the
evaluations. In the SUMMAC experiments, the Kappa
score (Carletta, 1996; Eugenio and Glass, 2004) for in-
terannotator agreement was reported to be 0.38 (Mani et
al., 2002). In fact, large variations have been found in the
initial summary scoring of an individual participant and a
subsequent scoring that occurs a few weeks later (Mani,
2001; van Halteren and Teufel, 2003).
This paper attempts to overcome the problem of in-
terannotator inconsistency by measuring summary effec-
tiveness in an extrinsic task using a much more consistent
form of user judgment instead of a gold standard. Us-
ing Relevance-Prediction increases the confidence in our
results and strengthens the statistical statements we can
make about the benefits of summarization.
The next section describes an alternative approach to
measuring task-based usefulness, where the usage of ex-
ternal judgments as a gold standard is replaced by the
3A topic is an event or activity, along with all directly re-
lated events and activities. An event is something that happens
at some specific time and place, and the unavoidable conse-
quences.
user?s own decisions on the full text. Following the lead
of earlier evaluations (Oka and Ueda, 2000; Mani et al,
2002; Sakai and Sparck-Jones, 2001), we focus on rele-
vance assessment as our extrinsic task.
3 Evaluation of Usefulness of Summaries
We define a new extrinsic measure of task-based useful-
ness called Relevance-Prediction, where we compare a
summary-based decision to the subject?s own full-text de-
cision rather than to a different subject?s decision. Our
findings differ from that of the SUMMAC results (Mani
et al, 2002) in that using Relevance-Prediction as an al-
ternative to comparision to a gold standard is a more re-
alistic agreement measure for assessing usefulness in a
relevance assessment task. For example, users perform-
ing browsing tasks must examine document surrogates,
but open the full-text only if they expect the document to
be interesting to them. They are not trying to decide if
the document will be interesting to someone else.
To determine the usefulness of summarization, we fo-
cus on two questions:
? Can users make judgments on summaries that are
consistent with their full-text judgments?
? Can users make judgments on summaries more
quickly than on full document text?
First we describe the Relevance-Prediction measure for
determining whether users can make accurate judgments
with a summary. Following this, we describe our exper-
iments and results using this measure, including the tim-
ing results of summaries compared to full documents.
3.1 Relevance-Prediction Measure
To answer the first question above, we define a mea-
sure called Relevance-Prediction, where subjects build
their own ?gold standard? based on the full-text docu-
ments. Agreement is measured by comparing subjects?
surrogate-based judgments against their own judgments
on the corresponding texts. The subject?s judgment is as-
signed a value of 1 if his/her surrogate judgment is the
same as the corresponding full-text judgment, and 0 oth-
erwise. These values were summed over all judgments
for a surrogate type and were divided by the total num-
ber of judgments for that surrogate type to determine the
effectiveness of the associated summary method.
Formally, given a summary/document pair (s, d), if
subjects make the same judgment on s that they did on
d, we say j(s, d) = 1. If subjects change their judg-
ment between s and d, we say j(s, d) = 0. Given a set
of summary/document pairs DSi associated with event i,
the Relevance-Prediction score is computed as follows:
Relevance-Prediction(i) =
?
s,d?DSi
j(s, d)
|DSi|
This approach provides a more reliable comparison
mechanism than gold standard judgments provided by
2
other individuals. Specifically, Relevance-Prediction is
more helpful in illuminating the usefulness of summaries
for a real-world scenario, e.g., a browsing environment,
where credit is given when an individual subject would
choose (or reject) a document under both conditions. To
our knowledge, this subject-driven approach to testing
usefulness has never before been used.
3.2 Experiment Design
Ten human subjects were recruited to evaluate full-text
documents and two summary types.4 The original text
documents were taken from the Topic Detection and
Tracking 3 (TDT-3) corpus (Allan et al, 1999) which
contains news stories and headlines, topic and event de-
scriptions, and a mapping between news stories and their
related topic and/or events. Although the TDT-3 collec-
tion contains transcribed speech documents, our investi-
gation was restricted to documents that were originally
text, i.e., newspaper or newswire, not broadcast news.
For our experiment we selected three distinct events
and related document sets5 from TDT-3. For each event,
the subjects were given a description of the event (writ-
ten by LDC) and then asked to judge relevance of a set
of 20 documents associated with that event (using three
different presentation types to be discussed below).
The events used from the TDT data set were events
from world news occurring in 1998. It is possible that
the subjects had some prior knowledge about the events,
yet we believe that this would not affect their ability to
complete the task. Subjects? background knowledge of an
event can also make this task more similar to real-world
browsing tasks, in which subjects are often familiar with
the event or topic they are searching for.
The 20 documents were retrieved by a search engine.
We used a constrained subset where exactly half (10)
were judged relevant by the LDC annotators. Because all
20 documents were somewhat similar to the event, this
approach ensured that our task would be more difficult
than it would be if we had chosen documents from com-
pletely unrelated events (where the choice of relevance
would be obvious even from a poorly written summary).
Each document was pre-annotated with the headline
associated with the original newswire source. These
headlines were used as the first summary type. We re-
fer to them as HEAD (Headline Surrogate). The average
length of the HEAD surrogates was 53 characters. In ad-
dition, we commissioned human-generated summaries6
of each document as the second summary type; we refer
4We required all human subjects to be native-English speak-
ers to ensure that the accuracy of judgments was not degraded
by language barriers.
5The three event and related document sets contained
enough data points to achieve statistically significant results.
6The human summarizers were instructed to create a sum-
mary no greater than 75 characters for each specified full text
document. The summaries were not compared for writing style
or quality.
to this as HUM (Human Surrogate). The average length
of the HUM surrogates was 72 characters. Although nei-
ther of these summaries was produced automatically, our
experiment allowed us to focus on the question of sum-
mary usefulness and to learn about the differences in pre-
sentation style as a first step toward experimentation with
the output of automatic summarization systems.
Two main factors were measured: (1) differences
in judgments for the three presentation types (HEAD,
HUM, and the full-text document) and (2) judgment time.
Each subject made a total of 60 judgments for each pre-
sentation type since there were 3 distinct events and 20
documents per event. To facilitate the analysis of the data,
the subjects? judgments were constrained to two possibil-
ities, relevant or not relevant.7
Although the HEAD and HUM surrogates were both
produced by humans, they differed in style. The HEAD
surrogates were shorter than the HUM surrogates by
26%. Many of these were ?eye-catchers? designed to en-
tice the reader to examine the entire document (i.e., pur-
chase the newspaper); that is, the HEAD surrogates were
not intended to stand in the place of the full document.
By contrast, the writers of the HUM surrogates were in-
structed to write text that conveyed what happened in the
full document. We observed that the HUM surrogates
used more words and phrases extracted from the full doc-
uments than the HEAD surrogates.
Experiments were conducted using a web browser (In-
ternet Explorer) on a PC in the presence of the experi-
menter. Subjects were given written and verbal instruc-
tions for completing their task and were asked to make
relevance judgments on a practice event set. The judg-
ments from the practice event set were not included in
our experimental results or used in our analyses. The
written instructions were given to aid subjects in deter-
mining requirements for relevance. For example, in an
Election event documents describing new people in of-
fice, new public officials, change in governments or par-
liaments were suggested as evidence for relevance.
Each of the ten subjects made judgments on 20 doc-
uments for each of three different events. After reading
each document or summary, the subjects clicked on a ra-
dio button corresponding to their judgment and clicked
a submit button to move to the next document descrip-
tion. Subjects were not allowed to move to the next sum-
mary/document until a valid selection was made. No
backing up was allowed. Judgment time was computed
as the number of seconds it took the subject to read the
full text document or surrogate, comprehend it, compare
it to the event description, and make a judgment (timed
up until the subject clicked the submit button).
7If we allowed subjects to make additional judgments such
as somewhat relevant, this could possibly encourage subjects to
always choose this when they were the least bit unsure. Previ-
ous experiments indicate that this additional selection method
may increase the level of variability in judgments (Zajic et al,
2004).
3
3.3 Order of Document/Surrogate Presentation
One concern with our evaluation methodology was the
issue of possible memory effects or priming: if the same
subjects saw a summary and a full document about the
same event, their answers might be tainted. Thus, prior to
the full experiment, we conducted pre-experiments (us-
ing 4 participants) with an extreme form of influence: we
presented the summary and full text in immediate suc-
cession. In these experiments, we compared two docu-
ment presentation approaches, termed ?Drill Down? and
?Complete Set.? In the ?Drill Down? document presen-
tation approach all three presentation types were shown
for each document, in sequence: first a single HEAD sur-
rogate, followed by the corresponding HUM surrogate,
followed by the full text document. This process was re-
peated 10 times.
In the ?Complete Set? document-presentation ap-
proach we presented the complete set of documents us-
ing one surrogate type, followed by the complete set us-
ing another surrogate type, and so on. That is, the 10
HEAD surrogates were displayed all at once, followed
by the corresponding 10 HUM surrogates, followed by
the corresponding 10 full-text documents.
The results indicated that there was almost no effect
between the two document-presentation approaches. The
performance varied only slightly and neither approach
consistently allowed subjects to perform better than the
other. Therefore, we determined that the subjects were
not associating a given summary with its corresponding
full-text documents. This may be due, in part, to the fact
that all 20 documents were related to the event?and ac-
cording to the LDC relevance judgments half of these
were actually about the same event.
Given that the variations were insignificant in these
pre-experiments, we selected only the Complete-Set ap-
proach (no Drill-Down) for the full experiment. How-
ever, we still needed to vary the ordering for the two sur-
rogate presentation types associated with each full-text
document. Thus, each 20-document set was divided in
half for each subject. In the first half, the subject saw the
first 10 documents as: (1) HEAD surrogates, then HUM
surrogates and then the full-text document; or (2) HUM
surrogates, then HEAD surrogates, and then the full-text
document. In the second half, the subject saw the alter-
native ordering, e.g., if a subject saw HEAD surrogates
before HUM surrogates in the first half, he/she saw the
HUM surrogates before HEAD surrogates for the sec-
ond half. Either way, the full-text document was always
shown last so as not to introduce judgment effects asso-
ciated with reading the entire document before either sur-
rogate type.
In addition to varying the ordering for the surrogate
type, the ordering of the surrogates and full documents
within the events were also varied. The subjects were
grouped in pairs, and each pair viewed the surrogates and
documents in a different order than the other pairs.
3.4 Experimental Hypotheses
We hypothesized that the summaries would allow sub-
jects to achieve a Relevance-Prediction rate of 70?90%.
Since these summaries were significantly shorter than the
original document text, we expected that the rate would
not be 100% compared to the judgments made on the full
document text. However, we expected higher than a 50%
ratio, i.e., higher than that of random judgments on all of
the surrogates. We also expected high performance be-
cause the meaning of the original document text is best
preserved when written by a human (Mani, 2001).
A second hypothesis is that the HEAD surrogates
would yield a significantly lower agreement rate than that
of the HUM surrogates. Our commissioned HUM surro-
gates were written to stand in place of the full document,
whereas the HEAD surrogates were written to catch a
reader?s interest. This suggests that the HEAD surrogates
might not provide as informative a description of the orig-
inal documents as the HUM surrogates.
We also tested a third hypothesis: that our Relevance-
Prediction measure would be more reliable than that of
the LDC-Agreement method used for SUMMAC-style
evaluations (thus providing a more stable framework for
evaluating summarization techniques). LDC-Agreement
compares a subject?s judgment on a surrogate or full text
against the ?correct? judgments as assigned by the TDT
corpus annotators (Linguistic Data Consortium 2001).
Finally, we tested the hypothesis that using a text sum-
mary for judging relevance would take considerably less
time than using the corresponding full-text document.
4 Experimental Results
Table 1 shows the subjects? judgments using both
Relevance-Prediction and LDC-Agreement for each of
three events. Using our Relevance-Prediction measure,
the HUM surrogates yield averages between 79% and
86%, with an overall average of 81%, thus confirming
our first hypothesis.
However, we failed to confirm our second hypothe-
sis. The HEAD Relevance-Prediction rates were between
71% and 82%, with an overall average of 76%, which
was lower than the rates for HUM, but the difference
was not statistically significant. It appeared that subjects
were able to make consistent relevance decisions from the
non-extractive HEAD surrogates, even though these were
shorter and less informative than the HUM surrogates.
A closer look reveals that the HEAD summaries some-
times contained enough information to judge relevance,
yielding almost the same number of true positives (and
true negatives) as the HUM summaries. For example, a
document about the formation of a coalition government
to avoid violence in Cambodia has the HEAD surrogate
Cambodians hope new government can avoid past mis-
takes. By contrast, the HUM surrogate for this same event
was Rival parties to form a coalition government to avoid
violence in Cambodia. Although the HEAD surrogate
4
Surrogate EVENT 1 EVENT 2 EVENT 3 Overall Avg Avg Time
LDC RP LDC RP LDC RP LDC RP (seconds)
HEAD 67% 76% 66% 71% 70% 82% 67% 76% 4.60
HUM 69% 80% 73% 86% 62% 79% 68% 81% 4.57
DOC ? ? ? ? ? ? ? ? 13.38
Table 1: Relevance-Prediction (RP) and LDC-Agreement (LDC) Rates for HEAD and HUM Surrogates for each Event
uses words that do not appear in the original document
(hope and mistakes), the subject may infer the relevance
of this surrogate by relating hope to the notion of forming
a coalition government and mistakes to violence.
On the other hand, we found that the lower degree of
informativeness of HEAD surrogates gave rise to over
50% more false negatives than the HUM summaries. This
statistically significant difference will be discussed fur-
ther in Section 6.
As for our third hypothesis, Table 1 illustrates a
substantial difference between the two agreement mea-
sures. For each of the three events, the Relevance-
Prediction rate is at least five percent higher than that
of the LDC-Agreement approach, with an average of
8.8% increase for the HEAD summary and a 13.3% aver-
age increase for the HUM summary. The average rates
across events show a statistically significant difference
between LDC-Agreement and Relevance-Prediction for
both HUM summaries with p<0.01 and HEAD sum-
maries with p<0.05. This significance was determined
through use of a single factor ANOVA statistical analysis.
The higher Relevance-Prediction rate supports our state-
ment that this approach provides a more stable framework
for evaluating different summarization techniques.
Finally, the average timing results shown in Table 1
confirm our fourth hypothesis. The subjects took 4-5 sec-
onds (on average) to make judgments on both the HEAD
and HUM summaries, as compared to about 13.4 seconds
to make judgments on full text documents. This shows
that it takes subjects almost 3 times longer to make judg-
ments on full text documents as it took to make judgments
on the summaries (HEAD and HUM). This finding is not
surprising since text summaries are an order of magnitude
shorter than full-text documents.
5 Correlation with Intrinsic Evaluation
Metric: ROUGE
We now turn to the task of correlating our extrinsic task
performance with scores produced by an intrinsic evalu-
ation measure. We used the Recall Oriented Understudy
for Gisting Evaluation (ROUGE) metric version 1.2.1. In
previous studies (Dorr et al, 2004) ROUGE was shown
to have a very low correlation with the LDC-Agreement
measurement results of the extrinsic task. This was at-
tributed to low interannotator agreement in the gold stan-
dard. Our goal was to test whether our new Relevance-
Prediction technique would allow us to induce higher cor-
relations with ROUGE.
5.1 Extrinsic Agreement Data
To reduce the effect of outliers on the correlation between
ROUGE and the human judgments, we averaged over all
judgments for each subject (20 judgments ? 3 events) to
produce 60 data points. These data points were then par-
titioned into either 1, 2, or 4 partitions of equal size. (Par-
titions of size four have 15 data points, partitions of size
two have 30 data points, and partitions of size one have
60 data points per subject?or a total of 600 datapoints
across all 10 subjects). To ensure that the correlation did
not depend on a specific partition, we repeated this same
process using 10,000 different (randomly generated) par-
titions for each of the three partition sizes.
Partitioned data points of size four provided a high de-
gree of noise reduction without compromising the size
of the data set (15 points). Larger partition sizes would
result in too few data points and compromise the statis-
tical significance of our correlation results. In order to
show the variation within a single partition, we used the
partitioning of size 4 with the smallest mean square er-
ror on the human headline compared to the other parti-
tionings as a representative partition. For this represen-
tative partitioning, the individual data points P1?P15 of
that partition are shown for each of the two agreement
measures in Tables 2 and 3. This shows that, across parti-
tions, the maximum and minimum Relevance-Prediction
rates for HEAD (93% and 60%) are higher than the cor-
responding LDC-Agreement rates (85% and 50%). The
same trend is seen with the HUM surrogates: Relevance-
Prediction maximum of 98%, minimum of 68%; and
LDC-Agreement maximum 88%, minimum of 55%.
5.2 Intrinsic ROUGE Score
To correlate the partitioned agreement scores above with
our intrinsic measure, we first ran ROUGE on all 120 sur-
rogates in our experiment (i.e., the HUM and HEAD sur-
rogates for each of the 60 event/document pairs) and then
averaged the ROUGE scores for all surrogates belong-
ing to the same partitions (for each of the three partition
sizes). These partitioned ROUGE values were then used
for detecting correlations with the corresponding parti-
tioned agreement scores described above.
Table 4 shows the ROUGE scores, based on 3 ref-
erence summaries per document, for partitions P1?P15
used in the previous tables.8 For brevity, we include
8We commissioned a total of 180 human-generated refer-
ence summaries (3 for each of 60 documents) (in addition to
the human generated summaries used in the experiment).
5
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15
HEAD 80% 80% 85% 70% 73% 60% 80% 75% 60% 75% 88% 68% 80% 93% 83%
HUM 83% 88% 85% 68% 75% 75% 93% 75% 98% 90% 75% 70% 80% 90% 78%
Table 2: Relevance-Prediction Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15
HEAD 70% 73% 85% 70% 63% 60% 60% 85% 50% 73% 70% 78% 65% 63% 73%
HUM 68% 75% 58% 68% 75% 70% 68% 80% 88% 58% 63% 55% 55% 60% 78%
Table 3: LDC-Agreement Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 Avg
HEAD .10 .23 .13 .27 .20 .24 .26 .22 .13 .08 .30 .16 .26 .27 .30 .211
HUM .16 .22 .17 .23 .19 .36 .39 .29 .28 .25 .37 .22 .22 .39 .27 .269
Table 4: Average Rouge-1 Scores for HEAD and HUM Surrogates (Representative Partition of Size 4)
only ROUGE 1-gram measurement (R1).9 The ROUGE
scores for HEAD surrogates were slightly lower than
those for HUM surrogates. This is consistent with
our statements earlier about the difference between non-
extractive ?eye-catchers? and informative headlines. Be-
cause ROUGE measures whether a particular summary
has the same words (or n-grams) as a reference summary,
a more constrained choice of words (as found in the ex-
tractive HUM surrogates) makes it more likely that the
summary would match the reference.
A summary in which the word choice is less
constrained?as in the non-extractive HEAD
surrogates?is less likely to share n-grams with the
reference. Thus, we may see non-extractive summaries
that have almost identical meanings, but very different
words. This raises the concern that ROUGE may be
sensitive to the style of summarization that is used.
Section 6 discusses this point further.
5.3 Intrinsic and Extrinsic Correlation
To test whether ROUGE correlates more highly with
Relevance-Prediction than with LDC-Agreement, we cal-
culated the correlation for the results of both techniques
using Pearson?s r (Siegel and Castellan, 1988):
?n
i=1(ri ? r?)(si ? s?)??n
i=1(ri ? r?)
2
??n
i=1(si ? s?)
2
where ri is the ROUGE score of surrogate i, r? is the av-
erage ROUGE score of all data points, si is the agree-
ment score of summary i (using Relevance-Prediction or
LDC-Agreement), and s? is the average agreement score.
Pearson?s statistics is commonly used in summarization
and machine translation evaluation, see e.g. (Lin, 2004;
Lin and Och, 2004).
As one might expect, there is some variability in the
correlation between ROUGE and human judgments for
9We also computed ROUGE 2-gram, ROUGE L and
ROUGE W, but the trend for these did not differ from ROUGE-
1.
Figure 1: Distribution of the Correlation Variation for
Relevance-Prediction on HEAD and HUM
the different partitions. However, the boxplots for both
HEAD and HUM indicate that the first and third quartile
were relatively close to the median (see Figure 1).
Table 5 shows the Pearson Correlations with ROUGE-
1 using Relevance-Prediction and LDC-Agreement. For
Relevance-Prediction, we observed a positive correlation
for both surrogate types, with a slightly higher corre-
lation for HEAD than HUM. For LDC-Agreement, we
observed no correlation (or a minimally negative one)
with ROUGE-1 scores, for both the HEAD and HUM
surrogates. The highest correlation was observed for
Relevance-Prediction on HEAD.
We conclude that ROUGE correlates more highly with
the Relevance-Prediction measurement than the LDC-
Agreement measurement, although we should add that
none of the correlations in Table 5 were statistically sig-
nificant at p < 0.05. The low LDC-Agreement scores are
consistent with previous studies where poor correlations
6
Surrogate P = 1 P = 2 P = 4
HEAD (RP) 0.1270 0.1943 0.3140
HUM (RP) 0.0632 0.1096 0.1391
HEAD (LDC) -0.0968 -0.0660 -0.0099
HUM (LDC) -0.0395 -0.0236 -0.0187
Table 5: Pearson Correlations with ROUGE-1 for
Relevance-Prediction (RP) and LDC-Agreement (LDC),
where Partition size (P) = 1, 2, and 4
were attributed to low interannotator agreement rates.
6 Discussion
Our results suggest that ROUGE may be sensitive to the
style of summarization that is used. As we observed
above, many of the HEAD surrogates were not actually
summaries of the full text, but were eye-catchers. Of-
ten, these surrogates did not allow the subject to judge
relevance correctly, resulting in lower agreement. In ad-
dition, these same surrogates often did not use a high per-
centage of words that were actually from the story, result-
ing in low ROUGE scores. (We noticed that most words
in the HUM surrogates appeared in the corresponding
stories.) There were three consequences of this difference
between HEAD and HUM: (1) The rate of agreement was
lower for HEAD than for HUM; (2) The average ROUGE
score was lower for HEAD than for HUM; and (3) The
correlation of ROUGE scores with agreement was higher
for HEAD than for HUM.
A further analysis supports the (somewhat counterin-
tuitive) third point above. Although the ROUGE scores
of true positives (and true negatives) were significantly
lower for HEAD surrogates (0.2127 and 0.2162) than
for HUM surrogates (0.2696 and 0.2715), the number of
false negatives was substantially higher for HEAD sur-
rogates than for HUM surrogates. These cases corre-
sponded to much lower ROUGE scores for HEAD sur-
rogates (0.1996) than for HUM (0.2586) surrogates.
A summary of this analysis is given in Table 6, where
true positives and negatives are indicated by Rel/Rel
and NonRel/NonRel, respectively, and false positives and
negatives are indicated by Rel/NonRel and NonRel/Rel,
respectively.10 The numbers in parentheses after each
ROUGE score refer to the standard deviation for that
10We also included (average) elapsed times for summary
judgments in each of the four categories. One might expect a
?relevant? judgment to be much quicker than a ?non-relevant?
judgment (since the latter might require reading the full sum-
mary). However, it turned out non-relevant judgments did not
always take longer. In fact, the NonRel/NonRel cases took con-
siderably less time than the Rel/Rel and Rel/NonRel cases. On
the other hand, the NonRel/Rel cases took considerably more
time?almost as much time as reading the full text documents?
an indication that the subjects may have re-read the summary a
number of times, perhaps vacillating back and forth. Still, the
overall time savings was significant, given that the vast major-
ity of the non-relevant judgments were in the NonRel/NonRel
category.
score. This was computed as follows:
Std .-Dev . =
?
?N
i=1(xi ? x?)
2
N
where N is the number of surrogates in a particular judg-
ment category (e.g., N = 245 for the HEAD-based Non-
Rel/Rel judgments), xi is the ROUGE score for the ith
surrogate, and r? is the average of all ROUGE scores in
that category.
Although there were very few false positives (less than
6% for both HEAD and HUM), the number of false nega-
tives (NonRel/Rel) was particularly high for HEAD (50%
higher than for HUM). This difference was statistically
significant at p<0.01 using the t-test. The large number
of false negatives with HEAD may be attributed to the
eye-catching nature of these surrogates. A subject may
be misled into thinking that this surrogate is not related
to an event because the surrogate does not contain words
from the event description and is too broad for the subject
to extract definitive information (e.g., the surrogate There
he goes again!). Because the false negatives were associ-
ated with the lowest average ROUGE score (0.1996), we
speculate that, if a correlation exists between Relevance-
Prediction and ROUGE, the false negatives may be a ma-
jor contributing factor.
Based on this experiment, we conjecture that ROUGE
may not be a good method for measuring the useful-
ness of summaries when the summaries are not extrac-
tive. That is, if someone intentionally writes summaries
that contain different words than the story, the summaries
will also likely contain different words than a reference
summary, resulting in low ROUGE scores. However,
the summaries, if well-written, could still result in high
agreement with the judgments made on the full text.
7 Conclusion
We have shown that two types of human summaries,
HEAD and HUM, can be useful for relevance assessment
in that they help a user achieve 70-85% agreement in rel-
evance judgments. We observed a 65% reduction in judg-
ment time between full texts and summaries. These find-
ings are important in that they establish the usefulness
of summarization and they support research and devel-
opment of additional summarization methods, including
automatic methods.
We introduced a new method for measuring agree-
ment, Relevance-Prediction, which takes a subject?s
full-text judgment as the standard against which the
same subject?s summary judgment is measured. Be-
cause Relevance-Prediction was more reliable than LDC-
Agreement judgments, we encourage others to use this
measure in future summarization evaluations.
Using this new method, we were able to find positive
correlations between relevance assessments and ROUGE
scores for HUM and HEAD surrogates, where only
7
Judgment HEAD HUM
(Surr/Doc) Raw R1-Avg Avg Time Raw R1-Avg Avg Time
Rel/Rel 211 (35%) 0.2127 (?0.120) 4.6 251 (42%) 0.2696 (?0.130) 4.2
Rel/NonRel 27 (5%) 0.2115 (?0.110) 7.1 35 (6%) 0.2725 (?0.131) 4.6
NonRel/Rel 117 (19%) 0.1996 (?0.127) 8.5 77 (13%) 0.2586 (?0.120) 13.8
NonRel/NonRel 245 (41%) 0.2162 (?0.126) 2.5 237 (39%) 0.2715 (?0.131) 1.9
TOTAL 600 (100%) 0.2115 (?0.124) 4.6 600 (100%) 0.2691 (?0.129) 4.6
Table 6: Subjects? Judgments and Corresponding Average ROUGE 1 Scores
negative correlations were found using LDC-Agreement
scores. We found that both the Relevance-Prediction and
the ROUGE-1 scores were higher for human-generated
summaries than for the original headlines. It appears
that most of the difference is induced by surrogates that
are eye-catchers (rather than true summaries), where both
agreement and ROUGE scores are low.
Our future work will include further experimentation
with automatic summarization methods to determine the
level of Relevance-Prediction. We aim to determine how
well automatic summarizers help users complete tasks,
and to investigate which automatic summarizers perform
better than others. We also plan to test for correlations
between ROUGE and human task performance with auto-
matic summaries, to further investigate whether ROUGE
is a good predictor of human task performance.
Acknowledgements
This work was supported in part by DARPA TIDES Cooperative
Agreement N66001-00-2-8910.
References
James Allan, Hubert Jin, Martin Rajman, Charles Wayne,
Daniel Gildea, Victor Lavrenko, Rose Hoberman, and David
Caputo. 1999. Topic-based Novelty Detection. Technical
Report 1999 Summer Workshop at CLSP Final Report, Johns
Hopkins, Maryland.
Jean Carletta. 1996. Assessing Agreement on Classification
Tasks: The Kappa Statistic. Computational Lingusitics,
22(2):249?254, June.
Bonnie J. Dorr, Christof Monz, Douglas Oard, Stacy President,
and David Zajic. 2004. Extrinsic Evaluation of Automatic
Metrics for Summarization. Technical report, University of
Maryland, College Park, MD. LAMP-TR-115, CAR-TR-
999, CS-TR-4610, UMIACS-TR-2004-48.
Barbara Di Eugenio and Michael Glass. 2004. Squibs and Dis-
cussions - The Kappa Statistic: A Second Look. Computa-
tional Linguistics, pages 95?101.
Donna Harman and Paul Over. 2004. Proceedings of the DUC
2004. Boston, MA.
Hongyan Jing, Regina Barzilay, Kathleen McKeown, and
Michael Elhadad. 1998. Summarization evaluation meth-
ods: Experiments and analysis. In Proceedings of the AAAI
Symposium on Intelligent Summarization, Stanford Univer-
sity, CA, March 23-25.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation
of Summaries Using N-gram Co-Occurrence Statistics. In
Proceedings of HLT-NAACL 2003 Workshop, pages 71?78,
Edmonton Canada, May-June.
Chin-Yew Lin and Franz Joseph Och. 2004. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics (COLING
2004), Geneva, Switzerland, August 23?27.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic Eval-
uation of Summaries. In Proceedings of the Workshop on
Text Summarization Branches Out (WAS 2004), Barcelona,
Spain, July 25?26.
I. Mani and E. Bloedorn. 1998. Summarizing Similarities and
Differences Among Related Documents. Information Re-
trieval, 1(1):35?67.
Inderjeet Mani, Gary Klein, David House, and Lynette
Hirschman. 2002. SUMMAC: a text summarization eval-
uation. Natural Language Engineering, 8(1):43?68.
Inderjeet Mani. 2001. Summarization Evaluation: An
Overview. In Proceedings of the NAACL 2001 Workshop on
Automatic Summarization.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluating
Content Selection in Summarization: The Pyramid Method.
In Proceedings of the NAACL 2004, Boston, MA.
Mamiko Oka and Yoshihiro Ueda. 2000. Evaluation of Phrase-
Representation Summarization Based on an Information Re-
trieval Task. In Proceedings of the ANLP/NAACL 2000
Workshop on Automatic Summarization, pages 59?68, New
Brunswick, NJ.
Dragomir Radev and Kathleen McKeown. 1998. Generat-
ing Natural Language Summaries from Multiple On-Line
Sources. Computational Linguistics, pages 469?500.
Tetsuya Sakai and Karen Sparck-Jones. 2001. Generic Sum-
maries for Indexing in Information Retrieval - Detailed Test
Results. Technical Report TR513, Computer Laboratory,
University of Cambridge.
Sidney Siegel and N. John Castellan, Jr. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill, New
York, second edition.
Karen Sparck-Jones and J.R. Gallier. 1996. Evaluating Natu-
ral Language Processing Systems: An Analysis and Review.
Springer, Berlin.
Anastasios Tombros and Mark Sanderson. 1998. Advantages
of query biased summaries in information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information Re-
trieval, pages 2?10.
Hans van Halteren and Simone Teufel. 2003. Examining the
Consensus Between Human Summaries: Initial Experiments
with Factoid Analysis. In Proceedings of the HLT-NAACL
03 Text Summarization Workshop.
David Zajic, Bonnie J. Dorr, Richard Schwartz, and Stacy Presi-
dent. 2004. Headline Evaluation Experiment Results. Tech-
nical report, University of Maryland, College Park, MD.
UMIACS-TR-2004-18.
8
Proceedings of the Third Workshop on Statistical Machine Translation, pages 183?186,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Incremental Hypothesis Alignment for Building Confusion Networks with
Application to Machine Translation System Combination
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
Confusion network decoding has been the
most successful approach in combining out-
puts from multiple machine translation (MT)
systems in the recent DARPA GALE and
NIST Open MT evaluations. Due to the vary-
ing word order between outputs from differ-
ent MT systems, the hypothesis alignment
presents the biggest challenge in confusion
network decoding. This paper describes an
incremental alignment method to build confu-
sion networks based on the translation edit rate
(TER) algorithm. This new algorithm yields
significant BLEU score improvements over
other recent alignment methods on the GALE
test sets and was used in BBN?s submission to
the WMT08 shared translation task.
1 Introduction
Confusion network decoding has been applied in
combining outputs from multiple machine transla-
tion systems. The earliest approach in (Bangalore
et al, 2001) used edit distance based multiple string
alignment (MSA) (Durbin et al, 1988) to build the
confusion networks. The recent approaches used
pair-wise alignment algorithms based on symmetric
alignments from a HMM alignment model (Matusov
et al, 2006) or edit distance alignments allowing
shifts (Rosti et al, 2007). The alignment method
described in this paper extends the latter by incre-
mentally aligning the hypotheses as in MSA but also
allowing shifts as in the TER alignment.
The confusion networks are built around a ?skele-
ton? hypothesis. The skeleton hypothesis defines
the word order of the decoding output. Usually, the
1-best hypotheses from each system are considered
as possible skeletons. Using the pair-wise hypoth-
esis alignment, the confusion networks are built in
two steps. First, all hypotheses are aligned against
the skeleton independently. Second, the confusion
networks are created from the union of these align-
ments. The incremental hypothesis alignment algo-
rithm combines these two steps. All words from the
previously aligned hypotheses are available, even if
not present in the skeleton hypothesis, when align-
ing the following hypotheses. As in (Rosti et al,
2007), confusion networks built around all skeletons
are joined into a lattice which is expanded and re-
scored with language models. System weights and
language model weights are tuned to optimize the
quality of the decoding output on a development set.
This paper is organized as follows. The incre-
mental TER alignment algorithm is described in
Section 2. Experimental evaluation comparing the
incremental and pair-wise alignment methods are
presented in Section 3 along with results on the
WMT08 Europarl test sets. Conclusions and future
work are presented in Section 4.
2 Incremental TER Alignment
The incremental hypothesis alignment is based on
an extension of the TER algorithm (Snover et al,
2006). The extension allows using a confusion net-
work as the reference. First, the algorithm finds the
minimum edit distance between the hypothesis and
the reference network by considering all word arcs
between two consecutive nodes in the reference net-
work as possible matches for a hypothesis word at
183
1 2 3 4 5 6I (3)
NULL (2)
like (3)
NULL (2)
big blue (1)
balloons (2)
blue (1) kites (1)
Figure 1: Network after pair-wise TER alignment.
that position. Second, shifts of blocks of words that
have an exact match somewhere else in the network
are tried in order to find a new hypothesis word or-
der with a lower TER. Each shifted block is con-
sidered a single edit. These two steps are executed
iteratively as a greedy search. The final alignment
between the re-ordered hypothesis and the reference
network may include matches, substitutions, dele-
tions, and insertions.
The confusion networks are built by creating a
simple confusion network from the skeleton hypoth-
esis. If the skeleton hypothesis has   words, the
initial network has   arcs and   nodes. Each
arc has a set of system specific confidence scores.
The score for the skeleton system is set to  and
the confidences for other systems are set to zeros.
For each non-skeleton hypothesis, a TER alignment
against the current network is executed as described
above. Each match found will increase the system
specific word arc confidence by 
	 where 
is the rank of the hypothesis in that system?s   -best
list. Each substitution will generate a new word arc
at the corresponding position in the network. The
word arc confidence for the system is set to 
	
and the confidences for other systems are set to ze-
ros. Each deletion will generate a new NULL word
arc unless one exists at the corresponding position
in the network. The NULL word arc confidences are
adjusted as in the case of a match or a substitution
depending on whether the NULL word arc exists or
not. Finally, each insertion will generate a new node
and two word arcs at the corresponding position in
the network. The first word arc will have the in-
serted word with the confidence set as in the case
of a substitution and the second word arc will have
a NULL word with confidences set by assuming all
previously aligned hypotheses and the skeleton gen-
erated the NULL word arc.
After all hypotheses have been added into the con-
fusion network, the system specific word arc confi-
dences are scaled to sum to one over all arcs between
1 2 3 4 5 6I (3) like (3)
kites (1)
NULL (2) NULL (1)
big (1) blue (2)
balloons (2)
Figure 2: Network after incremental TER alignment.
each set of two consecutive nodes. Other scores for
the word arc are set as in (Rosti et al, 2007).
2.1 Benefits over Pair-Wise TER Alignment
The incremental hypothesis alignment guarantees
that insertions between a hypothesis and the cur-
rent confusion network are always considered when
aligning the following hypotheses. This is not the
case in any pair-wise hypothesis alignment algo-
rithm. During the pair-wise hypothesis alignment,
an identical word in two hypotheses may be aligned
as an insertion or a substitution in a different posi-
tion with respect to the skeleton. This will result in
undesirable repetition and lower confidence for that
word in the final confusion network. Also, multiple
insertions are not handled implicitly.
For example, three hypotheses ?I like balloons?,
?I like big blue balloons?, and ?I like blue kites?
might be aligned by the pair-wise alignment, assum-
ing the first as the skeleton, as follows:
I like NULL balloons NULL
I like big blue balloons NULL
I like NULL balloons NULL
I like NULL blue kites
which results in the confusion network shown in
Figure 1. The number of hypotheses proposing each
word is shown in parentheses. The alignment be-
tween the skeleton and the second hypothesis has
two consecutive insertions ?big blue? which are not
available for matching when the third hypothesis is
aligned against the skeleton. Therefore, the word
?blue? appears twice in the confusion network. If
many hypotheses have multiple insertions at the
same location with respect to the skeleton, they have
to be treated as phrases or a secondary alignment
process has to be applied.
Assuming the same hypotheses as above, the in-
cremental hypothesis alignment may yield the fol-
lowing alignment:
184
System TER BLEU MTR
worst 53.26 33.00 63.15
best 42.30 48.52 67.71
syscomb pw 39.85 52.00 68.73
syscomb giza 40.01 52.24 68.68
syscomb inc 39.25 52.73 68.97
oracle 21.68 64.14 78.18
Table 1: Results on the Arabic GALE Phase 2 system
combination tuning set with four reference translations.
I like NULL NULL balloons
I like big blue balloons
I like NULL blue kites
which results in the confusion network shown in
Figure 2. In this case the word ?blue? is available
for matching when the third hypothesis is aligned.
It should be noted that the final confusion network
depends on the order in which the hypotheses are
added. The experiments so far have indicated that
different alignment order does not have a significant
influence on the final combination results as mea-
sured by the automatic evaluation metrics. Usually,
aligning the system outputs in the decreasing order
of their TER scores on the development set yields
the best scores.
2.2 Confusion Network Oracle
The extended TER algorithm can also be used to
estimate an oracle TER in a confusion network by
aligning the reference translations against the con-
fusion network. The oracle hypotheses can be ex-
tracted by finding a path with the maximum number
of matches. These hypotheses give a lower bound
on the TER score for the hypotheses which can be
generated from the confusion networks.
3 Experimental Evaluation
The quality of the final combination output depends
on many factors. Combining very similar outputs
does not yield as good gains as combining out-
puts from diverse systems. It is also important that
the development set used to tune the combination
weights is as similar to the evaluation set as possi-
ble. This development set should be different from
the one used to tune the individual systems to avoid
bias toward any system that may be over-tuned. Due
System TER BLEU MTR
worst 59.09 20.74 57.24
best 48.18 31.46 62.61
syscomb pw 46.31 33.02 63.18
syscomb giza 46.03 33.39 63.21
syscomb inc 45.45 33.90 63.45
oracle 27.53 49.10 71.81
Table 2: Results on the Arabic GALE Phase 2 evaluation
set with one reference translation.
to the tight schedule for the WMT08, there was no
time to experiment with many configurations. As
more extensive experiments have been conducted in
the context of the DARPA GALE program, results
on the Arabic GALE Phase 2 evaluation setup are
first presented. The translation quality is measured
by three MT evaluation metrics: TER (Snover et al,
2006), BLEU (Papineni et al, 2002), and METEOR
(Lavie and Agarwal, 2007).
3.1 Results on Arabic GALE Outputs
For the Arabic GALE Phase 2 evaluation, nine sys-
tems were combined. Five systems were phrase-
based, two hierarchical, one syntax-based, and one
rule-based. All statistical systems were trained on
common parallel data, tuned on a common genre
specific development set, and a common English to-
kenization was used. The English bi-gram and 5-
gram language models used in the system combina-
tion were trained on about 7 billion words of English
text. Three iterations of bi-gram decoding weight
tuning were performed followed by one iteration of
5-gram re-scoring weight tuning. All weights were
tuned to minimize the sum of TER and 1-BLEU.
The final 1-best outputs were true-cased and deto-
kenized before scoring.
The results on the newswire system combination
development set and the GALE Phase 2 evaluation
set are shown in Tables 1 and 2. The first two
rows show the worst and best scores from the in-
dividual systems. The scores may be from different
systems as the best performing system in terms of
TER was not necessarily the best performing system
in terms of the other metrics. The following three
rows show the scores of three combination outputs
where the only difference was the hypothesis align-
ment method. The first, syscomb pw, corresponds
185
BLEU
System de-en fr-en
worst 11.84 16.31
best 28.30 33.13
syscomb 29.05 33.63
Table 3: NIST BLEU scores on the German-English (de-
en) and French-English (fr-en) Europarl test2008 set.
to the pair-wise TER alignment described in (Rosti
et al, 2007). The second, syscomb giza, cor-
responds to the pair-wise symmetric HMM align-
ments from GIZA++ described in (Matusov et al,
2006). The third, syscomb inc, corresponds to
the incremental TER alignment presented in this pa-
per. Finally, oracle corresponds to an estimate of
the lower bound on the translation quality obtained
by extracting the TER oracle output from the con-
fusion networks generated by the incremental TER
alignment. It is unlikely that there exists a set of
weights that would yield the oracle output after de-
coding, though. The incremental TER alignment
yields significant improvements over all individual
systems and the combination outputs using the pair-
wise alignment methods.
3.2 Results on WMT08 Europarl Outputs
On the WMT08 shared translation task, transla-
tions for two language pairs and two tasks were
provided for the system combination experiments.
Twelve systems participated in the German-English
and fourteen in the French-English translation tasks.
The translations of the Europarl test (test2008) were
provided as the development set outputs and the
translations of the News test (newstest2008) were
provided as the evaluation set outputs. An English
bi-gram, 4-gram, and true-caser language models
were trained by using all English text available for
the WMT08 shared task, including Europarl mono-
lingual and news commentary parallel training sets.
The outputs were tokenized and lower-cased before
combination, and the final combination output was
true-cased and detokenized.
The results on the Europarl test set for both lan-
guage pairs are shown in table 3. The first two rows
have the NIST BLEU scores of the worst and the
best individual systems. The last row, syscomb,
corresponds to the system combination using the in-
cremental TER alignment. The improvements in the
NIST BLEU scores are fairly modest which is prob-
ably due to low diversity of the system outputs. It is
also unlikely that these weights are optimal for the
out-of-domain News test set outputs.
4 Conclusions
This paper describes a novel hypothesis alignment
algorithm for building confusion networks from
multiple machine translation system outputs. The al-
gorithm yields significant improvements on the Ara-
bic GALE evaluation set outputs and was used in
BBN?s submission to the WMT08 shared translation
task. The hypothesis alignment may benefit from
using stemming and synonymy in matching words.
Also, special handling of punctuation may improve
the alignment further. The future work will inves-
tigate the influence of better alignment to the final
combination outputs.
Acknowledgments
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program.
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proc. ASRU, pages 351?354.
R. Durbin, S.R. Eddy, A. Krogh, and G. Mitchison. 1988.
Biological Sequence Analysis: Probabilistic Models of
Proteins and Nucleic Acids. Cambridge Univ. Press.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for MT evaluation with high levels of cor-
relation with human judgments. In Proc. ACL/WMT,
pages 228?231.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proc. EACL, pages 33?40.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311?318.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proc. ACL 2007, pages 312?319.
M. Snover, B. Dorr, R. Schwartz, L. Micciula, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA, pages
223?231.
186
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 61?65,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Incremental Hypothesis Alignment with Flexible Matching for Building
Confusion Networks: BBN System Description for WMT09 System
Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
 
arosti,bzhang,smatsouk,schwartz  @bbn.com
Abstract
This paper describes the incremental hy-
pothesis alignment algorithm used in the
BBN submissions to the WMT09 system
combination task. The alignment algo-
rithm used a sentence specific alignment
order, flexible matching, and new shift
heuristics. These refinements yield more
compact confusion networks compared to
using the pair-wise or incremental TER
alignment algorithms. This should reduce
the number of spurious insertions in the
system combination output and the sys-
tem combination weight tuning converges
faster. System combination experiments
on the WMT09 test sets from five source
languages to English are presented. The
best BLEU scores were achieved by comb-
ing the English outputs of three systems
from all five source languages.
1 Introduction
Machine translation (MT) systems have different
strengths and weaknesses which can be exploited
by system combination methods resulting in an
output with a better performance than any indi-
vidual MT system output as measured by auto-
matic evaluation metrics. Confusion network de-
coding has become the most popular approach to
MT system combination. The first confusion net-
work decoding method (Bangalore et al, 2001)
was based on multiple string alignment (MSA)
(Durbin et al, 1988) borrowed from biological
sequence analysis. However, MSA does not al-
low re-ordering. The translation edit rate (TER)
(Snover et al, 2006) produces an alignment be-
tween two strings and allows shifts of blocks of
words. The availability of the TER software has
made it easy to build a high performance system
combination baseline (Rosti et al, 2007).
The pair-wise TER alignment originally de-
scribed by Sim et al (2007) has various limita-
tions. First, the hypotheses are aligned indepen-
dently against the skeleton which determines the
word order of the output. The same word from
two different hypotheses may be inserted in differ-
ent positions w.r.t. the skeleton and multiple inser-
tions require special handling. Rosti et al (2008)
described an incremental TER alignment to miti-
gate these problems. The incremental TER align-
ment used a global order in which the hypotheses
were aligned. Second, the TER software matches
words with identical surface strings. The pair-
wise alignment methods proposed by Ayan et al
(2008), He et al (2008), and Matusov et al (2006)
are able to match also synonyms and words with
identical stems. Third, the TER software uses a set
of heuristics which is not always optimal in de-
termining the block shifts. Karakos et al (2008)
proposed using inversion transduction grammars
to produce different pair-wise alignments.
This paper is organized as follows. A refined
incremental alignment algorithm is described in
Section 2. Experimental evaluation comparing
the pair-wise and incremental TER alignment al-
gorithms with the refined alignment algorithm on
WMT09 system combination task is presented in
Section 3. Conclusions and future work are pre-
sented in Section 4.
2 Incremental Hypothesis Alignment
with Flexible Matching
2.1 Sentence Specific Alignment Order
Rosti et al (2008) proposed incremental hypothe-
sis alignment using a system specific order. This
is not likely to be optimal since one MT system
may have better output on one sentence and worse
on another. More principled approach is similar to
MSA where the order is determined by the edit
distance of the hypothesis from the network for
61
17
0
1NULL(6.2e-7)
9
NULL(0.9999)
2cereal
NULL
3
thomas
4
jefferson
edison
5
says
6
eat
7
your
8
NULL
vegetables
NULL
10
eat
11
your 12cereal
NULL
13
thomas 14
edison
jefferson
15
says 16vegetables
NULL
NULL
(a) Alignment using the standard TER shift heuristics.
150
1NULL(0.5)
8
NULL(0.5)
2
thomas
3jefferson
edison
4
says
5
eat
6
your
7
vegetables
cereal NULL
9
eat
10
your
11
cereal
vegetables
12
thomas 13edison
jefferson
14
says
NULL
(b) Alignment using the modified shift heuristics.
Figure 1: Combined confusion networks using different shift heuristics. The initial NULL arcs include
the prior probability estimates in parentheses.
each sentence. The TER scores of the remaining
unaligned hypotheses using the current network as
the reference are computed. The hypothesis with
the lowest edit cost w.r.t. the network is aligned.
Given  systems, this increases the number of
alignments performed from  to 	
 .
2.2 Flexible Matching
The TER software assigns a zero cost for match-
ing tokens and a cost of one for all errors includ-
ing insertions, deletions, substitutions, and block
shifts. Ayan et al (2008) modified the TER soft-
ware to consider substitutions of synonyms with
a reduced cost. Recently, Snover et al (2009)
extended the TER algorithm in a similar fashion
to produce a new evaluation metric, TER plus
(TERp), which allows tuning of the edit costs in
order to maximize correlation with human judg-
ment. The incremental alignment with flexible
matching uses WordNet (Fellbaum, 1998) to find
all possible synonyms and words with identical
stems in a set of hypotheses. Substitutions involv-
ing synonyms and words with identical stems are
considered with a reduced cost of 0.2.
2.3 Modified Shift Heuristics
The TER is computed by trying shifts of blocks of
words that have an exact match somewhere else in
the reference in order to find a re-ordering of the
hypothesis with a lower edit distance to the refer-
ence. Karakos et al (2008) showed that the shift
heuristics in TER do not always yield an optimal
alignment. Their example used the following two
hypotheses:
1. thomas jefferson says eat your vegetables
2. eat your cereal thomas edison says
A system combination lattice using TER align-
ment is shown in Figure 1(a). The blocks
?eat your? are shifted when building both con-
fusion networks. Using the second hypothe-
sis as the skeleton seems to give a better align-
ment. The lower number of edits also results in a
higher skeleton prior shown between nodes 0 and
9. There are obviously some undesirable paths
through the lattice but it is likely that a language
model will give a higher score to the reasonable
hypotheses.
Since the flexible matching allows substitutions
with a reduced cost, the standard TER shift heuris-
tics have to be modified. A block of words may
have some words with identical matches and other
words with synonym matches. In TERp, synonym
and stem matches are considered as exact matches
for the block shifts, otherwise the TER shift con-
straints are used. In the flexible matching, the shift
heuristics were modified to allow any block shifts
62
that do not increase the edit cost. A system combi-
nation lattice using the modified shift heuristics is
shown in Figure 1(b). The optimal shifts of blocks
?eat your cereal? and ?eat your vegetables? were
found and both networks received equal skeleton
priors. TERp would yield this alignment only
if these blocks appear in the paraphrase table or
if ?cereal? and ?vegetables? are considered syn-
onyms. This example is artificial and does not
guarantee that optimal shifts are always found.
3 Experimental Evaluation
System combination experiments combining the
English WMT09 translation task outputs were per-
formed. A total of 96 English outputs were pro-
vided including primary, contrastive, and  -best
outputs. Only the primary  -best outputs were
combined due to time constraints. The numbers
of primary systems per source language were: 3
for Czech, 15 for German, 9 for Spanish, 15 for
French, and 3 for Hungarian. The English bigram
and 5-gram language models were interpolated
from four LM components trained on the English
monolingual Europarl (45M tokens) and News
(510M tokens) corpora, and the English sides of
the News Commentary (2M tokens) and Giga-
FrEn (683M tokens) parallel corpora. The interpo-
lation weights were tuned to minimize perplexity
on news-dev2009 set. The system combination
weights ? one for each system, LM weight, and
word and NULL insertion penalties ? were tuned
to maximize the BLEU (Papineni et al, 2002)
score on the tuning set (newssyscomb2009).
Since the system combination was performed on
tokenized and lower cased outputs, a trigram-
based true caser was trained on all News training
data. The tuning may be summarized as follows:
1. Tokenize and lower case the outputs;
2. Align hypotheses incrementally using each
output as a skeleton;
3. Join the confusion networks into a lattice
with skeleton specific prior estimates;
4. Extract a  -best list from the lattice given
the current weights;
5. Merge the  -best list with the hypotheses
from the previous iteration;
6. Tune new weights given the current merged
 -best list;
7. Iterate 4-6 three times;
8. Extract a  -best list from the lattice given
the best decoding weights and re-score hy-
potheses with a 5-gram;
9. Tune re-scoring weights given the final  -
best list;
10. Extract  -best hypotheses from the  -best
list given the best re-scoring weights, re-case,
and detokenize.
After tuning the system combination weights, the
outputs on a test set may be combined using the
same steps excluding 4-7 and 9. The hypothesis
scores and tuning are identical to the setup used in
(Rosti et al, 2007).
Case insensitive TER and BLEU scores for the
combination outputs using the pair-wise and in-
cremental TER alignment as well as the flexible
alignment on the tuning (dev) and test sets are
shown in Table 1. Only case insensitive scores
are reported since the re-casers used by different
systems are very different and some are trained
using larger resources than provided for WMT09.
The scores of the worst and best individual sys-
tem outputs are also shown. The best and worst
TER and BLEU scores are not necessarily from
the same system output. Both incremental
and flexible alignments used sentence spe-
cific alignment order. Combinations using the in-
cremental and flexible hypothesis alignment algo-
rithms consistently outperform the ones using the
pair-wise TER alignment. The flexible alignment
is slightly better than the incremental alignment on
Czech, Spanish, and Hungarian, and significantly
better on French to English test set scores.
Since the test sets for each language pair consist
of translations of the same documents, it is pos-
sible to combine outputs from many source lan-
guages to English. There were a total of 46 En-
glish primary  -best system outputs. Using all 46
outputs would have required too much memory in
tuning, so a subset of 11 outputs was chosen. The
11 outputs consist of google, uedin, and uka
outputs on all languages. Case insensitive TER
and BLEU scores for the xx-en combination are
shown in Table 2. In addition to incremental
and flexible alignment methods which used
sentence specific alignment order, scores for in-
cremental TER alignment with a fixed alignment
order used in the BBN submissions to WMT08
63
dev cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.30 17.63 82.01 6.83 65.64 19.74 69.19 15.21 78.70 10.33
best 58.16 23.12 57.24 23.20 53.02 29.48 49.78 32.27 66.77 13.59
pairwise 59.60 24.01 56.35 26.04 53.11 29.49 51.03 31.65 69.58 14.60
incremental 59.22 24.31 55.73 26.73 53.05 29.72 50.72 32.09 70.15 14.85
flexible 59.38 24.18 55.51 26.71 52.62 30.24 50.22 32.58 69.83 14.88
test cz-en de-en es-en fr-en hu-en
System TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU
worst 67.74 16.37 82.39 6.81 65.44 19.04 71.44 14.49 81.21 9.90
best 59.53 21.18 59.41 21.30 53.34 28.69 51.33 31.14 68.32 12.75
pairwise 61.02 21.25 58.75 23.41 53.65 28.15 53.17 29.83 71.50 13.39
incremental 60.63 21.67 58.13 23.96 53.47 28.38 52.51 30.45 71.69 13.60
flexible 60.34 21.87 58.05 23.86 53.13 28.57 51.98 31.30 71.17 13.84
Table 1: Case insensitive TER and BLEU scores on newssyscomb2009 (dev) and newstest2009
(test) for five source languages.
(Rosti et al, 2008) are marked as incr-wmt08.
The sentence specific alignment order yields about
a half BLEU point gain on the tuning set and a
one BLEU point gain on the test set. All system
combination experiments yield very good BLEU
gains on both sets. The scores are also signifi-
cantly higher than any combination from a single
source language. This shows that the outputs from
different source languages are likely to be more di-
verse than outputs from different MT systems on a
single language pair. The combination is not guar-
anteed to be the best possible as the set of outputs
was chosen arbitrarily.
The compactness of the confusion networks
may be measured by the average number of
nodes and arcs per segment. All xx-en con-
fusion networks for newssyscomb2009 and
newstest2009 after the incremental TER
alignment had on average 44.5 nodes and 112.7
arcs per segment. After the flexible hypothesis
alignment, there were on average 41.1 nodes and
104.6 arcs per segment. The number of NULL
word arcs may also be indicative of the alignment
quality. The flexible hypothesis alignment reduced
the average number of NULL word arcs from 29.0
to 24.8 per segment. The rate of convergence in
the  -best list based iterative tuning may be mon-
itored by the number of new hypotheses in the
merged  -best lists from iteration to iteration. By
the third tuning iteration, there were 10% fewer
new hypotheses in the merged  -best list when
using the flexible hypothesis alignment.
xx-en dev test
System TER BLEU TER BLEU
worst 74.21 12.80 75.84 12.05
best 49.78 32.27 51.33 31.14
pairwise 46.10 35.95 47.77 33.53
incr-wmt08 44.58 36.84 46.60 33.61
incremental 44.59 37.30 46.42 34.61
flexible 44.54 37.38 45.82 34.48
Table 2: Case insensitive TER and BLEU
scores on newssyscomb2009 (dev) and
newstest2009 (test) for xx-en combination.
4 Conclusions
This paper described a refined incremental hy-
pothesis alignment algorithm used in the BBN
submissions to the WMT09 system combination
task. The new features included sentence specific
alignment order, flexible matching, and modified
shift heuristics. The refinements yield more com-
pact confusion networks which should allow fewer
spurious insertions in the output and faster conver-
gence in tuning. The future work will investigate
tunable edit costs and methods to choose an opti-
mal subset of outputs for combination.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
64
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 33?
40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceed-
ings of the Automatic Speech Recognition and Un-
derstanding Workshop (ASRU), pages 351?354.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1988. Biological Sequence
Analysis: Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107.
Damianos Karakos, Jason Eisner, Sanjeev Khundan-
pur, and Markus Dreyer. 2008. Machine trans-
lation system combination using ITG-based align-
ments. In Proceedings of ACL-08: HLT, pages 81?
84.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Proceedings of the 11th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 33?40.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine
translation system combination. In Proceedings of
the 32nd IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 105?108.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation.
65
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259?268,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Fluency, Adequacy, or HTER?
Exploring Different Human Judgments with a Tunable MT Metric
Matthew Snover?, Nitin Madnani?, Bonnie J. Dorr? ? & Richard Schwartz? ?
?Laboratory for Computational Linguistics and Information Processing
?Institute for Advanced Computer Studies
?University of Maryland, College Park
?Human Language Technology Center of Excellence
?BBN Technologies
{snover,nmadnani,bonnie}@umiacs.umd.edu schwartz@bbn.com
Abstract
Automatic Machine Translation (MT)
evaluation metrics have traditionally been
evaluated by the correlation of the scores
they assign to MT output with human
judgments of translation performance.
Different types of human judgments, such
as Fluency, Adequacy, and HTER, mea-
sure varying aspects of MT performance
that can be captured by automatic MT
metrics. We explore these differences
through the use of a new tunable MT met-
ric: TER-Plus, which extends the Transla-
tion Edit Rate evaluation metric with tun-
able parameters and the incorporation of
morphology, synonymy and paraphrases.
TER-Plus was shown to be one of the
top metrics in NIST?s Metrics MATR
2008 Challenge, having the highest aver-
age rank in terms of Pearson and Spear-
man correlation. Optimizing TER-Plus
to different types of human judgments
yields significantly improved correlations
and meaningful changes in the weight of
different types of edits, demonstrating sig-
nificant differences between the types of
human judgments.
1 Introduction
Since the introduction of the BLEU metric (Pa-
pineni et al, 2002), statistical MT systems have
moved away from human evaluation of their per-
formance and towards rapid evaluation using au-
tomatic metrics. These automatic metrics are
themselves evaluated by their ability to generate
scores for MT output that correlate well with hu-
man judgments of translation quality. Numer-
ous methods of judging MT output by humans
have been used, including Fluency, Adequacy,
and, more recently, Human-mediated Translation
Edit Rate (HTER) (Snover et al, 2006). Fluency
measures whether a translation is fluent, regard-
less of the correct meaning, while Adequacy mea-
sures whether the translation conveys the correct
meaning, even if the translation is not fully flu-
ent. Fluency and Adequacy are frequently mea-
sured together on a discrete 5 or 7 point scale,
with their average being used as a single score
of translation quality. HTER is a more complex
and semi-automatic measure in which humans do
not score translations directly, but rather generate
a new reference translation that is closer to the
MT output but retains the fluency and meaning
of the original reference. This new targeted refer-
ence is then used as the reference translation when
scoring the MT output using Translation Edit Rate
(TER) (Snover et al, 2006) or when used with
other automatic metrics such as BLEU or ME-
TEOR (Banerjee and Lavie, 2005). One of the
difficulties in the creation of targeted references
is a further requirement that the annotator attempt
to minimize the number of edits, as measured by
TER, between the MT output and the targeted ref-
erence, creating the reference that is as close as
possible to the MT output while still being ade-
quate and fluent. In this way, only true errors in
the MT output are counted. While HTER has been
shown to be more consistent and finer grained than
individual human annotators of Fluency and Ade-
quacy, it is much more time consuming and tax-
ing on human annotators than other types of hu-
man judgments, making it difficult and expensive
to use. In addition, because HTER treats all edits
equally, no distinction is made between serious er-
rors (errors in names or missing subjects) and mi-
nor edits (such as a difference in verb agreement
259
or a missing determinator).
Different types of translation errors vary in im-
portance depending on the type of human judg-
ment being used to evaluate the translation. For
example, errors in tense might barely affect the ad-
equacy of a translation but might cause the trans-
lation be scored as less fluent. On the other hand,
deletion of content words might not lower the flu-
ency of a translation but the adequacy would suf-
fer. In this paper, we examine these differences
by taking an automatic evaluation metric and tun-
ing it to these these human judgments and exam-
ining the resulting differences in the parameteri-
zation of the metric. To study this we introduce
a new evaluation metric, TER-Plus (TERp)1 that
improves over the existing Translation Edit Rate
(TER) metric (Snover et al, 2006), incorporating
morphology, synonymy and paraphrases, as well
as tunable costs for different types of errors that
allow for easy interpretation of the differences be-
tween human judgments.
Section 2 summarizes the TER metric and dis-
cusses how TERp improves on it. Correlation re-
sults with human judgments, including indepen-
dent results from the 2008 NIST Metrics MATR
evaluation, where TERp was consistently one of
the top metrics, are presented in Section 3 to show
the utility of TERp as an evaluation metric. The
generation of paraphrases, as well as the effect of
varying the source of paraphrases, is discussed in
Section 4. Section 5 discusses the results of tuning
TERp to Fluency, Adequacy and HTER, and how
this affects the weights of various edit types.
2 TER and TERp
Both TER and TERp are automatic evaluation
metrics for machine translation that score a trans-
lation, the hypothesis, of a foreign language text,
the source, against a translation of the source text
that was created by a human translator, called a
reference translation. The set of possible cor-
rect translations is very large?possibly infinite?
and any single reference translation is just a sin-
gle point in that space. Usually multiple refer-
ence translations, typically 4, are provided to give
broader sampling of the space of correct transla-
tions. Automatic MT evaluation metrics compare
the hypothesis against this set of reference trans-
lations and assign a score to the similarity; higher
1Named after the nickname??terp??of the University of
Maryland, College Park, mascot: the diamondback terrapin.
scores are given to hypotheses that are more simi-
lar to the references.
In addition to assigning a score to a hypothe-
sis, the TER metric also provides an alignment be-
tween the hypothesis and the reference, enabling it
to be useful beyond general translation evaluation.
While TER has been shown to correlate well with
human judgments of translation quality, it has sev-
eral flaws, including the use of only a single ref-
erence translation and the measuring of similarity
only by exact word matches between the hypoth-
esis and the reference. The handicap of using a
single reference can be addressed by the construc-
tion of a lattice of reference translations. Such a
technique has been used with TER to combine the
output of multiple translation systems (Rosti et al,
2007). TERp does not utilize this methodology2
and instead focuses on addressing the exact match-
ing flaw of TER. A brief description of TER is pre-
sented in Section 2.1, followed by a discussion of
how TERp differs from TER in Section 2.2.
2.1 TER
One of the first automatic metrics used to evaluate
automatic machine translation (MT) systems was
Word Error Rate (WER) (Niessen et al, 2000),
which is the standard evaluation metric for Au-
tomatic Speech Recognition. WER is computed
as the Levenshtein (Levenshtein, 1966) distance
between the words of the system output and the
words of the reference translation divided by the
length of the reference translation. Unlike speech
recognition, there are many correct translations for
any given foreign sentence. These correct transla-
tions differ not only in their word choice but also
in the order in which the words occur. WER is
generally seen as inadequate for evaluation for ma-
chine translation as it fails to combine knowledge
from multiple reference translations and also fails
to model the reordering of words and phrases in
translation.
TER addresses the latter failing of WER by al-
lowing block movement of words, called shifts.
within the hypothesis. Shifting a phrase has the
same edit cost as inserting, deleting or substitut-
ing a word, regardless of the number of words
being shifted. While a general solution to WER
with block movement is NP-Complete (Lopresti
2The technique of combining references in this fashion
has not been evaluated in terms of its benefit when correlating
with human judgments. The authors hope to examine and
incorporate such a technique in future versions of TERp.
260
and Tomkins, 1997), TER addresses this by using
a greedy search to select the words to be shifted,
as well as further constraints on the words to be
shifted. These constraints are intended to simu-
late the way in which a human editor might choose
the words to shift. For exact details on these con-
straints, see Snover et al (2006). There are other
automatic metrics that follow the general formu-
lation as TER but address the complexity of shift-
ing in different ways, such as the CDER evaluation
metric (Leusch et al, 2006).
When TER is used with multiple references, it
does not combine the references. Instead, it scores
the hypothesis against each reference individually.
The reference against which the hypothesis has the
fewest number of edits is deemed the closet refer-
ence, and that number of edits is used as the nu-
merator for calculating the TER score. For the de-
nominator, TER uses the average number of words
across all the references.
2.2 TER-Plus
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms. In ad-
dition, it uses probabilistic phrasal substitutions
to align phrases in the hypothesis and reference.
These phrases are generated by considering possi-
ble paraphrases of the reference words. Matching
using stems and synonyms (Banerjee and Lavie,
2005) and using paraphrases (Zhou et al, 2006;
Kauchak and Barzilay, 2006) have previously been
shown to be beneficial for automatic MT evalu-
ation. Paraphrases have also been shown to be
useful in expanding the number of references used
for parameter tuning (Madnani et al, 2007; Mad-
nani et al, 2008) although they are not used di-
rectly in this fashion within TERp. While all edit
costs in TER are constant, all edit costs in TERp
are optimized to maximize correlation with human
judgments. This is because while a set of constant
weights might prove adequate for the purpose of
measuring translation quality?as evidenced by
correlation with human judgments both for TER
and HTER?they may not be ideal for maximiz-
ing correlation.
TERp uses all the edit operations of TER?
Matches, Insertions, Deletions, Substitutions and
Shifts?as well as three new edit operations: Stem
Matches, Synonym Matches and Phrase Substitu-
tions. TERp identifies words in the hypothesis and
reference that share the same stem using the Porter
stemming algorithm (Porter, 1980). Two words
are determined to be synonyms if they share the
same synonym set according to WordNet (Fell-
baum, 1998). Sequences of words in the reference
are considered to be paraphrases of a sequence of
words in the hypothesis if that phrase pair occurs
in the TERp phrase table. The TERp phrase table
is discussed in more detail in Section 4.
With the exception of the phrase substitutions,
the cost for all other edit operations is the same re-
gardless of what the words in question are. That
is, once the edit cost of an operation is determined
via optimization, that operation costs the same no
matter what words are under consideration. The
cost of a phrase substitution, on the other hand,
is a function of the probability of the paraphrase
and the number of edits needed to align the two
phrases according to TERp. In effect, the proba-
bility of the paraphrase is used to determine how
much to discount the alignment of the two phrases.
Specifically, the cost of a phrase substitution be-
tween the reference phrase, p1 and the hypothesis
phrase p2 is:
cost(p1, p2) =w1+
edit(p1, p2)?
(w2 log(Pr(p1, p2))
+ w3 Pr(p1, p2) + w4)
where w1, w2, w3, and w4 are the 4 free param-
eters of the edit cost, edit(p1, p2) is the edit cost
according to TERp of aligning p1 to p2 (excluding
phrase substitutions) and Pr(p1, p2) is the prob-
ability of paraphrasing p1 as p2, obtained from
the TERp phrase table. The w parameters of the
phrase substitution cost may be negative while still
resulting in a positive phrase substitution cost, as
w2 is multiplied by the log probability, which is al-
ways a negative number. In practice this term will
dominate the phrase substitution edit cost.
This edit cost for phrasal substitutions is, there-
fore, specified by four parameters, w1, w2, w3
and w4. Only paraphrases specified in the TERp
phrase table are considered for phrase substitu-
tions. In addition, the cost for a phrasal substi-
tution is limited to values greater than or equal to
0, i.e., the substitution cost cannot be negative. In
addition, the shifting constraints of TERp are also
relaxed to allow shifting of paraphrases, stems,
and synonyms.
261
In total TERp uses 11 parameters out of which
four represent the cost of phrasal substitutions.
The match cost is held fixed at 0, so that only the
10 other parameters can vary during optimization.
All edit costs, except for the phrasal substitution
parameters, are also restricted to be positive. A
simple hill-climbing search is used to optimize the
edit costs by maximizing the correlation of human
judgments with the TERp score. These correla-
tions are measured at the sentence, or segment,
level. Although it was done for the experiments
described in this paper, optimization could also
be performed to maximize document level correla-
tion ? such an optimization would give decreased
weight to shorter segments as compared to the seg-
ment level optimization.
3 Correlation Results
The optimization of the TERp edit costs, and com-
parisons against several standard automatic eval-
uation metrics, using human judgments of Ade-
quacy is first described in Section 3.1. We then
summarize, in Section 3.2, results of the NIST
Metrics MATR workshop where TERp was eval-
uated as one of 39 automatic metrics using many
test conditions and types of human judgments.
3.1 Optimization of Edit Costs and
Correlation Results
As part of the 2008 NIST Metrics MATR work-
shop (Przybocki et al, 2008), a development sub-
set of translations from eight Arabic-to-English
MT systems submitted to NIST?s MTEval 2006
was released that had been annotated for Ade-
quacy. We divided this development set into an
optimization set and a test set, which we then used
to optimize the edit costs of TERp and compare it
against other evaluation metrics. TERp was op-
timized to maximize the segment level Pearson
correlation with adequacy on the optimization set.
The edit costs determined by this optimization are
shown in Table 1.
We can compare TERp with other metrics by
comparing their Pearson and Spearman corre-
lations with Adequacy, at the segment, docu-
ment and system level. Document level Ade-
quacy scores are determined by taking the length
weighted average of the segment level scores. Sys-
tem level scores are determined by taking the
weighted average of the document level scores in
the same manner.
We compare TERp with BLEU (Papineni et al,
2002), METEOR (Banerjee and Lavie, 2005), and
TER (Snover et al, 2006). The IBM version of
BLEU was used in case insensitive mode with
an ngram-size of 4 to calculate the BLEU scores.
Case insensitivity was used with BLEU as it was
found to have much higher correlation with Ade-
quacy. In addition, we also examined BLEU using
an ngram-size of 2 (labeled as BLEU-2), instead
of the default ngram-size of 4, as it often has a
higher correlation with human judgments. When
using METEOR, the exact matching, porter stem-
ming matching, and WordNet synonym matching
modules were used. TER was also used in case
insensitive mode.
We show the Pearson and Spearman correlation
numbers of TERp and the other automatic metrics
on the optimization set and the test set in Tables 2
and 3. Correlation numbers that are statistically
indistinguishable from the highest correlation, us-
ing a 95% confidence interval, are shown in bold
and numbers that are actually not statistically sig-
nificant correlations are marked with a ?. TERp
has the highest Pearson correlation in all condi-
tions, although not all differences are statistically
significant. When examining the Spearman cor-
relation, TERp has the highest correlation on the
segment and system levels, but performs worse
than METEOR on the document level Spearman
correlatons.
3.2 NIST Metrics MATR 2008 Results
TERp was one of 39 automatic metrics evaluated
in the 2008 NIST Metrics MATR Challenge. In
order to evaluate the state of automatic MT eval-
uation, NIST tested metrics across a number of
conditions across 8 test sets. These conditions in-
cluded segment, document and system level corre-
lations with human judgments of preference, flu-
ency, adequacy and HTER. The test sets included
translations from Arabic-to-English, Chinese-to-
English, Farsi-to-English, Arabic-to-French, and
English-to-French MT systems involved in NIST?s
MTEval 2008, the GALE (Olive, 2005) Phase 2
and Phrase 2.5 program, Transtac January and July
2007, and CESTA run 1 and run 2, covering mul-
tiple genres. The version of TERp submitted to
this workshop was optimized as described in Sec-
tion 3.1. The development data upon which TERp
was optimized was not part of the test sets evalu-
ated in the Challenge.
262
Phrase Substitution
Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4
0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
Table 1: Optimized TERp Edit Costs
Optimization Set Test Set Optimization+Test
Metric Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.623 0.867 0.952 0.563 0.852 0.948 0.603 0.861 0.954
BLEU-2 0.661 0.888 0.946 0.591 0.876 0.953 0.637 0.883 0.952
METEOR 0.731 0.894 0.952 0.751 0.904 0.957 0.739 0.898 0.958
TER -0.609 -0.864 -0.957 -0.607 -0.860 -0.959 -0.609 -0.863 -0.961
TERp -0.782 -0.912 -0.996 -0.787 -0.918 -0.985 -0.784 -0.914 -0.994
Table 2: Optimization & Test Set Pearson Correlation Results
Due to the wealth of testing conditions, a sim-
ple overall view of the official MATR08 results re-
leased by NIST is difficult. To facilitate this anal-
ysis, we examined the average rank of each metric
across all conditions, where the rank was deter-
mined by their Pearson and Spearman correlation
with human judgments. To incorporate statistical
significance, we calculated the 95% confidence in-
terval for each correlation coefficient and found
the highest and lowest rank from which the cor-
relation coefficient was statistically indistinguish-
able, resulting in lower and upper bounds of the
rank for each metric in each condition. The aver-
age lower bound, actual, and upper bound ranks
(where a rank of 1 indicates the highest correla-
tion) of the top metrics, as well as BLEU and TER,
are shown in Table 4, sorted by the average upper
bound Pearson correlation. Full descriptions of the
other metrics3, the evaluation results, and the test
set composition are available from NIST (Przy-
bocki et al, 2008).
This analysis shows that TERp was consistently
one of the top metrics across test conditions and
had the highest average rank both in terms of Pear-
son and Spearman correlations. While this anal-
ysis is not comprehensive, it does give a general
idea of the performance of all metrics by syn-
thesizing the results into a single table. There
are striking differences between the Spearman and
Pearson correlations for other metrics, in particu-
lar the CDER metric (Leusch et al, 2006) had the
second highest rank in Spearman correlations (af-
3System description of metrics are also distributed
by AMTA: http://www.amtaweb.org/AMTA2008.
html
ter TERp), but was the sixth ranked metric accord-
ing to the Pearson correlation. In several cases,
TERp was not the best metric (if a metric was the
best in all conditions, its average rank would be 1),
although it performed well on average. In partic-
ular, TERp did significantly better than the TER
metric, indicating the benefit of the enhancements
made to TER.
4 Paraphrases
TERp uses probabilistic phrasal substitutions to
align phrases in the hypothesis with phrases in the
reference. It does so by looking up?in a pre-
computed phrase table?paraphrases of phrases in
the reference and using its associated edit cost as
the cost of performing a match against the hy-
pothesis. The paraphrases used in TERp were ex-
tracted using the pivot-based method as described
in (Bannard and Callison-Burch, 2005) with sev-
eral additional filtering mechanisms to increase
the precision. The pivot-based method utilizes the
inherent monolingual semantic knowledge from
bilingual corpora: we first identify English-to-F
phrasal correspondences, then map from English
to English by following translation units from En-
glish to F and back. For example, if the two En-
glish phrases e1 and e2 both correspond to the
same foreign phrase f, then they may be consid-
ered to be paraphrases of each other with the fol-
lowing probability:
p(e1|e2) ? p(e1|f) ? p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in comput-
263
Optimization Set Test Set Optimization+Test
Metric Seg Doc Sys Seg Doc Sys Seg Doc Sys
BLEU 0.635 0.816 0.714? 0.550 0.740 0.690? 0.606 0.794 0.738?
BLEU-2 0.643 0.823 0.786? 0.558 0.747 0.690? 0.614 0.799 0.738?
METEOR 0.729 0.886 0.881 0.727 0.853 0.738? 0.730 0.876 0.922
TER -0.630 -0.794 -0.810? -0.630 -0.797 -0.667? -0.631 -0.801 -0.786?
TERp -0.760 -0.834 -0.976 -0.737 -0.818 -0.881 -0.754 -0.834 -0.929
Table 3: MT06 Dev. Optimization & Test Set Spearman Correlation Results
Metric Average Rank by Pearson Average Rank by Spearman
TERp 1.49 6.07 17.31 1.60 6.44 17.76
METEOR v0.7 1.82 7.64 18.70 1.73 8.21 19.33
METEOR ranking 2.39 9.45 19.91 2.18 10.18 19.67
METEOR v0.6 2.42 10.67 19.11 2.47 11.27 19.60
EDPM 2.45 8.21 20.97 2.79 7.61 20.52
CDER 2.93 8.53 19.67 1.69 8.00 18.80
BleuSP 3.67 9.93 21.40 3.16 8.29 20.80
NIST-v11b 3.82 11.13 21.96 4.64 12.29 23.38
BLEU-1 (IBM) 4.42 12.47 22.18 4.98 14.87 24.00
BLEU-4 (IBM) 6.93 15.40 24.69 6.98 14.38 25.11
TER v0.7.25 8.87 16.27 25.29 6.93 17.33 24.80
BLEU-4 v12 (NIST) 10.16 18.02 27.64 10.96 17.82 28.16
Table 4: Average Metric Rank in NIST Metrics MATR 2008 Official Results
ing the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?) ? p(f ?|e2)
The corpus used for extraction was an Arabic-
English newswire bitext containing a million sen-
tences. A few examples of the extracted para-
phrase pairs that were actually used in a run of
TERp on the Metrics MATR 2008 development
set are shown below:
(brief ? short)
(controversy over? polemic about)
(by using power? by force)
(response? reaction)
A discussion of paraphrase quality is presented
in Section 4.1, followed by a brief analysis of the
effect of varying the pivot corpus used by the auto-
matic paraphrase generation upon the correlation
performance of the TERp metric in Section 4.2.
4.1 Analysis of Paraphrase Quality
We analyzed the utility of the paraphrase probabil-
ity and found that it was not always a very reliable
estimate of the degree to which the pair was se-
mantically related. For example, we looked at all
paraphrase pairs that had probabilities greater than
0.9, a set that should ideally contain pairs that are
paraphrastic to a large degree. In our analysis, we
found the following five kinds of paraphrases in
this set:
(a) Lexical Paraphrases. These paraphrase
pairs are not phrasal paraphrases but instead
differ in at most one word and may be con-
sidered as lexical paraphrases for all practical
purposes. While these pairs may not be very
valuable for TERp due to the obvious overlap
with WordNet, they may help in increasing
the coverage of the paraphrastic phenomena
that TERp can handle. Here are some exam-
ples:
(2500 polish troops? 2500 polish soldiers)
(accounting firms? auditing firms)
(armed source? military source)
(b) Morphological Variants. These phrasal
pairs only differ in the morphological form
264
for one of the words. As the examples show,
any knowledge that these pairs may provide
is already available to TERp via stemming.
(50 ton? 50 tons)
(caused clouds? causing clouds)
(syria deny? syria denies)
(c) Approximate Phrasal Paraphrases. This
set included pairs that only shared partial se-
mantic content. Most paraphrases extracted
by the pivot method are expected to be of this
nature. These pairs are not directly beneficial
to TERp since they cannot be substituted for
each other in all contexts. However, the fact
that they share at least some semantic content
does suggest that they may not be entirely
useless either. Examples include:
(mutual proposal? suggest)
(them were exiled? them abroad)
(my parents? my father)
(d) Phrasal Paraphrases. We did indeed find
a large number of pairs in this set that were
truly paraphrastic and proved the most useful
for TERp. For example:
(agence presse? news agency)
(army roadblock? military barrier)
(staff walked out? team withdrew)
(e) Noisy Co-occurrences. There are also pairs
that are completely unrelated and happen
to be extracted as paraphrases based on the
noise inherent in the pivoting process. These
pairs are much smaller in number than the
four sets described above and are not signif-
icantly detrimental to TERp since they are
rarely chosen for phrasal substitution. Exam-
ples:
(counterpart salam? peace)
(regulation dealing? list)
(recall one? deported)
Given this distribution of the pivot-based para-
phrases, we experimented with a variant of TERp
that did not use the paraphrase probability at all
but instead only used the actual edit distance be-
tween the two phrases to determine the final cost
of a phrase substitution. The results for this exper-
iment are shown in the second row of Table 5. We
can see that this variant works as well as the full
version of TERp that utilizes paraphrase probabil-
ities. This confirms our intuition that the proba-
bility computed via the pivot-method is not a very
useful predictor of semantic equivalence for use in
TERp.
4.2 Varying Paraphrase Pivot Corpora
To determine the effect that the pivot language
might have on the quality and utility of the ex-
tracted paraphrases in TERp, we used paraphrase
pairsmade available by Callison-Burch (2008).
These paraphrase pairs were extracted from Eu-
roparl data using each of 10 European languages
(German, Italian, French etc.) as a pivot language
separately and then combining the extracted para-
phrase pairs. Callison-Burch (2008) also extracted
and made available syntactically constrained para-
phrase pairs from the same data that are more
likely to be semantically related.
We used both sets of paraphrases in TERp as al-
ternatives to the paraphrase pairs that we extracted
from the Arabic newswire bitext. The results are
shown in the last four rows of Table 5 and show
that using a pivot language other than the one that
the MT system is actually translating yields results
that are almost as good. It also shows that the
syntactic constraints imposed by Callison-Burch
(2008) on the pivot-based paraphrase extraction
process are useful and yield improved results over
the baseline pivot-method. The results further sup-
port our claim that the pivot paraphrase probability
is not a very useful indicator of semantic related-
ness.
5 Varying Human Judgments
To evaluate the differences between human judg-
ment types we first align the hypothesis to the ref-
erences using a fixed set of edit costs, identical to
the weights in Table 1, and then optimize the edit
costs to maximize the correlation, without realign-
ing. The separation of the edit costs used for align-
ment from those used for scoring allows us to re-
move the confusion of edit costs selected for align-
ment purposes from those selected to increase cor-
relation.
For Adequacy and Fluency judgments, the
MTEval 2002 human judgement set4 was used.
This set consists of the output of ten MT sys-
tems, 3 Arabic-to-English systems and 7 Chinese-
4Distributed to the authors by request from NIST.
265
Pearson Spearman
Paraphrase Setup Seg Doc Sys Seg Doc Sys
Arabic pivot -0.787 -0.918 -0.985 -0.737 -0.818 -0.881
Arabic pivot and no prob -0.787 -0.933 -0.986 -0.737 -0.841 -0.881
Europarl pivot -0.775 -0.940 -0.983 -0.738 -0.865 -0.905
Europarl pivot and no prob -0.775 -0.940 -0.983 -0.737 -0.860 -0.905
Europarl pivot and syntactic constraints -0.781 -0.941 -0.985 -0.739 -0.859 -0.881
Europarl pivot, syntactic constraints and no prob -0.779 -0.946 -0.985 -0.737 -0.866 -0.976
Table 5: Results on the NIST MATR 2008 test set for several variations of paraphrase usage.
Human Phrase Substitution
Judgment Match Insert Deletion Subst. Stem Syn. Shift w1 w2 w3 w4
Alignment 0.0 0.26 1.43 1.56 0.0 0.0 0.56 -0.23 -0.15 -0.08 0.18
Adequacy 0.0 0.18 1.42 1.71 0.0 0.0 0.19 -0.38 -0.03 0.22 0.47
Fluency 0.0 0.12 1.37 1.81 0.0 0.0 0.43 -0.63 -0.07 0.12 0.46
HTER 0.0 0.84 0.76 1.55 0.90 0.75 1.07 -0.03 -0.17 -0.08 -0.09
Table 6: Optimized Edit Costs
to-English systems, consisting of a total, across
all systems and both language pairs, of 7,452 seg-
ments across 900 documents. To evaluate HTER,
the GALE (Olive, 2005) 2007 (Phase 2.0) HTER
scores were used. This set consists of the out-
put of 6 MT systems, 3 Arabic-to-English systems
and 3 Chinese-to-English systems, although each
of the systems in question is the product of system
combination. The HTER data consisted of a total,
across all systems and language pairs, of 16,267
segments across a total of 1,568 documents. Be-
cause HTER annotation is especially expensive
and difficult, it is rarely performed, and the only
source, to the authors? knowledge, of available
HTER annotations is on GALE evaluation data for
which no Fluency and Adequacy judgments have
been made publicly available.
The edit costs learned for each of these human
judgments, along with the alignment edit costs are
shown in Table 6. While all three types of human
judgements differ from the alignment costs used
in alignment, the HTER edit costs differ most sig-
nificantly. Unlike Adequacy and Fluency which
have a low edit cost for insertions and a very high
cost for deletions, HTER has a balanced cost for
the two edit types. Inserted words are strongly pe-
nalized against in HTER, as opposed to in Ade-
quacy and Fluency, where such errors are largely
forgiven. Stem and synonym edits are also penal-
ized against while these are considered equivalent
to a match for both Adequacy and Fluency. This
penalty against stem matches can be attributed to
Fluency requirements in HTER that specifically
penalize against incorrect morphology. The cost
of shifts is also increased in HTER, strongly penal-
izing the movement of phrases within the hypoth-
esis, while Adequacy and Fluency give a much
lower cost to such errors. Some of the differences
between HTER and both fluency and adequacy
can be attributed to the different systems used. The
MT systems evaluated with HTER are all highly
performing state of the art systems, while the sys-
tems used for adequacy and fluency are older MT
systems.
The differences between Adequacy and Fluency
are smaller, but there are still significant differ-
ences. In particular, the cost of shifts is over twice
as high for the fluency optimized system than the
adequacy optimized system, indicating that the
movement of phrases, as expected, is only slightly
penalized when judging meaning, but can be much
more harmful to the fluency of a translation. Flu-
ency however favors paraphrases more strongly
than the edit costs optimized for adequacy. This
might indicate that paraphrases are used to gener-
ate a more fluent translation although at the poten-
tial loss of meaning.
266
6 Discussion
We introduced a new evaluation metric, TER-Plus,
and showed that it is competitive with state-of-the-
art evaluation metrics when its predictions are cor-
related with human judgments. The inclusion of
stem, synonym and paraphrase edits allows TERp
to overcome some of the weaknesses of the TER
metric and better align hypothesized translations
with reference translations. These new edit costs
can then be optimized to allow better correlation
with human judgments. In addition, we have ex-
amined the use of other paraphrasing techniques,
and shown that the paraphrase probabilities esti-
mated by the pivot-method may not be fully ad-
equate for judgments of whether a paraphrase in
a translation indicates a correct translation. This
line of research holds promise as an external eval-
uation method of various paraphrasing methods.
However promising correlation results for an
evaluation metric may be, the evaluation of the
final output of an MT system is only a portion
of the utility of an automatic translation metric.
Optimization of the parameters of an MT system
is now done using automatic metrics, primarily
BLEU. It is likely that some features that make an
evaluation metric good for evaluating the final out-
put of a system would make it a poor metric for use
in system tuning. In particular, a metric may have
difficulty distinguishing between outputs of an MT
system that been optimized for that same metric.
BLEU, the metric most frequently used to opti-
mize systems, might therefore perform poorly in
evaluation tasks compared to recall oriented met-
rics such as METEOR and TERp (whose tuning
in Table 1 indicates a preference towards recall).
Future research into the use of TERp and other
metrics as optimization metrics is needed to better
understand these metrics and the interaction with
parameter optimization.
Finally, we explored the difference between
three types of human judgments that are often
used to evaluate both MT systems and automatic
metrics, by optimizing TERp to these human
judgments and examining the resulting edit costs.
While this can make no judgement as to the pref-
erence of one type of human judgment over an-
other, it indicates differences between these hu-
man judgment types, and in particular the differ-
ence between HTER and Adequacy and Fluency.
This exploration is limited by the the lack of a
large amount of diverse data annotated for all hu-
man judgment types, as well as the small num-
ber of edit types used by TERp. The inclusion
of additional more specific edit types could lead
to a more detailed understanding of which trans-
lation phenomenon and translation errors are most
emphasized or ignored by which types of human
judgments.
Acknowledgments
This work was supported, in part, by BBN Tech-
nologies under the GALE Program, DARPA/IPTO
Contract No. HR0011-06-C-0022 and in part by
the Human Language Technology Center of Ex-
cellence.. TERp is available on the web for down-
load at: http://www.umiacs.umd.edu/?snover/terp/.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL 2005 Workshop on Intrinsic and
Extrinsic Evaulation Measures for MT and/or Sum-
marization.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005),
pages 597?604, Ann Arbor, Michigan, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196?
205, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
http://www.cogsci.princeton.edu/?wn [2000,
September 7].
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the ACL, pages 455?
462.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block
Movements. In Proceedings of the 11th Confer-
enceof the European Chapter of the Association for
Computational Linguistics (EACL 2006).
V. I. Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions, and Reversals. Soviet
Physics Doklady, 10:707?710.
267
Daniel Lopresti and Andrew Tomkins. 1997. Block
edit models for approximate string matching. Theo-
retical Computer Science, 181(1):159?179, July.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie J. Dorr, and
Richard Schwartz. 2008. Are Multiple Reference
Translations Necessary? Investigating the Value
of Paraphrased Reference Translations in Parameter
Optimization. In Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, October.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evalua-
tion for MT research. In Proceedings of the 2nd In-
ternational Conference on Language Resources and
Evaluation (LREC-2000), pages 39?45.
Joseph Olive. 2005. Global Autonomous Language
Exploitation (GALE). DARPA/IPTO Proposer In-
formation Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Traslation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Martin F. Porter. 1980. An algorithm for suffic strip-
ping. Program, 14(3):130?137.
Mark Przybocki, Kay Peterson, and Sebas-
tian Bronsart. 2008. Official results
of the NIST 2008 ?Metrics for MAchine
TRanslation? Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/,
October.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 312?319, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas.
Liang Zhou, Chon-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating Machine Translation Results with
Paraphrase Support. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2006), pages 77?84.
268
A Lexically-Driven Algorithm for Disfluency Detection
Matthew Snover, Bonnie Dorr
Institute for Advanced Computer Studies1
University of Maryland
College Park, MD 20740
snover,bonnie@umiacs.umd.edu
Richard Schwartz
BBN
9861 Broken Land Parkway
Columbia, MD 21046
schwartz@bbn.com
Abstract
This paper describes a transformation-based learn-
ing approach to disfluency detection in speech tran-
scripts using primarily lexical features. Our method
produces comparable results to two other systems
that make heavy use of prosodic features, thus
demonstrating that reasonable performance can be
achieved without extensive prosodic cues. In addi-
tion, we show that it is possible to facilitate the iden-
tification of less frequently disfluent discourse mark-
ers by taking speaker style into account.
1 Introduction
Disfluencies in human speech are widespread and cause
problems for both downstream processing and human
readability of speech transcripts. Recent human studies
(Jones et al, 2003) have examined the effect of disfluen-
cies on the readability of speech transcripts. These results
suggest that the ?cleaning? of text by removing disfluent
words can increase the speed at which readers can process
text. Recent work on detecting edits for use in parsing
of speech transcripts (Core and Schubert, 1999), (Char-
niak and Johnson, 2001) has shown an improvement in
the parser error rate by modeling disfluencies.
Many researchers investigating disfluency detection
have focused on the use of prosodic cues, as opposed to
lexical features (Nakatani and Hirschberg, 1994). There
are different approaches to detecting disfluencies. In one
approach, one can first try to locate evidence of a gen-
eral disfluency, e.g., using prosodic features or language
model discontinuations. These locations are called inter-
ruption points (IPs). Following this, it is generally suffi-
cient to look in the nearby vicinity of the IP to find the dis-
1This work is supported in part by BBNT Con-
tract 9500006806 and an NSF CISE Infrastructure Award
EIA0130422.
fluent words. The most successful approaches so far com-
bine the detection of IPs using prosodic features and lan-
guage modeling techniques (Liu et al, 2003), (Shriberg
et al, 2001), (Stolcke et al, 1998).
Our work is based on the premise that the vast ma-
jority of disfluencies can be detected using primarily
lexical features?specifically the words themselves and
part-of-speech (POS) labels?without the use of exten-
sive prosodic cues. Lexical modeling of disfluencies with
only minimal acoustic cues has been shown to be suc-
cessful in the past using strongly statistical techniques
(Heeman and Allen, 1999). We shall discuss our algo-
rithm and compare it to two other algorithms that make
extensive use of acoustic features. Our algorithm per-
forms comparably on most of the tasks assigned and in
some cases outperforms systems that used both prosodic
and lexical features.
We discuss the task definition in Section 2. In Section
3 we describe our Transformation-Based Learning (TBL)
algorithm and its associated features. Section 4 presents
results for our system and two other systems that make
heavy use of prosodic features to detect disfluencies. We
then discuss the errors made by our system, in Section 5,
and discuss our conclusions and future work in Section 6.
2 EARS Disfluency Annotation
One of the major goals of the DARPA program for
Effective, Affordable, Reusable Speech-to-Text (EARS)
(Wayne, 2003) is to provide a rich transcription of speech
recognition output, including speaker identification, sen-
tence boundary detection and the annotation of disfluen-
cies in the transcript (This collection of additional fea-
tures is also known as Metadata). One result of this pro-
gram has been production of an annotation specification
for disfluencies in speech transcripts and the transcription
of sizable amounts of speech data, both from conversa-
tional telephone speech and broadcast news, according to
this specification (Strassel, 2003).
The task of disfluency detection is to distinguish flu-
ent from disfluent words. The EARS MDE (MetaData
Extraction) program addresses two types of disfluencies:
(i) edits?words that were not intended to be said and
that are normally replaced with the intended words, such
as repeats, restarts, and revisions; and (ii) fillers?words
with no meaning that are used as discourse markers and
pauses, such as ?you know? and ?um?.
3 The Algorithm
We set out to solve the task of disfluency detection using
primarily lexical features in a system we call System A.
This section describes our algorithm, including the set of
features we use to identify disfluencies.
The training data for the system are time aligned refer-
ence speech transcripts, with speaker identification, sen-
tence boundaries, edits, fillers and interruption points an-
notated. The input for evaluation is a transcript, either
a reference transcript or a speech recognizer output tran-
script. Some of the evaluation data may be marked with
sentence boundaries and speaker identification. The task
is to identify which words in the transcript are fillers, ed-
its, or fluent. The evaluation data was held out, and not
available for tuning system parameters.
The input to System A is a transcript of either con-
versational telephone speech (CTS) or broadcast news
speech (BNEWS). In all experiments, the system was
trained on reference transcripts, but was tested on both
reference and speech output transcripts.
We use a Transformation-Based Learning (TBL)
(Brill, 1995) algorithm to induce rules from the training
data. TBL is a technique for learning a set of rules that
transform an initial hypothesis for the purpose of reduc-
ing the error rate of the hypothesis. The set of possi-
ble rules is found by expanding rule templates, which are
given as an input. The algorithm greedily selects the rule
that reduces the error rate the most, applies it to the data,
and then searches for the next rule. The algorithm halts
when there are no more rules that can reduce the error
rate by more than the threshold. The output of the system
is an ordered set of rules, which can then be applied to
the test data to annotate it for disfluencies.
We allow one of three tags to be assigned to each word:
edit, filler or fluent. Since only 15% of the words in con-
versational speech are disfluent, we begin with the initial
hypothesis that all the words in the corpus are fluent. The
system then learns rules to relabel words as edits or fillers
in order to reduce the number of errors. The rules are it-
eratively applied to the data from left to right.
3.1 Feature Set
The rules learned by the system are conditioned on sev-
eral features of each of the words including the lexeme
(the word itself), a POS tag for the word, whether the
word is followed by a silence and whether the word is a
high frequency word. That is, whether the word is more
frequent for this speaker than in the rest of the corpus.
The last feature (high frequency of the word) is useful for
identifying when words that are usually fluent?but are
sometimes disfluent (such as ?like?)?are more likely to
be disfluencies, with the intuition being that if a speaker
is using the word ?like? very frequently, then it is likely
that the word is being used as a filler. The word ?like?
for example was only a disfluency 22% of the time it oc-
curred. So a rule that always tags ?like? as a disfluency
would hurt rather than help the system.2
3.2 Rule Templates
The system was given a set of 33 rule templates, which
were used to generate the set of possible rules. Not all
rule templates generated rules that were chosen by the
system. Below is a representative subset of rule templates
chosen by the system. Change the label of:
1. word X from L1 to L2.
2. word sequence X Y to L1.
3. left side of simple repeat to L1.
4. word with POS X from L1 to L2 if followed by word with
POS Y.
5. word from L1 to L2 if followed by words X Y.
6. word X with POS Y from L1 to L2.
7. A to L1 in the pattern A POS X B A, where A and B can
be any words.
8. left side of repeat with POS X in the middle to L1.
9. word with POS X from L1 to L2 if followed by silence
and followed by word with POS Y.
10. word X that is a high frequency word for the speaker from
L1 to L2.
4 Results
All of the results in this section are from training and eval-
uation on data produced by the Linguistic Data Consor-
tium (LDC) for the EARS Metadata community. There
were 491,543 tokens in the CTS training set and 189,766
tokens in the BNEWS training set. The CTS evaluation
set contained 33,670 tokens and the BNEWS evaluation
set contained 14,544 tokens.
We compare our System A to two other systems that
were designed for the same task, System B and System
C. System C was only applied to conversational speech,
so there are no results for it on broadcast news transcripts.
Our system was also given the same speech recognition
output as System C for the conversational speech condi-
tion, whereas System B used transcripts produced by a
different speech recognition system.
2We use a POS tagger (Ratnaparkhi, 1996) trained on
switchboard data with the additional tags of FP (filled pause)
and FRAG (word fragment).
System B used both prosodic cues and lexical informa-
tion to detect disfluencies. The prosodic cues were mod-
eled by a decision tree classifier, whereas the lexical in-
formation was modeled using a 4-gram language model,
separately trained for both CTS and BNEWS.
System C first inserts IPs into the text using a decision-
tree classifier based on both prosodic and lexical features
and then uses TBL. In addition to POS, System C?s fea-
ture set alo includes whether the word is commonly used
as a filler, edit, back-channel word, or is part of a short re-
peat. Turn and segment boundary flags were also used by
the system. Whereas System A only attempted to learn
three labels (filler, edit and fluent), System C attempted
to learn many subtypes of disfluencies, which were not
distinguished in the evaluation.
4.1 Lexeme Error Rate
We use Lexeme Error Rate (LER) as a measure of recog-
nition effectiveness. This measure is the same as the tra-
ditional word-error rate used in speech recognition, ex-
cept that filled pauses and fragments are not optionally
deletable. The LERs of the speech transcripts used by the
three systems were all fairly similar (about 25% for CTS
and 12% for BNEWS).
4.2 Top Rules Learned
A total of 106 rules were learned by the system for CTS?
the top 10 rules learned are:
1. Label all fluent filled pauses as fillers.
2. Label the left side of a simple repeat as an edit.
3. Label ?you know? as a filler.
4. Label fluent ?well?s with a UH part-of-speech as a filler.
5. Label fluent fragments as edits.
6. Label ?I mean? as a filler.
7. Label the left side of a simple repeat separated by a filled
pause as an edit.
8. Label the left side of a simple repeat separated by a frag-
ment as an edit.
9. Label edit filled pauses as fillers.
10. Label edit fragments at end of sentence as fluent.
Of the errors that system was able to fix in the CTS train-
ing data, the top 5 rules were responsible for correcting
86%, the top ten rules, for 94% and the top twenty, for
96%.
All systems were evaluated using rteval (Rich Tran-
scription Evaluation) version 2.3 (Kubala and Srivastava,
2003). Rteval aligns the system output to the annotated
reference transcripts in such a way as to minimize the lex-
eme error rate. The error rate is the number of disfluency
errors (insertions and deletions) divided by the number of
disfluent tokens in the reference transcript. Edit and filler
errors are calculated separately. The results of the evalu-
ation are shown in Table 1. Most of the small differences
in the CTS results were not found to be significantly dif-
ferent.
Data System Edit Err Filler Err
CTS Reference A 68.0% 18.1%
B 59.0% 18.2%
C 75.1% 23.2%
CTS Speech A 87.9% 48.8%
B 87.5% 46.9%
C 88.5% 51.0%
BNews Reference A 45.3% 6.5%
B 44.2% 7.9%
BNews Speech A 93.9% 57.2%
B 96.1% 50.4%
Table 1: Disfluency Detection Results
5 Error Analysis
It is clear from the discrepancies between the reference
and speech condition that a large portion of the errors (a
majority except in the case of edit detection for CTS) are
due to errors in the STT (Speech-To-Text). This is most
notable for fillers in broadcast news where the error rate
for our system increases from 6.5% to 57.2%. Such a
trend can be seen for the other systems, indicating that?
even with prosodic models?the other systems were not
more robust to the lexical errors.
All three systems produced comparable results on all
of the conditions, with the only large exception being edit
detection for CTS Reference, where System B had an er-
ror rate of 59% compared to our system?s error rate of
68%.3
The speech output condition suffers from several types
of errors due to errors in the transcript produced by the
speech transcription system. First, the system can output
the wrong word causing it to be misannotated. 27% of our
edit errors in CTS and 19% of our filler errors occurred
when the STT system misrecognized the word. If a filled
pause is hallucinated, the disfluency detection system will
always annotate it as a filler. Errors also occur (19% of
our edit and 12% of our filler error) when the recognizer
deletes a word that was an edit or a filler. Finally, errors
in the context words surrounding disfluencies can affect
disfluency detection as well.
One possible method to correct for the STT errors
would be to train our system on speech output from the
recognizer rather than on reference transcripts. Another
option would be to use a word recognition confidence
score from the recognizer as a feature in the TBL sys-
tem; these were not used. A more systematic analysis
of the errors caused by the recognizer and their effect on
disfluencies also needs to be performed.
System A has a much higher error for edits than fillers,
due, in large part, to the presence of long, difficult to
3This is possibly due to the prosodic model employed by
System B, though no significant gain was shown for the other
conditions.
detect edits. Consider the following word sequence: ?[
and whenever they come out with a warning ] you know
they were coming out with a warning about trains ?. The
portion within square brackets is the edit to be detected.
The difficulty in finding such regions is that the edit itself
appears very fluent. One can identify these regions by
examining what comes after the edit and finding that is
highly similar in content to the edit region. Prosodic fea-
tures can be useful in identifying the interruption point
at which the edit ends, but the degree to which the edit
extends backwards from this point still needs to be iden-
tified. Long distance dependencies should reveal the edit
region, and it is possible that parsing or semantic analysis
of the text would be a useful technique to employ. In ad-
dition there are other cues such as the filler ?you know?
after the edit which can be used to locate these edit re-
gions. Long edit regions (of length four or more) are re-
sponsible for 48% of the edit errors in the CTS reference
condition for our system.
6 Conclusions and Future Work
We have presented a TBL approach to detecting disfluen-
cies that uses primarily lexical features. Our system per-
formed comparably with other systems that relied on both
prosodic and lexical features. Our speaker style (high fre-
quency word) feature enabled us to detect rarer disfluen-
cies, although this was not a large factor in our perfor-
mance. It does appear to be a promising technique for
future research however.
The technique described here shows promise for ex-
tension to disfluency detection in other languages. Since
TBL is a weakly statistical technique, it does not require
a large training corpus and could be more rapidly applied
to new languages. Assuming the basic forms of disflu-
encies in other languages are similar to those in English,
very few modifications would be required.
The longer edits that the system currently misses may
be detectable using parsing, with the intuition that a
parser trained on fluent speech may perform poorly in the
presence of longer edits. Techniques using parse trees to
identify disfluencies have shown success in the past (Hin-
dle, 1983). The system could use portions of the parse
structure as features and could relabel entire subtrees of
the parse tree. Repeated words are another feature of the
longer edits, which we might leverage off of by perform-
ing a weighted alignment of the edit and the repair. Even-
tually it may prove that more elaborate acoustic cues will
be needed to identify these edits, at which point a model
of interruption points could be included as a feature in the
rules learned by the system.
References
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eugene Charniak and Mark Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings of the
NAACL.
Mark G. Core and Lenhart K. Schubert. 1999. A model of
speech repairs and other disruptions. In Susan E. Brennan,
Alain Giboin, and David Traum, editors, Working Papers of
the AAAI Fall Symposium on Psychological Models of Com-
munication in Collaborative Systems, pages 48?53, Menlo
Park, California. AAAI.
Peter Heeman and James Allen. 1999. Speech repairs, into-
national phrases, and discourse marker: Modeling speakers?
utterances in spoken dialogue. Computational Linguistics,
25(4).
Donald Hindle. 1983. Deterministic parsing of syntactic non-
fluencies. In Proceedings of ACL, pages 123?128.
Douglas Jones, Florian Wolf, Edward Gibson, Elliott Williams,
Evelina Fedorenko, Douglas Reynolds, and Marc Zissman.
2003. Measuring the readability of automatic speech-to-text
transcripts. In Proceedings of Eurospeech, Geneva.
Francis Kubala and Amit Srivastava. 2003. A Framework for
Evaluating Rich Transcription Technology. BBN Ears Web-
site. http://www.speech.bbn.com/ears.
Yang Liu, Elizabeth Shriberg, and Andreas Stolcke. 2003.
Automatic disfluency identification in coversational speech
using multiple knowledge sources. In Proceedings of Eu-
rospeech, Geneva.
Christine Nakatani and Julia Hirschberg. 1994. A corpus-based
study of repair cue in spontaneous speech. Journal of the
Acoustical Society of America, 95(3):160?1616.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In ACL-SIGDAT Proceedings of the
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 133?142, Philadelphia, PA.
Elizabeth Shriberg, Andreas Stolcke, and Dan Baron. 2001.
Can prosody aid the automatic processing of multi-party
meetings? evidence from predicting punctuation, disfluen-
cies, and overlapping speech. In Proceedings of ISCA Tuto-
rial and Research Workshop on Prosody in Speech Recogni-
tion and Understanding, pages 139?146, Red Bank, NJ.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Mari Os-
tendorf, Dilek Hakkani, Madelaine Plauche, Gokhan Tur,
and Yu Lu. 1998. Automatic detection of sentence bound-
aries and disfluencies based on recognized words. In Pro-
ceedings of the ICSLP, volume 5, pages 2247?2250, Sydney,
Australia.
Stephanie Strassel. 2003. Guidelines for RT-03 Transcription
? Version 2.2. Linguistic Data Consortium, Universitry of
Pennsylvannia.
Charles Wayne. 2003. Effective, Affordable, Reusable Speech-
to-Text (EARS). Official web site for DARPA/EARS Pro-
gram. http://www.darpa.muk/iao/EARS.htm.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 880?885,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Morphological Segmentation for Keyword Spotting
Karthik Narasimhan
1
, Damianos Karakos
2
, Richard Schwartz
2
, Stavros Tsakalidis
2
,
Regina Barzilay
1
1
Computer Science and Artificial Intelligence Laboratory,
Massachusetts Institute of Technology
2
Raytheon BBN Technologies
{karthikn, regina}@csail.mit.edu
{dkarakos, schwartz, stavros}@bbn.com
Abstract
We explore the impact of morpholog-
ical segmentation on keyword spotting
(KWS). Despite potential benefits, state-
of-the-art KWS systems do not use mor-
phological information. In this paper,
we augment a state-of-the-art KWS sys-
tem with sub-word units derived from su-
pervised and unsupervised morphological
segmentations, and compare with phonetic
and syllabic segmentations. Our exper-
iments demonstrate that morphemes im-
prove overall performance of KWS sys-
tems. Syllabic units, however, rival the
performance of morphological units when
used in KWS. By combining morphologi-
cal, phonetic and syllabic segmentations,
we demonstrate substantial performance
gains.
1 Introduction
Morphological analysis plays an increasingly im-
portant role in many language processing appli-
cations. Recent research has demonstrated that
adding information about word structure increases
the quality of translation systems and alleviates
sparsity in language modeling (Chahuneau et al.,
2013b; Habash, 2008; Kirchhoff et al., 2006; Stal-
lard et al., 2012).
In this paper, we study the impact of morpho-
logical analysis on the keyword spotting (KWS)
task. The aim of KWS is to find instances of a
given keyword in a corpus of speech data. The
task is particularly challenging for morphologi-
cally rich languages as many target keywords are
unseen in the training data. For instance, in the
Turkish dataset (Babel, 2013) we use, from the
2013 IARPA Babel evaluations, 36.06% of the test
words are unseen in the training data. However,
81.44% of these unseen words have a morpholog-
ical variant in the training data. Similar patterns
are observed in other languages used in the Babel
evaluations. This observation strongly supports
the use of morphological analysis to handle out-
of-vocabulary (OOV) words in KWS systems.
Despite this potential promise, state-of-the-art
KWS systems do not commonly use morphologi-
cal information. This surprising fact can be due to
multiple reasons, ranging from the accuracy of ex-
isting morphological analyzers to the challenge of
integrating morphological information into exist-
ing KWS architectures. While using morphemes
is likely to increase coverage, it makes recogni-
tion harder due to the inherent ambiguity in the
recognition of smaller units. Moreover, it is not
clear a priori that morphemes, which are based on
the semantics of written language, are appropriate
segmentation units for a speech-based application.
We investigate the above hypotheses in the
context of a state-of-the-art KWS architec-
ture (Karakos et al., 2013). We augment word
lattices with smaller units obtained via segmenta-
tion of words, and use these modified lattices for
keyword spotting. We consider multiple segmen-
tation algorithms, ranging from near-perfect su-
pervised segmentations to random segmentations,
along with unsupervised segmentations and purely
phonetic and syllabic segmentations. Our exper-
iments show how sub-word units can be used ef-
fectively to improve the performance of KWS sys-
tems. Further, we study the extent of impact of the
subwords, and the manner in which they can be
used in KWS systems.
2 Related Work
Prior research on applications of morphological
analyzers has focused on machine translation, lan-
guage modeling and speech recognition (Habash,
2008; Chahuneau et al., 2013a; Kirchhoff et al.,
2006). Morphological analysis enables us to link
together multiple inflections of the same root,
thereby alleviating word sparsity common in mor-
880
phologically rich languages. This results in im-
proved language model perplexity, better word
alignments and higher BLEU scores.
Recent work has demonstrated that even mor-
phological analyzers that use little or no supervi-
sion can help improve performance in language
modeling and machine translation (Chahuneau et
al., 2013b; Stallard et al., 2012). It has also been
shown that segmentation lattices improve the qual-
ity of machine translation systems (Dyer, 2009).
In this work, we leverage morphological seg-
mentation to reduce OOV rates in KWS. We in-
vestigate segmentations produced by a range of
models, including acoustic sub-word units. We in-
corporate these subword units into a lattice frame-
work within the KWS system. We also demon-
strate the value of using alternative segmentations
instead of or in combination with morphemes. In
addition to improving the performance of KWS
systems, this finding may also benefit other appli-
cations that currently use morphological segmen-
tation for OOV reduction.
3 Segmentation Methods
Supervised Morphological Segmentation Due
to the unavailability of gold morphological seg-
mentations for our corpus (Babel, 2013), we use
a resource-rich supervised system as a proxy. As
training data for this system, we use the Mor-
phoChallenge 2010 corpus
1
which consists of
1760 gold segmentations for Turkish.
We consider two supervised frameworks, both
made up of two stages. In the first stage, com-
mon to both systems, we use a FST-based mor-
phological parser (C??oltekin, 2010) that generates a
set of candidate segmentations, leveraging a large
database of Turkish roots and affixes. This stage
tends to overgenerate, segmenting each word in
eight different ways on average. In the next stage,
we filter the resulting segmentations using one of
two supervised filters (described below) trained on
the MorphoChallenge corpus.
In the first approach, we use a binary log-linear
classifier to accept/reject each segmentation hy-
pothesis. For each word, this classifier may ac-
cept multiple segmentations, or rule out all the al-
ternatives. In the second approach, to control the
number of segmentations per word, we train a log-
linear ranker that orders the segmentations for a
word in decreasing order of likelihood. In our
1
http://research.ics.aalto.fi/events/morphochallenge2010/
Feature Example
morpheme unigrams tak, acak
morpheme bigram ?tak, acak?
phonetic seq. unigrams t.a.k., 1v.dZ.a.k.
phonetic seq. bigram ?t.a.k., 1v.dZ.a.k.?
number of morphemes 2
morpheme lengths 3, 4
Table 1: Example of features used in the super-
vised filters for the segmentation tak-acak. Each
phone is followed by a dot for clarity.
training corpus, each word has on average 2.5 gold
segmentations. Hence, we choose the top two seg-
mentations per word from the output of the ranker
to use in our KWS system. In both filters, we
use several features like morpheme unigrams, bi-
grams, lengths, number of morphemes, and phone
sequences corresponding to the morphemes.
In our supervised systems, we can encode fea-
tures that go beyond individual boundaries, like
the total number of morphemes in the segmenta-
tion. This global view distinguishes our classi-
fier/ranker from traditional approaches that model
segmentation as a sequence tagging task (Ruoko-
lainen et al., 2013; Kudo et al., 2004; Kru-
engkrai et al., 2006). Another departure of our
approach is the use of phonetic information, in
the form of phonetic sequences corresponding to
the morpheme unigrams and bigrams. The hy-
pothesis is that syllabic boundaries are correlated
with morpheme boundaries to some extent. The
phonetic sequences for words are obtained using
a publicly available Text-to-Phone (T2P) system
(Lenzo, 1998).
Unsupervised Morphological Segmentation
We employ a widely-used unsupervised sys-
tem Morfessor (Creutz and Lagus, 2005) which
achieves state-of-the-art unsupervised perfor-
mance in the MorphoChallenge evaluation. Mor-
fessor uses probabilistic generative models with
sparse priors which are motivated by the Minimum
Description Length (MDL) principle. The system
derives segmentations from raw data, without re-
liance on extra linguistic sources. It outputs a sin-
gle segmentation per word.
Random Segmentation As a baseline, we in-
clude sub-word units from random segmentations,
where we mark a segmentation boundary at each
character position in a word with a fixed probabil-
ity p. For comparison purposes, we consider two
881
Sub-word units Example
Morphemes tak - acak
Random t - aka - c - a - k
Phones t - a - k - 1v - dZ - a - k
Syllables ta - k1v - dZak
Table 2: Segmentations of the word takacak into
different types of sub-word units.
types of random segmentations that match the su-
pervised morphological segmentations in terms of
the number of uniques morphemes and the average
morpheme length, respectively. These segmenta-
tions are obtained by adjusting the segmentation
probability p appropriately.
Phones and Syllables In addition to letter-
based segmentation, we also consider other sub-
word units that stem from word acoustics. In par-
ticular, we consider segmentation using phones
and syllables, which are available for the Babel
data we work with.
Table 2 shows examples of different segmenta-
tions for the Turkish word takacak.
4 Keyword Spotting
The keyword spotting system used in this work
follows, to a large extent, the pipeline of (Bulyko
et al., 2012). Using standard speech recognition
machinery, the system produces a detailed lattice
of word hypotheses. The resulting lattice is used to
extract keyword hits with nominal posterior prob-
ability scores.
We modify this basic architecture in two ways.
First, we use subwords instead of whole-words in
the decoding lexicon. Second, we represent key-
words using all possible paths in a lattice of sub-
words. For each sequence of matching arcs in the
lattice, the posteriors of these arcs are multiplied
together to form the score of detection (hit). A
post-processing step adds up (or takes the max of)
the scores of all hits of each keyword which have
significant overlap in time. Finally, the hit lists are
processed by the score normalization and combi-
nation method described in (Karakos et al., 2013).
We use whole-word extraction for words in vo-
cabulary, but rely on subword models for OOV
words. Since we combine the hits separately for
IV and OOV keywords, using subwords can only
improve the performance of the overall system.
Language Dev Set Eval Set
Turkish 403 226
Assamese 158 563
Bengali 176 629
Haitian 107 319
Lao 110 194
Tamil 238 700
Zulu 323 1251
Table 3: Number of OOV keywords in the differ-
ent Dev and Eval sets.
5 Experimental Setup
Data The segmentation algorithms described in
Section 3 are tested using the setup of the KWS
system described in Section 4. Our experiments
are conducted using the IARPA Babel Program
language collections for Turkish, Assamese, Ben-
gali, Haitian, Lao, Tamil and Zulu (Babel, 2013)
2
.
The dataset contains audio corpora and a set of
keywords. The training corpus for KWS consists
of 10 hours of speech, while the development and
test sets have durations of 10 and 5 hours, respec-
tively. We evaluate KWS performance over the
OOV keywords in the data, which are unseen in
the training set, but appear in the development/test
set. Table 3 contains statistics on the number of
OOV keywords in the data for each language.
In our experiments, we consider the pre-indexed
condition, where the keywords are known only af-
ter the decoding of the speech has taken place.
Evaluation Measures We consider two differ-
ent evaluation metrics. To evaluate the accuracy
of the different segmentations, we compare them
against gold segmentations from the MorphoChal-
lenge data for Turkish. This set consists of 1760
words, which are manually segmented. We use
a measure of word accuracy (WordAcc), which
captures the accuracy of all segmentation deci-
sions within the word. If one of the segmenta-
tion boundaries is wrong in a proposed segmen-
tation, then that segmentation does not contribute
towards the WordAcc score. We use 10-fold cross-
validation for the supervised segmentations, while
we use the entire set for unsupervised and acoustic
cases.
We evaluate the performance of our KWS sys-
tem using a widely used metric in KWS, the Ac-
2
We perform the experiments with supervised segmenta-
tion only on Turkish, due to the lack of gold morphological
data for the other languages.
882
tual Term Weighted Value (ATWV) measure, as
described in (Fiscus et al., 2007). This measure
uses a combination of penalties for misses and
false positives to score the system. The maximum
score achievable is 1.0, if there are no misses and
false positives, while the score can be lower than
0.0 if there are a lot of misses or false positives.
6 Results
Table 4 summarizes the performance of all con-
sidered segmentation systems in the KWS task on
Turkish. The quality of the segmentations com-
pared to the gold standard is also shown. Table 5
shows the OOV ATWV performance on the six
other languages, used in the second year of the
IARPA Babel project. We summarize below our
conclusions based on these results.
Using sub-word units improves overall KWS
performance If we use a word-based KWS sys-
tem, the ATWV score will be 0.0 since the OOV
keywords are not present in the lexicon. En-
riching our KWS system with sub-word segments
yields performance gains for all the segmentation
methods, including random segmentations. How-
ever, the observed gain exhibits significant vari-
ance across the segmentation methods. For in-
stance, the gap between the performance of the
KWS system using the best supervised classifier-
based segmenter (CP) and that using the unsuper-
vised segmenter (U) is 0.059, which corresponds
to a 43.7% in relative gain. Table 4 also shows that
while methods with shorter sub-units (U, P) yield
lower OOV rate, they do not necessarily fare better
in the KWS evaluation.
Syllabic units rival the performance of mor-
phological units A surprising discovery from our
experiments is the good performance of the syl-
labic segmentation-based KWS system (S). It out-
performs all the alternative segmentations on the
test set, and ranks second on the development set
behind the CP system. These units are particularly
attractive as they can easily be computed from
acoustic input and do not require any prior linguis-
tic knowledge. We hypothesize that the granular-
ity of this segmentation is crucial to its success.
For instance, a finer-grained phone-based segmen-
tation (P) performs substantially worse than other
segmentation algorithms as the derived sub-units
are shorter and hence, harder to recognize.
Improving morphological accuracy beyond a
certain level does not translate into improved
KWS performance We observe that the segmen-
tation accuracy and KWS performance are not
positively correlated. Clearly, bad segmentations
translate into poor ATWV scores, as in the case
of random and unsupervised segmentations. How-
ever, gains on segmentation accuracy do not al-
ways result in better KWS performance. For in-
stance, the ranker systems (RP, RNP) have better
accuracies on MC2010, while the classifier sys-
tems (CP, CNP) perform better on the KWS task.
This discrepancy in performance suggests that fur-
ther gains can be obtained by optimizing segmen-
tations directly with respect to KWS metrics.
Adding phonetic information improves mor-
phological segmentation For all the morpholog-
ical systems, adding phonetic information results
in consistent performance gains. For instance,
it increases segmentation accuracy by 4% when
added to the classifier (CNP and CP in table 4).
The phonetic information used in our experiments
is computed automatically using a T2P system
(Lenzo, 1998), and can be easily obtained for a
range of languages. This finding sheds new light
on the relation between phonetic and morphologi-
cal systems, and can be beneficial for morpholog-
ical analyzers developed for other applications.
Combining morphological, phonetic and syl-
labic segmentations gives better results than ei-
ther in isolation As table 4 shows, the best KWS
results are achieved when syllabic and morphemic
systems are combined. The best combination sys-
tem (CP+P+S) outperforms the best individual
system (S) by 5.5%. This result suggests that mor-
phemic, phonemic and syllabic segmentations en-
code complementary information which benefits
KWS systems in handling OOV keywords.
Morphological segmentation helps KWS
across different languages Table 5 demonstrates
that we can obtain gains in KWS performance
across different languages using unsupervised seg-
mentation. The improvement is significant in 3 of
the 6 languages - as high as 3.2% for Assamese
and Bengali, and 2.7% for Tamil (absolute per-
centages). As such, the results of Table 2 can-
not be directly compared to those of Table 1 since
the system architecture is slightly different
3
. How-
3
The keyword spotting pipeline is based on the one used
by the Babelon team in the 2014 NIST evaluation (Tsakalidis,
2014). The pipeline was much more involved than the one de-
scribed for Turkish; multiple search methods (with/without
fuzzy search) and data structures (lattices, confusion net-
works and generalized versions of these) were all used in
combination (Karakos and Schwartz, 2014). The recognition
883
Method
Unique
units
Avg. unit
length
Reduction
in OOV (abs)
WordAcc
Dev
ATWV
Test
ATWV
Phone-based (P) 51 1 36.06% 0.06% 0.099 0.164
Syllable-based (S) 2.1k 3.62 23.91% 10.29% 0.127 0.201
Classifier w/ phone info (CP) 18.5k 6.39 18.20% 80.41% 0.146 0.194
Classifier w/o phone info (CNP) 19k 6.42 21.50% 75.66% 0.133 0.181
Ranker w/ phone info (RP) 10k 5.62 16.86% 86.03% 0.104 0.153
Ranker w/o phone info (RNP) 10k 5.71 16.44% 84.19% 0.109 0.159
Unsupervised (U) 2.4k 5.44 22.45% 39.57% 0.080 0.135
RANDLen-Classifier 11.7k 6.39 0.73% 5.11% 0.061 0.086
RANDNum-Classifier 18.2k 3.03 8.56% 3.69% 0.111 0.154
RANDLen-Ranker 11.6k 5.62 1.94% 5.79% 0.072 0.136
RANDNum-Ranker 11.7k 6.13 1.15% 5.34% 0.081 0.116
CP + P - - - - 0.190 0.246
RP + P - - - - 0.150 0.210
CP + P + S - - - - 0.208 0.257
RP + P + S - - - - 0.186 0.249
Word-based for IV words - - - - 0.385 0.400
Table 4: Segmentation Statistics and ATWV scores on Babel Turkish data along with WordAcc on
MorphoChallenge 2010 data. All rows except the last are for OOV words. Absolute reduction is from an
initial OOV of 36.06%. Higher ATWV scores are better. Best system scores are shown in bold.
Assamese Bengali Haitian Lao Tamil Zulu
Dev Test Dev Test Dev Test Dev Test Dev Test Dev Test
P + S 0.213 0.230 0.277 0.296 0.371 0.342 0.228 0.139 0.349 0.267 0.279 0.215
P + S + U 0.214 0.263 0.294 0.328 0.393 0.342 0.237 0.146 0.395 0.284 0.275 0.218
Table 5: ATWV scores for languages used in the second year of the IARPA Babel project, using two
KWS systems: Phone + Syllable (P+S) and Phone + Syllable + Unsupervised Morphemes (P+S+U).
Bold numbers show significant performance gains obtained by adding morphemes to the system.
ever, they are indicative of the large gains (1.5%,
on average, over the six languages) that can be ob-
tained through unsupervised morphology, on top
of a very good combined phonetic/syllabic system.
7 Conclusion
We explore the extent of impact of morphological
segmentation on keyword spotting (KWS). To in-
vestigate this issue, we augmented a KWS system
with sub-word units derived by multiple segmen-
tation algorithms. Our experiments demonstrate
that morphemes improve the overall performance
of KWS systems. Syllabic units, however, rival the
performance of morphemes in the KWS task. Fur-
thermore, we demonstrate that substantial perfor-
mance gains in KWS performance are obtained by
combining morphological, phonetic and syllabic
was done with audio features supplied by BUT (Karafi?at et
al., 2014), which were improved versions of those used for
Turkish.
segmentations. Finally, we also show that adding
phonetic information improves the quality of mor-
phological segmentation.
Acknowledgements
This work was supported by the Intelligence
Advanced Research Projects Activity (IARPA)
via Department of Defense US Army Research
Laboratory contract number W911NF-12-C-0013.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoD/ARL, or the U.S. Govern-
ment. We thank the MIT NLP group and the
EMNLP reviewers for their comments and sugges-
tions.
884
References
IARPA Babel. 2013. Language collection re-
leases; Turkish: IARPA-babel105b-v0.4, As-
samese: IARPA-babel102b-v0.5a, Bengali: IARPA-
babel103b-0.4b, Haitian Creole: IARPA-babel201b-
v0.2b, Lao: IARPA-babel203b-v3.1a, Tamil:
IARPA-babel204b-v1.1b, Zulu: IARPA-babel206b-
v0.1e.
Ivan Bulyko, Owen Kimball, Man-Hung Siu, Jos?e Her-
rero, and Dan Blum. 2012. Detection of un-
seen words in conversational Mandarin. In Proc. of
ICASSP, Kyoto, Japan, Mar.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013a. Translating into morpholog-
ically rich languages with synthetic phrases. In
EMNLP, pages 1677?1687. ACL.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2013b. Knowledge-rich morphological priors for
bayesian language models. In HLT-NAACL, pages
1206?1215. The Association for Computational Lin-
guistics.
C?a?gr? C??oltekin. 2010. A freely available morpho-
logical analyzer for Turkish. In Proceedings of
the 7th International conference on Language Re-
sources and Evaluation (LREC2010), pages 820?
827.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of the Interna-
tional and Interdisciplinary Conference on Adaptive
Knowledge Representation and Reasoning (AKRR),
pages 106?113.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, NAACL ?09, pages 406?414, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jonathan G. Fiscus, Jerome Ajot, John S. Garofolo, and
George Doddington. 2007. Results of the 2006
spoken term detection evaluation. In Workshop on
Searching Spontaneous Conversational Speech.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, HLT-Short ?08, pages 57?60,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Martin Karafi?at, Franti?sek Gr?ezl, Mirko Hanne-
mann, Karel Vesel?y, Igor Szoke, and Jan ?Honza?
?
Cernock?y. 2014. BUT 2014 Babel system: Anal-
ysis of adaptation in NN based systems. In Pro-
ceedings of Interspeech 2014, Singapore, Septem-
ber. IEEE.
Damianos Karakos and Richard Schwartz. 2014. Sub-
word modeling. In IARPA Babel PI Meeting, July.
Damianos Karakos, Richard Schwartz, Stavros Tsaka-
lidis, Le Zhang, Shivesh Ranjan, Tim Ng, Roger
Hsiao, Guruprasad Saikumar, Ivan Bulyko, Long
Nguyen, John Makhoul, Frantisek Grezl, Mirko
Hannemann, Martin Karafiat, Igor Szoke, Karel
Vesely, Lori Lamel, and Viet-Bac Le. 2013. Score
normalization and system combination for improved
keyword spotting. In Proc. ASRU 2013, Olomouc,
Czech Republic.
Katrin Kirchhoff, Dimitra Vergyri, Jeff Bilmes, Kevin
Duh, and Andreas Stolcke. 2006. Morphology-
based language modeling for conversational arabic
speech recognition. Computer Speech and Lan-
guage, 20(4):589?608.
Canasai Kruengkrai, Virach Sornlertlamvanich, and
Hitoshi Isahara. 2006. A conditional random field
framework for Thai morphological analysis. In
LREC.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In In Proc. of
EMNLP, pages 230?237.
Kevin Lenzo. 1998. Text-to-phoneme converter
builder. http://www.cs.cmu.edu/afs/cs.
cmu.edu/user/lenzo/html/areas/t2p/.
Accessed: 2014-03-11.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning, pages 29?37, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012. Unsu-
pervised morphology rivals supervised morphology
for Arabic MT. In ACL (2), pages 322?327. The
Association for Computer Linguistics.
Stavros Tsakalidis. 2014. The Babelon OpenKWS14
systems. In IARPA Babel PI Meeting, July.
885
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation of Arabic Dialects
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan?, Chris Callison-Burch?
Raytheon BBN Technologies, Cambridge MA
?Microsoft Research, Redmond WA
?Johns Hopkins University, Baltimore MD
Abstract
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon?s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
1 Introduction
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
? We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon?s
Mechanical Turk crowdsourcing service (?3).
? We use the data to perform a variety of machine
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (?4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
49
2 Previous Work
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al, 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon?s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
3 Data Collection and Annotation
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
50
M
ag
hr
eb
i
E
gy
Ir
aq
i
G
ul
f
Ot
he
r
L
ev
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
3.1 Dialect Classification
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional ?General? dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
3.2 Sentence Segmentation
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to ?divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.? We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
3.3 Translation to English
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
51
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker?s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker?s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker?s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access ?preferred worker queue?. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, . . . , 4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word ? an order of magnitude cheaper than
professional translation.
4 Experiments in Dialectal Arabic-English
Machine Translation
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
52
Simple Segment MADA Segment
Training Tuning BLEU OOV BLEU OOV ?BLEU ?OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
4.1 Morphological Decomposition
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
3We also computed TER (Snover et al, 2006) andMETEOR
scores, but omit them because they demonstrated similar trends.
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
4.2 Effect of Dialectal Training Data Size
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
53
oh
 ti
me
 (s
pa
ce
 om
itt
ed
). 
Ap
pe
are
d w
ith
in
 a 
po
em
.
11
yA
zm
n

?
lik
e y
ou
 (c
or
ru
pti
on
 of
 M
SA
 m
vl
k)
.
10
m
tlk
"#
$
by
 m
uc
h (
co
rru
pti
on
 of
 M
SA
 bk
vy
r).
11
bk
ty
r
&'$
()
I m
iss
 yo
u (
sp
ok
en
 to
 a 
fe
ma
le)
 ?
Eg
yp
tia
n.
14
w
H
$t
yn
y
/0
'$1
2?
Th
e l
as
t n
am
e (
Al
-N
a'o
om
) o
f a
 fo
ru
m 
ad
mi
n.
16
A
ln
E
w
m
?:;
0<?
a l
oo
ot 
(c
or
ru
pti
on
 of
 M
SA
 kv
yr
A
).
17
kt
yy
yr
&''
'$?
rea
lly
/fo
r r
ea
l ?
Le
va
nti
ne
.
31
E
nj
d
DE
0F
En
gli
sh
 E
qu
iva
len
t
Co
un
t
TL
Ar
ab
ic
Table 5: The most frequent OOV?s (with counts ? 10) of the dialectal test sets against the MSA training data.
Source (EGY):  ? ? ??	
?   ? ! !
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declarationand not?
Dial-Sys. Output: You are making the advertisementfor him or what?
Reference: Are you promoting it or what?!!
Source (EGY):  01?. ??78 6 35 34? ?
 9:;? <=>
Transliteration: nfsY Atm}n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him afterhe saw this picture.
Reference: I wish to be sure that he is fineafter he saw this images
Source (LEV):  ?0??? E7770 ?F? G7H
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEV):  ?L M
 G3 0?;
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
Source (EGY):   	
 	  ? 
Transliteration: qAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEV):  "#$%& 
#'01 ?-%. ! -,%+? ?? ?2 
Transliteration: fbqrA w>HyAnA bqDyhA Em
>tslY mE rfqAty
MSA-Sys. Output: I read and sometimes with gowith my uncle.
Dial-Sys. Output: So I read, and sometimes I spendtrying to make my self comfortwith my friends
Reference: So i study and sometimes I spendthe time having fun with my friends
Source (LEV):  ?@ ?< ??' => +? &#:9? B:C12D E?
?? %$?+G 
Transliteration: Allh ysAmHkn hlq kl wAHd TAlb
qrb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is aclose student would want the bride
Reference: God forgive you. Is every oneasking to be close, want a bride!
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
54
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
%
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Egyptian web test
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Levantine web test
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8?1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOVwords
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
4.3 Cross-Dialect Training
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
55
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
4.4 Validation on Independent Test Data
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A lowOOV rate also indicated the correctness
of the mappings. By manually transforming the test
56
Training BLEU OOV BLEU OOV ?BLEU ?OOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
5 Conclusion
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a ?pivot language? for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
57
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
References
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. of ACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325?
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ?04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223?231, Cambridge, MA.
58
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
59
Proceedings of NAACL-HLT 2013, pages 612?616,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Systematic Comparison of Professional and Crowdsourced Reference
Translations for Machine Translation
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, John Makhoul
Raytheon BBN Technologies
Cambridge, MA 02138, USA
{rzbib,gmarkiew,smatsouk,schwartz,makhoul}@bbn.com
Abstract
We present a systematic study of the effect of
crowdsourced translations on Machine Trans-
lation performance. We compare Machine
Translation systems trained on the same data
but with translations obtained using Amazon?s
Mechanical Turk vs. professional translations,
and show that the same performance is ob-
tained from Mechanical Turk translations at
1/5th the cost. We also show that adding a Me-
chanical Turk reference translation of the de-
velopment set improves parameter tuning and
output evaluation.
1 Introduction
Online crowdsourcing services have been shown to
be a cheap and effective data annotation resource
for various Natural Language Processing (NLP)
tasks (Callison-Burch and Dredze, 2010; Zaidan and
Callison-Burch, 2011a; Zaidan and Callison-Burch,
2011b). The resulting quality of annotations is high
enough to be used for training statistical NLP mod-
els, with a saving in cost and time of up to an or-
der of magnitude. Statistical Machine Translation
(SMT) is one of the NLP tasks that can benefit from
crowdsourced annotations. With appropriate quality
control mechanisms, reference translations collected
by crowdsourcing have been successfully used for
training and evaluating SMT systems (Zbib et al,
2012; Zaidan and Callison-Burch, 2011b).
In this work, we used Amazon?s Mechanical Turk
(MTurk) to obtain alternative reference translations
of four Arabic-English parallel corpora previously
released by the Linguistic Data Consortium (LDC)
for the DARPA BOLT program. This data, totaling
over 500K Arabic tokens, was originally collected
from web discussion forums and translated profes-
sionally to English. We used alternative MTurk
translations of the same data to train and evalua-
tion MT systems; and conducted the first systematic
study that quantifies the effect of the reference trans-
lation process on MT output. We found that:
? Mechanical Turk can be used to translate
enough data for training an MT system at
1/10th the price of professional translation, and
at a much faster rate.
? Training MT systems on MTurk reference
translations gives the same performance as
training with professional translations at 20%
of the cost.
? A second translation of the development set ob-
tained via MTurk improves parameter tuning
and output evaluation.
2 Previous Work
There have been several publications on crowd-
sourcing data annotation for NLP. Callison-Burch
and Dredze (2010) give an overview of the NAACL-
2010 Workshop on using Mechanical Turk for data
annotation. They describe tasks for which MTurk
can be used, and summarize a set of best practices.
They also include references to the workshop con-
tributions.
Zaidan and Callison-Burch (2011a) created a
monolingual Arabic data set rich in dialectal con-
tent from user commentaries on newspaper web-
sites. They hired native Arabic speakers on MTurk
612
to identify the dialect level and used the collected la-
bels to train automatic dialect identification systems.
They did not translate the collected data, however.
Zaidan and Callison-Burch (2011b) obtained mul-
tiple translations of the NIST 2009 Urdu-English
evaluation set using MTurk. They trained a statis-
tical model on a set of features to select among the
multiple translations. They showed that the MTurk
translations selected by their model approached the
range of quality of professional translations, and that
the selected MTurk translations can be used reliably
to score the outputs of different MT systems submit-
ted to the NIST evaluation. Unlike our work, they
did not investigate the use of crowdsourced trans-
lations for training or parameter tuning. Zbib et al
(2012) trained a Dialectal Arabic to English MT sys-
tem using Mechanical Turk translations. But the
data they translated on MTurk does not have profes-
sional translations to conduct the systematic com-
parison we do in this paper.
It is well known that scoring MT output against
multiple references improves MT scores such as
BLEU significantly, since it increases the chance of
matching n-grams between the MT output and the
references. Tuning system parameter with multi-
ple references also improves machine translation for
the same reason Madnani et al (2007) and Madnani
et al (2008) showed that tuning on additional ref-
erences obtained by automatic paraphrasing helps
when only few tuning references are available.
3 Data Translation
The data we used are Arabic-English parallel cor-
pora released by the LDC for the DARPA BOLT
Phase 1 program1. The data was collected from
Egyptian online discussion forums, and consists of
separate discussion threads, each composed of an
initial user posting and multiple reply postings. The
data tends to be bimodal: the first posting in the
thread is often formal and expressed in Modern
Standard Arabic, while the subsequent threads use
a less formal style, and contain colloquial Egyptian
dialect. The data was manually segmented into sen-
tence units, and translated professionally.
We used non-professional translators hired on
MTurk to get second translations. We used several
1Corpora: LDC2012E15, LDC2012E19, LDC2012E55
measures to control the quality of translations and
detect cheaters. Those include the rendering of Ara-
bic sentences as images, comparing the output to
Google Translate and Bing Translator, and other au-
tomatic checks. The quality of individual worker?s
translations was quantified by asking a native Ara-
bic speaker judge to score a sample of the Turker?s
translations. The translation task unit (aka Human
Intelligence Task or HIT) consisted of a sequence
of contiguous sentences from a discussion thread
amounting to between 40 and 60 words. The in-
structions were simply to translate the Arabic source
fully and accurately, and to take surrounding sen-
tence segments into account to help resolve ambigu-
ities. The HIT rewards were set to 2.5? per word.
At the end of the effort, we had 26 different work-
ers translate 567K Arabic tokens in 4 weeks. The
resulting translations were less fluent than their pro-
fessional counterparts, and 10% shorter on average.
The following section presents results of MT exper-
iments using the MTurk translations.
4 MT Experiments
The MT system used is based on a string-to-
dependency-tree hierarchical model of Shen et
al. (2008). Sentence alignment was done using
GIZA++ (Och and Ney, 2003). Decoder fea-
tures include translation probabilities, smoothed lex-
ical probabilities, and a dependency tree language
model. Additionally, we used 50,000 sparse, binary-
valued source and target features based on Chiang
et al (2009). The English language model was
trained on 7 billion words from the LDC Gigaword
corpus and from a web crawl. We used expected
BLEU maximization (Devlin, 2009) to tune feature
weights.
We defined a tuning set (3581 segments, 43.3K
tokens) and a test set (4166 segments, 47.7K to-
kens) using LDC2012E30, the corpus designated
as a development set by the LDC, augmented with
around 50K Words held out from LDC2012E15
and LDC2012E19, to make a development set large
enough to tune the large number of feature weights2.
The remaining data was used for training. We de-
fined three nested training sets containing 100K,
200K and 400K Arabic tokens respectively, with
2Only full forum threads were held out
613
Training Web-forum Only Newswire(10MW)+Web-forum
100KW 200KW 400KW 0KW 100KW 200KW 400KW
Prof. refs 17.71 20.23 22.61 22.82 24.05 24.85 25.19
MTurk refs 16.41 18.43 20.08 22.82 23.79 24.20 24.51
Two Training refs 19.03 21.19 23.06 22.82 24.26 25.19 25.38
Add?l Training data - 19.80 21.53 22.82 - 24.31 25.16
Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference transla-
tions. All results use professional references for the tuning and test sets.
two versions of each set: one with the professional
reference translations for the target, and the other
with the same source data, but the MTurk transla-
tions. We defined two versions of the test and tuning
sets similarly. We report translation results in terms
of lower-case BLEU scores (Papineni et al, 2002).
4.1 Training Data References
We first study the effect of training data refer-
ences, varying the amount of training data and type
of translations, while using the same professional
translation references for tuning and scoring. The
first set of baseline experiments were trained on
web forum data only, using professional transla-
tions. The first line of Table 1 shows that doubling of
the training data adds 2.5 then 2.3 BLEU points. We
repeated the experiments, but with MTurk training
references, and saw that the scores are lower by 1.3-
2.5 BLEU points, depending on the size of training
data, and that the gain obtained from doubling the
training data decreases to 2.0 and 1.6 BLEU points.
The lower MT scores and slower learning curve of
the MTurk systems are both due to the lower quality
of the translations, and to the mismatch with the pro-
fessional development set translations (we discuss
this issue further in ?4.3). However, by interpolation
of the MT scores, we find that the same MT perfor-
mance can be obtained by using twice the amount of
MTurk translated data as professional data. Consid-
ering that the MTurk translations is 10 times cheaper
than professional translations (2.5? versus 25-30?),
this constitutes a cost ratio of 5x.
We repeated the above experiments, but this time
added 10 million words of parallel data from the
NIST MT 2012 corpora (mostly news) for training.
We weighted the web forum part of the training data
by a factor of 5. Note from the results in the right
half of Table 1 that the newswire data improves the
BLEU score by 2.5 to 6.3 BLEU points, depend-
ing on the size of the web forum data. This signif-
icant improvement is because some of the web fo-
rum user postings are formal and written in MSA
(?3). More relevant to our aims is the comparison
when we vary the web forum training references in
the presence of the newswire training. The differ-
ence between the MTurk translation systems and the
professional translation drops to 0.26-0.68 points.
We conclude that in a domain adaptation scenario,
where out-of-domain training data (i.e. newswire)
already exists, crowdsourced translations for the in-
domain (i.e. web forum) training data can be used
with little to no loss in MT performance.
4.2 More Data vs. Multiple Translations
To our knowledge no previous work has compared
using multiple reference translations for training
data versus using additional training data of the same
size. We studied this question by using both transla-
tions on the target side of the training data. Using the
MTurk translations in addition to the professional
translations in training gave a gain of 0.4 to 1.3
BLEU points (bottom half of Table 1). The gain was
smaller in the presence of the GALE newswire data.
When we compared with using the same amount of
different training data instead of multiple references,
we saw that training on new data with crowdsourced
translations is better: training on two translations of
100KW gives 19.03, compared to 19.80 when train-
ing on a single translation of 200KW. The advantage
of different-source data drops to 0.34 points when
we start with 200KW. With a larger initial corpus,
the additional source coverage of new data is not as
critical, and the advantage of more variety on the
target-side of the extracted translation rules becomes
more competitive. This coverage is even less criti-
cal in the presence of the news data, where the ad-
614
Training Tuning Test Training Data Size
100KW 200KW 400KW 400KW(no lex)
Prof. Prof. Prof. 17.71 20.23 22.61 20.01
Prof. Prof. Prof.+MTurk 22.53 25.75 28.38 25.42
Prof. Prof. (len=0.95) Prof.+MTurk 23.63 26.84 29.54 26.17
Prof. Prof.+MTurk Prof.+MTurk 25.26 28.44 30.94 27.22
MTurk MTurk MTurk 16.66 18.47 20.35 17.75
MTurk MTurk Prof.+MTurk 23.83 26.45 28.66 25.44
MTurk MTurk (len=1.05) Prof.+MTurk 23.73 26.19 28.74 25.87
MTurk Prof.+MTurk Prof.+MTurk 24.91 27.66 29.78 26.45
Table 2: Effect of Tuning and Scoring References on MT.
vantage of new web forum source data disappears
(lower-right quadrant of Table 1).
4.3 Development Data References
So far, we have focused on varying training data
conditions, and kept the tuning and evaluation con-
ditions fixed. But since we have re-translated the
tuning and test sets on MTurk as well, we can study
the effect of their reference translations on MT. As
Table 2 shows, scoring the MT output using both
reference translations, the BLEU scores increase by
over 5 points (and more for the MTurk-trained sys-
tem). This increase by itself is not remarkable. What
is important to note is that the gain obtained by dou-
bling the amount of training data is larger when mea-
sured using the multiple reference test set. We also
ran experiments with 400KW training data, but with
the lexical smoothing features (Koehn et al, 2003;
Devlin, 2009) turned off. The bigger gains show that
improvements in the MT output (from additional
training or new features) can be better measured us-
ing a second MTurk reference of the test set.
Finally, we study the effect of tuning the system
parameters using both translation references. Look-
ing at the system trained on the professional trans-
lations, we see a gain of 2.5 to 2.7 BLEU points
from adding the MTurk references to the tuning set.
But as we mentioned earlier, the MTurk transla-
tions are shorter than the professional translations
by around 10% on average. Tuning on both ref-
erences, therefore, shortens the system output by
around 5%. To neutralize the effect of length mis-
match, we compared to a fairer baseline tuned on
the professional references only, but we tuned the
output-to-reference length ratio to be 0.95 (thus pro-
ducing a shorter output). In this case, we see a gain
of 1.4 points from adding the MTurk references to
the tuning set.
We also used the multiple-reference tuning set
to retune the systems trained on MTurk transla-
tions. Comparing that to a baseline that is tuned and
scored using MTurk references only, we see a gain
of around 1%. Note, however, that in this case the
length mismach is reversed, and the output of the
multiple-reference system is around 5% longer than
that of the baseline. If we compare with a baseline
that is tuned with a length ratio of 1.05 (to produce a
longer output), we see the gain shrink only slightly.
To sum up this section, a second set of refer-
ence translations obtained via MTurk makes mea-
surements of improvement on the test set more re-
liable. Also, a second set of references for tuning
improves the output of the MT systems trained on
either professional or MTurk references.
5 Conclusion
We compared professional and crowdsourced trans-
lations of the same data for training, tuning and scor-
ing Arabic-English SMT systems. We showed that
the crowdsourced translations yield the same MT
performance as professional translations for as lit-
tle as 20% of the cost. We also showed that a sec-
ond crowsourced reference translation of the devel-
opment set alows for a more accurate evaluation of
MT output.
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
615
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. Distribution Statement A (Approved for Pub-
lic Release, Distribution Unlimited).
References
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Human Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 48?54, Edmonton, Canada.
Nitin Madnani, Necip Fazil, Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 120?127, Prague, Czech Republic.
Association for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie Dorr, and Richard
Schwartz. 2008. Are multiple reference transla-
tions necessary? investigating the value of paraphrased
reference translations in parameter optimization. In
Proceedings of the 8th Conf. of the Association for
Machine Translation in the Americas (AMTA 2008),
Waikiki, Hawaii, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In The
2012 Conference of the North American Chapter of the
Association for Computational Linguistics, Montreal,
June. Association for Computational Linguistics.
616
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370?1380,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Fast and Robust Neural Network Joint Models for Statistical Machine
Translation
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John Makhoul
Raytheon BBN Technologies, 10 Moulton St, Cambridge, MA 02138, USA
{jdevlin,rzbib,zhuang,tlamar,schwartz,makhoul}@bbn.com
Abstract
Recent work has shown success in us-
ing neural network language models
(NNLMs) as features in MT systems.
Here, we present a novel formulation for
a neural network joint model (NNJM),
which augments the NNLM with a source
context window. Our model is purely lexi-
calized and can be integrated into any MT
decoder. We also present several varia-
tions of the NNJM which provide signif-
icant additive improvements.
Although the model is quite simple, it
yields strong empirical results. On the
NIST OpenMT12 Arabic-English condi-
tion, the NNJM features produce a gain of
+3.0 BLEU on top of a powerful, feature-
rich baseline which already includes a
target-only NNLM. The NNJM features
also produce a gain of +6.3 BLEU on top
of a simpler baseline equivalent to Chi-
ang?s (2007) original Hiero implementa-
tion.
Additionally, we describe two novel tech-
niques for overcoming the historically
high cost of using NNLM-style models
in MT decoding. These techniques speed
up NNJM computation by a factor of
10,000x, making the model as fast as a
standard back-off LM.
This work was supported by DARPA/I2O Contract No.
HR0011-12-C-0014 under the BOLT program (Approved for
Public Release, Distribution Unlimited). The views, opin-
ions, and/or findings contained in this article are those of the
author and should not be interpreted as representing the of-
ficial views or policies, either expressed or implied, of the
Defense Advanced Research Projects Agency or the Depart-
ment of Defense.
1 Introduction
In recent years, neural network models have be-
come increasingly popular in NLP. Initially, these
models were primarily used to create n-gram neu-
ral network language models (NNLMs) for speech
recognition and machine translation (Bengio et al,
2003; Schwenk, 2010). They have since been ex-
tended to translation modeling, parsing, and many
other NLP tasks.
In this paper we use a basic neural network ar-
chitecture and a lexicalized probability model to
create a powerful MT decoding feature. Specifi-
cally, we introduce a novel formulation for a neu-
ral network joint model (NNJM), which augments
an n-gram target language model with an m-word
source window. Unlike previous approaches to
joint modeling (Le et al, 2012), our feature can be
easily integrated into any statistical machine trans-
lation (SMT) decoder, which leads to substantially
larger improvements than k-best rescoring only.
Additionally, we present several variations of this
model which provide significant additive BLEU
gains.
We also present a novel technique for training
the neural network to be self-normalized, which
avoids the costly step of posteriorizing over the
entire vocabulary in decoding. When used in con-
junction with a pre-computed hidden layer, these
techniques speed up NNJM computation by a fac-
tor of 10,000x, with only a small reduction on MT
accuracy.
Although our model is quite simple, we obtain
strong empirical results. We show primary results
on the NIST OpenMT12 Arabic-English condi-
tion. The NNJM features produce an improvement
of +3.0 BLEU on top of a baseline that is already
better than the 1st place MT12 result and includes
1370
a powerful NNLM. Additionally, on top of a sim-
pler decoder equivalent to Chiang?s (2007) origi-
nal Hiero implementation, our NNJM features are
able to produce an improvement of +6.3 BLEU ?
as much as all of the other features in our strong
baseline system combined.
We also show strong improvements on the
NIST OpenMT12 Chinese-English task, as well as
the DARPA BOLT (Broad Operational Language
Translation) Arabic-English and Chinese-English
conditions.
2 Neural Network Joint Model (NNJM)
Formally, our model approximates the probability
of target hypothesis T conditioned on source sen-
tence S. We follow the standard n-gram LM de-
composition of the target, where each target word
t
i
is conditioned on the previous n ? 1 target
words. To make this a joint model, we also condi-
tion on source context vector S
i
:
P (T |S) ? ?
|T |
i=1
P (t
i
|t
i?1
, ? ? ? , t
i?n+1
, S
i
)
Intuitively, we want to define S
i
as the window
that is most relevant to t
i
. To do this, we first say
that each target word t
i
is affiliated with exactly
one source word at index a
i
. S
i
is then them-word
source window centered at a
i
:
S
i
= s
a
i
?
m?1
2
, ? ? ? , s
a
i
, ? ? ? , s
a
i
+
m?1
2
This notion of affiliation is derived from the
word alignment, but unlike word alignment, each
target word must be affiliated with exactly one
non-NULL source word. The affiliation heuristic
is very simple:
(1) If t
i
aligns to exactly one source word, a
i
is
the index of the word it aligns to.
(2) If t
i
align to multiple source words, a
i
is the
index of the aligned word in the middle.
1
(3) If t
i
is unaligned, we inherit its affiliation
from the closest aligned word, with prefer-
ence given to the right.
2
An example of the NNJM context model for a
Chinese-English parallel sentence is given in Fig-
ure 1.
For all of our experiments we use n = 4 and
m = 11. It is clear that this model is effectively
an (n+m)-gram LM, and a 15-gram LM would be
1
We arbitrarily round down.
2
We have found that the affiliation heuristic is robust to
small differences, such as left vs. right preference.
far too sparse for standard probability models such
as Kneser-Ney back-off (Kneser and Ney, 1995)
or Maximum Entropy (Rosenfeld, 1996). Fortu-
nately, neural network language models are able
to elegantly scale up and take advantage of arbi-
trarily large context sizes.
2.1 Neural Network Architecture
Our neural network architecture is almost identi-
cal to the original feed-forward NNLM architec-
ture described in Bengio et al (2003).
The input vector is a 14-word context vector
(3 target words, 11 source words), where each
word is mapped to a 192-dimensional vector us-
ing a shared mapping layer. We use two 512-
dimensional hidden layers with tanh activation
functions. The output layer is a softmax over the
entire output vocabulary.
The input vocabulary contains 16,000 source
words and 16,000 target words, while the out-
put vocabulary contains 32,000 target words. The
vocabulary is selected by frequency-sorting the
words in the parallel training data. Out-of-
vocabulary words are mapped to their POS tag (or
OOV, if POS is not available), and in this case
P (POS
i
|t
i?1
, ? ? ? ) is used directly without fur-
ther normalization. Out-of-bounds words are rep-
resented with special tokens <src>, </src>,
<trg>, </trg>.
We chose these values for the hidden layer size,
vocabulary size, and source window size because
they seemed to work best on our data sets ? larger
sizes did not improve results, while smaller sizes
degraded results. Empirical comparisons are given
in Section 6.5.
2.2 Neural Network Training
The training procedure is identical to that of an
NNLM, except that the parallel corpus is used
instead of a monolingual corpus. Formally, we
seek to maximize the log-likelihood of the train-
ing data:
L =
?
i
log(P (x
i
))
where x
i
is the training sample, with one sample
for every target word in the parallel corpus.
Optimization is performed using standard back
propagation with stochastic gradient ascent (Le-
Cun et al, 1998). Weights are randomly initial-
ized in the range of [?0.05, 0.05]. We use an ini-
tial learning rate of 10
?3
and a minibatch size of
1371
Figure 1: Context vector for target word ?the?, using a 3-word target history and a 5-word source window
(i.e., n = 4 and m = 5). Here, ?the? inherits its affiliation from ?money? because this is the first aligned
word to its right. The number in each box denotes the index of the word in the context vector. This
indexing must be consistent across samples, but the absolute ordering does not affect results.
128.
3
At every epoch, which we define as 20,000
minibatches, the likelihood of a validation set is
computed. If this likelihood is worse than the pre-
vious epoch, the learning rate is multiplied by 0.5.
The training is run for 40 epochs. The training
data ranges from 10-30M words, depending on the
condition. We perform a basic weight update with
no L2 regularization or momentum. However, we
have found it beneficial to clip each weight update
to the range of [-0.1, 0.1], to prevent the training
from entering degenerate search spaces (Pascanu
et al, 2012).
Training is performed on a single Tesla K10
GPU, with each epoch (128*20k = 2.6M samples)
taking roughly 1100 seconds to run, resulting in
a total training time of ?12 hours. Decoding is
performed on a CPU.
2.3 Self-Normalized Neural Network
The computational cost of NNLMs is a significant
issue in decoding, and this cost is dominated by
the output softmax over the entire target vocabu-
lary. Even class-based approaches such as Le et
al. (2012) require a 2-20k shortlist vocabulary, and
are therefore still quite costly.
Here, our goal is to be able to use a fairly
large vocabulary without word classes, and to sim-
ply avoid computing the entire output layer at de-
code time.
4
To do this, we present the novel
technique of self-normalization, where the output
layer scores are close to being probabilities with-
out explicitly performing a softmax.
Formally, we define the standard softmax log
3
We do not divide the gradient by the minibatch size. For
those who do, this is equivalent to using an initial learning
rate of 10
?3
? 128 ? 10
?1
.
4
We are not concerned with speeding up training time, as
we already find GPU training time to be adequate.
likelihood as:
log(P (x)) = log
(
e
U
r
(x)
Z(x)
)
= U
r
(x)? log(Z(x))
Z(x) = ?
|V |
r
?
=1
e
U
r
?
(x)
where x is the sample, U is the raw output layer
scores, r is the output layer row corresponding to
the observed target word, and Z(x) is the softmax
normalizer.
If we could guarantee that log(Z(x)) were al-
ways equal to 0 (i.e., Z(x) = 1) then at decode
time we would only have to compute row r of the
output layer instead of the whole matrix. While
we cannot train a neural network with this guaran-
tee, we can explicitly encourage the log-softmax
normalizer to be as close to 0 as possible by aug-
menting our training objective function:
L =
?
i
[
log(P (x
i
))? ?(log(Z(x
i
))? 0)
2
]
=
?
i
[
log(P (x
i
))? ? log
2
(Z(x
i
))
]
In this case, the output layer bias weights are
initialized to log(1/|V |), so that the initial net-
work is self-normalized. At decode time, we sim-
ply use U
r
(x) as the feature score, rather than
log(P (x)). For our NNJM architecture, self-
normalization increases the lookup speed during
decoding by a factor of ?15x.
Table 1 shows the neural network training re-
sults with various values of the free parameter
?. In all subsequent MT experiments, we use
? = 10
?1
.
We should note that Vaswani et al (2013) im-
plements a method called Noise Contrastive Es-
timation (NCE) that is also used to train self-
normalized NNLMs. Although NCE results in
faster training time, it has the downside that there
1372
Arabic BOLT Val
? log(P (x)) | log(Z(x))|
0 ?1.82 5.02
10
?2
?1.81 1.35
10
?1
?1.83 0.68
1 ?1.91 0.28
Table 1: Comparison of neural network likelihood
for various ? values. log(P (x)) is the average
log-likelihood on a held-out set. | log(Z(x))| is
the mean error in log-likelihood when using U
r
(x)
directly instead of the true softmax probability
log(P (x)). Note that ? = 0 is equivalent to the
standard neural network objective function.
is no mechanism to control the degree of self-
normalization. By contrast, our ? parameter al-
lows us to carefully choose the optimal trade-off
between neural network accuracy and mean self-
normalization error. In future work, we will thor-
oughly compare self-normalization vs. NCE.
2.4 Pre-Computing the Hidden Layer
Although self-normalization significantly im-
proves the speed of NNJM lookups, the model
is still several orders of magnitude slower than a
back-off LM. Here, we present a ?trick? for pre-
computing the first hidden layer, which further in-
creases the speed of NNJM lookups by a factor of
1,000x.
Note that this technique only results in a signif-
icant speedup for self-normalized, feed-forward,
NNLM-style networks with one hidden layer. We
demonstrate in Section 6.6 that using one hidden
layer instead of two has minimal effect on BLEU.
For the neural network described in Section 2.1,
computing the first hidden layer requires mul-
tiplying a 2689-dimensional input vector
5
with
a 2689 ? 512 dimensional hidden layer matrix.
However, note that there are only 3 possible posi-
tions for each target word, and 11 for each source
word. Therefore, for every word in the vocabu-
lary, and for each position, we can pre-compute
the dot product between the word embedding and
the first hidden layer. These are computed offline
and stored in a lookup table, which is <500MB in
size.
Computing the first hidden layer now only re-
quires 15 scalar additions for each of the 512
hidden rows ? one for each word in the input
5
2689 = 14 words ? 192 dimensions + 1 bias
vector, plus the bias. This can be reduced to
just 5 scalar additions by pre-summing each 11-
word source window when starting a test sen-
tence. If our neural network has only one hid-
den layer and is self-normalized, the only remain-
ing computation is 512 calls to tanh() and a sin-
gle 513-dimensional dot product for the final out-
put score.
6
Thus, only ?3500 arithmetic opera-
tions are required per n-gram lookup, compared
to ?2.8M for self-normalized NNJM without pre-
computation, and ?35M for the standard NNJM.
7
Neural Network Speed
Condition lookups/sec sec/word
Standard 110 10.9
+ Self-Norm 1500 0.8
+ Pre-Computation 1,430,000 0.0008
Table 2: Speed of the neural network computa-
tion on a single CPU thread. ?lookups/sec? is the
number of unique n-gram probabilities that can be
computed per second. ?sec/word? is the amortized
cost of unique NNJM lookups in decoding, per
source word.
Table 2 shows the speed of self-normalization
and pre-computation for the NNJM. The decoding
cost is based on a measurement of ?1200 unique
NNJM lookups per source word for our Arabic-
English system.
8
By combining self-normalization and pre-
computation, we can achieve a speed of 1.4M
lookups/second, which is on par with fast back-
off LM implementations (Tanaka et al, 2013).
We demonstrate in Section 6.6 that using the self-
normalized/pre-computed NNJM results in only
a very small BLEU degradation compared to the
standard NNJM.
3 Decoding with the NNJM
Because our NNJM is fundamentally an n-gram
NNLM with additional source context, it can eas-
ily be integrated into any SMT decoder. In this
section, we describe the considerations that must
be taken when integrating the NNJM into a hierar-
chical decoder.
6
tanh() is implemented using a lookup table.
7
3500 ? 5? 512 + 2? 513; 2.8M ? 2? 2689? 512 +
2 ? 513; 35M ? 2 ? 2689 ? 512 + 2 ? 513 ? 32000. For
the sake of a fair comparison, these all use one hidden layer.
A second hidden layer adds 0.5M floating point operations.
8
This does not include the cost of duplicate lookups
within the same test sentence, which are cached.
1373
3.1 Hierarchical Parsing
When performing hierarchical decoding with an
n-gram LM, the leftmost and rightmost n ? 1
words from each constituent must be stored in the
state space. Here, we extend the state space to
also include the index of the affiliated source word
for these edge words. This does not noticeably in-
crease the search space. We also train a separate
lower-order n-gram model, which is necessary to
compute estimate scores during hierarchical de-
coding.
3.2 Affiliation Heuristic
For aligned target words, the normal affiliation
heuristic can be used, since the word alignment
is available within the rule. For unaligned words,
the normal heuristic can also be used, except when
the word is on the edge of a rule, because then the
target neighbor words are not necessarily known.
In this case, we infer the affiliation from the rule
structure. Specifically, if unaligned target word t
is on the right edge of an arc that covers source
span [s
i
, s
j
], we simply say that t is affiliated with
source word s
j
. If t is on the left edge of the arc,
we say it is affiliated with s
i
.
4 Model Variations
Recall that our NNJM feature can be described
with the following probability:
?
|T |
i=1
P (t
i
|t
i?1
, t
i?2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
This formulation lends itself to several natural
variations. In particular, we can reverse the trans-
lation direction of the languages, as well as the di-
rection of the language model.
We denote our original formulation as a source-
to-target, left-to-right model (S2T/L2R). We can
train three variations using target-to-source (T2S)
and right-to-left (R2L) models:
S2T/R2L
?
|T |
i=1
P (t
i
|t
i+1
, t
i+2
, ? ? ? , s
a
i
, s
a
i
?1
, s
a
i
+1
, ? ? ? )
T2S/L2R
?
|S|
i=1
P (s
i
|s
i?1
, s
i?2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
T2S/R2L
?
|S|
i=1
P (s
i
|s
i+1
, s
i+2
, ? ? ? , t
a
?
i
, t
a
?
i
?1
, t
a
?
i
+1
, ? ? ? )
where a
?
i
is the target-to-source affiliation, de-
fined analogously to a
i
.
The T2S variations cannot be used in decoding
due to the large target context required, and are
thus only used in k-best rescoring. The S2T/R2L
variant could be used in decoding, but we have not
found this beneficial, so we only use it in rescor-
ing.
4.1 Neural Network Lexical Translation
Model (NNLTM)
One issue with the S2T NNJM is that the prob-
ability is computed over every target word, so it
does not explicitly model NULL-aligned source
words. In order to assign a probability to every
source word during decoding, we also train a neu-
ral network lexical translation model (NNLMT).
Here, the input context is the 11-word source
window centered at s
i
, and the output is the tar-
get token t
s
i
which s
i
aligns to. The probabil-
ity is computed over every source word in the in-
put sentence. We treat NULL as a normal target
word, and if a source word aligns to multiple target
words, it is treated as a single concatenated token.
Formally, the probability model is:
?
|S|
i=1
P (t
s
i
|s
i
, s
i?1
, s
i+1
, ? ? ? )
This model is trained and evaluated like our
NNJM. It is easy and computationally inexpensive
to use this model in decoding, since only one neu-
ral network computation must be made for each
source word.
In rescoring, we also use a T2S NNLTM model
computed over every target word:
?
|T |
i=1
P (s
t
i
|t
i
, t
i?1
, t
i+1
, ? ? ? )
5 MT System
In this section, we describe the MT system used in
our experiments.
5.1 MT Decoder
We use a state-of-the-art string-to-dependency hi-
erarchical decoder (Shen et al, 2010). Our base-
line decoder contains a large and powerful set of
features, which include:
? Forward and backward rule probabilities
? 4-gram Kneser-Ney LM
? Dependency LM (Shen et al, 2010)
? Contextual lexical smoothing (Devlin, 2009)
? Length distribution (Shen et al, 2010)
? Trait features (Devlin and Matsoukas, 2012)
? Factored source syntax (Huang et al, 2013)
? 7 sparse feature types, totaling 50k features
(Chiang et al, 2009)
? LM adaptation (Snover et al, 2008)
1374
We also perform 1000-best rescoring with the
following features:
? 5-gram Kneser-Ney LM
? Recurrent neural network language model
(RNNLM) (Mikolov et al, 2010)
Although we consider the RNNLM to be part
of our baseline, we give it special treatment in the
results section because we would expect it to have
the highest overlap with our NNJM.
5.2 Training and Optimization
For Arabic word tokenization, we use the MADA-
ARZ tokenizer (Habash et al, 2013) for the BOLT
condition, and the Sakhr
9
tokenizer for the NIST
condition. For Chinese tokenization, we use a sim-
ple longest-match-first lexicon-based approach.
For word alignment, we align all of the train-
ing data with both GIZA++ (Och and Ney, 2003)
and NILE (Riesa et al, 2011), and concatenate the
corpora together for rule extraction.
For MT feature weight optimization, we use
iterative k-best optimization with an Expected-
BLEU objective function (Rosti et al, 2010).
6 Experimental Results
We present MT primary results on Arabic-English
and Chinese-English for the NIST OpenMT12 and
DARPA BOLT conditions. We also present a set
of auxiliary results in order to further analyze our
features.
6.1 NIST OpenMT12 Results
Our NIST system is fully compatible with the
OpenMT12 constrained track, which consists of
10M words of high-quality parallel training for
Arabic, and 25M words for Chinese.
10
The
Kneser-Ney LM is trained on 5B words of data
from English GigaWord. For test, we use
the ?Arabic-To-English Original Progress Test?
(1378 segments) and ?Chinese-to-English Orig-
inal Progress Test + OpenMT12 Current Test?
(2190 segments), which consists of a mix of
newswire and web data.
11
All test segments have
4 references. Our tuning set contains 5000 seg-
ments, and is a mix of the MT02-05 eval set as
well as held-out parallel training.
9
http://www.sakhr.com
10
We also make weak use of 30M-100M words of UN data
+ ISI comparable corpora, but this data provides almost no
benefit.
11
http://www.nist.gov/itl/iad/mig/openmt12results.cfm
NIST MT12 Test
Ar-En Ch-En
BLEU BLEU
OpenMT12 - 1st Place 49.5 32.6
OpenMT12 - 2nd Place 47.5 32.2
OpenMT12 - 3rd Place 47.4 30.8
? ? ? ? ? ? ? ? ?
OpenMT12 - 9th Place 44.0 27.0
OpenMT12 - 10th Place 41.2 25.7
Baseline (w/o RNNLM) 48.9 33.0
Baseline (w/ RNNLM) 49.8 33.4
+ S2T/L2R NNJM (Dec) 51.2 34.2
+ S2T NNLTM (Dec) 52.0 34.2
+ T2S NNLTM (Resc) 51.9 34.2
+ S2T/R2L NNJM (Resc) 52.2 34.3
+ T2S/L2R NNJM (Resc) 52.3 34.5
+ T2S/R2L NNJM (Resc) 52.8 34.7
?Simple Hier.? Baseline 43.4 30.1
+ S2T/L2R NNJM (Dec) 47.2 31.5
+ S2T NNLTM (Dec) 48.5 31.8
+ Other NNJMs (Resc) 49.7 32.2
Table 3: Primary results on Arabic-English and
Chinese-English NIST MT12 Test Set. The first
section corresponds to the top and bottom ranked
systems from the evaluation, and are taken from
the NIST website. The second section corresponds
to results on top of our strongest baseline. The
third section corresponds to results on top of a
simpler baseline. Within each section, each row
includes all of the features from previous rows.
BLEU scores are mixed-case.
Results are shown in the second section of Ta-
ble 3. On Arabic-English, the primary S2T/L2R
NNJM gains +1.4 BLEU on top of our baseline,
while the S2T NNLTM gains another +0.8, and
the directional variations gain +0.8 BLEU more.
This leads to a total improvement of +3.0 BLEU
from the NNJM and its variations. Considering
that our baseline is already +0.3 BLEU better than
the 1st place result of MT12 and contains a strong
RNNLM, we consider this to be quite an extraor-
dinary improvement.
12
For the Chinese-English condition, there is an
improvement of +0.8 BLEU from the primary
NNJM and +1.3 BLEU overall. Here, the base-
line system is already +0.8 BLEU better than the
12
Note that the official 1st place OpenMT12 result was our
own system, so we can assure that these comparisons are ac-
curate.
1375
best MT12 system. The smaller improvement on
Chinese-English compared to Arabic-English is
consistent with the behavior of our baseline fea-
tures, as we show in the next section.
6.2 ?Simple Hierarchical? NIST Results
The baseline used in the last section is a highly-
engineered research system, which uses a wide
array of features that were refined over a num-
ber of years, and some of which require linguis-
tic resources. Because of this, the baseline BLEU
scores are much higher than a typical MT system
? especially a real-time, production engine which
must support many language pairs.
Therefore, we also present results using a
simpler version of our decoder which emulates
Chiang?s original Hiero implementation (Chiang,
2007). Specifically, this means that we don?t
use dependency-based rule extraction, and our de-
coder only contains the following MT features: (1)
rule probabilities, (2) n-gram Kneser-Ney LM, (3)
lexical smoothing, (4) target word count, (5) con-
cat rule penalty.
Results are shown in the third section of Table 3.
The ?Simple Hierarchical? Arabic-English system
is -6.4 BLEU worse than our strong baseline, and
would have ranked 10th place out of 11 systems
in the evaluation. When the NNJM features are
added to this system, we see an improvement of
+6.3 BLEU, which would have ranked 1st place in
the evaluation.
Effectively, this means that for Arabic-English,
the NNJM features are equivalent to the combined
improvements from the string-to-dependency
model plus all of the features listed in Section 5.1.
For Chinese-English, the ?Simple Hierarchical?
system only degrades by -3.2 BLEU compared
to our strongest baseline, and the NNJM features
produce a gain of +2.1 BLEU on top of that.
6.3 BOLT Web Forum Results
DARPA BOLT is a major research project with the
goal of improving translation of informal, dialec-
tical Arabic and Chinese into English. The BOLT
domain presented here is ?web forum,? which was
crawled from various Chinese and Egyptian Inter-
net forums by LDC. The BOLT parallel training
consists of all of the high-quality NIST training,
plus an additional 3 million words of translated
forum data provided by LDC. The tuning and test
sets consist of roughly 5000 segments each, with
2 references for Arabic and 3 for Chinese.
Results are shown in Table 4. The baseline here
uses the same feature set as the strong NIST sys-
tem. On Arabic, the total gain is +2.6 BLEU,
while on Chinese, the gain is +1.3 BLEU.
BOLT Test
Ar-En Ch-En
BLEU BLEU
Baseline (w/o RNNLM) 40.2 30.6
Baseline (w/ RNNLM) 41.3 30.9
+ S2T/L2R NNJM (Dec) 42.9 31.9
+ S2T NNLTM (Dec) 43.2 31.9
+ Other NNJMs (Resc) 43.9 32.2
Table 4: Primary results on Arabic-English and
Chinese-English BOLT Web Forum. Each row
includes the aggregate features from all previous
rows.
6.4 Effect of k-best Rescoring Only
Table 5 shows performance when our S2T/L2R
NNJM is used only in 1000-best rescoring, com-
pared to decoding. The primary purpose of this is
as a comparison to Le et al (2012), whose model
can only be used in k-best rescoring.
BOLT Test
Ar-En
Without With
RNNLM RNNLM
BLEU BLEU
Baseline 40.2 41.3
S2T/L2R NNJM (Resc) 41.7 41.6
S2T/L2R NNJM (Dec) 42.8 42.9
Table 5: Comparison of our primary NNJM in de-
coding vs. 1000-best rescoring.
We can see that the rescoring-only NNJM per-
forms very well when used on top of a baseline
without an RNNLM (+1.5 BLEU), but the gain on
top of the RNNLM is very small (+0.3 BLEU).
The gain from the decoding NNJM is large in both
cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/
RNNLM). This demonstrates that the full power of
the NNJM can only be harnessed when it is used
in decoding. It is also interesting to see that the
RNNLM is no longer beneficial when the NNJM
is used.
1376
6.5 Effect of Neural Network Configuration
Table 6 shows results using the S2T/L2R NNJM
with various configurations. We can see that re-
ducing the source window size, layer size, or vo-
cab size will all degrade results. Increasing the
sizes beyond the default NNJM has almost no ef-
fect (102%). Also note that the target-only NNLM
(i.e., Source Window=0) only obtains 33% of the
improvements of the NNJM.
BOLT Test
Ar-En
BLEU % Gain
?Simple Hier.? Baseline 33.8 -
S2T/L2R NNJM (Dec) 38.4 100%
Source Window=7 38.3 98%
Source Window=5 38.2 96%
Source Window=3 37.8 87%
Source Window=0 35.3 33%
Layers=384x768x768 38.5 102%
Layers=192x512 38.1 93%
Layers=128x128 37.1 72%
Vocab=64,000 38.5 102%
Vocab=16,000 38.1 93%
Vocab=8,000 37.3 83%
Activation=Rectified Lin. 38.5 102%
Activation=Linear 37.3 76%
Table 6: Results with different neural net-
work architectures. The ?default? NNJM in
the second row uses these parameters: SW=11,
L=192x512x512, V=32,000, A=tanh. All mod-
els use a 3-word target history (i.e., 4-gram LM).
?Layers? refers to the size of the word embedding
followed by the hidden layers. ?Vocab? refers to
the size of the input and output vocabularies. ?%
Gain? is the BLEU gain over the baseline relative
to the default NNJM.
6.6 Effect of Speedups
All previous results use a self-normalized neural
network with two hidden layers. In Table 7, we
compare this to using a standard network (with
two hidden layers), as well as a pre-computed neu-
ral network.
13
The ?Simple Hierarchical? base-
line is used here because it more closely approx-
imates a real-time MT engine. For the sake of
speed, these experiments only use the S2T/L2R
NNJM+S2T NNLTM.
13
The difference in score for self-normalized vs. pre-
computed is entirely due to two vs. one hidden layers.
Each result from Table 7 corresponds to a row
in Table 2 of Section 2.4. We can see that go-
ing from the standard model to the pre-computed
model only reduces the BLEU improvement from
+6.4 to +6.1, while increasing the NNJM lookup
speed by a factor of 10,000x.
BOLT Test
Ar-En
BLEU Gain
?Simple Hier.? Baseline 33.8 -
Standard NNJM 40.2 +6.4
Self-Norm NNJM 40.1 +6.3
Pre-Computed NNJM 39.9 +6.1
Table 7: Results for the standard NNs vs. self-
normalized NNs vs. pre-computed NNs.
In Table 2 we showed that the cost of unique
lookups for the pre-computed NNJM is only
?0.001 seconds per source word. This does not
include the cost of n-gram creation or cached
lookups, which amount to ?0.03 seconds per
source word in our current implementation.
14
However, the n-grams created for the NNJM can
be shared with the Kneser-Ney LM, which reduces
the cost of that feature. Thus, the total cost in-
crease of using the NNJM+NNLTM features in
decoding is only ?0.01 seconds per source word.
In future work we will provide more detailed
analysis regarding the usability of the NNJM in a
low-latency, high-throughput MT engine.
7 Related Work
Although there has been a substantial amount of
past work in lexicalized joint models (Marino et
al., 2006; Crego and Yvon, 2010), nearly all of
these papers have used older statistical techniques
such as Kneser-Ney or Maximum Entropy. How-
ever, not only are these techniques intractable to
train with high-order context vectors, they also
lack the neural network?s ability to semantically
generalize (Mikolov et al, 2013) and learn non-
linear relationships.
A number of recent papers have proposed meth-
ods for creating neural network translation/joint
models, but nearly all of these works have ob-
tained much smaller BLEU improvements than
ours. For each related paper, we will briefly con-
14
In our decoder, roughly 95% of NNJM n-gram lookups
within the same sentence are duplicates.
1377
trast their methodology with our own and summa-
rize their BLEU improvements using scores taken
directly from the cited paper.
Auli et al (2013) use a fixed continuous-space
source representation, obtained from LDA (Blei
et al, 2003) or a source-only NNLM. Also, their
model is recurrent, so it cannot be used in decod-
ing. They obtain +0.2 BLEU improvement on top
of a target-only NNLM (25.6 vs. 25.8).
Schwenk (2012) predicts an entire target phrase
at a time, rather than a word at a time. He obtains
+0.3 BLEU improvement (24.8 vs. 25.1).
Zou et al (2013) estimate context-free bilingual
lexical similarity scores, rather than using a large
context. They obtain an +0.5 BLEU improvement
on Chinese-English (30.0 vs. 30.5).
Kalchbrenner and Blunsom (2013) implement
a convolutional recurrent NNJM. They score a
1000-best list using only their model and are able
to achieve the same BLEU as using all 12 standard
MT features (21.8 vs 21.7). However, additive re-
sults are not presented.
The most similar work that we know of is Le et
al. (2012). Le?s basic procedure is to re-order the
source to match the linear order of the target, and
then segment the hypothesis into minimal bilin-
gual phrase pairs. Then, he predicts each target
word given the previous bilingual phrases. How-
ever, Le?s formulation could only be used in k-
best rescoring, since it requires long-distance re-
ordering and a large target context.
Le?s model does obtain an impressive +1.7
BLEU gain on top of a baseline without an NNLM
(25.8 vs. 27.5). However, when compared to
the strongest baseline which includes an NNLM,
Le?s best models (S2T + T2S) only obtain an +0.6
BLEU improvement (26.9 vs. 27.5). This is con-
sistent with our rescoring-only result, which indi-
cates that k-best rescoring is too shallow to take
advantage of the power of a joint model.
Le?s model also uses minimal phrases rather
than being purely lexicalized, which has two main
downsides: (a) a number of complex, hand-crafted
heuristics are required to define phrase boundaries,
which may not transfer well to new languages, (b)
the effective vocabulary size is much larger, which
substantially increases data sparsity issues.
We should note that our best results use six sep-
arate models, whereas all previous work only uses
one or two models. However, we have demon-
strated that we can obtain 50%-80% of the to-
tal improvement with only one model (S2T/L2R
NNJM), and 70%-90% with only two models
(S2T/L2R NNJM + S2T NNLTM). Thus, the one
and two-model conditions still significantly out-
perform any past work.
8 Discussion
We have described a novel formulation for a neural
network-based machine translation joint model,
along with several simple variations of this model.
When used as MT decoding features, these models
are able to produce a gain of +3.0 BLEU on top of
a very strong and feature-rich baseline, as well as
a +6.3 BLEU gain on top of a simpler system.
Our model is remarkably simple ? it requires no
linguistic resources, no feature engineering, and
only a handful of hyper-parameters. It also has no
reliance on potentially fragile outside algorithms,
such as unsupervised word clustering. We con-
sider the simplicity to be a major advantage. Not
only does this suggest that it will generalize well to
new language pairs and domains, but it also sug-
gests that it will be straightforward for others to
replicate these results.
Overall, we believe that the following factors set
us apart from past work and allowed us to obtain
such significant improvements:
1. The ability to use the NNJM in decoding
rather than rescoring.
2. The use of a large bilingual context vector,
which is provided to the neural network in
?raw? form, rather than as the output of some
other algorithm.
3. The fact that the model is purely lexicalized,
which avoids both data sparsity and imple-
mentation complexity.
4. The large size of the network architecture.
5. The directional variation models.
One of the biggest goals of this work is to quell
any remaining doubts about the utility of neural
networks in machine translation. We believe that
there are large areas of research yet to be explored.
For example, creating a new type of decoder cen-
tered around a purely lexicalized neural network
model. Our short term ideas include using more
interesting types of context in our input vector
(such as source syntax), or using the NNJM to
model syntactic/semantic structure of the target.
1378
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In HLT-NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Josep Maria Crego and Franc?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, 24(2):159?
175.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-
based hypothesis selection for machine translation.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 528?532, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jacob Devlin. 2009. Lexical features for statistical
machine translation. Master?s thesis, University of
Maryland.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
analysis and disambiguation for dialectal arabic. In
HLT-NAACL, pages 426?432.
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.
2013. Factored soft source syntactic constraints for
hierarchical machine translation. In EMNLP, pages
556?566.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, NAACL HLT ?12, pages 39?
48, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yann LeCun, L?eon Bottou, Genevieve B Orr, and
Klaus-Robert M?uller. 1998. Efficient backprop. In
Neural networks: Tricks of the trade, pages 9?50.
Springer.
Jos?e B Marino, Rafael E Banchs, Josep M Crego, Adri`a
De Gispert, Patrik Lambert, Jos?e AR Fonollosa, and
Marta R Costa-Juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746?
751.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2012. On the difficulty of training recurrent neural
networks. arXiv preprint arXiv:1211.5063.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 497?507, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ronald Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer, Speech and Language, 10:187?228.
Antti Rosti, Bing Zhang, Spyros Matsoukas, and
Rich Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In
WMT/MetricsMATR, pages 321?326.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague
Bull. Math. Linguistics, 93:137?146.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671,
December.
1379
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 857?866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Tanaka, Yasuhara Toru, Jun-ya Yamamoto, and
Mikio Norimatsu. 2013. An efficient language
model using double-array structures.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393?1398.
1380
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 321?326,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BBN System Description for WMT10 System Combination Task
Antti-Veikko I. Rosti and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination out-
puts for Czech-English, German-English,
Spanish-English, French-English, and All-
English language pairs. All combinations
were based on confusion network decod-
ing. An incremental hypothesis alignment
algorithm with flexible matching was used
to build the networks. The bi-gram de-
coding weights for the single source lan-
guage translations were tuned directly to
maximize the BLEU score of the decod-
ing output. Approximate expected BLEU
was used as the objective function in gra-
dient based optimization of the combina-
tion weights for a 44 system multi-source
language combination (All-English). The
system combination gained around 0.4-
2.0 BLEU points over the best individual
systems on the single source conditions.
On the multi-source condition, the system
combination gained 6.6 BLEU points.
1 Introduction
The BBN submissions to the WMT10 system
combination task were based on confusion net-
work decoding. The confusion networks were
built using the incremental hypothesis alignment
algorithm with flexible matching introduced in the
BBN submission for the WMT09 system combi-
nation task (Rosti et al, 2009). This year, the
system combination weights were tuned to max-
imize the BLEU score (Papineni et al, 2002) of
the 1-best decoding output (lattice based BLEU
tuning) using downhill simplex method (Press et
al., 2007). A 44 system multi-source combina-
tion was also submitted. Since the gradient-free
optimization algorithms do not seem to be able to
handle more than 20-30 weights, a gradient ascent
to maximize an approximate expected BLEU ob-
jective was used to optimize the larger number of
weights.
The lattice based BLEU tuning may be imple-
mented using any optimization algorithm that does
not require the gradient of the objective function.
Due to the size of the lattices, the objective func-
tion evaluation may have to be distributed to mul-
tiple servers. The optimizer client accumulates the
BLEU statistics of the 1-best hypotheses from the
servers for given search weights, computes the fi-
nal BLEU score, and passes it to the optimiza-
tion algorithm which returns a new set of search
weights. The lattice based tuning explores the en-
tire search space and does not require multiple de-
coding iterations with N -best list merging to ap-
proximate the search space as in the standard min-
imum error rate training (Och, 2003). This allows
much faster turnaround in weight tuning.
Differentiable approximations of BLEU have
been proposed for consensus decoding. Tromble
et al (2008) used a linear approximation and Pauls
et al (2009) used a closer approximation called
CoBLEU. CoBLEU is based on the BLEU for-
mula but the n-gram counts are replaced by ex-
pected counts over a translation forest. Due to the
min-functions required in converting the n-gram
counts to matches and a non-differentiable brevity
penalty, a sub-gradient ascent must be used. In
this work, an approximate expected BLEU (Exp-
BLEU) defined over N -best lists was used as a
differentiable objective function. ExpBLEU uses
expected BLEU statistics where the min-function
is not needed as the statistics are computed off-
line and the brevity penalty is replaced by a dif-
ferentiable approximation. The ExpBLEU tun-
ing yields comparable results to direct BLEU tun-
ing using gradient-free algorithms on combina-
tions of small number of systems (fewer than 20-
30 weights). Results on a 44 system combination
show that the gradient based optimization is more
robust with larger number of weights.
321
This paper is organized as follows. Section
2 reviews the incremental hypothesis alignment
algorithm used to built the confusion networks.
Decoding weight optimization using direct lattice
1-best BLEU tuning and N -best list based Exp-
BLEU tuning are presented in Section 3. Exper-
imental results on combining single source lan-
guage to English outputs and all 44 English out-
puts are detailed in Section 4. Finally, Section 5
concludes this paper with some ideas for future
work.
2 Hypothesis Alignment
The confusion networks were built by using the
incremental hypothesis alignment algorithm with
flexible matching introduced in Rosti et al (2009).
The algorithm is reviewed in more detail here. It
is loosely related to the alignment performed in
the calculation of the translation edit rate (TER)
(Snover et al, 2006) which estimates the edit
distance between two strings allowing shifts of
blocks of words in addition to insertions, dele-
tions, and substitutions. Calculating an exact TER
for strings longer than a few tokens1 is not compu-
tationally feasible, so the tercom2 software uses
heuristic shift constraints and pruning to find an
upper bound of TER. In this work, the hypothe-
ses were aligned incrementally with the confusion
network, thus using tokens from all previously
aligned hypotheses in computing the edit distance.
Lower substitution costs were assigned to tokens
considered equivalent and the heuristic shift con-
straints of tercom were relaxed3.
First, tokens from all hypotheses are put into
equivalence classes if they belong to the same
WordNet (Fellbaum, 1998) synonym set or have
the same stem. The 1-best hypothesis from each
system is used as the confusion network skeleton
which defines the final word order of the decod-
ing output. Second, a trivial confusion network
is generated from the skeleton hypothesis by gen-
erating a single arc for each token. The align-
ment algorithm explores shifts of blocks of words
that minimize the edit distance between the cur-
rent confusion network and an unaligned hypothe-
1Hypotheses are tokenized and lower-cased prior to align-
ment. Tokens generally refer to words and punctuation.
2http://www.cs.umd.edu/?snover/tercom/
current version 0.7.25.
3This algorithm is not equivalent to an incremental TER-
Plus (Snover et al, 2009) due to different shift constraints and
the lack of paraphrase matching
30 1cat(1) 2sat(1) mat(1)
(a) Skeleton hypothesis.
40 1cat(1,1) 2sat(1,1) 3on(0,1)NULL(1,0) mat(1,1)
(b) Two hypotheses (insertion).
40 1cat(1,1,0)NULL(0,0,1) 2sat(1,1,1) 3on(0,1,0)NULL(1,0,1) mat(1,1,1)
(c) Three hypotheses (deletion).
40 1cat(1,1,0,1)NULL(0,0,1,0) 2sat(1,1,1,1) 3on(0,1,0,0)NULL(1,0,1,1) mat(1,1,1,0)hat(0,0,0,1)
(d) Four hypotheses (substitution).
Figure 1: Example of incrementally aligning ?cat
sat mat?, ?cat sat on mat?, ?sat mat?, and ?cat sat
hat?.
sis. Third, the hypothesis with the lowest edit dis-
tance to the current confusion network is aligned
into the network. The heuristically selected edit
costs used in the WMT10 system were 1.0 for
insertions, deletions, and shifts, 0.2 for substitu-
tions of tokens in the same equivalence class, and
1.0001 for substitutions of non-equivalent tokens.
An insertion with respect to the network always
results in a new node and two new arcs. The first
arc contains the inserted token and the second arc
contains a NULL token representing the missing
token from all previously aligned hypotheses. A
substitution/deletion results in a new token/NULL
arc or increase in the confidence of an existing to-
ken/NULL arc. The process is repeated until all
hypotheses are aligned into the network.
For example, given the following hypotheses
from four systems: ?cat sat mat?, ?cat sat on mat?,
?sat mat?, and ?cat sat hat?, an initial network in
Figure 1(a) is generated. The following two hy-
potheses have a distance of one edit from the initial
network, so the second can be aligned next. Figure
1(b) shows the additional node created and the two
new arcs for ?on? and ?NULL? tokens. The third
hypothesis has deleted token ?cat? and matches the
322
?NULL? token between nodes 2 and 3 as seen in
Figure 1(c). The fourth hypothesis matches all but
the final token ?hat? which becomes a substitution
for ?mat? in Figure 1(d). The binary vectors in
the parentheses following each token show which
system generated the token aligned to that arc. If
the systems generated N -best hypotheses, a frac-
tional increment could be added to these vectors
as in (Rosti et al, 2007). Given these system spe-
cific scores are normalized to sum to one over all
arcs connecting two consecutive nodes, they may
be viewed as system specific word arc posterior
estimates. Note, for 1-best hypotheses the scores
sum to one without normalization.
Given system outputs E = {E1, . . . , ENs},
an algorithm to build a set of Ns confusion
networks C = {C1, . . . , CNs} may be written
as:
for n = 1 to Ns do
Cn ? Init(En) {initialize confusion net-
work from the skeleton}
E ? ? E ? En {set of unaligned hypotheses}
while E ? 6= ? do
Em ? argminE?E ? Dist(E,Cn)
{compute edit distances}
Cn ? Align(Em, Cn) {align closest hy-
pothesis}
E ? ? E ? ? Em {update set of unaligned
hypotheses}
end while
end for
The set of Ns confusion networks are expanded to
separate paths with distinct bi-gram contexts and
connected in parallel into a big lattice with com-
mon start and end nodes with NULL token arcs.
A prior probability estimate is assigned to the sys-
tem specific word arc confidences connecting the
common start node and the first node in each sub-
network. A heuristic prior is estimated as:
pn =
1
Z
exp(?100
en
Nn
) (1)
where en is the total cost of aligning all hypothe-
ses when using system n as the skeleton, Nn is
the number of nodes in the confusion network be-
fore bi-gram expansion, and Z is a scaling factor
to guarantee pn sum to one. This gives a higher
prior for a network with fewer alignment errors
and longer expected decoding output.
3 Weight Optimization
Standard search algorithms may be used to find N -
best hypotheses from the final lattice. The score
for arc l is computed as:
sl = log
( Ns?
n=1
?nsnl
)
+ ?L(wl|wP (l)) + ?S(wl)
(2)
where ?n are the system weights constrained to
sum to one, snl are the system specific arc pos-
teriors, ? is a language model (LM) scaling fac-
tor, L(wl|wP (l)) is the bi-gram log-probability for
the token wl on the arc l given the token wP (l)
on the arc P (l) preceding the arc l, ? is the word
insertion scaling factor, and S(wl) is zero if wl
is a NULL token and one otherwise. The path
with the highest total score under summation is
the 1-best decoding output. The decoding weights
? = {?1, . . . , ?Ns , ?, ?} are tuned to optimize two
objective functions described next.
3.1 Lattice Based BLEU Optimization
Powell?s method (Press et al, 2007) on N -best
lists was used in system combination weight tun-
ing in Rosti et al (2007). This requires multiple
decoding iterations and merging the N -best lists
between tuning runs to approximate the full search
space as in Och (2003). To speed up the tuning
process, a distributed optimization method can be
used. The lattices are divided into multiple chunks
each of which are loaded into memory by a server.
A client runs the optimization algorithm relying
on the servers for parallelized objective function
evaluation. The client sends a new set of search
weights to the servers which decode the chunks
of lattices and return the 1-best hypothesis BLEU
statistics back to the client. The client accumulates
the BLEU statistics from all servers and computes
the final BLEU score used as the objective func-
tion by the optimization algorithm. Results similar
to Powell?s method can be obtained with fewer it-
erations by using the downhill simplex method in
multi-dimensions (Amoeba) (Press et al, 2007).
To enforce the sum to one constraint of the sys-
tem weights ?n, the search weights are restricted
to [0, 1] by assigning a large penalty if any cor-
responding search weight breaches the limits and
these restricted search weights are scaled to sum
to one before the objective function evaluation.
After optimizing the bi-gram decoding weights
directly on the lattices, a 300-best list are gener-
323
ated. The 300-best hypotheses are re-scored using
a 5-gram LM and another set of re-scoring weights
are tuned on the development set using the stan-
dard N -best list based method. Multiple random
restarts may be used in both lattice and N-best list
based optimization to decrease chances of finding
a local minimum. Twenty sets of initial weights
(the weights from the previous tuning and 19 ran-
domly perturbed weights) were used in all experi-
ments.
3.2 Approximate Expected BLEU
Optimization
The gradient-free optimization algorithms like
Powell?s method and downhill simplex work well
for up to around 20-30 weights. When the number
of weights is larger, the algorithms often get stuck
in local optima even if multiple random restarts
are used. The BLEU score for a 1-best output is
defined as follows:
BLEU =
4?
n=1
(?
i m
n
i?
i h
n
i
) 1
4
?
(
1 ?
?
i ri
?
i h
1
i
)
(3)
where mni is the number of n-gram matches be-
tween the hypothesis and reference for segment
i, hni is the number of n-grams in the hypothesis,
ri is the reference length (or the reference length
closest to the hypothesis if multiple references are
available), and ?(x) = min(1.0, ex) is the brevity
penalty. The first term in Equation 3 is a harmonic
mean of the n-gram precisions up to n = 4. The
selection of 1-best hypotheses is discrete and the
brevity penalty is not continuous, so the BLEU
score is not differentiable and gradient based op-
timization cannot be used. Given a posterior dis-
tribution over all possible decoding outputs could
be defined, an expected BLEU could be optimized
using gradient ascent. However, this posterior dis-
tribution can only be approximated by expensive
sampling methods.
A differentiable objective function over N -best
lists to approximate the BLEU score can be de-
fined using expected BLEU statistics and a con-
tinuous approximation of the brevity penalty. The
posterior probability for hypothesis j of segment i
is simply the normalized decoder score:
pij =
e?Sij
?
k e?Sik
(4)
where ? is a posterior scaling factor and Sij is the
total score of hypothesis j of segment i. The pos-
terior scaling factor controls the shape of the pos-
terior distribution: ? > 1.0 moves the probability
mass toward the 1-best hypothesis and ? < 1.0
flattens the distribution. The BLEU statistics in
Equation 3 are replaced by the expected statistics;
for example, m?ni =
?
j pijmij , and the brevity
penalty ?(x) is approximated by:
?(x) =
ex ? 1
e1000x + 1
+ 1 (5)
ExpBLEU has a closed form solution for the gra-
dient, provided the total decoder score is differen-
tiable.
The penalty used to restrict the search weights
corresponding to the system weights ?n in
gradient-free BLEU tuning is not differentiable.
For expected BLEU tuning, the search weights ?n
are unrestricted but the system weights are ob-
tained by a sigmoid transform and normalized to
sum to one:
?n =
?(?n)
?
m ?(?m)
(6)
where ?(?n) = 1/(1 + e??n).
The expected BLEU tuning is performed on N -
best lists in similar fashion to direct BLEU tuning.
Tuned weights from one decoding iteration are
used to generate a new N -best list, the new N -best
list is merged with the N -best list from the previ-
ous tuning run, and a new set of weights are op-
timized using limited memory Broyden-Fletcher-
Goldfarb-Shanno method (lBFGS) (Liu and No-
cedal, 1989). Since the posterior distribution is
affected by the size of the N -best list and differ-
ent decoding weights, the posterior scaling factor
can be set for each tuning run so that the perplex-
ity of the posterior distribution given the merged
N -best list is constant. A target perplexity of 5.0
was used in the experiments. Four iterations of
bi-gram decoding weight tuning were performed
using 300-best lists. The final 300-best list was re-
scored with a 5-gram and another set of re-scoring
weights was tuned on the development set.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined. Unpruned En-
glish bi-gram and 5-gram language model com-
ponents were trained using the WMT10 corpora:
EuroParl, GigaFrEn, NewsCommentary,
and News. Additional six Gigaword v4 com-
ponents were trained: AFP, APW, XIN+CNA,
324
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.99 13.85 68.45 15.07 60.86 21.02 71.17 15.00
best 56.77 22.84 57.76 25.05 51.81 30.10 53.66 28.64
syscomb 57.31 25.11 54.97 27.75 50.46 31.54 51.35 31.16
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 68.65 14.29 67.50 15.66 60.52 21.86 68.36 16.82
best 56.13 23.56 58.12 24.34 51.45 30.56 52.16 29.79
syscomb 56.89 25.12 55.60 26.38 50.33 31.59 51.36 30.16
Table 1: Case insensitive TER and BLEU scores on syscombtune (tune) and syscombtest (test)
for combinations of outputs from four source languages.
LTW, NYT, and Headlines+Datelines. In-
terpolation weights for the ten components
were tuned so as to minimize perplexity on
the newstest2009-ref.en development set.
The LMs used modified Kneser-Ney smoothing.
On the multi-source condition (xx-en) another
LM was trained from the system outputs and in-
terpolated with the general LM using an interpola-
tion weight 0.3 for the LM trained on the system
outputs. This LM is referred to as biasLM later.
A tri-gram true casing model was trained using all
available English data. This model was used to
restore the case of the lower-case system combi-
nation output.
All six 1-best system outputs on cz-en, 16
outputs on de-en, 8 outputs on es-en, and
14 outputs on fr-en were combined. The lat-
tice based BLEU tuning was used to optimize the
bi-gram decoding weights and N-best list based
BLEU tuning was used to optimize the 5-gram re-
scoring weights. Results for these single source
language experiments are shown in Table 1. The
gains on syscombtune were similar to those on
syscombtest for all but French-English. The
tuning set contained only 455 segments but ap-
peared to be well matched with the larger (2034
segments) test set. The characteristics of the indi-
vidual system outputs were probably different for
the tuning and test sets on French-English transla-
tion. In our experience, optimizing system com-
bination weights using the ExpBLEU tuning for
a small number of systems yields similar results
to lattice based BLEU tuning. The lattice based
BLEU tuning is faster as there is no need for mul-
tiple decoding and tuning iterations. Using the bi-
asLM on the single source combinations did not
xx-en tune test
System TER BLEU TER BLEU
worst 71.17 13.85 68.65 14.29
best 51.81 30.10 51.45 30.56
lattice 43.15 35.72 43.79 35.29
expBLEU 44.07 36.91 44.35 36.62
+biasLM 43.63 37.61 44.50 37.12
Table 2: Case insensitive TER and BLEU scores
on syscombtune (tune) and syscombtest
(test) for xx-en combination. Combinations us-
ing lattice BLEU tuning, expected BLEU tuning,
and after adding the system output biased LM are
shown.
yield any gains. The output for these conditions
probably did not contain enough data for biasLM
training given the small tuning set and small num-
ber of systems.
Finally, experiments combining all 44 1-best
system outputs were performed to produce a
multi-source combination output. The first experi-
ment used the lattice based BLEU tuning and gave
a 5.6 BLEU point gain on the tuning set as seen in
Table 2. The ExpBLEU tuning gave an additional
1.2 point gain which suggests that the direct lattice
based BLEU tuning got stuck in a local optimum.
Using the system output biased LM gave an addi-
tional 0.7 point gain. The gains on the test set were
similar and the best combination gave a 6.6 point
gain over the best individual system.
5 Conclusions
The BBN submissions for WMT10 system com-
bination task were described in this paper. The
combination was based on confusion network de-
325
coding. The confusion networks were built us-
ing an incremental hypothesis alignment algo-
rithm with flexible matching. The bi-gram de-
coding weights for the single source conditions
were optimized directly to maximize the BLEU
scores of the 1-best decoding outputs and the 5-
gram re-scoring weights were tuned on 300-best
lists. The BLEU gains over the best individual
system outputs were around 1.5 points on cz-en,
2.0 points on de-en, 1.0 points on es-en, and
0.4 points on fr-en. The system combination
weights on xx-en were tuned to maximize Exp-
BLEU, and a system output biased LM was used.
The BLEU gain over the best individual system
was 6.6 points. Future work will investigate tuning
of the edit costs used in the alignment. A lattice
based ExpBLEU tuning will be investigated. Also,
weights for more complicated functions with addi-
tional features may be tuned using ExpBLEU.
Acknowledgments
This work was supported by DARPA/IPTO Con-
tract No. HR0011-06-C-0022 under the GALE
program.
References
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Math-
ematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318.
Adam Pauls, John DeNero, and DanKlein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2007. Numerical
recipes: the art of scientific computing. Cambridge
University Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 61?65.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259?268.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing,
pages 620?629.
326
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 428?437,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Decision Trees for Lexical Smoothing in Statistical Machine
Translation
Rabih Zbib
?
and Spyros Matsoukas and Richard Schwartz and John Makhoul
BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
? Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA
Abstract
We present a method for incorporat-
ing arbitrary context-informed word at-
tributes into statistical machine trans-
lation by clustering attribute-qualified
source words, and smoothing their
word translation probabilities using bi-
nary decision trees. We describe two
ways in which the decision trees are
used in machine translation: by us-
ing the attribute-qualified source word
clusters directly, or by using attribute-
dependent lexical translation probabil-
ities that are obtained from the trees,
as a lexical smoothing feature in the de-
coder model. We present experiments
using Arabic-to-English newswire data,
and using Arabic diacritics and part-of-
speech as source word attributes, and
show that the proposed method im-
proves on a state-of-the-art translation
system.
1 Introduction
Modern statistical machine translation (SMT)
models, such as phrase-based SMT or hierar-
chical SMT, implicitly incorporate source lan-
guage context. It has been shown, however,
that such systems can still benefit from the
explicit addition of lexical, syntactic or other
kinds of context-informed word features (Vick-
rey et al, 2005; Gimpel and Smith, 2008;
Brunning et al, 2009; Devlin, 2009). But the
benefit obtained from the addition of attribute
information is in general countered by the in-
crease in the model complexity, which in turn
results in a sparser translation model when es-
timated from the same corpus of data. The
increase in model sparsity usually results in a
deterioration of translation quality.
In this paper, we present a method for using
arbitrary types of source-side context-informed
word attributes, using binary decision trees to
deal with the sparsity side-effect. The deci-
sion trees cluster attribute-dependent source
words by reducing the entropy of the lexi-
cal translation probabilities. We also present
another method where, instead of clustering
the attribute-dependent source words, the de-
cision trees are used to interpolate attribute-
dependent lexical translation probability mod-
els, and use those probabilities to compute a
feature in the decoder log-linear model.
The experiments we present in this paper
were conducted on the translation of Arabic-
to-English newswire data using a hierarchical
system based on (Shen et al, 2008), and using
Arabic diacritics (see section 2.3) and part-of-
speech (POS) as source word attributes. Pre-
vious work that attempts to use Arabic dia-
critics in machine translation runs against the
sparsity problem, and appears to lose most of
the useful information contained in the dia-
critics when using partial diacritization (Diab
et al, 2007). Using the methods proposed
in this paper, we manage to obtain consistent
improvements from diacritics against a strong
baseline. The methods we propose, though,
are not restrictive to Arabic-to-English trans-
lation. The same techniques can also be used
with other language pairs and arbitrary word
attribute types. The attributes we use in the
described experiments are local; but long dis-
tance features can also be used.
In the next section, we review relevant pre-
vious work in three areas: Lexical smoothing
and lexical disambiguation techniques in ma-
chine translation; using decision trees in nat-
ural language processing, and especially ma-
chine translation; and Arabic diacritics. We
present a brief exposition of Arabic orthogra-
428
phy, and refer to previous work on automatic
diacritization of Arabic text. Section 3 de-
scribes the procedure for constructing the deci-
sion trees, and the two methods for using them
in machine translation. In section 4 we de-
scribe the experimental setup and present ex-
perimental results. Finally, section 5 concludes
the paper and discusses future directions.
2 Previous Work
2.1 Lexical Disambiguation and
Lexical Smoothing
Various ways have been proposed to improve
the lexical translation choices of SMT systems.
These approaches typically incorporate local
context information, either directly or indi-
rectly.
The use of Word Sense Disambiguation
(WSD) has been proposed to enhance ma-
chine translation by disambiguating the source
words (Cabezas and Resnick, 2005; Carpuat
and Wu, 2007; Chan et al, 2007). WSD
usually requires that the training data be la-
beled with senses, which might not be avail-
able for many languages. Also, WSD is tra-
ditionally formulated as a classification prob-
lem, and therefore does not naturally lend it-
self to be integrated into the generative frame-
work of machine translation. Carpuat and Wu
(2007) formulate the SMT lexical disambigua-
tion problem as a WSD task. Instead of learn-
ing from word sense corpora, they use the SMT
training data, and use local context features to
enhance the lexical disambiguation of phrase-
based SMT.
Sarikaya et al (2007) incorporate context
more directly by using POS tags on the target
side to model word context. They augmented
the target words with POS tags of the word
itself and its surrounding words, and used the
augmented words in decoding and for language
model rescoring. They reported gains on Iraqi-
Arabic-to-English translation.
Finally, using word-to-word context-free lex-
ical translation probabilities has been shown
to improve the performance of machine trans-
lation systems, even those using much more
sophisticated models. This feature, usually
called lexical smoothing, has been used in
phrase-based systems (Koehn et al, 2003).
Och et al (2004) also found that including
IBM Model 1 (Brown et al, 1993) word prob-
abilities in their log-linear model works better
than most other higher-level syntactic features
at improving the baseline. The incorporation
of context on the source or target side en-
hances the gain obtained from lexical smooth-
ing. Gimpel and Smith (2008) proposed us-
ing source-side lexical features in phrase-based
SMT by conditioning the phrase probabilities
on those features. They used word context,
syntactic features or positional features. The
features were added as components into the
log-linear decoder model, each with a tunable
weight. Devlin (2009) used context lexical fea-
tures in a hierarchical SMT system, interpolat-
ing lexical counts based on multiple contexts.
It also used target-side lexical features.
The work in the paper incorporates con-
text information based on the reduction of the
translation probability entropy.
2.2 Decision Trees
Decision trees have been used extensively in
various areas of machine learning, typically
as a way to cluster patterns in order to im-
prove classification (Duda et al, 2000). They
have, for instance, been long used success-
fully in speech recognition to cluster context-
dependent phoneme model states (Young et
al., 1994).
Decision trees have also been used in ma-
chine translation, although to a lesser extent.
In this respect, our work is most similar to
(Brunning et al, 2009), where the authors ex-
tended word alignment models for IBM Model
1 and Hidden Markov Model (HMM) align-
ments. They used decision trees to cluster the
context-dependent source words. Contexts be-
longing to the same cluster were grouped to-
gether during Expectation Maximization (EM)
training, thus providing a more robust proba-
bility estimate. While Brunning et al (2009)
used the source context clusters for word align-
ments, we use the attribute-dependent source
words directly in decoding. The approach we
propose can be readily used with any align-
ment model.
Stroppa et al (2007) presented a general-
ization of phrase-based SMT (Koehn et al,
2003) that also takes into account source-
side context information. They conditioned
the target phrase probability on the source
429
phrase as well as source phrase context, such
as bordering words, or part-of-speech of bor-
dering words. They built a decision tree for
each source phrase extracted from the train-
ing data. The branching of the tree nodes
was based on the different context features,
branching on the most class-discriminative fea-
tures first. Each node is associated with the
set of aligned target phrases and correspond-
ing context-conditioned probabilities. The de-
cision tree thus smoothes the phrase probabil-
ities based on the different features, allowing
the model to back off to less context, or no
context at all depending on the presence of
that context-dependent source phrase in the
training data. The model, however, did not
provide for a back-off mechanism if the phrase
pair was not found in the extracted phrase ta-
ble. The method presented in this paper differs
in various aspects. We use context-dependent
information at the source word level, rather
than the phrase level, thus making it readily
applicable to any translation model and not
just phrase-based translation. By incorporat-
ing context at the word level, we can decode
directly with attribute-augmented source data
(see section 3.2).
2.3 Arabic Diacritics
Since an important part of the experiments
described in this paper use diacritized Arabic
source, we present a brief description of Arabic
orthography, and specifically diacritics.
The Arabic script, like that of most other
Semitic languages, only represents consonants
and long vowels using letters
1
. Short vowels
can be written as small marks written above
or below the preceding consonant, called di-
acritics. The diacritics are, however, omit-
ted from written text, except in special cases,
thus creating an additional level of lexical am-
biguity. Readers can usually guess the cor-
rect pronunciation of words in non-diacritized
text from the sentence and discourse context.
Grammatical case on nouns and adjectives are
also marked using diacritics at the end of
words. Arabic MT systems use undiacritized
text, since most available Arabic data is undi-
acritized.
1
Such writing systems are sometimes referred to as
Abjads (See Daniels, Peter T., et al eds. The World's
Writing Systems Oxford. (1996), p.4.)
Automatic diacritization of Arabic has been
done with high accuracy, using various genera-
tive and discriminative modeling techniques.
For example, Ananthakrishnan et al (2005)
used a generative model that incorporates
word level n-grams, sub-word level n-grams
and part-of-speech information to perform di-
acritization. Nelken and Shieber (2005) mod-
eled the generative process of dropping dia-
critics using weighted transducers, then used
Viterbi decoding to find the most likely gener-
ator. Zitouni et al (2006) presented a method
based on maximum entropy classifiers, us-
ing features like character n-grams, word n-
grams, POS and morphological segmentation.
Habash and Rambow (2007) determined vari-
ous morpho-syntactic features of the word us-
ing SVM classifiers, then chose the correspond-
ing diacritization. The experiments in this
paper use the automatic diacritizer by Sakhr
Software. The diacritizer determines word di-
acritics through rule-based morphological and
syntactic analysis. It outputs a diacritization
for both the internal stem and case ending
markers of the word, with an accuracy of 97%
for stem diacritization and 91% for full dia-
critization (i.e., including case endings).
There has been work done on using dia-
critics in Automatic Speech Recognition, e.g.
(Vergyri and Kirchhoff, 2004). However, the
only previous work on using diacritization for
MT is (Diab et al, 2007), which used the di-
acritization system described in (Habash and
Rambow, 2007). It investigated the effect
of using full diacritization as well as partial
diacritization on MT results. The authors
found that using full diacritics deteriorates MT
performance. They used partial diacritiza-
tion schemes, such as diacritizing only passive
verbs, keeping the case endings diacritics, or
only gemination diacritics. They also saw no
gain in most configurations. The authors ar-
gued that the deterioration in performance is
caused by the increase in the size of the vo-
cabulary, which in turn makes the translation
model sparser; as well as by errors during the
automatic diacritization process.
430
3 Decision Trees for Source Word
Attributes
3.1 Growing the Decision Tree
In this section, we describe the procedure
for growing the decision trees using context-
informed source word attributes.
The attribute-qualified source-side of the
parallel training data is first aligned to the
target-side data. If S is the set of attribute-
dependent forms of source word s, and tj is a
target word aligned to si ? S, then we define:
p (tj |si) =
count(si,tj)
count(si)
(1)
where count(si, tj) is the count of alignment
links between si and tj .
A separate binary decision tree is grown for
each source word. We start by including all the
attribute-dependent forms of the source word
at the root of the tree. We split the set of at-
tributes at each node into two child nodes, by
choosing the splitting that maximizes the re-
duction in weighted entropy of the probability
distribution in (1). In other words, at node n,
we choose the partition (S?1 , S
?
2) such that:
(S?1 , S
?
2) =
argmax
(S1,S2)
S1?S2=S
{h(S)? (h(S1) + h(S2))}
(2)
where h(S) is the entropy of the probabil-
ity distribution p(tj |si ? S), weighted by the
number of samples in the training data of the
source words in S. We only split a node if the
entropy is reduced by more than a threshold
?h. This step is repeated recursively until the
tree cannot be grown anymore.
Weighting the entropy by the source word
counts gives more weight to the context-
dependent source words with a higher number
of samples in the training data, sine the lex-
ical translation probability estimates for fre-
quent words can be trusted better. The ratio-
nale behind the splitting criterion used is that
the split that reduces the entropy of the lexical
translation probability distribution the most
is also the split that best separates the list of
forms of the source word in terms of the target
word translation. For a source word that has
multiple meanings, depending on its context,
the decision tree will tend to implicitly sepa-
rate those meanings using the information in
the lexical translation probabilities.
Although we describe this method as grow-
ing one decision tree for each word, and using
one attribute type at a time, a decision tree
can clearly be constructed for multiple words,
and more than one attribute type can be used
in the same decision tree.
3.2 Trees for Source Word Clustering
The source words could be augmented to ex-
plicitly incorporate the word attributes (dia-
critics or other attribute types). The aug-
mented source will be less ambiguous if the
attributes do in fact contain disambiguating
information. This, in principle, helps machine
translation performance. The flip side is that
the resulting increase in vocabulary size in-
creases the translation model sparsity, usually
with a detrimental effect on translation.
To mitigate the effect of the increase in vo-
cabulary, decision trees can be use to cluster
the attribute-augmented source words. More
specifically, a decision tree is grown for each
source word as described in the previous sec-
tion, using a predefined entropy threshold ?h.
When the tree cannot be expanded anymore,
its leaf nodes will contain a multi-set parti-
tioning of the list of attribute-dependent forms
of that source word. Each of the clusters is
treated as an equivalence class, and all forms
in that class are mapped to a unique form (e.g.
an arbitrarily chosen member of the cluster).
The mappings are used to map the tokens in
the parallel training data before alignment is
run on the mapped data. The test data is
also mapped consistently. This clustering pro-
cedure will only keep the attribute-dependent
forms of the source words that decrease the un-
certainty in the translation probabilities, and
are thus useful for translation.
The experiments we report on use diacritics
as an attribute type. The various diacritized
forms of a source word are thus used to train
the decision trees. The resulting clusters are
used to map the data into a subset of the vo-
cabulary that is used in translation training
and decoding (see section 4.2 for results). Di-
acritics are obviously specific to Arabic. But
this method can be used with other attribute
types, by first appending the source words with
431
{sijo
na,s
ijni}sjn
{sijo
na,s
ijni,
sajo
na,s
ajon
u,sa
jana
} {saj
ana
}
{saj
ona
,sajo
nu}
Figure 1: Decision tree for source word sjn using
diacritics as an attribute.
their context (e.g. attach to each source word
its part-of-speech tag or context), and then
training decision trees and mapping the source
side of the data.
Figure 1 shows an example of a decision
tree for the Arabic word sjn
2
using diacritics
as a source attribute. The root contains the
various diacritized forms (sijona `prison AC-
CUSATIVE', sijoni `prison DATIVE', sajona
`imprisonment ACCUSATIVE.', sajoni `im-
prisonment ACCUSATIVE.', sajana `he im-
prisoned '). The leaf nodes contain the
attribute-dependent clusters.
3.3 Trees for Lexical Smoothing
As mentioned in section 2.1, lexical smoothing,
computed from word-to-word translation prob-
abilities, is a useful feature, even in SMT sys-
tems that use sophisticated translation mod-
els. This is likely due to the robustness of
context-free word-to-word translation proba-
bility estimates compared to the probabilities
of more complicated models. In those models,
the rules and probabilities are estimated from
much larger sample spaces.
In our system, the lexical smoothing feature
is computed as follows:
f(U)=
?
tj?T (U)
(
1?
?
si?{S(U)?NULL}
(1?p?(tj |si))
)
(3)
where U is the modeling unit specific to the
translation model used. For a phrase-based
system, U is the phrase pair, and for a hierar-
chical system U is the translation rule. S (U)
2
Examples are written using Buckwalter transliter-
ation.
sjn
{sijo
na,s
ijni,
sajo
na,s
ajon
u,sa
jana
} {s
ajan
a}
{sijo
na}
{sijo
ni}
{saj
ona
}
{saj
onu
}
{sijo
na}
{sijo
ni}Figure 2: Decision tree for source word sjn grown
fully using diacritics.
is the set of terminals on the source side of U,
and T (U) is the set of terminals on its tar-
get. The NULL term in the equation above
accounts for unaligned target words, which we
found in our experiments to be beneficial. One
way of interpreting equation (3) is that f (U)
is the probability that for each target word tj
in U, tj is a likely translation of at least one
word si on the source side. The feature value
is then used as a component in the log-linear
model, with a tunable weight.
In this work, we generalize the lexical
smoothing feature to incorporate the source
word attributes. A tree is grown for each
source word as described in section 3.1, but
using an entropy threshold ?h = 0. In other
words, the tree is grown all the way until each
leaf node contains one attribute-dependent
form of the source word. Each node in the
tree contains a cluster of attribute-dependent
forms of the source word, and a corresponding
attribute-dependent lexical translation prob-
ability distribution. The lexical translation
probability models at the root nodes are those
of the regular attribute-independent lexical
translation probabilities. The models at the
leaf nodes are the most fine-grained, since they
are conditioned on only one attribute value.
Figure 2 shows a fully grown decision tree for
the same source word as the example in Figure
1.
The lexical probability distribution at the
leafs are from sparser data than the original
distributions, and are therefore less robust. To
address this, the attribute-dependent lexical
432
smoothing feature is estimated by recursively
interpolating the lexical translation probabil-
ities up the tree. The probability distribu-
tion pn at each node n is interpolated with
the probability of its parent node as follows:
pn =
{
pn if n is root,
wnpn + (1? wn)pm otherwise
where m is the parent of n
(4)
A fraction of the parent probability mass is
thus given to the probability of the child node.
If the probability estimate of an attribute-
dependent form of a source word with a cer-
tain target word t is not reliable, or if the
probability estimate is 0 (because the source
word in this context is not aligned with t),
then the model gracefully backs off by using
the probability estimates from other attribute-
dependent lexical translation probability mod-
els of the source word.
The interpolation weight is a logistic regres-
sion function of the source word count at a
node n:
wn =
1
1 + e???? log(count(Sn))
(5)
The weight varies depending on the count
of the attribute-qualified source word in each
node, thus reflecting the confidence in the es-
timates of each node's distribution. The two
global parameters of the function, a bias ? and
a scale ? are tuned to maximize the likelihood
of a set of alignment counts from a heldout
data set of 179K sentences. The tuning is done
using Powell's method (Brent, 1973).
During decoding, we use the probability dis-
tribution at the leaves to compute the feature
value f(R) for each hierarchical rule R. We
train and decode using the regular, attribute-
independent source. The source word at-
tributes are used in the decoder only to in-
dex the interpolated probability distribution
needed to compute f (R).
4 Experiments
4.1 Experimental Setup
As mentioned before, the experiments we re-
port on use a string-to-dependency-tree hier-
archical translation system based on the model
described in (Shen et al, 2008). Forward and
Likelihood %
baseline -1.29 -
Diacs.
dec. trees
-1.25 +2.98%
POS dec.
trees
-1.24 +3.41%
Table 1: Normalized likelihood of the test set algn-
ments without decision trees, then with decision trees
using diacritics and part-of-speech respectively.
backward context-free lexical smoothing are
used as decoder features in all the experiments.
Other features such as rule probabilities and
dependency tree language model (Shen et al,
2008) are also used. We use GIZA++ (Och
and Ney, 2003) for word alignments. The de-
coder model parameters are tuned using Mini-
mum Error Rate training (Och, 2003) to max-
imize the IBM BLEU score (Papineni et al,
2002).
For training the alignments, we use 27M
words from the Sakhr Arabic-English Paral-
lel Corpus (SSUSAC27). The language model
uses 7B words from the English Gigaword and
from data collected from the web. A 3-gram
language model is used during decoding. The
decoder produces an N-best list that is re-
ranked using a 5-gram language model.
We tune and test on two separate data sets
consisting of documents from the following col-
lections: the newswire portion of NIST MT04,
MT05, MT06, and MT08 evaluation sets, the
GALE Phase 1 (P1) and Phase 2 (P2) evalu-
ation sets, and the GALE P2 and P3 develop-
ment sets. The tuning set contains 1994 sen-
tences and the test set contains 3149 sentences.
The average length of sentences is 36 words.
Most of the documents in the two data sets
have 4 reference translations, but some have
only one. The average number of reference
translations per sentence is 3.94 for the tun-
ing set and 3.67 for the test set.
In the next section, we report on measure-
ments of the likelihood of test data, and de-
scribe the translation experiments in detail.
4.2 Results
In order to assess whether the decision trees
are in fact helpful in decreasing the uncer-
tainty in the lexical translation probabilities
433
54.254.354.454.554.654.754.854.955
MT Score  in BLEU
5454.154.254.354.454.554.654.754.854.955
0
25
50
100
Entr
opy T
hresh
old
Figure 3: BLEU scores of the clustering experiments
as a function of the entropy threshold on tuning set.
on unseen data, we compute the likelihood
of the test data with respect to these prob-
abilities with and without the decision tree
splitting. We align the test set with its ref-
erence using GIZA++, and then obtain the
link count l_count(si, tj) for each alignment
link i = (si,ti) in the set of alignment links I.
We calculate the normalized likelihood of the
alignments:
L = log
?
?
(
?
i
p(ti | si)
l_count(si,ti)
) 1
|I|
?
?
=
1
|I|
?
i?I
l_count(si, ti) log p? (ti | si) (6)
where p? (ti | si) is the probability for the word
pair (ti, si) in equation (4). If the same in-
stance of source word si is aligned to two tar-
get words ti and tj , then these two links are
counted separately. If a source in the test set
is out-of-vocabulary, or if a word pair (ti, si)
is aligned in the test alignment but not in the
training alignments (and thus has no probabil-
ity estimate), then it is ignored in the calcula-
tion of the log-likelihood.
Table 1 shows the likelihood for the baseline
case, where one lexical translation probability
distribution is used per source word. It also
shows the likelihoods calculated using the lex-
ical distributions in the leaf nodes of the de-
cision trees, when either diacritics or part-of-
speech are used as an attribute type. The table
shows an increase in the likelihood of 2.98% us-
ing diacritics, and 3.41% using part-of-speech.
The translation result tables present MT
scores in two different metrics: Translation
Edit Rate (Snover et al, 2006) and IBM
TER BLEU
Test
baseline 40.14 52.05
full diacritics 40.31 52.39
+0.17 +0.34
dec. trees, diac (?h = 50) 39.75 52.60
-0.39 +0.55
Table 2: Results of experiments using decision trees
to cluster source words.
BLEU. The reader is reminded that a higher
BLEU score and a lower TER are desired. The
tables also show the difference in scores be-
tween the baseline and each experiment. It is
worth noting that the gains reported are rela-
tive to a strong baseline that uses a state-of-
the-art system with many features, and a fairly
large training corpus.
The decision tree clustering experiment as
described in section 3.2 depends on a global
parameter, namely the threshold in entropy re-
duction ?h. We tune this parameter manually
on a tuning set. Figure 3 shows the BLEU
scores as a function of the threshold value, with
diacritics as an attribute type. The most gain
is obtained for an entropy threshold of 50.
The fully diacritized data has an average of
1.78 diacritized forms per source word. The av-
erage weighted by the number of occurrences is
6.28, which indicates that words with more di-
acritized forms tend to occur more frequently.
After clustering using a value of ?h = 50,
the average number of diacritized forms be-
comes 1.11, and the occurrence weighted av-
erage becomes 3.69. The clustering proce-
dure thus seems to eliminate most diacritized
forms, which likely do not contain helpful dis-
ambiguating information.
Table 2 lists the detailed results of experi-
ments using diacritics. In the first experiment,
we show that using full diacritization results in
a small gain on the BLEU score and no gain on
TER, which is somewhat consistent with the
result obtained by Diab et al (2007). The next
experiment shows the results of clustering the
diacritized source words using decision trees
for the entropy threshold of 50. The TER loss
of the full diacritics becomes a gain, and the
BLEU gain increases. This confirms our spec-
ulation that the use of fully diacritized data in-
434
TER BLEU
Test
baseline 40.14 52.05
dec. trees, diacs 39.75 52.55
-0.39 +0.50
dec. trees, POS 40.05 52.40
-0.09 +0.35
dec. trees, diacs, no interpolation 39.98 52.09
-0.16 +0.04
Table 3: Results of experiments using the word attribute-dependent lexical smoothing feature.
creases the model sparsity, which undoes most
of the benefit obtained from the disambiguat-
ing information that the diacritics contain. Us-
ing the decision trees to cluster the diacritized
source data prunes diacritized forms that do
not decrease the entropy of the lexical trans-
lation probability distributions. It thus finds
a sweet-spot between the negative effect of in-
creasing the vocabulary size and the positive
effect of disambiguation.
In our experiments, using diacritics with
case endings gave consistently better score
than using diacritics with no case endings, de-
spite the fact that they result in a higher vo-
cabulary size. One possible explanation is that
diacritics not only help in lexical disambigua-
tion, but they might also be indirectly help-
ing in phrase reordering, since the diacritics on
the final letter indicate the word's grammatical
function.
The results from using decision trees to in-
terpolate attribute-dependent lexical smooth-
ing features are summarized in table 3. In
the first experiment, we show the results of
using diacritics to estimate the interpolated
lexical translation probabilities. The results
show a gain of +0.5 BLEU points and 0.39
TER points. The gain is statistically signifi-
cant with a 95% confidence level. Using part-
of-speech as an attribute gives a smaller, but
still statistically significant gain. We also ran
a control experiment, where we used diacritic-
dependent lexical translation probabilities ob-
tained from the decision trees, but did not per-
form the probability interpolation of equation
(4). The gains mostly disappear, especially on
BLEU, showing the importance of the inter-
polation step for the proper estimation of the
lexical smoothing feature.
5 Conclusion and Future Directions
We presented in this paper a new method for
incorporating explicit context-informed word
attributes into SMT using binary decision
trees. We reported on experiments on Arabic-
to-English translation using diacritized Ara-
bic and part-of-speech as word attributes, and
showed that the use of these attributes in-
creases the likelihood of source-target word
pairs of unseen data. We proposed two spe-
cific ways in which the results of the decision
tree training process are used in machine trans-
lation, and showed that they result in better
translation results.
For future work, we plan on using multi-
ple source-side attributes at the same time.
Different attributes could have different dis-
ambiguating information, which could pro-
vide more benefit than using any of the at-
tributes alone. We also plan on investigat-
ing the use of multi-word trees; trees for word
clusters can for instance be grown instead
of growing a separate tree for each source
word. Although the experiments presented
in this paper use local word attributes, noth-
ing in principle prevents this method from be-
ing used with long-distance sentence context,
or even with document-level or discourse-level
features. Our future plans include the investi-
gation of using such features as well.
Acknowledgment
This work was supported by DARPA/IPTO
Contract No. HR0011-06-C-0022 under the
GALE program.
The views, opinions, and/or findings con-
tained in this article are those of the author
and should not be interpreted as representing
the official views or policies, either expressed
435
or implied, of the Defense Advanced Research
Projects Agency or the Department of Defense.
A pproved for Public Release, Distribution Un-
limited.
References
S. Ananthakrishnan, S. Narayanan, and S. Ban-
galore. 2005. Automatic diacritization of ara-
bic transcripts for automatic speech recognition.
Kanpur, India.
R. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
P. Brown, V. Della Pietra, S. Della Pietra, and
R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263311.
J. Brunning, A. de Gispert, and W. Byrne. 2009.
Context-dependent alignment models for statis-
tical machine translation. In NAACL '09: Pro-
ceedings of the 2009 Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguis-
tics, pages 110118.
C. Cabezas and P. Resnick. 2005. Using WSD
techniques for lexical selection in statistical ma-
chine translation. In Technical report, Insti-
tute for Advanced Computer Studies (CS-TR-
4736, LAMP-TR-124, UMIACS-TR-2005-42),
College Park, MD.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense dis-
ambiguation. In EMNLP-CoNLL-2007: Pro-
ceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning,
Prague, Czech Republic.
Y. Chan, H. Ng, and D. Chiang. 2007. Word
sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
J. Devlin. 2009. Lexical features for statistical
machine translation. Master's thesis, University
of Maryland, December 2009.
M. Diab, M. Ghoneim, and N. Habash. 2007. Ara-
bic diacritization in the context of statistical ma-
chine translation. InMT Summit XI, pages 143
149, Copenhagen, Denmark.
R. O. Duda, P. E. Hart, and D. G. Stork. 2000.
Pattern Classification. Wiley-Interscience Pub-
lication.
K. Gimpel and N. A. Smith. 2008. Rich source-
side context for statistical machine translation.
In StatMT '08: Proceedings of the Third Work-
shop on Statistical Machine Translation, pages
917, Columbus, Ohio.
N. Habash and O. Rambow. 2007. Arabic diacriti-
zation through full morphological tagging. In
Proceedings of the 2007 Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics, pages 5356, Rochester, New York.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
the 2003 Human Language Technology Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics, pages
4854, Edmonton, Canada.
R. Nelken and S. M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transduc-
ers. In Proceedings of the 2005 ACL Workshop
on Computational Approaches to Semitic Lan-
guages, Ann Arbor, Michigan.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):1951.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,
K. Yamada, A. Fraser, S. Kumar, L. Shen,
D. Smith, K. Eng, V. Jain, Z. Jin, and D. R.
Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT-NAACL,
pages 161168.
F. J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), Sapporo,
Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia,
PA.
Ruhi Sarikaya, Yonggang Deng, and Yuqing Gao.
2007. Context dependent word modeling for sta-
tistical machine translation using part-of-speech
tags. In Proceedings of INTERSPEECH 2007fs,
Antwerp, Belgium.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algo-
rithm with a target dependency language model.
In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics
(ACL), Columbus, Ohio.
M. Snover, B. Dorr, R. Schwartz, J. Makhoul, and
L. Micciulla. 2006. A study of translation error
436
rate with targeted human annotation. In Pro-
ceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA
2006), pages 223231, Cambridge, MA.
N. Stroppa, A. van den Bosch, and A Way.
2007. Exploiting source similarity for SMT us-
ing context-informed features. In Proceedings of
the 11th International Conference on Theoreti-
cal and Methodological Issues in Machine Trans-
lation (TMI-07), pages 231240.
D. Vergyri and K. Kirchhoff. 2004. Automatic
diacritization of arabic for acoustic modeling in
speech recognition. In Semitic '04: Proceedings
of the Workshop on Computational Approaches
to Arabic Script-based Languages, pages 6673,
Geneva, Switzerland.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005. Word-sense disambiguation for machine
translation. In HLT '05: Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Pro-
cessing, Vancouser, BC, Canada.
S.J. Young, J.J. Odell, and P.C. Woodland. 1994.
Tree-based state tying for high accuracy acoustic
modelling. In HLT'94: Proceedings of the Work-
shop on Human Language Technology, pages
307312.
I. Zitouni, J. S. Sorensen, and Ruhi Sarikaya. 2006.
Maximum entropy based restoration of arabic
diacritics. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and the 44th annual meeting of the Association
for Computational Linguistics, pages 577584,
Sydney, Australia.
437
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 159?165,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Expected BLEU Training for Graphs: BBN System Description for
WMT11 System Combination Task
Antti-Veikko I. Rosti? and Bing Zhang and Spyros Matsoukas and Richard Schwartz
Raytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138, USA
{arosti,bzhang,smatsouk,schwartz}@bbn.com
Abstract
BBN submitted system combination outputs
for Czech-English, German-English, Spanish-
English, and French-English language pairs.
All combinations were based on confusion
network decoding. The confusion networks
were built using incremental hypothesis align-
ment algorithm with flexible matching. A
novel bi-gram count feature, which can penal-
ize bi-grams not present in the input hypothe-
ses corresponding to a source sentence, was
introduced in addition to the usual decoder
features. The system combination weights
were tuned using a graph based expected
BLEU as the objective function while incre-
mentally expanding the networks to bi-gram
and 5-gram contexts. The expected BLEU
tuning described in this paper naturally gen-
eralizes to hypergraphs and can be used to
optimize thousands of weights. The com-
bination gained about 0.5-4.0 BLEU points
over the best individual systems on the official
WMT11 language pairs. A 39 system multi-
source combination achieved an 11.1 BLEU
point gain.
1 Introduction
The confusion networks for the BBN submissions
to the WMT11 system combination task were built
using incremental hypothesis alignment algorithm
?This work was supported by DARPA/I2O Contract No.
HR0011-06-C-0022 under the GALE program (Approved for
Public Release, Distribution Unlimited). The views, opinions,
and/or findings contained in this article/presentation are those of
the author/presenter and should not be interpreted as represent-
ing the official views or policies, either expressed or implied,
of the Defense Advanced Research Projects Agency or the De-
partment of Defense.
with flexible matching (Rosti et al, 2009). A novel
bi-gram count feature was used in addition to the
standard decoder features. The N-best list based ex-
pected BLEU tuning (Rosti et al, 2010), similar to
the one proposed by Smith and Eisner (2006), was
extended to operate on word lattices. This method is
closely related to the consensus BLEU (CoBLEU)
proposed by Pauls et al (2009). The minimum oper-
ation used to compute the clipped counts (matches)
in the BLEU score (Papineni et al, 2002) was re-
placed by a differentiable function, so there was
no need to use sub-gradient ascent as in CoBLEU.
The expected BLEU (xBLEU) naturally generalizes
to hypergraphs by simply replacing the forward-
backward algorithm with inside-outside algorithm
when computing the expected n-gram counts and
sufficient statistics for the gradient.
The gradient ascent optimization of the xBLEU
appears to be more stable than the gradient-free di-
rect 1-best BLEU tuning or N -best list based min-
imum error rate training (Och, 2003), especially
when tuning a large number of weights. On the of-
ficial WMT11 language pairs with up to 30 weights,
there was no significant benefit from maximizing
xBLEU. However, on a 39 system multi-source
combination (43 weights total), it yielded a signif-
icant gain over gradient-free BLEU tuning and N -
best list based expected BLEU tuning.
2 Hypothesis Alignment and Features
The incremental hypothesis alignment with flexible
matching (Rosti et al, 2009) produces a confusion
network for each system output acting as a skele-
ton hypothesis for the ith source sentence. A con-
fusion network is a graph where all paths visit all
159
vertices. Consecutive vertices are connected by one
or more edges representing alternatives. Each edge
l is associated with a token and a set of scores. A to-
ken may be a word, punctuation symbol, or special
NULL token indicating a deletion in the alignment.
The set of scores includes a vector ofNs system spe-
cific confidences, siln, indicating whether the token
was aligned from the output of the system n.1 Other
scores may include a language model (LM) score
as well as non-NULL and NULL token indicators
(Rosti et al, 2007). As Rosti et al (2010) described,
the networks for all skeletons are connected to a start
and end vertex with NULL tokens in order to form
a joint lattice with multiple parallel networks. The
edges connecting the start vertex to the initial ver-
tices in each network have a heuristic prior estimated
from the alignment statistics at the confidence cor-
responding to the skeleton system. The edges con-
necting the final vertices of each network to the end
vertex have all system confidences set to one, so the
final edge does not change the score of any path.
A single word confidence is produced from the
confidence vector by taking an inner product with
the system weights ?n which are constrained to sum
to one,2
?
n ?n = 1. The total edge score is pro-
duced by a log-linear interpolation of the word con-
fidence with other features film:
sil = log
( Ns?
n=1
?nsiln
)
+
?
m
?mfilm (1)
The usual features film include the LM score as well
as non-NULL and NULL token indicators. Based
on an analysis of the system combination outputs, a
large number of bi-grams not present in any input
hypothesis are often produced, some of which are
clearly ungrammatical despite the LM. These novel
bi-grams are due to errors in hypothesis alignment
and the confusion network structure where any word
from the incoming edges of a vertex can be followed
by any word from the outgoing edges. After expand-
ing and re-scoring the joint lattice with a bi-gram, a
new feature indicating the presence of a novel bi-
gram may be added on the edges. A negative weight
1The confidences are binary when aligning 1-best outputs.
More elaborate confidences may be estimated fromN -best lists;
see for example Rosti et al (2007).
2See (Rosti et al, 2010) for a differentiable constraint.
for this feature discourages novel bi-grams in the
output during decoding.
3 Weight Optimization
The most common objective function used in ma-
chine translation is the BLEU-N score (Papineni et
al., 2002) defined as follows:3
BLEU =
N?
n=1
(?
i m
n
i?
i h
n
i
) 1
N
?
(
1?
?
i ri
?
i h
1
i
)
(2)
where N is the maximum n-gram order (typically
N = 4), mni is the number of n-gram matches
(clipped counts) between the hypothesis ei and ref-
erence e?i for segment i, hni is the number of n-grams
in the hypothesis, ri is the reference length,4 and
?(x) = min(1.0, ex) is the brevity penalty. Using
gn to represent an arbitrary n-gram, cign to repre-
sent the count of gn in hypothesis ei, and c?ign to
represent the count of gn in reference e?i, the BLEU
statistics can be defined as follows:
mni =
?
gn
min(cign , c?ign) (3)
hni =
?
gn
cign (4)
The unigram count h1i is simply the hypothesis
length and higher order n-gram counts can be ob-
tained by hni = h
n?1
i ? 1. The reference n-gram
counts for each sentence can be stored in an n-gram
trie for efficient scoring.5
The BLEU score is not differentiable due to the
minimum operations on the matches mni and brevity
penalty ?(x). Therefore gradient-free optimization
algorithms, such as Powell?s method or downhill
simplex (Press et al, 2007), are often employed in
weight tuning (Och, 2003). System combination
weights tuned using the downhill simplex method
to directly optimize 1-best BLEU score of the de-
coder outputs served as the first baseline in the ex-
periments. The distributed optimization approach
used here was first described in (Rosti et al, 2010).
3Superscripts indicate the n-gram order in all variables in
this paper. They are used as exponents only for the constant e.
4If multiple references are available, ri is the reference
length closest to the hypothesis length, h1i .
5If multiple references are available, the maximum n-gram
counts are stored.
160
A set of system combination weights was first tuned
for unpruned lattices re-scored with a bi-gram LM.
Another set of re-scoring weights was tuned for 300-
best lists re-scored with a 5-gram LM.
3.1 Graph expected BLEU
Gradient-free optimization algorithms work well
with a relatively small number of weights. Weight
optimization for a 44 system combination in Rosti
et al (2010) was shown to be unstable with down-
hill simplex algorithm. Instead, an N-best list based
expected BLEU tuning with gradient ascent yielded
better results. This served as the second baseline in
the experiments. The objective function is defined
by replacing the n-gram statistics with expected n-
gram counts and matches as in (Smith and Eisner,
2006), and brevity penalty with a differentiable ap-
proximation:
?(x) =
ex ? 1
1 + e1000x
+ 1 (5)
An N-best list represents a subset of the search space
and multiple decoding iterations with N-best list
merging is required to improve convergence. In this
work, expected BLEU tuning is extended for lat-
tices by replacing the minimum operation in n-gram
matches with another differentiable approximation.
The expected n-gram statistics for path j, which cor-
respond to the standard statistics in Equations 3 and
4, are defined as follows:
m?ni =
?
gn
?
( ?
j?Ji
Pijcijgn , c?ign
)
(6)
h?ni =
?
gn
?
j?Ji
Pijcijgn (7)
where Ji is the set of all paths in a lattice or all
derivations in a hypergraph for the ith source sen-
tence, Pij is the posterior of path j, and cijgn is
the count of n-grams gn in hypothesis eij on path
j. The path posterior and approximate minimum are
defined by:
Pij =
?
l?j e
?sil
?
j??Ji
?
l?j? e?sil
(8)
?(x, c) =
x? c
1 + e1000(x?c)
+ c (9)
where sil is the total score on edge l defined in Equa-
tion 1 and ? is an edge score scaling factor. The
scaling factor affects the shape of the edge posterior
distribution; ? > 1.0 makes the edge posteriors on
the 1-best path higher than edge posteriors on other
paths and ? < 1.0 makes the posteriors on all paths
more uniform.
The graph expected BLEU can be factored as
xBLEU = ePB where:
P =
1
N
N?
n=1
(
log
?
i
m?ni ? log
?
i
h?ni
)
(10)
B = ?
(
1?
?
i ri
?
i h?
1
i
)
(11)
and ri is the reference length.6 This objective func-
tion is closely related to CoBLEU (Pauls et al,
2009). Unlike CoBLEU, xBLEU is differentiable
and standard gradient ascent algorithms can be used
to find weights that maximize the objective.
Note, the expected counts can be expressed in
terms of edge posteriors as:
?
j?Ji
Pijcijgn =
?
l?Li
pil?(c
n
il, g
n) (12)
where Li is the set of all edges for the ith sentence,
pil is the edge posterior, ?(x, c) is the Kronecker
delta function which is 1 if x = c and 0 if x 6= c, and
cnil is the n-gram context of edge l. The edge posteri-
ors can be computed via standard forward-backward
algorithm for lattices or inside-outside algorithm for
hypergraphs. As with the BLEU statistics, only ex-
pected unigram counts h?1i need to be accumulated
for the hypothesis n-gram counts in Equation 7 as
h?ni = h?
n?1
i ? 1 for n > 1. Also, the expected
n-gram counts for each graph can be stored in an
n-gram trie for efficient gradient computation.
3.2 Gradient of graph expected BLEU
The gradient of the xBLEU with respect to weight ?
can be factored as:
?xBLEU
??
=
?
i
?
l?Li
?sil
??
?
j?Ji
?xBLEU
? logPij
? logPij
?sil
(13)
where the gradient of the log-path-posterior with re-
spect to the edge score is given by:
? logPij
?sil
= ?
(
?(l ? j)? pil
)
(14)
6If multiple reference are available, ri is the reference length
closest to the expected hypothesis length h?1i .
161
?xBLEU
??
= ?eP
(
B
N
N?
n=1
?
i
(
m?nik ?m
n
ik
mn
?
h?nik ? h
n
ik
hn
))
+ C??(1? C)
?
i
h?1ik ? h
1
ik
h1
(15)
and ?(l ? j) is one if edge l is on path j, and zero
otherwise. Using the factorization xBLEU = ePB,
Equation 13 can be expressed using sufficient statis-
tics as shown in Equation 15, where ??(x) is the
derivative of ?(x) with respect to x, mn =
?
i m?
n
i ,
hn =
?
i h?
n
i , C =
?
ri/
?
i h?
1
i , and the remaining
sufficient statistics are given by:
??ign = ?
?
( ?
j?Ji
Pijcijgn , c?ign
)
mnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
??igncijgn
)
m?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
??igncijgn
hnik =
(
?
l?Li
pil
?sil
??
)(
?
j?Ji
Pij
?
gn
cijgn
)
h?nik =
?
l?Li
?sil
??
?
j:l?Ji
Pij
?
gn
cijgn
where ??(x, c) is the derivative of ?(x, c) with re-
spect to x, and the parentheses in the equations for
mnik and h
n
ik signify that the second terms do not de-
pend on the edge l.
3.3 Forward-backward algorithm under
expectation semiring
The sufficient statistics for graph expected BLEU
can be computed using expectation semirings (Li
and Eisner, 2009). Instead of computing single
forward/backward or inside/outside scores, addi-
tional n-gram elements are tracked for matches and
counts. For example in a bi-gram graph, the ele-
ments for edge l are represented by a 5-tuple7 sl =
?pl, r1lh, r
2
lh, r
1
lm, r
2
lm? where pl = e
?sil and:
rnlh =
?
gn
?(cnil, g
n)e?sil (16)
rnlm =
?
gn
??igne
?sil (17)
Assuming the lattice is topologically sorted, the for-
ward algorithm8 under expectation semiring for a 3-
7The sentence index i is dropped for brevity.
8For inside-outside algorithm, see (Li and Eisner, 2009).
tuple9 sl = ?pl, r1lh, r
1
lm? is defined by:
?0 = ?1, 0, 0? (18)
?v =
?
l?Iv
?u(l) ? sl (19)
where Iv is the set of all edges with target vertex
v and u(l) is the source vertex for edge l, and the
operations are defined by:
s1 ? s2 = ?p1 + p2, r
1
1h + r
1
2h, r
1
1m + r
1
2m?
s1 ? s2 = ?p1p2, p1r
1
2h + p2r
1
1h, p1r
1
2m + p2r
1
1m?
The backward algorithm for ?u can be implemented
via the forward algorithm in reverse through the
graph. The sufficient statistics for the gradient can
be accumulated during the backward pass noting
that:
?
j?Ji
Pij
?
gn
??igncijgn =
rnm(?0)
p(?0)
(20)
?
j?Ji
Pij
?
gn
cijgn =
rnh(?0)
p(?0)
(21)
where rnm(?) and r
n
h(?) extract the nth order r ele-
ments from the tuple for matches and counts, respec-
tively, and p(?) extracts the p element. The statistics
for the paths traveling via edge l can be computed
by:
?
j:l?Ji
Pij
?
gn
??igncijgn =
rnm(?u ? sl ? ?v)
p(?0)
(22)
?
j:l?Ji
Pij
?
gn
cijgn =
rnh(?u ? sl ? ?v)
p(?0)
(23)
where the u and v subscripts in ?u and ?v are the
start and end vertices for edge l. To avoid under-
flow, all the computations can be carried out in log
domain.
9A 3-tuple for uni-gram counts is used as an example in or-
der to save space. In a 5-tuple for bi-gram counts, all r elements
are computed independently of other r elements with the same
operations. Similarly, tri-gram counts require 7-tuples and four-
gram counts require 9-tuples.
162
tune cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 66.03 18.09 69.03 16.28 60.56 21.02 62.75 21.83
best 53.75 28.36 58.39 24.28 50.26 30.55 50.48 30.87
latBLEU 53.99 29.25 56.70 26.49 48.34 34.55 48.90 33.90
nbExpBLEU 54.43 29.04 56.36 27.33 48.44 34.73 48.58 34.23
latExpBLEU 53.89 29.37 56.24 27.36 48.27 34.93 48.53 34.24
test cz-en de-en es-en fr-en
System TER BLEU TER BLEU TER BLEU TER BLEU
worst 65.35 17.69 69.03 15.83 61.22 19.79 62.36 21.36
best 52.21 29.54 58.00 24.16 50.15 30.14 50.15 30.32
latBLEU 52.80 29.89 55.87 26.22 48.29 33.91 48.51 32.93
nbExpBLEU 52.97 29.93 55.77 26.52 48.39 33.86 48.25 32.94
latExpBLEU 52.68 29.99 55.74 26.62 48.30 34.10 48.17 32.91
Table 1: Case insensitive TER and BLEU scores on newssyscombtune (tune) and newssyscombtest (test)
for combinations of outputs from four source languages. Three tuning methods were used: lattice BLEU (latBLEU),
N-best list based expected BLEU (nbExpBLEU), and lattice expected BLEU (latExpBLEU).
3.4 Entropy on a graph
Expanding the joint lattice to n-gram orders above
n = 2 is often impractical without pruning. If the
edge posteriors are not reliable, which is usually
the case for unoptimized weights, pruning might re-
move good quality paths from the graph. As a com-
promise, an incremental expansion strategy may be
adopted by first expanding and re-scoring the lattice
with a bi-gram, optimizing weights for xBLEU-2,
and then expanding and re-scoring the lattice with
a 5-gram. Pruning should be more reliable with the
edge posteriors computed using the tuned bi-gram
weights. A second set of weights may be tuned with
the 5-gram graph to maximize xBLEU-4.
When the bi-gram weights are tuned, it may be
beneficial to increase the edge score scaling factor
to focus the edge posteriors to the 1-best path. On
the other hand, a lower scaling factor may be bene-
ficial when tuning the 5-gram weights. Rosti et al
(2010) determined the scaling factor automatically
by fixing the perplexity of the merged N -best lists
used in tuning. Similar strategy may be adopted in
incremental n-gram expansion of the lattices.
Entropy on a graph can also be computed using
the expectation semiring formalism (Li and Eisner,
2009) by defining sl = ?pl, rl? where pl = e?sil and
rl = log pl. The entropy is given by:
Hi = log p(?0)?
r(?0)
p(?0)
(24)
where p(?0) and r(?0) extract the p and r elements
from the 2-tuple ?0, respectively. The average target
entropy over all sentences was set manually to 3.0
in the experiments based on the tuning convergence
and size of the pruned 5-gram lattices.
4 Experimental Evaluation
System outputs for all language pairs with En-
glish as the target were combined (cz-en,
de-en, es-en, and fr-en). Unpruned English
bi-gram and 5-gram language model compo-
nents were trained using the WMT11 corpora:
EuroParl, GigaFrEn, UNDoc Es, UNDoc Fr,
NewsCommentary, News2007, News2008,
News2009, News2010, and News2011.
Additional six Gigaword v4 components in-
cluded: AFP, APW, XIN+CNA, LTW, NYT, and
Headlines+Datelines. The total number
of words used to train the LMs was about 6.4
billion. Interpolation weights for the sixteen
components were tuned to minimize perplexity on
the newstest2010-ref.en development set.
The modified Kneser-Ney smoothing (Chen and
163
Goodman, 1998) was used in training. Experiments
using a LM trained on the system outputs and inter-
polated with the general LM were also conducted.
The interpolation weights between 0.1 and 0.9 were
tried, and the weight yielding the highest BLEU
score on the tuning set was selected. A tri-gram true
casing model was trained on all the LM training
data. This model was used to restore the case of the
lower-case system combination output.
All twelve 1-best system outputs on cz-en, 26
outputs on de-en, 16 outputs on es-en, and 24
outputs on fr-en were combined. Three different
weight optimization methods were tried. First, lat-
tice based 1-best BLEU optimization of the bi-gram
decoding weights followed by N-best list based
BLEU optimization of 5-gram re-scoring weights
using 300-best lists, both using downhill simplex.
Second, N-best list based expected BLEU optimiza-
tion of the bi-gram and 5-gram weights using 300-
best lists with merging between bi-gram decoding
iterations. Third, lattice based expected BLEU opti-
mization of bi-gram and 5-gram decoding weights.
The L-BFGS (Liu and Nocedal, 1989) algorithm
was used in gradient ascent. Results for all four sin-
gle source experiments are shown in Table 1, includ-
ing case insensitive TER (Snover et al, 2006) and
BLEU scores for the worst and best systems, and
the system combination outputs for the three tuning
methods. The gains on tuning and test sets were con-
sistent, though relatively smaller on cz-en due to
a single system (online-B) dominating the other
systems by about 5-6 BLEU points. The tuning
method had very little influence on the test set scores
apart from de-en where the lattice BLEU opti-
mization yields slightly lower BLEU scores. This
seems to suggest that the gradient free optimization
is not as stable with a larger number of weights.10
The novel bi-gram feature did not have significant
influence on the TER or BLEU scores, but the num-
ber of novel bi-grams was reduced by up to 100%.
Finally, experiments combining 39 system out-
puts by taking the top half of the outputs from each
language pair were performed. The selection was
based on case insensitive BLEU scores on the tun-
ing set. Table 2 shows the scores for seven combi-
10A total number of 30 weights, 26 system and 4 feature
weights, were tuned for de-en.
xx-en tune test
System TER BLEU TER BLEU
worst 62.81 21.19 62.92 20.29
best 51.11 30.87 50.80 30.32
latBLEU 40.95 40.75 41.06 39.81
+biasLM 41.18 40.90 41.16 39.90
nbExpBLEU 40.81 41.36 41.05 40.15
+biasLM 40.72 41.99 40.65 40.89
latExpBLEU 40.57 41.68 40.62 40.60
+biasLM 40.42 42.23 40.52 41.38
-nBgF 40.85 41.41 40.88 40.55
Table 2: Case insensitive TER and BLEU scores on
newssyscombtune (tune) and newssyscombtest
(test) for xx-en combination. Combinations using lat-
tice BLEU tuning (latBLEU), N-best list based expected
BLEU tuning (nbExpBLEU), and lattice expected BLEU
tuning (latExpBLEU) with and without the system out-
put biased LM (biasLM) are shown. Final row, marked
nBgF, corresponds to the above tuning without the novel
bi-gram feature.
nations using the three tuning methods with or with-
out the system output biased LM, and finally without
the novel bi-gram count feature. There is a clear ad-
vantage from the expected BLEU tuning on the tun-
ing set, and lattice tuning yields better scores than
N-best list based tuning. The difference between
latBLEU and nbExpBLEU without biasLM is
not quite as large on the test set but latExpBLEU
yields significant gains over both. The biasLM also
yields significant gains on all but latBLEU tuning.
Finally, removing the novel bi-gram count feature
results in a significant loss, probably due to the large
number of input hypotheses. The number of novel
bi-grams in the test set output was reduced to zero
when using this feature.
5 Conclusions
The BBN submissions for WMT11 system combi-
nation task were described in this paper together
with a differentiable objective function, graph ex-
pected BLEU, which scales well for a large number
of weights and can be generalized to hypergraphs.
System output biased language model and a novel
bi-gram count feature also gave significant gains on
a 39 system multi-source combination.
164
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Computer Science
Group Harvard University.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 40?51.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system combi-
nation for machine translation. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 312?319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothe-
sis alignment with flexible matching for building con-
fusion networks: BBN system description for WMT09
system combination task. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
61?65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for WMT10 system combination task. In Proceedings
of the Fifth Workshop on Statistical Machine Transla-
tion, pages 321?326.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 787?
794.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231.
165
