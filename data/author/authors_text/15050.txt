Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 328?332,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Meta Learning Approach to Grammatical Error Correction 
 Hongsuck Seo1, Jonghoon Lee1, Seokhwan Kim2, Kyusong Lee1 Sechun Kang1, Gary Geunbae Lee1 1Pohang University of Science and Technology 2Institute for Infocomm Research {hsseo, jh21983}@postech.ac.kr, kims@i2r.a-star.edu.sg {kyusonglee, freshboy, gblee}@postech.ac.kr     Abstract We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora. 1. Introduction As language learning has drawn significant attention in the community, grammatical error correction (GEC), consequently, has attracted a fair amount of attention. Several organizations have built diverse resources including grammatical error (GE) tagged corpora. Although there are some publicly released GE tagged corpora, it is still challenging to train a good GEC model due to the lack of large GE tagged learner corpus. The available GE tagged corpora are mostly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers 
focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al, 2000; Lee, 2004; Nagata et al, 2006; Han et al, 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al, 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most significant sources of GE for the learners with various L1 backgrounds. Second, the effective features for article error correction are already well engineered allowing for quick analysis of the method. Our approach is distinguished from others by integrating the predictive models trained on several GE tagged learner corpora, rather than just one GE tagged corpus. Moreover, the framework is compatible to any classification technique. In this study, we also use a native corpus employing Dahlmeier and Ng?s approach. We demonstrate the effectiveness of the proposed method against baseline models in article error correction tasks. 
328
The remainder of this paper is organized as follows: Section 2 explains our proposed method. The experiments are presented in Section 3. Finally, Section 4 concludes the paper. 2. Method Our method predicts the type of article for a noun phrase within three classes: null, definite, and indefinite. A correction arises when the prediction disagrees with the observed article. The meta-learning technique is applied to this task to deal with multiple corpora obtained from different sources. A meta-classifier decides the final output based on the intermediate results obtained from several base classifiers. Each base classifier is trained on a different corpus than are the other classifiers. In this work, the feature extraction processes used for the base classifiers are identical to each other for simplicity, although they need not necessarily be identical. The meta-classifier takes the output scores of the base classifiers as its input and is trained on the held-out development data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the 
input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al, 2007; Zhang, 2007; Ayd?n, 2009; Menahem et al, 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-classifier finally takes the class having the maximum score. A common design of an ensemble is to train different base classifiers with the same dataset, but in this work one classification technique was used with different datasets each having different characteristics. Although only one classification method was used in this work, different methods each well-tuned to the individual corpora may be used to improve the performance. We employed the meta-learning method to generate synergy among corpora with diverse characteristics. More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier (Dzeroski and Zenko, 2004). 2.2. Base Classifiers In the meta-learning framework, the performance of the base classifiers is important because the improvement in base classification generally enha-
Figure 1: Overview of the proposed method 
329
nces the overall performance. The base classifiers can be expected to become more informative as more data are provided. We followed the structural learning approach (Ando and Zhang, 2005), which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus. Structural learning is a technique which trains multiple classifiers with common structure. The common structure chooses the hypothesis space of each individual classifier and the individual classifiers are trained separately once the hypothesis space is determined. The common structure can be obtained from auxiliary problems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning.  We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Experiments 3.1. Datasets In this work we used a native corpus and two GE tagged corpora. For the native corpus, we used                                                             1 http://nlp.stanford.edu/software/corenlp.shtml 
news data2 which is a large English text extracted from news articles. The First Certificate in English exams in the Cambridge Learner Corpus 3 (hereafter, CLC-FCE; Yannakoudakis et al, 2011) and the Japanese Learner English corpus (Izumi et. al., 2005) were used for the GE tagged corpora. We extracted noun phrases from each corpus by parsing the text of the respective corpora. (1) We parsed the native corpus from the beginning until approximately a million noun phrases are extracted. (2) About 90k noun phrases containing ~3,300 mistakes in article usage were extracted from the entire CLC-FCE corpus, and (3) about 30k noun phrases containing ~2,500 mistakes were extracted from the JLE corpus.  The extracted noun phrases were used for our training and test data. We hold out 10% of the data for the test. We applied 20% under-sampling to the training instances that do not have any errors to alleviate data imbalance in the training set. We emphasize the fact that the two learner corpora differ from each other in three aspects. The first aspect is the styles of the texts: the CLC is literary whereas the JLE is colloquial. The second is the error rate: about 3.5% for CLC-FCE and   8.5% for JLE. Finally, the third is the distribution of L1 languages of the learners: the learners of the CLC corpus have various L1 backgrounds whereas the learners of the JLE consist of only Japanese. These experiments demonstrate the effectiveness of the proposed method relying on the diversity of the corpora. The native corpus was used to find the common structure using structural learning and two GE tagged learner corpora are used to train the base classifiers by structural learning with the common structure obtained from the news corpus. We trained three classifiers for comparison; (1) the classifier (INTEG) trained with the integrated training set of the two GE tagged corpora, and two base classifiers used for the ensemble: (2) the base classifier (CB) trained only with the CLC-FCE and (3) the other base classifier (JB) trained with the JLE. 3.2. Results The accuracy obtained from the word selection task with the news corpus was 76.10%. Upon                                                             2 http://www.statmt.org/wmt09/translation-task.html 3 http://www.ilexir.com/ 
330
obtaining the parameters of the word selection task, the structural parameter ?  was calculated by singular value decomposition and was used for the structural learning of the main GEC task. We used three different test data sets: the CLC-FCE, the JLE and an integrated test set of the two. The accuracy (Acc.) and the precision (Prec.) of the INTEG was poorer than CB on the CLC-FCE test set (Table 1), whereas INTEG outperformed JB on the JLE test (Table 2).  Some instances extracted from the CLC-FCE corpus have similar characteristics to the instances from the JLE corpus. This overlap of instances affected the performance in both positive and negative ways. Prediction of instances similar to those in the JLE was enhanced. Consequently, INTEG model demonstrated better accuracy and precision for the JLE test set. Unfortunately, for the CLC test set, the instances resulted in lower accuracy and precision. The proposed model is able to alleviate this model bias due to similar instances observed in the INTEG model. The accuracy of the proposed model consistently increased by over 10% for all three data sets. The relative performance gain in terms of F1-score (F1) was 15% on the integrated set. This performance gain stems from the over   25% relative improvement of the precision (Table 1, 2 and 3). We believe the improvement comes from the contribution of reconfirming procedures performed 
by the meta-classifier. When the prediction of the two base classifiers conflicts with each other, the meta-classifier tends to choose the one with a higher confidence score; this choice improves the accuracy and precision because known features generate a higher confidence whereas unseen or less-weighted features generate a lower score. Although the proposed model introduced a tradeoff between precision and recall (Rec.), this tradeoff was tolerable in order to improve the overall F1-score. Since GEC is a task where false alarm is critical, obtaining high precision is very important. The low precision on the whole experiments is due to the data imbalance. Instances in the dataset are mostly not erroneous, e.g., only 3.5% of erroneous instances for the CLC corpus. The standard for correct prediction is also very strict and does not allow multiple answers. Performance can be evaluated in a more realistic way by applying a softer standard, e.g., by evaluating manually. 4. Conclusion We have presented a novel approach to grammatical error correction by building a meta-classifier using multiple GE tagged corpora with different characteristics in various aspects. The experiments showed that building a meta-classifier overcomes the interference that occurs when training with a set of heterogeneous corpora. The proposed method also outperforms the base classifier themselves tested on the same class of test set as the training set with which the base classifiers are trained. A better automatic evaluation metric would be needed as further research. Acknowledgments Industrial Strategic technology development program, 10035252, development of dialog-based spontaneous speech interface technology on mobile platform, funded by the Ministry of Knowledge Economy (MKE, Korea).   
Model Acc. Prec. Rec. F1 INTEG 73.37 4.69 72.39 8.82 CB 77.20 5.39 71.17 10.03 Proposed 86.99 6.17 45.77 10.88 Table 1: Best results for GEC task on CLC-FCE test set.  Model Acc. Prec. Rec. F1 INTEG 78.87 14.88 85.47 25.35 JB 78.02 14.49 86.32 24.82 Proposed 89.61 19.28 46.60 27.27 Table 2: Best results for GEC task on JLE test set. Model Acc. Prec. Rec. F1 INTEG 74.64 6.84 77.86 12.58 Proposed 87.50 8.61 46.12 14.52 Table 3: Best results for GEC task on the integrated set of CLC-FCE and JLE test sets.  
331
References  R.K. Ando and T. Zhang. 2005. A framework for learn- ing predictive structures from multiple tasks and un- labeled data. Journal of Machine Learning Research, 6, pp. 1817-1853. U. Ayd?n, S. Murat, Olcay T Y?ld?z, A. Ethem, 2009, Incremental construction of classifier and discriminant ensembles, Information Science, 179 (9), pp. 144-152. L. Breiman, 1996, Bagging predictors, Machine Learning, pp. 123?140. S. Cohen, L. Rokach, O. Maimon, 2007, Decision tree instance space decomposition with grouped gain-ratio, Information Science, 177 (17), pp. 3592?3612. D. Dahlmeier, H. T. Ng, 2011, Grammatical error correction with alternating structure optimization, In Proceedings of the 49th Annual Meeting of the ACL-HLT 2011, pp. 915-923. R. De Felice. 2008. Automatic Error Detection in Non- native English. Ph.D. thesis, University of Oxford. S. Dzeroski, B. Zenko, 2004, Is combining classifiers with stacking better than selecting the best one?, Machine Learning, 54 (3), pp. 255?273. J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the ACL, pp. 363-370. N.R. Han, M. Chodorow, and C. Leacock. 2006. De- tecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(02), pp. 115-129. N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010. Using an error-annotated learner corpus to develop an ESL/EFL error correction system. In Proceedings of LREC. D. Klein and C.D. Manning. 2003a. Accurate unlexical- ized parsing. In Proceedings of ACL, pp. 423-430. D. Klein and C.D. Manning. 2003b. Fast exact inference with a factored model for natural language processing. Advances in Neural Information Processing Systems (NIPS 2002), 15, pp. 3-10. K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of AAAI, pp. 779-784. J. Lee. 2004. Automatic article restoration. In Proceed- ings of HLT-NAACL, pp. 31-36. R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A feedback-augmented method for detecting errors in 
the writing of learners of English. In Proceedings of COLING-ACL, pp. 241--248. A. Mariko, 2007, Grammatical errors across proficiency levels in L2 spoken and written English, The Economic Journal of Takasaki City University of Economics, 49 (3, 4), pp. 117-129. E. Menahem, L. Rokach, Y. Elovici, 2009, Troika-An imporoved stacking schema for classification tasks, Information Science, 179 (24), pp. 4097-4122. G. Minnen, F. Bond, and A. Copestake. 2000. Memory- based learning for article generation. In Proceedings of CoNLL, pp. 43-48. E. Izumi, K. Uchimoto, H. Isahara, 2005, Error annotation for corpus of Japanese learner English, In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora, pp. 71-80. A. Rozovskaya and D. Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Pro- ceedings of HLT-NAACL, pp. 154-162. K. Toutanova and C. D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of the Joint SIGDAT Conference on EMNLP/VLC-2000, pp. 63-70. H.Yannakoudakis, T. Briscoe, B. Medlock, 2011, A new dataset and method for automatically grading ESOL texts, In Proceedings of ACL, pp. 180-189. G. P. Zhang, 2007, A neural network ensemble method with jittered training data for time series forecasting, Information Sciences: An International Journal, 177 (23), pp. 5329?5346. 
332
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 344?346,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
POMY: A Conversational Virtual Environment for Language Learning 
in POSTECH 
 
Hyungjong 
 Noh 
Kyusong  
Lee 
Sungjin  
Lee 
 
Gary Geunbae 
Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology, Pohang, South Korea 
{nohhj, kyusonglee, junion, gblee}@postech.ac.kr 
  
 
 
Abstract 
This demonstration will illustrate an inter-
active immersive computer game, POMY, 
designed to help Korean speakers learn 
English. This system allows learners to ex-
ercise their visual and aural senses, receiv-
ing a full immersion experience to increase 
their memory and concentration abilities to 
a greatest extent. In POMY, learners can 
have free conversations with game charac-
ters and receive corrective feedback to their 
errors. Game characters show various emo-
tional expressions based on learners? input 
to keep learners motivated. Through this 
system, learners can repeatedly practice 
conversations in everyday life setting in a 
foreign language with no embarrassment. 
1 Introduction 
The needs for computer-based methods for learn-
ing language skills and components are increasing. 
One of the ultimate goals of computer-assisted 
language learning is to provide learners with an 
immersive environment that facilitates acquiring 
communicative competence. According to Second 
Language Acquisition (SLA) theories, there are 
some essential factors for improving learners? con-
versational skills: 1) comprehensible inputs and 
outputs, 2) corrective feedback, and 3) motivation 
and attitude. SLA theories imply that providing 
learners with the opportunity to have free conver-
sations with someone who can correct their errors 
is very important for successful acquisition of for-
eign languages. Moreover, motivation is another 
crucial factor; therefore a good CALL system 
should have elements which can interest learners 
[1]. 
Considering these requirements, we have devel-
oped a conversational English education frame-
work, POMY (POstech iMmersive English studY). 
The program allows users to exercise their visual 
and aural senses to receive a full immersion expe-
rience to develop into independent English as a 
Foreign Language (EFL) learners and increase 
their memory and concentration abilities to a 
greatest extent [2].  
 
 
Figure 1: Example screenshots of POMY: path-finding, post office, and market 
344
2 Demonstrated System 
In order to provide learners with immersive world, 
we have developed a virtual reality environment 
using the Unity 3D game engine1. For the domains 
that learners are exposed to, we select such do-
mains as path-finding, market, post office, library, 
and movie theater (Figure 1) to ensure having 
learners practice conversations in everyday life 
setting. To keep learners motivated and interested 
during learning sessions, learners are encouraged 
to accomplish several missions. For example, the 
first mission in the post office is to send a camera 
to one?s uncle in England. The package must be 
insured and delivered by the next week. In order to 
send the package, a learner must talk to Non-
Player Characters (NPCs) to fill in the zip-code 
properly.  
All NPCs can perceive the utterances of learners, 
especially Korean learners of English. Korean 
learners? production of the sound is different from 
those of native speakers, resulting in numerous 
pronunciation errors. Therefore, we have collected 
a Korean-English corpus to train acoustic models. 
In addition, since language learners commit nu-
merous grammatical errors, we should consider 
this to understand their utterances. Thus, we statis-
tically infer the actual learners' intention by taking 
not only the utterance itself but also the dialog con-
text into consideration, as human tutors do [1]. 
While free conversation is invaluable to the 
acquisition process, it is not sufficient for learners 
to fully develop their L2 proficiency. Corrective 
feedback to learners? grammatical errors is 
necessary for improving accuracy in their 
interlanguage. For this purpose, we designed a 
                                                          
1 http://unity3d.com/ 
special character, Ghost Tutor, which plays the 
role of English tutor and helps learners to use more 
appropriate words and expressions during the game. 
When a learner produces ungrammatical utterances, 
the Ghost Tutor provides both implicit and explicit 
negative and positive feedback in a form of 
elicitation or recast, which was manifested as 
effective ways in the second language acquisition 
processes [3].  To provide corrective feedback on 
grammatical errors, we use a method which con-
sists of two sub-models: the grammaticality check-
ing model and the error type classification model 
[4]. Firstly, we automatically generate grammatical 
errors that learners usually commit [5-6], and con-
struct error patterns based on the articulated errors. 
Then the grammaticality checking model classifies 
the recognized user speech based on the similarity 
between the error patterns and the recognition re-
sult using confidence scores. After that, the error 
type classification model chooses the error type 
based on the most similar error pattern and the er-
ror frequency extracted from a learner corpus. 
Finally, the human perception of NPC?s emo-
tional expressions plays a crucial role in human 
computer interaction. Thus, all NPCs are provided 
with a number of communicative animations such 
as talking, laughing, waving, crying, thinking, and 
getting angry (Figure 2).The total number of ani-
mations is over thirty from which the system can  
select one based on the response of a learner. The 
system generates positive expressions such as 
clapping and laughing when the learner answers 
correctly, and negative expressions such as crying 
and getting angry for incorrect answers.  
 
 
 
 
 
Figure 2: Various character animations 
345
Acknowledgments 
This work was supported by the Industrial Strate-
gic technology development program, 10035252, 
development of dialog-based spontaneous speech 
interface technology on mobile platform, funded 
by the Ministry of Knowledge Economy (MKE, 
Korea), and by Basic Science Research Program 
through the National Research Foundation of Ko-
rea (NRF) funded by the Ministry of Education, 
Science and Technology (2010-0019523). 
References  
Lee, S., Noh, H., Lee, J., Lee, K., Lee, G. G., Sagong, S., 
Kim, M. 2011. On the Effectiveness of Robot-
Assisted Language Learning, ReCALL Journal, 
Vol.23(1). 
Lee, S., Noh, H., Lee, J., Lee, K., Lee, G. G. 2010. 
POSTECH Approaches for Dialog-based English 
Conversation Tutoring. Proceedings of the APSIPA 
annual summit and conference, Singapore. 
Long, M. H., Inagaki, S., Ortega, L. 1998. The Role of 
Input and Interaction in Second Language Acquisi-
tion. The Modern Language Journal, 82, 357-371.  
Lee, S., Noh, H., Lee, K., Lee, G. G. 2011. Grammatical 
error detection for corrective feedback provision in 
oral conversations. Proceedings of the 25th AAAI 
conference on artificial intelligence (AAAI-11), San 
Francisco. 
Lee, S., Lee J., Noh, H., Lee, K., Lee, G. G, 2011. 
Grammatical Error Simulation for Computer-
Assisted Language Learning, Knowledge-Based Sys-
tems (to be published). 
Lee, S. and Lee, G. G. 2009. Realistic grammar error 
simulation using markov logic. Proceedings of the 
ACL, Singapore. 
 
346
Proceedings of the SIGDIAL 2013 Conference, pages 349?353,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Counseling Dialog System with 5W1H Extraction 
 
 
Sangdo Han, Kyusong Lee, Donghyeon Lee, Gary Geunbae Lee 
Department of Computer Science and Engineering, POSTECH, South Korea 
{hansd,kyusonglee,semko,gblee}@postech.ac.kr 
 
  
 
Abstract 
In this paper, we introduce our counseling dia-
log system. Our system interacts with users by 
recognizing what the users say, predicting the 
context, and following the users? feelings. For 
this interaction, our system follows three basic 
counseling techniques: paraphrasing, asking 
open questions, and reflecting feelings. To fol-
low counseling techniques, we extracted 
5W1H information and user emotions from 
user utterances, and we generated system ut-
terances while using the counseling techniques. 
We used the conditional random field algo-
rithm to extract 5W1H information, and con-
structed our counseling algorithm using a dia-
log strategy that was based on counseling 
techniques. A total of 16 adults tested our sys-
tem and rated it with a higher score as an in-
teractive communicator compared with the 
baseline system. 
1 Introduction 
Over the past 45 years, suicide rates have in-
creased by 60% worldwide.1 To prevent suicide, 
suicide people need to counsel with counselors. 
However, counseling with a human counselor 
requires a substantial cost, and in addition, there 
is a location restriction. Developing a counseling 
dialog system could be an effective solution to 
address this problem because the system has no 
limitations with respect to time and location. 
In this study, we present a counseling dialog 
system. The system interacts with users by rec-
ognizing what the users say, predicting the con-
text, and following the users? feelings. We used 
three counseling techniques for our system, to 
interact with the users. The system performs par-
aphrasing, asks open questions, and reflects feel-
ings. 
                                                 
1 
http://www.who.int/mental_health/prevention/suicide/suicid
eprevent/en/ 
Paraphrasing is a technique that paraphrases 
user utterances. For example, when a user utter-
ance is ?My dog picked up the ball?, then it 
could be paraphrased by ?Oh, your dog picked 
up the ball?. The technique of asking open ques-
tions is to ask some questions to the user, to ob-
tain more information. For example, when a user 
says ?I played computer games?, then the coun-
selor could say ?When did you play?? or ?Where 
did you play??. Finally, reflecting a feeling is a 
similar technique to paraphrasing, but it includes 
emotional comments. For example, when a user 
says ?My dog died. I?m so sad?, then the counse-
lor could say, ?Oh, your dog died. You look de-
pressed.? or ?You look so sad?. 
In our approach, we extract 5W1H (who, what, 
when, where, why, how) information and four 
basic emotions (happy, afraid, sad, and angry) 
from user utterances. We generate system utter-
ances using 5W1H information and basic emo-
tions. 
2 Counseling Techniques 
Counselors show empathy with clients by listen-
ing and understanding them. Clients feel com-
fortable by a counselor?s attention. Counselors 
listen, ask questions, answer questions, and con-
centrate on clients. Attention and empathy is im-
portant for counseling. Counselors show interest 
and care about the clients? emotions. Our coun-
seling dialog system also focused on attending 
and empathy. 
Many counseling techniques are used in coun-
seling. Basic attending, self-expression, and mi-
cro-training skills are introduced in Theron et al 
(2008). Basic attending and self-expression skills 
are about non-verbal behavior, such as tone of 
voice and eye contact. Micro-training skills are 
the basic verbal counseling techniques that are 
learned for counseling beginners: open and 
closed questions, minimal encouragement, para-
phrasing, reflection of feelings and summariza-
tion. 
349
We chose three micro-training skills to attend 
and show empathy with clients. These skills are 
open questions, paraphrasing, and reflection of 
feelings because they are basic techniques to 
show emphasize effectively. 
3 Related Work 
The SEMAINE project aims to build a Sensitive 
Artificial Listeners (SAL) ? conversational 
agents that are designed to interact with a human 
user through robust recognition and the genera-
tion of non-verbal behavior (Schr?der et al, 
2008). This system detects user emotions by 
multimodal sensors (camera, microphone). A 
virtual face in this system shows facial expres-
sions based on user emotions, and it encourages 
the user to speak by reacting and asking ques-
tions. These techniques could show empathy 
with users. However, it has limited verbal skills 
because SEMAINE does not have language un-
derstanding module. In our research, our system 
follows user utterances and generates system ut-
terances based on user?s 5W1H. 
4 Data Collection 
We generated 4,284 utterances by using fifty-
three 5W1H information sets and four basic 
emotions (Figure 1). Each utterance could be 
generated by using part of the 5W1H information 
and four emotions. 
 
Wh When Where What How Why
My 
om
Yesterday Park Key Lost
Her pocket
was punctured
Emotion
Sad
My mom lost key yesterday.
Yesterday, my mom lost key at the park.
Sadly, my mom lost key yesterday.
My mom lost key because her pocket was punctured.
Given Situation
Collected Corpus
 
Figure 1. Counseling Corpus Collecting Process 
 
We tagged each 5W1H element in each utter-
ance and the user intention for each utterance 
(Table 1). The system?s actions were labeled by 
following counseling strategies which will be 
discussed in section 5.3. 
 
Tagged Corpus User Intention System Action
<who>My mom</who> <how>lost</how> <what>a 
key</what> <when>yesterday</when>.
Inform_5W1H Ask_Open_Question
<when>Yesterday</when>, <who>my mom</who> 
<how>lost</how> <what>a key</what> at the 
<where>park</where>.
Inform_5W1H Paraphrase
<who>My mom</who> <how>lost</how> <what>a 
key</what> <when>yesterday</when>. I?m so sad.
Inform_5W1H_
Emotion
Reflect_Feeling
I?m so sad. Inform_Emotion Reflect_Feeling
Thank you. Thank Welcome
Good bye. Bye Bye
 
Table 1.  Corpus Tagging Examples 
 
User intentions we defined can be separated in 
two groups: ?counseling? and ?others?. Utterances 
in ?counseling? group include 5W1H information 
or emotional information. Utterances which do 
not including them are in ?others? group. Greet-
ings, thanks, and farewells are included (Table 2). 
 
Couns ing group Others group
Inform_5W1H,
Inform_emotion, 
Inform_5W1H_emotion, ?
Thank, Bye, Greeting, Agree, 
Disagree,?
 
Table 2. Two Separated Groups of User Intentions 
5 Method 
5.1 Architecture 
Our system architecture is given in graph 2. 
When a user inputs a sentence, a natural lan-
guage understanding (NLU) module understands 
the main action (the user?s intention) and extracts 
the 5W1H entities from the user?s utterance. The 
emotion detection module detects the user?s 
emotions using the emotional keyword diction-
ary. The dialog management module decides the 
system?s action from the main action and the 
5W1H information from the trained module from 
the example dialog corpus. The natural language 
generation (NLG) module generates the system 
utterance using a system utterance template. We 
can generate the system utterance by replacing 
5W1H slots with entities. 
 
User
Natural 
Language 
Understanding
Dialog 
Manager
Natural 
Language 
Generation
Dialog 
Template
Emotion 
Detector
Output
Emotional 
Keyword
 
Figure 2. Counseling Dialog System Hierarchy 
350
5.2 Natural Language Understanding 
In our approach, the NLU module understands 
the user utterance by classifying the main action 
and the 5W1H entities from the user utterance. 
To classify user intention, we used maximum 
entropy model (Ratnaparkhi, 1998) trained on a 
linguistically motivated features. We used a lexi-
cal word features for the utterance model. The 
lexical word features are lexical trigrams using 
previous, current, and next lexical words. To ex-
tract 5W1H entities, we used a conditional ran-
dom field (CRF) model (Laffery et al, 2001). 
We also used lexical word features (lexical tri-
grams) to train model. 
5.3 Dialog Management with Counseling 
Strategy 
When we extract 5W1H information or user 
emotions, the dialog management module keeps 
them in the emotion slot or in the six 5W1H slots. 
This slot information is discussed in a dialog. 
The dialog management module decides the 
system?s action by the main action, the 5W1H 
entities, and the user?s emotions. Dialog man-
agement follows the rules in figure 3, which is 
our dialog strategy for the counseling system. In 
figure 3, ?Counseling group?? node finds users 
intentions included in ?others group? (rejection or 
thanks could be included). The ?User Emotion 
Detection? node figures out whether the user ut-
terance is to include emotional keywords or 
whether the user emotion is already known by 
the discourse. The ?6 slot empty? node checks 
whether the user utterance includes at least one 
of the 5W1H elements or whether the 5W1H en-
tity is already known. The ?6 slot full? node de-
cides whether the user utterance with a discourse 
has all six 5W1H entries. From this strategy, we 
can notice that we cannot reflect a user?s feeling 
without the user?s emotion. We cannot ask open 
questions when all of the 5W1H slots are filled. 
 
Yes
No
No
No
No
No
Yes
No
Yes
YesYes
Yes
6 slot 
empty
6 slot 
full
6 slot 
empty
6 slot 
full
Counseling 
group?
User 
Utterance
User 
Emotion 
Detection
Particular 
System Actions
Ask Open 
Question
Reflect
Feeling
Ask Open 
Question
Reflect
Feeling
Paraphrase
Ask Open 
Question
Paraphrase
Reflect 
Feeling
Paraphrase
Ask Open 
Question
Paraphrase
 
Figure 3. Dialog Strategy Architecture 
5.4 Emotion Detection 
The emotion detection module decides the user?s 
emotion with respect to the four basic emotions. 
To detect the user?s emotions, we find emotional 
keywords in the user?s utterances. If any emo-
tional keyword appears in a user utterance, we 
decide that the user?s emotion which includes 
that keyword. For this approach, we made a dic-
tionary of emotional keywords for each basic 
emotion. The dictionary has approximately 15 
emotional keywords for each basic emotion (Ta-
ble 3). 
 
E otion Keywords
Happy happy, joy, glad, pleasure, ?
Sad sad, depress, grief, sorrow,?
Angry anger, rage, indignation, wrath, ?
Afraid afraid, fear, terror, dread, ?
 
Table 3. Emotional Keywords for Emotions 
5.5 Natural Language Generation 
The NLG module generates the system?s utter-
ances by selecting an utterance template from the 
template database. There are various system 
templates for each system action, for the 5W1H 
slot entities, and for the user?s emotions (Figure 
4). When the system template is chosen, we can 
generate system utterances by replacing the 
5W1H slot with a 5W1H discourse. ?O? means 
we should have the entity, ?X? means we do not 
have the entity, and ?*? means we do not care 
whether we have the entity or not. 
 
Who When Where What How Why Emotion System Action
O X O O O X O Reflect Emotion
O X O O O X * Paraphrase
System Action System Template
Reflect Emotion Oh, <who> <how> <what>. You look so sad.
Paraphrase Oh, did <who> <how> <what>?
System Utterance
Oh, your mom lost a key. You look so sad.
Oh, did your mom lost a key?
Who When Where What How Why
My 
mom
Yesterday Park Key Lost
Her pocket
was punctured
Emotion
Sad
 
Figure 4. System Utterance Generation 
 
Our overall counseling dialog example is shown 
in (Table 4). 
351
 User My mom lost a key yesterday.
System Oh, where did you mom lost a key?
User At the park.
System Your mom lost a key at the park, right?
User Yes, that?s right. I?m so sad.
System You look so sad. Cheer up.
User Thank you.
System My pleasure.
 
Table 4. Counseling Dialog Example 
6 Experiment 
We measured how systems show empathy with 
users. Our baseline system is a Korean chat-
oriented dialog system (Kim et al, 2012). The 
chat-oriented dialog system shows empathy by 
understanding user utterances and making a con-
versation. In our experiment, 7 basic situations 
are given for each person. Situations are ex-
plained by 5W1H, and users generated various 
utterances using that information. Each person 
generated approximately 100 utterances during 
30 minutes and made estimates for each system. 
We recruited 16 volunteers to use our system and 
to estimate its effectiveness. Each user checked 
17 questions from 1 to 10. The questions ask us-
ers how does each system understand the user 
utterance, is it appropriate for counseling, and 
does it satisfy the users (Table 5). 
 
Question
Chat-
Oriented
Counseling
1-1. The system used counseling techniques: 
paraphrasing, open question, reflect feeling.
3.50 7.06
1-2. The system knows my emotion. 3.44 6.88
1-3. There was no break in the conversation. 2.63 6.88
1-4. The system acts like a counselor. 2.88 6.69
1-5. The system shows empathy with me. 4.69 7.31
1-6. I feel the system understands me. 2.56 6.50
2-1. The system understands what I said. 2.88 6.81
2-2. The system understands 5W1H information. 4.13 7.44
2-3. System utterances are appropriate. 2.75 6.94
2-4. System utterances have no problem. 3.50 5.50
3-1. I could speak about various situations. 4.31 6.38
3-2. I had a casual conversation. 4.75 6.88
3-3. Scenarios look expandable. 5.50 7.63
4-1. I satisfied overall conversation. 3.10 6.56
4-2. I satisfied overall counseling. 2.38 6.56
4-3. The system looks appropriate as a counselor. 2.50 6.38
4-4. I?ll recommend the system as a counselor to my
friends.
2.31 5.38
Mean 3.40 6.69
Standard Deviation 0.96 0.59
 
Table 5. Experiment Results 
 
Questions 1-1 to 1-6 ask users how each sys-
tem is appropriate as a counselor. Counseling 
system rated 6.89 for mean. Questions 2-1 to 2-4 
are about users? tterances understandability. In 
these questions, counseling system rated 6.67 on 
the average. Questions 3-1 to 3-3 show how var-
ious dialogs covered. Our system got 6.96 for 
mean. Finally, questions 4-1 to 4-4 are about 
overall satisfaction. These questions rated 6.22 
for mean. Our p-value through t-test was 
3.77*10-11. 
Counseling system got higher score than chat-
oriented system because users felt empathy better 
with our system than baseline system. As a coun-
selor, counseling system is much better than 
chat-oriented system. Our baseline system was 
not appropriate as a counselor because it rated 
3.39 for average. However, our system scored 
over 6.5 overall. It means our system is valuable 
as a counselor.  
7 Conclusion 
In this study, we introduced counseling tech-
niques that we used to implement counseling 
dialog system. The experimental results showed 
that our system shows empathy with users. Alt-
hough the results of this study bring us a step 
closer to implementing counseling dialog system, 
the results are only valid with 5W1H information 
in Korean. Our future works are to improve our 
counseling dialog system using new NLU mod-
ule which extracts 5W1H information from more 
general utterances, with new emotion detection 
method, and with more counseling techniques. 
 
Acknowledgments 
This research was supported by the Basic Sci-
ence Research Program through the National Re-
search Foundation of Korea(NRF) funded by the 
Ministry of Education, Science and Technolo-
gy(2012-0008835). 
This research was supported by the 
MSIP(Ministry of Science, ICT&Future Plan-
ning), Korea, under the ITRC(Information Tech-
nology Research Center) support program super-
vised by the NIPA(National IT Industry Promo-
tion Agency) (NIPA-2013-H0301-13-3002) 
References  
Kim, Y., Noh, H., & Lee, G. G. (2012). Dialog man-
agement on chatting system based on lexico-
syntactic patterns and named entity types. Proceed-
ings of Spring Conference of Korean Society of 
Speech Sciences, 41-42,  Seoul, Korea. 
352
Lafferty, J., McCallum, A., & Pereira, F. (2001). 
Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. 
Proceedings of the 18th International Confer-
ence on Machine Learning, 282-289. 
Ratnaparkhi, A. (1998). Maximum entropy models 
for natural language ambiguity resolution. 
Computer and Information Science, University 
of Pennsylvania, Philadelphia, USA.  
Schr?der, M., Cowie, R., Heylen, D., Pantic, M., Pe-
lachaud, C., & Shuller, B. (2008). Towards re-
sponsive sensitive artificial listeners. Workshop 
on Human-Computer Conversation, Bellagio, Italy. 
Theron, M. J. (2008). A manual for basic relational 
skills training in psychotherapy. Masters of Arts 
in Clinical Psychology, University of South Africa, 
South Africa. 
353
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 65?73,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
POSTECH Grammatical Error Correction System in the CoNLL-
2014 Shared Task 
 
 
Kyusong Lee, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology 
Pohang, Korea 
{Kyusonglee,gblee}@postech.ac.kr 
 
  
 
Abstract 
This paper describes the POSTECH gram-
matical error correction system. Various 
methods are proposed to correct errors 
such as rule-based, probability n-gram 
vector approaches and router-based ap-
proach. Google N-gram count corpus is 
used mainly as the correction resource. 
Correction candidates are extracted from 
NUCLE training data and each candidate 
is evaluated with development data to ex-
tract high precision rules and n-gram 
frames. Out of 13 participating teams, our 
system is ranked 4th on both the original 
and revised annotation.    
1 Introduction 
Automatic grammar error correction (GEC) is 
widely used by learners of English as a second 
language (ESL) in written tasks. Many methods 
have been proposed to correct grammatical errors; 
these include methods based on rules (Naber, 
2003), on statistical machine translation (Brockett 
et al., 2006), on machine learning, and on n-grams 
(Alam et al., 2006). Early research (Han et al., 
2006; De Felice, 2008; Knight & Chander, 1994; 
Nagata et al., 2006) on error correction for non-
native text was based on well-formed corpora.  
Most recent work (Cahill et al., 2013; 
Rozovskaya & Roth, 2011; Wu & Ng, 2013) has 
used machine learning methods that rely on a GE- 
tagged corpus such as NUCLE, Japanese English 
Learner corpus, and Cambridge Learner Corpus 
(Dahlmeier et al., 2013; Izumi et al., 2005; 
Nicholls, 2003), because well-formed and GE-
tagged approaches are closely related to each 
other, can be synergistically combined. Therefore, 
research using both types of data has also been 
conducted (Dahlmeier & Ng, 2011). Moreover, a 
meta-classification method using several GE-
tagged corpora and a native corpus has been pro-
posed to correct the grammatical errors (Seo et al., 
2012). A meta-classifier approach has been pro-
posed to combine a language model and error-spe-
cific classification for correction of article and 
preposition errors (Gamon, 2010). Web-scale 
well-formed corpora have been successfully ap-
plied to grammar error correction tasks instead of 
using error-tagged data (Bergsma et al., 2009; 
Gamon et al., 2009; Hermet et al., 2008). Espe-
cially in the CoNLL-2013 grammar error correc-
tion shared task, many of the high-ranked teams 
(Kao et al., 2013; Mark & Roth, 2013; Xing et al., 
2013) exploited the Google Web-1T n-gram cor-
pus. The major advantage of using these web-
scale corpora is that extremely large quantities of 
data are publicly available at no additional costs; 
thus fewer data sparseness problems arise com-
pared to previous approaches based on error-
tagged corpora. 
We also use the Google Web-1T n-gram corpus. 
We extract the candidate pairs (original erroneous 
text and its correction) from NUCLE training data. 
We use a router to choose the best frame to com-
pare the n-gram score difference between the orig-
inal and replacement in a given candidate pair.  
The intuition of our grammar error correction 
method is the following: First, if the uni-gram 
count is less than some threshold, we assume that 
the word is erroneous. Second, if the replacement 
word n-gram has more frequent than the original 
word n-gram, it presents strong evidence for cor-
rection. Third, depending on the candidate pair, 
tailored n-gram frames help to correct errors ac-
curately. Fourth, only high precision method and 
rules are applied. If correction precision on a can-
didate pair is less than 30% in development data, 
65
we do not make a correction for the candidate pair 
at runtime. 
In the CoNLL-Shared Task, objectives were 
presented yearly. In 2012, the objective was to 
correct article and preposition errors; in 2013, it 
was to correct article, preposition, noun number, 
verb form, and subject-verb agreement errors. 
This year, the objective is to correct all errors. 
Thus, our method should also correct prepro-
cessing and spelling errors.  Detailed description 
of the shared task set up, data set, and evaluation 
about the CoNLL-2014 Shared Task is explained 
in (Ng et al., 2014) 
2 Data and Recourse  
The Google Web-1T corpus contains 1012 words 
of running text and the counts for all 109 five-word 
sequences that appear > 40 times (Brants & Franz, 
2006). We used the NUS Corpus of Learner Eng-
lish (NUCLE) training data to extract the candi-
date pairs and CoNLL-2013 Official Shard Task 
test data as development data.  We used the Stan-
ford parser (De Marneffe & Manning, 2008) to 
extract part-of-speech, dependency, and constitu-
ency trees.  
3 Method 
3.1 Overall Process 
We correct the errors in the following order: 
Tokenizing ? spelling error correction ? punc-
tuation error correction ? N-gram Vector Ap-
proach for Noun number (Nn) ? Router-based 
                                                 
1 http://abisource.com/projects/enchant/ 
Correction (Deletion Correction ? Insertion Cor-
rection ? Replacement) for various error types ? 
Rule-based method for verb errors. Between each 
pair of step, we parse, tag, and tokenize again us-
ing the Stanford parser because the previous cor-
rection affects parsing, tagging, and tokenizing re-
sults.  
3.2 Preprocessing 
Because the correction task is no longer restricted 
to five error types, tokenizing and spelling error 
correction have become critical for error correc-
tion. To detect tokenizing error such as ?civiliza-
tions.It?, a re-tokenzing process is necessary. If a 
word contains a comma, punctuation (e.g., ?,? or 
?.?) and the word count in Google n-gram is less 
than some threshold (here, 1000), we tokenize the 
word, e.g., as ?civilizations . It?. We also correct 
spelling errors by referring to the Google n-gram 
word count. If the word uni-gram count is less 
than a threshold (here, 60000) and the part-of-
speech (POS) tag is not NNP or NNPS, we assume 
that the word has o  ne or more errors. The thresh-
old is set based on the development set. We use 
the Enchant Python Library to correct the spelling 
errors1. However, using only one best result is not 
very accurate. Thus, among the best results in the 
Enchant Python Library, we select the one best 
word, i.e. that word with the highest frequency in 
the Google n-gram corpus. Using NUCLE train-
ing data, rules are constructed for comma, punc-
tuation, and other errors (Table 3).  
 
 
 
Figure 1. Overall Process of Router-based Correction 
 
66
3.3 Candidate Generation 
Selecting appropriate correction candidates is crit-
ical for the precision of the method. In article and 
noun number correction, the number of candidates 
is small: ?a?,?an?,?the? in article correction, ?plural? 
or ?singular? in noun number correction. However, 
the number of correction candidates can be unlim-
ited in wrong collocation/idiom errors. Reducing 
the number of candidates is important in the gram-
mar error correction task.  
 
Nn Correction Candidate: noun number correc-
tion has just one replacement candidate. If the 
word is plural, its correction candidate is singular, 
and vice versa. The language tool2 can perform 
these changes. 
 
Other Correction Candidate: for corrections 
other than noun number, candidates are selected 
from the GE-tagged corpus. A total of 4206 pairs 
were extracted. We use the notation of candidate 
pair (o?r), which links the original word (o) and 
its correction candidate (r). In the deletion correc-
tion step, we determine whether or not the word 
should be deleted. In the insertion correction step, 
we select the insertion position in a sentence as a 
space between two words. If o is ?, insertion cor-
rection is required; if r is ?, the pair deletion cor-
rection is required. We use the Stanford constitu-
ency parser (De Marneffe & Manning, 2008) to 
extract a noun phrase; if it does not contain a de-
terminer or article, we insert one in front of the 
noun phrase; if the noun in the noun phrase is sin-
gular, ?the?, ?a?, and, ?an? are selected an insertion 
candidates; if the noun is plural, only ?the? is se-
lected as an insertion candidate. We only apply in-
sertion correction at ArtOrDet, comma errors, and 
preposition; we skip insertion correction for other 
error types because selecting an insertion position 
is difficult and if every position is selected as in-
sertion position, precision decrease. 
 
4 N-gram Approach 
We used the following notation. 
N(o) n-gram vector in original sentence 
N(r) n-gram vector in replacement sen-
tence 
n(o)i i th element in N(o) 
?(?)? i th element in N(r) 
N[i:j] n-gram vector from i th element to 
j th element 
                                                 
2http://www.languagetool.org 
 
Web-scale data have also been used successfully 
in many other research areas, such as lexical dis-
ambiguation (Bergsma et al., 2009). Most NLP 
systems resolve ambiguities with the help of a 
large corpus of text, e.g.: 
? The system tried to decide {among, between} 
the two confusable words.  
Disambiguation accuracy increases with the size 
of the corpus. Many systems incorporate the web 
count into their selection process. For the above 
example, a typical web-based system would query 
a search engine with the sequences ?decide among 
the? and ?decide between the? and select the can-
didate that returns the most hits. Unfortunately, 
this approach would fail when disambiguation re-
quires additional context. Bergsma (2009) sug-
gested using the context of samples of various 
lengths and positions. For example, from the 
above the example sentence, the following 5-gram 
patterns can be extracted: 
 
?  system tried to decide {among, between}  
?  tried to decide {among, between} the 
?  to decide {among, between} the two 
? decide {among, between} the two confusable 
? {among, between} the two confusable words 
 
Similarly, four 4-gram patterns, three 3-gram pat-
terns and two 2-gram patterns are extracted by 
spanning the target. A score for each pattern is cal-
culated by summing the log-counts. This method 
was successfully applied in lexical disambigua-
tion. Web-scale data were used with the count in-
formation specified as features. Kao et al. (2013) 
used a ?moving window (MW)? : 
 
???,?(w) = {???? , ? , ????+(??1), ? = 0, ? ? 1}  (1) 
 
where ?  denotes the position of the word, k the 
window size and w the original or replacement 
word at position ?. The window size is set to 2 to 
5 words. MW is the same concept as the SUMLM: 
 
??,?(?) = ? ?????(?????)
?????????(?)
(2) 
Both approaches apply the sum of all MWs in (1).  
Our approach is based on the MW method. The 
difference is that instead of summing all the MWs, 
we consider only one best MW which is referred 
to here as a frame. The following sentences 
 
67
demonstrate the case when the following words 
are the crucial features to correct errors: 
?  I will do it (in?at) home. 
?  We need (an??) equipment to solve problems. 
However, following sentences demonstrate the 
case when preceding words is the crucial feature 
to correct errors: 
?  One (are?is) deemed to death at a later stage . 
?  But data that (shows?show) the rising of life 
expectancies 
We investigated which frame is the best based on 
the development set, then router is trained to de-
cide on the frame depending on the candidate pair.  
 
4.1 Router-based N-gram Correction 
A frame is a sequence of words around the target 
position. A frame is divided into a preceding 
frame and a following frame. The target position 
can be either a position of a target word (Figure 
2a) or a position in which a candidate word is 
judged to be necessary (Figure 2b). Once the size 
(i.e., number of words) of frames is chosen, sev-
eral forms of frames (n; m) with different sizes of 
preceding (n) and following (m) words are possi-
ble. 
 
 
Figure 2.  Frame for n-gram 
 
The router is designed to take care of two stages 
(training, run-time) error correction. During train-
ing, the router selects the best frame for each can-
didate pair. By testing each candidate pair with 
each frame in the development data; the frame 
with the best precision is selected as the best 
frame among (1;1), (1;2), (1;3), (2;1),(2:2), etc.  
 At the end of the training stage, the router has 
a list of pairs (x) which matches the best frame (y) 
associated with it (Table 1) as a result of compar-
ing each candidate pair with one in the develop-
ment corpus. 
During runtime, the router assigns each candi-
date pair to the best frame to produce the output 
sentence (Figure 1). For example, for a sentence 
?This ability is not seen 40 years back where the 
technology advances were not as good as now .? 
the candidate pair for correction (back? ago) is 
suggested. The best frame assigned by the router 
for this pair (1;1), which is ?years back where?. 
The best candidate frame for this is ?year ago 
where?. At this point, we query the count of 
?years back where? and ?years ago where? from 
the Google N-gram Count Corpus; these counts 
are 46 and 1815 respectively. Because the count 
of ?years ago where? is greater than that of ?years 
back where?, the former is selected as the correct 
form. As a result, the sentence ?This ability is not 
seen 40 years back where the technology ad-
vances were not as good as now.? is corrected to 
?This ability is not seen 40 years ago where the 
technology advances were not as good as now.? 
Some words are allowed to have multiple best 
frames; in all the best frames, if a candidate word 
sequence is more frequent than an original word 
sequence in the Google count, then correction is 
made. The multiple frames are also trained from 
the development data set.  
4.2 Probability n-gram Vector 
We use the probability n-gram Vector approach to 
correct Nn. Most errors are corrected using the 
router-based method; however, training the router 
for every noun is difficult because the number of 
nouns is extremely large. Moreover, for noun 
number, we found that rather than considering one 
direction or one frame of n-gram, every direction 
of n-gram should be considered for better perfor-
mance such as forward, backward, and two-way. 
Thus, the probability n-gram vector algorithm is 
applied only in the noun number error correction.  
We propose the probability n-gram vector method 
to correct grammatical errors to consider both di-
rections, forward and backward. In a forward n-
gram, the probability of each word is estimated 
Table 1. Example of Trained Router 
x (o?r) y 
(another?other) (1;3) 
(less?fewer) (1;3) 
(rise?raise) (1;2) 
(back?ago) (1;1) 
(could?can) (2;1) 
(well?good) (2;1) 
(near??) No correction 
  
 
68
depending on the preceding word. On the other 
hand, in a backward n-gram the probability of 
each word is estimated depending on the follow-
ing words. When the probability of a candidate 
word is higher than original word, we replace the 
original with the candidate word in the correction 
step. 
Probability n-gram vectors are generated from the 
original word and a candidate word (Figure 3). 
Rather than using a single sequence of n-gram 
probability, we apply contexts of various lengths 
and positions. We applied the probability infor-
mation using the Google n-gram count infor-
mation as in the following equation: 
 P(??|???2, ???1) =
?(???2,???1??)
?(???2,???1)
 
 
Moreover, rather than calculating one word?s 
probability given n words such as 
P(??|???1, ???2, ???3), our model calculates the 
probability of m words given an n word sequence. 
The following is an example 4-gram with forward 
probability: 
? m = 3, n = 1 P(???2, ???1??|???3) 
? m = 2, n = 2 P(???1, ??|???3, ???2) 
? m = 1, n = 3 P(??|???3, ???2, ???1). 
We construct a 40-dimensional probability vector 
with forward and backward probabilities consid-
ering of twenty 5-grams, twelve 4-grams, six 3-
gram, and two 2-gram. Additionally, the elements 
of the n-gram vector are detailed in Table 2. 
 
Back-Off Model: A high-order n-gram is more 
effective than a low-order n-gram. Thus, we ap-
plied back-off methods (Katz, 1987) to assign 
higher priority to higher order probabilities. If all 
elements in 5-gram vectors are 0 for both the orig-
inal and candidate sentence, which means 
? {?(?)? + ?(?)?} = 0
19
?=0 , we consider 4-gram 
vectors (N[20:31]). If 4-gram vectors are 0, we con-
sider 3-gram vectors. Moreover, when the pro-
posed method calculates each of the forward, 
backward and two-way probabilities, the back-off 
method is used to get each score.  
 
Correction: Here, we explain the process of error 
correction using n-gram vectors. First, we gener-
ate Nn error candidates. Second, we construct the 
n-gram probability vector for each candidate. The  
back-off method is applied in N(o)+N(r), The vec-
tor contains various directions and ranges of prob-
abilities of words given a sample sentence. We 
then calculate forward n-gram score by summing 
even elements in the vector. We calculate the 
backward n-gram by summing odd elements in 
Table 2. Next, the two-way n-gram is calculated 
by summing all elements for both directions n-
gram. If forward, backward, and two-way n-
grams have higher probabilities for the candidate 
word, we select the candidate as corrected word 
(Figure 3). 
Table 2: The elements of n-gram vector  
5-GRAM 
?0 = ?(??|??+1??+2??+3??+4) backward 
?1 = ?(??|???4???3???2???1) forward 
?2 = ?(????+1|??+2??+3??+4) backward 
??.. 
4-GRAM 
?20 = ?(??|??+1??+2??+3) backward 
?21 = ?(??|???3???2???1) forward 
??.. 
 
3-GRAM 
?32 = ?(??|??+1??+2) backward 
?33 = ?(??|???2???1) forward 
?34 = ?(????+1|??+2) backward 
?35 = ?(???1??|???2) forward 
?36 = ?(???1??|??+1) backward 
?37 = ?(????+1|???1) forward 
2-GRAM 
?38 = ?(??|??+1) backward 
?39 = ?(??|???1) forward 
 
 
Figure 3. Overall process of Nn Correction 
69
 5 Verb Correction (Rule-based)  
There are several types of verb errors in non-na-
tive text such as verb tense, verb modal, missing  
verb, verb form, and subject-verb-agreement 
(SVA). Among these errors, we attempt to correct 
SVA errors using rule-based methods (Table 3). 
In non-native text, parsing and tagging errors are 
inevitable, and it may cause false alarm. Thus, in-
stead of dependency parsing to find subject and 
verb, we consider the preceding five words be-
cause erroneous sentences often contain depend-
ency errors. Moreover, in erroneous sentences, 
POS tagging accuracy is lower than native text. 
Thus, NN and VB are misclassified, as are VBZ 
and NNS. A rule is used that encodes the relevant 
linguistic knowledge that these words or POSs 
should not occur in the five positions preceding 
the VBZ: ?NN?, ?this?, ?it? ,?one?, ?VBG?. Moreover, 
words that preceded and follow ?which? should 
agree in verb form, as indicated in Rule3 and 
Rule4. 
 
6 Experiment 
The CoNLL-2014 training data consist of 1,397 
articles together with gold-standard annotation. 
Algorithm Rule1-Comma 
1: function rule1( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if  toksent[i] in [ However?, ?Therefore?, ?Thus?] and not  toksent[i + 1] == ?,?  then 
4: toksent[i]= toksent[i] + ? ,? 
 
Algorithm Rule2-preposition 
1: function rule2( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if  toksent[i] = ?according? and not  toksent [i+1] = ?to? 
4:  toksent [i+1] = ?to ?+  toksent [i+1] 
 
Algorithm Rule3-Subject Verb Agreement 
1: function rule3( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if  toksent[i] is ?which? 
4: if  tokpos[i ? 1] == ?NNS? and  tokpos[i + 1] == ?VBZ?  then 
5: toksent[i + 1]= changeWordForm (toksent[i + 1], ?VBP?) 
6: else if  tokpos[i ? 1]  == ?NNS? and  tokpos[i + 1] == ?NNS? then 
7: toksent[i + 1]=  =changeWordForm(toksent[i + 1], ?VBP?) 
8: else if  tokpos[i ? 1] == ?NN? and  tokpos[i + 1]== ?are? then 
9: toksent[i + 1]=  = is 
10: else if  tokpos[i ? 1] == ?NN? and  tokpos[i + 1] in [?VBP?,?VB?,?NN?] then 
11: toksent[i + 1]=  = makePlural(toksent[i + 1]) 
 
Algorithm Rule4-Subject Verb Agreement 
1: function rule4( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if not ( tokpos[i]is ?VBZ? and [?NN?,?this?,?it?,?one?,?VBG?] in  tokpos[i ? 5: i]) then 
4:  tokcand?changeWordForm( tokword[i], ?VBP?) 
5: else if not ( tokpos[i]is ?VBP? and [?I?,?we?,?they?,?and?] in  toksent[i ? 5: i]) then 
6:  tokcand ?changeWordForm( tokword[i], ?VBZ?) 
7: else if not ( tokpos[i]is ?NN? and [?be?,?ing?] in  toksent[i ? 5: i]) then 
8:  tokcand?changeWordForm( tokword[i], ?VBN?) 
9:         original = ngramCount( toksent), candidate =ngramCount(tokcand) 
10: If original < candidate then 
11: Return tokcand 
Table 3. Examples of Rules  
 
 
70
The documents are a subset of the NUS Corpus of 
Learner English (NUCLE). We use the Max-
Match (M2) scorer provided by the CoNLL-2014 
Shared Task. The M2 scorer works by using the 
set that maximally matches the set of gold-stand-
ard edits specified by the annotator as being equal 
to the set of system edits that are automatically 
computed and used in scoring (Dahlmeier & Ng, 
2012). The official evaluation metric is F0.5, 
weighting precision twice as much as recall. We 
achieve F0.5 of 30.88; precision of 34.51; recall 
of 21.73 in the original annotation (Table 4). After 
original official annotations announced by organ-
izers (i.e., only based on the annotations of the two 
annotators), another set of annotations is offered 
based on including the additional answers pro-
posed by the 3 teams (CAMB, CUUI, UMC). The 
improvement gap between the original annotation 
and the revised annotation of our team (POST) is 
5.89%.  We obtain the highest improvement rate 
except for the 3 proposed teams (Figure 4), F0.5 
of 36.77; precision of 41.28; recall of 25.59 in the 
revised annotation. Our system achieves the 4th 
highest scores of 13 participating teams based on 
both the original and revised annotations. To ana-
lyze the scores of each of the error types and mod-
ules, we apply the method of n-gram vector (Nn), 
rule-based (Verb, Mec), and router-based (others) 
separately in both the original and the revised an-
notation of all error types. We achieve high preci-
sion by rules at the Mec which indicates punctua-
tion, capitalization, spelling, and typos errors. Ad-
ditionally, the Nn type has the highest improve-
ment gap between the original and revised anno-
tation (17% ? 24.31 of F0.5).  In order for our 
team to improve the high precision in the rule-
based approach, we tested potential rules on the 
development data and kept a rule only if its preci-
sion on that data set was 30% or greater. When we 
trained router, the same strategy was conducted. 
If a frame could not achieve 30% precision, we 
assigned the candidate pair as ?no correction? in 
the router. These constraints achieve precision of 
30 % in most error types.  
7 Discussion  
Although preposition errors are frequently com-
mitted in non-native text, we mostly skip the cor-
rection of preposition error. This is because as-
signing prepositions correctly is extremely diffi-
cult, because (1) the preposition used can vary 
(e.g., Canada: ?on the weekend? vs. Britain ?at the 
weekend?); (2) in a given location, more than one 
preposition may be possible, and the choice af-
fects the meaning (e.g., ?on the wall?, vs. ?at the 
wall?). Verb errors can consist of many multi-
 
Figure 4. Improvement gap between the original annotation and revised annotation of each team 
 
0
2
4
6
8
10
 
Table 4. Performance on each error type 
 Original annotation  Revised annotation 
 Precision Recall F0.5  Precision Recall F0.5 
N-gram (Nn) 31.0 6.55 17.75  42.28 9.0 24.31 
Rule (Verb) 28.95 1.12 4.86  31.17 1.29 5.52 
Rule (Mec) 49.34 5.47 18.94  52.16 6.17 20.93 
Router (Others) 28.11 12.49 22.49  35.29 15.45 28.08 
All 34.51 21.73 30.88  41.28 25.59 36.77 
 
71
word errors due to errors of usages of passive and 
active voice. (e.g. release?be released). Our cur-
rent system cannot correct these multi-words er-
rors, for three reasons. First, if the original exam-
ple consists of one word and the optimal replace-
ment consists of two words, n-gram scores cannot 
be applied easily to compare probabilities be-
tween them. Second, the n-gram approach also 
fails if the distance between subject and verb is 
more than 5. Third, multiply dependent errors are 
critical for verb error correction. For example, 
noun number, determiner, and subject verb agree-
ment are often dependent upon each other: e.g. 
?And once this happens, privacy does not exist 
any more and people's (life?lives) (is?are) un-
der great threaten.? The correction order will be 
important when all error type must be corrected 
simultaneously.  
Grammar error correction is a challenging 
problem. In CoNNL-2013, more than half of the 
related teams obtained F-score < 10.0. This low 
performance in the grammar error correction can 
be explained by several reasons, which indicate 
the present limitations of grammar correction sys-
tems. 
Among a total of 4206 pairs, we only use small 
amount of candidate pairs, 215 pairs are used for 
candidate pairs. The other 3991 pairs are dis-
carded in the router training step because these 
pairs cannot be corrected by the n-gram approach. 
Various classification methods and statistical ma-
chine translation based methods will be investi-
gated in the router-based approach to find the tai-
lored methods for the given word. A demonstra-
tion and progress of our grammar error correction 
system is available to the public3.  
8 Conclusion 
We have described the POSTECH grammatical 
error correction system. We use the Google N-
gram count corpus to detect spelling errors, punc-
tuation, and comma errors. A rule-based method 
is used to correct verb, punctuation, comma errors 
and preposition errors. The Google corpus is also 
used for an n-gram vector approach and a router-
based approaches. Currently we use the router to 
select the best frame. In the future, we will train a 
router to select the best method among classifica-
tion, n-gram approach, statistical machine transla-
                                                 
3 http://isoft.postech.ac.kr/grammar 
tion-based method and pattern matching ap-
proaches. A machine learning method will be used 
to train the router with various features.   
Acknowledgements 
This research was supported by the MSIP(The Ministry 
of Science, ICT and Future Planning), Korea and Mi-
crosoft Research, under IT/SW Creative research pro-
gram supervised by the NIPA(National IT Industry 
Promotion Agency) (NIPA-2013- H0503-13-1006) 
and this research was supported by the Basic Science 
Research Program through the National Research 
Foundation of Korea(NRF) funded by the Ministry of 
Education, Science and Technology(2010-0019523). 
 
References 
Han, Na-Rae, Chodorow, Martin, & Leacock, Claudia. 
(2006). Detecting errors in English article 
usage by non-native speakers.  
Alam, Md Jahangir, UzZaman, Naushad, & Khan, 
Mumit. (2006). N-gram based statistical 
grammar checker for Bangla and English.  
Bergsma, Shane, Lin, Dekang, & Goebel, Randy. 
(2009). Web-Scale N-gram Models for 
Lexical Disambiguation. Paper presented at 
the IJCAI. 
Brants, Thorsten, & Franz, Alex. (2006). The Google 
Web 1T 5-gram corpus version 1.1. 
LDC2006T13.  
Brockett, Chris, Dolan, William B, & Gamon, Michael. 
(2006). Correcting ESL errors using phrasal 
SMT techniques. Paper presented at the 
Proceedings of the 21st International 
Conference on Computational Linguistics and 
the 44th annual meeting of the Association for 
Computational Linguistics. 
Cahill, Aoife, Madnani, Nitin, Tetreault, Joel, & 
Napolitano, Diane. (2013). Robust Systems 
for Preposition Error Correction Using 
Wikipedia Revisions. Paper presented at the 
Proceedings of NAACL-HLT. 
Dahlmeier, Daniel, & Ng, Hwee Tou. (2011). 
Grammatical error correction with 
alternating structure optimization. Paper 
presented at the Proceedings of the 49th 
Annual Meeting of the Association for 
Computational Linguistics: Human Language 
Technologies-Volume 1. 
Dahlmeier, Daniel, & Ng, Hwee Tou. (2012). Better 
evaluation for grammatical error correction. 
Paper presented at the Proceedings of the 
2012 Conference of the North American 
Chapter of the Association for Computational 
Linguistics: Human Language Technologies. 
 
72
Dahlmeier, Daniel, Ng, Hwee Tou, & Wu, Siew Mei. 
(2013). Building a large annotated corpus of 
learner English: The NUS corpus of learner 
English. Paper presented at the Proceedings of 
the Eighth Workshop on Innovative Use of 
NLP for Building Educational Applications. 
De Felice, Rachele. (2008). Automatic error detection 
in non-native English. University of Oxford.    
De Marneffe, Marie-Catherine, & Manning, 
Christopher D. (2008). The Stanford typed 
dependencies representation. Paper presented 
at the Coling 2008: Proceedings of the 
workshop on Cross-Framework and Cross-
Domain Parser Evaluation. 
Gamon, Michael. (2010). Using mostly native data to 
correct errors in learners' writing: a meta-
classifier approach. Paper presented at the 
Human Language Technologies: The 2010 
Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics. 
Gamon, Michael, Leacock, Claudia, Brockett, Chris, 
Dolan, William B, Gao, Jianfeng, Belenko, 
Dmitriy, & Klementiev, Alexandre. (2009). 
Using statistical techniques and web search to 
correct ESL errors. Calico Journal, 26(3), 
491-511.  
Hermet, Matthieu, D?silets, Alain, & Szpakowicz, Stan. 
(2008). Using the web as a linguistic resource 
to automatically correct lexico-syntactic 
errors.  
Izumi, Emi, Uchimoto, Kiyotaka, & Isahara, Hitoshi. 
(2005). Error annotation for corpus of 
Japanese learner English. Paper presented at 
the Proceedings of the Sixth International 
Workshop on Linguistically Interpreted 
Corpora. 
Kao, Ting-Hui, Chang, Yu-Wei, Chiu, Hsun-Wen, & 
Yen, Tzu-Hsi. (2013). CoNLL-2013 Shared 
Task: Grammatical Error Correction NTHU 
System Description. CoNLL-2013, 20.  
Katz, Slava. (1987). Estimation of probabilities from 
sparse data for the language model 
component of a speech recognizer. Acoustics, 
Speech and Signal Processing, IEEE 
Transactions on, 35(3), 400-401.  
Knight, Kevin, & Chander, Ishwar. (1994). Automated 
postediting of documents. Paper presented at 
the AAAI. 
Mark, Alla Rozovskaya Kai-Wei Chang, & Roth, 
Sammons Dan. (2013). The University of 
Illinois System in the CoNLL-2013 Shared 
Task. CoNLL-2013, 51, 13.  
Naber, Daniel. (2003). A rule-based style and grammar 
checker. Diploma Thesis 
Nagata, Ryo, Morihiro, Koichiro, Kawai, Atsuo, & Isu, 
Naoki. (2006). A feedback-augmented method 
for detecting errors in the writing of learners 
of English. Paper presented at the Proceedings 
of the 21st International Conference on 
Computational Linguistics and the 44th 
annual meeting of the Association for 
Computational Linguistics. 
Ng, Hwee Tou , Wu, Siew Mei , Briscoe, Ted , 
Hadiwinoto, Christian , Susanto, Raymond 
Hendy, & Bryant, Christopher (2014). The 
CoNLL-2014 Shared Task on Grammatical 
Error Correction. Paper presented at the the 
Eighteenth Conference on Computational 
Natural Language Learning: Shared Task 
(CoNLL-2014 Shared Task), Baltimore, 
Maryland, USA. 
Nicholls, Diane. (2003). The Cambridge Learner 
Corpus: Error coding and analysis for 
lexicography and ELT. Paper presented at the 
Proceedings of the Corpus Linguistics 2003 
conference. 
Rozovskaya, Alla, & Roth, Dan. (2011). Algorithm 
selection and model adaptation for ESL 
correction tasks. Urbana, 51, 61801.  
Seo, Hongsuck, Lee, Jonghoon, Kim, Seokhwan, Lee, 
Kyusong, Kang, Sechun, & Lee, Gary 
Geunbae. (2012). A meta learning approach 
to grammatical error correction. Paper 
presented at the Proceedings of the 50th 
Annual Meeting of the Association for 
Computational Linguistics: Short Papers-
Volume 2. 
Wu, Yuanbin, & Ng, Hwee Tou. (2013). Grammatical 
error correction using integer linear 
programming. Paper presented at the 
Proceedings of the 51st Annual Meeting of 
the Association for Computational 
Linguistics. 
Xing, Junwen, Wang, Longyue, Wong, Derek F, Chao, 
Lidia S, & Zeng, Xiaodong. (2013). UM-
Checker: A Hybrid System for English 
Grammatical Error Cor-rection. CoNLL-2013, 
34.  
Yannakoudakis, Helen, Briscoe, Ted, & Medlock, Ben. 
(2011). A New Dataset and Method for 
Automatically Grading ESOL Texts. Paper 
presented at the ACL. 
 
 
73
