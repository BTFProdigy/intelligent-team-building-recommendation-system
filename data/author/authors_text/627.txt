Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809?816,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Names and Similarities on the Web: Fact Extraction in the Fast Lane
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Dekang Lin
Google Inc.
Mountain View, CA 94043
lindek@google.com
Jeffrey Bigham?
University of Washington
Seattle, WA 98195
jbigham@cs.washington.edu
Andrei Lifchits?
University of British Columbia
Vancouver, BC V6T 1Z4
alifchit@cs.ubc.ca
Alpa Jain?
Columbia University
New York, NY 10027
alpa@cs.columbia.edu
Abstract
In a new approach to large-scale extrac-
tion of facts from unstructured text, dis-
tributional similarities become an integral
part of both the iterative acquisition of
high-coverage contextual extraction pat-
terns, and the validation and ranking of
candidate facts. The evaluation mea-
sures the quality and coverage of facts
extracted from one hundred million Web
documents, starting from ten seed facts
and using no additional knowledge, lexi-
cons or complex tools.
1 Introduction
1.1 Background
The potential impact of structured fact reposito-
ries containing billions of relations among named
entities on Web search is enormous. They en-
able the pursuit of new search paradigms, the pro-
cessing of database-like queries, and alternative
methods of presenting search results. The prepa-
ration of exhaustive lists of hand-written extrac-
tion rules is impractical given the need for domain-
independent extraction of many types of facts from
unstructured text. In contrast, the idea of boot-
strapping for relation and information extraction
was first proposed in (Riloff and Jones, 1999), and
successfully applied to the construction of seman-
tic lexicons (Thelen and Riloff, 2002), named en-
tity recognition (Collins and Singer, 1999), extrac-
tion of binary relations (Agichtein and Gravano,
2000), and acquisition of structured data for tasks
such as Question Answering (Lita and Carbonell,
2004; Fleischman et al, 2003). In the context of
fact extraction, the resulting iterative acquisition
?Work done during internships at Google Inc.
framework starts from a small set of seed facts,
finds contextual patterns that extract the seed facts
from the underlying text collection, identifies a
larger set of candidate facts that are extracted by
the patterns, and adds the best candidate facts to
the previous seed set.
1.2 Contributions
Figure 1 describes an architecture geared towards
large-scale fact extraction. The architecture is sim-
ilar to other instances of bootstrapping for infor-
mation extraction. The main processing stages are
the acquisition of contextual extraction patterns
given the seed facts, acquisition of candidate facts
given the extraction patterns, scoring and ranking
of the patterns, and scoring and ranking of the can-
didate facts, a subset of which is added to the seed
set of the next round.
Within the existing iterative acquisition frame-
work, our first contribution is a method for au-
tomatically generating generalized contextual ex-
traction patterns, based on dynamically-computed
classes of similar words. Traditionally, the ac-
quisition of contextual extraction patterns requires
hundreds or thousands of consecutive iterations
over the entire text collection (Lita and Carbonell,
2004), often using relatively expensive or restric-
tive tools such as shallow syntactic parsers (Riloff
and Jones, 1999; Thelen and Riloff, 2002) or
named entity recognizers (Agichtein and Gravano,
2000). Comparatively, generalized extraction pat-
terns achieve exponentially higher coverage in
early iterations. The extraction of large sets of can-
didate facts opens the possibility of fast-growth it-
erative extraction, as opposed to the de-facto strat-
egy of conservatively growing the seed set by as
few as five items (Thelen and Riloff, 2002) after
each iteration.
809
Acquisition of contextual extraction patterns
Distributional similaritiesText collection
Candidate facts
Acquisition of candidate facts
Occurrences of extraction patterns
Validation of candidate facts
Scored extraction patternsScored candidate facts
Scoring and ranking
Validated candidate facts
Seed facts
Occurrences of seed facts Extraction patterns
Validated extraction patterns
Validation of patterns
Generalized extraction patterns
Figure 1: Large-scale fact extraction architecture
The second contribution of the paper is a
method for domain-independent validation and
ranking of candidate facts, based on a similar-
ity measure of each candidate fact relative to the
set of seed facts. Whereas previous studies as-
sume clean text collections such as news cor-
pora (Thelen and Riloff, 2002; Agichtein and Gra-
vano, 2000; Hasegawa et al, 2004), the valida-
tion is essential for low-quality sets of candidate
facts collected from noisy Web documents. With-
out it, the addition of spurious candidate facts to
the seed set would result in a quick divergence of
the iterative acquisition towards irrelevant infor-
mation (Agichtein and Gravano, 2000). Further-
more, the finer-grained ranking induced by simi-
larities is necessary in fast-growth iterative acqui-
sition, whereas previously proposed ranking crite-
ria (Thelen and Riloff, 2002; Lita and Carbonell,
2004) are implicitly designed for slow growth of
the seed set.
2 Similarities for Pattern Acquisition
2.1 Generalization via Word Similarities
The extraction patterns are acquired by matching
the pairs of phrases from the seed set into docu-
ment sentences. The patterns consist of contigu-
ous sequences of sentence terms, but otherwise
differ from the types of patterns proposed in earlier
work in two respects. First, the terms of a pattern
are either regular words or, for higher generality,
any word from a class of similar words. Second,
the amount of textual context encoded in a pat-
tern is limited to the sequence of terms between
(i.e., infix) the pair of phrases from a seed fact that
could be matched in a document sentence, thus ex-
cluding any context to the left (i.e., prefix) and to
the right (i.e., postfix) of the seed.
The pattern shown at the top of Figure 2, which
(Irving Berlin, 1888)
    NNP       NNP       CD
Infix
Aurelio de la Vega was born November 28 , 1925 , in Havana , Cuba .
    FW       FW FW  NNP VBD  VBN      NNP           CD  ,    CD    ,  IN    NNP      ,   NNP    .
foundnot found
Infix
not found
Prefix PostfixInfix
Matching on sentences
Seed fact Infix?only pattern
The poet was born Jan. 13 , several years after the revolution .
not found
British ? native Glenn Cornick of Jethro Tull was born April 23 , 1947 .
   NNP     :      JJ         NNP        NNP       IN    NNP     NNP  VBD  VBN   NNP   CD  ,   CD     .
Infix
foundfound
Chester Burton Atkins was born June 20 , 1924 , on a farm near Luttrell .
   NNP          NNP       NNP     VBD  VBN  NNP  CD  ,   CD     ,  IN DT  NN     IN       NNP       .
Infix
Infix
found
The youngest child of three siblings , Mariah Carey was born March 27 ,
1970 in Huntington , Long Island in New York .
  DT       JJS            NN     IN   CD        NNS       ,    NNP        NNP    VBD  VBN    NNP     CD  ,
  CD    IN       NNP             ,    JJ         NN      IN  NNP    NNP   .
found
foundfound
(S1)
(S2)
(S3)
(S4)
(S5)
(Jethro Tull, 1947)  (Mariah Carey, 1970)  (Chester Burton Atkins, 1924)
Candidate facts
  DT    NN   VBD  VBN  NNP CD ,       JJ           NNS     IN     DT        NN           .
N/A          CL1 born CL2 00 ,              N/A
Figure 2: Extraction via infix-only patterns
contains the sequence [CL1 born CL2 00 .], illus-
trates the use of classes of distributionally similar
words within extraction patterns. The first word
class in the sequence, CL1, consists of words such
as {was, is, could}, whereas the second class in-
cludes {February, April, June, Aug., November}
and other similar words. The classes of words are
computed on the fly over all sequences of terms
in the extracted patterns, on top of a large set of
pairwise similarities among words (Lin, 1998) ex-
tracted in advance from around 50 million news
articles indexed by the Google search engine over
three years. All digits in both patterns and sen-
tences are replaced with a common marker, such
810
that any two numerical values with the same num-
ber of digits will overlap during matching.
Many methods have been proposed to compute
distributional similarity between words, e.g., (Hin-
dle, 1990), (Pereira et al, 1993), (Grefenstette,
1994) and (Lin, 1998). Almost all of the methods
represent a word by a feature vector, where each
feature corresponds to a type of context in which
the word appeared. They differ in how the feature
vectors are constructed and how the similarity be-
tween two feature vectors is computed.
In our approach, we define the features of a
word w to be the set of words that occurred within
a small window of w in a large corpus. The context
window of an instance of w consists of the clos-
est non-stopword on each side of w and the stop-
words in between. The value of a feature w? is de-
fined as the pointwise mutual information between
w? and w: PMI(w?, w) = ? log( P (w,w?)P (w)P (w?)). The
similarity between two different words w1 and w2,
S(w1, w2), is then computed as the cosine of the
angle between their feature vectors.
While the previous approaches to distributional
similarity have only applied to words, we applied
the same technique to proper names as well as
words. The following are some example similar
words and phrases with their similarities, as ob-
tained from the Google News corpus:
? Carey: Higgins 0.39, Lambert 0.39, Payne
0.38, Kelley 0.38, Hayes 0.38, Goodwin 0.38,
Griffin 0.38, Cummings 0.38, Hansen 0.38,
Williamson 0.38, Peters 0.38, Walsh 0.38, Burke
0.38, Boyd 0.38, Andrews 0.38, Cunningham
0.38, Freeman 0.37, Stephens 0.37, Flynn 0.37,
Ellis 0.37, Bowers 0.37, Bennett 0.37, Matthews
0.37, Johnston 0.37, Richards 0.37, Hoffman
0.37, Schultz 0.37, Steele 0.37, Dunn 0.37, Rowe
0.37, Swanson 0.37, Hawkins 0.37, Wheeler 0.37,
Porter 0.37, Watkins 0.37, Meyer 0.37 [..];
? Mariah Carey: Shania Twain 0.38, Christina
Aguilera 0.35, Sheryl Crow 0.35, Britney Spears
0.33, Celine Dion 0.33, Whitney Houston 0.32,
Justin Timberlake 0.32, Beyonce Knowles 0.32,
Bruce Springsteen 0.30, Faith Hill 0.30, LeAnn
Rimes 0.30, Missy Elliott 0.30, Aretha Franklin
0.29, Jennifer Lopez 0.29, Gloria Estefan 0.29,
Elton John 0.29, Norah Jones 0.29, Missy
Elliot 0.29, Alicia Keys 0.29, Avril Lavigne
0.29, Kid Rock 0.28, Janet Jackson 0.28, Kylie
Minogue 0.28, Beyonce 0.27, Enrique Iglesias
0.27, Michelle Branch 0.27 [..];
? Jethro Tull: Motley Crue 0.28, Black Crowes
0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sab-
bath 0.26, Doobie Brothers 0.26, Judas Priest 0.26,
Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24,
Black Flag 0.24, Godsmack 0.24, Grateful Dead
0.24, Grand Funk Railroad 0.24, Smashing Pump-
kins 0.24, Led Zeppelin 0.24, Aerosmith 0.24,
Limp Bizkit 0.24, Counting Crows 0.24, Echo
And The Bunnymen 0.24, Cold Chisel 0.24, Thin
Lizzy 0.24 [..].
To our knowledge, the only previous study that
embeds similarities into the acquisition of extrac-
tion patterns is (Stevenson and Greenwood, 2005).
The authors present a method for computing pair-
wise similarity scores among large sets of poten-
tial syntactic (subject-verb-object) patterns, to de-
tect centroids of mutually similar patterns. By as-
suming the syntactic parsing of the underlying text
collection to generate the potential patterns in the
first place, the method is impractical on Web-scale
collections. Two patterns, e.g. chairman-resign
and CEO-quit, are similar to each other if their
components are present in an external hand-built
ontology (i.e., WordNet), and the similarity among
the components is high over the ontology. Since
general-purpose ontologies, and WordNet in par-
ticular, contain many classes (e.g., chairman and
CEO) but very few instances such as Osasuna,
Crewe etc., the patterns containing an instance
rather than a class will not be found to be simi-
lar to one another. In comparison, the classes and
instances are equally useful in our method for gen-
eralizing patterns for fact extraction. We merge
basic patterns into generalized patterns, regardless
of whether the similar words belong, as classes or
instances, in any external ontology.
2.2 Generalization via Infix-Only Patterns
By giving up the contextual constraints imposed
by the prefix and postfix, infix-only patterns rep-
resent the most aggressive type of extraction pat-
terns that still use contiguous sequences of terms.
In the absence of the prefix and postfix, the outer
boundaries of the fact are computed separately for
the beginning of the first (left) and end of the sec-
ond (right) phrases of the candidate fact. For gen-
erality, the computation relies only on the part-
of-speech tags of the current seed set. Starting
forward from the right extremity of the infix, we
collect a growing sequence of terms whose part-
of-speech tags are [P1+ P2+ .. Pn+], where the
811
notation Pi+ represents one or more consecutive
occurrences of the part-of-speech tag Pi. The se-
quence [P1 P2 .. Pn] must be exactly the sequence
of part of speech tags from the right side of one of
the seed facts. The point where the sequence can-
not be grown anymore defines the boundary of the
fact. A similar procedure is applied backwards,
starting from the left extremity of the infix. An
infix-only pattern produces a candidate fact from
a sentence only if an acceptable sequence is found
to the left and also to the right of the infix.
Figure 2 illustrates the process on the infix-
only pattern mentioned earlier, and one seed fact.
The part-of-speech tags for the seed fact are [NNP
NNP] and [CD] for the left and right sides respec-
tively. The infix occurs in all sentences. How-
ever, the matching of the part-of-speech tags of the
sentence sequences to the left and right of the in-
fix, against the part-of-speech tags of the seed fact,
only succeeds for the last three sentences. It fails
for the first sentence S1 to the left of the infix, be-
cause [.. NNP] (for Vega) does not match [NNP
NNP]. It also fails for the second sentence S2 to
both the left and the right side of the infix, since [..
NN] (for poet) does not match [NNP NNP], and
[JJ ..] (for several) does not match [CD].
3 Similarities for Validation and Ranking
3.1 Revisiting Standard Ranking Criteria
Because some of the acquired extraction patterns
are too generic or wrong, all approaches to iter-
ative acquisition place a strong emphasis on the
choice of criteria for ranking. Previous literature
quasi-unanimously assesses the quality of each
candidate fact based on the number and qual-
ity of the patterns that extract the candidate fact
(more is better); and the number of seed facts ex-
tracted by the same patterns (again, more is bet-
ter) (Agichtein and Gravano, 2000; Thelen and
Riloff, 2002; Lita and Carbonell, 2004). However,
our experiments using many variations of previ-
ously proposed scoring functions suggest that they
have limited applicability in large-scale fact ex-
traction, for two main reasons. The first is that
it is impractical to perform hundreds of acquisi-
tion iterations on terabytes of text. Instead, one
needs to grow the seed set aggressively in each
iteration. Previous scoring functions were im-
plicitly designed for cautious acquisition strate-
gies (Collins and Singer, 1999), which expand the
seed set very slowly across consecutive iterations.
In that case, it makes sense to single out a small
number of best candidates, among the other avail-
able candidates. Comparatively, when 10,000 can-
didate facts or more need to be added to a seed set
of 10 seeds as early as after the first iteration, it
is difficult to distinguish the quality of extraction
patterns based, for instance, only on the percent-
age of the seed set that they extract. The second
reason is the noisy nature of the Web. A substan-
tial number of factors can and will concur towards
the worst-case extraction scenarios on the Web.
Patterns of apparently high quality turn out to pro-
duce a large quantity of erroneous ?facts? such as
(A-League, 1997), but also the more interesting
(Jethro Tull, 1947) as shown earlier in Figure 2, or
(Web Site David, 1960) or (New York, 1831). As
for extraction patterns of average or lower quality,
they will naturally lead to even more spurious ex-
tractions.
3.2 Ranking of Extraction Patterns
The intuition behind our criteria for ranking gen-
eralized pattern is that patterns of higher preci-
sion tend to contain words that are indicative of
the relation being mined. Thus, a pattern is more
likely to produce good candidate facts if its in-
fix contains the words language or spoken if ex-
tracting Language-SpokenIn-Country facts, or the
word capital if extracting City-CapitalOf-Country
relations. In each acquisition iteration, the scor-
ing of patterns is a two-pass procedure. The first
pass computes the normalized frequencies of all
words excluding stopwords, over the entire set of
extraction patterns. The computation applies sep-
arately to the prefix, infix and postfix of the pat-
terns. In the second pass, the score of an extraction
pattern is determined by the words with the high-
est frequency score in its prefix, infix and postfix,
as computed in the first pass and adjusted for the
relative distance to the start and end of the infix.
3.3 Ranking of Candidate Facts
Figure 3 introduces a new scheme for assessing the
quality of the candidate facts, based on the compu-
tation of similarity scores for each candidate rela-
tive to the set of seed facts. A candidate fact, e.g.,
(Richard Steele, 1672), is similar to the seed set if
both its phrases, i.e., Richard Steele and 1672, are
similar to the corresponding phrases (John Lennon
or Stephen Foster in the case of Richard Steele)
from the seed facts. For a phrase of a candidate
fact to be assigned a non-default (non-minimum)
812
...
Lennon
Lambert
McFadden
Bateson
McNamara
Costello
Cronin
Wooley
Baker
...
Foster
Hansen
Hawkins
Fisher
Holloway
Steele
Sweeney
Chris
John
James
Andrew
Mike
Matt
Brian
Christopher
...
John Lennon         1940
Seed facts
Stephen Foster      1826
Brian McFadden           1980
(4)(3)
Robert S. McNamara    1916
(6)(5)
Barbara Steele               1937
(7) (2)
Stan Hansen                  1949
(9)(8)
Similar wordsSimilar words
for: John
Similar words
for: Stephen
for: Lennon
Similar words
for: Foster
...
Stephen
Robert
Michael
Peter
William
Stan
Richard(1)
Barbara
(3)
(5)
(7) (2)
(8)
(9)
(4)
(6)
(2)(1)
Candidate facts
Jethro Tull                     1947
Richard Steele               1672
Figure 3: The role of similarities in estimating the
quality of candidate facts
similarity score, the words at its extremities must
be similar to one or more words situated at the
same positions in the seed facts. This is the case
for the first five candidate facts in Figure 3. For ex-
ample, the first word Richard from one of the can-
didate facts is similar to the first word John from
one of the seed facts. Concurrently, the last word
Steele from the same phrase is similar to Foster
from another seed fact. Therefore Robert Foster
is similar to the seed facts. The score of a phrase
containing N words is:
{
C1 +
?N
i=1 log(1 + Simi) , if Sim1,N > 0
C2 , otherwise.
where Simi is the similarity of the component
word at position i in the phrase, and C1 and C2
are scaling constants such that C2C1. Thus,
the similarity score of a candidate fact aggregates
individual word-to-word similarity scores, for the
left side and then for the right side of a candidate
fact. In turn, the similarity score of a component
word Simi is higher if: a) the computed word-to-
word similarity scores are higher relative to words
at the same position i in the seeds; and b) the com-
ponent word is similar to words from more than
one seed fact.
The similarity scores are one of a linear com-
bination of features that induce a ranking over the
candidate facts. Three other domain-independent
features contribute to the final ranking: a) a phrase
completeness score computed statistically over the
entire set of candidate facts, which demotes candi-
date facts if any of their two sides is likely to be
incomplete (e.g., Mary Lou vs. Mary Lou Retton,
or John F. vs. John F. Kennedy); b) the average
PageRank value over all documents from which
the candidate fact is extracted; and c) the pattern-
based scores of the candidate fact. The latter fea-
ture converts the scores of the patterns extracting
the candidate fact into a score for the candidate
fact. For this purpose, it considers a fixed-length
window of words around each match of a candi-
date fact in some sentence from the text collection.
This is equivalent to analyzing all sentence con-
texts from which a candidate fact can be extracted.
For each window, the word with the highest fre-
quency score, as computed in the first pass of the
procedure for scoring the patterns, determines the
score of the candidate fact in that context. The
overall pattern-based score of a candidate fact is
the sum of the scores over all its contexts of occur-
rence, normalized by the frequency of occurrence
of the candidate over all sentences.
Besides inducing a ranking over the candidate
facts, the similarity scores also serve as a valida-
tion filter over the candidate facts. Indeed, any
candidates that are not similar to the seed set can
be filtered out. For instance, the elimination of
(Jethro Tull, 1947) is a side effect of verifying that
Tull is not similar to any of the last-position words
from phrases in the seed set.
4 Evaluation
4.1 Data
The source text collection consists of three chunks
W1, W2, W3 of approximately 100 million doc-
uments each. The documents are part of a larger
snapshot of the Web taken in 2003 by the Google
search engine. All documents are in English.
The textual portion of the documents is cleaned
of Html, tokenized, split into sentences and part-
of-speech tagged using the TnT tagger (Brants,
2000).
The evaluation involves facts of type Person-
BornIn-Year. The reasons behind the choice of
this particular type are threefold. First, many
Person-BornIn-Year facts are probably available
on the Web (as opposed to, e.g., City-CapitalOf-
Country facts), to allow for a good stress test
for large-scale extraction. Second, either side of
the facts (Person and Year) may be involved in
many other types of facts, such that the extrac-
tion would easily divergence unless it performs
correctly. Third, the phrases from one side (Per-
son) have an utility in their own right, for lexicon
813
Table 1: Set of seed Person-BornIn-Year facts
Name Year Name Year
Paul McCartney 1942 John Lennon 1940
Vincenzo Bellini 1801 Stephen Foster 1826
Hoagy Carmichael 1899 Irving Berlin 1888
Johann Sebastian Bach 1685 Bela Bartok 1881
Ludwig van Beethoven 1770 Bob Dylan 1941
construction or detection of person names.
The Person-BornIn-Year type is specified
through an initial set of 10 seed facts shown in Ta-
ble 1. Similarly to source documents, the facts are
also part-of-speech tagged.
4.2 System Settings
In each iteration, the case-insensitive matching of
the current set of seed facts onto the sentences pro-
duces basic patterns. The patterns are converted
into generalized patterns. The length of the infix
may vary between 1 and 6 words. Potential pat-
terns are discarded if the infix contains only stop-
words.
When a pattern is retained, it is used as an
infix-only pattern, and allowed to generate at most
600,000 candidate facts. At the end of an itera-
tion, approximately one third of the validated can-
didate facts are added to the current seed set. Con-
sequently, the acquisition expands the initial seed
set of 10 facts to 100,000 facts (after iteration 1)
and then to one million facts (after iteration 2) us-
ing chunk W1.
4.3 Precision
A separate baseline run extracts candidate facts
from the text collection following the traditional
iterative acquisition approach. Pattern general-
ization is disabled, and the ranking of patterns
and facts follows strictly the criteria and scoring
functions from (Thelen and Riloff, 2002), which
are also used in slightly different form in (Lita
and Carbonell, 2004) and (Agichtein and Gravano,
2000). The theoretical option of running thou-
sands of iterations over the text collection is not
viable, since it would imply a non-justifiable ex-
pense of our computational resources. As a more
realistic compromise over overly-cautious acqui-
sition, the baseline run retains as many of the top
candidate facts as the size of the current seed,
whereas (Thelen and Riloff, 2002) only add the
top five candidate facts to the seed set after each it-
eration. The evaluation considers all 80, a sample
of the 320, and another sample of the 10,240 facts
retained after iterations 3, 5 and 10 respectively.
The correctness assessment of each fact consists
in manually finding some Web page that contains
clear evidence that the fact is correct. If no such
page exists, the fact is marked as incorrect. The
corresponding precision values after the three iter-
ations are 91.2%, 83.8% and 72.9%.
For the purpose of evaluating the precision of
our system, we select a sample of facts from
the entire list of one million facts extracted from
chunk W1, ranked in decreasing order of their
computed scores. The sample is generated auto-
matically from the top of the list to the bottom, by
retaining a fact and skipping the following consec-
utive N facts, where N is incremented at each step.
The resulting list, which preserves the relative or-
der of the facts, contains 1414 facts. The 115 facts
for which a Web search engine does not return any
documents, when the name (as a phrase) and the
year are submitted together in a conjunctive query,
are discarded from the sample of 1414 facts. In
those cases, the facts were acquired from the 2003
snapshot of the Web, but queries are submitted to
a search engine with access to current Web doc-
uments, hence the difference when some of the
2003 documents are no longer available or index-
able.
Based on the sample set, the average preci-
sion of the list of one million facts extracted from
chunk W1 is 98.5% over the top 1/100 of the list,
93.1% over the top half of the list, and 88.3% over
the entire list of one million facts. Table 2 shows
examples of erroneous facts extracted from chunk
W1. Causes of errors include incorrect approxima-
tions of the name boundaries (e.g., Alma in Alma
Theresa Rausch is incorrectly tagged as an adjec-
tive), and selection of the wrong year as birth year
(e.g., for Henry Lumbar).
In the case of famous people, the extracted facts
tend to capture the correct birth year for several
variations of the names, as shown in Table 3. Con-
versely, it is not necessary that a fact occur with
high frequency in order for it to be extracted,
which is an advantage over previous approaches
that rely strongly on redundancy (cf. (Cafarella et
al., 2005)). Table 4 illustrates a few of the cor-
rectly extracted facts that occur rarely on the Web.
4.4 Recall
In contrast to the assessment of precision, recall
can be evaluated automatically, based on external
814
Table 2: Incorrect facts extracted from the Web
Spurious Fact Context in Source Sentence
(Theresa Rausch, Alma Theresa Rausch was born
1912) on 9 March 1912
(Henry Lumbar, Henry Lumbar was born 1861
1937) and died 1937
(Concepcion Paxety, Maria de la Concepcion Paxety
1817) b. 08 Dec. 1817 St. Aug., FL.
(Mae Yaeger, Ella May/Mae Yaeger was born
1872) 20 May 1872 in Mt.
(Charles Whatley, Long, Charles Whatley b. 16
1821) FEB 1821 d. 29 AUG
(HOLT George W. HOLT (new line) George W. Holt
Holt, 1845) was born in Alabama in 1845
(David Morrish David Morrish (new line)
Canadian, 1953) Canadian, b. 1953
(Mary Ann, 1838) had a daughter, Mary Ann, who
was born in Tennessee in 1838
(Mrs. Blackmore, Mrs. Blackmore was born April
1918) 28, 1918, in Labaddiey
Table 3: Birth years extracted for both
pseudonyms and corresponding real names
Pseudonym Real Name Year
Gloria Estefan Gloria Fajardo 1957
Nicolas Cage Nicolas Kim Coppola 1964
Ozzy Osbourne John Osbourne 1948
Ringo Starr Richard Starkey 1940
Tina Turner Anna Bullock 1939
Tom Cruise Thomas Cruise Mapother IV 1962
Woody Allen Allen Stewart Konigsberg 1935
lists of birth dates of various people. We start by
collecting two gold standard sets of facts. The first
set is a random set of 609 actors and their birth
years from a Web compilation (GoldA). The sec-
ond set is derived from the set of questions used
in the Question Answering track (Voorhees and
Tice, 2000) of the Text REtrieval Conference from
1999 through 2002. Each question asking for the
birth date of a person (e.g., ?What year was Robert
Frost born??) results in a pair containing the per-
son?s name and the birth year specified in the an-
swer keys. Thus, the second gold standard set
contains 17 pairs of people and their birth years
(GoldT ). Table 5 shows examples of facts in each
of the gold standard sets.
Table 6 shows two types of recall scores com-
puted against the gold standard sets. The recall
scores over ?Gold take into consideration only the
set of person names from the gold standard with
some extracted year(s). More precisely, given that
some years were extracted for a person name, it
verifies whether they include the year specified in
the gold standard for that person name. Compar-
atively, the recall score denoted AllGold is com-
Table 4: Extracted facts that occur infrequently
Fact Source Domain
(Irvine J Forcier, 1912) geocities.com
(Marie Louise Azelie Chabert, 1861) vienici.com
(Jacob Shalles, 1750) selfhost.com
(Robert Chester Claggett, 1898) rootsweb.com
(Charoltte Mollett, 1843) rootsweb.com
(Nora Elizabeth Curran, 1979) jimtravis.com
Table 5: Composition of gold standard sets
Gold Set Composition and Examples of Facts
GoldA Actors (Web compilation) Nr. facts: 609
(Andie MacDowell, 1958), (Doris Day,
1924), (Diahann Carroll, 1935)
GoldT People (TREC QA track) Nr. facts: 17
(Davy Crockett, 1786), (Julius Caesar,
100 B.C.), (King Louis XIV, 1638)
puted over the entire set of names from the gold
standard.
For the GoldA set, the size of the ?Gold set of
person names changes little when the facts are ex-
tracted from chunk W1 vs. W2 vs. W3. The re-
call scores over ?Gold exhibit little variation from
one Web chunk to another, whereas the AllGold
score is slightly higher on the W3 chunk, prob-
ably due to a higher number of documents that
are relevant to the extraction task. When the facts
are extracted from a combination of two or three
of the available Web chunks, the recall scores
computed over AllGold are significantly higher as
the size of the ?Gold set increases. In compar-
ison, the recall scores over the growing ?Gold
set increases slightly with larger evaluation sets.
The highest value of the recall score for GoldA
is 89.9% over the ?Gold set, and 70.7% over
AllGold. The smaller size of the second gold stan-
dard set, GoldT , explains the higher variation of
the values shown in the lower portion of Table 6.
4.5 Comparison to Previous Results
Another recent approach specifically addresses the
problem of extracting facts from a similarly-sized
collection of Web documents. In (Cafarella et al,
2005), manually-prepared extraction rules are ap-
plied to a collection of 60 million Web documents
to extract entities of types Company and Country,
as well as facts of type Person-CeoOf-Company
and City-CapitalOf-Country. Based on manual
evaluation of precision and recall, a total of 23,128
company names are extracted at precision of 80%;
the number decreases to 1,116 at precision of 90%.
In addition, 2,402 Person-CeoOf-Company facts
815
Table 6: Automatic evaluation of recall, over two
gold standard sets GoldA (609 person names) and
GoldT (17 person names)
Gold Set Input Data Recall (%)
(Web Chunk) ?Gold AllGold
GoldA W1 86.4 49.4
W2 85.0 50.5
W3 86.3 54.1
W1+W2 88.5 64.5
W1+W2+W3 89.9 70.7
GoldT W1 81.8 52.9
W2 90.0 52.9
W3 100.0 64.7
W1+W2 81.8 52.9
W1+W2+W3 91.6 64.7
are extracted at precision 80%. The recall value is
80% at precision 90%. Recall is evaluated against
the set of company names extracted by the system,
rather than an external gold standard with pairs of
a CEO and a company name. As such, the result-
ing metric for evaluating recall used in (Cafarella
et al, 2005) is somewhat similar to, though more
relaxed than, the recall score over the ?Gold set
introduced in the previous section.
5 Conclusion
The combination of generalized extraction pat-
terns and similarity-driven ranking criteria results
in a fast-growth iterative approach for large-scale
fact extraction. From 10 Person-BornIn-Year facts
and no additional knowledge, a set of one million
facts of the same type is extracted from a collec-
tion of 100 million Web documents of arbitrary
quality, with a precision around 90%. This cor-
responds to a growth ratio of 100,000:1 between
the size of the extracted set of facts and the size
of the initial set of seed facts. To our knowledge,
the growth ratio and the number of extracted facts
are several orders of magnitude higher than in any
of the previous studies on fact extraction based on
either hand-written extraction rules (Cafarella et
al., 2005), or bootstrapping for relation and infor-
mation extraction (Agichtein and Gravano, 2000;
Lita and Carbonell, 2004). The next research steps
converge towards the automatic construction of a
searchable repository containing billions of facts
regarding people.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting
relations from large plaintext collections. In Proceedings
of the 5th ACM International Conference on Digital Li-
braries (DL-00), pages 85?94, San Antonio, Texas.
T. Brants. 2000. TnT - a statistical part of speech tagger.
In Proceedings of the 6th Conference on Applied Natural
Language Processing (ANLP-00), pages 224?231, Seattle,
Washington.
M. Cafarella, D. Downey, S. Soderland, and O. Etzioni.
2005. KnowItNow: Fast, scalable information extrac-
tion from the web. In Proceedings of the Human Lan-
guage Technology Conference (HLT-EMNLP-05), pages
563?570, Vancouver, Canada.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the 1999
Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP/VLC-99),
pages 189?196, College Park, Maryland.
M. Fleischman, E. Hovy, and A. Echihabi. 2003. Offline
strategies for online question answering: Answering ques-
tions before they are asked. In Proceedings of the 41st
Annual Meeting of the Association for Computational Lin-
guistics (ACL-03), pages 1?7, Sapporo, Japan.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus
Discovery. Kluwer Academic Publishers, Boston, Mas-
sachusetts.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415?
422, Barcelona, Spain.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of the 28th Annual
Meeting of the Association for Computational Linguistics
(ACL-90), pages 268?275, Pittsburgh, Pennsylvania.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of the 17th International Confer-
ence on Computational Linguistics and the 36th Annual
Meeting of the Association for Computational Linguistics
(COLING-ACL-98), pages 768?774, Montreal, Quebec.
L. Lita and J. Carbonell. 2004. Instance-based ques-
tion answering: A data driven approach. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-04), pages 396?403,
Barcelona, Spain.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clus-
tering of english words. In Proceedings of the 31st Annual
Meeting of the Association for Computational Linguistics
(ACL-93), pages 183?190, Columbus, Ohio.
E. Riloff and R. Jones. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In Pro-
ceedings of the 16th National Conference on Artificial In-
telligence (AAAI-99), pages 474?479, Orlando, Florida.
M. Stevenson and M. Greenwood. 2005. A semantic ap-
proach to IE pattern induction. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-05), pages 379?386, Ann Arbor, Michigan.
M. Thelen and E. Riloff. 2002. A bootstrapping method for
learning semantic lexicons using extraction pattern con-
texts. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-02),
pages 214?221, Philadelphia, Pennsylvania.
E.M. Voorhees and D.M. Tice. 2000. Building a question-
answering test collection. In Proceedings of the 23rd
International Conference on Research and Development
in Information Retrieval (SIGIR-00), pages 200?207,
Athens, Greece.
816
Proceedings of NAACL-HLT 2013, pages 201?210,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Text Alignment for Real-Time Crowd Captioning
Iftekhar Naim, Daniel Gildea, Walter Lasecki and Jeffrey P. Bigham
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
The primary way of providing real-time cap-
tioning for deaf and hard of hearing people
is to employ expensive professional stenogra-
phers who can type as fast as natural speak-
ing rates. Recent work has shown that a
feasible alternative is to combine the partial
captions of ordinary typists, each of whom
types part of what they hear. In this paper,
we describe an improved method for combin-
ing partial captions into a final output based
on weighted A? search and multiple sequence
alignment (MSA). In contrast to prior work,
our method allows the tradeoff between accu-
racy and speed to be tuned, and provides for-
mal error bounds. Our method outperforms
the current state-of-the-art on Word Error Rate
(WER) (29.6%), BLEU Score (41.4%), and
F-measure (36.9%). The end goal is for
these captions to be used by people, and so
we also compare how these metrics correlate
with the judgments of 50 study participants,
which may assist others looking to make fur-
ther progress on this problem.
1 Introduction
Real-time captioning provides deaf or hard of hear-
ing people access to speech in mainstream class-
rooms, at public events, and on live television. To
maintain consistency between the captions being
read and other visual cues, the latency between when
a word was said and when it is displayed must be
under five seconds. The most common approach to
real-time captioning is to recruit a trained stenogra-
pher with a special purpose phonetic keyboard, who
transcribes the speech to text within approximately 5
seconds. Unfortunately, professional captionists are
quite expensive ($150 per hour), must be recruited in
blocks of an hour or more, and are difficult to sched-
ule on short notice. Automatic speech recognition
(ASR) (Saraclar et al, 2002) attempts to solve this
TXLFNIR[OD]\GRJ
&RPELQHUWKHEURZQIR[MXPSHG
IR[MXPSHGRYHUWKHOD]\
WKHTXLFNEURZQIR[MXPSHGRYHUWKHOD]\GRJ
)LQDO&DSWLRQ
0HUJLQJ,QFRPSOHWH&DSWLRQV
&
&
&
Figure 1: General layout of crowd captioning systems.
Captionists (C1, C2, C3) submit partial captions that are
automatically combined into a high-quality output.
problem by converting speech to text completely au-
tomatically. However, the accuracy of ASR quickly
plummets to below 30% when used on an untrained
speaker?s voice, in a new environment, or in the ab-
sence of a high quality microphone (Wald, 2006b).
An alternative approach is to combine the efforts
of multiple non-expert captionists (anyone who can
type) (Lasecki et al, 2012; Lasecki and Bigham,
2012; Lasecki et al, 2013). In this approach, mul-
tiple non-expert human workers transcribe an audio
stream containing speech in real-time, and their par-
tial input is combined to produce a final transcript
(see Figure 1). This approach has been shown to
dramatically outperform ASR in terms of both accu-
racy and Word Error Rate (WER), even when us-
ing captionists drawn from Amazon?s Mechanical
Turk. Furthermore, recall approached and even ex-
ceeded that of a trained expert stenographer with
seven workers contributing, suggesting that the in-
formation is present to meet the performance of a
stenographer. However, combining these captions
involves real-time alignment of partial captions that
may be incomplete and that often have spelling er-
rors and inconsistent timestamps. In this paper,
we present a more accurate combiner that leverages
201
Multiple Sequence Alignment (MSA) and Natural
Language Processing to improve performance.
Gauging the quality of captions is not easy. Al-
though word error rate (WER) is commonly used in
speech recognition, it considers accuracy and com-
pleteness, not readability. As a result, a lower WER
does not always result in better understanding (Wang
et al, 2003). We compare WER with two other com-
monly used metrics: BLEU (Papineni et al, 2002)
and F-measure (Melamed et al, 2003), and report
their correlation with that of 50 human evaluators.
The key contributions of this paper are as follows:
? We have implemented an A?-search based Mul-
tiple Sequence Alignment algorithm (Lermen
and Reinert, 2000) that can trade-off speed and
accuracy by varying the heuristic weight and
chunk-size parameters. We show that it outper-
forms previous approaches in terms of WER,
BLEU score, and F-measure.
? We propose a beam-search based technique us-
ing the timing information of the captions that
helps to restrict the search space and scales ef-
fectively to align longer sequences efficiently.
? We evaluate the correlation of WER, BLEU,
and F-measure with 50 human ratings of cap-
tion readability, and found that WER was more
highly correlated than BLEU score (Papineni
et al, 2002), implying it may be a more useful
metric overall when evaluating captions.
2 Related Work
Most of the previous research on real-time caption-
ing has focused on Automated Speech Recognition
(ASR) (Saraclar et al, 2002; Cooke et al, 2001;
Praz?a?k et al, 2012). However, experiments show
that ASR systems are not robust enough to be ap-
plied for arbitrary speakers and in noisy environ-
ments (Wald, 2006b; Wald, 2006a; Bain et al, 2005;
Bain et al, 2012; Cooke et al, 2001).
2.1 Crowd Captioning
To address these limitations of ASR-based tech-
niques, the Scribe system collects partial captions
from the crowd and then uses a graph-based in-
cremental algorithm to combine them on the fly
(Lasecki et al, 2012). The system incrementally
builds a chain graph, where each node represents a
set of equivalent words entered by the workers and
the link between nodes are adjusted according to the
order of the input words. A greedy search is per-
formed to identify the path with the highest confi-
dence, based on worker input and an n-gram lan-
guage model. The algorithm is designed to be used
online, and hence has high speed and low latency.
However, due to the incremental nature of the algo-
rithm and due to the lack of a principled objective
function, it is not guaranteed to find the globally op-
timal alignment for the captions.
2.2 Multiple Sequence Alignment
The problem of aligning and combining multiple
transcripts can be mapped to the well-studied Mul-
tiple Sequence Alignment (MSA) problem (Edgar
and Batzoglou, 2006). MSA is an important prob-
lem in computational biology (Durbin et al, 1998).
The goal is to find an optimal alignment from a
given set of biological sequences. The pairwise
alignment problem can be solved efficiently using
dynamic programming in O(N2) time and space,
where N is the sequence length. The complexity of
the MSA problem grows exponentially as the num-
ber of sequences grows, and has been shown to be
NP-complete (Wang and Jiang, 1994). Therefore,
it is important to apply some heuristic to perform
MSA in a reasonable amount of time.
Most MSA algorithms for biological sequences
follow a progressive alignment strategy that first per-
forms pairwise alignment among the sequences, and
then builds a guide tree based on the pairwise simi-
larity between these sequences (Edgar, 2004; Do et
al., 2005; Thompson et al, 1994). Finally, the input
sequences are aligned according to the order spec-
ified by the guide tree. While not commonly used
for biological sequences, MSA with A?-style search
has been applied to these problems by Horton (1997)
and Lermen and Reinert (2000).
Lasecki et al explored MSA in the context of
merging partial captions by using the off-the-shelf
MSA tool MUSCLE (Edgar, 2004), replacing the nu-
cleotide characters by English characters (Lasecki
et al, 2012). The substitution cost for nucleotides
was replaced by the ?keyboard distance? between
English characters, learned from the physical lay-
out of a keyboard and based on common spelling
202
errors. However, MUSCLE relies on a progressive
alignment strategy and may result in suboptimal so-
lutions. Moreover, it uses characters as atomic sym-
bols instead of words. Our approach operates on a
per-word basis and is able to arrive at a solution that
is within a selectable error-bound of optimal.
3 Multiple Sequence Alignment
We start with an overview of the MSA problem us-
ing standard notations as described by Lermen and
Reinert (2000). Let S1, . . . , SK ,K ? 2, be the K
sequences over an alphabet ?, and having length
N1, . . . , NK . The special gap symbol is denoted by
??? and does not belong to ?. Let A = (aij) be a
K ? Nf matrix, where aij ? ? ? {?}, and the ith
row has exactly (Nf ? Ni) gaps and is identical to
Si if we ignore the gaps. Every column of A must
have at least one non-gap symbol. Therefore, the jth
column of A indicates an alignment state for the jth
position, where the state can have one of the 2K ? 1
possible combinations. Our goal is to find the op-
timum alignment matrix AOPT that minimizes the
sum of pairs (SOP) cost function:
c(A) =
?
1?i?j?K
c(Aij) (1)
where c(Aij) is the cost of the pairwise alignment
between Si and Sj according to A. Formally,
c(Aij) =
?Nf
l=1 sub(ail, ajl), where sub(ail, ajl)
denotes the cost of substituting ajl for ail. If ail
and ajl are identical, the substitution cost is usu-
ally zero. For the caption alignment task, we treat
each individual word as a symbol in our alphabet
?. The substitution cost for two words is estimated
based on the edit distance between two words. The
exact solution to the SOP optimization problem is
NP-Complete, but many methods solve it approxi-
mately. In this paper, we adapt weighted A? search
for approximately solving the MSA problem.
3.1 A? Search for MSA
The problem of minimizing the SOP cost func-
tion for K sequences is equivalent to estimating the
shortest path between a single source and single sink
node in a K-dimensional lattice. The total num-
ber of nodes in the lattice is (N1 + 1) ? (N2 +
Algorithm 1 MSA-A? Algorithm
Require: K input sequences S = {S1, . . . , SK} having
length N1, . . . , NK , heuristic weight w, beam size b
1: start? 0K , goal? [N1, . . . , NK ]
2: g(start)? 0, f(start)? w ? h(start).
3: Q? {start}
4: while Q 6= ? do
5: n? EXTRACT-MIN(Q)
6: for all s ? {0, 1}K ? {0K} do
7: ni ? n + s
8: if ni = goal then
9: Return the alignment matrix for the reconstructed
path from start to ni
10: else if ni 6? Beam(b) then
11: continue;
12: else
13: g(ni)? g(n) + c(n, ni)
14: f(ni)? g(ni) + w ? h(ni)
15: INSERT-ITEM(Q, ni, f(ni))
16: end if
17: end for
18: end while
1) ? ? ? ? ? (NK + 1), each corresponding to a dis-
tinct position in K sequences. The source node is
[0, . . . , 0] and the sink node is [N1, . . . , NK ]. The
dynamic programming algorithm for estimating the
shortest path from source to sink treats each node
position [n1, . . . , nK ] as a state and calculates a ma-
trix that has one entry for each node. Assuming the
sequences have roughly same length N , the size of
the dynamic programming matrix is O(NK). At
each vertex, we need to minimize the cost over all
its 2K ? 1 predecessor nodes, and, for each such
transition, we need to estimate the SOP objective
function that requires O(K2) operations. Therefore,
the dynamic programming algorithm has time com-
plexity of O(K22KNK) and space complexity of
O(NK), which is infeasible for most practical prob-
lem instances. However, we can efficiently solve it
via heuristic A? search (Lermen and Reinert, 2000).
We use A? search based MSA (shown in Algo-
rithm 1, illustrated in Figure 2) that uses a prior-
ity queue Q to store dynamic programming states
corresponding to node positions in the K dimen-
sional lattice. Let n = [n1, . . . , nK ] be any node
in the lattice, s be the source, and t be the sink. The
A? search can find the shortest path using a greedy
Best First Search according to an evaluation func-
tion f(n), which is the summation of the cost func-
203

Q Q 7
&
&
&
WKH TXLFN EURZQ IR[ MXPSHG RYHU WKH OD]\ GRJ
WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\ BBBBBBBB BBBB
BBBBBBBBBBBBBBBB
BBBBBBBBBBBBBB
WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\

WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\BBBBBB BBBB
BBBBBB
BBBB WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\BBBBBB BBBB
BBBBBBBBBBB
BBBB WKH EURZQ IR[ MXPSHG
TXLFN IR[ OD]\ GRJ
IR[ MXPSHG RYHU WKH OD]\ BBBBBBBB BBBB
BBBBBBBBBBBBBBBB
BBBBBBBBBBBBBB
L
BMXPSHG MXPSHG GRJ
BB
BB
IR[IR[
IR[6

 N
BBBB
RYHU
OD]\
BB
RYHU

BBBB
BB
&DSWLRQ
&DSWLRQ
&DSWLRQ
OD]\BB BB
Figure 2: A? MSA search algorithm. Each branch is one of 2K ? 1 possible alignments for the current input. The
branch with minimum sum of the current alignment cost and the expected heuristic value hpair (precomputed).
tions g(n) and the heuristic function h(n) for node
n. The cost function g(n) denotes the cost of the
shortest path from the source s to the current node
n. The heuristic function h(n) is the approximate
estimated cost of the shortest path from n to the des-
tination t. At each step of the A? search algorithm,
we extract the node with the smallest f(n) value
from the priority queue Q and expand it by one edge.
The heuristic function h(n) is admissible if it never
overestimates the cost of the cheapest solution from
n to the destination. An admissible heuristic func-
tion guarantees that A? will explore the minimum
number of nodes and will always find the optimal
solution. One commonly used admissible heuristic
function is hpair(n):
hpair(n) = L(n ? t) =
?
1?i<j?K
c(A?p(?ni , ?nj ))
(2)
where L(n ? t) denotes the lower bound on the
cost of the shortest path from n to destination t, A?p
is the optimal pairwise alignment, and ?ni is the suf-
fix of node n in the i-th sequence. A? search using
the pairwise heuristic function hpair significantly re-
duces the search space and also guarantees finding
the optimal solution. We must be able to estimate
hpair(n) efficiently. It may appear that we need to
estimate the optimal pairwise alignment for all the
pairs of suffix sequences at every node. However,
we can precompute the dynamic programming ma-
trix over all the pair of sequences (Si, Sj) once from
the backward direction, and then reuse these values
at each node. This simple trick significantly speeds
up the computation of hpair(n).
Despite the significant reduction in the search
space, the A? search may still need to explore a
large number of nodes, and may become too slow
for real-time captioning. However, we can further
improve the speed by following the idea of weighted
A? search (Pohl, 1970). We modify the evaluation
function f(n) = g(n)+hpair(n) to a weighted eval-
uation function f ?(n) = g(n) + whpair(n), where
w ? 1 is a weight parameter. By setting the value
of w to be greater than 1, we increase the relative
weight of the estimated cost to reach the destina-
tion. Therefore, the search prefers the nodes that are
closer to the destination, and thus reaches the goal
faster. Weighted A? search can significantly reduce
the number of nodes to be examined, but it also loses
the optimality guarantee of the admissible heuristic
function. We can trade-off between accuracy and
speed by tuning the weight parameter w.
3.2 Beam Search using Time-stamps
The computational cost of the A? search algorithm
grows exponentially with increase in the number of
sequences. However, in order to keep the crowd-
sourced captioning system cost-effective, only a
small number of workers are generally recruited at
a time (typically K ? 10). We, therefore, are more
concerned about the growth in computational cost as
the sequence length increases.
In practice, we break down the sequences into
smaller chunks by maintaining a window of a given
time interval, and we apply MSA only to the smaller
chunks of captions entered by the workers during
that time window. As the window size increases,
the accuracy of our MSA based combining system
increases, but so does the computational cost and la-
tency. Therefore, it is important to apply MSA with
a relatively small window size for real-time caption-
ing applications. Another interesting application can
be the offline captioning, for example, captioning an
entire lecture and uploading the captions later.
For the offline captioning problem, we can fo-
cus less on latency and more on accuracy by align-
ing longer sequences. To restrict the search space
from exploding with sequence length (N ), we apply
a beam constraint on our search space using the time
stamps of each captioned words. For example, if we
204
1. so now what i want to do is introduce some of the
2. what i wanna do is introduce some of the aspects of the class
3. so now what i want to do is is introduce some of the aspects of the class
4. so now what i want to do is introduce
5. so now what i want to do is introduce some of the operational of the class
6. so i want to introduce some of the operational aspects of the clas
C. so now what i want to do is introduce some of the operational aspects of the class
Figure 3: An example of applying MSA-A? (threshold tv = 2) to combine 6 partial captions (first 6 lines) by human
workers to obtain the final output caption (C).
set the beam size to be 20 seconds, then we ignore
any state in our search space that aligns two words
having more than 20 seconds time lag. Given a fixed
beam size b, we can restrict the number of priority
queue removals by the A? algorithm to O(NbK).
The maximum size of the priority queue is O(NbK).
For each node in the priority queue, for each of the
O(2K) successor states, the objective function and
heuristic estimation requires O(K2) operations and
each priority queue insertion requires O(log(NbK))
i.e. O(logN + K log b) operations. Therefore,
the overall worst case computational complexity is
O
(
NbK2K(K2 + logN + K log b)
)
. Note that for
fixed beam size b and number of sequences K, the
computational cost grows as O(N logN) with the
increase in N . However, in practice, weighted A?
search explores much smaller number of states com-
pared to this beam-restricted space.
3.3 Majority Voting after Alignment
After aligning the captions by multiple workers in a
given chunk, we need to combine them to obtain the
final caption. We do that via majority voting at each
position of the alignment matrix containing a non-
gap symbol. In case of tie, we apply the language
model to choose the most likely word.
Often workers type in nonstandard symbols, ab-
breviations, or misspelled words that do not match
with any other workers? input and end up as a sin-
gle word aligned to gaps in all the other sequences.
To filter out such spurious words, we apply a vot-
ing threshold (tv) during majority voting and filter
out words having less than tv votes. Typically we
set tv = 2 (see the example in Figure 3). While ap-
plying the voting threshold improves the word error
rate and readability, it runs the risk of loosing correct
words if they are covered by only a single worker.
3.4 Incorporating an N-gram Language Model
We also experimented with a version of our system
designed to incorporate the score from an n-gram
language model into the search. For this purpose,
we modified the alignment algorithm to produce a
hypothesized output string as it moves through the
input strings, as opposed to using voting to produce
the final string as a post-processing step. The states
for our dynamic programming are extended to in-
clude not only the current position in each input
string, but also the last two words of the hypothesis
string (i.e. [n1, . . . , nK , wi?1, wi?2]) for use in com-
puting the next trigram language model probability.
We replace our sum-of-all-pairs objective function
with the sum of the alignment cost of each input with
the hypothesis string, to which we add the log of the
language model probability and a feature for the to-
tal number of words in the hypothesis. Mathemati-
cally, we consider the hypothesis string to be the 0th
row of the alignment matrix, making our objective
function:
c(A) =
?
1?i?K
c(A0,i) + wlen
Nf
?
l=1
I(a0,l 6= ?)
+ wlm
Nf
?
l=1
logP (a0,l|a0,l?2, a0,l?1)
where wlm and wlen are negative constants indicat-
ing the relative weights of the language model prob-
ability and the length penalty.
Extending states with two previous words results
in a larger computational complexity. Given K se-
quences of length N each, we can have O(NK) dis-
tinct words. Therefore, the number distinct states
is O(NbK(NK)2) i.e. O(N3K2bK). Each state
can have O(K2K) successors, giving an overall
computational complexity of O(N3K3bK2K(K2 +
logN + logK + K log b)). Alternatively, if the vo-
205
cabulary size |V | is smaller than NK, the number of
distinct states is bounded by O(NbK |V |2).
3.5 Evaluation Metric for Speech to Text
Captioning
Automated evaluation of speech to text captioning is
known to be a challenging task (Wang et al, 2003).
Word Error Rate (WER) is the most commonly used
metric that finds the best pairwise alignment be-
tween the candidate caption and the ground truth
reference sentence. WER is estimated as S+I+DN ,
where S, I , and D is the number of incorrect word
substitutions, insertions, and deletions required to
match the candidate sentence with reference, and N
is the total number of words in the reference. WER
has several nice properties such as: 1) it is easy
to estimate, and 2) it tries to preserve word order-
ing. However, WER does not account for the overall
?readability? of text and thus does not always corre-
late well with human evaluation (Wang et al, 2003;
He et al, 2011).
The widely-used BLEU metric has been shown
to agree well with human judgment for evaluating
translation quality (Papineni et al, 2002). However,
unlike WER, BLEU imposes no explicit constraints
on the word ordering. BLEU has been criticized as
an ?under-constrained? measure (Callison-Burch et
al., 2006) for allowing too much variation in word
ordering. Moreover, BLEU does not directly esti-
mate recall, and instead relies on the brevity penalty.
Melamed et al (2003) suggest that a better approach
is to explicitly measure both precision and recall and
combine them via F-measure.
Our application is similar to automatic speech
recognition in that there is a single correct output,
as opposed to machine translation where many out-
puts can be equally correct. On the other hand, un-
like with ASR, out-of-order output is frequently pro-
duced by our alignment system when there is not
enough overlap between the partial captions to de-
rive the correct ordering for all words. It may be
the case that even such out-of-order output can be
of value to the user, and should receive some sort of
partial credit that is not possible using WER. For
this reason, we wished to systematically compare
BLEU, F-measure, and WER as metrics for our task.
We performed a study to evaluate the agreement
of the three metrics with human judgment. We ran-
Metric Spearman Corr. Pearson Corr.
1-WER 0.5258 0.6282
BLEU 0.3137 0.6181
F-measure 0.4389 0.6240
Table 1: The correlation of average human judgment with
three automated metrics: 1-WER, BLEU, and F-measure.
domly extracted one-minute long audio clips from
four MIT OpenCourseWare lectures. Each clip was
transcribed by 7 human workers, and then aligned
and combined using four different systems: the
graph-based system, and three different versions of
our weighted A? algorithm with different values of
tuning parameters. Fifty people participated in the
study and were split in two equal sized groups. Each
group was assigned two of the four audio clips,
and each person evaluated all four captions for both
clips. Each participant assigned a score between 1
to 10 to these captions, based on two criteria: 1) the
overall estimated agreement of the captions with the
ground truth text, and 2) the readability and under-
standability of the captions.
Finally, we estimated the correlation coefficients
(both Spearman and Pearson) for the three metrics
discussed above with respect to the average score
assigned by the human participants. The results
are presented in Table 1. Among the three metrics,
WER had the highest agreement with the human par-
ticipants. This indicates that reconstructing the cor-
rect word order is in fact important to the users, and
that, in this aspect, our task has more of the flavor of
speech recognition than of machine translation.
4 Experimental Results
We experiment with the MSA-A? algorithm for cap-
tioning different audio clips, and compare the results
with two existing techniques. Our experimental set
up is similar to the experiments by Lasecki et al
(2012). Our dataset consists of four 5-minute long
audio clips extracted from lectures available on MIT
OpenCourseWare. The audio clips contain speech
from electrical engineering and chemistry lectures.
Each audio clip is transcribed by ten non-expert hu-
man workers in real-time. We then combine these
inputs using our MSA-A? algorithm, and also com-
pare with the existing graph-based system and mul-
206
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.58 0.60
0.36
0.47
0.54
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.60
0.63
0.40
0.49
0.41
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.53 0.55
0.35
0.45 0.42
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.49 0.51
0.26
0.36
0.30
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.53 0.55
0.44
0.39 0.37
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.56 0.56
0.45
0.39
0.19
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.43 0.44 0.41
0.35
0.23
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.43
0.46
0.36
0.29
0.09
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.62 0.64
0.53
0.47
0.55
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 0.63 0.63
0.53
0.45 0.44
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.52 0.54 0.49
0.43 0.39
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.53 0.56
0.46
0.38
0.35
(1.0-WER) BLEU Score F-Measure
D
at
a 
Se
t 1
D
at
a 
Se
t 2
D
at
a 
Se
t 3
D
at
a 
Se
t 4
A*-10-t 
(c=10 sec, threshold=2)
A*-15-t 
(c=15 sec, threshold=2)
A*-15 
(c=15 sec, no threshold)
Graph-
based
MUSCLE
Figure 4: Evaluation of different systems on using three
different automated metrics for measuring transcription
quality: 1- Word Error Rate (WER), BLEU, and F-
measure on the four audio clips.
tiple sequence alignment using MUSCLE.
As explained earlier, we vary the four key pa-
rameters of the algorithm: the chunk size (c), the
heuristic weight (w), the voting threshold (tv), and
the beam size (b). The heuristic weight and chunk
size parameters help us to trade-off between speed
versus accuracy; the voting threshold tv helps im-
prove precision by pruning words having less than
tv votes, and beam size reduces the search space by
restricting states to be inside a time window/beam.
We use affine gap penalty (Edgar, 2004) with dif-
ferent gap opening and gap extension penalty. We
set gap opening penalty to 0.125 and gap extension
penalty to 0.05. We evaluate the performance using
the three standard metrics: Word Error Rate (WER),
BLEU, and F-measure. The performance in terms of
these metrics using different systems is presented in
Figure 4.
Out of the five systems in Figure 4, the first three
are different versions of our A? search based MSA
algorithm with different parameter settings: 1) A?-
10-t system (c = 10 seconds, tv = 2), 2) A?-15-t (c =
15 seconds, tv = 2), and 3) A?-15 (c = 15 seconds, tv
= 1 i.e. no pruning while voting). For all three sys-
tems, the heuristic weight parameter w is set to 2.5
and beam size b = 20 seconds. The other two sys-
tems are the existing graph-based system and mul-
tiple sequence alignment using MUSCLE. Among
the three A? based algorithms, both A?-15-t and A?-
10-t produce better quality transcripts and outper-
form the existing algorithms. Both systems apply
the voting threshold that improves precision. The
system A?-15 applies no threshold and ends up pro-
ducing many spurious words having poor agreement
among the workers, and hence it scores worse in all
the three metrics. The A?-15-t achieves 57.4% aver-
age accuracy in terms of (1-WER), providing 29.6%
improvement with respect to the graph-based sys-
tem (average accuracy 42.6%), and 35.4% improve-
ment with respect to the MUSCLE-based MSA sys-
tem (average accuracy 41.9%). On the same set of
audio clips, Lasecki et al (2012) reported 36.6% ac-
curacy using ASR (Dragon Naturally Speaking, ver-
sion 11.5 for Windows), which is worse than all the
crowd-based based systems used in this experiment.
To measure the statistical significance of this im-
provement, we performed a t-test at both the dataset
level (n = 4 clips) and the word level (n = 2862
words). The improvement over the graph-based
model was statistically significant with dataset level
p-value 0.001 and word level p-value smaller than
0.0001. The average time to align each 15 second
chunk with 10 input captions is ?400 milliseconds.
We have also experimented with a trigram lan-
guage model, trained on the British National Cor-
pus (Burnard, 1995) having ?122 million words.
The language-model-integrated A? search provided
a negligible 0.21% improvement in WER over the
A?-15-t system on average. The task of combin-
ing captions does not require recognizing words; it
only requires aligning them in the correct order. This
could explain why language model did not improve
accuracy, as it does for speech recognition. Since
the standard MSA-A? algorithm (without language
model) produced comparable accuracy and faster
running time, we used that version in the rest of the
207
2 3 4 5 6 7 8
0.42
0.44
0.46
0.48
0.5
0.52
0.54
0.56
0.58
1?
W
ER
Avg Running Time (in Seconds)
 
 
c = 5
c = 10
c = 15
c = 20
c = 40
c = 60
(a) Varying heuristic weights for fixed chunk sizes (c)
2 3 4 5 6 7 8
0.42
0.44
0.46
0.48
0.5
0.52
0.54
0.56
0.58
1?
W
ER
Avg Running Time (in Seconds)
 
 
w = 1.8
w = 2
w = 2.5
w = 3
w = 4
w = 6
w = 8
(b) Varying chunk size for fixed heuristic weight (w)
Figure 5: The trade-off between speed and accuracy for different heuristic weights and chunk size parameters.
experiments.
Next, we look at the critical speed versus accuracy
trade-off for different values of the heuristic weight
(w) and the chunk size (c) parameters. Since WER
has been shown to correlate most with human judg-
ment, we show the next results only with respect to
WER. First, we fix the chunk size at different val-
ues, and then vary the heuristic weight parameter:
w = 1.8, 2, 2.5, 3, 4, 6, and 8. The results are
shown in Figure 5(a), where each curve represents
how time and accuracy changed over the range of
values of w and a fixed value of c. We observe that
for smaller values of w, the algorithm is more accu-
rate, but comparatively slower. As w increases, the
search reaches the goal faster, but the quality of the
solution degrades as well. Next, we fix w and vary
chunk size c = 5, 10, 15, 20, 40, 60 second. We re-
peat this experiment for a range of values of w and
the results are shown in Figure 5(b). We can see that
the accuracy improves steeply up to c = 20 seconds,
and does not improve much beyond c = 40 seconds.
For all these benchmarks, we set the beam size (b)
to 20 seconds and voting threshold (tv) to 2.
In our tests, the beam size parameter (b) did not
play a significant role in performance, and setting it
to any reasonably large value (usually ? 15 seconds)
resulted in similar accuracy and running time. This
is because the A? search with hpair heuristic already
reduces the the search space significantly, and usu-
ally reaches the goal in a number of steps smaller
than the state space size after the beam restriction.
Finally, we investigate how the accuracy of our
algorithm varies with the number of inputs/workers.
We start with a pool of 10 input captions for one of
the audio clips. We vary the number of input cap-
tions (K) to the MSA-A? algorithm from 2 up to 10.
The quality of input captions differs greatly among
the workers. Therefore, for each value of K, we re-
peat the experiment min
(
20,
(10
K
))
times; each time
we randomly select K input captions out of the total
pool of 10. Figure 6 shows that accuracy steeply
increases as the number of inputs increases to 7,
and after that adding more workers does not pro-
vide much improvement in accuracy, but increases
running time.
5 Discussion and Future Work
In this paper, we show that the A? search based
MSA algorithm performs better than existing algo-
rithms for combining multiple captions. The exist-
ing graph-based model has low latency, but it usually
can not find a near optimal alignment because of its
incremental alignment. Weighted A? search on the
other hand performs joint multiple sequence align-
ment, and is guaranteed to produce a solution hav-
ing cost no more than (1 + ?) times the cost of the
optimal solution, given a heuristic weight of (1+ ?).
Moreover, A? search allows for straightforward in-
tegration of an n-gram language model during the
search.
Another key advantage of the proposed algorithm
is the ease with which we can trade-off between
208
0 2 4 6 8 10
0
0.1
0.2
0.3
0.4
0.5
0.6
Av
er
ag
e 
(1?
W
ER
)
Average Running Time (in sec)
Figure 6: Experiments showing how the accuracy of the
final caption by MSA-A? algorithm varies with the num-
ber of inputs from 2 to 10.
speed and accuracy. The algorithm can be tailored
to real-time by using a larger heuristic weight. On
the other hand, we can produce better transcripts for
offline tasks by choosing a smaller weight.
It is interesting to compare our results with those
achieved using the MUSCLE MSA tool of Edgar
(2004). One difference is that our system takes a hi-
erarchical approach in that it aligns at the word level,
but also uses string edit distance at the letter level
as a substitution cost for words. Thus, it is able to
take advantage of the fact that individual transcrip-
tions do not generally contain arbitrary fragments of
words. More fundamentally, it is interesting to note
that MUSCLE and most other commonly used MSA
tools for biological sequences make use of a guide
tree formed by a hierarchical clustering of the in-
put sequences. The guide tree produced by the algo-
rithms may or may not match the evolutionary tree
of the organisms whose genomes are being aligned,
but, nevertheless, in the biological application, such
an underlying evolutionary tree generally exists. In
aligning transcriptions, there is no particular reason
to expect individual pairs of transcriptions to be es-
pecially similar to one another, which may make the
guide tree approach less appropriate.
In order to get competitive results, the A? search
based algorithm aligns sequences that are at least 7-
10 seconds long. The delay for collecting the cap-
tions within a chunk can introduce latency, however,
each alignment usually takes less than 300 millisec-
onds, allowing us to repeatedly align the stream of
words, even before the window is filled. This pro-
vides less accurate but immediate response to users.
Finally, when we have all the words entered in a
chunk, we perform the final alignment and show the
caption to users for the entire chunk.
After aligning the input sequences, we obtain the
final transcript by majority voting at each alignment
position, which treats each worker equally and does
not take individual quality into account. Recently,
some work has been done for automatically estimat-
ing individual worker?s quality for crowd-based data
labeling tasks (Karger et al, 2011; Liu et al, 2012).
Extending these methods for crowd-based text cap-
tioning could be an interesting future direction.
6 Conclusion
In this paper, we have introduced a new A? search
based MSA algorithm for aligning partial captions
into a final output stream in real-time. This method
has advantages over prior approaches both in for-
mal guarantees of optimality and the ability to trade
off speed and accuracy. Our experiments on real
captioning data show that it outperforms prior ap-
proaches based on a dependency graph model and a
standard MSA implementation (MUSCLE). An ex-
periment with 50 participants explored whether ex-
iting automatic metrics of quality matched human
evaluations of readability, showing WER did best.
Acknowledgments Funded by NSF awards IIS-
1218209 and IIS-0910611.
References
Keith Bain, Sara Basson, A Faisman, and D Kanevsky.
2005. Accessibility, transcription, and access every-
where. IBM Systems Journal, 44(3):589?603.
Keith Bain, Eunice Lund-Lucas, and Janice Stevens.
2012. 22. transcribe your class: Using speech recogni-
tion to improve access for at-risk students. Collected
Essays on Learning and Teaching, 5.
Lou Burnard. 1995. Users Reference Guide British Na-
tional Corpus Version 1.0.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation research. In Proceedings of EACL,
volume 2006, pages 249?256.
209
Martin Cooke, Phil Green, Ljubomir Josifovski, and As-
cension Vizinho. 2001. Robust automatic speech
recognition with missing and unreliable acoustic data.
Speech Communication, 34(3):267?285.
Chuong B Do, Mahathi SP Mahabhashyam, Michael
Brudno, and Serafim Batzoglou. 2005. Prob-
cons: Probabilistic consistency-based multiple se-
quence alignment. Genome Research, 15(2):330?340.
Richard Durbin, Sean R Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence analy-
sis: probabilistic models of proteins and nucleic acids.
Cambridge university press.
Robert C Edgar and Serafim Batzoglou. 2006. Multi-
ple sequence alignment. Current opinion in structural
biology, 16(3):368?373.
Robert C Edgar. 2004. MUSCLE: multiple sequence
alignment with high accuracy and high throughput.
Nucleic Acids Research, 32(5):1792?1797.
Xiaodong He, Li Deng, and Alex Acero. 2011. Why
word error rate is not a good metric for speech rec-
ognizer training for the speech translation task? In
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2011, pages 5632?
5635. IEEE.
Phillip B Horton. 1997. Strings, algorithms, and ma-
chine learning applications for computational biology.
Ph.D. thesis, University of California, Berkeley.
David R Karger, Sewoong Oh, and Devavrat Shah. 2011.
Iterative learning for reliable crowdsourcing systems.
In Proceedings of Advances in Neural Information
Processing Systems (NIPS), volume 24, pages 1953?
1961.
Walter Lasecki and Jeffrey Bigham. 2012. Online qual-
ity control for real-time crowd captioning. In Pro-
ceedings of the 14th international ACM SIGACCESS
conference on Computers and accessibility (ASSETS
2012), pages 143?150. ACM.
Walter Lasecki, Christopher Miller, Adam Sadilek, An-
drew Abumoussa, Donato Borrello, Raja Kushalnagar,
and Jeffrey Bigham. 2012. Real-time captioning by
groups of non-experts. In Proceedings of the 25rd an-
nual ACM symposium on User interface software and
technology, UIST ?12.
Walter Lasecki, Christopher Miller, and Jeffrey Bigham.
2013. Warping time for more effective real-time
crowdsourcing. In Proceedings of the ACM confer-
ence on Human Factors in Computing Systems, CHI
?13, page To Appear, New York, NY, USA. ACM.
Martin Lermen and Knut Reinert. 2000. The prac-
tical use of the A* algorithm for exact multiple se-
quence alignment. Journal of Computational Biology,
7(5):655?671.
Qiang Liu, Jian Peng, and Alex Ihler. 2012. Varia-
tional inference for crowdsourcing. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS), volume 25, pages 701?709.
Dan Melamed, Ryan Green, and Joseph P Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings HLT-NAACL 2003, volume 2, pages 61?63.
Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of Association for Computational
Linguistics, pages 311?318. Association for Computa-
tional Linguistics.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(3):193?204.
Ales? Praz?a?k, Zdene?k Loose, Jan Trmal, Josef V Psutka,
and Josef Psutka. 2012. Captioning of Live
TV Programs through Speech Recognition and Re-
speaking. In Text, Speech and Dialogue, pages 513?
519. Springer.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), pages
1741?1744.
Julie D Thompson, Desmond G Higgins, and Toby J
Gibson. 1994. Clustal w: improving the sensitivity
of progressive multiple sequence alignment through
sequence weighting, position-specific gap penalties
and weight matrix choice. Nucleic Acids Research,
22(22):4673?4680.
Mike Wald. 2006a. Captioning for deaf and hard of
hearing people by editing automatic speech recogni-
tion in real time. Computers Helping People with Spe-
cial Needs, pages 683?690.
Mike Wald. 2006b. Creating accessible educational mul-
timedia through editing automatic speech recognition
captioning in real time. Interactive Technology and
Smart Education, 3(2):131?141.
Lusheng Wang and Tao Jiang. 1994. On the complexity
of multiple sequence alignment. Journal of Computa-
tional Biology, 1(4):337?348.
Ye-Yi Wang, Alex Acero, and Ciprian Chelba. 2003. Is
word error rate a good indicator for spoken language
understanding accuracy. In IEEE Workshop on Auto-
matic Speech Recognition and Understanding, 2003.
ASRU?03. 2003, pages 577?582. IEEE.
210
