Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 896?903,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Sequencing Model for Situation Entity Classification
Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith
Department of Linguistics
University of Texas at Austin
{alexispalmer,ponvert,jbaldrid,carlotasmith}@mail.utexas.edu
Abstract
Situation entities (SEs) are the events, states,
generic statements, and embedded facts and
propositions introduced to a discourse by
clauses of text. We report on the first data-
driven models for labeling clauses according
to the type of SE they introduce. SE classifi-
cation is important for discourse mode iden-
tification and for tracking the temporal pro-
gression of a discourse. We show that (a)
linguistically-motivated cooccurrence fea-
tures and grammatical relation information
from deep syntactic analysis improve clas-
sification accuracy and (b) using a sequenc-
ing model provides improvements over as-
signing labels based on the utterance alone.
We report on genre effects which support the
analysis of discourse modes having charac-
teristic distributions and sequences of SEs.
1 Introduction
Understanding discourse requires identifying the
participants in the discourse, the situations they par-
ticipate in, and the various relationships between and
among both participants and situations. Coreference
resolution, for example, is concerned with under-
standing the relationships between references to dis-
course participants. This paper addresses the prob-
lem of identifying and classifying references to situ-
ations expressed in written English texts.
Situation entities (SEs) are the events, states,
generic statements, and embedded facts and propo-
sitions which clauses introduce (Vendler, 1967;
Verkuyl, 1972; Dowty, 1979; Smith, 1991; Asher,
1993; Carlson and Pelletier, 1995). Consider the
text passage below, which introduces an event-type
entity in (1), a report-type entity in (2), and a state-
type entity in (3).
(1) Sony Corp. has heavily promoted the VideoWalkman
since the product?s introduction last summer ,
(2) but Bob Gerson , video editor of This Week in Con-
sumer Electronics , says
(3) Sony conceives of 8mm as a ?family of products ,
camcorders and VCR decks , ?
SE classification is a fundamental component in de-
termining the discourse mode of texts (Smith, 2003)
and, along with aspectual classification, for tempo-
ral interpretation (Moens and Steedman, 1988). It
may be useful for discourse relation projection and
discourse parsing.
Though situation entities are well-studied in lin-
guistics, they have received very little computational
treatment. This paper presents the first data-driven
models for SE classification. Our two main strate-
gies are (a) the use of linguistically-motivated fea-
tures and (b) the implementation of SE classification
as a sequencing task. Our results also provide empir-
ical support for the very notion of discourse modes,
as we see clear genre effects in SE classification.
We begin by discussing SEs in more detail. Sec-
tion 3 describes our two annotated data sets and pro-
vides examples of each SE type. Section 4 discusses
feature sets, and sections 5 and 6 present models,
experiments, and results.
896
2 Discourse modes and situation entities
In this section, we discuss some of the linguistic mo-
tivation for SE classification and the relation of SE
classification to discourse mode identification.
2.1 Situation entities
The categorization of SEs into aspectual classes is
motivated by patterns in their linguistic behavior.
We adopt an expanded version of a paradigm relat-
ing SEs to discourse mode (Smith, 2003) and char-
acterize SEs with four broad categories:
1. Eventualities. Events (E), particular states (S),
and reports (R). R is a sub-type of E for SEs
introduced by verbs of speech (e.g., say).
2. General statives. Generics (G) and generaliz-
ing sentences (GS). The former are utterances
predicated of a general class or kind rather than
of any specific individual. The latter are habit-
ual utterances that refer to ongoing actions or
properties predicated of specific individuals.
3. Abstract entities. Facts (F) and proposi-
tions (P).1
4. Speech-act types. Questions (Q) and impera-
tives (IMP).
Examples of each SE type are given in section 3.2.
There are a number of linguistic tests for iden-
tifying situation entities (Smith, 2003). The term
linguistic test refers to a rule which correlates an
SE type to particular linguistic forms. For exam-
ple, event-type verbs in simple present tense are a
linguistic correlate of GS-type SEs.
These linguistic tests vary in their precision and
different tests may predict different SE types for
the same clause. A rule-based implementation us-
ing them to classify SEs would require careful rule
ordering or mediation of rule conflicts. However,
since these rules are exactly the sort of information
extracted as features in data-driven classifiers, they
1In our system these two SE types are identified largely as
complements of factive and propositional verbs as discussed
in Peterson (1997). Fact and propositional complements have
some linguistic as well as some notional differences. Facts may
have causal effects, and facts are in the world. Neither of these
is true for propositions. In addition, the two have somewhat
different semantic consequences of a presuppositional nature.
can be cleanly integrated by assigning them empiri-
cally determined weights. We use maximum entropy
models (Berger et al, 1996), which are particularly
well-suited for tasks (like ours) with many overlap-
ping features, to harness these linguistic insights by
using features in our models which encode, directly
or indirectly, the linguistic correlates to SE types.
The features are described in detail in section 4.
2.2 Basic and derived situation types
Situation entities each have a basic situation type,
determined by the verb plus its arguments, the verb
constellation. The verb itself plays a key role in de-
termining basic situation type but it is not the only
factor. Changes in the arguments or tense of the verb
sometimes change the basic situation types:
(4) Mickey painted the house. (E)
(5) Mickey paints houses. (GS)
If SE type could be determined solely by the verb
constellation, automatic classification of SEs would
be a relatively straightforward task. However, other
parts of the clause often override the basic situation
type, resulting in aspectual coercion and a derived
situation type. For example, a modal adverb can
trigger aspectual coercion:
(6) Mickey probably paints houses. (P)
Serious challenges for SE classification arise from
the aspectual ambiguity and flexibility of many
predicates as well as from aspectual coercion.
2.3 Discourse modes
Much of the motivation of SE classification is
toward the broader goal of identifying discourse
modes, which provide a linguistic characterization
of textual passages according to the situation enti-
ties introduced. They correspond to intuitions as to
the rhetorical or semantic character of a text. Pas-
sages of written text can be classified into modes
of discourse ? Narrative, Description, Argument, In-
formation, and Report ? by examining concrete lin-
guistic cues in the text (Smith, 2003). These cues
are of two forms: the distribution of situation entity
types and the mode of progression (either temporal
or metaphorical) through the text.
897
For example, the Narration and Report modes
both contain mainly events and temporally bounded
states; they differ in their principles of temporal pro-
gression. Report passages progress with respect to
(deictic) speech time, whereas Narrative passages
progress with respect to (anaphoric) reference time.
Passages in the Description mode are predominantly
stative, and Argument mode passages tend to be
characterized by propositions and Information mode
passages by facts and states.
3 Data
This section describes the data sets used in the ex-
periments, the process for creating annotated train-
ing data, and preprocessing steps. Also, we give ex-
amples of the ten SE types.
There are no established data sets for SE classifi-
cation, so we created annotated training data to test
our models. We have annotated two data sets, one
from the Brown corpus and one based on data from
the Message Understanding Conference 6 (MUC6).
3.1 Segmentation
The Brown texts were segmented according to SE-
containing clausal boundaries, and each clause was
labeled with an SE label. Segmentation is itself a
difficult task, and we made some simplifications.
In general, clausal complements of verbs like say
which have clausal direct objects were treated as
separate clauses and given an SE label. Clausal com-
plements of verbs which have an entity as a direct
object and second clausal complement (such as no-
tify) were not treated as separate clauses. In addi-
tion, some modifying and adjunct clauses were not
assigned separate SE labels.
The MUC texts came to us segmented into ele-
mentary discourse units (EDUs), and each EDU was
labeled by the annotators. The two data sets were
segmented according to slightly different conven-
tions, and we did not normalize the segmentation.
The inconsistencies in segmentation introduce some
error to the otherwise gold-standard segmentations.
3.2 Annotation
Each text was independently annotated by two ex-
perts and reviewed by a third. Each clause was as-
signed precisely one SE label from the set of ten
possible labels. For clauses which introduce more
SE Text
S That compares with roughly paperback-book
dimensions for VHS.
G Accordingly, most VHS camcorders are usually
bulky and weigh around eight pounds or more.
S ?Carl is a tenacious fellow,?
R said a source close to USAir.
GS ?He doesn?t give up easily
GS and one should never underestimate what he can
or will do.?
S For Jenks knew
F that Bari?s defenses were made of paper.
E Mr. Icahn then proposed
P that USAir buy TWA,
IMP ?Fermate?!
R Musmanno bellowed to his Italian crewmen.
Q What?s her name?
S Quite seriously, the names mentioned as possibilities
were three male apparatchiks from the Beltway?s
Democratic political machine
N By Andrew B. Cohen Staff Reporter of The WSJ
Table 1: Example clauses and their SE annota-
tion. Horizontal lines separate extracts from differ-
ent texts.
than one SE, the annotators selected the most salient
one. This situation arose primarily when comple-
ment clauses were not treated as distinct clauses, in
which case the SE selected was the one introduced
by the main verb. The label N was used for clauses
which do not introduce any situation entity.
The Brown data set consists of 20 ?popular lore?
texts from section cf of the Brown corpus. Seg-
mentation of these texts resulted in a total of 4390
clauses. Of these, 3604 were used for training and
development, and 786 were held out as final test-
ing data. The MUC data set consists of 50 Wall
Street Journal newspaper articles segmented to a to-
tal of 1675 clauses. 137 MUC clauses were held
out for testing. The Brown texts are longer than
the MUC texts, with an average of 219.5 clauses
per document as compared to MUC?s average of
33.5 clauses. The average clause in the Brown data
contains 12.6 words, slightly longer than the MUC
texts? average of 10.9 words.
Table 1 provides examples of the ten SE types as
well as showing how clauses were segmented. Each
SE-containing example is a sequence of EDUs from
the data sets used in this study.
898
WWORDS words & punctuation
WT
W (see above)
POSONLY POS tag for each word
WORD/POS word/POS pair for each word
WTL
WT (see above)
FORCEPRED T if clause (or preceding clause)
contains force predicate
PROPPRED T if clause (or preceding clause)
contains propositional verb
FACTPRED T if clause (or preceding clause)
contains factive verb
GENPRED T if clause contains generic predicate
HASFIN T if clause contains finite verb
HASMODAL T if clause contains modal verb
FREQADV T if clause contains frequency adverb
MODALADV T if clause contains modal adverb
VOLADV T if clause contains volitional adverb
FIRSTVB lexical item and POS tag for first verb
WTLG
WTL (see above)
VERBS all verbs in clause
VERBTAGS POS tags for all verbs
MAINVB main verb of clause
SUBJ subject of clause (lexical item)
SUPER CCG supertag
Table 2: Feature sets for SE classification
3.3 Preprocessing
The linguistic tests for SE classification appeal to
multiple levels of linguistic information; there are
lexical, morphological, syntactic, categorial, and
structural tests. In order to access categorial and
structural information, we used the C&C2 toolkit
(Clark and Curran, 2004). It provides part-of-speech
tags and Combinatory Categorial Grammar (CCG)
(Steedman, 2000) categories for words and syntac-
tic dependencies across words.
4 Features
One of our goals in undertaking this study was to
explore the use of linguistically-motivated features
and deep syntactic features in probabilistic models
for SE classification. The nature of the task requires
features characterizing the entire clause. Here, we
describe our four feature sets, summarized in table 2.
The feature sets are additive, extending very basic
feature sets first with linguistically-motivated fea-
tures and then with deep syntactic features.
2svn.ask.it.usyd.edu.ap/trac/candc/wiki
4.1 Basic feature sets: W and WT
The WORDS (W) feature set looks only at the words
and punctuation in the clause. These features are
obtained with no linguistic processing.
WORDS/TAGS (WT) incorporates part-of-speech
(POS) tags for each word, number, and punctuation
mark in the clause and the word/tag pairs for each
element of the clause. POS tags provide valuable in-
formation about syntactic category as well as certain
kinds of shallow semantic information (such as verb
tense). The tags are useful for identifying verbs,
nouns, and adverbs, and the words themselves repre-
sent lexico-semantic information in the feature sets.
4.2 Linguistically-motivated feature set: WTL
The WORDS/TAGS/LINGUISTIC CORRELATES
(WTL) feature set introduces linguistically-
motivated features gleaned from the literature
on SEs; each feature encodes a linguistic cue that
may correlate to one or more SE types. These
features are not directly annotated; instead they are
extracted by comparing words and their tags for
the current and immediately preceding clauses to
lists containing appropriate triggers. The lists are
compiled from the literature on SEs.
For example, clauses embedded under predicates
like force generally introduce E-type SEs:
(7) I forced [John to run the race with me].
(8) * I forced [John to know French].
The feature force-PREV is extracted if a member
of the force-type predicate word list occurs in the
previous clause.
Some of the correlations discussed in the litera-
ture rely on a level of syntactic analysis not available
in the WTL feature set. For example, stativity of the
main verb is one feature used to distinguish between
event and state SEs, and particular verbs and verb
tenses have tendencies with respect to stativity. To
approximate the main verb without syntactic analy-
sis, WTL uses the lexical item of the first verb in the
clause and the POS tags of all verbs in the clause.
These linguistic tests are non-absolute, making
them inappropriate for a rule-based model. Our
models handle the defeasibility of these correlations
probabilistically, as is standard for machine learning
for natural language processing.
899
4.3 Addition of deep features: WTLG
The WORDS/TAGS/LINGUISTIC CORRE-
LATES/GRAMMATICAL RELATIONS (WTLG)
feature set uses a deeper level of syntactic analysis
via features extracted from CCG parse representa-
tions for each clause. This feature set requires an
additional step of linguistic processing but provides
a basis for more accurate classification.
WTL approximated the main verb by sloppily tak-
ing the first verb in the clause; in contrast, WTLG
uses the main verb identified by the parser. The
parser also reliably identifies the subject, which is
used as a feature.
Supertags ?CCG categories assigned to words?
provide an interesting class of features in WTLG.
They succinctly encode richer grammatical informa-
tion than simple POS tags, especially subcategoriza-
tion and argument types. For example, the tag S\NP
denotes an intransitive verb, whereas (S\NP)/NP
denotes a transitive verb. As such, they can be seen
as a way of encoding the verbal constellation and its
effect on aspectual classification.
5 Models
We consider two types of models for the automatic
classification of situation entities. The first, a la-
beling model, utilizes a maximum entropy model
to predict SE labels based on clause-level linguistic
features as discussed above. This model ignores the
discourse patterns that link multiple utterances. Be-
cause these patterns recur, a sequencing model may
be better suited to the SE classification task. Our
second model thus extends the first by incorporating
the previous n (0 ? n ? 6) labels as features.
Sequencing is standardly used for tasks like part-
of-speech tagging, which generally assume smaller
units to be both tagged and considered as context
for tagging. We are tagging at the clause level rather
than at the word level, but the structure of the prob-
lem is essentially the same. We thus adapted the
OpenNLP maximum entropy part-of-speech tagger3
(Hockenmaier et al, 2004) to extract features from
utterances and to tag sequences of utterances instead
of words. This allows the use of features of adjacent
clauses as well as previously-predicted labels when
making classification decisions.
3http://opennlp.sourceforge.net.
6 Experiments
In this section we give results for testing on Brown
data. All results are reported in terms of accu-
racy, defined as the percentage of correctly-labeled
clauses. Standard 10-fold cross-validation on the
training data was used to develop models and fea-
ture sets. The optimized models were then tested on
the held-out Brown and MUC data.
The baseline was determined by assigning S
(state), the most frequent label in both training sets,
to each clause. Baseline accuracy was 38.5% and
36.2% for Brown and MUC, respectively.
In general, accuracy figures for MUC are much
higher than for Brown. This is likely due to the fact
that the MUC texts are more consistent: they are all
newswire texts of a fairly consistent tone and genre.
The Brown texts, in contrast, are from the ?popular
lore? section of the corpus and span a wide range
of topics and text types. Nonetheless, the patterns
between the feature sets and use of sequence predic-
tion hold across both data sets; here, we focus our
discussion on the results for the Brown data.
6.1 Labeling results
The results for the labeling model appear in the two
columns labeled ?n=0? in table 3. On Brown, the
simple W feature set beats the baseline by 6.9% with
an accuracy of 45.4%. Adding POS information
(WT) boosts accuracy 4.5% to 49.9%. We did not
see the expected increase in performance from the
linguistically motivated WTL features, but rather a
slight decrease in accuracy to 48.9%. These features
may require a greater amount of training material to
be effective. Addition of deep linguistic information
with WTLG improved performance to 50.6%, a gain
of 5.2% over words alone.
6.2 Oracle results
To determine the potential effectiveness of sequence
prediction, we performed oracle experiments on
Brown by including previous gold-standard labels as
features. Figure 1 illustrates the results from ora-
cle experiments incorporating from zero to six pre-
vious gold-standard SE labels (the lookback). The
increase in performance illustrates the importance of
context in the identification of SEs and motivates the
use of sequence prediction.
900
42
44
46
48
50
52
54
56
58
60
0 1 2 3 4 5 6
Acc
Lookback
WWTWTLWTLG
Figure 1: Oracle results on Brown data.
6.3 Sequencing results
Table 3 gives the results of classification with the se-
quencing model on the Brown data. As with the la-
beling model, accuracy is boosted by WT and WTLG
feature sets. We see an unexpected degradation in
performance in the transition from WT to WTL.
The most interesting results here, though, are the
gains in accuracy from use of previously-predicted
labels as features for classification. When labeling
performance is relatively poor, as with feature set W,
previous labels help very little, but as labeling accu-
racy increases, previous labels begin to effect notice-
able increases in accuracy. For the best two feature
sets, considering the previous two labels raises the
accuracy 2.0% and 2.5%, respectively.
In most cases, though, performance starts to de-
grade as the model incorporates more than two pre-
vious labels. This degradation is illustrated in Fig-
ure 2. The explanation for this is that the model is
still very weak, with an accuracy of less than 54%
for the Brown data. The more previous predicted la-
bels the model conditions on, the greater the likeli-
hood that one or more of the labels is incorrect. With
gold-standard labels, we see a steady increase in ac-
curacy as we look further back, and we would need
a better performing model to fully take advantage of
knowledge of SE patterns in discourse.
The sequencing model plays a crucial role, partic-
ularly with such a small amount of training material,
and our results indicate the importance of local con-
text in discourse analysis.
42
44
46
48
50
52
54
0 1 2 3 4 5 6
WWTWTLWTLG
Figure 2: Sequencing results on Brown data.
BROWN Lookback (n)
0 1 2 3 4 5 6
W 45.4 45.2 46.1 46.6 42.8 43.0 42.4
WT 49.9 52.4 51.9 49.2 47.2 46.2 44.8
WTL 48.9 50.5 50.1 48.9 46.7 44.9 45.0
WTLG 50.6 52.9 53.1 48.1 46.4 45.9 45.7
Baseline 38.5
Table 3: SE classification results with sequencing
on Brown test set. Bold cell indicates accuracy at-
tained by model parameters that performed best on
development data.
6.4 Error analysis
Given that a single one of the ten possible labels
occurs for more than 35% of clauses in both data
sets, it is useful to look at the distribution of er-
rors over the labels. Table 4 is a confusion matrix
for the held-out Brown data using the best feature
set.4 The first column gives the label and number
of occurrences of that label, and the second column
is the accuracy achieved for that label. The next
two columns show the percentage of erroneous la-
bels taken by the labels S and GS. These two labels
are the most common labels in the development set
(38.5% and 32.5%). The final column sums the per-
centages of errors assigned to the remaining seven
labels. As one would expect, the model learns the
predominance of these two labels. There are a few
interesting points to make about this data.
First, 66% of G-type clauses are mistakenly as-
signed the label GS. This is interesting because
these two SE-types constitute the broader SE cat-
4Thanks to the anonymous reviewer who suggested this use-
ful way of looking at the data.
901
% Correct % Incorrect
Label Label S GS Other
S(278) 72.7 n/a 14.0 13.3
E(203) 50.7 37.0 11.8 0.5
GS(203) 44.8 46.3 n/a 8.9
R(26) 38.5 30.8 11.5 19.2
N(47) 23.4 31.9 23.4 21.3
G(12) 0.0 25.0 66.7 8.3
IMP(8) 0.0 75.0 25.0 0.0
P(7) 0.0 71.4 28.6 0.0
F(2) 0.0 100.0 0.0 0.0
Table 4: Confusion matrix for Brown held-out test
data, WTLG feature set, lookback n = 2. Numbers
in parentheses indicate how many clauses have the
associated gold standard label.
egory of generalizing statives. The distribution of
errors for R-type clauses points out another interest-
ing classification difficulty.5 Unlike the other cat-
egories, the percentage of false-other labels for R-
type clauses is higher than that of false-GS labels.
80% of these false-other labels are of type E. The
explanation for this is that R-type clauses are a sub-
type of the event class.
6.5 Genre effects in classification
Different text domains frequently have different
characteristic properties. Discourse modes are one
way of analyzing these differences. It is thus in-
teresting to compare SE classification when training
and testing material come from different domains.
Table 5 shows the performance on Brown when
training on Brown and/or MUC using the WTLG
feature set with simple labeling and with sequence
prediction with a lookback of two. A number of
things are suggested by these figures. First, the la-
beling model (lookback of zero), beats the baseline
even when training on out-of-domain texts (43.1%
vs. 38.5%), but this is unsurprisingly far below
training on in-domain texts (43.1% vs. 50.6%).
Second, while sequence prediction helps with in-
domain training (53.1% vs 50.6%), it makes no
difference with out-of-domain training (42.9% vs
43.1%). This indicates that the patterns of SEs in a
text do indeed correlate with domains and their dis-
course modes, in line with case-studies in the dis-
course modes theory (Smith, 2003). Finally, mix-
5Thanks to an anonymous reviewer for bringing this to our
attention.
lookback Brown test set
WTLG
train:Brown 0 50.6
2 53.1
train:MUC 0 43.1
2 42.9
train:all 0 50.4
2 49.5
Table 5: Cross-domain SE classification
ing out-of-domain training material with in-domain
material does not hurt labelling accuracy (50.4% vs
50.6%), but it does take away the gains from se-
quencing (49.5% vs 53.1%).
These genre effects are suggestive, but inconclu-
sive. A similar setup with much larger training and
testing sets would be necessary to provide a clearer
picture of the effect of mixed domain training.
7 Related work
Though we are aware of no previous work in SE
classification, others have focused on automatic de-
tection of aspectual and temporal data.
Klavans and Chodorow (1992) laid the founda-
tion for probabilistic verb classification with their
interpretation of aspectual properties as gradient and
their use of statistics to model the gradience. They
implement a single linguistic test for stativity, treat-
ing lexical properties of verbs as tendencies rather
than absolute characteristics.
Linguistic indicators for aspectual classification
are also used by Siegel (1999), who evaluates 14 in-
dicators to test verbs for stativity and telicity. Many
of his indicators overlap with our features.
Siegel and McKeown (2001) address classifica-
tion of verbs for stativity (event vs. state) and
for completedness (culminated vs. non-culminated
events). They compare three supervised and one un-
supervised machine learning systems. The systems
obtain relatively high accuracy figures, but they are
domain-specific, require extensive human supervi-
sion, and do not address aspectual coercion.
Merlo and Stevenson (2001) use corpus-based
thematic role information to identify and classify
unergative, unaccusative, and object-drop verbs.
Stevenson and Merlo note that statistical analysis
cannot and should not be separated from deeper lin-
guistic analysis, and our results support that claim.
902
The advantages of our approach are the broadened
conception of the classification task and the use of
sequence prediction to capture a wider context.
8 Conclusions
Situation entity classification is a little-studied but
important classification task for the analysis of dis-
course. We have presented the first data-driven mod-
els for SE classification, motivating the treatment of
SE classification as a sequencing task.
We have shown that linguistic correlations to sit-
uation entity type are useful features for proba-
bilistic models, as are grammatical relations and
CCG supertags derived from syntactic analysis of
clauses. Models for the task perform poorly given
very basic feature sets, but minimal linguistic pro-
cessing in the form of part-of-speech tagging im-
proves performance even on small data sets used for
this study. Performance improves even more when
we move beyond simple feature sets and incorpo-
rate linguistically-motivated features and grammat-
ical relations from deep syntactic analysis. Finally,
using sequence prediction by adapting a POS-tagger
further improves results.
The tagger we adapted uses beam search; this al-
lows tractable use of maximum entropy for each la-
beling decision but forgoes the ability to find the
optimal label sequence using dynamic programming
techniques. In contrast, Conditional Random Fields
(CRFs) (Lafferty et al, 2001) allow the use of max-
imum entropy to set feature weights with efficient
recovery of the optimal sequence. Though CRFs are
more computationally intensive, the small set of SE
labels should make the task tractable for CRFs.
In future, we intend to test the utility of SEs in dis-
course parsing, discourse mode identification, and
discourse relation projection.
Acknowledgments
This work was supported by the Morris Memorial
Trust Grant from the New York Community Trust.
The authors would like to thank Nicholas Asher,
Pascal Denis, Katrin Erk, Garrett Heifrin, Julie
Hunter, Jonas Kuhn, Ray Mooney, Brian Reese, and
the anonymous reviewers.
References
N. Asher. 1993. Reference to Abstract objects in Dis-
course. Kluwer Academic Publishers.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
G. Carlson and F. J. Pelletier, editors. 1995. The Generic
Book. University of Chicago Press, Chicago.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log?linear models. In Proceedings of ACL?
04, pages 104?111, Barcelona, Spain.
D. Dowty. 1979. Word Meaning and Montague Gram-
mar. Reidel, Dordrecht.
J. Hockenmaier, G. Bierner, and J. Baldridge. 2004. Ex-
tending the coverage of a CCG system. Research on
Language and Computation, 2:165?208.
J. L. Klavans and M. S. Chodorow. 1992. Degrees of
stativity: The lexical representation of verb aspect. In
Proceedings of COLING 14, Nantes, France.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings
of ICML, pages 282?289, Williamstown, USA.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics.
M. Moens and M. Steedman. 1988. Temporal ontol-
ogy and temporal reference. Computational Linguis-
tics, 14(2):15?28.
P. Peterson. 1997. Fact Proposition Event. Kluwer.
E. V. Siegel and K. R. McKeown. 2001. Learning meth-
ods to combine linguistic indicators: Improving as-
pectual classification and revealing linguistic insights.
Computational Linguistics, 26(4):595?628.
E. V. Siegel. 1999. Corpus-based linguistic indicators
for aspectual classification. In Proceedings of ACL37,
University of Maryland, College Park.
C. S. Smith. 1991. The Parameter of Aspect. Kluwer.
C. S. Smith. 2003. Modes of Discourse. Cambridge
University Press.
M. Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
Z. Vendler, 1967. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, New York.
H. Verkuyl. 1972. On the Compositional Nature of the
Aspects. Reidel, Dordrecht.
903
Proceedings of the ACL 2007 Student Research Workshop, pages 7?12,
Prague, June 2007. c?2007 Association for Computational Linguistics
Inducing Combinatory Categorial Grammars with Genetic Algorithms
Elias Ponvert
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
ponvert@mail.utexas.edu
Abstract
This paper proposes a novel approach to the
induction of Combinatory Categorial Gram-
mars (CCGs) by their potential affinity with
the Genetic Algorithms (GAs). Specifically,
CCGs utilize a rich yet compact notation for
lexical categories, which combine with rela-
tively few grammatical rules, presumed uni-
versal. Thus, the search for a CCG consists
in large part in a search for the appropri-
ate categories for the data-set?s lexical items.
We present and evaluates a system utilizing
a simple GA to successively search and im-
prove on such assignments. The fitness of
categorial-assignments is approximated by
the coverage of the resulting grammar on the
data-set itself, and candidate solutions are
updated via the standard GA techniques of
reproduction, crossover and mutation.
1 Introduction
The discovery of grammars from unannotated ma-
terial is an important problem which has received
much recent research. We propose a novel approach
to this effort by leveraging the theoretical insights of
Combinatory Categorial Grammars (CCG) (Steed-
man, 2000), and their potential affinity with Ge-
netic Algorithms (GA) (Goldberg, 1989). Specifi-
cally, CCGs utilize an extremely small set of gram-
matical rules, presumed near-universal, which op-
erate over a rich set of grammatical categories,
which are themselves simple and straightforward
data structures. A search for a CCG grammar for
a language can be construed as a search for ac-
curate category assignments to the words of that
language, albeit over a large landscape of poten-
tial solutions. GAs are biologically-inspired general
purpose search/optimization methods that have suc-
ceeded in these kinds of environments: wherein so-
lutions are straightforwardly coded, yet nevertheless
the solution space is complex and difficult.
We evaluate a system that uses a GA to suc-
cessively refine a population of categorial lexicons
given a collection of unannotated training material.
This is an important problem for several reasons.
First of all, the development of annotated training
material is expensive and difficult, and so schemes
to discover linguistic patterns from unannotated text
may help cut down the cost of corpora development.
Also, this project is closely related to the problem of
resolving lexical gaps in parsing, which is a dogged
problem for statistical parsing systems in CCG, even
trained in a supervised manner. Carrying over tech-
niques from this project to that could help solve a
major problem in CCG parsing technology.
Statistical parsing with CCGs is an active area
of research. The development of CCGbank (Hock-
enmaier and Steedman, 2005) based on the Penn
Treebank has allowed for the development of wide-
coverage statistical parsers. In particular, Hock-
enmaier and Steedman (2001) report a generative
model for CCG parsing roughly akin to the Collins
parser (Collins, 1997) specific to CCG. Whereas
Hockenmaier?s parser is trained on (normal-form)
CCG derivations, Clark and Curran (2003) present
a CCG parser trained on the dependency structures
within parsed sentences, as well as the possible
derivations for them, using a log-linear (Maximum-
Entropy) model. This is one of the most accurate
parsers for producing deep dependencies currently
available. Both systems, however, suffer from gaps
7
in lexical coverage.
The system proposed here was evaluated against
a small corpus of unannotated English with the goal
of inducing a categorial lexicon for the fragment.
The system is not ultimately successful and fails to
achieve the baseline category assignment accuracy,
however it does suggest directions for improvement.
2 Background
2.1 Genetic Algorithms
The basic insight of a GA is that, given a problem
domain for which solutions can be straightforwardly
encoded as chromosomes, and for which candidate
solutions can be evaluated using a faithful fitness
function, then the biologically inspired operations of
reproduction, crossover and mutation can in certain
cases be applied to multisets or populations of can-
didate solutions toward the discovery of true or ap-
proximate solutions.
Among the applications of GA to computational
linguistics, (Smith and Witten, 1995) and (Korkmaz
and U?c?oluk, 2001) each present GAs for the induc-
tion of phrase structure grammars, applied success-
fully over small data-sets. Similarly, (Losee, 2000)
presents a system that uses a GA to learn part-of-
speech tagging and syntax rules from a collection of
documents. Other proposals related specifically to
the acquisition of categorial grammars are cited in
?2.3.
2.2 Combinatory Categorial Grammar
CCG is a mildly context sensitive grammatical for-
malism. The principal design features of CCG is that
it posits a small set of grammatical rules that oper-
ate over rich grammatical categories. The categories
are, in the simplest case, formed by the atomic cate-
gories s (for sentence), np (noun phrase), n (com-
mon noun), etc., closed under the slash operators
/, \. There is not a substantive distinction between
lexical and phrasal categories. The intuitive inter-
pretation of non-atomic categories is as follows: a
word for phrase of type A/B is looking for an item
of type B on the right, to form an item of type A.
Likewise, an item of type A\B is looking for an item
of type B on the left. type A. For example, in the
derivation in Figure 1, ?scores? combines with the
np ?another goal? to form the verb phrase ?scores
Ronaldinho
np
scores
(s\np)/np
another
np/n
goal
n
>
np
>
s\np
<
s
Figure 1: Example CCG derivation
Application
A/B B ?> A B A\B ?< A
Composition
A/B B/C ?>B A/C B\C A\B ?<B A\C
Crossed-Composition
A/B B\C ?>B? A\C B/C A\B ?<B? A/C
Figure 2: CCG Rules
another goal?. This, in turn, combines with the np
?Ronaldinho? to form a sentence.
The example illustrates the rule of Application,
denoted with < and > in derivations. The schemata
for this rule, along with the Composition rule (B)
and the Crossed-Composition rule (B?), are given in
Figure 2. The rules of CCG are taken as universals,
thus the acquisition of a CCG grammar can be seen
as the acquisition of a categorial lexicon.
2.3 Related Work
In addition to the supervised grammar systems out-
lined in ?1, the following proposals have been put
forward toward the induction of categorial gram-
mars.
Watkinson and Mandahar (2000) report a Catego-
rial Grammar induction system related to that pro-
posed here. They generate a Categorial Grammar
using a fixed and limited set of categories and, uti-
lizing an unannotated corpus, successively refine the
lexicon by testing it against the corpus sentences one
at a time. Using a constructed corpus, their strategy
worked extremely well: 100% accuracy on lexical
category selection as well as 100% parsing accuracy
with the resulting statistical CG parser. With natu-
rally occurring text, however, their system does not
perform as well: approximately 77% lexical accu-
racy and 37% parsing accuracy.
One fundamental difference between the strategy
proposed here and that of Watkinson and Manda-
8
har is that we propose to successively generate and
evaluate populations of candidate solutions, rather
than refining a single solution. Also, while Watkin-
son and Mandahar use logical methods to construct
a probabilistic parser, the present system uses ap-
proximate methods and yet derives symbolic parsing
systems. Finally, Watkinson and Mandahar utilize
an extremely small set of known categories, smaller
than the set used here.
Clark (1996) outlines a strategy for the acquisi-
tion of Tree-Adjoining Grammars (Joshi, 1985) sim-
ilar to the one proposed here: specifically, he out-
lines a learning model based on the co-evolution of a
parser, which builds parse trees given an input string
and a set of category-assignments, and a shred-
der, which chooses/discovers category-assignments
from parse-trees. The proposed strategy is not im-
plemented and tested, however.
Briscoe (2000) models the acquisition of catego-
rial grammars using evolutionary techniques from a
different perspective. In his experiments, language
agents induced parameters for languages from other
language agents generating training material. The
acquisition of languages is not induced using GA per
se, but the evolutionary development of languages is
modeled using GA techniques.
Also closely related to the present proposal is the
work of Villavicencio (2002). Villavicencio presents
a system that learns a unification-based categorial
grammar from a semantically-annotated corpus of
child-directed speech. The learning algorithm is
based on a Principles-and-Parameters language ac-
quisition scheme, making use of logical forms and
word order to induce possible categories within a
typed feature-structure hierarchy. Her system has
the advantage of not having to pre-compile a list of
known categories, as did Watkinson and Mandahar
as well as the present proposal. However, Villav-
icencio does make extensive use of the semantics
of the corpus examples, which the current proposal
does not. This is related to the divergent motivations
of two proposals: Villavicencio aims to present a
psychologically realistic language learner and takes
it as psychologically plausible that logical forms are
accessible to the language learner; the current pro-
posal is preoccupied with grammar induction from
unannotated text, and assumes (sentence-level) log-
ical forms to be inaccessible.
n is the size of the population
A are candidate category assignments
F are fitness scores
E are example sentences
m is the likelihood of mutation
Initialize:
for i? 1 to n :
A[i]? RANDOMASSIGNMENT()
Loop:
for i? 1 to length[A] :
F [i]? 0
P? NEWPARSER(A[i])
for j? 1 to length[E] :
F [i]? F [i]+SCORE(P.PARSE(E[i]))
A? REPRODUCE(A,F)
. Crossover:
for i? 1 to n?1 :
CROSSOVER(A[i],A[i+1])
. Mutate:
for i? 1 to n :
if RANDOM() < m :
MUTATE(A[i])
Until: End conditions are met
Figure 3: Pseudo-code for CCG induction GA.
3 System
As stated, the task is to choose the correct CCG cat-
egories for a set of lexical items given a collection of
unannotated or minimally annotated strings. A can-
didate solution genotype is an assignment of CCG
categories to the lexical items (types rather than to-
kens) contained in the textual material. A candi-
date phenotype is a CCG parser initialized with these
category assignments. The fitness of each candi-
date solution is evaluated by how well its phenotype
(parser) parses the strings of the training material.
Pseudo-code for the algorithm is given in Fig. 3.
For the most part, very simple GA techniques were
used; specifically:
? REPRODUCE The reproduction scheme utilizes
roulette wheel technique: initialize a weighted
roulette wheel, where the sections of the wheel
correspond to the candidates and the weights
of the sections correspond to the fitness of the
candidate. The likelihood that a candidate is
selected in a roulette wheel spin is directly pro-
portionate to the fitness of the candidate.
? CROSSOVER The crossover strategy is a simple
partition scheme. Given two candidates C and
9
D, choose a center point 0 ? i ? n where n the
number of genes (category-assignments), swap
C[0, i]? D[0, i] and D[i, n]? C[i, n].
? MUTATE The mutation strategy simply swaps
a certain number of individual assignments in
a candidate solution with others. For the ex-
periments reported here, if a given candidate
is chosen to be mutated, 25% of its genes are
modified. The probability a candidate was se-
lected is 10%.
In the implementation of this strategy, the follow-
ing simplifying assumptions were made:
? A given candidate solution only posits a single
CCG category for each lexical item.
? The CCG categories to assign to the lexical
items are known a priori.
? The parser only used a subset of CCG ? pure
CCG (Eisner, 1996) ? consisting of the Appli-
cation and Composition rules.
3.1 Chromosome Encodings
A candidate solution is a simplified assignment of
categories to lexical items, in the following manner.
The system creates a candidate solution by assigning
lexical items a random category selection, as in:
Ronaldinho (s\np)/np
Barcelona pp
kicks (s\np)/(s\np)
...
Given the fixed vocabulary, and the fixed category
list, the representation can be simplified to lists of
indices to categories, indexed to the full vocabulary
list:
0 Ronaldinho
1 Barcelona
2 kicks
...
...
15 (s\np)/np
...
37 (s\np)/(s\np)
...
Then the category assignment can be construed as
a finite function from word-indices to category-
indices {0 7? 15,1 7? 42,2 7? 37, ...} or simply the
vector ?15,42,37, ...?. The chromosome encodings
for the GA scheme described here are just this: vec-
tors of integer category indices.
3.2 Fitness
The parser used is straightforward implementation
of the normal-form CCG parser presented by Eis-
ner (1996). The fitness of the parser is evaluated on
its parsing coverage on the individual strings, which
is a score based on the chart output. Several chart
fitness scores were evaluated, including:
? SPANS The number of spans parsed
? RELATIVE The number of spans the string
parsed divided by the string length
? WEIGHTED The sum of the lengths of the spans
parsed
See ?5.1 for a comparison of these fitness metrics.
Additionally, the following also factored into
scoring parses:
? S-BONUS Add an additional bonus to candi-
dates for each sentence they parse completely.
? PSEUDO-SMOOTHING Assign all parses at
least a small score, to help avoid premature
convergence. The metrics that count singleton
spans do this informally.
4 Evaluation
The system was evaluated on a small data-set of ex-
amples taken from the World Cup test-bed included
with the OpenCCG grammar development system1
and simplified considerably. This included 19 ex-
ample sentences with a total of 105 word-types and
613 tokens from (Baldridge, 2002).
In spite of the simplifying assumption that an in-
dividual candidate only assigns a single category to
a lexical item, one can derive a multi-assignment of
categories to lexemes from the population by choos-
ing the top category elected by the candidates. It
is on the basis of these derived assignments that the
system was evaluated. The examples chosen require
only 1-to-1 category assignment, hence the relevant
category from the test-bed constitutes the gold stan-
dard (minus Baldridge (2002)?s modalities). The
baseline for this dataset, assigning np to all lexical
items, was 28.6%. The hypothesis is that optimizing
1http://openccg.sf.net
10
Fitness Metric Accuracy
COUNT 18.5
RELATIVE 22.0
WEIGHTED 20.4
Table 1: Final accuracy of the metrics
parsing coverage with a GA scheme would correlate
with improved category-accuracy.
The end-conditions apply if the parsing coverage
for the derived grammar exceeds 90%. Such end-
conditions generally were not met; otherwise, ex-
periments ran for 100 generations, with a popula-
tion of 50 candidates. Because of the heavy reliance
of GAs on pseudo-random number generation, indi-
vidual experiments can show idiosyncratic success
or failure. To control for this, the experiments were
replicated 100 times each. The results presented
here are averages over the runs.
5 Results
5.1 Fitness Metrics
The various fitness metrics were each evaluated, and
their final accuracies are reported in Table 1. The re-
sults were negative, as category accuracy did not ap-
proach the baseline. Examining the average system
accuracy over time helps illustrate some of the issues
involved. Figure 4 shows the growth of category ac-
curacy for each of the metrics. Pathologically, the
random assignments at the start of each experiment
have better accuracy than after the application of GA
techniques.
Figure 5 compares the accuracy of the category
assignments to the GA?s internal measure of its fit-
ness, using the Count Spans metric as a point of ref-
erence. (The fitness metric is scaled for compari-
son with the accuracy.) While fitness, in the average
case, steadily increases, accuracy does not increase
with such steadiness and degrades significantly in
the early generations.
The intuitive reason for this is that, initially,
the random assignment of categories succeeds by
chance in many cases, however the likelihood of ac-
curate or even compatible assignments to words that
occur adjacent in the examples is fairly low. The
GA promotes these assignments over others, appar-
10
15
20
25
30
0 10 20 30 40 50 60 70 80 90 100
Generations
CountRelativeWeighted
Baseline
Figure 4: Comparison of fitness metrics
10
15
20
25
30
0 10 20 30 40 50 60 70 80 90 100
Generations
Accuracy
FitnessBaseline
Figure 5: Fitness and accuracy: COUNT
ently committing the candidates to incorrect assign-
ments early on and not recovering from these com-
mitments. The WEIGHTED and RELATIVE metrics
are designed to try to overcome these effects by pro-
moting grammars that parse longer spans, but they
do not succeed. Perhaps exponential rather than lin-
ear bonus for parsing spans of length greater than
two would be effective.
6 Conclusions
This project attempts to induce a grammar from
unannotated material, which is an extremely diffi-
cult problem for computational linguistics. Without
access to training material, logical forms, or other
relevant features to aid in the induction, the system
attempts to learn from string patterns alone. Using
GAs may aid in this process, but, in general, in-
duction from string patterns alone takes much larger
data-sets than the one discussed here.
The GA presented here takes a global perspective
on the progress of the candidates, in that the indi-
vidual categories assigned to the individual words
are not evaluated directly, but rather as members of
candidates that are scored. For a system such as
11
this to take advantage of the patterns that arise out
of the text itself, a much more fine-grained perspec-
tive is necessary, since the performance of individ-
ual category-assignments to words being the focus
of the task.
7 Acknowledgements
I would like to thank Jason Baldridge, Greg Kobele,
Mark Steedman, and the anonymous reviewers for
the ACL Student Research Workshop for valuable
feedback and discussion.
References
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Ted Briscoe. 2000. Grammatical acquisition: Inductive
bias and coevolution of language and the language ac-
quisition device. Language, 76:245?296.
Stephen Clark and James R Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Proceed-
ings of EMNLP-03, pages 97?105, Sapporo, Japan.
Robin Clark. 1996. Complexity and the induction of
Tree Adjoining Grammars. Unpublished manuscript,
University of Pennsylvania.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of ACL-
97, pages 16?23, Madrid, Spain.
Jason Eisner. 1996. Efficient normal-form parsing for
Combinatory Categorial Grammar. In Proceedings of
ACL-96, pages 79?86, Santa Cruz, USA.
David E. Goldberg. 1989. Genetic Algorithms in Search,
Optimization and Machine Learning. Addison-
Wesley.
Julia Hockenmaier and Mark Steedman. 2001. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proceedings of ACL, pages
335?342, Philadelphia, USA.
Julia Hockenmaier and Mark Steedman. 2005. CCG-
bank: User?s manual. Technical Report MC-SIC-05-
09, Department of Computer and Information Science,
University of Pennsylvania.
Aravind Joshi. 1985. An introduction to Tree Adjoining
Grammars. In A. Manaster-Ramer, editor, Mathemat-
ics of Language. John Benjamins.
Emin Erkan Korkmaz and Go?ktu?rk U?c?oluk. 2001. Ge-
netic programming for grammar induction. In 2001
Genetic and Evolutionary Computation Conference:
Late Breaking Papers, pages 245?251, San Francisco,
USA.
Rober M. Losee. 2000. Learning syntactic rules and tags
with genetic algorithms for information retrieval and
filtering: An empirical basis for grammatical rules. In-
formation Processing and Management, 32:185?197.
Tony C. Smith and Ian H. Witten. 1995. A genetic algo-
rithm for the induction of natural language grammars.
In Proc. of IJCAI-95 Workshop on New Approaches to
Learning for Natural Language Processing, pages 17?
24, Montreal, Canada.
Mark Steedman. 2000. The Syntactic Process. MIT,
Cambridge, Mass.
Aline Villavicencio. 2002. The Acquisition of a
Unification-Based Generalised Categorial Grammar.
Ph.D. thesis, University of Cambridge.
Stephen Watkinson and Suresh Manandhar. 2000. Un-
supervised lexical learning with categorial grammars
using the LLL corpus. In James Cussens and Sas?o
Dz?eroski, editors, Language Learning in Logic, pages
16?27, Berlin. Springer.
12
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Simple Unsupervised Grammar Induction
from Raw Text with Cascaded Finite State Models
Elias Ponvert, Jason Baldridge and Katrin Erk
Department of Linguistics
The University of Texas at Austin
Austin, TX 78712
{ponvert,jbaldrid,katrin.erk}@mail.utexas.edu
Abstract
We consider a new subproblem of unsuper-
vised parsing from raw text, unsupervised par-
tial parsing?the unsupervised version of text
chunking. We show that addressing this task
directly, using probabilistic finite-state meth-
ods, produces better results than relying on
the local predictions of a current best unsu-
pervised parser, Seginer?s (2007) CCL. These
finite-state models are combined in a cascade
to produce more general (full-sentence) con-
stituent structures; doing so outperforms CCL
by a wide margin in unlabeled PARSEVAL
scores for English, German and Chinese. Fi-
nally, we address the use of phrasal punctua-
tion as a heuristic indicator of phrasal bound-
aries, both in our system and in CCL.
1 Introduction
Unsupervised grammar induction has been an ac-
tive area of research in computational linguistics for
over twenty years (Lari and Young, 1990; Pereira
and Schabes, 1992; Charniak, 1993). Recent work
(Headden III et al, 2009; Cohen and Smith, 2009;
Ha?nig, 2010; Spitkovsky et al, 2010) has largely
built on the dependency model with valence of Klein
and Manning (2004), and is characterized by its re-
liance on gold-standard part-of-speech (POS) anno-
tations: the models are trained on and evaluated us-
ing sequences of POS tags rather than raw tokens.
This is also true for models which are not successors
of Klein and Manning (Bod, 2006; Ha?nig, 2010).
An exception which learns from raw text and
makes no use of POS tags is the common cover links
parser (CCL, Seginer 2007). CCL established state-
of-the-art results for unsupervised constituency pars-
ing from raw text, and it is also incremental and ex-
tremely fast for both learning and parsing. Unfortu-
nately, CCL is a non-probabilistic algorithm based
on a complex set of inter-relating heuristics and a
non-standard (though interesting) representation of
constituent trees. This makes it hard to extend.
Note that although Reichart and Rappoport (2010)
improve on Seginer?s results, they do so by select-
ing training sets to best match the particular test
sentences?CCL itself is used without modification.
Ponvert et al (2010) explore an alternative strat-
egy of unsupervised partial parsing: directly pre-
dicting low-level constituents based solely on word
co-occurrence frequencies. Essentially, this means
segmenting raw text into multiword constituents. In
that paper, we show?somewhat surprisingly?that
CCL?s performance is mostly dependent on its ef-
fectiveness at identifying low-level constituents. In
fact, simply extracting non-hierarchical multiword
constituents from CCL?s output and putting a right-
branching structure over them actually works better
than CCL?s own higher level predictions. This result
suggests that improvements to low-level constituent
prediction will ultimately lead to further gains in
overall constituent parsing.
Here, we present such an improvement by using
probabilistic finite-state models for phrasal segmen-
tation from raw text. The task for these models is
chunking, so we evaluate performance on identifica-
tion of multiword chunks of all constituent types as
well as only noun phrases. Our unsupervised chun-
kers extend straightforwardly to a cascade that pre-
dicts higher levels of constituent structure, similar
to the supervised approach of Brants (1999). This
forms an overall unsupervised parsing system that
outperforms CCL by a wide margin.
1077
Mrs. Ward for one was relieved
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
1
(a) Chunks: (Mrs. Ward), (for one), and (was relieved)
All
came
from
Cray Research
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(b) Only one chunk extracted: (Cray Research)
Fig. 1: Examples of constituent chunks extracted from
syntactic trees
2 Data
We use the standard data sets for unsupervised con-
stituency parsing research: for English, the Wall
Street Journal subset of the Penn Treebank-3 (WSJ,
Marcus et al 1999); for German, the Negra corpus
v2 (Krenn et al, 1998); for Chinese, the Penn Chi-
nese Treebank v5.0 (CTB, Palmer et al, 2006). We
lower-case text but otherwise do not alter the raw
text of the corpus. Sentence segmentation and tok-
enization from the treebank is used. As in previous
work, punctuation is not used for evaluation.
In much unsupervised parsing work the test sen-
tences are included in the training material. Like Co-
hen and Smith, Headden III et al, Spitkovsky et al,
we depart from this experimental setup and keep the
evaluation sets blind to the models during training.
For English (WSJ) we use sections 00-22 for train-
ing, section 23 for test and we develop using section
24; for German (Negra) we use the first 18602 sen-
tences for training, the last 1000 sentences for de-
velopment and the penultimate 1000 sentences for
testing; for Chinese (CTB) we adopt the data-split
of Duan et al (2007).
3 Tasks and Benchmark
Evaluation. By unsupervised partial parsing, or
simply unsupervised chunking, we mean the seg-
mentation of raw text into (non-overlapping) multi-
word constituents. The models are intended to cap-
ture local constituent structure ? the lower branches
of a constituent tree. For this reason we evaluate
WSJ
Chunks 203K
NPs 172K
Chnk ? NPs 161K
Negra
Chunks 59K
NPs 33K
Chnk ? NPs 23K
CTB
Chunks 92K
NPs 56K
Chnk ? NPs 43K
Table 1: Constituent chunks and base NPs in the datasets.
% constituents % words
WSJ
Chunks 32.9 57.7
NPs 27.9 53.1
Negra
Chunks 45.4 53.6
NPs 25.5 42.4
CTB
Chunks 32.5 55.4
NPs 19.8 42.9
Table 2: Percentage of gold standard constituents and
words under constituent chunks and base NPs.
using what we call constituent chunks, the subset
of gold standard constituents which are i) branch-
ing (multiword) but ii) non-hierarchical (do not con-
tain subconstituents). We also evaluate our models
based on their performance at identifying base noun
phrases, NPs that do not contain nested NPs.
Examples of constituent chunks extracted from
treebank constituent trees are in Fig. 1. In English
newspaper text, constituent chunks largely corre-
spond with base NPs, but this is less the case with
Chinese and German. Moreover, the relationship be-
tween NPs and constituent chunks is not a subset re-
lation: some base NPs do have internal constituent
structure. The numbers of constituent chunks and
NPs for the training datasets are in Table 1. The per-
centage of constituents in these datasets which fall
under these definitions, and the percentage of words
under these constituents, are in Table 2.
For parsing, the standard unsupervised parsing
metric is unlabeled PARSEVAL. It measures preci-
sion and recall on constituents produced by a parser
as compared to gold standard constituents.
CCL benchmark. We use Seginer?s CCL as a
benchmark for several reasons. First, there is a
free/open-source implementation facilitating exper-
1078
imental replication and comparison.1 More im-
portantly, until recently it was the only unsuper-
vised raw text constituent parser to produce re-
sults competitive with systems which use gold POS
tags (Klein and Manning, 2002; Klein and Man-
ning, 2004; Bod, 2006) ? and the recent improved
raw-text parsing results of Reichart and Rappoport
(2010) make direct use of CCL without modifica-
tion. There are other raw-text parsing systems of
note, EMILE (Adriaans et al, 2000), ABL (van Za-
anen, 2000) and ADIOS (Solan et al, 2005); how-
ever, there is little consistent treebank-based evalu-
ation of these models. One study by Cramer (2007)
found that none of the three performs particularly
well under treebank evaluation. Finally, CCL out-
performs most published POS-based models when
those models are trained on unsupervised word
classes rather than gold POS tags. The only excep-
tion we are aware of is Ha?nig?s (2010) unsuParse+,
which outperforms CCL on Negra, though this is
shown only for sentences with ten or fewer words.
Phrasal punctuation. Though punctuation is usu-
ally entirely ignored in unsupervised parsing re-
search, Seginer (2007) departs from this in one key
aspect: the use of phrasal punctuation ? punctuation
symbols that often mark phrasal boundaries within a
sentence. These are used in two ways: i) they im-
pose a hard constraint on constituent spans, in that
no constituent (other than sentence root) may extend
over a punctuation symbol, and ii) they contribute to
the model, specifically in terms of the statistics of
words seen adjacent to a phrasal boundary. We fol-
low this convention and use the following set:
. ? ! ; , -- ? ?
The last two are ideographic full-stop and comma.2
4 Unsupervised partial parsing
We learn partial parsers as constrained sequence
models over tags encoding local constituent struc-
ture (Ramshaw and Marcus, 1995). A simple tagset
is unlabeled BIO, which is familiar from supervised
chunking and named-entity recognition: the tag B
1http://www.seggu.net/ccl
2This set is essentially that of Seginer (2007). While it is
clear from our analysis of CCL that it does make use of phrasal
punctuation in Chinese, we are not certain whether ideographic
comma is included.
denotes the beginning of a chunk, I denotes mem-
bership in a chunk andO denotes exclusion from any
chunk. In addition we use the tag STOP for sentence
boundaries and phrasal punctuation.
HMMs and PRLGs. The models we use for un-
supervised partial parsing are hidden Markov mod-
els, and a generalization we refer to as probabilis-
tic right linear grammars (PRLGs). An HMM mod-
els a sequence of observed states (words) x =
{x1, x2, . . . , xN} and a corresponding set of hid-
den states y = {y1, y2, . . . , yN}. HMMs may be
thought of as a special case of probabilistic context-
free grammars, where the non-terminal symbols are
the hidden state space, terminals are the observed
states and rules are of the form NONTERM ?
TERM NONTERM (assuming y1 and yN are fixed
and given). So, the emission and transition emanat-
ing from yn would be characterized as a PCFG rule
yn ? xn yn+1. HMMs factor rule probabilities into
emission and transition probabilities:
P (yn ? xn yn+1) = P (xn, yn+1|yn)
? P (xn|yn) P (yn+1|yn).
However, without making this independence as-
sumption, we can model right linear rules directly:
P (xn, yn+1|yn) = P (xn|yn, yn+1) P (yn+1|yn).
So, when we condition emission probabilities on
both the current state yn and the next state yn+1, we
have an exact model. This direct modeling of the
right linear grammar rule yn ? xn yn+1 is what
we call a probabilistic right-linear grammar. To be
clear, a PRLG is just an HMM without the indepen-
dence of emissions and transitions. See Smith and
Johnson (2007) for a discussion, where they refer to
PRLGs as Mealy HMMs.
We use expectation maximization to estimate
model parameters. For the E step, the forward-
backward algorithm (Rabiner, 1989) works identi-
cally for the HMM and PRLG. For the M step, we
use maximum likelihood estimation with additive
smoothing on the emissions probabilities. So, for
the HMM and PRLG models respectively, for words
1079
STOP B
O I
1
Fig. 2: Possible tag transitions as a state diagram.
STOP B I O
STOP .33 .33 .33
B 1
I .25 .25 .25 .25
O .33 .33 .33
Fig. 3: Uniform initialization of transition probabilities
subject to the constraints in Fig. 2: rows correspond to
antecedent state, columns to following state.
w and tags s, t:
P? (w|t) =
C(t, w) + ?
C(t) + ?V
P? (w|s, t) =
C(t, w, s) + ?
C(t, s) + ?V
where C are the soft counts of emissions C(t, w),
rules C(t, w, s) = C(t ? w s), tags C(t) and tran-
sitions C(t, s) calculated during the E step; V is the
number of terms w, and ? is a smoothing parameter.
We fix ? = .1 for all experiments; more sophisti-
cated smoothing could avoid dependence on ?.
We do not smooth transition probabilities (so
P? (s|t) = C(t, s)/C(t)) for two reasons. First, with
four tags, there is no data-sparsity concern with re-
spect to transitions. Second, the nature of the task
imposes certain constraints on transition probabili-
ties: because we are only interested in multiword
chunks, we expressly do not want to generate a B
following a B ? in other words P (B|B) = 0.
These constraints boil down to the observation
that the B and I states will only be seen in BII? se-
quences. This may be expressed via the state transi-
tion diagram in Fig. 2. The constraints of also dic-
tate the initial model input to the EM process. We
use uniform probability distributions subject to the
constraints of Fig. 2. So, initial model transition
probabilities are given in Fig. 3. In EM, if a parame-
ter is equal to zero, subsequent iterations of the EM
process will not ?unset? this parameter; thus, this
form of initialization is a simple way of encoding
constraints on model parameters. We also experi-
mented with random initial models (subject to the
constraints in Fig. 2). Uniform initialization usu-
ally works slightly better; also, uniform initializa-
tion does not require multiple runs of each experi-
ment, as random initialization does.
Motivating the HMMand PRLG. This approach
? encoding a chunking problem as a tagging prob-
lem and learning to tag with HMMs ? goes back
to Ramshaw and Marcus (1995). For unsupervised
learning, the expectation is that the model will learn
to generalize on phrasal boundaries. That is, the
models will learn to associate terms like the and a,
which often occur at the beginnings of sentences and
rarely at the end, with the tag B, which cannot occur
at the end of a sentence. Likewise common nouns
like company or asset, which frequently occur at the
ends of sentences, but rarely at the beginning, will
come to be associated with the I tag, which cannot
occur at the beginning.
The basic motivation for the PRLG is the assump-
tion that information is lost due to the independence
assumption characteristic of the HMM. With so few
states, it is feasible to experiment with the more fine-
grained PRLG model.
Evaluation. Using the low-level predictions of
CCL as as benchmark, we evaluate the HMM and
PRLG chunkers on the tasks of constituent chunk
and base NP identification. Models were initialized
uniformly as illustrated in Fig. 3. Sequence models
learn via EM. We report accuracy only after conver-
gence, that is after the change in full dataset per-
plexity (log inverse probability) is less than %.01
between iterations. Precision, recall and F-score are
reported for full constituent identification ? brack-
ets which do not match the gold standard exactly are
false positives.
Model performance results on held-out test
datasets are reported in Table 3. ?CCL? refers to the
lowest-level constituents extracted from full CCL
output, as a benchmark chunker. The sequence mod-
els outperform the CCL benchmark at both tasks and
on all three datasets. In most cases, the PRLG se-
quence model performs better than the HMM; the
exception is CTB, where the PRLG model is behind
the HMM in evaluation, as well as behind CCL.
As the lowest-level constituents of CCL were not
specifically designed to describe chunks, we also
1080
English / WSJ German / Negra Chinese / CTB
Task Model Prec Rec F Prec Rec F Prec Rec F
Chunking
CCL 57.5 53.5 55.4 28.4 29.6 29.0 23.5 23.9 23.7
HMM 53.8 62.2 57.7 35.0 37.7 36.3 37.4 41.3 39.3
PRLG 76.2 63.9 69.5 39.6 47.8 43.3 23.0 18.3 20.3
NP
CCL 46.2 51.1 48.5 15.6 29.2 20.3 10.4 17.3 13.0
HMM 47.7 65.6 55.2 23.8 46.2 31.4 17.0 30.8 21.9
PRLG 76.8 76.7 76.7 24.6 53.4 33.6 21.9 28.5 24.8
Table 3: Unsupervised chunking results for local constituent structure identification and NP chunking on held-out test
sets. CCL refers to the lowest constituents extracted from CCL output.
WSJ Negra CTB
Chunking 57.8 36.0 25.5
NPs 57.8 38.8 23.2
Table 4: Recall of CCL on the chunking tasks.
checked the recall of all brackets generated by CCL
against gold-standard constituent chunks. The re-
sults are given in Table 4. Even compared to this,
the sequence models? recall is almost always higher.
The sequence models, as well as the CCL bench-
mark, show relatively low precision on the Negra
corpus. One possible reason for this lies in the
design decision of Negra to use relatively flat tree
structures. As a result, many structures that in
other treebanks would be prepositional phrases with
embedded noun phrases ? and thus non-local con-
stituents ? are flat prepositional phrases here. Exam-
ples include ?auf die Wiesbadener Staatsanwaelte?
(on Wiesbaden?s district attorneys) and ?in Han-
novers Nachbarstadt? (in Hannover?s neighbor city).
In fact, in Negra, the sequence model chunkers
often find NPs embedded in PPs, which are not an-
notated as such. For instance, in the PP ?hinter den
Kulissen? (behind the scenes), both the PRLG and
HMM chunkers identify the internal NP, though this
is not identified in Negra and thus considered a false
positive. The fact that the HMM and PRLG have
higher recall on NP identification on Negra than pre-
cision is further evidence towards this.
Comparing the HMM and PRLG. To outline
some of the factors differentiating the HMM and
PRLG, we focus on NP identification in WSJ.
The PRLG has higher precision than the HMM,
while the two models are closer in recall. Com-
paring the predictions directly, the two models of-
POS Sequence # of errors
TO VB 673
NNP NNP 450
MD VB 407
DT JJ 368
DT NN 280
Table 5: Top 5 POS sequences of the false positives pre-
dicted by the HMM.
ten have the same correct predictions and often miss
the same gold standard constituents. The improved
results of the PRLG are based mostly on the fewer
overall brackets predicted, and thus fewer false pos-
itives: for WSJ the PRLG incorrectly predicts 2241
NP constituents compared to 6949 for the HMM.
Table 5 illustrates the top 5 POS sequences of the
false positives predicted by the HMM.3 (Recall that
we use gold standard POS only for post-experiment
results analysis?the model itself does not have ac-
cess to them.) By contrast, the sequence represent-
ing the largest class of errors of the PRLG is DT NN,
with 165 errors ? this sequence represents the largest
class of predictions for both models.
Two of the top classes of errors, MD VB and
TO VB, represent verb phrase constituents, which
are often predicted by the HMM chunker, but not
by the PRLG. The class represented by NNP NNP
corresponds with the tendency of the HMM chun-
ker to split long proper names: for example, it sys-
tematically splits new york stock exchange into two
chunks, (new york) (stock exchange), whereas the
PRLG chunker predicts a single four-word chunk.
The most interesting class is DT JJ, which rep-
resents the difficulty the HMM chunker has at dis-
3For the Penn Treebank tagset, see Marcus et al (1993).
1081
1 Start with raw text:
there is no asbestos in our products now
2 Apply chunking model:
there (is no asbestos) in (our products) now
3 Create pseudowords:
there is in our now
4 Apply chunking model (and repeat 1?4 etc.):
(there is ) (in our ) now
5 Unwind and create a tree:
there
is no asbestos
in
our products
now
1Fig. 4: Cascaded chunking illustrated. Pseudowords are
indicated with boxes.
tinguishing determiner-adjective from determiner-
noun pairs. The PRLG chunker systematically gets
DT JJ NN trigrams as chunks. The greater con-
text provided by right branching rules allows the
model to explicitly estimate separate probabilities
forP (I ? recent I) versusP (I ? recent O). That
is, recent within a chunk versus ending a chunk. Bi-
grams like the acquisition allow the model to learn
rules P (B ? the I) and P (I ? acquisition O).
So, the PRLG is better able to correctly pick out the
trigram chunk (the recent acquisition).
5 Constituent parsing with a cascade of
chunkers
We use cascades of chunkers for full constituent
parsing, building hierarchical constituents bottom-
up. After chunking is performed, all multiword con-
stituents are collapsed and represented by a single
pseudoword. We use an extremely simple, but effec-
tive, way to create pseudoword for a chunk: pick the
term in the chunk with the highest corpus frequency,
and mark it as a pseudoword. The sentence is now a
string of symbols (normal words and pseudowords),
to which a subsequent unsupervised chunking model
is applied. This process is illustrated in Fig. 4.
Each chunker in the cascade chunks the raw text,
then regenerates the dataset replacing chunks with
pseudowords; this process is iterated until no new
chunks are found. The separate chunkers in the cas-
Text : Mr. Vinken is chairman of Elsevier N.V.
Level 1 :
Mr. Vinken
is chairman of
Elsevier N.V.
1Level 2 :
Mr. Vinken is chairman
of
Elsevier N.V.
1
Level 3 : Mr. Vinken is chairman of
Elsevier N.V.
1
Fig. 5: PRLG cascaded chunker output.
NPs PPs
Lev 1 Lev 2 Lev 1 Lev 2
WSJ
HMM 66.5 68.1 20.6 70.2
PRLG 77.5 78.3 9.1 77.6
Negra
HMM 54.7 62.3 24.8 48.1
PRLG 61.6 65.2 40.3 44.0
CTB
HMM 33.3 35.4 34.6 38.4
PRLG 30.9 33.6 31.6 47.1
Table 7: NP and PP recall at cascade levels 1 and 2. The
level 1 NP numbers differ from the NP chunking numbers
from Table 3 since they include root-level constituents
which are often NPs.
cade are referred to as levels. In our experiments the
cascade process took a minimum of 5 levels, and a
maximum of 7. All chunkers in the cascade have the
same settings in terms of smoothing, the tagset and
initialization.
Evaluation. Table 6 gives the unlabeled PARSE-
VAL scores for CCL and the two finite-state models.
PRLG achieves the highest F-score for all datasets,
and does so by a wide margin for German and Chi-
nese. CCL does achieve higher recall for English.
While the first level of constituent analysis has
high precision and recall on NPs, the second level
often does well finding prepositional phrases (PPs),
especially in WSJ; see Table 7. This is illustrated
in Fig. 5. This example also illustrates a PP attach-
ment error, which are a common problem for these
models.
We also evaluate using short ? 10-word or less ?
sentences. That said, we maintain the training/test
split from before. Also, making use of the open
1082
Parsing English / WSJ German / Negra Chinese / CTB
Model Prec Rec F Prec Rec F Prec Rec F
CCL 53.6 50.0 51.7 33.4 32.6 33.0 37.0 21.6 27.3
HMM 48.2 43.6 45.8 30.8 50.3 38.2 43.0 29.8 35.2
PRLG 60.0 49.4 54.2 38.8 47.4 42.7 50.4 32.8 39.8
Table 6: Unlabeled PARSEVAL scores for cascaded models.
source implementation by F. Luque,4 we compare
on WSJ and Negra to the constituent context model
(CCM) of Klein and Manning (2002). CCM learns
to predict a set of brackets over a string (in prac-
tice, a string of POS tags) by jointly estimating con-
stituent and distituent strings and contexts using an
iterative EM-like procedure (though, as noted by
Smith and Eisner (2004), CCM is deficient as a gen-
erative model). Note that this comparison is method-
ologically problematic in two respects. On the one
hand, CCM is evaluated using gold standard POS
sequences as input, so it receives a major source of
supervision not available to the other models. On the
other hand, the other models use punctuation as an
indicator of constituent boundaries, but all punctu-
ation is dropped from the input to CCM. Also, note
that CCM performs better when trained on short sen-
tences, so here CCM is trained only on the 10-word-
or-less subsets of the training datasets.5
The results from the cascaded PRLG chunker
are near or better than the best performance by
CCL or CCM in these experiments. These and the
full-length parsing results suggest that the cascaded
chunker strategy generalizes better to longer sen-
tences than does CCL. CCM does very poorly on
longer sentences, but does not have the benefit of us-
ing punctuation, as do the raw text models; unfortu-
nately, further exploration of this trade-off is beyond
the scope of this paper. Finally, note that CCM has
higher recall, and lower precision, generally, than
the raw text models. This is due, in part, to the chart
structure used by CCM in the calculation of con-
stituent and distituent probabilities: as in CKY pars-
ing, the chart structure entails the trees predicted will
be binary-branching. CCL and the cascaded models
can predict higher-branching constituent structures,
4http://www.cs.famaf.unc.edu.ar/
?francolq/en/proyectos/dmvccm/
5This setup is the same as Seginer?s (2007), except the
train/test split.
Prec Rec F
WSJ
CCM 62.4 81.4 70.7
CCL 71.2 73.1 72.1
HMM 64.4 64.7 64.6
PRLG 74.6 66.7 70.5
Negra
CCM 52.4 83.4 64.4
CCL 52.9 54.0 53.0
HMM 47.7 72.0 57.4
PRLG 56.3 72.1 63.2
CTB
CCL 54.4 44.3 48.8
HMM 55.8 53.1 54.4
PRLG 62.7 56.9 59.6
Table 8: Evaluation on 10-word-or-less sentences. CCM
scores are italicized as a reminder that CCM uses gold-
standard POS sequences as input, so its results are not
strictly comparable to the others.
so fewer constituents are predicted overall.
6 Phrasal punctuation revisited
Up to this point, the proposed models for chunking
and parsing use phrasal punctuation as a phrasal sep-
arator, like CCL. We now consider how well these
models perform in absence of this constraint.
Table 9a provides comparison of the sequence
models? performance on the constituent chunking
task without using phrasal punctuation in training
and evaluation. The table shows absolute improve-
ment (+) or decline (?) in precision and recall
when phrasal punctuation is removed from the data.
The punctuation constraint seems to help the chun-
kers some, but not very much; ignoring punctuation
seems to improve chunker results for the HMM on
Chinese. Overall, the effect of phrasal punctuation
on the chunker models? performance is not clear.
The results for cascaded parsing differ strongly
from those for chunking, as Table 9b indicates. Us-
ing phrasal punctuation to constrain bracket predic-
tion has a larger impact on cascaded parsing re-
1083
0 20 40 60
2
2.5
3
3.5
EM Iterations
Le
ng
th
a) Average Predicted Constituent Length
Actual average chunk length
1
0 20 40 60
20
30
40
50
EM Iterations
Pr
ec
isi
on
W/ Punctuation
No Punctuation
b) Chunking Precision
1
0 20 40 60
20
30
40
50
EM Iterations
Pr
ec
isi
on
c) Precision at All Brackets
1
Fig. 6: Behavior of the PRLG model on CTB over the course of EM.
WSJ Negra CTB
Prec Rec Prec Rec Prec Rec
HMM ?5.8 ?9.8 ?0.1 ?0.4 +0.7 +4.9
PRLG ?2.5 ?2.1 ?2.1 ?2.1 ?7.0 +1.2
a) Constituent Chunking
WSJ Negra CTB
Prec Rec Prec Rec Prec Rec
CCL ?14.1 ?13.5 ?10.7 ?4.6 ?11.6 ?6.0
HMM ?7.8 ?8.6 ?2.8 +1.7 ?13.4 ?1.2
PRLG ?10.1 ?7.2 ?4.0 ?4.5 ?22.0 ?11.8
b) (Cascade) Parsing
Table 9: Effects of dropping phrasal punctuation in un-
supervised chunking and parsing evaluations relative to
Tables 3 and 6.
sults almost across the board. This is not surpris-
ing: while performing unsupervised partial parsing
from raw text, the sequence models learn two gen-
eral patterns: i) they learn to chunk rare sequences,
such as named entities, and ii) they learn to chunk
high-frequency function words next to lower fre-
quency content words, which often correlate with
NPs headed by determiners, PPs headed by prepo-
sitions and VPs headed by auxiliaries. When these
patterns are themselves replaced with pseudowords
(see Fig. 4), the models have fewer natural cues to
identify constituents. However, within the degrees
of freedom allowed by punctuation constraints as
described, the chunking models continue to find rel-
atively good constituents.
While CCL makes use of phrasal punctuation in
previously reported results, the open source imple-
mentation allows it to be evaluated without this con-
straint. We did so, and report results in Table 9b.
CCL is, in fact, very sensitive to phrasal punctu-
ation. Comparing CCL to the cascaded chunkers
when none of them use punctuation constraints, the
cascaded chunkers (both HMMs and PRLGs) out-
perform CCL for each evaluation and dataset.
For the CTB dataset, best chunking performance
and cascaded parsing performance flips from the
HMM to the PRLG. More to the point, the PRLG
is actually with worst performing model at the con-
stituent chunking task, but the best performing cas-
cade parser; also, this model has the most serious
degrade in performance when phrasal punctuation is
dropped from input. To investigate, we track the
performance of the chunkers on the development
dataset over iterations of EM. This is illustrated in
Fig. 6 with the PRLG model. First of all, Fig. 6a re-
veals the average length of the constituents predicted
by the PRLG model increases over the course of
EM. However, the average constituent chunk length
is 2.22. So, the PRLG chunker is predicting con-
stituents that are longer than the ones targeted in
the constituent chunking task: regardless of whether
they are legitimate constituents or not, often they
will likely be counted as false positives in this evalu-
ation. This is confirmed by observing the constituent
chunking precision in Fig. 6b, which peaks when
the average predicted constituent length is about the
same the actual average length of those in the eval-
uation. The question, then, is whether the longer
chunks predicted correspond to actual constituents
or not. Fig. 6c shows that the PRLG, when con-
strained by phrasal punctuation, does continue to
improve its constituent prediction accuracy over the
course of EM. These correctly predicted constituents
are not counted as such in the constituent chunking
or base NP evaluations, but they factor directly into
1084
improved accuracy when this model is part of a cas-
cade.
7 Related work
Our task is the unsupervised analogue of chunking
(Abney, 1991), popularized by the 1999 and 2000
Conference on Natural Language Learning shared
tasks (Tjong et al, 2000). In fact, our models follow
Ramshaw and Marcus (1995), treating structure pre-
diction as sequence prediction using BIO tagging.
In addition to Seginer?s CCL model, the unsu-
pervised parsing model of Gao and Suzuki (2003)
and Gao et al (2004) also operates on raw text.
Like us, their model gives special treatment to lo-
cal constituents, using a language model to char-
acterize phrases which are linked via a dependency
model. Their output is not evaluated directly using
treebanks, but rather applied to several information
retrieval problems.
In the supervised realm, Hollingshead
et al (2005) compare context-free parsers with
finite-state partial parsing methods. They find that
full parsing maintains a number of benefits, in spite
of the greater training time required: they can train
on less data more effectively than chunkers, and are
more robust to shifts in textual domain.
Brants (1999) reports a supervised cascaded
chunking strategy for parsing which is strikingly
similar to the methods proposed here. In both,
Markov models are used in a cascade to predict hi-
erarchical constituent structure; and in both, the pa-
rameters for the model at each level are estimated
independently. There are major differences, though:
the models here are learned from raw text with-
out tree annotations, using EM to train parameters;
Brants? cascaded Markov models use supervised
maximum likelihood estimation. Secondly, between
the separate levels of the cascade, we collapse con-
stituents into symbols which are treated as tokens
in subsequent chunking levels; the Markov models
in the higher cascade levels in Brants? work actu-
ally emit constituent structure. A related approach
is that of Schuler et al (2010), who report a su-
pervised hierarchical hidden Markov model which
uses a right-corner transform. This allows the model
to predict more complicated trees with fewer levels
than in Brants? work or this paper.
8 Conclusion
In this paper we have introduced a new subprob-
lem of unsupervised parsing: unsupervised partial
parsing, or unsupervised chunking. We have pro-
posed a model for unsupervised chunking from raw
text that is based on standard probabilistic finite-
state methods. This model produces better local
constituent predictions than the current best unsu-
pervised parser, CCL, across datasets in English,
German, and Chinese. By extending these proba-
bilistic finite-state methods in a cascade, we obtain
a general unsupervised parsing model. This model
outperforms CCL in PARSEVAL evaluation on En-
glish, German, and Chinese.
Like CCL, our models operate from raw (albeit
segmented) text, and like it our models decode very
quickly; however, unlike CCL, our models are based
on standard and well-understood computational lin-
guistics technologies (hidden Markov models and
related formalisms), and may benefit from new re-
search into these core technologies. For instance,
our models may be improved by the application
of (unsupervised) discriminative learning techniques
with features (Berg-Kirkpatrick et al, 2010); or by
incorporating topic models and document informa-
tion (Griffiths et al, 2005; Moon et al, 2010).
UPPARSE, the software used for the experiments
in this paper, is available under an open-source li-
cense to facilitate replication and extensions.6
Acknowledgments. This material is based upon
work supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office un-
der grant number W911NF-10-1-0533. Support for
the first author was also provided by Mike Hogg En-
dowment Fellowship, the Office of Graduate Studies
at The University of Texas at Austin.
This paper benefited from discussion in the Natu-
ral Language Learning reading group at UT Austin,
especially from Collin Bannard, David Beaver,
Matthew Lease, Taesun Moon and Ray Mooney. We
also thank the three anonymous reviewers for in-
sightful questions and helpful comments.
6 http://elias.ponvert.net/upparse.
1085
References
S. Abney. 1991. Parsing by chunks. In R. Berwick,
S. Abney, and C. Tenny, editors, Principle-based Pars-
ing. Kluwer.
P. W. Adriaans, M. Trautwein, and M. Vervoort. 2000.
Towards high speed grammar induction on large text
corpora. In SOFSEM.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In HLT-NAACL.
R. Bod. 2006. Unsupervised parsing with U-DOP. In
CoNLL.
T. Brants. 1999. Cascaded markov models. In EACL.
E. Charniak. 1993. Statistical Language Learning. MIT.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In HLT-NAACL.
B. Cramer. 2007. Limitations of current grammar induc-
tion algorithms. In ACL-SRW.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic mod-
els for action-based Chinese dependency parsing. In
ECML/PKDD.
J. Gao and H. Suzuki. 2003. Unsupervised learning of
dependency structure for language modeling. In ACL.
J. Gao, J.Y. Nie, G. Wu, and G. Cao. 2004. Dependence
language model for information retrieval. In SIGIR.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-
baum. 2005. Integrating topics and syntax. In NIPS.
C. Ha?nig. 2010. Improvements in unsupervised co-
occurence based parsing. In CoNLL.
W. P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In HLT-NAACL.
K. Hollingshead, S. Fisher, and B. Roark. 2005. Com-
paring and combining finite-state and context-free
parsers. In HLT-EMNLP.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In ACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
B. Krenn, T. Brants, W. Skut, and Hans Uszkoreit. 1998.
A linguistically interpreted corpus of German newspa-
per text. In Proceedings of the ESSLLI Workshop on
Recent Advances in Corpus Annotation.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech & Language, 4:35 ? 56.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Compuational Linguistics, pages
313?330.
M.P. Marcus, B. Santorini, M.A. Marcinkiewicz, and
A. Taylor, 1999. Treebank-3. LDC.
T. Moon, J. Baldridge, and K. Erk. 2010. Crouching
Dirichlet, hidden Markov model: Unsupervised POS
tagging with context local tag generation. In EMNLP.
M. Palmer, F. D. Chiou, N. Xue, and T. K. Lee, 2005.
Chinese Treebank 5.0. LDC.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from paritally bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsu-
pervised prediction of low-level constituents. In ICSC.
L.R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proc. of Third
Workshop on Very Large Corpora.
R. Reichart and A. Rappoport. 2010. Improved fully
unsupervised parsing with Zoomed Learning. In
EMNLP.
W. Schuler, S. AbdelRahman, T. Miller, and L. Schwartz.
2010. Broad-coverage parsing using human-like
memory constraints. Compuational Linguistics, 3(1).
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In ACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic CFGs. Computational Lingusitics.
Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2005.
Unsupervised learning of natural languages. PNAS,
102.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in
unsupervised dependency parsing. In NAACL-HLT.
E. F. Tjong, K. Sang, and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In
CoNLL-LLL.
M. van Zaanen. 2000. ABL: Alignment-based learning.
In COLING.
1086
