The Basque lexical-sample task  
Eneko Agirre, Itziar Aldabe, Mikel Lersundi, David Martinez, Eli Pociello, Larraitz Uria(*) 
IxA NLP group, Basque Country University  
649 pk. 20.080 Donostia, Spain 
eneko@si.ehu.es 
 
Abstract 
In this paper we describe the Senseval 3 
Basque lexical sample task. The task 
comprised 40 words (15 nouns, 15 verbs and 
10 adjectives) selected from the Basque 
WordNet. 10 of the words were chosen in 
coordination with other lexical-sample tasks. 
The examples were taken from newspapers, an 
in-house balanced corpus and Internet texts. 
We additionally included a large set of 
untagged examples, and a lemmatised version 
of the data including lemma, PoS and case 
information. The method used to hand-tag the 
examples produced an inter-tagger agreement 
of 78.2% before arbitration. The eight 
competing systems attained results well above 
the most frequent baseline and the best system 
from Swarthmore College scored 70.4% 
recall. 
1 Introduction 
This paper reviews the Basque lexical-sample task 
organized for Senseval 3. Each participant was 
provided with a relatively small set of labelled 
examples (2/3 of 75+15*senses+7*multiwords) 
and a comparatively large set of unlabelled 
examples (roughly ten times more when possible) 
for around 40 words. The larger number of 
unlabelled data was released with the purpose to 
enable the exploration of semi-supervised systems. 
The test set comprised 1/3 of the tagged examples. 
The sense inventory was taken from the Basque 
WordNet, which is linked to WordNet version 1.6 
(Fellbaum, 1998). The examples came mainly from 
newspaper texts, although we also used a balanced 
in-house corpus and texts from Internet. The words 
selected for this task were coordinated with other 
lexical-sample tasks (such as Catalan, English, 
Italian, Romanian and Spanish) in order to share 
around 10 of the target words.  
The following steps were taken in order to carry 
out the task: 
                                                     
(*) Authors listed in alphabetic order. 
1. set the exercise  
a. choose sense inventory from a pre-existing 
resource 
b. choose target corpora 
c. choose target words  
d. lemmatize the corpus automatically 
e. select examples from the corpus 
2. hand-tagging 
a. define the procedure 
b. revise the sense inventory 
c. tag 
d. analyze the inter-tagger agreement 
e. arbitrate 
This paper is organized as follows: The 
following section presents the setting of the 
exercise. Section 3 reviews the hand-tagging, and 
Section 4 the details of the final release. Section 5 
shows the results of the participant systems. 
Section 6 discusses some main issues and finally, 
Section 7 draws the conclusions. 
2 Setting of the exercise  
In this section we present the setting of the 
Basque lexical-sample exercise. 
2.1 Basque 
As Basque is an agglutinative language, the 
dictionary entry takes each of the elements 
necessary to form the different functions. More 
specifically, the affixes corresponding to the 
determinant, number and declension case are taken 
in this order and independently of each other (deep 
morphological structure). For instance, ?etxekoari 
emaiozu? can be roughly translated as ?[to the one 
in the house] [give it]? where the underlined 
sequence of suffixes in Basque corresponds to ?to 
the one in the?.  
2.2 Sense inventory 
We chose the Basque WordNet, linked to 
WordNet 1.6, for the sense inventory. This way, 
the hand tagging enabled us to check the sense 
coverage and overall quality of the Basque 
WordNet, which is under construction. The Basque 
WordNet is available at http://ixa3.si.ehu.es/ 
wei3.html. 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2.3 Corpora used 
Being Basque a minority language it is not easy 
to find the required number of occurrences for each 
word. We wanted to have both balanced and 
newspaper examples, but we also had to include 
texts extracted from the web, specially for the 
untagged corpus. The procedure to find examples 
from the web was the following: for each target 
word all possible morphological declensions were 
automatically generated, searched in a search-
engine, documents retrieved, automatically 
lemmatized (Aduriz et al 2000), filtered using 
some heuristics to ensure quality of context, and 
finally filtered for PoS mismatches. Table 1 shows 
the number of examples from each source. 
2.4 Words chosen 
Basically, the words employed in this task are 
the same words used in Senseval 2 (40 words, 15 
nouns, 15 verbs and 10 adjectives), only the sense 
inventory changed. Besides, in Senseval 3 we 
replaced 5 verbs with new ones. The reason for this 
is that in the context of the MEANING project1 we 
are exploring multilingual lexical acquisition, and 
there are ongoing experiments that focus on those 
verbs. (Agirre et al 2004; Atserias et al 2004). 
In fact, 10 words in the English lexical-sample 
have translations in the Basque, Catalan, Italian, 
Romanian and Spanish lexical tasks: channel, 
crown, letter, program, party (nouns), simple 
(adjective), play, win, lose, decide (verbs).  
2.5 Selection of examples from corpora 
The minimum number of examples for each 
word according to the task specifications was 
calculated as follows: 
 
N=75+15*senses+7*multiwords  
 
As the number of senses in WordNet is very high, 
we decided to first estimate the number of senses 
and multiwords that really occur in the corpus. The 
taggers were provided with a sufficient number of 
examples, but they did not have to tag all. After 
they had tagged around 100 examples, they would 
count the number of senses and multiwords that 
had occurred and computed the N according to 
those counts.  
The context is constituted of 5 sentences, 
including the sentence with the target word 
appearing in the middle. Links were kept to the 
source corpus, document, and to the newspaper 
section when applicable.  
The occurrences were split at random in training 
set (2/3 of all occurrences) and test set (1/3).  
                                                     
1 http://www.lsi.upc.es/~nlp/meaning/meaning.html 
 Total (N) (B) (I)
# words 40  
# senses 316  
# number of tagged examples 7362 5695 924 743
# number of untagged examples 62498 - - 62498
# tags  9887  
Table 1: Some figures regarding the task. N, B and I 
correspond to the source of the examples: newspaper, 
balanced corpus and Internet respectively. 
3 Hand tagging 
Three persons, graduate linguistics students, 
took part in the tagging. They are familiar with 
word senses, as they are involved in the 
development of the Basque WordNet. The 
following procedure was defined in the tagging of 
each word. 
? Before tagging, one of the linguists (the editor) 
revised the 40 words in the Basque WordNet. 
She had to delete and add senses to the words, 
specially for adjectives and verbs, and was 
allowed to check the examples in the corpus.  
? The three taggers would meet, read the glosses 
and examples given in the Basque WordNet 
and discuss the meaning of each synset. They 
tried to agree and clarify the meaning 
differences among the synsets. For each word 
two hand-taggers and a referee is assigned by 
chance. 
? The number of senses of a word in the Basque 
WordNet might change during this meeting; 
that is, linguists could agree that one of the 
word?s senses was missing, or that a synset did 
not fit with a word. This was done prior to 
looking at the corpus. Then, the editor would 
update the Basque WordNet according to those 
decisions before giving the taggers the final 
synset list. Overall (including first bullet 
above), 143 senses were deleted and 92 senses 
added, leaving a total of 316 senses. This 
reflects the current situation of the Basque 
WordNet, which is still under construction. 
? Two taggers independently tagged all 
examples for the word. No communication was 
allowed while tagging the word. 
? Multiple synset tags were allowed, as well as 
the following tags: the lemma (in the case of 
multiword terms), U (unassignable), P (proper 
noun), and X (incorrectly lemmatized). Those 
with an X were removed from the final release. 
In the case of proper nouns and multiword 
terms no synset tag was assigned. Sometimes 
the U tag was used for word senses which are 
not in the Basque WordNet. For instance, the 
sense of kanal corresponding to TV channel, 
which is the most frequent sense in the 
examples, is not present in the Basque 
WordNet (it was not included in WordNet 1.6).  
? A program was used to compute agreement 
rates and to output those occurrences where 
there was disagreement. Those occurrences 
were  grouped by the senses assigned. 
? A third tagger, the referee, reviewed the 
disagreements and decided which one was the 
correct sense (or senses).  
The taggers were allowed to return more than one 
sense, and they returned 9887 tags (1.34 per 
occurrence). Overall, the two taggers agreed in at 
least one tag 78.2% of the time. Some words 
attained an agreement rate above 95% (e.g. nouns 
kanal or tentsio), but others like herri ?
town/people/nation? attained only 52% agreement. 
On average, the whole tagging task took 54 
seconds per occurrence for the tagger, and 20 
seconds for the referee. However, this average 
does not include the time the taggers and the 
referee spent in the meetings they did to 
understand the meaning of each synset. The 
comprehension of a word with all its synsets 
required 45.5 minutes on average. 
4 Final release 
Table 1 includes the total amount of hand-tagged 
and untagged examples that were released. In 
addition to the usual release, the training and 
testing data were also provided in a lemmatized 
version (Aduriz et al 2000) which included 
lemma, PoS and case information. The motivation 
was twofold: 
? to make participation of the teams easier, 
considering the deep inflection of Basque. 
? to factor out the impact of different 
lemmatizers and PoS taggers in the system 
comparison.  
5 Participants and Results 
5 teams took part in this task: Swarthmore 
College (swat), Basque Country University 
(BCU), Instituto per la Ricerca Scientifica e 
Tecnologica (IRST), University of Minnesota 
Duluth (Duluth) and University of Maryland 
(UMD). All the teams presented supervised systems 
which only used the tagged training data, and no 
other external resource. In particular, no system 
used the pointers to the full texts, or the additional 
untagged texts. All the systems used the lemma, 
PoS and case information provided, except the 
BCU team, which had additional access to number, 
determiner and ellipsis information directly from 
the analyzer. This extra information was not 
provided publicly because of representation issues.  
 
 Prec. Rec. Attempted
basque-swat_hk-bo 71.1  70.4  99.04 %
BCU_Basque_svm 69.9  69.9  100.00 %
BCU_-_Basque_Comb 69.5  69.5  100.00 %
swat-hk-basque 67.0  67.0  100.00 %
IRST-Kernels-bas 65.5  65.5  100.00 %
swat-basque 64.6  64.6  100.00 %
Duluth-BLSS 60.8  60.8  100.00 %
UMD_SST1 65.6  58.7  89.42 %
MFS 55.8  55.8  100.00 %
Table 2: Results of systems and MFS baseline, ordered 
according to Recall. 
We want to note that due to a bug, a few examples 
were provided without lemmas.  
The results for the fine-grained scoring are 
shown in Table 2, including the Most Frequent 
Sense baseline (MFS). We will briefly describe 
each of the systems presented by each team in 
order of best recall.  
? Swat presented three systems based in the 
same set of features: the best one was based on 
Adaboost, the second on a combination of five 
learners (Adaboost, maximum entropy, 
clustering system based on cosine similarity, 
decision lists, and na?ve bayes, combined by 
majority voting), and the third on a 
combination of three systems (the last three).  
? BCU presented two systems: the first one based 
on Support Vector Machines (SVM) and the 
second on a majority-voting combination of 
SVM, cosine based vectors and na?ve bayes.  
? IRST participated with a kernel-based method. 
? Duluth participated with a system that votes 
among three bagged decision trees. 
? UMD presented a system based on SVM. 
The winning system is the one using Adaboost 
from Swat, followed closely by the BCU system 
using SVM. 
6 Discussion 
These are the main issues we think are 
interesting for further discussion. 
Sense inventory. Using the Basque WordNet 
presented some difficulties to the taggers. The 
Basque WordNet has been built using the 
translation approach, that is, the English synsets 
have been ?translated? into Basque. The taggers 
had some difficulties to comprehend synsets, and 
especially, to realize what makes a synset different 
from another. In some cases the taggers decided to 
group some of the senses, for instance, in herri ?
town/people/nation? they grouped 6 senses. This 
explains the relatively high number of tags per 
occurrence (1.34). The taggers think that the 
tagging would be much more satisfactory if they 
had defined the word senses directly from the 
corpus.  
Basque WordNet quality. There was a 
mismatch between the Basque WordNet and the 
corpus: most of the examples were linked to a 
specific genre, and this resulted in i) having a 
handful of senses in the Basque WordNet that did 
not appear in our corpus and ii) having some 
senses that were not included in the Basque 
WordNet. Fortunately, we already predicted this 
and we had a preparation phase where the editor 
enriched WordNet accordingly. Most of the 
deletions in the preliminary part were due to the 
semi-automatic method to construct the Basque 
WordNet. All in all, we think that tagging corpora 
is the best way to ensure the quality of the 
WordNets and we plan to pursue this extensively 
for the improvement of the Basque WordNet.  
7 Conclusions and future work 
5 teams participated in the Basque lexical-
sample task with 8 systems. All of the participants 
presented supervised systems which used lemma, 
PoS and case information provided, but none used 
the large amount of untagged senses provided by 
the organizers. The winning system attained 70.4 
recall. Regarding the organization of the task, we 
found that the taggers were more comfortable 
grouping some of the senses in the Basque 
WordNet. We also found that tagging word senses 
is essential for enriching and quality checking of 
the Basque WordNet. 
Acknowledgements 
The work has been partially funded by the 
European Commission (MEANING project IST-
2001-34460). Eli Pociello has a PhD grant from 
the Basque Government.  
References  
I. Aduriz, E. Agirre, I. Aldezabal, I. Alegria, X. 
Arregi, J.M. Arriola, X. Artola, K. Gojenola, A. 
Maritxalar, K. Sarasola, M. Urkia. 2000. A 
Word-grammar Based Morphological Analyzer 
for Agglutinative Languages. In Proceedings of 
the International Conference on Computational 
Linguistics (COLING). Saarbrucken, Germany.  
E. Agirre, A. Atutxa, K. Gojenola, K. Sarasola. 
2004. Exploring portability of syntactic 
information from English to Basque. In 
Proceedings of the 4rd International Conference 
on Languages Resources and Evaluations 
(LREC). Lisbon, Portugal. 
J. Atserias, B. Magnini, O. Popescu, E. Agirre, A. 
Atutxa, G. Rigau, J. Carroll and R. Koeling 
2004. Cross-Language Acquisition of Semantic 
Models for Verbal Predicates. In Proceedings of 
the 4rd International Conference on Languages 
Resources and Evaluations (LREC). Lisbon, 
Portugal. 
C. Fellbaum. 1998. WordNet: An electronic 
Lexical Database. The MIT Press, Cambridge, 
Massachusetts.  
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 580?584, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
EHU-ALM: Similarity-Feature Based Approach for Student Response
Analysis
Itziar Aldabe, Montse Maritxalar
IXA NLP Group
University of Basque Country (UPV-EHU)
itziar.aldabe@ehu.es
montse.maritxalar@ehu.es
Oier Lopez de Lacalle
University of Edinburgh
IKERBASQUE,
Basque Foundation for Science
oier.lopezdelacalle@gmail.com
Abstract
We present a 5-way supervised system based
on syntactic-semantic similarity features. The
model deploys: Text overlap measures,
WordNet-based lexical similarities, graph-
based similarities, corpus-based similarities,
syntactic structure overlap and predicate-
argument overlap measures. These measures
are applied to question, reference answer and
student answer triplets. We take into account
the negation in the syntactic and predicate-
argument overlap measures. Our system uses
the domain-specific data as one dataset to
build a robust system. The results show that
our system is above the median and mean on
all the evaluation scenarios of the SemEval-
2013 task #7.
1 Introduction
In this paper we describe our participation with a
feature-based supervised system to the SemEval-
2013 task #7: The Joint Student Response Analy-
sis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al, 2013). The goal of our
participation is to build a generic system that is
robust enough across domains and scenarios. A
domain-specific system requires new training ex-
amples when shifting to a new domain. However,
domain-specific data is difficult to obtain and creat-
ing new resources is expensive.
We seek robustness by mixing the instances from
BEETLE and SCIENTSBANK. We show our strategy
is suitable to build a generic system that performs
competitively on any domain in the 5-way task.
The paper proceeds as follows. Section 2 de-
scribes the system presenting the learning features
and the runs. In Section 3 we show the optimiza-
tion details, followed by the results (Section 4) and
a preliminary error analysis (Section 5).
2 System description
Our system aims for robustness using the domain-
specific training data as one dataset. Therefore,
we do not differentiate between examples from the
given domains (BEETLE and SCIENTSBANK) when
training the system. In contrast, our approach dintin-
guishes between new questions (unseen answer vs.
unseen question) as well as question types (how,
what and why) by means of simple heuristics.
The runs are organized according to different sys-
tem designs. Although all the runs use the same fea-
ture set, we split the training set to build more spe-
cialized classifiers. Training examples are grouped
depending on: i) the answer is unseen; ii) the ques-
tion is unseen; and iii) the question type (i.e. what,
how, why). Each run defines a framework to explore
the different ways to approach the problem. While
the first run is the simplest and is the most generic
in nature, the third tries to split the task into simpler
problems and creates more specialized classifiers.
2.1 Similarity learning features
Our model is based on various text similarity fea-
tures. Almost all of the measures are computed be-
tween question, reference answer and student an-
swer triplets. The measures based on syntactic struc-
ture and predicate-argument overlaps are only ap-
plied to the student and reference answer pairs. In
580
total, we defined 30 features which can be grouped
as follows:
Text overlapmeasures The similarity of two texts
is computed based on the number of overlapping
words. We obtain the similarity of two texts based
on the F-Measure, the Dice Coefficient, The Cosine,
and the Lesk measures. For that, we use the imple-
mentation available in the Text::Similarity package1.
WordNet-based lexical similarities All the simi-
larity metrics based on WordNet (Miller, 1995) fol-
low the methodology proposed in (Mihalcea et al,
2006). For each open-class word in one of the in-
put texts, we obtain the maximun semantic similar-
ity or relatedness value matching the same open-
class words in the other input text. The values of
each matching are summed up and normalized by
the length of the two input texts as explained in
(Mihalcea et al, 2006). We compute the measures
of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow,
Wu-Palmer, Banerjee-Pedersen, and Patwardhan-
Pedersen provided in the WordNet::Similarity pack-
age (Patwardhan et al, 2003).
Graph-based similarities The similarity of two
texts is based on a graph-based representation
(Agirre and Soroa, 2009) of WordNet. The method
is a two-step process: first the personalized PageR-
ank over WordNet is computed for each text. This
produces a probability distribution over WordNet.
Then, the probability distributions are encoded as
vectors and the cosine similarity between those vec-
tors is calculated.
Corpus-based similarities We compute two
corpus-based similarity measures: Latent Semantic
Analysis (Deerwester et al, 1990) and Latent
Dirichlet Allocation (Blei et al, 2003). We estimate
100 dimensions for LSA and 50 topics for LDA.
Both models are obtained from a subset of the En-
glish Wikipedia following the hierarchy of science
categories. We started with a small set of categories
and recovered the articles below the sub-hierarchy.
We only went 3 levels down to avoid noisy articles
as the category system is rather flat. The similarity
of two texts is the cosine similarity between the
1http://www.d.umn.edu/ tpederse/text-similarity.html
resulting vectors associated with each text in the
latent space.
Syntactic structure overlap The role of syntax is
studied by the use of graph subsumption based on
the approach proposed in (McCarthy et al, 2008).
The text is mapped into a graph with nodes rep-
resenting words and links indicating syntactic de-
pendencies between them. The similarity of two
texts is computed based on the overlap of the syn-
tactic structures. Negation is handled explicitly in
the graph.
Predicate-argument overlap The similarity of
two texts is computed by analyzing the overlap of
the predicates and their associated semantic argu-
ments. The system looks for verbal and nominal
predicates. The similarity is also based on the ap-
proach proposed in (McCarthy et al, 2008). The
graph is represented with words as nodes and the
semantic role of arguments as links. First, the ver-
bal propositions and their arguments are automat-
ically obtained (Bjo?rkelund et al, 2009) as repre-
sented in PropBank (Palmer et al, 2005). Second,
a generalization of the predicates is obtained based
on VerbNet (Kipper, 2005) and NomBank (Meyers
et al, 2004). Finally, the similarity of two texts
is computed based on the overlap of the predicate-
argument relations.
2.2 Architecture of the runs
Generic Framework RUN1 This is the simplest
framework for the assessment of student answers.
The system relies on a single classifier, which has
been optimized on the unseen question scenario.
The scenario is simulated by splitting the training
set so that each question and its answers are in the
same fold.
Unseen Framework RUN2 This framework relies
on two classifiers. The first is tuned on an unseen
answer scenario and the second is prepared for the
question scenario (cf. RUN1). In order to build the
unseen answer classifier, we split the training set so
that answers to the same question can occur in dif-
ferent folders. In test time, the instance is classified
depending on whether it is an unseen answer or an
581
BEETLE SCIENTSBANK OVERALL
Uns-answ Uns-qst All Uns-answ Uns-qst Uns-dom All All
RUN1 0.499 (6) 0.352 (7) 0.404 0.396 (7) 0.283 (4) 0.345 (3) 0.348 0.406
RUN2 0.526 (4) 0.352 (7) 0.413 0.418 (6) 0.283 (4) 0.345 (3) 0.350 0.414
RUN3 0.502 (5) 0.370 (6) 0.415 0.424 (5) 0.260 (8) 0.337 (5) 0.340 0.403
LOWEST 0.170 0.173 - 0.089 0.095 0.121 - -
BEST 0.619 0.552 - 0.478 0.307 0.380 - -
MEAN 0.435 0.343 - 0.341 0.240 0.267 - -
MEDIAN 0.437 0.326 - 0.376 0.259 0.268 - -
Table 1: 5-way results of the runs in F1 macro-average on BEETLE and SCIENTSBANK domains across different
scenarios. Along with the runs, the LOWEST and the BEST system in each scenario are shown. The MEAN and
MEDIAN of the dataset are also presented. Finally, the OVERALL results are showed summing up both domains. Uns-
answ refers to unseen answers scenario, Uns-qst stands for unseen question, Uns-dom unseen domain and All refers
to the sum of all scenarios. The run results are presented together with the ranked position in the task.
unseen question2.
Question-type Framework RUN3 The run con-
sists of a set of question-type expert classifiers. We
divided the training set based on whether an instance
reflected a what, how or why question. We then par-
titioned each question type into unseen answer and
unseen question scenarios. In total, the framework
deploys 6 classifiers, i.e. a test instance is classified
according to the question type and scenario. We set
heuristics to automatically distinguish the instance
type.
3 Optimization on training set
We set a heuristic to create the training instances.
For each student answer, if the matching reference
answer is indicated in it, we create a triplet with the
question, the student answer, and the matching ref-
erence answer. If there is no matching answer, the
reference answer is randomly selected giving pref-
erence to the best reference answers.
Once we have a training set, we split it into dif-
ferent ways to simulate the scenarios described in
Section 2.2. All the models are optimized using 10-
fold cross-validation of the pertaining training set.
For the classifiers in RUN1 and RUN2 we used 8910
training instances. For RUN3 the instances were di-
vided as follows: 1235 instances for how questions,
3089 for what questions and 4589 for why ques-
tions. In total, we obtained 8 models which were
distributed through the runs.
2We treat unseen-domain instances as unseen-question in-
stances.
Our approach uses Support Vector Ma-
chine (Chang and Lin, 2011) to build the classifiers.
As the number of features is not high, we used the
gaussian kernel in order to solve the non-linear
problem. The main parameters of the kernel (? and
C) were tuned using grid search over the parameter
in the cross-validation setting. We focused on
optimizing the F1 macro average of the classifier
in order to avoid a bias towards the major classes.
Each of the 8 classifiers were tuned independently.
The triplets of question, student answer and ref-
erence answer of the test instances were always cre-
ated selecting the first reference answer of the given
set of answers.
4 Results
A total of 8 teams participated in the 5-way task,
submitting a total of 16 system runs (Dzikovska et
al., 2013). Table 1 shows the performance obtained
by our systems across domains and different scenar-
ios. Our three runs ranked differently based on the
evaluation scenario: beetle-uns-answ (6,4,5 rank for
RUN1, RUN2, RUN3, respectively); beetle-uns-qst
(7,7,6); scientsbank-uns-answ (7,6,5); scientsbank-
uns-qst (4,4,8) and scientsbank-uns-dom (3,3,5). We
also evaluated our runs on the entire domain (All
columns) and on the whole test set (OVERALL).
The results show we built robust systems. Despite
being below the best system of each evaluation sce-
nario, the results show that the runs are competitive.
All our runs are above the median and outperform
the average results on each evaluation. Overall, the
results attained in SCIENTSBANK are lower than in
582
BEETLE. This might be due to the questions and
answers being longer in SCIENTSBANK, making it
difficult to obtain good patterns.
As regards our runs, there is no significant overall
difference. While RUN3 performs better in BEETLE
unseen question and SCIENTSBANK unseen answer,
in the rest of scenarios RUN2 outperforms the rest
of the runs. As expected, RUN2 outperforms RUN1
in the unseen answer scenario since the former has
a module specializing in unseen answers. However,
although RUN3 is an ensemble of six classifiers, it is
not the best run. This is probably because the train-
ing sets are not big enough.
Unseen framework (RUN2)
Prec Rec F1
correct 0.552 0.677 0.608
partially correct 0.324 0.323 0.323
contradictory 0.239 0.121 0.160
irrelevant 0.472 0.377 0.419
non domain 0.415 0.849 0.557
Macro average 0.400 0.469 0.414
Micro average 0.443 0.464 0.446
Table 2: results of the RUN2 system on a entire test set.
Table 2 shows the detailed results of the RUN2
system on the entire test set. It is noticeable the
low results obtained on the contradictory class. This
might be because the defined features are not able
to model negation properly and do not deal with
antonymy. Surprisingly, the non domain class is not
the most problematic, even if the system was trained
on a low number of instances.
5 Preliminary Error Analysis
We conducted a preliminary error analysis and stud-
ied some of the misclassified test instances to detect
some problematic issues and to define improvements
to our approach.
Example 5.1 Sam and Jasmine were sitting on a
park bench eating their lunches. A mosquito landed
on Sam?s arm and Sam began slapping at it. When
he did that, he knocked Jasmine?s soda into her lap,
causing her to jump up. What was Sam?s response?
R: Sam?s response was to slap the mosquito.
S1: Sam?s response was to say sorry
S2: To smack the bee.
Some of the detected errors suggest that our use
of syntax and lexical overlap is not sufficient to iden-
tify the correct class. Our system marks the student
answer S1 from Example 5.13 as correct. The ref-
erence answer and the student answer share a great
number of words and the dependency trees are al-
most identical, but not the meanings. In addition, the
question contains additional information that may
require other types of features to correctly classify
the instance.
The predicate-argument overlap feature tries to
generalize the predicate information to find similar-
ities between verbs with the same meaning. How-
ever, our system does not always work in a correct
way. The verb smack in the student answer S2 and
the verb slap in the reference answer mean the same.
Our system classifies the answer incorrectly. If we
look at PropBank and VerbNet, we find that there
is not mapping between PropBank and VerbNet for
these particular verbs.
Example 5.2 Why do you think the other terminals
are being held in a different electrical state than that
of the negative terminal?
R: Terminals 4, 5 and 6 are not connected to the
negative battery terminal
S1: They are connected to the positive battery ter-
minal
We consider the negation as part of the syntac-
tic and predicate-argument overlap measures. How-
ever, our system does not characterize the similar-
ity between not connected to the negative and con-
nected to the positive (Example 5.2). This type of
examples suggest that the system needs to model the
negation and antonyms with additional features.
In the future, further error analysis will be car-
ried out to design features to better model the prob-
lem. We also anticipate creating a specialized fea-
ture space for each question type.
Acknowledgments
This research was partially funded by the Ber2Tek
project (IE12-333), the SKaTeR project (TIN2012-
38584-C06-02) and the NewsReader project (FP7-
ICT-2011-8-316404).
3R refers to the reference answer and S1 and S2 to student
answers.
583
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of The Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
pages 43?48.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Scott Deerwester, Susan Dumais, Goerge Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In D. Wilson and G. Sut-
cliffe, editors, Proceedings of the 21st International
Florida Artificial Intelligence Research Society Con-
ference, pages 201?206, Menlo Park, CA: The AAAI
Press.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The nombank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings the
American Association for Artificial Intelligence (AAAI
2006), Boston.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic role. Computational Linguistics, 31(1):71?
106.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics.
584
