Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 121?128
Manchester, August 2008
 
 
Other-Anaphora Resolution in Biomedical Texts with Automatically 
Mined Patterns 
 
Chen Bin#, Yang Xiaofeng$, Su Jian^ and Tan Chew Lim* 
#*School of Computing, National University of Singapore  
$^Institute for Infocomm Research, A-STAR, Singapore 
{#chenbin, *tancl}@comp.nus.edu.sg 
{$xiaofengy, ^sujian}@i2r.a-star.edu.sg 
? Abstract 
This paper proposes an other-anaphora 
resolution approach in bio-medical texts. 
It utilizes automatically mined patterns to 
discover the semantic relation between an 
anaphor and a candidate antecedent. The 
knowledge from lexical patterns is incor-
porated in a machine learning framework 
to perform anaphora resolution. The ex-
periments show that machine learning 
approach combined with the auto-mined 
knowledge is effective for other-
anaphora resolution in the biomedical 
domain. Our system with auto-mined pat-
terns gives an accuracy of 56.5%., yield-
ing 16.2% improvement against the base-
line system without pattern features, and 
9% improvement against the system us-
ing manually designed patterns.  
1 Introduction 
The last decade has seen an explosive growth in 
the amount of textual information in biomedi-
cine. There is a need for an effective and effi-
cient text-mining system to gather and utilize the 
knowledge encoded in the biomedical literature. 
For a correct discourse analysis, a text-mining 
system should have the capability of understand-
ing the reference relations among different ex-
pressions in texts. Hence, anaphor resolution, the 
task of resolving a given text expression to its 
referred expression in prior texts, is important for 
an intelligent text processing system. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
In linguistics, an expression that points back 
to a previously mentioned expression is called an 
anaphor, and the expression being referred to by 
the anaphor is called its antecedent. Most pre-
vious work on anaphora resolution aims at identi-
ty-anaphora in which both an anaphor and its 
antecedent are mentions of the same entity. 
In this paper, we focus on a special type of 
anaphora resolution, namely, other-anaphora 
resolution, in which an anaphor to be resolved 
has a prefix modifier ?other? or ?another?. The 
antecedent of an other-anaphor is a complement 
expression to the anaphor in a super set. In other 
words, an other-anaphor is a set of elements ex-
cluding the element(s) specified by the antece-
dent. If the modifier ?other? or ?another? is re-
moved, an anaphor becomes the super set includ-
ing the antecedent. Thus, other-anaphora in fact 
represents a ?part-whole? relation. Consider the 
following text  
 ?IL-10 inhibits nuclear stimulation of nuclear 
factor kappa B (NF kappa B).  
Several other transcription factors including NF- 
IL-6, AP-1, AP-2, GR, CREB, Oct-1, and Sp-1 
are not affected by IL-10.?  
Here, the expression ?other transcription fac-
tors? is an other-anaphor, while the ?NF kappa 
B? is its antecedent. The anaphor refers to any 
transcription factors except the antecedent.  By 
removing the lexical modifier ?other?, we can 
get a supper set ?transcription factors? that in-
cludes the antecedent. The anaphor and antece-
dent thus have a ?part-whole? relation1.  
Other-anaphora resolution is an important 
sub-task in information extraction for biomedical 
                                                 
1 Other-anaphora could be also held between ex-
pressions that have subset-set or member-collection 
relations. In this paper, we treat them in a uniform 
way by using the patterned-based method. 
121
  
domain. It also contributes to biomedical ontolo-
gy building as it targeted at a ?part-whole? rela-
tion which is in the same hierarchical orders as in 
ontology. Furthermore, other-anaphora resolu-
tion is a first-step exploration in the resolution of 
bridging anaphora. Furthermore, other-anaphora 
resolution is a first-step exploration in the resolu-
tion of bridging, a special anaphora phenomenon 
in which the semantic relation between an ana-
phor and its antecedent is more complex (e.g. 
part-whole) than co-reference. 
Previous work on other-anaphora resolution 
relies on knowledge resources, for example, on-
tology like WordNet to determine the ?part-
whole? relation. However, in the biomedical do-
main, a document is full of technical terms which 
are usually missing in a general-purpose ontolo-
gy. To deal with this problem, pattern-based ap-
proaches have been widely employed, in which a 
pattern that represents the ?part-whole? relation 
is designed. Two expressions are connected with 
the specific pattern and form a query. The query 
is searched in a large corpus for the occurrence 
frequency which would indicate how likely the 
two given expressions have the part-whole rela-
tion. The solution can avoid the efforts of con-
structing the ontology knowledge for the "part-
whole" relation. However, the pattern is designed 
in an ad-hoc method, usually from linguistic in-
tuition and its effectiveness for other-anaphora 
resolution is not guaranteed. 
In this paper, we propose a method to auto-
matically mine effective patterns for other-
anaphora resolution in biomedical texts. Our me-
thod runs on a small collection of seed word 
pairs. It searches a large corpus (e.g., PubMed 
abstracts as in our system) for the texts where the 
seed pairs co-occur, and collects the surrounding 
words as the surface patterns. The automatically 
found patterns will be used in a machine learning 
framework for other-anaphora resolution. To our 
knowledge, our work is the first effort of apply-
ing the pattern-base technique to other-anaphora 
resolution in biomedical texts. 
The rest of this paper is organized as follows. 
Section 2 introduces previous related work. Sec-
tion 3 describes the machine learning framework 
for other-anaphora resolution. Section 4 presents 
in detail our method for automatically pattern 
mining. Section 5 gives experiment results and 
has some discussions. Finally, Section 6 con-
cludes the paper and shows some future work. 
2 Related Work 
Previous work on other-anaphora resolution 
commonly depends on human engineered know-
ledge and/or deep semantic knowledge for the 
?part-whole? relation, and mostly works only in 
the news domain. 
Markert et al, (2003) presented a pattern-
based algorithm for other-anaphor resolution. 
They used a manually designed pattern ?ANTE-
CEDENT and/or other ANAPHOR ?. Given two 
expression to be resolved, a query is formed by 
instantiating the pattern with the two given ex-
pressions. The query is searched in the Web. The 
higher the hit number returned, the more likely 
that the anaphor and the antecedent candidate 
have the ?part-whole? relation. The anaphor is 
resolved to the candidate with the highest hit 
number. Their work was tested on 120 other-
anaphora cases extracted from Wall Street Jour-
nal. The final accuracy was 52.5%. 
Modjeska et al, (2003) also presented a simi-
lar pattern-based method for other-anaphora res-
olution, using the same pattern ?ANTECEDENT 
and/or other ANAPHOR?. The hit number re-
turned from the Web is used as a feature for a 
Na?ve Bayesian Classifier to resolve other-
anaphors. Other features include surface words, 
substring matching, distance, gender/number 
agreement, and semantic tag of the NP. They 
evaluated their method with 500 other-anaphor 
cases extracted from Wall Street Journal, and 
reported a result of 60.8% precision and 53.4% 
recall. 
Markert and Nissim (2005) compared three 
systems for other-anaphora resolution, using the 
same data set as in (Modjeska et al, 2003). 
The first system consults WordNet for the 
part-whole relation. The WordNet provides in-
formation on meronym/holonym (part-of rela-
tion) and hypernym/ hyponym (type-of relation). 
Their system achieves a performance of 56.8% 
for precision and 37.0% for recall. 
The second and third systems employ the pat-
tern based approach, employing the same manual 
pattern ?ANTECEDENT and/or other ANA-
PHOR?. The second system did search in British 
Nation Corpus, giving 62.6% precision and 
26.2% recall. The third system did search in the 
Web as in (Markert et al, 2003), giving 53.8% 
precision and 51.7% recall. 
122
  
3 Anaphora Resolution System 
3.1 Corpus 
In our study, we used the GENIA corpus2 for our 
other-anaphora resolution in biomedical texts. 
The corpus consists of 2000 MEDLINE abstracts 
(around 440,000 words). From the GENIA cor-
pus, we extracted 598 other-anaphora cases. The 
598 cases do not contain compound prepositions 
or idiomatic uses of ?other?, like ?on the other 
hand? and ?other than?. And all these anaphors 
have their antecedents found in the current and 
previous two sentences of the other-anaphor. On 
average, there are 15.33 candidate antecedents 
for each anaphor to be resolved. 
To conduct other-anaphora resolution, an in-
put document is preprocessed through a pipeline 
of NLP components, including tokenization, sen-
tence boundary detection, part-of-speech (POS) 
tagging, noun phrase (NP) chunking, and named-
entity recognition (NER). These preprocessing 
modules are aimed to determine the boundaries 
of each NP in a text, and to provide necessary 
information of an NP for subsequent processing. 
In our system, we employed the tool-kits built by 
our group for these components. The POS tagger 
was trained and tested on the GENIA corpus 
(version 2.1) and achieved an accuracy of 97.4%. 
The NP-chunking module, evaluated on UPEN 
WSJ TreeBank, produced 94% F-measure. The 
NER module, trained on GENIA corpus (version 
3.0), achieved 71.2% F-measure covering 22 ent-
ity types (e.g., Virus, Protein, Cell, DNA, etc). 
3.2 Learning Framework 
Our other-anaphora resolution system adopts the 
common learning-based model for identity-
anaphora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002). 
In the learning framework, a training or test-
ing instance has the form of ?? ?????? ,???  
where ??????  is the ?
th candidates of the antece-
dent of anaphor ???. An instance is labelled as 
positive if ??????  is the antecedent of  ??? , or 
negative if ??????  is not the antecedent of  ???. 
An instance is associated with a feature vector 
which records different properties and relations 
between ???  and ?????? . The features used in 
our system will be discussed later in the paper. 
During training, for each other-anaphor, we 
consider as the candidate antecedents the preced-
ing NPs in its current and previous two sentences. 
                                                 
2 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
A positive instance is formed by pairing the ana-
phor and the correct antecedent. And a set of 
negative instances is formed by pairing the ana-
phor and each of the other candidates.  
Based on these generated training instances, 
we can train a binary classifier using any dis-
criminative learning algorithm. In our work, we 
employed support vector machine (SVM) due to 
its good performance in high dimensional feature 
vector spaces. 
During the resolution process, for each other-
anaphor encountered, all of the preceding NPs in 
a three-sentence window are considered. A test 
instance is created for each of the candidate ante-
cedents. The feature vector is presented to the 
trained classifier to determine the other-
anaphoric relation. The candidate with highest 
SVM outcome value is selected as the antecedent.  
3.3 Baseline Features 
Knowledge is usually represented as features for 
machine learning. In our system, we used the 
following groups of features for other-anaphora 
resolution 
 
? Word Distance Indicator 
This feature measures the word distance between 
an anaphor and a candidate antecedent, with the 
assumption that the candidate closer to the ana-
phor has a higher preference to be the antecedent. 
? Same Sentence Indicator 
This feature is either 0 or 1 indicating whether an 
anaphor and a candidate antecedent are in the 
same sentence. Here, the assumption is that the 
candidate in the same sentence as the anaphor is 
preferred for the antecedent. 
? Semantic Group Indicators 
A named-entity can be classified to a semantic 
category such as ?DNA?, ?RNA?, ?Protein? and 
so on3. Thus we use a set of features to record the 
category pair of an anaphor and a candidate ante-
cedent. For example, ?DNA-DNA? is generated 
for the case when both anaphor and candidate are 
DNAs. And ?DNA-Protein? is generated if an 
anaphor is a DNA and a candidate is a protein. 
These features indicate whether a semantic group 
can refer to another.  
Note that an anaphor and its antecedent may 
possibly belong to different semantic categories. 
For example, in the GENIA corpus we found that 
                                                 
3 In our study, we followed the semantic categories defined 
in the annotation scheme of the GENIA corpus.  
123
  
in some cases an expression of a protein name 
actually denotes the gene that encodes the pro-
tein. Thus for a given anaphor and a candidate 
under consideration, it is necessary to record the 
pair-wise semantic groups, instead of using a 
single feature indicating whether two expressions 
are of the same group. 
The semantic group for a named entity is giv-
en by our preprocessing NER. For the common 
NPs produced from the NP chunker, we classify 
the semantic group by looking for the words in-
side NPs. For example, an NP ending with 
?cells? is classified to ?Cell? group while an NP 
ending with ?gene? or ?allele? is classified to 
?DNA? group. 
? Lexical Pattern Indicators 
In some cases, the surrounding words of an ana-
phor and a candidate antecedent strongly indicate 
the ?part-whole? relation. For example, in 
?...asthma and other hypereosinophilic diseas-
es?, the reference between ?other hypereosino-
philic diseases? and ?asthma? is clear if the in-
between words ?and other? are taken into con-
sideration. Another example of such a hint pat-
tern is ?one? the other ?? The feature is 1 if the 
specific patterns are present for the current ana-
phor and candidate pair. A candidate with such a 
feature is preferred to be the antecedent. 
? Hierarchical Name Indicator  
This feature indicates whether an antecedent 
candidate is a substring of an anaphor or vice 
versa. This feature is used to capture cases like 
?Jun? and ?JunB? (?Jun? is a family of protein 
while ?JunB? is a member of this family). In 
many cases, an expression that is a super set 
comes with certain postfix words, for example, 
?family members? in  
?Fludarabine caused a specific depletion of 
STAT1 protein (and mRNA) but not of other 
STAT family members.?  
This kind of phenomenon is more common in 
bio-medical texts than in news articles. 
3.4 SVM Training and Classification 
In our system, we utilized the open-source soft-
ware SVM-Light4 for the classifier training and 
testing.  SVM is a robust statistical model which 
has been applied to many NLP tasks. SVM tries 
to learn a separating line to separate the positive 
instances from negative instances. Kernel trans-
formations are applied for non-linear separable 
                                                 
4 http://svmlight.joachims.org/ 
cases (Vapnik, 1995). In our study, we just used 
the default learning parameters provided by 
SVM-Light with the linear kernel. A more so-
phisticated kernel may further improve the per-
formance. 
4 Using Auto-mined Pattern Features 
The baseline features listed in Section 3.3 only 
rely on shallow lexical, position and semantic 
information about an anaphor and a candidate 
antecedent. It could not, nevertheless, disclose 
the ?part-whole? relation between two given ex-
pressions. In section 2, we have shown some ex-
isting pattern-based solutions that mine the ?part-
whole? relation in a large corpus with some pat-
terns that can represent the relation. However, 
these manually designed patterns are usually se-
lected by heuristics, which may not necessarily 
lead to a high coverage with a good accuracy in 
different domains. To overcome this shortcom-
ing, we would like to use an automatic method to 
mine effective patterns from a large data set. 
First, we create a set of seed pairs of the ?part-
whole? relation. And then, we use the seed pairs 
to discover the patterns that encode the ?part-
whole? relation from a large data set (PubMed as 
in our system). Such a solution is supposed to 
improve the coverage of lexical patterns, while 
still retain the desired ?part-whole? relation for 
other-anaphora resolution. 
The overview of our system with the automat-
ic mined patterns is illustrated in figure 1. 
Seed Pairs 
Generation
Pattern Mining
SVM
GENIA 
Corpus
Seed 
Pairs
Lexical 
Patterns
GENIA
T st 
Cas s
PubMED 
Corpus
 
Figure 1: System Overview 
There are three major parts in our system, 
namely, seed-pairs generation, pattern mining 
and SVM learning and classification. In the sub-
sequent subsections, we will discuss each of the 
three parts in details. 
124
  
4.1 Seed Pairs Preparation 
A seed pair is a pair of phrases/words following 
?part-whole? order, for example,  
?integrin alpha? - ?adhesion molecules? 
where ?integrin alpha? is a kind of ?adhesion 
molecules?.  
We extracted the seed pairs automatically 
from the GENIA corpus. The auto-extracting 
procedure makes uses of some lexical clues like 
?A, such as B, C and D?, ?A (e.g. B and C)?, ?A 
including B? and etc. The capital letter A, B, C 
and D refer to a noun phrase such as ?integrin 
alpha? and ?adhesion molecules?. For each oc-
currence of ?A such as B, C and D?, the program 
will generate seed pairs ?B-A?, ?C-A? and ?D-
A?. 
Consider the following example, 
?Mouse thymoma line EL-4 cells produce cyto-
kines such as interleukin (IL) -2, IL-3, IL-4, IL-
10, and granulocyte-macrophage colony-
stimulating factor in response to phorbol 12-
myristate 13-acetate (PMA).? 
We can extract the following seed pairs, 
?interleukin (IL) -2? ? ?cytokines? 
?IL -3? ? ?cytokines? 
?IL -4? ? ?cytokines? 
?IL -10? ? ?cytokines? 
?granulocyte-macrophage colony-stimulating 
factor? ? ?cytokines?  
A similar action is taken for other lexical 
clues. Totally, we got 909 distinct seed pairs ex-
tracted from the GENIA corpus. 
After the seed pairs have been extracted, an 
automatic verification of the seed pairs is per-
formed. The first purpose of the verification is to 
correct chunking errors. For example, ?HLA 
Class II Gene? may likely be wrongly split into 
?HLA Class? and ?II Gene?. This kind of errors 
is repaired by several simple syntactic rules. The 
second purpose of the verification is to remove 
the inappropriate seed pairs. In our system, we 
abandoned the seed pairs containing pronouns 
like ?those?, ?they?, or nouns like ?element?, 
?member? and ?agent?. Such seed pairs may ei-
ther find no patterns, or lead to meaningless pat-
terns because ?those? or ?elements? have no spe-
cific semantics and could refer to anything. 
4.2 Pattern Mining 
Having obtained the set of seed pairs, we will use 
them to mine patterns for the ?part-whole? rela-
tion. For each seed pair ?antecedent - anaphor? 
(anaphor represents the NP for the ?whole?, 
while antecedent represents the NP for the 
?part?), our system will search in a large data set 
for two queries: ?antecedent * anaphor? and 
?anaphor * antecedent? where the ?*? denotes 
any sequence of words or symbols. For a re-
turned search results, the text in between ?ante-
cedent? and ?anaphora? is extracted as a pattern. 
In our study, we used PubMed 2007 data set 
for the pattern mining. The data set contains 
about 52,000 abstracts with around 9,400,000 
words, and is an ideal large-scale resource for 
pattern mining. 
Consider, as an example, a seed pair ?NK 
kappa B ? ? ?transcription factor?. Suppose that 
a returned sentence for the query ?NK kappa B * 
transcription factor? is  
?...NK kappa B family transcription factors...? 
And a returned sentence for the query ?transcrip-
tion factor * NK kappa B? is 
?...transcription factors, including NF kappa 
B...? 
We can extract a pattern, 
?ANTECEDENT family ANAPHOR? from the 
first sentence and a pattern 
?ANAPHOR, including ANTECEDENT? from 
the second sentence.  
We restrict the patterns so that no pattern span 
across two or more sentences. In other words, the 
pattern shall not contain the symbol ?.?. The vi-
olated patterns will be removed. 
The count that a pattern occurs in the PubMed 
for a seed pair is recorded. As a pattern could be 
reduced by different seed pairs, we define the 
occurrence frequency of a pattern as the sum of 
the counts of the pattern for all the seed pairs, 
using following formula: 
???? ? =  ???(???? , ?? )
????
                           ??(1)   
where ???? ?  is the frequency of pattern ???? ; ??  is 
a seed pair; ?  is the set of all seed pairs. 
???(???? , ?? ) is the count of the pattern ????  for 
?? . 
All the mined patterns are sorted according to 
its frequency as defined in ??(1). 
4.3 Pattern Application 
For classifier training and testing, the patterns 
with high frequency are used as features. In our 
system, we used the top 40 patterns, while we 
also examined the influence the number of the 
patterns on the performance. (See Section 5.2) 
Given an instance ??(?????? , ???) and a pat-
tern feature ????  , a query is constructed by in-
125
  
stantiating with ??????  and ??? . For example, 
for an instance ??("?? ????? ?", "???????-  
?????? ???????")  and a pattern feature ?ANA-
PHOR, including ANTECEDENT?, we can get 
a query ?transcription factors, including NF 
kappa B?. The query is searched in the PubMed 
data set. The count of the query is recorded. The 
value of the pattern feature of a candidate is cal-
culated by normalizing the occurrence frequency 
among all the candidates of the anaphor. 
For demonstration, suppose we have an ana-
phor ?other transcription factors? with two ante-
cedent candidates ?IL-10? and ?NF kappa B?. 
Given a pattern feature ?ANAPHOR, including 
ANTECEDENT?, the count of the query ?tran-
scription factors, including IL-10? is 100 while 
that for ?transcription factors, including NF-
Kappa B? is 300. Then the values of the pattern 
feature for ?IL-10? and ?NF kappa B? are 0.25 
(
100
100+300
) and 0.75 (
300
100+300
), respectively. 
The value of a pattern feature can be inter-
preted as a degree of belief that an anaphor and a 
candidate antecedent have the ?part-whole? rela-
tion, with regard to the specific pattern. Since the 
value of a pattern feature is normalized among 
all the candidates, it could indicate the preference 
of a candidate against other competing candi-
dates. 
5 Experiment Results 
5.1 Experiments Setup 
In our experiments, we conducted a 3-fold cross 
validation to evaluate the performances. The total 
598 other-anaphora cases were divided into 3 
sets of size 200, 199 and 199 respectively. For 
each experiment, two sets were used for training 
while the other set was used for testing.  
For evaluation, we used the accuracy as the 
performance metric, which is defined as the cor-
rectly resolved other-anaphors divided by all the 
testing other-anaphors, that is, 
 
???????? =
# of correctly resolved anaphors
 # of total anaphors
 
5.2 Experiments Results 
Table 1 shows the performance of different 
other-anaphora resolution systems. The first line 
is for the baseline system with only the normal 
features as described in Section 3.3. From the 
table, we can find that the baseline system only 
achieves around 40% accuracy. A performance is 
lower than a similar system in news domain by 
Modjeska et al, (2003) where they reported  
51.6 % precision with 40.6% recall. This differ-
ence is probably because they utilized more se-
mantic knowledge such as hypernymy and mero-
nymy acquired from WordNet. Such knowledge, 
nevertheless, is not easily available in the bio-
medical domain. 
 
Sys Fold-1 Fold-2 Fold-3 Overall 
Baseline 
No Pattern 
42.0 % 
84/200 
38.2 % 
76/199 
40.7 % 
81/199 
40.3 % 
241/598 
Manual 
Pattern 
49.0 % 
98/200 
45.7 % 
91/199 
47.7 % 
95/199 
47.5 % 
284/598 
Auto-
mined 
Pattern 
59.0 % 
118/200 
53.8 % 
107/199 
56.8 % 
113/199 
56.5 % 
338/598 
Table 1: Performance Comparisons 
In our experiments, we tested the system with 
manually designed pattern features. We tried 10 
patterns that can represent the ?part-whole? rela-
tion. Table 2 summaries the patterns used in the 
system. Among them, the pattern ?Anaphor such 
as Antecedent? and ?Antecedent and other Ana-
phor? are commonly used in previous pattern 
based approaches (Markert et al, 2003; Mod-
jeska et al, 2003). 
 
Pattern 
ANTECEDENT is a kind of ANAPHOR 
ANTECEDENT is a type of ANAPHOR 
ANTECEDENT is a member of ANAPHOR 
ANTECEDENT is a part of ANAPHOR 
ANAPHOR such as ANTECEDENT 
ANTECEDENT and other ANAPHOR 
ANTECEDENT within ANAPHOR 
ANTECEDENT is a component of ANAPHOR 
ANTECEDENT is a sort of ANAPHOR 
ANTECEDENT belongs to ANAPHOR 
Table 2: Manually Selected Patterns 
 
The second line of Table 1 shows the results 
of the system with the manual pattern features. 
We can find that adding these pattern features 
produces an overall accuracy of 47%, yielding an 
increase of 7% accuracy against the baseline sys-
tem without the pattern features.  
The improvement in accuracy is consistent 
with previous work using the pattern-based ap-
proaches in the news domain (Modjeska et al, 
2003). However, we found the performance in 
the biomedical domain is worse than that in the 
news domain. For example, Modjeska et al 
(2003) reported a precision around 53%. This 
difference of performance suggests that the ma-
126
  
nually designed patterns may not necessarily 
work equally well in different domains.  
The last system we examined in the experi-
ment is the one with the automatically mined 
pattern features. Table 3 summarizes the top 
mined patterns ranked based on their occurrence 
frequency. Some of the patterns are intuitively 
good representation of the ?part-whole? relation. 
For example, ?ANAPHOR, including ANTE-
CEDENT?. ?ANAPHOR, such as ANTECE-
DENT? and ?ANAPHOR and other ANTECE-
DENT? which are in the manually designed pat-
tern list, are generated.  
The last line of Table 1 lists the result of the 
system with automatically mined pattern fea-
tures. It outperforms the baseline system (up to 
16% accuracy), and the system with manually 
selected patterns (9% accuracy). These results 
prove that our pattern features are effective for 
the other-anaphora resolution.  
 
Pattern Freq 
ANAPHOR, including ANTECEDENT 1213 
ANAPHOR including ANTECEDENT 726 
ANTECEDENT family ANAPHOR 583 
ANAPHOR such as ANTECEDENT 542 
ANTECEDENT transcription ANAPHOR 439 
ANAPHOR, such as ANTECEDENT 295 
ANTECEDENT and other ANAPHOR 270 
ANAPHOR and ANTECEDENT 250 
ANTECEDENT, dendritic ANAPHOR 246 
ANTECEDENT and ANAPHOR 238 
ANTECEDENT human ANAPHOR 223 
ANAPHOR (e.g., ANTECEDENT  213 
ANTECEDENT/rel ANAPHOR 188 
ANTECEDENT-like ANAPHOR 188 
ANAPHOR against ANTECEDENT  163 
Table 3: Auto-Mined Patterns 
To further compare the manually designed 
patterns and the automatically discovered pat-
terns. We examined the coverage rate of the two 
pattern sets. The coverage rate measures the ca-
pability that a set of patterns could lead to posi-
tive anaphor-antecedent pairs. An other-anaphor 
is said to be covered by a pattern set, if the ana-
phor and its antecedent could be hit (i.e., the cor-
responding query has a non-zero hit number) by 
at least one pattern in the list. Thus the coverage 
rate could be defined as 
????????(?)  
=   
#anaphors covered by the pattern set P
# total anaphors
 
The coverage rates of the two pattern sets are 
tabulated in table 4. It is apparent that the auto-
mined patterns have a significantly higher cover-
age (more than twice) than the manually de-
signed patterns. 
 
Patterns Coverage Rate 
Manually Designed 36.0 % 
Auto-Mined 92.1 % 
Table 4: Coverage Comparison 
In our experiments we were also concerned 
about the usefulness of each individual pattern. 
For this purpose, we examined the loss of the 
accuracy when withdrawing a pattern feature 
from the feature list. The top 10 patterns with the 
largest accuracy loss are summarized in table 5. 
 
Pattern 
Acc 
Loss 
ANAPHOR, including ANTECEDENT 4.18% 
ANAPHOR including ANTECEDENT 3.18% 
ANAPHOR such as ANTECEDENT 2.84% 
ANTECEDENT transcription ANAPHOR 2.17% 
ANTECEDENT and other ANAPHOR 2.01% 
ANAPHOR, such as ANTECEDENT 1.84% 
ANTECEDENT family ANAPHOR 1.84% 
ANAPHOR (e.g., ANTECEDENT 1.51% 
ANTECEDENT-like ANAPHOR 1.17% 
ANTECEDENT/rel ANAPHOR 1.17% 
Table 5: Usefulness of Each Pattern 
The process of automatic pattern mining 
would generate numerous surface patterns. It is 
not reasonable to use all the patterns as features. 
As mentioned in section 4.3, we rank the pattern 
based on their occurrence frequency and select 
the top ones as the features. It would be interest-
ing to see how the number of patterns influences 
the performance of anaphora resolution. In figure 
2, we plot the accuracy under different number 
top pattern features. We can find by using more 
patterns, the coverage keeps increasing. The ac-
curacy also increases, but it reaches the peak 
with around 40 patterns. With more patterns, the 
accuracy remains at the same level. This is be-
cause the low frequency patterns usually are not 
that indicative of the ?part-whole? relation. In-
cluding these pattern features would bring noises 
but not help the performance. The flat curve after 
the peak point suggests that the machine learning 
algorithm can effectively identify the importance 
of the pattern features for the resolution decision, 
and therefore including non-indicative patterns 
would not damage the performance. 
In our experiment, we also interested to com-
pare the utility of PubMed with other general 
data sets. Thus, we tested pattern mining by us-
127
  
ing the Google-5-grams corpus5 which lists the 
hit number of all the queries of five words or less 
in the Web. Unfortunately, we found that the per-
formance is worse than using PubMed. The pat-
terns mined from the Web corpus only gives an 
accuracy of around 41%, almost the same as the 
baseline system without using any pattern fea-
tures. The bad performance is due to the fact that 
most of bio-medical names are quite long (2~4 
words) and occur infrequently in the non-
technique data set. Consequently, a query formed 
by a biomedical seed pair usually cannot be 
found in the Web corpus (We found the coverage 
of the auto-mined patterns mined from the corpus 
is only about 20%). 
 
Figure 2: Performance of Various No. of Patterns 
6 Conclusion & Future Works 
In this paper, we have presented how to automat-
ically mined pattern features for learning-based 
other-anaphora resolution in bio-medical texts. 
The patterns that represent the ?part-whole? rela-
tions are automatically mined from a large data 
set. They are used as features for a SVM-based 
classifier learning and testing. The results of our 
experiments show a reasonably good perfor-
mance with 56.5% accuracy). It outperforms 
(16% in accuracy) the baseline system without 
the pattern features, and also beats (9%) the sys-
tem with manually designed pattern features. 
There are several directions for future work. 
We would like to employ a pattern pruning 
process to remove those less indicative patterns 
such as ?ANAPHOR, ANTECEDENT?. And we 
also plan to perform pattern normalization which 
integrates two similar or literally identical pat-
                                                 
5 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?  
   catalogId=LDC2006T13 
terns into a single one. By doing so, the useful 
patterns may come to the top of the pattern list. 
Also we would like to explore ontology re-
sources like MESH and Genes Ontology, which 
can provide enriched hierarchies of bio-medical 
terms and thus would benefit other-anaphora res-
olution. 
Acknowledgements  
This study on co-reference resolution is partially supported 
by a Specific Targeted Research Project (STREP) of the 
European Union's 6th Framework Programme within IST 
call 4, Bootstrapping of Ontologies and Terminologies 
STrategic REsearch Project (BOOTStrep). 
References 
Castano J, Zhang J and Pustejovsky J. Anaphora Resolution 
in Biomedical Literature. Submitted to International Sym-
posium on Reference Resolution 2002, Alicante, Spain 
Clark H. Bridging. In Thinking. Readings in Cognitive 
Science. Johnson-Laird and Wason edition. Cambridge. 
Cambridge University Press; 1977.411?420 
Gasperin C and Vieira R. Using Word Similarity Lists for 
Resolving Indirect Anaphora. In Proceedings of ACL 
Workshop on Reference Resolution and Its Application. 
30 June 2004; Barcelona. 2004.40-46 
Girju R, Badulescu A and Moldovan D. Automatic Discov-
ery of Part-Whole Relations. Computational Linguistics, 
2006, 32(2):83-135 
Bernauer J.. Analysis of Part-Whole Relation and Subsump-
tion in Medical Domain. Data Knowledge Enginnering 
1996, 20:405-415 
Markert K. and Nissim M. Comparing Knowledge Sources 
for Nominal Anaphora Resolution. Computational Lin-
guistics, 2005, 31(3):367-402 
Markert K, Modjeska N and Nissim M. Using the Web for 
Nominal Anaphora Resolution. In Proceedings of EACL 
Workshop on the Computational Treatment of Anaphora. 
14 April 2003; Budapest. 2003.39-46 
Mitokov R. Anaphor Resolution. The State of The Art. 
Working Paper, University of Wolverhampton, UK, 1999 
Modjeska N, Markert K and Nissim M. Using the Web in 
Machine Learning for Other-anaphor Resolution. In Pro-
ceedings of the 2003 Conference on Empirical Methods in 
Natural Language Processing. July2003,Sapporo.176-183 
Soon WM, Ng HT and Lim CY. A Machine Learning Ap-
proach to Coreference Resolution of Noun Phrases. Com-
putational Linguistics, 2001, 27(4).521-544 
Vapnik, V. Chapter 5 Methods of Pattern Recognition. In 
The Nature of Statistical Learning Theory. New York. 
Springer-Verlag, 1995.123-167 
Varzi C.  Parts, Wholes, and Part-whole Relation. The Pros-
pects of the Mereotopology. Data & Knowledge Engi-
neering, 1996, 20.259-286 
Vieira R, Bick E, Coelho J, Muller V, Collovini S, Souza J 
and Rino L. Semantic Tagging for Resolution of Indirect 
Anaphora. In Proceedings of 7th SIGdial Workshop on 
Discourse and Dialogue. July 2006; Sydney.76-79 
Burges C. A Tutorial on Supporting Vector Machines for 
Pattern Recognition. Data Mining and Knowledge Dis-
covery 1998, 2:121-167 
Ng V. and Cardie C. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of An-
nual Conference for Association of Computational Lin-
guistics 2002, Philadelphia.104-111 
128
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961?968
Manchester, August 2008
Coreference Systems based on Kernels Methods
Yannick Versley
SFB 441
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Alessandro Moschitti
DISI
University of Trento
moschitti@disi.unitn.it
Massimo Poesio
DISI
University of Trento
massimo.poesio@unitn.it
Xiaofeng Yang
Data Mining Department
Institute for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Abstract
Various types of structural information -
e.g., about the type of constructions in
which binding constraints apply, or about
the structure of names - play a central role
in coreference resolution, often in combi-
nation with lexical information (as in ex-
pletive detection). Kernel functions ap-
pear to be a promising candidate to capture
structure-sensitive similarities and com-
plex feature combinations, but care is re-
quired to ensure they are exploited in the
best possible fashion. In this paper we
propose kernel functions for three subtasks
of coreference resolution - binding con-
straint detection, expletive identification,
and aliasing - together with an architec-
ture to integrate them within the standard
framework for coreference resolution.
1 Introduction
Information about coreference relations?i.e.,
which noun phrases are mentions of the same
entity?has been shown to be beneficial in a great
number of NLP tasks, including information
extraction (McCarthy and Lehnert 1995), text
planning (Barzilay and Lapata 2005) and sum-
marization (Steinberger et al 2007). However,
the performance of coreference resolvers on
unrestricted text is still quite low. One reason
for this is that coreference resolution requires a
great deal of information, ranging from string
matching to syntactic constraints to semantic
knowledge to discourse salience information to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
full common sense reasoning (Sidner 1979; Hobbs
1978, 1979; Grosz et al 1995; Vieira and Poesio
2000; Mitkov 2002). Much of this information
won?t be available to robust coreference resolvers
until better methods are found to represent and
encode common sense knowledge; but part of
the problem is also the need for better methods
to encode information that is in part structural,
in part lexical. Enforcing binding constraints
?e.g., ruling out Peter as antecedent of him in (1a)
requires recognizing that the anaphor occurs in a
particular type of construction (Chomsky 1981;
Lappin and Leass 1994; Yang et al 2006) whose
exact definition however has not yet been agreed
upon by linguists (indeed, it may only be definable
in a graded sense (Sturt 2003; Yang et al 2006)),
witness examples like (1b). Parallelism effects are
a good example of structural information inducing
preferences rather than constraints. Recognizing
that It in examples such as (1c,d) are expletives
requires a combination of structural information
and lexical information (Lappin and Leass 1994;
Evans 2001). But some sort of structure also
underlies our interpretation of other types of
coreference: e.g., knowledge about the structure
of names certainly plays a role in recognizing
that BJ Habibie is a possible antecedent for Mr.
Habibie.
(1) a. John thinks that Peter hates him.
b. John hopes that Jane is speaking only to
himself.
c. It?s lonely here.
d. It had been raining all day.
The need to capture such information suggests
a role for kernel methods (Vapnik 1995) in coref-
erence resolution. Kernel functions make it pos-
sible to capture the similarity between structures
961
without explicitly enumerating all the substruc-
tures, and have therefore been shown to be a vi-
able approach to feature engineering for natural
language processing for any task in which struc-
tural information plays a role, e.g. (Collins and
Duffy 2002; Zelenko et al 2003; Giuglea and Mos-
chitti 2006; Zanzotto and Moschitti 2006; Mos-
chitti et al 2007). Indeed, they have already been
used in NLP to encode the type of structural in-
formation that plays a role in binding constraints
(Yang et al 2006); however, the methods used in
this previous work do not make it possible to ex-
ploit the full power of kernel functions. In this
work, we extend the use of kernel functions for
coreference by designing and testing kernels for
three subtasks of the coreference task:
? Binding constraints
? Expletive detection
? Aliasing
and developing distinct classifiers for each of these
tasks. We show that our developed kernels produce
high accuracy for both distinct classifiers for these
subtasks as well as for the complete coreference
system.
In the remainder: Section 2, briefly describes
the basic kernel functions that we used; Section
3 illustrates our new kernels for expletive, binding
and name alias detection along with a coreference
context kernel; Section 4 reports the experiments
on individual classifiers on expletives, binding and
names whereas Section 5 shows the results on the
complete coreference task; Finally, Section 6 de-
rives the conclusions.
2 Kernel for Structured Data
We used three kernel functions in this work: the
String Kernel (SK) proposed in Shawe-Taylor and
Cristianini (2004) to evaluate the number of sub-
sequences between two sequences, the Syntactic
Tree Kernel (STK; see Collins and Duffy 2002)
which computes the number of syntactic tree frag-
ments and the Partial Tree Kernel (PTK; see Mos-
chitti 2006) which provides a more general repre-
sentation of trees in terms of tree fragments. We
discuss each in turn.
2.1 String Kernels (SK)
The string kernels that we consider count the num-
ber of substrings shared by two sequences contain-
ing gaps, i.e. some of the characters of the original
NP 
D N 
a 
  cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a 
   cat 
  cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
Figure 1: A tree with some of its STFs .
NP 
D N 
VP 
V 
brought 
a 
   cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
a 
   cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Figure 2: A tree with some of its PTFs.
string are skipped. Gaps penalize the weight asso-
ciated with the matched substrings. More in detail,
(a) longer subsequences receive lower weights.
(b) Valid substrings are sequences of the original
string with some characters omitted, i.e. gaps. (c)
Gaps are accounted by weighting functions and (d)
symbols of a string can also be whole words, i.e.
the word sequence kernel Cancedda et al (2003).
2.2 Tree Kernels
The main idea underlying tree kernels is to com-
pute the number of common tree fragments be-
tween two trees without explicitly considering the
whole fragment space. The type of fragments char-
acterize different kernel functions. We consider
syntactic tree fragments (STFs) and partial tree
fragments (PTFs)
2.2.1 Syntactic Tree Kernels (STK)
An STF is a connected subset of the nodes and
edges of the original tree, with the constraint that
any node must have all or none of its children. This
is equivalent to stating that the production rules
contained in the STF cannot be partial. For ex-
ample, Figure 1 shows a tree with its PTFs: [VP [V
NP]] is an STF, [VP [V]] or [VP [NP]] are not STFs.
2.2.2 Partial Tree Kernel (PTK)
If we relax the production rule constraint over
the STFs, we obtain a more general substructure
type, i.e. PTF, generated by the application of par-
tial production rules, e.g. Figure 2 shows that [VP
[NP[D]]] is indeed a valid fragment. Note that
PTK can be seen as a STK applied to all possible
child sequences of the tree nodes, i.e. a string ker-
nel combined with a STK.
2.3 Kernel Engineering
The Kernels of previous section are basic functions
that can be applied to feature vectors, strings and
962
trees. In order to make them effective for a specific
task, e.g. for coreference resolution: (a) we can
combine them with additive or multiplicative op-
erators and (b) we can design specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
It is worth noting that a basic kernel applied to
an innovative view of a structure yields a new ker-
nel (e.g. Moschitti and Bejan (2004); Moschitti
et al (2006)), as we show below:
Let K(t
1
, t
2
) = ?(t
1
) ? ?(t
2
) be a basic ker-
nel, where t
1
and t
2
are two trees. If we map t
1
and t
2
into two new structures s
1
and s
2
with a
mapping ?
M
(?), we obtain: K(s
1
, s
2
) = ?(s
1
) ?
?(s
2
) = ?(?
M
(t
1
)) ? ?(?
M
(t
2
)) = ?
?
(t
1
) ?
?
?
(t
2
)=K?(t
1
, t
2
), which is a noticeably different
kernel induced by the mapping ?? = ? ? ?
M
.
3 Kernels for Coreference Resolution
In this paper we follow the standard learning ap-
proach to coreference developed by Soon et al
(2001) and also used the few variants in Ng and
Cardie (2002). In this framework, training and
testing instances consist of a pair (anaphor, an-
tecedent). During training, a positive instance is
created for each anaphor encountered by pairing
the anaphor with its closest antecedent; each of the
non-coreferential mentions between anaphor and
antecedent is used to produce a negative instance.
During resolution, every mention to be resolved is
paired with each preceding antecedent candidate
to form a testing instance. This instance is pre-
sented to the classifier which then returns a class
label with a confidence value indicating the likeli-
hood that the candidate is the antecedent.
The nearest candidate with a positive classifica-
tion will be selected as the antecedent of the pos-
sible anaphor. The crucial point is that in this ap-
proach, the classifier is trained to identify positive
and negative instances of the resolution process. In
previous work on using kernel functions for coref-
erence (Yang et al 2006), structural information
in the form of tree features was included in the
instances. This approach is appropriate for iden-
tifying contexts in which the binding constraints
apply, but not, for instance, to recognize exple-
tives. In this work we adopted therefore a more
general approach, in which separate classifiers are
used to recognize each relevant configuration, and
their output is then used as an input to the coref-
erence classifier. In this section we discuss the
types of structures and kernel functions we used
for three different kinds of classifiers: expletive,
binding and alias classifiers. We then present the
results of these classifiers, and finally the results
with the coreference resolver as a whole.
3.1 Expletive Kernels
In written text, about a third of the occurrences
of the pronoun it are not coreferent to a previ-
ous mention, but either refer to a general discourse
topic (it?s a shame) or do not refer at all, as in the
case of extraposed subjects (it is thought that . . . )
or weather verbs (it?s raining). It is desirable to
minimize the impact that these non-anaphoric pro-
nouns have on the accuracy of a anaphora resolu-
tion: Lappin and Leass (1994), for example, use
several heuristics to filter out expletive pronouns,
including a check for patterns including modal ad-
jectives (it is good/necessary/. . . that . . . ), and cog-
nitive verbs (it is thought/believed/. . . that . . . ).
Newer approaches to the problem use machine-
learning on hand-annotated examples: Evans
(2001) compares a shallow approach based on
surrounding lemmas, part-of-speech tags, and the
presence of certain elements such as modal adjec-
tives and cognitive verbs, trained on 3171 exam-
ples from Susanne and the BNC to a reimplemen-
tation of a pattern-based approach due to Paice and
Husk (1987) and finds that the shallower machine-
learning approach compares favorably to it. Boyd
et al (2005) use an approach that combines some
of Evans? shallow features with hand-crafted pat-
terns in a memory based learning approach and
find that the more informative features are ben-
eficial for the system?s performance (88% accu-
racy against 71% for their reimplementation using
Evans? shallow features).
Evans? study also mentions that incorporating
the expletive classifier as a filter for a pronoun re-
solver gives a gain between 2.86% (for manually
determined weights) and 1% (for automatically op-
timized weights).
Tree kernels are a good fit for expletive classi-
fication since they can naturally represent the lex-
ical and structural context around a word. Our fi-
nal classifier uses the combination of an unmodi-
fied tree (UT) (where the embedding clause or verb
phrase of the pronoun is used as a tree), and a tree
that only preserves the most salient structural fea-
tures (ST).
The reduced representation prunes all nodes that
963
would not be seen as indicative in a pattern ap-
proach, essentially keeping verb argument struc-
ture and important lexical items, such as the gov-
erning verb and, in the case of copula construc-
tions, the predicate. For example, the phrase
(S (NP (PRP It))
(VP (VBZ has)
(NP (NP (DT no) (NN bearing))
(PP (IN on)
(NP (NP (PRP$ our)
(NN work)
(NN force))
(NP (NN today)))))
(. .))
would be reduced to the ST:
(S-I (NP-I (PRP-I It))
(VP (VBX have)
(NP))
(.))
or, in a similar fashion,
(S (NP (PRP it))
(VP (VBZ ?s)
(NP (NP (NN time))
(PP (IN for)
(NP (PRP$ their)
(JJ biannual)
(NN powwow))))))
would just be represented as the ST:
(S-I (NP-I (PRP-I it))
(VP (BE VBZ)
(NP-PRD (NN time))))
3.2 Binding Kernels
The resolution of pronominal anaphora heavily re-
lies on the syntactic information and relationships
between the anaphor and the antecedent candi-
dates, including binding and other constraints, but
also context-induced preferences in sub-clauses.
Some researchers (Lappin and Leass 1994;
Kennedy and Boguraev 1996) use manually de-
signed rules to take into account the grammati-
cal role of the antecedent candidates as well as
the governing relations between the candidate and
the pronoun, while others use features determined
over the parse tree in a machine-learning approach
(Aone and Bennett 1995; Yang et al 2004; Luo
and Zitouni 2005). However, such a solution has
limitations, since the syntactic features have to be
selected and defined manually, and it is still partly
an open question which syntactic properties should
be considered in anaphora resolution.
We follow (Yang et al 2006; Iida et al 2006) in
using a tree kernel to represent structural informa-
tion using the subtree that covers a pronoun and its
antecedent candidate. Given a sentence like ?The
Figure 3: The structure for binding detection for
the instance inst(?the man?, ?him?) in the sentence
?the man in the room saw him?
man in the room saw him.?, we represent the syn-
tactic relation between ?The man? and ?him?, by
the shortest node path connecting the pronoun and
the candidate, along with the first-level of the node
children in the path.
Figure 3 graphically shows such tree highlighted
with dash lines. More in detail we operate the fol-
lowing tree transformation:
(a) To distinguish from other words, we explic-
itly mark up in the structured feature the pronoun
and the antecedent candidate under consideration,
by appending a string tag ?ANA? and ?CANDI?
in their respective nodes, i.e. ?NN-CANDI? for
?man? and ?PRP-ANA? for ?him?.
(b) To reduce the data sparseness, the leaf nodes
representing the words are not incorporated in the
feature, except that the word is the word node of
the ?DET? type (this is to indicate the lexical prop-
erties of an expression, e.g., whether it is a definite,
indefinite or bare NP).
(c) If the pronoun and the candidate are not in the
same sentence, we do not include the nodes denot-
ing the sentences (i.e., ?S? nodes) before the can-
didate or after the pronoun.
The above tree structures will be jointly used
with the basic STK which extracts tree fragments
able to characterize the following information: (a)
the candidate is post-modified by a preposition
phrase, (the node ?PP? for ?in the room? is in-
cluded), (b) the candidate is a definite noun phrase
(the article word ?the? is included), (c) the candi-
date is in a subject position (NP-S-VP structure),
(d) the anaphor is an object of a verb (the node
?VB? for ?saw? is included) and (e) the candidate
is c-commanding the anaphor (the parent of the
NP node for ?the main in the room? is dominat-
ing the anaphor (?him?), which are important for
reference determination in the pronoun resolution.
964
3.3 Encoding Context via Word Sequence
Kernel
The previous structures aim at describing the in-
teraction between one referential and one referent;
if such interaction is observed on another mention
pair, an automatic algorithm can establish if they
corefer or not. This kind of information is the most
useful to characterize the target problem, however,
the context in which such interaction takes place is
also very important. Indeed, natural language pro-
poses many exceptions to linguistic rules and these
can only be detect by looking at the context. To be
able to represent context words or phrases, we use
context word windows around the mentions and
the subsequence kernel function (see section 2.1)
to extract many features from it.
For example, in the context of ?and so Bill
Gates says that?, a string kernel would ex-
tract features including: Bill Gates says that,
says that, Gates, Gates says that, Bill says that,
so Gates says that, and so that and so on.
Name Alias
BJ Habibie Mr. Habibie
Federal Express Fedex
Ju Rong Zhi Ju
Table 1: Examples of coreferent named entities
(aliases) taken from the MUC 6 corpus.
3.4 Kernels for Alias Resolution
Most methods currently employed by coreference
resolution (CR) systems for identifying coreferent
named entities, i.e. aliases, are fairly simplistic in
nature, relying on simple surface features such as
the edit distance between two strings representing
names. We investigate the potential of using the
structure contained within names. This can be very
useful to solve complex cases like those shown in
Table 1, taken from the MUC 6 corpus (Chinchor
and Sundheim 2003). For this purpose, we add
syntactic information to the feature set by tagging
the parts of a name (e.g. first name, last name, etc.)
as illustrated in Figure 4.
To automatically extract such structure we used
the High Accuracy Parsing of Name Internal Struc-
ture (HAPNIS) script1. HAPNIS takes a name as
input and returns a tagged name like what is shown
in Figure 4. It uses a series of heuristics in making
its classifications based on information such as the
1The script is freely available at
http://www.cs.utah.edu/ hal/HAPNIS/.
Figure 4: A proper name labeled with syntactic in-
formation.
serial positions of tokens in a name, the total num-
ber of tokens, the presence of meaningful punctua-
tion such as periods and dashes, as well as a library
of common first names which can be arbitrarily ex-
tended to any size. The tag set consists of the fol-
lowing: surname, forename, middle, link, role, and
suffix2.
Once the structure for a name has been de-
rived, we can apply tree kernels to represent it in
the learning algorithms thus avoiding the manual
feature design. Such structures are not based on
any particular grammar, therefore, any tree sub-
part may be relevant. In this case the most suitable
kernel is PTK, which extracts any tree subpart. It
is worth to note that the name tree structure can
be improved by inserting a separate node for each
name character and exploiting the string matching
approximation carried out by PTK. For example,
Microsoft Inc. will have a large match with Mi-
crosoft Incorporated whereas the standard string
matching would be null.
4 Experiments with Coreference Subtask
Classifiers
In these experiments we test the kernels devised for
expletive (see Section 3.1), binding (see Section
3.2) and alias detection (see Section 3.4), to study
the level of accuracy reachable by our kernel-based
classifiers. The baseline framework is constituted
by SVMs along with a polynomial kernel over the
Soon et al?s features.
4.1 Experiments on Expletive Classification
We used the BBN Pronoun corpus3 as a source of
examples, with the training set consisting of sec-
tions 00-19, yielding more than 5800 instances of
2Daume? reports a 99.1% accuracy rate on his test data set.
We therefore concluded that it was sufficient for our purposes.
3Ralph Weischedel and Ada Brunstein (2005): BBN Pro-
noun Coreference and Entity Type Corpus, LDC2005T33
965
it, with the testing set consisting of sections 20 and
21, using the corresponding parses from the Penn
Treebank for the parse trees. Additionally, we re-
port on the performance of the classifier learnt on
only the first 1000 instances to verify that our ap-
proach also works for small datasets. The results
in Table 2 show that full tree (UT) achieves good
results whereas the salient tree (ST) leads to a bet-
ter ability to generalize, and the combination ap-
proach outperforms both individual trees.
BBN large BBN small
Prec Recl Acc Prec Recl Acc
UT 83.87 61.54 84.35 78.76 52.66 80.85
ST 78.08 67.46 83.98 77.61 61.54 82.50
UT+ST 81.12 68.64 85.27 80.74 64.50 84.16
Table 2: Results for kernel-based expletive detec-
tion (using STK)
Note that the accuracy we get by training on
1000 examples (84% accuracy; see the small col-
umn in Table 2) is better than Boyd?s replication of
Evans (76% accuracy) or their decision tree clas-
sifier (81% accuracy) even though Boyd et al?s
dataset is three times bigger. On the other hand,
Boyd et als full system, which uses substantial
hand-crafted knowledge, gets a still better result
(88% accuracy), which is also higher than the ac-
curacy of our classifier even when trained on the
full 5800 instances.
MUC-6
Prec Recl F
Soon et al 51.25 55.51 53.29
STK 71.93 55.41 62.59
Table 3: Binding classifier: coreference classifica-
tion on same-sentence pronouns
4.2 Experiments with the Binding Classifier
To assess the effect of the binding classifier on
same-sentence pronoun links, we extracted 1398
mention pairs from the MUC-6 training data where
both mentions were in the same sentence and at
least one item of the pair included a pronoun, us-
ing the first 1000 for training and the remaining
398 examples for testing. The results (see Table 3)
show that the syntactic tree kernel (STK) consider-
ably improves the precision of classification of the
Soon et al?s features.
4.3 Experiments on Alias Classification
For our preliminary experiments, we extracted
only pairs in the MUC 6 testing set in which both
mentions were proper names, as determined by
the coreference resolver?s named entity recognizer.
This set of proper names contained about 37,000
pairs of proper names of which about 600 were
positive instances. About 5,500 pairs were ran-
domly selected as test instances and the rest were
used for training.
In the first experiment, we trained a decision
tree classifier to detect if two names are aliases.
For this task, we used either the string kernel score
over the sequence of characters or the edit distance.
The results in Table 4 show that the string kernel
score performs better by 21.6 percentage points in
F-measure.
In the second experiments we used SVMs
trained with the string kernel over the name-
character sequences and with PTK, which takes
into account the structure of names. The re-
sults in Table 5 show that the structure improves
alias detection by almost 5 absolute percent points.
This suggests that an effective coreference sys-
tem should embed PTK and name structures in the
coreference pair representation.
Recall Precision F-measure
String kernel 49.5% 60.8% 54.6%
Edit distance 23.9% 53.1% 33.0%
Table 4: Decision-tree based classification of name
aliases using string kernels and edit distance.
Recall Precision F-measure
String kernel 58.4% 67.5% 62.6%
PTK 64.8% 70.0% 67.3%
Table 5: SVM-based classification of name aliases
using string kernels and tree-based feature.
5 Experiments on Coreference Systems
In this section we evaluate the contribution in the
whole coreference task of the expletive classifier
and the binding kernel. The predictions of the for-
mer are used as a feature of our basic coreference
system whereas the latter is used directly in the
coreference classifier by adding it to the polyno-
mial kernel of the basic system.
Our basic system is based on the standard learn-
ing approach to coreference developed by Soon
et al (2001). It uses the features from Soon et
al?s work, including lexical properties, morpho-
logic type, distance, salience, parallelism, gram-
matical role and so on. The main difference with
966
Soon et al (2001) is the use of SVMs along with a
polynomial kernel.
MUC-6
Prec Recl F
plain 65.2 66.9 66.0
plain+expletive 66.1 66.9 66.5
upper limit 70.0 66.9 68.4
Table 6: Expletive classification: influence on pro-
noun resolution
5.1 Influence of Expletive classification
To see how useful a classifier for expletives can
be, we conducted experiments using the expletive
classifier learned on the BBN pronoun corpus on
the MUC-6 corpus. Preliminary experiments indi-
cated that perfect detection of expletives (i.e. using
gold standard annotation) could raise the precision
of pronoun resolution from 65.2% to 70.0%, yield-
ing a 2.4% improvement in the F-score for pronoun
resolution alone, or 0.6% improvement in the over-
all coreference F-score (see Table 6).
For a more realistic assessment, we used the
classifier learned on the BBN pronoun corpus ex-
amples as an additional feature to gauge the im-
provement that could be achieved using it. While
the gain in precision is small even in comparison
to the achievable error reduction, we need to keep
in mind that our baseline is in fact a well-tuned
system.
MUC-6 ACE02-BNews
R P F R P F
PK 64.3 63.1 63.7 58.9 68.1 63.1
PK+TK 65.2 80.1 71.9 65.6 69.7 67.6
Table 7: Results of the pronoun resolution
5.2 Binding and Context Kernels
In these experiments, we compared our corefer-
ence system based on Polynomial Kernel (PK)
against its combinations with Syntactic Tree Ker-
nels (STK) over the binding structures (Sec. 3.2)
and Word Sequence Kernel (WSK) on context
windows (Sec. 3.3). We experimented with
both the only pronoun and the complete corefer-
ence resolution tasks on the standard MUC-6 and
ACE03-BNews data sets.
On the validation set, the best kernel combina-
tion between PK and STK was STK(T1, T2) ?
PK(~x
1
, ~x
2
)+PK(~x
1
, ~x
2
). Then an improvement
arises when simply summing WSK.
Table 7 lists the results for the pronoun resolu-
tion. We used PK on the Soon et al?s features as
the baseline. On MUC-6, the system achieves a
recall of 64.3% and precision 63.1% and an over-
all F-measure of 63.7%. On ACE02-BNews, the
recall is lower 58.9% but the precision is higher,
i.e. 68.1%, for a resulting F-measure of 63.1%.
In contrast, adding the binding kernel (PK+STK)
leads to a significant improvement in 17% preci-
sion for MUC-6 with a small gain (1%) in recall,
whereas on the ACE data set, it also helps to in-
crease the recall by 7%. Overall, we can see an
increase in F-measure of around 8% for MUC and
4.5% for ACE02-BNews. These results suggest
that the structured feature is very effective for pro-
noun resolution.
MUC-6 ACE02-BNews
R P F R P F
PK 61.5 67.2 64.2 54.8 66.1 59.9
PK+STK 63.4 67.5 65.4 56.6 66.0 60.9
PK+STK+WSK 64.4 67.8 66.0 57.1 65.4 61.0
Table 8: Results of the coreference resolution
Table 8 lists the results on the coreference res-
olution. We note that adding the structured fea-
ture to the polynomial kernel, i.e. using the model
PK+STK, improves the recall of 1.9% for MUC-
6 and 1.8% for ACE-02-BNews and keeps invari-
ant the precision. Compared to pronoun resolu-
tion, the improvement of the overall F-measure is
smaller (about 1%). This occurs since the resolu-
tion of non-pronouns case does not require a mas-
sive use of syntactic knowledge as in the pronoun
resolution problem. WSK further improves the
system?s F1 suggesting that adding structured fea-
tures of different types helps in solving the coref-
erece task.
6 Conclusions
We presented four examples of using kernel-based
methods to take advantage of a structured repre-
sentation for learning problems that arise in coref-
erence systems, presenting high-accuracy classi-
fiers for expletive detection, binding constraints
and same-sentence pronoun resolution, and name
alias matching. We have shown the accuracy
of the individual classifiers for the above tasks
and the impact of expletives and binding classi-
fiers/kernels in the complete coreference resolu-
tion system. The improvement over the individual
and complete tasks suggests that kernel methods
967
are a promising research direction to achieve state-
of-the-art coreference resolution systems.
Future work is devoted on making the use of ker-
nels for coreference more efficient since the size of
the ACE-2 corpora prevented us to directly use the
combination of all kernels that we designed. In this
paper, we have also studied a solution which re-
lates to factoring out decisions into separate clas-
sifiers and using the decisions as binary features.
However, this solution shows some loss in terms of
accuracy. We are currently investigating methods
that allow us to combine the accuracy and flexibil-
ity of the integrated approach with the speed of the
separate classifier approach.
Acknowledgements Y. Versley was funded by the
Deutsche Forschungsgemeinschaft as part of SFB (Collabora-
tive Research Centre) 441. A. Moschitti has been partly sup-
ported by the FP6 IST LUNA project (contract No. 33549).
Part of the work reported in this paper was done at the Johns
Hopkins Summer Workshop in 2007, funded by NSF and
DARPA. We are especially grateful for Alan Jern?s implemen-
tation help for name structure identification.
References
Aone, C. and Bennett, S. W. (1995). Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proc. ACL 1995, pages 122?129.
Barzilay, R. and Lapata, M. (2005). Modelling local coher-
ence: An entity-based approach. In Proc. of ACL, Ann
Arbor, MI.
Boyd, A., Gegg-Harrison, W., and Byron, D. (2005). Iden-
tifying non-referential it: a machine learning approach in-
corporating linguistically motivated features. In Proc. ACL
WS on Feature Engineering for Machine Learning in Nat-
ural Language Processing.
Cancedda, N., Gaussier, E., Goutte, C., and Renders, J. M.
(2003). Word sequence kernels. JMLR, 3:1059?1082.
Chinchor, N. and Sundheim, B. (2003). Muc 6 corpus. Mes-
sage Understanding Conference (MUC) 6.
Chomsky, N. (1981). Lectures on government and binding.
Foris, Dordrecht, The Netherlands.
Collins, M. and Duffy, N. (2002). New ranking algorithms for
parsing and tagging: kernels over discrete structures and
the voted perceptron. In Proc. ACL 2002, pages 263?270.
Evans, R. (2001). Applying machine learning toward an au-
tomatic classification of it. Literary and Linguistic Com-
puting, 16(1):45?57.
Giuglea, A.-M. and Moschitti, A. (2006). Semantic role la-
beling via framenet, verbnet and propbank. In Proceedings
of Coling-ACL, Sydney, Australia.
Grosz, B., Joshi, A., and Weinstein, S. (1995). Centering: a
framework for modeling the local coherence of discourse.
CL, 21(2):203?225.
Hobbs, J. (1978). Resolving pronoun references. Lingua,
44:339?352.
Hobbs, J. (1979). Resolving pronoun references. Coherence
and Coreference, 3(1):67?90.
Iida, R., Inui, K., and Matsumoto, Y. (2006). Exploiting syn-
tactic patterns as clues in zero-anaphora resolution. In
Proc. Coling/ACL 2006, pages 625?632.
Kennedy, C. and Boguraev, B. (1996). Anaphora for every-
one: pronominal anaphora resolution without a parser. In
Proc. Coling 1996.
Lappin, S. and Leass, H. (1994). An algorithm for pronominal
anaphora resolution. CL, 20(4):525?561.
Luo, X. and Zitouni, I. (2005). Multi-lingual coreference res-
olution with syntactic features. In Proc. HLT/EMNLP 05.
McCarthy, J. and Lehnert, W. (1995). Using decision trees for
coreference resolution. In Proc. IJCAI 1995.
Mitkov, R. (2002). Anaphora resolution. Longman.
Moschitti, A. (2006). Efficient convolution kernels for depen-
dency and constituent syntactic trees. Proc. ECML 2006.
Moschitti, A. and Bejan, C. A. (2004). A semantic kernel for
predicate argument classification. In CoNLL-2004, USA.
Moschitti, A., Pighin, D., and Basili, R. (2006). Semantic
Role Labeling via Tree Kernel Joint Inference. In Pro-
ceedings of CoNLL-X.
Moschitti, A., Quarteroni, S., Basili, R., and Manandhar, S.
(2007). Exploiting syntactic and shallow semantic kernels
for question answer classification. In Proceedings ACL,
Prague, Czech Republic.
Ng, V. and Cardie, C. (2002). Improving machine learning
approaches to coreference resolution. In Proc. ACL 2002.
Paice, C. D. and Husk, G. D. (1987). Towards an automatic
recognition of anaphoric features in english text: The im-
personal pronoun ?it?. Computer Speech and Language,
2:109?132.
Shawe-Taylor, J. and Cristianini, N. (2004). Kernel Methods
for Pattern Analysis. Cambridge University Press.
Sidner, C. (1979). Toward a computational theory of definite
anaphora comprehension in english. Technical report AI-
TR-537, MIT, Cambridge, MA.
Soon, W., Ng, H., and Lim, D. (2001). A machine learning
approach to coreference resolution of noun phrases. CL,
27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K.
(2007). Two uses of anaphora resolution in summarization.
Information Processing and Management, 43:1663?1680.
Special issue on Summarization.
Sturt, P. (2003). The time-course of the application of binding
constraints in reference resolution. Journal of Memory and
Language.
Vapnik, V. (1995). The Nature of Statistical Learning Theory.
Springer.
Vieira, R. and Poesio, M. (2000). An empirically based sys-
tem for processing definite descriptions. CL, 27(4):539?
592.
Yang, X., Su, J., and Tan, C. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
COLING-ACL 06.
Yang, X., Su, J., Zhou, G., and Tan, C. (2004). Improving pro-
noun resolution by incorporating coreferential information
of candidates. In Proc. ACL 2004.
Zanzotto, F. M. and Moschitti, A. (2006). Automatic learn-
ing of textual entailments with cross-pair similarities. In
Proceedings of Coling-ACL, Sydney, Australia.
Zelenko, D., Aone, C., and Richardella, A. (2003). Kernel
methods for relation extraction. JMLR, 3(6):1083 ? 1106.
968
A Twin-Candidate Model for Learning-Based
Anaphora Resolution
Xiaofeng Yang?
Institute for Infocomm Research
Jian Su??
Institute for Infocomm Research
Chew Lim Tan?
School of Computing,
National University of Singapore
The traditional single-candidate learning model for anaphora resolution considers the antecedent
candidates of an anaphor in isolation, and thus cannot effectively capture the preference relation-
ships between competing candidates for its learning and resolution. To deal with this problem,
we propose a twin-candidate model for anaphora resolution. The main idea behind the model
is to recast anaphora resolution as a preference classification problem. Specifically, the model
learns a classifier that determines the preference between competing candidates, and, during
resolution, chooses the antecedent of a given anaphor based on the ranking of the candidates. We
present in detail the framework of the twin-candidate model for anaphora resolution. Further,
we explore how to deploy the model in the more complicated coreference resolution task. We
evaluate the twin-candidate model in different domains using the Automatic Content Extraction
data sets. The experimental results indicate that our twin-candidate model is superior to the
single-candidate model for the task of pronominal anaphora resolution. For the task of coreference
resolution, it also performs equally well, or better.
1. Introduction
Anaphora is reference to an entity that has been previously introduced into the dis-
course (Jurafsky and Martin 2000). The referring expression used is called the anaphor
and the expression being referred to is its antecedent. The anaphor is usually used
to refer to the same entity as the antecedent; hence, they are coreferential with each
other. The process of determining the antecedent of an anaphor is called anaphora
resolution. As a key problem in discourse and language understanding, anaphora
resolution is crucial inmany natural language applications, such asmachine translation,
text summarization, question answering, information extraction, and so on. In recent
? 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: xiaofengy@i2r.a-star.edu.sg.
?? 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: sujian@i2r.a-star.edu.sg.
? 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg.
Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication:
5 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
years, supervised learning approaches have been widely applied to anaphora resolu-
tion, and they have achieved considerable success (Aone and Bennett 1995; McCarthy
and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak
1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube andMueller 2003; Luo et al
2004; Ng et al 2005).
The strength of learning-based anaphora resolution is that resolution regularities
can be automatically learned from annotated data. Traditionally, learning-based ap-
proaches to anaphora resolution adopt the single-candidate model, in which the po-
tential antecedents (i.e., antecedent candidates) are considered in isolation for both
learning and resolution. In such a model, the purpose of classification is to determine if
a candidate is the antecedent of a given anaphor. A training or testing instance is formed
by an anaphor and each of its candidates, with features describing the properties of the
anaphor and the individual candidate. During resolution, the antecedent of an anaphor
is selected based on the classification results for each candidate.
One assumption behind the single-candidate model is that whether a candidate
is the antecedent of an anaphor is completely independent of the other competing
candidates. However, anaphora resolution can be more accurately represented as a
ranking problem in which candidates are ordered based on their preference and the best
one is the antecedent of the anaphor (Jurafsky and Martin 2000). The single-candidate
model, which only considers the candidates of an anaphor in isolation, is incapable
of effectively capturing the preference relationship between candidates for its training.
Consequently, the learned classifier cannot produce reliable results for preference deter-
mination during resolution.
To deal with this problem, we propose a twin-candidate learning model for
anaphora resolution. Themain idea behind themodel is to recast anaphora resolution as
a preference classification problem. The purpose of the classification is to determine the
preference between two competing candidates for the antecedent of a given anaphor. In
the model, an instance is formed by an anaphor and two of its antecedent candidates,
with features used to describe their properties and relationships. The antecedent is
selected based on the judged preference among the candidates.
In the article we focus on two issues about the twin-candidate model. In the first
part, we will introduce the framework of the twin-candidate model for anaphora reso-
lution, including detailed training procedures and resolution schemes. In the second
part, we will further explore how to deploy the twin-candidate model in the more
complicated task of coreference resolution. We will present an empirical evaluation of
the twin-candidate model in different domains, using the Automatic Content Extraction
(ACE) data sets. The experimental results indicate that the twin-candidate model is
superior to the single-candidate model for the task of pronominal anaphora resolution.
For the coreference resolution task, it also performs equally well, or better.
2. Related Work
To our knowledge, the first work on the twin-candidate model for anaphora resolution
was proposed by Connolly, Burger, and Day (1997). Their work relied on a set of features
that included lexical type, grammatical role, recency, and number/gender/semantic
agreement, and employed a simple linear search scheme to choose the most preferred
candidate. Their system produced a relatively low accuracy rate for pronoun reso-
lution (55.3%) and definite NP resolution (37.4%) on a set of selected news articles.
Iida et al (2003) used the twin-candidate model (called the tournament model in their
work) to perform Japanese zero-anaphora resolution. They utilized the same linear
328
Yang, Su, and Tan A Twin-Candidate Model for AR
scheme to search for antecedents. Compared with Connolly, Burger, and Day (1997),
they adopted richer features in which centering information was incorporated to cap-
ture contextual knowledge. Their system achieved an accuracy of around 70% on a
data set drawn from a corpus of newspaper articles. Both of these studies were carried
out on uncommon data sets, which makes it difficult to compare their results with
other baseline systems. In contrast to the previous work, we will explore the twin-
candidate model comprehensively by describing the model in more detail, trying more
effective resolution schemes, deploying the model in the more complicated coreference
resolution task, performing more extensive experiments, and evaluating the model in
more depth.
Denis and Baldridge (2007) proposed a pronoun resolution system that directly
used a ranking learning algorithm (based on Maximal Entropy) to train a preference
classifier for antecedent selection. They reported an accuracy of around 72?76% for
the different domains in the ACE data set. In our study, we will also investigate the
solution of using a general ranking learner (e.g., Ranking-SVM). By comparison, the
twin-candidate model is applicable to any discriminative learning algorithm, no matter
whether it is capable of ranking learning or not. Moreover, as the model is trained and
tested on pairwise candidates, it can effectively capture various relationships between
candidates for better preference learning and determination.
Ng (2005) presented a ranking model for coreference resolution. The model focused
on the preference between the potential partitions of NPs, instead of the potential
antecedents of an NP as in our work. Given an input document, the model first em-
ployed n pre-selected coreference resolution systems to generate n candidate partitions
of NPs. The model learned a preference classifier (trained using Ranking-SVM) that
could distinguish good and bad partitions during testing. The best rank partition would
be selected as the resolution output of the current text. The author evaluated the model
on the ACE data set and reported an F-measure of 55?69% for the different domains.
Although ranking-based, Ng?s model is quite different from ours as it operates at the
cluster-level whereas ours operates at the mention-level. In fact, the result of our twin-
candidate system can be used as an input to his model.
3. The Twin-Candidate Model for Anaphora Resolution
3.1 The Single-Candidate Model
Learning-based anaphora resolution uses a machine learning method to obtain p(ante
(Ck)|ana,C1,C2, . . . ,Cn), the probability that a candidate Ck is the antecedent of the
anaphor ana in the context of its antecedent candidates, C1,C2, . . . ,Cn. The single-
candidate model assumes that the probability that Ck is the antecedent is only de-
pendent on the anaphor ana and Ck, and independent of all the other candidates.
That is:
p (ante(Ck) | ana,C1,C2, . . . ,Cn) = p (ante(Ck) | ana,Ck) (1)
Thus, the probability of a candidate Ck being the antecedent can be approximated using
the classification result on the instance describing the anaphor and Ck alone.
The single-candidate model is widely used in most anaphora resolution sys-
tems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube and
Mueller 2003; Kehler et al 2004; Ng et al 2005). In our study, we also build as the
329
Computational Linguistics Volume 34, Number 3
Table 1
A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]
proposed to [3 legislators] in [4 September]. If [5 the government] can
stick with [6 them], [7 it] will be able to halve this year?s 120 billion
ruble (US $193 billion) deficit.
Table 2
Training instances generated under the single-candidate model for anaphora resolution.
Anaphor Training Instance Label
i{[6 them] , [1 Those figures]} 1
[6 them] i{[6 them] , [2 the government]} 0
i{[6 them] , [3 legislators]} 0
i{[6 them] , [4 September]} 0
i{[6 them] , [5 the government]} 0
i{[7 it] , [1 Those figures]} 0
i{[7 it] , [3 legislators]} 0
[7 it] i{[7 it] , [4 September]} 0
i{[7 it] , [5 the government]} 1
i{[7 it] , [6 them]} 0
baseline a system for pronominal anaphora resolution based on the single-candidate
model.
In the single-candidate model, an instance has the form of i{ana, candi}, where ana
is an anaphor and candi is an antecedent candidate.1 For training, instances are created
for each anaphor occurring in an annotated text. Specifically, given an anaphor ana and
its antecedent candidates, a set of negative instances (labeled ?0?) is formed by pairing
ana and each of the candidates that is not coreferential with ana. In addition, a single
positive instance (labeled ?1?) is formed by pairing ana and the closest antecedent, that
is, the closest candidate that is coreferential with ana.2 Note that it is possible that an
anaphor has two or more antecedents, but we only create one positive instance for the
closest antecedent as its reference relationship with the anaphor is usually the most
direct and thus the most confident.
As an example, consider the text in Table 1.
Here, [6 them] and [7 it] are two anaphors. [1 Those figures] and [5 the government] are
their closest antecedents, respectively. Supposing that the antecedent candidates of the
two anaphors are just all their preceding NPs in the current text, the training instances
to be created for the text segment are listed in Table 2.
1 In our study, we only consider anaphors whose antecedents are noun phrases. Typically, all the NPs
preceding an anaphor can be taken as the initial antecedent candidates. For better learning and
resolution, however, candidates can be filtered so that only those ?confident? NPs, which occur in the
specified search scope and meet constraints such as number/gender agreement, are considered. The
details of candidate selection in our system will be discussed later in the section on experiments.
2 We assume that at least one antecedent exists in the candidate set of an anaphor. However, for real
resolution, if none of the antecedents of an anaphor occur in the candidate set, we simply discard the
anaphor and do not create any training instance for it.
330
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 3
Feature set for pronominal anaphora resolution.
ana Reflexive whether the anaphor is a reflexive pronoun
ana PronType type of the anaphor if it is a pronoun (he, she, it or they?)
candi Def whether the candidate is a definite description
candi Indef whether the candidate is an indefinite NP
candi Name whether the candidate is a named entity
candi Pron whether the candidate is a pronoun
candi FirstNP whether the candidate is the first mentioned NP in the sentence
candi Subject whether the candidate is the subject of a sentence, the subject of a clause, or not.
candi Oject whether the candidate is the object of a verb, the object of a preposition,
or not
candi ParallelStruct whether the candidate has an identical collocation pattern with the anaphor
candi SentDist the sentence distance between the candidate and the anaphor
candi NearestNP whether the candidate is the candidate closest to the anaphor in position
Note that for [7 it], we do not use [2 the government] to create a positive training
instance as it is not the closest candidate that is coreferential with the anaphor.
A vector of features is specified for each training instance. The featuresmay describe
the characteristics of the anaphor and the candidate, as well as their relationships from
lexical, syntactic, semantic, and positional aspects. Table 3 lists the features used in our
study. All these features can be computed with high reliability, and have been proven
effective for pronoun resolution in previous work.
Based on the generated feature vectors, a classifier is trained using a certain learning
algorithm. During resolution, given a newly encountered anaphor, a test instance is
formed for each of the antecedent candidates. The instance is passed to the classifier,
which then returns a confidence value indicating the likelihood that the candidate is the
antecedent of the anaphor. The candidate with the highest confidence is selected as the
antecedent. For example, suppose [7 it] is an anaphor to be resolved. Six test instances
will be created for its six antecedent candidates, as listed in Table 4. The learned classifier
is supposed to give the highest confidence to i{[7 it] , [5 the government]}, indicating the
candidate [5 the government] is the antecedent of [7 it].
3.2 A Problem with the Single-Candidate Model
As described, the assumption behind the single-candidate model is that the probability
of a candidate being the antecedent of a given anaphor is completely independent of
Table 4
Test instances generated under the single-candidate model for anaphora resolution.
Anaphor Test Instance
i{[7 it] , [1 Those figures]}
i{[7 it] , [2 the government]}
i{[7 it] , [3 legislators]}
[7 it] i{[7 it] , [4 September]}
i{[7 it] , [5 the government]}
i{[7 it] , [6 them]}
331
Computational Linguistics Volume 34, Number 3
the other competing candidates. However, for an anaphor, the determination of the
antecedent is often subject to preference among the candidates (Jurafsky and Martin
2000). Whether a candidate is the antecedent depends on whether it is the ?best? among
the candidate set, that is, whether there exists no other candidate that is preferred over
it. Hence, simply considering one candidate individually is an indirect and unreliable
way to select the correct antecedent.
The idea of preference is common in linguistic theories on anaphora. Garnham
(2001) summarizes different factors that influence the interpretation of anaphoric
expressions. Some factors such as morphology (gender, number, animacy, and case)
or syntax (e.g., the role of binding and commanding relations [Chomsky 1981]) are
?eliminating,? forbidding certain NPs from being antecedents. However, many others
are ?preferential,? giving more preference to certain candidates over others; examples
include:
 Sentence-based factors: Pronouns in one clause prefer to refer to the
NP that is the subject of the previous clause (Crawley, Stevenson, and
Kleinman 1990). Also, the NP that is the first-mentioned expression is
preferred regardless of the syntactic and semantic role played by the
referring expression (Gernsbacher and Hargreaves 1988).
 Stylistic factors: Pronouns preferentially take parallel antecedents that play
the same role as the anaphor in their respective clauses (Grober, Beardsley,
and Caramazza 1978; Stevenson, Nelson, and Stenning 1995).
 Discourse-based factors: Items currently in focus are the prime candidates
for providing antecedents for anaphoric expressions. According to
centering theory (Grosz, Joshi, and Weinstein 1995), each utterance has a
set of forward-looking centers that have higher preference to be referred to
in later utterances. The forward-looking centers can be ranked based
on grammatical roles or other factors.
 Distance-based factors: Pronouns prefer candidates in the previous
sentence compared with those two or more sentences back (Clark
and Sengul 1979).
As a matter of fact, ?eliminating? factors could also be considered ?preferential? if
we think of the act of eliminating candidates as giving them low preference.
Preference-based strategies are also widely seen in earlier manual approaches to
pronominal anaphora resolution. For example, the SHRDLU system byWinograd (1972)
prefers antecedent candidates in the subject position over those in the object position.
The system by Wilks (1973) prefers candidates that satisfy selectional restrictions with
the anaphor. Hobbs?s algorithm (Hobbs 1978) prefers candidates that are closer to the
anaphor in the syntax tree, and the RAP algorithm (Lappin and Leass 1994) prefers
candidates that have a high salience value computed by aggregating the weights of
different factors.
During resolution, the single-candidate model does select an antecedent based on
preference by using classification confidence for candidates; that is, the higher con-
fidence value the classifier returns, the more likely the candidate is preferred as the
antecedent. Nevertheless, as the model considers only one candidate at a time during
training, it cannot effectively capture the preference between candidates for classifier
learning. For example, consider an anaphor and a candidate Ci. If there are no ?better?
332
Yang, Su, and Tan A Twin-Candidate Model for AR
candidates in the candidate set, Ci is the antecedent and forms a positive instance.
Otherwise, Ci is not selected as the antecedent and thus forms a negative instance.
Simply looking at a candidate alone cannot explain this, and may possibly result in
inconsistent training instances (i.e., the same feature vector but different class labels).
Consequently, the confidence values returned by the learned classifier cannot reliably
reflect the preference relationship between candidates.
3.3 The Twin-Candidate Model
To address the problem with the single-candidate model, we propose a twin-candidate
model to handle anaphora resolution. As opposed to the single-candidate model, the
model explicitly learns a preference classifier to determine the preference relationship
between candidates. Formally, the model considers the probability that a candidate
is the antecedent as the probability that the candidate is preferred over all the other
competing candidates. That is:
p (ante(Ck) | ana,C1,C2, . . . ,Cn)
= p (Ck  {C1, . . . ,Ck?1,Ck+1, . . .Cn} | ana,C1,C2, . . . ,Cn) (2)
= p(Ck  C1, . . . ,Ck  Ck?1,Ck  Ck+1, . . . ,Ck  Cn | ana,C1,C2, . . . ,Cn)
Assuming that the preference between Ck and Ci is independent of the preference
between Ck and the candidates other than Ci, we have:
p(Ck  C1, . . . ,Ck  Ck?1,Ck  Ck+1, . . . ,Ck  Cn | ana,C1,C2, . . . ,Cn)
=
?
1<i<n,i =k
p(Ck  Ci | ana,Ck,Ci) (3)
Thus:
ln p (ante(Ck) | ana,C1,C2, . . . ,Cn)
=
?
1<i<n,i =k
ln p(Ck  Ci | ana,Ck,Ci) (4)
This suggests that the probability that a candidate Ck is the antecedent can be esti-
mated using the classification results on the set of instances describing Ck and each of
the other competing candidates. To do this, we learn a classifier that, given any two can-
didates of a given anaphor, can determine which one is preferred to be the antecedent
of the anaphor. The final antecedent is identified based on the classified preference
relationships among the candidates. This is the main idea of the twin-candidate model.
In such a model, each instance consists of three elements: i{ana, Ci, Cj}, where ana
is an anaphor, and Ci and Cj are two of its antecedent candidates. The class label of
an instance represents the preference between the two candidates for the antecedent,
for example, ?01? indicating Cj is preferred over Ci and ?10? indicating Ci is preferred.
Being trained with instances built based on this principle, the classifier is capable of de-
termining the preference between any two candidates of a given anaphor by returning
333
Computational Linguistics Volume 34, Number 3
Table 5
A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]
proposed to [3 legislators] in [4 September]. If [5 the government]
can stick with [6 them], [7 it] will be able to halve this year?s
120 billion ruble (US $193 billion) deficit.
a class label, either ?01? or ?10?, accordingly. In the next section, we will introduce in
detail a system based on the twin-candidate model for anaphora resolution.
3.4 Framework of the Twin-Candidate Model
3.4.1 Instance Representation. In the twin-candidate model, an instance takes the form
i{ana,Ci,Cj}, where ana is an anaphor andCi andCj are two of its antecedent candidates.
We stipulate that Cj should be closer to ana than Ci in position (i.e., i < j). An instance is
labeled ?10? if Ci is preferred over Cj as the antecedent, or ?01? if otherwise.
A feature vector is associated with an instance, and it describes different properties
and relationships between ana and each of the candidates, Ci or Cj. In our study, the
system with the twin-candidate model adopts the same feature set as the baseline
system with the single-candidate model (shown in Table 3). The difference is that a
feature for the single candidate, candi X, has to be replaced by a pair of features for
the twin candidates, candi1 X and candi2 X. For example, feature candi Pron, which
describes whether a candidate is a pronoun, will be replaced by two features candi1 Pron
and candi2 Pron, which describe whether Ci and Cj are pronouns, respectively.
3.4.2 Training Instances Creation. To learn a preference classifier, a training instance for an
anaphor should be composed of two candidates with an explicit preference relationship,
for example, one being an antecedent and the other being a non-antecedent. A pair
of candidates that are both antecedents or both non-antecedents are not suitable for
instance creation because their preference cannot be explicitly represented for training,
although it does exist.
Based on this idea, during training, for an encountered anaphor ana, we take the
closest antecedent, Cante, as the anchor candidate.
3 Cante is paired with each of the
candidates Cnc that is not coreferential with ana. If Cante is closer to ana than Cnc, an
instance i{ana, Cnc, Cante} is created and labeled ?01?. Otherwise, if Cnc is closer, an
instance i{ana, Cante, Cnc} is created and labeled ?10? instead.
Consider again the sample text given in Table 1, which is repeated in Table 5. For the
anaphor [7 it], the closest antecedent, [5 the government] (denoted as NP5), is chosen as
the anchor candidate. It is paired with the four non-coreferential candidates (i.e., NP1,
NP3,NP4, andNP6) to create four training instances. Among them, the instances formed
withNP1,NP3 orNP4 are labeled ?01? and the one withNP6 is labeled ?10?. Table 6 lists
all the training instances to be generated for the text.
3.4.3 Classifier Generation. Based on the feature vectors for the generated training in-
stances, a classifier can be trained using a discriminative learning algorithm. Given a
test instance i{ana, Ci, Cj} (i < j), the classifier is supposed to return a class label of ?10?,
3 If no antecedent is found in the candidate set, we do not generate any training instance for the anaphor.
334
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 6
Training instances generated under the twin-candidate model for anaphora resolution.
Anaphor Training Instance Label
i{[6 them], [1 Those figures], [2 the government]} 10
[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10
i{[6 them], [1 Those figures], [4 September]} 10
i{[6 them], [1 Those figures], [5 the government]} 10
i{[7 it], [1 Those figures], [5 the government]} 01
i{[7 it], [3 legislators], [5 the government]} 01
[7 it] i{[7 it], [4 September], [5 the government]} 01
i{[7 it], [5 the government], [6 them]} 10
indicating that Ci is preferred over Cj for the antecedent of ana, or ?01?, indicating that
Cj is preferred.
3.4.4 Antecedent Identification. After training, the preference classifier can be used to
resolve anaphors. The process of determining the antecedent of a given anaphor, called
antecedent identification, could be thought of as a tournament, a competition in which
many participants play against each other in individual matches. The candidates are like
players in a tournament. A series of matches between candidates is held to determine
the champion of the tournament, that is, the final antecedent of the anaphor under con-
sideration. Here, the preference classifier is like the referee who judges which candidate
wins or loses in a match.
If an anaphor has only one antecedent candidate, it is resolved to the candidate
directly. For anaphors that have more than one candidate, two possible schemes can be
employed to find the antecedent.
Tournament Elimination Tournament Elimination is a type of tournament where
the loser in a match is immediately eliminated. Such a scheme is also applicable to
antecedent identification. In the scheme, candidates are compared linearly from the
beginning to the end. Specifically, the first candidate is compared with the second one,
forming a test instance, which is then passed to the classifier to determine the prefer-
ence. The ?losing? candidate that is judged less preferred by the classifier is eliminated
and never considered. The winner, that is, the preferred candidate, is compared with
the third candidate. The process continues until all the candidates are compared, and
the candidate that wins in the last comparison is selected as the antecedent.
For demonstration, we use the text in Table 5 as a test example. Suppose we have a
?perfect? classifier that can correctly determine the preference between candidates. That
is, the candidates that are coreferential with the anaphor will be classified as preferred
over those that are not. (If the two candidates are both coreferential or both non-
coreferential with the anaphor, the one closer to the anaphor in position is preferred.)
To resolve the anaphor [7 it], the candidate NP1 is first compared with NP2. The formed
instance is classified as ?01?, indicating NP2 is preferred. Thus, NP1 is eliminated and
NP2 continues to compete with NP3 and NP4 until it fails in the comparison with NP5.
Finally, NP5 beats NP6 in the last match and is selected as the antecedent. All the test
instances to be generated in sequence for the resolution of [6 them] and [7 it] are listed in
Table 7.
The Tournament Elimination scheme has a computational complexity of O(N),
where N is the number of the candidates. Thus, it enables a relatively large number
335
Computational Linguistics Volume 34, Number 3
Table 7
Test instances generated under the twin-candidate model with the Tournament Elimination
scheme.
Anaphor Test Instance Result
i{[6 them], [1 Those figures], [2 the government]} 10
[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10
i{[6 them], [1 Those figures], [4 September]} 10
i{[6 them], [1 Those figures], [5 the government]} 10
i{[7 it ], [1 Those figures], [2 the government]} 01
i{[7 it ], [2 the government], [3 legislators]} 10
[7 it ] i{[7 it ], [2 the government], [4 September]} 10
i{[7 it ], [2 the government], [5 the government]} 01
i{[7 it ], [5 the government], [6 them]} 10
of candidates to be processed. However, as our twin-candidate model imposes no
constraints that enforce transitivity of the preference relation, the preference classifier
would likely output C1  C2, C2  C3, and C3  C1. Hence, it is unreliable to eliminate
a candidate once it happens to lose in one comparison, without considering all of its
winning/losing results against the other candidates.
Round Robin In Section 3.3, we have shown that the probability that a candidate is
the antecedent can be calculated using the preference classification results between the
candidate and its opponents. The candidate with the highest preference is selected as
the antecedent, that is:
Antecedent(ana) = argimax p (ante(Ci) | ana,C1,C2, . . . ,Cn)
? argimax
?
j =i
CF(i{ana,Ci,Cj},Ci) (5)
where CF(i{ana, Ci, Cj}, Ci) is the confidence with which the classifier determines Ci to
be preferred over Cj as the antecedent of ana. If we define the score of Ci as:
Score(Ci) =
?
j =i
CF(i{ana,Ci,Cj},Ci) (6)
Then, the most preferred candidate is the candidate that has the maximum score. If we
simply use 1 to denote the result that Ci is classified as preferred over Cj, and ?1 if Cj is
preferred otherwise, then:
Score(Ci) = |{Cj|Ci  Cj}| ? |{Cj|Cj  Ci}| (7)
That is, the score of a candidate is the number of the opponents to which it is preferred,
less the number of the opponents to which it is less preferred. To obtain the scores, the
antecedent candidates are comparedwith each other. For each candidate, its comparison
336
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 8
Test instances generated under the twin-candidate model with the Round Robin scheme.
Anaphor Test Instance Result
i{[7 it], [1 Those figures], [2 the government]} 01
i{[7 it], [1 Those figures], [3 legislators]} 01
i{[7 it], [1 Those figures], [4 September]} 01
i{[7 it], [1 Those figures], [5 the government]} 01
i{[7 it], [1 Those figures], [6 them]} 01
i{[7 it], [2 the government], [3 legislators]} 10
i{[7 it], [2 the government], [4 September]} 10
[7 it] i{[7 it], [2 the government], [5 the government]} 01
i{[7 it], [2 the government], [6 them]} 10
i{[7 it], [3 legislators], [4 September]} 01
i{[7 it], [3 legislators], [5 the government]} 01
i{[7 it], [3 legislators], [6 them]} 01
i{[7 it], [4 September], [5 the government]} 01
i{[7 it], [4 September], [6 them]} 01
i{[7 it], [5 the government], [6 them]} 10
result against every other candidate is recorded. Its score increases by one if it wins a
match, or decreases by one if it loses. The candidate with the highest score is selected as
the antecedent.
Antecedent identification carried out in such a way corresponds to a type of tourna-
ment called Round Robin in which each participant plays every other participant once,
and the final champion is selected based on the winning?losing records of the players.
In contrast to the Elimination scheme, the Round Robin scheme is more reliable in
that the preference of a candidate is determined by overall comparisons with the other
competing candidates. The computational complexity of the scheme is O(N2), where N
is the number of the candidates.
To illustrate this, consider the example in Table 5 again. The test instances to be
generated for resolving the anaphor [7 it] are listed in Table 8. As shown, each of
the candidates is compared with every other competing candidate. The scores of the
candidates are summarized in Table 9. Here, the candidate NP5 beats all the opponents
in the comparisons and obtains the maximum score of five. Thus it will be selected as
the antecedent.
An extension of the above Round Robin scheme is called the Weighted Round
Robin scheme. In the weighted version, the confidence values returned by the classifier,
Table 9
Scores for the candidates under the Round Robin scheme.
NP1 NP2 NP3 NP4 NP5 NP6 Score
NP1 ?1 ?1 ?1 ?1 ?1 ?5
NP2 +1 +1 +1 ?1 +1 +3
NP3 +1 ?1 ?1 ?1 ?1 ?3
NP4 +1 ?1 +1 ?1 ?1 ?1
NP5 +1 +1 +1 +1 +1 +5
NP6 +1 ?1 +1 +1 ?1 +1
337
Computational Linguistics Volume 34, Number 3
Table 10
Statistics for the training and testing data sets.
NWire NPaper BNews
# Tokens 85k 72k 67kTrain
# Files 130 76 216
# Tokens 20k 18k 18kTest
# Files 29 17 51
instead of the simple 0 and 1, are employed to calculate the score of a candidate based
on the formula
Score(Ci) =
?
CiCj
CF(Ci  Cj)?
?
CkCi
CF(Ck  Ci) (8)
Here, CF is the confidence value that the classifier returns for the corresponding
instance.
3.5 Evaluation
3.5.1 Experimental Setup.We used the ACE (Automatic Content Extraction)4 coreference
data set for evaluation. All the experiments were done on the ACE-2 V1.0 corpus. It
contains two data sets, training and devtest, which were used for training and testing,
respectively. Each of these sets is further divided into three domains: newswire (NWire),
newspaper (NPaper), and broadcast news (BNews). Statistics for the data sets are sum-
marized in Table 10.
For both training and resolution, a raw input document was processed by a
pipeline of NLP modules including a Tokenizer, Part-of-Speech tagger, NP chunker,
Named-Entity (NE) Recognizer, and so on. These preprocessing modules were meant to
determine the boundary of each NP in a text, and to provide the necessary information
about an NP for subsequent processing. Trained and tested on the UPENWSJ TreeBank,
the POS tagger (Zhou and Su 2000) could obtain an accuracy of 97% and the NP
chunker (Zhou and Su 2000) could produce an F-measure above 94%. Evaluated for
the MUC-6 and MUC-7 Named-Entity task, the NER module (Zhou and Su 2002) could
provide an F-measure of 96.6% (MUC-6) and 94.1% (MUC-7).
In our experiments, we focused on the resolution of the third-personal pronominal
anaphors, including she, he, it, they as well as their morphologic variants (such as her, his,
him, its, itself, them, etc.). For both training and testing, we considered all the pronouns
that had at least one preceding NP in their respective annotated coreferential chains. We
used the accuracy rate as the evaluation metric, and defined it as follows:
Accuracy =
number of anaphors being correctly resolved
total number of anaphors to be resolved
(9)
Here, an anaphor is deemed ?correctly resolved? if the found antecedent is in the co-
referential chain of the anaphor.
4 See http://www.itl.nist.gov/iad/894.01/tests/ace for a detailed description of the ACE program.
338
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 11
Statistics of the training instances generated for the pronominal anaphora resolution task.
NWire NPaper BNews
0 instances 8,200 11,648 6,037
Single-Candidate 1 instances 1,241 1,466 1,291
01 instances 6,899 9,861 5,004
Twin-Candidate 10 instances 1,301 1,787 1,033
For pronoun resolution, the distance between the closest antecedent and the
anaphor is usually short, predominantly (98% for the current data set) limited to only
one or two sentences (McEnery, Tanaka, and Botley 1997). For this reason, given an
anaphor, we only took the NPs occurring within the current and previous two sentences
as initial antecedent candidates. The candidates with mismatched number and gender
agreement were filtered automatically from the candidate set. Also, pronouns or NEs
that disagreed in person with the anaphor were removed in advance. For training, there
were 1,241 (NWire), 1,466 (NPaper), and 1,291 (BNews) anaphors found with at least
one antecedent in the candidate set. For testing, the numbers were 313 (NWire), 399
(NPaper), and 271 (BNews). On average, an anaphor had nine antecedent candidates.
Table 11 summarizes the statistics of the training instances as well as the class
distribution. Note that for the single-candidate model, the number of ?1? instances
was identical to the number of anaphors in the training data, because we only used
the closest antecedents of anaphors to create the positive instances. The number of ?0?
instances was equal to the total number of ?01? and ?10? training instances for the twin-
candidate model.
We examined three different learning algorithms: C5 (Quinlan 1993), Maximum
Entropy (Berger, Della Pietra, and Della Pietra 1996), and SVM (linear kernel) (Vapnik
1995),5 using the software See5,6 OpenNlp.MaxEnt,7 and SVM-light,8 respectively. All
the classifiers were learned with the default learning parameters set in the respective
learning software.
3.5.2 Results and Discussions. Table 12 lists the performance of the different anaphora
resolution systems with the single-candidate (SC) and the twin-candidate (TC) models.
For the TC model, two antecedent identification schemes, Tournament Elimination and
Round Robin, were compared.
From the table, we can see that our baseline systemwith the single-candidate model
can obtain accuracy of up to 72.9% (NWire), 77.1% (NPaper), and 74.9% (BNews).
5 As MaxEnt learns a probability model, we used the returned probability as the confidence of a candidate
being the antecedent. For C5, the confidence value of a candidate was estimated based on the following
smoothed ratio:
CF =
p+ 1
t+ 2
where cwas the number of positive instances and twas the total number of instances stored in the
corresponding leaf node. For SVM, the returned value was used as the confidence value: the lower
(maybe negative) the less confident.
6 http://www.rulequest.com/see5-info.html
7 http://MaxEnt.sourceforge.net/
8 http://svmlight.joachims.org/
339
Computational Linguistics Volume 34, Number 3
Table 12
Accuracy in percent for the pronominal anaphora resolution.
NWire NPaper BNews Average
C5 SC 71.6 75.6 69.5 72.7
TC
- Elimination 71.6 81.3 74.5 76.4
- Round Robin 72.9 81.3 74.9 76.9
- Weighted Round Robin 72.9 80.5 75.6 76.7
MaxEnt SC 72.9 77.1 74.9 75.2
TC
- Elimination 75.1 79.1 77.5 77.4
- Round Robin 75.1 79.1 77.5 77.4
- Weighted Round Robin 75.7 78.6 77.1 77.3
SVM SC 72.9 77.3 74.2 75.1
TC
- Elimination 73.5 82.0 78.9 78.5
- Round Robin 74.4 82.0 78.9 78.7
- Weighted Round Robin 74.6 79.3 78.2 77.5
Rank SVM 73.5 79.3 76.4 76.7
The average accuracy is comparable to that reported by Kehler et al (2004) (around
75%), who also used the single-candidate model to do pronoun resolution with similar
features (using MaxEnt) on the ACE data sets. By contrast, the systems with the twin-
candidate model are able to achieve accuracy of up to 75.7% (NWire), 82.0% (NPaper),
and 78.9% (BNews). The average accuracy is 76.9% for C5, 77.4% for MaxEnt, and 78.7%
for SVM,which is statistically significantly9 better than the results of the baselines (4.2%,
2.2%, and 3.6% in accuracy). These results confirm our claim that the twin-candidate
model is more effective than the single-candidate model for the task of pronominal
anaphora resolution.
We see no significant difference between the accuracy rates (less than 1.0% accuracy)
produced by the two antecedent identification schemes, Tournament Elimination and
Round Robin. This is in contrast to our belief that the Round Robin scheme, which is
more reliable than the Tournament Elimination, should lead to much better results. One
possible reason could be that the classifier in our systems can make a correct preference
judgement (with accuracy above 92% as in our test) in the cases where one candidate is
the antecedent and the other is not. As a consequence, the simple linear search can find
the final antecedent as well as the Round Robin method. These results suggest that we
can use the Elimination scheme in a practical system to make antecedent identification
more efficient. (Recall that the Elimination scheme requires complexity ofO(N), instead
of O(N2) as in Round Robin.)
Ranking-SVM In our experiments, we were particularly interested in comparing
the results using the twin-candidate model and those directly using a preference learn-
ing algorithm. For this purpose, we built a system based on Ranking-SVM (Joachims
2002), an extension of SVM capable of preference learning.
9 Throughout our experiments, the significance was examined by using the paired t-test, with p < 0.05.
340
Yang, Su, and Tan A Twin-Candidate Model for AR
The system uses a similar framework to the single-candidate-based system. For
training, given an anaphor, a set of instances is created for each of the antecedent candi-
dates. To learn the preference between competing candidates, a ?query-ID? is specified
for each training instance in such a way that the instances formed by the candidates of
the same anaphor bear the same query-ID. The label of an instance represents the rank of
the candidate in the candidate set; here, ?1? for the instances formed by the candidates
that are the antecedents, and ?0? for the instances formed by the others. The training
instances are associated with features as defined in Table 3, to which the Ranking-
SVM algorithm is then applied to generate a preference classifier. During resolution, for
each candidate of a given anaphor, a test instance is formed and passed to the learned
classifier, which in turn returns a value to represent the rank of the candidate among all
the candidates. The anaphor is resolved to the one with the highest value.
In fact, if we look into the learning mechanism of Ranking-SVM, we can find
that the algorithm will, in the background, pair any two instances that have the same
query-ID but different rank labels. This is quite similar to the twin-candidate model,
which creates an instance by putting together two candidates with different preferences.
However, one advantage of the twin-candidate model is that it can explicitly record
various relationships between two competing candidates, for example, ?which one
of the two candidates is closer to the anaphor in position/syntax/semantics??10 Such
inter-candidate information can make the preference between candidates clearer, and
thus facilitate both preference learning and determination. In contrast, Ranking-SVM,
which constructs instances in the single-candidate form, cannot effectively capture this
kind of information.
The last line of Table 12 shows the results from such a system based on Ranking-
SVM. We can see that the system achieves an average accuracy of 76.7%, statistically
significantly better than the baseline system with the single-candidate model by 1.6%
(0.4% for NWire, 2.0% for NPaper, and 2.2% for BNews). The results lend support to our
claim that the preference relationships between candidates, if taken into consideration
for classifier training, can lead to better resolution performance. Still, we observe that
our twin-candidate model beats Ranking-SVM in average accuracy by 1.8% (Elimina-
tion scheme) and 2.0% (Round Robin).
Decision Tree One advantage of the C5 learning algorithm is that the generated
classifier can be easily interpreted by humans, and the importance of the features
can be visually illustrated. In Figures 1 and 2, we show the decision trees (top four
levels) output by C5 for the NWire domain, based on the single-candidate and the
twin-candidate models, respectively. As the twin-candidate model uses a larger pool
of features, the tree for the twin-candidate model is more complicated (180 nodes) than
the one for the single-candidate model (36 nodes).
From the two trees, we can see that bothmodels rely on similar features such as lexi-
cal, positional, and grammatical properties for pronoun resolution. However, we can see
that the preferential factors (e.g., subject preference, parallelism preference, and distance
preference as discussed in Section 3.2) are more clearly presented in the twin-candidate-
based tree. For example, if two candidates are both pronouns, the twin-candidate-based
tree will suggest that the one closer to the anaphor has a higher preference to be the
antecedent. By contrast, such a preference relationship has to be implicitly represented
10 In the current work, we only consider the positional relationship between candidates by stipulating
that i < j for an instance i{ana, Ci, Cj}. In our future work, we will explore more inter-candidate
relationships that are helpful for preference determination.
341
Computational Linguistics Volume 34, Number 3
Figure 1
Decision tree generated for pronoun resolution under the single-candidate model. For feature
ana Type, the values PRON SHE,PRON SHE,PRON SHE, and PRON THEY represent whether
the anaphor is a pronoun such as she, he, it, and they, respectively. For candi Subject, the values
SUBJ MAIN, SUBJ CLAUSE and NO represent whether the candidate is the subject of a main
sentence, or the subject of a clause, or not. For candi Object, the values OBJ VERB, OBJ PREP, and
NO represent whether the candidate is the object of a verb, a preposition, or not, respectively.
For other features, 0 and 1 represent yes/no.
in the single-candidate-based tree, with different confidence values being assigned to
the candidates in different sentences.
Learning Curve In our experiments, we were also concerned about how training
data size might influence anaphora resolution performance. For this purpose, we di-
vided the anaphors in the training documents into 10 batches, and then performed
resolution using the classifiers trained with 1, 2, . . . , 10 batches of anaphors. Figure 3
plots the learning curves of the systems with the single-candidate model and the twin-
candidate model (Round Robin scheme) for the NPaper domain. Each accuracy rate
shown in the figure is the average of the results from three trials trained on different
anaphors.
From the figure we can see that both the single-candidate model and the twin-
candidate model reach their peak performance with around six batches (around
880 anaphors). As shown, the twin-candidate model is not apparently superior to
the single-candidate model when the size of the training data is small (below two
batches, 290 anaphors). This is due to the fact that the number of features in the twin-
candidate model is nearly double that in the single-candidate model. As a result, the
twin-candidate model requires more training data than the single-candidate model to
avoid the data sparseness problem. Nevertheless, it does not need too much training
data to beat the latter; it can produce the accuracy rates consistently higher than the
342
Yang, Su, and Tan A Twin-Candidate Model for AR
Figure 2
Decision tree generated for pronoun resolution under the twin-candidate model.
Figure 3
Learning curves of different models for pronominal anaphora resolution in the NPaper Domain
(120 anaphors per batch).
343
Computational Linguistics Volume 34, Number 3
Table 13
A sample text for coreference resolution.
[1 Globalstar] still needs to raise [2 $600 million], and
[3 Schwartz] said [4 that company] would try to raise [5 the
money] in [6 the debt market].
single-candidate model when trained with more than two batches of anaphors. This
figure further demonstrates that the twin-candidate model is reliable and effective for
the pronominal anaphora resolution task.
4. Deploying the Twin-Candidate Model to Coreference Resolution
One task that is closely related to anaphora resolution is coreference resolution, the
process of identifying all the coreferential expressions in texts.11 Coreference resolution
is different from anaphor resolution. The latter focuses on how an anaphor can be suc-
cessfully resolved, and the resolution is done on given anaphors. The former, in contrast,
focuses on how the NPs that are coreferential with each other can be found correctly
and completely, and the resolution is done on all possible NPs. In a text, many NPs,
especially the non-pronouns, are non-anaphors that have no antecedent to be found
in the previous text. Hence, the task of coreference resolution is a more complicated
challenge than anaphora resolution, as a solution should not only be able to resolve
an anaphor to the correct antecedent, but should also refrain from resolving a non-
anaphor. In this section, we will explore how to deploy the learning models for anaphor
resolution in the coreference resolution task. As pronouns are usually anaphors, we will
focus mainly on the resolution of non-pronouns.
4.1 Coreference Resolution Based on the Single-Candidate Model
In practice, the single-candidate model can be applied to coreference resolution directly,
using the similar training and testing procedures to those used in anaphora resolution
(described in Section 2).
For training, we create ?0? and ?1? training instances for each encountered anaphor,
that is, the NP that is coreferential with at least one preceding NP. Specifically, given an
anaphor and its antecedent candidates, a positive instance is generated for the closest
antecedent and a set of negative instances is generated for each of the candidates that is
not coreferential with the anaphor.12
Consider the text in Table 13 as an example. In the text, [4 that company] and [5 the
money] are two anaphors, with [1 Globalstar] and [2 $600 million] being their antecedents,
respectively. Table 14 lists the training instances to be created for this text.
11 In our study, we only consider within-document noun phrase coreference resolution.
12 In some coreference resolution systems (Soon, Ng, and Lim 2001; Ng and Cardie 2002b), only the
non-coreferential candidates occurring between the closest antecedent and the anaphor are used to create
negative instances. In the experiments, we found that these sampling strategies for negative instances
led to a trade-off between recall and precision, but no significant difference in the overall F-measure.
344
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 14
Training instances generated under the single-candidate model for coreference resolution.
Anaphor Training Instance Label
i{[4 that company] , [1 Globalstar]} 1
[4 that company] i{[4 that company] , [2 $600 million]} 0
i{[4 that company] , [3 Schwartz]} 0
i{[5 the money] , [1 Globalstar]} 0
[5 the money] i{[5 the money] , [2 $600 million]} 1
i{[5 the money] , [3 Schwartz]} 0
i{[5 the money] , [4 that company]} 0
Table 15
Feature set for coreference resolution.
ana Def whether the possible anaphor is a definite description
ana Indef whether the possible anaphor is an indefinite NP
ana Name whether the possible anaphor is a named entity
candi Def whether the candidate is a definite description
candi Indef whether the candidate is an indefinite description
candi Name whether the candidate is a named-entity
candi SentDist the sentence distance between the possible anaphor and the candidate
candi NameAlias whether the candidate and the candidate are aliases for each other
candi Appositive whether the possible anaphor and the candidate are in an appositive
structure
candi NumberAgree whether the possible anaphor and the candidate agree in number
candi GenderAgree whether the possible anaphor and the candidate agree in gender
candi HeadStrMatch whether the possible anaphor and the candidate have the same head
string
candi FullStrMatch whether the possible anaphor and the candidate contain the same
strings (excluding the determiners)
candi SemAgree whether the possible anaphor and the candidate belong to the same
semantic category in WordNet
In Table 15, we list the features used in our study for coreference resolution, which
are similar to those proposed in Soon, Ng, and Lim?s (2001) system.13 All these features
are domain independent and the values can be computed with low cost but high
reliability.
After training, the learned classifier can be directly used for coreference resolution.
Given an NP to be resolved, a test instance is generated for each of its antecedent
candidates. The classifier, being given the instance, will determine the likelihood that
the candidate is the antecedent of the possible anaphor. If the confidence is below
a pre-specified threshold, the candidate is discarded. In the case where none of the
candidates have a confidence higher than the threshold, the current NP is deemed a
13 As we focus on coreference resolution for non-pronouns, we do not use the feature that describes
whether or not the NP to be resolved is a pronoun. Also, we do not use the feature that describes
whether or not a candidate is a pronoun, because, as will be discussed together with the experiments,
a pronoun is not taken as an antecedent candidate for a non-pronoun to be resolved.
345
Computational Linguistics Volume 34, Number 3
non-anaphor and left unresolved. Otherwise, it is resolved to the candidate with the
highest confidence.14
4.2 Coreference Resolution Based on the Twin-Candidate Model
The twin-candidate model presented in the previous section focuses on the preference
between candidates. The model will always select a ?best? candidate as the antecedent,
even if the current NP is a non-anaphor. To deal with this problem, we will teach the
preference classifier how to identify non-anaphors, by incorporating non-anaphors to
create a special class of training instances. For resolution, if the newly learned classifier
returns the special class label, wewill know that the current NP is a non-anaphor, and no
preference relationship holds between the two candidates under consideration. In this
way, the twin-candidate model is capable of carrying out both antecedent identification
and anaphoricity determination by itself, and thus can be deployed for coreference
resolution directly. In this section, we will describe the modified training and resolution
procedures of the twin-candidate model.
4.2.1 Training. As with anaphora resolution, an instance of the twin-candidate model
for coreference resolution takes the form i{ana, Ci, Cj}, where ana is a possible anaphor,
and Cj and Cj are two of its antecedent candidates (i < j). The feature set is similar to
that for the single-candidate model as defined in Table 15, except that a candi X feature
is replaced by a pair of features, cand1 x and candi2 x, for the two competing candi-
dates, respectively.
During training, if an encountered NP is an anaphor, we create ?01? or ?10? training
instances in the same way as in the original learning framework. If the NP is a non-
anaphor, we do the following:
 From the antecedent candidates,15 randomly select one as the anchor
candidate.
 Create a set of instances by pairing the anchor candidate and each of the
other non-coreferential candidates.
The instances formed by the non-anaphors are labeled ?00.?
Consider the sample text in Table 13. For the two anaphors [4 that company] and
[5 the money], we create the ?01? and ?10? instances as usual. For the non-anaphors
[3 Schwartz] and [6 the debt market], we generate two sets of ?00? instances. Table 16 lists
all the training instances for the text (supposing [1 Globalstar] and [2 $600 million] are the
anchor candidates for [3 Schwartz] and [6 the debt market], respectively).
The ?00? training instances are used together with the ?01? and ?10? ones to train
a classifier. Given a test instance i{ana, Ci, Cj} (i < j), the newly learned classifier is
supposed to return ?01? (or ?10?), indicating ana is an anaphor and Ci (or Cj) is preferred
as its antecedent, or return ?00?, indicating ana is a non-anaphor and no preference
exists between Ci and Cj.
14 Other clustering strategies are also available, for example, ?closest-first? where a possible anaphor is
resolved to the closest candidate with the confidence above the specified threshold, if any (Soon, Ng,
and Lim 2001).
15 For a non-anaphor, we also take the preceding NPs as its antecedent candidates. We will discuss this
issue later together with the experimental setup.
346
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 16
Training instances generated under the twin-candidate model for coreference resolution.
Possible Anaphor Training Instance Label
i{[4 that company], [1 Globalstar], [2 $600 million]} 10
[4 that company] i{[4 that company], [1 Globalstar], [3 Schwartz]} 10
i{[5 the money], [1 Globalstar], [2 $600 million]} 01
[5 the money] i{[5 the money], [2 $600 million], [3 Schwartz]} 10
i{[5 the money], [2 $600 million], [4 that company]} 10
[3 Schwartz] i{[3 Schwartz], [1 Globalstar], [2 $600 million]} 00
i{[6 the debt market], [1 Globalstar], [2 $600 million]} 00
i{[6 the debt market], [2 $600 million], [3 Schwartz]} 00
[6 the debt market] i{[6 the debt market], [2 $600 million], [4 that company]} 00
i{[6 the debt market], [2 $600 million], [5 the money]} 00
4.2.2 Antecedent Identification. Accordingly, we make a modification to the original Tour-
nament Elimination and the Round Robin schemes:
Tournament Elimination Scheme As with anaphora resolution, given an NP to be
resolved, candidates are compared linearly from the beginning to the end. If an instance
for two competing candidates is classified as ?01? or ?10?, the preferred candidate will
be compared with subsequent competitors while the loser is eliminated immediately.
If the instance is classified as ?00?, both the two candidates are discarded and the
comparison restarts with the next two candidates.16 The process continues until all the
candidates have been compared. If both of the candidates in the last match are judged to
be ?00?, the current NP is left unresolved. Otherwise, the NPwill be resolved to the final
winner, on the condition that the highest confidence that the winner has ever obtained
is above a pre-specified threshold.
Round Robin Scheme In the Round Robin scheme, each candidate is compared
with every other candidate. If two candidates are labeled ?00? in a match, both candi-
dates receive a penalty of ?1 in their respective scores. If no candidate has a positive
final score, then the NP is considered non-anaphoric and left unresolved. Otherwise,
it is resolved to the candidate with the highest score as usual. Here, we can also use a
threshold. That is, we will update the scores of the two candidates in a match if and
only if the preference confidence returned by the classifier is higher than a pre-specified
threshold.
In rare cases where an NP to be resolved has only one antecedent candidate, a
pseudo-instance is created by pairing the candidate with itself. The NP will be resolved
to the candidate unless the instance is labeled ?00?.
4.3 Evaluation
4.3.1 Experimental Setup. We used the same ACE data sets for coreference resolution
evaluation, as described in the previous section for anaphora resolution. A raw input
document was processed in advance by the same pipeline of NLP modules including
16 If only one candidate remains, it will be compared with the candidate eliminated last.
347
Computational Linguistics Volume 34, Number 3
Table 17
Statistics of the training instances generated for coreference resolution (non-pronoun).
NWire NPaper BNews
0 instances 78,191 105,152 33,748Single-Candidate 1 instances 3,197 3,792 2,094
00 instances 296,000 331,957 159,752
Twin-Candidate 01 instances 50,499 70,433 21,170
10 instances 27,692 34,719 12,578
POS-tagger, NP chunker, NE recognizer, and so on, to obtain all possible NPs and
related information (see Section 3.5.1).
For evaluation, we adopted Vilain et al?s (1995) scoring algorithm in which recall
and precision17 were computed by comparing the key chains (i.e., the annotated ?stan-
dard? coreferential chains) and the response chains (i.e., the chains generated by the
coreference resolution system).
As already mentioned, the twin-candidate model described in this section is mainly
meant for non-pronouns that are often not anaphoric. To better examine the utility
of the model in our experiments, we first focused on coreference resolution for non-
pronominal NPs. The recall and precision to be reported were computed based on the
response chains and the key chains from which all the pronouns are removed. We will
later show the results of overall coreference resolution for whole NPs by combining the
resolution of pronouns and non-pronouns.
In non-pronoun resolution, an anaphor and its antecedent do not often occur a short
distance apart as they do in pronoun resolution. For this reason, during training, we
took as antecedent candidates all the preceding non-pronominal NPs18 in the current
and previous four sentences; while during testing, we used all the preceding non-
pronouns, regardless of distance, as candidates.19 The statistics of the training instances
for each data set are summarized in Table 17.
Again, we examined the three learning algorithms: C5, MaxEnt, and SVM.20 As
both the single-candidate and the twin-candidate models used a threshold to block low-
confidence coreferential pairs, we performed three-fold cross-evaluation on the training
data to determine the thresholds for the coreference resolution systems.
4.3.2 Results and Discussions. Table 18 lists the results for the different systems on the
non-pronominal NP coreference resolution. We used as the baseline the system with the
single-candidate model described in Section 4.1. As mentioned, the system was trained
17 The overall F-measure was defined as
2 ? Recall ? Precision
Recall+ Precision
18 As suggested in Ng and Cardie (2002b), we did not include pronouns in the candidate set of a
non-pronoun, because a pronoun is usually anaphoric and cannot give much information about
the entity to which it refers.
19 Unlike in the case of pronoun resolution, we did not filter candidates that had mismatched
number/gender agreement as these constraints are not reliable for non-pronoun resolution (e.g.,
in our data set, around 15% of coreferential pairs do not agree in number). Instead, we took these
factors as features (see Table 15) and let the learning algorithm make the preference decision.
20 For SVM, we employed the one-against-all aggregation method for the 3-class learning and testing.
348
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 18
Recall (R), Precision (P), and F-measure (F) in percent for coreference resolution (non-pronoun).
NWire NPaper BNews
R P F R P F R P F
C5 SC
- baseline 63.3 48.1 54.7 63.8 42.2 50.8 63.5 53.7 58.2
- with non-anaphors 40.9 81.5 54.4 39.8 81.4 53.4 35.1 76.8 48.2
TC
- Elimination 50.8 63.0 56.2 56.6 60.1 58.3 44.6 71.2 54.9
- Round Robin 58.7 57.9 58.3 56.5 60.5 58.4 49.0 70.1 57.7
MaxEnt SC
- baseline 62.1 52.3 56.8 56.4 58.8 57.6 61.8 54.1 57.7
- with non-anaphors 59.6 54.0 56.7 54.2 62.6 58.1 53.8 58.4 56.0
TC
- Elimination 59.1 55.4 57.2 52.2 69.0 59.5 53.5 61.9 57.4
- Round Robin 58.7 55.9 57.2 53.4 65.9 59.0 54.3 62.8 58.3
SVM SC
- baseline 64.1 49.0 55.5 65.5 42.1 51.3 63.5 53.7 58.2
- with non-anaphors 42.3 70.0 52.7 40.0 76.6 52.5 35.7 77.0 48.8
TC
- Elimination 57.8 53.2 55.4 51.7 56.5 54.0 63.3 53.8 58.2
- Round Robin 54.3 56.9 55.6 56.1 58.1 57.1 63.7 53.8 58.3
on the instances formed by anaphors. For better comparison with the twin-candidate
model, we built another single-candidate-based system inwhich the non-anaphors were
also incorporated for training. Specifically, for each encountered non-anaphor during
training, we created a set of ?0? instances by pairing the non-anaphor with each of the
candidates. These instances were added to the original instances formed by anaphors
to learn a classifier,21 which was then applied for the resolution as usual.
The results for the two single-candidate based systems are listed in Table 18. When
trained with the instances formed only by anaphors, the system could achieve recall
above 60% and precision of around 50% for the three domains. When trained with the
instances formed by both anaphors and non-anaphors, the system yielded a significant
improvement in precision. In the case of using C5 and SVM, the system is capable of
producing precision rates of up to 80%. The increase in precision is reasonable since the
classifier tends to be stricter in blocking non-anaphors. Unfortunately, however, at the
same time recall drops significantly, and no apparent improvement can be observed in
the resulting overall F-measure.
When trained with non-anaphors incorporated, the systems with the twin-
candidate model, described in Section 4.2, are capable of yielding higher precision
against the baseline. Although recall also drops at the same time, the increase in
precision can compensate it well: We observe that in most cases, the system with the
twin-candidate model can achieve a better F-measure than the baseline system with
the single-candidate model. Also, the improvement is statistically significant (t-test,
p < 0.05) in the NWire domain when C5 is used (3.6%), and in the NPaper domain
21 The statistics of the ?0? instances shown in Table 17 become 392,646, 455,167, and 207,667 for NWire,
NPaper, and BNews, respectively.
349
Computational Linguistics Volume 34, Number 3
when any of the three learning algorithms, C5 (5.0%), MaxEnt (1.4%), and SVM (4.6%),
is used. These results suggest that our twin-candidate model can effectively identify
non-anaphors and block their invalid resolution, without affecting the accuracy of
determining antecedents for anaphors.
Compared with the pronoun resolution described in the previous section, here
we find that for non-pronoun resolution the superiority of the twin-candidate model
against the single-candidate model is not apparent. In some domains such as BNews,
the difference between the two models is not statistically significant. One possible
explanation is that for non-pronoun resolution, the features that really matter are quite
limited, that is, NameAlias, String-Matching, and Appositive (we will later show this
in the decision trees). A candidate that has any one of these features is most likely the
antecedent, regardless of the other competing candidates. In this situation, the single-
candidate model, which considers candidates in isolation, does as well as the twin-
candidate model. Still, the results suggest that the twin-candidate model is suitable for
both resolution tasks, no matter whether the features involved are strongly indicative
(as with non-pronoun resolution) or not (as with pronoun resolution).
As with anaphora resolution, we do not observe any apparent performance differ-
ence between the two twin-candidate identification schemes, Tournament Elimination
and Round Robin. The Round Robin scheme performs better than Elimination when
trained using C5 and SVM, by up to 2.8% and 2.9% in F-measure, respectively. However,
the Elimination scheme, when trained using MaxEnt, is capable of performing equally
well or slightly better (0.5% F-measure) than the Round Robin scheme.
Recall vs. Precision As discussed, the results in Table 18 show different recall and
precision patterns for different systems. The baseline system with the single-candidate
model tends to yield higher recall while the system with the twin-candidate model
tends to produce higher precision. Thus, a fairer comparison of the two systems is to
examine the precision rates that these systems achieve under the same recall rates. For
this purpose, in Figure 4, we plot the variant recall and precision rates that the two
systems are capable of obtaining (tested using MaxEnt, Round Robin scheme, for the
NPaper domain), focusing on precision rates above 50% and recall rates above 40%.
From the figure, we find that the systemwith the twin-candidate model achieves higher
precision for recall rates ranging from 40% and 55%, and performs equally well for recall
rates above 55%, which further proves the reliability of our twin-candidate model for
coreference resolution.
Decision Trees In Figures 5 and 6, we show the two decision trees (NWire domain)
generated by the systems with the single-candidate model and the twin-candidate
model, respectively. The tree from the single-candidate model contains only 13 nodes,
considerably smaller than that from the twin-candidate model, which contains around
1.2k nodes. From the figure, we can see that both models heavily rely on string-
matching, name-alias, and appositive features to perform non-pronoun resolution, in
contrast to pronoun resolution where lexical and positional features seem more impor-
tant (as shown in Figures 1 and 2).
Learning Curves In our experiments, we were also interested in evaluating the
resolution performance of the two learning models on different quantities of training
data. Figure 7 plots the learning curves for the systems using the single-candidate model
and the system using the twin-candidate model (NPaper domain). The F-measure is
averaged over three random trials trained on 5, 10, 15, . . . documents. Consistent with
the curves for the anaphora resolution task as depicted in Figure 3, the system with
the twin-candidate model outperforms the one with the single-candidate model on a
small amount of training data (less than five documents). When more data is available,
350
Yang, Su, and Tan A Twin-Candidate Model for AR
Figure 4
Various recall (%) and precision (%) of different models for non-pronoun resolution.
Figure 5
Decision tree generated for non-pronoun resolution under the single-candidate model.
the twin-candidate model also yields a consistently better F-measure than the single-
candidate model.
Overall Coreference Resolution Having demonstrated the performance of the
twin-candidate model on coreference resolution for non-pronouns, we now further
examine overall coreference resolution for whole NPs, combining both pronoun
resolution and non-pronoun resolution. Specifically, given an input test document,
we check each encountered NP from beginning to end. If it is a pronoun,22 we use
22 We identify the pleonastic use of it in advance (79.2% accuracy) using a set of predefined pattern rules
based on regular expressions. The first-person and second-person pronouns are heuristically resolved
to the closest pronoun of the same type or a speaker nearby, if any, with an average 61.8% recall and
79.5% precision.
351
Computational Linguistics Volume 34, Number 3
Figure 6
Decision tree generated for non-pronoun resolution under the twin-candidate model.
Figure 7
Learning curves of different models for non-pronoun resolution.
the pronominal anaphora resolution systems, as described in the previous section, to
resolve it to an antecedent. Otherwise, we use the non-pronoun coreference resolution
systems described in this section to resolve the NP to an antecedent, if any is found. All
the coreferential pairs are put together in a coreferential chain. The recall and precision
rates are computed by comparing the standard key chains and generated response
chains using Vilain et al?s (1995) algorithm.
352
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 19
Recall (R), Precision (P), and F-Measure (F) in percent for coreference resolution.
NWire NPaper BNews
R P F R P F R P F
C5 SC 62.2 52.6 57.0 64.9 50.6 56.9 62.9 58.5 60.6
TC
- Elimination 53.8 65.9 59.2 61.2 64.4 62.8 53.1 70.9 60.7
- Round Robin 59.0 61.2 60.1 62.0 64.3 63.1 56.0 69.9 62.2
MaxEnt SC 60.7 56.0 58.3 60.8 62.2 61.5 63.8 60.6 62.2
TC
- Elimination 59.5 59.2 59.3 58.6 67.8 62.9 59.3 66.4 62.7
- Round Robin 60.6 57.9 59.2 59.4 69.2 63.3 61.7 64.5 63.0
SVM SC 62.3 53.3 57.5 66.2 50.5 57.3 64.7 60.1 62.3
TC
- Elimination 57.6 57.0 57.3 58.5 62.6 60.5 65.0 60.6 62.7
- Round Robin 56.0 60.6 58.2 60.4 63.6 62.0 65.4 60.7 63.0
Table 19 lists the coreference resolution results of the systemswith different learning
models. We observe that the results for overall coreference resolution are better than
those of non-pronoun coreference resolution as shown in Table 18, which is due to the
comparatively high accuracy of the resolution of pronouns.
In line with the previous results for pronoun resolution and non-pronoun resolu-
tion, the twin-candidate model outperforms the single-candidate model in coreference
resolution for whole NPs. Consider the system trained with MaxEnt as an example.
The single-candidate-based system obtains F-measures of 58.3%, 61.5%, and 62.2% for
the NWire, NPaper, and BNews domains.23 By comparison, the twin-candidate-based
system (Round Robin scheme) can achieve F-measures of 59.2%, 63.3%, and 63.0% for
the three domains. The improvement over the single-candidate model in F-measure
(0.9%, 1.8%, and 0.8%) is larger than that for non-pronoun resolution (0.4%, 1.4%,
and 0.6% as shown in Table 18), owing to the higher gains obtained from pronoun
resolution. For the systems trained using C5 and SVM, similar patterns of performance
improvement may be observed.
5. Conclusion
In this article, we have presented a twin-candidate model for learning-based anaphora
resolution. The traditional single-candidate model considers candidates in isolation,
and thus cannot accurately capture the preference relationships between competing
candidates to provide reliable resolution. To deal with this problem, our proposed twin-
candidate model recasts anaphora resolution as a preference classification problem.
It learns a classifier that can explicitly determine the preference between competing
candidates, and then during resolution, choose the antecedent of an anaphor based on
the ranking of the candidates.
23 The results are comparable to the baseline system by Ng (2005), which also uses the single-candidate
model and is capable of F-measures of 50.1%, 62.1%, and 57.5% for the three domains, respectively.
353
Computational Linguistics Volume 34, Number 3
We have introduced in detail the framework of the twin-candidate model for
anaphora resolution, including instance representation, training procedure, and the an-
tecedent identification scheme. The efficacy of the twin-candidate model for pronominal
anaphora resolution has been evaluated in different domains, using ACE data sets. The
experimental results show that the model yields statistically significantly higher accu-
racy rates than the traditional single-candidate model (up to 4.2% in average accuracy
rate), suggesting that the twin-candidate model is superior to the latter for pronominal
anaphora resolution.
We have further investigated the deployment of the twin-candidate model in the
more complicated coreference resolution task, where not all the encountered NPs are
anaphoric. We have modified the model to make it directly applicable for coreference
resolution. The experimental results for non-pronoun resolution indicate that the twin-
candidate-based system performs equally well, and, in some domains, statistically
significantly better than the single-candidate based systems. When combined with
the results for pronoun resolution, the twin-candidate based system achieves further
improvement against the single-candidate-based systems in all the domains.
A number of further contributions can be made by extending this work in new
directions. Currently, we only adopt simple domain-independent features for learning.
Our recent work (Yang, Su, and Tan 2005) suggests that more complicated features, such
as statistics-based semantic compatibility, can be effectively incorporated in the twin-
candidate model for pronoun resolution. In future work, we intend to provide a more
in-depth investigation into the various kinds of knowledge that are suitable for the twin-
candidate model. Furthermore, in our current work for coreference resolution, all the
NPs preceding an anaphor are used as antecedent candidates, and all encountered non-
anaphors in texts are incorporated without filtering into training instance creation. For
more balanced training data and better classifier learning, we intend to explore some
instance-sampling techniques, such as those proposed by Ng and Cardie (2002a), to
remove in advance low-confidence candidates and the less informative non-anaphors.
We hope that these efforts can further improve the performance of the twin-candidate
model in both anaphora resolution and coreference resolution.
Acknowledgments
We would like to thank Guodong Zhou,
Alexia Leong, Stanley Wai Keong Yong,
and three anonymous reviewers for their
helpful comments and suggestions.
References
Aone, Ghinatsu and Scott W. Bennett.
1995. Evaluating automated and
manual acquisition of anaphora
resolution strategies. In Proceedings of
the 33rd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 122?129, Cambridge, Massachusetts.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?71.
Chomsky, Noam. 1981. Lectures on
Government and Binding. Foris,
Dordrecht, The Netherlands.
Clark, Herber H. and C. J. Sengul. 1979.
In search of referents for noun phrases
and pronouns.Memory and Cognition,
7:35?41.
Connolly, Dennis, John D. Burger, and
David S. Day, 1997. A machine learning
approach to anaphoric reference. In
New Methods in Language Processing,
pages 133?144, Taylor and Francis,
Bristol, Pennsylvania.
Crawley, Rosalind A., Rosemary J.
Stevenson, and David Kleinman. 1990.
The use of heuristic strategies in the
interpretation of pronouns. Journal of
Psycholinguistic Research, 19:245?264.
Denis, Pascal and Jason Baldridge. 2007. A
ranking approach to pronoun resolution.
In Proceedings of the 20th International Joint
354
Yang, Su, and Tan A Twin-Candidate Model for AR
Conference on Artificial Intelligence (IJCAI),
pages 1588?1593, Hyderabad, India.
Garnham, Alan, 2001.Mental Models and the
Interpretation of Anaphora. Psychology
Press Ltd., Hove, East Sussex, UK.
Ge, Niyu, John Hale, and Eugene Charniak.
1998. A statistical approach to anaphora
resolution. In Proceedings of the 6th
Workshop on Very Large Corpora,
pages 161?171, Montreal, Quebec, Canada.
Gernsbacher, Morton A. and David
Hargreaves. 1988. Accessing sentence
participants: The advantage of first
mention. Journal of Memory and Language,
27:699?717.
Grober, Ellen H., William Beardsley, and
Alfonso Caramazza. 1978. Parallel
function in pronoun assignment.
Cognition, 6:117?133.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Hobbs, Jerry. 1978. Resolving pronoun
references. Lingua, 44:339?352.
Iida, Ryu, Kentaro Inui, Hiroya Takamura,
and Yuji Matsumoto. 2003. Incorporating
contextual cues in trainable models for
coreference resolution. In Proceedings of the
10th Conference of EACL, Workshop ?The
Computational Treatment of Anaphora,?
pages 23?30, Budapest, Hungary.
Joachims, Thorsten. 2002. Optimizing search
engines using clickthrough data. In
Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining
(KDD), pages 133?142, Edmonton,
Alberta, Canada.
Jurafsky, Daniel and James H. Martin.
2000. Speech and Language Processing:
An Introduction to Natural Language
Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall,
Upper Saddle River, New Jersey.
Kehler, Andrew. 1997. Probabilistic
coreference in information extraction.
In Proceedings of the 2nd Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 163?173,
Providence, Rhode Island.
Kehler, Andrew, Douglas Appelt,
Lara Taylor, and Aleksandr Simma. 2004.
The (non)utility of predicate-argument
frequencies for pronoun interpretation.
In Proceedings of the North American
Chapter of the Association for Computational
Linguistics annual meeting (NAACL),
pages 289?296, Boston, MA.
Lappin, Shalom and Herbert J. Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):525?561.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan
Jing, Nanda Kambhatla, and Salim
Roukos. 2004. A mention-synchronous
coreference resolution algorithm
based on the bell tree. In Proceedings
of the 42nd Annual Meeting of the
Association for Computational
Linguistics (ACL), pages 135?142,
Barcelona, Spain.
McCarthy, Joseph F. and Wendy G. Lehnert.
1995. Using decision trees for coreference
resolution. In Proceedings of the 14th
International Conference on Artificial
Intelligences (IJCAI), pages 1050?1055,
Montreal, Quebec, Canada.
McEnery, A., I. Tanaka, and S. Botley.
1997. Corpus annotation and reference
resolution. In Proceedings of the ACL
Workshop on Operational Factors in
Practical Robust Anaphora Resolution
for Unrestricted Texts, pages 67?74,
Madrid, Spain.
Ng, Hwee Tou, Yu Zhou, Robert Dale,
and Mary Gardiner. 2005. Machine
learning approach to identification
and resolution of one-anaphora. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1105?1110, Edinburgh,
Scotland.
Ng, Vincent. 2005. Machine learning for
coreference resolution: From local
classification to global ranking. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 157?164,
Ann Arbor, Michigan.
Ng, Vincent and Claire Cardie. 2002a.
Combining sample selection and
error-driven pruning for machine
learning of coreference rules. In
Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 55?62,
Philadelphia, PA.
Ng, Vincent and Claire Cardie. 2002b.
Improving machine learning approaches
to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 104?111, Philadelphia, PA.
Preiss, Judita. 2001. Machine learning for
anaphora resolution. Technical Report
CS-01-10, University of Sheffield,
Sheffield, England.
355
Computational Linguistics Volume 34, Number 3
Quinlan, J. Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann
Publishers, San Francisco, CA.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine
learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521?544.
Stevenson, Rosemary J., Alexander W. R.
Nelson, and Keith Stenning. 1995. The role
of parallelism in strategies of pronoun
comprehension. Language and Speech,
29:393?418.
Strube, Michael and Christoph Mueller.
2003. A machine learning approach to
pronoun resolution in spoken dialogue.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 168?175,
Sapporo, Japan.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York, NY.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
pages 45?52, San Francisco, CA.
Wilks, Yorick. 1973. Preference Semantics.
Stanford AI Laboratory Memo AIM-206.
Stanford University.
Winograd, Terry. 1972. Understanding Natural
Language. Academic Press, New York.
Yang, Xiaofeng, Jian Su, and Chew Lim Tan.
2005. Improving pronoun resolution using
statistics-based semantic compatibility
information. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 165?172, Ann Arbor, MI.
Zhou, Guodong and Jian Su. 2000.
Error-driven HMM-based chunk tagger
with context-dependent lexicon. In
Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Very Large Corpora,
pages 71?79, Hong Kong.
Zhou, Guodong and Jian Su. 2002. Named
Entity recognition using a HMM-based
chunk tagger. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 473?480, Philadelphia, PA.
356
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge
Xiaofeng Yang? Jian Su? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
tancl@comp.nus.edu.sg
Abstract
Syntactic knowledge is important for pro-
noun resolution. Traditionally, the syntac-
tic information for pronoun resolution is
represented in terms of features that have
to be selected and defined heuristically.
In the paper, we propose a kernel-based
method that can automatically mine the
syntactic information from the parse trees
for pronoun resolution. Specifically, we
utilize the parse trees directly as a struc-
tured feature and apply kernel functions to
this feature, as well as other normal fea-
tures, to learn the resolution classifier. In
this way, our approach avoids the efforts
of decoding the parse trees into the set of
flat syntactic features. The experimental
results show that our approach can bring
significant performance improvement and
is reliably effective for the pronoun reso-
lution task.
1 Introduction
Pronoun resolution is the task of finding the cor-
rect antecedent for a given pronominal anaphor
in a document. Prior studies have suggested that
syntactic knowledge plays an important role in
pronoun resolution. For a practical pronoun res-
olution system, the syntactic knowledge usually
comes from the parse trees of the text. The is-
sue that arises is how to effectively incorporate the
syntactic information embedded in the parse trees
to help resolution. One common solution seen in
previous work is to define a set of features that rep-
resent particular syntactic knowledge, such as the
grammatical role of the antecedent candidates, the
governing relations between the candidate and the
pronoun, and so on. These features are calculated
by mining the parse trees, and then could be used
for resolution by using manually designed rules
(Lappin and Leass, 1994; Kennedy and Boguraev,
1996; Mitkov, 1998), or using machine-learning
methods (Aone and Bennett, 1995; Yang et al,
2004; Luo and Zitouni, 2005).
However, such a solution has its limitation. The
syntactic features have to be selected and defined
manually, usually by linguistic intuition. Unfor-
tunately, what kinds of syntactic information are
effective for pronoun resolution still remains an
open question in this research community. The
heuristically selected feature set may be insuffi-
cient to represent all the information necessary for
pronoun resolution contained in the parse trees.
In this paper we will explore how to utilize the
syntactic parse trees to help learning-based pro-
noun resolution. Specifically, we directly utilize
the parse trees as a structured feature, and then use
a kernel-based method to automatically mine the
knowledge embedded in the parse trees. The struc-
tured syntactic feature, together with other nor-
mal features, is incorporated in a trainable model
based on Support Vector Machine (SVM) (Vapnik,
1995) to learn the decision classifier for resolution.
Indeed, using kernel methods to mine structural
knowledge has shown success in some NLP ap-
plications like parsing (Collins and Duffy, 2002;
Moschitti, 2004) and relation extraction (Zelenko
et al, 2003; Zhao and Grishman, 2005). However,
to our knowledge, the application of such a tech-
nique to the pronoun resolution task still remains
unexplored.
Compared with previous work, our approach
has several advantages: (1) The approach uti-
lizes the parse trees as a structured feature, which
avoids the efforts of decoding the parse trees into
a set of syntactic features in a heuristic manner.
(2) The approach is able to put together the struc-
tured feature and the normal flat features in a
trainable model, which allows different types of
41
information to be considered in combination for
both learning and resolution. (3) The approach
is applicable for practical pronoun resolution as
the syntactic information can be automatically ob-
tained from machine-generated parse trees. And
our study shows that the approach works well un-
der the commonly available parsers.
We evaluate our approach on the ACE data set.
The experimental results over the different do-
mains indicate that the structured syntactic fea-
ture incorporated with kernels can significantly
improve the resolution performance (by 5%?8%
in the success rates), and is reliably effective for
the pronoun resolution task.
The remainder of the paper is organized as fol-
lows. Section 2 gives some related work that uti-
lizes the structured syntactic knowledge to do pro-
noun resolution. Section 3 introduces the frame-
work for the pronoun resolution, as well as the
baseline feature space and the SVM classifier.
Section 4 presents in detail the structured feature
and the kernel functions to incorporate such a fea-
ture in the resolution. Section 5 shows the exper-
imental results and has some discussion. Finally,
Section 6 concludes the paper.
2 Related Work
One of the early work on pronoun resolution rely-
ing on parse trees was proposed by Hobbs (1978).
For a pronoun to be resolved, Hobbs? algorithm
works by searching the parse trees of the current
text. Specifically, the algorithm processes one sen-
tence at a time, using a left-to-right breadth-first
searching strategy. It first checks the current sen-
tence where the pronoun occurs. The first NP
that satisfies constraints, like number and gender
agreements, would be selected as the antecedent.
If the antecedent is not found in the current sen-
tence, the algorithm would traverse the trees of
previous sentences in the text. As the searching
processing is completely done on the parse trees,
the performance of the algorithm would rely heav-
ily on the accuracy of the parsing results.
Lappin and Leass (1994) reported a pronoun
resolution algorithm which uses the syntactic rep-
resentation output by McCord?s Slot Grammar
parser. A set of salience measures (e.g. Sub-
ject, Object or Accusative emphasis) is derived
from the syntactic structure. The candidate with
the highest salience score would be selected as
the antecedent. In their algorithm, the weights of
Category: whether the candidate is a definite noun phrase,
indefinite noun phrase, pronoun, named-entity or others.
Reflexiveness: whether the pronominal anaphor is a reflex-
ive pronoun.
Type: whether the pronominal anaphor is a male-person
pronoun (like he), female-person pronoun (like she), sin-
gle gender-neuter pronoun (like it), or plural gender-neuter
pronoun (like they)
Subject: whether the candidate is a subject of a sentence, a
subject of a clause, or not.
Object: whether the candidate is an object of a verb, an
object of a preposition, or not.
Distance: the sentence distance between the candidate and
the pronominal anaphor.
Closeness: whether the candidate is the candidate closest
to the pronominal anaphor.
FirstNP: whether the candidate is the first noun phrase in
the current sentence.
Parallelism: whether the candidate has an identical collo-
cation pattern with the pronominal anaphor.
Table 1: Feature set for the baseline pronoun res-
olution system
salience measures have to be assigned manually.
Luo and Zitouni (2005) proposed a coreference
resolution approach which also explores the infor-
mation from the syntactic parse trees. Different
from Lappin and Leass (1994)?s algorithm, they
employed a maximum entropy based model to au-
tomatically compute the importance (in terms of
weights) of the features extracted from the trees.
In their work, the selection of their features is
mainly inspired by the government and binding
theory, aiming to capture the c-command relation-
ships between the pronoun and its antecedent can-
didate. By contrast, our approach simply utilizes
the parse trees as a structured feature, and lets the
learning algorithm discover all possible embedded
information that is necessary for pronoun resolu-
tion.
3 The Resolution Framework
Our pronoun resolution system adopts the com-
mon learning-based framework similar to those
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance is formed by a pronoun and one of its
antecedent candidate. During training, for each
pronominal anaphor encountered, a positive in-
stance is created by paring the anaphor and its
closest antecedent. Also a set of negative instances
is formed by paring the anaphor with each of the
42
non-coreferential candidates. Based on the train-
ing instances, a binary classifier is generated using
a particular learning algorithm. During resolution,
a pronominal anaphor to be resolved is paired in
turn with each preceding antecedent candidate to
form a testing instance. This instance is presented
to the classifier which then returns a class label
with a confidence value indicating the likelihood
that the candidate is the antecedent. The candidate
with the highest confidence value will be selected
as the antecedent of the pronominal anaphor.
3.1 Feature Space
As with many other learning-based approaches,
the knowledge for the reference determination is
represented as a set of features associated with
the training or test instances. In our baseline sys-
tem, the features adopted include lexical property,
morphologic type, distance, salience, parallelism,
grammatical role and so on. Listed in Table 1, all
these features have been proved effective for pro-
noun resolution in previous work.
3.2 Support Vector Machine
In theory, any discriminative learning algorithm is
applicable to learn the classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels to
incorporate the structured feature.
Suppose the training set S consists of labelled
vectors {(xi, yi)}, where xi is the feature vector
of a training instance and yi is its class label. The
classifier learned by SVM is
f(x) = sgn(
?
i=1
yiaix ? xi + b) (1)
where ai is the learned parameter for a support
vector xi. An instance x is classified as positive
(negative) if f(x) > 0 (f(x) < 0)1.
One advantage of SVM is that we can use ker-
nel methods to map a feature space to a particu-
lar high-dimension space, in case that the current
problem could not be separated in a linear way.
Thus the dot-product x1 ? x2 is replaced by a ker-
nel function (or kernel) between two vectors, that
is K(x1, x2). For the learning with the normal
features listed in Table 1, we can just employ the
well-known polynomial or radial basis kernels that
can be computed efficiently. In the next section we
1For our task, the result of f(x) is used as the confidence
value of the candidate to be the antecedent of the pronoun
described by x.
will discuss how to use kernels to incorporate the
more complex structured feature.
4 Incorporating Structured Syntactic
Information
4.1 Main Idea
A parse tree that covers a pronoun and its an-
tecedent candidate could provide us much syntac-
tic information related to the pair. The commonly
used syntactic knowledge for pronoun resolution,
such as grammatical roles or the governing rela-
tions, can be directly described by the tree struc-
ture. Other syntactic knowledge that may be help-
ful for resolution could also be implicitly repre-
sented in the tree. Therefore, by comparing the
common substructures between two trees we can
find out to what degree two trees contain similar
syntactic information, which can be done using a
convolution tree kernel.
The value returned from the tree kernel reflects
the similarity between two instances in syntax.
Such syntactic similarity can be further combined
with other knowledge to compute the overall simi-
larity between two instances, through a composite
kernel. And thus a SVM classifier can be learned
and then used for resolution. This is just the main
idea of our approach.
4.2 Structured Syntactic Feature
Normally, parsing is done on the sentence level.
However, in many cases a pronoun and an an-
tecedent candidate do not occur in the same sen-
tence. To present their syntactic properties and
relations in a single tree structure, we construct a
syntax tree for an entire text, by attaching the parse
trees of all its sentences to an upper node.
Having obtained the parse tree of a text, we shall
consider how to select the appropriate portion of
the tree as the structured feature for a given in-
stance. As each instance is related to a pronoun
and a candidate, the structured feature at least
should be able to cover both of these two expres-
sions. Generally, the more substructure of the tree
is included, the more syntactic information would
be provided, but at the same time the more noisy
information that comes from parsing errors would
likely be introduced. In our study, we examine
three possible structured features that contain dif-
ferent substructures of the parse tree:
Min-Expansion This feature records the mini-
mal structure covering both the pronoun and
43
Min-Expansion Simple-Expansion Full-Expansion
Figure 1: structured-features for the instance i{?him?, ?the man?}
the candidate in the parse tree. It only in-
cludes the nodes occurring in the shortest
path connecting the pronoun and the candi-
date, via the nearest commonly commanding
node. For example, considering the sentence
?The man in the room saw him.?, the struc-
tured feature for the instance i{?him?,?the
man?} is circled with dash lines as shown in
the leftmost picture of Figure 1.
Simple-Expansion Min-Expansion could, to
some degree, describe the syntactic relation-
ships between the candidate and pronoun.
However, it is incapable of capturing the
syntactic properties of the candidate or
the pronoun, because the tree structure
surrounding the expression is not taken into
consideration. To incorporate such infor-
mation, feature Simple-Expansion not only
contains all the nodes in Min-Expansion, but
also includes the first-level children of these
nodes2. The middle of Figure 1 shows such a
feature for i{?him?, ?the man?}. We can see
that the nodes ?PP? (for ?in the room?) and
?VB? (for ?saw?) are included in the feature,
which provides clues that the candidate is
modified by a prepositional phrase and the
pronoun is the object of a verb.
Full-Expansion This feature focusses on the
whole tree structure between the candidate
and pronoun. It not only includes all the
nodes in Simple-Expansion, but also the
nodes (beneath the nearest commanding par-
ent) that cover the words between the candi-
date and the pronoun3. Such a feature keeps
the most information related to the pronoun
2If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences
before the candidate or after the pronoun.
3We will not expand the nodes denoting the sentences
other than where the pronoun and the candidate occur.
and candidate pair. The rightmost picture of
Figure 1 shows the structure for feature Full-
Expansion of i{?him?, ?the man?}. As illus-
trated, different from in Simple-Expansion,
the subtree of ?PP? (for ?in the room?) is
fully expanded and all its children nodes are
included in Full-Expansion.
Note that to distinguish from other words, we
explicitly mark up in the structured feature the
pronoun and the antecedent candidate under con-
sideration, by appending a string tag ?ANA? and
?CANDI? in their respective nodes (e.g.,?NN-
CANDI? for ?man? and ?PRP-ANA? for ?him? as
shown in Figure 1).
4.3 Structural Kernel and Composite Kernel
To calculate the similarity between two structured
features, we use the convolution tree kernel that is
defined by Collins and Duffy (2002) and Moschitti
(2004). Given two trees, the kernel will enumerate
all their subtrees and use the number of common
subtrees as the measure of the similarity between
the trees. As has been proved, the convolution
kernel can be efficiently computed in polynomial
time.
The above tree kernel only aims for the struc-
tured feature. We also need a composite kernel
to combine together the structured feature and the
normal features described in Section 3.1. In our
study we define the composite kernel as follows:
Kc(x1, x2) = Kn(x1, x2)|Kn(x1, x2)| ?
Kt(x1, x2)
|Kt(x1, x2)|(2)
where Kt is the convolution tree kernel defined
for the structured feature, and Kn is the kernel
applied on the normal features. Both kernels are
divided by their respective length4 for normaliza-
tion. The new composite kernel Kc, defined as the
4The length of a kernel K is defined as |K(x1, x2)| =?
K(x1, x1) ?K(x2, x2)
44
multiplier of normalized Kt and Kn, will return a
value close to 1 only if both the structured features
and the normal features from the two vectors have
high similarity under their respective kernels.
5 Experiments and Discussions
5.1 Experimental Setup
In our study we focussed on the third-person
pronominal anaphora resolution. All the exper-
iments were done on the ACE-2 V1.0 corpus
(NIST, 2003), which contain two data sets, train-
ing and devtest, used for training and testing re-
spectively. Each of these sets is further divided
into three domains: newswire (NWire), newspa-
per (NPaper), and broadcast news (BNews).
An input raw text was preprocessed automati-
cally by a pipeline of NLP components, including
sentence boundary detection, POS-tagging, Text
Chunking and Named-Entity Recognition. The
texts were parsed using the maximum-entropy-
based Charniak parser (Charniak, 2000), based on
which the structured features were computed au-
tomatically. For learning, the SVM-Light soft-
ware (Joachims, 1999) was employed with the
convolution tree kernel implemented by Moschitti
(2004). All classifiers were trained with default
learning parameters.
The performance was evaluated based on the
metric success, the ratio of the number of cor-
rectly resolved5 anaphor over the number of all
anaphors. For each anaphor, the NPs occurring
within the current and previous two sentences
were taken as the initial antecedent candidates.
Those with mismatched number and gender agree-
ments were filtered from the candidate set. Also,
pronouns or NEs that disagreed in person with the
anaphor were removed in advance. For training,
there were 1207, 1440, and 1260 pronouns with
non-empty candidate set found pronouns in the
three domains respectively, while for testing, the
number was 313, 399 and 271. On average, a
pronoun anaphor had 6?9 antecedent candidates
ahead. Totally, we got around 10k, 13k and 8k
training instances for the three domains.
5.2 Baseline Systems
Table 2 lists the performance of different systems.
We first tested Hobbs? algorithm (Hobbs, 1978).
5An anaphor was deemed correctly resolved if the found
antecedent is in the same coreference chain of the anaphor.
NWire NPaper BNews
Hobbs (1978) 66.1 66.4 72.7
NORM 74.4 77.4 74.2
NORM MaxEnt 72.8 77.9 75.3
NORM C5 71.9 75.9 71.6
S Min 76.4 81.0 76.8
S Simple 73.2 82.7 82.3
S Full 73.2 80.5 79.0
NORM+S Min 77.6 82.5 82.3
NORM+S Simple 79.2 82.7 82.3
NORM+S Full 81.5 83.2 81.5
Table 2: Results of the syntactic structured fea-
tures
Described in Section 2, the algorithm uses heuris-
tic rules to search the parse tree for the antecedent,
and will act as a good baseline to compare with the
learned-based approach with the structured fea-
ture. As shown in the first line of Table 2, Hobbs?
algorithm obtains 66%?72% success rates on the
three domains.
The second block of Table 2 shows the baseline
system (NORM) that uses only the normal features
listed in Table 1. Throughout our experiments, we
applied the polynomial kernel on the normal fea-
tures to learn the SVM classifiers. In the table we
also compared the SVM-based results with those
using other learning algorithms, i.e., Maximum
Entropy (Maxent) and C5 decision tree, which are
more commonly used in the anaphora resolution
task.
As shown in the table, the system with normal
features (NORM) obtains 74%?77% success rates
for the three domains. The performance is simi-
lar to other published results like those by Keller
and Lapata (2003), who adopted a similar fea-
ture set and reported around 75% success rates
on the ACE data set. The comparison between
different learning algorithms indicates that SVM
can work as well as or even better than Maxent
(NORM MaxEnt) or C5 (NORM C5).
5.3 Systems with Structured Features
The last two blocks of Table 2 summarize the re-
sults using the three syntactic structured features,
i.e, Min Expansion (S MIN), Simple Expansion
(S SIMPLE) and Full Expansion (S FULL). Be-
tween them, the third block is for the systems us-
ing the individual structured feature alone. We
can see that all the three structured features per-
45
NWire NPaper BNews
Sentence Distance 0 1 2 0 1 2 0 1 2
(Number of Prons) (192) (102) (19) (237) (147) (15) (175) (82) (14)
NORM 80.2 72.5 26.3 81.4 75.5 33.3 80.0 65.9 50.0
S Simple 79.7 70.6 21.1 87.3 81.0 26.7 89.7 70.7 57.1
NORM+S Simple 85.4 76.5 31.6 87.3 79.6 40.0 88.6 74.4 50.0
Table 3: The resolution results for pronouns with antecedent in different sentences apart
NWire NPaper BNews
Type person neuter person neuter person neuter
(Number of Prons) (171) (142) (250) (149) (153) (118)
NORM 81.9 65.5 80.0 73.2 74.5 73.7
S Simple 81.9 62.7 83.2 81.9 82.4 82.2
NORM+S Simple 87.1 69.7 83.6 81.2 86.9 76.3
Table 4: The resolution results for different types of pronouns
form better than the normal features for NPaper
(up to 5.3% success) and BNews (up to 8.1% suc-
cess), or equally well (?1 ? 2% in success) for
NWire. When used together with the normal fea-
tures, as shown in the last block, the three struc-
tured features all outperform the baselines. Es-
pecially, the combinations of NORM+S SIMPLE
and NORM+S FULL can achieve significantly6
better results than NORM, with the success rate
increasing by (4.8%, 5.3% and 8.1%) and (7.1%,
5.8%, 7.2%) respectively. All these results prove
that the structured syntactic feature is effective for
pronoun resolution.
We further compare the performance of the
three different structured features. As shown in
Table 2, when used together with the normal
features, Full Expansion gives the highest suc-
cess rates in NWire and NPaper, but neverthe-
less the lowest in BNews. This should be be-
cause feature Full-Expansion captures a larger
portion of the parse trees, and thus can provide
more syntactic information than Min Expansion
or Simple Expansion. However, if the texts are
less-formally structured as those in BNews, Full-
Expansion would inevitably involve more noises
and thus adversely affect the resolution perfor-
mance. By contrast, feature Simple Expansion
would achieve balance between the information
and the noises to be introduced: from Table 2 we
can find that compared with the other two features,
Simple Expansion is capable of producing aver-
age results for all the three domains. And for this
6p < 0.05 by a 2-tailed t test.
reason, our subsequent reports will focus on Sim-
ple Expansion, unless otherwise specified.
As described, to compute the structured fea-
ture, parse trees for different sentences are con-
nected to form a large tree for the text. It would
be interesting to find how the structured feature
works for pronouns whose antecedents reside in
different sentences. For this purpose we tested
the success rates for the pronouns with the clos-
est antecedent occurring in the same sentence,
one-sentence apart, and two-sentence apart. Ta-
ble 3 compares the learning systems with/without
the structured feature present. From the table,
for all the systems, the success rates drop with
the increase of the distances between the pro-
noun and the antecedent. However, in most cases,
adding the structured feature would bring consis-
tent improvement against the baselines regardless
of the number of sentence distance. This observa-
tion suggests that the structured syntactic informa-
tion is helpful for both intra-sentential and inter-
sentential pronoun resolution.
We were also concerned about how the struc-
tured feature works for different types of pro-
nouns. Table 4 lists the resolution results for two
types of pronouns: person pronouns (i.e., ?he?,
?she?) and neuter-gender pronouns (i.e., ?it? and
?they?). As shown, with the structured feature in-
corporated, the system NORM+S Simple can sig-
nificantly boost the performance of the baseline
(NORM), for both personal pronoun and neuter-
gender pronoun resolution.
46
1 2 3 4 5 6 7 8 9 100.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple 2 4 6 8 10 120.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple 1 2 3 4 5 6 7 80.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple
NWire NPaper BNews
Figure 2: Learning curves of systems with different features
5.4 Learning Curves
Figure 2 plots the learning curves for the sys-
tems with three feature sets, i.e, normal features
(NORM), structured feature alone (S Simple),
and combined features (NORM+S Simple). We
trained each system with different number of in-
stances from 1k, 2k, 3k, . . . , till the full size. Each
point in the figures was the average over two trails
with instances selected forwards and backwards
respectively. From the figures we can find that
(1) Used in combination (NORM+S Simple), the
structured feature shows superiority over NORM,
achieving results consistently better than the nor-
mal features (NORM) do in all the three domains.
(2) With training instances above 3k, the struc-
tured feature, used either in isolation (S Simple)
or in combination (NORM+S Simple), leads to
steady increase in the success rates and exhibit
smoother learning curves than the normal features
(NORM). These observations further prove the re-
liability of the structured feature in pronoun reso-
lution.
5.5 Feature Analysis
In our experiment we were also interested to com-
pare the structured feature with the normal flat
features extracted from the parse tree, like fea-
ture Subject and Object. For this purpose we
took out these two grammatical features from the
normal feature set, and then trained the systems
again. As shown in Table 5, the two grammatical-
role features are important for the pronoun resolu-
tion: removing these features results in up to 5.7%
(NWire) decrease in success. However, when the
structured feature is included, the loss in success
reduces to 1.9% and 1.1% for NWire and BNews,
and a slight improvement can even be achieved for
NPaper. This indicates that the structured feature
can effectively provide the syntactic information
NWire NPaper BNews
NORM 74.4 77.4 74.2
NORM - subj/obj 68.7 76.2 72.7
NORM + S Simple 79.2 82.7 82.3
NORM + S Simple - subj/obj 77.3 83.0 81.2
NORM + Luo05 75.7 77.9 74.9
Table 5: Comparison of the structured feature and
the flat features extracted from parse trees
Feature Parser NWire NPaper BNews
Charniak00 73.2 82.7 82.3
S Simple Collins99 75.1 83.2 80.4
NORM+ Charniak00 79.2 82.7 82.3
S Simple Collins99 80.8 81.5 82.3
Table 6: Results using different parsers
important for pronoun resolution.
We also tested the flat syntactic feature set pro-
posed in Luo and Zitouni (2005)?s work. As de-
scribed in Section 2, the feature set is inspired
the binding theory, including those features like
whether the candidate is c commanding the pro-
noun, and the counts of ?NP?, ?VP?, ?S? nodes
in the commanding path. The last line of Table 5
shows the results by adding these features into the
normal feature set. In line with the reports in (Luo
and Zitouni, 2005) we do observe the performance
improvement against the baseline (NORM) for all
the domains. However, the increase in the success
rates (up to 1.3%) is not so large as by adding the
structured feature (NORM+S Simple) instead.
5.6 Comparison with Different Parsers
As mentioned, the above reported results were
based on Charniak (2000)?s parser. It would be
interesting to examine the influence of different
parsers on the resolution performance. For this
purpose, we also tried the parser by Collins (1999)
47
(Mode II)7, and the results are shown in Table 6.
We can see that Charniak (2000)?s parser leads to
higher success rates for NPaper and BNews, while
Collins (1999)?s achieves better results for NWire.
However, the difference between the results of the
two parsers is not significant (less than 2% suc-
cess) for the three domains, no matter whether the
structured feature is used alone or in combination.
6 Conclusion
The purpose of this paper is to explore how to
make use of the structured syntactic knowledge to
do pronoun resolution. Traditionally, syntactic in-
formation from parse trees is represented as a set
of flat features. However, the features are usu-
ally selected and defined by heuristics and may
not necessarily capture all the syntactic informa-
tion provided by the parse trees. In the paper, we
propose a kernel-based method to incorporate the
information from parse trees. Specifically, we di-
rectly utilize the syntactic parse tree as a struc-
tured feature, and then apply kernels to such a fea-
ture, together with other normal features, to learn
the decision classifier and do the resolution. Our
experimental results on ACE data set show that
the system with the structured feature included
can achieve significant increase in the success rate
by around 5%?8%, for all the different domains.
The deeper analysis on various factors like training
size, feature set or parsers further proves that the
structured feature incorporated with our kernel-
based method is reliably effective for the pronoun
resolution task.
References
C. Aone and S. W. Bennett. 1995. Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. In Proceedings of the 33rd Annual
Meeting of the Association for Compuational Lin-
guistics, pages 122?129.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of North American chapter
of the Association for Computational Linguistics an-
nual meeting, pages 132?139.
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: kernels over discrete
structures and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association
7As in their pulic reports on Section 23 of WSJ TreeBank,
Charniak (2000)?s parser achieves 89.6% recall and 89.5%
precision with 0.88 crossing brackets (words ? 100), against
Collins (1999)?s 88.1% recall and 88.3% precision with 1.06
crossing brackets.
for Computational Linguistics (ACL?02), pages 263?
270.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Hobbs. 1978. Resolving pronoun references. Lin-
gua, 44:339?352.
T. Joachims. 1999. Making large-scale svm learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain freqencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
C. Kennedy and B. Boguraev. 1996. Anaphora
for everyone: pronominal anaphra resolution with-
out a parser. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 113?118, Copenhagen, Denmark.
S. Lappin and H. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):525?561.
X. Luo and I. Zitouni. 2005. Milti-lingual coreference
resolution with syntactic features. In Proceedings of
Human Language Techonology conference and Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 660?667.
R. Mitkov. 1998. Robust pronoun resolution with lim-
ited knowledge. In Proceedings of the 17th Int. Con-
ference on Computational Linguistics, pages 869?
875.
A. Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL?04), pages 335?342.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 104?111,
Philadelphia.
W. Soon, H. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?
544.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improv-
ing pronoun resolution by incorporating coreferen-
tial information of candidates. In Proceedings of
42th Annual Meeting of the Association for Compu-
tational Linguistics, pages 127?134, Barcelona.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Ma-
chine Learning Research, 3(6):1083 ? 1106.
S. Zhao and R. Grishman. 2005. Extracting rela-
tions with integrated information using kernel meth-
ods. In Proceedings of 43rd Annual Meeting of the
Association for Computational Linguistics (ACL05),
pages 419?426.
48
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 528?535,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Coreference Resolution Using Semantic Relatedness Information from
Automatically Discovered Patterns
Xiaofeng Yang Jian Su
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
Abstract
Semantic relatedness is a very important fac-
tor for the coreference resolution task. To
obtain this semantic information, corpus-
based approaches commonly leverage pat-
terns that can express a specific semantic
relation. The patterns, however, are de-
signed manually and thus are not necessar-
ily the most effective ones in terms of ac-
curacy and breadth. To deal with this prob-
lem, in this paper we propose an approach
that can automatically find the effective pat-
terns for coreference resolution. We explore
how to automatically discover and evaluate
patterns, and how to exploit the patterns to
obtain the semantic relatedness information.
The evaluation on ACE data set shows that
the pattern based semantic information is
helpful for coreference resolution.
1 Introduction
Semantic relatedness is a very important factor for
coreference resolution, as noun phrases used to re-
fer to the same entity should have a certain semantic
relation. To obtain this semantic information, previ-
ous work on reference resolution usually leverages
a semantic lexicon like WordNet (Vieira and Poe-
sio, 2000; Harabagiu et al, 2001; Soon et al, 2001;
Ng and Cardie, 2002). However, the drawback of
WordNet is that many expressions (especially for
proper names), word senses and semantic relations
are not available from the database (Vieira and Poe-
sio, 2000). In recent years, increasing interest has
been seen in mining semantic relations from large
text corpora. One common solution is to utilize a
pattern that can represent a specific semantic rela-
tion (e.g., ?X such as Y? for is-a relation, and ?X
and other Y? for other-relation). Instantiated with
two given noun phrases, the pattern is searched in a
large corpus and the occurrence number is used as
a measure of their semantic relatedness (Markert et
al., 2003; Modjeska et al, 2003; Poesio et al, 2004).
However, in the previous pattern based ap-
proaches, the selection of the patterns to represent a
specific semantic relation is done in an ad hoc way,
usually by linguistic intuition. The manually se-
lected patterns, nevertheless, are not necessarily the
most effective ones for coreference resolution from
the following two concerns:
? Accuracy. Can the patterns (e.g., ?X such as
Y?) find as many NP pairs of the specific se-
mantic relation (e.g. is-a) as possible, with a
high precision?
? Breadth. Can the patterns cover a wide variety
of semantic relations, not just is-a, by which
coreference relationship is realized? For ex-
ample, in some annotation schemes like ACE,
?Beijing:China? are coreferential as the capital
and the country could be used to represent the
government. The pattern for the common ?is-
a? relation will fail to identify the NP pairs of
such a ?capital-country? relation.
To deal with this problem, in this paper we pro-
pose an approach which can automatically discover
effective patterns to represent the semantic relations
528
for coreference resolution. We explore two issues in
our study:
(1) How to automatically acquire and evaluate
the patterns? We utilize a set of coreferential NP
pairs as seeds. For each seed pair, we search a large
corpus for the texts where the two noun phrases co-
occur, and collect the surrounding words as the sur-
face patterns. We evaluate a pattern based on its
commonality or association with the positive seed
pairs.
(2) How to mine the patterns to obtain the seman-
tic relatedness information for coreference resolu-
tion? We present two strategies to exploit the pat-
terns: choosing the top best patterns as a set of pat-
tern features, or computing the reliability of seman-
tic relatedness as a single feature. In either strategy,
the obtained features are applied to do coreference
resolution in a supervised-learning way.
To our knowledge, our work is the first effort that
systematically explores these issues in the corefer-
ence resolution task. We evaluate our approach on
ACE data set. The experimental results show that
the pattern based semantic relatedness information
is helpful for the coreference resolution.
The remainder of the paper is organized as fol-
lows. Section 2 gives some related work. Section 3
introduces the framework for coreference resolution.
Section 4 presents the model to obtain the pattern-
based semantic relatedness information. Section 5
discusses the experimental results. Finally, Section
6 summarizes the conclusions.
2 Related Work
Earlier work on coreference resolution commonly
relies on semantic lexicons for semantic relatedness
knowledge. In the system by Vieira and Poesio
(2000), for example, WordNet is consulted to obtain
the synonymy, hypernymy and meronymy relations
for resolving the definite anaphora. In (Harabagiu
et al, 2001), the path patterns in WordNet are uti-
lized to compute the semantic consistency between
NPs. Recently, Ponzetto and Strube (2006) suggest
to mine semantic relatedness from Wikipedia, which
can deal with the data sparseness problem suffered
by using WordNet.
Instead of leveraging existing lexicons, many
researchers have investigated corpus-based ap-
proaches to mine semantic relations. Garera and
Yarowsky (2006) propose an unsupervised model
which extracts hypernym relation for resloving def-
inite NPs. Their model assumes that a definite NP
and its hypernym words usually co-occur in texts.
Thus, for a definite-NP anaphor, a preceding NP that
has a high co-occurrence statistics in a large corpus
is preferred for the antecedent.
Bean and Riloff (2004) present a system called
BABAR that uses contextual role knowledge to do
coreference resolution. They apply an IE component
to unannotated texts to generate a set of extraction
caseframes. Each caseframe represents a linguis-
tic expression and a syntactic position, e.g. ?mur-
der of <NP>?, ?killed <patient>?. From the case-
frames, they derive different types of contextual role
knowledge for resolution, for example, whether an
anaphor and an antecedent candidate can be filled
into co-occurring caseframes, or whether they are
substitutable for each other in their caseframes. Dif-
ferent from their system, our approach aims to find
surface patterns that can directly indicate the coref-
erence relation between two NPs.
Hearst (1998) presents a method to automate the
discovery of WordNet relations, by searching for the
corresponding patterns in large text corpora. She ex-
plores several patterns for the hyponymy relation,
including ?X such as Y? ?X and/or other Y?, ?X
including / especially Y? and so on. The use of
Hearst?s style patterns can be seen for the reference
resolution task. Modjeska et al (2003) explore the
use of the Web to do the other-anaphora resolution.
In their approach, a pattern ?X and other Y? is used.
Given an anaphor and a candidate antecedent, the
pattern is instantiated with the two NPs and forms a
query. The query is submitted to the Google search-
ing engine, and the returned hit number is utilized to
compute the semantic relatedness between the two
NPs. In their work, the semantic information is used
as a feature for the learner. Markert et al (2003) and
Poesio et al (2004) adopt a similar strategy for the
bridging anaphora resolution.
In (Hearst, 1998), the author also proposes to dis-
cover new patterns instead of using the manually
designed ones. She employs a bootstrapping algo-
rithm to learn new patterns from the word pairs with
a known relation. Based on Hearst?s work, Pan-
tel and Pennacchiotti (2006) further give a method
529
which measures the reliability of the patterns based
on the strength of association between patterns and
instances, employing the pointwise mutual informa-
tion (PMI).
3 Framework of Coreference Resolution
Our coreference resolution system adopts the
common learning-based framework as employed
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{NPi, NPj}, in which
NPj is a possible anaphor and NPi is one of its an-
tecedent candidates. An instance is associated with
a vector of features, which is used to describe the
properties of the two noun phrases as well as their
relationships. In our baseline system, we adopt the
common features for coreference resolution such as
lexical property, distance, string-matching, name-
alias, apposition, grammatical role, number/gender
agreement and so on. The same feature set is de-
scribed in (Ng and Cardie, 2002) for reference.
During training, for each encountered anaphor
NPj , one single positive training instance is created
for its closest antecedent. And a group of negative
training instances is created for every intervening
noun phrases between NPj and the antecedent.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm, like C5 in our study. For resolution, an
input document is processed from the first NP to the
last. For each encountered NPj , a test instance is
formed for each antecedent candidate, NPi1. This
instance is presented to the classifier to determine
the coreference relationship. NPj will be resolved
to the candidate that is classified as positive (if any)
and has the highest confidence value.
In our study, we augment the common framework
by incorporating non-anaphors into training. We fo-
cus on the non-anaphors that the original classifier
fails to identify. Specifically, we apply the learned
classifier to all the non-anaphors in the training doc-
uments. For each non-anaphor that is classified as
positive, a negative instance is created by pairing the
non-anaphor and its false antecedent. These neg-
1For resolution of pronouns, only the preceding NPs in cur-
rent and previous two sentences are considered as antecedent
candidates. For resolution of non-pronouns, all the preceding
non-pronouns are considered.
ative instances are added into the original training
instance set for learning, which will generate a clas-
sifier with the capability of not only antecedent iden-
tification, but also non-anaphorically identification.
The new classier is applied to the testing document
to do coreference resolution as usual.
4 Patterned Based Semantic Relatedness
4.1 Acquiring the Patterns
To derive patterns to indicate a specific semantic re-
lation, a set of seed NP pairs that have the relation of
interest is needed. As described in the previous sec-
tion, we have a set of training instances formed by
NP pairs with known coreference relationships. We
can just use this set of NP pairs as the seeds. That is,
an instance i{NPi, NPj} will become a seed pair
(Ei:Ej) in which NPi corresponds to Ei and NPj
corresponds to Ej . In creating the seed, for a com-
mon noun, only the head word is retained while for
a proper name, the whole string is kept. For ex-
ample, instance i{?Bill Clinton?, ?the former pres-
ident?} will be converted to a NP pair (?Bill Clin-
ton?:?president?).
We create the seed pair for every training instance
i{NPi, NPj}, except when (1) NPi or NPj is a
pronoun; or (2) NPi and NPj have the same head
word. We denote S+ and S- the set of seed pairs
derived from the positive and the negative training
instances, respectively. Note that a seed pair may
possibly belong to S+ can S- at the same time.
For each of the seed NP pairs (Ei:Ej), we search
in a large corpus for the strings that match the reg-
ular expression ?Ei * * * Ej? or ?Ej * * * Ei?,
where * is a wildcard for any word or symbol. The
regular expression is defined as such that all the co-
occurrences of Ei and Ej with at most three words
(or symbols) in between are retrieved.
For each retrieved string, we extract a surface pat-
tern by replacing expression Ei with a mark <#t1#>
and Ej with <#t2#>. If the string is followed by a
symbol, the symbol will be also included in the pat-
tern. This is to create patterns like ?X * * * Y [, . ?]?
where Y, with a high possibility, is the head word,
but not a modifier of another noun phrase.
As an example, consider the pair (?Bill Clin-
ton?:?president?). Suppose that two sentences in a
corpus can be matched by the regular expressions:
530
(S1) ? Bill Clinton is elected President of the
United States.?
(S2) ?The US President, Mr Bill Clinton, to-
day advised India to move towards nuclear non-
proliferation and begin a dialogue with Pakistan to
... ?.
The patterns to be extracted for (S1) and (S2), re-
spectively, are
P1: <#t1#> is elected <#t2#>
P2: <#t2#> , Mr <#t1#> ,
We record the number of strings matched by a pat-
tern p instantiated with (Ei:Ej), noted |(Ei, p, Ej)|,
for later use.
For each seed pair, we generate a list of surface
patterns in the above way. We collect all the pat-
terns derived from the positive seed pairs as a set
of reference patterns, which will be scored and used
to evaluate the semantic relatedness for any new NP
pair.
4.2 Scoring the Patterns
4.2.1 Frequency
One possible scoring scheme is to evaluate a pat-
tern based on its commonality to positive seed pairs.
The intuition here is that the more often a pattern is
seen for the positive seed pairs, the more indicative
the pattern is to find positive coreferential NP pairs.
Based on this idea, we score a pattern by calculating
the number of positive seed pairs whose pattern list
contains the pattern. Formally, supposing the pat-
tern list associated with a seed pair s is PList(s), the
frequency score of a pattern p is defined as
Freqency(p) = |{s|s ? S+, p ? PList(s)}| (1)
4.2.2 Reliability
Another possible way to evaluate a pattern is
based on its reliability, i.e., the degree that the pat-
tern is associated with the positive coreferential NPs.
In our study, we use pointwise mutual informa-
tion (Cover and Thomas, 1991) to measure associ-
ation strength, which has been proved effective in
the task of semantic relation identification (Pantel
and Pennacchiotti, 2006). Under pointwise mutual
information (PMI), the strength of association be-
tween two events x and y is defined as follows:
pmi(x, y) = log P (x, y)P (x)P (y) (2)
Thus the association between a pattern p and a
positive seed pair s:(Ei:Ej) is:
pmi(p, (Ei : Ej)) = log
|(Ei,p,Ej)|
|(?,?,?)|
|(Ei,?,Ej)|
|(?,?,?)|
|(?,p,?)|
|(?,?,?)|
(3)
where |(Ei,p,Ej)| is the count of strings matched
by pattern p instantiated with Ei and Ej . Asterisk *
represents a wildcard, that is:
|(Ei, ?, Ej)| =
?
p?PList(Ei:Ej)
|(Ei, p, Ej)| (4)
|(?, p, ?)| =
?
(Ei:Ej)?S+?S?
|(Ei, p, Ej)| (5)
|(?, ?, ?)| =
?
(Ei:Ej)?S+?S?;p?Plist(Ei:Ej)
|(Ei, p, Ej)| (6)
The reliability of pattern is the average strength of
association across each positive seed pair:
r(p) =
?
s?S+
pmi(p,s)
max pmi
|S + | (7)
Here max pmi is used for the normalization pur-
pose, which is the maximum PMI between all pat-
terns and all positive seed pairs.
4.3 Exploiting the Patterns
4.3.1 Patterns Features
One strategy is to directly use the reference pat-
terns as a set of features for classifier learning and
testing. To select the most effective patterns for
the learner, we rank the patterns according to their
scores and then choose the top patterns (first 100 in
our study) as the features.
As mentioned, the frequency score is based on the
commonality of a pattern to the positive seed pairs.
However, if a pattern also occurs frequently for the
negative seed pairs, it should be not deemed a good
feature as it may lead to many false positive pairs
during real resolution. To take this factor into ac-
count, we filter the patterns based on their accuracy,
which is defined as follows:
Accuracy(p) = |{s|s ? S+, p ? PList(s)}||{s|s ? S + ? S?, p ? PList(s)}| (8)
A pattern with an accuracy below threshold 0.5 is
eliminated from the reference pattern set. The re-
maining patterns are sorted as normal, from which
the top 100 patterns are selected as features.
531
NWire NPaper BNews
R P F R P F R P F
Normal Features 54.5 80.3 64.9 56.6 76.0 64.9 52.7 75.3 62.0
+ ?X such as Y? proper names 55.1 79.0 64.9 56.8 76.1 65.0 52.6 75.1 61.9
all types 55.1 78.3 64.7 56.8 74.7 64.4 53.0 74.4 61.9
+ ?X and other Y? proper names 54.7 79.9 64.9 56.4 75.9 64.7 52.6 74.9 61.8
all types 54.8 79.8 65.0 56.4 75.9 64.7 52.8 73.3 61.4
+ pattern features (frequency) proper names 58.7 75.8 66.2 57.5 73.9 64.7 54.0 71.1 61.4
all types 59.7 67.3 63.3 57.4 62.4 59.8 55.9 57.7 56.8
+ pattern features (filtered frequency) proper names 57.8 79.1 66.8 56.9 75.1 64.7 54.1 72.4 61.9
all types 58.1 77.4 66.4 56.8 71.2 63.2 55.0 68.1 60.9
+ pattern features (PMI reliability) proper names 58.8 76.9 66.6 58.1 73.8 65.0 54.3 72.0 61.9
all types 59.6 70.4 64.6 58.7 61.6 60.1 56.0 58.8 57.4
+ single reliability feature proper names 57.4 80.8 67.1 56.6 76.2 65.0 54.0 74.7 62.7
all types 57.7 76.4 65.7 56.7 75.9 64.9 55.1 69.5 61.5
Table 1: The results of different systems for coreference resolution
Each selected pattern p is used as a single fea-
ture, PFp. For an instance i{NPi, NPj}, a list of
patterns is generated for (Ei:Ej) in the same way as
described in Section 4.1. The value of PFp for the
instance is simply |(Ei, p, Ej)|.
The set of pattern features is used together with
the other normal features to do the learning and test-
ing. Thus, the actual importance of a pattern in
coreference resolution is automatically determined
in a supervised learning way.
4.3.2 Semantic Relatedness Feature
Another strategy is to use only one semantic fea-
ture which is able to reflect the reliability that a NP
pair is related in semantics. Intuitively, a NP pair
with strong semantic relatedness should be highly
associated with as many reliable patterns as possi-
ble. Based on this idea, we define the semantic re-
latedness feature (SRel) as follows:
SRel(i{NPi, NPj}) =
1000 ?
?
p?PList(Ei:Ej)
pmi(p, (Ei : Ej)) ? r(p) (9)
where pmi(p, (Ei:Ej)) is the pointwise mutual in-
formation between pattern p and a NP pair (Ei:Ej),
as defined in Eq. 3. r(p) is the reliability score of p
(Eq. 7). As a relatedness value is always below 1,
we multiple it by 1000 so that the feature value will
be of integer type with a range from 0 to 1000. Note
that among PList(Ei:Ej), only the reference patterns
are involved in the feature computing.
5 Experiments and Discussion
5.1 Experimental setup
In our study we did evaluation on the ACE-2 V1.0
corpus (NIST, 2003), which contains two data set,
training and devtest, used for training and testing re-
spectively. Each of these sets is further divided by
three domains: newswire (NWire), newspaper (NPa-
per), and broadcast news (BNews).
An input raw text was preprocessed automati-
cally by a pipeline of NLP components, includ-
ing sentence boundary detection, POS-tagging, Text
Chunking and Named-Entity Recognition. Two dif-
ferent classifiers were learned respectively for re-
solving pronouns and non-pronouns. As mentioned,
the pattern based semantic information was only ap-
plied to the non-pronoun resolution. For evaluation,
Vilain et al (1995)?s scoring algorithm was adopted
to compute the recall and precision of the whole
coreference resolution.
For pattern extraction and feature computing, we
used Wikipedia, a web-based free-content encyclo-
pedia, as the text corpus. We collected the English
Wikipedia database dump in November 2006 (re-
fer to http://download.wikimedia.org/). After all the
hyperlinks and other html tags were removed, the
whole pure text contains about 220 Million words.
5.2 Results and Discussion
Table 1 lists the performance of different coref-
erence resolution systems. The first line of the
table shows the baseline system that uses only
the common features proposed in (Ng and Cardie,
2002). From the table, our baseline system can
532
NO Frequency Frequency (Filtered) PMI Reliabilty
1 <#t1> <#t2> <#t2> | | <#t1> | <#t1> : <#t2>
2 <#t2> <#t1> <#t1> ) is a <#t2> <#t2> : <#t1>
3 <#t1> , <#t2> <#t1> ) is an <#t2> <#t1> . the <#t2>
4 <#t2> , <#t1> <#t2> ) is an <#t1> <#t2> ( <#t1> )
5 <#t1> . <#t2> <#t2> ) is a <#t1> <#t1> ( <#t2>
6 <#t1> and <#t2> <#t1> or the <#t2> <#t1> ( <#t2> )
7 <#t2> . <#t1> <#t1> ( the <#t2> <#t1> | | <#t2> |
8 <#t1> . the <#t2> <#t1> . during the <#t2> <#t2> | | <#t1> |
9 <#t2> and <#t1> <#t1> | <#t2> <#t2> , the <#t1>
10 <#t1> , the <#t2> <#t1> , an <#t2> <#t1> , the <#t2>
11 <#t2> . the <#t1> <#t1> ) was a <#t2> <#t2> ( <#t1>
12 <#t2> , the <#t1> <#t1> in the <#t2> - <#t1> , <#t2>
13 <#t2> <#t1> , <#t1> - <#t2> <#t1> and the <#t2>
14 <#t1> <#t2> , <#t1> ) was an <#t2> <#t1> . <#t2>
15 <#t1> : <#t2> <#t1> , many <#t2> <#t1> ) is a <#t2>
16 <#t1> <#t2> . <#t2> ) was a <#t1> <#t1> during the <#t2>
17 <#t2> <#t1> . <#t1> ( <#t2> . <#t1> <#t2> .
18 <#t1> ( <#t2> ) <#t2> | <#t1> <#t1> ) is an <#t2>
19 <#t1> and the <#t2> <#t1> , not the <#t2> <#t2> in <#t1> .
20 <#t2> ( <#t1> ) <#t2> , many <#t1> <#t2> , <#t1>
. . . . . . . . . . . .
Table 2: Top patterns chosen under different scoring schemes
achieve a good precision (above 75%-80%) with a
recall around 50%-60%. The overall F-measure for
NWire, NPaper and BNews is 64.9%, 64.9% and
62.0% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure of about 62% for the
same data set.
The rest lines of Table 1 are for the systems us-
ing the pattern based information. In all the sys-
tems, we examine the utility of the semantic infor-
mation in resolving different types of NP Pairs: (1)
NP Pairs containing proper names (i.e., Name:Name
or Name:Definites), and (2) NP Pairs of all types.
In Table 1 (Line 2-5), we also list the results of
incorporating two commonly used patterns, ?X(s)
such as Y? and ?X and other Y(s)?. We can find that
neither of the manually designed patterns has signif-
icant impact on the resolution performance. For all
the domains, the manual patterns just achieve slight
improvement in recall (below 0.6%), indicating that
coverage of the patterns is not broad enough.
5.2.1 Pattern Features
In Section 4.3.1 we propose a strategy that di-
rectly uses the patterns as features. Table 2 lists the
top patterns that are sorted based on frequency, fil-
tered frequency (by accuracy), and PMI reliability,
on the NWire domain for illustration.
From the table, evaluated only based on fre-
quency, the top patterns are those that indicate the
appositive structure like ?X, an/a/the Y?. However,
if filtered by accuracy, patterns of such a kind will
be removed. Instead, the top patterns with both high
frequency and high accuracy are those for the copula
structure, like ?X is/was/are Y?. Sorted by PMI reli-
ability, patterns for the above two structures can be
seen in the top of the list. These results are consis-
tent with the findings in (Cimiano and Staab, 2004)
that the appositive and copula structures are indica-
tive to find the is-a relation. Also, the two commonly
used patterns ?X(s) such as Y? and ?X and other
Y(s)? were found in the feature lists (not shown in
the table). Their importance for coreference resolu-
tion will be determined automatically by the learn-
ing algorithm.
An interesting pattern seen in the lists is ?X || Y |?,
which represents the cases when Y and X appear in
the same of line of a table in Wikipedia. For exam-
ple, the following text
?American || United States | Washington D.C. | . . . ?
is found in the table ?list of empires?. Thus the pair
?American:United States?, which is deemed coref-
erential in ACE, can be identified by the pattern.
The sixth till the eleventh lines of Table 1 list the
results of the system with pattern features. From the
table, adding the pattern features brings the improve-
ment of the recall against the baseline. Take the sys-
tem based on filtered frequency as an example. We
can observe that the recall increases by up to 3.3%
(for NWire). However, we see the precision drops
(up to 1.2% for NWire) at the same time. Over-
all the system achieves an F-measure better than the
baseline in NWire (1.9%), while equal (?0.2%) in
NPaper and BNews.
Among the three ranking schemes, simply using
frequency leads to the lowest precision. By contrast,
using filtered frequency yields the highest precision
with nevertheless the lowest recall. It is reasonable
since the low accuracy features prone to false posi-
533
NameAlias = 1: ...
NameAlias = 0:
:..Appositive = 1: ...
Appositive = 0:
:..P014 > 0:
:...P003 <= 4: 0 (3)
: P003 > 4: 1 (25)
P014 <= 0:
:..P004 > 0:...
P004 <= 0:
:..P027 > 0: 1 (25/7)
P027 <= 0:
:..P002 > 0: ...
P002 <= 0:
:..P005 > 0: 1 (49/22)
P005 <= 0:
:..String_Match = 1: .
String_Match = 0: .
// p002: <t1> ) is a <t2>
// P003: <t1> ) is an <t2>
// P004: <t2> ) is an <t1>
// p005: <t2> ) is a <t1>
// P014: <t1> ) was an <t2>
// p027: <t1> , ( <t2> ,
Figure 1: The decision tree (NWire domain) for the
system using pattern features (filtered frequency)
(feature String Match records whether the string of anaphor
NP j matches that of a candidate antecedent NP i)
tive NP pairs are eliminated, at the price of recall.
Using PMI Reliability can achieve the highest re-
call with a medium level of precision. However, we
do not find significant difference in the overall F-
measure for all these three schemes. This should be
due to the fact that the pattern features need to be
further chosen by the learning algorithm, and only
those patterns deemed effective by the learner will
really matter in the real resolution.
From the table, the pattern features only work
well for NP pairs containing proper names. Ap-
plied on all types of NP pairs, the pattern features
further boost the recall of the systems, but in the
meanwhile degrade the precision significantly. The
F-measure of the systems is even worse than that
of the baseline. Our error analysis shows that a
non-anaphor is often wrongly resolved to a false an-
tecedent once the two NPs happen to satisfy a pat-
tern feature, which affects precision largely (as an
evidence, the decrease of precision is less significant
when using filtered frequency than using frequency).
Still, these results suggest that we just apply the pat-
tern based semantic information in resolving proper
names which, in fact, is more compelling as the se-
mantic information of common nouns could be more
easily retrieved from WordNet.
We also notice that the patterned based semantic
information seems more effective in the NWire do-
main than the other two. Especially for NPaper, the
improvement in F-measure is less than 0.1% for all
the systems tested. The error analysis indicates it
may be because (1) there are less NP pairs in NPa-
per than in NWire that require the external seman-
tic knowledge for resolution; and (2) For many NP
pairs that require the semantic knowledge, no co-
occurrence can be found in the Wikipedia corpus.
To address this problem, we could resort to the Web
which contains a larger volume of texts and thus
could lead to more informative patterns. We would
like to explore this issue in our future work.
In Figure 1, we plot the decision tree learned
with the pattern features for non-pronoun resolution
(NWire domain, filtered frequency), which visually
illustrates which features are useful in the reference
determination. We can find the pattern features oc-
cur in the top of the decision tree, among the features
for name alias, apposition and string-matching that
are crucial for coreference resolution as reported in
previous work (Soon et al, 2001). Most of the pat-
tern features deemed important by the learner are for
the copula structure.
5.2.2 Single Semantic Relatedness Feature
Section 4.3.2 presents another strategy to exploit
the patterns, which uses a single feature to reflect the
semantic relatedness between NP pairs. The last two
lines of Table 1 list the results of such a system.
Observed from the table, the system with the sin-
gle semantic relatedness feature beats those with
other solutions. Compared with the baseline, the
system can get improvement in recall (up to 2.9%
as in NWire), with a similar or even higher preci-
sion. The overall F-measure it produces is 67.1%,
65.0% and 62.7%, better than the baseline in all the
domains. Especially in the NWire domain, we can
see the significant (t-test, p ? 0.05) improvement of
2.1% in F-measure. When applied on All-Type NP
pairs, the degrade of performance is less significant
as using pattern features. The resulting performance
is better than the baseline or equal. Compared with
the systems using the pattern features, it can still
achieve a higher precision and F-measure (with a lit-
tle loss in recall) .
There are several reasons why the single seman-
tic relatedness feature (SRel) can perform better than
the set of pattern features. Firstly, the feature value
of SRel takes into consideration the information of
all the patterns, instead of only the selected patterns.
Secondly, since the SRel feature is computed based
on all the patterns, it reduces the risk of false posi-
534
NameAlias = 1: ...
NameAlias = 0:
:..Appositive = 1: ...
Appositive = 0:
:..SRel > 28:
:..SRel > 47: ...
: SRel <= 47: ...
SRel <= 28:
:..String_Match = 1: ...
String_Match = 0: ...
Figure 2: The decision tree (Nwire) for the system
using the single semantic relatedness feature
tive when a NP pair happens to satisfy one or several
pattern features. Lastly, from the point of view of
machine learning, using only one semantic feature,
instead of hundreds of pattern features, can avoid
overfitting and thus benefit the classifier learning.
In Figure 2, we also show the decision tree learned
with the semantic relatedness feature. We observe
that the decision tree is simpler than that with pat-
tern features as depicted in Figure 1. After feature
name-alias and apposite, the classifier checks dif-
ferent ranges of the SRel value and make different
resolution decision accordingly. This figure further
illustrates the importance of the semantic feature.
6 Conclusions
In this paper we present a pattern based approach to
coreference resolution. Different from the previous
work which utilizes manually designed patterns, our
approach can automatically discover the patterns ef-
fective for the coreference resolution task. In our
study, we explore how to acquire and evaluate pat-
terns, and investigate how to exploit the patterns to
mine semantic relatedness information for corefer-
ence resolution. The evaluation on ACE data set
shows that the patterned based features, when ap-
plied on NP pairs containing proper names, can ef-
fectively help the performance of coreference res-
olution in the recall (up to 4.3%) and the overall
F-measure (up to 2.1%). The results also indicate
that using the single semantic relatedness feature has
more advantages than using a set of pattern features.
For future work, we intend to investigate our
approach in more difficult tasks like the bridging
anaphora resolution, in which the semantic relations
involved are more complicated. Also, we would like
to explore the approach in technical (e.g., biomedi-
cal) domains, where jargons are frequently seen and
the need for external knowledge is more compelling.
Acknowledgements This research is supported by a
Specific Targeted Research Project (STREP) of the European
Union?s 6th Framework Programme within IST call 4, Boot-
strapping Of Ontologies and Terminologies STrategic REsearch
Project (BOOTStrep).
References
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proceed-
ings of NAACL, pages 297?304.
P. Cimiano and S. Staab. 2004. Learning by googling.
SIGKDD Explorations Newsletter, 6(2):24?33.
T. Cover and J. Thomas. 1991. Elements of Information The-
ory. Hohn Wiley & Sons.
N. Garera and D. Yarowsky. 2006. Resolving and generating
definite anaphora by modeling hypernymy using unlabeled
corpora. In Proceedings of CoNLL , pages 37?44.
S. Harabagiu, R. Bunescu, and S. Maiorano. 2001. Text knowl-
edge mining for coreference resolution. In Proceedings of
NAACL, pages 55?62.
M. Hearst. 1998. Automated discovery of wordnet relations. In
Christiane Fellbaum, editor, WordNet: An Electronic Lexical
Database and Some of its Applications. MIT Press, Cam-
bridge, MA.
K. Markert, M. Nissim, and N. Modjeska. 2003. Using the
web for nominal anaphora resolution. In Proceedings of the
EACL workshop on Computational Treatment of Anaphora,
pages 39?46.
N. Modjeska, K. Markert, and M. Nissim. 2003. Using the
web in machine learning for other-anaphora resolution. In
Proceedings of EMNLP, pages 176?183.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of ACL,
pages 104?111, Philadelphia.
V. Ng. 2005. Machine learning for coreference resolution:
From local classification to global ranking. In Proceedings
of ACL, pages 157?164.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging
generic patterns for automatically harvesting semantic rela-
tions. In Proceedings of ACL, pages 113?1200.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceedings of
ACL, pages 143?150.
S. Ponzetto and M. Strube. 2006. Exploiting semantic role
labeling, wordnet and wikipedia for coreference resolution.
In Proceedings of NAACL, pages 192?199.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521?544.
R. Vieira and M. Poesio. 2000. An empirically based system
for processing definite descriptions. Computational Linguis-
tics, 27(4):539?592.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the Sixth Message understand-
ing Conference (MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
535
Proceedings of ACL-08: HLT, pages 843?851,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Entity-Mention Model for Coreference Resolution
with Inductive Logic Programming
Xiaofeng Yang1 Jian Su1 Jun Lang2
Chew Lim Tan3 Ting Liu2 Sheng Li2
1Institute for Infocomm Research
{xiaofengy,sujian}@i2r.a-star.edu.sg
2Harbin Institute of Technology
{bill lang,tliu}@ir.hit.edu.cn
lisheng@hit.edu.cn
3National University of Singapore,
tancl@comp.nus.edu.sg
Abstract
The traditional mention-pair model for coref-
erence resolution cannot capture information
beyond mention pairs for both learning and
testing. To deal with this problem, we present
an expressive entity-mention model that per-
forms coreference resolution at an entity level.
The model adopts the Inductive Logic Pro-
gramming (ILP) algorithm, which provides a
relational way to organize different knowledge
of entities and mentions. The solution can
explicitly express relations between an entity
and the contained mentions, and automatically
learn first-order rules important for corefer-
ence decision. The evaluation on the ACE data
set shows that the ILP based entity-mention
model is effective for the coreference resolu-
tion task.
1 Introduction
Coreference resolution is the process of linking mul-
tiple mentions that refer to the same entity. Most
of previous work adopts the mention-pair model,
which recasts coreference resolution to a binary
classification problem of determining whether or not
two mentions in a document are co-referring (e.g.
Aone and Bennett (1995); McCarthy and Lehnert
(1995); Soon et al (2001); Ng and Cardie (2002)).
Although having achieved reasonable success, the
mention-pair model has a limitation that informa-
tion beyond mention pairs is ignored for training and
testing. As an individual mention usually lacks ad-
equate descriptive information of the referred entity,
it is often difficult to judge whether or not two men-
tions are talking about the same entity simply from
the pair alone.
An alternative learning model that can overcome
this problem performs coreference resolution based
on entity-mention pairs (Luo et al, 2004; Yang et
al., 2004b). Compared with the traditional mention-
pair counterpart, the entity-mention model aims to
make coreference decision at an entity level. Classi-
fication is done to determine whether a mention is a
referent of a partially found entity. A mention to be
resolved (called active mention henceforth) is linked
to an appropriate entity chain (if any), based on clas-
sification results.
One problem that arises with the entity-mention
model is how to represent the knowledge related to
an entity. In a document, an entity may have more
than one mention. It is impractical to enumerate all
the mentions in an entity and record their informa-
tion in a single feature vector, as it would make the
feature space too large. Even worse, the number of
mentions in an entity is not fixed, which would re-
sult in variant-length feature vectors and make trou-
ble for normal machine learning algorithms. A solu-
tion seen in previous work (Luo et al, 2004; Culotta
et al, 2007) is to design a set of first-order features
summarizing the information of the mentions in an
entity, for example, ?whether the entity has any men-
tion that is a name alias of the active mention?? or
?whether most of the mentions in the entity have the
same head word as the active mention?? These fea-
tures, nevertheless, are designed in an ad-hoc man-
ner and lack the capability of describing each indi-
vidual mention in an entity.
In this paper, we present a more expressive entity-
843
mention model for coreference resolution. The
model employs Inductive Logic Programming (ILP)
to represent the relational knowledge of an active
mention, an entity, and the mentions in the entity. On
top of this, a set of first-order rules is automatically
learned, which can capture the information of each
individual mention in an entity, as well as the global
information of the entity, to make coreference deci-
sion. Hence, our model has a more powerful repre-
sentation capability than the traditional mention-pair
or entity-mention model. And our experimental re-
sults on the ACE data set shows the model is effec-
tive for coreference resolution.
2 Related Work
There are plenty of learning-based coreference reso-
lution systems that employ the mention-pair model.
A typical one of them is presented by Soon et al
(2001). In the system, a training or testing instance
is formed for two mentions in question, with a fea-
ture vector describing their properties and relation-
ships. At a testing time, an active mention is checked
against all its preceding mentions, and is linked with
the closest one that is classified as positive. The
work is further enhanced by Ng and Cardie (2002)
by expanding the feature set and adopting a ?best-
first? linking strategy.
Recent years have seen some work on the entity-
mention model. Luo et al (2004) propose a system
that performs coreference resolution by doing search
in a large space of entities. They train a classifier that
can determine the likelihood that an active mention
should belong to an entity. The entity-level features
are calculated with an ?Any-X? strategy: an entity-
mention pair would be assigned a feature X, if any
mention in the entity has the feature X with the ac-
tive mention.
Culotta et al (2007) present a system which uses
an online learning approach to train a classifier to
judge whether two entities are coreferential or not.
The features describing the relationships between
two entities are obtained based on the information
of every possible pair of mentions from the two en-
tities. Different from (Luo et al, 2004), the entity-
level features are computed using a ?Most-X? strat-
egy, that is, two given entities would have a feature
X, if most of the mention pairs from the two entities
have the feature X.
Yang et al (2004b) suggest an entity-based coref-
erence resolution system. The model adopted in the
system is similar to the mention-pair model, except
that the entity information (e.g., the global num-
ber/gender agreement) is considered as additional
features of a mention in the entity.
McCallum and Wellner (2003) propose several
graphical models for coreference analysis. These
models aim to overcome the limitation that pair-
wise coreference decisions are made independently
of each other. The simplest model conditions coref-
erence on mention pairs, but enforces dependency
by calculating the distance of a node to a partition
(i.e., the probability that an active mention belongs
to an entity) based on the sum of its distances to all
the nodes in the partition (i.e., the sum of the prob-
ability of the active mention co-referring with the
mentions in the entity).
Inductive Logic Programming (ILP) has been ap-
plied to some natural language processing tasks, in-
cluding parsing (Mooney, 1997), POS disambigua-
tion (Cussens, 1996), lexicon construction (Claveau
et al, 2003), WSD (Specia et al, 2007), and so on.
However, to our knowledge, our work is the first ef-
fort to adopt this technique for the coreference reso-
lution task.
3 Modelling Coreference Resolution
Suppose we have a document containing n mentions
{mj : 1 < j < n}, in which mj is the jth mention
occurring in the document. Let ei be the ith entity in
the document. We define
P (L|ei,mj), (1)
the probability that a mention belongs to an entity.
Here the random variable L takes a binary value and
is 1 if mj is a mention of ei.
By assuming that mentions occurring after mj
have no influence on the decision of linking mj to
an entity, we can approximate (1) as:
P (L|ei,mj)
? P (L|{mk ? ei, 1 ? k ? j ? 1},mj) (2)
? max
mk?ei,1?k?j?1
P (L|mk,mj) (3)
(3) further assumes that an entity-mention score
can be computed by using the maximum mention-
844
[ Microsoft Corp. ]11 announced [ [ its ]12 new CEO ]23
[ yesterday ]34. [ The company ]15 said [ he ]26 will . . .
Table 1: A sample text
pair score. Both (2) and (1) can be approximated
with a machine learning method, leading to the tra-
ditional mention-pair model and the entity-mention
model for coreference resolution, respectively.
The two models will be described in the next sub-
sections, with the sample text in Table 1 used for
demonstration. In the table, a mention m is high-
lighted as [ m ]eidmid, where mid and eid are the IDs
for the mention and the entity to which it belongs,
respectively. Three entity chains can be found in the
text, that is,
e1 : Microsoft Corp. - its - The company
e2 : its new CEO - he
e3 : yesterday
3.1 Mention-Pair Model
As a baseline, we first describe a learning framework
with the mention-pair model as adopted in the work
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{mk, mj}, in which mj is
an active mention and mk is a preceding mention.
An instance is associated with a vector of features,
which is used to describe the properties of the two
mentions as well as their relationships. Table 2 sum-
marizes the features used in our study.
For training, given each encountered anaphoric
mention mj in a document, one single positive train-
ing instance is created for mj and its closest an-
tecedent. And a group of negative training in-
stances is created for every intervening mentions
between mj and the antecedent. Consider the ex-
ample text in Table 1, for the pronoun ?he?, three
instances are generated: i(?The company?,?he?),
i(?yesterday?,?he?), and i(?its new CEO?,?he?).
Among them, the first two are labelled as negative
while the last one is labelled as positive.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm. During resolution, an input document is
processed from the first mention to the last. For each
encountered mention mj , a test instance is formed
for each preceding mention, mk. This instance is
presented to the classifier to determine the corefer-
ence relationship. mj is linked with the mention that
is classified as positive (if any) with the highest con-
fidence value.
3.2 Entity-Mention Model
The mention-based solution has a limitation that in-
formation beyond a mention pair cannot be captured.
As an individual mention usually lacks complete de-
scription about the referred entity, the coreference
relationship between two mentions may be not clear,
which would affect classifier learning. Consider
a document with three coreferential mentions ?Mr.
Powell?, ?he?, and ?Powell?, appearing in that or-
der. The positive training instance i(?he?, ?Powell?)
is not informative, as the pronoun ?he? itself dis-
closes nothing but the gender. However, if the whole
entity is considered instead of only one mention, we
can know that ?he? refers to a male person named
?Powell?. And consequently, the coreference rela-
tionships between the mentions would become more
obvious.
The mention-pair model would also cause errors
at a testing time. Suppose we have three mentions
?Mr. Powell?, ?Powell?, and ?she? in a document.
The model tends to link ?she? with ?Powell? be-
cause of their proximity. This error can be avoided,
if we know ?Powell? belongs to the entity starting
with ?Mr. Powell?, and therefore refers to a male
person and cannot co-refer with ?she?.
The entity-mention model based on Eq. (2) per-
forms coreference resolution at an entity-level. For
simplicity, the framework considered for the entity-
mention model adopts similar training and testing
procedures as for the mention-pair model. Specif-
ically, a training or testing instance has the form of
i{ei, mj}, in which mj is an active mention and ei
is a partial entity found before mj . During train-
ing, given each anaphoric mention mj , one single
positive training instance is created for the entity to
which mj belongs. And a group of negative train-
ing instances is created for every partial entity whose
last mention occurs between mj and the closest an-
tecedent of mj .
See the sample in Table 1 again. For the pronoun
?he?, the following three instances are generated for
845
Features describing an active mention, mj
defNP mj 1 if mj is a definite description; else 0
indefNP mj 1 if mj is an indefinite NP; else 0
nameNP mj 1 if mj is a named-entity; else 0
pron mj 1 if mj is a pronoun; else 0
bareNP mj 1 if mj is a bare NP (i.e., NP without determiners) ; else 0
Features describing a previous mention, mk
defNP mk 1 if mk is a definite description; else 0
indefNP mk 1 if mk is an indefinite NP; else 0
nameNP mk 1 if mk is a named-entity; else 0
pron mk 1 if mk is a pronoun; else 0
bareNP mk 1 if mk is a bare NP; else 0
subject mk 1 if mk is an NP in a subject position; else 0
Features describing the relationships between mk and mj
sentDist sentence distance between two mentions
numAgree 1 if two mentions match in the number agreement; else 0
genderAgree 1 if two mentions match in the gender agreement; else 0
parallelStruct 1 if two mentions have an identical collocation pattern; else 0
semAgree 1 if two mentions have the same semantic category; else 0
nameAlias 1 if two mentions are an alias of the other; else 0
apposition 1 if two mentions are in an appositive structure; else 0
predicative 1 if two mentions are in a predicative structure; else 0
strMatch Head 1 if two mentions have the same head string; else 0
strMatch Full 1 if two mentions contain the same strings, excluding the determiners; else 0
strMatch Contain 1 if the string of mj is fully contained in that of mk ; else 0
Table 2: Feature set for coreference resolution
entity e1, e3 and e2:
i({?Microsoft Corp.?, ?its?, ?The company?},?he?),
i({?yesterday?},?he?),
i({?its new CEO?},?he?).
Among them, the first two are labelled as negative,
while the last one is positive.
The resolution is done using a greedy clustering
strategy. Given a test document, the mentions are
processed one by one. For each encountered men-
tion mj , a test instance is formed for each partial en-
tity found so far, ei. This instance is presented to the
classifier. mj is appended to the entity that is classi-
fied as positive (if any) with the highest confidence
value. If no positive entity exists, the active mention
is deemed as non-anaphoric and forms a new entity.
The process continues until the last mention of the
document is reached.
One potential problem with the entity-mention
model is how to represent the entity-level knowl-
edge. As an entity may contain more than one candi-
date and the number is not fixed, it is impractical to
enumerate all the mentions in an entity and put their
properties into a single feature vector. As a base-
line, we follow the solution proposed in (Luo et al,
2004) to design a set of first-order features. The fea-
tures are similar to those for the mention-pair model
as shown in Table 2, but their values are calculated
at an entity level. Specifically, the lexical and gram-
matical features are computed by testing any men-
tion1 in the entity against the active mention, for ex-
1Linguistically, pronouns usually have the most direct coref-
ample, the feature nameAlias is assigned value 1 if
at least one mention in the entity is a name alias of
the active mention. The distance feature (i.e., sent-
Dist) is the minimum distance between the mentions
in the entity and the active mention.
The above entity-level features are designed in an
ad-hoc way. They cannot capture the detailed infor-
mation of each individual mention in an entity. In
the next section, we will present a more expressive
entity-mention model by using ILP.
4 Entity-mention Model with ILP
4.1 Motivation
The entity-mention model based on Eq. (2) re-
quires relational knowledge that involves informa-
tion of an active mention (mj), an entity (ei), and
the mentions in the entity ({mk ? ei}). How-
ever, normal machine learning algorithms work on
attribute-value vectors, which only allows the repre-
sentation of atomic proposition. To learn from rela-
tional knowledge, we need an algorithm that can ex-
press first-order logic. This requirement motivates
our use of Inductive Logic Programming (ILP), a
learning algorithm capable of inferring logic pro-
grams. The relational nature of ILP makes it pos-
sible to explicitly represent relations between an en-
tity and its mentions, and thus provides a powerful
expressiveness for the coreference resolution task.
erence relationship with antecedents in a local discourse.
Hence, if an active mention is a pronoun, we only consider the
mentions in its previous two sentences for feature computation.
846
ILP uses logic programming as a uniform repre-
sentation for examples, background knowledge and
hypotheses. Given a set of positive and negative ex-
ample E = E+ ? E?, and a set of background
knowledge K of the domain, ILP tries to induce a
set of hypotheses h that covers most of E+ with no
E?, i.e., K ? h |= E+ and K ? h 6|= E?.
In our study, we choose ALEPH2, an ILP imple-
mentation by Srinivasan (2000) that has been proven
well suited to deal with a large amount of data in
multiple domains. For its routine use, ALEPH fol-
lows a simple procedure to induce rules. It first se-
lects an example and builds the most specific clause
that entertains the example. Next, it tries to search
for a clause more general than the bottom one. The
best clause is added to the current theory and all the
examples made redundant are removed. The proce-
dure repeats until all examples are processed.
4.2 Apply ILP to coreference resolution
Given a document, we encode a mention or a par-
tial entity with a unique constant. Specifically, mj
represents the jth mention (e.g., m6 for the pronoun
?he?). ei j represents the partial entity i before the
jth mention. For example, e1 6 denotes the part of
e1 before m6, i.e., {?Microsoft Corp.?, ?its?, ?the
company?}, while e1 5 denotes the part of e1 be-
fore m5 (?The company?), i.e., {?Microsoft Corp.?,
?its?}.
Training instances are created as described in Sec-
tion 3.2 for the entity-mention model. Each instance
is recorded with a predicate link(ei j , mj), where mj
is an active mention and ei j is a partial entity. For
example, the three training instances formed by the
pronoun ?he? are represented as follows:
link(e1 6,m6).
link(e3 6,m6).
link(e2 6,m6).
The first two predicates are put into E?, while the
last one is put to E+.
The background knowledge for an instance
link(ei j , mj) is also represented with predicates,
which are divided into the following types:
1. Predicates describing the information related to
ei j and mj . The properties of mj are pre-
2http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/aleph toc.html
sented with predicates like f (m, v), where f
corresponds to a feature in the first part of Ta-
ble 2 (removing the suffix mj), and v is its
value. For example, the pronoun ?he? can be
described by the following predicates:
defNP(m6, 0). indefNP(m6, 0).
nameNP(m6, 0). pron(m6, 1).
bareNP(m6, 0).
The predicates for the relationships between
ei j and mj take a form of f (e, m, v). In our
study, we consider the number agreement (ent-
NumAgree) and the gender agreement (entGen-
derAgree) between ei j and mj . v is 1 if all
of the mentions in ei j have consistent num-
ber/gender agreement with mj , e.g,
entNumAgree(e1 6,m6, 1).
2. Predicates describing the belonging relations
between ei j and its mentions. A predicate
has mention(e, m) is used for each mention in
e 3. For example, the partial entity e1 6 has
three mentions, m1, m2 and m5, which can be
described as follows:
has mention(e1 6,m1).
has mention(e1 6,m2).
has mention(e1 6,m5).
3. Predicates describing the information related to
mj and each mention mk in ei j . The predi-
cates for the properties of mk correspond to the
features in the second part of Table 2 (removing
the suffix mk), while the predicates for the re-
lationships between mj and mk correspond to
the features in the third part of Table 2. For ex-
ample, given the two mentions m1 (?Microsoft
Corp.) and m6 (?he), the following predicates
can be applied:
nameNP(m1, 1).
pron(m1, 0).
. . .
nameAlias(m1,m6, 0).
sentDist(m1,m6, 1).
. . .
the last two predicates represent that m1 and
3If an active mention mj is a pronoun, only the previous
mentions in two sentences apart are recorded by has mention,
while the farther ones are ignored as they have less impact on
the resolution of the pronoun.
847
m6 are not name alias, and are one sentence
apart.
By using the three types of predicates, the dif-
ferent knowledge related to entities and mentions
are integrated. The predicate has mention acts as
a bridge connecting the entity-mention knowledge
and the mention-pair knowledge. As a result, when
evaluating the coreference relationship between an
active mention and an entity, we can make use of
the ?global? information about the entity, as well as
the ?local? information of each individual mention
in the entity.
From the training instances and the associated
background knowledge, a set of hypotheses can be
automatically learned by ILP. Each hypothesis is
output as a rule that may look like:
link(A,B):-
predi1, predi2, . . . , has mention(A,C), . . . , prediN.
which corresponds to first-order logic
?A,B(predi1 ? predi2 ? . . .?
?C(has mention(A,C) ? . . . ? prediN)
? link(A,B))
Consider an example rule produced in our system:
link(A,B) :-
has mention(A,C), numAgree(B,C,1),
strMatch Head(B,C,1), bareNP(C,1).
Here, variables A and B stand for an entity and an
active mention in question. The first-order logic is
implemented by using non-instantiated arguments C
in the predicate has mention. This rule states that a
mention B should belong to an entity A, if there ex-
ists a mention C in A such that C is a bare noun
phrase with the same head string as B, and matches
in number with B. In this way, the detailed informa-
tion of each individual mention in an entity can be
captured for resolution.
A rule is applicable to an instance link(e, m), if
the background knowledge for the instance can be
described by the predicates in the body of the rule.
Each rule is associated with a score, which is the
accuracy that the rule can produce for the training
instances.
The learned rules are applied to resolution in a
similar way as described in Section 3.2. Given an
active mention m and a partial entity e, a test in-
stance link(e, m) is formed and tested against every
rule in the rule set. The confidence that m should
Train Test
#entity #mention #entity #mention
NWire 1678 9861 411 2304
NPaper 1528 10277 365 2290
BNews 1695 8986 468 2493
Table 3: statistics of entities (length > 1) and contained
mentions
belong to e is the maximal score of the applicable
rules. An active mention is linked to the entity with
the highest confidence value (above 0.5), if any.
5 Experiments and Results
5.1 Experimental Setup
In our study, we did evaluation on the ACE-2003
corpus, which contains two data sets, training and
devtest, used for training and testing respectively.
Each of these sets is further divided into three do-
mains: newswire (NWire), newspaper (NPaper), and
broadcast news (BNews). The number of entities
with more than one mention, as well as the number
of the contained mentions, is summarized in Table 3.
For both training and resolution, an input raw
document was processed by a pipeline of NLP
modules including Tokenizer, Part-of-Speech tag-
ger, NP Chunker and Named-Entity (NE) Recog-
nizer. Trained and tested on Penn WSJ TreeBank,
the POS tagger could obtain an accuracy of 97% and
the NP chunker could produce an F-measure above
94% (Zhou and Su, 2000). Evaluated for the MUC-
6 and MUC-7 Named-Entity task, the NER mod-
ule (Zhou and Su, 2002) could provide an F-measure
of 96.6% (MUC-6) and 94.1%(MUC-7). For evalu-
ation, Vilain et al (1995)?s scoring algorithm was
adopted to compute recall and precision rates.
By default, the ALEPH algorithm only generates
rules that have 100% accuracy for the training data.
And each rule contains at most three predicates. To
accommodate for coreference resolution, we loos-
ened the restrictions to allow rules that have above
50% accuracy and contain up to ten predicates. De-
fault parameters were applied for all the other set-
tings in ALEPH as well as other learning algorithms
used in the experiments.
5.2 Results and Discussions
Table 4 lists the performance of different corefer-
ence resolution systems. For comparison, we first
848
NWire NPaper BNews
R P F R P F R P F
C4.5
- Mention-Pair 68.2 54.3 60.4 67.3 50.8 57.9 66.5 59.5 62.9
- Entity-Mention 66.8 55.0 60.3 64.2 53.4 58.3 64.6 60.6 62.5
- Mention-Pair (all mentions in entity) 66.7 49.3 56.7 65.8 48.9 56.1 66.5 47.6 55.4
ILP
- Mention-Pair 66.1 54.8 59.5 65.6 54.8 59.7 63.5 60.8 62.1
- Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5
Table 4: Results of different systems for coreference resolution
examined the C4.5 algorithm4 which is widely used
for the coreference resolution task. The first line of
the table shows the baseline system that employs the
traditional mention-pair model (MP) as described in
Section 3.1. From the table, our baseline system
achieves a recall of around 66%-68% and a preci-
sion of around 50%-60%. The overall F-measure
for NWire, NPaper and BNews is 60.4%, 57.9% and
62.9% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure ranging in 50-60% for
the same data set. As our system relies only on sim-
ple and knowledge-poor features, the achieved F-
measure is around 2-4% lower than the state-of-the-
art systems do, like (Ng, 2007) and (Yang and Su,
2007) which utilized sophisticated semantic or real-
world knowledge. Since ILP has a strong capability
in knowledge management, our system could be fur-
ther improved if such helpful knowledge is incorpo-
rated, which will be explored in our future work.
The second line of Table 4 is for the system
that employs the entity-mention model (EM) with
?Any-X? based entity features, as described in Sec-
tion 3.2. We can find that the EM model does not
show superiority over the baseline MP model. It
achieves a higher precision (up to 2.6%), but a lower
recall (2.9%), than MP. As a result, we only see
?0.4% difference between the F-measure. The re-
sults are consistent with the reports by Luo et al
(2004) that the entity-mention model with the ?Any-
X? first-order features performs worse than the nor-
mal mention-pair model. In our study, we also tested
the ?Most-X? strategy for the first-order features as
in (Culotta et al, 2007), but got similar results with-
out much difference (?0.5% F-measure) in perfor-
4http://www.rulequest.com/see5-info.html
mance. Besides, as with our entity-mention predi-
cates described in Section 4.2, we also tried the ?All-
X? strategy for the entity-level agreement features,
that is, whether all mentions in a partial entity agree
in number and gender with an active mention. How-
ever, we found this bring no improvement against
the ?Any-X? strategy.
As described, given an active mention mj , the MP
model only considers the mentions between mj and
its closest antecedent. By contrast, the EM model
considers not only these mentions, but also their an-
tecedents in the same entity link. We were interested
in examining what if the MP model utilizes all the
mentions in an entity as the EM model does. As
shown in the third line of Table 4, such a solution
damages the performance; while the recall is at the
same level, the precision drops significantly (up to
12%) and as a result, the F-measure is even lower
than the original MP model. This should be because
a mention does not necessarily have direct corefer-
ence relationships with all of its antecedents. As the
MP model treats each mention-pair as an indepen-
dent instance, including all the antecedents would
produce many less-confident positive instances, and
thus adversely affect training.
The second block of the table summarizes the per-
formance of the systems with ILP. We were first con-
cerned with how well ILP works for the mention-
pair model, compared with the normally used algo-
rithm C4.5. From the results shown in the fourth
line of Table 4, ILP exhibits the same capability in
the resolution; it tends to produce a slightly higher
precision but a lower recall than C4.5 does. Overall,
it performs better in F-measure (1.8%) for Npaper,
while slightly worse (<1%) for Nwire and BNews.
These results demonstrate that ILP could be used as
849
link(A,B) :-
bareNP(B,0), has mention(A,C), appositive(C,1).
link(A,B) :-
has mention(A,C), numAgree(B,C,1), strMatch Head(B,C,1), bareNP(C,1).
link(A,B) :-
nameNP(B,0), has mention(A,C), predicative(C,1).
link(A,B) :-
has mention(A,C), strMatch Contain(B,C,1), strMatch Head(B,C,1), bareNP(C,0).
link(A,B) :-
nameNP(B,0), has mention(A,C), nameAlias(C,1), bareNP(C,0).
link(A,B) :-
pron(B,1), has mention(A,C), nameNP(C,1), has mention(A,D), indefNP(D,1),
subject(D, 1).
...
Figure 1: Examples of rules produced by ILP (entity-
mention model)
a good classifier learner for the mention-pair model.
The fifth line of Table 4 is for the ILP based entity-
mention model (described in Section 4.2). We can
observe that the model leads to a better performance
than all the other models. Compared with the sys-
tem with the MP model (under ILP), the EM version
is able to achieve a higher precision (up to 4.6% for
BNews). Although the recall drops slightly (up to
1.8% for BNews), the gain in the precision could
compensate it well; it beats the MP model in the
overall F-measure for all three domains (2.3% for
Nwire, 0.4% for Npaper, 1.4% for BNews). Es-
pecially, the improvement in NWire and BNews is
statistically significant under a 2-tailed t test (p <
0.05). Compared with the EM model with the man-
ually designed first-order feature (the second line),
the ILP-based EM solution also yields better perfor-
mance in precision (with a slightly lower recall) as
well as the overall F-measure (1.0% - 1.8%).
The improvement in precision against the
mention-pair model confirms that the global infor-
mation beyond a single mention pair, when being
considered for training, can make coreference rela-
tions clearer and help classifier learning. The bet-
ter performance against the EM model with heuristi-
cally designed features also suggests that ILP is able
to learn effective first-order rules for the coreference
resolution task.
In Figure 1, we illustrate part of the rules pro-
duced by ILP for the entity-mention model (NWire
domain), which shows how the relational knowledge
of entities and mentions is represented for decision
making. An interesting finding, as shown in the last
rule of the table, is that multiple non-instantiated ar-
guments (i.e. C and D) could possibly appear in
the same rule. According to this rule, a pronominal
mention should be linked with a partial entity which
contains a named-entity and contains an indefinite
NP in a subject position. This supports the claims
in (Yang et al, 2004a) that coreferential informa-
tion is an important factor to evaluate a candidate an-
tecedent in pronoun resolution. Such complex logic
makes it possible to capture information of multi-
ple mentions in an entity at the same time, which is
difficult to implemented in the mention-pair model
and the ordinary entity-mention model with heuris-
tic first-order features.
6 Conclusions
This paper presented an expressive entity-mention
model for coreference resolution by using Inductive
Logic Programming. In contrast to the traditional
mention-pair model, our model can capture infor-
mation beyond single mention pairs for both training
and testing. The relational nature of ILP enables our
model to explicitly express the relations between an
entity and its mentions, and to automatically learn
the first-order rules effective for the coreference res-
olution task. The evaluation on ACE data set shows
that the ILP based entity-model performs better than
the mention-pair model (with up to 2.3% increase in
F-measure), and also beats the entity-mention model
with heuristically designed first-order features.
Our current work focuses on the learning model
that calculates the probability of a mention be-
longing to an entity. For simplicity, we just use a
greedy clustering strategy for resolution, that is, a
mention is linked to the current best partial entity.
In our future work, we would like to investigate
more sophisticated clustering methods that would
lead to global optimization, e.g., by keeping a large
search space (Luo et al, 2004) or using integer
programming (Denis and Baldridge, 2007).
Acknowledgements This research is supported
by a Specific Targeted Research Project (STREP)
of the European Union?s 6th Framework Programme
within IST call 4, Bootstrapping Of Ontologies and
Terminologies STrategic REsearch Project (BOOT-
Strep).
850
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 122?129.
V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003.
Learning semantic lexicons from a part-of-speech and
semantically tagged corpus using inductive logic pro-
gramming. Journal of Machine Learning Research,
4:493?525.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolution.
In Proceedings of the Annual Meeting of the North
America Chapter of the Association for Computational
Linguistics (NAACL), pages 81?88.
J. Cussens. 1996. Part-of-speech disambiguation using
ilp. Technical report, Oxford University Computing
Laboratory.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North America Chapter of the Association for
Computational Linguistics (NAACL), pages 236?243.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
135?142.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proceedings of IJCAI-
03 Workshop on Information Integration on the Web,
pages 79?86.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of
the 14th International Conference on Artificial Intel-
ligences (IJCAI), pages 1050?1055.
R. Mooney. 1997. Inductive logic programming for nat-
ural language processing. In Proceedings of the sixth
International Inductive Logic Programming Work-
shop, pages 3?24.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 104?111,
Philadelphia.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
157?164.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 536?543.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
L. Specia, M. Stevenson, and M. V. Nunes. 2007. Learn-
ing expressive models for words sense disambiguation.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
41?48.
A. Srinivasan. 2000. The aleph manual. Technical re-
port, Oxford University Computing Laboratory.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage understanding Conference (MUC-6), pages 45?
52, San Francisco, CA. Morgan Kaufmann Publishers.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automati-
cally discovered patterns. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 528?535.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004a. Improv-
ing pronoun resolution by incorporating coreferential
information of candidates. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 127?134, Barcelona.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004b. An
NP-cluster approach to coreference resolution. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 219?225, Geneva.
G. Zhou and J. Su. 2000. Error-driven HMM-based
chunk tagger with context-dependent lexicon. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 71?79, Hong Kong.
G. Zhou and J. Su. 2002. Named Entity recognition us-
ing a HMM-based chunk tagger. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 473?480, Philadel-
phia.
851
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
BART: A Modular Toolkit for Coreference Resolution
Yannick Versley
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Simone Paolo Ponzetto
EML Research gGmbH
ponzetto@eml-research.de
Massimo Poesio
University of Essex
poesio@essex.ac.uk
Vladimir Eidelman
Columbia University
vae2101@columbia.edu
Alan Jern
UCLA
ajern@ucla.edu
Jason Smith
Johns Hopkins University
jsmith@jhu.edu
Xiaofeng Yang
Inst. for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Alessandro Moschitti
University of Trento
moschitti@dit.unitn.it
Abstract
Developing a full coreference system able
to run all the way from raw text to seman-
tic interpretation is a considerable engineer-
ing effort, yet there is very limited avail-
ability of off-the shelf tools for researchers
whose interests are not in coreference, or for
researchers who want to concentrate on a
specific aspect of the problem. We present
BART, a highly modular toolkit for de-
veloping coreference applications. In the
Johns Hopkins workshop on using lexical
and encyclopedic knowledge for entity dis-
ambiguation, the toolkit was used to ex-
tend a reimplementation of the Soon et al
(2001) proposal with a variety of additional
syntactic and knowledge-based features, and
experiment with alternative resolution pro-
cesses, preprocessing tools, and classifiers.
1 Introduction
Coreference resolution refers to the task of identify-
ing noun phrases that refer to the same extralinguis-
tic entity in a text. Using coreference information
has been shown to be beneficial in a number of other
tasks, including information extraction (McCarthy
and Lehnert, 1995), question answering (Morton,
2000) and summarization (Steinberger et al, 2007).
Developing a full coreference system, however, is
a considerable engineering effort, which is why a
large body of research concerned with feature en-
gineering or learning methods (e.g. Culotta et al
2007; Denis and Baldridge 2007) uses a simpler but
non-realistic setting, using pre-identified mentions,
and the use of coreference information in summa-
rization or question answering techniques is not as
widespread as it could be. We believe that the avail-
ability of a modular toolkit for coreference will sig-
nificantly lower the entrance barrier for researchers
interested in coreference resolution, as well as pro-
vide a component that can be easily integrated into
other NLP applications.
A number of systems that perform coreference
resolution are publicly available, such as GUITAR
(Steinberger et al, 2007), which handles the full
coreference task, and JAVARAP (Qiu et al, 2004),
which only resolves pronouns. However, literature
on coreference resolution, if providing a baseline,
usually uses the algorithm and feature set of Soon
et al (2001) for this purpose.
Using the built-in maximum entropy learner
with feature combination, BART reaches 65.8%
F-measure on MUC6 and 62.9% F-measure on
MUC7 using Soon et al?s features, outperforming
JAVARAP on pronoun resolution, as well as the
Soon et al reimplementation of Uryupina (2006).
Using a specialized tagger for ACE mentions and
an extended feature set including syntactic features
(e.g. using tree kernels to represent the syntactic
relation between anaphor and antecedent, cf. Yang
et al 2006), as well as features based on knowledge
extracted from Wikipedia (cf. Ponzetto and Smith, in
preparation), BART reaches state-of-the-art results
on ACE-2. Table 1 compares our results, obtained
using this extended feature set, with results from
Ng (2007). Pronoun resolution using the extended
feature set gives 73.4% recall, coming near special-
ized pronoun resolution systems such as (Denis and
Baldridge, 2007).
9
Figure 1: Results analysis in MMAX2
2 System Architecture
The BART toolkit has been developed as a tool to
explore the integration of knowledge-rich features
into a coreference system at the Johns Hopkins Sum-
mer Workshop 2007. It is based on code and ideas
from the system of Ponzetto and Strube (2006), but
also includes some ideas from GUITAR (Steinberger
et al, 2007) and other coreference systems (Versley,
2006; Yang et al, 2006). 1
The goal of bringing together state-of-the-art ap-
proaches to different aspects of coreference res-
olution, including specialized preprocessing and
syntax-based features has led to a design that is very
modular. This design provides effective separation
of concerns across several several tasks/roles, in-
cluding engineering new features that exploit dif-
ferent sources of knowledge, designing improved or
specialized preprocessing methods, and improving
the way that coreference resolution is mapped to a
machine learning problem.
Preprocessing To store results of preprocessing
components, BART uses the standoff format of the
MMAX2 annotation tool (Mu?ller and Strube, 2006)
with MiniDiscourse, a library that efficiently imple-
ments a subset of MMAX2?s functions. Using a
generic format for standoff annotation allows the use
of the coreference resolution as part of a larger sys-
tem, but also performing qualitative error analysis
using integrated MMAX2 functionality (annotation
1An open source version of BART is available from
http://www.sfs.uni-tuebingen.de/?versley/BART/.
diff, visual display).
Preprocessing consists in marking up noun
chunks and named entities, as well as additional in-
formation such as part-of-speech tags and merging
these information into markables that are the start-
ing point for the mentions used by the coreference
resolution proper.
Starting out with a chunking pipeline, which
uses a classical combination of tagger and chun-
ker, with the Stanford POS tagger (Toutanova et al,
2003), the YamCha chunker (Kudoh and Mat-
sumoto, 2000) and the Stanford Named Entity Rec-
ognizer (Finkel et al, 2005), the desire to use richer
syntactic representations led to the development of
a parsing pipeline, which uses Charniak and John-
son?s reranking parser (Charniak and Johnson, 2005)
to assign POS tags and uses base NPs as chunk
equivalents, while also providing syntactic trees that
can be used by feature extractors. BART also sup-
ports using the Berkeley parser (Petrov et al, 2006),
yielding an easy-to-use Java-only solution.
To provide a better starting point for mention de-
tection on the ACE corpora, the Carafe pipeline
uses an ACE mention tagger provided by MITRE
(Wellner and Vilain, 2006). A specialized merger
then discards any base NP that was not detected to
be an ACE mention.
To perform coreference resolution proper, the
mention-building module uses the markables cre-
ated by the pipeline to create mention objects, which
provide an interface more appropriate for corefer-
ence resolution than the MiniDiscourse markables.
These objects are grouped into equivalence classes
by the resolution process and a coreference layer is
written into the document, which can be used for de-
tailed error analysis.
Feature Extraction BART?s default resolver goes
through all mentions and looks for possible an-
tecedents in previous mentions as described by Soon
et al (2001). Each pair of anaphor and candi-
date is represented as a PairInstance object,
which is enriched with classification features by fea-
ture extractors, and then handed over to a machine
learning-based classifier that decides, given the fea-
tures, whether anaphor and candidate are corefer-
ent or not. Feature extractors are realized as sepa-
rate classes, allowing for their independent develop-
10
Figure 2: Example system configuration
ment. The set of feature extractors that the system
uses is set in an XML description file, which allows
for straightforward prototyping and experimentation
with different feature sets.
Learning BART provides a generic abstraction
layer that maps application-internal representations
to a suitable format for several machine learning
toolkits: One module exposes the functionality of
the the WEKA machine learning toolkit (Witten
and Frank, 2005), while others interface to special-
ized state-of-the art learners. SVMLight (Joachims,
1999), in the SVMLight/TK (Moschitti, 2006) vari-
ant, allows to use tree-valued features. SVM Classi-
fication uses a Java Native Interface-based wrapper
replacing SVMLight/TK?s svm classify pro-
gram to improve the classification speed. Also in-
cluded is a Maximum entropy classifier that is
based upon Robert Dodier?s translation of Liu and
Nocedal?s (1989) L-BFGS optimization code, with
a function for programmatic feature combination.2
Training/Testing The training and testing phases
slightly differ from each other. In the training phase,
the pairs that are to be used as training examples
have to be selected in a process of sample selection,
whereas in the testing phase, it has to be decided
which pairs are to be given to the decision function
and how to group mentions into equivalence rela-
tions given the classifier decisions.
This functionality is factored out into the en-
2see http://riso.sourceforge.net
coder/decoder component, which is separate from
feature extraction and machine learning itself. It
is possible to completely change the basic behav-
ior of the coreference system by providing new
encoders/decoders, and still rely on the surround-
ing infrastructure for feature extraction and machine
learning components.
3 Using BART
Although BART is primarily meant as a platform for
experimentation, it can be used simply as a corefer-
ence resolver, with a performance close to state of
the art. It is possible to import raw text, perform
preprocessing and coreference resolution, and either
work on the MMAX2-format files, or export the re-
sults to arbitrary inline XML formats using XSL
stylesheets.
Adapting BART to a new coreferentially anno-
tated corpus (which may have different rules for
mention extraction ? witness the differences be-
tween the annotation guidelines of MUC and ACE
corpora) usually involves fine-tuning of mention cre-
ation (using pipeline and MentionFactory settings),
as well as the selection and fine-tuning of classi-
fier and features. While it is possible to make rad-
ical changes in the preprocessing by re-engineering
complete pipeline components, it is usually possi-
ble to achieve the bulk of the task by simply mix-
ing and matching existing components for prepro-
cessing and feature extraction, which is possible by
modifying only configuration settings and an XML-
11
BNews NPaper NWire
Recl Prec F Recl Prec F Recl Prec F
basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533
extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627
Ng 2007? 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633
?: ?expanded feature set? in Ng 2007; Ng trains on the entire ACE training corpus.
Table 1: Performance on ACE-2 corpora, basic vs. extended feature set
based description of the feature set and learner(s)
used.
Several research groups focusing on coreference
resolution, including two not involved in the ini-
tial creation of BART, are using it as a platform
for research including the use of new information
sources (which can be easily incorporated into the
coreference resolution process as features), different
resolution algorithms that aim at enhancing global
coherence of coreference chains, and also adapting
BART to different corpora. Through the availability
of BART as open source, as well as its modularity
and adaptability, we hope to create a larger com-
munity that allows both to push the state of the art
further and to make these improvements available to
users of coreference resolution.
Acknowledgements We thank the CLSP at Johns
Hopkins, NSF and the Department of Defense for
ensuring funding for the workshop and to EML
Research, MITRE, the Center for Excellence in
HLT, and FBK-IRST, that provided partial support.
Yannick Versley was supported by the Deutsche
Forschungsgesellschaft as part of SFB 441 ?Lin-
guistic Data Structures?; Simone Paolo Ponzetto has
been supported by the Klaus Tschira Foundation
(grant 09.003.2004).
References
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc. ACL
2005.
Culotta, A., Wick, M., and McCallum, A. (2007). First-order
probabilistic models for coreference resolution. In Proc.
HLT/NAACL 2007.
Denis, P. and Baldridge, J. (2007). A ranking approach to pro-
noun resolution. In Proc. IJCAI 2007.
Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorpo-
rating non-local information into information extraction sys-
tems by Gibbs sampling. In Proc. ACL 2005, pages 363?370.
Joachims, T. (1999). Making large-scale SVM learning prac-
tical. In Scho?lkopf, B., Burges, C., and Smola, A., editors,
Advances in Kernel Methods - Support Vector Learning.
Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector
Machines for chunk identification. In Proc. CoNLL 2000.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
method for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees
for coreference resolution. In Proc. IJCAI 1995.
Morton, T. S. (2000). Coreference for NLP applications. In
Proc. ACL 2000.
Moschitti, A. (2006). Making tree kernels practical for natural
language learning. In Proc. EACL 2006.
Mu?ller, C. and Strube, M. (2006). Multi-level annotation of
linguistic data with MMAX2. In Braun, S., Kohn, K., and
Mukherjee, J., editors, Corpus Technology and Language
Pedagogy: New Resources, New Tools, New Methods. Peter
Lang, Frankfurt a.M., Germany.
Ng, V. (2007). Shallow semantics for coreference resolution. In
Proc. IJCAI 2007.
Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006). Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL 2006.
Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. HLT/NAACL 2006.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference
implementation of the RAP anaphora resolution algorithm.
In Proc. LREC 2004.
Soon, W. M., Ng, H. T., and Lim, D. C. Y. (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).
Two uses of anaphora resolution in summarization. Informa-
tion Processing and Management, 43:1663?1680. Special
issue on Summarization.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proc. NAACL 2003, pages 252?259.
Uryupina, O. (2006). Coreference resolution with and without
linguistic knowledge. In Proc. LREC 2006.
Versley, Y. (2006). A constraint-based approach to noun phrase
coreference resolution in German newspaper text. In Kon-
ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS
2006).
Wellner, B. and Vilain, M. (2006). Leveraging machine read-
able dictionaries in discriminative sequence models. In Proc.
LREC 2006.
Witten, I. and Frank, E. (2005). Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
Yang, X., Su, J., and Tan, C. L. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
CoLing/ACL-2006.
12
An NP-Cluster Based Approach to Coreference Resolution
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Traditionally, coreference resolution is done
by mining the reference relationships be-
tween NP pairs. However, an individual NP
usually lacks adequate description informa-
tion of its referred entity. In this paper,
we propose a supervised learning-based ap-
proach which does coreference resolution by
exploring the relationships between NPs and
coreferential clusters. Compared with indi-
vidual NPs, coreferential clusters could pro-
vide richer information of the entities for bet-
ter rules learning and reference determina-
tion. The evaluation done on MEDLINE
data set shows that our approach outper-
forms the baseline NP-NP based approach
in both recall and precision.
1 Introduction
Coreference resolution is the process of linking
as a cluster1 multiple expressions which refer
to the same entities in a document. In recent
years, supervised machine learning approaches
have been applied to this problem and achieved
considerable success (e.g. Aone and Bennett
(1995); McCarthy and Lehnert (1995); Soon et
al. (2001); Ng and Cardie (2002b)). The main
idea of most supervised learning approaches is to
recast this task as a binary classification prob-
lem. Specifically, a classifier is learned and then
used to determine whether or not two NPs in a
document are co-referring. Clusters are formed
by linking coreferential NP pairs according to a
certain selection strategy. In this way, the identi-
fication of coreferential clusters in text is reduced
to the identification of coreferential NP pairs.
One problem of such reduction, however,
is that the individual NP usually lacks ade-
quate descriptive information of its referred en-
tity. Consequently, it is often difficult to judge
whether or not two NPs are talking about the
1In this paper the term ?cluster? can be interchange-
ably used as ?chain?, while the former better emphasizes
the equivalence property of coreference relationship.
same entity simply from the properties of the
pair alone. As an example, consider the pair of a
non-pronoun and its pronominal antecedent can-
didate. The pronoun itself gives few clues for the
reference determination. Using such NP pairs
would have a negative influence for rules learn-
ing and subsequent resolution. So far, several
efforts (Harabagiu et al, 2001; Ng and Cardie,
2002a; Ng and Cardie, 2002b) have attempted to
address this problem by discarding the ?hard?
pairs and select only those confident ones from
the NP-pair pool. Nevertheless, this eliminat-
ing strategy still can not guarantee that the NPs
in ?confident? pairs bear necessary description
information of their referents.
In this paper, we present a supervised
learning-based approach to coreference resolu-
tion. Rather than attempting to mine the ref-
erence relationships between NP pairs, our ap-
proach does resolution by determining the links
of NPs to the existing coreferential clusters. In
our approach, a classifier is trained on the in-
stances formed by an NP and one of its possi-
ble antecedent clusters, and then applied dur-
ing resolution to select the proper cluster for an
encountered NP to be linked. As a coreferen-
tial cluster offers richer information to describe
an entity than a single NP in the cluster, we
could expect that such an NP-Cluster framework
would enhance the resolution capability of the
system. Our experiments were done on the the
MEDLINE data set. Compared with the base-
line approach based on NP-NP framework, our
approach yields a recall improvement by 4.6%,
with still a precision gain by 1.3%. These results
indicate that the NP-Cluster based approach is
effective for the coreference resolution task.
The remainder of this paper is organized as
follows. Section 2 introduces as the baseline the
NP-NP based approach, while Section 3 presents
in details our NP-Cluster based approach. Sec-
tion 4 reports and discusses the experimental re-
sults. Section 5 describes related research work.
Finally, conclusion is given in Section 6.
2 Baseline: the NP-NP based
approach
2.1 Framework description
We built a baseline coreference resolution sys-
tem, which adopts the common NP-NP based
learning framework as employed in (Soon et al,
2001).
Each instance in this approach takes the form
of i{NPj , NPi}, which is associated with a fea-
ture vector consisting of 18 features (f1 ? f18) as
described in Table 2. Most of the features come
from Soon et al (2001)?s system. Inspired by the
work of (Strube et al, 2002) and (Yang et al,
2004), we use two features, StrSim1 (f17) and
StrSim2 (f18), to measure the string-matching
degree of NPj and NPi. Given the following sim-
ilarity function:
Str Simlarity(Str1, Str2) = 100? |Str1 ? Str2|Str1
StrSim1 and StrSim2 are computed
using Str Similarity(SNPj , SNPi) and
Str Similarity(SNPi , SNPj ), respectively. Here
SNP is the token list of NP, which is obtained
by applying word stemming, stopword removal
and acronym expansion to the original string as
described in Yang et al (2004)?s work.
During training, for each anaphor NPj in a
given text, a positive instance is generated by
pairing NPj with its closest antecedent. A set
of negative instances is also formed by NPj and
each NP occurring between NPj and NPi.
When the training instances are ready, a clas-
sifier is learned by C5.0 algorithm (Quinlan,
1993). During resolution, each encountered noun
phrase, NPj , is paired in turn with each preced-
ing noun phrase, NPi. For each pair, a test-
ing instance is created as during training, and
then presented to the decision tree, which re-
turns a confidence value (CF)2 indicating the
likelihood that NPi is coreferential to NPj . In
our study, two antecedent selection strategies,
Most Recent First (MRF) and Best First (BF),
are tried to link NPj to its a proper antecedent
with CF above a threshold (0.5). MRF (Soon
et al, 2001) selects the candidate closest to the
anaphor, while BF (Aone and Bennett, 1995; Ng
2The confidence value is obtained by using the
smoothed ratio p+1t+2 , where p is the number of positiveinstances and t is the total number of instances contained
in the corresponding leaf node.
and Cardie, 2002b) selects the candidate with
the maximal CF.
2.2 Limitation of the approach
Nevertheless, the problem of the NP-NP based
approach is that the individual NP usually lacks
adequate description information about its re-
ferred entity. Consequently, it is often difficult
to determine whether or not two NPs refer to
the same entity simply from the properties of
the pair. See the the text segment in Table 1,
for example,
[1 A mutant of [2 KBF1/p50] ], unable to
bind to DNA but able to form homo- or [3 het-
erodimers] , has been constructed.
[4 This protein] reduces or abolishes the DNA
binding activity of wild-type proteins of [5 the
same family ([6 KBF1/p50] , c- and v-rel)].
[7 This mutant] also functions in vivo as a
transacting dominant negative regulator:. . .
Table 1: An Example from the data set
In the above text, [1 A mutant of KBF1/p50],
[4 This protein] and [7 This mutant] are anno-
tated in the same coreferential cluster. Accord-
ing to the above framework, NP7 and its closest
antecedent, NP4, will form a positive instance.
Nevertheless, such an instance is not informa-
tive in that NP4 bears little information related
to the entity and thus provides few clues to ex-
plain its coreference relationship with NP7.
In fact, this relationship would be clear if [1 A
mutant of KBF1/p50], the antecedent of NP4,
is taken into consideration. NP1 gives a de-
tailed description of the entity. By comparing
the string of NP7 with this description, it is ap-
parent that NP7 belongs to the cluster of NP1,
and thus should be coreferential to NP4. This
suggests that we use the coreferential cluster,
instead of its single element, to resolve an NP
correctly. In our study, we propose an approach
which adopts an NP-Cluster based framework to
do resolution. The details of the approach are
given in the next section.
3 The NP-Cluster based approach
Similar to the baseline approach, our approach
also recasts coreference resolution as a binary
classification problem. The difference, however,
is that our approach aims to learn a classifier
which would select the most preferred cluster,
instead of the most preferred antecedent, for an
encountered NP in text. We will give the frame-
work of the approach, including the instance rep-
Features describing the relationships between NPj and NPi
1. DefNp 1 1 if NPj is a definite NP; else 0
2. DemoNP 1 1 if NPj starts with a demonstrative; else 0
3. IndefNP 1 1 if NPj is an indefinite NP; else 0
4. Pron 1 1 if NPj is a pronoun; else 0
5. ProperNP 1 1 if NPj is a proper NP; else 0
6. DefNP 2 1 if NPi is a definite NP; else 0
7. DemoNP 2 1 if NPi starts with a demonstrative; else 0
8. IndefNP 2 1 if NPi is an indefinite NP; else 0
9. Pron 2 1 if NPi is a pronoun; else 0
10. ProperNP 2 1 if NPi is a proper NP; else 0
11. Appositive 1 if NPi and NPj are in an appositive structure; else 0
12. NameAlias 1 if NPi and NPj are in an alias of the other; else 0
13. GenderAgree 1 if NPi and NPj agree in gender; else 0
14. NumAgree 1 if NPi and NPj agree in number; else 0
15. SemanticAgree 1 if NPi and NPj agree in semantic class; else 0
16. HeadStrMatch 1 if NPi and NPj contain the same head string; else 0
17. StrSim 1 The string similarity of NPj against NPi
18. StrSim 2 The string similarity of NPi against NPj
Features describing the relationships between NPj and cluster Ck
19. Cluster NumAgree 1 if Ck and NPj agree in number; else 0
20. Cluster GenAgree 1 if Ck and NPj agree in gender; else 0
21. Cluster SemAgree 1 if Ck and NPj agree in semantic class; else 0
22. Cluster Length The number of elements contained in Ck
23. Cluster StrSim The string similarity of NPj against Ck
24. Cluster StrLNPSim The string similarity of NPj against the longest NP in Ck
Table 2: The features in our coreference resolution system (Features 1 ? 18 are also used in the
baseline system using NP-NP based approach)
resentation, the training and the resolution pro-
cedures, in the following subsections.
3.1 Instance representation
An instance in our approach is composed of three
elements like below:
i{NPj , Ck, NPi}
where NPj , like the definition in the baseline,
is the noun phrase under consideration, while Ck
is an existing coreferential cluster. Each cluster
could be referred by a reference noun phrase NPi,
a certain element of the cluster. A cluster would
probably contain more than one reference NPs
and thus may have multiple associated instances.
For a training instance, the label is positive if
NPj is annotated as belonging to Ck, or negative
if otherwise.
In our system, each instance is represented as
a set of 24 features as shown in Table 2. The
features are supposed to capture the properties
of NPj and Ck as well as their relationships. In
the table we divide the features into two groups,
one describing NPj and NPi and the other de-
scribing NPj and Ck. For the former group, we
just use the same features set as in the baseline
system, while for the latter, we introduce 6 more
features:
Cluster NumAgree, Cluster GenAgree
and Cluster SemAgree: These three fea-
tures mark the compatibility of NPj and Ck
in number, gender and semantic agreement,
respectively. If NPj mismatches the agreement
with any element in Ck, the corresponding
feature is set to 0.
Cluster Length: The number of NPs in the
cluster Ck. This feature reflects the global
salience of an entity in the sense that the more
frequently an entity is mentioned, the more im-
portant it would probably be in text.
Cluster StrSim: This feature marks the string
similarity between NPj and Ck. Suppose
SNPj is the token set of NPj , we compute
the feature value using the similarity function
Str Similarity(SNPj , SCk), where
SCk =
?
NPi?Ck
SNPi
Cluster StrLNPSim: It marks the string
matching degree of NPj and the noun phrase
in Ck with the most number of tokens. The
intuition here is that the NP with the longest
string would probably bear richer description in-
formation of the referent than other elements in
the cluster. The feature is calculated using the
similarity function Str Similarity(SNPj , SNPk),
where
NPk = arg maxNPi?Ck |SNPi |
3.2 Training procedure
Given an annotated training document, we pro-
cess the noun phrases from beginning to end.
For each anaphoric noun phrase NPj , we consider
its preceding coreferential clusters from right to
left3. For each cluster, we create only one in-
stance by taking the last NP in the cluster as
the reference NP. The process will not terminate
until the cluster to which NPj belongs is found.
To make it clear, consider the example in Ta-
ble 1 again. For the noun phrase [7 This mu-
tant], the annotated preceding coreferential clus-
ters are:
C1: { . . . , NP2, NP6 }
C2: { . . . , NP5 }
C3: { NP1, NP4 }
C4: { . . . , NP3 }
Thus three training instances are generated:
i{ NP7, C1, NP6 }
i{ NP7, C2, NP5 }
i{ NP7, C3, NP4 }
Among them, the first two instances are la-
belled as negative while the last one is positive.
After the training instances are ready, we use
C5.0 learning algorithm to learn a decision tree
classifier as in the baseline approach.
3.3 Resolution procedure
The resolution procedure is the counterpart of
the training procedure. Given a testing docu-
ment, for each encountered noun phrase, NPj ,
we create a set of instances by pairing NPj with
each cluster found previously. The instances are
presented to the learned decision tree to judge
the likelihood that NPj is linked to a cluster.
The resolution algorithm is given in Figure 1.
As described in the algorithm, for each clus-
ter under consideration, we create multiple in-
stances by using every NP in the cluster as the
reference NP. The confidence value of the cluster
3We define the position of a cluster as the position of
the last NP in the cluster.
algorithm RESOLVE (a testing document d)
ClusterSet = ?;
//suppose d has N markable NPs;
for j = 1 to N
foreach cluster in ClusterSet
CFcluster = maxNPi?clusterCFi(NPj ,cluster,NPi)
select a proper cluster, BestCluster, according
to a ceterin cluster selection strategy;
if BestCluster != NULL
BestCluster = BestCluster ? {NPj};
else
//create a new cluster
NewCluster = { NPj };
ClusterSet = ClusterSet ? {NewCluster};
Figure 1: The clusters identification algorithm
is the maximal confidence value of its instances.
Similar to the baseline system, two cluster selec-
tion strategies, i.e. MRF and BF, could be ap-
plied to link NPj to a proper cluster. For MRF
strategy, NPj is linked to the closest cluster with
confidence value above 0.5, while for BF, it is
linked to the cluster with the maximal confidence
value (above 0.5).
3.4 Comparison of NP-NP and
NP-Cluster based approaches
As noted above, the idea of the NP-Cluster based
approach is different from the NP-NP based ap-
proach. However, due to the fact that in our
approach a cluster is processed based on its refer-
ence NPs, the framework of our approach could
be reduced to the NP-NP based framework if
the cluster-related features were removed. From
this point of view, this approach could be con-
sidered as an extension of the baseline approach
by applying additional cluster features as the
properties of NPi. These features provide richer
description information of the entity, and thus
make the coreference relationship between two
NPs more apparent. In this way, both rules
learning and coreference determination capabili-
ties of the original approach could be enhanced.
4 Evaluation
4.1 Data collection
Our coreference resolution system is a compo-
nent of our information extraction system in
biomedical domain. For this purpose, an anno-
tated coreference corpus have been built 4, which
4The annotation scheme and samples are avail-
able in http://nlp.i2r.a-star.edu.sg/resources/GENIA-
coreference
MRF BF
Experiments R P F R P F
Baseline 80.2 77.4 78.8 80.3 77.5 78.9
AllAnte 84.4 70.2 76.6 85.7 71.4 77.9
Our Approach 84.4 78.2 81.2 84.9 78.8 81.7
Table 3: The performance of different coreference resolution systems
consists of totally 228 MEDLINE abstracts se-
lected from the GENIA data set. The aver-
age length of the documents in collection is 244
words. One characteristic of the bio-literature
is that pronouns only occupy about 3% among
all the NPs. This ratio is quite low compared
to that in newswire domain (e.g. above 10% for
MUC data set).
A pipeline of NLP components is applied to
pre-process an input raw text. Among them,
NE recognition, part-of-speech tagging and text
chunking adopt the same HMM based engine
with error-driven learning capability (Zhou and
Su, 2002). The NE recognition component
trained on GENIA (Shen et al, 2003) can
recognize up to 23 common biomedical entity
types with an overall performance of 66.1 F-
measure (P=66.5% R=65.7%). In addition, to
remove the apparent non-anaphors (e.g., em-
bedded proper nouns) in advance, a heuristic-
based non-anaphoricity identification module is
applied, which successfully removes 50.0% non-
anaphors with a precision of 83.5% for our data
set.
4.2 Experiments and discussions
Our experiments were done on first 100 docu-
ments from the annotated corpus, among them
70 for training and the other 30 for testing.
Throughout these experiments, default learning
parameters were applied in the C5.0 algorithm.
The recall and precision were calculated auto-
matically according to the scoring scheme pro-
posed by Vilain et al (1995).
In Table 3 we compared the performance of
different coreference resolution systems. The
first line summarizes the results of the baseline
system using traditional NP-NP based approach
as described in Section 2. Using BF strategy,
Baseline obtains 80.3% recall and 77.5% preci-
sion. These results are better than the work by
Castano et al (2002) and Yang et al (2004),
which were also tested on the MEDLINE data
set and reported a F-measure of about 74% and
69%, respectively.
In the experiments, we evaluated another NP-
NP based system, AllAnte. It adopts a similar
learning framework as Baseline except that dur-
ing training it generates the positive instances by
paring an NP with all its antecedents instead of
only the closest one. The system attempts to use
such an instance selection strategy to incorpo-
rate the information from coreferential clusters.
But the results are nevertheless disappointing:
although this strategy boosts the recall by 5.4%,
the precision drops considerably by above 6% at
the same time. The overall F-measure is even
lower than the baseline systems.
The last line of Table 3 demonstrates the re-
sults of our NP-Cluster based approach. For BF
strategy, the system achieves 84.9% recall and
78.8% precision. As opposed to the baseline sys-
tem, the recall rises by 4.6% while the precision
still gains slightly by 1.3%. Overall, we observe
the increase of F-measure by 2.8%.
The results in Table 3 also indicate that the
BF strategy is superior to the MRF strategy.
A similar finding was also reported by Ng and
Cardie (2002b) in the MUC data set.
To gain insight into the difference in the per-
formance between our NP-Cluster based system
and the NP-NP based system, we compared the
decision trees generated in the two systems in
Figure 2. In both trees, the string-similarity
features occur on the top portion, which sup-
ports the arguments by (Strube et al, 2002)
and (Yang et al, 2004) that string-matching is a
crucial factor for NP coreference resolution. As
shown in the figure, the feature StrSim 1 in left
tree is completely replaced by the Cluster StrSim
and Cluster StrLNPSim in the right tree, which
means that matching the tokens with a cluster
is more reliable than with a single NP. More-
over, the cluster length will also be checked when
the NP under consideration has low similarity
against a cluster. These evidences prove that
the information from clusters is quite important
for the coreference resolution on the data set.
The decision tree visualizes the importance of
the features for a data set. However, the tree is
learned from the documents where coreferential
clusters are correctly annotated. During resolu-
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
: NameAlias = 0:
: :...Appositive = 0: 0 (13095/265)
: Appositive = 1: 1 (15/4)
HeadMatch = 1:
:...StrSim_1 > 71:
:...DemoNP_1 = 0: 1 (615/29)
: DemoNP_1 = 1:
: :...NumAgree = 0: 0 (5)
: NumAgree = 1: 1 (26)
StrSim_1 <= 71:
:...DemoNP_2 = 1: 1 (12/2)
DemoNP_2 = 0:
:...StrSim_2 <= 77: 0 (144/17)
StrSim_2 > 77:
:...StrSim_1 <= 33: 0 (42/11)
StrSim_1 > 33: 1 (38/11)
HeadMatch = 1:
:...Cluster_StrSim > 66: 1 (663/36)
: Cluster_StrSim <= 66:
: :...StrSim_2 <= 85: 0 (140/14)
: StrSim_2 > 85:
: :...Cluster_StrLNPSim > 50: 1 (16/1)
: Cluster_StrLNPSim <= 50:
: :...Cluster_Length <= 5: 0 (59/17)
: Cluster_Length > 5: 1 (4)
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
NameAlias = 0:
:...Appositive = 1: 1 (15/4)
Appositive = 0:
:...StrSim_2 <= 54:
:..
StrSim_2 > 54:
:..
Figure 2: The resulting decision trees for the NP-NP and NP-Cluster based approaches
Features R P F
f1?21 80.3 77.5 78.9
f1?21, f22 84.1 74.4 79.0
f1?21, f23 84.7 78.8 81.6
f1?21, f24 84.3 78.0 81.0
f1?21, f23, f22 84.9 78.6 81.6
f1?21, f23, f24 84.9 78.9 81.8
f1?21, f23, f24, f22 84.9 78.8 81.7
Table 4: Performance using combined features
(fi refers to the i(th) feature listed in Table 2)
tion, unfortunately, the found clusters are usu-
ally not completely correct, and as a result the
features important in training data may not be
also helpful for testing data. Therefore, in the
experiments we were concerned about which fea-
tures really matter for the real coreference res-
olution. For this purpose, we tested our system
using different features and evaluated their per-
formance in Table 4. Here we just considered fea-
ture Cluster Length (f22), Cluster StrSim (f23)
and Cluster StrLNPSim (f24), as Figure 2 has
indicated that among the cluster-related features
only these three are possibly effective for resolu-
tion. Throughout the experiment, the Best-First
strategy was applied.
As illustrated in the table, we could observe
that:
1. Without the three features, the system is
equivalent to the baseline system in terms
of the same recall and precision.
2. Cluster StrSim (f23) is the most effective
as it contributes most to the system per-
formance. Simply using this feature boosts
the F-measure by 2.7%.
3. Cluster StrLNPSim (f24) is also effective by
improving the F-measure by 2.1% alone.
When combined with f23, it leads to the
best F-measure.
4. Cluster Length (f22) only brings 0.1% F-
measure improvement. It could barely
increase, or even worse, reduces the F-
measure when used together with the the
other two features.
5 Related work
To our knowledge, our work is the first
supervised-learning based attempt to do coref-
erence resolution by exploring the relationship
between an NP and coreferential clusters. In the
heuristic salience-based algorithm for pronoun
resolution, Lappin and Leass (1994) introduce
a procedure for identifying anaphorically linked
NP as a cluster for which a global salience value
is computed as the sum of the salience values of
its elements. Cardie and Wagstaff (1999) have
proposed an unsupervised approach which also
incorporates cluster information into considera-
tion. Their approach uses hard constraints to
preclude the link of an NP to a cluster mismatch-
ing the number, gender or semantic agreements,
while our approach takes these agreements to-
gether with other features (e.g. cluster-length,
string-matching degree,etc) as preference factors
for cluster selection. Besides, the idea of cluster-
ing can be seen in the research of cross-document
coreference, where NPs with high context simi-
larity would be chained together based on certain
clustering methods (Bagga and Biermann, 1998;
Gooi and Allan, 2004).
6 Conclusion
In this paper we have proposed a supervised
learning-based approach to coreference resolu-
tion. Rather than mining the coreferential re-
lationship between NP pairs as in conventional
approaches, our approach does resolution by ex-
ploring the relationships between an NP and the
coreferential clusters. Compared to individual
NPs, coreferential clusters provide more infor-
mation for rules learning and reference determi-
nation. In the paper, we first introduced the con-
ventional NP-NP based approach and analyzed
its limitation. Then we described in details the
framework of our NP-Cluster based approach,
including the instance representation, training
and resolution procedures. We evaluated our ap-
proach in the biomedical domain, and the experi-
mental results showed that our approach outper-
forms the NP-NP based approach in both recall
(4.6%) and precision (1.3%).
While our approach achieves better perfor-
mance, there is still room for further improve-
ment. For example, the approach just resolves
an NP using the cluster information available so
far. Nevertheless, the text after the NP would
probably give important supplementary infor-
mation of the clusters. The ignorance of such
information may affect the correct resolution of
the NP. In the future work, we plan to work out
more robust clustering algorithm to link an NP
to a globally best cluster.
References
C. Aone and S. W. Bennett. 1995. Evaluating
automated and manual acquistion of anaphora
resolution strategies. In Proceedings of the
33rd Annual Meeting of the Association for
Compuational Linguistics, pages 122?129.
A. Bagga and A. Biermann. 1998. Entity-based
cross document coreferencing using the vector
space model. In Proceedings of the 36th An-
nual Meeting of the Association for Computa-
tional Linguisticsthe 17th International Con-
ference on Computational Linguistics, pages
79?85.
C. Cardie and K. Wagstaff. 1999. Noun phrase
coreference as clustering. In Proceedings of
the Joint Conference on Empirical Methods in
NLP and Very Large Corpora.
J. Castano, J. Zhang, and J. Pustejovsky. 2002.
Anaphora resolution in biomedical literature.
In International Symposium on Reference Res-
olution, Alicante, Spain.
C. Gooi and J. Allan. 2004. Cross-document
coreference on a large scale corpus. In Pro-
ceedings of 2004 Human Language Technology
conference / North American chapter of the
Association for Computational Linguistics an-
nual meeting.
S. Harabagiu, R. Bunescu, and S. Maiorano.
2001. Text knowledge mining for coreference
resolution. In Proceedings of the 2nd An-
nual Meeting of the North America Chapter of
the Association for Compuational Linguistics,
pages 55?62.
S. Lappin and H. Leass. 1994. An algorithm
for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):525?561.
J. McCarthy and Q. Lehnert. 1995. Using de-
cision trees for coreference resolution. In Pro-
ceedings of the 14th International Conference
on Artificial Intelligences, pages 1050?1055.
V. Ng and C. Cardie. 2002a. Combining sam-
ple selection and error-driven pruning for ma-
chine learning of coreference rules. In Proceed-
ings of the conference on Empirical Methods
in Natural Language Processing, pages 55?62,
Philadelphia.
V. Ng and C. Cardie. 2002b. Improving ma-
chine learning approaches to coreference res-
olution. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 104?111, Philadelphia.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publishers,
San Francisco, CA.
D. Shen, J. Zhang, G. Zhou, J. Su, and
C. Tan. 2003. Effective adaptation of hid-
den markov model-based named-entity recog-
nizer for biomedical domain. In Proceedings of
ACL03 Workshop on Natural Language Pro-
cessing in Biomedicine, Japan.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference resolu-
tion of noun phrases. Computational Linguis-
tics, 27(4):521?544.
M. Strube, S. Rapp, and C. Muller. 2002. The
influence of minimum edit distance on refer-
ence resolution. In Proceedings of the Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 312?319, Philadel-
phia.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan. 2004. Im-
proving noun phrase coreference resolution by
matching strings. In Proceedings of the 1st In-
ternational Joint Conference on Natural Lan-
guage Processing, Hainan.
G. Zhou and J. Su. 2002. Named Entity recog-
nition using a HMM-based chunk tagger. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, Philadelphia.
A Twin-Candidate Model of Coreference
Resolution with Non-Anaphor
Identification Capability
Xiaofeng Yang1,2, Jian Su1, and Chew Lim Tan2
1 Institute for Infocomm Research,
21, Heng Mui Keng Terrace, Singapore, 119613
{xiaofengy, sujian}@i2r.a-star.edu.sg
2 Department of Computer Science,
National University of Singapore, Singapore, 117543
{yangxiao, tancl}@comp.nus.edu.sg
Abstract. Although effective for antecedent determination, the tradi-
tional twin-candidate model can not prevent the invalid resolution of
non-anaphors without additional measures. In this paper we propose a
modified learning framework for the twin-candidate model. In the new
framework, we make use of non-anaphors to create a special class of
training instances, which leads to a classifier capable of identifying the
cases of non-anaphors during resolution. In this way, the twin-candidate
model itself could avoid the resolution of non-anaphors, and thus could
be directly deployed to coreference resolution. The evaluation done on
newswire domain shows that the twin-candidate based system with our
modified framework achieves better and more reliable performance than
those with other solutions.
1 Introduction
In recent years supervised learning approaches have been widely used in corefer-
ence resolution task and achieved considerable success [1,2,3,4,5]. Most of these
approaches adopt the single-candidate learning model, in which coreference rela-
tion is determined between a possible anaphor and one individual candidate at a
time [1,3,4]. However, it has been claimed that the reference between an anaphor
and its candidate is often subject to the other competing candidates [5]. Such
information is nevertheless difficult to be captured in the single-candidate model.
As an alternative, several researchers proposed a twin-candidate model [2,5,6].
Instead of directly determining coreference relations, this model would judge the
preference between candidates and then select the most preferred one as the an-
tecedent. The previous work has reported that such a model can effectively help
antecedent determination for anaphors [5,6].
However, one problem exits with the twin-candidate model. For every encoun-
tered NP during resolution, the model would always pick out a ?best? candidate
as the antecedent, even if the current NP is not an anaphor. The twin-candidate
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 719?730, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
720 X. Yang, J. Su, and C.L. Tan
model itself could not identify and block such invalid resolution of non-anaphors.
Therefore, to apply such a model to coreference resolution, some additional ef-
forts have to be required, e.g., using an anaphoricity determination module to
eliminate non-anaphors in advance [5], or using threshold to prevent the selection
of a candidate if the confidence it wins other competitors is low [6].
In this paper, we explore how to effectively apply the twin-candidate model
to the coreference resolution task. We propose a modified learning framework
with the capability of processing non-anaphors. In the framework, we make use of
non-anaphors to create training instances. This special class of instances would
enable the learned classifier to identify the test instances formed by non-anaphors
during resolution. Thus, the resulting model could avoid resolving a non-anaphor
to a non-existent antecedent by itself, without specifying a threshold or using an
additional anaphoricity determination module. Our experiments on MUC data
set systematically evaluated effectiveness of our modified learning framework.
We found that with this new framework, the twin-candidate based system could
not only outperform the single-candidate based one, but also achieve better and
more reliable results than those twin-candidate based systems using the two
mentioned solutions.
The rest of the paper is organized as follows. Section 2 describes the original
framework of the twin-candidate model. Section 3 presents in details the modified
framework, including the training and resolution procedures. Section 4 reports
and discusses the experimental results and finally Section 6 gives the conclusions.
2 The Original Framework of the Twin-Candidate Model
The basic idea of the twin-candidate model is to learn a binary classifier which
could judge the preference between candidates of an anaphor. In this section we
will describe a general framework of such a model.
2.1 Instance Representation
In the twin-candidate model, an instance takes a form like i{C1, C2, M }, where
M is a possible anaphor and C1 and C2 are two of its antecedent candidates.
We stipulate that C2 should be closer to M than C1 in distance. An instance is
labelled as ?10? if C1 is preferred to C2 to be the antecedent, or ?01? if otherwise.
A feature vector would be specified for an instance. The features may describe
the lexical, syntactic, semantic and positional relationships between M and each
one of the candidates, C1 or C2. In addition, inter-candidate features could
be used to represent the relationships between the pair of candidates, e.g. the
distance between C1 and C2 in position.
2.2 Training Procedure
For each anaphor Mana in a given training text, its closet antecedent, Cante,
would be selected as the anchor candidate to compare with other candidates.
A Twin-Candidate Model of Coreference Resolution 721
A set of ?10? instances, i{Cante, Cp, Mana}, is generated by pairing Mana and
Cante, as well as each of the interning candidates Cp. Also a set of ?01? instances,
i{Ca, Cante, Mana}, is created by pairing Cante and each non-antecedental can-
didate Ca before Cante.
Table 1. An example text
[1 Globalstar] still needs to raise [2 $600 million], and [3
Schwartz] said [4 that company] would try to raise [5 the money]
in [6 the debt market] .
Consider the example in Table 1. In the text segment, [4 that company] and
[5 the money] are two anaphors with [1 Globalstar] and [2 $600 million] being
their antecedents respectively. Thus the training instances to be created for this
text would be:
i{[1 Globalstar], [2 $600 million], [4 that company]} : 10
i{[1 Globalstar], [3 Schwartz], [4 that company]} : 10
i{[1 Globalstar], [2 $600 million], [5 the money]} : 01
i{[2 $600 million], [3 Schwartz], [5 the money]} : 10
i{[2 $600 million], [4 that company], [5 the money]} : 10
Based on the training instances, a classifier is trained using a certain machine
learning algorithm. Given the feature vector of a test instance, the classifier
would return ?10? or ?01? indicating which one of the two candidates under
consideration is preferred.
2.3 Resolution
After the classifier is ready, it could be employed to select the antecedent for
an encountered anaphor. The resolution algorithm is shown in Figure 1. In the
algorithm, a round-robin model is employed, in which each candidate is compared
with every other candidate and the final winner is determined by the won-lost
records. The round-robin model would be fair for each competitor and the result
is reliable to represent the rank of the candidates.
As described in the algorithm, after each match between two candidates,
the record of the winning candidate (i.e., the one judged as preferred by the
classifier) will increase and that of the loser will decrease. The algorithm simply
uses a unit of one as the increment and decrement. Therefore, the final record of
a candidate is its won-lost difference in the round-robin matches. Alternatively,
we can use the confidence value returned by the classifier as the in(de)crement,
while we found no much performance difference between these two recording
strategies in experiments.
722 X. Yang, J. Su, and C.L. Tan
algorithm ANTE-SEL
input:
M : the anaphor to be resolved
candidate set: the set of antecedent candidates of M,
{C1, C2, . . . , Ck}
for i = 1 to K
Score[ i ] = 0;
for j = K downto 2
for i = j - 1 downto 1
/*CR returns the classification result*/
if CR(i{Ci, Cj, M}) ) = = 10 then
Score[ i ]++;
Score[ j ]??;
if CR(i{Ci, Cj, M}) ) = = 01 then
Score[ i ]??;
Score[ j ]++;
SelectedIdx = arg
i
max
Ci?candidate set
Score[i];
return CSelectedIdx
Fig. 1. The original antecedent selection algorithm
3 Modified Framework for Coreference Resolution Task
3.1 Non-anaphor Processing
In the task of coreference resolution, it is often that an encountered NP is non-
anaphoric, that is, no antecedent exists among its possible candidates. However,
the resolution algorithm described in the previous section would always try to
pick out a ?best? candidate as the antecedent for each given NP, and thus could
not be applied for coreference resolution directly.
One natural solution to this is to use an anaphoricity determination (AD)
module to identify the non-anaphoric NPs in advance (e.g. [5]). If an NP is judged
as anaphoric, then we deploy the resolution algorithm to find its antecedent.
Otherwise we just leave the NP unresolved. This solution, however, would heavily
rely on the performance of the AD module. Unfortunately, the accuracy that
most state-of-the-art AD systems could provide is still not high enough (around
80% as reported in [7]) for our coreference resolution task.
Another possible solution is to set a threshold to avoid selecting a candidate
that wins with low confidence (e.g. [6]). Specifically, for two candidates in a
match, we update their match records only if the confidence returned from the
classifier is above the specified threshold. If no candidate has a positive record in
the end, we deem the NP in question as non-anaphoric and leave it unresolved.
In other words, a NP would be resolved to a candidate only if the candidate won
at least one competitor with confidence above the threshold.
The assumption under this solution is that the classifier would return low con-
fidence for the test instances formed by non-anaphors. Although it may be true,
A Twin-Candidate Model of Coreference Resolution 723
there exist other cases for which the classifier would also assign low confidence
values, for example, when the two candidates of an anaphoric NP both have
strong or weak preference. The solution of using threshold could not discrimi-
nate these different cases and thus may not be reliable for coreference resolution.
In fact, the above problem could be addressed if we could teach the classi-
fier to explicitly identify the cases of non-anaphors, instead of using threshold
implicitly. To do this, we need to provide a special set of instances formed by
the non-anaphors to train the classifier. Given a test instance formed by a non-
anaphor, the newly learned classifier is supposed to give a class label different
from the instances formed by anaphors. This special label would indicate that
the current NP is a non-anaphor, and no preference relationship is held be-
tween the two candidates under consideration. In this way, the twin-candidate
model could do the anaphoricity determination by itself, without any additional
pre-possessing module. We will describe the modified training and resolution
procedures in the subsequent subsections.
3.2 Training
In the modified learning framework, an instance also takes a form like i{C1,
C2, M }. During training, for an encountered anaphor, we create ?01? or ?10?
training instances in the same way as in the original learning framework, while
for a non-anaphor Mnon ana, we
? From the candidate set, randomly select a candidate Crand as the anchor
candidate.
? Create an instance by pairing Mnon ana, Crand, and each of the candidates
other than Crand.
The above instances formed by non-anaphors would be labelled as ?00?. Note that
an instance may have a form like i{Ca, Crand, Mnon ana} if candidate Ca is pre-
ceding Crand, or like i{Crand, Cp, Mnon ana} if candidate Cp is following Crand.
Consider the text in Table 1 again. For the non-anaphors [3 Schwartz] and
[6 the debt market], supposing the selected anchor candidates are [1 Globalstar]
and [2 $600 million], respectively. The ?00? instances generated for the text are:
i{[1 Globalstar], [2 $600 million], [3 Schwartz]} : 00
i{[1 Globalstar], [2 $600 million], [6 the debt market]} : 00
i{[2 $600 million], [3 Schwartz], [6 the debt market]} : 00
i{[2 $600 million], [4 that company], [6 the debt market]} : 00
i{[2 $600 million], [5 the money], [6 the debt market]} : 00
3.3 Resolution
The ?00? training instances are used together with the ?01? and ?10? ones to
train a classifier. The resolution procedure is described in Figure 2. Like in the
original algorithm, each candidate is compared with every other candidate. The
724 X. Yang, J. Su, and C.L. Tan
difference is that, if two candidates are judged as ?00? in a match, both candi-
dates would receive a penalty of ?1 in their respective record; If no candidate
has a positive final score, then the NP would be deemed as non-anaphoric and
left unresolved. Otherwise, it would be resolved to the candidate with highest
score as usual. In the case when an NP has only one antecedent candidate, a
pseudo-instance is created by paring the candidate with itself. The NP would be
resolved to the candidate if the return label is not ?00?.
Note that in the algorithm a threshold could still be used, for example, to
update the match record only if the classification confidence is high enough.
algorithm ANTE-SEL
input:
M : the new NP to be resolved
candidate set: the candidates set of M, {C1, C2, . . . , Ck}
for i = 1 to K
Score[ i ] = 0;
for j = K downto 2
for i = j - 1 downto 1
if CR(i{Ci, Cj, M}) ) = = 10 then
Score[ i ]++;
Score[ j ]??;
if CR(i{Ci, Cj, M}) ) = = 01 then
Score[ i ]??;
Score[ j ]++;
if CR(i{Ci, Cj, M}) ) = = 00 then
Score[ i ]??;
Score[ j ]??;
SelectedIdx = arg
i
max
Ci?candidate set
Score[i];
if (Score[SelectedIdx] <= 0)
return nil;
return CSelectedIdx;
Fig. 2. The new antecedent selection algorithm
4 Evaluation and Discussion
4.1 Experiment Setup
The experiments were done on the newswire domain, using MUC coreference
data set (Wall Street Journal articles). For MUC-6 [8] and MUC-7 [9], 30 ?dry-
run? documents were used for training as well as 20-30 documents for testing.
In addition, another 100 annotated documents from MUC-6 corpus were also
prepared for the purpose of deeper system analysis. Throughout the experiments,
C5 was used as the learning algorithm [10]. The recall and precision rates of
the coreference resolution systems were calculated based on the scoring scheme
proposed by Vilain et al [11].
A Twin-Candidate Model of Coreference Resolution 725
Table 2. Features for coreference resolution using the twin-candidate model
Features describing the new markable M :
1. M DefNP 1 if M is a definite NP; else 0
2. M IndefNP 1 if M is an indefinite NP; else 0
3. M ProperNP 1 if M is a proper noun; else 0
4. M Pronoun 1 if M is a pronoun; else 0
Features describing the candidate, C1 or C2, of M
5. candi DefNp 1(2) 1 if C1 (C2) is a definite NP; else 0
6. candi IndefNp 1(2) 1 if C1 (C2) is an indefinite NP; else 0
7. candi ProperNp 1(2) 1 if C1 (C2) is a proper noun; else 0
8. candi Pronoun 1(2) 1 if C1 (C2) is a pronoun; else 0
Features describing the relationships between C1(C2) and M :
9. Appositive 1(2) 1 if C1 (C2) and M are in an appositive structure; else 0
10. NameAlias 1(2) 1 if C1 (C2) and M are in an alias of the other; else 0
11. GenderAgree 1(2) 1 if C1 (C2) and M agree in gender; else 0 if disagree; -1
if unknown
12. NumAgree 1(2) 1 if C1 (C2) and M agree in number; else 0 if disagree;
-1 if unknown
13. SentDist 1(2) Distance between C1 (C2) in sentences
14. HeadStrMatch 1(2) 1 if C1 (C2) and M match in head string; else 0
15. NPStrMatch 1(2) 1 if C1 (C2) and M match in full strings; else 0
16. StrSim 1(2) The ratio of the common strings between C1 (C2) and
M , over the strings of C1 (C2)
17. SemSim 1(2) The semantic agreement of C1 (C2) against M in Word-
Net
Features describing the relationships between C1 and C2
18. inter SentDist Distance between C1 and C2 in sentences
19. inter StrSim 0, 1, 2 if StrSim 1(C1, M) is equal to, larger or less than
StrSim 1(C2, M)
20. inter SemSim 0, 1, 2 if SemSim 1(C1, M) is equal to, larger or less than
SemSim 1(C2, M)
The candidates of a markable to be resolved were selected as follows. During
training, for each encountered markable, the preceding markables in the current
and previous four sentences were taken as the candidates. During resolution, for
a non-pronoun, all the preceding markables were included into the candidate
set, while for a pronoun, only the markables in the previous four sentences were
used, as the antecedent of a pronoun usually occurs in a short distance.
For MUC-6 and MUC-7, our modified framework generated 207k training
training instances, three times larger than the single-candidate based system
by Soon et al[3]. Among them, the ratio of ?00?,?01? and ?10? instances was
around 8:2:1. The distribution of the class labels was more balanced than in Soon
et al?s system, where only 5% training instances were positive while others were
all negative.
In our study we only considered domain-independent features that could be
obtained with low computational cost but with high reliability. Table 2 summa-
rizes the features with their respective possible values. Features f1-f17 record
726 X. Yang, J. Su, and C.L. Tan
the properties of a new markable and its two candidates, as well as their relation-
ships. Most of these features could be found in previous systems on coreference
resolution (e.g. [3], [4]). In addition, three inter-candidate features, f18-f20,
mark the relationship between the two candidates. The first one, inter SentDist,
records the distance between the two candidates in sentences, while the latter
two, inter StrSim and inter SemSim compare the similarity scores of the two
candidates, in string-matching and semantics respectively.
To provide necessary information of feature computation, an input raw text
was preprocessed automatically by a pipeline of NLP components. Among them,
the chunking component was trained and tested for the shared task for CoNLL-
2000 and achieved 92% F-score. The HMM based NE recognition component
was capable of recognizing the MUC-style NEs with F-scores of 96.9% (MUC-6)
and 94.3% (MUC-7).
4.2 Results and Discussion
In the experiment we compared four systems:
SC. The system based on the single-candidate model. It was a duplicate of the
system by Soon et al [3]. The feature set used in the baseline system was
similar to those listed in Table 2, except that no inter-candidate feature
would be used and only one set of features related to the single candidate
was required.
TC AD. The system based on the twin-candidate mode with the original learn-
ing framework, in which non-anaphors were eliminated by an anaphoricity
determination module in advance. We built a supervised learning based AD
module similar to the system proposed by Ng and Cardie [7]. We trained
the AD classifier on the additional 100 MUC-6 documents. By adjusting the
misclassification cost parameter of C5, we obtained a set of classifiers capable
of identifying ?positive? anaphors with variant recall and precision rates.
TC THRESH. The system based on the twin-candidate mode with the origi-
nal learning framework, using threshold to discard the low-confidenced com-
parison results between candidates.
TC NEW. The system based on the twin-candidate mode, with our modified
learning framework.
The results of the four systems on MUC-6 and MUC-7 are summarized in
Table 3. In these experiments, five-fold cross-evaluation was performed on the
training data to select the resolution parameters, for example, the threshold for
systems TC THRESH and TC NEW, and final AD classifier for TC AD.
As shown in the table, the baseline system SC achieves 66.1% and 65.9%
F-measure for MUC-6 and MUC-7 data sets. This performance is better than
that reported by Soon et al [3], and is comparable to that of the state-of-the-art
systems on the same data sets.
From the table we could find system TC AD achieves a comparatively high pre-
cision but a low recall, resulting in a F-measure worse than that of SC. The analysis
A Twin-Candidate Model of Coreference Resolution 727
Table 3. The performance of different coreference resolution systems
30 Docs 100 Docs
MUC-6 MUC-7 MUC-6 MUC-7
Experiments R P F R P F R P F R P F
SC 70.4 62.4 66.1 69.8 62.5 65.9 67.9 62.1 64.9 69.8 62.5 65.9
TC AD 62.6 66.4 64.4 60.8 64.7 62.7 61.6 65.4 63.4 60.8 64.6 62.7
TC THRESH 70.7 59.1 64.4 70.0 61.7 65.6 71.0 60.7 65.4 70.6 60.9 65.4
TC NEW 64.8 70.1 67.3 66.0 68.6 67.2 67.0 70.2 68.5 67.0 69.2 68.1
of the AD classifier reveals that it successfully identifies 79.3% anaphors (79.48%
precision) for MUC-6, and 70.9% anaphors (76.3% precision) for MUC-6. That
means, although the pre-processing AD module could partly avoid the wrong res-
olution of a non-anaphor, it eliminates many anaphors at the same, which leads to
the low recall for coreference resolution. Although in resolution different AD clas-
sifiers could be applied, we only observe the tradeoff between recall and precision,
with no effective resolution improvement in F-measure.
In contrast to TC AD, system TC THRESH yields large gains in recall. The
recall, up to above 70%, is higher than all the other three systems. However, the
precision at the same time is unfortunately the lowest. Such a pattern of high
recall and low precision indicates that using threshold could reduce, to some
degree, the risk of eliminating true anaphors, but it would be too lenient to
effectively block the resolution of non-anaphors.
Compared with TC AD and TC THRESH, TC NEW produces large gains in
the precision rates, which rank the highest among all the four systems. Although
the recall also drops at the same time, the increase in the precision could compen-
sate it well; we observe a F-measure of 67.3% for MUC-6 and 67.2% for MUC-7,
significantly better (p ? 0.05, by a sign test) than the other twin-candidate
based systems. These results suggest that with our modified framework, the
twin-candidate model could effectively identify non-anaphors and block their in-
valid resolution, without affecting the accuracy of the antecedent determination
for anaphors.
In our experiment we were interested to evaluate the resolution performance
of TC NEW under different sizes of training data. For this purpose, we used the
additional 100 annotated documents for training, and plotted the learning curve
in Figure 3. The curve indicates that the system could perform well with a small
number of training data, while the performance would get further improved with
more training data (the best performance is obtained on 90 documents).
In Table 3, we also summarized the results of different systems trained on 100
documents. In contrast to TC NEW, we find for system SC, there is no much
performance difference between using 30 and 100 training documents. This is
consistent with the report by Soon et al [3] that the single-candidate model
would achieve the peak performance with a moderate size of data. In the table
we could also find that the performance improvement of TC NEW against the
other three systems is apparently larger on 100 training documents than on
30 documents.
728 X. Yang, J. Su, and C.L. Tan
0 10 20 30 40 50 60 70 80 90 100
60
61
62
63
64
65
66
67
68
69
70
Number of Documents Trained On
F?
m
ea
su
re
Fig. 3. Learning curve of system TC NEW on MUC-7
40 50 60 70 80 90 100
30
35
40
45
50
55
60
65
70
75
80
Recall
Pr
ec
is
io
n
TC_THRESH
TC_AD
TC_NEW
Fig. 4. Recall and precision results for the twin-candidate based systems
In Figure 4, we plotted the variant recall and precision scores that the three
twin-candidate based systems were capable of producing when trained on 100
documents. (Here we only showed the results for MUC-7. Similar results could
be obtained for MUC-6). In line with the results in Table 3, system TC AD
tends to obtain a high precision but low recall, while system TC THRESH
tends to obtain a high recall but low precision. Comparatively, system TC NEW
produces even recall and precision. For the range of recall within which the
three systems coincide, TC NEW yields higher precision than the other two
systems. This figure further proves the effectiveness of our modified
learning framework.
As mentioned, in systems TC THRESH and TC NEW the threshold pa-
rameter could be adjusted. It would be interesting to evaluate the influence of
different thresholds on the resolution performance. In Figure 5 we compared the
recall and precision of two systems, with thresholds ranging from 65 to 100.
In TC THRESH, when the threshold is low, the recall is almost 100% while
the precision is quite low. In such a case, all the markables, regardless anaphors or
A Twin-Candidate Model of Coreference Resolution 729
65 70 75 80 85 90 95 100
30
40
50
60
70
80
90
100
Threshold
R
es
ul
ts
TC_THRESH
Recall
Precision
65 70 75 80 85 90 95 100
0
10
20
30
40
50
60
70
80
90
Threshold
R
es
ul
ts
TC_NEW
Recall
Precision
Fig. 5. Performance of TC THRESH and TC NEW under different thresholds
non-anaphors, will be resolved. As a consequence, all the occurring markables in a
document tends to be linked together. In fact, the effective range of the threshold
that leads to an acceptable performance is quite short. The threshold would only
work when it is considerably high (above 95). Before that, the precision remains
very low (less than 40%) while the recall keeps going down with the increase of
the threshold.
By contrast, in TC NEW, both the recall and precision vary little unless the
threshold is extremely high. That means, the threshold would not impose much
influence on the resolution performance of TC NEW. This should be because in
the modified framework, the cases of non-anaphors are determined by the special
class label ?00?, instead of the threshold as in TC THRESH. The purpose of
using threshold in TC NEW is not to identify the non-anaphors, but to improve
the accuracy of class labelling. Indeed, we could obtain a good result without
using any threshold in TC NEW. These further confirm our claims that the
modified learning framework could perform more reliably than the solution of
using threshold.
5 Conclusions
In this paper we aimed to find an effective way to apply the twin-candidate model
into coreference resolution task. We proposed a modified learning framework in
which non-anaphors were utilized to create a special class of training instances.
With such instances, the resulting classifier could avoid the invalid resolution of
non-anaphors, which enables the twin-candidate model to be directly deployed to
coreference resolution, without using an additional anaphoricity determination
module or using a pre-defined threshold.
In the paper we evaluated the effectiveness of our modified framework on
the MUC data set. The results show that the system with the new framework
outperforms the single-candidate based system, as well as the twin-candidate
730 X. Yang, J. Su, and C.L. Tan
based systems using other solutions. Especially, the analysis of the results indi-
cates that our modified framework could lead to more reliable performance than
the solution of using threshold. All these suggest that the twin-candidate model
with the new framework is effective for coreference resolution.
References
1. McCarthy, J., Lehnert, Q.: Using decision trees for coreference resolution. In:
Proceedings of the 14th International Conference on Artificial Intelligences. (1995)
1050?1055
2. Connolly, D., Burger, J., Day, D. New Methods in Language Processing. In: A
machine learning approach to anaphoric reference. (1997) 133?144
3. Soon, W., Ng, H., Lim, D.: A machine learning approach to coreference resolution
of noun phrases. Computational Linguistics 27 (2001) 521?544
4. Ng, V., Cardie, C.: Improving machine learning approaches to coreference resolu-
tion. In: Proceedings of the 40th Annual Meeting of the Association for Compu-
tational Linguistics, Philadelphia (2002) 104?111
5. Yang, X., Zhou, G., Su, J., Tan, C.: Coreference resolution using competition
learning approach. In: Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics, Japan (2003)
6. Iida, R., Inui, K., Takamura, H., Matsumoto, Y.: Incorporating contextual cues in
trainable models for coreference resolution. In: Proceedings of the 10th Conference
of EACL, Workshop ?The Computational Treatment of Anaphora?. (2003)
7. Ng, V., Cardie, C.: Identifying anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In: Proceedings of the 19th International Conference
on Computational Linguistics (COLING02). (2002)
8. MUC-6: Proceedings of the Sixth Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA (1995)
9. MUC-7: Proceedings of the Seventh Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA (1998)
10. Quinlan, J.R.: C4.5: Programs for machine learning. Morgan Kaufmann Publishers,
San Francisco, CA (1993)
11. Vilain, M., Burger, J., Aberdeen, J., Connolly, D., Hirschman, L.: A model-
theoretic coreference scoring scheme. In: Proceedings of the Sixth Message under-
standing Conference (MUC-6), San Francisco, CA, Morgan Kaufmann Publishers
(1995) 45?52
Coreference Resolution Using Competition Learning Approach 
Xiaofeng Yang*+ Guodong Zhou*  Jian Su* Chew Lim Tan + 
*Institute for Infocomm Research, 
21 Heng Mui Keng Terrace, 
Singapore 119613 
+Department of Computer Science, 
National University of Singapore,  
Singapore 117543  
*{xiaofengy,zhougd,sujian}@ 
i2r.a-star.edu.sg 
+(yangxiao,tancl)@comp.nus.edu.sg
  
Abstract 
In this paper we propose a competition 
learning approach to coreference resolu-
tion. Traditionally, supervised machine 
learning approaches adopt the single-
candidate model. Nevertheless the prefer-
ence relationship between the antecedent 
candidates cannot be determined accu-
rately in this model. By contrast, our ap-
proach adopts a twin-candidate learning 
model. Such a model can present the 
competition criterion for antecedent can-
didates reliably, and ensure that the most 
preferred candidate is selected. Further-
more, our approach applies a candidate 
filter to reduce the computational cost and 
data noises during training and resolution. 
The experimental results on MUC-6 and 
MUC-7 data set show that our approach 
can outperform those based on the single-
candidate model.  
1 Introduction 
Coreference resolution is the process of linking 
together multiple expressions of a given entity. The 
key to solve this problem is to determine the ante-
cedent for each referring expression in a document.  
In coreference resolution, it is common that two 
or more candidates compete to be the antecedent of 
an anaphor (Mitkov, 1999). Whether a candidate is 
coreferential to an anaphor is often determined by 
the competition among all the candidates. So far, 
various algorithms have been proposed to deter-
mine the preference relationship between two can-
didates. Mitkov?s knowledge-poor pronoun 
resolution method (Mitkov, 1998), for example, 
uses the scores from a set of antecedent indicators 
to rank the candidates. And centering algorithms 
(Brennan et al, 1987; Strube, 1998; Tetreault, 
2001), sort the antecedent candidates based on the 
ranking of the forward-looking or backward-
looking centers. 
In recent years, supervised machine learning 
approaches have been widely used in coreference 
resolution (Aone and Bennett, 1995; McCarthy, 
1996; Soon et al, 2001; Ng and Cardie, 2002a), 
and have achieved significant success. Normally, 
these approaches adopt a single-candidate model in 
which the classifier judges whether an antecedent 
candidate is coreferential to an anaphor with a con-
fidence value. The confidence values are generally 
used as the competition criterion for the antecedent 
candidates. For example, the ?Best-First? selection 
algorithms (Aone and Bennett, 1995; Ng and 
Cardie, 2002a) link the anaphor to the candidate 
with the maximal confidence value (above 0.5). 
One problem of the single-candidate model, 
however, is that it only takes into account the rela-
tionships between an anaphor and one individual 
candidate at a time, and overlooks the preference 
relationship between candidates. Consequently, the 
confidence values cannot accurately represent the 
true competition criterion for the candidates. 
In this paper, we present a competition learning 
approach to coreference resolution. Motivated by 
the research work by Connolly et al (1997), our 
approach adopts a twin-candidate model to directly 
learn the competition criterion for the antecedent 
candidates. In such a model, a classifier is trained 
based on the instances formed by an anaphor and a 
pair of its antecedent candidates. The classifier is 
then used to determine the preference between any 
two candidates of an anaphor encountered in a new 
document. The candidate that wins the most com-
parisons is selected as the antecedent. In order to 
reduce the computational cost and data noises, our 
approach also employs a candidate filter to elimi-
nate the invalid or irrelevant candidates.  
The layout of this paper is as follows. Section 2 
briefly describes the single-candidate model and 
analyzes its limitation. Section 3 proposes in de-
tails the twin-candidate model and Section 4 pre-
sents our coreference resolution approach based on 
this model. Section 5 reports and discusses the ex-
perimental results. Section 6 describes related re-
search work. Finally, conclusion is given in 
Section 7. 
2 The Single-Candidate Model 
The main idea of the single-candidate model for 
coreference resolution is to recast the resolution as 
a binary classification problem. 
During training, a set of training instances is 
generated for each anaphor in an annotated text. 
An instance is formed by the anaphor and one of 
its antecedent candidates. It is labeled as positive 
or negative based on whether or not the candidate 
is tagged in the same coreferential chain of the 
anaphor. 
After training, a classifier is ready to resolve the 
NPs1 encountered in a new document. For each NP 
under consideration, every one of its antecedent 
candidates is paired with it to form a test instance. 
The classifier returns a number between 0 and 1 
that indicates the likelihood that the candidate is 
coreferential to the NP. 
The returned confidence value is commonly 
used as the competition criterion to rank the candi-
date. Normally, the candidates with confidences 
less than a selection threshold (e.g. 0.5) are dis-
carded. Then some algorithms are applied to 
choose one of the remaining candidates, if any, as 
the antecedent. For example, ?Closest-First? (Soon 
et al, 2001) selects the candidate closest to the 
anaphor, while ?Best-First? (Aone and Bennett, 
1995; Ng and Cardie, 2002a) selects the candidate 
with the maximal confidence value.  
One limitation of this model, however, is that it 
only considers the relationships between a NP en-
countered and one of its candidates at a time dur-
ing its training and testing procedures. The 
confidence value reflects the probability that the 
candidate is coreferential to the NP in the overall 
                                                          
1 In this paper a NP corresponds to a Markable in MUC 
coreference resolution tasks. 
distribution 2 , but not the conditional probability 
when the candidate is concurrent with other com-
petitors. Consequently, the confidence values are 
unreliable to represent the true competition crite-
rion for the candidates.  
To illustrate this problem, just suppose a data 
set where an instance could be described with four 
exclusive features: F1, F2, F3 and F4. The ranking 
of candidates obeys the following rule: 
CSF1 >> CSF2 >> CSF3 >> CSF4 
Here CSFi ( 41 ?? i ) is the set of antecedent can-
didates with the feature Fi on. The mark of ?>>? 
denotes the preference relationship, that is, the 
candidates in CSF1 is preferred to those in CSF2, and 
to those in CSF3 and CSF4.  
Let CF2 and CF3 denote the class value of a leaf 
node ?F2 = 1? and ?F3 = 1?, respectively. It is pos-
sible that CF2 < CF3, if the anaphors whose candi-
dates all belong to CSF3 or CSF4 take the majority in 
the training data set.  In this case, a candidate in 
CSF3 would be assigned a larger confidence value 
than a candidate in CSF2. This nevertheless contra-
dicts the ranking rules. If during resolution, the 
candidates of an anaphor all come from CSF2 or 
CSF3, the anaphor may be wrongly linked to a can-
didate in CSF3 rather than in CSF2. 
3 The Twin-Candidate Model 
Different from the single-candidate model, the 
twin-candidate model aims to learn the competition 
criterion for candidates. In this section, we will 
introduce the structure of the model in details. 
3.1 Training Instances Creation 
Consider an anaphor ana and its candidate set can-
didate_set, {C1, C2, ?, Ck}, where Cj is closer to 
ana than Ci if j > i. Suppose positive_set is the set 
of candidates that occur in the coreferential chain 
of ana, and negative_set is the set of candidates not 
in the chain, that is, negative_set = candidate_set  
- positive_set. The set of training instances based 
on ana, inst_set, is defined as follows:  
                                                          
2 Suppose we use C4.5 algorithm and the class value takes the 
smoothed ration, 
2
1
+
+
t
p , where p is the number of positive 
instances and t is the total number of instances contained in 
the corresponding leaf node. 
} _  C  , _Cj,i |{
  } _  C  ,_ C j,i |{
_
ji),,(
ji),,(
setpositvesetnegativeinst
setnegativesetpositveinst
setinst
anaCjCi
anaCjCi
??>
??>
=
U
 
From the above definition, an instance is 
formed by an anaphor, one positive candidate and 
one negative candidate. For each instance, 
)ana,cj,ci(inst , the candidate at the first position, Ci, 
is closer to the anaphor than the candidate at the 
second position, Cj.  
A training instance )ana,cj,ci(inst is labeled as 
positive if Ci ?  positive-set and Cj ?  negative-set; 
or negative if Ci ?  negative-set and Cj ?  positive-
set.  
See the following example:  
 
Any design to link China's accession to the WTO 
with the missile tests1 was doomed to failure.  
 ?If some countries2 try to block China TO acces-
sion, that will not be popular and will fail to win the 
support of other countries3? she said.  
Although no governments4 have suggested formal 
sanctions5 on China over the missile tests6, the United 
States has called them7 ?provocative and reckless? and 
other countries said they could threaten Asian stability.  
 
In the above text segment, the antecedent can-
didate set of the pronoun ?them7? consists of six 
candidates highlighted in Italics. Among the can-
didates, Candidate 1 and 6 are in the coreferential 
chain of ?them7?, while Candidate 2, 3, 4, 5 are not. 
Thus, eight instances are formed for ?them7?:  
 
(2,1,7)  (3,1,7)  (4,1,7)  (5,1,7) 
(6,5,7)  (6,4,7)  (6,3,7)  (6,2,7) 
 
Here the instances in the first line are negative, 
while those in the second line are all positive.  
3.2 Features Definition 
A feature vector is specified for each training or 
testing instance. Similar to those in the single-
candidate model, the features may describe the 
lexical, syntactic, semantic and positional relation-
ships of an anaphor and any one of its candidates. 
Besides, the feature set may also contain inter-
candidate features characterizing the relationships 
between the pair of candidates, e.g. the distance 
between the candidates in the number distances or 
paragraphs. 
3.3 Classifier Generation 
Based on the feature vectors generated for each 
anaphor encountered in the training data set, a 
classifier can be trained using a certain machine 
learning algorithm, such as C4.5, RIPPER, etc. 
Given the feature vector of a test instance 
)ana,cj,ci(inst  (i > j), the classifier returns the posi-
tive class indicating that Ci is preferred to Cj as the 
antecedent of ana; or negative indicating that Cj is 
preferred.  
3.4 Antecedent Identification 
Let CR( )ana,cj,ci(inst ) denote the classification re-
sult for an instance )ana,cj,ci(inst . The antecedent of 
an anaphor is identified using the algorithm shown 
in Figure 1.  
 
Algorithm ANTE-SEL 
Input: ana: the anaphor under consideration  
candidate_set: the set of antecedent can-
didates of ana, {C1, C2,?,Ck} 
 
for i = 1 to K do 
   Score[ i ] = 0; 
for  i = K downto 2 do 
for j = i ? 1 downto 1 do 
  if  CR( )ana,cj,ci(inst ) = = positive then  
Score[ i ]++; 
else  
Score[ j ] ++; 
  endif 
SelectedIdx= ][maxarg
_
iScore
setcandidateCii ?
 
return CselectedIdx; 
Figure 1:The antecedent identification algorithm
 
Algorithm ANTE-SEL takes as input an ana-
phor and its candidate set candidate_set, and re-
turns one candidate as its antecedent. In the 
algorithm, each candidate is compared against any 
other candidate. The classifier acts as a judge dur-
ing each comparison. The score of each candidate 
increases by one every time when it wins. In this 
way, the final score of a candidate records the total 
times it wins. The candidate with the maximal 
score is singled out as the antecedent.  
If two or more candidates have the same maxi-
mal score, the one closest to the anaphor would be 
selected. 
3.5 Single-Candidate Model: A Special Case 
of Twin-Candidate Model? 
While the realization and the structure of the twin-
candidate model are significantly different from 
the single-candidate model, the single-candidate 
model in fact can be regarded as a special case of 
the twin-candidate model.  
To illustrate this, just consider a virtual ?blank? 
candidate C0 such that we could convert an in-
stance )ana,ci(inst in the single-candidate model to 
an instance )ana,c,ci( 0inst in the twin-candidate 
model. Let )ana,c,ci( 0inst have the same class label 
as )ana,ci(inst , that is, )ana,c,ci( 0inst is positive if Ci is 
the antecedent of ana; or negative if not.  
Apparently, the classifier trained on the in-
stance set { )ana,ci(inst }, T1, is equivalent to that 
trained on { )ana,c,ci( 0inst }, T2.  T1 and T2 would 
assign the same class label for the test instances 
)ana,ci(inst  and )ana,c,ci( 0inst , respectively. That is to 
say, determining whether Ci is coreferential to ana 
by T1 in the single-candidate model equals to 
determining whether Ci is better than C0 w.r.t ana 
by T2 in the twin-candidate model. Here we could 
take C0 as a ?standard candidate?. 
While the classification in the single-candidate 
model can find its interpretation in the twin-
candidate model, it is not true vice versa. Conse-
quently, we can safely draw the conclusion that the 
twin-candidate model is more powerful than the 
single-candidate model in characterizing the rela-
tionships among an anaphor and its candidates. 
4 The Competition Learning Approach 
Our competition learning approach adopts the 
twin-candidate model introduced in the Section 3. 
The main process of the approach is as follows: 
1. The raw input documents are preprocessed to 
obtain most, if not all, of the possible NPs.  
2. During training, for each anaphoric NP, we 
create a set of candidates, and then generate 
the training instances as described in Section 3.  
3. Based on the training instances, we make use 
of the C5.0 learning algorithm (Quinlan, 1993) 
to train a classifier. 
4. During resolution, for each NP encountered, 
we also construct a candidate set. If the set is 
empty, we left this NP unresolved; otherwise 
we apply the antecedent identification algo-
rithm to choose the antecedent and then link 
the NP to it.  
4.1 Preprocessing 
To determine the boundary of the noun phrases, a 
pipeline of Nature Language Processing compo-
nents are applied to an input raw text: 
z Tokenization and sentence segmentation 
z Named entity recognition 
z Part-of-speech tagging 
z Noun phrase chunking 
Among them, named entity recognition, part-of-
speech tagging and text chunking apply the same 
Hidden Markov Model (HMM) based engine with 
error-driven learning capability (Zhou and Su, 
2000 & 2002). The named entity recognition 
component recognizes various types of MUC-style 
named entities, i.e., organization, location, person, 
date, time, money and percentage.  
4.2 Features Selection 
For our study, in this paper we only select those 
features that can be obtained with low annotation 
cost and high reliability. All features are listed in 
Table 1 together with their respective possible val-
ues.  
4.3 Candidates Filtering 
For a NP under consideration, all of its preceding 
NPs could be the antecedent candidates. Neverthe-
less, since in the twin-candidate model the number 
of instances for a given anaphor is about the square 
of the number of its antecedent candidates, the 
computational cost would be prohibitively large if 
we include all the NPs in the candidate set. More-
over, many of the preceding NPs are irrelevant or 
even invalid with regard to the anaphor. These data 
noises may hamper the training of a good-
performanced classifier, and also damage the accu-
racy of the antecedent selection: too many com-
parisons are made between incorrect candidates. 
Therefore, in order to reduce the computational 
cost and data noises, an effective candidate filter-
ing strategy must be applied in our approach. 
During training, we create the candidate set for 
each anaphor with the following filtering algorithm: 
1. If the anaphor is a pronoun,  
(a) Add to the initial candidate set al the pre-
ceding NPs in the current and the previous 
two sentences. 
(b) Remove from the candidate set those that 
disagree in number, gender, and person. 
(c) If the candidate set is empty, add the NPs in 
an earlier sentence and go to 1(b). 
2. If the anaphor is a non-pronoun, 
(a) Add all the non-pronominal antecedents to 
the initial candidate set. 
(b) For each candidate added in 2(a), add the 
non-pronouns in the current, the previous 
and the next sentences into the candidate set. 
During resolution, we filter the candidates for 
each encountered pronoun in the same way as dur-
ing training. That is, we only consider the NPs in 
the current and the preceding 2 sentences. Such a 
context window is reasonable as the distance be-
tween a pronominal anaphor and its antecedent is 
generally short. In the MUC-6 data set, for exam-
ple, the immediate antecedents of 95% pronominal 
anaphors can be found within the above distance. 
Comparatively, candidate filtering for non-
pronouns during resolution is complicated. A po-
tential problem is that for each non-pronoun under 
consideration, the twin-candidate model always 
chooses a candidate as the antecedent, even though 
all of the candidates are ?low-qualified?, that is, 
unlikely to be coreferential to the non-pronoun un-
der consideration.  
In fact, the twin-candidate model in itself can 
identify the qualification of a candidate. We can 
compare every candidate with a virtual ?standard 
candidate?, C0. Only those better than C0 are 
deemed qualified and allowed to enter the ?round 
robin?, whereas the losers are eliminated. As we 
have discussed in Section 3.5, the classifier on the 
pairs of a candidate and C0 is just a single-
candidate classifier. Thus, we can safely adopt the 
single-candidate classifier as our candidate filter.  
The candidate filtering algorithm during resolu-
tion is as follows:  
Features describing the candidate: 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10 
ante_DefNp_1(2) 
ante_IndefNP_1(2) 
ante_Pron_1(2) 
ante_ProperNP_1(2) 
ante_M_ProperNP_1(2) 
ante_ProperNP_APPOS_1(2) 
ante_Appositive_1(2) 
ante_NearestNP_1(2) 
ante_Embeded_1(2) 
ante_Title_1(2) 
1 if Ci (Cj) is a definite NP; else 0 
1 if Ci (Cj) is an indefinite NP; else 0 
1 if Ci (Cj) is a pronoun; else 0 
1 if Ci (Cj) is a proper NP; else 0  
1 if Ci (Cj) is a mentioned proper NP; else 0 
1 if Ci (Cj) is a proper NP modified by an appositive; else 0 
1 if Ci (Cj) is in a apposition structure; else 0 
1 if Ci (Cj) is the nearest candidate to the anaphor; else 0 
1 if Ci (Cj) is in an embedded NP; else 0 
1 if Ci (Cj) is in a title; else 0 
Features describing the anaphor: 
11. 
12. 
13. 
14. 
15. 
 
16. 
ana_DefNP 
ana_IndefNP 
ana_Pron 
ana_ProperNP 
ana_PronType 
 
ana_FlexiblePron 
1 if ana is a definite NP; else 0 
1 if ana is an indefinite NP; else 0 
1 if ana is a pronoun; else 0 
1 if ana is a proper NP; else 0 
1 if ana is a third person pronoun; 2 if a single neuter pro-
noun; 3 if a plural neuter pronoun; 4 if other types 
1 if ana is a flexible pronoun; else 0 
Features describing the candidate and the anaphor: 
17. 
18. 
 
18. 
 
20. 
21. 
ante_ana_StringMatch_1(2) 
ante_ana_GenderAgree_1(2) 
 
ante_ana_NumAgree_1(2) 
 
ante_ana_Appositive_1(2) 
ante_ana_Alias_1(2) 
1 if Ci (Cj) and ana match in string; else 0 
1 if Ci (Cj) and ana agree in gender; else 0 if disagree; -1 if 
unknown 
1 if Ci (Cj) and ana agree in number; 0 if disagree; -1 if un-
known 
1 if Ci (Cj) and ana are in an appositive structure; else 0 
1 if Ci (Cj) and ana are in an alias of the other; else 0 
Features describing the two candidates 
22. 
23. 
inter_SDistance 
inter_Pdistance 
Distance between Ci and Cj in sentences 
Distance between Ci and Cj in paragraphs 
Table 1:  Feature set for coreference resolution (Feature 22, 23 and features involving Cj are not 
used in the single-candidate model) 
1. If the current NP is a pronoun, construct the 
candidate set in the same way as during training.  
2. If the current NP is a non-pronoun,  
(a) Add all the preceding non-pronouns to the ini-
tial candidate set. 
(b) Calculate the confidence value for each candi-
date using the single-candidate classifier. 
(c) Remove the candidates with confidence value 
less than 0.5. 
5 Evaluation and Discussion 
Our coreference resolution approach is evaluated 
on the standard MUC-6 (1995) and MUC-7 (1998) 
data set. For MUC-6, 30 ?dry-run? documents an-
notated with coreference information could be used 
as training data. There are also 30 annotated train-
ing documents from MUC-7. For testing, we util-
ize the 30 standard test documents from MUC-6 
and the 20 standard test documents from MUC-7. 
5.1 Baseline Systems 
In the experiment we compared our approach with 
the following research works:  
1. Strube?s S-list algorithm for pronoun resolu-
tion (Stube, 1998).  
2. Ng and Cardie?s machine learning approach to 
coreference resolution (Ng and Cardie, 2002a).  
3. Connolly et al?s machine learning approach to 
anaphora resolution (Connolly et al, 1997).  
Among them, S-List, a version of centering 
algorithm, uses well-defined heuristic rules to rank 
the antecedent candidates; Ng and Cardie?s ap-
proach employs the standard single-candidate 
model and ?Best-First? rule to select the antece-
dent; Connolly et al?s approach also adopts the 
twin-candidate model, but their approach lacks of 
candidate filtering strategy and uses greedy linear 
search to select the antecedent (See ?Related 
work? for details). 
We constructed three baseline systems based on 
the above three approaches, respectively. For com-
parison, in the baseline system 2 and 3, we used 
the similar feature set as in our system (see table 1).  
5.2 Results and Discussion 
Table 2 and 3 show the performance of different 
approaches in the pronoun and non-pronoun reso-
lution, respectively. In these tables we focus on the 
abilities of different approaches in resolving an 
anaphor to its antecedent correctly. The recall 
measures the number of correctly resolved ana-
phors over the total anaphors in the MUC test data 
set, and the precision measures the number of cor-
rect anaphors over the total resolved anaphors. The 
F-measure F=2*RP/(R+P) is the harmonic mean of 
precision and recall. 
The experimental result demonstrates that our 
competition learning approach achieves a better 
performance than the baseline approaches in re-
solving pronominal anaphors. As shown in Table 2, 
our approach outperforms Ng and Cardie?s single-
candidate based approach by 3.7 and 5.4 in F-
measure for MUC-6 and MUC-7, respectively. 
Besides, compared with Strube?s S-list algorithm, 
our approach also achieves gains in the F-measure 
by 3.2 (MUC-6), and 1.6 (MUC-7). In particular, 
our approach obtains significant improvement 
(21.1 for MUC-6, and 13.1 for MUC-7) over Con-
nolly et al?s twin-candidate based approach. 
 
MUC-6 MUC-7  
 R P F R P F 
Strube (1998)  76.1 74.3 75.1 62.9 60.3 61.6 
Ng and Cardie (2002a) 75.4 73.8 74.6 58.9 56.8 57.8 
Connolly et al (1997) 57.2 57.2 57.2 50.1 50.1 50.1 
Our approach 79.3 77.5 78.3 64.4 62.1 63.2 
Table 2:  Results for the pronoun resolution  
 
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 51.0 89.9 65.0 39.1 86.4 53.8 
Connolly et al (1997) 52.2 52.2 52.2 43.7 43.7 43.7 
Our approach  51.3 90.4 65.4 39.7 87.6 54.6 
Table 3:  Results for the non-pronoun resolution  
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 62.2 78.8 69.4 48.4 74.6 58.7 
Our approach 64.0 80.5 71.3 50.1 75.4 60.2 
Table 4: Results for the coreference resolution  
 
Compared with the gains in pronoun resolution, 
the improvement in non-pronoun resolution is 
slight. As shown in Table 3, our approach resolves 
non-pronominal anaphors with the recall of 51.3 
(39.7) and the precision of 90.4 (87.6) for MUC-6 
(MUC-7). In contrast to Ng and Cardie?s approach, 
the performance of our approach improves only 0.3 
(0.6) in recall and 0.5 (1.2) in precision. The rea-
son may be that in non-pronoun resolution, the 
coreference of an anaphor and its candidate is usu-
ally determined only by some strongly indicative 
features such as alias, apposition, string-matching, 
etc (this explains why we obtain a high precision 
but a low recall in non-pronoun resolution). There-
fore, most of the positive candidates are coreferen-
tial to the anaphors even though they are not the 
?best?. As a result, we can only see comparatively 
slight difference between the performances of the 
two approaches.  
Although Connolly et al?s approach also adopts 
the twin-candidate model, it achieves a poor per-
formance for both pronoun resolution and non-
pronoun resolution. The main reason is the absence 
of candidate filtering strategy in their approach 
(this is why the recall equals to the precision in the 
tables). Without candidate filtering, the recall may 
rise as the correct antecedents would not be elimi-
nated wrongly. Nevertheless, the precision drops 
largely due to the numerous invalid NPs in the 
candidate set. As a result, a significantly low F-
measure is obtained in their approach. 
Table 4 summarizes the overall performance of 
different approaches to coreference resolution. Dif-
ferent from Table 2 and 3, here we focus on 
whether a coreferential chain could be correctly 
identified. For this purpose, we obtain the recall, 
the precision and the F-measure using the standard 
MUC scoring program (Vilain et al 1995) for the 
coreference resolution task. Here the recall means 
the correct resolved chains over the whole 
coreferential chains in the data set, and precision 
means the correct resolved chains over the whole 
resolved chains.  
In line with the previous experiments, we see 
reasonable improvement in the performance of the 
coreference resolution: compared with the baseline 
approach based on the single-candidate model, the 
F-measure of approach increases from 69.4 to 71.3 
for MUC-6, and from 58.7 to 60.2 for MUC-7.  
6 Related Work 
A similar twin-candidate model was adopted in the 
anaphoric resolution system by Connolly et al 
(1997). The differences between our approach and 
theirs are: 
(1) In Connolly et al?s approach, all the preceding 
NPs of an anaphor are taken as the antecedent 
candidates, whereas in our approach we use 
candidate filters to eliminate invalid or irrele-
vant candidates.  
(2) The antecedent identification in Connolly et 
al.?s approach is to apply the classifier to 
successive pairs of candidates, each time 
retaining the better candidate. However, due to 
the lack of strong assumption of transitivity, 
the selection procedure is in fact a greedy 
search. By contrast, our approach evaluates a 
candidate according to the times it wins over 
the other competitors. Comparatively this 
algorithm could lead to a better solution. 
(3) Our approach makes use of more indicative 
features, such as Appositive, Name Alias, 
String-matching, etc. These features are effec-
tive especially for non-pronoun resolution. 
7 Conclusion 
In this paper we have proposed a competition 
learning approach to coreference resolution. We 
started with the introduction of the single-
candidate model adopted by most supervised ma-
chine learning approaches. We argued that the con-
fidence values returned by the single-candidate 
classifier are not reliable to be used as ranking cri-
terion for antecedent candidates. Alternatively, we 
presented a twin-candidate model that learns the 
competition criterion for antecedent candidates 
directly. We introduced how to adopt the twin-
candidate model in our competition learning ap-
proach to resolve the coreference problem. Particu-
larly, we proposed a candidate filtering algorithm 
that can effectively reduce the computational cost 
and data noises.  
The experimental results have proved the effec-
tiveness of our approach. Compared with the base-
line approach using the single-candidate model, the 
F-measure increases by 1.9 and 1.5 for MUC-6 and 
MUC-7 data set, respectively. The gains in the 
pronoun resolution contribute most to the overall 
improvement of coreference resolution. 
Currently, we employ the single-candidate clas-
sifier to filter the candidate set during resolution. 
While the filter guarantees the qualification of the 
candidates, it removes too many positive candi-
dates, and thus the recall suffers. In our future 
work, we intend to adopt a looser filter together 
with an anaphoricity determination module (Bean 
and Riloff, 1999; Ng and Cardie, 2002b). Only if 
an encountered NP is determined as an anaphor, 
we will select an antecedent from the candidate set 
generated by the looser filter. Furthermore, we 
would like to incorporate more syntactic features 
into our feature set, such as grammatical role or 
syntactic parallelism. These features may be help-
ful to improve the performance of pronoun resolu-
tion.  
References 
Chinatsu Aone and Scott W.Bennett. 1995. Evaluating 
automated and manual acquisition of anaphora reso-
lution strategies. In Proceedings of the 33rd Annual 
Meeting of the Association for Computational Lin-
guistics, Pages 122-129. 
D.Bean and E.Riloff. 1999. Corpus-Based identification 
of non-anaphoric noun phrases. In Proceedings of the 
37th Annual Meeting of the Association for Computa-
tional Linguistics, Pages 373-380. 
Brennan, S, E., M. W. Friedman and C. J. Pollard. 1987. 
A Centering approach to pronouns. In Proceedings of 
the 25th Annual Meeting of The Association for Com-
putational Linguistics, Page 155-162. 
Dennis Connolly, John D. Burger and David S. Day. 
1997. A machine learning approach to anaphoric ref-
erence. New Methods in Language Processing, Page 
133-144.  
Joseph F. McCarthy. 1996. A trainable approach to 
coreference resolution for Information Extraction. 
Ph.D. thesis. University of Massachusetts. 
Ruslan Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. In Proceedings of the 17th Int. 
Conference on Computational Linguistics (COLING-
ACL'98), Page 869-875. 
Ruslan Mitkov. 1999. Anaphora resolution: The state of 
the art. Technical report. University of Wolverhamp-
ton, Wolverhampton. 
MUC-6. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). Morgan Kauf-
mann, San Francisco, CA. 
MUC-7. 1998. Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan Kauf-
mann, San Francisco, CA. 
Vincent Ng and Claire Cardie. 2002a. Improving ma-
chine learning approaches to coreference resolution. 
In Proceedings of the 40rd Annual Meeting of the As-
sociation for Computational Linguistics, Pages 104-
111. 
Vincent Ng and Claire Cardie. 2002b. Identifying ana-
phoric and non-anaphoric noun phrases to improve 
coreference resolution. In Proceedings of 19th Inter-
national Conference on Computational Linguistics 
(COLING-2002). 
J R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA. 
Wee Meng Soon, Hwee Tou Ng and Daniel Chung 
Yong Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4), Page 521-544. 
Michael Strube. Never look back: An alternative to 
Centering. 1998. In Proceedings of the 17th Int. Con-
ference on Computational Linguistics and 36th An-
nual Meeting of ACL, Page 1251-1257 
Joel R. Tetreault. 2001. A Corpus-Based evaluation of 
Centering and pronoun resolution. Computational 
Linguistics, 27(4), Page 507-520. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and 
L.Hirschman. 1995. A model-theoretic coreference 
scoring scheme. In Proceedings of the Sixth Message 
understanding Conference (MUC-6), Pages 42-52. 
GD Zhou and J. Su, 2000. Error-driven HMM-based 
chunk tagger with context-dependent lexicon. In 
Proceedings of the Joint Conference on Empirical 
Methods on Natural Language Processing and Very 
Large Corpus (EMNLP/ VLC'2000).  
GD Zhou and J. Su. 2002. Named Entity recognition 
using a HMM-based chunk tagger. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics, P473-478. 
Improving Pronoun Resolution by Incorporating Coreferential
Information of Candidates
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Coreferential information of a candidate, such
as the properties of its antecedents, is important
for pronoun resolution because it reflects the
salience of the candidate in the local discourse.
Such information, however, is usually ignored in
previous learning-based systems. In this paper
we present a trainable model which incorporates
coreferential information of candidates into pro-
noun resolution. Preliminary experiments show
that our model will boost the resolution perfor-
mance given the right antecedents of the can-
didates. We further discuss how to apply our
model in real resolution where the antecedents
of the candidate are found by a separate noun
phrase resolution module. The experimental re-
sults show that our model still achieves better
performance than the baseline.
1 Introduction
In recent years, supervised machine learning ap-
proaches have been widely explored in refer-
ence resolution and achieved considerable suc-
cess (Ge et al, 1998; Soon et al, 2001; Ng and
Cardie, 2002; Strube and Muller, 2003; Yang et
al., 2003). Most learning-based pronoun res-
olution systems determine the reference rela-
tionship between an anaphor and its antecedent
candidate only from the properties of the pair.
The knowledge about the context of anaphor
and antecedent is nevertheless ignored. How-
ever, research in centering theory (Sidner, 1981;
Grosz et al, 1983; Grosz et al, 1995; Tetreault,
2001) has revealed that the local focusing (or
centering) also has a great effect on the pro-
cessing of pronominal expressions. The choices
of the antecedents of pronouns usually depend
on the center of attention throughout the local
discourse segment (Mitkov, 1999).
To determine the salience of a candidate
in the local context, we may need to check
the coreferential information of the candidate,
such as the existence and properties of its an-
tecedents. In fact, such information has been
used for pronoun resolution in many heuristic-
based systems. The S-List model (Strube,
1998), for example, assumes that a co-referring
candidate is a hearer-old discourse entity and
is preferred to other hearer-new candidates.
In the algorithms based on the centering the-
ory (Brennan et al, 1987; Grosz et al, 1995), if
a candidate and its antecedent are the backward-
looking centers of two subsequent utterances re-
spectively, the candidate would be the most pre-
ferred since the CONTINUE transition is al-
ways ranked higher than SHIFT or RETAIN.
In this paper, we present a supervised
learning-based pronoun resolution system which
incorporates coreferential information of candi-
dates in a trainable model. For each candi-
date, we take into consideration the properties
of its antecedents in terms of features (hence-
forth backward features), and use the supervised
learning method to explore their influences on
pronoun resolution. In the study, we start our
exploration on the capability of the model by
applying it in an ideal environment where the
antecedents of the candidates are correctly iden-
tified and the backward features are optimally
set. The experiments on MUC-6 (1995) and
MUC-7 (1998) corpora show that incorporating
coreferential information of candidates boosts
the system performance significantly. Further,
we apply our model in the real resolution where
the antecedents of the candidates are provided
by separate noun phrase resolution modules.
The experimental results show that our model
still outperforms the baseline, even with the low
recall of the non-pronoun resolution module.
The remaining of this paper is organized as
follows. Section 2 discusses the importance of
the coreferential information for candidate eval-
uation. Section 3 introduces the baseline learn-
ing framework. Section 4 presents and evaluates
the learning model which uses backward fea-
tures to capture coreferential information, while
Section 5 proposes how to apply the model in
real resolution. Section 6 describes related re-
search work. Finally, conclusion is given in Sec-
tion 7.
2 The Impact of Coreferential
Information on Pronoun
Resolution
In pronoun resolution, the center of attention
throughout the discourse segment is a very im-
portant factor for antecedent selection (Mitkov,
1999). If a candidate is the focus (or center)
of the local discourse, it would be selected as
the antecedent with a high possibility. See the
following example,
<s> Gitano1 has pulled off a clever illusion2
with its3 advertising4. <s>
<s> The campaign5 gives its6 clothes a
youthful and trendy image to lure consumers
into the store. <s>
Table 1: A text segment from MUC-6 data set
In the above text, the pronoun ?its6? has
several antecedent candidates, i.e., ?Gitano1?,
?a clever illusion2?, ?its3?, ?its advertising4?
and ?The campaign5?. Without looking back,
?The campaign5? would be probably selected
because of its syntactic role (Subject) and its
distance to the anaphor. However, given the
knowledge that the company Gitano is the fo-
cus of the local context and ?its3? refers to
?Gitano1?, it would be clear that the pronoun
?its6? should be resolved to ?its3? and thus
?Gitano1?, rather than other competitors.
To determine whether a candidate is the ?fo-
cus? entity, we should check how the status (e.g.
grammatical functions) of the entity alternates
in the local context. Therefore, it is necessary
to track the NPs in the coreferential chain of
the candidate. For example, the syntactic roles
(i.e., subject) of the antecedents of ?its3? would
indicate that ?its3? refers to the most salient
entity in the discourse segment.
In our study, we keep the properties of the an-
tecedents as features of the candidates, and use
the supervised learning method to explore their
influence on pronoun resolution. Actually, to
determine the local focus, we only need to check
the entities in a short discourse segment. That
is, for a candidate, the number of its adjacent
antecedents to be checked is limited. Therefore,
we could evaluate the salience of a candidate
by looking back only its closest antecedent in-
stead of each element in its coreferential chain,
with the assumption that the closest antecedent
is able to provide sufficient information for the
evaluation.
3 The Baseline Learning Framework
Our baseline system adopts the common
learning-based framework employed in the sys-
tem by Soon et al (2001).
In the learning framework, each training or
testing instance takes the form of i{ana, candi},
where ana is the possible anaphor and candi is
its antecedent candidate1. An instance is associ-
ated with a feature vector to describe their rela-
tionships. As listed in Table 2, we only consider
those knowledge-poor and domain-independent
features which, although superficial, have been
proved efficient for pronoun resolution in many
previous systems.
During training, for each anaphor in a given
text, a positive instance is created by paring
the anaphor and its closest antecedent. Also a
set of negative instances is formed by paring the
anaphor and each of the intervening candidates.
Based on the training instances, a binary classi-
fier is generated using C5.0 learning algorithm
(Quinlan, 1993). During resolution, each possi-
ble anaphor ana, is paired in turn with each pre-
ceding antecedent candidate, candi, from right
to left to form a testing instance. This instance
is presented to the classifier, which will then
return a positive or negative result indicating
whether or not they are co-referent. The pro-
cess terminates once an instance i{ana, candi}
is labelled as positive, and ana will be resolved
to candi in that case.
4 The Learning Model Incorporating
Coreferential Information
The learning procedure in our model is similar
to the above baseline method, except that for
each candidate, we take into consideration its
closest antecedent, if possible.
4.1 Instance Structure
During both training and testing, we adopt the
same instance selection strategy as in the base-
line model. The only difference, however, is the
structure of the training or testing instances.
Specifically, each instance in our model is com-
posed of three elements like below:
1In our study candidates are filtered by checking the
gender, number and animacy agreements in advance.
Features describing the candidate (candi)
1. candi DefNp 1 if candi is a definite NP; else 0
2. candi DemoNP 1 if candi is an indefinite NP; else 0
3. candi Pron 1 if candi is a pronoun; else 0
4. candi ProperNP 1 if candi is a proper name; else 0
5. candi NE Type 1 if candi is an ?organization? named-entity; 2 if ?person?, 3 if
other types, 0 if not a NE
6. candi Human the likelihood (0-100) that candi is a human entity (obtained
from WordNet)
7. candi FirstNPInSent 1 if candi is the first NP in the sentence where it occurs
8. candi Nearest 1 if candi is the candidate nearest to the anaphor; else 0
9. candi SubjNP 1 if candi is the subject of the sentence it occurs; else 0
Features describing the anaphor (ana):
10. ana Reflexive 1 if ana is a reflexive pronoun; else 0
11. ana Type 1 if ana is a third-person pronoun (he, she,. . . ); 2 if a single
neuter pronoun (it,. . . ); 3 if a plural neuter pronoun (they,. . . );
4 if other types
Features describing the relationships between candi and ana:
12. SentDist Distance between candi and ana in sentences
13. ParaDist Distance between candi and ana in paragraphs
14. CollPattern 1 if candi has an identical collocation pattern with ana; else 0
Table 2: Feature set for the baseline pronoun resolution system
i{ana, candi, ante-of-candi}
where ana and candi, similar to the defini-
tion in the baseline model, are the anaphor and
one of its candidates, respectively. The new
added element in the instance definition, ante-
of-candi, is the possible closest antecedent of
candi in its coreferential chain. The ante-of-
candi is set to NIL in the case when candi has
no antecedent.
Consider the example in Table 1 again. For
the pronoun ?it6?, three training instances will
be generated, namely, i{its6, The compaign5,
NIL}, i{its6, its advertising4, NIL}, and
i{its6, its3, Gitano1}.
4.2 Backward Features
In addition to the features adopted in the base-
line system, we introduce a set of backward fea-
tures to describe the element ante-of-candi. The
ten features (15-24) are listed in Table 3 with
their respective possible values.
Like feature 1-9, features 15-22 describe the
lexical, grammatical and semantic properties of
ante-of-candi. The inclusion of the two features
Apposition (23) and candi NoAntecedent (24) is
inspired by the work of Strube (1998). The
feature Apposition marks whether or not candi
and ante-of-candi occur in the same appositive
structure. The underlying purpose of this fea-
ture is to capture the pattern that proper names
are accompanied by an appositive. The entity
with such a pattern may often be related to the
hearers? knowledge and has low preference. The
feature candi NoAntecedent marks whether or
not a candidate has a valid antecedent in the
preceding text. As stipulated in Strube?s work,
co-referring expressions belong to hearer-old en-
tities and therefore have higher preference than
other candidates. When the feature is assigned
value 1, all the other backward features (15-23)
are set to 0.
4.3 Results and Discussions
In our study we used the standard MUC-
6 and MUC-7 coreference corpora. In each
data set, 30 ?dry-run? documents were anno-
tated for training as well as 20-30 documents
for testing. The raw documents were prepro-
cessed by a pipeline of automatic NLP com-
ponents (e.g. NP chunker, part-of-speech tag-
ger, named-entity recognizer) to determine the
boundary of the NPs, and to provide necessary
information for feature calculation.
In an attempt to investigate the capability of
our model, we evaluated the model in an opti-
mal environment where the closest antecedent
of each candidate is correctly identified. MUC-
6 and MUC-7 can serve this purpose quite well;
the annotated coreference information in the
data sets enables us to obtain the correct closest
Features describing the antecedent of the candidate (ante-of-candi):
15. ante-candi DefNp 1 if ante-of-candi is a definite NP; else 0
16. ante-candi IndefNp 1 if ante-of-candi is an indefinite NP; else 0
17. ante-candi Pron 1 if ante-of-candi is a pronoun; else 0
18. ante-candi Proper 1 if ante-of-candi is a proper name; else 0
19. ante-candi NE Type 1 if ante-of-candi is an ?organization? named-entity; 2 if ?per-
son?, 3 if other types, 0 if not a NE
20. ante-candi Human the likelihood (0-100) that ante-of-candi is a human entity
21. ante-candi FirstNPInSent 1 if ante-of-candi is the first NP in the sentence where it occurs
22. ante-candi SubjNP 1 if ante-of-candi is the subject of the sentence where it occurs
Features describing the relationships between the candidate (candi) and ante-of-candi :
23. Apposition 1 if ante-of-candi and candi are in an appositive structure
Features describing the candidate (candi):
24. candi NoAntecedent 1 if candi has no antecedent available; else 0
Table 3: Backward features used to capture the coreferential information of a candidate
antecedent for each candidate and accordingly
generate the training and testing instances. In
the next section we will further discuss how to
apply our model into the real resolution.
Table 4 shows the performance of different
systems for resolving the pronominal anaphors 2
in MUC-6 and MUC-7. Default learning param-
eters for C5.0 were used throughout the exper-
iments. In this table we evaluated the perfor-
mance based on two kinds of measurements:
? ?Recall-and-Precision?:
Recall = #positive instances classified correctly#positive instances
Precision = #positive instances classified correctly#instances classified as positive
The above metrics evaluate the capability
of the learned classifier in identifying posi-
tive instances3. F-measure is the harmonic
mean of the two measurements.
? ?Success?:
Success = #anaphors resolved correctly#total anaphors
The metric4 directly reflects the pronoun
resolution capability.
The first and second lines of Table 4 compare
the performance of the baseline system (Base-
2The first and second person pronouns are discarded
in our study.
3The testing instances are collected in the same ways
as the training instances.
4In the experiments, an anaphor is considered cor-
rectly resolved only if the found antecedent is in the same
coreferential chain of the anaphor.
ante-candi_SubjNP = 1: 1 (49/5)
ante-candi_SubjNP = 0:
:..candi_SubjNP = 1:
:..SentDist = 2: 0 (3)
: SentDist = 0:
: :..candi_Human > 0: 1 (39/2)
: : candi_Human <= 0:
: : :..candi_NoAntecedent = 0: 1 (8/3)
: : candi_NoAntecedent = 1: 0 (3)
: SentDist = 1:
: :..ante-candi_Human <= 50 : 0 (4)
: ante-candi_Human > 50 : 1 (10/2)
:
candi_SubjNP = 0:
:..candi_Pron = 1: 1 (32/7)
candi_Pron = 0:
:..candi_NoAntecedent = 1:
:..candi_FirstNPInSent = 1: 1 (6/2)
: candi_FirstNPInSent = 0: ...
candi_NoAntecedent = 0: ...
Figure 1: Top portion of the decision tree
learned on MUC-6 with the backward features
line) and our system (Optimal), where DTpron
and DTpron?opt are the classifiers learned in
the two systems, respectively. The results in-
dicate that our system outperforms the base-
line system significantly. Compared with Base-
line, Optimal achieves gains in both recall (6.4%
for MUC-6 and 4.1% for MUC-7) and precision
(1.3% for MUC-6 and 9.0% for MUC-7). For
Success, we also observe an apparent improve-
ment by 4.7% (MUC-6) and 3.5% (MUC-7).
Figure 1 shows the portion of the pruned deci-
sion tree learned for MUC-6 data set. It visual-
izes the importance of the backward features for
the pronoun resolution on the data set. From
Testing Backward feature MUC-6 MUC-7
Experiments classifier assigner* R P F S R P F S
Baseline DTpron NIL 77.2 83.4 80.2 70.0 71.9 68.6 70.2 59.0
Optimal DTpron?opt (Annotated) 83.6 84.7 84.1 74.7 76.0 77.6 76.8 62.5
RealResolve-1 DTpron?opt DTpron?opt 75.8 83.8 79.5 73.1 62.3 77.7 69.1 53.8
RealResolve-2 DTpron?opt DTpron 75.8 83.8 79.5 73.1 63.0 77.9 69.7 54.9
RealResolve-3 DT?pron DTpron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
RealResolve-4 DT?pron DT
?
pron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7
(*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to
RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon?pron.)
the tree we could find that:
1.) Feature ante-candi SubjNP is of the most
importance as the root feature of the tree.
The decision tree would first examine the
syntactic role of a candidate?s antecedent,
followed by that of the candidate. This
nicely proves our assumption that the prop-
erties of the antecedents of the candidates
provide very important information for the
candidate evaluation.
2.) Both features ante-candi SubjNP and
candi SubjNP rank top in the decision tree.
That is, for the reference determination,
the subject roles of the candidate?s referent
within a discourse segment will be checked
in the first place. This finding supports well
the suggestion in centering theory that the
grammatical relations should be used as the
key criteria to rank forward-looking centers
in the process of focus tracking (Brennan
et al, 1987; Grosz et al, 1995).
3.) candi Pron and candi NoAntecedent are
to be examined in the cases when the
subject-role checking fails, which confirms
the hypothesis in the S-List model by
Strube (1998) that co-refereing candidates
would have higher preference than other
candidates in the pronoun resolution.
5 Applying the Model in Real
Resolution
In Section 4 we explored the effectiveness of
the backward feature for pronoun resolution. In
those experiments our model was tested in an
ideal environment where the closest antecedent
of a candidate can be identified correctly when
generating the feature vector. However, during
real resolution such coreferential information is
not available, and thus a separate module has
algorithm PRON-RESOLVE
input:
DTnon?pron: classifier for resolving non-pronouns
DTpron: classifier for resolving pronouns
begin:
M1..n:= the valid markables in the given docu-
ment
Ante[1..n] := 0
for i = 1 to N
for j = i - 1 downto 0
if (Mi is a non-pron and
DTnon?pron(i{Mi,Mj}) == + )
or
(Mi is a pron and
DTpron(i{Mi,Mj , Ante[j]}) == +)
then
Ante[i] := Mj
break
return Ante
Figure 2: The pronoun resolution algorithm by
incorporating coreferential information of can-
didates
to be employed to obtain the closest antecedent
for a candidate. We describe the algorithm in
Figure 2.
The algorithm takes as input two classifiers,
one for the non-pronoun resolution and the
other for pronoun resolution. Given a testing
document, the antecedent of each NP is identi-
fied using one of these two classifiers, depending
on the type of NP. Although a separate non-
pronoun resolution module is required for the
pronoun resolution task, this is usually not a
big problem as these two modules are often in-
tegrated in coreference resolution systems. We
just use the results of the one module to improve
the performance of the other.
5.1 New Training and Testing
Procedures
For a pronominal candidate, its antecedent can
be obtained by simply using DTpron?opt. For
Training Procedure:
T1. Train a non-pronoun resolution clas-
sifier DTnon?pron and a pronoun resolution
classifier DTpron, using the baseline learning
framework (without backward features).
T2. Apply DTnon?pron and DTpron to iden-
tify the antecedent of each non-pronominal
and pronominal markable, respectively, in a
given document.
T3. Go through the document again. Gen-
erate instances with backward features as-
signed using the antecedent information ob-
tained in T2.
T4. Train a new pronoun resolution classifier
DT?pron on the instances generated in T3.
Testing Procedure:
R1. For each given document, do T2?T3.
R2. Resolve pronouns by applying DT?pron.
Table 5: New training and testing procedures
a non-pronominal candidate, we built a non-
pronoun resolution module to identify its an-
tecedent. The module is a duplicate of the
NP coreference resolution system by Soon et
al. (2001)5 , which uses the similar learn-
ing framework as described in Section 3. In
this way, we could do pronoun resolution
just by running PRON-RESOLVE(DTnon?pron,
DTpron?opt), where DTnon?pron is the classifier
of the non-pronoun resolution module.
One problem, however, is that DTpron?opt is
trained on the instances whose backward fea-
tures are correctly assigned. During real resolu-
tion, the antecedent of a candidate is found by
DTnon?pron or DTpron?opt, and the backward
feature values are not always correct. Indeed,
for most noun phrase resolution systems, the
recall is not very high. The antecedent some-
times can not be found, or is not the closest
one in the preceding coreferential chain. Con-
sequently, the classifier trained on the ?perfect?
feature vectors would probably fail to output
anticipated results on the noisy data during real
resolution.
Thus we modify the training and testing pro-
cedures of the system. For both training and
testing instances, we assign the backward fea-
ture values based on the results from separate
NP resolution modules. The detailed proce-
dures are described in Table 5.
5Details of the features can be found in Soon et al
(2001)
algorithm REFINE-CLASSIFIER
begin:
DT1pron := DT
?
pron
for i = 1 to ?
Use DTipron to update the antecedents of
pronominal candidates and the correspond-
ing backward features;
Train DTi+1pron based on the updated training
instances;
if DTi+1pron is not better than DTipron then
break;
return DTipron
Figure 3: The classifier refining algorithm
The idea behind our approach is to train
and test the pronoun resolution classifier on
instances with feature values set in a consis-
tent way. Here the purpose of DTpron and
DTnon?pron is to provide backward feature val-
ues for training and testing instances. From this
point of view, the two modules could be thought
of as a preprocessing component of our pronoun
resolution system.
5.2 Classifier Refining
If the classifier DT?pron outperforms DTpron
as expected, we can employ DT?pron in place
of DTpron to generate backward features for
pronominal candidates, and then train a clas-
sifier DT??pron based on the updated training in-
stances. Since DT?pron produces more correct
feature values than DTpron, we could expect
that DT??pron will not be worse, if not better,
than DT?pron. Such a process could be repeated
to refine the pronoun resolution classifier. The
algorithm is described in Figure 3.
In algorithm REFINE-CLASSIFIER, the it-
eration terminates when the new trained clas-
sifier DTi+1pron provides no further improvement
than DTipron. In this case, we can replace
DTi+1pron by DTipron during the i+1(th) testing
procedure. That means, by simply running
PRON-RESOLVE(DTnon?pron,DTipron), we can
use for both backward feature computation and
instance classification tasks, rather than apply-
ing DTpron and DT?pron subsequently.
5.3 Results and Discussions
In the experiments we evaluated the perfor-
mance of our model in real pronoun resolution.
The performance of our model depends on the
performance of the non-pronoun resolution clas-
sifier, DTnon?pron. Hence we first examined the
coreference resolution capability of DTnon?pron
based on the standard scoring scheme by Vi-
lain et al (1995). For MUC-6, the module ob-
tains 62.2% recall and 78.8% precision, while for
MUC-7, it obtains 50.1% recall and 75.4% pre-
cision. The poor recall and comparatively high
precision reflect the capability of the state-of-
the-art learning-based NP resolution systems.
The third block of Table 4 summarizes the
performance of the classifier DTpron?opt in real
resolution. In the systems RealResolve-1 and
RealResolve-2, the antecedents of pronominal
candidates are found by DTpron?opt and DTpron
respectively, while in both systems the an-
tecedents of non-pronominal candidates are by
DTnon?pron. As shown in the table, compared
with the Optimal where the backward features
of testing instances are optimally assigned, the
recall rates of two systems drop largely by 7.8%
for MUC-6 and by about 14% for MUC-7. The
scores of recall are even lower than those of
Baseline. As a result, in comparison with Op-
timal, we see the degrade of the F-measure and
the success rate, which confirms our hypothesis
that the classifier learned on perfect training in-
stances would probably not perform well on the
noisy testing instances.
The system RealResolve-3 listed in the fifth
line of the table uses the classifier trained
and tested on instances whose backward fea-
tures are assigned according to the results from
DTnon?pron and DTpron. From the table we can
find that: (1) Compared with Baseline, the sys-
tem produces gains in recall (2.1% for MUC-6
and 2.8% for MUC-7) with no significant loss
in precision. Overall, we observe the increase in
F-measure for both data sets. If measured by
Success, the improvement is more apparent by
4.7% (MUC-6) and 1.8% (MUC-7). (2) Com-
pared with RealResolve-1(2), the performance
decrease of RealResolve-3 against Optimal is
not so large. Especially for MUC-6, the system
obtains a success rate as high as Optimal.
The above results show that our model can
be successfully applied in the real pronoun res-
olution task, even given the low recall of the
current non-pronoun resolution module. This
should be owed to the fact that for a candidate,
its adjacent antecedents, even not the closest
one, could give clues to reflect its salience in
the local discourse. That is, the model prefers a
high precision to a high recall, which copes well
with the capability of the existing non-pronoun
resolution module.
In our experiments we also tested the clas-
sifier refining algorithm described in Figure 3.
We found that for both MUC-6 and MUC-7
data set, the algorithm terminated in the second
round. The comparison of DT2pron and DT1pron
(i.e. DT?pron) showed that these two trees were
exactly the same. The algorithm converges fast
probably because in the data set, most of the
antecedent candidates are non-pronouns (89.1%
for MUC-6 and 83.7% for MUC-7). Conse-
quently, the ratio of the training instances with
backward features changed may be not substan-
tial enough to affect the classifier generation.
Although the algorithm provided no further
refinement for DT?pron, we can use DT
?
pron, as
suggested in Section 5.2, to calculate back-
ward features and classify instances by running
PRON-RESOLVE(DTnon?pron, DT?pron). The
results of such a system, RealResolve-4, are
listed in the last line of Table 4. For both MUC-
6 and MUC-7, RealResolve-4 obtains exactly
the same performance as RealResolve-3.
6 Related Work
To our knowledge, our work is the first ef-
fort that systematically explores the influence of
coreferential information of candidates on pro-
noun resolution in learning-based ways. Iida et
al. (2003) also take into consideration the con-
textual clues in their coreference resolution sys-
tem, by using two features to reflect the ranking
order of a candidate in Salience Reference List
(SRL). However, similar to common centering
models, in their system the ranking of entities
in SRL is also heuristic-based.
The coreferential chain length of a candidate,
or its variants such as occurrence frequency and
TFIDF, has been used as a salience factor in
some learning-based reference resolution sys-
tems (Iida et al, 2003; Mitkov, 1998; Paul et
al., 1999; Strube and Muller, 2003). However,
for an entity, the coreferential length only re-
flects its global salience in the whole text(s), in-
stead of the local salience in a discourse segment
which is nevertheless more informative for pro-
noun resolution. Moreover, during resolution,
the found coreferential length of an entity is of-
ten incomplete, and thus the obtained length
value is usually inaccurate for the salience eval-
uation.
7 Conclusion and Future Work
In this paper we have proposed a model which
incorporates coreferential information of candi-
dates to improve pronoun resolution. When
evaluating a candidate, the model considers its
adjacent antecedent by describing its properties
in terms of backward features. We first exam-
ined the effectiveness of the model by applying
it in an optimal environment where the clos-
est antecedent of a candidate is obtained cor-
rectly. The experiments show that it boosts
the success rate of the baseline system for both
MUC-6 (4.7%) and MUC-7 (3.5%). Then we
proposed how to apply our model in the real res-
olution where the antecedent of a non-pronoun
is found by an additional non-pronoun resolu-
tion module. Our model can still produce Suc-
cess improvement (4.7% for MUC-6 and 1.8%
for MUC-7) against the baseline system, de-
spite the low recall of the non-pronoun resolu-
tion module.
In the current work we restrict our study only
to pronoun resolution. In fact, the coreferential
information of candidates is expected to be also
helpful for non-pronoun resolution. We would
like to investigate the influence of the coreferen-
tial factors on general NP reference resolution in
our future work.
References
S. Brennan, M. Friedman, and C. Pollard.
1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting of
the Association for Compuational Linguis-
tics, pages 155?162.
N. Ge, J. Hale, and E. Charniak. 1998. A
statistical approach to anaphora resolution.
In Proceedings of the 6th Workshop on Very
Large Corpora.
B. Grosz, A. Joshi, and S. Weinstein. 1983.
Providing a unified account of definite noun
phrases in discourse. In Proceedings of the
21st Annual meeting of the Association for
Computational Linguistics, pages 44?50.
B. Grosz, A. Joshi, and S. Weinstein. 1995.
Centering: a framework for modeling the
local coherence of discourse. Computational
Linguistics, 21(2):203?225.
R. Iida, K. Inui, H. Takamura, and Y. Mat-
sumoto. 2003. Incorporating contextual cues
in trainable models for coreference resolu-
tion. In Proceedings of the 10th Confer-
ence of EACL, Workshop ?The Computa-
tional Treatment of Anaphora?.
R. Mitkov. 1998. Robust pronoun resolution
with limited knowledge. In Proceedings of the
17th Int. Conference on Computational Lin-
guistics, pages 869?875.
R. Mitkov. 1999. Anaphora resolution: The
state of the art. Technical report, University
of Wolverhampton.
MUC-6. 1995. Proceedings of the Sixth Message
Understanding Conference. Morgan Kauf-
mann Publishers, San Francisco, CA.
MUC-7. 1998. Proceedings of the Seventh
Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA.
V. Ng and C. Cardie. 2002. Improving machine
learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, pages 104?111, Philadelphia.
M. Paul, K. Yamamoto, and E. Sumita. 1999.
Corpus-based anaphora resolution towards
antecedent preference. In Proceedings of
the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop ?Coreference and It?s Applications?,
pages 47?52.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publish-
ers, San Francisco, CA.
C. Sidner. 1981. Focusing for interpretation
of pronouns. American Journal of Computa-
tional Linguistics, 7(4):217?231.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference reso-
lution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
M. Strube and C. Muller. 2003. A machine
learning approach to pronoun resolution in
spoken dialogue. In Proceedings of the 41st
Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Japan.
M. Strube. 1998. Never look back: An alterna-
tive to centering. In Proceedings of the 17th
Int. Conference on Computational Linguis-
tics and 36th Annual Meeting of ACL, pages
1251?1257.
J. R. Tetreault. 2001. A corpus-based eval-
uation of centering and pronoun resolution.
Computational Linguistics, 27(4):507?520.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan.
2003. Coreference resolution using competi-
tion learning approach. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, Japan.
Proceedings of the 43rd Annual Meeting of the ACL, pages 165?172,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Improving Pronoun Resolution Using Statistics-Based
Semantic Compatibility Information
Xiaofeng Yang?? Jian Su? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
In this paper we focus on how to improve
pronoun resolution using the statistics-
based semantic compatibility information.
We investigate two unexplored issues that
influence the effectiveness of such in-
formation: statistics source and learning
framework. Specifically, we for the first
time propose to utilize the web and the
twin-candidate model, in addition to the
previous combination of the corpus and
the single-candidate model, to compute
and apply the semantic information. Our
study shows that the semantic compatibil-
ity obtained from the web can be effec-
tively incorporated in the twin-candidate
learning model and significantly improve
the resolution of neutral pronouns.
1 Introduction
Semantic compatibility is an important factor for
pronoun resolution. Since pronouns, especially neu-
tral pronouns, carry little semantics of their own,
the compatibility between an anaphor and its an-
tecedent candidate is commonly evaluated by ex-
amining the relationships between the candidate and
the anaphor?s context, based on the statistics that the
corresponding predicate-argument tuples occur in a
particular large corpus. Consider the example given
in the work of Dagan and Itai (1990):
(1) They know full well that companies held tax
money aside for collection later on the basis
that the government said it1 was going to col-
lect it2.
For anaphor it1, the candidate government should
have higher semantic compatibility than money be-
cause government collect is supposed to occur more
frequently than money collect in a large corpus. A
similar pattern could also be observed for it2.
So far, the corpus-based semantic knowledge has
been successfully employed in several anaphora res-
olution systems. Dagan and Itai (1990) proposed
a heuristics-based approach to pronoun resolu-
tion. It determined the preference of candidates
based on predicate-argument frequencies. Recently,
Bean and Riloff (2004) presented an unsupervised
approach to coreference resolution, which mined
the co-referring NP pairs with similar predicate-
arguments from a large corpus using a bootstrapping
method.
However, the utility of the corpus-based se-
mantics for pronoun resolution is often argued.
Kehler et al (2004), for example, explored the
usage of the corpus-based statistics in supervised
learning based systems, and found that such infor-
mation did not produce apparent improvement for
the overall pronoun resolution. Indeed, existing
learning-based approaches to anaphor resolution
have performed reasonably well using limited
and shallow knowledge (e.g., Mitkov (1998),
Soon et al (2001), Strube and Muller (2003)).
Could the relatively noisy semantic knowledge give
us further system improvement?
In this paper we focus on improving pronominal
anaphora resolution using automatically computed
semantic compatibility information. We propose to
enhance the utility of the statistics-based knowledge
from two aspects:
Statistics source. Corpus-based knowledge usu-
ally suffers from data sparseness problem. That is,
many predicate-argument tuples would be unseen
even in a large corpus. A possible solution is the
165
web. It is believed that the size of the web is thou-
sands of times larger than normal large corpora, and
the counts obtained from the web are highly corre-
lated with the counts from large balanced corpora
for predicate-argument bi-grams (Keller and Lapata,
2003). So far the web has been utilized in nominal
anaphora resolution (Modjeska et al, 2003; Poesio
et al, 2004) to determine the semantic relation be-
tween an anaphor and candidate pair. However, to
our knowledge, using the web to help pronoun reso-
lution still remains unexplored.
Learning framework. Commonly, the predicate-
argument statistics is incorporated into anaphora res-
olution systems as a feature. What kind of learn-
ing framework is suitable for this feature? Previous
approaches to anaphora resolution adopt the single-
candidate model, in which the resolution is done on
an anaphor and one candidate at a time (Soon et al,
2001; Ng and Cardie, 2002). However, as the pur-
pose of the predicate-argument statistics is to eval-
uate the preference of the candidates in semantics,
it is possible that the statistics-based semantic fea-
ture could be more effectively applied in the twin-
candidate (Yang et al, 2003) that focusses on the
preference relationships among candidates.
In our work we explore the acquisition of the se-
mantic compatibility information from the corpus
and the web, and the incorporation of such semantic
information in the single-candidate model and the
twin-candidate model. We systematically evaluate
the combinations of different statistics sources and
learning frameworks in terms of their effectiveness
in helping the resolution. Results on the MUC data
set show that for neutral pronoun resolution in which
an anaphor has no specific semantic category, the
web-based semantic information would be the most
effective when applied in the twin-candidate model:
Not only could such a system significantly improve
the baseline without the semantic feature, it also out-
performs the system with the combination of the cor-
pus and the single-candidate model (by 11.5% suc-
cess).
The rest of this paper is organized as follows. Sec-
tion 2 describes the acquisition of the semantic com-
patibility information from the corpus and the web.
Section 3 discusses the application of the statistics
in the single-candidate and twin-candidate learning
models. Section 4 gives the experimental results,
and finally, Section 5 gives the conclusion.
2 Computing the Statistics-based Semantic
Compatibility
In this section, we introduce in detail how to com-
pute the semantic compatibility, using the predicate-
argument statistics obtained from the corpus or the
web.
2.1 Corpus-Based Semantic Compatibility
Three relationships, possessive-noun, subject-verb
and verb-object, are considered in our work. Be-
fore resolution a large corpus is prepared. Doc-
uments in the corpus are processed by a shallow
parser that could generate predicate-argument tuples
of the above three relationships1.
To reduce data sparseness, the following steps are
applied in each resulting tuple, automatically:
? Only the nominal or verbal heads are retained.
? Each Named-Entity (NE) is replaced by a com-
mon noun which corresponds to the seman-
tic category of the NE (e.g. ?IBM? ? ?com-
pany?) 2.
? All words are changed to their base morpho-
logic forms (e.g. ?companies ? company?).
During resolution, for an encountered anaphor,
each of its antecedent candidates is substituted with
the anaphor . According to the role and type of the
anaphor in its context, a predicate-argument tuple is
extracted and the above three steps for data-sparse
reduction are applied. Consider the sentence (1),
for example. The anaphors ?it1? and ?it2? indicate
a subject verb and verb object relationship, respec-
tively. Thus, the predicate-argument tuples for the
two candidates ?government? and ?money? would
be (collect (subject government)) and (collect (sub-
ject money)) for ?it1?, and (collect (object govern-
ment)) and (collect (object money)) for ?it2?.
Each extracted tuple is searched in the prepared
tuples set of the corpus, and the times the tuple oc-
curs are calculated. For each candidate, its semantic
1The possessive-noun relationship involves the forms like
?NP2 of NP1? and ?NP1?s NP2?.
2In our study, the semantic category of a NE is identified
automatically by the pre-processing NE recognition component.
166
compatibility with the anaphor could be represented
simply in terms of frequency
StatSem(candi, ana) = count(candi, ana) (1)
where count(candi, ana) is the count of the tuple
formed by candi and ana, or alternatively, in terms
of conditional probability (P (candi, ana|candi)),
where the count of the tuple is divided by the count
of the single candidate in the corpus. That is
StatSem(candi, ana) = count(candi, ana)count(candi) (2)
In this way, the statistics would not bias candidates
that occur frequently in isolation.
2.2 Web-Based Semantic Compatibility
Unlike documents in normal corpora, web pages
could not be preprocessed to generate the predicate-
argument reserve. Instead, the predicate-argument
statistics has to be obtained via a web search engine
like Google and Altavista. For the three types of
predicate-argument relationships, queries are con-
structed in the forms of ?NPcandi VP? (for subject-
verb), ?VP NPcandi? (for verb-object), and ?NPcandi
?s NP? or ?NP of NPcandi? (for possessive-noun).
Consider the following sentence:
(2) Several experts suggested that IBM?s account-
ing grew much more liberal since the mid 1980s
as its business turned sour.
For the pronoun ?its? and the candidate ?IBM?, the
two generated queries are ?business of IBM? and
?IBM?s business?.
To reduce data sparseness, in an initial query only
the nominal or verbal heads are retained. Also, each
NE is replaced by the corresponding common noun.
(e.g, ?IBM?s business? ? ?company?s business? and
?business of IBM? ? ?business of company?).
A set of inflected queries is generated by ex-
panding a term into all its possible morphologi-
cal forms. For example, in Sentence (1), ?collect
money? becomes ?collected|collecting|... money?,
and in (2) ?business of company? becomes ?business
of company|companies?). Besides, determiners are
inserted for every noun. If the noun is the candidate
under consideration, only the definite article the is
inserted. For other nouns, instead, a/an, the and the
empty determiners (for bare plurals) would be added
(e.g., ?the|a business of the company|companies?).
Queries are submitted to a particular web search
engine (Google in our study). All queries are per-
formed as exact matching. Similar to the corpus-
based statistics, the compatibility for each candidate
and anaphor pair could be represented using either
frequency (Eq. 1) or probability (Eq. 2) metric. In
such a situation, count(candi, ana) is the hit num-
ber of the inflected queries returned by the search
engine, while count(candi) is the hit number of the
query formed with only the head of the candidate
(i.e.,?the + candi?).
3 Applying the Semantic Compatibility
In this section, we discuss how to incorporate the
statistics-based semantic compatibility for pronoun
resolution, in a machine learning framework.
3.1 The Single-Candidate Model
One way to utilize the semantic compatibility is to
take it as a feature under the single-candidate learn-
ing model as employed by Ng and Cardie (2002).
In such a learning model, each training or testing
instance takes the form of i{C, ana}, where ana is
the possible anaphor and C is its antecedent candi-
date. An instance is associated with a feature vector
to describe their relationships.
During training, for each anaphor in a given text,
a positive instance is created by pairing the anaphor
and its closest antecedent. Also a set of negative in-
stances is formed by pairing the anaphor and each
of the intervening candidates. Based on the train-
ing instances, a binary classifier is generated using a
certain learning algorithm, like C5 (Quinlan, 1993)
in our work.
During resolution, given a new anaphor, a test in-
stance is created for each candidate. This instance is
presented to the classifier, which then returns a pos-
itive or negative result with a confidence value indi-
cating the likelihood that they are co-referent. The
candidate with the highest confidence value would
be selected as the antecedent.
3.2 Features
In our study we only consider those domain-
independent features that could be obtained with low
167
Feature Description
DefNp 1 if the candidate is a definite NP; else 0
Pron 1 if the candidate is a pronoun; else 0
NE 1 if the candidate is a named entity; else 0
SameSent 1 if the candidate and the anaphor is in the same sentence; else 0
NearestNP 1 if the candidate is nearest to the anaphor; else 0
ParalStuct 1 if the candidate has an parallel structure with ana; else 0
FirstNP 1 if the candidate is the first NP in a sentence; else 0
Reflexive 1 if the anaphor is a reflexive pronoun; else 0
Type Type of the anaphor (0: Single neuter pronoun; 1: Plural neuter pronoun; 2:
Male personal pronoun; 3: Female personal pronoun)
StatSem? the statistics-base semantic compatibility of the candidate
SemMag?? the semantic compatibility difference between two competing candidates
Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model
while **ed feature is only for the twin-candidate mode)
computational cost but with high reliability. Table 1
summarizes the features with their respective possi-
ble values. The first three features represent the lex-
ical properties of a candidate. The POS properties
could indicate whether a candidate refers to a hearer-
old entity that would have a higher preference to be
selected as the antecedent (Strube, 1998). SameSent
and NearestNP mark the distance relationships be-
tween an anaphor and the candidate, which would
significantly affect the candidate selection (Hobbs,
1978). FirstNP aims to capture the salience of the
candidate in the local discourse segment. ParalStuct
marks whether a candidate and an anaphor have sim-
ilar surrounding words, which is also a salience fac-
tor for the candidate evaluation (Mitkov, 1998).
Feature StatSem records the statistics-based se-
mantic compatibility computed, from the corpus or
the web, by either frequency or probability metric,
as described in the previous section. If a candidate
is a pronoun, this feature value would be set to that
of its closest nominal antecedent.
As described, the semantic compatibility of a can-
didate is computed under the context of the cur-
rent anaphor. Consider two occurrences of anaphors
?. . . it1 collected . . . ? and ?. . . it2 said . . . ?. As ?NP
collected? should occur less frequently than ?NP
said?, the candidates of it1 would generally have
predicate-argument statistics lower than those of it2.
That is, a positive instance for it1 might bear a lower
semantic feature value than a negative instance for
it2. The consequence is that the learning algorithm
would think such a feature is not that ?indicative?
and reduce its salience in the resulting classifier.
One way to tackle this problem is to normalize the
feature by the frequencies of the anaphor?s context,
e.g., ?count(collected)? and ?count(said)?. This,
however, would require extra calculation. In fact,
as candidates of a specific anaphor share the same
anaphor context, we can just normalize the semantic
feature of a candidate by that of its competitor:
StatSemN (C, ana) = StatSem(C, ana)max
ci?candi set(ana)
StatSem(ci, ana)
The value (0 ? 1) represents the rank of the
semantic compatibility of the candidate C among
candi set(ana), the current candidates of ana.
3.3 The Twin-Candidate Model
Yang et al (2003) proposed an alternative twin-
candidate model for anaphora resolution task. The
strength of such a model is that unlike the single-
candidate model, it could capture the preference re-
lationships between competing candidates. In the
model, candidates for an anaphor are paired and
features from two competing candidates are put to-
gether for consideration. This property could nicely
deal with the above mentioned training problem of
different anaphor contexts, because the semantic
feature would be considered under the current can-
didate set only. In fact, as semantic compatibility is
168
a preference-based factor for anaphor resolution, it
would be incorporated in the twin-candidate model
more naturally.
In the twin-candidate model, an instance takes a
form like i{C1, C2, ana}, where C1 and C2 are two
candidates. We stipulate that C2 should be closer to
ana than C1 in distance. The instance is labelled as
?10? if C1 the antecedent, or ?01? if C2 is.
During training, for each anaphor, we find its
closest antecedent, Cante. A set of ?10? instances,
i{Cante, C, ana}, is generated by pairing Cante and
each of the interning candidates C. Also a set of ?01?
instances, i{C, Cante, ana}, is created by pairing
Cante with each candidate before Cante until another
antecedent, if any, is reached.
The resulting pairwise classifier would return
?10? or ?01? indicating which candidate is preferred
to the other. During resolution, candidates are paired
one by one. The score of a candidate is the total
number of the competitors that the candidate wins
over. The candidate with the highest score would be
selected as the antecedent.
Features The features for the twin-candidate
model are similar to those for the single-candidate
model except that a duplicate set of features has to
be prepared for the additional candidate. Besides,
a new feature, SemMag, is used in place of Stat-
Sem to represent the difference magnitude between
the semantic compatibility of two candidates. Let
mag = StatSem(C1, ana)/StatSem(C2, ana), feature
SemMag is defined as follows,
SemMag(C1, C2, ana) =
{
mag ? 1 : mag >= 1
1?mag?1 : mag < 1
The positive or negative value marks the times that
the statistics of C1 is larger or smaller than C2.
4 Evaluation and Discussion
4.1 Experiment Setup
In our study we were only concerned about the third-
person pronoun resolution. With an attempt to ex-
amine the effectiveness of the semantic feature on
different types of pronouns, the whole resolution
was divided into neutral pronoun (it & they) reso-
lution and personal pronoun (he & she) resolution.
The experiments were done on the newswire do-
main, using MUC corpus (Wall Street Journal ar-
ticles). The training was done on 150 documents
from MUC-6 coreference data set, while the testing
was on the 50 formal-test documents of MUC-6 (30)
and MUC-7 (20). Throughout the experiments, de-
fault learning parameters were applied to the C5 al-
gorithm. The performance was evaluated based on
success, the ratio of the number of correctly resolved
anaphors over the total number of anaphors.
An input raw text was preprocessed automati-
cally by a pipeline of NLP components. The noun
phrase identification and the predicate-argument ex-
traction were done based on the results of a chunk
tagger, which was trained for the shared task of
CoNLL-2000 and achieved 92% accuracy (Zhou et
al., 2000). The recognition of NEs as well as their
semantic categories was done by a HMM based
NER, which was trained for the MUC NE task
and obtained high F-scores of 96.9% (MUC-6) and
94.3% (MUC-7) (Zhou and Su, 2002).
For each anaphor, the markables occurring within
the current and previous two sentences were taken
as the initial candidates. Those with mismatched
number and gender agreements were filtered from
the candidate set. Also, pronouns or NEs that dis-
agreed in person with the anaphor were removed in
advance. For the training set, there are totally 645
neutral pronouns and 385 personal pronouns with
non-empty candidate set, while for the testing set,
the number is 245 and 197.
4.2 The Corpus and the Web
The corpus for the predicate-argument statistics
computation was from the TIPSTER?s Text Re-
search Collection (v1994). Consisting of 173,252
Wall Street Journal articles from the year 1988 to
1992, the data set contained about 76 million words.
The documents were preprocessed using the same
POS tagging and NE-recognition components as in
the pronoun resolution task. Cass (Abney, 1996), a
robust chunker parser was then applied to generate
the shallow parse trees, which resulted in 353,085
possessive-noun tuples, 759,997 verb-object tuples
and 1,090,121 subject-verb tuples.
We examined the capacity of the web and the
corpus in terms of zero-count ratio and count num-
ber. On average, among the predicate-argument tu-
ples that have non-zero corpus-counts, above 93%
have also non-zero web-counts. But the ratio is only
around 40% contrariwise. And for the predicate-
169
Neutral Pron Personal Pron Overall
Learning Model System Corpus Web Corpus Web Corpus Web
baseline 65.7 86.8 75.1
+frequency 67.3 69.9 86.8 86.8 76.0 76.9
Single-Candidate +normalized frequency 66.9 67.8 86.8 86.8 75.8 76.2
+probability 65.7 65.7 86.8 86.8 75.1 75.1
+normalized probability 67.7 70.6 86.8 86.8 76.2 77.8
baseline 73.9 91.9 81.9
Twin-Candidate +frequency 76.7 79.2 91.4 91.9 83.3 84.8
+probability 75.9 78.0 91.4 92.4 82.8 84.4
Table 2: The performance of different resolution systems
Relationship N-Pron P-Pron
Possessive-Noun 0.508 0.517
Verb-Object 0.503 0.526
Subject-Verb 0.619 0.676
Table 3: Correlation between web and corpus counts
on the seen predicate-argument tuples
argument tuples that could be seen in both data
sources, the count from the web is above 2000 times
larger than that from the corpus.
Although much less sparse, the web counts are
significantly noisier than the corpus count since no
tagging, chunking and parsing could be carried out
on the web pages. However, previous study (Keller
and Lapata, 2003) reveals that the large amount of
data available for the web counts could outweigh the
noisy problems. In our study we also carried out a
correlation analysis3 to examine whether the counts
from the web and the corpus are linearly related,
on the predicate-argument tuples that can be seen
in both data sources. From the results listed in Ta-
ble 3, we observe moderately high correlation, with
coefficients ranging from 0.5 to 0.7 around, between
the counts from the web and the corpus, for both
neutral pronoun (N-Pron) and personal pronoun (P-
Pron) resolution tasks.
4.3 System Evaluation
Table 2 summarizes the performance of the systems
with different combinations of statistics sources and
learning frameworks. The systems without the se-
3All the counts were log-transformed and the correlation co-
efficients were evaluated based on Pearsons? r.
mantic feature were used as the baseline. Under the
single-candidate (SC) model, the baseline system
obtains a success of 65.7% and 86.8% for neutral
pronoun and personal pronoun resolution, respec-
tively. By contrast, the twin-candidate (TC) model
achieves a significantly (p ? 0.05, by two-tailed t-
test) higher success of 73.9% and 91.9%, respec-
tively. Overall, for the whole pronoun resolution,
the baseline system under the TC model yields a
success 81.9%, 6.8% higher than SC does4. The
performance is comparable to most state-of-the-art
pronoun resolution systems on the same data set.
Web-based feature vs. Corpus-based feature
The third column of the table lists the results us-
ing the web-based compatibility feature for neutral
pronouns. Under both SC and TC models, incorpo-
ration of the web-based feature significantly boosts
the performance of the baseline: For the best sys-
tem in the SC model and the TC model, the success
rate is improved significantly by around 4.9% and
5.3%, respectively. A similar pattern of improve-
ment could be seen for the corpus-based semantic
feature. However, the increase is not as large as
using the web-based feature: Under the two learn-
ing models, the success rate of the best system with
the corpus-based feature rises by up to 2.0% and
2.8% respectively, about 2.9% and 2.5% less than
that of the counterpart systems with the web-based
feature. The larger size and the better counts of the
web against the corpus, as reported in Section 4.2,
4The improvement against SC is higher than that reported
in (Yang et al, 2003). It should be because we now used 150
training documents rather than 30 ones as in the previous work.
The TC model would benefit from larger training data set as it
uses more features (more than double) than SC.
170
should contribute to the better performance.
Single-candidate model vs. Twin-Candidate
model The difference between the SC and the TC
model is obvious from the table. For the N-Pron
and P-Pron resolution, the systems under TC could
outperform the counterpart systems under SC by
above 5% and 8% success, respectively. In addition,
the utility of the statistics-based semantic feature is
more salient under TC than under SC for N-Pron res-
olution: the best gains using the corpus-based and
the web-based semantic features under TC are 2.9%
and 5.3% respectively, higher than those under the
SC model using either un-normalized semantic fea-
tures (1.6% and 3.3%), or normalized semantic fea-
tures (2.0% and 4.9%). Although under SC, the nor-
malized semantic feature could result in a gain close
to under TC, its utility is not stable: with metric fre-
quency, using the normalized feature performs even
worse than using the un-normalized one. These re-
sults not only affirm the claim by Yang et al (2003)
that the TC model is superior to the SC model for
pronoun resolution, but also indicate that TC is more
reliable than SC in applying the statistics-based se-
mantic feature, for N-Pron resolution.
Web+TC vs. Other combinations The above
analysis has exhibited the superiority of the web
over the corpus, and the TC model over the
SC model. The experimental results also re-
veal that using the the web-based semantic fea-
ture together with the TC model is able to further
boost the resolution performance for neutral pro-
nouns. The system with such a Web+TC combi-
nation could achieve a high success of 79.2%, de-
feating all the other possible combinations. Es-
pecially, it considerably outperforms (up to 11.5%
success) the system with the Corpus+SC combina-
tion, which is commonly adopted in previous work
(e.g., Kehler et al (2004)).
Personal pronoun resolution vs. Neutral pro-
noun resolution Interestingly, the statistics-based
semantic feature has no effect on the resolution of
personal pronouns, as shown in the table 2. We
found in the learned decision trees such a feature
did not occur (SC) or only occurred in bottom nodes
(TC). This should be because personal pronouns
have strong restriction on the semantic category (i.e.,
human) of the candidates. A non-human candidate,
even with a high predicate-argument statistics, could
Feature Group Isolated Combined
SemMag (Web-based) 61.2 61.2
Type+Reflexive 53.1 61.2
ParaStruct 53.1 61.2
Pron+DefNP+InDefNP+NE 57.1 67.8
NearestNP+SameSent 53.1 70.2
FirstNP 65.3 79.2
Table 4: Results of different feature groups under
the TC model for N-pron resolution
SameSent_1 = 0:
:..SemMag > 0:
: :..Pron_2 = 0: 10 (200/23)
: : Pron_2 = 1: ...
: SemMag <= 0:
: :..Pron_2 = 1: 01 (75/1)
: Pron_2 = 0:
: :..SemMag <= -28: 01 (110/19)
: SemMag > -28: ...
SameSent_1 = 1:
:..SameSent_2 = 0: 01 (1655/49)
SameSent_2 = 1:
:..FirstNP_2 = 1: 01 (104/1)
FirstNP_2 = 0:
:..ParaStruct_2 = 1: 01 (3)
ParaStruct_2 = 0:
:..SemMag <= -151: 01 (27/2)
SemMag > -151:...
Figure 1: Top portion of the decision tree learned
under TC model for N-pron resolution (features ended
with ? 1? are for the first candidate C1 and those with ? 2? are
for C2.)
not be used as the antecedent (e.g. company said in
the sentence ?. . . the company . . . he said . . . ?). In
fact, our analysis of the current data set reveals that
most P-Prons refer back to a P-Pron or NE candidate
whose semantic category (human) has been deter-
mined. That is, simply using features NE and Pron
is sufficient to guarantee a high success, and thus the
relatively weak semantic feature would not be taken
in the learned decision tree for resolution.
4.4 Feature Analysis
In our experiment we were also concerned about the
importance of the web-based compatibility feature
(using frequency metric) among the feature set. For
this purpose, we divided the features into groups,
and then trained and tested on one group at a time.
Table 4 lists the feature groups and their respective
results for N-Pron resolution under the TC model.
171
The second column is for the systems with only the
current feature group, while the third column is with
the features combined with the existing feature set.
We see that used in isolation, the semantic compati-
bility feature is able to achieve a success up to 61%
around, just 4% lower than the best indicative fea-
ture FirstNP. In combination with other features, the
performance could be improved by as large as 18%
as opposed to being used alone.
Figure 1 shows the top portion of the pruned deci-
sion tree for N-Pron resolution under the TC model.
We could find that: (i) When comparing two can-
didates which occur in the same sentence as the
anaphor, the web-based semantic feature would be
examined in the first place, followed by the lexi-
cal property of the candidates. (ii) When two non-
pronominal candidates are both in previous sen-
tences before the anaphor, the web-based semantic
feature is still required to be examined after FirstNP
and ParaStruct. The decision tree further indicates
that the web-based feature plays an important role in
N-Pron resolution.
5 Conclusion
Our research focussed on improving pronoun reso-
lution using the statistics-based semantic compati-
bility information. We explored two issues that af-
fect the utility of the semantic information: statis-
tics source and learning framework. Specifically, we
proposed to utilize the web and the twin-candidate
model, in addition to the common combination of
the corpus and single-candidate model, to compute
and apply the semantic information.
Our experiments systematically evaluated differ-
ent combinations of statistics sources and learn-
ing models. The results on the newswire domain
showed that the web-based semantic compatibility
could be the most effectively incorporated in the
twin-candidate model for the neutral pronoun res-
olution. While the utility is not obvious for per-
sonal pronoun resolution, we can still see the im-
provement on the overall performance. We believe
that the semantic information under such a config-
uration would be even more effective on technical
domains where neutral pronouns take the majority
in the pronominal anaphors. Our future work would
have a deep exploration on such domains.
References
S. Abney. 1996. Partial parsing via finite-state cascades. In
Workshop on Robust Parsing, 8th European Summer School
in Logic, Language and Information, pages 8?15.
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proceed-
ings of 2004 North American chapter of the Association for
Computational Linguistics annual meeting.
I. Dagan and A. Itai. 1990. Automatic processing of large cor-
pora for the resolution of anahora references. In Proceedings
of the 13th International Conference on Computational Lin-
guistics, pages 330?332.
J. Hobbs. 1978. Resolving pronoun references. Lingua,
44:339?352.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proceedings of 2004 North American
chapter of the Association for Computational Linguistics an-
nual meeting.
F. Keller and M. Lapata. 2003. Using the web to obtain
freqencies for unseen bigrams. Computational Linguistics,
29(3):459?484.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proceedings of the 17th Int. Conference on
Computational Linguistics, pages 869?875.
N. Modjeska, K. Markert, and M. Nissim. 2003. Using the web
in machine learning for other-anaphora resolution. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing, pages 176?183.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 104?111, Philadelphia.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceedings of
42th Annual Meeting of the Association for Computational
Linguistics.
J. R. Quinlan. 1993. C4.5: Programs for machine learning.
Morgan Kaufmann Publishers, San Francisco, CA.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521?544.
M. Strube and C. Muller. 2003. A machine learning approach
to pronoun resolution in spoken dialogue. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 168?175, Japan.
M. Strube. 1998. Never look back: An alternative to centering.
In Proceedings of the 17th Int. Conference on Computational
Linguistics and 36th Annual Meeting of ACL, pages 1251?
1257.
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference reso-
lution using competition learning approach. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, Japan.
G. Zhou and J. Su. 2002. Named Entity recognition using a
HMM-based chunk tagger. In Proceedings of the 40th An-
nual Meeting of the Association for Computational Linguis-
tics, Philadelphia.
G. Zhou, J. Su, and T. Tey. 2000. Hybrid text chunking. In
Proceedings of the 4th Conference on Computational Natu-
ral Language Learning, pages 163?166, Lisbon, Portugal.
172
