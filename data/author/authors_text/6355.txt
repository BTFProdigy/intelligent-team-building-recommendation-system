Example-Based Metonymy Recognition for Proper Nouns
Yves Peirsman
Quantitative Lexicology and Variational Linguistics
University of Leuven, Belgium
yves.peirsman@arts.kuleuven.be
Abstract
Metonymy recognition is generally ap-
proached with complex algorithms that
rely heavily on the manual annotation of
training and test data. This paper will re-
lieve this complexity in two ways. First,
it will show that the results of the cur-
rent learning algorithms can be replicated
by the ?lazy? algorithm of Memory-Based
Learning. This approach simply stores all
training instances to its memory and clas-
sifies a test instance by comparing it to all
training examples. Second, this paper will
argue that the number of labelled training
examples that is currently used in the lit-
erature can be reduced drastically. This
finding can help relieve the knowledge ac-
quisition bottleneck in metonymy recog-
nition, and allow the algorithms to be ap-
plied on a wider scale.
1 Introduction
Metonymy is a figure of speech that uses ?one en-
tity to refer to another that is related to it? (Lakoff
and Johnson, 1980, p.35). In example (1), for in-
stance, China and Taiwan stand for the govern-
ments of the respective countries:
(1) China has always threatened to use force
if Taiwan declared independence. (BNC)
Metonymy resolution is the task of automatically
recognizing these words and determining their ref-
erent. It is therefore generally split up into two
phases: metonymy recognition and metonymy in-
terpretation (Fass, 1997).
The earliest approaches to metonymy recogni-
tion identify a word as metonymical when it vio-
lates selectional restrictions (Pustejovsky, 1995).
Indeed, in example (1), China and Taiwan both
violate the restriction that threaten and declare
require an animate subject, and thus have to be
interpreted metonymically. However, it is clear
that many metonymies escape this characteriza-
tion. Nixon in example (2) does not violate the se-
lectional restrictions of the verb to bomb, and yet,
it metonymically refers to the army under Nixon?s
command.
(2) Nixon bombed Hanoi.
This example shows that metonymy recognition
should not be based on rigid rules, but rather
on statistical information about the semantic and
grammatical context in which the target word oc-
curs.
This statistical dependency between the read-
ing of a word and its grammatical and seman-
tic context was investigated by Markert and Nis-
sim (2002a) and Nissim and Markert (2003;
2005). The key to their approach was the in-
sight that metonymy recognition is basically a sub-
problem of Word Sense Disambiguation (WSD).
Possibly metonymical words are polysemous, and
they generally belong to one of a number of pre-
defined metonymical categories. Hence, like WSD,
metonymy recognition boils down to the auto-
matic assignment of a sense label to a polysemous
word. This insight thus implied that all machine
learning approaches to WSD can also be applied to
metonymy recognition.
There are, however, two differences between
metonymy recognition and WSD. First, theo-
retically speaking, the set of possible readings
of a metonymical word is open-ended (Nunberg,
1978). In practice, however, metonymies tend to
stick to a small number of patterns, and their la-
bels can thus be defined a priori. Second, classic
71
WSD algorithms take training instances of one par-
ticular word as their input and then disambiguate
test instances of the same word. By contrast, since
all words of the same semantic class may undergo
the same metonymical shifts, metonymy recogni-
tion systems can be built for an entire semantic
class instead of one particular word (Markert and
Nissim, 2002a).
To this goal, Markert and Nissim extracted
from the BNC a corpus of possibly metonymical
words from two categories: country names
(Markert and Nissim, 2002b) and organization
names (Nissim and Markert, 2005). All these
words were annotated with a semantic label
? either literal or the metonymical cate-
gory they belonged to. For the country names,
Markert and Nissim distinguished between
place-for-people, place-for-event
and place-for-product. For the organi-
zation names, the most frequent metonymies
are organization-for-members and
organization-for-product. In addition,
Markert and Nissim used a label mixed for
examples that had two readings, and othermet
for examples that did not belong to any of the
pre-defined metonymical patterns.
For both categories, the results were promis-
ing. The best algorithms returned an accuracy of
87% for the countries and of 76% for the orga-
nizations. Grammatical features, which gave the
function of a possibly metonymical word and its
head, proved indispensable for the accurate recog-
nition of metonymies, but led to extremely low
recall values, due to data sparseness. Therefore
Nissim and Markert (2003) developed an algo-
rithm that also relied on semantic information, and
tested it on the mixed country data. This algo-
rithm used Dekang Lin?s (1998) thesaurus of se-
mantically similar words in order to search the
training data for instances whose head was sim-
ilar, and not just identical, to the test instances.
Nissim and Markert (2003) showed that a combi-
nation of semantic and grammatical information
gave the most promising results (87%).
However, Nissim and Markert?s (2003) ap-
proach has two major disadvantages. The first of
these is its complexity: the best-performing al-
gorithm requires smoothing, backing-off to gram-
matical roles, iterative searches through clusters of
semantically similar words, etc. In section 2, I will
therefore investigate if a metonymy recognition al-
gorithm needs to be that computationally demand-
ing. In particular, I will try and replicate Nissim
and Markert?s results with the ?lazy? algorithm of
Memory-Based Learning.
The second disadvantage of Nissim and Mark-
ert?s (2003) algorithms is their supervised nature.
Because they rely so heavily on the manual an-
notation of training and test data, an extension of
the classifiers to more metonymical patterns is ex-
tremely problematic. Yet, such an extension is es-
sential for many tasks throughout the field of Nat-
ural Language Processing, particularly Machine
Translation. This knowledge acquisition bottle-
neck is a well-known problem in NLP, and many
approaches have been developed to address it. One
of these is active learning, or sample selection, a
strategy that makes it possible to selectively an-
notate those examples that are most helpful to the
classifier. It has previously been applied to NLP
tasks such as parsing (Hwa, 2002; Osborne and
Baldridge, 2004) and Word Sense Disambiguation
(Fujii et al, 1998). In section 3, I will introduce
active learning into the field of metonymy recog-
nition.
2 Example-based metonymy recognition
As I have argued, Nissim and Markert?s (2003)
approach to metonymy recognition is quite com-
plex. I therefore wanted to see if this complexity
can be dispensed with, and if it can be replaced
with the much more simple algorithm of Memory-
Based Learning. The advantages of Memory-
Based Learning (MBL), which is implemented in
the TiMBL classifier (Daelemans et al, 2004)1, are
twofold. First, it is based on a plausible psycho-
logical hypothesis of human learning. It holds
that people interpret new examples of a phenom-
enon by comparing them to ?stored representa-
tions of earlier experiences? (Daelemans et al,
2004, p.19). This contrasts to many other classi-
fication algorithms, such as Naive Bayes, whose
psychological validity is an object of heavy de-
bate. Second, as a result of this learning hypothe-
sis, an MBL classifier such as TiMBL eschews the
formulation of complex rules or the computation
of probabilities during its training phase. Instead
it stores all training vectors to its memory, together
with their labels. In the test phase, it computes the
distance between the test vector and all these train-
1This software package is freely available and can be
downloaded from http://ilk.uvt.nl/software.html.
72
ing vectors, and simply returns the most frequent
label of the most similar training examples.
One of the most important challenges in
Memory-Based Learning is adapting the algorithm
to one?s data. This includes finding a represen-
tative seed set as well as determining the right
distance measures. For my purposes, however,
TiMBL?s default settings proved more than satis-
factory. TiMBL implements the IB1 and IB2 algo-
rithms that were presented in Aha et al (1991), but
adds a broad choice of distance measures. Its de-
fault implementation of the IB1 algorithm, which
is called IB1-IG in full (Daelemans and Van den
Bosch, 1992), proved most successful in my ex-
periments. It computes the distance between two
vectors X and Y by adding up the weighted dis-
tances ? between their corresponding feature val-
ues xi and yi:
?(X,Y ) =
n
?
i=1
wi?(xi, yi)(3)
The most important element in this equation is the
weight that is given to each feature. In IB1-IG,
features are weighted by their Gain Ratio (equa-
tion 4), the division of the feature?s Information
Gain by its split info. Information Gain, the nu-
merator in equation (4), ?measures how much in-
formation it [feature i] contributes to our knowl-
edge of the correct class label [...] by comput-
ing the difference in uncertainty (i.e. entropy) be-
tween the situations without and with knowledge
of the value of that feature? (Daelemans et al,
2004, p.20). In order not ?to overestimate the rel-
evance of features with large numbers of values?
(Daelemans et al, 2004, p.21), this Information
Gain is then divided by the split info, the entropy
of the feature values (equation 5). In the following
equations, C is the set of class labels, H(C) is the
entropy of that set, and Vi is the set of values for
feature i.
wi =
H(C)? ?v?Vi P (v)?H(C|v)
si(i)(4)
si(i) = ?
?
v?Vi
P (v)log2P (v)(5)
The IB2 algorithm was developed alongside IB1
in order to reduce storage requirements (Aha et
al., 1991). It iteratively saves only those instances
that are misclassified by IB1. This is because these
will likely lie close to the decision boundary, and
hence, be most informative to the classifier. My
experiments showed, however, that IB2?s best per-
formance lay more than 2% below that of IB1. It
will therefore not be treated any further here.
2.1 Experiments with grammatical
information only
In order to see if Memory-Based Learning is able
to replicate Nissim and Markert?s (2003; 2005) re-
sults, I used their corpora for a number of experi-
ments. These corpora consist of one set with about
1000 mixed country names, another with 1000 oc-
currences of Hungary, and a final set with about
1000 mixed organization names.2 Evaluation was
performed with ten-fold cross-validation.
The first round of experiments used only gram-
matical information. The experiments for the lo-
cation data were similar to Nissim and Mark-
ert?s (2003), and took the following features into
account:
? the grammatical function of the word (subj,
obj, iobj, pp, gen, premod, passive subj,
other);
? its head;
? the presence of a second head;
? the second head (if present).
The experiments for the organization names used
the same features as Nissim and Markert (2005):
? the grammatical function of the word;
? its head;
? its type of determiner (if present) (def, indef,
bare, demonst, other);
? its grammatical number (sing, plural);
? its number of grammatical roles (if present).
The number of words in the organization name,
which Nissim and Markert used as a sixth and fi-
nal feature, led to slightly worse results in my ex-
periments and was therefore dropped.
The results of these first experiments clearly
beat the baselines of 79.7% (countries) and 63.4%
(organizations). Moreover, despite its extremely
2This data is publicly available and can be downloaded
from http://homepages.inf.ed.ac.uk/mnissim/mascara.
73
Acc P R F
TiMBL 86.6% 80.2% 49.5% 61.2%
N&M 87.0% 81.4% 51.0% 62.7%
Table 1: Results for the mixed country data.
TiMBL: my TiMBL results
N&M: Nissim and Markert?s (2003) results
simple learning phase, TiMBL is able to replicate
the results from Nissim and Markert (2003; 2005).
As table 1 shows, accuracy for the mixed coun-
try data is almost identical to Nissim and Mark-
ert?s figure, and precision, recall and F-score for
the metonymical class lie only slightly lower.3
TiMBL?s results for the Hungary data were simi-
lar, and equally comparable to Markert and Nis-
sim?s (Katja Markert, personal communication).
Note, moreover, that these results were reached
with grammatical information only, whereas Nis-
sim and Markert?s (2003) algorithm relied on se-
mantics as well.
Next, table 2 indicates that TiMBL?s accuracy
for the mixed organization data lies about 1.5% be-
low Nissim and Markert?s (2005) figure. This re-
sult should be treated with caution, however. First,
Nissim and Markert?s available organization data
had not yet been annotated for grammatical fea-
tures, and my annotation may slightly differ from
theirs. Second, Nissim and Markert used several
feature vectors for instances with more than one
grammatical role and filtered all mixed instances
from the training set. A test instance was treated as
mixed only when its several feature vectors were
classified differently. My experiments, in contrast,
were similar to those for the location data, in that
each instance corresponded to one vector. Hence,
the slightly lower performance of TiMBL is prob-
ably due to differences between the two experi-
ments.
These first experiments thus demonstrate that
Memory-Based Learning can give state-of-the-art
performance in metonymy recognition. In this re-
spect, it is important to stress that the results for
the country data were reached without any se-
mantic information, whereas Nissim and Mark-
ert?s (2003) algorithm used Dekang Lin?s (1998)
clusters of semantically similar words in order
to deal with data sparseness. This fact, together
3Precision, recall and F-score are given for the metonymi-
cal class only, since this is the category that metonymy recog-
nition is concerned with.
Acc P R F
TiMBL 74.63% 78.65% 55.53% 65.10%
N&M 76.0% ? ? ?
Table 2: Results for the mixed organization data.
TiMBL: my TiMBL results
N&M: Nissim and Markert?s (2005) results
with the psychological plausibility and the simple
learning phase, adds to the attractivity of Memory-
Based Learning.
2.2 Experiments with semantic and
grammatical information
It is still intuitively true, however, that the inter-
pretation of a possibly metonymical word depends
mainly on the semantics of its head. The ques-
tion is if this information is still able to improve
the classifier?s performance. I therefore performed
a second round of experiments with the location
data, in which I also made use of semantic infor-
mation. In this round, I extracted the hypernym
synsets of the head?s first sense from WordNet.
WordNet?s hierarchy of synsets makes it possible
to quantify the semantic relatedness of two words:
the more hypernyms two words share, the more
closely related they are. I therefore used the ten
highest hypernyms of the first head as features 5
to 14. For those heads with fewer than ten hyper-
nyms, a copy of their lowest hypernym filled the
?empty? features. As a result, TiMBL would first
look for training instances with ten identical hy-
pernyms, then with nine, etc. It would thus com-
pare the test example to the semantically most sim-
ilar training examples.
However, TiMBL did not perform better with
this semantic information. Although F-scores for
the metonymical category went up slightly, the
system?s accuracy hardly changed. This result was
not due to the automatic selection of the first (most
frequent) WordNet sense. By manually disam-
biguating all the heads in the training and test set
of the country data, I observed that this first sense
was indeed often incorrect, but that choosing the
correct sense did not lead to a more robust system.
Clearly, the classifier did not benefit from Word-
Net information as Nissim and Markert?s (2003)
did from Lin?s (1998) thesaurus.
The learning curves for the country set alow
us to compare the two types of feature vectors
74
Figure 1: Accuracy learning curves for the mixed
country data with and without semantic informa-
tion.
in more detail.4 As figure 1 indicates, with re-
spect to overall accuracy, semantic features have
a negative influence: the learning curve with both
features climbs much more slowly than that with
only grammatical features. Hence, contrary to my
expectations, grammatical features seem to allow
a better generalization from a limited number of
training instances. With respect to the F-score on
the metonymical category in figure 2, the differ-
ences are much less outspoken. Both features give
similar learning curves, but semantic features lead
to a higher final F-score. In particular, the use of
semantic features results in a lower precision fig-
ure, but a higher recall score. Semantic features
thus cause the classifier to slightly overgeneralize
from the metonymic training examples.
There are two possible reasons for this inabil-
ity of semantic information to improve the clas-
sifier?s performance. First, WordNet?s synsets do
not always map well to one of our semantic la-
bels: many are rather broad and allow for several
readings of the target word, while others are too
specific to make generalization possible. Second,
there is the predominance of prepositional phrases
in our data. With their closed set of heads, the
number of examples that benefits from semantic
information about its head is actually rather small.
Nevertheless, my first round of experiments has
indicated that Memory-Based Learning is a sim-
ple but robust approach to metonymy recogni-
tion. It is able to replace current approaches that
need smoothing or iterative searches through a the-
saurus, with a simple, distance-based algorithm.
4These curves were obtained by averaging the results of
10 experiments. They show performance on a test set of 40%
of the data, with the other 60% as training data.
Figure 2: F-score learning curves for the mixed
country data with and without semantic informa-
tion.
Moreover, in contrast to some other successful
classifiers, it incorporates a plausible hypothesis
of human learning.
3 Distance-based sample selection
The previous section has shown that a simple algo-
rithm that compares test examples to stored train-
ing instances is able to produce state-of-the-art re-
sults in the field of metonymy recognition. This
leads to the question of how many examples we
actually need to arrive at this performance. Af-
ter all, the supervised approach that we explored
requires the careful manual annotation of a large
number of training instances. This knowledge ac-
quisition bottleneck compromises the extrapola-
tion of this approach to a large number of seman-
tic classes and metonymical patterns. This section
will therefore investigate if it is possible to auto-
matically choose informative examples, so that an-
notation effort can be reduced drastically.
For this round of experiments, two small
changes were made. First, since we are focusing
on metonymy recognition, I replaced all specific
metonymical labels with the label met, so that
only three labels remain: lit, met and mixed.
Second, whereas the results in the previous section
were obtained with ten-fold cross-validation, I ran
these experiments with a training and a test set.
On each run, I used a random 60% of the data for
training; 40% was set aside for testing. All curves
give the average of twenty test runs that use gram-
matical information only.
In general, sample selection proceeds on the
basis of the confidence that the classifier has in
its classification. Commonly used metrics are the
probability of the most likely label, or the entropy
75
Figure 3: Accuracy learning curves for the coun-
try data with random and maximum-distance se-
lection of training examples.
over all possible labels. The algorithm then picks
those instances with the lowest confidence, since
these will contain valuable information about the
training set (and hopefully also the test set) that is
still unknown to the system.
One problem with Memory-Based Learning al-
gorithms is that they do not directly output prob-
abilities. Since they are example-based, they can
only give the distances between the unlabelled in-
stance and all labelled training instances. Never-
theless, these distances can be used as a measure
of certainty, too: we can assume that the system
is most certain about the classification of test in-
stances that lie very close to one or more of its
training instances, and less certain about those that
are further away. Therefore the selection function
that minimizes the probability of the most likely
label can intuitively be replaced by one that max-
imizes the distance from the labelled training in-
stances.
However, figure 3 shows that for the mixed
country instances, this function is not an option.
Both learning curves give the results of an algo-
rithm that starts with fifty random instances, and
then iteratively adds ten new training instances to
this initial seed set. The algorithm behind the solid
curve chooses these instances randomly, whereas
the one behind the dotted line selects those that
are most distant from the labelled training exam-
ples. In the first half of the learning process, both
functions are equally successful; in the second the
distance-based function performs better, but only
slightly so.
There are two reasons for this bad initial per-
formance of the active learning function. First, it
is not able to distinguish between informative and
Figure 4: Accuracy learning curves for the coun-
try data with random and maximum/minimum-
distance selection of training examples.
unusual training instances. This is because a large
distance from the seed set simply means that the
particular instance?s feature values are relatively
unknown. This does not necessarily imply that
the instance is informative to the classifier, how-
ever. After all, it may be so unusual and so badly
representative of the training (and test) set that the
algorithm had better exclude it ? something that
is impossible on the basis of distances only. This
bias towards outliers is a well-known disadvantage
of many simple active learning algorithms. A sec-
ond type of bias is due to the fact that the data has
been annotated with a few features only. More par-
ticularly, the present algorithm will keep adding
instances whose head is not yet represented in the
training set. This entails that it will put off adding
instances whose function is pp, simply because
other functions (subj, gen, . . . ) have a wider
variety in heads. Again, the result is a labelled set
that is not very representative of the entire training
set.
There are, however, a few easy ways to increase
the number of prototypical examples in the train-
ing set. In a second run of experiments, I used an
active learning function that added not only those
instances that were most distant from the labelled
training set, but also those that were closest to it.
After a few test runs, I decided to add six distant
and four close instances on each iteration. Figure 4
shows that such a function is indeed fairly success-
ful. Because it builds a labelled training set that is
more representative of the test set, this algorithm
clearly reduces the number of annotated instances
that is needed to reach a given performance.
Despite its success, this function is obviously
not yet a sophisticated way of selecting good train-
76
Figure 5: Accuracy learning curves for the organi-
zation data with random and distance-based (AL)
selection of training examples with a random seed
set.
ing examples. The selection of the initial seed set
in particular can be improved upon: ideally, this
seed set should take into account the overall dis-
tribution of the training examples. Currently, the
seeds are chosen randomly. This flaw in the al-
gorithm becomes clear if it is applied to another
data set: figure 5 shows that it does not outper-
form random selection on the organization data,
for instance.
As I suggested, the selection of prototypical or
representative instances as seeds can be used to
make the present algorithm more robust. Again, it
is possible to use distance measures to do this: be-
fore the selection of seed instances, the algorithm
can calculate for each unlabelled instance its dis-
tance from each of the other unlabelled instances.
In this way, it can build a prototypical seed set
by selecting those instances with the smallest dis-
tance on average. Figure 6 indicates that such an
algorithm indeed outperforms random sample se-
lection on the mixed organization data. For the
calculation of the initial distances, each feature re-
ceived the same weight. The algorithm then se-
lected 50 random samples from the ?most proto-
typical? half of the training set.5 The other settings
were the same as above.
With the present small number of features, how-
ever, such a prototypical seed set is not yet alays
as advantageous as it could be. A few experiments
indicated that it did not lead to better performance
on the mixed country data, for instance. However,
as soon as a wider variety of features is taken into
account (as with the organization data), the advan-
5Of course, the random algorithm in fi gure 6 still ran-
domly selected its seeds from the entire training set.
Figure 6: Accuracy learning curves for the organi-
zation data with random and distance-based (AL)
selection of training examples with a prototypical
seed set.
tages of a prototypical seed set will definitely be-
come more obvious.
In conclusion, it has become clear that a careful
selection of training instances may considerably
reduce annotation effort in metonymy recognition.
Functions that construct a prototypical seed set
and then use MBL?s distance measures to select in-
formative as well as typical samples are extremely
promising in this respect and can already consid-
erably reduce annotation effort. In order to reach
an accuracy of 85% on the country data, for in-
stance, the active learning algorithm above needs
44% fewer training instances than its random com-
petitor (on average). On the organisation data, re-
duction is typically around 30%. These relatively
simple algorithms thus constitute a good basis for
the future development of robust active learning
techniques for metonymy recognition. I believe
in particular that research in this field should go
hand in hand with an investigation of new infor-
mative features, since the present, limited feature
set does not yet alow us to measure the classifier?s
confidence reliably.
4 Conclusions and future work
In this paper I have explored an example-based ap-
proach to metonymy recognition. Memory-Based
Learning does away with the complexity of cur-
rent supervised metonymy recognition algorithms.
Even without semantic information, it is able to
give state-of-the-art results similar to those in the
literature. Moreover, not only is the complexity of
current learning algorithms unnecessary; the num-
ber of labelled training instances can be reduced
drastically, too. I have argued that selective sam-
77
pling can help choose those instances that are most
helpful to the classifier. A few distance-based al-
gorithms were able to drastically reduce the num-
ber of training instances that is needed for a given
accuracy, both for the country and the organization
names.
If current metonymy recognition algorithms are
to be used in a system that can recognize all pos-
sible metonymical patterns across a broad variety
of semantic classes, it is crucial that the required
number of labelled training examples be reduced.
This paper has taken the first steps along this path
and has set out some interesting questions for fu-
ture research. This research should include the
investigation of new features that can make clas-
sifiers more robust and allow us to measure their
confidence more reliably. This confidence mea-
surement can then also be used in semi-supervised
learning algorithms, for instance, where the clas-
sifier itself labels the majority of training exam-
ples. Only with techniques such as selective sam-
pling and semi-supervised learning can the knowl-
edge acquisition bottleneck in metonymy recogni-
tion be addressed.
Acknowledgements
I would like to thank Mirella Lapata, Dirk Geer-
aerts and Dirk Speelman for their feedback on this
project. I am also very grateful to Katja Markert
and Malvina Nissim for their helpful information
about their research.
References
D. W. Aha, D. Kibler, and M. K. Albert. 1991.
Instance-based learning algorithms. Machine
Learning, 6:37?66.
W. Daelemans and A. Van den Bosch. 1992. Generali-
sation performance of backpropagation learning on a
syllabifi cation task. In M. F. J. Drossaers and A. Ni-
jholt, editors, Proceedings of TWLT3: Connection-
ism and Natural Language Processing, pages 27?37,
Enschede, The Netherlands.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2004. TiMBL: TilburgMemory-
Based Learner. Technical report, Induction of
Linguistic Knowledge, Computational Linguistics,
Tilburg University.
D. Fass. 1997. Processing Metaphor and Metonymy.
Stanford, CA: Ablex.
A. Fujii, K. Inui, T. Tokunaga, and H. Tanaka.
1998. Selective sampling for example-based word
sense disambiguation. Computational Linguistics,
24(4):573?597.
R. Hwa. 2002. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253?276.
G. Lakoff and M. Johnson. 1980. Metaphors We Live
By. London: The University of Chicago Press.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the International Con-
ference on Machine Learning, Madison, USA.
K. Markert and M. Nissim. 2002a. Metonymy res-
olution as a classification task. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), Philadelphia,
USA.
K. Markert and M. Nissim. 2002b. Towards a cor-
pus annotated for metonymies: the case of location
names. In Proceedings of the Third International
Conference on Language Resources and Evaluation
(LREC 2002), Las Palmas, Spain.
M. Nissim and K. Markert. 2003. Syntactic features
and word similarity for supervised metonymy res-
olution. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-03), Sapporo, Japan.
M. Nissim and K. Markert. 2005. Learning to buy a
Renault and talk to BMW: A supervised approach
to conventional metonymy. In H. Bunt, editor, Pro-
ceedings of the 6th International Workshop on Com-
putational Semantics, Tilburg, The Netherlands.
G. Nunberg. 1978. The Pragmatics of Reference.
Ph.D. thesis, City University of New York.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL). Boston,
USA.
J. Pustejovsky. 1995. The Generative Lexicon. Cam-
bridge, MA: MIT Press.
78
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 648?656,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Predicting Strong Associations on the Basis of Corpus Data
Yves Peirsman
Research Foundation ? Flanders &
QLVL, University of Leuven
Leuven, Belgium
yves.peirsman@arts.kuleuven.be
Dirk Geeraerts
QLVL, University of Leuven
Leuven, Belgium
dirk.geeraerts@arts.kuleuven.be
Abstract
Current approaches to the prediction of
associations rely on just one type of in-
formation, generally taking the form of
either word space models or collocation
measures. At the moment, it is an open
question how these approaches compare
to one another. In this paper, we will
investigate the performance of these two
types of models and that of a new ap-
proach based on compounding. The best
single predictor is the log-likelihood ratio,
followed closely by the document-based
word space model. We will show, how-
ever, that an ensemble method that com-
bines these two best approaches with the
compounding algorithm achieves an in-
crease in performance of almost 30% over
the current state of the art.
1 Introduction
Associations are words that immediately come to
mind when people hear or read a given cue word.
For instance, a word like pepper calls up salt,
and wave calls up sea. Aitchinson (2003) and
Schulte im Walde and Melinger (2005) show that
such associations can be motivated by a number
of factors, from semantic similarity to colloca-
tion. Current computational models of associa-
tion, however, tend to focus on one of these, by us-
ing either collocation measures (Michelbacher et
al., 2007) or word space models (Sahlgren, 2006;
Peirsman et al, 2008). To this day, two gen-
eral problems remain. First, the literature lacks
a comprehensive comparison between these gen-
eral types of models. Second, we are still looking
for an approach that combines several sources of
information, so as to correctly predict a larger va-
riety of associations.
Most computational models of semantic rela-
tions aim to model semantic similarity in particu-
lar (Landauer and Dumais, 1997; Lin, 1998; Pado?
and Lapata, 2007). In Natural Language Process-
ing, these models have applications in fields like
query expansion, thesaurus extraction, informa-
tion retrieval, etc. Similarly, in Cognitive Science,
such models have helped explain neural activa-
tion (Mitchell et al, 2008), sentence and discourse
comprehension (Burgess et al, 1998; Foltz, 1996;
Landauer and Dumais, 1997) and priming patterns
(Lowe and McDonald, 2000), to name just a few
examples. However, there are a number of appli-
cations and research fields that will surely bene-
fit from models that target the more general phe-
nomenon of association. For instance, automat-
ically predicted associations may prove useful in
models of information scent, which seek to ex-
plain the paths that users follow in their search
for relevant information on the web (Chi et al,
2001). After all, if the visitor of a web shop
clicks on music to find the prices of iPods, this
behaviour is motivated by an associative relation
different from similarity. Other possible applica-
tions lie in the field of models of text coherence
(Landauer and Dumais, 1997) and automated es-
say grading (Kakkonen et al, 2005). In addition,
all research in Cognitive Science that we have re-
ferred to above could benefit from computational
models of association in order to study the effects
of association in comparison to those of similarity.
Our article is structured as follows. In sec-
tion 2, we will discuss the phenomenon of asso-
ciation and introduce the variety of relations that
it is motivated by. Parallel to these relations, sec-
tion 3 presents the three basic types of approaches
that we use to predict strong associations. Sec-
tion 4 will first compare the results of these three
approaches, for a total of 43 models. Section 5
will then show how these results can be improved
by the combination of several models in an ensem-
ble. Finally, section 6 wraps up with conclusions
and an outlook for future research.
648
cue association
amfibie (?amphibian?) kikker (?frog?)
peper (?pepper?) zout (?salt?)
roodborstje (?robin?) vogel (?bird?)
granaat (?grenade?) oorlog (?war?)
helikopter (?helicopter?) vliegen (?to fly?)
werk (?job?) geld (?money?)
acteur (?actor?) film (?film?)
cello (?cello?) muziek (?music?)
kruk (?stool?) bar (?bar?)
Table 1: Examples of cues and their strongest as-
sociation.
2 Associations
There are several reasons why a word may be asso-
ciated to its cue. According to Aitchinson (2003),
the four major types of associations are, in or-
der of frequency, co-ordination (co-hyponyms like
pepper and salt), collocation (like salt and wa-
ter), superordination (insect as a hypernym of but-
terfly) and synonymy (like starved and hungry).
As a result, a computational model that is able to
predict associations accurately has to deal with a
wide range of semantic relations. Past systems,
however, generally use only one type of informa-
tion (Wettler et al, 2005; Sahlgren, 2006; Michel-
bacher et al, 2007; Peirsman et al, 2008; Wand-
macher et al, 2008), which suggests that they are
relatively restricted in the number of associations
they will find.
In this article, we will focus on a set of Dutch
cue words and their single strongest association,
collected from a large psycholinguistic experi-
ment. Table 1 gives a few examples of such cue?
association pairs. It illustrates the different types
of linguistic phenomena that an association may
be motivated by. The first three word pairs are
based on similarity. In this case, strong associ-
ations can be hyponyms (as in amphibian?frog),
co-hyponyms (as in pepper?salt) or hypernyms of
their cue (as in robin?bird). The next three pairs
represent semantic links where no relation of sim-
ilarity plays a role. Instead, the associations seem
to be motivated by a topical relation to their cue,
which is possibly reflected by their frequent co-
occurrence in a corpus. The final three word pairs
suggest that morphological factors might play a
role, too. Often, a cue and its association form
the building blocks of a compound, and it is possi-
ble that one part of a compound calls up the other.
The examples show that the process of compound-
ing can go in either direction: the compound may
consist of cue plus association (as in cellomuziek
?cello music?), or of association plus cue (as in
filmacteur ?film actor?). While it is not clear if it
is the compounds themselves that motivate the as-
sociation, or whether it is just the topical relation
between their two parts, they might still be able to
help identify strong associations.
3 Approaches
Motivated by the three types of cue?association
pairs that we identified in Table 1, we study three
sources of information (two types of distributional
information, and one type of morphological infor-
mation) that may provide corpus-based evidence
for strong associatedness: collocation measures,
word space models and compounding.
3.1 Collocation measures
Probably the most straightforward way to pre-
dict strong associations is to assume that a cue
and its strong association often co-occur in text.
As a result, we can use collocation measures
like point-wise mutual information (Church and
Hanks, 1989) or the log-likelihood ratio (Dunning,
1993) to predict the strong association for a given
cue. Point-wise mutual information (PMI) tells
us if two words w1 and w2 occur together more or
less often than expected on the basis of their indi-
vidual frequencies and the independence assump-
tion:
PMI(w1, w2) = log2
P (w1, w2)
P (w1) ? P (w2)
The log-likelihood ratio compares the like-
lihoods L of the independence hypothesis (i.e.,
p = P (w2|w1) = P (w2|?w1)) and the de-
pendence hypothesis (i.e., p1 = P (w2|w1) 6=
P (w2|?w1) = p2), under the assumption that the
words in a text are binomially distributed:
log ? = log
L(P (w2|w1); p) ? L(P (w2|?w1); p)
L(P (w2|w1); p1) ? L(P (w2|?w1); p2)
3.2 Word Space Models
A respectable proportion (in our data about 18%)
of the strong associations are motivated by se-
mantic similarity to their cue. They can be syn-
onyms, hyponyms, hypernyms, co-hyponyms or
649
antonyms. Collocation measures, however, are not
specifically targeted towards the discovery of se-
mantic similarity. Instead, they model similarity
mainly as a side effect of collocation. Therefore
we also investigated a large set of computational
models that were specifically developed for the
discovery of semantic similarity. These so-called
word space models or distributional models of lex-
ical semantics are motivated by the distributional
hypothesis, which claims that semantically simi-
lar words appear in similar contexts. As a result,
they model each word in terms of its contexts in
a corpus, as a so-called context vector. Distribu-
tional similarity is then operationalized as the sim-
ilarity between two such context vectors. These
models will thus look for possible associations by
searching words with a context vector similar to
the given cue.
Crucial in the implementation of word space
models is their definition of context. In the cur-
rent literature, there are basically three popular ap-
proaches. Document-based models use some sort
of textual entity as features (Landauer and Du-
mais, 1997; Sahlgren, 2006). Their context vec-
tors note what documents, paragraphs, articles or
similar stretches of text a target word appears in.
Without dimensionality reduction, in these mod-
els two words will be distributionally similar if
they often occur together in the same paragraph,
for instance. This approach still bears some simi-
larity to the collocation measures above, since it
relies on the direct co-occurrence of two words
in text. Second, syntax-based models focus on
the syntactic relationships in which a word takes
part (Lin, 1998). Here two words will be sim-
ilar when they often appear in the same syntac-
tic roles, like subject of fly. Third, word-
based models simply use as features the words
that appear in the context of the target, without
considering the syntactic relations between them.
Context is thus defined as the set of n words
around the target (Sahlgren, 2006). Obviously, the
choice of context size will again have a major in-
fluence on the behaviour of the model. Syntax-
based and word-based models differ from collo-
cation measures and document-based models in
that they do not search for words that co-occur
directly. Instead, they look for words that often
occur together with the same context words or
syntactic relations. Even though all these models
were originally developed to model semantic sim-
ilarity relations, syntax-based models have been
shown to favour such relations more than word-
based and document-based models, which might
capture more associative relationships (Sahlgren,
2006; Van der Plas, 2008).
3.3 Compounding
As we have argued before, one characteristic of
cues and their strong associations is that they can
sometimes be combined into a compound. There-
fore we developed a third approach which dis-
covers for every cue the words in the corpus that
in combination with it lead to an existing com-
pound. Since in Dutch compounds are generally
written as one word, this is relatively easy. We at-
tached each candidate association to the cue (both
in the combination cue+association and associ-
ation+cue), following a number of simple mor-
phological rules for compounding. We then de-
termined if any of these hypothetical compounds
occurred in the corpus. The possible associa-
tions that led to an observed compound were then
ranked according to the frequency of that com-
pound.1 Note that, for languages where com-
pounds are often spelled as two words, like En-
glish, our approach will have to recognize multi-
word units to deal with this issue.
3.4 Previous research
In previous research, most attention has gone out
to the first two of our models. Sahlgren (2006)
tries to find associations with word space mod-
els. He argues that document-based models are
better suited to the discovery of associations than
word-based ones. In addition, Sahlgren (2006) as
well as Peirsman et al (2008) show that in word-
based models, large context sizes are more effec-
tive than small ones. This supports Wandmacher
et al?s (2008) model of associations, which uses a
context size of 75 words to the left and right of the
target. However, Peirsman et al (2008) find that
word-based distributional models are clearly out-
performed by simple collocation measures, par-
ticularly the log-likelihood ratio. Such colloca-
tion measures are also used by Michelbacher et al
(2007) in their classification of asymmetric associ-
ations. They show the chi-square metric to be a ro-
bust classifier of associations as either symmetric
or asymmetric, while a measure based on condi-
tional probabilities is particularly suited to model
1If both compounds cue+association and association+cue
occurred in the corpus, their frequencies were summed.
650
ll
l l l l l l l l
2 4 6 8 10
2
5
10
20
50
10
0
context size
m
ed
ian
 ra
nk
 of
 m
os
t fr
eq
ue
nt 
ass
oci
atio
n l word?based no stoplistword?based stoplist
pmi statistic
log?likelihood statistic
compound?based
syntax?based
document?based
Figure 1: Median rank of the strong associations.
the magnitude of asymmetry. In a similar vein,
Wettler et al (2005) successfully predict associa-
tions on the basis of co-occurrence in text, in the
framework of associationist learning theory. De-
spite this wealth of systems, it is an open question
how their results compare to each other. More-
over, a model that combines several of these sys-
tems might outperform any basic approach.
4 Experiments
Our experiments were inspired by the association
prediction task at the ESSLLI-2008 workshop on
distributional models. We will first present this
precise setup and then go into the results and their
implications.
4.1 Setup
Our data was the Twente Nieuws Corpus (TwNC),
which contains 300 million words of Dutch news-
paper articles. This corpus was compiled at the
University of Twente and subsequently parsed by
the Alpino parser at the University of Gronin-
gen (van Noord, 2006). The newspaper arti-
cles in the corpus served as the contextual fea-
tures for the document-based system; the depen-
dency triples output by Alpino were used as in-
put for the syntax-based approach. These syntactic
features of the type subject of fly covered
eight syntactic relations ? subject, direct object,
prepositional complement, adverbial prepositional
phrase, adjective modification, PP postmodifica-
tion, apposition and coordination. Finally, the col-
location measures and word-based distributional
models took into account context sizes ranging
from one to ten words to the left and right of the
target.
Because of its many parameters, the precise im-
plementation of the word space models deserves a
bit more attention. In all cases, we used the con-
text vectors in their full dimensionality. While this
is somewhat of an exception in the literature, it
has been argued that the full dimensionality leads
to the best results for word-based models at least
(Bullinaria and Levy, 2007). For the syntax-based
and word-based approaches, we only took into ac-
count features that occurred at least two times to-
gether with the target. For the word-based models,
we experimented with the use of a stoplist, which
allowed us to exclude semantically ?empty? words
as features. The simple co-occurrence frequencies
in the context vectors were replaced by the point-
wise mutual information between the target and
the feature (Bullinaria and Levy, 2007; Van der
Plas, 2008). The similarity between two vectors
was operationalized as the cosine of the angle be-
651
similar related, not similar
models mean med rank1 mean med rank1
pmi context 10 16.4 4 23% 25.2 9 10%
log-likelihood ratio context 10 12.8 2 41% 18.0 3 31%
syntax-based 16.3 4 22% 61.9 70 2%
word-based context 10 stoplist 10.7 3 27% 36.9 17 12%
document-based 10.1 3 26% 20.2 4 26%
compounding 80.7 101 5% 51.9 26 12%
Table 2: Performance of the models on semantically similar cue-association pairs and related but not
similar pairs.
med = median; rank1 = number of associations at rank 1
tween them. This measure is more or less stan-
dard in the literature and leads to state-of-the-art
results (Schu?tze, 1998; Pado? and Lapata, 2007;
Bullinaria and Levy, 2007). While the cosine is a
symmetric measure, however, association strength
is asymmetric. For example, snelheid (?speed?)
triggered auto (?car?) no fewer than 55 times in
the experiment, whereas auto evoked snelheid a
mere 3 times. Like Michelbacher et al (2007), we
solve this problem by focusing not on the similar-
ity score itself, but on the rank of the association in
the list of nearest neighbours to the cue. We thus
expect that auto will have a much higher rank in
the list of nearest neighbours to snelheid than vice
versa.
Our Gold Standard was based on a large-scale
psycholinguistic experiment conducted at the Uni-
versity of Leuven (De Deyne and Storms, 2008).
In this experiment, participants were asked to list
three different associations for all cue words they
were presented with. Each of the 1425 cues was
given to at least 82 participants, resulting in a to-
tal of 381,909 responses. From this set, we took
only noun cues with a single strong association.
This means we found the most frequent associ-
ation to each cue, and only included the pair in
the test set if the association occurred at least 1.5
times more often than the second most frequent
one. This resulted in a final test set of 593 cue-
association pairs. Next we brought together all the
associations in a set of candidate associations, and
complemented it with 1000 random words from
the corpus with a frequency of at least 200. From
these candidate words, we had each model select
the 100 highest scoring ones (the nearest neigh-
bours). Performance was then expressed as the
median and mean rank of the strongest association
in this list. Associations absent from the list auto-
matically received a rank of 101. Thus, the lower
the rank, the better the performance of the system.
While there are obviously many more ways of as-
sembling a test set and scoring the several systems,
we found these all gave very similar results to the
ones reported here.
4.2 Results and discussion
The median ranks of the strong associations for all
models are plotted in Figure 1. The means show
the same pattern, but give a less clear indication of
the number of associations that were suggested in
the top n most likely candidates. The most suc-
cessful approach is the log-likelihood ratio (me-
dian 3 with a context size of 10, mean 16.6),
followed by the document-based model (median
4, mean 18.4) and point-wise mutual informa-
tion (median 7 with a context size of 10, mean
23.1). Next in line are the word-based distribu-
tional models with and without a stoplist (high-
est medians at 11 and 12, highest means at 30.9
and 33.3, respectively), and then the syntax-based
word space model (median 42, mean 51.1). The
worst performance is recorded for the compound-
ing approach (median 101, mean 56.7). Overall,
corpus-based approaches that rely on direct co-
occurrence thus seem most appropriate for the pre-
diction of strong associations to a cue. This is
probably a result of two factors. First, collocation
itself is an important motivation for human asso-
ciations (Aitchinson, 2003). Second, while col-
location approaches in themselves do not target
semantic similarity, semantically similar associa-
tions are often also collocates to their cues. This is
particularly the case for co-hyponyms, like pepper
and salt, which score very high both in terms of
collocation and in terms of similarity.
Let us discuss the results of all models in a bit
652
ll
l
cue frequency
Index
me
dian
 ran
k of
 stro
nge
st a
ssoc
iatio
n
high mid low1
2
5
10
20
50
100
l
l
l
association frequency
Index
me
dian
 ran
k of
 stro
nge
st a
ssoc
iatio
n
high mid low1
2
5
10
20
50
100 l pmi context 10log?likelihood context 10
syntax?based
word?based context 10 stoplistdocument?based
compounding
Figure 2: Performance of the models in three cue and association frequency bands.
more detail. A first factor of interest is the dif-
ference between associations that are similar to
their cue and those which are related but not simi-
lar. Most of our models show a crucial difference
in performance with respect to these two classes.
The most important results are given in Table 2.
The log-likelihood ratio gives the highest number
of associations at rank 1 for both classes. Par-
ticularly surprising is its strong performance with
respect to semantic similarity, since this relation
is only a side effect of collocation. In fact, the
log-likelihood ratio scores better at predicting se-
mantically similar associations than related but not
similar associations. Its performance moreover
lies relatively close to that of the word space mod-
els, which were specifically developed to model
semantic similarity. This underpins the observa-
tion that even associations that are semantically
similar to their cues are still highly motivated by
direct co-occurrence in text. Interestingly, only the
compounding approach has a clear preference for
associations that are related to their cue, but not
similar.
A second factor that influences the performance
of the models is frequency. In order to test its
precise impact, we split up the cues and their as-
sociations in three frequency bands of compara-
ble size. For the cues, we constructed a band
for words with a frequency of less than 500 in
the corpus (low), between 500 and 2,500 (mid)
and more than 2,500 (high). For the associations,
we had bands for words with a frequency of less
than 7,500 (low), between 7,500 and 20,000 (mid)
and more than 20,000 (high). Figure 2 shows
the performance of the most important models in
these frequency bands. With respect to cue fre-
quency, the word space models and compound-
ing approach suffer most from low frequencies
and hence, data sparseness. The log-likelihood
ratio is much more robust, while point-wise mu-
tual information even performs better with low-
frequency cues, although it does not yet reach
the performance of the document-based system
or the log-likelihood ratio. With respect to asso-
ciation frequency, the picture is different. Here
the word-based distributional models and PMI per-
form better with low-frequency associations. The
document-based approach is largely insensitive to
association frequency, while the log-likelihood ra-
tio suffers slightly from low frequencies. The per-
formance of the compounding approach decreases
most. What is particularly interesting about this
plot is that it points towards an important differ-
ence between the log-likelihood ratio and point-
wise mutual information. In its search for nearest
neighbours to a given cue word, the log-likelihood
ratio favours frequent words. This is an advanta-
geous feature in the prediction of strong associa-
tions, since people tend to give frequent words as
associations. PMI, like the syntax-based and word-
based models, lacks this characteristic. It therefore
fails to discover mid- and high-frequency associa-
tions in particular.
Finally, despite the similarity in results between
the log-likelihood ratio and the document-based
word space model, there exists substantial varia-
tion in the associations that they predict success-
fully. Table 3 gives an overview of the top ten as-
sociations that are predicted better by one model
than the other, according to the difference be-
653
model cue?association pairs
document-based model cue?billiards, amphibian?frog, fair?doughnut ball, sperm whale?sea,
map?trip, avocado?green, carnivore?meat, one-wheeler?circus,
wallet?money, pinecone?wood
log-likelihood ratio top?toy, oven?hot, sorbet?ice cream, rhubarb?sour, poppy?red,
knot?rope, pepper?red, strawberry?red, massage?oil, raspberry?red
Table 3: A comparison of the document-based model and the log-likelihood ratio on the basis of the
cue?target pairs with the largest difference in log ranks between the two approaches.
tween the models in the logarithm of the rank of
the association. The log-likelihood ratio seems
to be biased towards ?characteristics? of the tar-
get. For instance, it finds the strong associative
relation between poppy, pepper, strawberry, rasp-
berry and their shared colour red much better than
the document-based model, just like it finds the re-
latedness between oven and hot and rhubarb and
sour. The document-based model recovers more
associations that display a strong topical connec-
tion with their cue word. This is thanks to its re-
liance on direct co-occurrence within a large con-
text, which makes it less sensitive to semantic sim-
ilarity than word-based models. It also appears to
have less of a bias toward frequent words than the
log-likelihood ratio. Note, for instance, the pres-
ence of doughnut ball (or smoutebol in Dutch) as
the third nearest neighbour to fair, despite the fact
it occurs only once (!) in the corpus. This com-
plementarity between our two most successful ap-
proaches suggests that a combination of the two
may lead to even better results. We therefore in-
vestigated the benefits of a committee-based or en-
semble approach.
5 Ensemble-based prediction of strong
associations
Given the varied nature of cue?association rela-
tions, it could be beneficial to develop a model that
relies on more than one type of information. En-
semble methods have already proved their effec-
tiveness in the related area of automatic thesaurus
extraction (Curran, 2002), where semantic similar-
ity is the target relation. Curran (2002) explored
three ways of combining multiple ordered sets of
words: (1) mean, taking the mean rank of each
word over the ensemble; (2) harmonic, taking the
harmonic mean; (3) mixture, calculating the mean
similarity score for each word. We will study only
the first two of these approaches, as the different
metrics of our models cannot simply be combined
in a mean relatedness score. More particularly, we
will experiment with ensembles taking the (har-
monic) mean of the natural logarithm of the ranks,
since we found these to perform better than those
working with the original ranks.2
Table 4 compares the results of the most im-
portant ensembles with that of the single best ap-
proach, the log-likelihood ratio with a context size
of 10. By combining the two best approaches
from the previous section, the log-likelihood ra-
tio and the document-based model, we already
achieve a substantial increase in performance. The
mean rank of the association goes from 3 to 2,
the mean from 16.6 to 13.1 and the number of
strong associations with rank 1 climbs from 194
to 223. This is a statistically significant increase
(one-tailed paired Wilcoxon test, W = 30866,
p = .0002). Adding another word space model
to the ensemble, either a word-based or syntax-
based model, brings down performance. However,
the addition of the compound model does lead to a
clear gain in performance. This ensemble finds the
strongest association at a median rank of 2, and a
mean of 11.8. In total, 249 strong associations (out
of a total 593) are presented as the best candidate
by the model ? an increase of 28.4% compared
to the log-likelihood ratio. Hence, despite its poor
performance as a simple model, the compound-
based approach can still give useful information
about the strong association of a cue word when
combined with other models. Based on the origi-
nal ranks, the increase from the previous ensem-
ble is not statistically significant (W = 23929,
p = .31). If we consider differences at the start
of the neighbour list more important and compare
the logarithms of the ranks, however, the increase
becomes significant (W = 29787.5, p = 0.0008).
Its precise impact should thus further be investi-
gated.
2In the case of the harmonic mean, we actually take the
logarithm of rank+1, in order to avoid division by zero.
654
mean harmonic mean
systems med mean rank1 med mean rank1
loglik10 (baseline) 3 16.6 194
loglik10 + doc 2 13.1 223 3 13.4 211
loglik10 + doc + word10 3 13.8 182 3 14.2 187
loglik10 + doc + syn 3 14.4 179 4 14.7 184
loglik10 + doc + comp 2 11.8 249 2 12.2 221
Table 4: Results of ensemble methods.
loglik10 = log-likelihood ratio with context size 10;
doc = document-based model;
word10 = word-based model with context size 10 and a stoplist;
syn = syntax-based model;
comp = compound-based model;
med = median; rank1 = number of associations at rank 1
Let us finally take a look at the types of strong
associations that still tend to receive a low rank in
this ensemble system. The first group consists of
adjectives that refer to an inherent characteristic of
the cue word that is rarely mentioned in text. This
is the case for tennis ball?yellow, cheese?yellow,
grapefruit?bitter. The second type brings together
polysemous cues whose strongest association re-
lates to a different sense than that represented by
its corpus-based nearest neighbour. This applies
to Dutch kant, which is polysemous between side
and lace. Its strongest association, Bruges, is
clearly related to the latter meaning, but its corpus-
based neighbours ball and water suggest the for-
mer. The third type reflects human encyclopaedic
knowledge that is less central to the semantics of
the cue word. Examples are police?blue, love?red,
or triangle?maths. In many of these cases, it ap-
pears that the failure of the model to recover the
strong associations results from corpus limitations
rather than from the model itself.
6 Conclusions and future research
In this paper, we explored three types of basic ap-
proaches to the prediction of strong associations
to a given cue. Collocation measures like the log-
likelihood ratio simply recover those words that
strongly collocate with the cue. Word space mod-
els look for words that appear in similar contexts,
defined as documents, context words or syntac-
tic relations. The compounding approach, finally,
searches for words that combine with the target to
form a compound. The log-likelihood ratio with
a large context size emerged as the best predic-
tor of strong association, followed closely by the
document-based word space model. Moreover,
we showed that an ensemble method combining
the log-likelihood ratio, the document-based word
space model and the compounding approach, out-
performed any of the basic methods by almost
30%.
In a number of ways, this paper is only a first
step towards the successful modelling of cue?
association relations. First, the newspaper cor-
pus that served as our data has some restrictions,
particularly with respect to diversity of genres. It
would be interesting to investigate to what degree
a more general corpus ? a web corpus, for in-
stance ? would be able to accurately predict a
wider range of associations. Second, the mod-
els themselves might benefit from some additional
features. For instance, we are curious to find
out what the influence of dimensionality reduction
would be, particularly for document-based word
space models. Finally, we would like to extend
our test set from strong associations to more asso-
ciations for a given target, in order to investigate
how well the discussed models predict relative as-
sociation strength.
References
Jean Aitchinson. 2003. Words in the Mind. An Intro-
duction to the Mental Lexicon. Blackwell, Oxford.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
haviour Research Methods, 39:510?526.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: Words, sentences,
discourse. Discourse Processes, 25:211?257.
655
Ed H. Chi, Peter Pirolli, Kim Chen, and James Pitkow.
2001. Using information scent to model user infor-
mation needs and actions on the web. In Proceed-
ings of the ACM Conference on Human Factors and
Computing Systems (CHI 2001), pages 490?497.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicog-
raphy. In Proceedings of ACL-27, pages 76?83.
James R. Curran. 2002. Ensemble methods for au-
tomatic thesaurus extraction. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), pages 222?229.
Simon De Deyne and Gert Storms. 2008. Word asso-
ciations: Norms for 1,424 Dutch words in a contin-
uous task. Behaviour Research Methods, 40:198?
205.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19:61?74.
Peter W. Foltz. 1996. Latent Semantic Analysis for
text-based research. Behaviour Research Methods,
Instruments, and Computers, 29:197?202.
Tuomo Kakkonen, Niko Myller, Jari Timonen, and
Erkki Sutinen. 2005. Automatic essay grading with
probabilistic latent semantic analysis. In Proceed-
ings of the 2nd Workshop on Building Educational
Applications Using NLP, pages 29?36.
Thomas K. Landauer and Susan T. Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768?774, Montreal, Canada.
Will Lowe and Scott McDonald. 2000. The di-
rect route: Mediated priming in semantic space.
In Proceedings of COGSCI 2000, pages 675?680.
Lawrence Erlbaum Associates.
Lukas Michelbacher, Stefan Evert, and Hinrich
Schu?tze. 2007. Asymmetric association measures.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-07).
Tom M. Mitchell, Svetlana V. Shinkareva, An-
drew Carlson, Kai-Min Chang, Vicente L. Malva,
Robert A. Mason, and Marcel Adam Just. 2008.
Predicting human brain activity associated with the
meanings of nouns. Science, 320:1191?1195.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts.
2008. Size matters. Tight and loose context defini-
tions in English word space models. In Proceedings
of the ESSLLI Workshop on Distributional Lexical
Semantics, pages 9?16.
Magnus Sahlgren. 2006. The Word-Space Model.
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations Between Words
in High-dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
Sabine Schulte im Walde and Alissa Melinger. 2005.
Identifying semantic relations and functional prop-
erties of human verb associations. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 612?619.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Lonneke Van der Plas. 2008. Automatic Lexico-
Semantic Acquisition for Question Answering.
Ph.D. thesis, University of Groningen, Groningen,
The Netherlands.
Gertjan van Noord. 2006. At last parsing is now oper-
ational. In Piet Mertens, Ce?drick Fairon, Anne Dis-
ter, and Patrick Watrin, editors, Verbum Ex Machina.
Actes de la 13e Confe?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN), pages
20?42.
Tonio Wandmacher, Ekaterina Ovchinnikova, and
Theodore Alexandrov. 2008. Does Latent Seman-
tic Analysis reflect human associations? In Pro-
ceedings of the ESSLLI Workshop on Distributional
Lexical Semantics, pages 63?70.
Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier.
2005. Free word associations correspond to contigu-
ities between words in texts. Journal of Quantitative
Linguistics, 12(2/3):111?122.
656
What?s in a name?
The automatic recognition of metonymical location names.
Yves Peirsman
Quantitative Lexicology and Variational Linguistics
University of Leuven, Belgium
yves.peirsman@arts.kuleuven.be
Abstract
The correct identification of metonymies
is not normally a problem for most peo-
ple. For computers, things are different,
however. In Natural Language Processing,
metonymy recognition is therefore usu-
ally addressed with complex algorithms
that rely on hundreds of labelled train-
ing examples. This paper investigates two
approaches to metonymy recognition that
dispense with this complexity, albeit in
different ways. The first, an unsuper-
vised approach to Word Sense Discrimi-
nation, does not require any labelled train-
ing instances. The second, Memory-Based
Learning, replaces the complexity of cur-
rent algorithms by a ?lazy? learning phase.
While the first approach is often able to
identify a metonymical and a literal clus-
ter in the data, it is the second in particular
that produces state-of-the-art results.
1 Introduction
In the last few years, metonymy has emerged as
an important focus of research in many areas of
linguistics. In Cognitive Linguistics, it is often de-
fined as ?a cognitive process in which one con-
ceptual entity, the vehicle, provides mental access
to another conceptual entity, the target, within the
same domain, or idealized cognitive model (ICM)?
(Ko?vecses, 2002, p.145). In example (1), for in-
stance, China and Taiwan provide mental access
to the governments of the respective countries:
(1) China has always threatened to use force
if Taiwan declared independence. (BNC)
This paper is concerned with algorithms that au-
tomatically recognize such metonymical country
names. These are extremely relevant in Natural
Language Processing, since any system that auto-
matically builds semantic representations of utter-
ances needs to be able to recognize and interpret
metonymical words.
Early approaches to metonymy recognition,
such as Pustejovsky?s (1995), identified a word as
metonymical when it violated certain selectional
restrictions. Indeed, in example (1), China and
Taiwan both violate the restriction that threaten
and declare require an animate subject, and thus
have to be interpreted metonymically. This view
is present in the psycholinguistic literature, too.
Some authors argue that a figurative interpreta-
tion of a word typically comes about when all lit-
eral interpretations fail; see Gibbs (1994) for an
overview. This failure is often due to the violation
of selectional restrictions.
However, in psycholinguistics as well as in
computational linguistics, this approach has lost
much of its appeal. It has become clear to re-
searchers in both fields that many metonymies
do not violate any restrictions at all. In to like
Shakespeare, for instance, there is no explicit lin-
guistic trigger for the metonymical interpretation
of Shakespeare. Rather, it is our world knowl-
edge that pre-empts a literal reading of the au-
thor?s name. Examples like this one demonstrate
that metonymy recognition should not be based on
rigid rules, but rather, on information about the se-
mantic class of the target word and the semantic
and grammatical context in which it occurs. In
psycholinguistics, this insight (among others) has
given rise to theories claiming that a figurative in-
terpretation does not follow the failure of a literal
one, but that both processes occur in parallel (Fris-
son and Pickering, 1999). In computational lin-
guistics, it has led to the development of statisti-
25
cal, corpus-based approaches to metonymy recog-
nition.
This view was first put into computational prac-
tice by Markert and Nissim (2002a). Their key
to success was the realization that metonymy
recognition is a sub-problem of Word Sense
Disambiguation (WSD). They found that most
metonymies in the same semantic class belong to
one of a limited number of metonymical patterns
that can be defined a priori. The task of metonymy
recognition thus consists of the automatic assign-
ment of one of these readings to a target word.
Since all words in the same semantic class may
undergo the same semantic shifts, there only has
to be one classifier per class (and not per word, as
in classic WSD).
In this paper I will be concerned with the
automatic identification of metonymical location
names. More particularly, I will test two new
approaches to metonymy recognition on the basis
of Markert and Nissim?s (2002b) corpora of 1,000
mixed country names and 1,000 instances of
the country name Hungary.1 The most impor-
tant metonymical patterns in these corpora are
place-for-people, place-for-event
and place-for-product. In addition, there
is a label mixed for examples that have two
readings, and othermet for examples that do
not belong to any of the pre-defined metonymical
patterns.
On the mixed country data, Nissim and Mark-
ert?s (2003) classifiers achieved an accuracy of
87%. This was the result of a combination of
both grammatical and semantic information. Their
grammatical information included the function of
a target word and its head. The semantic informa-
tion, in the form of Dekang Lin?s (1998) thesaurus
of semantically similar words, allowed the classi-
fier to search the training set for instances whose
head was similar, and not just identical, to that of
a test instance.
Markert and Nissim?s (2002a) and Nissim and
Markert?s (2003) study is the only one to approach
metonymy recognition from a data-driven, statisti-
cal perspective. However, it also has a number of
disadvantages. First, it requires the annotation of
a large number of training and test instances. This
compromises its possible application to a wide va-
riety of metonymical patterns across a large num-
1This data is publicly available and can be downloaded
from http://homepages.inf.ed.ac.uk/mnissim/mascara.
ber of semantic categories. Second, its algorithms
are rather complex. In the training phase, they
calculate smoothed probabilities on the basis of
a large annotated training corpus and in the test
phase, they iteratively search through a thesaurus
of semantically similar words. This leads to the
question if this complexity is indeed necessary in
metonymy recognition.
This paper investigates two approaches that
each tackle one of these problems. The unsuper-
vised algorithm in section 2 has the intuitive ap-
peal of not requiring any annotated training in-
stances. I will show that it is nevertheless often
able to distinguish between two data clusters that
correlate with the two target readings. In section 3,
I will again take recourse to a supervised learn-
ing method, but one that explicitly incorporates
a much simpler learning phase than its competi-
tors in the literature ? Memory-Based Learning. I
will demonstrate that this algorithm of ?lazy learn-
ing? gives state-of-the-art results in metonymy
recognition. Moreover, although their psychologi-
cal validity is not a focus of the present investiga-
tion, the two studied algorithms have clear links to
models of human behaviour.
2 An unsupervised approach to
metonymy recognition
2.1 Background
Unsupervised machine learning algorithms do
not need any labelled training examples. In-
stead, the machine itself has to try and group
the training instances into a pre-defined number
of clusters, which ideally correspond to the im-
plicit target labels. The approach studied here
is Schu?tze?s (1998) Word Sense Discrimination,
which uses second-order co-occurrence in order to
identify clusters of senses.
Schu?tze?s (1998) algorithm first maps all words
in the training corpus onto word vectors, which
contain frequency information about the word?s
first-order co-occurrents. It then builds a vector
representation for each of the contexts of the target
by adding up the word vectors of the words in this
context. These second-order context vectors get
clustered (often after some form of dimensionality
reduction), and each of the clusters is assumed to
correspond to one of the senses of the target. The
classification of a test word, finally, proceeds by
assigning it to the cluster whose centroid lies near-
est to its context vector. Schu?tze showed that, with
26
about 8,000 training instances on average, this al-
gorithm obtains very promising results.
This unsupervised algorithm is not just attrac-
tive from a computational point of view; it is also
related to human behaviour. First, it was inspired
by Miller and Charles? (1991) observation that hu-
mans rely on contextual similarity in order to de-
termine semantic similarity. Schu?tze (1998) there-
fore hypothesized that there must be a correla-
tion between contextual similarity and word mean-
ing as well: ?a sense is a group of contextually
similar occurrences of a word? (Schu?tze, 1998,
p.99). Second, this algorithm lies at the basis of
Latent Semantic Analysis (LSA). Although the
psycholinguistic merits of LSA are an object of
debate, its performance in several language tasks
compares well to that of humans (Landauer and
Dumais, 1997). Let us therefore investigate if it is
able to tackle metonymy recognition as well.
Schu?tze?s (1998) approach has been imple-
mented in the SenseClusters program (Purandare
and Pedersen, 2004)2, which also incorporates
some interesting variations on and extensions to
the original algorithm. First, Purandare and Ped-
ersen (2004) defend the use of bigram features in-
stead of simple word features. Bigrams are ?or-
dered pairs of words that co-occur within five po-
sitions of each other? (Purandare and Pedersen,
2004, p.2) and will be used throughout this pa-
per. Second, they also found that the hybrid algo-
rithm of Repeated Bisections performs better than
Schu?tze?s (1998) clustering algorithm ? at least
for sparse data ? so I will use it here, too. Finally,
as with all word sense discrimination techniques,
evaluation proceeds indirectly: SenseClusters au-
tomatically finds the alignment of senses and clus-
ters that leads to the fewest misclassifications ?
the confusion matrix that maximizes the diagonal
sum.
2.2 Experiments
On the basis of Markert and Nissim?s location
corpora, I tested if unsupervised learning can be
applied to metonymy recognition. 60% of the
instances were used as training data, 40% as test
data, and the number of pre-defined clusters was
set to two. The experiments were designed with
five specific research questions in mind:
2This software package is freely available and can be
downloaded from http://senseclusters.sourceforge.net.
? Does unsupervised clustering work better
with one-word sets?
Since the unsupervised WSD approach stud-
ied here uses lexical features only, I antici-
pated it to work better with the Hungary data
than with the mixed country set. After all, we
can expect one word to have fewer typical co-
occurrences than an entire semantic class, so
its contexts may be easier to cluster.
? Should a stoplist be used?
Unsupervised clustering on the basis of co-
occurrences usually ignores a number of
words that are thought to be uninforma-
tive about the reading of the target. Exam-
ples of such words are prepositions and ex-
tremely frequent verbs (be, give, go, . . . ). In
metonymy recognition, however, these words
may be much more useful than in classic
WSD. If a location name occurs in a preposi-
tional phrase with in, for instance, it is prob-
ably used literally. Similarly, verbs such as
give and go determine the interpretation of a
possibly metonymical word in contexts like
give sth. to a country (metonymical) and go
to a country (literal). Stoplists may therefore
be less useful in metonymy recognition.
? Are smaller context windows better than
large ones?
Markert and Nissim (2002a) discovered that,
with co-occurrence features, the reduction of
window sizes from 10 to about 3 led to a rad-
ical improvement in precision (from 25% to
above 50%) and recall (from 4% to above
20%). Schu?tze?s (1998) original algorithm,
however, used context windows of 25 words
on either side of the target.
? Does Singular Value Decomposition result
in better performance?3
Schu?tze (1998) found that his algorithm per-
forms better with SVD than without. SVD is
said to abstract away from word dimensions,
and to discover topical dimensions instead.
This helps tackle vocabulary issues such as
synonymy and polysemy, and moreover ad-
dresses data sparseness. However, as Mark-
ert and Nissim (2002a) argue, the sense dis-
tinctions between the literal and metonymi-
cal meanings of a word are not of a topical
3With SVD, I set the number of dimensions to 300, as in
Purandare & Pedersen (2004).
27
+LL, +SVD +LL, -SVD -LL, +SVD -LL, -SVD
context Acc F Acc F Acc F Acc F
20 62.70 37.84** 73.78 11.01 60.54 28.43 54.72 26.24
15 55.95 34.54* 51.08 34.18 55.68 30.51 56.49 27.15
12 58.92 38.71** 60.54 39.67** 65.14 18.87 66.22 18.30
10 55.68 36.92** 59.46 41.41** 61.35 20.99 65.95 20.25
7 54.32 35.25** 66.76 32.79** 59.73 26.60 65.14 25.43
5 66.76 38.81** 52.97 33.08 54.32 28.09 67.03 29.07
3 58.38 37.40** 70.54 14.17 61.62 37.17** 61.62 36.04**
Table 1: Results on the mixed country data of four algorithms with varying context sizes and without a
stoplist.
+LL : statistical feature selection
-LL : frequency-based feature selection
+SVD : dimensionality reduction with SVD
-SVD : no dimensionality reduction
** : F-score is significantly better than random assignment of data to clusters (p < 0.05)
* : difference between F-score and random assignment approaches significance (p < 0.10)
nature. Word dimensions may thus lead to
better performance.
? Should features be selected on the basis of
a statistical test?4
Purandare and Pedersen (2004) used a log-
likelihood test to select their features, prob-
ably because of the intuition that ?candidate
words whose occurrence depends on whether
the ambiguous word occurs will be indica-
tive of one of the senses of the ambiguous
word and hence useful for disambiguation?
(Schu?tze, 1998, p.102). Schu?tze, in con-
trast, found that statistical selection is outper-
formed by frequency-based selection when
SVD is not used.
Like Nissim and Markert (2003), I used four
measures to evaluate the experimental results: pre-
cision, recall and F-score for the metonymical cat-
egory, and overall accuracy. They are defined in
the following way:
? Overall accuracy is the total number of in-
stances that is classified correctly.
? Precision for the metonymical category is the
percentage of metonymical labels that the
classifier assigns correctly.
? Recall for the metonymical category is the
percentage of metonymies that the classifier
recognizes.
4I again followed Purandare & Pedersen (2004) by select-
ing bigrams with a log-likelihood score of 3.841 or more.
? F-score is the harmonic mean between preci-
sion and recall:
F = 2? P ?RP +R(2)
Let us use the confidence matrix below to illustrate
these measures:
LIT MET
LIT 208 86
MET 37 39
If the rows represent the correct labels and the
columns the labels returned by the classifier, we
get the following results:
Acc = 208 + 39208 + 86 + 37 + 39 = 66.76%(3)
P = 3939 + 86 = 31.20%(4)
R = 3939 + 37 = 51.32%(5)
F = 2? 31.20%? 51.32%31.20% + 51.32% = 38.81%(6)
In engineering terms, a WSD system is only use-
ful when its accuracy beats the so-called majority
baseline. This is the accuracy of a system that sim-
ply gives the same, most frequent, label to all test
instances. Such a classifier reaches an accuracy of
79.46% on the test corpus of mixed country names
and of 77.35% on the test corpus with instances of
Hungary.
28
+LL, +SVD +LL, -SVD -LL, +SVD -LL, -SVD
context Acc F Acc F Acc F Acc F
20 58.52 35.06* 73.28 14.63 57.51 32.39 57.00 34.75
15 54.96 36.10* 60.05 33.19 54.20 34.31 57.25 35.38*
12 53.18 38.67** 54.71 32.06 57.76 34.65 54.96 36.10*
10 55.47 34.46 55.47 33.96 56.23 32.81 55.22 32.31
7 51.91 35.93 51.91 24.70 51.91 35.93 65.90 33.00**
5 54.20 21.74 67.18 36.45** 63.87 35.45** 63.36 28.71
3 65.14 33.82** 64.89 35.51** 57.00 35.25* 59.80 36.80**
Table 2: Results on the Hungary data of four algorithms with varying context sizes and without a stoplist.
+LL, +SVD +LL, -SVD -LL, +SVD -LL, -SVD
context Acc F Acc F Acc F Acc F
20 67.51 15.89 67.94 16.00 57.76 36.64** 61.07 35.98**
15 70.74 10.85 70.23 12.03 58.52 36.58** 64.89 34.91**
12 66.92 30.11* 72.01 11.29 64.89 35.51** 64.89 34.29**
10 63.87 29.00 61.83 27.88 63.87 29.00 63.87 29.00
7 67.18 27.93 62.60 27.59 64.38 29.29 64.38 29.29
5 67.18 29.51 67.43 29.67* 67.18 29.51 66.16 28.11
3 68.70 30.51** 68.70 30.51** 68.19 28.57 68.19 28.57
Table 3: Results on the Hungary data of four algorithms with varying context sizes and with a stoplist.
2.3 Experimental results
Compared to this majority baseline, the results of
the unsupervised approach fall below the mark.
None of the accuracy values in tables 1, 2 and 3
lies above this baseline. With baselines of almost
80%, however, this result comes as no surprise.
Moreover, the classifier?s failure to beat the ma-
jority baseline does not necessarily mean that it is
unable to identify a ?metonymical? and a ?literal?
cluster in the data. This ability should be inves-
tigated with a ?2-test instead, which helps us de-
termine if there is a correlation between a test in-
stance?s cluster on the one hand and its label on
the other. If we compare the results with this ?2-
baseline, it emerges that in many cases, the iden-
tified clusters indeed significantly correlate with
the reading of the target words. The default (+LL
+SVD) algorithm, for instance, typically identifies
a metonymical and a literal cluster in the mixed
country data (table 1). It also becomes clear that
the best algorithms are not those with the highest
accuracy values. After all, an accuracy close to the
baseline often results from the identification of one
huge ?literal? cluster that covers most metonymies
as well.
Let us now evaluate the algorithms with respect
to the five research questions I mentioned above.
First, a comparison between the results on the
mixed country data in table 1 and the Hungary
data in table 2 shows that the former are more con-
sistent than the latter. The (+LL +SVD) algorithm
in particular is very successful on the country data.
There is thus no sign of the anticipated difficulty
with sets of mixed target words.
Second, when the algorithm is applied to the set
of mixed country names, it should not use a sto-
plist. Not a single time did the resulting clusters
correlate significantly with the target labels ? the
results were therefore not included here. A possi-
ble reason may be that the useful co-occurrences
in this data tend to be words on the stoplist, but it
should be studied more carefully if this is indeed
the case.
On the Hungary data, the use of a stoplist has a
different effect. Overall success rate remains more
or less the same (although F-scores with a stoplist
are slightly lower on average), but the results dis-
play a different pattern. Broadly speaking, a sto-
plist is most beneficial when feature selection pro-
ceeds on the basis of frequency and when large
contexts are used. Smaller contexts are more suc-
cessful without a stoplist. There is a logic to this:
as I observed above, stoplist words may be infor-
mative about the reading of a possibly metonymi-
cal word, but their usefulness increases when they
are closer to the target. If go occurs within three
words of a country name, it may point towards
a literal reading; if it occurs within a context of
twenty words, it is less likely to do so. This ex-
plains why stoplists work best in combination with
bigger contexts.
Overall, the influence of context is hard to de-
termine. Small windows of three words on either
29
side of the target are generally most successful, but
the context size that should be chosen depends on
other characteristics of the algorithm. The same
is true for dimensionality reduction and statisti-
cal feature selection. In general, the anticipated
negative effects of dimensionality reduction were
not observed, and frequency-based feature selec-
tion clearly benefited algorithms with a stoplist on
the Hungary data. However, the algorithms should
be applied to more data sets in order to investigate
the precise effect of these factors.
In short, although the investigated unsupervised
algorithms never beat the majority baseline for
Markert and Nissim?s (2002b) data, they are often
able to identify two clusters of data that correlate
with the two possible readings. This is true for
the set with one target word as well as for the set
with mixed country names. In general, the algo-
rithms that incorporate both statistical feature se-
lection and Singular Value Decomposition lead to
the best results, except for the Hungary data when
no stoplist is used. In this last case, statistical fea-
ture selection is best dropped and a large context
window should be chosen.
3 Memory-based metonymy recognition
3.1 Background
Memory-Based Learning (MBL), which is imple-
mented in the TiMBL classifier (Daelemans et al,
2004)5 rests on the hypothesis that people inter-
pret new examples of a phenomenon by comparing
them to ?stored representations of earlier experi-
ences? (Daelemans et al, 2004, p.19). It is thus
related to Case-Based reasoning, which holds that
?[r]eference to previous similar situations is often
necessary to deal with the complexities of novel
situations? (Kolodner, 1993, p.5). As a result of
this learning hypothesis, an MBL classifier such as
TiMBL eschews the formulation of complex rules
or the computation of probabilities during its train-
ing phase. Instead it remembers all training vec-
tors and gives a test vector the most frequent label
of the most similar training vectors.
TiMBL implements a number of MBL algo-
rithms. In my experiments, the so-called IB1-IG
algorithm (Daelemans and Van den Bosch, 1992)
proved most successful. It computes the distance
between two vectors X and Y by adding up the
5This software package is freely available and can be
downloaded from http://ilk.uvt.nl/software.html.
weighted distances ? between their corresponding
feature values, as in equation (7):
?(X,Y ) =
n
?
i=1
wi?(xi, yi)(7)
By default, TiMBL determines the weights for each
feature on the basis of the feature?s Information
Gain (the increase in information that the knowl-
edge of that feature?s value brings with it) and the
number of values that the feature can have. The
precise equations are discussed in Daelemans et
al. (2004) and need not concern us any further
here.
3.2 Experiments
I again applied this IB1-IG algorithm to Mark-
ert and Nissim?s (2002b) location corpora. In or-
der to make my results as comparable as possi-
ble to Markert and Nissim?s (2002a) and Nissim
and Markert?s (2003), I made two changes in the
evaluation process. First, evaluation was now per-
formed with 10-fold cross-validation. Second, in
the calculation of accuracy, I made a distinction
between the several metonymical labels, so that a
misclassification within the metonymical category
was penalized as well.
I conducted two rounds of experiments. The
first used only grammatical features: the grammat-
ical function of the word (subj, obj, iobj, pp, gen,
premod, passive subj, other), its head, the pres-
ence of a second head, and the second head (if
present). Such features can be expected to iden-
tify metonymies with a high precision, but since
metonymies may have a wide variety of heads,
performance will likely suffer from data sparse-
ness (Nissim and Markert, 2003). I therefore con-
ducted a second round of experiments, in which I
added semantic information to the feature sets, in
the form of the WordNet hypernym synsets of the
head?s first sense.
WordNet is a machine-readable lexical database
that, among other things, structures English verbs,
nouns and adjectives in a hierarchy of so-called
?synonym sets? or synsets (Fellbaum, 1998). Each
word belongs to such a group of synonyms, and
each synset ?is related to its immediately more
general and more specific synsets via direct hyper-
nym and hyponym relations? (Jurafsky and Mar-
tin, 2000, p.605). Fear, for instance, belongs to
the synset fear, fearfulness, fright, which has emo-
tion as its most immediate, and psychological fea-
30
Acc P R F
TiMBL 86.6% 80.2% 49.5% 61.2%
N&M 87.0% 81.4% 51.0% 62.7%
Table 4: Results for the mixed country data.
TiMBL: TiMBL?s results
N&M: Nissim and Markert?s (2003) results
ture as its highest hypernym. This tree structure
of synsets thus corresponds to a hierarchy of se-
mantic classes that can be used to add semantic
knowledge to a metonymy recognition system.
My experiments investigated a few constella-
tions of semantic features. The simplest of these
used the highest hypernym synset of the head?s
first sense as an extra feature. A second approach
added to the feature vector the head?s highest hy-
pernym synsets, with a maximum of ten. If the
head did not have 10 hypernyms, its own synset
would fill the remaining features. The result of
this last approach is that the MBL classifier first
looks for heads within the same synset as the test
head. If it does not find a word that shares all hy-
pernyms with the test instance, it gradually climbs
the synset hierarchy until it finds the training in-
stances that share as many hypernyms as possi-
ble. Obviously, this approach is able to make more
fine-grained semantic distinctions than the previ-
ous one.
3.3 Experimental results
The experiments with grammatical information
showed that TiMBL is able to replicate Nissim and
Markert?s (2003) results. The obtained accuracy
and F-scores for the mixed country names in ta-
ble 4 are almost identical to Nissim and Mark-
ert?s figures. The results for the Hungary data in
table 5 lie slightly lower, but again mirror Nis-
sim and Markert?s figures closely (Katja Markert,
personal communication). This is all the more
promising since my results were reached without
any semantic information. Remember that Nis-
sim and Markert?s algorithm, in contrast, used
Dekang Lin?s (1998) clusters of semantically sim-
ilar words in order to deal with data sparseness.
Memory-Based Learning does not appear to need
this semantic information to arrive at state-of-the-
art performance. Instead, it tackles possible data
sparseness by its automatic back-off to the gram-
matical role if the target?s head is not found among
the training data.
Acc P R F
84.7% 80.4% 51.9% 63.1%
Table 5: Results for the Hungary data.
Of course, the grammatical role of a target
word is often not sufficient for determining its lit-
eral or metonymical status. Therefore my second
round of experiments investigated if performance
can still be improved by the addition of seman-
tic information. This does not appear to be the
case. Although F-scores for the metonymical cat-
egory tended to increase slightly (as a result of
higher recall values), the system?s accuracy hardly
changed. In order to check if this was due to
the automatic selection of the head?s first Word-
Net sense, I manually disambiguated all heads in
the data. This showed that the first WordNet sense
was indeed often incorrect, but the selection of the
correct sense did not improve performance. The
reason for the failure of WordNet information to
give higher results must thus be found elsewhere.
A first possible explanation is the mismatch be-
tween WordNet?s synsets and our semantic labels.
Many synsets cover such a wide variety of words
that they allow for several readings of the target,
while others are too specific to make generaliza-
tion possible. A second possible explanation is the
predominance of prepositional heads in the data,
for which extra semantic information is useless.
In short, the experiments above demonstrate
convincingly that Memory-Based Learning is a
simple but robust approach to metonymy recog-
nition. This simplicity is a major asset, and is
in stark contrast to the competing approaches to
metonymy recognition in the literature. It should
be studied, however, if there are other features that
can further increase the classifier?s performance.
Attachment information is one such source of in-
formation that certainly deserves further attention.
4 Conclusions
This paper has investigated two computational ap-
proaches to metonymy recognition that both in
their own way are less complex than their com-
petitors in the literature. The unsupervised algo-
rithm in section 2 does not need any labelled train-
ing data; the supervised algorithm of Memory-
Based Learning incorporates an extremely simple
learning phase. Both approaches moreover have
a clear relation to models of human behaviour.
31
Schu?tze?s (1998) approach is related to LSA, a
model whose output correlates with human perfor-
mance on a number of language tasks. Memory-
Based Learning is akin to Case-Based Reasoning,
which holds that people approach a problem by
comparing it to similar instances in their memory.
Rather than presenting a psycholinguistic cri-
tique of these approaches, this paper has investi-
gated their ability to recognize metonymical loca-
tion names. Not surprisingly, it was shown that
the unsupervised approach is not yet a good basis
for a robust metonymy recognition system. Nev-
ertheless, it was often able to distinguish two clus-
ters in the data that correlate with the literal and
metonymical readings. It is striking that this is
also the case for a set of mixed target words from
the same category ? a type of data set that, to
my knowledge, this algorithm had not yet been ap-
plied to. Memory-Based Learning, finally, proved
to be a reliable way of recognizing metonymi-
cal words. Although this approach is much sim-
pler than many competing algorithms, it produced
state-of-the-art results, even without semantic in-
formation.
Acknowledgements
I would like to thank Mirella Lapata, Dirk Geer-
aerts and Dirk Speelman for their feedback on this
project. I am also very grateful to Katja Markert
and Malvina Nissim for their helpful information
about their research.
References
W. Daelemans and A. Van den Bosch. 1992. Generali-
sation performance of backpropagation learning on a
syllabification task. In M. F. J. Drossaers and A. Nij-
holt, editors, Proceedings of TWLT3: Connection-
ism and Natural Language Processing, pages 27?37,
Enschede, The Netherlands.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2004. TiMBL: Tilburg Memory-
Based Learner. Technical report, Induction of
Linguistic Knowledge, Computational Linguistics,
Tilburg University.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
S. Frisson and M. J. Pickering. 1999. The processing
of metonymy: Evidence from eye movements. Jour-
nal of Experimental Psychology: Learning, Memory
and Cognition, 25:1366?1383.
R. W. Jr. Gibbs. 1994. The Poetics of Mind. Figura-
tive Thought, Language and Understanding. Cam-
bridge: Cambridge University Press.
D. Jurafsky and J. H. Martin. 2000. Speech and Lan-
guage Processing. Upper Saddle River, NJ: Prentice
Hall.
J. Kolodner. 1993. Case-Based Reasoning. San Ma-
teo, CA: Morgan Kaufmann Publishers.
Z. Ko?vecses. 2002. Metaphor: A Practical Introduc-
tion. Oxford: Oxford University Press.
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis theory
of the acquisition, induction, and representation of
knowledge. Psychological Review, 104:211?240.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the International Con-
ference on Machine Learning, Madison, USA.
K. Markert and M. Nissim. 2002a. Metonymy res-
olution as a classification task. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), Philadelphia,
USA.
K. Markert and M. Nissim. 2002b. Towards a cor-
pus annotated for metonymies: the case of location
names. In Proceedings of the Third International
Conference on Language Resources and Evaluation
(LREC 2002), Las Palmas, Spain.
G. A. Miller and W. G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
M. Nissim and K. Markert. 2003. Syntactic features
and word similarity for supervised metonymy res-
olution. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-03), Sapporo, Japan.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Confer-
ence on Computational Natural Language Learning,
Boston, USA.
J. Pustejovsky. 1995. The Generative Lexicon. Cam-
bridge, MA: MIT Press.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?124.
32
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 9?16,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Word Space Models of Lexical Variation
Yves Peirsman
Research Foundation ? Flanders &
QLVL, University of Leuven
Leuven, Belgium
yves.peirsman@arts.kuleuven.be
Dirk Speelman
QLVL, University of Leuven
Leuven, Belgium
dirk.speelman@arts.kuleuven.be
Abstract
In the recognition of words that are typical
of a specific language variety, the classic
keyword approach performs rather poorly.
We show how this keyword analysis can be
complemented with a word space model
constructed on the basis of two corpora:
one representative of the language variety
under investigation, and a reference cor-
pus. This combined approach is able to
recognize the markers of a language va-
riety as words that not only have a sig-
nificantly higher frequency as compared
to the reference corpus, but also a differ-
ent distribution. The application of word
space models moreover makes it possible
to automatically discover the lexical alter-
native to a specific marker in the reference
corpus.
1 Introduction
Different varieties of the same language often
come with their lexical peculiarities. Some words
may be restricted to a specific register, while other
ones may have different meanings in different re-
gions. In corpus linguistics, the most straightfor-
ward way of finding such words that are typical
of one language variety is to compile a corpus of
that variety and compare it to a reference corpus
of another variety. The most obvious comparison
takes on the form of a keyword analysis, which
looks for the words that are significantly more fre-
quent in the one corpus as compared to the other
(Dunning, 1993; Scott, 1997; Rayson et al, 2004).
For the purposes of a language-variational study,
this classic keyword approach often does not suf-
fice, however. As Kilgarriff has argued, keyword
statistics are far too sensitive to high frequencies
or topical differences to be used in the study of vo-
cabulary differences (Kilgarriff, 2001). We there-
fore put forward an approach that combines key-
word statistics with distributional models of lex-
ical semantics, or word space models (Sahlgren,
2006; Bullinaria and Levy, 2007; Pado? and Lap-
ata, 2007; Peirsman, 2008). In this way, we not
only check whether two words have significantly
different frequencies in the two relevant language
varieties, but also to what degree their distribution
varies between the corpora.
In this paper, we will focus on the lexical dif-
ferences between two regional varieties of Dutch.
Dutch is interesting because it is the official lan-
guage of two neighbouring countries, Belgium and
the Netherlands. Between these two countries,
there exists a considerable amount of lexical vari-
ation (Speelman et al, 2006). There are words
much more frequently used in one of the two va-
rieties as well as terms that have a different mean-
ing in the two regions. We will call such words
markers of a specific lect ? a general term for re-
giolects, dialects, or other language varieties that
are specific to a certain region, genre, etc. By con-
structing a word space model on the basis of two
corpora instead of one, we will show how the dis-
tributional approach to lexical semantics can aid
the recognition of such lectal variation.
In the next section, we will point out the weak-
nesses of the classic keyword approach, and show
how word space models can provide a solution. In
section 3, we will discuss how our approach recog-
nizes markers of a given lect. In section 4, we will
demonstrate how it can automatically find the al-
ternatives in the other language variety. Section 5
wraps up with conclusions and an outlook for fu-
ture research.
2 Bilectal Word Spaces
Intuitively, the most obvious way of looking for
words that mark a particular language variety, is
to take a corpus that represents this variety, and
calculate its keywords with respect to a reference
9
?2 log-likelihood
keyword ?2 keyword log-likelihood
frank/noun (?franc?) 262492.0 frank/noun (?franc?) 335587.3
meer/adj (?more?) 149505.0 meer/adj (?more?) 153811.6
foto/noun (?photograph?) 84286.7 Vlaams/adj (?Flemish?) 93723.2
Vlaams/adj (?Flemish?) 83663.0 foto/noun (?photograph?) 87235.1
veel/adj (?much?/?many?) 73655.5 vrijdag/noun (?Friday?) 77865.5
Belgisch/adj (?Belgian?) 62280.2 veel/adj (?much?/?many?) 74167.1
vrijdag/noun (?Friday?) 59135.9 Belgisch/adj (?Belgian?) 64786.0
toekomst/noun (?future?) 42440.5 toekomst/noun (?future?) 55879.1
dossier/noun (?file?) 34623.3 dossier/noun (?file?) 45570.0
Antwerps/adj (?Antwerp?) 33659.1 ziekenhuis/noun (?hospital?) 44093.3
Table 1: Top 10 keywords for the Belgian newspaper corpus, as compared to the Twente Nieuws Corpus.
corpus (Dunning, 1993; Scott, 1997; Rayson et al,
2004). This keyword approach has two important
weaknesses, however. First, it has been shown that
statistically significant differences in the relative
frequencies of a word may arise from high abso-
lute frequencies rather than real lexical variation
(Kilgarriff, 2001). Second, in the explicit com-
parison of two language varieties, the keyword ap-
proach offers no way of telling what word in the
reference corpus, if any, serves as the alternative
to an identified marker. Word space models offer
a solution to both of these problems.
We will present this solution on the basis of two
corpora of Dutch. The first is the Twente Nieuws
Corpus (TwNC), a 300 million word corpus of
Netherlandic Dutch newspaper articles from be-
tween 1999 and 2002. The second is a corpus of
Belgian Dutch we compiled ourselves, with the
goal of making it as comparable to the Twente
Nieuws Corpus as possible. With newspaper arti-
cles from six major Belgian newspapers from the
years 1999 to 2005, it totals over 1 billion word
tokens. Here we will work with a subset of this
corpus of around 200 million word tokens.
2.1 Keywords
As our starting point, we calculated the keywords
of the Belgian corpus with respect to the Nether-
landic corpus, both on the basis of a chi-square test
(with Yates? continuity correction) (Scott, 1997)
and the log-likelihood ratio (Dunning, 1993). We
considered only words with a total frequency of
at least 200 that moreover occurred at least five
times in each of the five newspapers that make up
the Belgian corpus. This last restriction was im-
posed in order to exclude idiosyncratic language
use in any of those newspapers. The top ten re-
sulting keywords, listed in Table 1, show an over-
lap of 90% between the tests. The words fall into
a number of distinct groups. Frank, Vlaams, Bel-
gisch and Antwerps (this last word appears only in
the ?2 top ten) indeed typically occur in Belgian
Dutch, simply because they are so tightly con-
nected with Belgian culture. Dossier may reflect
a Belgian preference for this French loanword.
Why the words meer, veel, vrijdag, toekomst and
ziekenhuis (only in the log-likelihood top ten) are
in the lists, however, is harder to explain. There
does not appear to be a linguistically significant
difference in use between the two language va-
rieties, neither in frequency nor in sense. The
presence of foto, finally, may reflect certain pub-
lishing habits of Belgian newspapers, but again,
there is no obvious difference in use between Bel-
gium and the Netherlands. In sum, these Belgian
keywords illustrate the weakness of this approach
in the modelling of lexical differences between
two language varieties. This problem was already
noted by Kilgarriff (2001), who argues that ?[t]he
LOB-Brown differences cannot in general be in-
terpreted as British-American differences?. One
of the reasons is that ?for very common words,
high ?2 values are associated with the sheer quan-
tity of evidence and are not necessarily associated
with a pre-theoretical notion of distinctiveness?.
One way to solve this issue is presented by Speel-
man et al (2008). In their so-called stable lexical
markers analysis, the word frequencies in one cor-
pus are compared to those in several reference cor-
pora. The keyness of a word then corresponds to
the number of times it appears in the resulting key-
word lists of the first corpus. This repetitive test
10
helps filter out spurious keywords whose statistical
significance does not reflect a linguistically signif-
icant difference in frequency. Here we explore an
alternative solution, which scores candidate mark-
ers on the basis of their contextual distribution in
the two corpora, in a so-called bilectal word space.
2.2 Bilectal Word Spaces
Word space models (Sahlgren, 2006; Bullinaria
and Levy, 2007; Pado? and Lapata, 2007; Peirsman,
2008) capture the semantic similarity between two
words on the basis of their distribution in a cor-
pus. In these models, two words are similar when
they often occur with the same context words, or
when they tend to appear in the same syntactic re-
lationships. For our purposes, we need to build a
word space on the basis of two corpora, more or
less in the vein of Rapp?s (1999) method for the
identification of translation equivalents. The main
difference is that we use two corpora of the same
language, each of which should be representative
of one of the language varieties under investiga-
tion. All other variables should be kept as constant
as possible, so that we can attribute differences in
word use between the two corpora to lexical dif-
ferences between the two lects. Next, we select
the words that occur in both corpora (or a subset
of the nmost frequent words to reduce dimension-
ality) as the dimensions of the word space model.
For each target word, we then build two context
vectors, one for each corpus. These context vec-
tors contain information about the distribution of
the target word. We finally calculate the similarity
between two context vectors as the cosine of the
angle between them.
One crucial parameter in the construction of
word space models is their definition of distribu-
tion. Some models consider the syntactic relation-
ships in which a target word takes part (Pado? and
Lapata, 2007), while other approaches look at the
collocation strength between a target and all of the
words that occur within n words to its left and
right (Bullinaria and Levy, 2007). With these last
word-based approaches, it has been shown that
small context sizes in particular lead to good mod-
els of the semantic similarity between two words
(Bullinaria and Levy, 2007; Peirsman, 2008). So
far, we have therefore performed experiments with
context sizes of one, two and three words to the
left and right of the target. These all gave very sim-
ilar results. Experiments with other context sizes
and with syntactic features will be carried out in
the near future. In this paper, we report on the
results of a word-based model with context size
three.
In order to identify the markers of Belgian
Dutch, we start from the keyword lists above. For
each of the keywords, we get their context vector
from the Belgian corpus, and find the 100 most
similar context vectors from the Netherlandic cor-
pus. The words that correspond to these context
vectors are called the ?nearest neighbours? to the
keyword. In the construction of our word space
model, we selected from both corpora the 4,000
most frequent words, and used the cross-section
of 2,538 words as our set of dimensions or context
features. The model then calculated the point-wise
mutual information between the target and each
of the 2,538 context words that occurred at least
twice in its context. All words in the Netherlandic
Dutch corpus with a frequency of at least 200, plus
the target itself, were considered possible nearest
neighbours to the target.
Generally, where there are no major differences
in the use of a keyword between the two lects,
it will have itself as its nearest neighbour. If
this is not the case, this may identify the key-
word as a marker of Belgian Dutch. For exam-
ple, six words from the lists above have them-
selves as their nearest neighbour: meer, foto, veel,
vrijdag, toekomst and ziekenhuis. These are in-
deed the keywords that made little sense from a
language-variational perspective. Dossier is its
own second nearest neighbour, which indicates
that there is slightly less of a match between its
Belgian and Netherlandic use. Finally, the words
linked to Belgian culture ? frank, Vlaams, Bel-
gisch and Antwerps ? are much lower in their
own lists of nearest neighbours, or totally absent,
which correctly identifies them as markers of Bel-
gian Dutch. In short, the keyword analysis ensures
that the word occurs much more frequently in Bel-
gian Dutch than in Netherlandic Dutch; the word
space approach checks if it also has a different dis-
tribution in the two corpora.
For markers of Belgian Dutch, we can interpret
the nearest neighbour suggested by the system as
the other variety?s alternative to that marker. For
instance, dossier has rapport as its nearest neigh-
bour, a synonym which indeed has a high keyword
value for our Netherlandic Dutch corpus. Simi-
larly, the culture-related words have their Dutch
11
equivalents as their distributionally most simi-
lar words: frank has gulden (?guilder?), Vlaams
and Belgisch both have Nederlands (?Dutch?), and
Antwerps has Amsterdams (?Amsterdam (adj.)?).
This makes intuitive sense if we take meaning to
be a relative concept, where for instance a con-
cept like ?currency of this country? is instantiated
by the franc in Belgium and the guilder in Holland
? at least in the pre-Euro period. These findings
suggest that our combined method can be applied
more generally in order to automatically discover
lexical differences between the two language vari-
eties.
3 Recognizing lectal differences
First we want to investigate whether a bilectal
word space model can indeed contribute to the cor-
rect identification of markers of Belgian Dutch on
a larger scale. We therefore had both types of
approaches ? the simple keyword approach and
the combined method ? suggest a top 2,000 of
possible markers on the basis of our two corpora.
The combined approach uses the same word space
method we described above, with 2,538 dimen-
sions and a context size of three. Basing itself
on the lists of nearest neighbours, it then reorders
the list of keywords, so as to arrive at a ranking
that reflects lectal variation better than the original
one. To this goal, each keyword receives a new
score, which is the multiplication of two individ-
ual numbers. The first number is its rank in the
original keyword list. At this point we considered
only the 5,000 highest scoring keywords. The sec-
ond is based on a list that ranks the words accord-
ing to their difference in distribution between the
two corpora. Words that do not occur in their own
list of 100 nearest neighbours appear at the top of
the list (rank 1), followed by words that are their
own 100th nearest neighbour (rank 2), and so on
to the words that have themselves as nearest neigh-
bour (rank 101). In the future we plan to consider
different numbers of neighbours in order to pun-
ish words with very different distributions more
or less heavily. At this stage, however, restrict-
ing the method to 100 nearest neighbours gives
fine results. These two ranks are then multiplied
to give a combined score, on the basis of which a
final list of candidates for lectal variation is com-
puted. The lower this combined score (reflecting
either high keyword values, very different distri-
butions in the two corpora, or both), the higher
candidate marker evaluation
frank/noun (?franc?) culture
Vlaams/adj (?Flemish?) culture
match/noun (?match?) literature
info/noun (?info?)
rijkswacht/noun (?state police?) RBBN
weekend/noun (?weekend?)
schepen/noun (?alderman?) RBBN
fr./noun (?franc?) culture
provinciaal/adj (?provincial?) RBBN
job/noun (?job?) RBBN
Table 2: Top ten candidate markers suggested by
the combined method on the basis of the log-
likelihood ratio.
the likelihood that the word is a marker of Belgian
Dutch. This approach thus ensures that words that
have very different distributions in the two corpora
are promoted with respect to the original keyword
list, while words with very similar distributions are
downgraded.
As our Gold Standard we used the Reference
List of Belgian Dutch (Referentiebestand Belgisch
Nederlands, RBBN), a list of almost 4,000 words
and expressions that are typical of Belgian Dutch
(Martin, 2005). These are classified into a number
of groups ? culturally-related terms (e.g., names
of political parties), Belgian markers that are not
lexicalized in Netherlandic Dutch, markers that
are lexicalized in Netherlandic Dutch, etc. We
used a subset of 717 one-word nouns, verbs and
adjectives that appear at least 200 times in our
Belgian corpus to evaluate our approach. Even
if we informally explore the first ten candidate
markers, the advantages of combining the log-
likelihood ratio with the word space model already
become clear (see table 2). Four of these candi-
dates are in the RBBN gold standard. Similarly,
frank, Vlaams and fr. are culturally related to Bel-
gium, while match has been identified as a typ-
ically Belgian word in previous corpus-linguistic
research (Geeraerts et al, 1999). Info and week-
end are not present in the external sources we con-
sulted, but nevertheless show an interesting distri-
bution with respect to their respective synonyms.
In the Belgian corpus, info occurs more often than
the longer and more formal information (32,009
vs 30,171), whereas in the Dutch corpus the latter
is used about 25 times as frequently as the former
(1,681 vs 41,429). Similarly, the Belgian corpus
12
500 1000 1500 2000
0.0
0
0.0
5
0.1
0
0.1
5
0.2
0
0.2
5
0.3
0
number of candidates
F?
sco
re
chi?squared
log?likelihood
chi?squared + word space
log?likelihood + word space
0.0 0.1 0.2 0.3 0.4 0.5
0.0
0
0.0
5
0.1
0
0.1
5
0.2
0
0.2
5
0.3
0
recall
pre
cis
ion
chi?squared
log?likelihood
chi?squared + word space
log?likelihood + word space
Figure 1: Precision and recall figures of the keyword methods and the combined approaches.
contains far more instances of weekend than of
its synonym weekeinde (35,406 vs 6,390), while
the Dutch corpus shows the reverse pattern (6,974
vs 28,234). These words are thus far better can-
didate markers than the original keywords meer,
foto, veel, vrijdag, toekomst or ziekenhuis, which
have disappeared from the top ten.
Let us now evaluate the methods more broadly,
on the basis of the top 2,000 keywords they sug-
gest. The left plot in Figure 1 shows their F-scores
in function of the number of suggested markers;
the right graph plots precision in function of re-
call. The two keyword approaches score rather
similarly, with the log-likelihood ratio achieving
slightly better results than the chi-square test. This
superiority of the log-likelihood approach was al-
ready noted by Rayson et al (2004). Both com-
bined methods give a very clear advantage over the
simple keyword statistics, again with the best re-
sults for the log-likelihood ratio. For example, ten
of the first 100 candidates suggested by both key-
word approaches are present in our Gold Standard,
giving a precision of 10% and a recall of 1.4% (F-
score 2.4%). Adding our word space model makes
this figure rise to 29 correct markers, resulting in
a precision of 29% and a recall of 4% (F-score
7.1%). This large advantage in performance is
maintained further down the list. At 1000 can-
didates, the keyword approaches have a recall of
around 20% (chi-square 19%, log-likelihood 21%)
and a precision of around 14% (chi-square 14%,
log-likelihood 15%). At the same point, the com-
bined approaches have reached a recall of over
30% (chi-square 31%, log-likelihood 32%) with
a precision of around 22% (chi-square 22%, log-
likelihood 23%). Expressed differently, the best
keyword approach needs around 500 candidates
to recover 10% of the gold standard, 1000 to re-
cover 20% and 2000 to recover 40%. This linear
increase is outperformed by the best combined ap-
proach, which needs only 300, 600 and 1500 can-
didate markers to reach the same recall figures.
This corresponds to relative gains of 40%, 40%
and 25%. As these results indicate, the perfor-
mance gain starts to diminish after 1000 candi-
dates. Future experiments will help determine if
this issue can be resolved with different parameter
settings.
Despite these large gains in performance, the
combined method still has problems with a num-
ber of Belgian markers. A manual analysis of
these cases shows that they often have several
senses, only one of which is typical of Belgian
Dutch. The Reference List for instance contains
fout (?mistake?) and mossel (?mussel?) as Belgian
markers, with their specialized meanings ?foul (in
sports)? and ?weakling?. Not only do these words
have very low keyword values for the Belgian cor-
pus; they also have very similar distributions in
the two corpora, and are their own first and sec-
ond neighbour, respectively. Sometimes a fail-
ure to recognize a particular marker is more due
13
top 100 top 500
class n % n %
in Gold Standard 29 29% 127 25.4%
in Van Dale 11 22% 47 9.4%
related 2 2% 23 4.6%
cultural terms 25 25% 60 12%
total 67 67% 257 51.4%
Table 3: Manual analysis of the top 500 words
suggested by the combined approach.
to the results of one individual method. This
is for instance the case with the correct Belgian
marker home (?(old people?s) home?). Although
the word space model does not find this word in its
own list of nearest Netherlandic neighbours, it re-
mains low on the marker list due to its fairly small
log-likelihood ratio. Conversely, punt, graad and
klaar are rather high on the keyword list of the
Belgian corpus, but are downgraded, as they have
themselves as their nearest neighbour. This is
again because their status as a marker only applies
to one infrequent meaning (?school mark?, ?two-
year cycle of primary education? and ?clear?) in-
stead of the dominant meanings (?final stop, point
(e.g., in sports)?, ?degree? and ?ready?), which are
shared between the two regional varieties. How-
ever, this last disadvantage applies to all markers
that are much more frequently used in Belgium but
still sometimes occur in the Netherlandic corpus
with a similar distribution.
Finally, because our Gold Standard is not an
exhaustive list of Belgian Dutch markers, the re-
sults in Figure 1 are an underestimate of real per-
formance. We therefore manually went through
the top 500 markers suggested by the best com-
bined approach and classified them into three new
groups. The results of this analysis are pre-
sented in Table 3. First, we consulted the Van
Dale Groot Woordenboek der Nederlandse taal
(Den Boon and Geeraerts, 2005), the major dictio-
nary of Dutch, which contains about 3,000 words
marked with the label ?Belgian Dutch?. 11% of
the first 100 and 9.4% of the first 500 candidates
that were initially judged incorrect carry this label
or have a definition that explicitly refers to Bel-
gium. Second, we counted the words that are mor-
phologically related to words in the Gold Standard
or to Belgian words found in Van Dale. These are
for instance compound nouns one of whose parts
is present in the Gold Standard, which means that
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
1.0
number of nearest neighbours
rec
all
nouns
adjectives
verbs
Figure 2: Percentage of markers of Belgian Dutch
whose Netherlandic alternative is present among
their n nearest neighbours.
they are correct markers of Belgian Dutch as well.
They represent 2% of the top 100 and 4.6% of the
top 500. Third, we counted the words that are in-
herently linked to Belgian culture, mostly in the
form of place names. This group corresponds to
25% of the first 100 and 12% of the first 500 can-
didate markers. This suggests that the true preci-
sion of our method at 100 and 500 candidates is
thus at least 67% and 51.4%, respectively.
4 Finding alternatives
The Reference List of Belgian Dutch not only
lists Belgian Dutch words and expressions, but
also gives their Netherlandic Dutch alternative, if
one exists. Our word space model offers us a
promising way of determining this alternative au-
tomatically, by looking at the nearest Netherlandic
neighbours to a Belgian marker. As our Gold Stan-
dard, we selected from the Reference List those
words with a frequency of at least 200 in the Bel-
gian corpus whose Dutch alternative also had a
frequency of at least 200 in the Dutch corpus. This
resulted in a test set of 315 words: 240 nouns,
45 verbs and 30 adjectives. For each of these
words, we used our word space model to find the
100 nearest Netherlandic neighbours, again with
context size three but now with as dimensions all
words shared between the two corpora, in order to
improve performance. We then determined if the
14
Dutch alternative was indeed in the list of nearest
neighbours to the target. We started by looking
at the single nearest neighbour only, and then step
by step extended the list to include the 100 nearest
neighbours. If a word had itself among its nearest
neighbours, this neighbour was discarded and re-
placed by the next one down the list. The results
are shown in Figure 2. 11 out of 30 adjectives
(36.7%), 10 out of 45 verbs (22.2%) and 56 out
of 240 nouns (23.3%) had their Dutch alternative
as their nearest neighbour. At ten nearest neigh-
bours, these figures had risen to 60.0%, 48.9%
and 44.6%. These encouraging results underpin
the usefulness of word space models in language-
variational research.
A manual analysis of Belgian markers for which
the approach does not find the Netherlandic alter-
native again reveals that a large majority of these
errors occur when polysemous words have only
one, infrequent meaning that is typical of Bel-
gian Dutch. For example, the dominant sense
of the word tenor is obviously the ?male singer?
meaning. In Belgium, however, this term can
also refer to a leading figure, for instance in a
political party or a sports discipline. Since this
metaphorical sense is far less frequent than the lit-
eral one, the context vector fails to pick it up, and
almost all nearest Netherlandic neighbours are re-
lated to opera or music. One way to solve this
problem would be to abandon word space models
that build only one context vector per word. In-
stead, we could cluster all individual contexts of a
word, with the aim of identifying context clusters
that correspond to the several senses of that word
(Schu?tze, 1998). This is outside the scope of the
current paper, however.
5 Conclusions and future research
We have presented an application of word space
models to language-variational research. To our
knowledge, the construction of word space mod-
els on the basis of two corpora of the same lan-
guage instead of one is new to both variational
linguistics and Natural Language Processing. It
complements the classic keyword approach in that
it helps recognize those keywords that, in addition
to their different relative frequencies in two lan-
guage varieties, also have a substantially different
distribution. An application of this method to Bel-
gian Dutch showed that the keywords that pass this
test indeed much more often represent markers of
the language variety under investigation. More-
over, often the word space model also succeeded
in identifying the Netherlandic Dutch alternative
to the Belgian marker.
As the development of this approach is still in its
early stages, we have committed ourselves more
to its general presentation than to the precise pa-
rameter settings. In the near future, we therefore
aim to investigate more fully the possible varia-
tion that the method allows. First, we will focus
on the implementation of the word space model,
by studying word-based models with other context
sizes as well as syntax-based approaches. Sec-
ond, we want to examine other ways in which
the word-based model and the classic keyword ap-
proach can be combined, apart from the multipli-
cation of ranks that we have proposed here. While
this large freedom in parameter settings could be
seen as a weakness of the proposed method, the
fact that we obtained similar results for all settings
we have tried out so far, adds to our confidence
that word space models present a sensible com-
plementation of the classic keyword approaches,
irrespective of the precise parameter settings.
In addition to those modelling issues, there are
a number of other extensions we would like to ex-
plore. First, the Gold Standard we have used so
far is rather limited in scope. We therefore plan
to incorporate more sources on language variation
to test the robustness of our approach. Finally, as
we have observed a number of times, the method
in its present form is not sensitive to possibly in-
frequent meanings of a polysemous word. This
may be solved by the application of a clustering
approach that is able to cluster a word?s contexts
into several sense clusters (Schu?tze, 1998). Still,
the promising results in this paper encourage us to
believe that the current approach has a future as a
new method in language-variational research and
as a tool for lexicography.
References
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
haviour Research Methods, 39:510?526.
Ton Den Boon and Dirk Geeraerts. 2005. Van Dale
Groot Woordenboek van de Nederlandse taal (14e
ed.). Van Dale Lexicografie, Utrecht/Antwerp.
Ted Dunning. 1993. Accurate methods for the statis-
15
tics of surprise and coincidence. Computational
Linguistics, 19:61?74.
Dirk Geeraerts, Stefan Grondelaers, and Dirk Speel-
man. 1999. Convergentie en Divergentie in de Ned-
erlandse Woordenschat. Meertens Instituut, Ams-
terdam.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6(1):1?37.
Willy Martin. 2005. Het Belgisch-Nederlands
anders bekeken: het Referentiebestand Belgisch-
Nederlands (RBBN). Technical report, Vrije Uni-
versiteit Amsterdam, Amsterdam, Holland.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Yves Peirsman. 2008. Word space models of seman-
tic similarity and relatedness. In Proceedings of the
13th ESSLLI Student Session, pages 143?152.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 519?526, College Park, Mary-
land.
Paul Rayson, Damon Berridge, and Brian Francis.
2004. Extending the cochran rule for the com-
parison of word frequencies between corpora. In
Proceedings of the 7ie`mes Journe?es Internationales
d?Analyse Statistique des Donne?es Textuelles (JADT
2004), pages 926?936, Louvain-la-Neuve, Belgium.
Magnus Sahlgren. 2006. The Word-Space Model.
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations Between Words
in High-dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Mike Scott. 1997. PC analysis of key words ? and key
key words. System, 25(2):233?245.
Dirk Speelman, Stefan Grondelaers, and Dirk Geer-
aerts. 2006. A profile-based calculation of re-
gion and register variation: The synchronic and di-
achronic status of the two main national varieties
of Dutch. In Andrew Wilson, Dawn Archer, and
Paul Rayson, editors, Corpus Linguistics around the
World, pages 195?202. Rodopi, Amsterdam.
Dirk Speelman, Stefan Grondelaers, and Dirk Geer-
aerts. 2008. Variation in the choice of adjectives
in the two main national varieties of Dutch. In
Gitte Kristiansen and Rene? Dirven, editors, Cogni-
tive Sociolinguistics. Language Variation, Cultural
Models, Social Systems, pages 205?233. Mouton de
Gruyter, Berlin.
16
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 921?929,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Cross-lingual Induction of Selectional Preferences
with Bilingual Vector Spaces
Yves Peirsman
QLVL, University of Leuven
Research Foundation ? Flanders (FWO)
yves.peirsman@arts.kuleuven.be
Sebastian Pad?
IMS, University of Stuttgart
pado@ims.uni-stuttgart.de
Abstract
We describe a cross-lingual method for the in-
duction of selectional preferences for resource-
poor languages, where no accurate monolin-
gual models are available. The method uses
bilingual vector spaces to ?translate? foreign
language predicate-argument structures into
a resource-rich language like English. The
only prerequisite for constructing the bilin-
gual vector space is a large unparsed corpus
in the resource-poor language, although the
model can profit from (even noisy) syntactic
knowledge. Our experiments show that the
cross-lingual predictions correlate well with
human ratings, clearly outperforming monolin-
gual baseline models.
1 Introduction
Selectional preferences capture the empirical observa-
tion that not all words are equally good arguments to
a given verb in a particular argument position (Wilks,
1975; Resnik, 1996). For instance, the subjects of
the English verb to shoot are generally people, while
the direct objects can be people or animals. This is
reflected in speakers? intuitions. Table 1 shows that
the combination the hunter shot the deer is judged
more plausible than the deer shot the hunter. Selec-
tional preferences do not only play an important role
in human sentence processing (McRae et al, 1998),
but are also helpful for NLP tasks like word sense
disambiguation (McCarthy and Carroll, 2003) and
semantic role labeling (Gildea and Jurafsky, 2002).
Computational models of selectional preferences
predict such plausibilities for triples of a predicate p,
an argument position a, and a head word h, such as
Predicate Relation Noun Plausibility
shoot subject hunter 6.9
shoot object hunter 2.8
shoot subject deer 1.0
shoot object deer 6.4
Table 1: Predicate-relation-noun triples with human plau-
sibility judgments on a 7-point scale (McRae et al, 1998)
(shoot,object,hunter). All recent models take a two-
step approach: (1), they extract all triples (p, a, h)
from a large corpus; (2), they apply some type of
generalization to make predictions for unseen items.
Clearly, the accuracy of these models relies crucially
on the quality and coverage of the extracted triples,
and thus on the syntactic analysis of the corpus. Un-
fortunately, corpora that are both large enough and
have a very good syntactic analysis are only available
for a handful of Western and Asian languages, which
leaves all other languages without reliable selectional
preference models.
In this paper, we propose a cross-lingual knowl-
edge transfer approach to this problem: We automat-
ically translate triples (p, a, h) from resource-poor
languages into English, where large and high-quality
parsed corpora are available and we can compute a
reliable plausibility estimate. The translations are
extracted from a bilingual semantic space, which can
be constructed via bootstrapping from large unparsed
corpora in the two languages, without the need for
parallel corpora or bilingual lexical resources.
Structure of the paper. Section 2 reviews models
for selectional preferences. In Section 3, we describe
our approach. Section 4 introduces our experimental
setup, and Sections 5 and 6 present and discuss our
experiments. Section 7 wraps up.
921
2 Selectional Preferences
The first broad-coverage model of selectional prefer-
ences was developed by Resnik (1996). To estimate
the plausibility of a triple (p, a, h), Resnik first ex-
tracted all head words seen with predicate p in posi-
tion a, Seena(p), from a corpus. He then used the
WordNet hierarchy to generalize over the head words
and to create predictions for unseen ones. A number
of studies has followed the same approach, exploring
different ways of using the structure of WordNet (Abe
and Li, 1996; Clark and Weir, 2002). While these
approaches show good results, they can only make
predictions for argument heads that are covered by
WordNet. This is already a problem for English, and
much more so in other languages, where comparable
resources are often much smaller or entirely absent.
A promising alternative approach is to derive
the generalizations from distributional informa-
tion (Prescher et al, 2000; Pad? et al, 2007; Bergsma
et al, 2008). For example, the Pad? et al (2007)
model computes vector space representations for all
head words h and defines the plausibility of the triple
(p, a, h) as a weighted mean of the vector space simi-
larities between h and all h? in Seena(p):
Pl(p, a, h) =
?
h??Seena(p)
w(h?)? sim(h, h?)
?
h? w(h
?)
(1)
where w(h?) is a weight, typically frequency.
In this model, the generalization is provided by dis-
tributional similarity, which can be computed from a
large corpus, without the need for additional lexical
resources. Pad? et al found it to outperform Resnik?s
approach in an evaluation against human plausibility
judgments. However, note that competitive results
are only obtained by representing the head words in
?syntactic? vector spaces whose dimensions consist
of context words with their syntactic relation to the
target rather than just context words. This is not sur-
prising: Presumably, hunter and deer share a domain
and are likely to have similar word-based context
distributions, even though they differ with regard to
their plausibility for particular predicate-argument
positions. Only when the vector space can capture
their different syntactic co-occurrence patterns can
the model predict different plausibilities.
English triple
German triple
(schie?en,obj,Hirsch)
monolingual 
selectional
preference
model
monolingual 
selectional
preference
model
(shoot,obj,deer)
bilingual
vector space
deer
Hirsch
schie?en 
shoot
Figure 1: Predicting selectional preferences for a source
language (e.g. German) by translating into a target lan-
guage (e.g. English) with a bilingual vector space.
3 Cross-lingual selectional preferences
In order to compute reliable selectional preference
representations, distributional models need to see
at least some head words for each (p, a) combina-
tion. Manually annotated treebank corpora, which
are becoming available for an increasing number of
languages, are too small for this task. We therefore
explore the idea of predicting the selectional pref-
erences for such languages by taking advantage of
large corpora with high-quality syntactic analyses in
resource-rich languages like English. This idea falls
into the general approach of cross-lingual knowledge
transfer (see e.g. Hwa et al, 2005). The application
to selectional preferences was suggested by Agirre et
al. (2003), who demonstrated its feasibility by man-
ual translation between Basque and English. We
extend their experiments to an automatic model that
predicts plausibility judgments in a resource-poor
language (source language) by exploiting a model in
a resource-rich language (target language).
Figure 1 sketches our method. We assume that
there is not enough high-quality data to build a mono-
lingual selectional preference model for the source
language (shown by dotted lines). However, we can
use a bilingual vector space, that is, a semantic space
in which words of both the source and the target
language are represented, to translate each source
language word s into the target language by identify-
ing its nearest (most similar) target word tr(s):
tr(s) = argmaxt sim(s, t) (2)
Now we can use a target language selectional prefer-
ence model to obtain plausibilities for source triples:
Pls(p, a, h) = Plt(tr(p), a, tr(h)) (3)
where the superscript indicates the language.
922
Eq. (3) gives rise to three questions: (1), How can
we construct the bilingual space to model tr? (2), Is
translating actually the appropriate way of transfer-
ring selectional preferences? (3), Is it reasonable to
retain the source language argument positions like
subject or object? The following subsections discuss
(1) and (2); we will address (3) in Sections 5 and 6.
3.1 Bilingual Vector Spaces
Bilingual vector spaces are vector spaces in which
words from two languages are represented (cf. Fig. 2).
The dimensions of this space are labeled with bilin-
gual context word pairs (like secretly/heimlich and
rifle/Gewehr for German?English) that are mutual
translations. By treating such context word pairs as
single dimensions, the vector space can represent tar-
get words from both languages, counting the target
words? co-occurrences with the context words from
the respective language. In other words, a source-
target word pair (s, t) will be assigned similar vectors
in the semantic space if the context words of s are
translations of the context words of t. Cross-lingual
semantic similarity between words can be measured
using standard vector space similarity (Lee, 1999).
Importantly, bilingual vector spaces can be built
on the basis of co-occurrences drawn from two un-
related corpora for the source and target languages.
Their construction does not require resources such
as parallel corpora or bilingual translation lexicons,
which might not be available for resource-poor source
languages. Where parallel corpora exist, they often
cover specific domains (e.g., politics), while many
bilingual lexicons are prone to ambiguity problems.
The main challenge in constructing bilingual vec-
tor spaces is determining the set of dimensions,
i.e., bilingual word pairs, using as little knowledge as
possible. Most often, such pairs are extracted from
small bilingual lexicons (Fung and McKeown, 1997;
Rapp, 1999; Chiao and Zweigenbaum, 2002). As
mentioned above, such resources might not be avail-
able. We thus follow an alternative approach by using
frequent cognates, words that are shared between the
two languages (Mark? et al, 2005). Cognates can
be extracted by simple string matching between the
corpora, and mostly share their meaning (Koehn and
Knight, 2002). However, they account for (at most) a
small percentage of all interesting translation pairs.
To extend the set of dimensions available for the
shoot
hit
stalk
rifle/
Gewehr
secretly/
heimlich
schie?en
anschleichen
Figure 2: Sketch of a bilingual vector space for English
(solid dots) and German (empty circles).
bilingual space, we use these cognates merely as a
starting point for a bootstrapping process: We build
a bilingual vector space with the initial word pairs as
dimensions, and identify nearest neighbors between
the two languages in the space. These are added as
dimensions of the bilingual space, and the process
is repeated. Since the focus is on identifying reli-
able source-target word pairs rather than complete
coverage as in Eq. (2), we adopt a symmetrical defi-
nition of translation that pairs up only mutual nearest
neighbors, and allows words to remain untranslated:1
trsym(s) = t iff tr(s) = t and tr(t) = s (4)
From the second iteration onward, this process intro-
duces dimensions that are not identical graphemes,
such as Kind?child and Geschwindigkeit?speed, and
is iterated until convergence. Since each word of
either language can only participate in at most one
dimension, dimensions acquired in later steps can cor-
rect wrong pairs from previous steps, like the ?false
friend? German Kind ?child? ? English kind, which
is part of the initial set of cognates.
3.2 Translation and Selectional Preferences
As Figure 1 shows, the easiest way of exploiting a
bilingual semantic space is to identify for each source
word the target language word with the highest se-
mantic similarity. For example, in Figure 2, the best
translation of German schie?en is its English nearest
neighbor, shoot. However, it is risky to rely on the
single nearest neighbor ? it might simply be wrong.
Even if it is correct, data sparsity is an issue: The
translations may be infrequent in the target language,
or the two translations of p and h may form unlikely
collocates for target language-internal reasons (like
1To avoid unreliable vectors, we also adopt only the 50%
most frequent of the trsym pairs. Frequency is defined as the
geometric mean of the two words? monolingual frequencies.
923
difference in register) that do not reflect plausibility.
A third issue are monolingual semantic phenomena
like polysemy and idioms: The implausible German
triple (schie?en,obj,Brise) will be judged as very plau-
sible due to the English idiom to shoot the breeze.
A look at the broader neighborhood of schie?en
suggests that its second and third-best English neigh-
bors, hit, and stalk, can be used to smooth plausibility
estimates for schie?en. Instead of translating source
language words by their single nearest neighbor, we
will take its k nearest neighbors into account. This
is defensible also from a more fundamental point of
view, which suggests that the cross-lingual transfer of
selectional preferences does not require literal trans-
lation in order to work. First, ontological models
like Resnik?s assume that synonymous words behave
similarly with respect to selectional preferences. Sec-
ond, recent work by Chambers and Jurafsky (2009)
has induced ?narrative chains?, i.e., likely sequences
of events, by their use of similar head words. Thus,
we expect that all k nearest neighbors of a source
predicate s are informative for the selectional prefer-
ences of s (like schie?en) as long as they are either
synonyms of its literal translation (shoot/hit) or come
from the same narrative chain (stalk/kill/. . . ).
It is also clear that smoothing does not always
equate better predictions. Closeness in a word-based
vector space can also just reflect semantic association.
For example, Spanish tenista ?tennis player? is highly
associated with English tennis, but is a bad translation
in terms of selectional preferences. We assume that
this problem is more acute for nouns than for verbs:
The context of verbs is dominated by their arguments,
which is not true for nouns. Consequently, close
nouns in vector space can differ widely in ontological
type, while close verbs generally have one or more
similar argument slots. In our model, we will thus
consider several verb translations, but just the best
head word translation. For details, see Section 5.
4 Experimental Setup
Our evaluation uses English as the target language
and two source languages: German (as a very close
neighbor of English) and Spanish (as a more distant
one). Neither of these languages are really resource-
poor, but they allow us to compare our cross-lingual
model against monolingual models, to emulate dif-
ferent levels of ?resource poorness? and to examine
the model?s learning curve.
Plausibility Data. For German, we used the plau-
sibility judgments collected by Brockmann (2002).
The dataset contains human judgments for ninety
triples sampled from the manually annotated 1 mil-
lion word TiGer corpus (Brants et al, 2002): ten
verbs with three argument positions (subject [SUBJ],
direct object [DOBJ], and oblique (prepositional) ob-
ject [POBJ]) combined with three head words. Mod-
els are evaluated against such datasets by correlating
predicted plausibilities with the (not normally dis-
tributed) human judgments using Spearman?s ?, a
non-parametric rank-order correlation coefficient.
We constructed a similar 90-triple data set for
Spanish by sampling triples from two Spanish cor-
pora (see below) using Brockmann?s (2002) crite-
ria. Human judgments for the triples were collected
through the Amazon Mechanical Turk (AMT) crowd-
sourcing platform (Snow et al, 2008). We asked
native speakers of Spanish to rate the plausibility of
a simple sentence with the relevant verb-argument
combination on a five-point Likert scale, obtaining
between 12 and 17 judgments for each triple. For
each datapoint, we removed the single lowest and
highest judgments and computed the mean. We as-
sessed the reliability of our data by replicating Brock-
mann?s experiment for German with our AMT setup.
With a Spearman ? of almost .90, our own judgments
correlate very well with Brockmann?s original data.
Monolingual Prior Work and Baselines. For
German, Brockmann and Lapata (2003) evaluated
ontology-based models trained on TiGer triples and
the GermaNet ontology. The results in Table 2 show
that while both models are able to predict the data
significantly, neither of the models can predict all of
the data. We attribute this to the small size of TiGer.2
To gauge the limits of monolingual knowledge-
lean approaches, we constructed two monolingual
distributional models for German and Spanish ac-
cording to the Pad? et al (2007) model (Eq. (1)).
Recall that this model performs generalization in a
syntax-based vector space model. We computed vec-
tor spaces from dependency-parsed corpora for the
2For each of the three argument positions and ?all?, Brock-
mann and Lapata report the results for the best parametrization
of the models, which explains the apparently inconsistent results.
924
Resnik Clark & Weir
SUBJ .408* .268
DOBJ .430* .611***
POBJ .330 .597***
all .374*** .232*
Table 2: Monolingual baselines 1. Spearman correla-
tions for ontology-based models in German as reported by
Brockmann and Lapata (2003). *: p < .05; ***: p < .001
Lang. German Spanish
Corpus Schulte?s HGC AnCora Encarta
? Cov. ? Cov. ? Cov.
SUBJ .34? 90% .44* 80% .14 100%
DOBJ .51** 97% .29 83% -.05 100%
POBJ .41* 93% -.03 100% ? ?3
all .33** 93% .16 88% .11 67%
Table 3: Monolingual baselines 2. Spearman correlation
and coverage for distributional models. ? : p < .1; *: p <
.05; **: p < .01.
two languages, using the 2,000 most frequent lemma-
dependency relation pairs as dimensions and adopt-
ing the popular pointwise mutual information metric
as co-occurrence statistic. For German, we used
Schulte im Walde?s verb frame resource (Schulte im
Walde et al, 2001), which contains the frequency of
triples calculated from probabilistic parses of 30M
words from the Huge German Corpus (HGC) of
newswire. For Spanish, we consulted two syntac-
tically analyzed corpora: the AnCora (Taul? et al,
2008) and the Encarta corpus (Calvo et al, 2005). At
0.5M words, the AnCora corpus is small, but man-
ually annotated, whereas the larger, automatically
parsed Encarta corpus amounts to over 18M tokens.
Table 3 shows the results for the distributional
monolingual models. For German, we get significant
correlations for DOBJ and POBJ, an almost signif-
icant correlation for SUBJs, and high significance
for the complete dataset (p < 0.01). These figures
rival the performance of the ontological models (cf.
Table 2), without using ontological information. For
Spanish, the only significant correlation with human
judgments is obtained for subjects, the most frequent
argument position, with the clean AnCora data. An-
Cora is presumably too sparse for the other argument
positions. The large Encarta corpus, in turn, is very
noisy, supporting our concerns from Section 2.
3Since the Encarta data consists of individual dependency
n noun adj verb all
German 7340 .61 .57 .43 .56
Spanish 4143 .62 .67 .41 .58
Table 4: First-translation accuracy for German-English
and Spanish-English translation (n: size of gold standard).
Cross-lingual Selectional Preferences. Our archi-
tecture for the cross-lingual prediction of selectional
preferences shown in Figure 1 consists of two com-
ponents, namely the bilingual vector space and a
selectional preference model in the target language.
As our English selectional preference model, we
again use the Pad? et al (2007) model, trained on
a version of the BNC parsed with MINIPAR (Lin,
1993). The parameters of the syntactic vector space
were the same as for the monolingual baseline mod-
els. The bilingual vector spaces were constructed
from three large, unparsed, comparable monolin-
gual corpora. For German, we used the HGC de-
scribed above. For Spanish, we obtained a corpus
with around 100M words, consisting of 2.5 years of
crawled text from two major Spanish newspapers.
For English, we used the BNC.
We first constructed initial sets of bilingual labels.
For German?English, we identified 1064 graphem-
ically identical word pairs that occurred more than
4 times per million words. Due to the larger lex-
ical distance between Spanish and English, there
are fewer graphemically identical tokens for this lan-
guage pair. We therefore applied a Porter stemmer
and found 2104 identical stems, at a higher risk of
?false friends?. We then applied the bootstrapping
cycle from Section 3.1. The set of dimensions con-
verged after around five iterations.
We evaluated the (asymmetric) nearest neighbor
pairs from the final spaces, (s, tr(s)), against two
online dictionaries.4 Table 4 shows that 55% to 60%
of the pairs are listed in the dictionaries, with parallel
tendencies for both language pairs. The bilingual
space performs fairly well for nouns and adjectives,
but badly for verbs, which is a well-known weakness
of distributional models (Peirsman et al, 2008).
Even taking into account the incompleteness of
dictionaries, this looks like a negative result: more
relations rather than trees, we could not model the POBJ data.
4DE-EN: www.dict.cc; ES-EN: www.freelang.net.
Pairs (s, tr(s)) were only evaluated if the dictionary listed s.
925
than half of all verb translations are incorrect. How-
ever, following up on our intuitions from Section 3.2,
we performed an analysis of the ?incorrect? transla-
tions. It revealed that many of the errors in Table 4
are informative, semantically related words. Near-
est neighbor target language verbs in particular tend
to represent the same event type and take the same
kinds of arguments as the source verb. Examples
are German gef?hrden ?threaten? ? English affect,
and German Neugier ?curiosity? ? English enthusi-
asm. We concluded that literal translation quality is
a misleading figure of merit for our task.
Experimental rationale. Section 3 introduced one
major design decision of our model: the question of
how to treat the argument position, which cannot
be translated by the bilingual vector space, in the
cross-lingual transfer. We present two experiments
that investigate the model?s behavior in the absence
and presence of knowledge about argument positions.
Experiment 1 uses no syntactic knowledge about the
source language whatsoever. In this situation, the
best we can do is to assume that source language
argument positions like SUBJ will correspond to the
same argument position in the target language. Exper-
iment 2 attempts to identify, for each source language
argument position, the ?best fit? position in the target
language. This results in better plausibility estimates,
but also means that we need at least some syntac-
tic information about the source language. In both
experiments, we vary the number of translations we
consider for each verb.
5 Exp. 1: Induction without syntactic
knowledge in the source language
This experiment assumes that argument positions
simply carry over between languages. While this
assumption clearly simplifies linguistic reality, it has
the advantage of not needing any syntactic informa-
tion about the source language. We thus model Ger-
man and Spanish SUBJ relations by English SUBJ
relations and DOBJs by DOBJs. In the case of (lex-
icalized) POBJs, where we cannot assume identity,
we compute plausibility scores for all English POBJs
that account for at least 10% of the predicate?s ar-
gument tokens, and select the PP with the highest
plausibility estimate. The k best ?translations? of the
predicate p, trk(p), are turned into a single prediction
using maximization, yielding the final model:
Plsnosyn(p, a, h) = max
pt?trk(p)
Plt(pt, a, tr(h)) (5)
Note that this model does not use any source lan-
guage information, except the bilingual vector space.
The results of Experiment 1 are given in Table 5
(coverage always 100%). For German, all predictions
correlate significantly with human ratings, and most
even at p < 0.01, despite our naive assumption about
the cross-lingual argument position identity. The
results exceed both monolingual model types (onto-
logical, Tab. 2, and distributional, Tab. 3), notably
without the use of syntactic data. In particular, the
results for the POBJs, notoriously difficult to model
monolingually, are higher than for SUBJs or DOBJs.
We attribute this to the cross-lingual generalization
which takes all prepositional arguments into account.
The Spanish dataset is harder to model overall.
We obtain significantly high correlations for SUBJ,
but non-significant results for DOBJ and POBJ. This
corresponds well to the patterns for the monolingual
AnCora corpus (Table 3). However, we outperform
AnCora on the complete dataset, where it did not
achieve significance, while the cross-lingual model
does at p < 0.01 ? again, even without the use of
syntactic analyses. We attribute the overall lower
results compared to German to systematic syntactic
differences between English and Spanish. For exam-
ple, animate direct objects in Spanish are realized
as POBJs headed by the preposition a. Estimating
the plausibility of such objects by looking at English
POBJs is unlikely to yield good results. The use of
a larger number of verb translations yields a clear
increase in correlation for the German data, but in-
conclusive results for Spanish.
6 Exp. 2: Induction with syntactic
knowledge in the source language
As discussed in Section 3.2, verbs that are semanti-
cally similar in the bilingual vector space may very
well realize their (semantic) argument positions dif-
ferently in the surface syntax. For example, German
teilnehmen is correctly translated to English attend,
but the crucial event argument is realized differently,
namely as a POBJ headed by an in German and as
a DOBJ in English. To address this problem, we
926
DE 1-best 2-best 3-best 4-best 5-best
SUBJ .44* .47** .45* .47** .54**
DOBJ .39* .39* .52** .54** .55**
POBJ .58** .61** .61** .61** .62**
all .35** .37** .37** .38** .40**
ES 1-best 2-best 3-best 4-best 5-best
SUBJ .58** .64** .64** .58** .58**
DOBJ .13 .16 .11 .07 .07
POBJ .13 .13 .09 .14 .14
all .34** .36** .34** .32** .32**
Table 5: Exp.1: Spearman correlation between syntaxless
cross-lingual model and human judgments for k best verb
translations. Best k for each argument position marked in
boldface. Coverage of all models: 100%.
learn a mapping function m that identifies the argu-
ment position at of a target language predicate pt
that corresponds best to an argument position a of a
predicate p in the source language. Our simple model
is in the same spirit as the cross-lingual plausibility
model itself: It returns the argument position at of
pt for which the seen head words of (p, a) are most
plausible when translated into the target language:5
m(p, a, pt) = argmax
at
?
h?Seena(p)
Plt(pt, at, tr(h))
Parallel to Eq. (5), the cross-lingual model is now:
Plssyn(p, a, h) = max
pt?trk(p)
Plt(pt,m(p, a, pt), tr(h))
(6)
This model can recover English argument positions
that correspond better to the original ones than the
identity mapping. For example, on our data, it discov-
ers the mapping for teilnehmen an/attend discussed
above. A second example concerns the incorrect, but
informative translation of stagnieren ?stagnate? as
boost. Here the model recognizes that the SUBJ of
stagnieren (the stagnating entity) corresponds to the
DOBJ of boost.
Establishing m requires syntactic information in
the source language, in order to obtain the set of
seen head words Seenas(ps). For this reason, Exp. 2
uses the parsed subset of the HGC (German), and the
AnCora and Encarta corpora (Spanish). The results
are shown in Table 6. We generally improve over
5To alleviate sparse data, we ignore argument positions of
English verbs that represent less than 10% of its argument tokens.
DE 1-best 2-best 3-best 4-best 5-best
SUBJ .55** .59** .49** .52** .54**
DOBJ .52** .52** .66** .66** .68**
POBJ .61** .68** .70** .69** .70**
all .41** .44** .44* .46** .48**
ES-A 1-best 2-best 3-best 4-best 5-best
SUBJ .52** .47* .42* .41* .42*
DOBJ .52*c .64**c .54*c .42*c .42*c
POBJ .32? .18 .13 .13 .24
all .47** .41** .36** .33** .37**
ES-E 1-best 2-best 3-best 4-best 5-best
SUBJ .40* .42* .39* .39* .41*
DOBJ .21 .02 .06 .13 .20
Table 6: Exp.2: Spearman correlation between syntax-
aware cross-lingual model and human judgments for k
best verb translations. ES-A: AnCora corpus, ES-E: En-
carta corpus. Best k for each argument position in bold-
face. Coverage of all models: 100%, except c: 60%.
Exp. 1. For German, every single model now corre-
lates highly significantly with human judgments (p
< 0.01), and the correlation for the complete dataset
increases from .40 to .48. For Spanish, we see very
good results for the AnCora corpus. Compared to
Exp. 1, we see a slight degradation for the SUBJs;
however, the correlations remain significant for all
values of k. Conversely, all predictions for DOBJs
are now significant,6 and the POBJs have improved at
least numerically, which validates our analysis of the
problems in Exp. 1. The best correlation for the com-
plete dataset improves from .36 to .47. The results
for the Encarta corpus disappoint, though. SUBJs
are significant, but worse than for AnCora, and the
DOBJs remain non-significant throughout. With re-
gard to increasing the number of verb translations,
Exp. 2 shows an almost universal benefit for Ger-
man, but still mixed results for Spanish, which may
indicate that verb translations for Spanish are still
?looser? than the German ones.
In fact, most remaining poor judgments are the
result of problematic translations, which stem from
three main sources. The first one is sparse data. Infre-
quent German and Spanish words often receive unre-
liable vector representations. Some examples are the
6Note, however, that AnCora has an imperfect coverage for
DOBJs (60%). This is because our Spanish dataset contains
verbs sampled from Encarta that do not occur in AnCora.
927
German Tau (?dew?, frequency of 180 in the HGC),
translated as alley, and Reifepr?fung (German SAT,
frequency 120), translated as affiliation. Both of these
may also be due to the difference in genre between
the HGC and the BNC. A second problem is formed
by nearest neighbors that are ontologically dissimi-
lar, as in the tenista ?tennis player?/tennis example
from above. A final issue relates to limitations of the
Pad? et al (2007) model, whose architecture is sus-
ceptible to polysemy-related problems. For instance,
the Spanish combination (excavar, obj, terreno) was
judged by speakers as very plausible, but its English
equivalent (excavate, obj, land) is assigned a very
low score by the model. This might be due to the
fact that in the BNC, land occurs often in its political
meaning, and forms an outlier among the head words
for (excavate,obj).
How much syntactic information is necessary?
The syntax-aware model requires syntactic infor-
mation about the source language, which seems to
run counter to our original motivation of developing
methods for resource-poor languages. To address this
point, we analyzed the behavior of the syntax-aware
model for small syntactically analyzed corpora that
contained only at most m occurrences for each pred-
icate. We obtained the m occurrences by sampling
from the syntactically analyzed part of the HGC; if
fewer than m occurrences were present in the corpus,
we simply used these. Figure 3 shows the training
curve with 1 verb translation, averaged over n rounds
(n = 10 for 5 arguments, n = 5 for 10 arguments,
n = 4 for 20, 50 and 100 arguments). The general
picture is clear: most of the benefit of the syntactic
data is drawn form the first five occurrences for each
argument position. This shows that a small amount of
targeted syntactic annotation can improve the cross-
lingual model substantially.
7 Conclusions
In this article, we have presented a first unsuper-
vised cross-lingual model of selectional preferences.
Our model proceeds by automatically translating
(predicate, argument position, head word) triples for
resource-poor source languages into a resource-rich
target language, where accurate selectional prefer-
ence models are available. The translation is based on
a bilingual vector space, which can be bootstrapped
l
l l
l l l
number of observed heads
Spe
arm
an's
 rho
l SUBJDOBJPOBJ
all
0 5 10 20 50 100
0.3
0.4
0.5
0.6
0.7
Figure 3: Training curve for the bilingual German?English
model as a function of the number of observed head words
per argument position in the source language.
from large unparsed corpora in the two languages.
Our results indicate that bilingual methods can go
a long way towards the modeling of selectional pref-
erences in resource-poor languages, where bilingual
lexicons, parallel corpora, or ontologies might not be
available. Our experiments have looked at German
and Spanish, where the cross-lingual models rival
and even exceed monolingual methods that typically
have to rely on small, clean ?treebank?-style corpora
or large, very noisy, automatically parsed corpora.
We have also demonstrated that noisy syntactic data
from the source language can be integrated in our
model, where it helps improve the cross-lingual han-
dling of argument positions. The linguistic distance
between the languages can impact (1) the ability to
find accurate translations and (2) the degree of syntac-
tic overlap; nevertheless, as Agirre et al (2003) show,
the transfer is possible even for unrelated languages.
In this paper, we have instantiated the selectional
preference model in the target language (English)
with the distributional model by Pad? et al (2007).
However, our approach is modular and can be com-
bined with any other selectional preference model.
We see two main avenues for future work: (1), The
construction of properly bilingual models where
source language information can also help to fur-
ther improve the target language model (Diab and
Resnik, 2002); (2), The extension of our cross-lingual
mapping for the argument position to mappings that
hold across multiple predicates as well as argument-
dependent mappings like the Spanish direct objects,
whose realization depends on their animacy.
928
References
Naoki Abe and Hang Li. 1996. Learning word association
norms using tree cut pair models. In Proc. ICML, pages
3?11, Bari, Italy.
Eneko Agirre, Izaskun Aldezabal, and Eli Pociello. 2003.
A pilot study of English selectional preferences and
their cross-lingual compatibility with Basque. In Proc.
TSD, pages 12?19, Brno, Czech Republic.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference from
unlabeled text. In Proc. EMNLP, pages 59?68, Hon-
olulu, HI.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proc. Workshop on Treebanks and Linguistic
Theories, Sozopol, Bulgaria.
Carsten Brockmann and Mirella Lapata. 2003. Evaluating
and combining approaches to selectional preference
acquisition. In Proc. EACL, pages 27?34, Budapest,
Hungary.
Carsten Brockmann. 2002. Evaluating and combining ap-
proaches to selectional preference acquisition. Master?s
thesis, Universit?t des Saarlandes, Saarbr?cken.
Hiram Calvo, Alexander Gelbukh, and Adam Kilgarriff.
2005. Distributional thesaurus vs. wordnet: A compari-
son of backoff techniques for unsupervised PP attach-
ment. In Proc. CICLing, pages 177?188, Mexico City,
Mexico.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proc. ACL, pages 602?610, Singapore.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proc. COLING, pages
1?5, Taipei, Taiwan.
Stephen Clark and David Weir. 2002. Class-based proba-
bility estimation using a semantic hierarchy. Computa-
tional Linguistics, 28(2):187?206.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proc. ACL, pages 255?262, Philadelphia, PA.
Pascale Fung and Kathleen McKeown. 1997. Finding
terminology translations from non-parallel corpora. In
Proc. 3rd Annual Workshop on Very Large Corpora,
pages 192?202, Hong Kong.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245?288.
Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(3):311?325.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Proc.
ACL-02 Workshop on Unsupervised Lexical Acquisi-
tion, pages 9?16, Philadelphia, PA.
Lillian Lee. 1999. Measures of distributional similarity.
In Proc. ACL, pages 25?32, College Park, MD.
Dekang Lin. 1993. Principle-based parsing without over-
generation. In Proc. ACL, pages 112?120.
Korn?l Mark?, Stefan Schulz, Olena Medelyan, and Udo
Hahn. 2005. Bootstrapping dictionaries for cross-
language information retrieval. In Proc. SIGIR, pages
528?535, Seattle, WA.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically
acquired selectional preferences. Computational Lin-
guistics, 29(4):639?654.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of thematic
fit (and other constraints) in on-line sentence compre-
hension. Journal of Memory and Language, 38:283?
312.
Sebastian Pad?, Ulrike Pad?, and Katrin Erk. 2007. Flex-
ible, corpus-based modelling of human plausibility
judgements. In Proc. EMNLP-CoNLL, pages 400?409,
Prague, Czech Republic.
Yves Peirsman, Kris Heylen, and Dirk Geeraerts. 2008.
Size matters. Tight and loose context definitions in
English word space models. In Proc. ESSLLI Workshop
on Lexical Semantics, pages 9?16, Hamburg, Germany.
Detlef Prescher, Stefan Riezler, and Mats Rooth. 2000.
Using a probabilistic class-based lexicon for lexical
ambiguity resolution. In Proc. COLING, pages 649?
655, Saarbr?cken, Germany.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proc. ACL, pages 519?526, College Park, MD.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational real-
ization. Cognition, 61:127?159.
Sabine Schulte im Walde, Helmut Schmid, Mats Rooth,
Stefan Riezler, and Detlef Prescher. 2001. Statistical
Grammar Models and Lexicon Acquisition. In Linguis-
tic Form and its Computation, pages 389?440. CSLI
Publications, Stanford, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural language
tasks. In Proc. EMNLP, pages 254?263, Honolulu, HI.
Mariona Taul?, M. Ant?nia Mart?, and Marta Recasens.
2008. Ancora: Multilevel annotated corpora for Cata-
lan and Spanish. In Proc. LREC, Marrakech, Morocco.
Yorick Wilks. 1975. Preference semantics. In E. Keenan,
editor, Formal Semantics of Natural Language. Cam-
bridge University Press.
929
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 28?34,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Stanford?s Multi-Pass Sieve Coreference Resolution System at the
CoNLL-2011 Shared Task
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers,
Mihai Surdeanu, Dan Jurafsky
Stanford NLP Group
Stanford University, Stanford, CA 94305
{heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu
Abstract
This paper details the coreference resolution
system submitted by Stanford at the CoNLL-
2011 shared task. Our system is a collection
of deterministic coreference resolution mod-
els that incorporate lexical, syntactic, seman-
tic, and discourse information. All these mod-
els use global document-level information by
sharing mention attributes, such as gender and
number, across mentions in the same cluster.
We participated in both the open and closed
tracks and submitted results using both pre-
dicted and gold mentions. Our system was
ranked first in both tracks, with a score of 57.8
in the closed track and 58.3 in the open track.
1 Introduction
This paper describes the coreference resolution sys-
tem used by Stanford at the CoNLL-2011 shared
task (Pradhan et al, 2011). Our system extends
the multi-pass sieve system of Raghunathan et
al. (2010), which applies tiers of deterministic coref-
erence models one at a time from highest to lowest
precision. Each tier builds on the entity clusters con-
structed by previous models in the sieve, guarantee-
ing that stronger features are given precedence over
weaker ones. Furthermore, this model propagates
global information by sharing attributes (e.g., gender
and number) across mentions in the same cluster.
We made three considerable extensions to the
Raghunathan et al (2010) model. First, we added
five additional sieves, the majority of which address
the semantic similarity between mentions, e.g., us-
ing WordNet distance, and shallow discourse under-
standing, e.g., linking speakers to compatible pro-
nouns. Second, we incorporated a mention detection
sieve at the beginning of the processing flow. This
sieve filters our syntactic constituents unlikely to be
mentions using a simple set of rules on top of the
syntactic analysis of text. And lastly, we added a
post-processing step, which guarantees that the out-
put of our system is compatible with the shared task
and OntoNotes specifications (Hovy et al, 2006;
Pradhan et al, 2007).
Using this system, we participated in both the
closed1 and open2 tracks, using both predicted and
gold mentions. Using predicted mentions, our sys-
tem had an overall score of 57.8 in the closed track
and 58.3 in the open track. These were the top scores
in both tracks. Using gold mentions, our system
scored 60.7 in the closed track in 61.4 in the open
track.
We describe the architecture of our entire system
in Section 2. In Section 3 we show the results of sev-
eral experiments, which compare the impact of the
various features in our system, and analyze the per-
formance drop as we switch from gold mentions and
annotations (named entity mentions and parse trees)
to predicted information. We also report in this sec-
tion our official results in the testing partition.
1Only the provided data can be used, i.e., WordNet and gen-
der gazetteer.
2Any external knowledge source can be used. We used
additional animacy, gender, demonym, and country and states
gazetteers.
28
2 System Architecture
Our system consists of three main stages: mention
detection, followed by coreference resolution, and
finally, post-processing. In the first stage, mentions
are extracted and relevant information about men-
tions, e.g., gender and number, is prepared for the
next step. The second stage implements the ac-
tual coreference resolution of the identified men-
tions. Sieves in this stage are sorted from highest
to lowest precision. For example, the first sieve (i.e.,
highest precision) requires an exact string match be-
tween a mention and its antecedent, whereas the
last one (i.e., lowest precision) implements pronom-
inal coreference resolution. Post-processing is per-
formed to adjust our output to the task specific con-
straints, e.g., removing singletons.
It is important to note that the first system stage,
i.e., the mention detection sieve, favors recall heav-
ily, whereas the second stage, which includes the ac-
tual coreference resolution sieves, is precision ori-
ented. Our results show that this design lead to
state-of-the-art performance despite the simplicity
of the individual components. This strategy has
been successfully used before for information ex-
traction, e.g., in the BioNLP 2009 event extraction
shared task (Kim et al, 2009), several of the top sys-
tems had a first high-recall component to identify
event anchors, followed by high-precision classi-
fiers, which identified event arguments and removed
unlikely event candidates (Bjo?rne et al, 2009). In
the coreference resolution space, several works have
shown that applying a list of rules from highest to
lowest precision is beneficial for coreference reso-
lution (Baldwin, 1997; Raghunathan el al., 2010).
However, we believe we are the first to show that this
high-recall/high-precision strategy yields competi-
tive results for the complete task of coreference res-
olution, i.e., including mention detection and both
nominal and pronominal coreference.
2.1 Mention Detection Sieve
In our particular setup, the recall of the mention de-
tection component is more important than its preci-
sion, because any missed mentions are guaranteed
to affect the final score, but spurious mentions may
not impact the overall score if they are left as sin-
gletons, which are discarded by our post-processing
step. Therefore, our mention detection algorithm fo-
cuses on attaining high recall rather than high preci-
sion. We achieve our goal based on the list of sieves
sorted by recall (from highest to lowest). Each sieve
uses syntactic parse trees, identified named entity
mentions, and a few manually written patterns based
on heuristics and OntoNotes specifications (Hovy et
al., 2006; Pradhan et al, 2007). In the first and
highest recall sieve, we mark all noun phrase (NP),
possessive pronoun, and named entity mentions in
each sentence as candidate mentions. In the follow-
ing sieves, we remove from this set al mentions that
match any of the exclusion rules below:
1. We remove a mention if a larger mention with
the same head word exists, e.g., we remove The
five insurance companies in The five insurance
companies approved to be established this time.
2. We discard numeric entities such as percents,
money, cardinals, and quantities, e.g., 9%,
$10, 000, Tens of thousands, 100 miles.
3. We remove mentions with partitive or quanti-
fier expressions, e.g., a total of 177 projects.
4. We remove pleonastic it pronouns, detected us-
ing a set of known expressions, e.g., It is possi-
ble that.
5. We discard adjectival forms of nations, e.g.,
American.
6. We remove stop words in a predetermined list
of 8 words, e.g., there, ltd., hmm.
Note that the above rules extract both mentions in
appositive and copulative relations, e.g., [[Yongkang
Zhou] , the general manager] or [Mr. Savoca] had
been [a consultant. . . ]. These relations are not an-
notated in the OntoNotes corpus, e.g., in the text
[[Yongkang Zhou] , the general manager], only the
larger mention is annotated. However, appositive
and copulative relations provide useful (and highly
precise) information to our coreference sieves. For
this reason, we keep these mentions as candidates,
and remove them later during post-processing.
2.2 Mention Processing
Once mentions are extracted, we sort them by sen-
tence number, and left-to-right breadth-first traversal
29
order in syntactic trees in the same sentence (Hobbs,
1977). We select for resolution only the first men-
tions in each cluster,3 for two reasons: (a) the first
mention tends to be better defined (Fox, 1993),
which provides a richer environment for feature ex-
traction; and (b) it has fewer antecedent candidates,
which means fewer opportunities to make a mis-
take. For example, given the following ordered list
of mentions, {m11, m
2
2, m
2
3, m
3
4, m
1
5, m
2
6}, where
the subscript indicates textual order and the super-
script indicates cluster id, our model will attempt
to resolve only m22 and m
3
4. Furthermore, we dis-
card first mentions that start with indefinite pronouns
(e.g., some, other) or indefinite articles (e.g., a, an)
if they have no antecedents that have the exact same
string extents.
For each selected mention mi, all previous men-
tions mi?1, . . . , m1 become antecedent candidates.
All sieves traverse the candidate list until they find
a coreferent antecedent according to their criteria
or reach the end of the list. Crucially, when com-
paring two mentions, our approach uses informa-
tion from the entire clusters that contain these men-
tions instead of using just information local to the
corresponding mentions. Specifically, mentions in
a cluster share their attributes (e.g., number, gen-
der, animacy) between them so coreference decision
are better informed. For example, if a cluster con-
tains two mentions: a group of students, which is
singular, and five students, which is plural,
the number attribute of the entire cluster becomes
singular or plural, which allows it to match
other mentions that are both singular and plural.
Please see (Raghunathan et al, 2010) for more de-
tails.
2.3 Coreference Resolution Sieves
2.3.1 Core System
The core of our coreference resolution system is
an incremental extension of the system described in
Raghunathan et al (2010). Our core model includes
two new sieves that address nominal mentions and
are inserted based on their precision in a held-out
corpus (see Table 1 for the complete list of sieves
deployed in our system). Since these two sieves use
3We initialize the clusters as singletons and grow them pro-
gressively in each sieve.
Ordered sieves
1. Mention Detection Sieve
2. Discourse Processing Sieve
3. Exact String Match Sieve
4. Relaxed String Match Sieve
5. Precise Constructs Sieve (e.g., appositives)
6-8. Strict Head Matching Sieves A-C
9. Proper Head Word Match Sieve
10. Alias Sieve
11. Relaxed Head Matching Sieve
12. Lexical Chain Sieve
13. Pronouns Sieve
Table 1: The sieves in our system; sieves new to this pa-
per are in bold.
simple lexical constraints without semantic informa-
tion, we consider them part of the baseline model.
Relaxed String Match: This sieve considers two
nominal mentions as coreferent if the strings ob-
tained by dropping the text following their head
words are identical, e.g., [Clinton] and [Clinton,
whose term ends in January].
Proper Head Word Match: This sieve marks two
mentions headed by proper nouns as coreferent if
they have the same head word and satisfy the fol-
lowing constraints:
Not i-within-i - same as Raghunathan et al (2010).
No location mismatches - the modifiers of two men-
tions cannot contain different location named entities,
other proper nouns, or spatial modifiers. For example,
[Lebanon] and [southern Lebanon] are not coreferent.
No numeric mismatches - the second mention cannot
have a number that does not appear in the antecedent, e.g.,
[people] and [around 200 people] are not coreferent.
In addition to the above, a few more rules are
added to get better performance for predicted men-
tions.
Pronoun distance - sentence distance between a pronoun
and its antecedent cannot be larger than 3.
Bare plurals - bare plurals are generic and cannot have a
coreferent antecedent.
2.3.2 Semantic-Similarity Sieves
We first extend the above system with two
new sieves that exploit semantics from WordNet,
Wikipedia infoboxes, and Freebase records, drawing
on previous coreference work using these databases
(Ng & Cardie, 2002; Daume? & Marcu, 2005;
Ponzetto & Strube, 2006; Ng, 2007; Yang & Su,
30
2007; Bengston & Roth, 2008; Huang et al, 2009;
inter alia). Since the input to a sieve is a collection of
mention clusters built by the previous (more precise)
sieves, we need to link mention clusters (rather than
individual mentions) to records in these three knowl-
edge bases. The following steps generate a query for
these resources from a mention cluster.
First, we select the most representative mention
in a cluster by preferring mentions headed by proper
nouns to mentions headed by common nouns, and
nominal mentions to pronominal ones. In case of
ties, we select the longer string. For example, the
mention selected from the cluster {President George
W. Bush, president, he} is President George W.
Bush. Second, if this mention returns nothing from
the knowledge bases, we implement the following
query relaxation algorithm: (a) remove the text fol-
lowing the mention head word; (b) select the lowest
noun phrase (NP) in the parse tree that includes the
mention head word; (c) use the longest proper noun
(NNP*) sequence that ends with the head word; (d)
select the head word. For example, the query pres-
ident Bill Clinton, whose term ends in January is
successively changed to president Bill Clinton, then
Bill Clinton, and finally Clinton. If multiple records
are returned, we keep the top two for Wikipedia and
Freebase, and all synsets for WordNet.
Alias Sieve
This sieve addresses name aliases, which are de-
tected as follows. Two mentions headed by proper
nouns are marked as aliases (and stored in the same
entity cluster) if they appear in the same Wikipedia
infobox or Freebase record in either the ?name? or
?alias? field, or they appear in the same synset in
WordNet. As an example, this sieve correctly de-
tects America Online and AOL as aliases. We also
tested the utility of Wikipedia categories, but found
little gain over morpho-syntactic features.
Lexical Chain Sieve
This sieve marks two nominal mentions as coref-
erent if they are linked by a WordNet lexical chain
that traverses hypernymy or synonymy relations. We
use all synsets for each mention, but restrict it to
mentions that are at most three sentences apart, and
lexical chains of length at most four. This sieve cor-
rectly links Britain with country, and plane with air-
craft.
To increase the precision of the above two sieves,
we use additional constraints before two mentions
can match: attribute agreement (number, gender, an-
imacy, named entity labels), no i-within-i, no loca-
tion or numeric mismatches (as in Section 2.3.1),
and we do not use the abstract entity synset in Word-
Net, except in chains that include ?organization?.
2.3.3 Discourse Processing Sieve
This sieve matches speakers to compatible pro-
nouns, using shallow discourse understanding to
handle quotations and conversation transcripts. Al-
though more complex discourse constraints have
been proposed, it has been difficult to show improve-
ments (Tetreault & Allen, 2003; 2004).
We begin by identifying speakers within text. In
non-conversational text, we use a simple heuristic
that searches for the subjects of reporting verbs (e.g.,
say) in the same sentence or neighboring sentences
to a quotation. In conversational text, speaker infor-
mation is provided in the dataset.
The extracted speakers then allow us to imple-
ment the following sieve heuristics:
? ?I?s4 assigned to the same speaker are coreferent.
? ?you?s with the same speaker are coreferent.
? The speaker and ?I?s in her text are coreferent.
For example, I, my, and she in the following sen-
tence are coreferent: ?[I] voted for [Nader] because
[he] was most aligned with [my] values,? [she] said.
In addition to the above sieve, we impose speaker
constraints on decisions made by subsequent sieves:
? The speaker and a mention which is not ?I? in the
speaker?s utterance cannot be coreferent.
? Two ?I?s (or two ?you?s, or two ?we?s) assigned to
different speakers cannot be coreferent.
? Two different person pronouns by the same speaker
cannot be coreferent.
? Nominal mentions cannot be coreferent with ?I?,
?you?, or ?we? in the same turn or quotation.
? In conversations, ?you? can corefer only with the
previous speaker.
For example, [my] and [he] are not coreferent in the
above example (third constraint).
4We define ?I? as ?I?, ?my?, ?me?, or ?mine?, ?we? as first
person plural pronouns, and ?you? as second person pronouns.
31
Annotations Coref R P F1
Gold Before 92.8 37.7 53.6
Gold After 75.1 70.1 72.6
Not gold Before 87.9 35.6 50.7
Not gold After 71.7 68.4 70.0
Table 2: Performance of the mention detection compo-
nent, before and after coreference resolution, with both
gold and actual linguistic annotations.
2.4 Post Processing
To guarantee that the output of our system matches
the shared task requirements and the OntoNotes
annotation specification, we implement two post-
processing steps:
? We discard singleton clusters.
? We discard the mention that appears later in
text in appositive and copulative relations. For
example, in the text [[Yongkang Zhou] , the
general manager] or [Mr. Savoca] had been
[a consultant. . . ], the mentions Yongkang Zhou
and a consultant. . . are removed in this stage.
3 Results and Discussion
Table 2 shows the performance of our mention de-
tection algorithm. We show results before and after
coreference resolution and post-processing (when
singleton mentions are removed). We also list re-
sults with gold and predicted linguistic annotations
(i.e., syntactic parses and named entity recognition).
The table shows that the recall of our approach is
92.8% (if gold annotations are used) or 87.9% (with
predicted annotations). In both cases, precision is
low because our algorithm generates many spurious
mentions due to its local nature. However, as the ta-
ble indicates, many of these mentions are removed
during post-processing, because they are assigned
to singleton clusters during coreference resolution.
The two main causes for our recall errors are lack
of recognition of event mentions (e.g., verbal men-
tions such as growing) and parsing errors. Parsing
errors often introduce incorrect mention boundaries,
which yield both recall and precision errors. For
example, our system generates the predicted men-
tion, the working meeting of the ?863 Program? to-
day, for the gold mention the working meeting of the
?863 Program?. Due to this boundary mismatch,
all mentions found to be coreferent with this pre-
dicted mention are counted as precision errors, and
all mentions in the same coreference cluster with the
gold mention are counted as recall errors.
Table 3 lists the results of our end-to-end system
on the development partition. ?External Resources?,
which were used only in the open track, includes: (a)
a hand-built list of genders of first names that we cre-
ated, incorporating frequent names from census lists
and other sources, (b) an animacy list (Ji and Lin,
2009), (c) a country and state gazetteer, and (d) a de-
monym list. ?Discourse? stands for the sieve intro-
duced in Section 2.3.3. ?Semantics? stands for the
sieves presented in Section 2.3.2. The table shows
that the discourse sieve yields an improvement of
almost 2 points to the overall score (row 1 versus
3), and external resources contribute 0.5 points. On
the other hand, the semantic sieves do not help (row
3 versus 4). The latter result contradicts our initial
experiments, where we measured a minor improve-
ment when these sieves were enabled and gold men-
tions were used. Our hypothesis is that, when pre-
dicted mentions are used, the semantic sieves are
more likely to link spurious mentions to existing
clusters, thus introducing precision errors. This sug-
gests that a different tuning of the sieve parameters
is required for the predicted mention scenario. For
this reason, we did not use the semantic sieves for
our submission. Hence, rows 2 and 3 in the table
show the performance of our official submission in
the development set, in the closed and open tracks
respectively.
The last three rows in Table 3 give insight on the
impact of gold information. This analysis indicates
that using gold linguistic annotation yields an im-
provement of only 2 points. This implies that the
quality of current linguistic processors is sufficient
for the task of coreference resolution. On the other
hand, using gold mentions raises the overall score by
15 points. This clearly indicates that pipeline archi-
tectures where mentions are identified first are inad-
equate for this task, and that coreference resolution
might benefit from the joint modeling of mentions
and coreference chains.
Finally, Table 4 lists our results on the held-out
testing partition. Note that in this dataset, the gold
mentions included singletons and generic mentions
32
Components MUC B3 CEAFE BLANC
ER D S GA GM R P F1 R P F1 R P F1 R P F1 avg F1?
58.8 56.5 57.6 68.0 68.7 68.4 44.8 47.1 45.9 68.8 73.5 70.9 57.3?
59.1 57.5 58.3 69.2 71.0 70.1 46.5 48.1 47.3 72.2 78.1 74.8 58.6? ?
60.1 59.5 59.8 69.5 71.9 70.7 46.5 47.1 46.8 73.8 78.6 76.0 59.1? ? ?
60.3 58.5 59.4 69.9 71.1 70.5 45.6 47.3 46.4 73.9 78.2 75.8 58.8? ? ?
63.8 61.5 62.7 71.4 72.3 71.9 47.1 49.5 48.3 75.6 79.6 77.5 61.0? ? ?
73.6 90.0 81.0 69.8 89.2 78.3 79.4 52.5 63.2 79.1 89.2 83.2 74.2? ? ? ?
74.0 90.1 81.3 70.2 89.3 78.6 79.7 53.1 63.7 79.5 89.6 83.6 74.5
Table 3: Comparison between various configurations of our system. ER, D, S stand for External Resources, Discourse,
and Semantics sieves. GA and GM stand for Gold Annotations, and Gold Mentions. The top part of the table shows
results using only predicted annotations and mentions, whereas the bottom part shows results of experiments with gold
information. Avg F1 is the arithmetic mean of MUC, B3, and CEAFE. We used the development partition for these
experiments.
MUC B3 CEAFE BLANC
Track Gold Mention Boundaries R P F1 R P F1 R P F1 R P F1 avg F1
Close Not Gold 61.8 57.5 59.6 68.4 68.2 68.3 43.4 47.8 45.5 70.6 76.2 73.0 57.8
Open Not Gold 62.8 59.3 61.0 68.9 69.0 68.9 43.3 46.8 45.0 71.9 76.6 74.0 58.3
Close Gold 65.9 62.1 63.9 69.5 70.6 70.0 46.3 50.5 48.3 72.0 78.6 74.8 60.7
Open Gold 66.9 63.9 65.4 70.1 71.5 70.8 46.3 49.6 47.9 73.4 79.0 75.8 61.4
Table 4: Results on the official test set.
as well, whereas in development (lines 6 and 7 in Ta-
ble 3), gold mentions included only mentions part of
an actual coreference chain. This explains the large
difference between, say, line 6 in Table 3 and line 4
in Table 4.
Our scores are comparable to previously reported
state-of-the-art results for coreference resolution
with predicted mentions. For example, Haghighi
and Klein (2010) compare four state-of-the-art sys-
tems on three different corpora and report B3 scores
between 63 and 77 points. While the corpora used
in (Haghighi and Klein, 2010) are different from the
one in this shared task, our result of 68 B3 suggests
that our system?s performance is competitive. In this
task, our submissions in both the open and the closed
track obtained the highest scores.
4 Conclusion
In this work we showed how a competitive end-to-
end coreference resolution system can be built using
only deterministic models (or sieves). Our approach
starts with a high-recall mention detection compo-
nent, which identifies mentions using only syntactic
information and named entity boundaries, followed
by a battery of high-precision deterministic corefer-
ence sieves, applied one at a time from highest to
lowest precision. These models incorporate lexical,
syntactic, semantic, and discourse information, and
have access to document-level information (i.e., we
share mention attributes across clusters as they are
built). For this shared task, we extended our ex-
isting system with new sieves that model shallow
discourse (i.e., speaker identification) and seman-
tics (lexical chains and alias detection). Our results
demonstrate that, despite their simplicity, determin-
istic models for coreference resolution obtain com-
petitive results, e.g., we obtained the highest scores
in both the closed and open tracks (57.8 and 58.3
respectively). The code used for this shared task is
publicly released.5
Acknowledgments
We thank the shared task organizers for their effort.
This material is based upon work supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
5See http://nlp.stanford.edu/software/
dcoref.shtml for the standalone coreference resolution
system and http://nlp.stanford.edu/software/
corenlp.shtml for Stanford?s suite of natural language
processing tools, which includes this coreference resolution
system.
33
References
B. Baldwin. 1997. CogNIAC: high precision corefer-
ence with limited knowledge and linguistic resources.
In Proceedings of a Workshop on Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts.
E. Bengston & D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
H. Daume? III and D. Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint entity
detection and tracking model. In EMNLP-HLT.
B. A. Fox 1993. Discourse structure and anaphora:
written and conversational English. Cambridge Uni-
versity Press.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In Proc. of HLT-
NAACL.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R.
Weischedel 2006. OntoNotes: The 90% Solution. In
HLT/NAACL.
Z. Huang, G. Zeng, W. Xu, and A. Celikyilmaz 2009.
Accurate semantic class classifier for coreference res-
olution. In EMNLP.
J.R. Hobbs. 1977. Resolving pronoun references. Lin-
gua.
H. Ji and D. Lin. 2009. Gender and animacy knowl-
edge discovery from web-scale n-grams for unsuper-
vised person mention detection. In PACLIC.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of the BioNLP?09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP?09).
V. Ng 2007. Semantic Class Induction and Coreference
Resolution. In ACL.
V. Ng and C. Cardie. 2002. Improving Machine Learn-
ing Approaches to Coreference Resolution. in ACL
2002
S. Ponzetto and M. Strube. 2006. Exploiting semantic
role labeling, Wordnet and Wikipedia for coreference
resolution. Proceedings of NAACL.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Indentifying Entities and Events
in OntoNotes. In Proceedings of the IEEE Interna-
tional Conference on Semantic Computing (ICSC).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011).
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning 2010.
A Multi-Pass Sieve for Coreference Resolution. In
EMNLP.
J. Tetreault and J. Allen. 2003. An Empirical Evalua-
tion of Pronoun Resolution and Clausal Structure. In
Proceedings of the 2003 International Symposium on
Reference Resolution.
J. Tetreault and J. Allen. 2004. Dialogue Structure and
Pronoun Resolution. In DAARC.
X. Yang and J. Su. 2007. Coreference Resolution Us-
ing Semantic Relatedness Information from Automat-
ically Discovered Patterns. In ACL.
34
