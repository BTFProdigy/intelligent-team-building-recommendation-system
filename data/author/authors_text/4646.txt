Heuristic Methods for Reducing Errors of
Geographic Named Entities Learned
by Bootstrapping
Seungwoo Lee and Gary Geunbae Lee
Department of Computer Science and Engineering,
Pohang University of Science and Technology,
San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Republic of Korea
Abstract. One of issues in the bootstrapping for named entity recogni-
tion is how to control annotation errors introduced at every iteration. In
this paper, we present several heuristics for reducing such errors using
external resources such as WordNet, encyclopedia and Web documents.
The bootstrapping is applied for identifying and classifying fine-grained
geographic named entities, which are useful for applications such as in-
formation extraction and question answering, as well as standard named
entities such as PERSON and ORGANIZATION. The experiments show
the usefulness of the suggested heuristics and the learning curve evalu-
ated at each bootstrapping loop. When our approach was applied to a
newspaper corpus, it could achieve 87 F1 value, which is quite promising
for the fine-grained named entity recognition task.
1 Introduction
A bootstrapping process for named entity recognition is usually as follows. In
the initial stage, it selects seeds and annotates a raw corpus using the seeds.
From the annotation, internal and contextual patterns are learned and applied
to the corpus again to obtain new candidates of each type. Several methods
are adopted to reduce over-generation and incorrect annotation and accept only
correct ones. One sense per discourse heuristic may also be adopted to expand
the annotated instances. It repeats until no more new patterns and entities
are learned.
There are several issues in bootstrapping approaches for named entity recog-
nition task to achieve successful performance. One of them is how to control
annotation errors introduced in the bootstrapping process, on which we are fo-
cusing in this paper. As iteration continues, the bootstrapping expands previous
annotation to increase recall. But this expansion may also introduce annotation
errors and, as a result, decrease the precision. Ambiguous entities may be mis-
classified since learning speed per class depends on seeds. For example, ?New
York? may be misclassified to a city name before the patterns that correctly
classify it to a state name are learned. Especially such errors in the early stage
of the bootstrapping are quite harmful because the errors are accumulated. The
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 658?669, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Heuristic Methods for Reducing Errors of Geographic Named Entities 659
annotation errors are classified into following four cases: inclusion, crossing, type
conflict and spurious. The first three errors occur when a learned entity overlaps
a true entity whereas the last one occurs when a learned entity does not overlap
any true entity. Most previous works depend only on the statistics (e.g., scores
of patterns) obtained from the previous annotation to control such errors. How-
ever, this strategy is not always the best because some trivial errors can also
be corrected by simple heuristics. We suggest several heuristics that control the
annotation errors in Section 4. The heuristics are embedded in a bootstrapping
algorithm, which is modified and improved from [4] and shortly described in
Section 3.
Unlike the traditional named entity task, we deal with sub-categorized ge-
ographic named entities (i.e., locations) in addition to PERSON and ORGA-
NIZATION. Geographic named entities can be classified into many sub-types
that are critical for applications such as information extraction and question
answering. As a first step, we define their ten sub-classes: COUNTRY, STATE,
COUNTY, CITY, MOUNTAIN, RIVER, ISLAND, LAKE, CONTINENT and
OCEAN. We attempt to identify and classify all instances of the eleven classes as
well as PERSON and ORGANIZATION in plain text. Annotation of geographic
named entities is a formidable task. Geographic named entities are frequently
shared between their sub-classes as well as with person names. For example,
?Washington? may indicate a person in one context but may also mean a city or
state in another context. Even country names cannot be exceptions. For some
Americans, ?China? and ?Canada? may be cities where they live. Geographic
named entities such as ?Turkey? and ?Chile? can also be shared with common
nouns. Contextual similarity among geographic named entities is much higher
than the one between PLO (Person-Location-Organization) entities since they
are much closer semantically. These make geographic named entity annotation
task more difficult than that of the traditional named entity task.
The remainder of this paper is as follows. Section 2 presents and compares
related works to our approach. The bootstrapping algorithm is shortly described
in Section 3 and several heuristics for controlling the annotation errors are ex-
plained in Section 4. Section 5 gives some experimental results verifying our
approach, which is followed by conclusions and future works in Section 6.
2 Related Works
Most bootstrapping approaches start with incomplete annotations and patterns
obtained from selected seeds and learn to obtain more complete annotations and
patterns. However, the incompleteness is apt to cause annotation errors to be
introduced in each bootstrapping iteration. Most previous works have designed
their own statistical measures to control such errors. Phillips and Riloff [9] de-
veloped evidence and exclusivity measures to filter out ambiguous terms and
Yangarber et al [12] calculated accuracy, confidence and score of their pat-
terns to select better patterns. However, those statistical measures are calcu-
lated only using data obtained from their training corpus which cannot often
660 S. Lee and G.G. Lee
give enough information. Instead, other resources like World Wide Web as well
as a gazetteer can be incorporated to compensate the lack of information from the
training corpus.
Research on analysis of geographic references recently started to appear and
has two directions. One is to focus on building gazetteer databases [6,11] and
the other is to focus on classifying geographic entity instances in text [5].
Manov et al [6] presented KIM (Knowledge and Information Management)
that consists of an ontology and a knowledge base. They used it for information
extraction but did not show notable results. Uryupina [11] presented a boot-
strapping method to obtain gazetteers from the internet. By searching for seed
names on the internet, she obtained lexical patterns and learned each classi-
fier for six location sub-types, such as COUNTRY, CITY, ISLAND, RIVER,
MOUNTAIN and REGION. Then she obtained and classified candidate names
by searching the patterns in the internet. Li et al [5] suggested a hybrid approach
to classify geographic entities already identified as location by an existing named
entity tagger. They first matched local context patterns and then used a max-
imum spanning tree search for discourse analysis. They also applied a default
sense heuristic as well as one sense per discourse principle. According to their
experiments, the default sense heuristic showed the highest contribution.
3 Bootstrapping
Our bootstrapping algorithm was modified and improved from [4] and the boot-
strapping flow has one initial step and four iterative steps, as shown in Figure 1.
In the initial step, we annotate a raw corpus with seeds automatically obtained
from various gazetteers. Starting and ending boundary patterns are learned from
the annotation and applied to the corpus again to obtain new candidates of
each type. Then we eliminate annotation errors in the candidates using several
Initial Annotation
Corpus
(raw  tagged
(partial  complete))
Learn
Boundary Patterns
Extract
Entity Candidates
Control 
Annotation Errors
Expand/Accept 
Annotation
Boundary
Patterns
Entity
Candidates
Correct
Entities
Seeds
Accepted
Entities
Fig. 1. The bootstrapping overview
Heuristic Methods for Reducing Errors of Geographic Named Entities 661
linguistic heuristics, which is described in detail in Section 4. Finally, the re-
maining entity candidates propagate their annotations into other occurrences
within the same document by one sense per discourse principle [2]. This loop
continues until there are no new patterns learned. The algorithm is summarized
as follows:
Step 0: Seed Preparation and Initial Annotation
We prepare seeds from the gazetteer and obtain initial entity candidate set, C1,
by marking occurrences of the seeds in the training raw corpus.
C1 = {ei|ei is an entity candidate obtained from seeds but not accepted
yet};
And we initialize the number of iteration (k), the set of accepted boundary
patterns (P0) and the set of accepted entities (E0) as follows:
k = 1; P0 = ?; E0 = ?;
Step 1: Controlling the Annotation Errors
We filter out annotation errors among the entity candidates (Ck) using several
heuristics with external resources and construct Ek, a set of entities checked as
correct (see Section 4).
Ek = {ei|ei ? Ck and ei is checked as correct by heuristics};
Step 2: Expanding and Accepting the Annotation
After removing erroneous candidates, we expand the correct entities by applying
one sense per document heuristic and then accept M1 top-ranked entities to
construct a new Ek, the set of currently accepted entities.
Ek = {ei|ei ? Ek or is an instance expanded from ej ? Ek, and
Rank(ei) ? Rank(ei+1), 1 ? i ? M};
Ek = Ek?1 ? Ek;
The rank of an entity candidate, Rank(ei), is computed as follows:
Rank(ei) = 1 ? {1 ? Score(BPs(ei))} ? {1 ? Score(BPe(ei))} (1)
BPs(e) and BPe(e) indicate starting and ending boundary patterns of an entity
e, respectively.
1 M was set to 300 in our experiment.
662 S. Lee and G.G. Lee
Step 3: Learning Boundary Patterns
From the currently accepted entity set, Ek, we learn a new boundary pattern can-
didate set, P?k. We generate starting and ending boundary patterns and compute
the accuracy (Acc(pi)) of each pattern pi which is used to filter out inaccurate
patterns below ?a2 and construct P?k. Then we compute the score (Score(pi))
of each pattern pi and add new N3 top-scored patterns among P?k to the ac-
cepted boundary pattern set, Pk, if there exist new patterns in P?k. Otherwise,
the bootstrapping process stops.
P?k = {pi|pi = BP (e), e ? Ek and pi /? Pk?1 and Acc(pi) ? ?a};
If P?k = ? then stop;
Otherwise, Pk = Pk?1 ? {pi|pi ? P?k and Score(pi) ? Score(pi+1), 1 ?
i ? N};
The accuracy, Acc(p) and the score, Score(p), of a boundary pattern, p, are
computed as follows:
Acc(p) =
pos(p)
pos(p) + neg(p)
?
1 ? 1pos(p)2+1
1 ? 1Np2+1
, (2)
Score(p) =
pos(p)
pos(p) + 2 ? neg(p) + unk(p) ?
1 ? 1ln(pos(p)+3)
1 ? 1ln(Np+3)
, (3)
where pos(p) is the number of instances that are matched to p and already
annotated with the same entity type; neg(p) is the number of instances that are
matched to p but already annotated with a different type or previously filtered
out; unk(p) is the number of instances that are matched to p but not annotated
yet; Np is the maximum value of pos(p).
Step 4: Applying Boundary Patterns and Extracting Candidates
We extract new entity candidates, Ck+1, for the next iteration by applying the
accepted boundary patterns, Pk, to the training corpus and then go to Step 1.
Ck+1 = {ei|BPs(ei) ? Pk and BPe(ei) ? Pk and ei /? Ek};
k := k + 1;
Go to Step 1.
Since each pattern determines only one ? i.e., starting or ending ? boundary, a
candidate is identified and classified by a pair of starting and ending boundary
patterns with the same type.
2 ?a was set to 0.1 in our experiment.
3 N was set to 700 in our experiment.
Heuristic Methods for Reducing Errors of Geographic Named Entities 663
4 Error Controls
The annotation errors introduced in the bootstrapping process are classified into
following four cases, based on the inconsistency between an erroneous entity can-
didate and a true entity: inclusion, crossing, type conflict and spurious. Inclusion
occurs when a candidate is a sub-phrase of a true entity ? e.g., ?U.S.? in ?U.S.
Army?. Crossing occurs when a candidate partially overlaps with a true entity
? e.g., ?Columbia River? in ?British Columbia River?, which means a river in
?British Columbia?. Type conflict occurs when a candidate has the same text
span but different type from a true entity ? e.g., ?New York? may be misclassi-
fied into STATE but it is CITY. Spurious indicates that a candidate is spurious
and does not interfere with any true entities.
To resolve these inconsistencies, we basically use statistical measures such as
the score of a boundary pattern, Score(p), and the rank of an entity candidate,
Rank(e), as in most previous works. However, this strategy is not always the
best because some trivial errors can also be removed by simple heuristics and
linguistic knowledge. Especially, the strategy cannot be applied to erroneous
entities whose inconsistencies cannot be detected since their true entities are not
identified yet. We call it potential inconsistency. We examine potential inclusion
and potential type conflict for each entity candidate using the gazetteer and Web
resources. To overcome this limitation of statistical measures obtained from the
training corpus, we design several methods that incorporate linguistic knowledge
and external resources, which are described in the following subsections.
4.1 Co-occurrence Information
Co-occurrence information (CI) has been widely used to resolve word sense am-
biguity [3,8,10] and also can be employed to resolve crossing and type conflict
inconsistencies, which can be regarded as word sense ambiguity problem. We
assume that two instances of an ambiguous entity that occur in different texts
can be classified into the same class if they share their CI. CI can be collected
from definition statements of an entity of an encyclopedia. For example, the
underlined phrases are collected as CI of an entity ?Clinton? with class CITY
from a statement ?Clinton is a city in Big Stone County, Minnesota, USA?. In
this way, we could construct initial CI for 18000 entities from the Probert En-
cyclopedia (http://www.probertencyclopaedia.com/places.htm), most of which
are geographic entities. We also augment CI from the accepted entity instances
during the bootstrapping process. We consider capitalized nouns or noun phrases
in the window of up to left/right 60 words, within sentence boundary, from an
entity as its CI. Then, the score of an entity e with class t, Coinfo(e, t), is
calculated as the similarity of CI:
Coinfo(e, t) =
?N
i=1 freq(cwi, e, t) ? count(cwi, e)
N
, (4)
where N is the number of co-occurrence information cwi, freq(cwi, e, t) means
the frequency of cwi co-occurring with an entity e of class t in the learned
664 S. Lee and G.G. Lee
co-occurrence information and count(cwi, e) means the frequency of cwi co-
occurring with the entity in the current pending context. When two candidates
cause crossing or type conflict, the candidate having smaller Coinfo is consid-
ered to be incorrect and removed.
4.2 Gazetteer with Locator
Most entities are often mentioned with geographic entities where they are lo-
cated, especially when they are not familiar to general readers. For example,
?Dayton? in ?the Dayton Daily News, Dayton, Ohio? is restricted to an entity
in ?Ohio?. This means that we can classify ?Dayton? into CITY if we know a
fact that there is a city named ?Dayton? and located at ?Ohio?. We can say that
the locator information is a special case of the co-occurrence information. The
locator information was also collected from the Probert Encyclopedia. If one of
two entity candidates causing crossing or type conflict has a verified locator, the
other can be regarded as an error and removed.
4.3 Prior Probability
Ambiguous entities often have different prior probability according to each class.
For example, ?China? appears frequently in general text as a country name but
rarely as a city name. ?Canada? is another example. This means that when
two entity candidates cause type conflict we can remove one having lower prior
probability. It is hard to acquire such probabilities if we do not have a large
annotated corpus. However, WordNet [7] can give us the information that is
needed to infer the relative prior probability since the sense order in WordNet
reflects the frequency that the sense appears in text. According to WordNet, for
example, ?New York? is more frequently mentioned as a city name than as a state
name and, therefore, is classified into CITY if its context does not give strong
information that it is a state name. We could construct relative prior probabilities
for 961 ambiguous gazetteer entries from WordNet. The prior probability of
entity e with type t based on WordNet, PriorWN (e, t), is calculated as follows:
PriorWN (e, t)=
{
1
N+1 + ?WN ?
(m+1)?Sense#W N (e,t)
? m
i=1 i
if there exist in WordNet
1
N+1 ? ?WN otherwise,
(5)
where N is the number of possible types of entity candidate e, m is the number of
types of entity candidate e registered in WordNet, and Sense#WN (e, t) means
the WordNet sense no. of entity candidate e with type t. ?WN and ?WN are
calculated as follows:
?WN = ?WN ? (N ? m) +
1
N + 1
?WN =
m
(N + 1)2
Heuristic Methods for Reducing Errors of Geographic Named Entities 665
Based on these formulas, the prior probabilities of an entity ?New York ? are
given as follows according to its type: (CITY, 0.44), (STATE, 0.32), (TOWN,
0.12), and (COUNTY, 0.12).4
Although this prior probability is quite accurate, it does not have sufficient
applicability. Therefore, we need to develop another method that can acquire
prior probabilities of much more entities and Web can be one alternative. For
each ambiguous entity X, we query ?X is a/an? to at least two Web search
engines5 and extract and collect a noun phrase Y matching to ?X is a/an Y?.
Then, we determine a type, which Y belongs to, using WordNet and count its
frequency. This frequency for each possible type of the entity X is regarded
as sense order information. That is, we can assign to each possible type a sense
number in the descending order of the frequency. Now, the prior probability of an
entity e with type t based on the Web, PriorWeb(e, t), can be similarly calculated.
Then, the final prior probability, Prior(e, t), is computed by arithmetic mean
of PriorWN (e, t) and PriorWeb(e, t). Combined with the Web search, the prior
probabilities of the above example are changed as follows: (CITY, 0.36), (STATE,
0.29), (TOWN, 0.18), and (COUNTY, 0.17).
4.4 Default Type
When an ambiguous candidate causing type conflict is not registered in WordNet
and cannot be detected by the Web search, we can apply default type heuristic.
Unlike the prior probability, default type indicates a priority between any two
target classes regardless of each individual entity. In general, we can say that, for
an ambiguous entity between COUNTRY and CITY, COUNTRY is more dom-
inant than CITY since a country name is more familiar to common people. We
built up default types between all pairs of target classes using human linguistic
knowledge and prior probability described in the previous subsection.
4.5 Part of Other Entity
Potential inclusion is often not exposed at a bootstrapping iteration since bound-
ary patterns for each class are generated at different speeds and, in addition, all
required boundary patterns cannot be generated from seeds. For this, we design
two methods in addition to gazetteer consulting.
First, we check if there exists an acronym for a super-phrase. [1] says that
we can consult a commonly-used acronym to determine extent of a named en-
tity. In other words, ?University of California, Los Angeles?, for example, must
4 WordNet does not have COUNTY and TOWN senses of ?New York ?.
5 We used eight well-known Web search engines such as Google
(http://www.google.com/), Ask Jeeves (http://web.ask.com/), AltaVista
(http://www.altavista.com/), LookSmart (http://search.looksmart.com/),
Teoma (http://s.teoma.com/), AlltheWeb (http://www.alltheweb.com/), Ly-
cos (http://search.lycos.com/), and Yahoo! (http://search.yahoo.com/). We
specially thank to the service providers.
666 S. Lee and G.G. Lee
be annotated as a unique organization name since the university is commonly
referred to as ?UCLA?. As an another example, ?U.S.? in ?U.S. Navy? should
not be annotated as a country name but ?U.S.? in ?U.S. President? should be
since ?U.S. Navy? is represented as the acronym ?USN? but ?U.S. President? is
not represented as ?USP?. To check the existence of their acronyms, we can con-
sult Web search engines by querying the suspected phrases with their possible
acronyms, such as ?U.S. Navy (USN)? and ?U.S. President (USP)?, respectively,
with exact match option.
Another solution is to check if a super-phrase beginning with a candidate
whose class is one of geographic classes can be modified by a prepositional phrase
which is derived by in or comma (,) plus the candidate (denoted as in-loc). For
example, we can decide that ?Beijing? in ?Beijing University? is a part of the
university name, since the phrase ?Beijing University in Beijing? is found by
Web search engines. If the ?Beijing? denotes CITY, ?Beijing University? means
a university in Beijing and is not modified by the prepositional phrase ?in
Beijing? duplicately.
5 Experiments
The bootstrapping algorithm was developed and trained on part of New York
Times articles (the first half of June, 1998; 28MB; 5,330 articles) from the
AQUAINT corpus. We manually annotated 107 articles for test and the counts
of annotated instances were listed in Table 1. A gazetteer composed of 80,000
entries was compiled from several Web sites6. This includes non-target entities
as well as various aliases of entity names.
Table 1. The counts of instances annotated in the test corpus
C
O
U
N
T
R
Y
S
T
A
T
E
C
O
U
N
T
Y
C
IT
Y
R
IV
E
R
M
O
U
N
T
.
IS
L
A
N
D
C
O
N
T
I.
O
C
E
A
N
L
A
K
E
P
E
R
S
O
N
O
R
G
A
N
.
T
ot
al
596 422 61 868 26 15 29 74 19 9 2,660 1,436 6,215
We first examined the usefulness of the heuristics, based on the instances
(i.e., key instances) annotated in the test corpus. Applicability (app.) is defined
as the number of key instances (denoted as #app), to which the heuristic can be
applied, divided by the number of ambiguous ones (denoted as #ambi). Accuracy
(acc.) is defined as the number of instances correctly resolved (denoted as #corr)
divided by #app. There were 2250 ambiguous key instances in the test corpus.
6 http://www.census.gov/, http://crl.nmsu.edu/Resources/resource.htm,
http://www.timeanddate.com/, http://www.probertencyclopaedia.com/places.htm,
http://www.world-of-islands.com/, and http://islands.unep.ch/isldir.htm
Heuristic Methods for Reducing Errors of Geographic Named Entities 667
Applicability and accuracy of the first four heuristics for resolving type conflict
are summarized in Table 2. As shown in the table, the first two heuristics ? co-
occurrence information and gazetteer with locator ? have very low applicability
but very high accuracy. On the contrary, the last two heuristics ? prior probability
and default type ? show moderate accuracy with relatively high applicability.
Based on this result, we combine the four heuristics in sequence such as high
accurate one first and high applicable one last.
We also examined how well the heuristics such as acronym and in-loc can
detect potential inclusion of an entity. In case of acronym, there were 2,555 key
instances (denoted as #app) composed of more than one word and we searched
the Web to check the existence of any possible acronym of each instance. As
a result, we found out the correct acronyms for 1,143 instances (denoted as
#corr). On the contrary, just 47 instances were incorrectly matched when we
tried to search any acronyms of super-phrases of each key instance. In other
words, acronym can detect potential inclusion at 46.58 applicability and 96.05
accuracy. In case of in-loc, 1,282 key instances beginning with a geographic word
are tried to be checked if they appear with in-loc pattern in Web documents and
313 instances of them were confirmed. On the contrary, only 1 super-phrase of
a key instance was incorrectly detected. Therefore, in-loc can detect potential
inclusion at 24.49 applicability and 99.68 accuracy. These are summarized in
Table 3. It says that the heuristics can detect quite accurately the extent of
named entities although they do not have high applicability.
Finally, we evaluated the bootstrapping with the heuristics by investigating
the performance change at every iteration. 50,349 seeds were selected from the
gazetteer after removing ambiguous ones and only 3,364 seeds among them,
which could be applied to the training corpus, were used for training. The recall
and precision were measured using the standard MUC named entity scoring
scheme and plotted in Figure 2. Starting at low recall and high precision, it
gradually increases recall but slightly degrades precision, and it arrived at 87 F1
Table 2. Applicability and accuracy of the heuristics for resolving the inconsistency
of 2,250 ambiguous instances (#ambi=2,250)
#app #corr app. acc.
co. info. 44 42 1.96 95.45
gaz. loc. 148 141 6.58 95.27
prior prob. 2,072 1,741 92.09 84.03
def. type 2,225 1,367 98.89 61.44
Table 3. Applicability and accuracy of the heuristics for detecting potential inclusion
#ambi #app #corr app. acc.
acronym 2,555 1,190 1,143 46.58 96.05
in-loc 1,282 314 313 24.49 99.68
668 S. Lee and G.G. Lee
85
90
95
100
0 10 20 30 40 50 60 70 80 90
Recall
Pr
e c
is i
on
without heuristics
with heuristics
Fig. 2. The learning curve of the bootstrapping with the heuristics
(81 recall and 93 precision) after 1,100 iterations. We think that this performance
is quite notable considering our fine-grained target classes, and the suggested
heuristics work well to prevent incorrect entity candidates from being accepted
during bootstrapping process.
6 Conclusions
In this paper, we observed four kinds of inconsistencies that degrade the per-
formance of bootstrapping for named entity recognition with fine-grained geo-
graphic classes. To resolve such inconsistencies, we suggested several heuristics
incorporating human linguistic knowledge and external resources like encyclope-
dia and Web documents. By analyzing the capability of each heuristic, we com-
bined them in sequence. The bootstrapping with the heuristics was evaluated.
Starting at low recall and high precision, the bootstrapping largely increased
recall at a small cost of precision, and finally it achieved 87 F1. This means
that the suggested approach is quite promising for the fine-grained named entity
recognition task and the suggested heuristics can effectively reduce incorrect can-
didates introduced at the intermediate bootstrapping steps. In future, we plan
to design a uniform statistical method that can augment the suggested heuris-
tics especially using Web resources and also incorporate our heuristic knowledge
used for filtering into the statistical model.
Acknowledgements
This work was supported by 21C Frontier Project on Human-Robot Interface
(MOCIE) and by BK21 Project (Ministry of Education).
Heuristic Methods for Reducing Errors of Geographic Named Entities 669
References
1. Chinchor, N., Brown, E., Ferro, L., Robinson, P.: 1999 Named Entity
Recognition Task Definition (version 1.4). http://www.nist.gov/speech/tests/ie-
er/er 99/doc/ne99 taskdef v1 4.pdf (1999)
2. Gale, W.A., Church, K.W., Yarowsky, D.: One Sense Per Discourse. In: Proceedings
of the 4th DARPA Speech and Natural Language Workshop. (1992) 233?237
3. Guthrie, J.A., Guthrie, L., Wilks, Y., Aidinejad, H.: Subject-dependent Co-
occurrence and Word Sense Disambiguation. In: Proceedings of the 29th Annual
Meeting of the Association for Computational Linguistics (ACL), Berkeley, CA
(1991) 146?152
4. Lee, S., Lee, G.G.: A Bootstrapping Approach for Geographic Named Entity
Annotation. In: Proceedings of the 2004 Conference on Asia Information Retrieval
Symposium (AIRS2004), Beijing, China (2004) 128?133
5. Li, H., Srihari, R.K., Niu, C., Li, W.: InfoXtract location normalization: a hybrid
approach to geographic references in information extraction. In: Proceedings of
the HLT-NAACL 2003 Workshop on Analysis of Geographic References, Alberta,
Canada (2003) 39?44
6. Manov, D., Kirjakov, A., Popov, B., Bontcheva, K., Maynard, D., Cunningham, H.:
Experiments with geographic knowledge for information extraction. In: Proceed-
ings of the HLT-NAACL 2003 Workshop on Analysis of Geographic References,
Alberta, Canada (2003) 1?9
7. Miller, G.A.: WordNet: A lexical database for English. Communications of the
ACM 38 (1995) 39?41
8. Niwa, Y., Nitta, Y.: Co-occurrence Vectors from Corpora vs Distance Vectors from
Dictionaries. In: Proceedings of the 15th International Conference on Computa-
tional Linguistics (COLING?94), Kyoto, Japan (1994) 304?309
9. Phillips, W., Riloff, E.: Exploiting Strong Syntactic Heuristics and Co-Training
to Learn Semantic Lexicons. In: Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP2002), Philadelphia, PA (2002)
125?132
10. Shin, S., soek Choi, Y., Choi, K.S.: Word Sense Disambiguation Using Vectors
of Co-occurrence Information. In: Proceedings of the Sixth Natural Language
Processing Pacific Rim Symposium (NLPRS2001), Tokyo, Japan (2001) 49?55
11. Uryupina, O.: Semi-supervised learning of geographical gazetteers from the inter-
net. In: Proceedings of the HLT-NAACL 2003 Workshop on Analysis of Geographic
References, Alberta, Canada (2003) 18?25
12. Yangarber, R., Lin, W., Grishman, R.: Unsupervised Learning of Generalized
Names. In: Proceedings of the 19th International Conference on Computational
Linguistics (COLING 2002), Taipei, Taiwan (2002) 1135?1141
Automatic Acquisition of Named Entity Tagged Corpus from World Wide
Web
Joohui An
Dept. of CSE
POSTECH
Pohang, Korea 790-784
minnie@postech.ac.kr
Seungwoo Lee
Dept. of CSE
POSTECH
Pohang, Korea 790-784
pinesnow@postech.ac.kr
Gary Geunbae Lee
Dept. of CSE
POSTECH
Pohang, Korea 790-784
gblee@postech.ac.kr
Abstract
In this paper, we present a method that
automatically constructs a Named En-
tity (NE) tagged corpus from the web
to be used for learning of Named En-
tity Recognition systems. We use an NE
list and an web search engine to col-
lect web documents which contain the
NE instances. The documents are refined
through sentence separation and text re-
finement procedures and NE instances are
finally tagged with the appropriate NE cat-
egories. Our experiments demonstrates
that the suggested method can acquire
enough NE tagged corpus equally useful
to the manually tagged one without any
human intervention.
1 Introduction
Current trend in Named Entity Recognition (NER) is
to apply machine learning approach, which is more
attractive because it is trainable and adaptable, and
subsequently the porting of a machine learning sys-
tem to another domain is much easier than that of a
rule-based one. Various supervised learning meth-
ods for Named Entity (NE) tasks were successfully
applied and have shown reasonably satisfiable per-
formance.((Zhou and Su, 2002)(Borthwick et al,
1998)(Sassano and Utsuro, 2000)) However, most
of these systems heavily rely on a tagged corpus for
training. For a machine learning approach, a large
corpus is required to circumvent the data sparseness
problem, but the dilemma is that the costs required
to annotate a large training corpus are non-trivial.
In this paper, we suggest a method that automati-
cally constructs an NE tagged corpus from the web
to be used for learning of NER systems. We use an
NE list and an web search engine to collect web doc-
uments which contain the NE instances. The doc-
uments are refined through the sentence separation
and text refinement procedures and NE instances are
finally annotated with the appropriate NE categories.
This automatically tagged corpus may have lower
quality than the manually tagged ones but its size
can be almost infinitely increased without any hu-
man efforts. To verify the usefulness of the con-
structed NE tagged corpus, we apply it to a learn-
ing of NER system and compare the results with the
manually tagged corpus.
2 Automatic Acquisition of an NE Tagged
Corpus
We only focus on the three major NE categories (i.e.,
person, organization and location) because others
are relatively easier to recognize and these three cat-
egories actually suffer from the shortage of an NE
tagged corpus.
Various linguistic information is already held in
common in written form on the web and its quantity
is recently increasing to an almost unlimited extent.
The web can be regarded as an infinite language re-
source which contains various NE instances with di-
verse contexts. It is the key idea that automatically
marks such NE instances with appropriate category
labels using pre-compiled NE lists. However, there
should be some general and language-specific con-
Web documents
W1W2W3
?
URL1URL2URL3
?
Web search engine
Web robot
Sentence separator
Textrefinement
S1S2S3
?
1.html2.html
?
1.ans2.ans
?
NE list Web page URL
Separated sentences
Refined sentences
NE taggeneration
S1(t)S2(t)S3(t)
?
NE taggedcorpus
Figure 1: Automatic generation of NE tagged corpus
from the web
siderations in this marking process because of the
word ambiguity and boundary ambiguity of NE in-
stances. To overcome these ambiguities, the auto-
matic generation process of NE tagged corpus con-
sists of four steps. The process first collects web
documents using a web search engine fed with the
NE entries and secondly segments them into sen-
tences. Next, each sentence is refined and filtered
out by several heuristics. An NE instance in each
sentence is finally tagged with an appropriate NE
category label. Figure 1 explains the entire proce-
dure to automatically generate NE tagged corpus.
2.1 Collecting Web Documents
It is not appropriate for our purpose to randomly col-
lect documents from the web. This is because not all
web documents actually contain some NE instances
and we also do not have the list of all NE instances
occurring in the web documents. We need to col-
lect the web documents which necessarily contain
at least one NE instance and also should know its
category to automatically annotate it. This can be
accomplished by using a web search engine queried
with pre-compiled NE list.
As queries to a search engine, we used the list
of Korean Named Entities composed of 937 per-
son names, 1,000 locations and 1,050 organizations.
Using a Part-of-Speech dictionary, we removed am-
biguous entries which are not proper nouns in other
contexts to reduce errors of automatic annotation.
For example, ?E?(kyunggi, Kyunggi/business con-
ditions/a game)? is filtered out because it means a lo-
cation (proper noun) in one context, but also means
business conditions or a game (common noun) in
other contexts. By submitting the NE entries as
queries to a search engine1, we obtained the max-
imum 500 of URL?s for each entry. Then, a web
robot visits the web sites in the URL list and fetches
the corresponding web documents.
2.2 Splitting into Sentences
Features used in the most NER systems can be clas-
sified into two groups according to the distance from
a target NE instance. The one includes internal fea-
tures of NE itself and context features within a small
word window or sentence boundary and the other in-
cludes name alias and co-reference information be-
yond a sentence boundary. In fact, it is not easy to
extract name alias and co-reference information di-
rectly from manually tagged NE corpus and needs
additional knowledge or resources. This leads us to
focus on automatic annotation in sentence level, not
document level. Therefore, in this step, we split the
texts of the collected documents into sentences by
(Shim et al, 2002) and remove sentences without
target NE instances.
2.3 Refining the Web Texts
The collected web documents may include texts ac-
tually matched by mistake, because most web search
engines for Korean use n-gram, especially, bi-gram
matching. This leads us to refine the sentences to ex-
clude these erroneous matches. Sentence refinement
is accomplished by three different processes: sep-
aration of functional words, segmentation of com-
pound nouns, and verification of the usefulness of
the extracted sentences.
An NE is often concatenated with more than one
josa, a Korean functional word, to compose a
Korean word. Therefore we need to separate the
functional words from an NE instance to detect the
boundary of the NE instance and this is achieved
by a part-of-speech tagger, POSTAG, which can
detect unknown words (Lee et al, 2002). The
separation of functional words gives us another
benefit that we can resolve the ambiguities between
an NE and a common noun plus functional words
1We used Empas (http://www.empas.com)
Person Location Organization
Training Automatic 29,042 37,480 2,271Manual 1,014 724 1,338
Test Manual 102 72 193
Table 1: Corpus description (number of NE?s) (Au-
tomatic: Automatically annotated corpus, Manual:
Manually annotated corpus
and filter out erroneous matches. For example,
?E??(kyunggi-do)? can be interpreted as
either ?E??(Kyunggi Province)? or ?E?+?(a
game also)? according to its context. We can remove
the sentence containing the latter case.
A josa-separated Korean word can be a com-
pound noun which only contains a target NE as a
substring. This requires us to segment the compound
noun into several correct single nouns to match with
the target NE. If the segmented single nouns are not
matched with a target NE, the sentence can be fil-
tered out. For example, we try to search for an NE
entry, ???(Fin.KL, a Korean singer group)? and
may actually retrieve sentences including ????
?(surfing club)?. The compound noun, ??????,
can be divided into ???(surfing)? and ???(club)?
by a compound-noun segmenting method (Yun et
al., 1997). Since both ???? and ???? are not
matched with our target NE, ????, we can delete
the sentences. Although a sentence has a correct tar-
get NE, if it does not have context information, it is
not useful as an NE tagged corpus. We also removed
such sentences.
2.4 Generating an NE tagged corpus
The sentences selected by the refining process ex-
plained in previous section are finally annotated with
the NE label. We acquired the NE tagged corpus in-
cluding 68,793 NE instances through this automatic
annotation process. We can annotate only one NE
instance per sentence but almost infinitely increase
the size of the corpus because the web provides un-
limited data and our process is fully automatic.
3 Experimental Results
3.1 Usefulness of the Automatically Tagged
Corpus
For effectiveness of the learning, both the size and
the accuracy of the training corpus are important.
Training corpus Precision Recall F-measure
Seeds only 84.13 42.91 63.52
Manual 80.21 86.11 83.16
Automatic 81.45 85.41 83.43
Manual + Automatic 82.03 85.94 83.99
Table 2: Performance of the decision list learning
Generally, the accuracy of automatically created NE
tagged corpus is worse than that of hand-made cor-
pus. Therefore, it is important to examine the useful-
ness of our automatically tagged corpus compared
to the manual corpus. We separately trained the de-
cision list learning features using the automatically
annotated corpus and hand-made one, and compared
the performances. Table 1 shows the details of the
corpus used in our experiments.2
Through the results in Table 2, we can verify that
the performance with the automatic corpus is supe-
rior to that with only the seeds and comparable to
that with the manual corpus.Moreover, the domain
of the manual training corpus is same with that of
the test corpus, i.e., news and novels, while the do-
main of the automatic corpus is unlimited as in the
web. This indicates that the performance with the
automatic corpus should be regarded as much higher
than that with the manual corpus because the per-
formance generally gets worse when we apply the
learned system to different domains from the trained
ones. Also, the automatic corpus is pretty much self-
contained since the performance does not gain much
though we use both the manual corpus and the auto-
matic corpus for training.
3.2 Size of the Automatically Tagged Corpus
As another experiment, we tried to investigate how
large automatic corpus we should generate to get the
satisfiable performance. We measured the perfor-
mance according to the size of the automatic cor-
pus. We carried out the experiment with the deci-
sion list learning method and the result is shown in
Table 3. Here, 5% actually corresponds to the size of
the manual corpus. When we trained with that size
of the automatic corpus, the performance was very
low compared to the performance of the manual cor-
pus. The reason is that the automatic corpus is com-
2We used the manual corpus used in Seon et al (2001) as
training and test data.
Corpus size (words) Precision Recall F-measure
90,000 (5%) 72.43 6.94 39.69
448,000 (25%) 73.17 41.66 57.42
902,000 (50%) 75.32 61.53 68.43
1,370,000 (75%) 78.23 77.19 77.71
1,800,000 (100%) 81.45 85.41 83.43
Table 3: Performance according to the corpus size
Corpus size (words) Precision Recall F-measure
700,000 79.41 81.82 80.62
1,000,000 82.86 85.29 84.08
1,200,000 83.81 86.27 85.04
1,300,000 83.81 86.27 85.04
Table 4: Saturation point of the performance for
?person? category
posed of the sentences searched with fewer named
entities and therefore has less lexical and contextual
information than the same size of the manual cor-
pus. However, the automatic generation has a big
merit that the size of the corpus can be increased al-
most infinitely without much cost. From Table 3,
we can see that the performance is improved as the
size of the automatic corpus gets increased. As a
result, the NER system trained with the whole au-
tomatic corpus outperforms the NER system trained
with the manual corpus.
We also conducted an experiment to examine the
saturation point of the performance according to the
size of the automatic corpus. This experiment was
focused on only ?person? category and the result is
shown in Table 4. In the case of ?person? category,
we can see that the performance does not increase
any more when the corpus size exceeds 1.2 million
words.
4 Conclusions
In this paper, we presented a method that automat-
ically generates an NE tagged corpus using enor-
mous web documents. We use an internet search en-
gine with an NE list to collect web documents which
may contain the NE instances. The web documents
are segmented into sentences and refined through
sentence separation and text refinement procedures.
The sentences are finally tagged with the NE cat-
egories. We experimentally demonstrated that the
suggested method could acquire enough NE tagged
corpus equally useful to the manual corpus without
any human intervention. In the future, we plan to ap-
ply more sophisticated natural language processing
schemes for automatic generation of more accurate
NE tagged corpus.
Acknowledgements
This research was supported by BK21 program of
Korea Ministry of Education and MOCIE strategic
mid-term funding through ITEP.
References
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting Diverse
Knowledge Sources via Maximum Entropy in Named
Entity Recognition. In Proceedings of the Sixth Work-
shop on Very Large Corpora, pages 152?160, New
Brunswick, New Jersey. Association for Computa-
tional Linguistics.
Gary Geunbae Lee, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable Pattern-based Unknown Mor-
pheme Segmentation and Estimation for Hybrid Part-
Of-Speech Tagging of Korean. Computational Lin-
guistics, 28(1):53?70.
Manabu Sassano and Takehito Utsuro. 2000. Named
Entity Chunking Techniques in Supervised Learning
for Japanese Named Entity Recognition. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 705?711,
Germany.
Choong-Nyoung Seon, Youngjoong Ko, Jeong-Seok
Kim, and Jungyun Seo. 2001. Named Entity Recog-
nition using Machine Learning Methods and Pattern-
Selection Rules. In Proceedings of the Sixth Natural
Language Processing Pacific Rim Symposium, pages
229?236, Tokyo, Japan.
Junhyeok Shim, Dongseok Kim, Jeongwon Cha,
Gary Geunbae Lee, and Jungyun Seo. 2002. Multi-
strategic Integrated Web Document Pre-processing for
Sentence and Word Boundary Detection. Information
Processing and Management, 38(4):509?527.
Bo-Hyun Yun, Min-Jeung Cho, and Hae-Chang Rim.
1997. Segmenting Korean Compound Nouns using
Statistical Information and a Preference Rule. Jour-
nal of Korean Information Science Society, 24(8):900?
909.
GuoDong Zhou and Jian Su. 2002. Named Entity
Recognition using an HMM-based Chunk Tagger. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
473?480, Philadelphia, USA.
