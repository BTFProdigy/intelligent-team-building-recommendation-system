Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts,
pages 1?2, Dublin, Ireland, August 23-29 2014.
Biomedical/clinical NLP
Ozlem Uzuner
Information Studies
University of Albany, SUNY
Albany, NY
ouzuner@albany.edu
Meliha Yetis?gen
Biomedical and
Health Informatics
University of Washington
Seattle, WA
melihay@uw.edu
Amber Stubbs
Library and
Information Science
Simmons College
Boston, MA
stubbs@simmons.edu
Introduction
Recent years have seen a rapid growth in the use of biomedical documents and narrative clinical records
for applications outside of direct patient care. Accordingly, recent years have also seen an increase in
the development of NLP technologies for concept and relation extraction, summarization, and question
answering on these data.
This tutorial will present an overview of the biomedical and clinical NLP data, tools, and methods
with the intent of providing the researchers with a jump-start into these domains. We will focus on the
demand for NLP in biomedical and clinical domains, the potential for impact, and the required NLP
tasks. We will introduce this information in the following categories:
Overview of biomedical/clinical NLP
Biomedical narratives are often dense with domain-specific jargon. Clinical narratives, in addition to be-
ing dense with domain-specific jargon, exhibit the complexities of a specialized sub-language. They are
written by the domain experts and for the domain experts. Their primary purpose is to assist in inform-
ing future decisions about the care of the patients. As a result, both biomedical and clinical narratives
present challenges for existing open-domain NLP technologies and require special considerations for
their accurate understanding and interpretation.
In this section, we will discuss the data sources currently available to researchers, as well as provide
an overview of the research questions both domains. On the clinical side, this includes using EHRs
for phenotyping and decision support systems. The biomedical side uses NLP to explore fields such as
literature-based discovery and literature searches.
Current research questions in biomedical and clinical NLP
NLP applications are generally built to answer specific questions about data. In this section, we will pro-
vide examples of the types of questions researchers are asking in the clinical and biomedical domains.
Additionally, we will discuss how different linguistic aspects of these data are addressed by looking at ex-
isting syntactic (part of speech tagging, parsing) and semantic (concept extraction, temporal information
extraction, coreference resolution) systems.
Datasets and the annotation process
Building annotated corpora for any task can be challenging, but the biomedical and clinical domains have
additional barriers that make creating these corpora difficult. In this section, we will discuss available
annotated resources in both domains, and discuss challenges in biomedical and clinical corpus building,
such as restrictions on data access and the need for domain experts to be part of the annotation process.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Clinical Annotation Case Study
The 2014 i2b2 NLP Shared Task
1
involved two NLP challenges: 1) de-identification of medical records,
and 2) identification of risk factors for coronary artery disease in diabetic patients. Each of these tracks
required a separate annotation effort, and in this portion of the tutorial we will describe the end-to-end
process of creating this annotated resource, from data selection to writing the annotation guidelines to
creating the final gold standards.
NLP Methods
Just as there are many research questions in the biomedical and clinical domains, there are many existing
NLP systems that address these questions. In this portion of the tutorial, we will describe the three main
approaches (rule-based, statistical, and hybrid) commonly used to process biomedical and clinical text.
Additionally, we will present on-going research projects from our research labs including (1) extracting
structure and semantics from clinical text through section segmentation and assertion analysis and (2)
clinical applications such as phenotype modeling and specific examples of information extraction in the
radiology domain.
Open questions and future directions
Research in the fields of biomedical and clinical NLP is far from complete; the end of this tutorial will
look at current unsolved problems in these fields, as well as look ahead towards potential future research
questions.
Acknowledgements
This tutorial was supported in part by Informatics for Integrating Biology and the Bedside (i2b2) award
No. 2U54LM008748 from the National Institutes of Health (NIH)/National Q5 Library of Medicine
(NLM), by award No. 1R13LM01141101 from the NIH NLM, by award No. 1R21EB016872 from NIH
NIBIB, and by Institute of Translational Health Sciences award No. UL1TR000423 from NIH NCATS.
1
https://www.i2b2.org/NLP/HeartDisease/
2
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 71?79,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Annotating Large Email Datasets for Named Entity Recognition with 
Mechanical Turk
Nolan Lawson, Kevin Eustice, 
Mike Perkowitz
Meliha Yetisgen-Yildiz
Kiha Software Biomedical and Health Informatics
100 South King Street, Suite 320 University of Washington
Seattle, WA 98104 Seattle, WA 98101
{nolan,kevin,mikep}@kiha.com melihay@u.washington.edu
Abstract
Amazon's  Mechanical  Turk service  has  been 
successfully applied to many natural language 
processing tasks. However, the task of named 
entity recognition presents unique challenges. 
In  a  large  annotation  task  involving  over 
20,000 emails, we demonstrate that a compet?
itive bonus system and inter?annotator agree?
ment  can  be  used  to  improve the  quality  of 
named  entity  annotations  from  Mechanical 
Turk.  We also build several statistical named 
entity  recognition  models  trained  with  these 
annotations, which compare favorably to sim?
ilar models trained on expert annotations.
1    Introduction
It  is well known that the performance of many 
machine learning systems is heavily determined 
by the size and quality of the data used as input 
to the training algorithms.  Additionally, for cer?
tain applications in natural language processing 
(NLP),  it  has been noted that  the particular  al?
gorithms or feature sets used tend to become ir?
relevant  as  the  size  of  the  corpus  increases 
(Banko and Brill 2001).  It is therefore not sur?
prising that obtaining large annotated datasets is 
an  issue  of  great  practical  importance  for  the 
working  researcher.  Traditionally,  annotated 
training data have been provided by experts  in 
the field or the researchers themselves, often at 
great  costs  in  terms  of  time  and  money.   Re?
cently,  however,  attempts  have  been  made  to 
leverage  non?expert  annotations  provided  by 
Amazon's Mechanical  Turk (MTurk) service to 
create large training corpora at a fraction of the 
usual costs (Snow et al 2008).  The initial results 
seem promising, and a new avenue for enhancing 
existing  sources  of  annotated  data  appears  to 
have been opened.
Named entity recognition (NER) is one of the 
many fields of NLP that rely on machine learn?
ing  methods,  and  therefore  large  training  cor?
pora.  Indeed, it is a field where more is almost 
always better, as indicated by the traditional use 
of named entity gazetteers (often culled from ex?
ternal sources) to simulate data that would have 
been inferred from a larger training set (Minkov 
et al 2005; Mikheev et al 1999).  Therefore, it 
appears to be a field that could profit from the 
enormous  bargain?price  workforce  available 
through MTurk.
It  is  not  immediately  obvious,  though, that 
MTurk is well?suited for the task of NER annota?
tion.  Commonly,  MTurk has been used for the 
classification  task  (Snow et  al. 2008) or  for 
straightforward data entry.  However, NER does 
not fit well into either of these formats.  As poin?
ted out by Kozareva (2006), NER can be thought 
of as a composition of two subtasks: 1) determin?
ing the start and end boundaries of a textual en?
tity, and 2) determining the label of the identified 
span.   The  second  task  is  the  well?understood 
classification  task,  but  the  first  task  presents 
subtler  problems.   One  is  that  MTurk's  form?
based user interface is inappropriate for the task 
of identifying textual spans.  Another problem is 
that  MTurk's fixed?fee payment system encour?
ages  low  recall  on  the  part  of  the  annotators, 
since they receive the same pay no matter how 
many entities they identify.
This  paper addresses  both of these problems 
by  describing  a custom  user interface and com?
petitive  payment  system  that  together  create  a 
fluid user  experience  while  encouraging  high?
quality  annotations.   Further,  we demonstrate 
that MTurk successfully scales to the task of an?
notating  a  very  large  set  of  documents  (over 
20,000), with  each document annotated by mul?
tiple  workers.   We  also present  a  system  for 
resolving  inter?annotator  conflicts  to  create  the 
final  training  corpus,  and  determine  the  ideal 
agreement threshold to maximize precision and 
recall  of  a  statistical  named  entity  recognition 
model.  Finally,  we  demonstrate  that  a  model 
71
trained on our  corpus is on par with one trained 
from  expert  annotations,  when  applied  to  a 
labeled test set.
2    Related Work
Mechanical Turk is a virtual market in which any 
requester can post tasks that are simple for hu?
mans  but  difficult  for  computers.  MTurk has 
been adopted  for  a  variety of  uses  both in  in?
dustry and academia from user studies (Kittur et  
al. 2008) to image labeling (Sorokin and Forsyth 
2008). In March 2007, Amazon claimed the user 
base of  MTurk consisted of over 100,000 users 
from 100 countries (Pontin 2007).
In  the  scope  of  this  paper,  we  examine  the 
feasibility of  MTurk in creating large?scale cor?
pora for training statistical named entity recogni?
tion models. However, our work was not the first 
application of MTurk in the NLP domain. Snow 
et al (2008) examined the quality of labels cre?
ated by MTurk workers for various NLP tasks in?
cluding word sense disambiguation, word simil?
arity,  text  entailment,  and  temporal  ordering. 
Since  the  publication  of  Snow  et  al.?s  paper, 
MTurk has  become increasingly  popular  as  an 
annotation tool for NLP research. Examples in?
clude Nakov?s work on creating a manually an?
notated resource for noun?noun compound inter?
pretation based on paraphrasing verbs by MTurk 
(Nakov  2008)  and  Callison?Burch?s  machine 
translation evaluation study with MTurk (Callis?
on?Burch 2009).  In  contrast  to  the  existing  re?
search, we both evaluated the quality of corpora 
generated by MTurk in different named entity re?
cognition  tasks  and explored  ways  to  motivate 
the workers to do higher quality work. We be?
lieve  the  experiences  we  present  in  this  paper 
will  contribute  greatly  to  other  researchers  as 
they design similar large?scale annotation tasks. 
3    General Problem Definition 
Named entity recognition (NER) is a well?known 
subtask of information extraction.  Traditionally, 
the task has been based on identifying words and 
phrases that refer to various entities of interest, 
including persons,  locations,  and  organizations, 
(Nadeau and Sekine 2007).  The problem is usu?
ally posed as a sequence labeling task similar to 
the  part?of?speech  (POS) tagging  or  phrase?
chunking tasks,  where  each  token in  the  input 
text corresponds to a label in the output, and  is 
solved with  sequential  classification algorithms 
(such as CRF, SVMCMM, or MEMM).
Previous works have tackled NER within the 
biomedical domain (Settles 2004), newswire do?
main (Grishman and Sundheim 1996), and email 
domain (Minkov et al 2005).   In this paper, we 
focus on extracting entities from email text.
It  should  be noted that  email  text  has  many 
distinctive features that create a unique challenge 
when applying NER.  For one, email text tends to 
be more informal  than either  newswire  or  bio?
medical  text,  which  reduces  the  usefulness  of 
learned features that depend on patterns of capit?
alization and spelling.  Also,  the choice of cor?
pora in email text is  particularly important.  As 
email corpora tend to come from either a single 
company (e.g., the  Enron Email  Dataset1)  or  a 
small group of people (e.g., the Sarah Palin email 
set2), it is easy to build a classifier that overfits 
the data.  For instance, a classifier trained to ex?
tract  personal  names  from Enron emails  might 
show an especially high preference to words such 
as ?White,? ?Lay,? and ?Germany,? because they 
correspond to the names of Enron employees.
Within the newswire and biomedical domains, 
such overfitting may be benign or actually bene?
ficial, since documents in those domains tend to 
deal with a relatively small and pre?determined 
set of named entities (e.g., politicians and large 
corporations for newswire text, gene and protein 
names  for  biomedical  text).   For  NER  in  the 
email domain, however, such overfitting is unac?
ceptable.  The personal nature of emails ensures 
that they will almost always contain references to 
people, places, and organizations not covered by 
the training data.  Therefore, for the classifier to 
be useful on any spontaneous piece of email text, 
a large, heterogeneous training set is desired.
To achieve this effect, we chose four different 
sources of unlabeled email text to be annotated 
by the Mechanical  Turk workers for input  into 
the training algorithms:
1. The Enron Email Dataset.
2. The  2005  TREC   Public  Spam Corpus 
(non?spam only).3
3. The 20 Newsgroups Dataset.4
4. A private mailing list for synthesizer afi?
cionados called ?Analogue Heaven.?
4   Mechanical Turk for NER
As described previously, MTurk is not explicitly 
designed for NER tasks.  Because of this, we de?
1 http://www.cs.cmu.edu/~enron/
2 http://palinemail.crivellawest.net/
3 http://plg.uwaterloo.ca/~gvcormac/treccorpus/
4 http://people.csail.mit.edu/jrennie/20Newsgroups/
72
cided to build a custom user interface and bonus 
payment system that largely circumvents the de?
fault  MTurk web interface and instead performs 
its operations through the MTurk Command Line 
Tools.5  Additionally, we built a separate set of 
tools designed to determine the ideal number of 
workers to assign per email.
4.1    User Interface 
In order to adapt the task of NER annotation to 
the  Mechanical  Turk  format,  we  developed  a 
web?based graphical user interface using JavaS?
cript that allowed the user to select a span of text 
with the mouse cursor and choose different cat?
egories of entities from a dropdown menu.  The 
interface also used simple tokenization heuristics 
to divide the text into highlightable spans and re?
solve  partial  overlaps  or  double?clicks  into  the 
next largest span.  For instance, highlighting the 
word ?Mary? from ?M? to ?r? would result in the 
entire word being selected.
Each Human Intelligence Task (or HIT, a unit 
of payable work in the Mechanical Turk system) 
presented the entire subject and body of an email 
from one of the four corpora.  To keep the HITs 
at a reasonable size, emails with bodies having 
less than 60 characters or more than 900 charac?
ters were omitted.  The average email length, in?
cluding both subject and body, was 405.39 char?
acters.
For the labeling task, we chose three distinct 
entity  types  to  identify:  PERSON,  LOCATION,  and 
ORGANIZATION.  To reduce potential worker confu?
sion  and  make  the  task  size  smaller,  we  also 
broke up each individual HIT by entity type, so 
the user only had to concentrate on one at a time.
For the  PERSON and  LOCATION entity types, we 
noticed during initial tests that there was a user 
5 http://mturkclt.sourceforge.net
tendency to  conflate  unnamed references  (such 
as  ?my  mom?  and  ?your  house?)  with  true 
named references.  Because NER is intended to 
be limited only to named entities (i.e., references 
that contain proper nouns), we asked the users to 
distinguish  between  ?named?  and  ?unnamed? 
persons and locations, and to tag both separately. 
The inclusion of unnamed entities was intended 
to keep their named counterparts pure and undi?
luted; the unnamed entities were discarded after 
the annotation process was complete.  The same 
mechanism  could  have  been  used  for  the 
ORGANIZATION entity type, but the risk of unnamed 
references seemed smaller.
Initially, we ran a small trial with a base rate 
of $0.03 for each HIT.  However, after compiling 
the results  we noticed that  there was a general 
tendency for the workers to under?tag the entit?
ies.   Besides outright freeloaders (i.e., workers 
who  simply  clicked  ?no  entities?  each  time), 
there were also many who would highlight  the 
first one or two entities, and then ignore the rest 
of the email.
This may have been due to a misunderstanding 
of the HIT instructions, but we conjectured that a 
deeper reason was that  we were paying a base 
rate regardless of the number of entities identi?
fied.  Ideally, a HIT with many entities to high?
light  should pay more than  a  HIT with fewer. 
However, the default fixed?rate system was pay?
ing the same for both, and the workers were re?
sponding to such an inflexible incentive system 
accordingly.   To remedy this  situation,  we  set 
about to create a payment system that would mo?
tivate higher recall  on entity?rich emails,  while 
still  discouraging  the  opposite  extreme  of  ran?
dom over?tagging.
Fig. 1: Sample of the interface presented to workers.
73
4.2    Bonus Payment System 
Mechanical Turk provides two methods for pay?
ing  workers:  fixed  rates  on  each  HIT  and  bo?
nuses to individual workers for especially good 
work.   We chose to  leverage these  bonuses  to 
form the core of our payment system.  Each HIT 
would pay a base rate of $0.01, but each tagged 
entity   could  elicit  a  bonus  of  $0.01?$0.02. 
PERSON entities  paid  $0.01,  while  LOCATION and 
ORGANIZATION entities paid $0.02 (since they were 
rarer).
To ensure quality and discourage over?tagging, 
bonuses for each highlighted entity were limited 
based  on  an  agreement  threshold  with  other 
workers.  This threshold was usually set such that 
a majority agreement was required, which was an 
arbitrary  decision we made  in  order  to  control 
costs.  The terms of the bonuses were explained 
in detail in the instructions page for each HIT.
Additionally, we decided to leverage this bo?
nus system to encourage improvements in work?
er performance over time.  Since the agreed?upon 
spans that elicited bonuses were assumed to be 
mostly correct, we realized we could give feed?
back to the workers on these entities to encour?
age similar performance in the future.
In general,  worker  bonuses  are  a  mysterious 
and poorly understood motivational mechanism. 
Our  feedback  system  attempted  to  make  these 
bonuses more predictable and transparent.  The 
system we built uses Amazon's ?NotifyWorkers? 
REST  API  to  send  messages  directly  to  the 
workers' email accounts.  Bonuses were batched 
on a daily basis, and the notification emails gave 
a summary description of the day's bonuses. 
Both the UI and the bonus/notification system 
were works in progress that were continually re?
fined based on comments from the worker com?
munity.   We  were  pleasantly  surprised  to  find 
that,  throughout  the  annotation  process,  the 
Mechanical Turk workers were generally enthu?
siastic about the HITs, and also interested in im?
proving the quality of their annotations.  Out of 
169,156 total HITs, we received 702 comments 
from 140 different workers, as well as over 50 
email responses and a dedicated thread at Turk?
erNation.com6.  Most of the feedback was posit?
ive, and negative feedback was almost solely dir?
ected at the UI.  Based on their comments, we 
continually  tweaked  and  debugged  the  UI  and 
HIT instructions, but kept the basic structure of 
the bonus system.
4.3    Worker Distribution 
With the bonus system in place, it was still ne?
cessary to determine the ideal number of workers 
to  assign  per  email.   Previously,  Snow  et al 
(2008) used expert annotations to find how many 
Mechanical  Turk workers could ?equal? an ex?
pert in terms of annotation quality.   Because we 
lacked expert  annotations,  we developed an al?
ternative system to determine the ideal number of 
workers  based  purely  on  inter?annotator  agree?
ment.
As described in the previous section, the most 
significant problem faced with our HITs was that 
of low recall.  Low precision was generally not 
considered to be a problem, since, with enough 
annotators,  inter?annotator  agreement  could  al?
ways be set arbitrarily high in order to weed out 
false positives.  Recall, on the other hand, could 
be consistently expected to improve as more an?
notators were added to the worker pool.  There?
fore, the only problem that remained was to cal?
culate the marginal utility (in terms of recall) of 
each additional annotator assigned to an email.
In order to estimate this marginal recall  gain 
for each entity type, we first ran small initial tests 
with a relatively large number of workers.  From 
these results, we took all the entities identified by 
at least two workers and set  those aside as the 
gold standard annotations;  any  overlapping an?
notations  were  collapsed  into  the  larger  one. 
Next, for each  n number of workers between 2 
and the size of the entire worker pool, we ran?
domly sampled n workers from the pool, re?cal?
culated the entities based on agreement from at 
least two workers within that group, and calcu?
lated the recall relative to the gold standard an?
notation.  The threshold of 2 was chosen arbitrar?
ily for the purpose of this experiment.
From this data we generated a marginal recall 
curve  for  each  entity  type,  which  roughly  ap?
proximates how many workers are required per 
email  before  recall  starts  to  drop  off  signific?
antly.   As expected, each graph shows a plateau?
like behavior as the number of workers increases, 
but some entity types reach their plateau earlier 
6 http://turkers.proboards.com/index.cgi?action 
=display&board=everyoneelse&thread=3177
In?recognition?of?your?performance,?you?
were?awarded?a?bonus?of?$0.5?($0.02x25)?
for?catching?the?following?span(s):?['ve?
gas',?'Mt.?Hood',?'Holland',?[...]
Fig. 2: Example bonus notification.
74
than others.  Most saliently, Person entities seem 
to require only a few workers to reach a relat?
ively  high  recall,  compared  to  LOCATION or 
ORGANIZATION entities.
Based on the expected diminishing returns for 
each entity type, we determined some number of 
workers to assign per email that we felt  would 
maximize entity recall while staying within our 
budgetary limits.  After some tinkering and ex?
perimentation with marginal recall curves, we ul?
timately settled on 4 assignments for PERSON en?
tities,  6  for  LOCATION entities,  and  7  for 
ORGANIZATION entities.
5    Corpora and Experiments
We ran our Mechanical Turk tasks over a period 
of  about  three  months,  from  August  2008  to 
November  2008.   We typically  processed 500?
1,500 documents per day.  In the end, the work?
ers annotated 20,609 unique emails which totaled 
7.9 megabytes, including both subject and body.
All in all, we were pleasantly surprised by the 
speed at which each HIT series was completed. 
Out of 39 total HIT series, the average comple?
tion time (i.e. from when the HITs were first pos?
ted to MTurk.com until  the last HIT was com?
pleted)  was  3.13  hours,  with  an  average  of 
715.34 emails per HIT series.  The fastest com?
pletion time per number of emails was 1.9 hours 
for a 1,000?email task, and the slowest was 5.13 
hours  for  a  100?email  task.   We noticed,  that, 
paradoxically, larger HIT series were often com?
pleted  more  quickly  ?  most  likely  because 
Amazon promotes  the  larger  tasks  to  the  front 
page.
5.1    Corpora Annotation
In Table 1, we present several statistics regard?
ing the annotation tasks, grouped by corpus and 
entity type.  Here, ?Cost? is the sum of all bo?
nuses and base rates for the HITs, ?Avg.  Cost? 
is  the  average amount  we paid in bonuses and 
base rates per email, ?Avg. # Workers? is the av?
erage  number  of  workers  assigned  per  email, 
?Avg.  Bonus?  is  the  average  bonus  per  HIT, 
?Avg. # Spans? is the average number of entities 
highlighted per HIT, and ?Avg. Time? is the av?
erage  time  of  completion  per  HIT  in  seconds. 
Precision and recall  are reported relative to the 
?gold standards? determined by the bonus agree?
ment thresholds.  None of the reported costs in?
clude fees paid to Amazon, which varied based 
on how the bonuses were batched.
A  few  interesting  observations  emerge  from 
these data.  For one, the average bonus was usu?
ally a bit more than the base rate of $0.01.   The 
implication  is  that  bonuses  actually  comprised 
the majority of the compensation, somewhat call?
ing into question their role as a ?bonus.?
Also noteworthy is  that  ORGANIZATION entities 
took  less  time  per  identified  span  to  complete 
than either location or person entities.  However, 
we suspect that this is due to the fact that we ran 
the  ORGANIZATION tasks  last  (after  PERSON and 
LOCATION),  and by that  time we had ironed out 
several bugs in the UI, and our workers had be?
come more adept at using it.
5.2    Worker Performance
In the end, we had 798 unique workers complete 
169,156  total  HITs.   The  average  number  of 
HITs per worker was 211.97, but the median was 
only  30.   Ten workers  who  tagged no  entities 
were  blocked,  and  the  1,029  HITs  they  com?
pleted were rejected without payment.
For the most part, a small number of dedicated 
workers completed the majority of the tasks.  Out 
of all non?rejected HITs, the top 10 most prolific 
workers  completed  22.51%,  the  top  25  com?
pleted  38.59%,  the  top  50  completed  55.39%, 
and the top 100 completed 74.93%.
Callison?Burch  (2009)  found  in  their  own 
Mechanical  Turk  system that  the  workers  who 
contributed more tended to show lower quality, 
2 3 4
0
0.2
0.4
0.6
0.8
1
2 3 4 5 6 7 8
0
0.2
0.4
0.6
0.8
1
2 3 4 5 6 7 8 9 10
0
0.2
0.4
0.6
0.8
1
Fig. 3: Marginal recall curves for PERSON, LOCATION, and ORGANIZATION entity types, from a trial run of 
900?1,000 emails.  Recall is plotted on the y?axis, the number of annotators on the x?axis.
75
as measured by agreement with an expert.  We 
had hoped that our bonus system, by rewarding 
quality  work  with  higher  pay,  would  yield  the 
opposite effect, and in practice, our most prolific 
workers did indeed tend to show the highest en?
tity recall.
0 1000 2000 3000 4000 5000 6000 7000 8000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 4: # HITs Completed vs. Recall
Figure 4 shows how each of the non?rejected 
workers fared in terms of entity recall (relative to 
the  ?gold  standard?  determined  by  the  bonus 
agreement threshold), compared to the number of 
HITs completed.  As the chart shows, out of the 
10 most productive workers, only one had an av?
erage recall score below 60%, and the rest all had 
scores above 80%.  While there are still quite a 
few  underperforming  workers  within  the  core 
group of high?throughput annotators, the general 
trend seemed to be that the more HITs a worker 
completes, the more likely he/she is to agree with 
the other annotators.  This chart may be directly 
compared  to  a  similar  one  in  Callison?Burch 
(2009), where the curve takes largely the oppos?
ite shape.  One interpretation of this is that our 
bonus system had the desired effect on annotator 
quality.
5.3    Annotation Quality Experiments
To  evaluate  the  quality  of  the  worker  annota?
tions, one would ideally like to have at least  a 
subset annotated by an expert, and then compare 
the expert's judgments with the Mechanical Turk 
workers'.  However, in our case we lacked expert 
annotations  for  any  of  the  annotated  emails. 
Thus, we devised an alternative method to evalu?
ate the annotation quality, using the NER system 
built into the open?source MinorThird toolkit.7
MinorThird is a popular machine learning and 
natural language processing library that has pre?
viously been applied to the problem of NER with 
some success (Downey et al 2007).  For our pur?
poses, we wanted to minimize the irregularity in?
troduced by deviating from the core features and 
algorithms  available  in  MinorThird,  and  there?
fore did not apply any feature selection or feature 
engineering in our experiments.  We chose to use 
MinorThird's default  ?CRFLearner,? which is a 
module that learns feature weights using the IITB 
CRF library8 and then applies them to a condi?
tional Markov model?based extractor.  All of the 
parameters were set to their default value, includ?
ing  the  built?in  ?TokenFE?  feature  extractor, 
which extracts features for the lowercase value of 
each  token  and  its  capitalization  pattern.   The 
version of MinorThird used was 13.7.10.8.
In order to convert  the  Mechanical  Turk an?
notations to a format that could be input as train?
ing data to the NER system, we had to resolve 
the conflicting annotations of the multiple work?
ers into a unified set of labeled documents.  Sim?
ilarly to the bonus system, we achieved this using 
a simple voting scheme.  In contrast to the bonus 
system, though, we experimented with multiple 
inter?annotator  agreement  thresholds  between 1 
and 4.  For the PERSON corpora this meant a relat?
ively stricter threshold than for the  LOCATION or 
7 http://minorthird.sourceforge.net
8 http://crf.sourceforge.net
Table 1: Statistics by corpus and entity type (omitting rejected work).
Corpus Entity Cost #Emails Avg. Cost Avg. #Workers Avg. Bonus Avg. #Spans Avg. Precision Avg. Recall Avg. Time
20N. 315.68 1999 0.1579 6 0.0163 1.6885 0.5036 0.7993 144.34
A.H. 412.2 2500 0.1649 6.4 0.0158 1.1924 0.6881 0.8092 105.34
323.54 3000 0.1078 6.23 0.0073 1.0832 0.3813 0.7889 105.25
TREC 274.88 2500 0.1100 6 0.0083 1.1847 0.3794 0.7864 122.97
20N. 438.44 3500 0.1253 7 0.0079 1.2396 0.3274 0.6277 105.68
A.H. 396.48 2500 0.1586 7 0.0127 1.2769 0.4997 0.7062 92.01
539.19 2500 0.2157 8.6 0.0151 1.3454 0.5590 0.7415 80.55
TREC 179.94 1500 0.1200 7 0.0071 0.8923 0.4414 0.6992 84.23
20N. Per. 282.51 2500 0.1130 4 0.0183 2.8693 0.7267 0.9297 152.77
A.H. Per. 208.78 2500 0.0835 4 0.0109 1.6529 0.7459 0.9308 112.4
Per. 54.11 400 0.1353 6.14 0.0120 2.7360 0.8343 0.8841 111.23
TREC Per. 214.37 2500 0.0857 4 0.0114 1.5918 0.7950 0.9406 103.73
Loc.
Loc.
Enron Loc.
Loc.
Org.
Org.
Enron Org.
Org.
Enron
76
ORGANIZATION corpora,  since the  PERSON corpora 
typically had only 4 annotations per document. 
Mail subjects and bodies were split into separate 
documents.
Four separate experiments were run with these 
corpora.  The first was a 5?fold cross?evaluation 
(i.e.,  a  80%/20% split)  train/test  experiment on 
each of the twelve corpora.  Because this test did 
not  rely on  any expert  annotations  in  the  gold 
standard,  our  goal  here  was  only  to  roughly 
measure the ?cohesiveness? of the corpus.  Low 
precision  and  recall  scores  should  indicate  a 
messy corpus, where annotations in the training 
portion do not  necessarily help the extractor to 
discover  annotations  in  the  test  portion.   Con?
versely, high precision and recall  scores should 
indicate a more cohesive corpus ? one that is at 
least  somewhat  internally  consistent  across  the 
training and test portions.
The second test was another train/test experi?
ment, but with the entire Mechanical Turk corpus 
as  training  data,  and  with  a  small  set  of  182 
emails, of which 99 were from the W3C Email 
Corpus9 and 83 were from emails belonging to 
various  Kiha Software employees,  as  test  data. 
These 182 test  emails  were hand?annotated for 
the three entity types by the authors.  Although 
this  test  data  was  small,  our  goal  here  was  to 
demonstrate  how  well  the  trained  extractors 
could fare against email text from a completely 
different source than the training data.
The third test  was similar  to the second,  but 
used as its test data 3,116 Enron emails annotated 
for  PERSON entities.10  The labels were manually 
corrected by the authors before testing.  The goal 
here  was the same as  with the  second test,  al?
though it must be acknowledged that the  PERSON 
training data did make use of 400 Enron emails, 
and therefore the test data was not from a com?
pletely separate domain.
The fourth test  was intended to  increase  the 
comparability of our own results with those that 
others have shown in NER on email text.  For the 
test  data,  we  chose  two  subsets  of  the  Enron 
Email  Corpus  used  in  Minkov  et  al.  (2005).11 
The first, ?Enron?Meetings,? contains 244 train?
ing documents, 242 tuning documents, and 247 
test documents.  The second, ?Enron?Random,? 
contains 89 training documents, 82 tuning docu?
ments,  and  83  test  documents.   For  each,  we 
9 http://tides.umiacs.umd.edu/webtrec/trecent/parsed
_w3c_corpus.html
10 http://www.cs.cmu.edu/~wcohen/repository.tgz 
and http://www.cs.cmu.edu/~einat/datasets.html.
11 http://www.cs.cmu.edu/~einat/datasets.html.
tested our statistical recognizers against all three 
divisions combined as well as the test set alne.
6    Results
The results from these four tests are presented in 
Tables 2?5.  In these tables, ?Agr.? refers to in?
ter?annotator agreement, ?TP? to token precision, 
?SP?  to  span  precision,  ?TR?  to  token  recall, 
?SR? to span recall,  ?TF? to token F?measure, 
and ?SF? to span F?measure.  ?Span? scores do 
not award partial credit for entities, and are there?
fore a stricter measure than ?token? scores.
Table 2: Cross?validation test results.
The cross?validation test results seem to indic?
ate that, in general, an inter?annotator agreement 
threshold of 2 produces the most cohesive cor?
pora  regardless  of  the  number  of  workers  as?
signed  per  email.   In  all  cases,  the  F?measure 
peaks at 2 and then begins to drop afterwards.
The  results  from  the  second  test,  using  the 
W3C and Kiha emails as test data, tell a slightly 
different story, however.  One predictable obser?
vation from these data is that precision tends to 
increase as more inter?annotator agreement is re?
quired, while recall decreases.  We believe that 
Table 3: Results from the second test.
Entity TP TR TF
1 65.90% 37.52% 47.82%
2 83.33% 56.28% 67.19%
3 84.05% 48.12% 61.20%
4 84.21% 26.10% 39.85%
1 41.03% 35.54% 38.09%
2 62.89% 30.77% 41.32%
3 66.00% 15.23% 24.75%
4 84.21% 9.85% 17.63%
Per. 1 85.48% 70.81% 77.45%
2 69.93% 69.72% 69.83%
3 86.95% 64.40% 73.99%
4 95.02% 43.29% 59.49%
Agr.
Loc.
Org.
Entity TP TR TF
1 60.07% 54.65% 57.23%
2 75.47% 70.51% 72.90%
3 71.59% 60.99% 65.86%
4 59.50% 41.40% 48.83%
1 70.79% 49.34% 58.15%
2 77.98% 55.97% 65.16%
3 38.96% 57.87% 46.57%
4 64.68% 50.19% 56.52%
Per. 1 86.67% 68.27% 76.38%
2 89.97% 77.36% 83.19%
3 87.58% 76.19% 81.49%
4 75.19% 63.76% 69.00%
Agr.
Loc.
Org.
77
this is due to the fact that entities that were con?
firmed by more workers tended to be less contro?
versial  or  ambiguous  than  those  confirmed  by 
fewer.   Most  surprising  about  these  results  is 
that, although F?measure peaks with the 2?agree?
ment corpora for both LOCATION and ORGANIZATION 
entities,  PERSON entities actually show the worst 
precision when using the 2?agreement corpus.  In 
the case of  PERSON entities, the corpus generated 
using no inter?annotator agreement at all, i.e., an?
notator  agreement  of  1,  actually  performs  the 
best in terms of F?measure.
Table 4: Results from the third test.
Data TP TR TF SP SR SF
E?M 1 100% 57.16% 72.74% 100% 50.10% 66.75%
(All) 2 100% 64.31% 78.28% 100% 56.11% 71.88%
3 100% 50.44% 67.06% 100% 45.11% 62.18%
4 100% 31.41% 47.81% 100% 27.91% 43.64%
E?M 1 100% 62.17% 76.68% 100% 51.30% 67.81%
(Test) 2 100% 66.36% 79.78% 100% 54.28% 70.36%
3 100% 55.72% 71.56% 100% 45.72% 62.76%
4 100% 42.24% 59.39% 100% 36.06% 53.01%
E?R 1 36.36% 59.91% 45.25% 40.30% 53.75% 46.07%
(All) 2 70.83% 65.32% 67.96% 67.64% 57.68% 62.26%
3 88.69% 58.63% 70.60% 82.93% 54.38% 65.68%
4 93.59% 43.68% 59.56% 89.33% 41.22% 56.41%
E?R 1 100% 60.87% 75.68% 100% 54.82% 70.82%
(Test) 2 100% 64.70% 78.56% 100% 59.05% 74.26%
3 100% 63.06% 77.34% 100% 58.38% 73.72%
4 100% 43.04% 60.18% 100% 40.10% 57.25%
Agr.
Table 5: Results from the fourth test.
With  the  third  test,  however,  the  results  are 
more in line with those from the cross?validation 
tests: F?measure peaks with the 2?agreement cor?
pus  and  drops  off  as  the  threshold  increases. 
Most likely these results can be considered more 
significant than those from the second test, since 
this  test  corpus  contains  almost  20  times  the 
number of documents.
For the fourth test, we report both token?level 
statistics  and  span?level  statistics  (i.e.,  where 
credit  for  partially  correct  entity  boundaries  is 
not awarded) in order to increase comparability 
with Minkov et al  (2005).  With one exception, 
these tests seem to show again that the highest F?
measure comes from the annotator created using 
an agreement level of 2, confirming results from 
the first and third tests.
The fourth test may also be directly compared 
to the results in Minkov et al  (2005), which re?
port span F?measure scores of 59.0% on Enron?
Meetings  and  68.1% on  Enron?Random,  for  a 
CRF?based recognizer using the ?Basic? feature 
set  (which  is  identical  to  ours)  and  using  the 
?train? division for training and the ?test? divi?
sion for testing.  In both cases, our best?perform?
ing annotators exceed these scores ? an 11.5% 
improvement  on  Enron?Meetings  and  a  6.16% 
improvement on Enron?Random.  This is an en?
couraging  result,  given  that  our  training  data 
largely come from a different source than the test 
data, and that the labels come from non?experts. 
We see this as confirmation that very large cor?
pora annotated by Mechanical Turk workers can 
surpass the quality of smaller corpora annotated 
by experts.
7    Conclusion
In order to quickly and economically build a 
large annotated dataset  for  NER,  we leveraged 
Amazon?s Mechanical Turk.  MTurk allowed us 
to  build a dataset of 20,609 unique emails with 
169,156  total  annotations  in  less  than  four 
months.  The  MTurk worker population respon?
ded well to NER tasks, and in particular respon?
ded well to the bonus and feedback scheme we 
put into place to improve annotation quality.  The 
bonus feedback system was designed to improve 
the transparency of the compensation system and 
motivate higher quality work over time.  Encour?
agingly, our results indicate that the workers who 
completed the most documents also had consist?
ently high entity recall, i.e., agreement with other 
workers, indicating that the system achieved the 
desired effect.
Given a large body of MTurk annotated docu?
ments, we were able to leverage inter?annotator 
agreement to control the precision and recall of a 
CRF?based recognizer trained on the data.  Im?
portantly,  we  also  showed  that  inter?annotator 
agreement can be used to predict the appropriate 
number of workers to assign to a given email in 
order to maximize entity recall and reduce costs.
Finally, a direct comparison of the entity re?
cognizers generated from  MTurk annotations to 
those  generated  from  expert  annotations  was 
very promising, suggesting that Mechanical Turk 
is  appropriate  for  NER annotation  tasks,  when 
care is taken to manage annotator error.
Acknowledgments
Thanks to Dolores Labs for the initial version of 
the UI, and thanks to Amazon and the 798 Mech?
anical Turk workers for making this work pos?
sible.
TP TR TF
1 80.56% 62.55% 70.42%
2 85.08% 67.66% 75.37%
3 93.25% 57.13% 70.86%
4 95.61% 39.67% 56.08%
Agr.
78
References 
Michele Banko and Eric Brill. 2001. Scaling to very 
very large corpora for natural language disambigu?
ation. In ACL '01:26?33.
Chris Callison?Burch. 2009. Fast, cheap, and creative: 
evaluating  translation  quality  using  Amazon's 
Mechanical Turk. In EMNLP '09:286?295.
Doug  Downey,  Matthew  Broadhead,  and  Oren  Et?
zioni.  2007.  Locating  complex  named  entities  in 
web text. In IJCAI '07.
Ralph Grishman and Beth Sundheim. 1996. Message 
Understanding  Conference?6:  a  brief  history.  In 
Proceedings of  the 16th conference on Computa?
tional Linguistics:466?471.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. 
Crowdsourcing user studies with Mechanical Turk. 
In Proceedings of CHI 2008.
Zornitsa Kozareva. 2006. Bootstrapping named entity 
recognition with automatically generated gazetteer 
lists. In Proceedings of the Eleventh Conference of  
the European Chapter of the Association for Com?
putational Linguistics:15?21.
Andrei  Mikheev,  Marc  Moens,  and  Claire  Grover. 
1999. Named entity recognition without gazetteers. 
In  Proceedings  of  the  Ninth  Conference  of  the  
European chapter of the Association for Computa?
tional Linguistics:1?8.
Einat Minkov, Richard C. Wang, and William W. Co?
hen. 2005. Extracting personal names from email: 
applying named entity recognition to informal text. 
In HTL '05:443?450.
David Nadeau and Satoshi Sekine. 2007. A survey of 
named  entity  recognition  and  classification.  Lin?
guisticae Investigationes, 30(1):3?26.
Preslav Nakov. 2008. Noun compound interpretation 
using paraphrasing verbs: Feasibility study. In Pro?
ceedings  of  the 13th international  conference  on  
Artificial  Intelligence:  Methodology,  Systems and  
Applications (AIMSA 2008):103?117.
Jason Pontin. 2007. Artificial Intelligence, With Help 
From the Humans.  In New York Times (March 25, 
2007). 
Burr Settles. 2004. Biomedical named entity recogni?
tion using conditional random fields and rich fea?
ture sets. In Proceedings of the COLING 2004 In?
ternational Joint Workshop on Natural Language  
Processing in Biomedicine and its Applications.
Rion Snow, Brendan O'Connor, Daniel Jurafsky, and 
Andrew Y. Ng. 2008. Cheap and fast  ? but is  it 
good?: evaluating non?expert annotations for natur?
al language tasks. In EMNLP '08:254?263.
Alexander Sorokin and David Forsyth. Utility data an?
notation with Amazon MTurk. In  Proceedings of  
Computer  Vision  and Pattern  Recognition Work?
shop at CVPR?08.
79
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 180?183,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Preliminary Experience with Amazon?s Mechanical Turk  for Annotating Medical Named Entities  Meliha Yetisgen-Yildiz, Imre Solti  Fei Xia, Scott Russell Halgrim Biomedical & Health Informatics Department of Linguistics University of Washington University of Washington Seattle, WA 98195, USA Seattle, WA 98195, USA {melihay,solti}@uw.edu {fxia,captnpi}@uw.edu  Abstract Amazon?s Mechanical Turk (MTurk) service is becoming increasingly popular in Natural Language Processing (NLP) research. In this paper, we report our findings in using MTurk to annotate medical text extracted from clini-cal trial descriptions with three entity types: medical condition, medication, and laboratory test. We compared MTurk annotations with a gold standard manually created by a domain expert. Based on the good performance re-sults, we conclude that MTurk is a very prom-ising tool for annotating large-scale corpora for biomedical NLP tasks. 1 Introduction The manual construction of annotated corpora is ex-tremely expensive both in terms of time and money. Snow et al (2008) demonstrated the potential power of Amazon?s Mechanical Turk (MTurk) service in annotat-ing large corpora for natural language tasks cheaply and quickly. We are working on a Natural Language Proc-essing (NLP) project to automate the clinical trial eligi-bility screening of patients. This project involves building statistical models for medical named entity recognition which requires a large-scale annotated cor-pus for training. As part of corpus development, we tested the feasibility of using MTurk for the annotation of medical named entities in biomedical text and we report our findings in this paper. In the following sections we describe how we used MTurk to annotate the biomedical corpus created from publicly available clinical trial announcements. The main goal of our study was to understand how well non-experts perform compared to medical expert in annotat-ing the biomedical text.  2 Related Work MTurk1 is an online micro-task market that allows re-questers to distribute work to a large number of workers from all over the world. The inspiration of the system                                                            1 https://www.MTurk.com/MTurk/welcome 
was to have human workers complete simple tasks that would otherwise be extremely difficult for computers to perform (Kittur et al, 2008). A complex task is broken down into simple, one-time tasks called Human Intelli-gence Tasks (HITs). Requesters post their HITs on the MTurk marketplace by specifying the amount paid for the completion of each task, and the workers select from the available HITs the ones that they would like to work on. In 2007, Amazon claimed that the user base of MTurk consisted of over 100,000 users from 100 coun-tries2. MTurk has been adopted for a variety of uses both in industry and academia, ranging from user studies (Kittur et al, 2008) to image labeling (Sorokin and Forsyth, 2008). Snow et al (2008) examined the quality of labels created by MTurk workers for various NLP tasks in-cluding word sense disambiguation, word similarity, text entailment, and temporal ordering. Since the publi-cation of Snow et al?s paper, MTurk has become in-creasingly popular as an annotation tool for NLP research. Nakov (2008) used MTurk to create a manu-ally annotated resource for noun-noun compound inter-pretation based on paraphrasing verbs. In a different NLP task, Callison-Burch (2009) used MTurk to evalu-ate machine translation quality. With a budget of only $10, Callison-Burch demonstrated the feasibility of per-forming manual evaluations of machine translation quality by recreating judgments from a WMT08 transla-tion task.  In our pilot study we used MTurk to annotate entities in the biomedical text. To our knowledge, this is the first study that investigates the feasibility of MTurk for bio-medical named entity annotation. 3 Annotation Task Description In this section we will describe the types of entities in our annotation task and the details of our corpus crea-tion process.  
                                                           2 Source: New York Times article  ?Artificial Intelligence, With Help from the Humans? , Available at: http://www.nytimes.com/2007/03/25/business/yourmoney/25Stream.html 
180
3.1 Entity Types We used MTurk to annotate the biomedical text for the following three entity types:  ? Medical Conditions Example: First-degree relative who developed <Medical_Condition>breast cancer </Medical_Condition> at ? 50 years of age. ? Medications Example: Previous treatment with an <Medica-tion>anthracycline</Medication> in the metas-tatic breast cancer setting. ? Laboratory Test Example: <Laboratory_Test>Platelet count >=100,000 cells/mL</Laboratory_Test>. 3.2 Corpus  Our corpus came from the publicly available clinical trial announcements available at the ClinicalTrials.gov website. This website is a registry of federally and pri-vately supported clinical trials conducted in the United States and around the world. The objectives and proce-dures of each clinical trial are explained in detail along with participant selection criteria and logistical informa-tion such as locations and contact information.  For this task we selected 50,109 announcements from the roughly 85,000 announcements posted on the Clini-calTrials.gov site. For selection criteria we relied on the following keywords: "heart | cancer | tumor | influenza | alzheimer | parkinson | malignant | stroke | respiratory | diabetes | pneumonia | nephritis | nephrotic | nephrosis | septicemia | liver | cirrhosis | hypertension | renal | neo-plasm". We chose these keywords because they were part of the phrases of diagnoses for the top 12 leading causes of death excluding suicide, homicide and acci-dents (Heron et al, 2009). We limited the selection to trials for "Adult" or "Senior" patients.  After downloading the corpus of XML files we con-verted them to ANSI text using ABC Amber XML Converter3. 49,794 files successfully converted to ANSI text format. Using a simple regular expression search we selected documents that had both the "Inclusion Cri-teria" and "Exclusion Criteria" phrases. The final selec-tion process resulted in 35,385 files. From this latest set we randomly selected 100 files to build the corpus for our pilot study. One of the authors, who has medical training, then manually annotated the three entity types in those selected files. We used this annotated set as the gold standard to measure the quality of the MTurk workers? annotations.  4 HIT Design                                                             3 ABC Amber XML Converter. Available at: http://www.processtext.com/abcxml.html. 
Biomedical text is full of jargon, and finding the three entity types in such text can be difficult for non-expert annotators. To make the annotation task more conven-ient for the MTurk workers, we used a customized user interface and provided detailed annotation guidelines. We also tested the bonus system available in the MTurk environment and evaluated the performance of the workers. 4.1 User Interface In order to adapt the task of entity annotation to the MTurk format, we used an in-house web-based graphi-cal user interface that allows the worker to select a span of text with the mouse cursor. The interface also uses simple tokenization heuristics to divide the text into highlightable spans and resolve partial token highlights or double-clicks into the next largest span. For instance, highlighting the word ?cancer? from the second ?c? to ?e? will result in the entire span ?cancer? being high-lighted.  4.2 Annotation Guidelines We created three separate annotation tasks, one for each entity type. For each task, we wrote annotation guide-lines that explained the task and showed examples of entities that should be tagged and the ones that should not.  4.3 Bonus System MTurk provides two methods for paying workers ? fixed rates on each document and bonuses to workers for especially good work. In this study, we experi-mented with the bonus system to see its effect on per-formance and annotation time. Annotating a document would receive a base rate of $0.01-$0.05, but each tagged entity span could elicit a bonus of $0.01. The base rate would cover the case where the document truly contained no entities, but the bonus amount could potentially be much larger than the base rate if the document was entity-rich. Bonuses for each tagged en-tity span were awarded based on an agreement threshold with peer workers. In this study, each document was annotated by four workers and we granted bonuses for entity spans that were agreed upon by at least three workers. 4.4 Performance Monitoring We monitored a worker?s performance by comparing the worker?s annotations with his/her peer workers? annotations. After we posted the HITs, we continuously monitored the workers? performance and rejected the annotations from the ones who tried to cheat the system by either not doing any annotations (e.g., immediately submitting the document after accepting it) or con-
181
stantly doing wrong annotations (e.g., always annotating the first word of the text). Those rejected documents were automatically re-posted on the MTurk so other workers could work on them. In this pilot study, per-formance monitoring was done mainly manually. As future work, we plan to automate the process in order to scale it for larger annotation tasks. 4.5 Communication with Workers The workers could send us their questions and com-ments about the individual documents or the general annotation task through a text box in the interface. Dur-ing this study we received more than 100 messages from the workers. The majority of the messages were positive messages (?thank you?, ?easy hit!?). However, some of the comments included questions such as: ?Is pregnancy a medical condition?? or ?Text doesn?t men-tion the type of insulin but I highlighted it because insu-lin is a medication!?. We responded to the questions in a timely manner to increase the quality of annotations.  5 Annotation Experiments In our annotation experiments, each of 100 documents in our corpus was annotated by four workers, resulting in 100?4=400 files per experiment. We experimented with different pay scales to understand how they affect the quality and speed of the annotations. 5.1 Cost of Annotations We investigated the cost of annotations both in terms of money and time. The summary of the results is in Table 1. We ran five different MTurk annotation experiments for our corpus of 100 documents. A total of 139 workers were involved in our experiments, and we identified eight of those workers as cheaters and rejected their annotation. The remaining workers spent 138.86 hours to complete 1872 files. The slowest experiment was MedicalCondition-I, in which we paid a base document rate of $0.01 without any bonuses. With this pay scale, it took 71.16 hours for workers to annotate 272 out of 400 files. We suspected we could not attract enough 
workers to finish the annotation task on time so we stopped the experiment before all 400 files were com-pleted. When we compiled the results, we noticed that there was a general tendency for the workers to tag the first one or two entities and then ignore the rest of the document. Based on this observation, we decided to add bonuses to motivate the workers to read through the whole document. We ran the same annotation task, MedicalCondition-II, with a higher base document rate of $0.05 and a bonus rate of $0.01. With this new pay-ment scale the annotation task was fully completed in 7.28 hours.  We also compared the effect of base rates when the bo-nus amounts were kept the same. For medication anno-tations, increasing the base document rate from $0.01 to $0.05 decreased the total amount of annotation time from 31.65 hours to 4.36 hours and also decreased the number of workers from 45 to 17. We ordered the workers based on the number files they annotated. The top ranked 5 workers in Medication-I annotated 187 files (46%) and the top ranked 5 workers in Medication-II annotated 313 files (78%). The difference between those two values was interesting since it indicated that by increasing the base rate, we managed to attract work-ers who worked on more documents.  The average amount of time workers spent per docu-ment varied based on entity type. They spent the longest amount of time for medical condition and shortest amount of time for laboratory test. This can be ex-plained by the richness of documents in terms of enti-ties. In the manually created gold standard there were 1159 mentions of medical condition, 518 mentions of medication, and 249 mentions of laboratory tests. An-other observation was that the change in pay scales did not affect the average annotation time per document. 5.2 Quality of Annotations We measured the quality of the MTurk annotations at different inter-annotator agreement levels by comparing the agreed entity spans with the spans in the gold stan-dard.  
Table 1. Cost analysis of annotation experiments (?File? in this table  means the annotation of a document. There are 100 documents, and each document is  annotated by four workers.) MONETARY COST TIME COST File Count Pay Rate ($) Total Cost ($) Completion Time Experiment Label Total Completed Total Worker Count File Bonus File Bonus Per file  (seconds) Total (hours) MedicalCondition-I 400 272 45 0.01 0 2.72 0 156.09 71.16 MedicalCondition-II 400 400 30 0.05 0.01 20 22.61 162.66 7.28 Medication-I 400 400 45 0.01 0.01 4 4.43 87.96 31.65 Medication-II 400 400 17 0.05 0.01 20 6.11 89.06 4.36 Laboratory Test  400 400 26 0.05 0.01 20 1.49 75.61 24.41  
182
Given a document annotated by multiple workers and an agreement level k, there are different ways of creating a new span file that includes only the spans that are agreed by at least k workers. One method is to go over each span in each annotation and output only the spans that are marked by at least k workers. This method does not work well when the spans are long and the workers could disagree on the boundary. We used an alternative method which first goes over each word position in the document and marks the positions that are part of spans in at least k annotations, and then outputs the spans that cover those marked positions. We call the new span file agreement-k file. Once we have created agreement-k file, we compare it with the gold standard to calculate precision, recall, and F-measure. A span in agreement-k file and a span in the gold standard are called an exact match if they are iden-tical and are called an overlap match if they overlap (exact match is a special case of overlap match). Table 2 shows the performance for the MedicalCondition-II, Medication-II, and LaboratoryTest experiments at dif-ferent agreement levels (k). As can be seen from the table, as the value of k increased, the precision values increased and the recall values decreased. For all of the experiments, the best F-Score was achieved at agree-ment-level 2.  Of the three entity types, laboratory test was the hardest partly because laboratory test entities tend to be longer (the average length for entities in gold standard was 5.25 words, compared to 1.84 words for medication and 3.18 words for medical condition), making the exact boundary harder to define. The results for MedicalCon-dition-II and Medication-II were higher than Laborato-ryTest. In addition, accuracy for Medication-I (not shown here due to space limit) and Medication-II were similar, indicating that pay rate did not affect accuracy much in our experiments. In the future, we plan to in-crease the number of annotations for each document, which we believe could further improve the perform-ance.  6   Conclusion Human annotation is crucial for many NLP tasks. In this paper, we demonstrated the potential of using MTurk 
for annotating medical text. By continuously monitoring the workers? performance and using the bonus system, we acquired high quality annotations from non-expert MTurk workers with limited time and budget.  As future work, we plan to analyze the MTurk annota-tions in detail in order to understand the problematic areas. Based on our observations, we will redesign our annotation tasks and continue our experiments with MTurk to create large-scale annotated corpora to be used in biomedical NLP projects.  Acknowledgement This project was supported in part by NIH Grants 1K99LM010227-0110 and 5 U54 LM008748. References  [1] Chris Callison-Burch. 2009. Fast, Cheap, and Crea-tive: Evaluating Translation Quality Using Amazon?s Mechanical Turk. In Proceedings of EMNLP?09. [2] Melonie Heron, Donna L. Hoyert, Sherry L. Mur-phy, Jiaquan Xu, Kenneth D. Kochanek, and Bet-zaida Tejada-Vera. 2009. Deaths: Final data for 2006. National Vital Statistics Reports, 57:14. [3] Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. Crowdsourcing User Studies with Mechanical Turk. In Proceedings of CHI?08. [4] Preslav Nakov. 2008. Noun compound interpretation using paraphrasing verbs: Feasibility study. In Pro-ceedings of the 13th international conference on Arti-ficial Intelligence: Methodology, Systems and Applications (AIMSA 2008), 103?117. [5] Philip V. Ogren. 2006. Knowtator: a prot?g? plug-in for annotated corpus construction. In Proceedings NAACL HLT?06, 273-275. [6] Rion Snow, Brendan O'Connor, Daniel Jurafsky and Andrew Y. Ng. 2008. Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natu-ral Language Tasks. In Proceedings of EMNLP?08, 254-263.  [7] Alexander Sorokin and David Forsyth. Utility data annotation with Amazon Mechanical Turk. In Pro-ceedings of Computer Vision and Pattern Recogni-tion Workshop at CVPR?08.  
Table 2. Quality measurement of MTurk annotations (k: Agreement level, P: Precision, R: Recall, F: F-measure; the highest value for each column is in boldface) Medical Condition-II Medication-II Laboratory Test Exact Overlap Exact Overlap Exact Overlap k P R F P R F P R F P R F P R F P R F 1 0.51 0.66 0.58 0.70 0.99 0.79 0.43 0.73 0.54 0.50 0.84 0.62 0.30 0.52 0.38 0.42 0.73 0.53 2 0.64 0.66 0.65 0.84 0.87 0.86 0.71 0.66 0.68 0.79 0.73 0.76 0.47 0.43 0.45 0.72 0.65 0.68 3 0.63 0.52 0.57 0.89 0.73 0.80 0.78 0.38 0.51 0.93 0.45 0.61 0.29 0.13 0.18 0.86 0.40 0.54 4 0.60 0.31 0.41 0.93 0.48 0.63 0.76 0.10 0.18 0.89 0.12 0.21 0.05 0.00 0.01 1.00 0.08 0.14  
183
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 47?51,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Annotating Change of State for Clinical Events  
 
Lucy Vanderwende 
One Microsoft Way 
Redmond, WA 98052 
lucyv@microsoft.com 
 
Fei Xia 
University of Washington 
Seattle, WA 98195 
fxia@uw.edu 
 
Meliha Yetisgen-Yildiz 
University of Washington 
Seattle, WA 98195 
melihay@uw.edu 
 
 
 
 
 
 
 
Abstract 
Understanding the event structure of sentences 
and whole documents is an important step in 
being able to extract meaningful information 
from the text. Our task is the identification of 
phenotypes, specifically, pneumonia, from 
clinical narratives. In this paper, we consider 
the importance of identifying the change of 
state for events, in particular, events that 
measure and compare multiple states across 
time. Change of state is important to the clini-
cal diagnosis of pneumonia; in the example 
?there are bibasilar opacities that are un-
changed?, the presence of bibasilar opacities 
alone may suggest pneumonia, but not when 
they are unchanged, which suggests the need 
to modify events with change of state infor-
mation. Our corpus is comprised of chest X-
ray reports, where we find many descriptions 
of change of state comparing the volume and 
density of the lungs and surrounding areas. 
We propose an annotation schema to capture 
this information as a tuple of <location, attrib-
ute, value, change-of-state, time-reference>. 
1 Introduction 
The narrative accompanying chest X-rays contains 
a wealth of information that is used to assess the 
health of a patient. X-rays are obviously a single 
snapshot in time, but the X-ray report narrative 
often makes either explicit or, more often, implicit 
reference to a previous X-ray. In this way, the se-
quence of X-ray reports is used not only to assess a 
patient?s health at a moment in time but also to 
monitor change.  Phenotypes such as pneumonia 
are consensus-defined diseases, which means that 
the diagnosis is typically established by human 
inspection of the data rather than by means of a 
test.  Our recent efforts have focused on building a 
phenotype detection system. In order to train and 
evaluate the system, we asked medical experts to 
annotate the X-ray report with phenotype labels 
and to highlight the text snippets in the X-ray re-
port that supported their phenotype labeling. 
   Analysis of the text snippets that support the la-
beling of pneumonia and the Clinical Pulmonary 
Infection Score (CPIS) reveal that most of these 
snippets mention a change of state or the lack of a 
change of state (i.e. persistent state).  This is un-
derstandable given our task, which is to monitor 
patients for ventilator associated pneumonia 
(VAP), which can develop over time as a patient is 
kept on a ventilator for medical reasons. 
   Change of state (COS) is most often understood 
as an aspectual difference that is reflected in verb 
morphology (Comrie, 1976), where a state is de-
scribed as initiating, continuing or terminating (see 
also Quirk et al, 1973, Section 3.36). In our cor-
pus, however, COS is often reflected not in verbs, 
but more frequently in nouns. A careful analysis of 
our data indicates that the states expressed as 
nouns don?t have the traditional aspects but rather 
exhibit COS more closely associated with compar-
atives, as they are susceptible to subjective and to 
objective measurement (Quirk et al, 1973, Section 
5.38).  These events compare two states across 
time or comparing one state against an accepted 
norm. Monitoring the state of the patient, and 
47
therefore comparing current state with previous 
states, is of paramount importance in the clinical 
scenario. We therefore propose in this paper to ex-
pand the annotation of COS to include the compar-
ison of states over time. 
2 The Task  
Early detection and treatment of ventilator associ-
ated pneumonia (VAP) is important as it is the 
most common healthcare-associated infection in 
critically ill patients. Even short?term delays in 
appropriate antibiotic therapy for patients with 
VAP are associated with higher mortality rates, 
longer?term mechanical ventilation, and excessive 
hospital costs. Interpretation of meaningful infor-
mation from the electronica medical records at the 
bedside is complicated by high data volume, lack 
of integrated data displays and text-based clinical 
reports that can only be reviewed by manual 
search. This cumbersome data management strate-
gy obscures the subtle signs of early infection.  
   Our research goal is to build NLP systems that 
identify patients who are developing critical ill-
nesses in a manner timely enough for early treat-
ment. As a first step, we have built a system that 
determines whether a patient has pneumonia based 
on the patient?s chest X-ray reports; see Figure 1 
for an example. 
 
 
01 CHEST, PORTABLE 1 VIEW 
02 INDICATION: 
03 Shortness of breath 
04 COMPARISON: July 16 10 recent prior 
05 FINDINGS: 
06 Left central line, tip at mid-SVC. 
07 Cardiac and mediastinal contours as before 
08 No pneumothorax. 
09 Lungs: Interval increase in right lung base  
10 pulmonary opacity with air bronchograms,  
11 increasing  pneumonitis / atelectasis. 
 
Figure 1. Sample chest X-ray report 
 
2.1 Annotation 
To train and evaluate the system, we created a cor-
pus of 1344 chest X-ray reports from our institu-
tion (Xia and Yetisgen-Yildiz, 2012). Two 
annotators, one a general surgeon and the other  a 
data analyst in a surgery department, read each 
report and determined whether the patient has 
pneumonia (PNA) and also what the clinical pul-
monary infection score (CPIS) is for the patient. 
The CPIS is used to assist in the clinical diagnosis 
of VAP by predicting which patients will benefit 
from obtaining pulmonary cultures, an invasive 
procedure otherwise avoided. There are three pos-
sible labels for PNA: (2a) no suspicion (negative 
class), (2b) suspicion of PNA, and (2c) probable 
PNA (positive class). Likewise, there are three la-
bels for CPIS: (1a) no infiltrate, report can include 
mention of edema or pleural effusion, (1b) diffuse 
infiltrate or atelectasis (i.e. reduced lung volume), 
and (1c) localized infiltrate, where one opacity is 
specifically highlighted and either PNA or infec-
tion is also mentioned. 
   In addition to the labels, we also asked the anno-
tators to highlight the text snippet they used to as-
sign the CPIS and PNA categories to reports (see 
(Yu et al, 2011) for similar approach to capturing 
expert knowledge). Thus, the snippets represent the 
support found for the CPIS and PNA label deter-
mination. The snippet found in lines 9-11, in figure 
1, for example, was support for both the CPIS (1c) 
and the PNA label (2c). 
2.2 Preliminary Results 
We used this corpus to train two SVM classifiers, 
one for CPIS and the other for PNA, and evaluated 
them using 5-fold cross validation (for details, see 
Tepper et al, 2013). The micro F1-score of the 
CPIS classifier was 85.8% with unigram features 
and 85.2% with unigram+bigram features. The 
micro F1-score of the PNA classifier was 78.5% 
with unigrams and 78.0% with unigram+bigrams.  
   We analyzed errors made by the CPIS and PNA 
classifiers and observed that many of them were 
due to lack of in-depth semantic analysis of text. 
Consider the snippet ?The previously noted right 
upper lobe opacity consistent with right upper lobe 
collapse has resolved?, which is labeled in the gold 
standard 1A (no infiltrate). The system mislabeled 
it 1C, (localized infiltrate), because the snippet 
supports 1C entirely up until the crucial words ?has 
resolved?. This error analysis motivated the clini-
cal event annotation task described in this paper. 
3 Change of State for Clinical Events 
In our data, clinically relevant events are often ex-
pressed as nouns. A text that mentions ?a clear 
48
lung?, for instance, implicitly describes the event 
of checking the lung density for that patient and 
finding it to be clear1. The TimeML annotation 
guidelines (Saur? et al, 2012) specify that states 
are to be annotated when they ?identifiably change 
over the course of a document being marked up?.  
In our scenario, where the document is the collec-
tion of the patient?s medical notes during hospital 
stay, a noun phrase such as ?lung capacity? is then 
a state that can certainly change over the course of 
the document.  
   Our corpus contains radiology reports and high-
lighted snippets of text where annotators found 
support for their finding. It is noteworthy that these 
snippets frequently describe observations of 
change, either in lung volume or in density. In fact, 
these changes of state (henceforth COS) appear 
more often in these snippets than non-snippets. 
Taking a random sample of 100 snippets, we found 
that 83/100 included some signal for COS, while a 
random sample of 100 non-snippet sentences in-
cluded only 61/100 mentions of COS. 
   Let us consider some examples of snippets in 
which the clinical events, in italics, are referred to 
using nouns, a shorthand for examination / meas-
urement of the noun in question. We have marked 
the signal words expressing a comparison across 
time in bold.  
1. The lungs are clear. 
2. Lungs: No focal opacities. 
3. The chest is otherwise unchanged. 
4. Left base opacity has increased and right 
base opacity persists which could repre-
sent atelectasis, aspiration, or pneumonia. 
Snippets 1 and 2 describe states in the current X-
ray report and do not express a COS. A close look 
at 3 and 4, however, reveals language that indicates 
that the experts are comparing the state in the cur-
rent X-ray with at least one other X-ray for that 
patient and in doing so, are describing a COS. 
Consider the phrases ?otherwise unchanged? in 
snippet 3, and ?increased? and ?persists? in snippet 
                                                          
1 The guidelines for the 2012 i2b2 temporal relation challenge  
define events as ?clinically relevant events and situations, 
symptoms, tests, procedures, ?? (Sun et al, 2013)   
4. Such words signal that the radiologist is examin-
ing more than one report at a time and making 
comparisons across these X-rays, without explicit 
reference to the other X-rays. There are other ex-
amples which exhibit explicit reference, for exam-
ple, snippets 5 and 6, where the signal words and 
the explicit reference are in boldface, and the clini-
cal events in italics: 
5. Bilateral lower lobe opacities are similar 
to those seen on DATE 
6. Since the prior examination lung vol-
umes have diminished    
Previous COS analyses (e.g., (Sun et al, 2013;  
Saur?, 2005)) have largely been limited to an anal-
ysis where events are expressed as verbs, and so is 
usually restricted to aspectual distinctions such as 
start, stop, and continue. In our data, however, 
many of the events are expressed as nouns and so 
we propose to extend the COS analysis to include 
measurements comparing two or more successive 
states and so will include concepts such as more, 
less, and equal2.  
4 Annotating change of state 
While previous event annotation (Uzuner et al, 
2010; Uzuner et al, 2011; Albright et al, 2013) 
marks multiple types of events, temporal expres-
sions, and event relations, our annotation focuses 
on tracking changes in a patient?s medical condi-
tions.  An event in our corpus is represented as a 
(loc, attr, val, cos, ref) tuple, where loc is the ana-
tomical location (e.g., ?lung?), attr is an attribute 
of the location that the event is about (e.g., ?densi-
ty?), val is a possible value for the attribute (e.g., 
?clear?), cos indicates the change of state for the 
attribute value compared to some previous report 
(e.g., ?unchanged?), and ref is a link to the re-
port(s) that the change of state is compared to (e.g., 
?prior examination?). Not all the fields in the tuple 
will be present in an event. When a field is absent, 
either it can be inferred from the context or it is 
unspecified.  
                                                          
2 In English, the morphology provides evidence, though rarely, 
that the comparative is a property of the change of state of an 
adjective. Consider the verb ?redden?, a derived form of the 
adjective ?red?, which means ?to become more red?, combin-
ing the inchoative and comparative (Chris Brockett, pc.) 
49
   The annotations for Snippets 1-6 are as follows: 
a dash indicates that the field is unspecified, and 
<?> indicates the field is unspecified but can be 
inferred from the location and the attribute value. 
For instance, the attribute value clear when refer-
ring to the location lungs implies that the attribute 
being discussed is the density of the lung. 
 
Ex1: (lungs, <density>, clear, -, -) 
Ex2: (lungs, <density>, no focal opacities, -, -)  
Ex3: (chest, -, -, unchanged, -) 
Ex4: (left base, <density>, opacity, increased, -), 
and (right base, <density>, opacity, persists, -) 
Ex5: (Bilateral lower lobe, <density>, opacities, 
similar, DATE) 
Ex6: (lung, volumes, -, diminished, prior examina-
tion) 
 
  A few points are worth noting.  First, the mapping 
from the syntactic structure to fields in event tuples 
is many-to-many. For example, a noun phrase con-
sisting of an adjective and noun may correspond to 
one or more fields in an event tuple. For instance, 
in the NP left base opacity in example 4, left base 
is loc, and opacity is val.  In example 6, the NP 
lung volumes will be annotated with lung as loc 
and volumes as attr, but no val. Similarly, an adjec-
tive can be part of a loc (e.g., bilateral in example 
5), a val (e.g., clear in example 1), or a cos (e.g., 
unchanged in example 3). Finally, the cos field 
may also be filled by a verb (e.g., increase and 
persists, in example 4). Making such distinctions 
will not be easy, especially for annotators with no 
medical training.  
    Second, events often have other attributes such 
as polarity (positive or negative) and modality 
(e.g., factual, conditional, possible). Most events in 
X-ray reports are positive and factual. We will add 
those attributes to our representations if needed. 
5 Summary 
Annotating events in a general domain without 
targeting a particular application can be challeng-
ing because it is often not clear what should be 
marked as an event. Our annotation focuses on the 
marking of COS in medical reports because COS is 
an important indicator of the patient?s medical 
condition. We propose to extend COS analysis to 
include comparison of state over time.  
   We are currently annotating a corpus of X-ray 
reports with the COS events. Once the corpus is 
complete, we will use it to train a system to detect 
such events automatically. The events identified by 
the event detector will then be used as features for 
phenotype detection. We expect that the COS fea-
tures will improve phenotype detection accuracy, 
in the same way that using features that encode 
negation and assertion types improves classifica-
tion results as demonstrated by Bejan et al (2012).  
    Our ultimate goal is to use event detection, phe-
notype detection, and other NLP systems to moni-
tor patients? medical conditions over time and 
prompt physicians with early warning, and thus 
improve patient healthcare quality while reducing 
the overall cost of healthcare. 
Acknowledgments 
We wish to thank the anonymous reviewers for 
their comments and also our colleagues Heather 
Evans at UW Medicine, and Michael Tepper, 
Cosmin Bejan and Prescott Klassen at the Univer-
sity of Washington. This work is funded in part by 
Microsoft Research Connections and University of 
Washington Research Royalty Fund. 
References  
Daniel Albright, Arrick Lanfranchi, Anwen Fredriksen, 
William F. Styler IV, Colin Warner, Jena D. Hwang, 
Jinho D. Choi, Dmitry Dligach, Rodney D. Nielsen, 
James Martin, Wayne Ward, Martha Palmer, and 
Guergana K. Savova. 2013. Towards comprehensive 
syntactic and semantic annotations of the clinical 
narrative. Journal of American Medical Informatics 
Association (JAMIA). [Epub ahead of print]. 
Cosmin A. Bejan, Lucy Vanderwende, Fei Xia, and 
Meliha Yetisgen-Yildiz. 2013. Assertion modeling 
and its role in clinical phenotype identification. Jour-
nal of Biomedical Informatics, 46(1):68-74. 
Bernard Comrie. 1976. Aspect.  Cambridge Textbooks 
in Linguistics. 
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, 
and Jan Svartvik, 1973. A Grammar of Contempo-
rary English. . Longman Group Ltd, London 
Roser Saur?, Jessica Littman, Bob Knippen, Robert Gai-
zauskas, Andrea Setzer, and James Pustejovsky. 
2005. TimeML Annotation Guidelines Version 1.2.1. 
Manuscript, Available at 
http://www.timeml.org/site/publications/specs.html 
Weiyi Sun, Anna Rumshisky, Ozlem Uzuner. 2013. 
Evaluating temporal relations in clinical text: 2012 
i2b2 Challenge. In Journal of the American Medical 
50
Informatics Association (JAMIA). Published Online 
First: 5 April 2013 10.1136/amiajnl-2013-001628.  
Michael Tepper, Heather. Evans, Fei Xia, and Meliha 
Yetisgen-Yildiz. 2013. Modeling Annotator Ration-
ales with Application to Pneumonia Classification. In 
Proceedings of Expanding the Boundaries of Health 
Informatics Using AI Workshop in conjunction with  
AAAI'2013.  
?zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag. 
2010. Community annotation experiment for ground 
truth generation for the i2b2 medication challenge. 
Journal of American Medical Informatics Associa-
tion (JAMIA), 17(5):519-23.  
?zlem Uzuner, Brent R. South, Shuying Shen, and Scott 
L. DuVall. 2011. 2010 i2b2/VA challenge on con-
cepts, assertions, and relations in clinical text. Jour-
nal of American Medical Informatics Association 
(JAMIA), 18(5):552-556. 
Fei Xia and Meliha Yetisgen-Yildiz. 2012. Clinical 
corpus annotation: challenges and strategies. In Pro-
ceedings of the Third Workshop on Building and 
Evaluating Resources for Biomedical Text Mining 
(BioTxtM'2012) in conjunction with the International 
Conference on Language Resources and Evaluation 
(LREC), Istanbul, Turkey. 
Shipeng Yu, Faisal Farooq, Balaji Krishnapuram, and 
Bharat Rao.  2011. Leveraging Rich Annotations to 
Improve Learning of Medical Concepts from Clinical 
Free Text. In Proceedings of the ICML workshop on 
Learning from Unstructured Clinical Text. Bellevue, 
WA. 
51
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 10?17,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identification of Patients with Acute Lung Injury from  
Free-Text Chest X-Ray Reports 
 
 
Meliha Yetisgen-Yildiz 
University of Washington 
Seattle, WA 98195 
melihay@uw.edu 
 
Cosmin Adrian Bejan 
University of Washington 
Seattle, WA 98195 
bejan@uw.edu 
 
Mark M. Wurfel 
University of Washington 
Seattle, WA 98195 
mwurfel@uw.edu 
 
  
 
Abstract 
Identification of complex clinical phenotypes 
among critically ill patients is a major chal-
lenge in clinical research. The overall research 
goal of our work is to develop automated ap-
proaches that accurately identify critical illness 
phenotypes to prevent the resource intensive 
manual abstraction approach. In this paper, we 
describe a text processing method that uses 
Natural Language Processing (NLP) and su-
pervised text classification methods to identify 
patients who are positive for Acute Lung Inju-
ry (ALI) based on the information available in 
free-text chest x-ray reports. To increase the 
classification performance we enhanced the 
baseline unigram representation with bigram 
and trigram features, enriched the n-gram fea-
tures with assertion analysis, and applied sta-
tistical feature selection. We used 10-fold 
cross validation for evaluation and our best 
performing classifier achieved 81.70% preci-
sion (positive predictive value), 75.59% recall 
(sensitivity), 78.53% f-score, 74.61% negative 
predictive value, 76.80% specificity in identi-
fying patients with ALI. 
1 Introduction 
Acute lung injury (ALI) is a critical illness con-
sisting of acute hypoxemic respiratory failure 
with bilateral pulmonary infiltrates that is associ-
ated with pulmonary and non-pulmonary risk 
factors. ALI and its more severe form, acute res-
piratory distress syndrome (ARDS), represent a 
major health problem with an estimated preva-
lence of 7% of intensive care unit admissions 
(Rubenfeld et al, 2005) for which the appropri-
ate treatment is often instituted too late or not at 
all (Ferguson et al, 2005; Rubenfeld et al, 
2004). Early detection of ALI syndrome is essen-
tial for appropriate application of the only thera-
peutic intervention demonstrated to improve 
mortality in ALI, lung protective ventilation 
(LPV).   
The identification of ALI requires recognition 
of a precipitating cause, either due to direct lung 
injury from trauma or pneumonia or secondary to 
another insult such as sepsis, transfusion, or pan-
creatitis. The consensus criteria for ALI include 
the presence of bilateral pulmonary infiltrates on 
chest radiograph, representing non-cardiac pul-
monary edema as evidenced by the absence of 
left atrial hypertension (Pulmonary Capillary 
Wedge Pressure < 18 mmHg (2.4 kPa)) or ab-
sence of clinical evidence of congestive heart 
failure, and oxygenation impairment as defined 
by an arterial vs. inspired oxygen level ratio 
(PaO2/FiO2) <300 mmHg (40 kPa))  (Argitas et 
al., 1998; Dushianthan et al, 2011; Ranieri et al, 
2012).  
In this paper, we describe a text processing 
approach to identify patients who are positive for 
ALI based only on the free-text chest x-ray re-
ports. 
2 Related Work 
Several studies demonstrated the value of Natu-
ral Language Processing (NLP) in a variety of 
health care applications including phenotype ex-
traction from electronic medical records (EMR) 
(Demner-Dushman et al, 2009). Within this do-
main, chest x-ray reports have been widely stud-
ied to extract different types of pneumonia (Tep-
per et al, 2013; Elkin et al, 2008; Aronsky et al, 
2001; Fiszman et al, 2000). Chest x-ray reports 
have also been studied for ALI surveillance by 
other researchers. Two of the prior studies relied 
on rule-based keyword search approaches. He-
rasevich et al (2009) included a free text Boole-
an query containing trigger words bilateral, infil-
trate, and edema. Azzam et al (2009) used a 
more extensive list of trigger words and phrases 
to identify the presence of bilateral infiltrates and 
10
ALI. In another study, Solti et al (2009) repre-
sented the content of chest x-ray reports using 
character n-grams and applied supervised classi-
fication to identify chest x-ray reports consistent 
with ALI. In our work, different from prior re-
search, we proposed a fully statistical approach 
where (1) the content of chest x-ray reports was 
represented by token n-grams, (2) statistical fea-
ture selection was applied to select the most in-
formative features, and (3) assertion analysis was 
used to enrich the n-gram features. We also im-
plemented Azzam et al?s approach based on the 
information available in their paper and used it as 
a baseline to compare performance results of our 
approach to theirs. 
3 Methods  
The overall architecture of our text processing 
approach for ALI identification is illustrated in 
Figure 1. In the following sections, we will de-
scribe the main steps of the text processing ap-
proach as well as the annotated chest x-ray cor-
pus used in training and test. 
3.1 Chest X-ray Corpora 
To develop the ALI extractor, we created a cor-
pus composed of 1748 chest x-ray reports gener-
ated for 629 patients (avg number of re-
ports=2.78, min=1, max=3). Subjects for this 
corpus were derived from a cohort of intensive 
care unit (ICU) patients at Harborview Medical 
Center that has been described previously (Gla-
van et al, 2011). We selected 629 subjects who 
met the oxygenation criteria for ALI 
(PaO2/FiO2<300 mmHg) and then three con-
secutive chest radiographs were pulled from the 
radiology database. Three Critical Care Medicine 
specialists reviewed the chest radiograph images 
for each patient and annotated the radiographs as 
consistent (positive) or not-consistent (negative) 
with ALI. We assigned ALI status for each sub-
ject based on the number of physician raters call-
ing the chest radiographs consistent or not con-
sistent with ALI. Table 1 shows the number of 
physicians with agreement on the radiograph in-
terpretation. There were 254 patients in the posi-
tive set (2 or more physicians agreeing on ALI 
positive) and 375 patients in the negative set (2 
or more physicians agreeing on ALI negative). 
Table 1 includes the distribution of patients over 
the positive and negative classes at different 
agreement levels. We will refer to this annotated 
corpus as the development set in the remaining 
of the paper.  
For validation, we used a second dataset gen-
erated in a similar fashion to the development 
set.  We obtained chest radiographs for 55 sub-
jects that were admitted to ICU and who met ox-
ygenation criteria for ALI (1 radiograph and re-
port per patient). A specialized chest radiologist 
annotated each report for the presence of ALI. 
There were 21 patients in the positive set and 34 
in the negative set. We will refer to this corpus as 
the validation set in the remaining of the paper. 
The retrospective review of the reports in both 
corpora was approved by the University of 
Washington Human Subjects Committee of Insti-
tutional Review Board who waived the need for 
informed consent. 
3.2 Pre-processing ? Section and Sentence 
Segmentation 
Although radiology reports are in free text for-
mat, they are somewhat structured in terms of 
sections. We used a statistical section segmenta-
tion approach we previously built to identify the 
boundaries of the sections and their types in our 
corpus of chest x-ray reports (Tepper et al, 
2012). The section segmenter was trained and 
tested with a corpus of 100 annotated radiology 
reports and produced 93% precision, 91% recall 
and 92% f-score (5-fold cross validation).  
R diology
Reports
Data Processor
Sections,
Sentences
Ranked
n-grams
ALI Learner ALI Predictor
Training
instances
Test
instances
Yes No
Assertion Cl ssifier
Feature Extractor
As ertion
cl sses
Top n-grams
 
Figure 1 Overall system architecture of ALI ex-
tractor. 
Annotation Agreement Patient Count 
ALI positive 
patients 
3 147 
2 107 
ALI negative 
patients 
3 205 
2 170 
Table 1 Agreement levels 
11
After identifying the report sections, we used the 
OpenNLP 1  sentence chunker to identify the 
boundaries of sentences in the section bodies. 
This pre-processing step identified 8,659 sec-
tions and 15,890 sentences in 1,748 reports of the 
development set and 206 sections and 414 sen-
tences in 55 reports of the validation set. We 
used the section information to filter out the sec-
tions with clinician signatures (e.g., Interpreted 
By, Contributing Physicians, Signed By). We 
used the sentences to extract the assertion values 
associated with n-gram features as will be ex-
plained in a later section. 
3.3 Feature Selection 
Representing the information available in the 
free-text chest x-ray reports as features is critical 
in identifying patients with ALI. In our represen-
tation, we created one feature vector for each 
patient. We used unigrams as the baseline repre-
sentation. In addition, we used bigrams and tri-
grams as features. We observed that the chest x-
ray reports in our corpus are short and not rich in 
terms of medical vocabulary usage. Based on this 
observation, we decided not to include any medi-
cal knowledge-based features such as UMLS 
concepts or semantic types. Table 2 summarizes 
the number of distinct features for each feature 
type used to represent the 1,748 radiology reports 
for 629 patients. 
As can be seen from the table, for bigrams and 
trigrams, the feature set sizes is quite high. Fea-
ture selection algorithms have been successfully 
applied in text classification in order to improve 
the classification accuracy (Wenqian et al, 
2007). In previous work, we applied statistical 
feature selection to the problem of pneumonia 
detection from ICU reports (Bejan et al, 2012). 
By significantly reducing the dimensionality of 
the feature space, they improved the efficiency of 
the pneumonia classifiers and provided a better 
understanding of the data. 
We used statistical hypothesis testing to de-
termine whether there is an association between 
a given feature and the two categories of our 
problem (i.e, positive and negative ALI). Specif-
ically, we computed the ?2 statistics (Manning 
                                                 
1 OpenNLP. Available at: http://opennlp.apache.org/ 
and Schutze, 1999) which generated an ordering 
of features in the training set. We used 10-fold 
cross validation (development set) in our overall 
performance evaluation. Table 3 lists the top 15 
unigrams, bigrams, and trigrams ranked by ?2 
statistics in one of ten training sets we used in 
evaluation. As can be observed from the table, 
many of the features are closely linked to ALI. 
Once the features were ranked and their corre-
sponding threshold values (N) were established, 
we built a feature vector for each patient. Specif-
ically, given the subset of N relevant features 
extracted from the ranked list of features, we 
considered in the representation of a given pa-
tient?s feature vector only the features from the 
subset of relevant features that were also found 
in the chest x-ray reports of the patient. There-
fore, the size of the feature space is equal to the 
size of relevant features subset (N) whereas the 
length of each feature vector will be at most this 
value.  
3.4 Assertion Analysis 
We extended our n-gram representation with as-
sertion analysis. We built an assertion classifier 
(Bejan et al, 2013) based on the annotated cor-
pus of 2010 Integrating Biology and the Beside 
(i2b2) / Veteran?s Affairs (VA) NLP challenge 
(Uzuner et al, 2011). The 2010 i2b2/VA chal-
lenge introduced assertion classification as a 
Unigram Bigram Trigram 
Diffuse diffuse lung opacities con-
sistent with 
Atelectasis lung opacities diffuse lung opaci-
ties 
Pulmonary pulmonary edema change in diffuse 
Consistent consistent with lung opacities 
consistent 
Edema opacities consistent in diffuse lung 
Alveolar in diffuse with pulmonary 
edema 
Opacities diffuse bilateral consistent with 
pulmonary 
Damage with pulmonary low lung volumes 
Worsening alveolar damage or alveolar damage 
Disease edema or pulmonary edema 
pneumonia 
Bilateral low lung diffuse lung dis-
ease 
Clear edema pneumonia edema pneumonia 
no 
Severe or alveolar diffuse bilateral 
opacities 
Injury lung disease lungs are clear 
Bibasilar pulmonary opacities lung volumes with 
Table 3 Top 15 most informative unigrams, bigrams, 
and trigrams for ALI classification according to ?2 
statistics. 
Feature Type # of Distinct Features 
Unigram (baseline) 1,926 
Bigram 10,190 
Trigram 17,798 
Table 2 Feature set sizes of the development set. 
12
shared task, formulated such that each medical 
concept mentioned in a clinical report (e.g., 
asthma) is associated with a specific assertion 
category (present, absent, conditional, hypothet-
ical, possible, and not associated with the pa-
tient). We defined a set of novel features that 
uses the syntactic information encoded in de-
pendency trees in relation to special cue words 
for these categories. We also defined features to 
capture the semantics of the assertion keywords 
found in the corpus and trained an SVM multi-
class classifier with default parameter settings. 
Our assertion classifier outperformed the state-
of-the-art results and achieved 79.96% macro-
averaged F-measure and 94.23% micro-averaged 
F-measure on the i2b2/VA challenge test data.  
For each n-gram feature (e.g., pneumonia), we 
used the assertion classifier to determine whether 
it is present or absent based on contextual infor-
mation available in the sentence the feature ap-
peared in (e.g., Feature: pneumonia, Sentence: 
There is no evidence of pneumonia, congestive 
heart failure, or other acute process., Assertion: 
absent). We added the identified assertion value 
to the feature (e.g., pneumonia_absent). The fre-
quencies of each assertion type in our corpus are 
presented in Table 4. Because chest x-rays do not 
include family history, there were no instances of 
not associated with the patient. We treated the 
three assertion categories that express hedging 
(conditional, hypothetical, possible) as the pre-
sent category. 
3.5 Classification  
For our task of classifying ALI patients, we 
picked the Maximum Entropy (MaxEnt) algo-
rithm due to its good performance in text classi-
fication tasks (Berger et al, 1996). In our exper-
iments, we used the MaxEnt implementation in a 
machine learning package called Mallet2.  
4 Results 
4.1 Metrics  
We evaluated the performance by using precision 
(positive predictive value), recall (sensitivity), 
negative predictive value, specificity, f-score, 
and accuracy. We used 10-fold cross validation 
to measure the performance of our classifiers on 
the development set. We evaluated the best per-
forming classifier on the validation set.  
4.2 Experiments with Development Set  
We designed three groups of experiments to ex-
plore the effects of (1) different n-gram features, 
(2) feature selection, (3) assertion analysis of 
features on the classification of ALI patients. We 
defined two baselines to compare the perfor-
mance of our approaches. In the first baseline, 
we implemented the Azzam et. al.?s rule-based 
approach (2009). In the second baseline, we only 
represented the content of chest x-ray reports 
with unigrams. 
4.3 N-gram Experiments  
Table 5 summarizes the performance of n-gram 
features. When compared to the baseline uni-
gram representation, gradually adding bigrams 
(uni+bigram) and trigrams (uni+bi+trigram) to 
the baseline increased the precision and specifici-
ty by 4%. Recall and NPV remained the same. 
Azzam et. al.?s rule-based baseline generated 
higher recall but lower precision when compared 
to n-gram features. The best f-score (64.45%) 
was achieved with the uni+bi+trigram represen-
tation. 
4.4 Feature Selection Experiments  
To understand the effect of large feature space on 
classification performance, we studied how the 
performance of our system evolves for various 
threshold values (N) on the different combina-
tions of ?2 ranked unigram, bigram, and trigram 
features. Table 6 includes a subset of the results 
we collected for different values of N. As listed 
                                                 
2 Mallet. Available at: http://mallet.cs.umass.edu 
Assertion Class Frequency  
Present 206,863 
Absent 13,961 
Conditional 4 
Hypothetical 330 
Possible 3,980 
Table 4 Assertion class frequencies. 
System configuration TP TN FP FN 
Precision/ 
PPV 
Recall/ 
Sensitivity 
NPV Specificity F-Score Accuracy 
Baseline#1?Azzam et. al. (2009) 201 184 191 53 51.27 79.13 77.64 49.07 62.23 61.21 
Baseline#2?unigram 156 288 87 98 64.20 61.42 74.61 76.80 62.78 70.59 
Uni+bigram 156 296 79 98 66.38 61.42 75.13 78.93 63.80 71.86 
Uni+bi+trigram 155 303 72 99 68.28 61.02 75.37 80.80 64.45 72.81 
Table 5 Performance evaluation on development set with no feature selection. TP: True positive, TN: True nega-
tive, FP: False positive, FN: False negative, PPV: Positive predictive value, NPV: Negative predictive value. The 
row with the heighted F-Score is highlighted. 
 
 
13
in this table, for N=100, the unigram represen-
tation performed better than uni+bigram, 
uni+bi+trigram feature combinations; however, 
as N increased, the performance of 
uni+bi+trigram performed better, reaching the 
best f-score (78.53%) at N=800. When compared 
to the two defined baselines, the performance 
results of uni+bi+trigram at N=800 were signifi-
cantly better than those of the baselines.  
4.5 Assertion Analysis Experiments  
We ran a series of experiments to understand the 
effect of assertion analysis on the classification 
performance. We used the best performing clas-
N Feature configuration TP TN FP FN 
Precision/ 
PPV 
Recall/  
Sensitivity 
NPV Specificity F-Score Accuracy 
100 
Unigram 191 316 59 63 76.40 75.20 83.38 84.27 75.79 80.60 
Uni+bigram 180 313 62 74 74.38 70.87 80.88 83.47 72.58 78.38 
Uni+bi+trigram 183 317 58 71 75.93 72.05 81.70 84.53 73.94 79.49 
200 
Unigram 189 312 63 65 75.00 74.41 82.76 83.20 74.70 79.65 
Uni+bigram 183 321 54 71 77.22 72.05 81.89 85.60 74.54 80.13 
Uni+bi+trigram 190 322 53 64 78.19 74.80 83.42 85.87 76.46 81.40 
300 
Unigram 185 311 64 69 74.30 72.83 81.84 82.93 73.56 78.86 
Uni+bigram 188 322 53 66 78.01 74.02 82.99 85.87 75.96 81.08 
Uni+bi+trigram 187 331 44 67 80.95 73.62 83.17 88.27 77.11 82.35 
400 
Unigram 179 315 60 75 74.90 70.47 80.77 84.00 72.62 78.54 
Uni+bigram 184 319 56 70 76.67 72.44 82.01 85.07 74.49 79.97 
Uni+bi+trigram 184 325 50 70 78.63 72.44 82.28 86.67 75.41 80.92 
500 
Unigram 177 310 65 77 73.14 69.69 80.10 82.67 71.37 77.42 
Uni+bigram 178 321 54 76 76.72 70.08 80.86 85.60 73.25 79.33 
Uni+bi+trigram 187 325 50 67 78.90 73.62 82.91 86.67 76.17 81.40 
600 
Unigram 179 305 70 75 71.89 70.47 80.26 81.33 71.17 76.95 
Uni+bigram 177 320 55 77 76.29 69.69 80.60 85.33 72.84 79.01 
Uni+bi+trigram 189 325 50 65 79.08 74.41 83.33 86.67 76.67 81.72 
700 
Unigram 176 308 67 78 72.43 69.29 79.79 82.13 70.82 76.95 
Uni+bigram 180 323 52 74 77.59 70.87 81.36 86.13 74.07 79.97 
Uni+bi+trigram 189 328 47 65 80.08 74.41 83.46 87.47 77.14 82.19 
800 
Unigram 172 311 64 82 72.88 67.72 79.13 82.93 70.20 76.79 
Uni+bigram 180 327 48 74 78.95 70.87 81.55 87.20 74.69 80.60 
Uni+bi+trigram 192 332 43 62 81.70 75.59 84.26 88.53 78.53 83.31 
900 
Unigram 174 311 64 80 73.11 68.50 79.54 82.93 70.73 77.11 
Uni+bigram 182 328 47 72 79.48 71.65 82.00 87.47 75.36 81.08 
Uni+bi+trigram 187 333 42 67 81.66 73.62 83.25 88.80 77.43 82.67 
1000 
Unigram 177 313 62 77 74.06 69.69 80.26 83.47 71.81 77.90 
Uni+bigram 185 326 49 69 79.06 72.83 82.53 86.93 75.82 81.24 
Uni+bi+trigram 190 327 48 64 79.83 74.80 83.63 87.20 77.24 82.19 
Table 6 Performance evaluation on development set with feature selection. TP: True positive, TN: True neg-
ative, FP: False positive, FN: False negative, PPV: Positive predictive value, NPV: Negative predictive value. 
The row with the heighted F-Score is highlighted. 
 
Assertion configuration TP TN FP FN 
Precision/ 
PPV 
Recall/ 
Sensitivity 
NPV Specificity F-Score Accuracy 
Assertion_none 192 332 43 62 81.70 75.59 84.26 88.53 78.53 83.31 
Assertion_all 188 328 47 66 80.00 74.02 83.25 87.47 76.89 82.03 
Assertion_top_10 191 328 47 63 80.25 75.20 83.89 87.47 77.64 82.51 
Assertion_top_20 190 329 46 64 80.51 74.80 83.72 87.73 77.55 82.51 
Assertion_top_30 190 331 44 64 81.20 74.80 83.80 88.27 77.87 82.83 
Assertion_top_40 190 328 47 64 80.17 74.80 83.67 87.47 77.39 82.35 
Assertion_top_50 190 330 45 65 80.85 74.51 83.54 88.00 77.55 82.54 
Table 7 Performance evaluation on development set with the assertion feature (uni+bi+trigram at N=800). 
TP: True positive, TN: True negative, FP: False positive, FN: False negative, PPV: Positive predictive value, 
NPV: Negative predictive value. The row with the heighted F-Score is highlighted. 
 
System configuration TP TN FP FN 
Precision/ 
PPV 
Recall/  
Sensitivity 
NPV Specificity F-Score Accuracy 
Baseline#1?Azzam et. al. (2009) 10 18 16 11 38.46 47.62 62.07 52.94 42.55 50.91 
Baseline#2?unigram 12 29 5 9 70.53 57.14 76.32 85.29 63.16 74.55 
Uni+bi+trigram at k=800 9 30 4 12 69.23 42.86 71.43 88.24 52.94 70.91 
Table 8 Performance evaluation on validation set. TP: True positive, TN: True negative, FP: False positive, 
FN: False negative, PPV: Positive predictive value, NPV: Negative predictive value. The row with the 
heighted F-Score is highlighted. 
14
sifier with uni+bi+trigram at N=800 in our ex-
periments. We applied assertion analysis to all 
800 features as well as only a small set of top 
ranked 10?k (1?k?5) features which were ob-
served to be closely related to ALI (e.g., diffuse, 
opacities, pulmonary edema). We hypothesized 
applying assertion analysis would inform the 
classifier on the presence and absence of those 
terms which would potentially decrease the false 
positive and negative counts. 
Table 7 summarizes the results of our experi-
ments. When we applied assertion analysis to all 
800 features, the performance slightly dropped 
when compared to the performance with no as-
sertion analysis. When assertion analysis applied 
to only top ranked features, the best f-score per-
formance was achieved with assertion analysis 
with top 30 features; however, it was still slightly 
lower than the f-score with no assertion analysis.  
The differences are not statistically significant. 
4.6 Experiments with Validation Set  
We used the validation set to explore the general-
izability of the proposed approach. To accom-
plish this we run the best performing classifier 
(uni+bi+trigram at N=800) and two defined 
baselines on the validation set. We re-trained the 
uni+bi+trigram at N=800 classifier and unigram 
baseline on the complete development set. 
Table 8 includes the performance results. The 
second baseline with unigrams performed the 
best and Azzam et. al.?s baseline performed the 
worst in identifying the patients with ALI in the 
validation set. 
5 Discussion 
Our best system achieved an f-score of 78.53 
(precision=81.70, recall=75.59) on the develop-
ment set. While the result is encouraging and 
significantly better than the f-score of a previous-
ly published system (f-score=62.23, preci-
sion=51.27, recall=79.13), there is still room for 
improvement. 
There are several important limitations to our 
current development dataset. First, the annotators 
who are pulmonary care specialists used only the 
x-ray images to annotate the patients. However, 
the classifiers were trained based on the features 
extracted from the radiologists? free-text inter-
pretation of the x-ray images. In one false posi-
tive case, the radiologist has written ?Bilateral 
diffuse opacities, consistent with pulmonary 
edema. Bibasilar atelectasis.? in the chest x-ray 
report, however all three pulmonary care special-
ists annotated the case as negative based on their 
interpretation of images. Because the report con-
sisted of many very strong features indicative of 
ALI, our classifier falsely identified the patient 
as positive with a very high prediction probabil-
ity 0.96. Second, although three annotators anno-
tated the development set, there was full agree-
ment on 42.12% (107/254) of the positive pa-
tients and 45.33% (170/375) of the negative pa-
tients. Table 9 includes the false positive and 
negative statistics of the best performing classifi-
er (uni+bi+trigrams at N=800). As can be seen 
from the table, the classifier made more mistakes 
on patients where the annotator agreement was 
not perfect. The classifier predicted 13 of the 28 
false positives and 23 of the 39 false negatives 
with probabilities higher than 0.75. When we 
investigated the reports of those 13 false posi-
tives, we observed that the radiologists used 
many very strong ALI indicative features (e.g., 
diffuse lung opacities, low lung volumes) to de-
scribe the images. On the contrary, radiologists 
did not use as many ALI indicative features in 
the reports of 23 false negative cases. 
In our experiments on the development set, we 
demonstrated the positive impact of statistical 
feature selection on the overall classification per-
formance. We achieved the best f-score, when 
we used only 2.67% (800/29,914) of the com-
plete n-gram feature space. We enriched the 
highly ranked features with assertion analysis. 
However, unlike feature selection, assertion 
analysis did not improve the overall perfor-
mance. To explore the reasons, we analyzed re-
ports from our corpus and found out that the cur-
rent six assertion classes (present, absent, condi-
tional, hypothetical, possible) were not sufficient 
to capture true meaning in many cases. For ex-
ample, our assertion classifier assigned the class 
present to the bigram bibasilar opacities based 
on the sentence ?There are bibasilar opacities 
that are unchanged?. Although present was the 
correct assignment for bibasilar opacities, the 
more important piece of information was the 
change of state in bibasilar opacities for ALI 
diagnosis. X-rays describe a single snapshot of 
time but the x-ray report narrative makes explicit 
Error Type Agreement Frequency Percentage 
False Positives 
3 15 10.20% (15/147) 
2 28 26.17% (28/107) 
False Negatives 
3 24 11.70% (24/205) 
2 39 22.94% (39/170) 
Table 9 False positive and false negative statistics at 
different agreement levels. 
15
or, more often implicit references to a previous 
x-ray. In this way, the sequence of x-ray reports 
is used not only to assess a patient?s health at a 
moment in time but also to monitor the change. 
We recently defined a schema to annotate change 
of state for clinical events in chest x-ray reports 
(Vanderwende et al, 2013). We will use this an-
notation schema to create an annotated corpus 
for training models to enrich the assertion fea-
tures for ALI classification.  
The results on the validation set revealed that 
the classification performance degraded signifi-
cantly when training and test data do not come 
from the same dataset. There are multiple rea-
sons to this effect. First, the two datasets had dif-
ferent language characteristics. Although both 
development and validation sets included chest 
x-ray reports, only 2,488 of the 3,305 (75.28%) 
n-gram features extracted from the validation set 
overlapped with the 29,914 n-gram features ex-
tracted from the development set. We suspect 
that this is the main reason why our best per-
forming classifier with feature selection trained 
on the development set did not perform as well 
as the unigram baseline on the validation set. 
Second, the validation set included only 55 pa-
tients and each patient had only one chest x-ray 
report unlike the development set where each 
patient had 2.78 reports on the average. In other 
words, the classifiers trained on the development 
set with richer content made poor predictions on 
the validation set with more restricted content. 
Third, because the number of patients in the val-
idation set was too small, each false positive and 
negative case had a huge impact on the overall 
performance. 
6 Conclusion 
In this paper, we described a text processing ap-
proach to identify patients with ALI from the 
information available in their corresponding free-
text chest x-ray reports. To increase the classifi-
cation performance, we (1) enhanced the base-
line unigram representation with bigram and tri-
gram features, (2) enriched the n-gram features 
with assertion analysis, and (3) applied statistical 
feature selection. Our proposed methodology of 
ranking all the features using statistical hypothe-
sis testing and selecting only the most relevant 
ones for classification resulted in significantly 
improving the performance of a previous system 
for ALI identification. The best performing clas-
sifier achieved 81.70% precision (positive pre-
dictive value), 75.59% recall (sensitivity), 
78.53% f-score, 74.61% negative predictive val-
ue, 76.80% specificity in identifying patients 
with ALI when using the uni+bi+trigram repre-
sentation at N=800. Our experiments showed 
that assertion values did not improve the overall 
performance. For future work, we will work on 
defining new semantic features that will enhance 
the current assertion definition and capture the 
change of important events in radiology reports.  
Acknowledgements 
The work is partly supported by the Institute of 
Translational Health Sciences (UL1TR000423), 
and Microsoft Research Connections. We would 
also like to thank the anonymous reviewers for 
helpful comments. 
References  
Aronsky D, Fiszman M, Chapman WW, Haug PJ. 
Combining decision support methodologies to di-
agnose pneumonia. AMIA Annu Symp Proc., 
2001:12-16. 
Artigas A, Bernard GR, Carlet J, Dreyfuss D, Gatti-
noni L, Hudson L, Lamy M, Marini JJ, Matthay 
MA, Pinsky MR, Spragg R, Suter PM. The Ameri-
can-European Consensus Conference on ARDS, 
part 2: Ventilatory, pharmacologic, supportive 
therapy, study design strategies, and issues related 
to recovery and remodeling. Acute respiratory dis-
tress syndrome. Am J Respir Crit Care Med. 
1998;157(4 Pt1):1332-47. 
Azzam HC, Khalsa SS, Urbani R, Shah CV, Christie 
JD, Lanken PN, Fuchs BD. Validation study of an 
automated electronic acute lung injury screening 
tool. J Am Med Inform Assoc. 2009; 16(4):503-8.  
Bejan CA, Xia F, Vanderwende L, Wurfel M, Yet-
isgen-Yildiz M. Pneumonia identification using 
statistical feature selection. J Am Med Inform As-
soc. 2012; 19(5):817-23. 
Bejan CA, Vanderwende L, Xia F, Yetisgen-Yildiz 
M. Assertion Modeling and its role in clinical phe-
notype identification. J Biomed Inform. 2013; 
46(1):68-74. 
Berger AL, Pietra SAD, Pietra VJD. A maximum 
entropy approach to natural language processing. 
Journal of Computational Linguistics. 1996; 
22(1):39-71. 
Demner-Fushman D, Chapman WW, McDonald CJ. 
What can natural language processing do for clini-
cal decision support? J Biomed Inform. 2009; 
42(5):760-72. 
Dushianthan A, Grocott MPW, Postle AD, Cusack R. 
Acute respiratory distress syndrome and acute lung 
injury. Postgrad Med J. 2011; 87:612-622. 
16
Elkin PL, Froehling D, Wahner-Roedler D, Trusko B, 
Welsh G, Ma H, Asatryan AX, Tokars JI, Rosen-
bloom ST, Brown SH. NLP-based identification of 
pneumonia cases from free-text radiological re-
ports. AMIA Annu Symp Proc. 2008; 6:172-6. 
Ferguson ND, Frutos-Vivar F, Esteban A, Fern?ndez-
Segoviano P, Aramburu JA, N?jera L, Stewart TE. 
Acute respiratory distress syndrome: underrecogni-
tion by clinicians and diagnostic accuracy of three 
clinical definitions. Crit Care Med. 2005; 
33(10):2228-34. 
Fiszman M, Chapman WW, Aronsky D, Evans RS, 
Haug PJ. Automatic detection of acute bacterial 
pneumonia from chest X-ray reports. J Am Med In-
form Assoc. 2000;7(6):593-604. 
Glavan BJ, Holden TD, Goss CH, Black RA, Neff 
MJ, Nathens AB, Martin TR, Wurfel MM; 
ARDSnet Investigators. Genetic variation in the 
FAS gene and associations with acute lung injury. 
Am J Respir Crit Care Med. 2011;183(3):356-63. 
Herasevich V, Yilmaz M, Khan H, Hubmayr RD, 
Gajic O. Validation of an electronic surveillance 
system for acute lung injury. Intensive Care Med. 
2009; 35(6):1018-23. 
Manning CD, Schutze H. Foundations of statistical 
natural language processing. MIT Press 1999. 
Ranieri VM, Rubenfeld GD, Thompson BT, Ferguson 
ND, Caldwell E, Fan E, Camporota L, Slutsky AS. 
Acute Respiratory Distress Syndrome. The Berlin 
Definition. JAMA. 2012; 307(23): 2526-2533. 
Rubenfeld GD, Caldwell E, Peabody E, Weaver J, 
Martin DP, Neff M, Stern EJ, Hudson LD. Inci-
dence and outcomes of acute lung injury. N Engl J 
Med. 2005; 353(16):1685-93. 
Rubenfeld GD, Cooper C, Carter G, Thompson BT, 
Hudson LD. Barriers to providing lung-protective 
ventilation to patients with acute lung injury. Crit 
Care Med. 2004; 32(6):1289-93. 
Solti I, Cooke CR, Xia F, Wurfel MM. Automated 
Classification of Radiology Reports for Acute 
Lung Injury: Comparison of Keyword and Ma-
chine Learning Based Natural Language Pro-
cessing Approaches. Proceedings (IEEE Int Conf 
Bioinformatics Biomed). 2009;314-319. 
Tepper M, Capurro D, Xia F, Vanderwende L, Yet-
isgen-Yildiz M. Statistical Section Segmentation in 
Free-Text Clinical Records. Proceedings of the In-
ternational Conference on Language Resources and 
Evaluation (LREC), Istanbul, May 2012.  
Tepper M, Evans HL, Xia F, Yetisgen-Yildiz M. 
Modeling Annotator Rationales with Application 
to Pneumonia Classification. Proceedings of Ex-
panding the Boundaries of Health Informatics Us-
ing AI Workshop of AAAI'2013, Bellevue, WA; 
2013. 
Uzuner O, South BR, Shen S, DuVall SL. 2010 
i2b2/VA challenge on concepts, assertions, and re-
lations in clinical text. J Am Med Inform Assoc. 
2011; 18(5):552?556. 
Vanderwende L, Xia F, Yetisgen-Yildiz M. Annotat-
ing Change of State for Clinical Events.  
Proceedings of the 1st Workshop on EVENTS: 
Definition, Detection, Coreference, and Represen-
tation Workshop of NAACL?2013, Atlanta, June 
2013. 
Wenqian W, Houkuan H, Haibin Z et al A novel fea-
ture selection algorithm for text categorization. Ex-
pert Syst Appl 2007;33:1?5. 
17
