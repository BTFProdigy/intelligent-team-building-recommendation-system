BioGrapher: Biography Questions as a
Restricted Domain Question Answering Task
Oren Tsur
Text and Data Mining Group
Bar Ilan University
tsuror@cs.biu.ac.il
Maarten de Rijke
Informatics Institute
University of Amsterdam
mdr@science.uva.nl
Khalil Sima?an
Institute for Logic, Language
and Computation
University of Amsterdam
simaan@science.uva.nl
Abstract
We address Question Answering (QA) for biograph-
ical questions, i.e., questions asking for biographi-
cal facts about persons. The domain of biographical
documents differs from other restricted domains in
that the available collections of biographies are in-
herently incomplete: a major challenge is to answer
questions about persons for whom biographical in-
formation is not present in biography collections.
We present BioGrapher, a biographical QA system
that addresses this problem by machine learning al-
gorithms for biography classification. BioGrapher
first attempts to answer a question by searching in
a given collection of biographies, using techniques
tailored for the restricted nature of the domain. If
a biography is not found, BioGrapher attempts to
find an answer on the web: it retrieves documents
using a web search engine, filters these using the bi-
ography classifier, and then extracts answers from
documents classified as biographies. Our empirical
results show that biographical classification, prior to
answer extraction, improves the results.
1 Introduction
Although most current research in question answer-
ing (QA) is oriented towards open domains, as
witnessed by evaluation exercises such as TREC,
CLEF, and NTCIR, various significant applications
concern restricted domains, e.g., software manuals.
In restricted domains, a QA system faces questions
and documents that exhibit less variation in lan-
guage use (e.g., words and fixed phrases, more spe-
cific terminology) than in an open domain, and it
could access high-quality knowledge sources that
cover the entire domain. Open domain QA as it
is assessed at TREC, CLEF, and NTCIR concerns
a broad variety of fairly restricted question types,
such as location questions, monetary questions, bi-
ography questions, questions that ask for concept
definitions, etc. How useful or effective is it to
adopt a restricted domain approach to some of these
question types? In this paper we explore so-called
biographical questions, e.g., Who was Algar Hiss?
or Who is Sir John Hale?, i.e., questions that de-
mand answers consisting of biograpical key facts
that are typically found in biographies and that typ-
ically involve fixed phrases about, e.g., birthdates,
education, societal roles. This type of questions was
found to be quite frequent in search engine logs. We
believe that biographical questions can be usefully
viewed as defining a restricted domain for QA: the
domain of biographical information as represented
by biographies.
Ideally, biographical questions are answered by
retrieving a biography from some existing collec-
tion of biographies (such as biography.com)
and extracting snippets from it. Such resources,
however, have a limited coverage. There will al-
ways be people whose biographical information is
not contained in any of the existing collections.
This necessitates retrieval of ?biography-like? doc-
uments, i.e., documents with biographical informa-
tion. The problem of identifying biography-like
documents by machine learning algorithms turns
out to be a challenging but rewarding task as we will
see below.
In this paper we address the problem of question
answering within the biographical domain. We de-
scribe BioGrapher, a restricted domain QA system
for answering bibliographical questions in which a
baseline approach, that exploits biography collec-
tions, is extended with a trainable biography classi-
fier operating on the web in order to enhance cov-
erage. The baseline system helps us understand the
usefulness of existing high quality biography col-
lections within a QA system. The extension of our
baseline approach concerns the problem of identi-
fying biography-like documents, and the extraction
from such documents of answers for questions that
could not be answered using biography collections.
A main challenge lies in constructing an algorithm
for identifying documents containing usefull bio-
graphical information that may provide an answer
for a given question. To addresss this challenge, we
explore two machine learning algorithms: Ripper
(Cohen and Singer, 1996) and Support-Vector Ma-
chines (Joachims, 1998).
Section 2 provides some background, and in Sec-
tion 3 we briefly describe our baseline QA sys-
tem, based on external knowledge sources comple-
mented with a naive approach to retrieving biogra-
phy snippets using a web search engine. In Sec-
tion 4 we prepare the ground for our text classifica-
tion experiments. In Section 5 we present two clas-
sifiers: one loosely based on the Ripper algorithm
and the other based on an SVM classifier. In Sec-
tion 6 we compare the performanc of the baseline
against versions of the system integrated with the
two classifiers. Section 7 discusses the results and
considers the possibility of applying our approach
to other restricted domains. We conclude in Sec-
tion 8.
2 Related Work
Two kinds of related work are relevant to this pa-
per: question answering against external knowledge
sources, and genre detection (using classifiers). We
briefly discuss both.
Many QA research groups employed External
Knowledge Sources in order to improve perfor-
mance. For instance, (Chu-Carroll and Prager,
2002) used WordNet to answer what is questions,
using the isa hierarchy supported by WordNet.
(Hovy et al, 2002; Lin, 2002) used dictionaries
such as WordNet and web search results to re-rank
answers. (Yang et al, 2003) preformed structure
analysis of the knowledge obtained from WordNet
and the Web in order to further improve perfor-
mance.
We refer to (Sebastiani, 2002) for extensive re-
view about machine learning in automated text clas-
sification. (Lewis, 1992) were among the first to
use machine learning for genre detection trying to
categorize Reuters articles to predefined categories.
Probabilistic classifiers were used by many groups
(Lewis, 1998). Much current text classification re-
search is focused on Support Vector Machines, first
used for genre detection by (Joachims, 1998).
3 A Na??ve Baseline
In this section we describe our baseline QA system.
This system was used at TREC 2003, to produce an-
swers to so-called person definition questions (Jij-
koun et al, 2004; Voorhees, 2004). We present the
results and give a short analysis of the system?s per-
formance; as we will see, this provides further mo-
tivation for the use of text classification for identi-
fying biography-like documents.
Definition questions at TREC 2003
The QA track at TREC 2003 featured a subtask
devoted to definition questions. The latter came
in three flavors: person definitions (e.g., Who is
Colin Powell?), organization definitions (e.g., What
is the U.N.?), and concept definitions (e.g., What is
goth?). Here, we are only interested person defini-
tions.
In response to a definition question, systems had
to return an unordered set of snippets; each snippet
was supposed to be a facet in the definition of the
target. There were no limits placed on either the
length of an individual answer string or on the num-
ber of snippets in the list, although systems were
penalized for retrieving extraneous information.
As our primary strategy for handling person def-
inition questions, we consulted external resources.
The main resource used is biography.com.
While such resources contain biographies of many
historical and well-known people, they often lack
biographies of contemporary people that are not too
well-known. To be able to deal with such cases we
backed-off to using a web search engine (Google),
and applied a na??ve heuristic approach. We hand-
crafted a set of features (such as ?born?, ?gradu-
ated?, ?suffered?, etc.) that we felt would trigger
for biography-like snippets. Various subsets of the
large feature set, together with the target of the def-
inition question, were combined to form queries for
the web search engine.
Given a set of candidate answer snippets, we per-
formed two filtering steps before presenting the final
answer: we separated non-relevant snippets from
valuable snippets and we identified semantically-
close snippets. We addressed the first step by ana-
lyzing the distances between query terms submitted
to the search engine and the sets of features, and by
means of shallow syntactic aspects of the different
features such as sentence boundaries. To address
the second step we developed a snippet similarity
metric based on stemming, stopword removal and
keyword overlap by sorting and calculating the Lev-
enshtein distance measure of similarity.1. An ex-
ample of the snippets filtering can be found in Ta-
ble 1. The table presents 3 of the returned snippets
for the question Who is Sir John Hale?. The first
and third snippet are filtered out, the first one for
non-relevancy and the third for its semantic similar-
ity with the second, shorter, snippet.
1The Levenshtein measure is a measure of the similarity be-
tween two strings, which are refered to as the source string s
and the target string t. The distance is the number of deletions,
insertions, or substitutions required to transform s into t
1 Sir Malcolm Bradbury (writer/teacher) Dead.
Heart trouble. . . . Heywood Hale Broun (com-
mentator, writer ) ? Dead. John Brunner (au-
thor) Dead. Stroke. . . . Description: Debunks
the rumor of his death. . .
2 . . . Professor Sir John Hale woke up, had . . . For
her birthday in 1996, he wrote on the . . . John
Hale died in his sleep - possibly following an-
other stroke . . .
3 Observer On 29 July 1992, Professor Sir John
Hale woke up . . . her birthday in 1996, he wrote
on the . . . John Hale died in his sleep - possibly
following another stroke.
Table 1: Snippets filtering for Who is Sir John Hale?
Evaluation
Evaluation of individual person definition questions
was done using the F-measure: F = (?2 + 1)P ?
R)/(?2P + R), where P is precision (to be de-
fined shortly), R is recall (to be defined shortly),
and ? was set to 5, indicating that precision was
more important than recall. Length was used as a
crude approximation to precision; it gives a system
an allowance of 100 (non-white-space) characters
for each correct snippet it retrieved. The precision
score is set to one if the response is no longer than
this allowance, otherwise it is downgraded using the
function P = 1? ((length ? allowance)/length).
As to recall, for each question, the TREC asses-
sors marked some snippets as vital and the remain-
der as non-vital. The non-vital snippets act as ?don?t
care? condition. That is, systems should be penal-
ized for not retrieving vital nuggets, and for retriev-
ing snippets that are not in the assessors? snippet
lists at all, but should be neither penalized nor re-
warded for returning a non-vital snippet. To im-
plement the ?don?t care? condition, snippet recall is
computed only over vital snippets (Voorhees, 2004).
In total, 30 person definition questions were eval-
uated at the TREC 2003 QA track. The overall F
score of a run was obtained by averaging over all
the individual questions.
Results and Analysis
The F score obtained by the naive system described
in this section, on the TREC 2003 person definition
questions, was 0.392. An analysis of the results
shows that, for questions that could be answered
from external biography resources, the baseline sys-
tem obtains an F score of 0.586.
In post-submission experiments we changed the
subsets of features we use in the queries sent to
Google as well as the number of queries/subsets we
use. The snippet similarity threshold was also tuned
in order to filter out more snippets. This resulted in
fewer unanswered questions, while the average an-
swer length was decreased as well, by close to 50%.
All in all, an informal evaluation showed increase in
recall, precision and in the overall F score.
From our experience with our baseline system
we learned the following important lesson: having
a (relatively) small number of high quality biogra-
phy sources as a basis for each question?s answer
is far better than using a broad and large variety of
snippets returned by a web search engine. While
extending available biography resources so as to se-
riously boost their coverage is not a feasible option,
we want to do the next best thing: make sure we
identify good biography-like documents online, so
that we can use these to mine snippets from; to this
end we will use text classification.
4 Preparing for Text Classification
In the previous section we suggested that using a
text classifier might improve the performance of bi-
ography QA. Using text classifiers, we aim to iden-
tify biography-like documents from which we can
extract answers. In this section we detail the docu-
ment representations on which we will operate.
Document and Text Representation
Text classifiers represent a document as a set of fea-
tures d = {f1, f2,. . . , fn} where n is the number of
active features, that is, features that occur in the doc-
ument. A feature f can be a word, a set of words, a
stem or any phrasal structure, depending on its text
representation. Each feature has a weight, usually
representing the number of occurrences of this fea-
ture in the document.
What is a suitable abstract representation of doc-
uments for our biography domain? We have defined
7 clusters, groups of words (terms/tokens) with a
high degree of pair-wise semantic relatedness. Each
cluster has a meta-tag symbol (as can be seen in Ta-
ble 2) and all occurrences of members of a cluster
were substituted by the cluster?s meta-tag. An ex-
ample of a document abstraction can be found in Ta-
ble 3. This abstraction captures typical similarities
between biographical strings; e.g., for the two sen-
tences John Kennedy was born in 1917 and William
Shakespeare was born in 1564 we get the same ab-
straction <NAME> <NAME> was born in <YEAR>.
It is worth noting that some of the clusters,
such as <CAP> and <PLACE>, <CAP> and <PN>
and others may overlap. Looking at the exam-
ple in Table 3, we see that Abbey was born in
Chicago, Illinois, but the automatic abstractor mis-
interpreted the token ?Ill.,? marking it is <CAP> for
capitalized (possibly meaningful) word, but not as
<NAME> the name of the subject of the biography
<YEAR> four digits surrounded by white space,
probably a year
<D> sequence of number of digits other than
four digits, can be part of a date, age etc.
<CAP> a capitalized word in the middle of a
sentence that wasn?t substituted by any
other tag
<PN> a proper name that is not the subject of
the biography It substitutes any name
out of a list of thousand names
<PLACE> denotes a name of a place, city or coun-
try out of a list of more than thousand
places
<MONTH> denotes one of the twelve months
<NOM> denotes a nominative
Table 2: Seven meta-tags used for document ab-
straction
<PLACE>. A similar thing happens with the name
?Wooldridge? that is not very common; it should
have been <PN> instead of <CAP>.
All procedures described below are preformed on
abstract-meta-tagged documents.
5 Identifying Biography Documents
Given a document, the task of a biography classifier
is to decide whether a given document is a biogra-
phy or not. In this section we address the problem
of acquiring biography classifiers by training ma-
chine learning algorithms on data. We present two
biography classification algorithms: a naive classi-
fier based on Ripper (Cohen and Singer, 1996), and
another based on SVM (Joachims, 1998). The two
methods differ radically both in the way they rep-
resent the training data (i.e., document representa-
tion), and in their learning approaches. The naive
classifier is obtained by a repetitive rule learning al-
Original Lincoln, Abbey (b. Anna Marie
Wooldridge) 1930 ? Jazz singer, com-
poser/arranger, movie actress; born in
Chicago, Ill. While a teenager she
sang at school and church functions
and then toured locally with a dance
band.
Abstraction <NAME>, <NAME> ( b . <PN> <CAP>
<CAP> ) <YEAR> - <CAP> singer ,
composer/arranger , movie actress ;
born in <PLACE> <CAP> . While
a teenager <NOM> sang at school and
church functions and then toured lo-
cally with a dance band .
Table 3: Abstraction of jazz singer Abbey Wool-
ridge?s biography
gorithm. We modified this algorithm to specifically
fit the task of identifying biographies. The SVM
learns ?linear decision boundaries? between the dif-
ferent classes. We employ here the implementation
of SVMs by (Joachims, 1998). Next we discuss the
details of how each algorithm was used for learning
a biography classifier.
Naive Classifier
We employ this algorithm for its simplicity and scal-
ability. This algorithm learns user-friendly rules,
i.e., human-readable conjunctions of propositions,
which can be converted to queries for a Boolean
search engine. Furthermore, it is known to exhibit
relatively good results across a wide variety of class-
fication problems, including tasks that involve large
collections of noisy data, similar to the large doc-
ument collections that we face in definitional QA.
The naive classifier consists of two main stages:
(1) Rules building. This is similar to Ripper?s first
stage of building an initial rule set. Our algorithm
deviates from standard implementations of Ripper
in that the terms that serve as the literals in the
rules are n-grams of various lengths. We feel that
n-grams, as opposed to individual literals (as in (Co-
hen and Singer, 1996), better capture contextual ef-
fects, which could be crucial in text classification.
Our learner learns the rules as follows. The set of
k-most frequent n-grams representing the training
documents is split into two frequency-ordered lists:
TLP (term-list-positive) containing the positive ex-
ample set and TLN (term-list-negative) containing
the negative examples set. The vector ~w is initial-
ized to be TLP/(TLP ? TLN), i.e., the most fre-
quent n-grams extracted from the positive set that
are not top frequent in the negative set.
(2) Rule optimization. Instead of Ripper?s rule
pruning stage, our algorithm assigns a weight to
each rule/n-gram r in the rules vector according
to the formula g(n)?f(r)C , where g(n) is an increas-
ing function in the length of the n-gram (longer n-
grams receive higher weights), f(r) is the ratio of
the frequency of r in the positive examples to its
frequency in the negative examples, and C is the
size of the training set. The normalization by C
is merely for the purpose of tracking variations of
the weights in different sizes of training sets. The
preference for longer n-grams can be justified by
the intuition that longer n-grams are more informa-
tive as they stand for stricter contexts. For example,
the string ?(<NAME> , <NAME> born in <YEAR>?
seems more informative than the shorter string in
<YEAR>).
Training material. The corpus we used as our
training set is a collection of 350 biographies. Most
of the biographies were randomly sampled from
biography.com, while the rest were collected
from the web. About 130 documents from the New
York Times (NYT) 2000 collection were randomly
selected as negative example set. The volumes of
the positive and negative sets are equal.
Various considerations played a role in building
this corpus. The biographies from biography.
com are ?clean? in the sense that all of them were
written as biographies. To enable the learning of
features of informal biographies, some other ?noisy?
biographies such as biography-like newspaper re-
views were added. Furthermore, a small number of
different biographies of the same person were man-
ually added in order to enforce variation in style.
We also added a small number of biographies from
other different sources to avoid any bias towards the
biography.com domain.
Validation and tuning. We tuned the naive algo-
rithm on a separate validation set of documents. The
validation set was collected in the same way as the
training set. It contained 60 biographies, of which
40 were randomly sampled from biography.
com, 10 ?clean? biographies were collected from
various online sources, 10 other documents were
noisy biographies such as newspaper reviews. In
addition, another 40 non-biographical documents
were randomly retrieved from the web.
The vector ~w is now used to rank the documents
of the validation set V in order to set a threshold
? that minimizes the false-positive and the false-
negative errors. Each document dj ? V in the val-
idation set is represented by a vector ~x, where xi
counts the occurrences of wi in dj . The score of the
document is the normalized inner product of ~x and
~w given by the function score(dj) = ~x?~wlength(dj) .
In the validation stage some heuristic modifica-
tions were applied by the algorithm. For example,
when the person name tag is absent, the document
gets the score of zero even though other parameters
of the vector may be present. We also normalized
document scores by document length.
Support Vector Machines (SVMs)
Now we describe the learning of a biography clas-
sifier using SVMs. Unlike many other classifiers,
SVMs are capable of learning classification even of
non-linearly-separable classes. It is assumed that
classes that are non-linearly separable in one di-
mension may be linearly separable in higher di-
mension. SVMs offer two important advantages
for text classification (Joachims, 1998; Sebastiani,
2002): (1) Term selection is often not needed, as
SVMs tend to be fairly robust to overfitting and
can scale up to considerable dimensionalities, and
(2) No human and machine effort in parameter tun-
ing on a validation set is needed, as there is a the-
oretically motivated ?default? choice of parameter
settings which have been shown to provide best re-
sults.
The key idea behind SVMs is to boost the dimen-
sion of the representation vectors and then to find
the best line or hyper-plane from the widest set of
parallel hyper-planes. This hyper-plane maximizes
the distance between two elements in the set. The
elements in the set are the support vectors. Theo-
retically, the classifier is determined by a very small
number of examples defining the category frontier,
the support vectors. Practically, finding the support
vectors is not a trivial task.
Training SVMs. The implementation used is
SVM-light v.5 (Joachims, 1999). The classifier was
run with its default setting, with linear kernel func-
tion and no kernel optimization tricks. The SVM-
light was trained on the very same (meta-tagged)
training corpus the naive classifier was trained on.
Since SVM is supposed to be robust and to fit big
and noisy collections, no feature selection method
was applied. The special feature underlying SVMs
is the high dimensional representation of a docu-
ment, allowing categorization by a hyper-plane of
high dimension; therefore each document was rep-
resented by the vector of its stems. The dimen-
sion was boosted to include all the stems from the
positive set. The boosted vector dimension was
7935, the number of different stems in the collec-
tion. The number of support vectors discovered was
17, which turned out to be too small for this task.
Testing this model on the test set (the same test set
used to test the naive classifier from previous sec-
tion) yielded very poor results. It seemed that the
classification was totally random. Testing the classi-
fier on smaller subsets of the training set (200, 300,
400 documents) exposed signs of convergence, sug-
gesting the training set is too sparse for the SVM.
To overcome sparse data, more documents were
needed. The size of the training set was more than
doubled. A total of 9968 documents was used as
the training set. Just like the original training set,
most of the biographies were randomly extracted
from biography.com, while a few dozen bi-
ographies were manually extracted from various
online sources to correct for a possible bias in
biography.com. Training SVM-light on the
new training set yielded 232 support vectors, which
seems enough to perform this classification tasks.
6 Experimental Results
In order to test the effectiveness of the biography
classifiers in improving question answering, we in-
tegrated each one of them with the naive baseline
biographical QA system and tested the integrated
system, called BioGrapher (Figure 1). Before dis-
cussing the results of this experiment, we briefly
mention how the two classifiers performed on the
pure classification task. For this purpose, we cre-
ated a test set including 47 documents that were re-
trieved from the web. The evaluation measure was
the accuracy of the classifiers in recognizing biogra-
phies. The Ripper-based algorithm achieved 89%
success, outranking the SVM which achieved 83%.
A discussion of this difference is beyond the scope
of this paper (see (Tsur, 2003) for details).
We tested BioGrapher on 11 out of the 30 bio-
graphical questions in the TREC 2003 QA track.
Those 11 questions were chosen as a test set be-
cause the baseline system (Section 3) scored poorly
on them, suggesting that our baseline heuristics are
incapable of effectively dealing with this type of
questions.
Question
Question 
analyzer
Snippets filter
Answer 
snippets
Biography 
collection
Web
Retrieval 
engine
Document 
classifier
Figure 1: BioGrapher system overview
Two experiments were carried out, one for the
Ripper-based classifier and another for the SVM-
based one. For each definitional question Biogra-
pher submits two simple queries to a web search
engine (e.g., Sir John Hale and Sir John Hale biog-
raphy). It retrieves the top 20 documents returned
by the search engine, thus obtaining, for each ques-
tion, 40 documents amongst which it should find
a biography. BioGrapher then classifies the doc-
uments into biographies and non-biographic doc-
uments. The distribution of documents that were
classified as biographies can be found in Table 4.
To simplify the experiments, and especially the
Question Naive Classifier SVM
Who is Alberto Tomba? 2 4
Who is Albert Ghiorso? 8 2
Who is Alexander Pope? 13 11
Who is Alice Rivlin? 3 2
Who is Absalom? 2 3
Who is Nostradamus? 1 1
Who is Machiavelli? 3 6
Who is Andrea Bocceli? 1 1
Who is Al Sharpton? 2 6
Who is Aga Khan? 4 1
Who is Ben Hur? 2 1
Table 4: Distribution of documents retrieved (in-
cluding false-positive)
error analysis, we set up BioGrapher to return an-
swer snippets from a single biography or biography-
like document only. Recall, the test questions
were such that there were no biographies for the
question targets in the biography collection we
used (biography.com): the biographies used
were ones that BioGrapher identified on the web.
We evaluated BioGrapher in the following manner.
The assessor first determines whether the document
from which BioGrapher extracts answer snippets is
a proper biography or not. In case the document is
not a pure biography the F-score given to this ques-
tion is zero. Otherwise, the F-score was determined
in the manner described in Section 3.2
BioGrapher with the Ripper-based Classifier
The total number of documents that were classi-
fied as biographies is 41 (out of 440 retrieved docu-
ments). However, analysis of the results reveals that
the false positive ratio is high; only 20 of the 41 cho-
sen documents were proper biography-like pages,
the other ?biographies? were very noisy.
For 4 out of the 11 test questions, a proper
biography was returned as the top ranking docu-
ment. While all 4 questions scored 0 at the origi-
nal TREC evaluation, now their average F-score is
0.732, improving the average F-score over all biog-
raphy questions by 9.6% to 0.4659.
BioGrapher with the SVM Classifier
The total number of documents that were classi-
fied as biographies is 38 (out of 440 retrieved docu-
2Obviously, the F-score for snippets extracted from doc-
uments incorrectly classified as biographies could be higher
than zero because these documents could still contain valuable
pieces of biographical information that would contribute to the
answer?s F-score. However, we decided to compute precision
and recall only for snippets extracted from documents correctly
classified as biographies as we think of the biography classi-
fier as a means to identify (?on-the-fly?) quality documents that
could in principle be added to a biography collection.
ments). However, just like in the case of the Ripper-
based classifier, an analysis of the results reveals
that the false positive ratio is high; only 18 of the
38 chosen documents were biography-like.
The SVM classifier managed to return proper bi-
ographies (as top ranking documents) for 5 out of
11 questions. The average F-score for those ques-
tions is 0.674 instead of the original zero, improving
the average F-score over all biography questions by
9.7% to 0.4665.
No biographies at all were retrieved for 4 of the
11 definition targets in the test set, the same four
definition targets for which the Ripper-based classi-
fier did not find biographies. A closer look reveals
the same problems as with the Ripper-based classi-
fier: a relatively high false-positive error ratio and
weak ranking of the classified biographies.
7 Discussion
The results of the experiments using both classifiers
are quite similar. The system integrated with the
SVM-based classifier achieved a slightly higher F-
score but it still falls within the standard deviation.
Our experiment serves as a proof of concept for the
hypothesis that using text classification methods im-
proves the baseline QA system in a principled way.
In spite of the major improvement in the system?s
performance, we have found two main problems
with the classifiers. First, although the classifiers
managed to identify biography-like documents, they
have a high false-positive ratio and too many er-
rors in filtering out some of the non-pure-biography
documents. This happens when the documents re-
trieved by the web search engine simply cannot be
regarded as clean biographies by human assessors,
although they do contain many biographic details.
Second, most of the definition targets had biogra-
phies retrieved and even classified as biographies,
but the biographies were ranked below other noisy
biographical documents, therefore the best biogra-
phy was not presented as a source from which to
extract answer snippets. There are various obvi-
ous paths to improve over the current system: (1)
Improve the classifiers by better training and other
classification algorithms; (2) Enable the extraction
of answers from ?noisy? biography-like documents
in such a way that the gain in recall is not reversed
by a loss of precision; and (3) Allow for the extrac-
tion of answer snippets from multiple biography-
like documents, while avoiding to return overlap-
ping snippets.
8 Conclusion
In this paper we have addressed the problem of bi-
ographical question answering. The main challenge
in this restricted domain is the fact that the available
collections of biography documents are (unavoid-
ably) too small to admit answering all biographi-
cal questions. We use the web as a backoff source
for finding biography-like documents from which to
extract answers in case a given biography collec-
tion does not contain information about the ques-
tion target. We demonstrated the benefits of inte-
grating a text classifier into a restricted domain QA
system as a filter to web retrieval. Finally, we be-
lieve that the use of text classifiers can be benefi-
cial for definitional QA, especially for identifying
documents from which the final answer should be
extracted. Future work will address the weakness
of the current implementation: improved biography
classification and improved answer extraction from
biography-like documents.
Acknowledgments
Maarten de Rijke was supported by the Nether-
lands Organization for Scientific Research (NWO)
under project numbers 612-13-001, 365-20-
005, 612.069.006, 612.000.106, 220-80-001,
612.000.207, and 612.066.302.
References
J. Chu-Carroll and J. Prager. 2002. Use of Word-
Net hypernyms for answering what-is questions.
In Proceedings of the Tenth Text REtrieval Con-
ference (TREC 2001). NIST Special Publication
500-250.
W. Cohen and Y. Singer. 1996. Context sensitive
learning methods. In Proceedings of the 19th
ACM International Conference on Research and
Development in Information Retrieval (SIGIR-
96), pages 307?315. ACM Press.
E. Hovy, U. Hermjakob, and C.Y. Lin. 2002. The
use of external knowledge in factoid QA. In Pro-
ceedings of the Tenth Text REtrieval Conference
(TREC 2001). NIST Special Publication 500-
250.
V. Jijkoun, G. Mishne, C. Monz, M. de Rijke,
S. Schlobach, and O. Tsur. 2004. The Univer-
sity of Amsterdam at the TREC 2003 Question
Answering Track. In E.M. Voorhees, editor, Pro-
ceedings TREC 2003. NIST Special Publication
SP 500-255.
T. Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many rele-
vant features. In Proceedings of ECML-98, 10th
European Conference on Machine Learning.
T. Joachims. 1999. Svm-light v.5, making large-
scale svm learning practical. advances in kernel
methods - support vector learning. B. Scholkopf
and C. Burges and A. Smola (ed.) MIT-Press.
D.D. Lewis. 1992. Representation and learning
in information retrieval. Ph.D. thesis, Graduate
School of the University of Maassachusetts.
D.D. Lewis. 1998. Naive (bayes) at forty: The in-
dependence assumption in information retrieval.
In Proceedings of the 10th European Conference
on Machine Learning, pages 137?142. Springer-
Verlag.
C.Y. Lin. 2002. The effectiveness of dictionary and
web based answer reranking. In The 19th Inter-
national Conference on Computational Linguis-
tics (COLING 2002).
F. Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
O. Tsur. 2003. Definitional question answering
using trainable text classifiers. Master?s thesis,
ILLC, University of Amsterdam.
E.M. Voorhees. 2004. Overview of the TREC 2003
question answering track. In Text REtrieval Con-
ference (TREC 2004). NIST Special Publication:
SP 500-255.
H. Yang, T.-S. Chua, S. Wang, and C.-K. Koh.
2003. Structured use of external knowledge for
event-based open domain question answering.
In Proceedings of the 26th annual international
ACM SIGIR conference on Research and de-
velopment in informaion retrieval, pages 33?40.
ACM Press.
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9?16,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
Using Classifier Features for Studying the Effect of Native Language on the
Choice of Written Second Language Words
Oren Tsur
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
oren@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
www.cs.huji.ac.il/?arir
Abstract
We apply machine learning techniques to
study language transfer, a major topic in
the theory of Second Language Acquisition
(SLA). Using an SVM for the problem of
native language classification, we show that
a careful analysis of the effects of various
features can lead to scientific insights. In
particular, we demonstrate that character bi-
grams alone allow classification levels of
about 66% for a 5-class task, even when con-
tent and function word differences are ac-
counted for. This may show that native lan-
guage has a strong effect on the word choice
of people writing in a second language.
1 Introduction
While advances in NLP achieve improved results for
NLP applications such as machine translation, ques-
tion answering and document summarization, there
are other fields of research that can benefit from the
methods used by the NLP community. Second Lan-
guage Acquisition (SLA), a major area in Applied
Linguistics and Cognitive Science, is one such field.
In this paper we demonstrate how modern machine
learning tools can contribute to SLA theory. In par-
ticular, we address the major SLA topic of language
transfer, the effect of native language on second lan-
guage learners. Using an SVM for the computa-
tional problem of native language classification, we
study in detail the effects of various SVM features.
Surprisingly, character bi-grams alone lead to a clas-
sification accuracy of about 66% in a 5-class task,
even when accounting for differences in content and
function words.
This result leads us to form a novel hypothesis on
the role of language transfer in SLA: that the choice
of words people make when writing in a second lan-
guage is strongly influenced by the phonology of
their native language.
As far as we know, this is the first time that such
a hypothesis has beed formulated. Moreover, this is
the first statistical learning-supported hypothesis in
language transfer. Our results should be further sub-
stantiated by additional psycholinguistic and com-
putational experiments; nonetheless, we provide a
strong starting point.
The next section provides some essential back-
ground. In Section 3 we describe our experimen-
tal setup and feature selection, and in Section 4 we
detail an array of variations of experiments for rul-
ing out some possible types of bias that might have
affected the results. In Section 5 we discuss our hy-
pothesis in the context of psycho-linguistic theory.
We conclude with directions for future research.
2 Background
Our hypothesis is tested within an algorithm ad-
dressing the practical problem of determining the
native language of an anonymous writer writing in a
foreign language. The problem is applicable to dif-
ferent fields, such as language instructing, tailored
error correction, security applications and psycho-
linguistic research.
As background, we start from the somewhat re-
lated problem of authorship attribution. The au-
thorship attribution problem was addressed by lin-
9
guists and other literary experts trying to pinpoint
an anonymous author, such as that of The Federalist
Papers (Holmes and Forsyth, 1995). Traditionally,
authorship experts analyzed topics, stylistic idiosyn-
crasies and personal information about the possible
candidates in order to determine an author.
While authorship is usually addressed with deep
human inspection of the texts in question, it has al-
ready been shown that automatic text analysis based
on various stylistic features can identify the gender
of an anonymous author with accuracy above 80%
(Argamon et al 2003). Various papers (Diedrich et
al, 2003; Koppel and Schler, 2003; Koppel et al
2005a; Stamatatos et al 2004) report relative suc-
cess in machine based authorship attribution tasks
for small sets of known candidates.
Native language detection is a harder problem
than the authorship attribution problem, since we
wish to characterize the writing style of a set of
writers rather than the unique style of a single
person. There are several works presenting non-
native speech recognition and dialect analysis sys-
tems (Bouselmi et al 2005; Bouselmi et al 2006;
Hansen et al 2004). However, all those works are
based on acoustic signals, not on written texts.
Koppel et al(2005a) report an accuracy of 80% in
the task of determining a writer?s native language.
To the best of our knowledge, this is the only pub-
lished work on automated classification of an au-
thor?s native language (along with another version
of the paper by the same authors (Koppel et al
2005b)). Koppel et alused an SVM (Scho?lkopf and
Smola, 2002) and a combination of features in their
system (such as errors analysis and POS-error co-
occurrences, as described in section 2.2), but sur-
prisingly, it appears that a very naive set of features
achieves a relatively high accuracy. The charac-
ter bi-gram frequencies feature performs rather well,
and definitely outperforms the intuitive contribution
of frequent bigrams in this type of task.
3 Experimental Setting
3.1 The Corpus
The corpus that served for all of the experiments
described in this paper is the International Corpus
of Learner English (ICLE) (Granger et al 2002),
which was also the one used by Koppel et al(2005a;
2005b). The corpus was compiled for the purpose of
studying the English writing of non-native speakers.
All contributors to the corpus are advanced English
students and are roughly the same age. The corpus is
combined from a number of sub-corpora, each con-
taining one native language. The corpus was assem-
bled in ten years of international collaboration be-
tween a number of universities and it contains more
than 2 million words of writing by students from 19
different native language backgrounds. We followed
Koppel et al(2005a) and worked on 5 sub-corpora,
each containing 238 randomly selected essays by na-
tive speakers of the following languages: Bulgarian,
Czech, French, Russian and Spanish. Each of the
texts in the corpus was written by a different author
and is of length between 500 to 1,000 words. Each
of the sub corpora contains about 180,000 (unique)
types, for a total of 886,677 tokens.
Essays in the corpus are of two types: argumen-
tative essays and literature examination papers. De-
scriptive, narrative or technical subjects were not in-
cluded in the corpus. The literature examination es-
says were restricted to no more than 25% of each
sub-corpus. Each contributor was requested to fill a
learner profile that was used to fine-proof the corpus
as needed.
In order to verify our results we used another con-
trol corpus containing the Dutch and Italian sub-
corpora contained in the ICLE instead of the Bul-
garian and French ones.
3.2 Document Representation
In the original experiment by Koppel et al(2005a)
each document was represented by a numerical vec-
tor of 1,035 dimensions. Each vector entry rep-
resented the frequency (relative to the document?s
length) of a given feature. The features were of 4
types:
? 400 function words
? 200 most frequent letter n-grams
? 250 rare POS bi-gram
? 185 error types
While the first three types of attributes are relatively
straightforward, the fourth is more complex. It rep-
resents clusters of families of spelling errors as well
as co-occurrences of errors and POS tags. Document
10
representation is described in detail in (Koppel et al
2005a; Koppel et al 2005b).
A multi-class SVM (Witten and Frank, 2005) was
employed for learning and evaluating the classifica-
tion model. The experiment was run in a 10-fold
cross validation manner in order to test the effec-
tiveness of the model.
3.3 Previous Results
Koppel et al(2005a) report that when all features
types were used in tandem, an accuracy of 80.2%
was achieved. In the discussion section they an-
alyze the frequency of a few function words, er-
ror types, the co-occurrences of POS tags and er-
rors, and the co-occurrences of POS tags and certain
function words that seem to have significance in the
support vectors learnt by the SVM.
The goal of their research was to obtain the best
classification, therefore the results obtained by us-
ing only bi-grams of characters were not particularly
noted, although, surprisingly, representing each doc-
ument by only using the relative frequency of the
top 200 characters bi-grams achieves an accuracy of
about 66%. We believe that this surprising fact ex-
poses some fundamental phenomenon of human lan-
guage behavior. In the next section we describe a set
of experiments designed to isolate the causes of this
phenomenon.
4 Experimental Variations and Results
Intuitively, we do not expect the most frequent char-
acter n-grams to serve as good native language pre-
dictors, expecting that these will only reflect the
most frequent English words (and characters se-
quences). Accordingly, without language transfer
effects, a naive baseline classifier based on an n-
gram model is expected to achieve about 20% ac-
curacy in a 5 native languages classification task.
However, using classification based on the relative
frequency of top 200 bi-grams achieves about 66%1
in all experiments, substantially higher than the ran-
dom baseline. These results are so surprising that
they suggest that the characters bi-grams classifi-
cation masks some other bias or noise in the cor-
pus, or, conversely, that it mirrors other simple-to-
1Koppel et aldid not report these results explicitly. How-
ever, they can be roughly estimated from their graph.
Figure 1: Classification accuracy of the different
variations of document representation. b-g: bi-
grams, f-w: function words, c-w: content words.
explain phenomena such as shallow language trans-
fer through the use of function words, or content
bias. The following sub-sections describe different
variations of the experiment, ruling out the effect of
these different types of bias.
4.1 Unigram Baseline
We first implemented a naive baseline classifier. We
represented each document by the normalized fre-
quencies of the (de-capitalized) letters it contains2.
These frequencies are simply a unigram model of
the sub-corpora. Using the multi-class SVM (Wit-
ten and Frank, 2005) we obtained 46.78% accu-
racy. This accuracy is more than twice the ran-
dom baseline accuracy. This result is in accordance
with our bi-grams results. Our discussion focuses on
bi-grams rather than unigrams because the former?s
results are much higher and because bi-grams are
much closer to the phonology of the language (for
alphabetic scripts, of course).
4.2 Bi-grams Based Classification
Choosing the 200 most frequent character bi-grams
in the corpus, we used a vector of the same dimen-
sion. Each vector entry contained the normalized
frequency of one of the bi-grams. Using a multi-
class SVM in a 10-fold cross validation manner we
2White spaces were considered a letter. However, sequences
of white spaces and tabs were collapsed to a single white space.
All the experiments that make use of character frequencies were
performed twice, including and excluding punctuation marks.
Results for both experiments are similar, therefore all the num-
bers reported in this paper are based on letters and punctuation
marks.
11
Bulg. Czech French Russian Spanish
dr 170 183 n/a 195 n/a
am 117 135 142 140 152
m 121 120 133 119 139
iv 104 138 144 148 148
y 161 181 196 183 166
la 122 123 122 142 105
Table 1: Some of the separating bi-grams found in
the feature selection process. ? ? indicates a white
space. The numbers are the frequency ranking of
the bi-grams in each sub-corpus (e.g., there are 103
bi-grams more frequent than ?iv? in the Bulgarian
corpus). n/a indicates that this bi-gram is not one of
the 200 most frequent bi-grams of the sub-corpus.
achieved 65.60% accuracy with standard deviation
of 3.99.
The bi-grams features in the 200 dimensional vec-
tor are the 200 most frequent bi-grams in the whole
corpus, regardless of their frequency in each sub-
corpus. We note that the effect of misspelled words
on the 200 most frequent bi-grams is negligible.
A more sophisticated feature selection could re-
duce the dimension of the representation vector
without detracting from the results. Careful fea-
ture selection can also give a better intuition regard-
ing the support vectors. We performed feature se-
lection in the following manner: we chose the top
200 bi-grams of each sub-corpus, getting 245 unique
bi-grams in total. We then chose all the bi-grams
that were ranked significantly higher or significantly
lower in one language than in at least one other
language, assuming that those bi-grams have strong
separating power. With the threshold of significance
set to 20 we obtained 84 separating bi-grams. Table
1 shows some of the separating bi-grams thus found.
For example, ?la? is a good separator between Rus-
sian and Spanish (its rank in the Spanish corpus is
much higher than that in the Russian corpus), but
not between other pairs.
Using only those 84 bigrams we obtained clas-
sification accuracy of 61.38%, a drop of only 4%
compared to the results achieved with the 200 di-
mensional vectors. These results show that increas-
ing the dimension of the representation vector using
additional bi-grams contribute a marginal improve-
ment while it does not introduce substantial noise.
4.3 Using Tri-gram Frequencies as Features
Repeating the same experiment with the top 200 tri-
grams, we obtained an accuracy of 59.67%, which
is 40% higher than the expected baseline and 15%
higher than the uni-grams baseline. These results
show that the texts in our corpus can be classified
by only using naive n-gram models, while the op-
timal n of the n-gram is a different question that
might be addressed in a different work (and might
be language-dependent).
4.4 Function Words Based Classification
Function words are words that have a little lexical
meaning but instead serve to express grammatical
relations within a sentence or specify the attitude of
the speaker (function words should not be confused
with stopwords, although the lists of most frequent
function words and the stopword list share a large
subset). We used the same list of 460 function words
used by Koppel et al(2005a). A partial list includes:
{a, afterward, although, because, cannot, do, enter,
eventually, fifteenth, hither, hath, hence, lastly, oc-
casionally, presumable, said, seldom, undoubtedly,
was}.
In this variation of the experiment, we represented
each document only by the relative frequencies of
the function words it contained. Using the same
experimental setup as before, we achieved an ac-
curacy of 66.7%. These results are less surprising
than the results obtained by the character n-grams
vectors, since we do expect native speakers of a cer-
tain language to use, misuse or ignore certain func-
tion words as a result from language transfer mech-
anisms (Odlin, 1989). For example, it is well known
that native speakers of Russian tend to omit English
articles.
4.5 Function Words Bias
The previous results suggest that the n-gram based
classification is simply the result of the different
uses of function words by speakers of different na-
tive languages. In order to rule out the effect of the
function words on the bi-gram-based classification,
we removed all function words from the corpus, re-
calculated the bi-gram frequencies and ran the ex-
periment once again, this time achieving an accuracy
of 62.92% in the 10-fold cross validation test.
12
These results, obtained on the function words-free
corpus, clearly show that n-gram based classification
is not a mere artifact masking the use of function
words.
4.6 Content Bias
Bi-gram frequencies could also reflect content bias
rather than language use. By content bias we mean
that the subject matter of the documents in the dif-
ferent sub-corpora could exhibit internal sub-corpus
uniformity and external sub-corpus disparity. In or-
der to rule this out, we employed a variation on the
Term Frequency ? Inverted Document Frequency
(tf-idf ) content analysis metric.
The tf-idf measure is a statistical measure that is
used in information retrieval tasks to evaluate how
important a word/term is to a document in a collec-
tion or corpus (Salton and Buckley, 1988). Given a
collection of documents D, the tf-idf weight of term
t in a document d ? D is computed as follows:
tfidft = ft,d ? log
|D|
ft,D
where ft,d is the frequency of term t in document
d, and ft,D is the number of documents in which t
appears. Therefore, the weight of term t ? d is max-
imal if it is a common term in d while the number of
documents it appears in is relatively low.
We used the tf-idf weights in the information re-
trieval sense in order to discover the dominant con-
tent words of each sub-corpus. We treated each sub-
corpus (set of documents by writers who share a
native language) as a single document and calcu-
lated the tf-idf of each word. In order to determine
whether there is a content bias or not, we set a domi-
nance threshold, and removed all words such that the
difference between their tf-idf score in two different
sub-corpora is higher than the dominance threshold.
Given a threshold t, the dominance Dw,t, of a token
w is given by:
Dw,t = maxi,j |tfidfw,i ? tfidfw,j |
where tfidfw,k is the tf-idf score of token w in
sub-corpus k. Changing the threshold in 0.0005 in-
tervals, we removed from 1 to 340 unique content
words (between 1,545 and 84,725 word tokens in to-
tal). However, the classification accuracy was essen-
tially the same (see Figure 2), with a slight drop of
Word Bulg. Czech Fr. Rus. Spa.
europe 0 0.3 2.7 0.2 0.2
european 0 0.3 3 0.1 0.5
imagination 4.3 2 0.8 1 0.8
television 0 3.6 1.9 3.1 0.3
women 0.4 1.7 1.2 5.5 2.6
Table 2: The tf-idf score of some of the most domi-
nant words, multiplied by 1,000 for easier reading.
Subcorpus content function unique
words words stems
Bulgarian 1543 94685 11325
Czech 2784 110782 12834
French 2059 67016 9474
Russian 2730 112410 12338
Spanish 2985 108052 12627
Total 12101 492945 36474
Table 3: Numbers of dominant content words (with
a threshold of 0.0025) and function words that were
removed from each sub-corpus. The unique stems
column indicates the number of unique stems (types)
that remained after removal of c-w and f-w.
only 2% after removing 51 content words (by using
a threshold of 0.0015).
We calculated the tf-idf weights after stop-words
removal and stemming (using a Porter stemmer
(Porter, 1980)), trying to pinpoint dominant stems.
The results were similar to the word?s tf-idf and no
significantly dominant stem was found in either of
the sub-corpora.
A drop of only 3% in accuracy was noticed after
removing both dominant content words and function
words. These results show that if a content bias ex-
ists in the corpus it has only a minor effect on the
SVM classification, and that the n-grams based clas-
Figure 2: Classification accuracy as a function of the
threshold (removed content words).
13
Thresh. 0.004 0.003 0.0025 0.0015 0.0012 c-w 9 c-w 15 c-w 51 c-w 113 c-w
Bulg. 77 908 1543 3955 7426
Czech 306 1829 2784 5139 8588
French 665 1829 2059 3603 6205
Russian 781 1886 2730 6302 9918
Spanish 389 1418 2985 6548 10521
Total 2218 7970 12101 25547 42658
Table 4: Number of occurrences of content words
that were removed from each sub-corpus for some
of the thresholds. The numbers in the top row indi-
cate the threshold and the number of unique content
words that were found with this threshold.
sification is not an artifact of a content bias.
We ran the same experiment five more times, each
time on 4 sub-corpora instead of 5, removing one
(different) language each time. The results in all 5
4-class experiments were essentially the same, and
similar to those of the 5 language task (beyond the
fact that the random baseline for the former is 25%
rather than 20%).
4.7 Suffix Bias
Bias might also be attributed to the use of suf-
fixes. There are numerous types of English suf-
fixes, which, roughly speaking, may be categorized
as derivational or inflectional. It is reasonable to ex-
pect that just like a use of function words, use or mis-
use of certain suffixes might occur due to language
transfer. Frequent use of a certain suffix or avoid-
ance of the use of a certain suffix may influence the
bi-grams statistics and thus the bi-grams classifica-
tion may be only an artifact of the suffixes usage.
Checking the use of the 50 most productive suf-
fixes taken from a standard list (e.g. ing, ed, less,
able, most, en) we have found that only a small num-
ber of suffixes are not equally used by speakers of all
5 languages. Most notable are the differences in the
use of ing between native French speakers and na-
tive Czech speakers and the differences of use of less
between Bulgarian and Spanish speakers (Table 5).
However, no real bias can be attributed to the use of
any of the suffixes because their relative aggregate
effect on the values in the support vector entries is
very small.
Suffix Bulg. Czech French Russian Spanish
ing 872 719 932 903 759
less 47 36 39 45 32
Table 5: Counts of two of the suffixes whose fre-
quency of use differs the most between sub-corpora.
4.8 Control Corpus
Finally, we have also ran the experiment on a differ-
ent corpus replacing the French and the Spanish sub-
corpora by the Dutch and Italian ones, introducing a
new Roman language and a new Germanic language
to the corpus. We obtained 64.66% accuracy, essen-
tially the same as in the original 5-language setting.
The corpus was compiled from works of advanced
English students of the same level who write essays
of approximately the same length, on a set of ran-
domly and roughly equally distributed topics. We
expected that these students will use roughly the
same n-grams distribution. However, the results de-
scribed above suggest that there exists some mecha-
nism that influences the authors? choice of words. In
the next section we present a computational psycho-
linguistic framework that might explain our results.
5 Statistical Learning and Language
Transfer in SLA
5.1 Statistical Learning by Infants
Psychologists, linguists, and cognitive science re-
searchers try to understand the process of language
learning by infants. Many models for language
learning and cognitive language modeling were sug-
gested (Clark, 2003).
Infants learn their first language by a combina-
tion of speech streams, vocal cues and body ges-
tures. Infants as young as 8 months old have a
limited grasp of their native tongue as they react
to familiar words. In that age they already under-
stand the meaning of single words, they learn to spot
these words in a speech stream, and very soon they
learn to combine different words into new sentential
units. Parental speech stream analysis shows that it
is impossible to separate between words by identi-
fying sequences of silence between words (Saffran,
2001). Recent studies of infant language learning
are in favor of the statistical framework (Saffran,
2001; Saffran et al 1996). Saffran (2002) exam-
14
ined 8 month-old to one year-old infants who were
stimulated by speech sequences. The infants showed
a significant discrimination between word and non-
word stimuli. In a different experimental setup in-
fants showed a significant discrimination between
frequent syllable n-grams and non frequent sylla-
ble n-grams, heard as part of a gibberish speech se-
quence generated by a computer according to var-
ious statistical language models. In a third experi-
mental setup infants showed a significant discrimi-
nation in favor of English-like gibberish speech se-
quences upon non-English-like gibberish speech se-
quences. These findings along with the established
finding (Jusczyk, 1997) that infants prefer the sound
of their native tongue suggest that humans learn ba-
sic language units in a statistical manner and that
they store some statistical parameters pertaining to
these units. We should note that some researchers
doubt these conclusions (Yang, 2004).
5.2 Language Transfer in SLA
The role of the first language in second language ac-
quisition is under a continuous debate (Ellis, 1999).
Language Transfer between L1 and L2 is the pro-
cess in which a language learner of L2 whose na-
tive language is L1, is influenced by L1 when using
L2 (actually, when building his/her inter-language).
This influence might appear helpful when L2 is rel-
atively close to L1, but it interferes with the learn-
ing process due to over- and under-generalization or
other problems. Although there is clear evidence
that language learners use constructs of their first
language when learning a foreign language (James,
1980; Odlin, 1989), it is not clear that the majority
of learner errors can be attributed to the L1 transfer
(Ellis, 1999).
5.3 Sound Transfer Hypothesis
For alphabetic scripts, character bi-grams reflect ba-
sic sounds and sound sequences of the language3.
We have shown that native language strongly corre-
lates with character bi-grams when people write in
English as a second language. After ruling out usage
of function words, content bias, and morphology-
related influences, the most plausible explanation is
3Note that for English, they do not directly correspond to
phonemes or syllables. Nonetheless, they do reflect English
phonology to some extent.
that these are language transfer effects related to L1
sounds.
We hypothesize that there are language transfer
effects related to L1 sounds and manifested by the
words that people choose to use when writing in a
second language. (We say ?writing? because we have
only experimented with written texts; a more gen-
eral hypothesis covering speaking and writing can
be formulated as well.)
Furthermore, since the acquisition and represen-
tation of phonology is strongly influenced by statis-
tical considerations (Section 5.1), we speculate that
the general language transfer phenomenon might be
related to frequency. This does not directly follow
from our findings, of course, but is an exciting direc-
tion to investigate, and it is in accordance with the
growing body of work on the effects of frequency
on language learning and the emergence of syntax
(Ellis, 2002; Bybee, 2006).
We note that there is one obvious and well-known
lexical transfer effect: the usage of cognates (words
that have similar form (sound) and meaning in two
different languages). However, the languages we
used in our experiments contain radically differing
amounts of cognates of English words (just consider
French vs. Bulgarian, for example), while the clas-
sification results were about the same for all 5 lan-
guages. Hence, cognates might play a role, but they
do not constitute a single major explaining factor for
our findings.
We note that the hypothesis put forward in the
present paper is the first that attributes a language
transfer phenomenon to a cognitive representation
(phonology) whose statistical nature has been seri-
ously substantiated.
6 Conclusion
In this paper we have demonstrated how modern ma-
chine learning can aid other fields, here the impor-
tant field of Second Language Acquisition (SLA).
Our analysis of the features useful for a multi-class
SVM in the task of native language classification has
resulted in the formulation of a hypothesis of poten-
tial significance in the theory of language transfer
in SLA. We hypothesize language transfer effects at
the level of basic sounds and short sound sequences,
manifested by the words that people choose when
15
writing in a second language. In other words, we
hypothesize that use of L2 words is strongly influ-
enced by L1 sounds and sound patterns.
As noted above, further experiments (psycholog-
ical and computational) must be conducted for vali-
dating our hypothesis. In particular, construction of
a wide-scale learners? corpus with tight control over
content bias is essential for reaching stronger con-
clusions.
Additional future work should address sound se-
quences vs. the orthographic sequences that were
used in this work. If our hypothesis is correct, then
using spoken language corpora should produce even
stronger results, since (1) writing systems rarely
show a 1-1 correspondence with how words are at
the phonological level; and (2) writing allows more
conscious thinking that speaking, thus potentially re-
duces transfer effects. Our eventual goal is creating
a unified model of statistical transfer mechanisms.
References
Argamon S., Koppel M. and Shimoni A. 2003. Gender,
Genre, and Writing Style in Formal Written Texts. Text
23(3).
Bouselmi G., Fohr D., Illina, I., and Haton J.P.
2005. Fully Automated Non-Native Speech Recog-
nition Using Confusion-Based Acoustic Model. Eu-
rospeech/Interspeech ?05.
Bouselmi G., Fohr D., Illina I., and Haton J.P. 2006.
Fully Automated Non-Native Speech Recognition Us-
ing Confusion-Based Acoustic Model Integration and
Graphemic Constraints. IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
2006.
Bybee J. 2006. Frequency of Use and the Organization
of Language. Oxford University Press.
Clark, E. 2003. First Language Acquisition. Cambridge
University Press.
Diederich J., Kindermann J., Leopold E. and Paass G.
2004. Authorship Attribution with Support Vector Ma-
chines. Applied Intelligence, 109?123.
Ellis N. 2002. Frequency Effects in Language Pro-
cessing. Studies in Second Language Acquisition,
24(2):143?188.
Ellis R. 1999. Understanding Second Language Acqui-
sition. Oxford University Press.
Granger S., Dagneaux E. and Meunier F. 2002. Inter-
national Corpus of Learner English. Presses universi-
taires de Louvain.
Hansen J. H., Yapanel U., Huang, R. and Ikeno A. 2004.
Dialect Analysis and Modeling for Automatic Classi-
fication. Interspeech-2004/ICSLP-2004: International
Conference Spoken Language Processing. Jeju Island,
South Korea.
Holmes D. and Forsyth R. 1995. The Federalist Revis-
ited: New Directions in Authorship Attribution. Liter-
ary and Linguistic Computing, pp. 111?127.
James C. E. 1980. Contrastive Analysis. New York:
Longman.
Jusczyk P. W. 1997. The Discovery of Spoken Language.
MIT Press.
Koppel M. and Schler J. 2003. Exploiting Stylistic Id-
iosyncrasies for Authorship Attribution. In Proceed-
ings of IJCAI ?03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis. Acapulco,
Mexico.
Koppel M., Schler J. and Zigdon K. 2005(a). Determin-
ing an Author?s Native Language by Mining a Text for
Errors. Proceedings of KDD ?05. Chicago IL.
Koppel M., Schler J. and Zigdon K. 2005(b). Auto-
matically Determining an Anonymous Author?s Native
Language. In Intelligence and Security Informatics
(pp. 209?217). Berlin / Heidelberg: Springer.
Odlin T. 1989. Language Transfer: Cross-Linguistic In-
fluence in Language Learning. Cambridge University
Press.
Porter F. M. 1980. An Algorithm for Suffix Stripping.
Program, 14(3):130?137.
Saffran J. R. 2001. Words in a Sea of Sounds: The Output
of Statistical Learning. Cognition, 81, 149?169.
Saffran J. R. 2002. Constraints on Statistical Language
Learning. Journal of Memory and Language, 47, 172?
196.
Saffran J. R., Aslin R. N. and Newport E. N. 1996. Sta-
tistical Learning by 8-month Old Infants. Science, is-
sue 5294, 1926?1928.
Salton G. and Buckley C. 1988. Term Weighing Ap-
proaches in Automatic Text Retrieval. Information
Processing and Management, 24(5):513?523.
Scho?lkopf B,. Smola A 2002. Learning with Kernels.
MIT Press.
Stamatatos E,. Fakotakis N. and Kokkinakis G. 2004.
Computer-Based Authorship Attribution Without Lex-
ical Measures. Computers and the Humanities, 193?
214.
Witten I. H. and Frank E. 2005. Data Mining: Practical
Machine Learning Tools and Techniques. San Fran-
cisco: Morgan Kaufmann.
Yang C. 2004. Universal Grammar, Statistics, or Both?.
Trends in Cognitive Science 8(10):451?456, 2004.
16
Coling 2010: Poster Volume, pages 241?249,
Beijing, August 2010
Enhanced Sentiment Learning Using Twitter Hashtags and Smileys
Dmitry Davidov? 1 Oren Tsur? 2
1ICNC / 2Institute of Computer Science
The Hebrew University
{oren,arir}@cs.huji.ac.il
Ari Rappoport 2
Abstract
Automated identification of diverse sen-
timent types can be beneficial for many
NLP systems such as review summariza-
tion and public media analysis. In some of
these systems there is an option of assign-
ing a sentiment value to a single sentence
or a very short text.
In this paper we propose a supervised
sentiment classification framework which
is based on data from Twitter, a popu-
lar microblogging service. By utilizing
50 Twitter tags and 15 smileys as sen-
timent labels, this framework avoids the
need for labor intensive manual annota-
tion, allowing identification and classifi-
cation of diverse sentiment types of short
texts. We evaluate the contribution of dif-
ferent feature types for sentiment classifi-
cation and show that our framework suc-
cessfully identifies sentiment types of un-
tagged sentences. The quality of the senti-
ment identification was also confirmed by
human judges. We also explore dependen-
cies and overlap between different sen-
timent types represented by smileys and
Twitter hashtags.
1 Introduction
A huge amount of social media including news,
forums, product reviews and blogs contain nu-
merous sentiment-based sentences. Sentiment is
defined as ?a personal belief or judgment that
?* Both authors equally contributed to this paper.
is not founded on proof or certainty?1. Senti-
ment expressions may describe the mood of the
writer (happy/sad/bored/grateful/...) or the opin-
ion of the writer towards some specific entity (X
is great/I hate X, etc.).
Automated identification of diverse sentiment
types can be beneficial for many NLP sys-
tems such as review summarization systems, dia-
logue systems and public media analysis systems.
Sometimes it is directly requested by the user to
obtain articles or sentences with a certain senti-
ment value (e.g Give me all positive reviews of
product X/ Show me articles which explain why
movie X is boring). In some other cases obtaining
sentiment value can greatly enhance information
extraction tasks like review summarization. While
the majority of existing sentiment extraction sys-
tems focus on polarity identification (e.g., positive
vs. negative reviews) or extraction of a handful of
pre-specified mood labels, there are many useful
and relatively unexplored sentiment types.
Sentiment extraction systems usually require
an extensive set of manually supplied sentiment
words or a handcrafted sentiment-specific dataset.
With the recent popularity of article tagging, some
social media types like blogs allow users to add
sentiment tags to articles. This allows to use blogs
as a large user-labeled dataset for sentiment learn-
ing and identification. However, the set of senti-
ment tags in most blog platforms is somewhat re-
stricted. Moreover, the assigned tag applies to the
whole blog post while a finer grained sentiment
extraction is needed (McDonald et al, 2007).
With the recent popularity of the Twitter micro-
blogging service, a huge amount of frequently
1WordNet 2.1 definitions.
241
self-standing short textual sentences (tweets) be-
came openly available for the research commu-
nity. Many of these tweets contain a wide vari-
ety of user-defined hashtags. Some of these tags
are sentiment tags which assign one or more senti-
ment values to a tweet. In this paper we propose a
way to utilize such tagged Twitter data for classi-
fication of a wide variety of sentiment types from
text.
We utilize 50 Twitter tags and 15 smileys as
sentiment labels which allow us to build a clas-
sifier for dozens of sentiment types for short tex-
tual sentences. In our study we use four different
feature types (punctuation, words, n-grams and
patterns) for sentiment classification and evaluate
the contribution of each feature type for this task.
We show that our framework successfully identi-
fies sentiment types of the untagged tweets. We
confirm the quality of our algorithm using human
judges.
We also explore the dependencies and overlap
between different sentiment types represented by
smileys and Twitter tags.
Section 2 describes related work. Section 3
details classification features and the algorithm,
while Section 4 describes the dataset and labels.
Automated and manual evaluation protocols and
results are presented in Section 5, followed by a
short discussion.
2 Related work
Sentiment analysis tasks typically combine two
different tasks: (1) Identifying sentiment expres-
sions, and (2) determining the polarity (sometimes
called valence) of the expressed sentiment. These
tasks are closely related as the purpose of most
works is to determine whether a sentence bears a
positive or a negative (implicit or explicit) opinion
about the target of the sentiment.
Several works (Wiebe, 2000; Turney, 2002;
Riloff, 2003; Whitelaw et al, 2005) use lexical re-
sources and decide whether a sentence expresses
a sentiment by the presence of lexical items (sen-
timent words). Others combine additional feature
types for this decision (Yu and Hatzivassiloglou,
2003; Kim and Hovy, 2004; Wilson et al, 2005;
Bloom et al, 2007; McDonald et al, 2007; Titov
and McDonald, 2008a; Melville et al, 2009).
It was suggested that sentiment words may have
different senses (Esuli and Sebastiani, 2006; An-
dreevskaia and Bergler, 2006; Wiebe and Mihal-
cea, 2006), thus word sense disambiguation can
improve sentiment analysis systems (Akkaya et
al., 2009). All works mentioned above identify
evaluative sentiment expressions and their polar-
ity.
Another line of works aims at identifying a
broader range of sentiment classes expressing var-
ious emotions such as happiness, sadness, bore-
dom, fear, and gratitude, regardless (or in addi-
tion to) positive or negative evaluations. Mihalcea
and Liu (2006) derive lists of words and phrases
with happiness factor from a corpus of blog posts,
where each post is annotated by the blogger with
a mood label. Balog et al (2006) use the mood
annotation of blog posts coupled with news data
in order to discover the events that drive the dom-
inant moods expressed in blogs. Mishne (2005)
used an ontology of over 100 moods assigned
to blog posts to classify blog texts according to
moods. While (Mishne, 2005) classifies a blog en-
try (post), (Mihalcea and Liu, 2006) assign a hap-
piness factor to specific words and expressions.
Mishne used a much broader range of moods.
Strapparava and Mihalcea (2008) classify blog
posts and news headlines to six sentiment cate-
gories.
While most of the works on sentiment analy-
sis focus on full text, some works address senti-
ment analysis in the phrasal and sentence level,
see (Yu and Hatzivassiloglou, 2003; Wilson et al,
2005; McDonald et al, 2007; Titov and McDon-
ald, 2008a; Titov and McDonald, 2008b; Wilson
et al, 2009; Tsur et al, 2010) among others.
Only a few studies analyze the sentiment and
polarity of tweets targeted at major brands. Jansen
et al (2009) used a commercial sentiment ana-
lyzer as well as a manually labeled corpus. Davi-
dov et al (2010) analyze the use of the #sarcasm
hashtag and its contribution to automatic recogni-
tion of sarcastic tweets. To the best of our knowl-
edge, there are no works employing Twitter hash-
tags to learn a wide range of emotions and the re-
lations between the different emotions.
242
3 Sentiment classification framework
Below we propose a set of classification features
and present the algorithm for sentiment classifica-
tion.
3.1 Classification features
We utilize four basic feature types for sentiment
classification: single word features, n-gram fea-
tures, pattern features and punctuation features.
For the classification, all feature types are com-
bined into a single feature vector.
3.1.1 Word-based and n-gram-based features
Each word appearing in a sentence serves as a
binary feature with weight equal to the inverted
count of this word in the Twitter corpus. We also
took each consecutive word sequence containing
2?5 words as a binary n-gram feature using a sim-
ilar weighting strategy. Thus n-gram features al-
ways have a higher weight than features of their
component words, and rare words have a higher
weight than common words. Words or n-grams
appearing in less than 0.5% of the training set sen-
tences do not constitute a feature. ASCII smileys
and other punctuation sequences containing two
or more consecutive punctuation symbols were
used as single-word features. Word features also
include the substituted meta-words for URLs, ref-
erences and hashtags (see Subsection 4.1).
3.1.2 Pattern-based features
Our main feature type is based on surface pat-
terns. For automated extraction of patterns, we
followed the pattern definitions given in (Davidov
and Rappoport, 2006). We classified words into
high-frequency words (HFWs) and content words
(CWs). A word whose corpus frequency is more
(less) than FH (FC) is considered to be a HFW
(CW).We estimate word frequency from the train-
ing set rather than from an external corpus. Unlike
(Davidov and Rappoport, 2006), we consider all
single punctuation characters or consecutive se-
quences of punctuation characters as HFWs. We
also consider URL, REF, and HASHTAG tags as
HFWs for pattern extraction. We define a pattern
as an ordered sequence of high frequency words
and slots for content words. Following (Davidov
and Rappoport, 2008), the FH and FC thresholds
were set to 1000 words per million (upper bound
for FC) and 100 words per million (lower bound
for FH )2.
The patterns allow 2?6 HFWs and 1?5 slots for
CWs. To avoid collection of patterns which cap-
ture only a part of a meaningful multiword ex-
pression, we require patterns to start and to end
with a HFW. Thus a minimal pattern is of the
form [HFW] [CW slot] [HFW]. For each sentence
it is possible to generate dozens of different pat-
terns that may overlap. As with words and n-gram
features, we do not treat as features any patterns
which appear in less than 0.5% of the training set
sentences.
Since each feature vector is based on a single
sentence (tweet), we would like to allow approx-
imate pattern matching for enhancement of learn-
ing flexibility. The value of a pattern feature is
estimated according the one of the following four
scenarios3:
?
????????????????????????
????????????????????????
1
count(p) : Exact match ? all the pattern components
appear in the sentence in correct
order without any additional words.
?
count(p) : Sparse match ? same as exact match
but additional non-matching words can
be inserted between pattern components.
??n
N?count(p) : Incomplete match ? only n > 1 of N
pattern components appear in
the sentence, while some non-matching
words can be inserted in-between.
At least one of the appearing components
should be a HFW.
0 : No match ? nothing or only a single
pattern component appears in the sentence.
0 ? ? ? 1 and 0 ? ? ? 1 are parameters we use
to assign reduced scores for imperfect matches.
Since the patterns we use are relatively long, ex-
act matches are uncommon, and taking advantage
of partial matches allows us to significantly re-
duce the sparsity of the feature vectors. We used
? = ? = 0.1 in all experiments.
This pattern based framework was proven effi-
cient for sarcasm detection in (Tsur et al, 2010;
2Note that the FH and FC bounds allow overlap between
some HFWs and CWs. See (Davidov and Rappoport, 2008)
for a short discussion.
3As with word and n-gram features, the maximal feature
weight of a pattern p is defined as the inverse count of a pat-
tern in the complete Twitter corpus.
243
Davidov et al, 2010).
3.1.3 Efficiency of feature selection
Since we avoid selection of textual features
which have a training set frequency below 0.5%,
we perform feature selection incrementally, on
each stage using the frequencies of the features
obtained during the previous stages. Thus first
we estimate the frequencies of single words in
the training set, then we only consider creation
of n-grams from single words with sufficient fre-
quency, finally we only consider patterns com-
posed from sufficiently frequent words and n-
grams.
3.1.4 Punctuation-based features
In addition to pattern-based features we used
the following generic features: (1) Sentence
length in words, (2) Number of ?!? characters in
the sentence, (3) Number of ??? characters in the
sentence, (4) Number of quotes in the sentence,
and (5) Number of capitalized/all capitals words
in the sentence. All these features were normal-
ized by dividing them by the (maximal observed
value times averaged maximal value of the other
feature groups), thus the maximal weight of each
of these features is equal to the averaged weight
of a single pattern/word/n-gram feature.
3.2 Classification algorithm
In order to assign a sentiment label to new exam-
ples in the test set we use a k-nearest neighbors
(kNN)-like strategy. We construct a feature vec-
tor for each example in the training and the test
set. We would like to assign a sentiment class to
each example in the test set. For each feature vec-
tor V in the test set, we compute the Euclidean
distance to each of the matching vectors in the
training set, where matching vectors are defined as
ones which share at least one pattern/n-gram/word
feature with v.
Let ti, i = 1 . . . k be the k vectors with low-
est Euclidean distance to v4 with assigned labels
Li, i = 1 . . . k. We calculate the mean distance
d(ti, v) for this set of vectors and drop from the set
up to five outliers for which the distance was more
then twice the mean distance. The label assigned
4We used k = 10 for all experiments.
to v is the label of the majority of the remaining
vectors.
If a similar number of remaining vectors have
different labels, we assigned to the test vector the
most frequent of these labels according to their
frequency in the dataset. If there are no matching
vectors found for v, we assigned the default ?no
sentiment? label since there is significantly more
non-sentiment sentences than sentiment sentences
in Twitter.
4 Twitter dataset and sentiment tags
In our experiments we used an extensive Twit-
ter data collection as training and testing sets. In
our training sets we utilize sentiment hashtags and
smileys as classification labels. Below we de-
scribe this dataset in detail.
4.1 Twitter dataset
We have used a Twitter dataset generously pro-
vided to us by Brendan O?Connor. This dataset
includes over 475 million tweets comprising
roughly 15% of all public, non-?low quality?
tweets created from May 2009 to Jan 2010.
Tweets are short sentences limited to 140 UTF-
8 characters. All non-English tweets and tweets
which contain less than 5 proper English words5
were removed from the dataset.
Apart of simple text, tweets may contain URL
addresses, references to other Twitter users (ap-
pear as @<user>) or a content tags (also called
hashtags) assigned by the tweeter (#<tag>)
which we use as labels for our supervised clas-
sification framework.
Two examples of typical tweets are: ?#ipad
#sucks and 6,510 people agree. See more on Ipad
sucks page: http://j.mp/4OiYyg??, and ?Pay no
mind to those who talk behind ur back, it sim-
ply means that u?re 2 steps ahead. #ihatequotes?.
Note that in the first example the hashtagged
words are a grammatical part of the sentence (it
becomes meaningless without them) while #ihate-
qoutes of the second example is a mere sentiment
label and not part of the sentence. Also note that
hashtags can be composed of multiple words (with
no spaces).
5Identification of proper English words was based on an
available WN-based English dictionary
244
Category # of tags % agreement
Strong sentiment 52 87
Likely sentiment 70 66
Context-dependent 110 61
Focused 45 75
No sentiment 3564 99
Table 1: Annotation results (2 judges) for the 3852 most
frequent tweeter tags. The second column displays the av-
erage number of tags, and the last column shows % of tags
annotated similarly by two judges.
During preprocessing, we have replaced URL
links, hashtags and references by URL/REF/TAG
meta-words. This substitution obviously had
some effect on the pattern recognition phase (see
Section 3.1.2), however, our algorithm is robust
enough to overcome this distortion.
4.2 Hashtag-based sentiment labels
The Twitter dataset contains above 2.5 million dif-
ferent user-defined hashtags. Many tweets include
more than a single tag and 3852 ?frequent? tags
appear in more than 1000 different tweets. Two
human judges manually annotated these frequent
tags into five different categories: 1 ? strong sen-
timent (e.g #sucks in the example above), 2 ?
most likely sentiment (e.g., #notcute), 3 ? context-
dependent sentiment (e.g., #shoutsout), 4 ? fo-
cused sentiment (e.g., #tmobilesucks where the
target of the sentiment is part of the hashtag), and
5 ? no sentiment (e.g. #obama). Table 1 shows
annotation results and the percentage of similarly
assigned values for each category.
We selected 50 hashtags annotated ?1? or ?2?
by both judges. For each of these tags we automat-
ically sampled 1000 tweets resulting in 50000 la-
beled tweets. We avoided sampling tweets which
include more than one of the sampled hashtags.
As a no-sentiment dataset we randomly sampled
10000 tweets with no hashtags/smileys from the
whole dataset assuming that such a random sam-
ple is unlikely to contain a significant amount of
sentiment sentences.
4.3 Smiley-based sentiment labels
While there exist many ?official? lists of possible
ASCII smileys, most of these smileys are infre-
quent or not commonly accepted and used as sen-
timent indicators by online communities. We used
the Amazon Mechanical Turk (AMT) service in
order to obtain a list of the most commonly used
and unambiguous ASCII smileys. We asked each
of ten AMT human subjects to provide at least 6
commonly used ASCII mood-indicating smileys
together with one or more single-word descrip-
tions of the smiley-related mood state. From the
obtained list of smileys we selected a subset of 15
smileys which were (1) provided by at least three
human subjects, (2) described by at least two hu-
man subject using the same single-word descrip-
tion, and (3) appear at least 1000 times in our
Twitter dataset. We then sampled 1000 tweets for
each of these smileys, using these smileys as sen-
timent tags in the sentiment classification frame-
work described in the previous section.
5 Evaluation and Results
The purpose of our evaluation was to learn how
well our framework can identify and distinguish
between sentiment types defined by tags or smi-
leys and to test if our framework can be success-
fully used to identify sentiment types in new un-
tagged sentences.
5.1 Evaluation using cross-validation
In the first experiment we evaluated the consis-
tency and quality of sentiment classification us-
ing cross-validation over the training set. Fully
automated evaluation allowed us to test the per-
formance of our algorithm under several dif-
ferent feature settings: Pn+W-M-Pt-, Pn+W+M-Pt-,
Pn+W+M+Pt-, Pn-W-M-Pt+ and FULL, where +/?
stands for utilization/omission of the following
feature types: Pn:punctuation, W:Word, M:n-
grams (M stands for ?multi?), Pt:patterns. FULL
stands for utilization of all feature types.
In this experimental setting, the training set was
divided to 10 parts and a 10-fold cross validation
test is executed. Each time, we use 9 parts as the
labeled training data for feature selection and con-
struction of labeled vectors and the remaining part
is used as a test set. The process was repeated ten
times. To avoid utilization of labels as strong fea-
tures in the test set, we removed all instances of
involved label hashtags/smileys from the tweets
used as the test set.
245
Setup Smileys Hashtags
random 0.06 0.02
Pn+W-M-Pt- 0.16 0.06
Pn+W+M-Pt- 0.25 0.15
Pn+W+M+Pt- 0.29 0.18
Pn-W-M-Pt+ 0.5 0.26
FULL 0.64 0.31
Table 2: Multi-class classification results for smileys and
hashtags. The table shows averaged harmonic f-score for 10-
fold cross validation. 51 (16) sentiment classes were used for
hashtags (smileys).
Multi-class classification. Under multi-class
classification we attempt to assign a single label
(51 labels in case of hashtags and 16 labels in case
of smileys) to each of vectors in the test set. Note
that the random baseline for this task is 0.02 (0.06)
for hashtags (smileys). Table 2 shows the perfor-
mance of our framework for these tasks.
Results are significantly above the random
baseline and definitely nontrivial considering the
equal class sizes in the test set. While still rel-
atively low (0.31 for hashtags and 0.64 for smi-
leys), we observe much better performance for
smileys which is expected due to the lower num-
ber of sentiment types.
The relatively low performance of hashtags can
be explained by ambiguity of the hashtags and
some overlap of sentiments. Examination of clas-
sified sentences reveals that many of them can
be reasonably assigned to more than one of the
available hashtags or smileys. Thus a tweet ?I?m
reading stuff that I DON?T understand again! ha-
haha...wth am I doing? may reasonably match
tags #sarcasm, #damn, #haha, #lol, #humor, #an-
gry etc. Close examination of the incorrectly
classified examples also reveals that substantial
amount of tweets utilize hashtags to explicitly in-
dicate the specific hashtagged sentiment, in these
cases that no sentiment value could be perceived
by readers unless indicated explicitly, e.g. ?De
Blob game review posted on our blog. #fun?.
Obviously, our framework fails to process such
cases and captures noise since no sentiment data
is present in the processed text labeled with a spe-
cific sentiment label.
Binary classification. In the binary classifica-
tion experiments, we classified a sentence as ei-
ther appropriate for a particular tag or as not bear-
Hashtags Avg #hate #jealous #cute #outrageous
Pn+W-M-Pt- 0.57 0.6 0.55 0.63 0.53
Pn+W+M-Pt- 0.64 0.64 0.67 0.66 0.6
Pn+W+M+Pt- 0.69 0.66 0.67 0.69 0.64
Pn-W-M-Pt+ 0.73 0.75 0.7 0.69 0.69
FULL 0.8 0.83 0.76 0.71 0.78
Smileys Avg :) ; ) X( : d
Pn+W-M-Pt- 0.64 0.66 0.67 0.56 0.65
Pn+W+M-Pt- 0.7 0.73 0.72 0.64 0.69
Pn+W+M+Pt- 0.7 0.74 0.75 0.66 0.69
Pn-W-M-Pt+ 0.75 0.78 0.75 0.68 0.72
FULL 0.86 0.87 0.9 0.74 0.81
Table 3: Binary classification results for smileys and hash-
tags. Avg column shows averaged harmonic f-score for 10-
fold cross validation over all 50(15) sentiment hashtags (smi-
leys).
ing any sentiment6. For each of the 50 (15) labels
for hashtags (smileys) we have performed a bi-
nary classification when providing as training/test
sets only positive examples of the specific senti-
ment label together with non-sentiment examples.
Table 3 shows averaged results for this case and
specific results for selected tags. We can see that
our framework successfully identifies diverse sen-
timent types. Obviously the results are much bet-
ter than those of multi-class classification, and the
observed > 0.8 precision confirms the usefulness
of the proposed framework for sentiment classifi-
cation of a variety of different sentiment types.
We can see that even for binary classification
settings, classification of smiley-labeled sentences
is a substantially easier task compared to classifi-
cation of hashtag-labeled tweets. Comparing the
contributed performance of different feature types
we can see that punctuation, word and pattern fea-
tures, each provide a substantial boost for classi-
fication quality while we observe only a marginal
boost when adding n-grams as classification fea-
tures. We can also see that pattern features con-
tribute the performance more than all other fea-
tures together.
5.2 Evaluation with human judges
In the second set of experiments we evaluated our
framework on a test set of unseen and untagged
tweets (thus tweets that were not part of the train-
6Note that this is a useful application in itself, as a filter
that extracts sentiment sentences from a corpus for further
focused study/processing.
246
ing data), comparing its output to tags assigned by
human judges. We applied our framework with
its FULL setting, learning the sentiment tags from
the training set for hashtags and smileys (sepa-
rately) and executed the framework on the reduced
Tweeter dataset (without untagged data) allowing
it to identify at least five sentences for each senti-
ment class.
In order to make the evaluation harsher, we re-
moved all tweets containing at least one of the
relevant classification hashtags (or smileys). For
each of the resulting 250 sentences for hashtags,
and 75 sentences for smileys we generated an ?as-
signment task?. Each task presents a human judge
with a sentence and a list of ten possible hash-
tags. One tag from this list was provided by our
algorithm, 8 other tags were sampled from the re-
maining 49 (14) available sentiment tags, and the
tenth tag is from the list of frequent non-sentiment
tags (e.g. travel or obama). The human judge was
requested to select the 0-2 most appropriate tags
from the list. Allowing assignment of multiple
tags conforms to the observation that even short
sentences may express several different sentiment
types and to the observation that some of the se-
lected sentiment tags might express similar senti-
ment types.
We used the Amazon Mechanical Turk service
to present the tasks to English-speaking subjects.
Each subject was given 50 tasks for Twitter hash-
tags or 25 questions for smileys. To ensure the
quality of assignments, we added to each test five
manually selected, clearly sentiment bearing, as-
signment tasks from the tagged Twitter sentences
used in the training set. Each set was presented to
four subjects. If a human subject failed to provide
the intended ?correct? answer to at least two of
the control set questions we reject him/her from
the calculation. In our evaluation the algorithm
is considered to be correct if one of the tags se-
lected by a human judge was also selected by the
algorithm. Table 4 shows results for human judge-
ment classification. The agreement score for this
task was ? = 0.41 (we consider agreement when
at least one of two selected items are shared).
Table 4 shows that the majority of tags selected
by humans matched those selected by the algo-
rithm. Precision of smiley tags is substantially
Setup % Correct % No sentiment Control
Smileys 84% 6% 92%
Hashtags 77% 10% 90%
Table 4: Results of human evaluation. The second col-
umn indicates percentage of sentences where judges find no
appropriate tags from the list. The third column shows per-
formance on the control set.
Hashtags #happy #sad #crazy # bored
#sad 0.67 - - -
#crazy 0.67 0.25 - -
#bored 0.05 0.42 0.35 -
#fun 1.21 0.06 1.17 0.43
Smileys :) ; ) : ( X(
; ) 3.35 - - -
: ( 3.12 0.53 - -
X( 1.74 0.47 2.18 -
: S 1.74 0.42 1.4 0.15
Table 5: Percentage of co-appearance of tags in tweeter
corpus.
higher than of hashtag labels, due to the lesser
number of possible smileys and the lesser ambi-
guity of smileys in comparison to hashtags.
5.3 Exploration of feature dependencies
Our algorithm assigns a single sentiment type
for each tweet. However, as discussed above,
some sentiment types overlap (e.g., #awesome and
#amazing). Many sentences may express several
types of sentiment (e.g., #fun and #scary in ?Oh
My God http://goo.gl/fb/K2N5z #entertainment
#fun #pictures #photography #scary #teaparty?).
We would like to estimate such inter-sentiment
dependencies and overlap automatically from the
labeled data. We use two different methods for
overlap estimation: tag co-occurrence and feature
overlap.
5.3.1 Tag co-occurrence
Many tweets contain more than a single hash-
tag or a single smiley type. As mentioned, we ex-
clude such tweets from the training set to reduce
ambiguity. However such tag co-appearances can
be used for sentiment overlap estimation. We cal-
culated the relative co-occurrence frequencies of
some hashtags and smileys. Table 5 shows some
of the observed co-appearance ratios. As expected
some of the observed tags frequently co-appear
with other similar tags.
247
Hashtags #happy #sad #crazy # bored
#sad 12.8 - - -
#crazy 14.2 3.5 - -
#bored 2.4 11.1 2.1 -
#fun 19.6 2.1 15 4.4
Smileys :) ; ) : ( X(
; ) 35.9 - - -
: ( 31.9 10.5 - -
X( 8.1 10.2 36 -
: S 10.5 12.6 21.6 6.1
Table 6: Percentage of shared features in feature vectors
for different tags.
Interestingly, it appears that a relatively high
ratio of co-appearance of tags is with opposite
meanings (e.g., ?#ilove eating but #ihate feeling
fat lol? or ?happy days of training going to end
in a few days #sad #happy?). This is possibly due
to frequently expressed contrast sentiment types
in the same sentence ? a fascinating phenomena
reflecting the great complexity of the human emo-
tional state (and expression).
5.3.2 Feature overlap
In our framework we have created a set of fea-
ture vectors for each of the Twitter sentiment tags.
Comparison of shared features in feature vector
sets allows us to estimate dependencies between
different sentiment types even when direct tag co-
occurrence data is very sparse. A feature is con-
sidered to be shared between two different senti-
ment labels if for both sentiment labels there is
at least a single example in the training set which
has a positive value of this feature. In order to au-
tomatically analyze such dependencies we calcu-
late the percentage of sharedWord/n-gram/Pattern
features between different sentiment labels. Table
6 shows the observed feature overlap values for
selected sentiment tags.
We observe the trend of results obtained by
comparison of shared feature vectors is similar to
those obtained by means of label co-occurrence,
although the numbers of the shared features are
higher. These results, demonstrating the pattern-
based similarity of conflicting, sometimes contra-
dicting, emotions are interesting from a psycho-
logical and cognitive perspective.
6 Conclusion
We presented a framework which allows an au-
tomatic identification and classification of various
sentiment types in short text fragments which is
based on Twitter data. Our framework is a su-
pervised classification one which utilizes Twitter
hashtags and smileys as training labels. The sub-
stantial coverage and size of the processed Twit-
ter data allowed us to identify dozens of senti-
ment types without any labor-intensive manually
labeled training sets or pre-provided sentiment-
specific features or sentiment words.
We evaluated diverse feature types for senti-
ment extraction including punctuation, patterns,
words and n-grams, confirming that each fea-
ture type contributes to the sentiment classifica-
tion framework. We also proposed two different
methods which allow an automatic identification
of sentiment type overlap and inter-dependencies.
In the future these methods can be used for au-
tomated clustering of sentiment types and senti-
ment dependency rules. While hashtag labels are
specific to Twitter data, the obtained feature vec-
tors are not heavily Twitter-specific and in the fu-
ture we would like to explore the applicability of
Twitter data for sentiment multi-class identifica-
tion and classification in other domains.
References
Akkaya, Cem, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
EMNLP.
Andreevskaia, A. and S. Bergler. 2006. Mining word-
net for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL.
Balog, Krisztian, Gilad Mishne, and Maarten de Ri-
jke. 2006. Why are they excited? identifying and
explaining spikes in blog mood levels. In EACL.
Bloom, Kenneth, Navendu Garg, and Shlomo Arga-
mon. 2007. Extracting appraisal expressions. In
HLT/NAACL.
Davidov, D. and A. Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
COLING-ACL.
248
Davidov, D. and A. Rappoport. 2008. Unsuper-
vised discovery of generic relationships using pat-
tern clusters and its evaluation by automatically gen-
erated sat analogy questions. In ACL.
Davidov, D., O. Tsur, and A. Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In CoNLL.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In LREC.
Jansen, B.J., M. Zhang, K. Sobel, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. Journal of the American Society for Infor-
mation Science and Technology.
Kim, S.M. and E. Hovy. 2004. Determining the senti-
ment of opinions. In COLING.
McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models
for fine-to-coarse sentiment analysis. In ACL.
Melville, Prem, Wojciech Gryc, and Richard D.
Lawrence. 2009. Sentiment analysis of blogs by
combining lexical knowledge with text classifica-
tion. In KDD. ACM.
Mihalcea, Rada and Hugo Liu. 2006. A corpus-
based approach to finding happiness. In In AAAI
2006 Symposium on Computational Approaches to
Analysing Weblogs. AAAI Press.
Mishne, Gilad. 2005. Experiments with mood clas-
sification in blog posts. In Proceedings of the 1st
Workshop on Stylistic Analysis Of Text.
Riloff, Ellen. 2003. Learning extraction patterns for
subjective expressions. In EMNLP.
Strapparava, Carlo and Rada Mihalcea. 2008. Learn-
ing to identify emotions in text. In SAC.
Titov, Ivan and Ryan McDonald. 2008a. A joint
model of text and aspect ratings for sentiment sum-
marization. In ACL/HLT, June.
Titov, Ivan and Ryan McDonald. 2008b. Modeling
online reviews with multi-grain topic models. In
WWW, pages 111?120, New York, NY, USA. ACM.
Tsur, Oren, Dmitry Davidov, and Ari Rappoport.
2010. Icwsm ? a great catchy name: Semi-
supervised recognition of sarcastic sentences in
product reviews. In AAAI-ICWSM.
Turney, Peter D. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL ?02, volume 40.
Whitelaw, Casey, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In CIKM.
Wiebe, Janyce and Rada Mihalcea. 2006. Word sense
and subjectivity. In COLING/ACL, Sydney, AUS.
Wiebe, Janyce M. 2000. Learning subjective adjec-
tives from corpora. In AAAI.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399?433.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In EMNLP.
249
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1880?1891,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Authorship Attribution of Micro-Messages
Roy Schwartz Oren Tsur Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
{roys02|oren|arir}@cs.huji.ac.il
Moshe Koppel
Department of Computer Science
Bar Ilan University
koppel@macs.biu.ac.il
Abstract
Work on authorship attribution has tradition-
ally focused on long texts. In this work, we
tackle the question of whether the author of
a very short text can be successfully iden-
tified. We use Twitter as an experimental
testbed. We introduce the concept of an au-
thor?s unique ?signature?, and show that such
signatures are typical of many authors when
writing very short texts. We also present a new
authorship attribution feature (?flexible pat-
terns?) and demonstrate a significant improve-
ment over our baselines. Our results show that
the author of a single tweet can be identified
with good accuracy in an array of flavors of
the authorship attribution task.
1 Introduction
Research in authorship attribution has developed
substantially over the last decade (Stamatatos,
2009). The vast majority of such research has been
dedicated towards finding the author of long texts,
ranging from single passages to book chapters. In
recent years, the growing popularity of social me-
dia has created special interest, both theoretical and
computational, in short texts. This has led to many
recent authorship attribution projects that experi-
mented with web data such as emails (Abbasi and
Chen, 2008), web forum messages (Solorio et al,
2011) and blogs (Koppel et al, 2011b). This paper
addresses the question to what extent the authors of
very short texts can be identified. To answer this
question, we experiment with Twitter tweets.
Twitter messages (tweets) are limited to 140 char-
acters. This restriction imposes major difficulties on
authorship attribution systems, since authorship at-
tribution methods that work well on long texts are
often not as useful when applied to short texts (Bur-
rows, 2002; Sanderson and Guenter, 2006).
Nonetheless, tweets are relatively self-contained
and have smaller sentence length variance com-
pared to excerpts from longer texts (see Section 3).
These characteristics make Twitter data appealing as
a testbed when focusing on short texts. Moreover,
an authorship attribution system of tweets may have
various applications. Specifically, a range of cyber-
crimes can be addressed using such a system, includ-
ing identity fraud and phishing.
In this paper, we introduce the concept of k-
signatures. We denote the k-signatures of an author
a as the features that appear in at least k% of a?s
training samples, while not appearing in the training
set of any other author. When k is large, such signa-
tures capture a unique style used by a. An analysis
of our training set reveals that unique k-signatures
are typical of many authors. Moreover, a substantial
portion of the tweets in our training set contain at
least one such signature. These findings suggest that
a single tweet, although short and sparse, often con-
tains sufficient information for identifying its author.
Our results show that this is indeed the case.
We train an SVM classifier with a set of features
that include character n-grams and word n-grams.
We use a rigorous experimental setup, with varying
number of authors (values between 50-1,000) and
various sizes of the training set, ranging from 50 to
1,000 tweets per author. In all our experiments, a
single tweet is used as test document. We also use
a setting in which the system is allowed to respond
don?t know in cases of uncertainty. Applying this
option results in higher precision, at the expense of
1880
lower recall.
Our results show that the author of a tweet can be
successfully identified. For example, when using a
dataset of as many as 1,000 authors with 200 train-
ing tweets per author, we are able to obtain 30.3%
accuracy (as opposed to a random baseline of only
0.1%). Using a dataset of 50 authors with as few
as 50 training tweets per author, we obtain 50.7%
accuracy. Using a dataset of 50 authors with 1,000
training tweets per author, our results reach as high
as 71.2% in the standard classification setting, and
exceed 91% accuracy with 60% recall in the don?t
know setting.
We also apply a new set of features, never previ-
ously used for this task ? flexible patterns. Flexi-
ble patterns essentially capture the context in which
function words are used. The effectiveness of func-
tion words as authorship attribution features (Koppel
et al, 2009) suggests using flexible pattern features.
The fact that flexible patterns are learned from plain
text in a fully unsupervised manner makes them
domain and language independent. We demon-
strate that using flexible patterns gives significant
improvement over our baseline system. Further-
more, using flexible patterns, our system obtains a
6.1% improvement over current state-of-the-art re-
sults in authorship attribution on Twitter.
To summarize, the contribution of this paper is
threefold.
? We provide the most extensive research to date
on authorship attribution of micro-messages,
and show that authors of very short texts can
be successfully identified.
? We introduce the concept of an author?s unique
k-signature, and demonstrate that such signa-
tures are used by many authors in their writing
of micro-messages.
? We present a new feature for authorship attri-
bution ? flexible patterns ? and show its sig-
nificant added value over other methods. Us-
ing this feature, our system obtains a 6.1% im-
provement over the current state-of-the-art.
The rest of the paper is organized as follows. Sec-
tions 2 and 3 describe our methods and our experi-
mental testbed (Twitter). Section 4 presents the con-
cept of k-signatures. Sections 5 and 6 present our
experiments and results. Flexible patterns are pre-
sented in Section 7 and related work is presented in
Section 8.
2 Methodology
In the following we briefly describe the main fea-
tures employed by our system. The features below
are binary features.
Character n-grams. Character n-gram features
are especially useful for authorship attribution on
micro-messages since they are relatively tolerant
to typos and non-standard use of punctuation (Sta-
matatos, 2009). These are common in the non-
formal style generally applied in social media ser-
vices. Consider the example of misspelling ?Brit-
ney? as ?Brittney?. The misspelled name shares the
4-grams ?Brit? and ?tney? with the correct name. As
a result, these features provide information about the
author?s style (or at least her topic of interest), which
is not available through lexical features.
Following standard practice, we use 4-grams
(Sanderson and Guenter, 2006; Layton et al, 2010;
Koppel et al, 2011b). White spaces are considered
characters (i.e., a character n-gram may be com-
posed of letters from two different words). A sin-
gle white-space is appended to the beginning and
the end of each tweet. For efficiency, we consider
only character n-gram features that appear at least
tcng times in the training set of at least one author
(see Section 5).
Word n-grams. We hypothesize that word n-gram
features would be useful for authorship attribution
on micro-messages. We assume that under a strict
length restriction, many authors would prefer using
short, repeating phrases (word n-grams).
In our experiments, we consider 2 ? n ? 5.1
We regard sequences of punctuation marks as words.
Two special words are added to each tweet to indi-
cate the beginning and the end of the tweet. For effi-
ciency, we consider only word n-gram features that
appear at least twng times in the training set of at
least one author (see Section 5).
Model. We use libsvm?s Matlab implementation
of a multi-class SVM classifier with a linear kernel
1We skip unigrams as they are generally captured by the
character n-gram features.
1881
(Chang and Lin, 2011). We use ten-fold cross vali-
dation on the training set to select the best regular-
ization factor between 0.5 and 0.005.2
3 Experimental Testbed
Our main research question in this paper is to deter-
mine the extent to which authors of very short texts
can be identified. A major issue in working with
short texts is selecting the right dataset. One ap-
proach is breaking longer texts into shorter chunks
(Sanderson and Guenter, 2006). We take a differ-
ent approach and experiment with micro-messages
(specifically, tweets).
Tweets have several properties making them an
ideal testbed for authorship attribution of short texts.
First, tweets are posted as single units and do not
necessarily refer to each other. As a result, they tend
to be self contained. Second, tweets have more stan-
dardized length distribution compared to other types
of web data. We compared the mean and standard
deviation of sentence length in our Twitter dataset
and in a corpus of English web data (Ferraresi et al,
2008).3 We found that (a) tweets are shorter than
standard web data (14.2 words compared to 20.9),
and (b) the standard deviation of the length of tweets
is much smaller (6.4 vs. 21.4).
Pre-Processing. We use a Twitter corpus that in-
cludes approximately 5 ? 108 tweets.4 All non-
English tweets and tweets that contain fewer than
3 words are removed from the dataset. We also re-
move tweets marked as retweets (using the RT sign,
a standard Twitter symbol to indicate that this tweet
was written by a different user). As some users
retweet without using the RT sign, we also remove
tweets that are an exact copy of an existing tweet
posted in the previous seven days.
Apart from plain text, some tweets contain ref-
erences to other Twitter users (in the format of
@<user>). Since using reference information
makes this task substantially easier (Layton et al,
2010), we replace each user reference with the spe-
cial meta tag REF. For sparsity reasons, we also re-
place web addresses with the meta tag URL, num-
2In practice, 0.05 or 0.1 are selected in almost all cases.
3http://wacky.sslmit.unibo.it
4These comprise ?15% of all public tweets created from
May 2009 to March 2010.
0 5 10 15 20 25 30 35 40 45 >50
0
10
20
30
40
50
60
70
80
90
Number of k?signatures per user
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 1: Number of users with at least x k-signatures
(100 authors, 180 training tweets per author).
bers with the meta tag NUM, time of day with the
meta tag TIME and dates with the meta tag DATE.
4 k-Signatures
In this section, we show that many authors adopt
a unique style when writing micro-messages. This
style can be detected by a strong classification algo-
rithm (such as SVM), and be sufficient to correctly
identify the author of a single tweet.
We define the concept of the k-signature of an au-
thor a to be a feature that appears in at least k% of
a?s training set, while not appearing in the training
set of any other user. Such signatures can be useful
for identifying future (unlabeled) tweets written by
a.
To validate our hypothesis, we use a dataset of
100 authors with 180 tweets per author. We com-
pute the number of k-signatures used by each of
the authors in our dataset. Figure 1 shows our re-
sults for a range of k values (2%, 5%, 10%, 20%
and 50%). Results demonstrate that 81 users use
at least one 2%-signature, 43 users use at least one
5%-signature, and 17 users use at least one 10%-
signature. These results indicate that a large portion
of the users adopt a unique signature (or set of sig-
natures) when writing short texts. Table 1 provides
examples of 10%-signatures.
1882
Signature Type 10%-signature Examples
Character n-grams
? ? ??
REF oh ok ? ? Glad you found it!
Hope everyone is having a good afternoon ? ?
REF Smirnoff lol keeping the goose in the freezer ? ?
?yew ?
gurl yew serving me tea nooch
REF about wen yew and ronnie see each other
REF lol so yew goin to check out tini?s tonight huh???
Word n-grams
.. lal
REF aww those are cool where u get those.. how do ppl react.. lal
Ludas album is gone be hott.. lal
Dayum refs don?t get injury timeouts.. lal.. get him off the field..
smoochies , e3
I?m just back after takin? a very long, icy cold
shower........Shivering smoochies,E3 http://bit.ly/4CzzP9
A blue stout or two would be nice as well, Purr!Blue smooth
smoochies,E3 http://bit.ly/75D4fO
That is sooooooooooooooooooo unfair!Double smoochies,E3
http://bit.ly/07sXRGX
Table 1: Examples of 10%-signatures.
Results also show that seven users use one or
more 20%-signatures, and five users even use one
or more 50%-signatures. Looking carefully at these
users, we find that they write very structured mes-
sages, and are probably bots, such as news feeds,
bidding systems, etc. Table 2 provides examples of
tweets posted by such users.5
Another interesting question is how many tweets
contain at least one k-signature. Figure 2 shows
for each user the number of tweets in her training
set for which at least one k-signature is found. Re-
sults demonstrate that a total of 18.6% of the train-
ing tweets contain at least one 2%-signature, 10.3%
the training tweets contain at least one 5%-signature
and 6.5% of the training tweets contain at least one
10%-signature. These findings validate our assump-
tion that many users use k-signatures in short texts.
These findings also have direct implications on
authorship attribution of micro-messages, since k-
signatures are reliable classification features. As
a result, texts written by authors that tend to use
k-signatures are likely to be easily identified by a
reasonable classification algorithm. Consequently,
k-signatures provide a possible explanation for the
high quality results presented in this paper.
In the broader context, the presence (and contri-
5Our k-signature method can actually be useful for automat-
ically identifying such users. We defer this to future work.
0 20 40 60 80 100 120 140 160 180
0
10
20
30
40
50
60
70
80
90
Number of Tweets with at least one k?Signature
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 2: Number of users with at least x training tweets
that contain at least one k-signature (100 authors, 180
training tweets per author).
bution) of k-signatures is in line with the hypothesis
proposed by (Davidov et al, 2010a): while still us-
ing an informal and unstructured (grammatical) lan-
guage, authors tend to use typical and unique struc-
tures in order to allow a short message to stand alone
without a clear conversational context.
1883
User 20%-signature Examples
1 I?m listening to :
I?m listening to: Sigur R?s ? Intro:
http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb
I?m listening to: Tina Arena ? In Command:
http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25
I?m listening to: Midnight Oil ? Under the Overpass:
http://www.last.fm/music/Midnight+Oil http://bit.ly/7IH4cg
2 news now ( str )
#Hotel News Now(STR) 5 things to know: 27 May 2009: From the desks of
the HotelNewsNow.com editor... http://bit.ly/aZTZOq #Tourism #Lodging
#Hotel News Now(STR) Five sales renegotiating tactics: As bookings rep-
resentatives press to reneg... http://bit.ly/bHPn2L
#Hotel News Now(STR) Risk of hotel recession retreats: The Hotel Indus-
try?s Pulse Index increases... http://bit.ly/a8EKrm #Tourism #Lodging
3
( NUM bids )
end date :
NEW PINK NINTENDO DS LITE CONSOLE WITH 21 GIFTS +
CASE: &#163;66.50 (13 Bids) End Date: Tuesday Dec-08-2009 17:..
http://bit.ly/7uPt6V
Microsoft Xbox 360 Game System - Console Only - Working: US $51.99
(25 Bids) End Date: Saturday Dec-12-2009 13:.. http://bit.ly/8VgdTv
Microsoft Sony Playstation 3 (80 GB) Console 6 Months Old:
&#163;190.00 (25 Bids) End Date: Sunday Dec-13-2009 21:21:39 G..
http://bit.ly/7kwtDS
Table 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%-
signatures.
5 Experiments
We report of three different experimental configu-
rations. In the experiments described below, each
dataset is divided into training and test sets using
ten-fold cross validation. On the test phase, each
document contains a single tweet.
Experimenting with varying Training Set Sizes.
In order to test the affect of the training set size,
we experiment with an increasingly larger number
of tweets per author. Experimenting with a range of
training set sizes serves two purposes: (a) to check
whether the author of a tweet can be identified us-
ing a very small number of (short) training samples,
and (b) check howmuch our system can benefit from
training on a larger corpus.
In our experiments we only consider users who
posted between 1,000?2,000 tweets6 (a total of
6This range is selected since on one hand we want at least
1,000 tweets per author for our experiments, and on the other
hand we noticed that users with a larger number of tweets in
corpus tend to be spammers or bots that are very easy to identify,
so we limit this number to 2,000.
10,183 users), and randomly select 1,000 tweets per
user. From these users, we select 10 groups of 50
users each.7 We perform a set of classification ex-
periments, selecting for each author an increasingly
larger subset of her 1,000 tweets as training set. Sub-
set sizes are (50, 100, 200, 500, 1,000). Thresh-
old values for our features in each setting (see Sec-
tion 2) are (2, 2, 4, 10, 20) for tcng and (2, 2, 2, 3, 5)
for twng, respectively.
Experimenting with varying Numbers of Au-
thors. In a second set of experiments, we use an
increasingly larger number of authors (values be-
tween 100-1,000), in order to check whether the au-
thor of a very short text can be identified in a ?needle
in a haystack? type of setting.
Due to complexity issues, we only experiment
with 200 tweets per author as training set. We se-
lect groups of size 100, 200, 500 and 1,000 users
(one group per size). We use the same threshold val-
ues as the 200 tweets per author setting previously
described (tcng = 4, twng = 2).
7An eleventh group is selected as development set.
1884
0 100 200 300 400 500 600 700 800 900 1000
45
50
55
60
65
70
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 3: Authorship attribution accuracy for 50 authors
with various training set sizes. The values are averaged
over 10 groups. The random baseline is 2%.
Recall-Precision Tradeoff. Another aspect of our
research question is the level of certainty our system
has when suggesting an author for a given tweet.
In cases of uncertainty, many real life applications
would prefer not to get any response instead of get-
ting a response with low certainty. Moreover, in real
life applications we are often not even sure that the
real author is part of our training set. Consequently,
we allow our system to respond ?don?t know? in
cases of low confidence (Koppel et al, 2006; Kop-
pel et al, 2011b). This allows our system to obtain
higher precision, at the expense of lower recall.
To implement this feature, we use SVM?s proba-
bility estimates, as implemented in libsvm. These
estimates give a score to each potential author.
These scores reflect the probability that this author
is the correct author, as decided by the prediction
model. The selected author is always the one with
the highest probability estimate.
As selection criterion, we use a set of increasingly
larger thresholds (0.05-0.9) for the probability of the
selected author. This means that we do not select test
samples for which the selected author has a proba-
bility estimate value lower than the threshold.
0 100 200 300 400 500 600 700 800 900 1000
25
30
35
40
45
50
55
60
Number of Candidate Authors
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 4: Authorship attribution accuracy with varying
number of candidate authors, using 200 training tweets
per author. The random baselines for 509, 100, 200, 500
and 1,000 authors are 2%, 1%, 0.5%, 0.2% and 0.1%,
respectively.
6 Basic Results
Experimenting with varying Training Set Sizes.
Figure 3 shows results for our experiments with
50 authors and various training set sizes. Results
demonstrate that authors of very short texts can be
successfully identified, even with as few as 50 tweets
per author (49.5%). When given more training sam-
ples, authors are identified much more accurately
(up to 69.7%). Results also show that, according to
our hypothesis, word n-gram features substantially
improve over character n-grams features only (3%
averaged improvement over all settings).
Experimenting with varying Numbers of Au-
thors. Figure 4 shows our results for various num-
bers of authors, using 200 tweets per author as train-
ing set. Results demonstrate that authors of an
unknown tweet can be identified to a large extent
even when there are as many as 1,000 candidate au-
thors (30.3%, as opposed to a random baseline of
only 0.1%). Results further validate that word n-
gram features substantially improve over character
9Results for 50 authors with 200 tweets per author are taken
from Figure 3.
1885
0 10 20 30 40 50 60 70 80 90 100
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
1,000 tweets/author
500 tweets/author
200 tweets/author
100 tweets/author
50 tweets/author
Figure 5: Recall-precision curves for 50 authors with
varying training set sizes.
n-grams features (2.6% averaged improvement).
Recall-Precision Tradeoff. Figure 5 shows the
recall-precision curves for our experiments with 50
authors and varying training set sizes. Results
demonstrate that we are able to obtain very high pre-
cision (over 90%) while still maintaining a relatively
high recall (from ?35% recall for 50 tweets per au-
thor up to> 60% recall for 1,000 tweets per author).
Figure 6 shows the recall-precision curves for our
experiments with varying number of authors. Re-
sults demonstrate that even in the 1,000 authors set-
ting, we are able to obtain high precision values
(90% and 70%) with reasonable recall values (18%
and ?30%, respectively).
7 Flexible Patterns
In previous sections we provided strong evidence
that authors of micro-messages can be successfully
identified using standard methods. In this section we
present a new feature, never previously used for this
task ? flexible patterns. We show that flexible pat-
terns can be used to improve classification results.
Flexible patterns are a generalization of word n-
grams, in the sense that they capture potentially un-
seen word n-grams. As a result, flexible patterns
can pick up fine-grained differences between au-
thors? styles. Unlike other types of pattern features,
0 10 20 30 40 50 60 70 80 90 100
30
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
50 authors
100 authors
200 authors
500 authors
1,000 authors
Figure 6: Recall-precision curves for varying number of
authors.
flexible patterns are computed automatically from
plain text. As such, they can be applied to various
tasks, independently of domain and language. We
describe them in detail.
Word Frequency. Flexible patterns are composed
of high frequency words (HFW) and content words
(CW). Every word in the corpus is defined as either
HFW or CW. This clustering is performed by count-
ing the number of times each word appears in the
corpus of size s. A word that appears more than
10?4?s times in a corpus is considered HFW. A
word that appears less than 10?3?s times in a cor-
pus is considered CW. Some words may serve both
as HFWs and CWs (see Davidov and Rappoport
(2008b) for discussion).
Structure of a Flexible Pattern. Flexible patterns
start and end with an HFW. A sequence of zero or
more CWs separates consecutive HFWs. At least
one CW must appear in every pattern.10 For effi-
ciency, at most six HFWs (and as a result, five CW
sequences) may appear in a flexible pattern. Exam-
ples of flexible patterns include
1. ?theHFW CW ofHFW theHFW?
10Omitting this treats word n-grams as flexible patterns.
1886
Flexible Pattern Features. Flexible patterns can
serve as binary classification features; a tweet
matches a given flexible pattern if it contains the
flexible pattern sequence. For example, (1) is
matched by (2).
2. ?Go to theHFW houseCW ofHFW theHFW rising sun?
Partial Flexible Patterns. A flexible pattern may
appear in a given tweet with additional words not
originally found in the flexible pattern, and/or with
only a subset of the HFWs (Davidov et al, 2010a).
For example, (3) is a partial match of (1), since the
word ?great? is not part of the original flexible pat-
tern. Similarly, (4) is another partial match of (1),
since (a) the word ?good? is not part of the original
flexible pattern and (b) the second occurrence of the
word ?the? does not appear in (4) (missing word is
marked by ).
3. ?TheHFW greatHFW kingCW ofHFW theHFW ring?
4. ?TheHFW goodHFW kingCW ofHFW Spain?
We use such cases as features with lower weight,
proportional to the number of found HFWs in the
tweet (w = 0.5?nfoundnexpected ). For example, (1) receives a
weight of 1 (complete match) against (2). Against
(3), it receives a weight of 0.5 (= 0.5?33 , partial
match with no missing HFWs). Against (4) it re-
ceives a weight of 1/3 (= 0.5?23 , partial match with
only 2/3 HFWs found).
Experimenting with Flexible Pattern Features.
We repeat our experiments with varying training set
sizes (see Section 5) with two more systems: one
that uses character n-grams and flexible pattern fea-
tures, and another that uses character n-grams, word
n-grams and flexible patterns. High frequency word
counts are computed separately for each author us-
ing her training set. We only consider flexible pat-
tern features that appear at least tfp times in the
training set of at least one author. Values of tfp for
training set sizes (50, 100, 200, 500, 1,000) are (2,
3, 7, 7, 8), respectively.
Results. Figure 7 shows our results. Results
demonstrate that flexible pattern features have an
added value over both character n-grams alone (av-
eraged 2.9% improvement) and over character n-
grams and word n-grams together (averaged 1.5%
0 100 200 300 400 500 600 700 800 900 1000
35
40
45
50
55
60
65
70
75
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams, Word N?grams &
Flex. Patt. Feats.
Char. N?grams + Flex. Patt. Feats.
Char. N?grams + Word N?grams
Char. N?grams
SCAP
Naive Bayes
Figure 7: Authorship attribution accuracy for 50 authors
with various training set sizes and various feature sets.
The values are averaged over 10 groups. The random
baseline is 2%.
Comparison to previous work: SCAP ? SCAP algo-
rithm results, as reported by (Layton et al, 2010), Naive
Bayes ? Naive Bayes algorithm results, as reported by
(Boutwell, 2011).
improvement). We perform t-tests on each of our
training set sizes to check whether the latter im-
provement is significant. Results demonstrate that
it is highly significant in all settings, with p-values
smaller than values between 10?3 (for 50 tweets per
author) and 10?8 (1,000 tweets per author).
Comparison to Previous Works. Figure 7 also
shows results for the only two works that experi-
mented in some of the settings we experimented in:
Layton et al (2010) and Boutwell (2011) (see Sec-
tion 8). Our system substantially outperforms these
two systems, by margins of 5.9% to 19%. These
margins are explained by the choice of algorithm
(SVM and not SCAP/naive Bayes) and our set of
features (character n-grams + word n-grams + flex-
ible patterns compared to character n-grams only).
In order to rule out the possibility that these mar-
gins stem from using different datasets, we tested
our system on the dataset used in (Layton et al,
2010). Our system obtains even higher results on
this dataset than on our datasets (61.6%, a total im-
1887
provement of 6.1% over (Layton et al, 2010)).
Discussion. To illustrate the additional contribu-
tion of flexible patterns over word n-grams, consider
the following tweets, written by the same author.
5. ?. . . theHFW wayCW IHFW treatedCW herHFW?
6. ?. . . half of theHFW thingsCW IHFW have seen?
7. ?. . . theHFW friendsCW IHFW have had for years?
8. ?. . . in theHFW neighborhoodCW IHFW grew up in?
Consider a case where (5) is part of the test set,
while (6-8) appear in the training set. As (5) shares
no sequence of words with (6-8), no word n-gram
feature is able to identify the author?s style in (5).
However, this style can be successfully identified us-
ing the flexible pattern (9), shared by (5-8).
9. theHFW CW IHFW
This demonstrates the added value flexible pattern
features have over word n-gram features.
8 Related Work
Authorship attribution dates back to the end of 19th
century, when (Mendenhall, 1887) applied sentence
length and word length features to plays of Shake-
speare. Ever since, many methods have been devel-
oped for this task. For recent surveys, see (Koppel
et al, 2009; Stamatatos, 2009; Juola, 2012).
Authorship attribution methods can be generally
divided into two categories (Stamatatos, 2009). In
similarity-based methods, an anonymous text is at-
tributed to some author whose writing style is most
similar (by some distance metric). In machine learn-
ing methods, which we follow in this paper, anony-
mous texts are classified, using machine learning al-
gorithms, into different categories (in this case, dif-
ferent authors).
Machine learning papers differ from each other by
the features and machine learning algorithm. Exam-
ples of features include HFWs (Mosteller and Wal-
lace, 1964; Argamon et al, 2007), character n-gram
(Kjell, 1994; Hoorn et al, 1999; Stamatatos, 2008),
word n-grams (Peng et al, 2004), part-of-speech
n-grams (Koppel and Schler, 2003; Koppel et al,
2005) and vocabulary richness (Abbasi and Chen,
2005).
The various machine learning algorithms used in-
clude naive Bayes (Mosteller and Wallace, 1964;
Kjell, 1994), neural networks (Matthews and Mer-
riam, 1993; Kjell, 1994), K-nearest neighbors (Kjell
et al, 1995; Hoorn et al, 1999) and SVM (De Vel et
al., 2001; Diederich et al, 2003; Koppel and Schler,
2003).
Traditionally, authorship attribution systems have
mainly been evaluated against long texts such as
theater plays (Mendenhall, 1887), essays (Yule,
1939; Mosteller and Wallace, 1964), biblical books
(Mealand, 1995; Koppel et al, 2011a) and book
chapters (Argamon et al, 2007; Koppel et al, 2007).
In recent year, many works focused on web data
such as emails (De Vel et al, 2001; Koppel and
Schler, 2003; Abbasi and Chen, 2008), web forum
messages (Abbasi and Chen, 2005; Solorio et al,
2011), blogs (Koppel et al, 2006; Koppel et al,
2011b) and chat messages (Abbasi and Chen, 2008).
Some works focused on SMS messages (Mohan et
al., 2010; Ishihara, 2011).
Authorship Attribution on Twitter. The perfor-
mance of authorship attribution systems on short
texts is affected by several factors (Stamatatos,
2009). These factors include the number of candi-
date authors, the training set size and the size of the
test document.
Very few authorship attribution works experi-
mented with Twitter. Unlike our work, all used a
single group of authors (group sizes varied between
3-50). Layton et al (2010) used the SCAP method-
ology (Frantzeskou et al, 2007) with character n-
gram features. They experimented with 50 authors
and compared different numbers of tweets per au-
thor (values between 20-200). Surprisingly, they
showed that their system does not improve when
given more training tweets. In our work, we no-
ticed a different trend, and showed that more data
can be extremely valuable for authorship attribution
systems on micro-messages (see Section 6). Silva
et al (2011) trained an SVM classifier with various
features (e.g., punctuation and vocabulary features)
on a small dataset of three authors only, with vary-
ing training set size. Although their work used a
set of Twitter-specific features that we do not explic-
itly use, our features implicitly cover a large portion
of their features (such as punctuation and emoticon
1888
features, which are largely covered by character n-
grams).
Boutwell (2011) used a naive Bayes classifier
with character n-gram features. She experimented
with 50 authors and two training size values (120
and 230). She also provided a set of experiments that
studied the effect of joining several tweets into a sin-
gle document. Mikros and Perifanos (2013) trained
an SVM classifier with character n-gram and word
n-grams. They experimented with 10 authors of
Greek text, and also joined several tweets into a sin-
gle document. Joining several tweets into a longer
document is appealing since it can lead to substantial
improvement of the classification results, as demon-
strated by the works above. However, this approach
requires the test data to contain several tweets that
are known a-priori to be written by the same author.
This assumption is not always realistic. In our paper,
we intentionally focus on a single tweet as document
size.
Flexible Patterns. Patterns were introduced by
(Hearst, 1992), who used hand crafted patterns
to discover hyponyms. Hard coded patterns
were used for many tasks, such as discovering
meronymy (Berland and Charniak, 1999), noun cat-
egories (Widdows and Dorow, 2002), verb relations
(Chklovski and Pantel, 2004) and semantic class
learning (Kozareva et al, 2008).
Patterns were first extracted in a fully unsuper-
vised manner (?flexible patterns?) by (Davidov and
Rappoport, 2006), who used flexible patterns in or-
der to establish noun categories, and (Bicic?i and
Yuret, 2006) who used them for analogy question
answering. Ever since, flexible patterns were used
as features for various tasks such as extraction of
semantic relationships (Davidov et al, 2007; Tur-
ney, 2008b; Bollegala et al, 2009), detection of
synonyms (Turney, 2008a), disambiguation of nom-
inal compound relations (Davidov and Rappoport,
2008a), sentiment analysis (Davidov et al, 2010b)
and detection of sarcasm (Tsur et al, 2010).
9 Conclusion
The main goal of this paper is to measure to what
extent authors of micro-messages can be identified.
We have shown that authors of very short texts
can be successfully identified in an array of au-
thorship attribution settings reported for long doc-
uments. This is the first work on micro-messages
to address some of these settings. We introduced
the concept of k-signature. Using this concept, we
proposed an interpretation of our results. Last, we
presented the first authorship attribution system that
uses flexible patterns, and demonstrated that using
these features significantly improves over other sys-
tems. Our system obtains 6.1% improvement over
the current state-of-the-art.
Acknowledgments
We would like to thank Elad Eban and Susan Good-
man for their helpful advice, as well as Robert Lay-
ton for providing us with his dataset. This research
was funded (in part) by the Harry and Sylvia Hoff-
man leadership and responsibility program (for the
first author) and the Intel Collaborative Research In-
stitute for Computational Intelligence (ICRI-CI).
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying au-
thorship analysis to extremist-group web forum mes-
sages. IEEE Intelligent Systems, 20:67?75.
Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints:
A stylometric approach to identity-level identification
and similarity detection in cyberspace. ACM Transac-
tions on Information Systems, 26(2):7:1?7:29.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features: Research articles. J. Am. Soc. Inf. Sci.
Technol., 58(6):802?822.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proc. of ACL, pages
57?64, College Park, Maryland, USA.
Ergun Bicic?i and Deniz Yuret. 2006. Clustering word
pairs to answer analogy questions. In Proc. of TAINN,
pages 1?8.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2009. Measuring the similarity between
implicit semantic relations from the web. In Proc. of
WWW, New York, New York, USA. ACM Press.
Sarah R. Boutwell. 2011. Authorship Attribution of
Short Messages Using Multimodal Features. Master?s
thesis, Naval Postgraduate School.
John Burrows. 2002. ?Delta?: a Measure of Stylistic
Difference and a Guide to Likely Authorship. Literary
and Linguistic Computing, 17(3):267?287.
1889
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Dekang Lin and Dekai Wu, editors, Proc.
of EMNLP, pages 33?40, Barcelona, Spain.
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proc.
of ACL-Coling, pages 297?304, Sydney, Australia.
Dmitry Davidov and Ari Rappoport. 2008a. Classifi-
cation of semantic relationships between nominals us-
ing pattern clusters. In Proceedings of ACL-08: HLT,
pages 227?235, Columbus, Ohio, June. Association
for Computational Linguistics.
Dmitry Davidov and Ari Rappoport. 2008b. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
SAT analogy questions. In Proc. of ACL-HLT, pages
692?700, Columbus, Ohio.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of ACL,
pages 232?239, Prague, Czech Republic.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proc. of CoNLL, pages 107?
116, Uppsala, Sweden.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proc. of Coling, pages 241?249, Bei-
jing, China.
Olivier De Vel, Alison Anderson, Malcolm Corney, and
George Mohay. 2001. Mining e-mail content for au-
thor identification forensics. ACM Sigmod Record,
30(4):55?64.
JoachimDiederich, Jo?rg Kindermann, Edda Leopold, and
Gerhard Paass. 2003. Authorship attribution with
support vector machines. Applied intelligence, 19(1-
2):109?123.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large web-derived corpus of english. In
Proc. of the 4th Web as Corpus Workshop, WAC-4.
Georgia Frantzeskou, Efstathios Stamatatos, Stefanos
Gritzalis, and Carole E Chaski. 2007. Identifying au-
thorship by byte-level n-grams: The source code au-
thor profile (scap) method. Int Journal of Digital Evi-
dence, 6(1):1?18.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of Coling
? Volume 2, pages 539?545, Stroudsburg, PA, USA.
Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, and
Floor van der Ham. 1999. Neural network identifi-
cation of poets using letter sequences. Literary and
Linguistic Computing, 14(3):311?338.
Shunichi Ishihara. 2011. A forensic authorship clas-
sification in sms messages: A likelihood ratio based
approach using n-gram. In Proc. of the Australasian
Language Technology Association Workshop 2011,
pages 47?56, Canberra, Australia.
Patrick Juola. 2012. Large-scale experiments in author-
ship attribution. English Studies, 93(3):275?283.
Bradley Kjell, W Addison Woods, and Ophir Frieder.
1995. Information retrieval using letter tuples with
neural network and nearest neighbor classifiers. In
IEEE International Conference on Systems, Man and
Cybernetics, volume 2, pages 1222?1226. IEEE.
Bradley Kjell. 1994. Authorship determination using let-
ter pair frequency features with neural network classi-
fiers. Literary and Linguistic Computing, 9(2):119?
124.
Moshe Koppel and Jonathan Schler. 2003. Exploiting
stylistic idiosyncrasies for authorship attribution. In
Proc. of IJCAI?03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis, volume 69,
page 72.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, KDD ?05, pages 624?628, New York,
NY, USA.
Moshe Koppel, Jonathan Schler, Shlomo Argamon, and
EranMesseri. 2006. Authorship attribution with thou-
sands of candidate authors. In SIGIR, pages 659?660.
Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-
Dokow. 2007. Measuring differentiability: Unmask-
ing pseudonymous authors. JMLR, 8:1261?1276.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9?26.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011a. Unsupervised decom-
position of a document into authorial components. In
Proc. of ACL-HLT, pages 1356?1364, Portland, Ore-
gon, USA.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011b. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
1890
pattern linkage graphs. In Proc. of ACL-HLT, pages
1048?1056, Columbus, Ohio.
Robert Layton, PaulWatters, and Richard Dazeley. 2010.
Authorship attribution for twitter in 140 characters or
less. In Proc. of the 2010 Second Cybercrime and
Trustworthy Computing Workshop, CTC ?10, pages 1?
8, Washington, DC, USA. IEEE Computer Society.
Robert AJ Matthews and Thomas VN Merriam. 1993.
Neural computation in stylometry i: An application to
the works of shakespeare and fletcher. Literary and
Linguistic Computing, 8(4):203?209.
DL Mealand. 1995. Correspondence analysis of luke.
Literary and linguistic computing, 10(3):171?182.
Thomas Corwin Mendenhall. 1887. The characteristic
curves of composition. Science, ns-9(214S):237?246.
George K Mikros and Kostas Perifanos. 2013. Author-
ship attribution in greek tweets using authors multi-
level n-gram profiles. In 2013 AAAI Spring Sympo-
sium Series.
Ashwin Mohan, Ibrahim M Baggili, and Marcus K
Rogers. 2010. Authorship attribution of sms mes-
sages using an n-grams approach. Technical report,
CERIAS Tech Report 2011.
Frederick Mosteller and David Lee Wallace. 1964.
Inference and disputed authorship: The Federalist.
Addison-Wesley.
Fuchun Peng, Dale Schuurmans, and Shaojun Wang.
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Information Retrieval, 7(3-
4):317?345.
Conrad Sanderson and Simon Guenter. 2006. Short text
authorship attribution via sequence kernels, markov
chains and author unmasking: An investigation. In
Proc. of EMNLP, pages 482?491, Sydney, Australia.
Rui Sousa Silva, Gustavo Laboreiro, Lu??s Sarmento, Tim
Grant, Euge?nio Oliveira, and Belinda Maia. 2011.
?twazn me!!! ;(? automatic authorship analysis of
micro-blogging messages. In Proc. of the 16th inter-
national conference on Natural language processing
and information systems, NLDB?11, pages 161?168,
Berlin, Heidelberg. Springer-Verlag.
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and
Manuel Montes-Gomez. 2011. Modality specific
meta features for authorship attribution in web forum
posts. In Proc. of IJCNLP, pages 156?164, Chiang
Mai, Thailand, November.
Efstathios Stamatatos. 2008. Author identification: Us-
ing text sampling to handle the class imbalance prob-
lem. Inf. Process. Manage., 44(2):790?799.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm?a great catchy name: Semi-supervised recog-
nition of sarcastic sentences in online product reviews.
In Proc. of ICWSM.
Peter Turney. 2008a. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proc. of
Coling, pages 905?912,Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Peter D. Turney. 2008b. The latent relation mapping en-
gine: Algorithm and experiments. Journal of Artificial
Intelligence Research, 33:615?655.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Proc.
of Coling, pages 1?7, Stroudsburg, PA, USA.
George Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: with application
to two cases of disputed authorship. Biometrika, 30(3-
4):363?390.
1891
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107?116,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Semi-Supervised Recognition of Sarcastic Sentences
in Twitter and Amazon
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem, Israel
dmitry@alice.nc.huji.ac.il
Oren Tsur
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
oren@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
arir@cs.huji.ac.il
Abstract
Sarcasm is a form of speech act in which
the speakers convey their message in an
implicit way. The inherently ambiguous
nature of sarcasm sometimes makes it hard
even for humans to decide whether an ut-
terance is sarcastic or not. Recognition of
sarcasm can benefit many sentiment analy-
sis NLP applications, such as review sum-
marization, dialogue systems and review
ranking systems.
In this paper we experiment with semi-
supervised sarcasm identification on two
very different data sets: a collection of
5.9 million tweets collected from Twit-
ter, and a collection of 66000 product re-
views from Amazon. Using the Mechani-
cal Turk we created a gold standard sam-
ple in which each sentence was tagged by
3 annotators, obtaining F-scores of 0.78 on
the product reviews dataset and 0.83 on
the Twitter dataset. We discuss the dif-
ferences between the datasets and how the
algorithm uses them (e.g., for the Amazon
dataset the algorithm makes use of struc-
tured information). We also discuss the
utility of Twitter #sarcasm hashtags for the
task.
1 Introduction
Sarcasm (also known as verbal irony) is a sophis-
ticated form of speech act in which the speakers
convey their message in an implicit way. One in-
herent characteristic of the sarcastic speech act is
that it is sometimes hard to recognize. The dif-
ficulty in recognition of sarcasm causes misun-
derstanding in everyday communication and poses
problems to many NLP systems such as online
review summarization systems, dialogue systems
or brand monitoring systems due to the failure of
state of the art sentiment analysis systems to detect
sarcastic comments. In this paper we experiment
with a semi-supervised framework for automatic
identification of sarcastic sentences.
One definition for sarcasm is: the activity of
saying or writing the opposite of what you mean,
or of speaking in a way intended to make someone
else feel stupid or show them that you are angry
(Macmillan English Dictionary (2007)). Using the
former definition, sarcastic utterances appear in
many forms (Brown, 1980; Gibbs and O?Brien,
1991). It is best to present a number of examples
which show different facets of the phenomenon,
followed by a brief review of different aspects of
the sarcastic use. The sentences are all taken from
our experimental data sets:
1. ?thank you Janet Jackson for yet another
year of Super Bowl classic rock!? (Twitter)
2. ?He?s with his other woman: XBox 360. It?s
4:30 fool. Sure I can sleep through the gun-
fire? (Twitter)
3. ?Wow GPRS data speeds are blazing fast.?
(Twitter)
4. ?[I] Love The Cover? (book, amazon)
5. ?Defective by design? (music player, ama-
zon)
Example (1) refers to the supposedly lame mu-
sic performance in super bowl 2010 and attributes
it to the aftermath of the scandalous performance
of Janet Jackson in the previous year. Note that the
previous year is not mentioned and the reader has
to guess the context (use universal knowledge).
The words yet and another might hint at sarcasm.
107
Example (2) is composed of three short sentences,
each of them sarcastic on its own. However, com-
bining them in one tweet brings the sarcasm to
its extreme. Example (3) is a factual statement
without explicit opinion. However, having a fast
connection is a positive thing. A possible sar-
casm emerges from the over exaggeration (?wow?,
?blazing-fast?).
Example (4) from Amazon, might be a genuine
compliment if it appears in the body of the review.
However, recalling the expression ?don?t judge a
book by its cover?, choosing it as the title of the
review reveals its sarcastic nature. Although the
negative sentiment is very explicit in the iPod re-
view (5), the sarcastic effect emerges from the pun
that assumes the knowledge that the design is one
of the most celebrated features of Apple?s prod-
ucts. (None of the above reasoning was directly
introduced to our algorithm.)
Modeling the underlying patterns of sarcastic
utterances is interesting from the psychological
and cognitive perspectives and can benefit var-
ious NLP systems such as review summariza-
tion (Popescu and Etzioni, 2005; Pang and Lee,
2004; Wiebe et al, 2004; Hu and Liu, 2004) and
dialogue systems. Following the ?brilliant-but-
cruel? hypothesis (Danescu-Niculescu-Mizil et al,
2009), it can help improve ranking and recommen-
dation systems (Tsur and Rappoport, 2009). All
systems currently fail to correctly classify the sen-
timent of sarcastic sentences.
In this paper we utilize the semi-supervised sar-
casm identification algorithm (SASI) of (Tsur et
al., 2010). The algorithm employs two modules:
semi supervised pattern acquisition for identify-
ing sarcastic patterns that serve as features for a
classifier, and a classification stage that classifies
each sentence to a sarcastic class. We experiment
with two radically different datasets: 5.9 million
tweets collected from Twitter, and 66000 Amazon
product reviews. Although for the Amazon dataset
the algorithm utilizes structured information, re-
sults for the Twitter dataset are higher. We discuss
the possible reasons for this, and also the utility
of Twitter #sarcasm hashtags for the task. Our al-
gorithm performed well in both domains, substan-
tially outperforming a strong baseline based on se-
mantic gap and user annotations. To further test its
robustness we also trained the algorithm in a cross
domain manner, achieving good results.
2 Data
The datasets we used are interesting in their own
right for many applications. In addition, our algo-
rithm utilizes some aspects that are unique to these
datasets. Hence, before describing the algorithm,
we describe the datasets in detail.
Twitter Dataset. Since Twitter is a relatively
new service, a somewhat lengthy description of
the medium and the data is appropriate.
Twitter is a very popular microblogging service.
It allows users to publish and read short messages
called tweets (also used as a verb: to tweet: the act
of publishing on Twitter). The tweet length is re-
stricted to 140 characters. A user who publishes a
tweet is referred to as a tweeter and the readers are
casual readers or followers if they are registered to
get al tweets by this tweeter.
Apart from simple text, tweets may contain ref-
erences to url addresses, references to other Twit-
ter users (these appear as @<user>) or a con-
tent tag (called hashtags) assigned by the tweeter
(#<tag>). An example of a tweet is: ?listen-
ing to Andrew Ridgley by Black Box Recorder on
@Grooveshark: http://tinysong.com/cO6i #good-
music?, where ?grooveshark? is a Twitter user
name and #goodmusic is a tag that allows to
search for tweets with the same tag. Though fre-
quently used, these types of meta tags are optional.
In order to ignore specific references we substi-
tuted such occurrences with special tags: [LINK],
[USER] and [HASHTAG] thus we have ?listen-
ing to Andrew Ridgley by Black Box Recorder on
[USER]: [LINK] [HASHTAG]?. It is important
to mention that hashtags are not formal and each
tweeter can define and use new tags as s/he likes.
The number of special tags in a tweet is only
subject to the 140 characters constraint. There is
no specific grammar that enforces the location of
special tags within a tweet.
The informal nature of the medium and the 140
characters length constraint encourages massive
use of slang, shortened lingo, ascii emoticons and
other tokens absent from formal lexicons.
These characteristics make Twitter a fascinat-
ing domain for NLP applications, although posing
great challenges due to the length constraint, the
complete freedom of style and the out of discourse
nature of tweets.
We used 5.9 million unique tweets in our
dataset: the average number of words is 14.2
108
words per tweet, 18.7% contain a url, 35.3% con-
tain reference to another tweeter and 6.9% contain
at least one hashtag1.
The #sarcasm hashtag One of the hashtags
used by Twitter users is dedicated to indicate sar-
castic tweets. An example of the use of the tag
is: ?I guess you should expect a WONDERFUL
video tomorrow. #sarcasm?. The sarcastic hashtag
is added by the tweeter. This hashtag is used in-
frequently as most users are not aware of it, hence,
the majority of sarcastic tweets are not explicitly
tagged by the tweeters. We use tagged tweets as
a secondary gold standard. We discuss the use of
this tag in Section 5.
Amazon dataset. We used the same dataset
used by (Tsur et al, 2010), containing 66000 re-
views for 120 products from Amazon.com. The
corpus contained reviews for books from differ-
ent genres and various electronic products. Ama-
zon reviews are much longer than tweets (some
reach 2000 words, average length is 953 charac-
ters), they are more structured and grammatical
(good reviews are very structured) and they come
in a known context of a specific product. Reviews
are semi-structured as besides the body of the re-
view they all have the following fields: writer,
date, star rating (the overall satisfaction of the re-
view writer) and a one line summary.
Reviews refer to a specific product and rarely
address each other. Each review sentence is, there-
fore, part of a context ? the specific product, the
star rating, the summary and other sentences in
that review. In that sense, sentences in the Ama-
zon dataset differ radically from the contextless
tweets. It is worth mentioning that the majority
of reviews are on the very positive side (star rating
average of 4.2 stars).
3 Classification Algorithm
Our algorithm is semi-supervised. The input is
a relatively small seed of labeled sentences. The
seed is annotated in a discrete range of 1 . . . 5
where 5 indicates a clearly sarcastic sentence and
1 indicates a clear absence of sarcasm. A 1 . . . 5
scale was used in order to allow some subjectiv-
ity and since some instances of sarcasm are more
explicit than others.
1The Twitter data was generously provided to us by Bren-
dan O?Connor.
Given the labeled sentences, we extracted a set
of features to be used in feature vectors. Two basic
feature types are utilized: syntactic and pattern-
based features. We constructed feature vectors for
each of the labeled examples in the training set and
used them to build a classifier model and assign
scores to unlabeled examples. We next provide a
description of the algorithmic framework of (Tsur
et al, 2010).
Data preprocessing A sarcastic utterance usu-
ally has a target. In the Amazon dataset these
targets can be exploited by a computational al-
gorithm, since each review targets a product, its
manufacturer or one of its features, and these are
explicitly represented or easily recognized. The
Twitter dataset is totally unstructured and lacks
textual context, so we did not attempt to identify
targets.
Our algorithmic methodology is based on
patterns. We could use patterns that include
the targets identified in the Amazon dataset.
However, in order to use less specific patterns,
we automatically replace each appearance
of a product, author, company, book name
(Amazon) and user, url and hashtag (Twitter)
with the corresponding generalized meta tags
?[PRODUCT]?,?[COMPANY]?,?[TITLE]? and
?[AUTHOR]? tags2 and ?[USER]?,?[LINK]? and
?[HASHTAG]?. We also removed all HTML tags
and special symbols from the review text.
Pattern extraction Our main feature type is
based on surface patterns. In order to extract such
patterns automatically, we followed the algorithm
given in (Davidov and Rappoport, 2006). We clas-
sified words into high-frequency words (HFWs)
and content words (CWs). A word whose cor-
pus frequency is more (less) than FH (FC) is con-
sidered to be a HFW (CW). Unlike in (Davidov
and Rappoport, 2006), we consider all punctuation
characters as HFWs. We also consider [product],
[company], [title], [author] tags as HFWs for pat-
tern extraction. We define a pattern as an ordered
sequence of high frequency words and slots for
content words. The FH and FC thresholds were
set to 1000 words per million (upper bound for
FC) and 100 words per million (lower bound for
FH )3.
2Appropriate names are provided with each review so this
replacement can be done automatically.
3Note that FH and FC set bounds that allow overlap be-
tween some HFWs and CWs.
109
The patterns allow 2-6 HFWs and 1-6 slots for
CWs. For each sentence it is possible to gener-
ate dozens of patterns that may overlap. For ex-
ample, given a sentence ?Garmin apparently does
not care much about product quality or customer
support?, we have generated several patterns in-
cluding ?[COMPANY] CW does not CW much?,
?does not CW much about CW CW or?, ?not CW
much? and ?about CW CW or CW CW.?. Note
that ?[COMPANY]? and ?.? are treated as high
frequency words.
Pattern selection The pattern extraction stage
provides us with hundreds of patterns. However,
some of them are either too general or too specific.
In order to reduce the feature space, we have used
two criteria to select useful patterns.
First, we removed all patterns which appear
only in sentences originating from a single prod-
uct/book (Amazon). Such patterns are usually
product-specific. Next we removed all patterns
which appear in the seed both in some example la-
beled 5 (clearly sarcastic) and in some other exam-
ple labeled 1 (obviously not sarcastic). This filters
out frequent generic and uninformative patterns.
Pattern selection was performed only on the Ama-
zon dataset as it exploits review?s meta data.
Pattern matching Once patterns are selected,
we have used each pattern to construct a single en-
try in the feature vectors. For each sentence we
calculated a feature value for each pattern as fol-
lows:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1 : Exact match ? all the pattern components
appear in the sentence in correct
order without any additional words.
? : Sparse match ? same as exact match
but additional non-matching words can be
inserted between pattern components.
? ? n/N : Incomplete match ? only n > 1 of N pattern
components appear in the sentence,
while some non-matching words can
be inserted in-between. At least one of the
appearing components should be a HFW.
0 : No match ? nothing or only a single
pattern component appears in the sentence.
0 ? ? ? 1 and 0 ? ? ? 1 are parameters we use
to assign reduced scores for imperfect matches.
Since the patterns we use are relatively long, ex-
act matches are uncommon, and taking advantage
of partial matches allows us to significantly re-
duce the sparsity of the feature vectors. We used
?\? 0.05 0.1 0.2
0.05 0.48 0.45 0.39
0.1 0.50 0.51 0.40
0.2 0.40 0.42 0.33
Table 1: Results (F-Score for ?no enrichment? mode) of
cross validation with various values for ? and ? on Twit-
ter+Amazon data
? = ? = 0.1 in all experiments. Table 1 demon-
strates the results obtained with different values
for ? and ?.
Thus, for the sentence ?Garmin apparently does
not care much about product quality or customer
support?, the value for ?[company] CW does not?
would be 1 (exact match); for ?[company] CW
not? would be 0.1 (sparse match due to insertion
of ?does?); and for ?[company] CW CW does not?
would be 0.1 ? 4/5 = 0.08 (incomplete match
since the second CW is missing).
Punctuation-based features In addition to
pattern-based features we used the following
generic features: (1) Sentence length in words,
(2) Number of ?!? characters in the sentence, (3)
Number of ??? characters in the sentence, (4)
Number of quotes in the sentence, and (5) Num-
ber of capitalized/all capitals words in the sen-
tence. All these features were normalized by di-
viding them by the (maximal observed value ? av-
eraged maximal value of the other feature groups),
thus the maximal weight of each of these fea-
tures is equal to the averaged weight of a single
pattern/word/n-gram feature.
Data enrichment Since we start with only a
small annotated seed for training (particularly, the
number of clearly sarcastic sentences in the seed is
modest) and since annotation is noisy and expen-
sive, we would like to find more training examples
without requiring additional annotation effort.
To achieve this, we posited that sarcastic sen-
tences frequently co-appear in texts with other sar-
castic sentences (i.e. example (2) in Section 1).
We performed an automated web search using the
Yahoo! BOSS API4, where for each sentence s in
the training set (seed), we composed a search en-
gine query qs containing this sentence5. We col-
lected up to 50 search engine snippets for each
example and added the sentences found in these
snippets to the training set. The label (level of sar-
4http://developer.yahoo.com/search/boss.
5If the sentence contained more than 6 words, only the
first 6 words were included in the search engine query.
110
casm) Label(sq) of a newly extracted sentence sq
is similar to the label Label(s) of the seed sen-
tence s that was used for the query that acquired it.
The seed sentences together with newly acquired
sentences constitute the (enriched) training set.
Data enrichment was performed only for the
Amazon dataset where we have a manually tagged
seed and the sentence structure is closer to stan-
dard English grammar. We refer the reader to
(Tsur et al, 2010) for more details about the en-
richment process and for a short discussion about
the usefulness of web-based data enrichment in the
scope of sarcasm recognition.
Classification In order to assign a score to new
examples in the test set we use a k-nearest neigh-
bors (kNN)-like strategy. We construct feature
vectors for each example in the training and test
sets. We would like to calculate the score for each
example in the test set. For each feature vector v in
the test set, we compute the Euclidean distance to
each of the matching vectors in the extended train-
ing set, where matching vectors share at least one
pattern feature with v.
Let ti, i = 1..k be the k vectors with lowest
Euclidean distance to v6. Then v is classified with
a label l as follows:
Count(l) = Fraction of training vectors with label l
Label(v) =
[
1
k
?
i
Count(Label(ti)) ? Label(ti)
?
j Count(label(tj))
]
Thus the score is a weighted average of the k clos-
est training set vectors. If there are less than k
matching vectors for the given example then fewer
vectors are used in the computation. If there are
no matching vectors found for v, we assigned the
default value Label(v) = 1, since sarcastic sen-
tences are fewer in number than non-sarcastic ones
(this is a ?most common tag? strategy).
4 Evaluation Setup
Seed and extended training sets (Amazon). As
described in the previous section, SASI is semi su-
pervised, hence requires a small seed of annotated
data. We used the same seed of 80 positive (sar-
castic) examples and 505 negative examples de-
scribed at (Tsur et al, 2010).
After automatically expanding the training set,
our training data now contains 471 positive exam-
ples and 5020 negative examples. These ratios are
6We used k = 5 for all experiments.
to be expected, since non-sarcastic sentences out-
number sarcastic ones, definitely when most on-
line reviews are positive (Liu et al, 2007). This
generally positive tendency is also reflected in our
data ? the average number of stars is 4.12.
Seed training set with #sarcasm (Twitter). We
used a sample of 1500 tweets marked with the
#sarcasm hashtag as a positive set that represents
sarcasm styles special to Twitter. However, this set
is very noisy (see discussion in Section 5).
Seed training set (cross domain). Results ob-
tained by training on the 1500 #sarcasm hash-
tagged tweets were not promising. Examination of
the #sarcasm tagged tweets shows that the annota-
tion is biased and noisy as we discuss in length
in Section 5. A better annotated set was needed
in order to properly train the algorithm. Sarcas-
tic tweets are sparse and hard to find and annotate
manually. In order to overcome sparsity we used
the positive seed annotated on the Amazon dataset.
The training set was completed by manually se-
lected negative example from the Twitter dataset.
Note that in this setting our training set is thus of
mixed domains.
4.1 Star-sentiment baseline
Many studies on sarcasm suggest that sarcasm
emerges from the gap between the expected utter-
ance and the actual utterance (see echoic mention,
allusion and pretense theories in Related Work
Section( 6)). We implemented a baseline designed
to capture the notion of sarcasm as reflected by
these models, trying to meet the definition ?saying
the opposite of what you mean in a way intended
to make someone else feel stupid or show you are
angry?.
We exploit the meta-data provided by Amazon,
namely the star rating each reviewer is obliged
to provide, in order to identify unhappy review-
ers. From this set of negative reviews, our base-
line classifies as sarcastic those sentences that ex-
hibit strong positive sentiment. The list of positive
sentiment words is predefined and captures words
typically found in reviews (for example, ?great?,
?excellent?, ?best?, ?top?, ?exciting?, etc).
4.2 Evaluation procedure
We used two experimental frameworks to test
SASI?s accuracy. In the first experiment we eval-
uated the pattern acquisition process, how consis-
tent it is and to what extent it contributes to correct
111
classification. We did that by 5-fold cross valida-
tion over the seed data.
In the second experiment we evaluated SASI on
a test set of unseen sentences, comparing its out-
put to a gold standard annotated by a large number
of human annotators (using the Mechanical Turk).
This way we verify that there is no over-fitting and
that the algorithm is not biased by the notion of
sarcasm of a single seed annotator.
5-fold cross validation (Amazon). In this ex-
perimental setting, the seed data was divided to 5
parts and a 5-fold cross validation test is executed.
Each time, we use 4 parts of the seed as the train-
ing data and only this part is used for the feature
selection and data enrichment. This 5-fold pro-
cess was repeated ten times. This procedure was
repeated with different sets of optional features.
We used 5-fold cross validation and not the
standard 10-fold since the number of seed exam-
ples (especially positive) is relatively small hence
10-fold is too sensitive to the broad range of possi-
ble sarcastic patterns (see the examples in Section
1).
Classifying new sentences (Amazon & Twitter).
Evaluation of sarcasm is a hard task due to the
elusive nature of sarcasm, as discussed in Sec-
tion 1. In order to evaluate the quality of our al-
gorithm, we used SASI to classify all sentences
in both corpora (besides the small seed that was
pre-annotated and was used for the evaluation in
the 5-fold cross validation experiment). Since it
is impossible to created a gold standard classifica-
tion of each and every sentence in the corpus, we
created a small test set by sampling 90 sentences
which were classified as sarcastic (labels 3-5) and
90 sentences classified as not sarcastic (labels 1,2).
The sampling was performed on the whole corpus
leaving out only the seed data.
Again, the meta data available in the Amazon
dataset alows us a stricter evaluation. In order
to make the evaluation harder for our algorithm
and more relevant, we introduced two constraints
to the sampling process: i) we sampled only sen-
tences containing a named-entity or a reference to
a named entity. This constraint was introduced in
order to keep the evaluation set relevant, since sen-
tences that refer to the named entity (the target of
the review) are more likely to contain an explicit
or implicit sentiment. ii) we restricted the non-
sarcastic sentences to belong to negative reviews
(1-3 stars) so that all sentences in the evaluation
set are drawn from the same population, increas-
ing the chances they convey various levels of di-
rect or indirect negative sentiment7.
Experimenting with the Twitter dataset, we sim-
ply classified each tweet into one of 5 classes
(class 1: not sarcastic, class 5: clearly sarcastic)
according to the label given by the algorithm. Just
like the evaluation of the algorithm on the Amazon
dataset, we created a small evaluation set by sam-
pling 90 sentences which were classified as sarcas-
tic (labels 3-5) and 90 sentences classified as not
sarcastic (labels 1,2).
Procedure Each evaluation set was randomly
divided to 5 batches. Each batch contained 36 sen-
tences from the evaluation set and 4 anchor sen-
tences: two with sarcasm and two sheer neutral.
The anchor sentences were not part of the test set
and were the same in all five batches. The purpose
of the anchor sentences is to control the evaluation
procedure and verify that annotation is reasonable.
We ignored the anchor sentences when assessing
the algorithm?s accuracy.
We used Amazon?s Mechanical Turk8 service
in order to create a gold standard for the evalua-
tion. We employed 15 annotators for each eval-
uation set. We used a relatively large number of
annotators in order to overcome the possible bias
induced by subjectivity (Muecke, 1982). Each an-
notator was asked to assess the level of sarcasm of
each sentence of a set of 40 sentences on a scale of
1-5. In total, each sentence was annotated by three
different annotators.
Inter Annotator Agreement. To simplify the
assessment of inter-annotator agreement, the scal-
ing was reduced to a binary classification where 1
and 2 were marked as non-sarcastic and 3-5 as sar-
castic (recall that 3 indicates a hint of sarcasm and
5 indicates ?clearly sarcastic?). We checked the
Fleiss? ? statistic to measure agreement between
multiple annotators. The inter-annotator agree-
ment statistic was ? = 0.34 on the Amazon dataset
and ? = 0.41 on the Twitter dataset.
These agreement statistics indicates a fair
agreement. Given the fuzzy nature of the task at
7Note that the second constraint makes the problem less
easy. If taken from all reviews, many of the sentences would
be positive sentences which are clearly non-sarcastic. Doing
this would bias selection to positive vs. negative samples in-
stead of sarcastic-nonsarcastic samples.
8https://www.mturk.com/mturk/welcome
112
Prec. Recall Accuracy F-score
punctuation 0.256 0.312 0.821 0.281
patterns 0.743 0.788 0.943 0.765
pat+punct 0.868 0.763 0.945 0.812
enrich punct 0.4 0.390 0.832 0.395
enrich pat 0.762 0.777 0.937 0.769
all: SASI 0.912 0.756 0.947 0.827
Table 2: 5-fold cross validation results on the Amazon gold
standard using various feature types. punctuation: punctua-
tion mark;, patterns: patterns; enrich: after data enrichment;
enrich punct: data enrichment based on punctuation only; en-
rich pat: data enrichment based on patterns only; SASI: all
features combined.
hand, this ? value is certainly satisfactory. We at-
tribute the better agreement on the twitter data to
the fact that in twitter each sentence (tweet) is con-
text free, hence the sentiment in the sentence is ex-
pressed in a way that can be perceived more easily.
Sentences from product reviews come as part of a
full review, hence the the sarcasm sometimes re-
lies on other sentences in the review. In our evalu-
ation scheme, our annotators were presented with
individual sentences, making the agreement lower
for those sentences taken out of their original con-
text. The agreement on the control set (anchor sen-
tences) had ? = 0.53.
Using Twitter #sarcasm hashtag. In addition to
the gold standard annotated using the Mechanical
Turk, we collected 1500 tweets that were tagged
#sarcastic by their tweeters. We call this sample
the hash-gold standard. It was used to further eval-
uate recall. This set (along with the negative sam-
ple) was used for a 5-fold cross validation in the
same manner describe for Amazon.
5 Results and discussion
5-fold cross validation (Amazon). Results are
analyzed and discussed in detail in (Tsur et al,
2010), however, we summarize it here (Table 2)
in order to facilitate comparison with the results
obtained on the Twitter dataset. SASI, including
all components, exhibits the best overall perfor-
mances with 91.2% precision and with F-Score
of 0.827. Interestingly, although data enrichment
brings SASI to the best performance in both preci-
sion and F-score, patterns+punctuations achieves
almost comparable results.
Newly introduced sentences (Amazon). In the
second experiment we evaluated SASI based on a
gold standard annotation created by 15 annotators.
Table 3 presents the results of our algorithm as
well as results of the heuristic baseline that makes
Prec. Recall FalsePos FalseNeg F Score
Star-sent. 0.5 0.16 0.05 0.44 0.242
SASI (AM) 0.766 0.813 0.11 0.12 0.788
SASI (TW) 0.794 0.863 0.094 0.15 0.827
Table 3: Evaluation on the Amazon (AM) and the Twitter
(TW) evaluation sets obtained by averaging on 3 human an-
notations per sentence. TW results were obtained with cross-
domain training.
Prec. Recall Accuracy F-score
punctuation 0.259 0.26 0.788 0.259
patterns 0.765 0.326 0.889 0.457
enrich punct 0.18 0.316 0.76 0.236
enrich pat 0.685 0.356 0.885 0.47
all no enrich 0.798 0.37 0.906 0.505
all SASI: 0.727 0.436 0.896 0.545
Table 4: 5-fold cross validation results on the Twitter hash-
gold standard using various feature types. punctuation: punc-
tuation marks; patterns: patterns; enrich: after data enrich-
ment; enrich punct: data enrichment based on punctuation
only; enrich pat: data enrichment based on patterns only;
SASI: all features combined.
use of meta-data, designed to capture the gap be-
tween an explicit negative sentiment (reflected by
the review?s star rating) and explicit positive senti-
ment words used in the review. Precision of SASI
is 0.766, a significant improvement over the base-
line with precision of 0.5.
The F-score shows more impressive improve-
ment as the baseline shows decent precision but a
very limited recall since it is incapable of recog-
nizing subtle sarcastic sentences. These results fit
the works of (Brown, 1980; Gibbs and O?Brien,
1991) claiming many sarcastic utterances do not
conform to the popular definition of ?saying or
writing the opposite of what you mean?. Table 3
also presents the false positive and false negative
ratios. The low false negative ratio of the baseline
confirms that while recognizing a common type
of sarcasm, the naive definition of sarcasm cannot
capture many other types sarcasm.
Newly introduced sentences (Twitter). Results
on the Twitter dataset are even better than those
obtained on the Amazon dataset, with accuracy of
0.947 (see Table 3 for precision and recall).
Tweets are less structured and are context free,
hence one would expect SASI to perform poorly
on tweets. Moreover, the positive part of the seed
is taken from the Amazon corpus hence might
seem tailored to sarcasm type targeted at prod-
ucts and part of a harsh review. On top of that,
the positive seed introduces some patterns with
tags that never occur in the Twitter test set ([prod-
uct/company/title/author]).
113
Our explanation of the excellent results is three-
fold: i) SASI?s robustness is achieved by the sparse
match (?) and incomplete match (?) that toler-
ate imperfect pattern matching and enable the use
of variations of the patterns in the learned feature
vector. ? and ? allow the introduction of patterns
with components that are absent from the posi-
tive seed, and can perform even with patterns that
contain special tags that are not part of the test
set. ii) SASI learns a model which spans a feature
space with more than 300 dimensions. Only part
of the patterns consist of meta tags that are spe-
cial to product reviews, the rest are strong enough
to capture the structure of general sarcastic sen-
tences and not product-specific sarcastic sentences
only. iii) Finally, in many cases, it might be that
the contextless nature of Twitter forces tweeters to
express sarcasm in a way that is easy to understand
from individual sentence. Amazon sentences co-
appear with other sentences (in the same review)
thus the sarcastic meaning emerges from the con-
text. Our evaluation scheme presents the annota-
tors with single sentences therefore Amazon sen-
tences might be harder to agree on.
hash gold standard (Twitter). In order to fur-
ther test out algorithm we built a model consist-
ing of the positive sample of the Amazon training,
the #sarcasm hash-tagged tweets and a sample of
non sarcastic tweets as the negative training set.
We evaluated it in a 5-fold cross validation man-
ner (only against the hash-gold standard). While
precision is still high with 0.727, recall drops to
0.436 and the F-Score is 0.545.
Looking at the hash-gold standard set, we ob-
served three main uses for the #sarcasm hashtag.
Differences between the various uses can explain
the relatively low recall. i) The tag is used as a
search anchor. Tweeters add the hashtag to tweets
in order to make them retrievable when searching
for the tag. ii) The tag is often abused and added
to non sarcastic tweets, typically to clarify that a
previous tweet should have been read sarcastically,
e.g.: ?@wrightfan05 it was #Sarcasm ?. iii) The
tag serves as a sarcasm marker in cases of a very
subtle sarcasm where the lack of context, the 140
length constraint and the sentence structure make
it impossible to get the sarcasm without the ex-
plicit marker. Typical examples are: ?#sarcasm
not at all.? or ?can?t wait to get home tonite #sar-
casm.?, which cannot be decided sarcastic without
the full context or the #sarcasm marker.
These three observations suggest that the hash-
gold standard is noisy (containing non-sarcastic
tweets) and is biased toward the hardest (insepa-
rable) forms of sarcasm where even humans get
it wrong without an explicit indication. Given
the noise and the bias, the recall is not as bad as
the raw numbers suggest and is actually in synch
with the results obtained on the Mechanical Turk
human-annotated gold standard. Table 4 presents
detailed results and the contribution of each type
of feature to the classification.
We note that the relative sparseness of sarcas-
tic utterances in everyday communication as well
as in these two datasets make it hard to accurately
estimate the recall value over these huge unanno-
tated data sets. Our experiment, however, indi-
cates that we achieve reasonable recall rates.
Punctuation Surprisingly, punctuation marks
serve as the weakest predictors, in contrast to Tep-
permann et al (2006). An exception is three con-
secutive dots, which when combine with other fea-
tures constitute a strong predictor. Interestingly
though, while in the cross validation experiments
SASI performance varies greatly (due to the prob-
lematic use of the #sarcasm hashtag, described
previously), performance based only on punctua-
tion are similar (Table 2 and Table 4).
Tsur et al (2010) presents some additional ex-
amples for the contribution of each type of feature
and their combinations.
6 Related Work
While the use of irony and sarcasm is well stud-
ied from its linguistic and psychologic aspects
(Muecke, 1982; Stingfellow, 1994; Gibbs and Col-
ston, 2007), automatic recognition of sarcasm is a
novel task, addressed only by few works. In the
context of opinion mining, sarcasm is mentioned
briefly as a hard nut that is yet to be cracked, see
comprehensive overview by (Pang and Lee, 2008).
Tepperman et al (2006) identify sarcasm in
spoken dialogue systems, their work is restricted
to sarcastic utterances that contain the expres-
sion ?yeah-right? and it depends heavily on cues
in the spoken dialogue such as laughter, pauses
within the speech stream, the gender (recognized
by voice) of the speaker and prosodic features.
Burfoot and Baldwin (2009) use SVM to deter-
mine whether newswire articles are true or satir-
ical. They introduce the notion of validity which
models absurdity via a measure somewhat close to
114
PMI. Validity is relatively lower when a sentence
includes a made-up entity or when a sentence con-
tains unusual combinations of named entities such
as, for example, those in the satirical article be-
ginning ?Missing Brazilian balloonist Padre spot-
ted straddling Pink Floyd flying pig?. We note
that while sarcasm can be based on exaggeration
or unusual collocations, this model covers only a
limited subset of the sarcastic utterances.
Tsur et al (2010) propose a semi supervised
framework for recognition of sarcasm. The pro-
posed algorithm utilizes some features specific to
(Amazon) product reviews. This paper continues
this line, proposing SASI a robust algorithm that
successfully captures sarcastic sentences in other,
radically different, domains such as twitter.
Utsumi (1996; 2000) introduces the implicit dis-
play theory, a cognitive computational framework
that models the ironic environment. The complex
axiomatic system depends heavily on complex for-
malism representing world knowledge. While
comprehensive, it is currently impractical to im-
plement on a large scale or for an open domain.
Mihalcea and Strapparava (2005) and Mihalcea
and Pulman (2007) present a system that identi-
fies humorous one-liners. They classify sentences
using naive Bayes and SVM. They conclude that
the most frequently observed semantic features are
negative polarity and human-centeredness. These
features are also observed in some sarcastic utter-
ances.
Some philosophical, psychological and linguis-
tic theories of irony and sarcasm are worth refer-
encing as a theoretical framework: the constraints
satisfaction theory (Utsumi, 1996; Katz, 2005),
the role playing theory (Clark and Gerrig, 1984),
the echoic mention framework (Wilson and Sper-
ber, 1992) and the pretence framework (Gibbs,
1986). These are all based on violation of the max-
ims proposed by Grice (1975).
7 Conclusion
We used SASI, the first robust algorithm for recog-
nition of sarcasm, to experiment with a novel
Twitter dataset and compare performance with an
Amazon product reviews dataset. Evaluating in
various ways and with different parameters con-
figurations, we achieved high precision, recall and
F-Score on both datasets even for cross-domain
training and with no need for domain adaptation.
In the future we will test the contribution of
sarcasm recognition for review ranking and sum-
marization systems and for brand monitoring sys-
tems.
References
R. L. Brown. 1980. The pragmatics of verbal irony.
In R. W. Shuy and A. Snukal, editors, Language use
and the uses of language, pages 111?127. George-
town University Press.
Clint Burfoot and Timothy Baldwin. 2009. Automatic
satire detection: Are you having a laugh? In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 161?164, Suntec, Singapore, August.
Association for Computational Linguistics.
H. Clark and R. Gerrig. 1984. On the pretence the-
ory of irony. Journal of Experimental Psychology:
General, 113:121?126.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study on
amazon.com helpfulness votes. Jun.
D. Davidov and A. Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
COLING-ACL.
Macmillan English Dictionary. 2007. Macmillan En-
glish Dictionary. Macmillan Education, 2 edition.
Raymond W Gibbs and Herbert L. Colston, editors.
2007. Irony in Language and Thought. Routledge
(Taylor and Francis), New York.
R. W. Gibbs and J. E. O?Brien. 1991. Psychological
aspects of irony understanding. Journal of Pragmat-
ics, 16:523?530.
R. Gibbs. 1986. On the psycholinguistics of sar-
casm. Journal of Experimental Psychology: Gen-
eral, 105:3?15.
H. P. Grice. 1975. Logic and conversation. In Peter
Cole and Jerry L. Morgan, editors, Syntax and se-
mantics, volume 3. New York: Academic Press.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD ?04: Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 168?177, New York, NY, USA. ACM.
A. Katz. 2005. Discourse and social-cultural factors
in understanding non literal language. In Colston H.
and Katz A., editors, Figurative language compre-
hension: Social and cultural influences, pages 183?
208. Lawrence Erlbaum Associates.
115
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang,
and Ming Zhou. 2007. Low-quality product re-
view detection in opinion summarization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 334?342.
Rada Mihalcea and Stephen G. Pulman. 2007. Char-
acterizing humour: An exploration of features in hu-
morous texts. In CICLing, pages 337?347.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. pages 531?538, Vancouver, Canada.
D.C. Muecke. 1982. Irony and the ironic. Methuen,
London, New York.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL, pages 271?278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers Inc, July.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 339?346,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Frank Jr. Stingfellow. 1994. The Meaning of Irony.
State University of NY, New York.
J. Tepperman, D. Traum, and S. Narayanan. 2006.
Yeah right: Sarcasm recognition for spoken dialogue
systems. In InterSpeech ICSLP, Pittsburgh, PA.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most help-
ful book reviews. In International AAAI Conference
on Weblogs and Social Media.
Oren Tsur, Dmitry Davidiv, and Ari Rappoport. 2010.
Icwsm ? a great catchy name: Semi-supervised
recognition of sarcastic sentences in product re-
views. In International AAAI Conference on We-
blogs and Social Media.
Akira Utsumi. 1996. A unified theory of irony and
its computational formalization. In COLING, pages
962?967.
Akira Utsumi. 2000. Verbal irony as implicit dis-
play of ironic environment: Distinguishing ironic
utterances from nonirony. Journal of Pragmatics,
32(12):1777?1806.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277? 308, January.
D. Wilson and D. Sperber. 1992. On verbal irony. Lin-
gua, 87:53?76.
116
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 83?87,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
As Long as You Name My Name Right:
Social Circles and Social Sentiment in the Hollywood Hearings
Oren Tsur
??
Dan Calacci
?
David Lazer
?b
orentsur@seas.harvard.edu dcalacci@ccs.neu.edu d.lazer@neu.edu
?
Lazer Laboratory, Northeastern University
?
School of Engeneering and Applied Sciences, Harvard University
b
Harvard Kennedy School, Harvard University
Abstract
The Hollywood Blacklist was based on
a series of interviews conducted by the
House Committee on Un-American Activ-
ities (HUAC), trying to identify members
of the communist party. We use various
NLP algorithms in order to automatically
analyze a large corpus of interview tran-
scripts and construct a network of the in-
dustry members and their ?naming? rela-
tions. We further use algorithms for Senti-
ment Analysis in order to add a psycholog-
ical dimension to the edges in the network.
In particular, we test how different types
of connections are manifested by different
sentiment types and attitude of the inter-
viewees. Analysis of the language used in
the hearings can shed new light on the mo-
tivation and role of network members.
1 Introduction
A growing body of computational research is
focused on how language is used and how it
shapes/is shaped by a community of speakers.
Computational works in the nexus of language
and the social arena deal with various topics such
as language accommodation (Danescu-Niculescu-
Mizil and Lee, 2011; Danescu-Niculescu-Mizil
et al., 2011), demographic language variation
(Eisenstein et al., 2010; O?Connor et al., 2010),
the factors that facilitate the spread of information
in Q&A forums and social networks (Adamic et
al., 2008; Bian et al., 2009; Romero et al., 2011) or
the correlation between words and social actions
(Adali et al., 2012).
All of these works analyze the language and the
social dynamics in online communities, mainly
due to the increasing popularity of online social
networks and greater availability of such data.
However, large scale socio-linguistic analysis
should not be restricted to online communities and
can be applied in many social and political settings
beyond the online world. Two examples are the
study of power structures in arguments before the
U.S. Supreme Court (Danescu-Niculescu-Mizil et
al., 2012) and the evolution of specific words and
phrases over time as reflected in Google Books
(Goldberg and Orwant, 2013).
In this paper we propose using network science
and linguistic analysis in order to understand the
social dynamics in the entertainment industry dur-
ing one of its most controversial periods ? the ?red
scare? and the witch hunt for Communists in Hol-
lywood during 1950?s.
Historical background The Hollywood hear-
ings (often confused with Senator McCarthy?s
hearings and allegations) were a series of inter-
views conducted by the House Committee on Un-
American Activities (HUAC) in the years 1947?
1956. The purpose of the committee was to
conduct ?hearings regarding the communist in-
filtration of the motion picture industry? (from
the HUAC Annual Report). The committee sub-
poenaed witnesses such as Ayn Rand (writer),
Arthur Miller (writer), Walt Disney (producer), fu-
ture U.S. president Ronald Reagan (Screen Actors
Guild), Elia Kazan (writer, actor, director) and Al-
bert Maltz (Screen Writers Guild). Some of the
witnesses were ?friendly? while some others were
uncooperative
1
, refusing to ?name names? or self
incriminate
2
. Those who were named and/or were
uncooperative were often jailed or effectively lost
their job.
Arguably, many friendly witnesses felt they
were complying with their patriotic duty. Many
1
A note about terminology: by using the terms friendly
and uncooperative there is no implied moral judgment ? these
are the terms used in the literature.
2
It should be noted that being a member of the Communist
party was not illegal, however, some individuals avoided self
?incrimination? either in an effort to protect their job or as
an ideological declaration in favor of privacy protection as a
civil right protected by the constitution.
83
others were threatened or simply manipulated to
name names, and some later admitted to coop-
erating for other reasons such as protecting their
work or out of personal vendettas and professional
jealousies. It is also suspected that some nam-
ing occurred due to increasing professional ten-
sion between some producers and the Screen Writ-
ers Guild or (Navasky, 2003).
Motivation In this work we analyze a collection
of HUAC hearings. We wish to answer the follow-
ing questions:
1. Do sentiment and other linguistic categories
correlate with naming relations?
2. Can we gain any insight on the social dynam-
ics between the people in the network?
3. Does linguistic and network analysis support
any of the social theories about dynamics at
Hollywood during that time?
In order to answer the questions above we build
a social graph of members of the entertainment in-
dustry based on the hearings and add sentiment la-
bels on the graph edges. Layering linguistic fea-
tures on a the social graph may provide us with
new insights related to the questions at hand. In
this short paper we describe the research frame-
work, the various challenges posed by the data and
present some initial promising results.
2 Data
In this work we used two types of datasets: Hear-
ing Transcripts and Annual Reports. Snippets
from hearings can be found in Figures 1(a) and
1(b), Figure 1(c) shows a snippet from an annual
report. The transcripts data is based on 47 inter-
views conducted by the HUAC in the years 1951?
2. Each interview is either a long statement (1(a) )
or a sequence of questions by the committee mem-
bers and answers by a witness (1(b)). In total, our
hearings corpus consists of 2831 dialogue acts and
half a million words.
3 Named Entity Recognition and
Anaphora Resolution
The snippets in Figure 1 illustrates some of the
challenges in processing HUAC data. The first
challenge is introduced by the low quality of the
available documents. Due to the low quality of
(a) A snippet from the testimony of Elia Kazan, (actor, writer and director, 3
times Academy Awards winner), 4.10.1952.
(b) A snippet from the testimony of Harold Ashe?s (journalist) testimony 9.17-
19.1951.
(c) A snippet from 1951 annual report.
Figure 1: Snippets from HUAC hearings and an
annual report.
the documents the OCR output is noisy, contain-
ing misidentified characters, wrong alignment of
sentences and missing words. These problems in-
troduce complications in tasks like named entity
recognition and properly parsing sentences.
Beyond the low graphic quality of the docu-
ments, the hearings present the researcher with the
typical array of NLP challenges. For example, the
hearing excerpt in 1(b) contains four dialogue acts
that need to be separated and processed. The com-
mittee member (Mr. Tavenner) mentions the name
Stanley Lawrence, later referred to by the witness
(Mr. Ashe) as Mr. Lawrence and he thus corefer-
ence resolution is required before the graph con-
struction and the sentiment analysis phases.
As a preprocessing stage we performed named
entity recognition (NER), disambiguation and uni-
fication. For the NER task we used the Stanford
NER (Finkel et al., 2005) and for disambiguation
and unification we used a number of heuristics
based on edit distance and name distribution.
84
We used the Stanford Deterministic Corefer-
ence Resolution System (Lee et al., 2011) to re-
solve anaphoric references.
4 Naming Graph vs. Mentions Graph
In building the network graph of the members of
the entertainment industry we distinguish between
mentioning and naming in our data. While many
names may be mentioned in a testimony (either by
a committee member or by the witness, see ex-
ample in Figures 1(a) and 1(b)), not all names are
practically ?named? (=identified) as Communists.
We thus use the hearings dataset in order to build
a social graph of mentions (MG) and the annual re-
ports are used to build a naming graph (NG). The
NG is used as a ?gold standard? in the analysis
of the sentiment labels in the MG. Graph statistics
are presented in Table 1.
While the hearings are commonly perceived as
an ?orgy of informing? (Navasky, 2003), the dif-
ference in network structure of the graphs portrays
a more complex picture. The striking difference in
the average out degree suggests that while many
names were mentioned in the testimonies (either
in a direct question or in an answer) ? majority of
the witnesses avoided mass-explicit naming
3
. The
variance in outdegree suggests that most witnesses
did not cooperate at all or gave only a name or
two, while only a small number of witnesses gave
a long list of names. These results are visually
captured in the intersection graph (Figure 2) and
were also manually verified.
The difference between the MG and the NG
graph in the number of nodes with out-going edges
(214 vs. 66) suggests that the HUAC used other
informers that were not subpoenaed to testify in a
hearing
4
.
In the remainder of this paper we analyze the the
distribution of the usage of various psychological
categories based on the role the witnesses play.
5 Sentiment Analysis and Psychological
Categories
5.1 Sentiment Analysis
We performed the sentiment analysis in two dif-
ferent settings: lexical and statistical. In the lexi-
3
Ayn Rand and Ronald Reagan, two of the most ?friendly?
witnesses (appeared in front of the HUAC in 1947), did not
name anyone.
4
There might be some hearings and testimonies that are
classified or still not publicly accessible.
MG NG Intersection
Num of nodes 1353 631 122
Num of edges 2434 842 113
Nodes / Edges 0.55 0.467 1
Avg. out degree 36.87 3.93 8.7
Avg. in degree 1.82 1.83 1.04
Var(outdegree) 3902.62 120.75 415.59
Var(indegree) 4.0 2.51 1.04
Nodes with out going edges 66 214 13
Nodes with incoming edges 1341 459 109
Reciprocity 0.016 0.012 0
Table 1: Network features of the Mentions graph,
the Naming graph and the intersection of the
graphs.
Figure 2: Naming graph based on the intersec-
tion of the mentions and the naming data. Larger
node size indicates a bigger out degree; Color in-
dicates the in degree (darker nodes were named
more times).
cal setting we combine (Ding et al., 2008) and the
LIWC lexicon (Tausczik and Pennebaker, 2010).
In the statistical setting we use NaSent (Socher et
al., 2013).
The motivation to use both methods is twofold:
first ? while statistical models are generally more
robust, accurate and sensitive to context, they re-
quire parsing of the processed sentences. Parsing
our data is often problematic due to the noise in-
troduced by the OCR algorithm due to the poor
quality of the documents (see Figure 1). We ex-
pected the lexicon-based method to be more toler-
ant to noisy or ill-structured sentences. We opted
for the LIWC since it offers an array of sentiment
and psychological categories that might be rele-
vant in the analysis of such data.
85
Stanford LIWC
Pos 75 292
Neg 254 37
Table 2: Confusion matrix for Stanford and LIWC
sentiment algorithms.
Aggregated Sentiment A name may be men-
tioned a number of times in a single hearing, each
time with a different sentiment type or polarity.
The aggregated sentiment weight of a witness i to-
ward a mentioned name j is computed as follows:
sentiment(i, j) = max
c?CAT
?
k?U
ij
score(u
k
ij
, c)
|U
ij
|
(1)
Where CAT is the set of categories used by
LIWC or Stanford Sentiment and U
ij
is the set
of all utterances (dialogue acts) in which witness
i mentions the name j. The score() function is
defined slightly different for each setting. In the
LIWC setting we define score as:
score(u
k
ij
, c) =
|{w ? u
k
ij
|w ? c}|
|u
k
ij
|
(2)
In the statistical setting, Stanford Sentiment re-
turns a sentiment category and a weight, we there-
fore use:
score(u
k
ij
, c) =
{
w
c
, if sentiment found
0, if c was not returned
(3)
Unfortunately, both approaches to sentiment
analysis were not as useful as expected. Most
graph edges did not have any sentiment label, ei-
ther due to the limited sentiment lexicon of the
LIWC or due to the noise induced in the OCR
process, preventing the Stanford Sentiment engine
from parsing many of the sentences. Interestingly,
the two approaches did not agree on most sen-
tences (or dialogue acts). The sentiment confu-
sion matrix is presented in Table 2, illustrating the
challenge posed by the data.
5.2 Psychological Categories
The LIWC lexicon contains more than just posi-
tive/negative categories. Table 3 presents a sample
of LIWC categories and associated tokens. Fig-
ure 3 presents the frequencysave in which each
psychological category is used by friendly and un-
cooperative witnesses. While the Pronoun cate-
gory is equally used by both parties, the uncooper-
ative witnesses tend to use the I, Self and You cate-
gories while the friendly witnesses tend to use the
Other and Social. A somewhat surprising result
is that the Tentat category is used more by friendly
witnesses ? presumably reflecting their discomfort
with their position as informers.
Figure 3: Frequencies of selected LIWC cate-
gories in friendly vs. uncooperative testimonies.
Category Typical Words
Cogmech abandon, accept, avoid, admit, know, question
Excl although, besides, but, except
I I, I?d, I?ll, I?m, I?ve, me, mine, my, myself
Insight accept, acknowledge, conclude, know, rational
job work, position, benefit, duty
Negate no, nope, nothing, neither, never, isn?t , can?t
Other he, him, herself, them
Preps about, against, along, from, outside, since
Pronouns I, anybody, anyone, something, they, you
Self I, mine, ours, myself, us
Social acquaintance, admit, party, comrade, confess, friend, human
Tentat ambiguous, tentative, undecided, depend, hesitant, guess
You thou, thoust, thy, y?all, ya, ye, you, you?d
Table 3: LIWC categories and examples of typical
words
6 Conclusion and Future Work
In this short paper we take a computational ap-
proach in analyzing a collection of HUAC hear-
ings. We combine Natural Language Process-
ing and Network Science techniques in order to
gain a better understanding of the social dynam-
ics within the entertainment industry in its dark-
est time. While sentiment analysis did not prove
as useful as expected, analysis of network struc-
tures and the language usage in an array of psycho-
logical dimensions reveals differences between
friendly and uncooperative witnesses.
Future work should include a better preprocess-
ing of the data, which is also expected to improve
the sentiment analysis. In future work we will an-
alyze the language use in a finer granularity of wit-
ness categories, such as the ideological informer,
the naive informer and the vindictive informer. We
also hope to expand the hearings corpora to in-
clude testimonies from more years.
References
Sibel Adali, Fred Sisenda, and Malik Magdon-Ismail.
2012. Actions speak as loud as words: Predicting
86
relationships from social behavior data. In Proceed-
ings of the 21st international conference on World
Wide Web, pages 689?698. ACM.
Lada A Adamic, Jun Zhang, Eytan Bakshy, and Mark S
Ackerman. 2008. Knowledge sharing and yahoo
answers: everyone knows something. In Proceed-
ings of the 17th international conference on World
Wide Web, pages 665?674. ACM.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World Wide
Web, pages 51?60. ACM.
Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011. Chameleons in imagined conversations: A
new approach to understanding coordination of lin-
guistic style in dialogs. In Proceedings of the Work-
shop on Cognitive Modeling and Computational
Linguistics, ACL 2011.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words! Lin-
guistic style accommodation in social media. In
Proceedings of WWW, pages 745?754.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: Language effects and power differences in
social interaction. In Proceedings of WWW, pages
699?708.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the 2008 International Conference
on Web Search and Data Mining, WSDM ?08, pages
231?240, New York, NY, USA. ACM.
Jacob Eisenstein, Brendan O?Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1277?1287. Asso-
ciation for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Yoav Goldberg and Jon Orwant. 2013. Syntactic-
ngrams over time from a very large corpus of english
books. In Second Joint Conference on Lexical and
Computational Semantics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28?34. Association for Computational
Linguistics.
Victor S Navasky. 2003. Naming Names: With a New
Afterword by the Author. Macmillan.
Brendan O?Connor, Jacob Eisenstein, Eric P Xing, and
Noah A Smith. 2010. A mixture model of demo-
graphic lexical variation. In Proceedings of NIPS
workshop on machine learning in computational so-
cial science, pages 1?7.
Daniel M Romero, Brendan Meeder, and Jon Klein-
berg. 2011. Differences in the mechanics of in-
formation diffusion across topics: idioms, politi-
cal hashtags, and complex contagion on twitter. In
Proceedings of the 20th international conference on
World wide web, pages 695?704. ACM.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631?1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Com-
puterized Text Analysis Methods. Journal of Lan-
guage and Social Psychology, 29(1):24?54, March.
87
