An HPSG-to-CFG Approximation of Japanese 
Bernd Kiefer, Hans-Ulrich Krieger, Melanie Siegel 
German Research Center for Artificial Intell igence (DFKI )  
Stuhlsatzenhausweg 3, D-66123 Saarbri icken 
{kiefer, krieger, siegel}@dfki, de 
Abstract 
We present asimple approximation method for turn- 
ing a Head-Driven Phrase Structure Grammar into a 
context-free grammar. The approximation method 
can be seen as the construction of the least fixpoint 
of a certain monotonic hmction. We discuss an ex- 
periment with a large HPSG for Japanese. 
1 In t roduct ion  
This paper presents a simple approximation 
method for turning an HPSG (Pollard and Sag, 
1994) into a context-free grmnmar. The the- 
oretical underpinning is established through a 
least fixpoint construction over a certain mono- 
tonic function, similar to the instantiation of 
a rule in a bottom-up passive chart parser or 
to partial evaluation in logic programming; see 
(Kiefer and Krieger, 2000a). 
1.1 Bas ic  Idea  
The intuitive idea underlying our approach is 
to generalize in a first step the set of all lexicon 
entries. The resulting structures form equiv- 
alence classes, since they abstract from word- 
specific information, such as FORN or STEM. The 
abstraction is specified by means of a restrictor 
(Shiet)er, 1985), the so-called lexicon rcstrictor. 
The grammar rules/schemata are then instan- 
tiated via unification, using the abstracted lexi- 
con entries, yielding derivation trees of depth 1. 
We apply the rule restrictor to each resulting 
feature structure, which removes all information 
contained only in the daughters of the rule. Due 
to the Locality Principle of HPSG, this deletion 
does not alter the set of derivable feature struc- 
tures. Since we are interested in a finite fixpoint 
from a practical point of view, the restriction 
also gets rid of information that will lead to in- 
finite growth of feature structures during deriva- 
tion. Additionally, we throw away information 
that will not restrict the search space (typically, 
parts of tile semantics). The restricted fea- 
ture structures (together with older ones) then 
serve as tile basis for the next instantiation step. 
Again, this gives us feature structures encoding 
a derivation, and again we are applying the rule 
restrictor. We proceed with the iteration, until 
we reach a fixpoint, meaning that further itera- 
tion steps will not add (or remove) new (o1" old) 
feature structures. 
Our goal, however, is to obtain a context-fl'ee 
grammar, trot since we have reached a fixpoint, 
we can use the entire feature structures as (com- 
plex) context-free symbols (e.g., by nlapping 
them to integers). By instantiating the HPSG 
rules a final time with feature structures from 
the fixpoint, applying the rule restrictor and 
finally classifying the resulting structure (i.e., 
find tile right structure from the fixpoint), one 
can easily obtain tile desired context-free grain- 
mar (CFG). 
1.2 Why is it Wor th?  
Approximating an HPSG through a CFG ~ is 
interesting for the following practical reason: 
assuming that we have a CFG that comes close 
to an HPSG, we can use the CFG as a cheap fil- 
ter (running time complexity is O(IGI 2 x n 3) for 
an arbitrary sentence of length n). The main 
idea is to use the CFG first and then let the 
HPSG deterministically replay the derivations 
licensed by the CFG. The important point here 
is that one can find for every CF production 
exactly one and only one HPSG rule. (Kasper 
et al, 1996) describe such an approach for word 
graph parsing which employs only the relatively 
unspecific CF backbone of an HPSG-like grmn- 
mar. (Diagne et al, 1995) replaces the CF back- 
bone through a restriction of the original HPSG. 
This grammar, however, is still an unification- 
1046 
based grammar, since it employs coreference 
constraints. 
1..3 Content  of  Paper 
In tile next section, we describe the Japanese 
HPSG that is used in Verbmobil, a project that 
deals with the translation of spontaneously spo- 
ken dialogues between English, German, and 
Japanese speakers. After that, section 3 ex- 
plains a simplified, albeit correct version of the 
implemented algorithm. Section 4 then dis- 
cusses the outcome of the approximation pro- 
cess .  
2 Japanese  Grammar  
The grammar was developed for machine trans- 
lation of spoken dialogues. It is capable of deal- 
ing with spoken language phenomena nd un- 
grammatical or corrupted input. This leads on 
the one hand to the necessity of robustness and 
on the other hand to mnbiguitics that must be 
dealt with. Being used in an MT system for spo- 
ken language, the grammar must firstly accept 
fragmentary input and bc able to deliver partial 
analyses, where no spanning analysis is awdl- 
able. A coinplete fragmentary utterance could, 
e.g., be: 
dai~oubu 
OKay 
This is an adjective without any noun or (cop- 
ula) verb. There is still an analysis available. 
If an utterance is corrupted by not being fully 
recognized~ the grammar delivers analyses for 
those parts that could be understood. An ex- 
ample would be the following transliteration of
input to the MT system: 
son desu ne watakushi 
so COP TAG i 
no hou wa dai~oubu 
GEN side 'FOP okay 
desu da ga kono hi 
COP but this day 
wa kayoubi desu ~te 
TOP Tuesday COP TAG 
(lit.: Well, it is okay for my side, but 
this day is ~l~msday, isn't it?) 
Here, analyses for the following fragments arc 
delivered (where the parser found opera wa in 
the word lattice of the speech recognizer): 
sou dcsu nc watakushi 
so COP TAG I 
no hou wa dai{oubu 
GEN side TOP okay 
dCSlt 
COP 
(Well, it is okay for my side.) 
era  TOP 
(The opera) 
hone hi wa kayoubi 
this day TOP Tlmsday 
desu nc 
COP TAG 
(This (lay is 3hmsday, isn't it?) 
Another necessity for partial analysis comes 
fl'om real-time restrictions imposed by the MT 
system. If tile parser is not allowed to produce 
a spanning analysis, it delivers best partial frag- 
ments. 
rl'tle grammar must also be applicable to phe- 
nomena of spoken language. A typical problem 
is tile extensive use of topicalization and even 
omission of particles. Also serialization of parti- 
cles occur nlore often than in written language, 
as described in (Siegel, 1999). A well-defined 
type hierarchy of Japanese particles is necessary 
here to describe their functions in the dialogues. 
Extensive use of honorification is another sig- 
nificance of spoken Japanese. A detailed de- 
scription is necessary for different purposes in 
an MT system: honorification is a syntactic 
restrictor in subject-verb agreement and com- 
plement sentences, l~lrthermore, it is a very 
useflfl source of information for the solution 
of zero pronominalization (Metzing and Siegel, 
1994). It is finally necessary for Japanese gener- 
ation in order to tind the appropriate honorific 
forms. The sign-based in%rmation structure of 
HPSG (Pollard and Sag, 1994) is predestined 
to describe honorification on the different levels 
of linguistics: on the syntactic level for agree- 
ment phenomena, on tile contextual level for 
anaphora resolution and connection to speaker 
and addressee reference, and via co-indexing on 
the semantic level. Connected to honorification 
is the extensive use of auxiliary and light verb 
constructions that require solutions in the areas 
of morphosyntax, semantics, and context (see 
(Siegel, 2000) for a more detailled description). 
Finally, a severe problem of tile Japanese 
grammar in the MT system is the high po- 
1047 
tential of ambiguity arising from the syntax of 
Japanese itself, and especially from the syntax 
of Japanese spoken language. For example, the 
Japanese particle ga marks verbal argmnents in 
most cases. There are, however, occurrences of 
ga that are assigned to verbal adjuncts. Allow- 
ing g a in any case to mark arguments or ad- 
juncts would lead to a high potential of (spuri- 
ous) ambiguity. Thus, a restriction was set on 
the adjunctive g a, requiring the modified verb 
not to have any unsaturated ga arguments. 
The Japanese language allows many verbal 
arguments to be optional. For example, pro- 
nouns are very often not uttered. This phe- 
nomenon is basic for spoken Japanese, such that 
a syntax urgently needs a clear distinction be- 
tween optional and obligatory (and adjacent) 
arguments. We therefore used a description 
of subcategorization that differs from standard 
HPSG description in that it explicitly states the 
optionality of arguments. 
3 Bas ic  Algor i thm 
We stm't with the description of the top-level 
function HPSG2CFG which initiates the ap- 
proximation process (cf. section 1.1 for the 
main idea). Let 7~ be the set of all rules/rule 
schemata, 12 the set of all lexicon entries, R 
the rule restrictor, and L the lexicon restrictor. 
We begin the approximation by first abstract- 
ing from the lexicon entries /2 with the help of 
the lexicon restrictor L (line 5 of the algorithm). 
This constitutes our initial set To (line 6). Fi- 
nally, we start the fixpoint iteration calling It- 
crate with the necessary parameters. 
1 HPSG2CFG(T~, 12, R, L) :~==~ 
2 local To; 
3 T0 := (~; 
4 for each  l E/2 
5 l :=  L(1); 
6 To := To u {l}; 
7 Iterate(T~, R, To). 
After that, the instantiation of the rule 
schemata with rule/lexicon-restricted elements 
from the previous iteration Ti begins (line 11- 
14). Instantiation via unification is performed 
by Fill-Daughters which takes into account a 
single rule r and Ti, returning successful instan- 
tiations (line 12) to which we apply the rule 
restrictor (line 13). The outcome of this restric- 
tion is added to the actual set of rule-restricted 
feature structures Ti+l iff it is new (remember 
how set union works; line 14). In case that re- 
ally new feature structures have not been added 
during the current iteration (line 15), meaning 
that we have reached a fixpoint, we immediately 
exit with T/ (line 16) from which we generate 
the context-free rules as indicated in section 1.1. 
Otherwise, we proceed with the iteration (line 
17). 
8 Iterate(g, R, Ti) :?==v 
9 local Ti+j; 
10 Ti+~ := Ti; 
11 for each r E T~ 
12 for each t C Fill-Daughters(r, Ti) do 
13 t := R(t); 
14 Ti+I := Ti+I U {t}; 
15 i f  Ti = T/+I 
16 then  re turn  Cornpute-CF-Rules(TG i)
17 else Iterate(7~, R, Ti+l). 
We note here that the pseudo code above is 
only a naYve version of the implemented algo- 
rithm. It is still correct, but not computation- 
ally tractable when dealing with large HPSG 
grammars. Technical details and optimizations 
of the actual algorithm, together with a descrip- 
tion of the theoretical foundations are described 
in (Kiefer and Krieger, 2000a). Due to space 
limitations, we can only give a glimpse of the 
actual implementation. 
Firstly, the most obvious optimization applies 
to the function Fill-Daughters (line 12), where 
the number of unifications is reduced by avoid- 
ing recomputation of combinations of daugh- 
ters and rules that already have been checked. 
To do this in a simple way, we split the set Ti 
into Ti \ T/.-1 and T/_I and fill a rule with only 
those permutations of daughters which contain 
at least one element from T/ \ r / _  1 . This guaran- 
tees checking of only those configurations which 
were enabled by the last iteration. 
Secondly, we use techniques developed in 
(Kiefer et al, 1999), namely the so-called rule 
filter and the quick-check method. The rule fil- 
ter precomputes the applicability of rules into 
each other and thus is able to predict a fail- 
ing unification using a simple and fast table 
lookup. The quick-check method exploits the 
1048 
flint that unification fails snore often at cer- 
tain points in feature structnres than at oth- 
ers. In an off  line stage, we parse a test cor- 
pus, using a special unifier that records all fail- 
ures instead of bailing out after the first one 
in order to determine the most prominent fail- 
ure points/paths. These points constitute the 
so-called quick-check vector. When executing a
unification during approximation, those points 
are efficiently accessed and checked using type 
unification prior to the rest of the structure. Ex- 
actly these quick-check points are used to build 
the lexicon and the rule restrictor as described 
earlier (see fig. 1). During ore: experinmnts, 
nearly 100% of all failing unifications in Fill- 
Daughters could be quickly detected using the 
above two techniques. 
Thirdly, instead of using set union we use 
tlhe more elaborate operation during the addi- 
tion of new feature structures to T/.+I. In fact, 
we add a new structure only if it is not sub- 
sumed by some structure already in tile set. To 
do this efficiently, tile quick-check vectors de- 
scribed above are employed here: before per- 
fl)rming full feature structure subsnmption, we 
pairwise check the elements of the vectors us- 
ing type subsumption and only if this succeeds 
do a full subsmnption test. If we add a new 
structure, we also remove all those structures in 
7)ql that are subsumed by the slew structure 
in order to keep the set small. This does not 
change the language of tile resulting CF gram- 
mar because a more general structure can be 
put into at least those daughter positions which 
can be fillcd by the more specific one. Conse- 
quently, fbr each production that employs the 
more specific structure, there will be a (pos- 
sibly) more general production employing the 
more general structure in the same daughter po- 
sitions. Extending feature structure subsump- 
lion by quick-check subsumption definitely pays 
off: more than 98% of all failing subsumptions 
could be detected early. 
Further optimizations to make the algorithm 
works in practice are described in (Kiefer and 
Krieger, 2000b). 
4 Eva luat ion  
The Japanese HPSG grammar used in our ex- 
periment consists of 43 rule sdmmata (28 unary, 
15 binary), 1,208 types and a test lexicon of 
2,781 highly diverse entries. The lexicon restric- 
tot, as introduced in section 1.1 and depicted in 
figure 1, maps these entries onto 849 lexical ab- 
stractions. This restrictor tells us which parts of 
a feature structure have to be deleted---it s the 
kind of restrictor which we are usually going to 
use. We call this a negative restrictor, contrary 
to tile positive restrictors used in the PATR- 
II system that specii\[y those parts of a feature 
structure which will survive after restricting it. 
Since a restrictor could have reentrance points, 
one can even define a reeursivc (or cyclic) re- 
strictor to foresee recursive mbeddings as is the 
case in HPSG. 
The rule restrictor looks quite silnilar, cut- 
ling off additionally information contained only 
in the daughters. Since both restrictors remove 
the CONTENT feature (and hence the semantics 
which is a source of infinite growth), it hal> 
pened that two very productive head-adjunct 
schemata could be collapsed into a single rule. 
Tiffs has helped to keep the number of feature 
structures in the fixpoint relatively small. 
We reached the fixpoint after 5 iteration 
steps, obtaining 10,058 featnre structures. The 
comtmtation of the fixpoint took about 27.3 
CPU hours on a 400MHz SUN Ultrasparc 2with 
t~?anz Allegro Common Lisp under Solaris 2.5. 
Given tim feature structures from the fixpoint, 
the 43 rules might lead to 28 x 10,058-t- 15 x 
10,058 x 10,058 = 1, 51.7,732,084 CF produc- 
tions in the worst case. Our method produces 
19,198,592 productions, i.e., 1.26% of all pos- 
sible ones. We guess that the enormous et of 
productions is due tile fact that the grammar 
was developed for spoken Japanese (recall sec- 
tion 2 on the mnbiguity of Japanese). Likewise, 
the choice of a 'wrong' restrictor often leads to a 
dramatic increase of structures in the fixpoint, 
and hence of CF rules--we are not sure at this 
point whether our restrictor is a good compro- 
mise between tile specificity of the context-free 
language and the number of context-free rules. 
We are currently implementing a CF parser that 
can handle such an enormous et of CF rules. 
In (Kiefer and Krieger, 2000b), we report on 
a similar experiment that we carried out using 
the English Verbmobil grmnmar, developed at 
CSLI, Stanford. In this paper, we showed that 
the workload on the HPSG side can be drasti- 
cally reduced by using a CFG filter, obtained 
1049 
-PHON 
FORM 
SYNSEM LOCAL 
-CONTENT 
CONTEXT 
HEAD 
CAT 
SUBCAT 
3PEC ~\ ]  
"NONLOCAL 
-CONTENT 
CONTEXT 
LOCAL 
~AT 
M0D 
MARK \[~\] 
FORMAL 
MODUS 
P0S 
PTYPE 
10BJIII I 
I OBJ2 I i I I 
VAL ~ 1 
I SpR l J  ! 
I SUBJ I 1 I I 
SUBCAT 
HEAD 
POS 11 SPEC FORMAl MARK 
MOD 
Figure 1: The lexicon restrictor used during the approximation of the Japanese grammar. In 
addition, the rule restrictor cuts off the DAUGHTERS feature. 
from the HPSG. Our hope is that these results 
can be carried over to the Japanese grammar. 
Acknowledgments  
This research was supported by the German 
Ministry for Education, Science, Research, and 
Technology under grant no. 01 IV 701 V0. 
Re ferences  
Abdel Kader Diagne, Walter Kasper, and Hans- 
Ulrich Krieger. 1995. Distributed parsing with 
HPSG grammars. In Proceedings of the ~th Inter- 
national Workshop on Parsing Technologies~ IW- 
PT'95, pages 79-86. 
Walter Kasper, Hans-Ulrich Krieger, JSrg Spilker, 
and Hans Webcr. 1996. From word hypotheses to
logical form: An efficient interleaved approach. In 
D. Gibbon, editor, Natural Language Processing 
and Speech Technology, pages 77-88. Mouton de 
Gruyter, Berlin. 
Bernd Kiefer and Hans-Ulrich Krieger. 2000a. 
A context-free approximation of Head-Driven 
Phrase Structure Grmnmar. In Proceedings of the 
6th International Workshop on Parsing Technolo- 
gies, IWPT2000, pages 135-146. 
Bernd Kicfcr and Hans-UMch Kricger. 2000b. Ex- 
periments with an HPSG-to-CFG approximation. 
Research report. 
Bernd Kicfcr, Hans-Ulrich Kricger, John Carroll, 
and Rob Malouf. 1999. A bag of useful techniques 
for emcient and robust parsing. In Proceedings of 
the 37th Annual Meeting of the Association for 
Computational Linguistics, pages 473-480. 
Dieter Metzing and Melanin Siegel. 1994. Zero pro- 
noun processing: Some requirements tbr a Verb- 
mobil system. Verbmobil-Memo 46. 
Carl Pollard and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar. Studies in Contem- 
porary Linguistics. University of Chicago Press, 
Chicago. 
Stuart M. Shiebcr. 1985. Using restriction to extend 
parsing algorithms for complex-feature-based for- 
malisms. In Proceedings of the 23rd Annual Meet- 
ing of the Association for Computational Linguis- 
tics, pages 145-152. 
Melanin Siegel. 1999. The syntactic processing of 
particles in Japanese spoken language. In PTv- 
cecdings of the 13th Pacific Asia Confcrcncc on 
Language, Information and Computation, pages 
313-320. 
Melanic Siegel. 2000. Japanese honorification i an 
HPSG framework. In Proceedings of the l~th Pa- 
cific Asia Conference on Language, Information 
and Computation, pages 289-300. 
1050 
Generating and Visualizing a Soccer Knowledge Base
Paul Buitelaar, Thomas Eigner, Greg Gul-
rajani, Alexander Schutz, Melanie Siegel,
Nicolas Weber
Language Technology Lab, DFKI GmbH
Saarbr?cken, Germany
{paulb,siegel}@dfki.de
Philipp Cimiano, G?nter Ladwig,
Matthias Mantel, Honggang Zhu
Institute AIFB, University of Karlsruhe
Karlsruhe, Germany
cimiano@aifb.uni-karlsruhe.de
Abstract
This demo abstract describes the SmartWeb
Ontology-based Annotation system (SOBA).
A key feature of SOBA is that all informa-
tion is extracted and stored with respect to
the SmartWeb Integrated Ontology
(SWIntO). In this way, other components of
the systems, which use the same ontology,
can access this information in a straightfor-
ward way. We will show how information
extracted by SOBA is visualized within its
original context, thus enhancing the browsing
experience of the end user.
1 Introduction
SmartWeb1 is a multi-modal dialog system,
which derives answers from unstructured re-
sources such as the Web, from automatically ac-
quired knowledge bases and from web services.
In this paper we describe the current status of
the SmartWeb Ontology-Based Annotation
(SOBA) system. SOBA automatically populates
a knowledge base by information extraction from
soccer match reports as available on the web.
The extracted information is defined with respect
to SWIntO, the underlying SmartWeb Integrated
Ontology (Oberle et al, in preparation) in order
to be smoothly integrated into the system.
The ability to extract information and describe
it ontologically is a basic requirement for more
complex processing tasks such as reasoning and
discourse analysis (for related work on ontology-
based information extraction see e.g. Maedche et
al., 2002; Lopez and Motta, 2004; M?ller et al,
2004; Nirenburg and Raskin, 2004).
1 http://www.smartweb-projekt.de/start_en.html
2 System Overview
The SOBA system consists of a web crawler,
linguistic annotation components and a compo-
nent for the transformation of linguistic annota-
tions into an ontology-based representation.
The web crawler acts as a monitor on relevant
web domains (i.e. the FIFA2 and UEFA3 web
sites), automatically downloads relevant
documents from them and sends them to a
linguistic annotation web service.
Linguistic annotation and information
extraction is based on the Heart-of-Gold (HoG)
architecture (Callmeier et al 2004), which
provides a uniform and flexible infrastructure for
building multilingual applications that use
semantics- and XML-based natural language
processing components.
The linguistically annotated documents are
further processed by the transformation
component, which generates a knowledge base
of soccer-related entities (players, teams, etc.)
and events (matches, goals, etc.) by mapping
annotated entities or events to ontology classes
and their properties.
Finally, an automatic hyperlinking component
is used for the visualization of extracted entities
and events. This component is based on the
VieWs system, which was developed
independently of SmartWeb (Buitelaar et al,
2005). In what follows we describe the different
components of the system in detail.
2.1 Web Crawler
The crawler enables the automatic creation of a
football corpus, which is kept up-to-date on a
daily basis. The crawler data is compiled from
texts, semi-structured data and copies of original
2 http://fifaworldcup.yahoo.com/
3 http://www.uefa.com/
123
HTML documents. For each football match, the
data source contains a sheet of semi-structured
data with tables of players, goals, referees, etc.
Textual data comprise of match reports as well as
news articles.
The crawler is able to extract data from two
different sources: FIFA and UEFA. Semi-
structured data, news articles and match reports
covering the WorldCup2006 are identified and
collected from the FIFA website. Match reports
and news articles are extracted from the UEFA
website. The extracted data are labeled by IDs
that match the filename. The IDs are derived
from the corresponding URL and are thus
unique.
The crawler is invoked continuously each day
with the same configuration, extracting only data
which is not yet contained in the corpus. In order
to distinguish between available new data and
data already present in the corpus, the URLs of
all available data from the website are matched
against the IDs of the already extracted data.
2.2 Linguistic Annotation and Information
Extraction
As mentioned before, linguistic annotation in the
system is based on the HoG architecture, which
provides a uniform and flexible infrastructure for
building multilingual applications that use
semantics- and XML-based natural language
processing components.
For the annotation of soccer game reports, we
extended the rule set of the SProUT (Drozdzyn-
ski et al 2004) named-entity recognition compo-
nent in HoG with gazetteers, part-of-speech and
morphological information. SProUT combines
finite-state techniques and unification-based al-
gorithms. Structures to be extracted are ordered
in a type hierarchy, which we extended with soc-
cer-specific rules and output types.
SProUT has basic grammars for the annotation
of persons, locations, numerals and date and time
expressions. On top of this, we implemented
rules for soccer-specific entities, such as actors in
soccer (trainer, player, referee ?), teams, games
and tournaments. Using these, we further imple-
mented rules for soccer-specific events, such as
player activities (shots, headers ?), game events
(goal, card ?) and game results. A soccer-
specific gazetteer contains soccer-specific enti-
ties and names and is supplemented to the gen-
eral named-entity gazetteer.
As an example, consider the linguistic annota-
tion for the following German sentence from one
of the soccer game reports:
Guido Buchwald wurde 1990 in Italien Welt-
meister (Guido Buchwald became world cham-
pion in 1990 in Italy)
<FS type="player_action">
<F name="GAME_EVENT">
<FS type="world champion"/>
<F name="ACTION_TIME">
<FS type="1990"/>
<F name="ACTION_LOCATION">
<FS type="Italy"/>
<F name="AGENT">
<FS type="player">
<F name="SURNAME">
<FS type="Buchwald"/>
<F name="GIVEN_NAME">
<FS type="Guido"/>
2.3 Knowledge Base Generation
The SmartWeb SportEventOntology (a subset of
SWIntO) contains about 400 direct classes onto
which named-entities and other, more complex
structures are mapped. The mapping is repre-
sented in a declarative fashion specifying how
the feature-based structures produced by SProUT
are mapped into structures which are compatible
with the underlying ontology. Further, the newly
extracted information is also interpreted in the
context of additional information about the
match in question.
This additional information is obtained by
wrapping the semi-structured data on relevant
soccer matches, which is also mapped to the on-
tology. The information obtained in this way
about the match in question can then be used as
contextual background with respect to which the
newly extracted information is interpreted.
The feature structure for player as displayed
above will be translated into the following F-
Logic (Kifer et al 1995) statements, which are
then automatically translated to RDF and fed to
the visualization component:
soba#player124:sportevent#FootballPlayer
[sportevent#impersonatedBy ->
soba#Guido_BUCHWALD].
soba#Guido_BUCHWALD:dolce#"natural-person"
[dolce#"HAS-DENOMINATION" ->
soba#Guido_BUCHWALD_Denomination].
soba#Guido_BUCHWALD_Denomination":dolce#"
natural-person-denomination"
[dolce#LASTNAME -> "Buchwald";
dolce#FIRSTNAME -> "Guido"].
124
2.4 Knowledge Base Visualization
The generated knowledge base is visualized by
way of automatically inserted hyperlink menus
for soccer-related named-entities such as players
and teams. The visualization component is based
on the VIeWs4 system. VIeWs allows the user to
simply browse a web site as usual, but is addi-
tionally supported by the automatic hyperlinking
system that adds additional information from a
(generated) knowledge base.
For some examples of this see the included
figures below, which show extracted information
for the Panama team (i.e. all of the football play-
ers in this team in Figure 1) and for the player
Roberto Brown (i.e. his team and events in which
he participated in Figure 2).
3 Implementation
All components are implemented in Java 1.5 and
are installed as web applications on a Tomcat
web server. SOAP web services are used for
communication between components so that the
system can be installed in a centralized as well as
decentralized manner. Data communication is
handled by XML-based exchange formats. Due
to a high degree of flexibility of components,
only a simple configuration over environment
variables is needed.
4 Conclusions and Future Work
We presented an ontology-based approach to
information extraction in the soccer domain that
aims at the automatic generation of a knowledge
base from match reports and the subsequent
visualization of the extracted information
through automatic hyperlinking. We argue that
such an approach is innovative and enhances the
user experience.
Future work includes the extraction of more
complex events, for which deep linguistic analy-
sis and/or semantic inference over the ontology
and knowledge base is required. For this purpose
we will use an HPSG-based parser that is avail-
able within the HoG architecture (Callmeier,
2000) and combine this with a semantic infer-
ence approach based on discourse analysis
(Cimiano et al, 2005).
4 http://views.dfki.de
Acknowledgements
This research has been supported by grants for
the projects SmartWeb (by the German Ministry
of Education and Research: 01 IMD01 A) and
VIeWs (by the Saarland Ministry of Economic
Affairs).
References
Paul Buitelaar, Thomas Eigner, Stefania Racioppa
Semantic Navigation with VIeWs In: Proc. of the
Workshop on User Aspects of the Semantic Web at
the European Semantic Web Conference, Herak-
lion, Greece, May 2005.
Callmeier, Ulrich (2000). PET ? A platform for ex-
perimentation with efficient HPSG processing
techniques. In: Natural Language Engineering, 6
(1) UK: Cambridge University Press pp. 99?108.
Callmeier, Ulrich, Eisele, Andreas, Sch?fer, Ulrich
and Melanie Siegel. 2004. The DeepThought Core
Architecture Framework In Proceedings of LREC
04, Lisbon, Portugal, pages 1205-1208.
Cimiano, Philipp, Saric, Jasmin and Uwe Reyle.
2005. Ontology-driven discourse analysis for in-
formation extraction, Data Knowledge Engineering
55(1).
Drozdzynski, Witold, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Sch?fer, and Feiyu Xu. 2004.
Shallow processing with unification and typed fea-
ture structures ? foundations and applications.
K?nstliche Intelligenz, 1:17-23.
Kifer, M., Lausen, G. and J.Wu. 1995. Logical Foun-
dations of Object-Oriented and Frame-Based Lan-
guages. Journal of the ACM 42, pp. 741-843.
Lopez, V. and E. Motta. 2004. Ontology-driven Ques-
tion Answering in AquaLog In Proceedings of 9th
International Conference on applications of natural
language to information systems.
Maedche, Alexander, G?nter Neumann and Steffen
Staab. 2002. Bootstrapping an Ontology-Based In-
formation Extraction System. In: Studies in Fuzzi-
ness and Soft Computing, editor J. Kacprzyk. Intel-
ligent Exploration of the Web, Springer.
M?ller HM, Kenny EE and PW Sternberg. 2004.
Textpresso: An ontology-based information re-
trieval and extraction system for biological litera-
ture. PLoS Biol 2: e309.
Nirenburg, Sergei and Viktor Raskin. 2004. Ontologi-
cal Semantics. MIT Press.
Oberle et al The SmartWeb Integrated Ontology
SWIntO, in preparation.
125
Figure 2: Generated hyperlink on ?Roberto Brown? with extracted information on his
team and events in which he participated
Figure 1: Generated hyperlink on ?Panama? with extracted information on this team
126
Integration of a Lexical Type Database with a Linguistically Interpreted
Corpus
Chikara Hashimoto,? Francis Bond,? Takaaki Tanaka,? Melanie Siegel?
? Graduate School of Informatics, Kyoto University
? Machine Translation Research Group, NTT Communication Science Laboratories
? Language Technology Lab, DFKI
? hasimoto@pine.kuee.kyoto-u.ac.jp
?{takaaki,bond}@cslab.kecl.ntt.co.jp
? siegel@dfki.de
Abstract
We have constructed a large scale
and detailed database of lexical types
in Japanese from a treebank that in-
cludes detailed linguistic information.
The database helps treebank annota-
tors and grammar developers to share
precise knowledge about the grammat-
ical status of words that constitute the
treebank, allowing for consistent large
scale treebanking and grammar devel-
opment. In this paper, we report on
the motivation and methodology of the
database construction.
1 Introduction
Treebanks constructed with detailed linguistic in-
formation play an important role in various as-
pects of natural language processing; for exam-
ple, grammatical knowledge acquisition; world
knowledge acquisition (Bond et al, 2004b);
and statistical language model induction. Such
treebanks are typically semi-automatically con-
structed by a linguistically rich computational
grammar.
A detailed grammar in turn is a fundamen-
tal component for precise natural language pro-
cessing. It provides not only detailed syntactic
and morphological information on linguistic ex-
pressions but also precise and usually language-
independent semantic structures of them.
However, such a deep linguistic treebank and
a grammar are often difficult to keep consistent
through development cycles. This is both because
multiple people, often in different locations, par-
ticipate in a development activity, and because
deep linguistic treebanks and grammars are com-
plicated by nature. Thus, it is often the case that
developers lose sight of the current state of the
treebank and grammar, resulting in inconsistency.
We have constructed a linguistically enriched
treebank named ?Hinoki? (Bond et al, 2004a),
which is based on the same framework as the
Redwoods treebank (Oepen et al, 2002) and uses
the Japanese grammar JACY (Siegel and Ben-
der, 2002) to construct the treebank.1 In the con-
struction process, we have also encountered the
problem just mentioned. We are aiming to re-
solve this problem, which we expect many other
project groups that are constructing detailed lin-
guistic treebanks have encountered. Our strategy
is to take a ?snapshot? of one important aspect of
the treebank and grammar for each development
cycle. To be more precise, we extract informa-
tion about lexical items that are being used in tree-
banking from the treebank and grammar and con-
vert it into an electronically accesible structured
database (the lexical-type database). Such a snap-
shot, the database, certainly helps treebank anno-
tators and grammar developers to share precise
and detailed knowledge of the treebank and gram-
mar and thus to make them consistent throughout
the development cycle.2
Lexical items whose information is included
1Currently, the Hinoki treebank contains about 121,000
sentences (about 10 words per sentence).
2We think we also need another snapshot, that of the
grammar rules and principles being used. In this paper, how-
ever, we do not deal with it, and hopefully we will report on
it some other time.
31
in the database are grouped together according
to their grammatical behavior, and we will refer
to each of the groups as a lexical type in the
rest of the paper. A typical lexical item con-
sists of an identifier, and then a triple consist-
ing of the orthography, lexical-type and predicate:
e.g., inu n 1 = ? ???, common-noun-lex,
dog n animal?. The grammar treats all mem-
bers of the same lexical type in the same way. the
lexical type is the locus of syntactic and structural
semantic information. Examples of lexical types
will be described in ?2.
The database could also benefit a wide range
of language researchers, not just those who are
treebanking. As the development of the treebank
and grammar proceeds, together they describe the
language (Japanese in this study) with increasing
accuracy. As a result, the database that we ob-
tain from the sophisticated treebank and grammar
can be thought of as showing us the real view of
the Japanese lexicon. Thus, though many of the
details of the treebank and grammar are frame-
work dependent, the database will provide NLP
researchers who are aiming at deep linguistic pro-
cessing of Japanese with a basic and reliable ref-
erence Japanese lexicon. The correctness can be
verified by examining the treebanked examples.
Such a resource is useful for Japanese language
teachers, lexicographers, and linguists, in addi-
tion to NLP researchers.
The next section describes the framework of
treebanking and motivates the need for the lexical
type database. The third section discusses what
information the lexical type database should con-
tain to facilitate treebanking and grammar devel-
opment; illustrates the contents of the database;
and shows how the database is created. The fourth
section discusses the usefulness of the lexical type
database for many purposes other than treebank-
ing. An overview of related works follows in the
fifth section. Finally, we conclude the paper with
a discussion of our plans for future work.
2 Background to the Database
The treebanking process is illustrated in Fig-
ure 1. As the figure shows, our treebank is
semi-automatically generated by a computational
grammar (and a parser). Each sentence is parsed
and the intended reading chosen from the possi-
Development (refinement)
GRAMMAR
Treebanking (manual annotation)
TREEBANK
automatic parsingfeedback
Figure 1: Treebanking Cycles
ble interpretations. In doing so, we find the gram-
mar?s flaws such as insufficient coverage and spu-
rious ambiguities. The feedback allows us to re-
fine the grammar so that it can have wider cov-
erage and be more appropriately restricted. Cur-
rently this process is carried out by several people,
distributed over four continents.
Although most treebanks are rarely updated,
we consider the updating an integral part of the
process. Thus our treebank is dynamic in the
sense of Oepen et al (2004).
As is often the case with detailed linguistic
treebanking, our grammar and treebank consist
of very fine-grained linguistic information. For
example, our grammar, hence our treebank, dis-
tinguishes several usages of the Japanese dative
marker ni. The Japanese sentence (1) can repre-
sent the two meanings described in (1a) and (1b).
Lexical type names for each usage of ni are writ-
ten in typewriter font.3
(1) hanasiai-wa
discussion-TOP
sinya-ni
midnight-DAT
itaru
reach
a. ?The discussion comes (to a conclusion)
at midnight.?
ni as adv-p-lex-1
b. ?The discussion continues until mid-
night.?
ni as ga-wo-ni-p-lex
The dative phrase, sinya-ni (midnight-DAT), can
act as either an adjunct (1a)4 or an object of itaru
?reach? (1b). Below is an example showing other
usages of ni.
(2) Ken-wa
-TOP
yuka-o
floor-ACC
kirei-ni
clean-DAT
migaku
polish
3These are actual names of the lexical types implemented
in our grammar and might not be understandable to people
in general.
4The object, a conclusion, is expressed by a phonolog-
ically null pronoun. This is the so-called ?pro-drop? phe-
nomenon.
32
a. ?Ken polishes a floor clean.?
(The floor is clean.)
ni as naadj2adv-end-lex
b. ?Ken cleanly polishes a floor.?
(His way of polishing the floor is clean.)
ni as adv-p-lex-6
The dative phrase, kirei-ni (clean-DAT), is used as
an adjunct in both (2a) and (2b), but their usages
and meanings are different. The usage in (2b) is
an ordinary adverb that describes the manner of
Ken?s polishing the floor as clean, while in (2a)
the dative phrase describes the resulting situation
of the floor after polishing as clean. In addition,
the nis in (1) and (2) are different in that the for-
mer takes nouns as its complement while the lat-
ter takes adjectives. Thus, the four usages in (1a),
(1b), (2a) and (2b) must be distinguished so that
we can obtain correct syntactic structures and se-
mantic representations. In our terms, these nis are
said to belong to different lexical types.5 Simi-
larly, our grammar distinguishes usages of other
words, notably functional ones.
However, as we augment the grammar with
finer distinctions, the grammar becomes more and
more opaque and difficult to maintain, and so is
the treebank. This is problematic in three ways.
Firstly, when we annotate parser outputs of one
sentence, we have to see which parse is correct
for the sentence. Consequently, we have to distin-
guish which word usage is correct for each word
in the sentence. However, this task is not always
trivial, since our grammar?s word usage distinc-
tion is very fine grained as shown above. Sec-
ondly, when we add a word to the grammar to
get wider coverage, we have to see which lexical
type the word belongs to. That is, we are required
to be familiar with lexical types of the grammar.
Thirdly, in collaborative grammar development, it
sometimes happens that a developer accidentally
introduces a new lexical type that represents over-
lapping functionality with an existing type. This
causes spurious ambiguity. As a result, the gram-
mar will be unnecessarily bloated, and the tree-
bank will also be easily inconsistent. Again, we
see that comprehensive knowledge of the gram-
mar?s lexical types is indispensable.
5Usages of the Japanese dative marker, ni, are extensively
discussed in, for example, Sadakane and Koizumi (1995).
In summary, it is important to make clear (i)
what lexical types are assumed in a grammar and
a treebank and (ii) how differently they are used
from each other, so that we can make the treebank
annotation and grammar development consistent.
Our solution to the problem is to construct a
lexical type database of a treebank and a gram-
mar. The database is expected to give us explicit
information on (i) what lexical types are imple-
mented in the grammar and are used in the tree-
bank and (ii) how a word is used in Japanese and
is distinguished from other words.
3 Architecture of the Database
This section details the content of the
database and the method of its construc-
tion. The database itself is on-line at
http://pc1.ku-ntt-unet.ocn.ne.
jp/tomcat/lextypeDB/.
3.1 Content of the Database
First of all, what information should be included
in such a database to help treebank annotators and
grammar developers to work consistently? Ob-
viously, once we construct an electronic lexicon,
whatever information it includes, we can easily
see what lexical types are assumed in the gram-
mar and treebank. But we have to carefully con-
sider what to include in the database to make it
clear how each of the lexical types are used and
distinguished.
We include five kinds of information:
(3) Contents of the Database
a. Linguistic discussion
i Name
ii Definition
iii Criteria to judge a word as belong-
ing to a given lexical type
iv Reference to relevant literature
b. Exemplification
i Words that appear in a treebank
ii Sentences in a treebank that contain
the words
c. Implementation
i The portion of grammar source file
that corresponds to the usage
33
ii Comments related to the portion
iii TODOs
d. Links to ?confusing? lexical types
e. Links to other dictionaries
That is, we describe each lexical type in
depth (3a?3c) and present users (treebank an-
notators and grammar developers) explicit links
to other lexical types that share homony-
mous words (3d) (e.g. adv-p-lex-1 vs
ga-wo-ni-case-p-lex in (1)) to make it
clear what distinguishes between them. Further,
we present correspondences to other computa-
tional dictionaries (3e).
Linguistic discussion To understand lexical
types precisely, linguistic observations and anal-
yses are a basic source of information.
Firstly, the requirements for naming lexical-
types in a computational system (3ai) are that
they be short (so that they can be displayed in
large trees) and easily distinguishable. Type
names are not necessarily understandable for any-
one but the developers, so it is useful to link
them to more conventional names. For example
ga-wo-ni-p-lex is a Case Particle (???).
Next, the definition field (3aii) contains a
widely accepted definition statement of the lexi-
cal type. For example, ga-wo-ni-p-lex (1b)
can be defined as ?a particle that indicates that a
noun it attaches to functions as an argument of a
predicate.? Users can grasp the main characteris-
tics from this.
Thirdly, the criteria field (3aiii) provides users
with means of investigating whether a given word
belongs to the class. That is, it provides posi-
tive and negative usage examples. By such us-
age examples, developers can easily find dif-
ferences among lexical types. For example,
adv-p-lex-1 (1a) subcategorizes for nouns,
while adv-p-lex-6 (2b) subcategorizes for
adjectives. Sentences like (1a) and (2b) that fit
such criteria should also be treebanked so that
they can be used to test that the grammar covers
what it claims. This is especially important for
regression testing after new development.
Finally, the reference field (3aiv) points to rep-
resentative papers or books dealing with the lex-
ical type. This allows the grammar developers to
quickly check against existing analyses, and al-
lows users as well to find more information.
Exemplification Examples help users under-
stand lexical types concretely. As we have con-
structed a treebank that is annotated with linguis-
tic information, we can automatically extract rele-
vant examples exhaustively. We give the database
two kinds of examples: words, that are instances
of the lexical types (3bi), and sentences, tree-
banked examples that contain the words (3bii).
This link to the linguistically annotated corpus
examples helps treebankers to check for consis-
tency, and grammar developers to check that the
lexical types are grounded in the corpus data.
Implementation Grammar developers need to
know the actual implementation of lexical types
(3ci). Comments about the implementation (3cii)
are also helpful to ascertain the current status.
Although this section is necessarily framework-
dependent information, all project groups that are
constructing detailed linguistic treebanks need to
document this kind of information. We take our
examples from JACY (Siegel and Bender, 2002),
a large grammar of Japanese built in the HPSG
framework. As actual implementations are gen-
erally incomplete, we use this resource to store
notes about what remains to be done. TODOs
(3ciii) should be explicitly stated to inform gram-
mar developers of what they have to do next.
We currently show the actual TDL defini-
tion, its parent type or types, category of
the head (SYNSEM.LOCAL.CAT.HEAD), valency
(SYNSEM.LOCAL.CAT.VAL), and the semantic
type (SYNSEM.LOCAL.CONT).
Links to ?confusing? lexical types For users to
distinguish phonologically identical but syntacti-
cally or semantically distinct words, it is impor-
tant to link confusing lexical types to one another
within the database. For example, the four lexical
types in (1) and (2) are connected with each other
in terms of ni. That way, users can compare those
words in detail and make a reliable decision when
trying to disambiguate usage examples.6
6Note that this information is not explicitly stored in
the database. Rather, it is dynamically compiled from the
database together with a lexicon database, one of the com-
ponent databases explained below, when triggered by a user
query. User queries are words like ni.
34
Links to other dictionaries This information
helps us to compare our grammar?s treatment
with that of other dictionaries. This compar-
ison would then facilitate understanding of
lexical types and extension of the lexicon. We
currently link lexical types of our grammar
to those of ChaSen (Matsumoto et al, 2000),
Juman (Kurohashi and Nagao, 2003), ALT-
J/E (Ikehara et al, 1991) and EDICT (Breen,
2004). For example, ga-wo-ni-case-p-lex
is linked to ChaSen?s ??-???-??
(particle-case particle-general),
Juman?s ??? (case particle), and
ALT-J/E?s ???-???-???????
(adjunct-case particle-noun/par-
ticle suffix).
Figure 2 shows the document generated from
the lexical type database that describes the lexical
type, ga-wo-ni-p-lex.
3.2 Method of Database Construction
The next question is how to construct such a
database. Needless to say, fully manual construc-
tion of the database is not realistic, since there
are about 300 lexical types and more than 30,000
words in our grammar. In addition, we assume
that we will refer to the database each time we
annotate parser outputs to build the treebank and
that we develop the grammar based on the tree-
banking result. Thus the database construction
process must be quick enough not to delay the
treebanking and grammar development cycles.
To meet the requirement, our method of con-
struction for the lexical type database is semi-
automatic; most of the database content is con-
structed automatically, while the rest must be en-
tered manually. This is depicted in Figure 3.
? Content that is constructed automatically
? Lexical Type ID (Grammar DB)
? Exemplification (3b) (Treebank DB)
? Implementation (3ci,ii) (Grammar DB)
? Link to ?confusing? lexical types (3d)
(Lexicon DB)
? Link to Other Lexicons (3e) (OtherLex
DB)
? Content that is constructed manually
? Linguistic discussion (3a)
? TODOs (3ciii)
3.2.1 Component Databases
To understand the construction process, de-
scription of the four databases that feed the lex-
ical type database is in order. These are the gram-
mar database, the treebank database, the lexicon
database, and the OtherLex database.
? The grammar database contains the actual
implementation of the grammar, written as
typed feature structures using TDL (Krieger
and Schafer, 1994). Although it contains the
whole implementation (lexical types, phrasal
types, types for principles and so on), only
lexical types are relevant to our task.
? The lexicon database gives us mappings be-
tween words in the grammar, their orthogra-
phy, and their lexical types. Thus we can see
what words belong to a given lexical type.
The data could be stored as TDL, but we
use the Postgresql lexdb (Copestake et al,
2004), which simplifies access.
? The treebank database stores all treebank in-
formation, including syntactic derivations,
words, and the lexical type for each word.
The main treebank is stored as structured
text using the [incr tsdb()] (Oepen et al,
2002). We have also exported the deriva-
tion trees for the treebanked sentences into
an SQL database for easy access. The leaves
of the parse data consist of words, and their
lexicon IDs, stored with the ID of the sen-
tence in which the word appears.
? We also use databases from other sources,
such as ChaSen, Juman and Edict.
3.2.2 Automatic Construction
Next we move on to describe the automatic
construction. Firstly, we collect all lexical types
assumed in the grammar and treebank from the
grammar database. Each type constitutes the ID
of a record of the lexical type database.
Secondly, we extract words that are judged to
belong to a given lexical type and sentences that
contains the words (Example (3b)) from the tree-
bank database compiled from the Hinoki tree-
bank (Bond et al, 2004a). The parsed sentences
35
???, ga-wo-ni-p-lex (?,?,?)
Linguistic Discussion
ga-wo-ni-p-lex particles attach to a noun and indicate what grammatical relation (e.g., subject or object)
the noun takes on in relation to a predicate. It does not mean anything by itself.
Right Wrong
????????? ???????????
?????????? 10???????
Literature
[1] Koichi Takezawa. A Configurational Approach to Case Marking in Japanese. Ph.D. dissertation,
University of Washington, 1987.
[ bib ]
[2] Shigeru Miyagawa. Structure and Case Marking in Japanese (Syntax and Semantics 22). Academic
Press, 1989.
[ bib ]
Examples
Lexical Entries (6)
? (ga),? (ni-case),? (o)
Example Sentences (54280)
Examples for? (ga)
?????????? ??????????????????????????????
?????????
??????????????????????????? ?????
???????????????
Examples for? (ni-case)
???????? ?????
????????
???????????
Examples for? (o)
?????
?????????
?????????
More Examples
TDL Summary
TDL Definition
ga-wo-ni-p-lex := case-p-lex & 
                [SYNSEM.LOCAL.CAT.VAL.COMPS.FIRST.LOCAL.CAT.HEAD noun_head].
Supertype Head Category Valency Content
case-p-lex overt-case-p_head p_sat mrs
TODO
Dative subjects of stative predicates are not recognized.
"????????????"
See also mental-stem-lex.
Links
CHASEN?s Lexical type JUMAN?s Lexical type ALT-J/E?s Lexical type
??-???-?? ??? ???-???-???????
Lexical Type List
Figure 2: Screenshot of the lexical type ga-wo-ni-p-lex
36
Manual
Input
Grammar DB
- Lexical Type ID
- Source
OtherLex DB
- Other Lex ID
- Other Lex Type
- Orthography
Lexical Type DB
- Lexical Type ID
- Linguistic Discussion
- Exemplification
- Implementation
- TODOs
- Other Lexicons
Lexicon DB
- Lexicon ID
- Orthography
- Lexical Type ID
Treebank DB
- Lexicon ID
- Orthography
- Sentence ID
User
:
-)
OtherLex
Interface
Query
?Confusing?
Links
Figure 3: The Lexical Type Database Construction
can be seen in various forms: plain text, phrase
structure trees, derivation trees, and minimal re-
cursion semantics representations. We use com-
ponents from the Heart-of-Gold middleware to
present these as HTML (Callmeier et al, 2004).
Thirdly, implementation information except for
TODOs is extracted from the grammar database
(3ci,ii).
Fourthly, in order to establish ?confusing? lex-
ical type links (3d), we collect from the lexicon
database homonyms of a word that users enter as
a query. To be more precise, the lexicon database
presents all the words with the same orthogra-
phy as the query but belonging to different lexical
types. These lexical types are then linked to each
other as ?confusing? in terms of the query word.
Fifthly, we construct links between our lexical
types and POS?s of other lexicons such as ChaSen
from OtherLex DB (3e). To do this, we prepare
an interface (a mapping table) between our lexi-
cal type system and the other lexicon?s POS sys-
tem. As this is a finite mapping it could be made
manually, but we semi-automate its construction.
The similarity between types in the two databases
(JACY and some other lexicon ) is calculated as
the Dice coefficient, where W (LA) is the number
of words W in lexical type L:
sim(LA, LB) =
2? |(W (LA ? LB)|
|W (LA)|+ |W (LB)|
(1)
The Dice coefficient was chosen because of its
generality and ease of calculation. Any pair
where sim(LA, LB) is above a threshold should
potentially be mapped. The threshold must be set
low, as the granularity of different systems can
vary widely.
3.2.3 Manual Construction
Linguistic discussion (3a) and implementation
TODOs (3ciii) have to be entered manually. Lin-
guistic discussion is especially difficult to collect
exhaustively since the task requires an extensive
background in linguistics. We have several lin-
guists in our group, and our achievements in this
task owe much to them. We plan to make the in-
terface open, and encourage the participation of
anyone interested in the task.
The on-line documentation is designed to com-
plement the full grammar documentation (Siegel,
2004). The grammar documentation gives a top
down view of the grammar, giving the overall mo-
tivation for the analyses. The lexical-type docu-
mentation gives bottom up documentation. It can
easily be updated along with the grammar.
Writing implementation TODOs also requires
expertise in grammar development and linguis-
tic background. But grammar developers usually
take notes on what remains to be done for each
lexical type anyway, so this is a relatively simple
task.
After the database is first constructed, how is
it put to use and updated in the treebanking cy-
cles described in Figure 1? Figure 4 illustrates
this. Each time the grammar is revised based on
treebank annotation feedback, grammar develop-
ers consult the database to see the current status
of the grammar. After finishing the revision, the
grammar and lexicon DBs are updated, as are the
corresponding fields of the lexical type database.
Each time the treebank is annotated, annotators
can consult the database to make sure the chosen
parse is correct. Following annotation, the tree-
bank DB is updated, and so is the lexical type
database. In parallel to this, collaborators who are
37
Development (refinement)
GRAMMAR
Treebanking (manual annotation)
TREEBANK
automatic
parsingfeedback
LEXICAL TYPE
DATABASE
WWW
Reference
Updating Grammar and Lexicon DBs
Reference
Updating Treebank DB
Linguistic Discussion
Figure 4: Database Construction Intergrated with Treebanking Cycles
ChaSenJuman ALT-J/E
The Lexical Type
Database
EDICT LexicalResource2Lexical
Resource1
ChaSen
InterfaceJumanInterface
ALT-J/E
Interface
EDICT
Interface Interface2Interface1
Figure 5: Synthesis of Lexical Resources
familiar with linguistics continue to enter relevant
linguistic discussions via the WWW.
4 Lexical Type Database as a General
Linguistic Resource
In this section, we discuss some of the ways the
database can benefit people other than treebank
annotators and grammar developers.
One way is by serving as a link to other lexi-
cal resources. As mentioned in the previous sec-
tion, our database includes links to ChaSen, Ju-
man, ALT-J/E, and EDICT. Currently, in Japanese
NLP (and more generally), various lexical re-
sources have been developed, but their intercor-
respondences are not always clear. These lexical
resources often play complementary roles, so syn-
thesizing them seamlessly will make a Japanese
lexicon with the widest and deepest knowledge
ever. Among our plans is to realize this by
means of the lexical type database. Consider Fig-
ure 5. Assuming that most lexical resources con-
tain lexical type information, no matter how fine
or coarse grained it is, it is natural to think that
the lexical type database can act as a ?hub? that
links those lexical resources together. This will
be achieved by preparing interfaces between the
lexical type database and each of the lexical re-
sources. Clearly, this is an intelligent way to syn-
thesize lexical resources. Otherwise, we have to
prepare nC2 interfaces to synthesize n resources.
The problem is that construction of such an inter-
face is time consuming. We need to further test
generic ways to do this, such as with similarity
scores, though we will not go on further with this
issue in this paper.
Apart from NLP, how can the database be used?
In the short term our database is intended to pro-
vide annotators and grammar developers with a
clear picture of the current status of the treebank
and the grammar. In the long term, we expect to
create successively better approximations of the
Japanese language, as long as our deep linguistic
broad coverage grammar describes Japanese syn-
tax and semantics precisely. Consequently, the
database would be of use to anyone who needs an
accurate description of Japanese. Japanese lan-
guage teachers can use its detailed descriptions
of word usages, the links to other words, and the
real examples from the treebank to show for stu-
dents subtle differences among words that look
the same but are grammatically different. Lexi-
cographers can take advantage of its comprehen-
siveness and the real examples to compile a dic-
tionary that contains full linguistic explanations.
The confidence in the linguistic descriptions is
based on the combination of the precise grammar
linked to the detailed treebank. Each improves the
other through the treebank annotation and gram-
mar development cycle as depicted in Figure 1.
5 Related Work
Tsuchiya et al (2005) have been constructing a
database that summarizes multiword functional
38
expressions in Japanese. That describes each
expression?s linguistic behavior, usage and ex-
amples in depth. Notable differences between
their database and ours are that their database is
mostly constructed manually while ours is con-
structed semi-automatically, and that they target
only functional expressions while we deal with all
kinds of lexical types.
Hypertextual Grammar development (Dini and
Mazzini, 1997) attempted a similar task, but fo-
cused on documenting the grammar, not on link-
ing it to a dynamic treebank. They suggested cre-
ating the documentation in the same file along
with the grammar, in the style of literate program-
ming. This is an attractive approach, especially
for grammars that change constantly. However,
we prefer the flexibility of combining different
knowledge sources (the grammar, treebank and
linguistic description, in addition to external re-
sources).
The Montage project (Bender et al, 2004) aims
to develop a suite of software whose primary au-
dience is field linguists working on underdocu-
mented languages. Among their tasks is to fa-
cilitate traditional grammatical description from
annotated texts by means of one of their products,
the Grammar export tool. Although in the paper
there is little explicit detail about what the ?tradi-
tional grammatical description? is, they seem to
share a similar goal with us: in the case of Mon-
tage, making grammatical knowledge assumed in
underdocumented languages explicit, while in our
case making lexical types assumed in the treebank
and the computational grammar understandable
to humans. Also, some tools they use are used
in our project as well. Consequently, their pro-
cess of grammatical description and documenta-
tion looks quite similar to ours. The difference
is that their target is underdocumented languages
whose grammatical knowledge has so far not been
made clear enough, while we target a familiar
language, Japanese, that is well understood but
whose computational implementation is so large
and complex as to be difficult to fully compre-
hend.
Another notable related work is the COMLEX
syntax project (Macleod et al, 1994). Their goal
is to create a moderately-broad-coverage lexicon
recording the syntactic features of English words
for purposes of computational language analysis.
They employed elves (?elf? = enterer of lexical
features) to create such a lexicon by hand. Natu-
rally, the manual input task is error-prone. Thus
they needed to prepare a document that describes
word usages by which they intended to reduce
elves? errors. It is evident that the document
plays a role similar to our lexical type database,
but there are important divergences between the
two. First, while their document seems to be con-
structed manually (words chosen as examples of
lexical types in the documentation are not always
in the lexicon!), the construction process of our
database is semi-automated. Second, somewhat
relatedly, our database is electronically accessible
and well-structured. Thus it allows more flexi-
ble queries than a simple document. Third, unlike
COMLEX, all the lexical types in the database
are actually derived from the working Japanese
grammar with which we are building the tree-
bank. That is, all the lexical types are defined
formally. Fourth, examples in our database are all
real ones in that they actually appear in the tree-
bank, while most of the COMLEX examples were
created specifically for the project. Finally, we are
dealing with all kinds of lexical types that appear
in the treebank, but the COMLEX project targets
only nouns, adjectives, and verbs.
6 Future Work
We are currently experimenting with moving
some of the information (in particular the type
name and criteria) into the actual grammar files,
in the same way as Dini and Mazzini (1997). This
would make it easier to keep the information in
sync with the actual grammar.
We have discussed the motivation, contents and
construction of the lexical type database. We plan
to evaluate the database (i) by measuring tree-
bank inter-annotator agreement and (ii) by evalu-
ating the coverage, the amount of spurious ambi-
guity, and efficiency of the grammar before and
after introducing the database in the treebank-
ing and grammar development cycles. We ex-
pect that treebank annotators will be more con-
sistent when they can refer to the database and
that grammar developers can more easily find the
grammar?s flaws (like lack of lexical items and
overlapping implementations of the same lexical
39
type) by looking into the database.
Although this paper deals with a lexical type
database of Japanese, the importance of such a
database certainly holds for any large scale deep
grammar. We use the tools from the DELPH-
IN collaboration7 and plan to make our addi-
tions available for groups working with other lan-
guages. In particular, we plan to construct a lex-
ical type database for the Redwoods treebank,
which is semi-automatically constructed from the
English Resource Grammar (ERG) (Flickinger,
2000).
Acknowledgements
We would like to thank the other members
of Machine Translation Research Group, Dan
Flickinger, Stephen Oepen, and Jason Katz-
Brown for their stimulating discussion.
References
Emily M. Bender, Dan Flickinger, Jeff Good, and Ivan A.
Sag. 2004. Montage: Leveraging Advances in Grammar
Engineering, Linguistic Ontologies, and Mark-up for the
Documentation of Underdescribed Languages. In Pro-
ceedings of the Workshop on First Steps for the Documen-
tation of Minority Languages: Computational Linguistic
Tools for Morphology, Lexicon and Corpus Compilation,
LREC2004, Lisbon, Portugal.
Francis Bond, Sanae Fujita, Chikara Hashimoto, Shigeko
Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka,
and Shigeaki Amano. 2004a. The Hinoki Treebank
? Toward Text Understanding. In Proceedings of the
5th International Workshop on Linguistically Interpreted
Corpora (LINC-04), pages 7?10, Geneva.
Francis Bond, Eric Nichols, and Sanae Fujita Takaaki
Tanaka. 2004b. Acquiring an Ontology for a Funda-
mental Vocabulary. In 20th International Conference
on Computational Linguistics (COLING-2004), pages
1319?1325, Geneva.
J. W. Breen. 2004. JMDict: a Japanese-mulitlingual dictio-
nary. In Coling 2004 Workshop on Multilingual Linguis-
tic Resources, pages 71?78, Geneva.
Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, and
Melanie Siegel. 2004. The DeepThought core archi-
tecture framework. In Proceedings of LREC-2004, vol-
ume IV, Lisbon.
Ann Copestake, Fabre Lambeau, Benjamin Waldron, Fran-
cis Bond, Dan Flickinger, and Stephan Oepen. 2004. A
lexicon module for a grammar development environment.
In 4th International Conference on Language Resources
and Evaluation (LREC 2004), volume IV, pages 1111?
1114, Lisbon.
7http://www.delph-in.net/
Luca Dini and Giampolo Mazzini. 1997. Hypertextual
Grammar Development. In Computational Environments
for Grammar Development and Linguistic Engineering,
pages 24?29, Madrid. ACL.
Dan Flickinger. 2000. On building a more effi cient gram-
mar by exploiting types. Natural Language Engineering,
6 (1) (Special Issue on Efficient Proceeding with HPSG,
pages 15?28.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing ? effects of new methods in ALT-J/E?. In Third
Machine Translation Summit: MT Summit III, pages 101?
106, Washington DC. (http://xxx.lanl.gov/
abs/cmp-lg/9510008).
Hans-Ulrich Krieger and Ulrich Schafer. 1994. T DL ? a
type description language for constraint-based grammars.
In Proceedings of the 15th International Conference on
Computational Linguistics.
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. chapter 14, pages 249?260.
Catherine Macleod, Ralph Grishman, and Adam Meyers.
1994. The Comlex Syntax Project: The First Year. In
Proceedings of the 1994 ARPA Human Language Tech-
nology Workshop.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, Kazuma Takaoka, and
Masayuki Asahara, 2000. Morphological Analysis Sys-
tem ChaSen version 2.2.1 Manual. Nara Institute of Sci-
ence and Technology, Dec.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2002. LinGO Redwoods: A
Rich and Dynamic Treebank for HPSG. In Proceedings
of The First Workshop on Treebanks and Linguistic The-
ories, pages 139?149, Sozopol, Bulgaria.
Stephan Oepen, Dan Flickinger, and Francis Bond. 2004.
Towards Holistic Grammar Engineering and Testing.
Grafting Treebank Maintenance into the Grammar Re-
vision Cycle. In Proceedings of the IJCNLP Workshop
Beyond Shallow Analysis, Hainan,China.
Kumi Sadakane and Masatoshi Koizumi. 1995. On the na-
ture of the ?dative? particle ni in Japanese. Linguistics,
33:5?33.
Melanie Siegel and Emily M. Bender. 2002. Effi cient Deep
Processing of Japanese. In Proceedings of the 3rd Work-
shop on Asian Language Resources and International
Standardization, Taipei, Taiwan.
Melanie Siegel. 2004. JACY a practical Japanese HPSG.
ms.
Masatoshi Tsuchiya, Takehito Utsuro, Suguru Matsuyoshi,
Satoshi Sato, and Seiichi Nakagawa. 2005. A corpus
for classifying usages of japanese compound functional
expressions. In Proceedings of Pacific Association for
Computational Linguistics 2005, Tokyo, Japan.
40
Annotating Honorifics Denoting Social Ranking of Referents
Shigeko Nariyama*!, Hiromi Nakaiwa*, Melanie Siegel"
Communication Science Lab, NTT Kyoto, Japan
*{shigekon, hiromi}@cslab.kecl.ntt.co.jp
! The University of Melbourne, Australia shigeko@unimelb.edu.au
"DFKI GmbH Saarbr?cken, Germany siegel@dfki.de
Abstract
This paper proposes an annotating
scheme that encodes honorifics
(respectful words). Honorifics are used
extensively in Japanese, reflecting the
social relationship (e.g. social ranks and
age) of the referents. This referential
information is vital for resolving zero
pronouns and improving machine
translation outputs. Annotating honorifics
is a complex task that involves
identifying a predicate with honorifics,
assigning ranks to referents of the
predicate, calibrating the ranks, and
connecting referents with their predicates.
1 Introduction
To varying extents, languages have ways to
reflect the speaker?s deference towards the
addressee and people being referred to in
utterances (c.f. Brown and Levinson 1987): by
adopting a more polite air or tone of voice,
avoiding coarse language, and modifying one?s
choice of specific vocabulary. This is prominent
in Asian languages, Japanese and Korean in
particular, which exhibit an extensive use of
honorifics (respectful words).
Morphologically for example, French has a
choice of the familiar tu and the formal vous (a
third person plural) for the second person
referent. Similarly Greek has the same choice:
esei and eseis respectively. European languages
commonly project one?s deference by the use of
different personal pronouns and titles (e.g. Mr.,
Dr., and Hon.).
Japanese and Korean, on the other hand, have
numerous ways to say ?I? or ?you? calibrated by
social position, age, gender and other factors.
The projection of honorifics extends over the
vocabulary of verbs, adjectives, and nouns as
well as sentence structures, to elevate a person
or humble oneself. (1) and (2) below from
Japanese are such examples, which use honorific
verbs instead of neutral forms kuru ?come?, iku
?go/accompany?, and motomeru ?seek?:
(1)??????????????
Irasshat-tara, otomosuru.
come-when accompany
?When (an honouree) comes, (an honourer)
accompanies (the honoree).?
(2)???????
Enjo-o aoida.
help-OB sought
?(A lower ranked person) turns to (a higher
ranked person) for help.?
Examples (1) and (2) also reveal the
notorious problem of zero pronoun resolution in
Japanese, where the subject and the object of a
sentence are frequently left unexpressed
(Nakaiwa 2002, Nariyama 2003, inter alia). It
is clear from the examples that coding the
honorific relations of referents provides vital
information for identifying what zero pronouns
refer to; namely, to know whether or not a
predicate denotes disparity of social rank
between referents and to identify the rank of the
referents. This is what this paper proposes to
do. Siegel (2000) reported that 23.9% of
Japanese zero pronouns in task-oriented
dialogues can be resolved using information
gleaned from honorification.
Coding of honorifics also improves machine
translation outputs into Japanese in choosing
the correct predicate depending on the
relationship of the referents. Inappropriate use
of honorifics, in particular the use of the plain
form where an honorific form should be used, is
rude and can be offensive.
91
Section 2 reviews some earlier work on this
topic in NLP; Section 3 elaborates on honorifics;
Section 4 formulates the ranking factors; Section
5 proposes a way to assign ranks to referents;
Section 6 discusses a way to calibrate rankings
of referents, as ranks are relative to the ranks of
other referents in the sentence; Section 7
describes our annotation scheme, and finally
conclusion in Section 8.
2 Earlier studies
The Japanese honorification system has been
studied extensively in linguistics, particularly in
sociolinguistics. Because of its importance and
frequent use in the Japanese language, there has
been some related work in NLP; within the
framework of grammar formalism, GPSG by
Ikeya (1983), JPSG by Gunji (1987), and more
recently HPSG by Siegel (2000); work from a
view point of resolving zero pronouns in
dialogues by Dohsaka (1990).
Of these, the most thorough work on
Japanese honorification is seen in JACY, a
Japanese HPSG grammar (Siegel 2000, Siegel
and Bender 2002). It extends the BACKGR
(owe ? honour) relation (Pollard and Sag 1994),
which accounts only for subject honorifics, to
accommodate the other types of honorification
used in Japanese (see Section 3.1 for the types).
The full account of the Japanese
honorification system requires syntactic and
pragmatic information in many dimensions, with
more input from the latter, the gathering of
which is an extremely convoluted task. This
paper builds on the basics from JACY and
complements it in two ways to extend the JACY
annotation presented in Section 7.
1. Ranking referents in social hierarchy
2. Calibrating the ranks
Regarding 1, honorifics tell which referent is
higher in rank, so each referent must be assigned
a rank to make use of honorific information.
This is crucial when generating sentences to
assign appropriate forms of honorific nouns and
predicates in machine translation output into
Japanese. In processing, ranking referents is not
usually of importance when referents are overt,
but it is when referents are zero pronouns. The
identification of zero pronouns relies heavily on
the honorific information conveyed in the
predicates.
Regarding 2, social rank is not absolute, but
relative, so that the same referent may be higher
or lower depending on which referent it appears
with in a sentence. For example, the president of
a company is socially regarded as ranked higher
than the managers, who are in turn higher than
clerks, but this rank is outweighed when their
clients come in the sentence, in which case the
president is ranked lower than their clients.
3 Honorifics
Honorifics is a term used to represent words that
convey esteem or respect. Extensive studies on
Japanese honorification revealed many forms of
honorifics in use. The use of honorification is
mandatory in many social situation. 1 Hence,
every sentence can be viewed as coded for
honorification if we consider the lack of an
honorific marking as a sign that there is no
hierarchical difference between referents.
Types of honorifics that indicate who is
shown respect are described in Subsection 3.1,
and forms of honorifics in Subsection 3.2.
3.1 Types of honorifics
Honorifics in modern (post-war) Japanese are
generally classified into the following three
categories, depending on who is shown respect
(Martin 1964, Matsumoto 1997, Nariyama 2003,
inter alia). The first two types are often referred
to as ?propositional (referential) honorifics?.
i. Subject honorifics (called Sonkeigo in
Japanese): to elevate or show respect
towards the subject of a sentence
ii. Non-subject honorifics (called Humility,
or Kenjogo): to humble oneself by showing
respect to the non-subject referent,
generally the object
iii. Addressee honorifics (?polite?,
Teineigo): to show respect towards the
listener
Note that the expressions of deference are by
nature made essentially with human referents,
i.e. between an honouree and an honourer.
1 However, sometimes honorification is uncoded even for
respected referents, especially when the respected person is
not present at the site of an utterance.
92
However, paying respect often extends to things
and events related to the honouree in Japanese.
This is often expressed with an honorific prefix
o- or go- to the nouns. For example, the passage
(3) is used by train conductors for ticket
inspection. The use of non-subject honorific
form means that the unexpressed subject (i.e. the
train conductor) is showing respect towards the
tickets, which belong to or have some relation to
his honourees (i.e. the passengers).
(3)?????????
Kippu-o haikenshi-masu.
ticket-OB look[NsubH]-Polite
'(An honourer) is going to inspect (his honourees?)
tickets.' ! 'Let me inspect your tickets, please.'
3.2 Forms of honorifics
Honorifics in Japanese take various forms that
are reflected in the word forms, either in lexical
choice or in inflections ? verbs in Subsection
3.2.1, adjectives and nouns in Subsection 3.2.2,
and also sentence structures in Subsection 3.2.3.
3.2.1 Verbs
There are five ways of expressing referent
honorification in verbs, depending on the type of
verb and the level of respect that is intended.2
Types 3 displays the highest deference, and
3>2>1>4 in descending order. Type 5 displays a
formality rather than deference towards the
referent. The larger the gap in the hierarchy, the
more disparity of referents in rank we expect.
3.2.1.1 Type 1: Alternation of verb forms
Verbs can be transformed into subject honorific
(SubH) and non-subject honorific (NsubH)
structures as follows:
SubH: o + verb stem + ni naru (?become?).
NsubH: o + verb stem + suru (?do?).
2 According to Wenger (1983:283-292), 70% of verbs have
Subject honorific forms, while only 36% of verbs have
non-subject honorific forms. He explains why not all verbs
have forms of honorification, although he does not explain
why there are fewer non-subject honorific forms.
Honorification cannot occur, 1) unless the subject is
human; this explains why there are no honorific forms for
verbs such as kooru ?freeze? and hoeru ?bark?; and 2) on
verbs that have negative connotations, such as kuiarasu ?eat
greedily?.
Accordingly, machi ?wait?, for example, can be
turned into two different forms of honorifics:
(4a) O-machi-ni naru. [SubH]
'(An honouree) waits (for someone/something).'
(4b) O-machi-suru. [NsubH]
'(An honourer) waits (for an honouree).?
The honorific prefix o- can be go-, as shown
below. Basically, o- is used for Japanese native
verbs and nouns and go- for Sino-Japanese
(Chinese originated) words.
(5a) Go-shichaku-ni naru. [SubH]
'(An honouree) tries on (clothes).'
(5b) Go-hookoku-suru [NsubH]
'(An honourer) reports (to an honouree).'
3.2.1.2 Type 2: Suppletive forms
Different lexical items are used for some (more
frequently used) verbs. For example, the
following examples all mean '? eat':
(6a) Taberu. [non-honorific: neutral]
'(Someone) eats.'
(6b) Meshiagaru. [SubH]
'(An honouree) eats.'
(6c) Itadaku. [NsubH]
'(An honourer) eats.'
Table 1 shows some examples of other
suppletive forms of honorification.
Neutral SubH NsubH
do suru nasaru itasu
exist/stay iru irassharu/
o-ide-ni-naru oru
go iku irassharu/ mairu/
o-ide-ni-naru ukagau
come kuru irassharu/
o-ide-ni-naru mairu
say iu ossharu moosu
eat/drink taberu/nomu meshiagaru itadaku
Table 1: Suppletive forms of honorification
Notice that some honorific forms are shared
by very different meanings of verbs. For
instance, irassharu can mean either ?come?,
?go?, or ?stay?. The nature of honorification is
said to be indirect in expression. This semantic
neutralization poses problems in machine
translation outputs from Japanese.
93
3.2.1.3 Type 3: Combination of Types 1 & 2
This usage is restricted to some verbs, for
example:
(7) O-meshiagari-ni naru.
'(Someone highly respected) eats.'
3.2.1.4 Type 4: Use of passive form -rare
The passive -rare is suffixed to the verb stem to
display subject honorifics instead of the passive
interpretation; for example:
(8) Tabe-rare-ru.
'(An honouree) eats.'
c.f. (6a) Taberu. [non-honorific: neutral]
'(Someone) eats.'
Note that there are no corresponding
constructions of Types 3 and 4 for non-subject
honorific forms.
3.2.1.5 Type 5: Lexical semantics
The semantics of some verbs give rise to
referential restrictions, in that the subject must
be higher or lower than the non-subject referent.
This has been neglected in previous studies of
honorification. Analogous to the example in (1),
insotsu ?? ?take? has a restricted usage as ?(a
higher ranked person) leads (a group of lower
ranked people).?
We used Lexeed (Bond et al 2004) - a
manually built self-contained lexicon, to extract
verbs and verbal nouns with such referential
restrictions. It consists of words and their
definitions for the most familiar 28,000 words in
Japanese, as measured by native speakers. This
set is formulated to cover the most basic words,
which cover 72.2% of the words in a typical
Japanese newspaper. Since honorification tends
to be found more in sophisticated words than in
basic words, we used those extracted verbs as
seeds to expand the list using the Goi-Taikei
thesaurus (Ikehara et al 1997).
For example, the semantic class meirei ??
?command? (Class Number 1824) lists
synonyms, such as iitsukeru ????? ?tell?,
and shiji ?? ?instruct?, all of which exhibit the
same referential restriction: a high ranked person
as the subject and a low ranked person as the
object. However, this is not always the case. For
instance, kyoka ?? ?permit? (Class Number
1735) includes as its synonyms dooi ?? ?agree?
and sansei ?? ?agree/approve? that do not
exhibit the same referential restriction as kyoka.
We manually extracted from Lexeed 698
such verbs (397 of these are ?a higher ranked
person does to a lower person? and the rest 301
are the reverse), and from Goi-Taikei further 429
(228, 201 respectively), 1127 in total.
3.2.2 Nouns and adjectives
Honorification is also expressed on nouns
(including verbal nouns) and adjectives by the
honorific prefix o- with variants on-, go-, and mi.
Honorific prefixes have four functions:
[1] An entity/action belongs to the honouree.
[2] An entity/action has an implication to the
honouree, even when it belongs to the
speaker.3
[3] Addressee honorifics to show formality of
speech/politeness to the addressee.4
[4] Conventional usage5
The use of the honorific particles in [1]
provides important information on the type of
referents. Possessors are seldom expressed and
there are no definite/indefinite articles in
Japanese (Bond 2005), but honorific particles
can take on these functions. For example, o-
nimotsu (honorable luggage) means 'your/his/...
luggage', and go-ryokoo means 'your/his/... trip'.
Although the exact identity of the honoree-
possessors is context dependent, as the following
minimal pair of sentences show, in (9a) the
possessors can never be the speaker or the
speaker's in-group member (see Subsection 4.2
for ?in-group?), as indicted by *. In contrast, the
identity of the subject, as in (9b) without an
honorific particle, is generally the speaker or his
in-group member.
3 For example, o-tegami (literally, ?honourable letter?) is
used when the letter is something to do with the honouree;
it could be the letter that the honouree wrote, a letter sent
by someone else to the honouree, or a letter written by the
speaker to the honouree.
4 For example, o-hana (flowers) and o-shokuji (meal) are
such cases where possession is not a concern.
5 The standard example of this type is go-han ?honourable-
rice? meaning 'rice/meal'. Such honorific particles do not
convey honorifics, but are seen as part of set phrases.
94
(9a)???????
O-genki de iru.
Hon.-good health be stay
'(The honouree/*I/*In-group) is in good health.'
(9b) ??????
?-Genki de iru
good health be stay
'(*The honouree /I/ In-group) is well.'
3.2.3 Sentence structures
Honorification is manifested also in the choice
of sentence structure. The causative construction
can be used only when the causer is superior in
social hierarchy to the causee, as shown in (10).
If the causee is equal to or superior over the
causer, the benefactive construction is used,
conveying the same proposition with the
connotation that the causee has accepted the
causer's request instead of command, as in (11).
Thus, the sentence structure reveals the
referential disparity in rank.
(10) ???????????
Watashi-wa otooto-ni hon-o yom-ase-ta.
I younger brother-IO book-OB read-Caus-Past
'I made my younger brother read the book.'
(11) ???????????????
Watashi-wa sensei-ni hon-o yon-de morat-ta.
I teacher-IO book-OB read-and receive-Past
'(I requested my teacher to read the book for me, and)
my teacher read the book for me.'
4 Ranking factors
Section 3 explained the various forms that
indicate disparity of referents in rank. This
section describes three factors that induce such
disparity in rank: Social hierarchy, in-group and
out-group distinction, and unfamiliarity of the
addressee.
4.1 Social hierarchy
Social hierarchy is the core rank-inducing
factor, which can be overridden by the other
two factors. It refers to social ranks in such
social settings as company, school, family, as
well as general age/generational rank. For
example, an employer is perceived as ranked
higher than his employees, and a teacher is
higher than his students, and the older a person
is, the higher he is ranked.
Social hierarchy functions similar to the
Subject-Verb agreement in terms of person,
number and gender seen in many European
languages. Although Japanese has no syntactic
coding of such a S-V agreement, verbs agree
with the referential relation of the subject and
other referents in terms of social hierarchy (the
same view is held by Pollard and Sag 1994).
4.2 In-group and out-group distinction
Referents are also classified according to the in-
group and out-group distinction, depending on
the social relation among three parties: the
speaker, the addressee, and the people being
referred to.6 For example, in (12) an officer of
a company (the speaker) talks about the
president of his company (referent) to his boss
(addressee). The officer is ranked lower than
the president and his boss, and accordingly the
subject honorific and addressee honorific
(?Polite?) are used. However in (13), when he
reports the same proposition to people outside
the company, the president is regarded as a ?in-
group? member to the speaker, and therefore the
description of him uses the non-subject
honorific form of verb, the same as the speaker
would use to describe himself. In other words,
the rank assigned from social hierarchy is
overridden by the in-group and out-group
distinction.
(12) ??????????????
Shachoo-ga irrashai-mashi-ta.
president-SB come[SubH]-Polite-Past
?The president has arrived.'
(13) ?????????
Shachoo-ga mairi-mashi-ta.
president-SB come[NsubH]-Polite-Past
?The president has arrived.'
Thus, the dichotomy of in-group/out-group
distinction is relative. This is prominently seen
in the use of family terms, as shown in Table 2.
When someone talks to her/his mother or about
her with her/his family, 'mother' is referred to as
6 Generally, the type of honorific use is also determined by
the three parties. To be more precise, setting and bystander
also play a part in determining the type of honorifics to be
used (Brown and Levinson 1987).
95
okaasan, using the out-group (OG) form, while
when talking about her to outsiders, she is
referred to as haha, using the in-group (IG)
form.
There are three lexical types that reflect the
in-group and out-group distinction.
1) the deictic prefixes:
too- hon-, hei-, setu, etc. for the in-group
use, translated into English as 'my/our', and
ki-, o-, on-, etc. for the out-group use,
translated as 'your/his/her/their'. For
example, too-koo (my/our school) versus ki-
koo (your/their school).
2) the suffixes -san/-sama/-dono:
for instance, gakusei 'a student' is referred to
as gakusei-san out of deference to a
respected out-group person (e.g. ?your
students?, ?student of your school?).
3) suppletive forms:
some examples are shown in Table 2.
IG referent OG referent
mother haha o-kaasan
father chichi o-toosan
wife tsuma, kanai o-kusan
son segare, musuko go-shisoku,
musuko-san
daughter musume o-joo-san
Table 2: Referential forms by in-group and
out-group
4.3 Unfamiliarity of the addressee
In apparent absence of disparity in social
ranking and age, honorifics can still be used
subjectively in formal settings, when
communicating with unfamiliar people,
particularly by female speakers.
5 Assigning referents with ranks using
Goi-Taikei thesaurus
In order to make use of honorific information,
each referent must be assigned with a rank to
determine which referent in a sentence is
ranked the highest. We use Goi-Taikei for this
assignment (Ikehara et al 1997). It has a
semantic feature tree with over 3,000 nodes of
semantic classes organised with a maximum of
12 levels (see Figure 1). It includes in its
semantic classes information on occupational
status, generation, family composition; the sort
of information needed for this assignment.
Noun Level 1
Concrete Abstract
Agent Place Concrete
Person Organization
Human Person (Occupation/Status/Role)
Human Occupation Status Role Level 6
Gender Seniority Specialised king minister director ?
Male Fem Infant ?Adult Elderly
Teacher Student
? ? ? ? ? ? Level 12
Figure 1: Excerpt from Goi-Taikei thesaurus
In addition, the following two tasks are
required:
1) Group some semantic classes together from
different nodes.
For the honorific use, some semantic classes
that are scattered over different nodes in the tree
should be grouped together. For instance, the
information relevant to Social hierarchy is found
not only under Occupation (status) but also
under Organization, Family, and so forth.
2) Rank the semantic classes where relevant.
Figure 2 is a preliminary result showing
ranks of referents in selected semantic classes,
noted as class names followed by their semantic
class numbers in Goi-Taikei listed in ascending
order.
96
Social hierarchy
- senior 142 > junior 143
- experienced 145 > less experienced 146
- master 139 > apprentice 140
- teacher 237 > student 238
- king/emperor 320 > aristocrats 321
- minister 322> clerk 326
- directors 323 > deputy director 324 >
executive 325 >
Age/Generation
- elderly 63 > adult 60 > youth 57 >
boy/girl 54 > infant 51
- ancestors 84 > grandparents 81 > parents
78 > children 86 > grandchildren 89 >
descendants 92
- older sibling 94 > younger sibling 97
- uncle/aunt 101 > nephew/niece 104
Figure 2: Strings of ranks
This list needs to be expanded. As the list is
taken exhaustively from Goi-Taikei, these
entries must be augmented with other thesauri,
organisation charts, genealogical trees, and other
ways as well as by hand.
6 Calibration of ranks
Ranks of referents are not absolute, but relative
to the other referents in the sentence. For
example, an adult referent is ranked higher than
a youth, but the same referent is ranked lower
when appearing with an elderly in the same
sentence. Similarly, manager in a company is
higher than workers with no title, but the same
manager is lower than the company president.
Thus, the calibration of rankings is necessary.
However, calibrating ranks while capturing
relative ranks is an extremely complicated task,
as any combination of referents and ranking
categories can appear in a sentence as well as
the fact that one referent may belong to multiple
categories. For example, a measure has to be
taken in case one referent is ranked in one
string (e.g. ?minister?) than the other referent
(e.g. ?clerk?) in the same sentence, but he is
lower in another string (e.g. ?less experienced?
in the profession or younger in age) (see Figure
2); or when the in-group and out-group
distinction takes the precedence in the form of
honorifics, for instance a referent is senior than
the other referent, but he is an in-group member
to the speaker.
More complicated still, within the same
class, there may exist a disparity in rank. For
example, the age difference, even by one year,
can determine the use of honorifics, so that
honorifics is used between two referents under
the same class adult.
Considering the above, we propose the
following calibration scheme as an initial step
of dealing with the complex phenomena of
honorifics.
[1] create referential links, example modules
of which are suggested in Figure 3. Each
string of ranks in Figure 2 constitutes a
module, which is connected to another
module. Figure 3 shows that a referent
?JOHN? is a student as well as a child of his
parents that is depicted. JOHN belongs to
other modules of strings; he may be an elder
bother at home, and may be a senior student
at school, each of which is a member of a
module and is connected to other modules.
Connections between two modules may be
more than one, for example, ?grandparent?
may be a teacher of ?teacher? of JOHN.
It is necessary to identify as many
modules as identifiable and to link them in
order to accurately determine the ranks of each
referent for a sentence.
Out-group members
Prime Minister
Ministry of Education In-group members
Principal ancestor
Head teacher grandparent
teacher PTA/parent
student ?JOHN? child
grandchild
descendant
Figure 3: Two modules of referential links
97
[2] a diagram for calibrating ranks
Figure 4 is proposed to capture the
mechanisms of honorifics that determine the
ranks of referents for a sentence.
The referential links are the first (core)
rank determining factor. When one referent
belongs to multiple strings, for instance, a
string from Social hierarchy and another
from Age, then the former takes the higher
rank, which is noted as ?Social > Age?. The
case where two referents belong to the same
class but still appear with honorifics is due to
the subtle difference in rank, noted as ?The
same class?.
These ranks assigned by the referential
links can be overridden by ?in/out group
precedence?, which is determined by the type
of modules, as shown in Figure 3.
The use of honorifics in absence of
disparity in social rank is interpreted as lack
of familiarity of the addressee.
Referential links
NB: Multiple links (Social > Age)
NB: The same class
Unfamiliarity (no disparity in rank)
Figure 4: Diagram of calibrating ranks
7 Annotation
Our annotation method is an extension of the
framework of JACY, a Japanese HPSG grammar
(Siegel 2000), as discussed in Section 2.
Subsection 7.1 describes the JACY annotation
and Subsection 7.2 is the extension we made
from this research.
7.1 JACY annotation
The JACY annotation scheme for honorification
can be seen in Figure 5 with examples on the
bottom of the tree. It annotates honorification
concerning referential nouns (honorific entity),
predicative honorifics (subject honorifics) that
are triggered by honorific entities, and predicates
of the addressee honorifics. The notion of
polarity is used to denote the three types of
value; a polarity value ?+? means a subject
honorific form, ?-? denotes a non-subject
honorific form, and ?bool? is indeterminate. It is
capable of accounting for the basic types of
honorification, as being expressed by verb
forms, suppletive forms, passive, nouns and
adjectives.
Figure 5: JACY annotation for honorifics
(with examples)
7.2 Extended JACY annotation
Based on our findings, we extend the JACY
annotation Figure 5 to Figure 6 by adding two
relations in the honorification, Social ranking
and In-group relation.
As for Social ranking, Subsection 3.2.1.5
introduced those verbs with referential
restrictions, such as insotsu ?? ?take? has a
restricted usage as ?(a higher ranked person)
leads (a group of lower ranked people).? These
lexical items are added to honorific information
in JACY, as being part of the lexical type
hierarchy. In addition, the use of causative that
imposes the interpretation ?a high ranked person
acts on the lower? is accounted for under Social
ranking (see Subsection 3.2.3).
We notate the relation deriving from social
ranking as social_ranking_rel. It has two
arguments, which show the semantic indices of
the verbal arguments, the first (or left) argument
being ranked higher. The relation is triggered by
the lexical types and the causative usage.
Example 10, 'I made my younger brother read
the book', is annotated with social_ranking-rel
(watashi, otooto), while example 11 'my teacher
read the book for me' is annotated with
social_ranking-rel (sensei, watashi).
In/out group precedence
Honorifics
Propositional
honorifics
Subject
honorific
Addressee
honorifics
(Polarity -)(Polarity +)
o-hanasi ni nar-u, o-hanasi su-ru,
hanasi-mas-u, hanas-u,(poliitiy -)
Honorifics
entity
sensei watashi
(Polarity +) (Polarity -)(Polarity +)
98
Figure 6: Extended JACY annotation for
honorifics (with examples)
The distinction between in-group and out-
group makes it necessary to add a further
relation, called in_group_rel. It has two
arguments, relating the speaker with the
predicate's subject. As in the other honorific
relations, it gets a POLARITY feature, showing
an in-group relation with [POLARITY +] and an
out-group relation with [POLARITY -], and
?bool? for indeterminate. The nominal
expressions that trigger in-group relations (such
as okaasan and haha in Table 2) add this
relation to the CONTEXT.
For a predicate, such as Example 13, with
subject honorific information [POLARITY -]
and a subject with honorific entity information
[POLARITY +], an in_group_rel is added to
relate the speaker and the subject, annotated as
in_group_rel (speaker, shachoo).
To better understand the interaction of Social
ranking and In-group relation, we refer to
examples 12 and 13. In processing, predicative
honorifics is identified not by the referential
nouns, but by the predicates. So, if the predicate
is minus_shon (- SubH) and the subject is
plus_ohon (+ entity honorifics), i.e. (13), then
there is an in-group relation. On the other hand,
with an out-group relation as in (12), the
predicate is plus_shon (+ SubH) and the subject
is plus_ohon (+ entity honorifics).
(12) ??????????????
Shachoo-ga irrashai-mashi-ta.
president-SB come[SubH]-Polite-Past
?The president has arrived.'
(13) ?????????
Shachoo-ga mairi-mashi-ta.
president-SB come[NsubH]-Polite-Past
?The president has arrived.'
(14) is an example that combines different
types of honorific information. Its CONTEXT
annotation is described in Figure 7.7 The usage
of the noun haha triggers an in_group_rel
(speaker, haha) with [POLARITY +], while the
usage of the noun okaasan will trigger an
in_group_rel (speaker, okaasan) with
[POLARITY -]. The extraction of social ranking
information from Goi-Taikei shown in Figure 2
makes use of this relation social_ranking_rel
(arg1, arg2) between the entities in the sentence,
for example 14 social_ranking_rel (sensei,
haha).
(14) ?????????????????
Haha-wa senseo-ni denwa-o
Monther-Top teacher-Dat call-Acc
site-morai-mashi-ta.
do-receive-Polite-past
?My mother got the teacher to call.?
CONTEXT C-INDS SPEAKER #1
ADDRESSEE #2
entity-honor-rel
HONORER #1
HONORED sensei
POLARITY +
addr-honor-rel
HONORER #1
HONORED #2
POLARITY +
empathy-rel
BACKGR  EMPER #1
EMPEE haha
in-group-rel
ARG1 #1
AEG2 haha
POLARITY +
Social-ranking-rel
ARG1 sensei
ARG2 haha
Figure 7: Context annotation of complex
honorification
7 The values of HONORED or ARGx are actually pointers
to the indices of the entities, which are written here as the
orthographic realization for readability.
Honorifics
Propositional
honorifics
Subject
honorifics
Addressee
honorifics
(Polarity -)(Polarity +)
o-hanasi ni nar-u,
etc.
o-hanasi su-ru, etc.
hanasi-mas-u, etc.
hanas-u, etc.
(Polarity -)
Entity
honorifics
sensei watashi
(Polarity +) (Polarity -)(Polarity +)
Social
ranking
In-group
relation
(Polarity +)
(Polarity -)Yomaseru
(Causative),
Insotsu, etc
haha okaasan
99
8 Conclusion
This paper has proposed a scheme to realise the
complex linguistic phenomena of the Japanese
honorifics in tangible forms for auto-processing.
Ranking referents is an extremely complex task
that requires a combined understanding of
syntax, semantics and pragmatics in many
dimensions.
In future work, the referential links and their
calibration need to be expanded to make an
annotation more meaningful. This will be an
incremental process and takes a substantial
amount of work, perhaps comparable to that
required in creating a thesaurus or knowledge
base.
The annotated data will be a valuable
resource for research on zero pronoun resolution
and Machine Translation of generating Japanese
sentences. As the Korean honorification system
is quite similar to the Japanese, it will be
feasible to make use of the approach also for
Korean. Furthermore, a part of the approach can
be extended as well for Chinese, since Japanese
makes use of the Chinese characters.
Acknowledgement
We are grateful to Kyonghee Paik for her
invaluable advice.
References
F. Bond. 2005. Translating the untranslatable:
A solution to the problem of generating
English determiners. CSLI Publications.
California
F. Bond et al 2004. The Hinoki Treebank: A
Treebank for Text Understanding, In Proc. of
the First IJCNLP, Lecture Notes in Computer
Science. Springer Verlag
P. Brown and S.C. Levinson. 1987. Politeness:
Some universals in language usage.
Cambridge University Press
K. Dohsaka. 1990. Identifying the referents of
zero-pronouns in Japanese dialogues. In Proc.
Of the 9th European Conference on Artificial
Intelligence. 240-245
T. Gunji. 1987. Japanese phrase structure
grammar. Dordrecht: Reidel
S.I. Harada. 1976. Honorifics. In Shibatani (ed).
Japanese Generative Grammar. Syntax and
Semantics Series Vol.5. New York: Academic
Press. 499-561
M. Hoelter. 1999. Lexical-semantic information
in Head-Driven Phrase Structure grammar
and natural language processing. Lincom
Theoretical Linguistics
S. Ikehara et al (eds). 1997. Japanese Lexicon.
Iwanami Publishing
A. Ikeya. 1983. Japanese honorific systems. In
Proc. of the 3rd Korean-Japanese Joint
Workshop. Seoul
S. Martin. 1964. Speech levels in Japanese and
Korean. In Hymes. Language in culture and
society. Harper and Row. 407-415
Y. Matsumoto. 1997. The rise and fall of
Japanese nonsubject honorifics. Journal of
Pragmatics. 28:719-740
H. Nakaiwa. 2002. Studies on zero pronoun
resolution for the Japanese-to-English
Machine translation (in Japanese).
S. Nariyama. 2003. Ellipsis and Reference-
tracking in Japanese. SLCS 66, Amsterdam:
John Benjamins
S. Nariyama et al 2005. Extracting
Representative Arguments from Dictionaries
for Resolving Zero Pronouns. In Proc. of the
MT Summit X, Phuket
C. Pollard and I.A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Chicago:
University of Chicago Press
M. Siegel. 2000. Japanese Honorification in an
HPSG Framework. In Proc. of the 14th
Pacific Asia Conference on Language,
Information and Computation. 289-300.
Waseda University, Tokyo
M. Siegel and E M. Bender. 2002. Efficient
deep processing of Japanese. In Proc. of the
3rd Workshop on Asian Language Resources
and International Standardization at
COLING.Taipei
J. Wenger. 1983. Variation and change in
Japanese honorific forms. In Miyagawa and
Kitagawa (eds). Studies in Japanese language
use. vol.16. Edmonton: Linguistic Research
Inc. 283-292
100
An Integrated Architecture for Shallow and Deep Processing
Berthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,
Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,
Feiyu Xu, Markus Becker and Hans-Ulrich Krieger
DFKI GmbH
Stuhlsatzenhausweg 3
Saarbru?cken, Germany
whiteboard@dfki.de
Abstract
We present an architecture for the integra-
tion of shallow and deep NLP components
which is aimed at flexible combination
of different language technologies for a
range of practical current and future appli-
cations. In particular, we describe the inte-
gration of a high-level HPSG parsing sys-
tem with different high-performance shal-
low components, ranging from named en-
tity recognition to chunk parsing and shal-
low clause recognition. The NLP com-
ponents enrich a representation of natu-
ral language text with layers of new XML
meta-information using a single shared
data structure, called the text chart. We de-
scribe details of the integration methods,
and show how information extraction and
language checking applications for real-
world German text benefit from a deep
grammatical analysis.
1 Introduction
Over the last ten years or so, the trend in application-
oriented natural language processing (e.g., in the
area of term, information, and answer extraction)
has been to argue that for many purposes, shallow
natural language processing (SNLP) of texts can
provide sufficient information for highly accurate
and useful tasks to be carried out. Since the emer-
gence of shallow techniques and the proof of their
utility, the focus has been to exploit these technolo-
gies to the maximum, often ignoring certain com-
plex issues, e.g. those which are typically well han-
dled by deep NLP systems. Up to now, deep natural
language processing (DNLP) has not played a sig-
nificant role in the area of industrial NLP applica-
tions, since this technology often suffers from insuf-
ficient robustness and throughput, when confronted
with large quantities of unrestricted text.
Current information extractions (IE) systems
therefore do not attempt an exhaustive DNLP analy-
sis of all aspects of a text, but rather try to analyse or
?understand? only those text passages that contain
relevant information, thereby warranting speed and
robustness wrt. unrestricted NL text. What exactly
counts as relevant is explicitly defined by means
of highly detailed domain-specific lexical entries
and/or rules, which perform the required mappings
from NL utterances to corresponding domain knowl-
edge. However, this ?fine-tuning? wrt. a particular
application appears to be the major obstacle when
adapting a given shallow IE system to another do-
main or when dealing with the extraction of com-
plex ?scenario-based? relational structures. In fact,
(Appelt and Israel, 1997) have shown that the cur-
rent IE technology seems to have an upper perfor-
mance level of less than 60% in such cases. It seems
reasonable to assume that if a more accurate analy-
sis of structural linguistic relationships could be pro-
vided (e.g., grammatical functions, referential rela-
tionships), this barrier might be overcome. Actually,
the growing market needs in the wide area of intel-
ligent information management systems seem to re-
quest such a break-through.
In this paper we will argue that the quality of cur-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 441-448.
                         Proceedings of the 40th Annual Meeting of the Association for
rent SNLP-based applications can be improved by
integrating DNLP on demand in a focussed manner,
and we will present a system that combines the fine-
grained anaysis provided by HPSG parsing with a
high-performance SNLP system into a generic and
flexible NLP architecture.
1.1 Integration Scenarios
Owing to the fact that deep and shallow technologies
are complementary in nature, integration is a non-
trivial task: while SNLP shows its strength in the
areas of efficiency and robustness, these aspects are
problematic for DNLP systems. On the other hand,
DNLP can deliver highly precise and fine-grained
linguistic analyses. The challenge for integration is
to combine these two paradigms according to their
virtues.
Probably the most straightforward way to inte-
grate the two is an architecture in which shallow and
deep components run in parallel, using the results of
DNLP, whenever available. While this kind of ap-
proach is certainly feasible for a real-time applica-
tion such as Verbmobil, it is not ideal for processing
large quantities of text: due to the difference in pro-
cessing speed, shallow and deep NLP soon run out
of sync. To compensate, one can imagine two possi-
ble remedies: either to optimize for precision, or for
speed. The drawback of the former strategy is that
the overall speed will equal the speed of the slow-
est component, whereas in case of the latter, DNLP
will almost always time out, such that overall preci-
sion will hardly be distinguishable from a shallow-
only system. What is thus called for is an integrated,
flexible architecture where components can play at
their strengths. Partial analyses from SNLP can be
used to identify relevant candidates for the focussed
use of DNLP, based on task or domain-specific crite-
ria. Furthermore, such an integrated approach opens
up the possibility to address the issue of robustness
by using shallow analyses (e.g., term recognition)
to increase the coverage of the deep parser, thereby
avoiding a duplication of efforts. Likewise, integra-
tion at the phrasal level can be used to guide the
deep parser towards the most likely syntactic anal-
ysis, leading, as it is hoped, to a considerable speed-
up.
shallow
NLP
components
NLP
deep
components internal repr.
layer
multi
chart
annot.
XML
external repr.
generic OOP
component
interface
WHAM
application
specification
input and
result
Figure 1: The WHITEBOARD architecture.
2 Architecture
The WHITEBOARD architecture defines a platform
that integrates the different NLP components by en-
riching an input document through XML annota-
tions. XML is used as a uniform way of represent-
ing and keeping all results of the various processing
components and to support a transparent software
infrastructure for LT-based applications. It is known
that interesting linguistic information ?especially
when considering DNLP? cannot efficiently be
represented within the basic XML markup frame-
work (?typed parentheses structure?), e.g., linguistic
phenomena like coreferences, ambiguous readings,
and discontinuous constituents. The WHITEBOARD
architecture employs a distributed multi-level repre-
sentation of different annotations. Instead of trans-
lating all complex structures into one XML docu-
ment, they are stored in different annotation layers
(possibly non-XML, e.g. feature structures). Hyper-
links and ?span? information together support effi-
cient access between layers. Linguistic information
of common interest (e.g. constituent structure ex-
tracted from HPSG feature structures) is available in
XML format with hyperlinks to full feature struc-
ture representations externally stored in correspond-
ing data files.
Fig. 1 gives an overview of the architecture of
the WHITEBOARD Annotation Machine (WHAM).
Applications feed the WHAM with input texts and
a specification describing the components and con-
figuration options requested. The core WHAM en-
gine has an XML markup storage (external ?offline?
representation), and an internal ?online? multi-level
annotation chart (index-sequential access). Follow-
ing the trichotomy of NLP data representation mod-
els in (Cunningham et al, 1997), the XML markup
contains additive information, while the multi-level
chart contains positional and abstraction-based in-
formation, e.g., feature structures representing NLP
entities in a uniform, linguistically motivated form.
Applications and the integrated components ac-
cess the WHAM results through an object-oriented
programming (OOP) interface which is designed
as general as possible in order to abstract from
component-specific details (but preserving shallow
and deep paradigms). The interfaces of the actu-
ally integrated components form subclasses of the
generic interface. New components can be inte-
grated by implementing this interface and specifying
DTDs and/or transformation rules for the chart.
The OOP interface consists of iterators that walk
through the different annotation levels (e.g., token
spans, sentences), reference and seek operators that
allow to switch to corresponding annotations on a
different level (e.g., give all tokens of the current
sentence, or move to next named entity starting
from a given token position), and accessor meth-
ods that return the linguistic information contained
in the chart. Similarily, general methods support
navigating the type system and feature structures of
the DNLP components. The resulting output of the
WHAM can be accessed via the OOP interface or as
XML markup.
The WHAM interface operations are not only
used to implement NLP component-based applica-
tions, but also for the integration of deep and shallow
processing components itself.
2.1 Components
2.1.1 Shallow NL component
Shallow analysis is performed by SPPC, a rule-
based system which consists of a cascade of
weighted finite?state components responsible for
performing subsequent steps of the linguistic anal-
ysis, including: fine-grained tokenization, lexico-
morphological analysis, part-of-speech filtering,
named entity (NE) recognition, sentence bound-
ary detection, chunk and subclause recognition,
see (Piskorski and Neumann, 2000; Neumann and
Piskorski, 2002) for details. SPPC is capable of pro-
cessing vast amounts of textual data robustly and ef-
ficiently (ca. 30,000 words per second in standard
PC environment). We will briefly describe the SPPC
components which are currently integrated with the
deep components.
Each token identified by a tokenizer as a poten-
tial word form is morphologically analyzed. For
each token, its lexical information (list of valid read-
ings including stem, part-of-speech and inflection
information) is computed using a fullform lexicon
of about 700,000 entries that has been compiled out
from a stem lexicon of about 120,000 lemmas. Af-
ter morphological processing, POS disambiguation
rules are applied which compute a preferred read-
ing for each token, while the deep components can
back off to all readings. NE recognition is based on
simple pattern matching techniques. Proper names
(organizations, persons, locations), temporal expres-
sions and quantities can be recognized with an av-
erage precision of almost 96% and recall of 85%.
Furthermore, a NE?specific reference resolution is
performed through the use of a dynamic lexicon
which stores abbreviated variants of previously rec-
ognized named entities. Finally, the system splits
the text into sentences by applying only few, but
highly accurate contextual rules for filtering implau-
sible punctuation signs. These rules benefit directly
from NE recognition which already performs re-
stricted punctuation disambiguation.
2.1.2 Deep NL component
The HPSG Grammar is based on a large?scale
grammar for German (Mu?ller, 1999), which was
further developed in the VERBMOBIL project for
translation of spoken language (Mu?ller and Kasper,
2000). After VERBMOBIL the grammar was adapted
to the requirements of the LKB/PET system (Copes-
take, 1999), and to written text, i.e., extended with
constructions like free relative clauses that were ir-
relevant in the VERBMOBIL scenario.
The grammar consists of a rich hierarchy of
5,069 lexical and phrasal types. The core grammar
contains 23 rule schemata, 7 special verb move-
ment rules, and 17 domain specific rules. All rule
schemata are unary or binary branching. The lexicon
contains 38,549 stem entries, from which more than
70% were semi-automatically acquired from the an-
notated NEGRA corpus (Brants et al, 1999).
The grammar parses full sentences, but also other
kinds of maximal projections. In cases where no full
analysis of the input can be provided, analyses of
fragments are handed over to subsequent modules.
Such fragments consist of maximal projections or
single words.
The HPSG analysis system currently integrated
in the WHITEBOARD system is PET (Callmeier,
2000). Initially, PET was built to experiment
with different techniques and strategies to process
unification-based grammars. The resulting sys-
tem provides efficient implementations of the best
known techniques for unification and parsing.
As an experimental system, the original design
lacked open interfaces for flexible integration with
external components. For instance, in the beginning
of the WHITEBOARD project the system only ac-
cepted fullform lexica and string input. In collabora-
tion with Ulrich Callmeier the system was extended.
Instead of single word input, input items can now
be complex, overlapping and ambiguous, i.e. essen-
tially word graphs. We added dynamic creation of
atomic type symbols, e.g., to be able to add arbitrary
symbols to feature structures. With these enhance-
ments, it is possible to build flexible interfaces to
external components like morphology, tokenization,
named entity recognition, etc.
3 Integration
Morphology and POS The coupling between the
morphology delivered by SPPC and the input needed
for the German HPSG was easily established. The
morphological classes of German are mapped onto
HPSG types which expand to small feature struc-
tures representing the morphological information in
a compact way. A mapping to the output of SPPC
was automatically created by identifying the corre-
sponding output classes.
Currently, POS tagging is used in two ways. First,
lexicon entries that are marked as preferred by the
shallow component are assigned higher priority than
the rest. Thus, the probability of finding the cor-
rect reading early should increase without excluding
any reading. Second, if for an input item no entry is
found in the HPSG lexicon, we automatically create
a default entry, based on the part?of?speech of the
preferred reading. This increases robustness, while
avoiding increase in ambiguity.
Named Entity Recognition Writing HPSG gram-
mars for the whole range of NE expressions etc. is
a tedious and not very promising task. They typi-
cally vary across text sorts and domains, and would
require modularized subgrammars that can be easily
exchanged without interfering with the general core.
This can only be realized by using a type interface
where a class of named entities is encoded by a gen-
eral HPSG type which expands to a feature structure
used in parsing. We exploit such a type interface for
coupling shallow and deep processing. The classes
of named entities delivered by shallow processing
are mapped to HPSG types. However, some fine-
tuning is required whenever deep and shallow pro-
cessing differ in the amount of input material they
assign to a named entity.
An alternative strategy is used for complex syn-
tactic phrases containing NEs, e.g., PPs describ-
ing time spans etc. It is based on ideas from
Explanation?based Learning (EBL, see (Tadepalli
and Natarajan, 1996)) for natural language analy-
sis, where analysis trees are retrieved on the basis
of the surface string. In our case, the part-of-speech
sequence of NEs recognised by shallow analysis is
used to retrieve pre-built feature structures. These
structures are produced by extracting NEs from a
corpus and processing them directly by the deep
component. If a correct analysis is delivered, the
lexical parts of the analysis, which are specific for
the input item, are deleted. We obtain a sceletal
analysis which is underspecified with respect to the
concrete input items. The part-of-speech sequence
of the original input forms the access key for this
structure. In the application phase, the underspeci-
fied feature structure is retrieved and the empty slots
for the input items are filled on the basis of the con-
crete input.
The advantage of this approach lies in the more
elaborate semantics of the resulting feature struc-
tures for DNLP, while avoiding the necessity of
adding each and every single name to the HPSG lex-
icon. Instead, good coverage and high precision can
be achieved using prototypical entries.
Lexical Semantics When first applying the origi-
nal VERBMOBIL HPSG grammar to business news
articles, the result was that 78.49% of the miss-
ing lexical items were nouns (ignoring NEs). In
the integrated system, unknown nouns and NEs can
be recognized by SPPC, which determines morpho-
syntactic information. It is essential for the deep sys-
tem to associate nouns with their semantic sorts both
for semantics construction, and for providing se-
mantically based selectional restrictions to help con-
straining the search space during deep parsing. Ger-
maNet (Hamp and Feldweg, 1997) is a large lexical
database, where words are associated with POS in-
formation and semantic sorts, which are organized in
a fine-grained hierarchy. The HPSG lexicon, on the
other hand, is comparatively small and has a more
coarse-grained semantic classification.
To provide the missing sort information when re-
covering unknown noun entries via SPPC, a map-
ping from the GermaNet semantic classification to
the HPSG semantic classification (Siegel et al,
2001) is applied which has been automatically ac-
quired. The training material for this learning pro-
cess are those words that are both annotated with se-
mantic sorts in the HPSG lexicon and with synsets
of GermaNet. The learning algorithm computes a
mapping relevance measure for associating seman-
tic concepts in GermaNet with semantic sorts in the
HPSG lexicon. For evaluation, we examined a cor-
pus of 4664 nouns extracted from business news
that were not contained in the HPSG lexicon. 2312
of these were known in GermaNet, where they are
assigned 2811 senses. With the learned mapping,
the GermaNet senses were automatically mapped to
HPSG semantic sorts. The evaluation of the map-
ping accuracy yields promising results: In 76.52%
of the cases the computed sort with the highest rel-
evance probability was correct. In the remaining
20.70% of the cases, the correct sort was among the
first three sorts.
3.1 Integration on Phrasal Level
In the previous paragraphs we described strategies
for integration of shallow and deep processing where
the focus is on improving DNLP in the domain of
lexical and sub-phrasal coverage.
We can conceive of more advanced strategies for
the integration of shallow and deep analysis at the
length cover- complete LP LR 0CB   2CB
age match
  40 100 80.4 93.4 92.9 92.1 98.9
 40 99.8 78.6 92.4 92.2 90.7 98.5
Training: 16,000 NEGRA sentences
Testing: 1,058 NEGRA sentences
Figure 2: Stochastic topological parsing: results
level of phrasal syntax by guiding the deep syntac-
tic parser towards a partial pre-partitioning of com-
plex sentences provided by shallow analysis sys-
tems. This strategy can reduce the search space, and
enhance parsing efficiency of DNLP.
Stochastic Topological Parsing The traditional
syntactic model of topological fields divides basic
clauses into distinct fields: so-called pre-, middle-
and post-fields, delimited by verbal or senten-
tial markers. This topological model of German
clause structure is underspecified or partial as to
non-sentential constituent boundaries, but provides
a linguistically well-motivated, and theory-neutral
macrostructure for complex sentences. Due to its
linguistic underpinning the topological model pro-
vides a pre-partitioning of complex sentences that is
(i) highly compatible with deep syntactic structures
and (ii) maximally effective to increase parsing ef-
ficiency. At the same time (iii) partiality regarding
the constituency of non-sentential material ensures
the important aspects of robustness, coverage, and
processing efficiency.
In (Becker and Frank, 2002) we present a corpus-
driven stochastic topological parser for German,
based on a topological restructuring of the NEGRA
corpus (Brants et al, 1999). For topological tree-
bank conversion we build on methods and results
in (Frank, 2001). The stochastic topological parser
follows the probabilistic model of non-lexicalised
PCFGs (Charniak, 1996). Due to abstraction from
constituency decisions at the sub-sentential level,
and the essentially POS-driven nature of topologi-
cal structure, this rather simple probabilistic model
yields surprisingly high figures of accuracy and cov-
erage (see Fig.2 and (Becker and Frank, 2002) for
more detail), while context-free parsing guarantees
efficient processing.
The next step is to elaborate a (partial) map-
ping of shallow topological and deep syntactic struc-
tures that is maximally effective for preference-gui-
Topological Structure:
CL-V2
VF-TOPIC LK-FIN MF RK-t
NN VVFIN ADV NN PREP NN VVFIN
[ 	 [ 
	 Peter] [ 
 i?t] [ 
 gerne Wu?rstchen mit Kartoffelsalat] [ Efficient Deep Processing of Japanese 
 
Melanie SIEGEL 
DFKI GmbH 
Stuhlsatzenhausweg 3 
66123 Saarbr?cken, Germany 
siegel@dfki.de 
Emily M. BENDER 
CSLI Stanford 
220 Panama Street 
Stanford, CA, 94305-4115, USA  
bender@csli.stanford.edu  
 
Abstract  
We present a broad coverage Japanese 
grammar written in the HPSG formalism 
with MRS semantics. The grammar is 
created for use in real world applications, 
such that robustness and performance issues 
play an important role. It is connected to a 
POS tagging and word segmentation tool. 
This grammar is being developed in a 
multilingual context, requiring MRS 
structures that are easily comparable across 
languages. 
Introduction 
Natural language processing technology has 
recently reached a point where applications that 
rely on deep linguistic processing are becoming 
feasible.  Such applications (e.g. message 
extraction systems, machine translation and 
dialogue understanding systems) require natural 
language understanding, or at least an 
approximation thereof.  This, in turn, requires 
rich and highly precise information as the output 
of a parse.  However, if the technology is to 
meet the demands of real-world applications, 
this must not come at the cost of robustness.  
Robustness requires not only wide coverage by 
the grammar (in both syntax and semantics), but 
also large and extensible lexica as well as 
interfaces to preprocessing systems for named 
entity recognition, non-linguistic structures such 
as addresses, etc.  Furthermore, applications 
built on deep NLP technology should be 
extensible to multiple languages.  This requires 
flexible yet well-defined output structures that 
can be adapted to grammars of many different 
languages.  Finally, for use in real-world 
applications, NLP systems meeting the above 
desiderata must also be efficient. 
In this paper, we describe the development of 
a broad coverage grammar for Japanese that is 
used in an automatic email response application. 
The grammar is based on work done in the 
Verbmobil project (Siegel 2000) on machine 
translation of spoken dialogues in the domain of 
travel planning. It has since been greatly 
extended to accommodate written Japanese and 
new domains. 
The grammar is couched in the theoretical 
framework of Head-Driven Phrase Structure 
Grammar (HPSG) (Pollard & Sag 1994), with 
semantic representations in Minimal Recursion 
Semantics (MRS) (Copestake et al 2001).  
HPSG is well suited to the task of multilingual 
development of broad coverage grammars: It is 
flexible enough (analyses can be shared across 
languages but also tailored as necessary), and 
has a rich theoretical literature from which to 
draw analyzes and inspiration.  The 
characteristic type hierarchy of HPSG also 
facilitates the development of grammars that are 
easy to extend.  MRS is a flat semantic 
formalism that works well with typed feature 
structures and is flexible in that it provides 
structures that are under-specified for scopal 
information. These structures give compact 
representations of ambiguities that are often 
irrelevant to the task at hand.    
HPSG and MRS have the further advantage 
that there are practical and useful open-source 
tools for writing, testing, and efficiently 
processing grammars written in these 
formalisms. The tools we are using in this 
project include the LKB system (Copestake 
2002) for grammar development, [incr tsdb()] 
(Oepen & Carroll 2000) for testing the grammar 
and tracking changes, and PET (Callmeier 
2000), a very efficient HPSG parser, for 
processing.  We also use the ChaSen tokenizer 
and POS tagger (Asahara & Matsumoto 2000).  
While couched within the same general 
framework (HPSG), our approach differs from 
that of Kanayama et al(2000).  The work 
described there achieves impressive coverage 
(83.7% on the EDR corpus of newspaper text) 
with an underspecified grammar consisting of a 
small number of lexical entries, lexical types 
associated with parts of speech, and six 
underspecified grammar rules.  In contrast, our 
grammar is much larger in terms of the number 
of lexical entries, the number of grammar rules, 
and the constraints on both,1 and takes 
correspondingly more effort to bring up to that 
level of coverage.  The higher level of detail 
allows us to output precise semantic 
representations as well as to use syntactic, 
semantic and lexical information to reduce 
ambiguity and rank parses. 
1 Japanese HPSG Syntax 
The fundamental notion of an HPSG is the sign. 
A sign is a complex feature structure 
representing information of different linguistic 
levels of a phrase or lexical item. The attribute-
value matrix of a sign in the Japanese HPSG is 
quite similar to a sign in the LinGO English 
Resource Grammar (henceforth ERG) 
(Flickinger 2000), with information about the 
orthographical realization of the lexical sign in 
PHON, syntactic and semantic information in 
SYNSEM, information about the lexical status in 
LEX, nonlocal information in NONLOC, head 
information that goes up the tree in HEAD and 
information about subcategorization in SUBCAT.  
The grammar implementation is based on a 
system of types. There are 900 lexical types that 
define the syntactic, semantic and pragmatic 
properties of the Japanese words, and 188 types 
that define the properties of phrases and lexical 
rules. The grammar includes 50 lexical rules for 
inflectional and derivational morphology and 47 
phrase structure rules. The lexicon contains 5100 
stem entries. As the grammar is developed for 
use in applications, it treats a wide range of 
                                                     
1 We do also make use of generic lexical entries for 
certain parts of speech as a means of extending our 
lexicon.  See section 3 below. 
basic constructions of Japanese.  Only some of 
these phenomena can be described here. 
1.1 Subcategorization 
The structure of SUBCAT is different from the 
ERG SUBCAT structure. This is due to 
differences in subcategorization between 
Japanese and English. A fundamental difference 
is the fact that, in Japanese, verbal arguments are 
frequently omitted. For example, arguments that 
refer to the speaker, addressee, and other 
arguments that can be inferred from context are 
often omitted in spoken language. Additionally, 
optional verbal arguments can scramble. On the 
other hand, some arguments are not only 
obligatory, but must also be realized adjacent to 
the selecting head. 
To account for this, our subcategorization 
contains the attributes SAT and VAL. The SAT 
value encodes whether a verbal argument is 
already saturated (such that it cannot be 
saturated again), optional or adjacent. VAL 
contains the agreement information for the 
argument. When an argument is realized, its 
SAT value on the mother node is specified as sat 
and its SYNSEM is unified with its VAL value on 
the subcategorizing head. The VAL value on the 
mother is none. Adjacency must be checked in 
every rule that combines heads and arguments or 
adjuncts. This is the principle of adjacency, 
stated as follows: 
In a headed phrase, the SUBCAT.SAT value 
on the non-head daughter must not contain 
any adjacent arguments. In a head-
complement structure, the SUBCAT.SAT 
value of the head daughter must not contain 
any adjacent arguments besides the non-
head daughter. In a head-adjunct structure, 
the SUBCAT.SAT value of the head daughter 
must not contain any adjacent arguments. 
1.2 Verbal inflection 
Japanese verb stems combine with endings that 
provide information about honorification, tense, 
aspect, voice and mode. Inflectional rules for the 
different types of stems prepare the verb stems 
for combination with the verbal endings. For 
example, the verb stem yomu must be inflected 
to yon to combine with the past tense ending da. 
Morphological features constrain the 
combination of stem and ending. In the above 
example, the inflectional rule changes the mu 
character to the n character and assigns the value 
nd-morph to the morphological feature 
RMORPH-BIND-TYPE. The ending da selects 
for a verbal stem with this value.  
Endings can be combined with other endings, 
as in -sase-rare-mashi-ta (causative-potential-
honorific-past), but not arbitrarily: 
 *-sase-mashi-rare-ta 
 *-sase-ta-mashi-rare 
 -sase-ta 
 -rare-mashi-ta 
This is accounted for with two kinds of rules 
which realize mutually selected elements. In the 
combination of stem and ending, the verb stem 
selects for the verbal ending via the head feature 
SPEC. In the case of the combination of two 
verbal endings, the first ending selects for the 
second one via the head feature MARK. In both 
cases, the right element subcategorizes for the 
left one via SUBCAT.VAL.SPR. Using this 
mechanism, it is possible to control the sequence 
of verbal endings: Verb stems select verbal 
endings via SPEC and take no SPR, derivational 
morphemes (like causative or potential) select 
tense endings or other derivational morphemes 
via MARK and subcategorize for verb stems 
and/or verb endings via SPR (sase takes only 
verb stems), and tense endings take verb stems 
or endings as SPR and take no MARK or SPEC 
(as they occur at the end of the sequence).  
1.3 Complex Predicates 
A special treatment is needed for Japanese 
verbal noun + light verb constructions. In these 
cases, a word that combines the qualities of a 
noun with those of a verb occurs in a 
construction with a verb that has only marginal 
semantic information. The syntactic, semantic 
and pragmatic information on the complex is a 
combination of the information of the two.  
Consider example 1. The verbal noun 
benkyou contains subcategorization information 
(transitive), as well as semantic information (the 
benkyou-relation and its semantic arguments). 
The light verb shi-ta supplies tense information 
(past). Pragmatic information can be supplied by 
both parts of the construction, as in the formal 
form o-benkyou shi-mashi-ta. The rule that 
licenses this type of combination is the vn-light-
rule, a subtype of the head-marker-rule. 
Example 1: 
Benkyou shi-ta. 
study do-past 
'Someone has studied.' 
Japanese auxiliaries combine with verbs and 
provide either aspectual or perspective 
information or information about honorification. 
In a verb-auxiliary construction, the information 
about subcategorization is a combination of the 
SUBCAT information of verb and auxiliary, 
depending on the type of auxiliary. The rule 
responsible for the information combination in 
these cases is the head-specifier-rule. We have 
three basic types of auxiliaries.  The first type is 
aspect auxiliaries.  These are treated as raising 
verbs, and include such elements as iru (roughly, 
progressive) and aru (roughly, perfective), as 
can be seen in example 2.  The other two classes 
of auxiliaries provide information about 
perspective or the point of view from which a 
situation is being described.  Both classes of 
auxiliaries add a ni (dative) marked argument to 
the argument structure of the whole predicate.  
The classes differ in how they relate their 
arguments to the arguments of the verb.  One 
class (including kureru 'give'; see example 3) are 
treated as subject control verbs.  The other class 
(including morau 'receive', see example 4) 
establishes a control relation between the ni-
marked argument and the embedded subject. 
Example 2: 
Keeki wo tabe-te iru. 
cake ACC eat progressive 
'Someone is eating cake.' 
Example 3: 
Sensei wa watashi ni hon wo  
teacher TOP  I DAT book ACC  
katte kure-ta. 
buy give-past 
'The teacher bought me a book.' 
Example 4: 
Watashi  ga sensei ni hon wo 
I  NOM teacher DAT book ACC
katte morat-ta. 
buy get-past 
'The teacher bought me a book.' 
1.4 Particles in a type hierarchy 
The careful treatment of Japanese particles is 
essential, because they are the most frequently 
occurring words and have various central 
functions in the grammar. It is difficult, because 
one particle can fulfill more than one function 
and they can co-occur, but not arbitrarily. The 
Japanese grammar thus contains a type hierarchy 
of 44 types for particles. See Siegel (1999) for a 
more detailed description of relevant phenomena 
and solutions. 
1.5 Numeral Expressions 
Number names, such as sen kyuu hyaku juu 
'1910' constitute a notable exception to the 
general head-final pattern of Japanese phrases. 
We found Smith's (1999) head-medial analysis 
of English number names to be directly 
applicable to the Japanese system as well 
(Bender 2002).  This analysis was easily 
incorporated into the grammar, despite the 
oddity of head positioning, because the type 
hierarchy of HPSG is well suited to express the 
partial generalizations that permeate natural 
language. 
On the other hand, number names in 
Japanese contrast sharply with number names in 
English in that they are rarely used without a 
numeral classifier.  
Example 5:  
Juu *(hiki no) neko ga ki-ta. 
ten   CL GEN cat NOM arrive-past 
'Ten cats arrived.' 
The grammar provides for 'true' numeral 
classifiers like hon, ko, and hiki, as well as 
formatives like en 'yen' and do 'degree' which 
combine with number names just like numeral 
classifiers do, but never serve as numeral 
classifiers for other nouns.  In addition, there are 
a few non-branching rules that allow bare 
number names to surface as numeral classifier 
phrases with specific semantic constraints.  
1.6 Pragmatic information 
Spoken language and email correspondence both 
encode references to the social relation of the 
dialogue partners. Utterances can express social 
distance between addressee and speaker and 
third persons. Honorifics can even express 
respect towards inanimates. Pragmatic 
information is treated in the CONTEXT layer of 
the complex signs. Honorific information is 
given in the CONTEXT.BACKGROUND and 
linked to addressee and speaker anchors.  
The expression of empathy or in-group vs. 
out-group is quite prevalent in Japanese. One 
means of expressing empathy is the perspective 
auxiliaries discussed above. For example, two 
auxiliaries meaning roughly 'give' (ageru and 
kureru) contrast in where they place the 
empathy.  In the case of ageru, it is with the 
giver.  In the case of kureru, it is with the 
recipient. We model this within the sign by 
positing a feature EMPATHY within CONTEXT 
and linking it to the relevant arguments' indices. 
2 Japanese MRS Semantics  
In the multilingual context in which this 
grammar has been developed, a high premium is 
placed on parallel and consistent semantic 
representations between grammars for different 
languages.  Ensuring this parallelism enables the 
reuse of the same downstream technology, no 
matter which language is used as input.  
Integrating MRS representations parallel to 
those used in the ERG into the Japanese 
grammar took approximately 3 months.  Of 
course, semantic work is on-going, as every new 
construction treated needs to be given a suitable 
semantic representation.  For the most part, 
semantic representations developed for English 
were straightforwardly applicable to Japanese.  
This section provides a brief overview of those 
cases where the Japanese constructions we 
encountered led to innovations in the semantic 
representations and/or the correspondence 
between syntactic and semantic structures.  Due 
to space limitations, we discuss these analyses in 
general terms and omit technical details. 
2.l Nominalization and Verbal Nouns 
Nominalization is of course attested in English 
and across languages.  However, it is much more 
prevalent in Japanese than in English, primarily 
because of verbal nouns.  As noted in Section 
1.3 above, a verbal noun like benkyou 'study' can 
appear in syntactic contexts requiring nouns, or, 
in combination with a light verb, in contexts 
requiring verbs.  One possible analysis would 
provide two separate lexical entries, one with 
nominal and one with verbal semantics.  
However, this would not only be redundant 
(missing the systematic relationship between 
these uses of verbal nouns) but would also 
contradict the intuition that even in its nominal 
use, the arguments of benkyou are still present. 
Example 6: 
Nihongo no benkyou wo hajimeru. 
Japanese GEN study ACC begin 
'Someone begins the study of Japanese.' 
In order to capture this intuition, we opted for an 
analysis that essentially treats verbal nouns as 
underlyingly verbal.  The nominal uses are 
produced by a lexical rule which nominalizes the 
verbal nouns.  The semantic effect of this rule is 
to provide a nominal relation which introduces a 
variable which can in turn be bound by 
quantifiers. The nominal relation subordinates 
the original verbal relation supplied by the 
verbal noun.  The rule is lexical as we have not 
yet found any cases where the verb's arguments 
are clearly filled by phrases in the syntax.  If 
they do appear, it is with genitive marking (e.g., 
nihongo no in the example above).  In order to 
reduce ambiguity, we leave the relationship 
between these genitive marked NPs and the 
nominalized verbal noun underspecified.  There 
is nothing in the syntax to disambiguate these 
cases, and we find that they are better left to 
downstream processing, where there may be 
access to world knowledge. 
2.2 Numeral Classifiers 
As noted in Section1.5, the internal syntax of 
number names is surprisingly parallel between 
English and Japanese, but their external syntax 
differs dramatically. English number names can 
appear directly as modifiers of NPs and are 
treated semantically as adjectives in the ERG.  
Japanese number names can only modify nouns 
in combination with numeral classifiers. In 
addition, numeral classifier phrases can appear 
in NP positions (akin to partitives in English). 
Finally, some numeral-classifier-like elements 
do not serve the modifier function but can only 
head phrases that fill NP positions.   
This constellation of facts required the 
following innovations: a representation of 
numbers that doesn't treat them as adjectives (in 
MRS terms, a feature structure without the ARG 
feature), a representation of the semantic 
contribution of numeral classifiers (a relation 
between numbers and the nouns they modify, 
this time with an ARG feature), and a set of 
rules for promoting numeral classifier phrases to 
NPs that contribute the appropriate nominal 
semantics (underspecified in the case of ordinary 
numeral classifiers or specific in the case of 
words like en 'yen'). 
2.3 Relative Clauses and Adjectives 
The primary issue in the analysis of relative 
clauses and adjectives is the possibility of 
extreme ambiguity, due to several intersecting 
factors:  Japanese has rampant pro-drop and 
does not have any relative pronouns.  In 
addition, a head noun modified by a relative 
clause need not correspond to any gap in the 
relative clause, as shown by examples like the 
following (Matsumoto 1997): 
Example 7: 
atama ga yoku naru hon 
head NOM better become book 
'a book that makes one smarter' 
Therefore, if we were to posit an attributive 
adjective + noun construction (distinct from the 
relative clause + noun possibility) we would 
have systematic ambiguities for NPs like akai 
hon ('red book'), ambiguities which could never 
be resolved based on information in the 
sentence.  Instead, we have opted for a relative 
clause analysis of any adjective + noun 
combination in which the adjective could 
potentially be used predicatively.  Furthermore, 
because of gapless relative clauses like the one 
cited above, we have opted for a non-extraction 
analysis of relative clauses.2  
Nonetheless, the well-formedness constraints 
on MRS representations require that there be 
                                                     
2 There is in fact some linguistic evidence for 
extraction in some relative clauses in Japanese  (see 
e.g., Baldwin 2001).  However, we saw no practical 
need to allow for this possibility in our grammar, and 
particularly not one that would justify the increase in 
ambiguity. There is also evidence that some 
adjectives are true attributives and cannot be used 
predicatively (Yamakido 2000). These are handled by 
a separate adjective + noun rule restricted to just 
these cases. 
some relationship between the head noun and 
the relative clause.  We picked the topic relation 
for this purpose (following Kuno 1973).  The 
topic relation is introduced into the semantics by 
the relative clause rule.  As with main clause 
topics (which we also give a non-extraction 
analysis), we rely on downstream anaphora 
resolution to refine the relationship.   
2.4 Summary 
For the most part, semantic representations and 
the syntax-semantic interface already worked 
out in the ERG were directly applicable to the 
Japanese grammar.  In those cases where 
Japanese presented problems not yet 
encountered (or at least not yet tackled) in 
English, it was fairly straightforward to work out 
suitable MRS representations and means of 
building them up.  Both of these points illustrate 
the cross-linguistic validity and practical utility 
of MRS representations. 
3 Integration of a Morphological 
Analyzer 
As Japanese written text does not have word 
segmentation, a preprocessing system is 
required. We integrated ChaSen (Asahara & 
Matsumoto 2000), a tool that provides word 
segmentation as well as POS tags and 
morphological information such as verbal 
inflection. As the lexical coverage of ChaSen is 
higher than that of the HPSG lexicon, default 
part-of-speech entries are inserted into the 
lexicon. These are triggered by the part-of-
speech information given by ChaSen, if there is 
no existing entry in the lexicon. These specific 
default entries assign a type to the word that 
contains features typical to its part-of-speech. It 
is therefore possible to restrict the lexicon to 
those cases where the lexical information 
contains more than the typical information for a 
certain part-of-speech. This default mechanism 
is often used for different kinds of names and 
'ordinary' nouns, but also for adverbs, 
interjections and verbal nouns (where we 
assume a default transitive valence pattern).3 
                                                     
3 Kanayama et al (2000) use a similar mechanism for 
most words. They report only 105 grammar-inherent 
lexical entries.  
The ChaSen lexicon is extended with a domain-
specific lexicon, containing, among others, 
names in the domain of banking. 
For verbs and adjectives, ChaSen gives 
information about stems and inflection that is 
used in a similar way. The inflection type is 
translated to an HPSG type. These types interact 
with the inflectional rules in the grammar such 
that the default entries are inflected just as 
'known' words would be. 
In addition to the preprocessing done by 
ChaSen, an additional (shallow) preprocessing 
tool recognizes numbers, date expressions, 
addresses, email addresses, URLs, telephone 
numbers and currency expressions.  The output 
of the preprocessing tool replaces these 
expressions in the string with placeholders.  The 
placeholders are parsed by the grammar using 
special placeholder lexical entries. 
4 Robustness and Performance Issues 
The grammar is aimed at working with real-
world data, rather than at experimenting with 
linguistic examples. Therefore, robustness and 
performance issues play an important role. 
While grammar development is carried out in 
the LKB (Copestake 2002), processing (both in 
the application domain and for the purposes of 
running test suites) is done with the highly 
efficient PET parser (Callmeier 2000). Figures 1 
and 2 show the performance of PET parsing of 
hand-made and real data, respectively. 
Phenomenon  items 
# 
etasks 
? 
filter 
% 
edges 
? 
first 
? (s) 
total 
? (s) 
tcpu
? (s)
gc 
? (s)
space 
? (kb) 
Total 742 946 95.7 303 0.06 0.11 0.11 0 833 
Fig.1 Performance parsing banking data, generated 
by [incr tsdb()] 
 
Phenomenon items 
# 
etasks 
? 
filter 
% 
edges 
? 
first 
? (s) 
total 
? (s) 
tcpu 
? (s)
tgc 
? (s) 
space
? (kb)
Total 316 2020 96.5 616 0.23 0.26 0.26 0 1819
Fig.2 Performance parsing document request data, 
generated by [incr tsdb()]  
 One characteristic of real-world data is the 
variety of punctuation marks that occur and the 
potential for ambiguity that they bring. In our 
grammar, certain punctuation marks are given 
lexical entries and processed by grammar rules. 
Take, for example, quotation marks. Ignoring 
them (as done in most development-oriented 
grammars and smaller grammars), leads to a 
significant loss of structural information: 
Example 8: 
"Botan wo osu" to it-ta 
button ACC push COMPL say-past 
'Someone said: ?push the button. "?  
The formative to is actually ambiguous between 
a complementizer and a conjunction.  Since the 
phrase before to is a complete sentence, this 
string is ambiguous if one ignores the quotation 
marks.  With the quotation marks, however, only 
the complementizer to is possible.  Given the 
high degree of ambiguity inherent in broad-
coverage grammars, we have found it extremely 
useful to parse punctuation rather than ignore it. 
The domains we have been working on (like 
many others) contain many date and number 
expressions. While a shallow tool recognizes 
general structures, the grammar contains rules 
and types to process these.  
Phenomena occurring in semi-spontaneous 
language (email correspondence), such as 
interjections (e.g. maa 'well'), contracted verb 
forms (e.g. tabe-chatta < tabete-shimatta 
'(someone) ate it all up'), fragmentary sentences 
(e.g. bangou: 1265 'number: 1265') and NP 
fragments (e.g. bangou? 'number?') must be 
covered as well as the 'ordinary' complete 
sentences found in more carefully edited text.  
Our grammar includes types, lexical entries, and 
grammar rules for dealing with such phenomena. 
Perhaps the most important performance 
issue for broad coverage grammars is ambiguity. 
At one point in the development of this 
grammar, the average number of readings 
doubled in two months of work. We currently 
have two strategies for addressing this problem: 
First, we include a mechanism into the grammar 
rules that chooses left-branching rules in cases 
of compounds, genitive modification and 
conjuncts, as we don?t have enough lexical-
semantic information represented to choose the 
right dependencies in these cases.4 Secondly, we 
use a mechanism for hand-coding reading 
preferences among rules and lexical entries. 
                                                     
4Consider, for example, genitive modification: The 
semantic relationship between modifier and modifiee 
is dependent on their semantic properties: toukyou no 
kaigi - 'the meeting in Tokyo', watashi no hon - 'my 
book'. More lexical-semantic information is needed  
to choose the correct parse in more complex 
structures, such as in watashi no toukyou no imooto ? 
?My sister in Tokyo?. 
Restrictions like head-complement preferred to 
head-adjunct are quite obvious. Others require 
domain-specific mechanisms that shall be 
subject of further work.  Stochastic 
disambiguation methods being developed for the 
ERG by the Redwoods project at Stanford 
University (Oepen et al 2002) should be 
applicable to this grammar as well.  
5 Evaluation 
The grammar currently covers 93.4% of 
constructed examples for the banking domain 
(747 sentences) and 78.2% of realistic email 
correspondence data (316 sentences), concerning 
requests for documents. During three months of 
work, the coverage in the banking domain 
increased 48.49%. The coverage of the 
document request data increased 51.43% in the 
following two weeks. 
Phenomenon total 
items 
# 
positive
items 
# 
word
string
% 
lexical 
items 
? 
parser 
analyses 
   ? 
total 
results 
# 
overall 
coverage 
% 
Total 747 747 101 75.24 6.54 698 93.4 
Fig.3 Coverage of banking data, generated by 
 [incr tsdb()] 
Phenomenon total 
items 
# 
positive
items 
# 
word
string
% 
lexical 
items 
? 
parser 
analyses 
   ? 
total 
results 
# 
overall 
coverage 
% 
Total 316 316 1.00 83.90 39.91 247 78.2 
Fig.4 Coverage of document request data, generated 
by [incr tsdb()] 
We applied the grammar to unseen data in one 
of the covered domains, namely the FAQ site of 
a Japanese bank. The coverage was 61%. 91.2% 
of the parses output were associated with all 
well-formed MRSs. That means that we could 
get correct MRSs in 55.61% of all sentences.  
Conclusion 
We described a broad coverage Japanese 
grammar, based on HPSG theory. It encodes 
syntactic, semantic, and pragmatic information. 
The grammar system is connected to a 
morphological analysis system and uses default 
entries for words unknown to the HPSG lexicon.  
Some basic constructions of the Japanese 
grammar were described. As the grammar is 
aimed at working in applications with real-world 
data, performance and robustness issues are 
important.  
The grammar is being developed in a 
multilingual context, where much value is 
placed on parallel and consistent semantic 
representations.  The development of this 
grammar constitutes an important test of the 
cross-linguistic validity of the MRS formalism. 
The evaluation shows that the grammar is at 
a stage where domain adaptation is possible in a 
reasonable amount of time. Thus, it is a 
powerful resource for linguistic applications for 
Japanese. 
In future work, this grammar could be further 
adapted to another domain, such as the EDR 
newspaper corpus (including a headline 
grammar).  As each new domain is approached, 
we anticipate that the adaptation will become 
easier as resources from earlier domains are 
reused.  Initial evaluation of the grammar on 
new domains and the growth curve of grammar 
coverage should bear this out.  
References  
Asahara, Masayuki and Yuji Matsumoto (2000). 
Extended Models and Tools for High-performance 
Part-of-speech Tagger. In Proceedings of the 18th 
International Conference on Computational 
Linguistics, Coling 2000, 21-27. Saarbr?cken, 
Germany. 
Baldwin, Timothy (2001). Making Lexical Sense of 
Japanese-English Machine Translation: A 
Disambiguation Extravaganza. PhD thesis, Tokyo 
Institute of Technology. 
Bender, Emily M. (2002). Number Names in 
Japanese: A Head-Medial Construction in a Head-
Final Language.  Paper presented at the 76th 
annual meeting of the LSA, San Francisco. 
Callmeier, Ulrich (2000). PET ? a platform for 
experimentation with efficient HPSG processing 
techniques. Journal of Natural Language 
Engineering, Special Issue on Efficient Processing 
with HPSG: Methods, Systems, Evaluation, pages 
99-108. 
Copestake, Ann (2002). Implementing Typed 
Feature-Structure Grammars. Stanford: CSLI. 
Copestake, Ann, Alex Lascarides, and Dan Flickinger 
(2001). An Algebra for Semantic Construction in 
Constraint-based Grammars. Proceedings of the 
39th Annual Meeting of the Association for 
Computational Linguistics (ACL 2001), Toulouse, 
France. 
Flickinger, Dan (2000).  On Building a More 
Efficient Grammar by Exploiting Types.  Natural 
Language Engineering 6(1) (Special Issue on 
Efficient Processing with HPSG), pages 15-28. 
Kanayama, Hiroshi, Kentaro Torisawa, Yutaka 
Mitsuishi and Jun?ichi Tsujii (2000). A Hybrid 
Japanese Parser with Hand-crafted Grammar and 
Statistics. In Proceedings of the 18th International 
Conference on Computational Linguistics, Coling 
2000. Saarbr?cken, Germany. 
Kuno, Susumu (1973). The Structure of the Japanese 
Language. Cambridge, MA: The MIT Press. 
Matsumoto, Yoshiko (1997). Noun-Modifying 
Constructions in Japanese: A Frame Semantic 
Approach. John Benjamins.  
Oepen, Stephan and John Carroll (2000). 
Performance Profiling for Parser Engineering. 
Journal of Natural Language Engineering, Special 
Issue on Efficient Processing with HPSG: Methods, 
Systems, Evaluation, pages 81-97. 
Oepen, Stephan, Kristina Toutanova, Stuart Shieber, 
Chris Manning, Dan Flickinger and Thorsten 
Brants (2002). The LinGO Redwoods Treebank. 
Motivation and Preliminary Applications. In 
Proceedings of the 19th International Conference 
on Computational Linguistics, Coling 2002. Tapei, 
Taiwan. . 
Pollard, Carl and Ivan A. Sag (1994). Head-Driven 
Phrase Structure Grammar. University of Chicago 
Press. 
Siegel, Melanie (1999). The Syntactic Processing of 
Particles in Japanese Spoken Language. In: Wang, 
Jhing-Fa and Wu, Chung-Hsien (eds.): 
Proceedings of the 13th Pacific Asia Conference 
on Language, Information and Computation, 
Taipei 1999.  
Siegel, Melanie (2000). HPSG Analysis of Japanese. 
In: W. Wahlster (ed.): Verbmobil: Foundations of 
Speech-to-Speech Translation. Springer Verlag.  
Smith, Jeffrey D. (1999). English number names in 
HPSG. In Gert Webelhuth, Andreas Kathol, and 
Jean-Pierre Koenig (eds.), Lexical and 
Constructional Aspects of Linguistic Explanation. 
Stanford: CSLI. 145-160. 
Yamakido, Hiroko (2000). Japanese attributive 
adjectives are not (all) relative clauses.  In Roger 
Billerey and Brook Danielle Lillehaugen (eds.), 
WCCFL 19: Proceedings of the 19th West Coast 
Conference on Formal Linguistics. Somerville, 
MA: Cascadilla Press.  588-602. 
 
Parallel Distributed Grammar Engineering for Practical Applications
Stephan Oepen?, Emily M. Bender?, Uli Callmeier?, Dan Flickinger??, Melanie Siegel?
?CSLI Stanford ?YY Technologies ?DFKI GmbH
Stanford (CA) Mountain View (CA) Saarbru?cken (Germany)
?
?
?
oe
bender
dan
?
?
?
@csli.stanford.edu
{
uc
dan
}
@yy.com siegel@dfki.de
Abstract
Based on a detailed case study of paral-
lel grammar development distributed across
two sites, we review some of the require-
ments for regression testing in grammar en-
gineering, summarize our approach to sys-
tematic competence and performance profil-
ing, and discuss our experience with gram-
mar development for a commercial applica-
tion. If possible, the workshop presentation
will be organized around a software demon-
stration.
1 Background
The production of large-scale constraint-based
grammars and suitable processing environments is
a labour- and time-intensive process that, maybe,
has become somewhat of a growth industry over
the past few years, as companies explore products
that incorporate grammar-based language process-
ing. Many broad-coverage grammars have been
developed over several years, sometimes decades,
typically coordinated by a single grammarian who
would often draw on additional contributors (e.g.
the three HPSG implementations developed as part
of the VerbMobil effort, see Flickinger, Copes-
take, & Sag, 2000, Mu?ller & Kasper, 2000, and
Siegel, 2000; or the LFG implementations devel-
oped within the ParGram consortium, Butt, King,
Nin?o, & Segond, 1999).
More recently, we also find genuinely shared
and distributed development of broad-coverage
grammars, and we will use one such initiative as an
example?viz. an open-source HPSG implementa-
tion for Japanese jointly developed between DFKI
Saarbru?cken (Germany) and YY Technologies
(Mountain View, CA)?to demonstrate the techno-
logical and methodological challenges present in
distributed grammar and system engineering.
2 Parallel Distributed Grammar
Development?A Case Study
The Japanese grammar builds on earlier work per-
formed jointly between DFKI and the Computa-
tional Linguistics Department at Saarland Univer-
sity (Germany) within VerbMobil; much like for
the German VerbMobil grammar, two people were
contributing to the grammar in parallel, one build-
ing out syntactic analyses, the other charged with
integrating semantic composition into the syntax.
This relatively strict separation of responsibilities
mostly enabled grammarians to serialize incre-
mental development of the resource: the syntacti-
cian would supply a grammar with extended cov-
erage to the semanticist and, at the onset of the fol-
lowing iteration, start subsequent work on syntax
from the revised grammar.
In the DFKI ? YY cooperation the situation was
quite different. Over a period of eight months,
both partners had a grammarian working on syn-
tax and semantics simultaneously on a day-to-
day basis; both grammarians were submitting
changes to a joint, version-controlled source repos-
itory and usually would start the work day by re-
trieving the most recent revisions. At the same
time, product building and the development of
so-called ?domain libraries? (structured collections
of knowledge about a specific domain that is in-
stantiated from semantic representations delivered
from grammatical analysis) at YY already incorpo-
rated the grammar and depended on it for actual,
customer-specific contracts. Due to a continuous
demand for improvements in coverage and analy-
sis accuracy, the grammar used in the main product
line would be updated from the current develop-
ment version about once or twice a week. Parallel
to work on the Japanese grammar (and simultane-
ous work on grammars for English and Spanish),
both the grammar development environment (the
open-source LKB system; Copestake, 2002) and
the HPSG run-time component powering the YY
linguistic analysis engine (the open-source PET
parser; Callmeier, 2002) continued to evolve, as
did the YY-proprietary mapping of meaning repre-
sentations extracted from the HPSG grammars into
domain knowledge?all central parts of a complex
system of interacting components and constraints.
As has been argued before (see, for exam-
ple, Oepen & Flickinger, 1998), the nature of a
large-scale constraint-based grammar and the sub-
tle interactions of lexical and constructional con-
straints make it virtually impossible to predict how
a change in one part of the grammar affects over-
all system behaviour. A relatively minor repair in
one lexical class, numeral adjectives as in ?three
books were ordered? for instance, will have the po-
tential of breaking the interaction of that class with
the construction deriving named (numeric) entities
from a numeral (e.g. as in ?three is my favourite
number?) or the partitive construction (e.g. as in
?three have arrived already?). A ripple effect of
a single change can thus corrupt the semantics
produced for any of these cases and in the con-
sequence cause failure or incorrect behaviour in
the back-end system. In addition to these qual-
ity assurance requirements on grammatical cover-
age and correctness, the YY application (like most
applications for grammar-based linguistic analy-
sis) utilizes a set of hand-constructed parse rank-
ing heuristics that enables the parser to operate
in best-first search mode and to return only one
reading, i.e. the analysis that is ranked best by the
heuristic component. The parse ranking machin-
ery builds on preferences that are associated with
individual or classes of lexical items and construc-
tions. The set of preferences is maintained in par-
allel to the grammar, in a sense providing a layer
of performance-oriented annotations over the basic
building blocks of the core competence grammar.
Without discussing the details of the parse ranking
approach, it creates an additional element of un-
certainty in assessing grammar changes: since the
preference for a specific analysis results implic-
itly from a series of local preferences (of lexical
items and constructions contributing to the com-
plete derivation), introducing additional elements
(i.e. new local or global ambiguity) into the search
space and subjecting them to the partial ordering
can quickly skew the overall result.
Summing up, the grammar and application engi-
neering example presented here illustrates a num-
ber of highly typical requirements on the engi-
neering environment. First, all grammarians and
system engineers participating in the development
process need to keep frequent, detailed, and accu-
rate records of a large number of relevant parame-
ters, including but not limited to grammatical cov-
erage, correctness of syntactic analyses and cor-
responding semantic forms, parse selection accu-
racy, and overall system performance. Second, as
modifications to the system as a whole are made
daily?and sometimes several times each day?all
developers must be able to assess the impact of
recent changes and track their effects on all rele-
vant parameters; gathering the data and analyzing
it must be simple, fast, and automated as much as
possible. Third, not all modifications (to the gram-
mar or underlying software) will result in ?mono-
tonic? or backwards-compatible effects. A change
in the treatment of optional nominal complements,
for example, may affect virtually all derivation
trees and render a comparison of results at this
level uninformative. At the same time, a primarily
syntactic change of this nature will not cause an ef-
fect in associated meaning representations, so that
a semantic equivalence test over analyses should
be expected to yield an exact match to earlier re-
sults. Hence, the machinery for representation and
comparison of relevant parameters needs to facil-
itate user-level specification of informative tests
and evolution criteria. Finally, the metrics used in
tracking grammar development cannot be isolated
from measurements of system resource consump-
tion and overall performance (specific properties
of a grammar may trigger idiosyncrasies or soft-
ware bugs in a particular version of the process-
ing system); therefore, and to enable exchange of
reference points and comparability of experiments,
grammarians and system developers alike should
use the same, homogenuous set of relevant param-
eters.
3 Integrated Competence and
Performance Profiling
The integrated competence and performance pro-
filing methodology and associated engineering
platform, dubbed [incr tsdb()] (Oepen & Callmeier,
2000)1 and reviewed in the remainder of this sec-
1See ?http://www.coli.uni-sb.de/itsdb/?
for the (draft) [incr tsdb()] user manual, pronunciation rules,
and instructions on obtaining and installing the package.
tion, was designed to meet al of the requirements
identified in the DFKI ? YY case study. Generally
speaking, the [incr tsdb()] environment is an in-
tegrated package for diagnostics, evaluation, and
benchmarking in practical grammar and system
engineering. The toolkit implements an approach
to grammar development and system optimization
that builds on precise empirical data and system-
atic experimentation, as it has been advocated by,
among others, Erbach & Uszkoreit (1990), Erbach
(1991), and Carroll (1994). [incr tsdb()] has been
integrated with, as of June 2002, nine different
constraint-based grammar development and pars-
ing systems (including both environments in use at
YY, i.e. the LKB and PET), thus providing a pre-
standard reference point for a relatively large (and
growing) community of NLP developers. The [incr
tsdb()] environment builds on the following com-
ponents and modules:
? test and reference data stored with annota-
tions in a structured database; annotations
can range from minimal information (unique
test item identifier, item origin, length et al)
to fine-grained linguistic classifications (e.g.
regarding grammaticality and linguistic phe-
nomena presented in an item), as they are rep-
resented in the TSNLP test suites, for example
(Oepen, Netter, & Klein, 1997);
? tools to browse the available data, identify
suitable subsets and feed them through the
analysis component of processing systems
like the LKB and PET, LiLFeS (Makino,
Yoshida, Torisawa, & Tsujii, 1998), TRALE
(Penn, 2000), PAGE (Uszkoreit et al, 1994),
and others;
? the ability to gather a multitude of precise and
fine-grained (grammar) competence and (sys-
tem) performance measures?like the num-
ber of readings obtained per test item, various
time and memory usage statistics, ambigu-
ity and non-determinism metrics, and salient
properties of the result structures?and store
them in a uniform, platform-independent data
format as a competence and performance pro-
file; and
? graphical facilities to inspect the resulting
profiles, analyze system competence (i.e.
grammatical coverage and overgeneration)
and performance (e.g. cpu time and memory
usage, parser search space, constraint solver
'
&
$
%
Parser 3Parser 2Parser 1Grammar 3Grammar 2Grammar 1
TestSet 3TestSet 2TestSet 1
ParallelVirtualMachine
C and Lisp APIRelationalDBMS Batch ControlStatistics UserInterface
ANSI C Common-Lisp Tcl/Tk
Figure 1: Rough sketch of [incr tsdb()] architec-
ture: the core engine comprises the database man-
agement, batch control and statistics component,
and the user interface.
workload, and others) at variable granulari-
ties, aggregate, correlate, and visualize the
data, and compare among profiles obtained
from previous grammar or system versions or
other processing environments.
As it is depicted in Figure 1, the [incr tsdb()]
architecture can be broken down into three major
parts: (i) the underlying database management sys-
tem (DBMS), (ii) the batch control and statistics
kernel (providing a C and Lisp application pro-
gram interface to client systems that can be dis-
tributed across the network), and (iii) the graphi-
cal user interface (GUI). Although, historically, the
DBMS was developed independently and the ker-
nel can be operated without the GUI, the full func-
tionality of the integrated competence and perfor-
mance laboratory?as demonstrated below?only
emerges from the combination of all three com-
ponents. Likewise, the flexibility of a clearly de-
fined API to client systems and its ability to par-
allelize batch processing and distribute test runs
across the network have greatly contributed to the
success of the package. The following paragraphs
review some of the fundamental aspects in more
detail, sketch essential functionality, and comment
on how they have been exploited in the DFKI ? YY
cooperation.
Abstraction over Processors The [incr tsdb()]
environment, by virtue of its generalized pro-
file format, abstracts over specific processing en-
vironments. While grammar engineers in the
DFKI ? YY collaboration regularly use both the
LKB (primarily for interactive development) and
PET (mostly for batch testing and the assessment
of results obtained in the YY production envi-
ronment), usage of the [incr tsdb()] profile anal-
ysis routines in most aspects hides the specifics
of the token processor used in obtaining a profile.
Both platforms interprete the same typed feature
structure formalism, load the same set of gram-
mar source files, and (unless malfunctioning) pro-
duce equivalent results. Using [incr tsdb()], gram-
marians can obtain summary views of grammati-
cal coverage and overgeneration, inspect relevant
subsets of the available data, break down analysis
views according to various aggregation schemes,
and zoom in on specific aggregates or individual
test items as appropriate. Moreover, processing
results obtained from the (far more efficient) PET
parser (that has no visualization or debugging sup-
port built in), once recorded as an [incr tsdb()] pro-
file, can be used in conjunction with the LKB (con-
tingent on the use of identical grammars), thereby
facilitating graphical inspection of parse trees and
semantic formulae.
Parallelization of Test Runs The [incr tsdb()] ar-
chitecture (see Figure 1) separates the batch con-
trol and statistics kernel from what is referred to
as client processors (i.e. parsing systems like the
LKB or PET) through an application program inter-
face (API) and the Parallel Virtual Machine (PVM;
Geist, Bequelin, Dongarra, Manchek, & Sun-
deram, 1994) message-passing protocol layer. The
use of PVM?in connection with task scheduling,
error recovery, and roll-over facilities in the [incr
tsdb()] kernel?enables developers to transparently
parallelize and distribute execution of batch pro-
cessing. At YY, grammarians had a cluster of net-
worked Linux compute servers configured as a sin-
gle PVM instance, so that execution of a test run?
using the efficient PET run-time engine?could be
completed as a matter of a few seconds. The com-
bination of near-instantaneous profile creation and
[incr tsdb()] facilities for quick, semi-automated as-
sessment of relevant changes (see below) enabled
developers to pursue a strongly empiricist style of
grammar engineering, assessing changes and their
effects on actual system behavior in small incre-
ments (often many times per hour).
Structured Comparison One of the facilities
that has proven particularly useful in the dis-
tributed grammar engineering setup outlined in
Section 2 above is the flexible comparison of com-
petence and performance profiles. The [incr tsdb()]
package eases comparison of results on a per-
item basis, using an approach similar to Un?x
diff(1), but generalized for structured data sets.
By selection of a set of parameters for intersec-
tion (and optionally a comparison predicate), the
user interface allows browsing the subset of test
items (and associated results) that fail to match
in the selected properties. One dimension that
grammarians found especially useful in intersect-
ing profiles is on the number of readings assigned
per item?detecting where coverage was lost or
added?and on derivation trees (bracketed struc-
tures labeled with rule names and identifiers of lex-
ical items) associated with each parser analysis?
assessing where analyses have changed. Addition-
ally, using a user-supplied equivalence predicate,
the same technique was regularly used at YY to
track the evolution of meaning representations (as
they form the interface from linguistic analysis into
the back-end knowledge processing engine), both
for all readings and the analysis ranked best by the
parse selection heuristics.
Zooming and Interactive Debugging In
analysing a new competence and performance
profile, grammarians typically start from summary
views (overall grammatical coverage, say), then
single out relevant (or suspicious) subsets of
profile data, and often end up zooming in to
the level of individual test items. For most [incr
tsdb()] analysis views the ?success? criteria can be
varied according to user decisions: in assessing
grammatical coverage, for example, the scoring
function can refer to virtually arbitrary profile
elements?ranging from the most basic coverage
measure (assigning at least one reading) to more
refined or application-specific metrics, the produc-
tion of a well-formed meaning representation, say.
Although the general approach allows output an-
notations on the test data (full or partial constituent
structure descriptions, for example), developers so
far have found the incremental, semi-automated
comparison against earlier results a more adequate
means of regression testing. It would appear
that, especially in an application-driven and
tightly scheduled engineering situation like the
DFKI ? YY partnership, the pace of evolution
and general lack of locality in changes (see the
examples discussed in Section 2) precludes the
construction of a static, ?gold-standard? target for
comparison. Instead, the structured comparison
facilities of [incr tsdb()] enable developers to
incrementally approximate target results and, even
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
40
45
50
55
60
65
70
75
80
85
90
95
Grammatical Coverage (Per Cent)
(generated by [incr tsdb()] at 29-jun-2002 (20:49 h))
?
? ???
? ??
?
???
?
?
?
?
?
?
?
?
?
???
??
?
??
?
?
? ?? ??
?
?
?
??
?? ? ?
?
?
?
?
?
?
?
? ???
?
?
????
??
?? ????
?? ?banking?
?? ?trading?
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
0
10
20
30
40
50
60
70
80
90
Ambiguity (Average Number of Analyses)
(generated by [incr tsdb()] at 29-jun-2002 (20:59 h))
?
?
? ? ?
?
?? ?? ? ???? ?? ? ?
?
?
?
?
?
? ?
?
??
??? ?
????
?? ?banking?
?? ?trading?
Figure 2: Evolution of grammatical coverage and average ambiguity (number of readings per test item) over
a five-month period; ?banking? and ?trading? are two data sets (of some 700 and 400 sentences, respectively)
of domain data.
in a highly dynamic environment where grammar
and processing environment evolve in parallel,
track changes and identify regression with great
confidence.
4 Looking Back?Quantifying Evolution
Over time, the [incr tsdb()] profile storage accu-
mulates precise data on the grammar development
process. Figure 2 summarizes two aspects of
grammatical evolution compiled over a five-month
period (and representing some 130 profiles that
grammarians put aside for future reference): gram-
matical coverage over two representative samples
of customer data?one for an on-line banking ap-
plication, the other from an electronic stock trad-
ing domain?is contrasted with the development
of global ambiguity (i.e. the average number of
analyses assinged to each test item). As should
be expected, grammatical coverage on both data
sets increases significantly as grammar develop-
ment focuses on these domains (?banking? for the
first three months, ?trading? from there on). While
the collection of available profiles, apparently, in-
cludes a number of data points corresponding to
?failed? experiments (fairly dramatic losses in cov-
erage), the larger picture shows mostly monotonic
improvement in coverage. As a control experi-
ment, the coverage graph includes another data
point for the ?banking? data towards the end of the
reporting period. Two months of focussed devel-
opment on the ?trading? domain have not nega-
tively affected grammatical coverage on the data
set used earlier. Corresponding to the (desirable)
increase in coverage, the graph on the right of Fig-
ure 2 depicts the evolution of grammatical ambi-
guity. As hand-built linguistic grammars put great
emphasis on the precision of grammatical analy-
sis and the exclusion of ungrammatical input, the
overall average of readings assigned to each sen-
tence varies around relatively small numbers. For
the moderately complex email data2 the grammar
often assigns less than ten analyses, rarely more
than a few dozens. However, not surprisingly
the addition of grammatical coverage comes with
a sharp increase in ambiguity (which may indi-
cate overgeneration): the graphs in Figure 2 clearly
show that, once coverage on the ?trading? data was
above eighty per cent, grammarians shifted their
engineering focus on ?tightening? the grammar, i.e.
the elimination of spurious ambiguity and overgen-
eration (see Siegel & Bender, 2002, for details on
the grammar).
Another view on grammar evolution is pre-
sented in Figure 3, depicting the ?size? of the
Japanese grammar over the same five-month de-
velopment cycle. Although measuring the size of
2Quantifying input complexity for Japanese is a non-
trivial task, as the count of the number of input words would
depend on the approach to string segmentation used in a spe-
cific system (the fairly aggressive tokenizer of ChaSen, Asa-
hara & Matsumoto, 2000, in our case); to avoid potential for
confusion, we report input complexity in the (overtly system-
specific) number of lexical items stipulated by the grammar
instead: around 50 and 80, on average, for the ?banking? and
?trading? data sets, respectively (as of February 2002).
12-sep-2001 (13:24 h) ? 14-feb-2002 (17:14 h)
8800
9000
9200
9400
9600
9800
10000
10200
88
90
92
94
96
98
100
102
104
106
Grammar Size
(generated by [incr tsdb()] at 30-jun-2002 (16:09 h))????
???
???
????
??
?????
?
?
?
?
??
?
?
? ???????
?????? ?
??























 





 

?? types
? rules
Figure 3: Evolution of grammar size (in the num-
bers of types, plotted against the left axis, and
grammar rules, plotted against the right axis) over
a five-month period.
computational grammars is a difficult challenge,
for the HPSG framework two metrics suggest them-
selves: the number of types (i.e. the size of the
grammatical ontology) and the number of gram-
mar rules (i.e. the inventory of construction types).
As would be expected, both numbers increase
more or less monotonically over the reporting pe-
riod, where the shift of focus from the ?banking?
into the ?trading? domain is marked with a sharp
increase in (primarily lexical) types. Contrasted
to the significant gains in grammatical coverage
(a relative improvement of more than seventy per
cent on the ?banking? data), the increase in gram-
mar size is moderate, though: around fifteen and
twenty per cent in the number of types and rules,
respectively.
5 Conclusions
At YY and cooperating partners (primarily DFKI
Saarbru?cken and CSLI Stanford), grammarians
(for all languages) as well as developers of both the
grammar development tools and of the production
system all used the competence and performance
profiling environment as part of their daily engi-
neering toolbox. The combination of [incr tsdb()]
facilities to parallelize test run processing and a
break-through in client system efficiency (using
the PET parser; Callmeier, 2002) has created an ex-
perimental development environment where gram-
marians can obtain near-instantaneous feedback on
the effects of changes they explore.
For the Japanese grammar specifically, the
grammar developers at both ends would typically
spend the first ten to twenty minutes of the day ob-
taining fresh profiles for a number of shared test
sets and diagnostic corpora, thereby assessing the
most recent set of changes through empirical anal-
ysis of their effects. In conjunction with a certain
rigor in documentation and communication, it was
the ability of both partners to regularly, quickly,
and semi-automatically monitor the evolution of
the joint resource with great confidence that has
enabled truly parallel development of a single,
shared HPSG grammar across continents. Within
a relatively short time, the partners succeeded
in adapting an existing grammar to a new genre
(email rather than spoken language) and domain
(customer service requests rather than appointment
scheduling), greatly extending grammatical cov-
erage (from initially around forty to above ninety
per cent on representative customer corpora), and
incorporating the grammar-based analysis engine
into a commercial product. And even though in
February 2002, for business reasons, YY decided
to reorganize grammar development for Japanese,
the distributed, parallel grammar development ef-
fort positively demonstrates that methodological
and technological advances in constraint-based
grammar engineering have enabled commercial
development and deployment of broad-coverage
HPSG implementations, a paradigm that until re-
cently was often believed to still lack the maturity
for real-world applications.
Acknowledgements
The DFKI ? YY partnership involved a large group
of people at both sites. We would like to thank
Kirk Oatman, co-founder of YY and first CEO,
and Hans Uszkoreit, Scientific Director at DFKI,
for their initiative and whole-hearted support to
the project; it takes vision for both corporate and
academic types to jointly develop an open-source
resource. Atsuko Shimada (from Saarland Uni-
versity), as part of a two-month internship at YY,
has greatly contributed to the preparation of repre-
sentative data samples, development of robust pre-
processing rules, and extensions to lexical cover-
age. Our colleague and friend Asahara-san (of the
Nara Advanced Institute of Technology, Japan),
co-developer of the open-source ChaSen tokenizer
and morphological analyzer for Japanese, was in-
strumental in the integration of ChaSen into the
YY product and also helped a lot in adapting and
(sometimes) fixing tokenization and morphology.
References
Asahara, M., & Matsumoto, Y. (2000). Extended
models and tools for high-performance part-of-
speech tagger. In Proceedings of the 18th In-
ternational Conference on Computational Lin-
guistics (pp. 21 ? 27). Saarbru?cken, Germany.
Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.
(1999). A grammar writer?s cookbook. Stan-
ford, CA: CSLI Publications.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, & H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in ef-
ficient grammar-based processing. Stanford,
CA: CSLI Publications. (forthcoming)
Carroll, J. (1994). Relating complexity to practi-
cal performance in parsing with wide-coverage
unification grammars. In Proceedings of the
32nd Meeting of the Association for Computa-
tional Linguistics (pp. 287 ? 294). Las Cruces,
NM.
Copestake, A. (2002). Implementing typed fea-
ture structure grammars. Stanford, CA: CSLI
Publications.
Erbach, G. (1991). An environment for exper-
imenting with parsing strategies. In J. My-
lopoulos & R. Reiter (Eds.), Proceedings of
IJCAI 1991 (pp. 931 ? 937). San Mateo, CA:
Morgan Kaufmann Publishers.
Erbach, G., & Uszkoreit, H. (1990). Grammar
engineering. Problems and prospects (CLAUS
Report # 1). Saarbru?cken, Germany: Compu-
tational Linguistics, Saarland University.
Flickinger, D., Copestake, A., & Sag, I. A. (2000).
HPSG analysis of English. In W. Wahlster
(Ed.), Verbmobil. Foundations of speech-to-
speech translation (Artificial Intelligence ed.,
pp. 321 ? 330). Berlin, Germany: Springer.
Geist, A., Bequelin, A., Dongarra, J., Manchek, W.
J. R., & Sunderam, V. (Eds.). (1994). PVM ?
parallel virtual machine. A users? guide and tu-
torial for networked parallel computing. Cam-
bridge, MA: The MIT Press.
Makino, T., Yoshida, M., Torisawa, K., & Tsu-
jii, J. (1998). LiLFeS ? towards a practical
HPSG parser. In Proceedings of the 17th In-
ternational Conference on Computational Lin-
guistics and the 36th Annual Meeting of the
Association for Computational Linguistics (pp.
807 ? 11). Montreal, Canada.
Mu?ller, S., & Kasper, W. (2000). HPSG analy-
sis of German. In W. Wahlster (Ed.), Verbmo-
bil. Foundations of speech-to-speech transla-
tion (Artificial Intelligence ed., pp. 238 ? 253).
Berlin, Germany: Springer.
Oepen, S., & Callmeier, U. (2000). Measure for
measure: Parser cross-fertilization. Towards
increased component comparability and ex-
change. In Proceedings of the 6th International
Workshop on Parsing Technologies (pp. 183 ?
194). Trento, Italy.
Oepen, S., & Flickinger, D. P. (1998). Towards
systematic grammar profiling. Test suite tech-
nology ten years after. Journal of Computer
Speech and Language, 12 (4) (Special Issue on
Evaluation), 411 ? 436.
Oepen, S., Netter, K., & Klein, J. (1997). TSNLP
? Test Suites for Natural Language Process-
ing. In J. Nerbonne (Ed.), Linguistic Databases
(pp. 13 ? 36). Stanford, CA: CSLI Publica-
tions.
Penn, G. (2000). Applying constraint handling
rules to HPSG. In Proceedings of the first in-
ternational conference on computational logic
(pp. 51 ? 68). London, UK.
Siegel, M. (2000). HPSG analysis of Japanese. In
W. Wahlster (Ed.), Verbmobil. Foundations of
speech-to-speech translation (Artificial Intelli-
gence ed., pp. 265 ? 280). Berlin, Germany:
Springer.
Siegel, M., & Bender, E. M. (2002). Efficient
deep processing of japanese. In Proceedings
of the 19th International Conference on Com-
putational Linguistics. Taipei, Taiwan.
Uszkoreit, H., Backofen, R., Busemann, S., Di-
agne, A. K., Hinkelman, E. A., Kasper, W.,
Kiefer, B., Krieger, H.-U., Netter, K., Neu-
mann, G., Oepen, S., & Spackman, S. P.
(1994). DISCO ? an HPSG-based NLP
system and its application for appointment
scheduling. In Proceedings of the 15th Interna-
tional Conference on Computational Linguis-
tics. Kyoto, Japan.
