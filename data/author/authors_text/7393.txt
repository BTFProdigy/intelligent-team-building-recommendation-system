Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 145?148,
New York, June 2006. c?2006 Association for Computational Linguistics
Selecting relevant text subsets from web-data for building topic specic
language models
Abhinav Sethy, Panayiotis G. Georgiou, Shrikanth Narayanan
Speech Analysis and Interpretation Lab
Integrated Media Systems Center
Viterbi School of Engineering
Department of Electrical Engineering-Systems
University of Southern California
Abstract
In this paper we present a scheme to se-
lect relevant subsets of sentences from a
large generic corpus such as text acquired
from the web. A relative entropy (R.E)
based criterion is used to incrementally se-
lect sentences whose distribution matches
the domain of interest. Experimental re-
sults show that by using the proposed sub-
set selection scheme we can get signif-
icant performance improvement in both
Word Error Rate (WER) and Perplexity
(PPL) over the models built from the en-
tire web-corpus by using just 10% of the
data. In addition incremental data selec-
tion enables us to achieve significant re-
duction in the vocabulary size as well as
number of n-grams in the adapted lan-
guage model. To demonstrate the gains
from our method we provide a compar-
ative analysis with a number of methods
proposed in recent language modeling lit-
erature for cleaning up text.
1 Introduction
One of the main challenges in the rapid deployment
of NLP applications is the lack of in-domain data
required for training statistical models. Language
models, especially n-gram based, are key compo-
nents of most NLP applications, such as speech
recognition and machine translation, where they
serve as priors in the decoding process. To estimate
a n-gram language model we require examples of
in-domain transcribed utterances, which in absence
of readily available relevant corpora have to be col-
lected manually. This poses severe constraints in
terms of both system turnaround time and cost.
This led to a growing interest in using the World
Wide Web (WWW) as a corpus for NLP (Lapata,
2005; Resnik and Smith, 2003). The web can serve
as a good resource for automatically gathering data
for building task-specific language models. Web-
pages of interest can be identified by generating
query terms either manually or automatically from
an initial set of in-domain sentences by measures
such as TFIDF or Relative Entropy (R.E). These
webpages can then be converted to a text corpus
(which we will refer to as web-data) by appropri-
ate preprocessing. However text gathered from the
web will rarely fit the demands or the nature of the
domain of interest completely. Even with the best
queries and web crawling schemes, both the style
and content of the web-data will usually differ sig-
nificantly from the specific needs. For example, a
speech recognition system requires conversational
style text whereas most of the data on the web is
literary.
The mismatch between in-domain data and web-
data can be seen as a semi-supervised learning prob-
lem. We can model the web-data as a mix of sen-
tences from two classes: in-domain (I) and noise
(N) (or out-of-domain). The labels I and N are la-
tent and unknown for the sentences in web-data but
we usually have a small number of examples of in-
domain examples I. Selecting the right labels for
the unlabeled set is important for benefiting from it.
145
Recent research on semi-supervised learning shows
that in many cases (Nigam et al, 2000; Zhu, 2005)
poor preprocessing of unlabeled data might actually
lower the performance of classifiers. We found sim-
ilar results in our language modeling experiments
where the presence of a large set of noisy N ex-
amples in training actually lowers the performance
slightly in both perplexity and WER terms. Recent
literature on building language models from text ac-
quired from the web addresses this issue partly by
using various rank-and-select schemes for identify-
ing the set I (Ostendorf et al, 2005; Sethy, 2005;
Sarikaya, 2005). However we believe that simi-
lar to the question of balance (Zhu, 2005) in semi-
supervised learning for classification, we need to ad-
dress the question of distributional similarity while
selecting the appropriate utterances for building a
language model from noisy data. The subset of sen-
tences from web-data which are selected to build the
adaptation language should have a distribution sim-
ilar to the in-domain data model.
To address the issue of distributional similarity we
present an incremental algorithm which compares
the distribution of the selected set and the in-domain
examples by using a relative entropy (R.E) criterion.
We will review in section 2 some of the ranking
schemes which provide baselines for performance
comparison and in section 3 we describe the pro-
posed algorithm. Experimental results are provided
in section 4, before we conclude with a summary of
this work and directions for the future.
2 Rank and select methods for text
cleaning
The central idea behind text cleanup schemes in re-
cent literature, on using web-data for language mod-
eling, is to use a scoring function that measures the
similarity of each observed sentence in the web-
data to the in-domain set and assigns an appropri-
ate score. The subsequent step is to set a threshold
in terms of either the minimum score or the num-
ber of top scoring sentences. The threshold can usu-
ally be fixed using a heldout set. Ostendorf (2005)
use perplexity from an in-domain n-gram language
model as a scoring function. More recently, a mod-
ified version of the BLEU metric which measures
sentence similarity in machine translation has been
proposed by Sarikaya (2005) as a scoring function.
Instead of explicit ranking and thresholding it is also
possible to design a classifier in a learning from pos-
itive and unlabeled examples framework (LPU) (Liu
et al, 2003). In this system, a subset of the unla-
beled set is selected as the negative or the noise set
N. A two class classifier is then trained using the
in-domain set and the negative set. The classifier
is then used to label the sentences in the web-data.
The classifier can then be iteratively refined by us-
ing a better and larger subset of the I/N sentences
selected in each iteration.
Rank ordering schemes do not address the issue of
distributional similarity and select many sentences
which already have a high probability in the in-
domain text. Adapting models on such data has the
tendency to skew the distribution even further to-
wards the center. For example, in our doctor-patient
interaction task short sentences containing the word
?okay? such as ?okay?,?yes okay?, ?okay okay? were
very frequent in the in-domain data. Perplexity or
other similarity measures give a high score to all
such examples in the web-data boosting the prob-
ability of these words even further while other perti-
nent sentences unseen in the in-domain data such as
?Can you stand up please?? are ranked low and get
rejected.
3 Incremental Selection
To address the issue of distributional similarity we
developed an incremental greedy selection scheme
based on relative entropy which selects a sentence
if adding it to the already selected set of sentences
reduces the relative entropy with respect to the in-
domain data distribution.
Let us denote the language model built from in-
domain data as P and let Pinit be a language modelfor initialization purposes which we estimate by
bagging samples from the same in-domain data. To
describe our algorithm we will employ the paradigm
of unigram probabilities though the method general-
izes to higher n-grams also.
Let W (i) be a initial set of counts for the words
i in the vocabulary V initialized using Pinit. We de-note the count of word i in the j th sentence sj ofweb-data with mij . Let nj = ?i mij be the num-ber of words in the sentence and N = ?i W (i) be
146
the total number of words already selected. The rel-
ative entropy of the maximum likelihood estimate of
the language model of the selected sentences to the
initial model P is given by
H(j ? 1) = ?
?
i
P (i) ln P (i)W (i)/N
If we select the sentence sj , the updated R.E
H(j) = ?
?
i
P (i) ln P (i)(W (i) + mij)/(N + nj)
Direct computation of R.E using the above ex-
pressions for every sentence in the web-data will
have a very high computational cost since O(V )
computations per sentence in the web-data are re-
quired. However given the fact that mij is sparse,we can split the summation H(j) into
H(j) = ?
?
i
P (i) ln P (i) +
+
?
i
P (i) ln W (i) + mijN + nj
= H(j ? 1) + ln N + njN
? ?? ?
T1
?
?
i,mij 6=0
P (i) ln (W (i) + mij)W (i)
? ?? ?
T2
Intuitively, the term T1 measures the decrease
in probability mass because of adding nj wordsmore to the corpus and the term T2 measures the
in-domain distribution P weighted improvement in
probability for words with non-zero mij .For the R.E to decrease with selection of sentence
sj we require T1 < T2. To make the selection morerefined we can impose a condition T1 + thr(j) <
T2 where thr(j) is a function of j. A good choice
for thr(j) based on empirical study is a function that
declines at the same rate as the ratio ln (N+nj)N ?
nj/N ? 1/kj where k is the average number ofwords for every sentence.
The proposed algorithm is sequential and greedy
in nature and can benefit from randomization of the
order in which it scans the corpus. We generate per-
mutes of the corpus by scanning through the corpus
and randomly swapping sentences. Next we do se-
quential selection on each permutation and merge
the selected sets.
The choice of using maximum likelihood estima-
tion for estimating the intermediate language mod-
els for W (j) is motivated by the simplification in
the entropy calculation which reduces the order from
O(V ) to O(k). However, maximum likelihood esti-
mation of language models is poor when compared
to smoothing based estimation. To balance the com-
putation cost and estimation accuracy, we modify
the counts W (j) using Kneser-Ney smoothing pe-
riodically after fixed number of sentences.
4 Experiments
Our experiments were conducted on medical do-
main data collected for building the English ASR
of our English-Persian Speech to Speech translation
project (Georgiou et al, 2003). We have 50K in-
domain sentences for this task available. We down-
loaded around 60GB data from the web using au-
tomatically generated queries which after filtering
and normalization amount to 150M words. The test
set for perplexity evaluations consists of 5000 sen-
tences(35K words) and the heldout set had 2000
sentences (12K words). The test set for word er-
ror rate evaluation consisted of 520 utterances. A
generic conversational speech language model was
built from the WSJ, Fisher and SWB corpora in-
terpolated with the CMU LM. All language models
built from web-data and in-domain data were inter-
polated with this language model with the interpola-
tion weight determined on the heldout set.
We first compare our proposed algorithm against
baselines based on perplexity(PPL), BLEU and LPU
classification in terms of test set perplexity. As the
comparison shows the proposed algorithm outper-
forms the rank-and-select schemes with just 1/10th
of data. Table 1 shows the test set perplexity with
different amounts of initial in-domain data. Table 2
shows the number of sentences selected for the best
perplexity on the heldout set by the above schemes.
The average relative perplexity reduction is around
6%. In addition to the PPL and WER improvements
we were able to acheive a factor of 5 reduction in
the number of estimated language model parameters
(bigram+trigram) and a 30% reduction in the vocab-
147
10K 20K 30K 40K
No Web 60 49.6 42.2 39.7
AllWeb 57.1 48.1 41.8 38.2
PPL 56.1 48.1 41.8 38.2
BLEU 56.3 48.2 42.0 38.3
LPU 56.3 48.2 42.0 38.3
Proposed 54.8 46.8 40.7 38.1
Table 1: Perplexity of testdata with the web adapted
model for different number of initial sentences.
ulary size. No Web refers to the language model built
from just in-domain data with no web-data. All-
Web refers to the case where the entire web-data was
used.
The WER results in Table 3 show that adding data
from the web without proper filtering can actually
harm the performance of the speech recognition sys-
tem when the initial in-domain data size increases.
This can be attributed to the large increase in vo-
cabulary size which increases the acoustic decoder
perplexity. The average reduction in WER using the
proposed scheme is close to 3% relative. It is inter-
esting to note that for our data selection scheme the
perplexity improvments correlate surprisingly well
with WER improvments. A plausible explanation
is that the perplexity improvments are accompanied
by a significant reduction in the number of language
model parameters.
5 Conclusion and Future Work
In this paper we have presented a computationally
efficient scheme for selecting a subset of data from
an unclean generic corpus such as data acquired
from the web. Our results indicate that with this
scheme, we can identify small subsets of sentences
(about 1/10th of the original corpus), with which we
can build language models which are substantially
smaller in size and yet have better performance in
10K 20K 30K 40K
PPL 93 92 91 91
BLEU 91 90 89 89
LPU 90 88 87 87
Proposed 12 11 11 12
Table 2: Percentage of web-data selected for differ-
ent number of initial sentences.
10K 20K 30K 40K
No Web 19.8 18.9 18.3 17.9
AllWeb 19.5 19.1 18.7 17.9
PPL 19.2 18.8 18.5 17.9
BLEU 19.3 18.8 18.5 17.9
LPU 19.2 18.8 18.5 17.8
Proposed 18.3 18.2 18.2 17.3
Table 3: Word Error Rate (WER) with web adapted
models for different number of initial sentences.
both perplexity and WER terms compared to models
built using the entire corpus. Although our focus in
the paper was on web-data, we believe the proposed
method can be used for adaptation of topic specific
models from large generic corpora.
We are currently exploring ways to use multiple
bagged in-domain language models for the selection
process. Instead of sequential scan of the corpus, we
are exploring the use of rank-and-select methods to
give a better search sequence.
References
Abhinav Sethy and Panayiotis Georgiou et al. Building topic
specific language models from web-data using competitive
models. Proceedings of Eurospeech. 2005
Bing Liu and Yang Dai et al. Building Text Classifiers Using
Positive and Unlabeled Examples. Proceedings of ICDM.
2003
Kamal Nigam and Andrew Kachites McCallum et al. Text
Classification from Labeled and Unlabeled Documents using
EM. Journal of Machine Learning. 39(2:3)103?134. 2000
Mirella Lapata and Frank Keller. Web-based models for natu-
ral language processing. ACM Transactions on Speech and
Language Processing. 2(1),2005.
Philip Resnik and Noah A. Smith. The Web as a parallel cor-
pus. Computational Linguistics. 29(3),2003.
P.G. Georgiou and S.Narayanan et al. Transonics: A speech to
speech system for English-Persian Interactions. Proceedings
of IEEE ASRU. 2003
Ruhi Sarikaya and Agustin Gravano et al Rapid Language
Model Development Using External Resources For New
Spoken Dialog Domains Proceedings of ICASSP. 2005
Tim Ng and Mari Ostendorf et al. Web-data Augmented Lan-
guage Model for Mandarin Speech Recognition. Proceed-
ings of ICASSP. 2005
Xiaojin Zhu. Semi-Supervised Learning Literature Survey.
Computer Science, University of Wisconsin-Madison.
148
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 382?389,
Sydney, July 2006. c?2006 Association for Computational Linguistics
382
383
384
385
386
387
388
389
Proceedings of NAACL HLT 2009: Short Papers, pages 277?280,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Fast decoding for open vocabulary spoken term detection
1B. Ramabhadran,1A. Sethy, 2J. Mamou?1 B. Kingsbury, 1 U. Chaudhari
1IBM T. J. Watson Research Center
Yorktown Heights,NY
2IBM Haifa Research Labs
Mount Carmel,Haifa
Abstract
Information retrieval and spoken-term detec-
tion from audio such as broadcast news, tele-
phone conversations, conference calls, and
meetings are of great interest to the academic,
government, and business communities. Mo-
tivated by the requirement for high-quality in-
dexes, this study explores the effect of using
both word and sub-word information to find
in-vocabulary and OOV query terms. It also
explores the trade-off between search accu-
racy and the speed of audio transcription. We
present a novel, vocabulary independent, hy-
brid LVCSR approach to audio indexing and
search and show that using phonetic confu-
sions derived from posterior probabilities es-
timated by a neural network in the retrieval
of OOV queries can help in reducing misses.
These methods are evaluated on data sets from
the 2006 NIST STD task.
1 Introduction
Indexing and retrieval of speech content in vari-
ous forms such as broadcast news, customer care
data and on-line media has gained a lot of interest
for a wide range of applications from market in-
telligence gathering, to customer analytics and on-
line media search. Spoken term detection (STD) is
a key information retrieval technology which aims
open vocabulary search over large collections of
spoken documents. An approach for solving the out-
of-vocabulary (OOV) issues (Saraclar and Sproat,
2004) consists of converting speech into phonetic,
?TThe work done by J. Mamou was partially funded by the
EU projects SAPIR and HERMES
syllabic or word-fragment transcripts and represent-
ing the query as a sequence of phones, syllables or
word-fragments respectively. Popular approaches
include subword decoding (Clements et al, 2002;
Mamou et al, 2007; Seide et al, 2004; Siohan and
Bacchiani, 2005) and representations enhanced with
phone confusion probabilities and approximate sim-
ilarity measures (Chaudhari and Picheny, 2007).
2 Fast Decoding Architecture
The first step in converting speech to a searchable in-
dex involves the use of an ASR system that produces
word, word-fragment or phonetic transcripts. In
this paper, the LVCSR system is a discriminatively
trained speaker-independent recognizer using PLP-
derived features and a quinphone acoustic model
with approximately 1200 context dependent states
and 30000 Gaussians. The acoustic model is trained
on 430 hours of audio from the 1996 and 1997 En-
glish Broadcast News Speech corpus (LDC97S44,
LDC98S71) and the TDT4 Multilingual Broadcast
News Speech corpus (LDC2005S11).
The language model used for decoding is a tri-
gram model with 84087 words trained on a collec-
tion of 335M words from the following data sources:
Hub4 Language Model data, EARS BN03 closed
captions and GALE Broadcast news and conversa-
tions data. A word-fragment language model is built
on this same data after tokenizing the text to frag-
ments using a fragment inventory of size 21000. A
greedy search algorithm assigns the longest possi-
ble matching fragment first and iteratively uses the
next longest possible fragment until the entire pro-
nunciation of the OOV term has been represented
277
0 5 10 15 20 25 30
30
40
50
60
70
80
90
Real Time Factor
W
ER
Figure 1: Speed vs WER
by sub-word units.
The speed and accuracy of the decoding are con-
trolled using two forms of pruning. The first is the
standard likelihood-based beam pruning that is used
in many Viterbi decoders. The second is a form
of Gaussian shortlisting in which the Gaussians in
the acoustic model are clustered into 1024 clusters,
each of which is represented by a single Gaussian.
When the decoder gets a new observation vector, it
computes the likelihood of the observation under all
1024 cluster models and then ranks the clusters by
likelihood. Observation likelihoods are then com-
puted only for those mixture components belonging
to the top maxL1 clusters; for components outside
this set a default, low likelihood is used. To illus-
trate the trade-offs in speed vs. accuracy that can
be achieved by varying the two pruning parame-
ters, we sweep through different values for the pa-
rameters and measure decoding accuracy, reported
as word error rate (WER), and decoding speed, re-
ported as times faster than real time (xfRT). For ex-
ample, a system that operates at 20xfRT will require
one minute of time (measured as elapsed time) to
process 20 minutes of speech. Figure 1 illustrates
this effect on the NIST 2006 Spoken Term Detec-
tion Dev06 test set.
3 Lucene Based Indexing and Search
The main difficulty with retrieving information from
spoken data is the low accuracy of the transcription,
particularly on terms of interest such as named en-
tities and content words. Generally, the accuracy
of a transcript is measured by its word error rate
(WER), which is characterized by the number of
substitutions, deletions, and insertions with respect
to the correct audio transcript. Mamou (Mamou
et al, 2007) presented the enhancement in recall
and precision by searching on word confusion net-
works instead of considering only the 1-best path
word transcript. We used this model for searching
in-vocabulary queries.
To handle OOV queries, a combination of
word and phonetic search was presented by
Mamou (Mamou et al, 2007). In this paper, we ex-
plore fuzzy phonetic search extending Lucene1, an
Apache open source search library written in Java,
for indexing and search. When searching for these
OOVs in word-fragment indexes, they are repre-
sented phonetically (and subsequently using word-
fragments) using letter-to-phoneme (L2P) rules.
3.1 Indexing
Each transcript is composed of basic units (e.g.,
word, word-fragment, phones) associated with a be-
gin time, duration and posterior probability. An
inverted index is used in a Lucene-based indexing
scheme. Each occurrence of a unit of indexing u in
a transcript D is indexed on its timestamp. If the
posterior probability is provided, we store the confi-
dence level of the occurrence of u at the time t that
is evaluated by its posterior probability Pr(u|t,D).
Otherwise, we consider its posterior probability to
be one. This representation allows the indexing of
different types of transcripts into a single index.
3.2 Retrieval
Since the vocabulary of the ASR system used to gen-
erate the word transcripts is known, we can easily
identify IV and OOV parts of the query. We present
two different algorithms, namely, exact and fuzzy
search on word-fragment transcripts. For search
on word-fragment or phonetic transcripts, the query
terms are converted to their word-fragment or pho-
netic representation.
Candidate lists of each query unit are extracted
from the inverted index. For fuzzy search, we re-
trieve several fuzzy matches from the inverted in-
dex for each unit of the query using the edit distance
weighted by the substitution costs provided by the
confusion matrix. Only the matches whose weighted
1http://lucene.apache.org/
278
edit distance is below a given threshold are returned.
We use a dynamic programming algorithm to incor-
porate the confusion costs specified in the matrix
in the distance computation. Our implementation is
fail-fast since the procedure is aborted if it is discov-
ered that the minimal cost between the sequences is
greater than a certain threshold.
The score of each occurrence aggregates the pos-
terior probability of each indexed unit. The occur-
rence of each unit is also weighted (user defined
weight) according to its type, for example, a higher
weight can be assigned to word matches instead of
word-fragment or phonetic matches. Given the na-
ture of the index, a match for any query term cannot
span across two consecutively indexed units.
3.3 Hybrid WordFragment Indexing
For the hybrid system we limited the word portion
of the ASR system?s lexicon to the 21K most fre-
quent (frequency greater than 5) words in the acous-
tic training data. This resulted in roughly 11M
(3.1%) OOV tokens in the hybrid LM training set
and 1127(2.5%) OOV tokens in the evaluation set.
A relative entropy criterion described in (Siohan and
Bacchiani, 2005) based on a 5-gram phone language
model was used to identify fragments. We selected
21K fragments to complement the 21K words result-
ing in a composite 42K vocabulary. The language
model text (11M (3.1%) fragment tokens and 320M
word tokens) was tokenized to contain words and
word-fragments (for the OOVs) and the resulting hy-
brid LM was used in conjunction with the acoustic
models described in Section 2.
4 Neural Network Based Posteriors for
Fuzzy Search
In assessing the match of decoded transcripts with
search queries, recognition errors must be accounted
for. One method relies on converting both the de-
coded transcripts and queries into phonetic represen-
tations and modeling the confusion between phones,
typically represented as a confusion matrix. In this
work, we derive this matrix from broadcast news de-
velopment data. In particular, two systems: HMM
based automatic speech recognition (ASR) (Chaud-
hari and Picheny, 2007) and a neural network based
acoustic model (Kingsbury, 2009), are used to ana-
lyze the data and the results are compared to produce
confusion estimates.
Let X = {xt} represent the input feature frames
and S the set of context dependent HMM states.
Associated with S is a many to one map M from
each member sj ? S to a phone in the phone set
pk ? P. This map collapses the beginning, mid-
dle, and end context dependent states to the central
phone identity. The ASR system is used to generate
a state based alignment of the development data to
the training transcripts. This results in a sequence
of state labels (classes) {st}, st ? S , one for each
frame of the input data. Note that the aligned states
are collapsed to the phone identity with M, so the
frame class labels are given by {ct}, ct ? P.
Corresponding to each frame, we also use the
state posteriors derived from the output of a Neu-
ral Network acoustic model and the prior probabil-
ities computed on the training set. Define Xt =
{. . . , xt, . . .} to be the sub-sequence of the input
speech frames centered around time index t. The
neural network takes Xt as input and produces
lt(sj) = y(sj|Xt)? l(sj), sj ? S
where y is the neural network output and l is the
prior probability, both in the log domain. Again, the
state labels are mapped using M, so the above pos-
terior is interpreted as that for the collapsed phone:
lt(sj) ? lt(M(sj)) = lt(pj), pj = M(sj).
The result of both analyses gives the following set of
associations:
c0 ? l0(p0), l0(p1), l0(p2), . . .
c1 ? l1(p0), l1(p1), l1(p2), . . .
.
.
ct ? lt(p0), lt(p1), lt(p2), . . .
Each log posterior li(pj) is converted into a count
ni,j = ceil[N ? eli(pj)],
where N is a large constant, i ranges over the
time index, and j ranges over the context dependent
states. From the counts, the confusion matrix entries
are computed. The total count for each state is
nj(k) =
?
i:ci=pj
ni,k,
279
where k is an index over the states.
?
????
n1(1) n1(2) . . .
n2(1) n2(2) . . .
.
.
?
????
The rows of the above matrix correspond to the ref-
erence and the columns to the observations. By nor-
malizing the rows, the entries can be interpreted as
?probability? of an observed phone (indicated by the
column) given the true phone.
5 Experiments and Results
The performance of a spoken term detection system
is measured using DET curves that plot the trade-off
between false alarms (FAs) and misses. This NIST
STD 2006 evaluation metric used Actual/Maximum
Term Weighted Value (ATWV/MTWV) that allows
one to weight FAs and Misses per the needs of the
task at hand (NIST, 2006).
Figure 2 illustrates the effect of speed on ATWV
on the NIST STD 2006 Dev06 data set using 1107
query terms. As the speed of indexing is increased to
many times faster than real time, the WER increases,
which in turn decreases the ATWV measure. It can
be seen that the use of word-fragments improves
the performance on OOV queries thus making the
combined search better than simple word search.
The primary advantage of using a hybrid decoding
scheme over a separate word and fragment based
decoding scheme is the speed of transforming the
audio into indexable units. The blue line in the fig-
ure illustrates that when using a hybrid setup, the
same performance can be achieved at speeds twice
as fast. For example, with the combined search
on two different decodes, an ATWV of 0.1 can be
achieved when indexing at a speed 15 times faster
than real time, but with a hybrid system, the same
performance can be reached at an indexing speed 30
times faster than real time. The ATWV on the hybrid
system also degrades gracefully with faster speeds
when compared to separate word and word-fragment
systems. Preliminary results indicate that fuzzy
search on one best output gives the same ATWV
performance as exact search (Figure 2) on consen-
sus output. Also, a closer look at the retrieval results
of OOV terms revealed that many more OOVs are
retrieved with the fuzzy search.
0 5 10 15 20 25 30 35
?0.4
?0.2
0
0.2
0.4
0.6
0.8
1
Real Time Factor
AT
W
V
 
 
exactWord
exactWordAndFrag
exactHybrid
Figure 2: Effect of WER on ATWV. Note that the cuves
for exactWord and exactWordAndFrag lie on top of each
other.
6 CONCLUSION
In this paper, we have presented the effect of rapid
decoding on a spoken term detection task. We
have demonstrated that hybrid systems perform well
and fuzzy search with phone confusion probabilities
help in OOV retrieval.
References
U. V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. In Proc. of ASRU.
M. Clements, S. Robertson, and M. S. Miller. 2002.
Phonetic searching applied to on-line distance learning
modules. In Proc. of IEEE Digital Signal Processing
Workshop.
B. Kingsbury. 2009. Lattice-based optimization
of sequence classification criteria for neural-network
acoustic modeling. In Proc. of ICASSP.
J. Mamou, B. Ramabhadran, and O. Siohan. 2007. Vo-
cabulary independent spoken term detection. In Proc.
of ACM SIGIR.
NIST. 2006. The spoken term de-
tection (STD) 2006 evaluation plan.
http://www.nist.gov/speech/tests/std/docs/std06-
evalplan-v10.pdf.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
F. Seide, P. Yu, C. Ma, and E. Chang. 2004. Vocabulary-
independent search in spontaneous speech. In Proc. of
ICASSP.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary in-
dependent audio search using path based graph index-
ing. In Proc. of Interspeech.
280
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 190?197,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Model Adaptation using Information-Theoretic Criterion
Ariya Rastrow1, Frederick Jelinek1, Abhinav Sethy2 and Bhuvana Ramabhadran2
1Human Language Technology Center of Excellence, and
Center for Language and Speech Processing, Johns Hopkins University
{ariya, jelinek}@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
{asethy, bhuvana}@us.ibm.com
Abstract
In this paper we propose a novel general
framework for unsupervised model adapta-
tion. Our method is based on entropy which
has been used previously as a regularizer in
semi-supervised learning. This technique in-
cludes another term which measures the sta-
bility of posteriors w.r.t model parameters, in
addition to conditional entropy. The idea is to
use parameters which result in both low con-
ditional entropy and also stable decision rules.
As an application, we demonstrate how this
framework can be used for adjusting language
model interpolation weight for speech recog-
nition task to adapt from Broadcast news data
to MIT lecture data. We show how the new
technique can obtain comparable performance
to completely supervised estimation of inter-
polation parameters.
1 Introduction
All statistical and machine learning techniques for
classification, in principle, work under the assump-
tion that
1. A reasonable amount of training data is avail-
able.
2. Training data and test data are drawn from the
same underlying distribution.
In fact, the success of statistical models is cru-
cially dependent on training data. Unfortunately,
the latter assumption is not fulfilled in many appli-
cations. Therefore, model adaptation is necessary
when training data is not matched (not drawn from
same distribution) with test data. It is often the case
where we have plenty of labeled data for one specific
domain/genre (source domain) and little amount of
labeled data (or no labeled data at all) for the de-
sired domain/genre (target domain). Model adapta-
tion techniques are commonly used to address this
problem. Model adaptation starts with trained mod-
els (trained on source domain with rich amount of la-
beled data) and then modify them using the available
labeled data from target domain (or instead unla-
beled data). A survey on different methods of model
adaptation can be found in (Jiang, 2008).
Information regularization framework has been
previously proposed in literature to control the la-
bel conditional probabilities via input distribution
(Szummer and Jaakkola, 2003). The idea is that la-
bels should not change too much in dense regions
of the input distribution. The authors use the mu-
tual information between input features and labels as
a measure of label complexity. Another framework
previously suggested is to use label entropy (condi-
tional entropy) on unlabeled data as a regularizer to
Maximum Likelihood (ML) training on labeled data
(Grandvalet and Bengio, 2004).
Availability of resources for the target domain cat-
egorizes these techniques into either supervised or
unsupervised. In this paper we propose a general
framework for unsupervised adaptation using Shan-
non entropy and stability of entropy. The assump-
tion is that in-domain and out-of-domain distribu-
tions are not too different such that one can improve
the performance of initial models on in-domain data
by little adjustment of initial decision boundaries
(learned on out-of-domain data).
190
2 Conditional Entropy based Adaptation
In this section, conditional entropy and its relation
to classifier performance are first described. Next,
we introduce our proposed objective function for do-
main adaptation.
2.1 Conditional Entropy
Considering the classification problem whereX and
Y are the input features and the corresponding class
labels respectively, the conditional entropy is a mea-
sure of the class overlap and is calculated as follows
H(Y|X) = EX[H(Y|X = x)] =
?
?
p(x)
(
?
y
p(y|x) log p(y|x)
)
dx (1)
Through Fano?s Inequality theorem, one can see
how conditional entropy is related to classification
performance.
Theorem 1 (Fano?s Inequality) Suppose
Pe = P{Y? 6= Y} where Y? = g(X) are the
assigned labels for the data points, based on the
classification rule. Then
Pe ?
H(Y|X)? 1
log(|Y| ? 1)
where Y is the number of possible classes and
H(Y |X) is the conditional entropy with respect to
true distibution.
The proof to this theorem can be found in (Cover and
Thomas, 2006). This inequality indicates thatY can
be estimated with low probability of error only if the
conditional entropy H(Y|X) is small.
Although the above theorem is useful in a sense
that it connects the classification problem to Shan-
non entropy, the true distributions are almost never
known to us1. In most classification methods, a spe-
cific model structure for the distributions is assumed
and the task is to estimate the model parameters
within the assumed model space. Given the model
1In fact, Theorem 1 shows how relevant the input features
are for the classification task by putting a lower bound on the
best possible classifier performance. As the overlap between
features from different classes increases, conditional entropy in-
creases as well, thus lowering the performance of the best pos-
sible classifier.
structure and parameters, one can modify Fano?s In-
equality as follows,
Corollary 1
Pe(?) = P{Y? 6= Y |?} ? H?(Y|X)? 1log(|Y| ? 1)
(2)
where Pe(?) is the classifier probability of error
given model parameters, ? and
H?(Y|X) =
?
?
p(x)
(
?
y
p?(y|x) log p?(y|x)
)
dx
Here, H?(Y|X) is the conditional entropy imposed
by model parameters.
Eqn. 2 indicates the fact that models with low
conditional entropy are preferable. However, a low
entropy model does not necessarily have good per-
formance (this will be reviewed later on) 2
2.2 Objective Function
Minimization of conditional entropy as a framework
in the classification task is not a new concept and
has been tried by researchers. In fact, (Grandvalet
and Bengio, 2004) use this along with the maxi-
mum likelihood criterion in a semi-supervised set
up such that parameters with both maximum like-
lihood on labeled data and minimum conditional en-
tropy on unlabeled data are chosen. By minimiz-
ing the entropy, the method assumes a prior which
prefers minimal class overlap. Entropy minimiza-
tion is used in (Li et al, 2004) as an unsupervised
non-parametric clustering method and is shown to
result in significant improvement over k-mean, hier-
archical clustering and etc.
These methods are all based on the fact that mod-
els with low conditional entropy have their decision
boundaries passing through low-density regions of
the input distribution, P (X). This is consistent with
the assumption that classes are well separated so that
one can expect to take advantage of unlabeled exam-
ples (Grandvalet and Bengio, 2004).
In many cases shifting from one domain to an-
other domain, initial trained decision boundaries (on
2Imagine a model which classifies any input as class 1.
Clearly for this model H?(Y|X) = 0.
191
out-of-domain data) result in high conditional en-
tropy for the new domain, due to mismatch be-
tween distributions. Therefore, there is a need to
adjust model parameters such that decision bound-
aries goes through low-density regions of the distri-
bution. This motivates the idea of using minimum
conditional entropy criterion for adapting to a new
domain. At the same time, two domains are often
close enough that one would expect that the optimal
parameters for the new domain should not deviate
too much from initial parameters. In order to formu-
late the technique mentioned in the above paragraph,
let us define ?init to be the initial model parame-
ters estimated on out-of-domain data (using labeled
data). Assuming the availability of enough amount
of unlabeled data for in-domain task, we try to min-
imize the following objective function w.r.t the pa-
rameters,
?new = argmin
?
H?(Y|X) + ? ||? ? ?init||p
(3)
where ||? ? ?init||p is an Lp regularizer and tries to
prevent parameters from deviating too much from
their initial values3.
Once again the idea here is to adjust the param-
eters (using unlabeled data) such that low-density
separation between the classes is achieved. In the
following section we will discuss the drawback of
this objective function for adaptation in realistic sce-
narios.
3 Issues with Minimum Entropy Criterion
It is discussed in Section 2.2 that the model param-
eters are adapted such that a minimum conditional
entropy is achieved. It was also discussed how this is
related to finding decision boundaries through low-
density regions of input distribution. However, the
obvious assumption here is that the classes are well
separated and there in fact exists low-density regions
between classes which can be treated as boundaries.
Although this is a suitable/ideal assumption for clas-
sification, in most practical problems this assump-
tion is not satisfied and often classes overlap. There-
fore, we can not expect the conditional entropy to be
3The other reason for using a regularizer is to prevent trivial
solutions of minimum entropy criterion
convex in this situation and to achieve minimization
w.r.t parameters (other than the trivial solutions).
Let us clarify this through an example. Consider
X to be generated by mixture of two 2-D Gaus-
sians (each with a particular mean and covariance
matrix) where each Gaussian corresponds to a par-
ticular class ( binary class situation) . Also in order
to have linear decision boundaries, let the Gaussians
have same covariance matrix and let the parameter
being estimated be the prior for class 1, P (Y = 1).
Fig. 1 shows two different situations with over-
lapping classes and non-overlapping classes. The
left panel shows a distribution in which classes are
well separated whereas the right panel corresponds
to the situation where there is considerable overlap
between classes. Clearly, in the later case there is
no low-density region separating the classes. There-
fore, as we change the parameter (here, the prior on
the class Y = 1), there will not be any well defined
point with minimum entropy. This can be seen from
Fig. 2 where model conditional entropy is plotted
vs. class prior parameter for both cases. In the case
of no-overlap between classes, entropy is a convex
function w.r.t the parameter (excluding trivial solu-
tions which happens at P (Y = 1) = 0, 1) and is
minimum at P (Y = 1) = 0.7 which is the true prior
with which the data was generated.
We summarize issues with minimum entropy cri-
terion and our proposed solutions as follows:
? Trivial solution: this happens when we put de-
cision boundaries such that both classes are
considered as one class (this can be avoided us-
ing the regularizer in Eqn. 3 and the assump-
tion that initial models have a reasonable solu-
tion, e.g. close to the optimal solution for new
domain )
? Overlapped Classes: As it was discussed in
this section, if the overlap is considerable then
the entropy will not be convex w.r.t to model
parameters. We will address this issue in
the next section by introducing the entropy-
stability concept.
4 Entropy-Stability
It was discussed in the previous section that a mini-
mum entropy criterion can not be used (by itself) in
192
?3 ?2 ?1 0 1 2 3 4 5 6 7?4
?2
0
2
4
6
8
10
X1
X 2
?3 ?2 ?1 0 1 2 3 4 5 6 7?3
?2
?1
0
1
2
3
4
5
6
7
X1
X 2
Figure 1: Mixture of two Gaussians and the corresponding Bayes decision boundary: (left) with no class overlap
(right) with class overlap
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0	 ?
0.005	 ?
0.01	 ?
0.015	 ?
0.02	 ?
0.025	 ?
0.03	 ?
0.035	 ?
0	 ? 0.1	 ? 0.2	 ? 0.3	 ? 0.4	 ? 0.5	 ? 0.6	 ? 0.7	 ? 0.8	 ? 0.9	 ? 1	 ?
Con
di?n
al	 ?E
ntro
py	 ?
P(Y=1)	 ?
without	 ?overlap	 ?
with	 ?overlap	 ?
Figure 2: Condtional entropy vs. prior parameter, P (Y =
1)
situations where there is a considerable amount of
overlap among classes. Assuming that class bound-
aries happen in the regions close to the tail of class
distributions, we introduce the concept of Entropy-
Stability and show how it can be used to detect
boundary regions. Define Entropy-Stability to be the
reciprocal of the following
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
?
?
p(x)
?
(?
y p?(y|x) log p?(y|x)
)
?? dx
?
?
?
?
?
?
?
?
?
?
?
?
p
(4)
Recall: since ? is a vector of parameters, ?H?(Y|X)??
will be a vector and by using Lp norm Entropy-
stability will be a scalar.
The introduced concept basically measures the
stability of label entropies w.r.t the model parame-
ters. The idea is that we prefer models which not
only have low-conditional entropy but also have sta-
ble decision rules imposed by the model. Next, we
show through the following theorem how Entropy-
Stability measures the stability over posterior prob-
abilities (decision rules) of the model.
Theorem 2
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
p(x)
(
?
y
?p?(y|x)
?? log p?(y|x)
)
dx
?
?
?
?
?
?
?
?
?
?
p
where the term inside the parenthesis is the weighted
sum (by log-likelihood) over the gradient of poste-
rior probabilities of labels for a given sample x
Proof The proof is straight forward and uses the fact
that
? ?p?(y|x)
?? = ?(
P
p?(y|x))
?? = 0 .
Using Theorem 2 and Eqn. 4, it should be clear
how Entropy-Stability measures the expected sta-
bility over the posterior probabilities of the model.
A high value of
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
implies models with
less stable decision rules. In order to explain how
this is used for detecting boundaries (overlapped
193
regions) we once again refer back to our mixture
of Gaussians? example. As the decision boundary
moves from class specific regions to overlapped re-
gions (by changing the parameter which is here class
prior probability) we expect the entropy to continu-
ously decrease (due to the assumption that the over-
laps occur at the tail of class distributions). How-
ever, as we get close to the overlapping regions the
added data points from other class(es) will resist
changes in the entropy. resulting in stability over the
entropy until we enter the regions specific to other
class(es).
In the following subsection we use this idea to
propose a new objective function which can be used
as an unsupervised adaptation method even for the
case of input distribution with overlapping classes.
4.1 Better Objective Function
The idea here is to use the Entropy-Stability con-
cept to accept only regions which are close to the
overlapped parts of the distribution (based on our
assumption, these are valid regions for decision
boundaries) and then using the minimum entropy
criterion we find optimum solutions for our parame-
ters inside these regions. Therefore, we modify Eqn.
3 such that it also includes the Entropy-Stability
term
?new = argmin
?
(
H?(Y|X) + ?
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p?
+ ? ||? ? ?init||p
)
(5)
The parameter ? and ? can be tuned using small
amount of labeled data (Dev set).
5 Speech Recognition Task
In this section we will discuss how the proposed
framework can be used in a speech recognition task.
In the speech recognition task, Y is the sequence
of words and X is the input speech signal. For a
given speech signal, almost every word sequence is
a possible output and therefore there is a need for
a compact representation of output labels (words).
For this, word graphs (Lattices) are generated dur-
ing the recognition process. In fact, each lattice is
an acyclic directed graph whose nodes correspond
to particular instants of time, and arcs (edges con-
necting nodes) represent possible word hypotheses.
Associated with each arc is an acoustic likelihood
and language model likelihood scores. Fig. 3 shows
an example of recognition lattice 4 (for the purpose
of demonstration likelihood scores are not shown).L. Mangu et al: Finding Consensus in Speech Recognition 6
(a) Input lattice (?SIL? marks pauses)
SIL
SIL
SIL
SIL
SIL
SIL
VEAL
VERY
HAVE
MOVE
HAVE
HAVE
IT
MOVE
HAVE IT
VERY
VERY
VERY
VERY
OFTEN
OFTEN
FINE
FINE
FAST
I
I
I
(b) Multiple alignment (?-? marks deletions)
- -
I
MOVE
HAVE IT VEAL 
VERY
FINE
OFTEN
FAST
Figure 1: Sample recognition lattice and corresponding multiple alignment represented as
confusion network.
alignment (which gives rise to the standard string edit distance WE (W,R)) with
a modified, multiple string alignment. The new approach incorporates all lattice
hypotheses7 into a single alignment, and word error between any two hypotheses
is then computed according to that one alignment. The multiple alignment thus
defines a new string edit distance, which we will call MWE (W,R). While the
new alignment may in some cases overestimate the word error between two
hypotheses, as we will show in Section 5 it gives very similar results in practice.
The main benefit of the multiple alignment is that it allows us to extract
the hypothesis with the smallest expected (modified) word error very efficiently.
To see this, consider an example. Figure 1 shows a word lattice and the corre-
sponding hypothesis alignment. Each word hypothesis is mapped to a position
in the alignment (with deletions marked by ?-?). The alignment also supports
the computation of word posterior probabilities. The posterior probability of a
word hypothesis is the sum of the posterior probabilities of all lattice paths of
which the word is a part. Given an alignment and posterior probabilities, it is
easy to see that the hypothesis with the lowest expected word error is obtained
by picking the word with the highest posterior at each position in the alignment.
We call this the consensus hypothesis.
7In practice we apply some pruning of the lattice to remove low probability word hypotheses
(see Section 3.4).
Figure 3: Lattice Example
Since lattices contain all the likely hypotheses
(unlikely hypotheses are pruned during recognition
and will not be included in the lattice), conditional
entropy for any given input speech signal, x, can be
approximated by the conditional entropy of the lat-
tice. That is,
H?(Y|X = xi) = H?(Y|Li)
whereLi is the corresponding decoded lattice (given
speech recognizer parameters) of utterance xi.
For the calculation of entropy we need to
know the distribution of X because H?(Y|X) =
EX [H?(Y|X = x)] and since this distribution is not
known to us, we use Law of Large Numbers to ap-
proximate it by the empirical average
H?(Y|X) ? ? 1N
N?
i=1
?
y?Li
p?(y|Li) log p?(y|Li) (6)
Here N indicates the number of unlabeled utter-
ances for which we calculate the empirical value of
conditional entropy. Similarly, expectation w.r.t in-
put distribution in entropy-stability term is also ap-
proximated by the empirical average of samples.
Since the number of paths (hypotheses) in the lat-
tice is very large, it would be computationally infea-
sibl to c ute the conditi nal entropy y enumer-
ating all possible paths in the lattice and calculating
4The figure is adopted from (Mangu et al, 1999)
194
Element ?p, r?
?p1, r1?? ?p2, r2? ?p1p2, p1r2 + p2r1?
?p1, r1?? ?p2, r2? ?p1 + p2, r1 + r2?
0 ?0, 0?
1 ?1, 0?
Table 1: First-Order (Expectation) semiring: Defining
multiplication and sum operations for first-order semir-
ings.
their corresponding posterior probabilities. Instead
we use Finite-State Transducers (FST) to represent
the hypothesis space (lattice). To calculate entropy
and the gradient of entropy, the weights for the FST
are defined to be First- and Second-Order semirings
(Li and Eisner, 2009). The idea is to use semirings
and their corresponding operations along with the
forward-backward algorithm to calculate first- and
second-order statistics to compute entropy and the
gradient of entropy respectively. Assume we are in-
terested in calculating the entropy of the lattice,
H(p) = ??
d?Li
p(d)
Z log(
p(d)
Z )
= logZ ? 1Z
?
d?Li
p(d) log p(d)
= logZ ? r?Z (7)
where Z is the total probability of all the paths in
the lattice (normalization factor). In order to do so,
we need to compute ?Z, r?? on the lattice. It can
be proved that if we define the first-order semir-
ing ?pe, pe log pe? (pe is the non-normalized score of
each arc in the lattice) as our FST weights and define
semiring operations as in Table. 1, then applying the
forward algorithm will result in the calculation of
?Z, r?? as the weight (semiring weight) for the final
node.
The details for using Second-Order semirings for
calculating the gradient of entropy can be found
in (Li and Eisner, 2009). The same paper de-
scribes how to use the forward-backward algorithm
to speed-up the this procedure.
6 Language Model Adaptation
Language Model Adaptation is crucial when the
training data does not match the test data being de-
coded. This is a frequent scenario for all Automatic
Speech Recognition (ASR) systems. The applica-
tion domain very often contains named entities and
N-gram sequences that are unique to the domain of
interest. For example, conversational speech has
a very different structure than class-room lectures.
Linear Interpolation based methods are most com-
monly used to adapt LMs to a new domain. As
explained in (Bacchiani et al, 2003), linear inter-
polation is a special case of Maximum A Posterior
(MAP) estimation, where an N-gram LM is built on
the adaptation data from the new domain and the two
LMs are combined using:
p(wi|h) = ?pB(wi|h) + (1? ?)pA(wi|h)
0 ? ? ? 1
where pB refers to out-of-domain (background)
models and pA is the adaptation (in-domain) mod-
els. Here ? is the interpolation weight.
Conventionally, ? is calculated by optimizing per-
plexity (PPL) or Word Error Rate (WER) on some
held-out data from target domain. Instead using
our proposed framework, we estimate ? on enough
amount of unlabeled data from target domain. The
idea is that resources on the new domain have al-
ready been used to build domain specific models
and it does not make sense to again use in-domain
resources for estimating the interpolation weight.
Since we are trying to just estimate one parameter
and the performance of the interpolated model is
bound by in-domain/out-of-domain models, there is
no need to include a regularization term in Eqn. 5.
Also
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
= |?H?(Y|X)?? | because we only
have one parameter. Therefore, interpolation weight
will be chosen by the following criterion
?? = argmin
0???1
H?(Y|X) + ?|?H?(Y|X)?? | (8)
For the purpose of estimating one parameter ?, we
use ? = 1 in the above equation
7 Experimental Setup
The large vocabulary continuous speech recognition
(LVCSR) system used throughout this paper is based
on the 2007 IBM Speech transcription system for
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models used in this system
195
are state-of-the-art discriminatively trained models
and are the same ones used for all experiments pre-
sented in this paper.
For LM adaptation experiments, the out-of-
domain LM (pB , Broadcast News LM) training
text consists of 335M words from the follow-
ing broadcast news (BN) data sources (Chen et
al., 2006): 1996 CSR Hub4 Language Model
data, EARS BN03 closed captions, GALE Phase
2 Distillation GNG Evaluation Supplemental Mul-
tilingual data, Hub4 acoustic model training tran-
scripts, TDT4 closed captions, TDT4 newswire, and
GALE Broadcast Conversations and GALE Broad-
cast News. This language model is of order 4-gram
with Kneser-Ney smoothing and contains 4.6M n-
grams based on a lexicon size of 84K.
The second source of data is the MIT lectures data
set (J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D.
Huynh, and R. Barzilay, 2007) . This serves as the
target domain (in-domain) set for language model
adaptation experiments. This set is split into 8 hours
for in-domain LM building, another 8 hours served
as unlabeled data for interpolation weight estimation
using criterion in Eqn. 8 (we refer to this as unsuper-
vised training data) and finally 2.5 hours Dev set for
estimating the interpolation weight w.r.t WER (su-
pervised tuning) . The lattice entropy and gradient
of entropy w.r.t ? are calculated on the unsupervised
training data set. The results are discussed in the
next section.
8 Results
In order to optimize the interpolation weight ? based
on criterion in Eqn. 8, we devide [0, 1] to 20 differ-
ent points and evaluate the objective function (Eqn.
8) on those points. For this, we need to calculate
entropy and gradient of the entropy on the decoded
lattices of the ASR system on 8 hours of MIT lecture
set which is used as an unlabeled training data. Fig.
4 shows the value of the objective function against
different values of model parameters (interpolation
weight ?). As it can be seen from this figure just
considering the conditional entropy will result in a
non-convex objective function whereas adding the
entropy-stability term will make the objective func-
tion convex. For the purpose of the evaluation, we
show the results for estimating ? directly on the tran-
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy
Model Entropy+Entropy-Stability
BN-LM MIT-LM?
Figure 4: Objective function with and without including
Entropy-Stability term vs. interpolation weight ? on 8
hours MIT lecture unlabeled data
scription of the 8 hour MIT lecture data and compare
it to estimated value using our framework. The re-
sults are shown in Fig. 5. Using ? = 0 and ? = 1
the WERs are 24.7% and 21.1% respectively. Us-
ing the new proposed objective function, the optimal
? is estimated to be 0.6 with WER of 20.1% (Red
circle on the figure). Estimating ? w.r.t 8 hour train-
ing data transcription (supervised adaptation) will
result in ? = 0.7 (green circle) andWER of 20.0%.
Instead ? = 0.8 will be chosen by tuning the inter-
polation weight on 2.5 hour Dev set with compara-
ble WER of 20.1%. Also it is clear from the figure
that the new objective function can be used to pre-
dict the WER trend w.r.t the interpolation weight
parameter.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy + Entropy Stability
WER
24.7%
20.0%
21.1%
supervised tuning
?
Figure 5: Estimating ? based on WER vs. the
information-theoretic criterion
Therefore, it can be seen that the new unsuper-
196
vised method results in the same performance as su-
pervised adaptation in speech recognition task.
9 Conclusion and Future Work
In this paper we introduced the notion of entropy
stability and presented a new criterion for unsu-
pervised adaptation which combines conditional en-
tropy minimization with entropy stability. The en-
tropy stability criterion helps in selecting parameter
settings which correspond to stable decision bound-
aries. Entropy minimization on the other hand tends
to push decision boundaries into sparse regions of
the input distributions. We show that combining
the two criterion helps to improve unsupervised pa-
rameter adaptation in real world scenario where
class conditional distributions show significant over-
lap. Although conditional entropy has been previ-
ously proposed as a regularizer, to our knowledge,
the gradient of entropy (entropy-stability) has not
been used previously in the literature. We presented
experimental results where the proposed criterion
clearly outperforms entropy minimization. For the
speech recognition task presented in this paper, the
proposed unsupervised scheme results in the same
performance as the supervised technique.
As a future work, we plan to use the proposed
criterion for adapting log-linear models used in
Machine Translation, Conditional Random Fields
(CRF) and other applications. We also plan to ex-
pand linear interpolation Language Model scheme
to include history specific (context dependent)
weights.
Acknowledgments
The Authors want to thank Markus Dreyer and
Zhifei Li for their insightful discussions and sugges-
tions.
References
M. Bacchiani, B. Roark, and M. Saraclar. 2003. Un-
supervised language model adaptation. In Proc.
ICASSP, pages 224?227.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, 3rd edition.
Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization. In
Advances in neural information processing systems
(NIPS), volume 17, pages 529?536.
J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,
and R. Barzilay. 2007. Recent progress in MIT spo-
ken lecture processing project. In Proc. Interspeech.
Jing Jiang. 2008. A literature survey on domain adapta-
tion of statistical classifiers, March.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP.
Haifeng Li, Keshu Zhang, and Tao Jiang. 2004. Min-
imum entropy clustering and applications to gene ex-
pression analysis. In Proceedings of IEEE Computa-
tional Systems Bioinformatics Conference, pages 142?
151.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 1999.
Finding consensus among words: Lattice-based word
error minimization. In Sixth European Conference on
Speech Communication and Technology.
M. Szummer and T. Jaakkola. 2003. Information regu-
larization with partially labeled data. In Advances in
Neural Information Processing Systems, pages 1049?
1056.
197
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 712?721,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Sub-Word Units for Open Vocabulary Speech Recognition
Carolina Parada1, Mark Dredze1, Abhinav Sethy2, and Ariya Rastrow1
1Human Language Technology Center of Excellence, Johns Hopkins University
3400 N Charles Street, Baltimore, MD, USA
carolinap@jhu.edu, mdredze@cs.jhu.edu, ariya@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
asethy@us.ibm.com
Abstract
Large vocabulary speech recognition systems
fail to recognize words beyond their vocab-
ulary, many of which are information rich
terms, like named entities or foreign words.
Hybrid word/sub-word systems solve this
problem by adding sub-word units to large vo-
cabulary word based systems; new words can
then be represented by combinations of sub-
word units. Previous work heuristically cre-
ated the sub-word lexicon from phonetic rep-
resentations of text using simple statistics to
select common phone sequences. We pro-
pose a probabilistic model to learn the sub-
word lexicon optimized for a given task. We
consider the task of out of vocabulary (OOV)
word detection, which relies on output from
a hybrid model. A hybrid model with our
learned sub-word lexicon reduces error by
6.3% and 7.6% (absolute) at a 5% false alarm
rate on an English Broadcast News and MIT
Lectures task respectively.
1 Introduction
Most automatic speech recognition systems operate
with a large but limited vocabulary, finding the most
likely words in the vocabulary for the given acoustic
signal. While large vocabulary continuous speech
recognition (LVCSR) systems produce high quality
transcripts, they fail to recognize out of vocabulary
(OOV) words. Unfortunately, OOVs are often infor-
mation rich nouns, such as named entities and for-
eign words, and mis-recognizing them can have a
disproportionate impact on transcript coherence.
Hybrid word/sub-word recognizers can produce a
sequence of sub-word units in place of OOV words.
Ideally, the recognizer outputs a complete word for
in-vocabulary (IV) utterances, and sub-word units
for OOVs. Consider the word ?Slobodan?, the given
name of the former president of Serbia. As an un-
common English word, it is unlikely to be in the vo-
cabulary of an English recognizer. While a LVCSR
system would output the closest known words (e.x.
?slow it dawn?), a hybrid system could output a
sequence of multi-phoneme units: s l ow, b ax,
d ae n. The latter is more useful for automatically
recovering the word?s orthographic form, identify-
ing that an OOV was spoken, or improving perfor-
mance of a spoken term detection system with OOV
queries. In fact, hybrid systems have improved OOV
spoken term detection (Mamou et al, 2007; Parada
et al, 2009), achieved better phone error rates, espe-
cially in OOV regions (Rastrow et al, 2009b), and
obtained state-of-the-art performance for OOV de-
tection (Parada et al, 2010).
Hybrid recognizers vary in a number of ways:
sub-word unit type: variable-length phoneme
units (Rastrow et al, 2009a; Bazzi and Glass, 2001)
or joint letter sound sub-words (Bisani and Ney,
2005); unit creation: data-driven or linguistically
motivated (Choueiter, 2009); and how they are in-
corporated in LVCSR systems: hierarchical (Bazzi,
2002) or flat models (Bisani and Ney, 2005).
In this work, we consider how to optimally cre-
ate sub-word units for a hybrid system. These units
are variable-length phoneme sequences, although in
principle our work can be use for other unit types.
Previous methods for creating the sub-word lexi-
712
con have relied on simple statistics computed from
the phonetic representation of text (Rastrow et al,
2009a). These units typically represent the most fre-
quent phoneme sequences in English words. How-
ever, it isn?t clear why these units would produce the
best hybrid output. Instead, we introduce a prob-
abilistic model for learning the optimal units for a
given task. Our model learns a segmentation of a
text corpus given some side information: a mapping
between the vocabulary and a label set; learned units
are predictive of class labels.
In this paper, we learn sub-word units optimized
for OOV detection. OOV detection aims to identify
regions in the LVCSR output where OOVs were ut-
tered. Towards this goal, we are interested in select-
ing units such that the recognizer outputs them only
for OOV regions while prefering to output a com-
plete word for in-vocabulary regions. Our approach
yields improvements over state-of-the-art results.
We begin by presenting our log-linear model for
learning sub-word units with a simple but effective
inference procedure. After reviewing existing OOV
detection approaches, we detail how the learned
units are integrated into a hybrid speech recognition
system. We show improvements in OOV detection,
and evaluate impact on phone error rates.
2 Learning Sub-Word Units
Given raw text, our objective is to produce a lexicon
of sub-word units that can be used by a hybrid sys-
tem for open vocabulary speech recognition. Rather
than relying on the text alone, we also utilize side
information: a mapping of words to classes so we
can optimize learning for a specific task.
The provided mapping assigns labels Y to the cor-
pus. We maximize the probability of the observed
labeling sequence Y given the text W : P (Y |W ).
We assume there is a latent segmentation S of this
corpus which impacts Y . The complete data likeli-
hood becomes: P (Y |W ) =
?
S P (Y, S|W ) during
training. Since we are maximizing the observed Y ,
segmentation S must discriminate between different
possible labels.
We learn variable-length multi-phone units by
segmenting the phonetic representation of each word
in the corpus. Resulting segments form the sub-
word lexicon.1 Learning input includes a list of
words to segment taken from raw text, a mapping
between words and classes (side information indi-
cating whether token is IV or OOV), a pronuncia-
tion dictionaryD, and a letter to sound model (L2S),
such as the one described in Chen (2003). The cor-
pus W is the list of types (unique words) in the raw
text input. This forces each word to have a unique
segmentation, shared by all common tokens. Words
are converted into phonetic representations accord-
ing to their most likely dictionary pronunciation;
non-dictionary words use the L2S model.2
2.1 Model
Inspired by the morphological segmentation model
of Poon et al (2009), we assume P (Y, S|W ) is a
log-linear model parameterized by ?:
P?(Y, S|W ) =
1
Z(W )
u?(Y, S,W ) (1)
where u?(Y, S,W ) defines the score of the pro-
posed segmentation S for words W and labels Y
according to model parameters ?. Sub-word units
? compose S, where each ? is a phone sequence, in-
cluding the full pronunciation for vocabulary words;
the collection of ?s form the lexicon. Each unit
? is present in a segmentation with some context
c = (?l, ?r) of the form ?l??r. Features based on
the context and the unit itself parameterize u?.
In addition to scoring a segmentation based on
features, we include two priors inspired by the Min-
imum Description Length (MDL) principle sug-
gested by Poon et al (2009). The lexicon prior
favors smaller lexicons by placing an exponential
prior with negative weight on the length of the lex-
icon
?
? |?|, where |?| is the length of the unit ?
in number of phones. Minimizing the lexicon prior
favors a trivial lexicon of only the phones. The
corpus prior counters this effect, an exponential
prior with negative weight on the number of units
in each word?s segmentation, where |si| is the seg-
mentation length and |wi| is the length of the word
in phones. Learning strikes a balance between the
two priors. Using these definitions, the segmenta-
tion score u?(Y, S,W ) is given as:
1Since sub-word units can expand full-words, we refer to
both words and sub-words simply as units.
2The model can also take multiple pronunciations (?3.1).
713
s l ow b ax d ae n
s l ow
(#,#, , b, ax)
b ax
(l,ow, , d, ae)
d ae n
(b,ax, , #, #)
Figure 1: Units and bigram phone context (in parenthesis)
for an example segmentation of the word ?slobodan?.
u?(Y, S,W ) = exp
(
?
?,y
??,yf?,y(S, Y )
+
?
c,y
?c,yfc,y(S, Y )
+ ? ?
?
??S
|?|
+ ? ?
?
i?W
|si|/|wi|
)
(2)
f?,y(S, Y ) are the co-occurrence counts of the pair
(?, y) where ? is a unit under segmentation S and y
is the label. fc,y(S, Y ) are the co-occurrence counts
for the context c and label y under S. The model
parameters are ? = {??,y, ?c,y : ??, c, y}. The neg-
ative weights for the lexicon (?) and corpus priors
(?) are tuned on development data. The normalizer
Z sums over all possible segmentations and labels:
Z(W ) =
?
S?
?
Y ?
u?(Y
?, S?,W ) (3)
Consider the example segmentation for the word
?slobodan? with pronunciation s,l,ow,b,ax,d,ae,n
(Figure 1). The bigram phone context as a four-tuple
appears below each unit; the first two entries corre-
spond to the left context, and last two the right con-
text. The example corpus (Figure 2) demonstrates
how unit features f?,y and context features fc,y are
computed.
3 Model Training
Learning maximizes the log likelihood of the ob-
served labels Y ? given the words W :
`(Y ?|W ) = log
?
S
1
Z(W )
u?(Y
?, S,W ) (4)
We use the Expectation-Maximization algorithm,
where the expectation step predicts segmentations S
Labeled corpus: president/y = 0 milosevic/y = 1
Segmented corpus: p r eh z ih d ih n t/0 m ih/1 l aa/1
s ax/1 v ih ch/1
Unit-feature:Value p r eh z ih d ih n t/0:1 m ih/1:1
l aa/1:1 s ax/1:1 v ih ch/1:1
Context-feature:Value
(#/0,#/0, ,l/1,aa/1):1,
(m/1,ih/1, ,s/1,ax/1):1,
(l/1,aa/1, ,v/1,ih/1):1,
(s/1,ax/1, ,#/0,#/0):1,
(#/0,#/0, ,#/0,#/0):1
Figure 2: A small example corpus with segmentations
and corresponding features. The notation m ih/1:1
represents unit/label:feature-value. Overlapping context
features capture rich segmentation regularities associated
with each class.
given the model?s current parameters ? (?3.1), and
the maximization step updates these parameters us-
ing gradient ascent. The partial derivatives of the
objective (4) with respect to each parameter ?i are:
?`(Y ?|W )
??i
= ES|Y ?,W [fi]? ES,Y |W [fi] (5)
The gradient takes the usual form, where we en-
courage the expected segmentation from the current
model given the correct labels to equal the expected
segmentation and expected labels. The next section
discusses computing these expectations.
3.1 Inference
Inference is challenging since the lexicon prior ren-
ders all word segmentations interdependent. Con-
sider a simple two word corpus: cesar (s,iy,z,er),
and cesium (s,iy,z,iy,ax,m). Numerous segmen-
tations are possible; each word has 2N?1 possible
segmentations, where N is the number of phones in
its pronunciation (i.e., 23 ? 25 = 256). However,
if we decide to segment the first word as: {s iy,
z er}, then the segmentation for ?cesium?:{s iy,
z iy ax m} will incur a lexicon prior penalty for
including the new segment z iy ax m. If instead
we segment ?cesar? as {s iy z, er}, the segmen-
tation {s iy, z iy ax m} incurs double penalty
for the lexicon prior (since we are including two new
units in the lexicon: s iy and z iy ax m). This
dependency requires joint segmentation of the entire
corpus, which is intractable. Hence, we resort to ap-
proximations of the expectations in Eq. (5).
One approach is to use Gibbs Sampling: it-
erating through each word, sampling a new seg-
714
mentation conditioned on the segmentation of all
other words. The sampling distribution requires
enumerating all possible segmentations for each
word (2N?1) and computing the conditional prob-
abilities for each segmentation: P (S|Y ?,W ) =
P (Y ?, S|W )/P (Y ?|W ) (the features are extracted
from the remaining words in the corpus). Using M
sampled segmentations S1, S2, . . . Sm we compute
ES|Y ?,W [fi] as follows:
ES|Y ?,W [fi] ?
1
M
?
j
fi[Sj ]
Similarly, to compute ES,Y |W we sample a seg-
mentation and a label for each word. We com-
pute the joint probability of P (Y, S|W ) for each
segmentation-label pair using Eq. (1). A sampled
segmentation can introduce new units, which may
have higher probability than existing ones.
Using these approximations in Eq. (5), we update
the parameters using gradient ascent:
??new = ??old + ??`??(Y
?|W )
where ? > 0 is the learning rate.
To obtain the best segmentation, we use determin-
istic annealing. Sampling operates as usual, except
that the parameters are divided by a value, which
starts large and gradually drops to zero. To make
burn in faster for sampling, the sampler is initialized
with the most likely segmentation from the previous
iteration. To initialize the sampler the first time, we
set al the parameters to zero (only the priors have
non-zero values) and run deterministic annealing to
obtain the first segmentation of the corpus.
3.2 Efficient Sampling
Sampling a segmentation for the corpus requires
computing the normalization constant (3), which
contains a summation over all possible corpus seg-
mentations. Instead, we approximate this constant
by sampling words independently, keeping fixed all
other segmentations. Still, even sampling a single
word?s segmentation requires enumerating probabil-
ities for all possible segmentations.
We sample a segmentation efficiently using dy-
namic programming. We can represent all possible
segmentations for a word as a finite state machine
(FSM) (Figure 3), where arcs weights arise from
scoring the segmentation?s features. This weight is
the negative log probability of the resulting model
after adding the corresponding features and priors.
However, the lexicon prior poses a problem for
this construction since the penalty incurred by a new
unit in the segmentation depends on whether that
unit is present elsewhere in that segmentation. For
example, consider the segmentation for the word
ANJANI: AA N, JH, AA N, IY. If none of these units
are in the lexicon, this segmentation yields the low-
est prior penalty since it repeats the unit AA N. 3 This
global dependency means paths must encode the full
unit history, making computing forward-backward
probabilities inefficient.
Our solution is to use the Metropolis-Hastings al-
gorithm, which samples from the true distribution
P (Y, S|W ) by first sampling a new label and seg-
mentation (y?, s?) from a simpler proposal distribu-
tion Q(Y, S|W ). The new assignment (y?, s?) is ac-
cepted with probability:
?(Y ?, S?|Y, S,W )=min
?
1,
P (Y ?, S?|W )Q(Y, S|Y ?, S?,W )
P (Y, S|W )Q(Y ?, S?|Y, S,W )
?
We choose the proposal distribution Q(Y, S|W )
as Eq. (1) omitting the lexicon prior, removing the
challenge for efficient computation. The probability
of accepting a sample becomes:
?(Y ?, S?|Y, S,W )=min
?
1,
P
??S? |?|P
??S |?|
?
(6)
We sample a path from the FSM by running the
forward-backward algorithm, where the backward
computations are carried out explicitly, and the for-
ward pass is done through sampling, i.e. we traverse
the machine only computing forward probabilities
for arcs leaving the sampled state.4 Once we sample
a segmentation (and label) we accept it according to
Eq. (6) or keep the previous segmentation if rejected.
Alg. 1 shows our full sub-word learning proce-
dure, where sampleSL (Alg. 2) samples a segmen-
tation and label sequence for the entire corpus from
P (Y, S|W ), and sampleS samples a segmentation
from P (S|Y ?,W ).
3Splitting at phone boundaries yields the same lexicon prior
but a higher corpus prior.
4We use OpenFst?s RandGen operation with a costumed arc-
selector (http://www.openfst.org/).
715
0 1AA
5
AA_N_JH_AA_N
4
AA_N_JH_AA
3
AA_N_JH 2
AA_N
N_JH_AA_N
N_JH_AA
N_JH
N
6
N_JH_AA_N_IY
IY
N
AA_NAA
AA_N_IY
JH_AA_N
JH_AA
JH
JH_AA_N_IY
Figure 3: FSM representing all segmentations for the word ANJANI with pronunciation: AA,N,JH,AA,N,IY
Algorithm 1 Training
Input: Lexicon L from training text W , Dictionary D,
Mapping M , L2S pronunciations, Annealing temp T .
Initialization:
Assign label y?m = M [wm]. ??0 = 0?
S0 = random segmentation for each word in L.
for i = 1 to K do
/* E-Step */
Si = bestSegmentation(T, ?i?1, Si?1).
for k = 1 to NumSamples do
(S?k, Y
?
k) = sampleSL(P (Y, Si|W ),Q(Y, Si|W ))
S?k = sampleS(P (Si|Y ?,W ),Q(Si|Y ?,W ))
end for
/* M-Step */
ES,Y |W [fi] =
1
NumSamples
?
k f?,l[S
?
k, Y
?
k]
ES|Y ?,W [f?,l] =
1
NumSamples
?
k f?,l[S?k, Y
?]
??i = ??i?1 + ??L??(Y
?|W )
end for
S = bestSegmentation(T, ?K , S0)
Output: Lexicon Lo from S
4 OOV Detection Using Hybrid Models
To evaluate our model for learning sub-word units,
we consider the task of out-of-vocabulary (OOV)
word detection. OOV detection for ASR output can
be categorized into two broad groups: 1) hybrid
(filler) models: which explicitly model OOVs us-
ing either filler, sub-words, or generic word mod-
els (Bazzi, 2002; Schaaf, 2001; Bisani and Ney,
2005; Klakow et al, 1999; Wang, 2009); and
2) confidence-based approaches: which label un-
reliable regions as OOVs based on different con-
fidence scores, such as acoustic scores, language
models, and lattice scores (Lin et al, 2007; Burget
et al, 2008; Sun et al, 2001; Wessel et al, 2001).
In the next section we detail the OOV detection
approach we employ, which combines hybrid and
Algorithm 2 sampleSL(P (S, Y |W ), Q(S, Y |W ))
for m = 1 to M (NumWords) do
(s?m, y
?
m) = Sample segmentation/label pair for
word wm according to Q(S, Y |W )
Y ? = {y1 . . . ym?1y?mym+1 . . . yM}
S? = {s1 . . . sm?1s?msm+1 . . . sM}
?=min
(
1,
P
??S? |?|P
??S |?|
)
with prob ? : ym,k = y?m, sm,k = s
?
m
with prob (1? ?) : ym,k = ym, sm,k = sm
end for
return (S?k, Y
?
k) = [(s1,k, y1,k) . . . (sM,k, yM,k)]
confidence-based models, achieving state-of-the art
performance for this task.
4.1 OOV Detection Approach
We use the state-of-the-art OOV detection model of
Parada et al (2010), a second order CRF with fea-
tures based on the output of a hybrid recognizer.
This detector processes hybrid recognizer output, so
we can evaluate different sub-word unit lexicons for
the hybrid recognizer and measure the change in
OOV detection accuracy.
Our model (?2.1) can be applied to this task by
using a dictionary D to label words as IV (yi = 0 if
wi ? D) and OOV (yi = 1 if wi /? D). This results
in a labeled corpus, where the labeling sequence Y
indicates the presence of out-of-vocabulary words
(OOVs). For comparison we evaluate a baseline
method (Rastrow et al, 2009b) for selecting units.
Given a sub-word lexicon, the word and sub-
words are combined to form a hybrid language
model (LM) to be used by the LVCSR system. This
hybrid LM captures dependencies between word and
sub-words. In the LM training data, all OOVs are
represented by the smallest number of sub-words
which corresponds to their pronunciation. Pronun-
ciations for all OOVs are obtained using grapheme
716
to phone models (Chen, 2003).
Since sub-words represent OOVs while building
the hybrid LM, the existence of sub-words in ASR
output indicate an OOV region. A simple solution to
the OOV detection problem would then be reduced
to a search for the sub-words in the output of the
ASR system. The search can be on the one-best
transcripts, lattices or confusion networks. While
lattices contain more information, they are harder
to process; confusion networks offer a trade-off be-
tween richness (posterior probabilities are already
computed) and compactness (Mangu et al, 1999).
Two effective indications of OOVs are the exis-
tence of sub-words (Eq. 7) and high entropy in a
network region (Eq. 8), both of which are used as
features in the model of Parada et al (2010).
Sub-word Posterior =
?
??tj
p(?|tj) (7)
Word-Entropy =?
?
w?tj
p(w|tj) log p(w|tj) (8)
tj is the current bin in the confusion network and
? is a sub-word in the hybrid dictionary. Improving
the sub-word unit lexicon, improves the quality of
the confusion networks for OOV detection.
5 Experimental Setup
We used the data set constructed by Can et al
(2009) (OOVCORP) for the evaluation of Spoken
Term Detection of OOVs since it focuses on the
OOV problem. The corpus contains 100 hours of
transcribed Broadcast News English speech. There
are 1290 unique OOVs in the corpus, which were
selected with a minimum of 5 acoustic instances per
word and short OOVs inappropriate for STD (less
than 4 phones) were explicitly excluded. Example
OOVs include: NATALIE, PUTIN, QAEDA,
HOLLOWAY, COROLLARIES, HYPERLINKED,
etc. This resulted in roughly 24K (2%) OOV tokens.
For LVCSR, we used the IBM Speech Recogni-
tion Toolkit (Soltau et al, 2005)5 to obtain a tran-
script of the audio. Acoustic models were trained
on 300 hours of HUB4 data (Fiscus et al, 1998)
and utterances containing OOV words as marked in
OOVCORP were excluded. The language model was
trained on 400M words from various text sources
5The IBM system used speaker adaptive training based on
maximum likelihood with no discriminative training.
with a 83K word vocabulary. The LVCSR system?s
WER on the standard RT04 BN test set was 19.4%.
Excluded utterances amount to 100hrs. These were
divided into 5 hours of training for the OOV detec-
tor and 95 hours of test. Note that the OOV detector
training set is different from the LVCSR training set.
We also use a hybrid LVCSR system, combin-
ing word and sub-word units obtained from ei-
ther our approach or a state-of-the-art baseline ap-
proach (Rastrow et al, 2009a) (?5.2). Our hybrid
system?s lexicon has 83K words and 5K or 10K
sub-words. Note that the word vocabulary is com-
mon to both systems and only the sub-words are se-
lected using either approach. The word vocabulary
used is close to most modern LVCSR system vo-
cabularies for English Broadcast News; the result-
ing OOVs are more challenging but more realistic
(i.e. mostly named entities and technical terms). The
1290 words are OOVs to both the word and hybrid
systems.
In addition we report OOV detection results on a
MIT lectures data set (Glass et al, 2010) consisting
of 3 Hrs from two speakers with a 1.5% OOV rate.
These were divided into 1 Hr for training the OOV
detector and 2 Hrs for testing. Note that the LVCSR
system is trained on Broadcast News data. This out-
of-domain test-set help us evaluate the cross-domain
performance of the proposed and baseline hybrid
systems. OOVs in this data set correspond mainly to
technical terms in computer science and math. e.g.
ALGORITHM, DEBUG, COMPILER, LISP.
5.1 Learning parameters
For learning the sub-words we randomly selected
from training 5,000 words which belong to the 83K
vocabulary and 5,000 OOVs6. For development we
selected an additional 1,000 IV and 1,000 OOVs.
This was used to tune our model hyper parameters
(set to ? = ?1, ? = ?20). There is no overlap
of OOVs in training, development and test sets. All
feature weights were initialized to zero and had a
Gaussian prior with variance ? = 100. Each of the
words in training and development was converted to
their most-likely pronunciation using the dictionary
6This was used to obtain the 5K hybrid system. To learn sub-
words for the 10K hybrid system we used 10K in-vocabulary
words and 10K OOVs. All words were randomly selected from
the LM training text.
717
for IV words or the L2S model for OOVs.7
The learning rate was ?k =
?
(k+1+A)? , where k is
the iteration,A is the stability constant (set to 0.1K),
? = 0.4, and ? = 0.6. We used K = 40 itera-
tions for learning and 200 samples to compute the
expectations in Eq. 5. The sampler was initialized
by sampling for 500 iterations with deterministic an-
nealing for a temperature varying from 10 to 0 at 0.1
intervals. Final segmentations were obtained using
10, 000 samples and the same temperature schedule.
We limit segmentations to those including units of at
most 5 phones to speed sampling with no significant
degradation in performance. We observed improved
performance by dis-allowing whole word units.
5.2 Baseline Unit Selection
We used Rastrow et al (2009a) as our baseline
unit selection method, a data driven approach where
the language model training text is converted into
phones using the dictionary (or a letter-to-sound
model for OOVs), and a N-gram phone LM is es-
timated on this data and pruned using a relative en-
tropy based method. The hybrid lexicon includes
resulting sub-words ? ranging from unigrams to 5-
gram phones, and the 83K word lexicon.
5.3 Evaluation
We obtain confusion networks from both the word
and hybrid LVCSR systems. We align the LVCSR
transcripts with the reference transcripts and tag
each confusion region as either IV or OOV. The
OOV detector classifies each region in the confusion
network as IV/OOV. We report OOV detection accu-
racy using standard detection error tradeoff (DET)
curves (Martin et al, 1997). DET curves measure
tradeoffs between false alarms (x-axis) and misses
(y-axis), and are useful for determining the optimal
operating point for an application; lower curves are
better. Following Parada et al (2010) we separately
evaluate unobserved OOVs.8
7In this work we ignore pronunciation variability and sim-
ply consider the most likely pronunciation for each word. It
is straightforward to extend to multiple pronunciations by first
sampling a pronunciation for each word and then sampling a
segmentation for that pronunciation.
8Once an OOV word has been observed in the OOV detector
training data, even if it was not in the LVCSR training data, it is
no longer truly OOV.
6 Results
We compare the performance of a hybrid sys-
tem with baseline units9 (?5.2) and one with units
learned by our model on OOV detection and phone
error rate. We present results using a hybrid system
with 5k and 10k sub-words.
We evaluate the CRF OOV detector with two dif-
ferent feature sets. The first uses only Word En-
tropy and Sub-word Posterior (Eqs. 7 and 8) (Fig-
ure 4)10. The second (context) uses the extended
context features of Parada et al (2010) (Figure 5).
Specifically, we include all trigrams obtained from
the best hypothesis of the recognizer (a window of 5
words around current confusion bin). Predictions at
different FA rates are obtained by varying a proba-
bility threshold.
At a 5% FA rate, our system (This Paper 5k) re-
duces the miss OOV rate by 6.3% absolute over the
baseline (Baseline 5k) when evaluating all OOVs.
For unobserved OOVs, it achieves 3.6% absolute
improvement. A larger lexicon (Baseline 10k and
This Paper 10k ) shows similar relative improve-
ments. Note that the features used so far do not nec-
essarily provide an advantage for unobserved ver-
sus observed OOVs, since they ignore the decoded
word/sub-word sequence. In fact, the performance
on un-observed OOVs is better.
OOV detection improvements can be attributed to
increased coverage of OOV regions by the learned
sub-words compared to the baseline. Table 1 shows
the percent of Hits: sub-word units predicted in
OOV regions, and False Alarms: sub-word units
predicted for in-vocabulary words. We can see
that the proposed system increases the Hits by over
8% absolute, while increasing the False Alarms by
0.3%. Interestingly, the average sub-word length
for the proposed units exceeded that of the baseline
units by 0.3 phones (Baseline 5K average length
was 2.92, while that of This Paper 5K was 3.2).
9Our baseline results differ from Parada et al (2010). When
implementing the lexicon baseline, we discovered that their hy-
brid units were mistakenly derived from text containing test
OOVs. Once excluded, the relative improvements of previous
work remain, but the absolute error rates are higher.
10All real-valued features were normalized and quantized us-
ing the uniform-occupancy partitioning described in White et
al. (2007). We used 50 partitions with a minimum of 100 train-
ing values per partition.
718
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 4: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on OOVCORP data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA10
20
30
40
50
60
70
80
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 5: Effect of adding context features to baseline and discriminative hybrid systems on OOVCORP data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
Consistent with previously published results, in-
cluding context achieves large improvement in per-
formance. The proposed hybrid system (This Pa-
per 10k + context-features) still improves over the
baseline (Baseline 10k + context-features), however
the relative gain is reduced. In this case, we ob-
tain larger gains for un-observed OOVs which ben-
efit less from the context clues learned in training.
Lastly, we report OOV detection performance on
MIT Lectures. Both the sub-word lexicon and the
LVCSR models were trained on Broadcast News
data, helping us evaluate the robustness of learned
sub-words across domains. Note that the OOVs
in these domains are quite different: MIT Lec-
tures? OOVs correspond to technical computer sci-
Hybrid System Hits FAs
Baseline (5k) 18.25 1.49
This Paper (5k) 26.78 1.78
Baseline (10k) 24.26 1.82
This Paper (10k) 28.96 1.92
Table 1: Coverage of OOV regions by baseline and pro-
posed sub-words in OOVCORP.
ence and math terms, while in Broadcast News they
are mainly named-entities.
Figure 6 and 7 show the OOV detection results in
the MIT Lectures data set. For un-observed OOVs,
the proposed system (This Paper 10k) reduces the
miss OOV rate by 7.6% with respect to the base-
line (Baseline 10k) at a 5% FA rate. Similar to
Broadcast News results, we found that the learned
sub-words provide larger coverage of OOV regions
in MIT Lectures domain. These results suggest that
the proposed sub-words are not simply modeling the
training OOVs (named-entities) better than the base-
line sub-words, but also describe better novel unex-
pected words. Furthermore, including context fea-
tures does not seem as helpful. We conjecture that
this is due to the higher WER11 and the less struc-
tured nature of the domain: i.e. ungrammatical sen-
tences, disfluencies, incomplete sentences, making
it more difficult to predict OOVs based on context.
11WER = 32.7% since the LVCSR system was trained on
Broadcast News data as described in Section 5.
719
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 6: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on MIT Lectures data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 7: Effect of adding context features to baseline and discriminative hybrid systems on MIT Lectures data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
6.1 Improved Phonetic Transcription
We consider the hybrid lexicon?s impact on Phone
Error Rate (PER) with respect to the reference tran-
scription. The reference phone sequence is obtained
by doing forced alignment of the audio stream to the
reference transcripts using acoustic models. This
provides an alignment of the pronunciation variant
of each word in the reference and the recognizer?s
one-best output. The aligned words are converted to
the phonetic representation using the dictionary.
Table 2 presents PERs for the word and differ-
ent hybrid systems. As previously reported (Ras-
trow et al, 2009b), the hybrid systems achieve bet-
ter PER, specially in OOV regions since they pre-
dict sub-word units for OOVs. Our method achieves
modest improvements in PER compared to the hy-
brid baseline. No statistically significant improve-
ments in PER were observed on MIT Lectures.
7 Conclusions
Our probabilistic model learns sub-word units for
hybrid speech recognizers by segmenting a text cor-
pus while exploiting side information. Applying our
System OOV IV All
Word 1.62 6.42 8.04
Hybrid: Baseline (5k) 1.56 6.44 8.01
Hybrid: Baseline (10k) 1.51 6.41 7.92
Hybrid: This Paper (5k) 1.52 6.42 7.94
Hybrid: This Paper (10k) 1.45 6.39 7.85
Table 2: Phone Error Rate for OOVCORP.
method to the task of OOV detection, we obtain an
absolute error reduction of 6.3% and 7.6% at a 5%
false alarm rate on an English Broadcast News and
MIT Lectures task respectively, when compared to a
baseline system. Furthermore, we have confirmed
previous work that hybrid systems achieve better
phone accuracy, and our model makes modest im-
provements over a baseline with a similarly sized
sub-word lexicon. We plan to further explore our
new lexicon?s performance for other languages and
tasks, such as OOV spoken term detection.
Acknowledgments
We gratefully acknowledge Bhuvaha Ramabhadran
for many insightful discussions and the anonymous
reviewers for their helpful comments. This work
was funded by a Google PhD Fellowship.
720
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
eling. In EuroSpeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flat hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
D. Can, E. Cooper, A. Sethy, M. Saraclar, and C. White.
2009. Effect of pronounciations on OOV queries in
spoken term detection. Proceedings of ICASSP.
Stanley F. Chen. 2003. Conditional and joint models
for grapheme-to-phoneme conversion. In Eurospeech,
pages 2033?2036.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
James Glass, Timothy Hazen, Lee Hetherington, and
Chao Wang. 2010. Analysis and processing of lec-
ture audio data: Preliminary investigations. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In Proceedings of SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The det curve in assessment of
detection task performance. In Eurospeech.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for oov terms. In ASRU.
Carolina Parada, Mark Dredze, Denis Filimonov, and
Fred Jelinek. 2010. Contextual information improves
oov detection in speech. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsu-
pervised morphological segmentation with log-linear
models. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009a. A new method for OOV detection
using hybrid word/fragment system. Proceedings of
ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The ibm 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
721
