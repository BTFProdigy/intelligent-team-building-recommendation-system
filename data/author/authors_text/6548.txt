Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 93?96,
Prague, June 2007. c?2007 Association for Computational Linguistics
Generating Usable Formats for Metadata and
Annotations in a Large Meeting Corpus
Andrei Popescu-Belis and Paula Estrella
ISSCO/TIM/ETI, University of Geneva
40, bd. du Pont-d?Arve
1211 Geneva 4 - Switzerland
{andrei.popescu-belis, paula.estrella}@issco.unige.ch
Abstract
The AMI Meeting Corpus is now publicly
available, including manual annotation files
generated in the NXT XML format, but
lacking explicit metadata for the 171 meet-
ings of the corpus. To increase the usability
of this important resource, a representation
format based on relational databases is pro-
posed, which maximizes informativeness,
simplicity and reusability of the metadata
and annotations. The annotation files are
converted to a tabular format using an eas-
ily adaptable XSLT-based mechanism, and
their consistency is verified in the process.
Metadata files are generated directly in the
IMDI XML format from implicit informa-
tion, and converted to tabular format using
a similar procedure. The results and tools
will be freely available with the AMI Cor-
pus. Sharing the metadata using the Open
Archives network will contribute to increase
the visibility of the AMI Corpus.
1 Introduction
The AMI Meeting Corpus (Carletta and al., 2006)
is one of the largest and most extensively annotated
data sets of multimodal recordings of human interac-
tion. The corpus contains 171 meetings, in English,
for a total duration of ca. 100 hours. The meetings
either follow the remote control design scenario, or
are naturally occurring meetings. In both cases, they
have between 3 and 5 participants.
Perhaps the most valuable resources in this cor-
pus are the high quality annotations, which can be
used to train and test NLP tools. The existing anno-
tation dimensions include, beside transcripts, forced
temporal alignment, named entities, topic segmen-
tation, dialogue acts, abstractive and extractive sum-
maries, as well as hand and head movement and pos-
ture. However, these dimensions as well as the im-
plicit metadata for the corpus are difficult to exploit
by NLP tools due to their particular coding schemes.
This paper describes work on the generation of
annotation and metadata databases in order to in-
crease the usability of these components of the AMI
Corpus. In the following sections we describe the
problem, present the current solutions and give fu-
ture directions.
2 Description of the Problem
The AMI Meeting Corpus is publicly available at
http://corpus.amiproject.org and con-
tains the following media files: audio (headset mikes
plus lapel, array and mix), video (close up, wide
angle), slides capture, whiteboard and paper notes.
In addition, all annotations described in Section 1
are available in one large bundle. Annotators fol-
lowed dimension-specific guidelines and used the
NITE XML Toolkit (NXT) to support their task,
generating annotations in NXT format (Carletta and
al., 2003; Carletta and Kilgour, 2005). Using the
NXT/XML schema makes the annotations consis-
tent along the corpus but more difficult to use with-
out the NITE toolkit. A less developed aspect of
the corpus is the metadata encoding all auxiliary in-
formation about meetings in a more structured and
informative manner. At the moment, metadata is
spread implicitly along the corpus data, for example
93
it is encoded in the file or folder names or appears to
be split in several resource files.
We define here annotations as the time-dependent
information which is abstracted from the input me-
dia, i.e. ?higher-level? phenomena derived from
low-level mono- or multi-modal features. Con-
versely, metadata is defined as the static information
about a meeting that is not directly related to its con-
tent (see examples in Section 4). Therefore, though
not necessarily time-dependent, structural informa-
tion derived from meeting-related documents would
constitute an annotation and not metadata. These
definitions are not universally accepted, but they al-
low us to separate the two types of information.
The main goal of the present work is to facilitate
the use of the AMI Corpus metadata and annota-
tions as part of the larger objective of automating
the generation of annotation and metadata databases
to enhance search and browsing of meeting record-
ings. This goal can be achieved by providing plug-
and-play databases, which are much easier to ac-
cess than NXT files and provide declarative rather
than implicit metadata. One of the challenges in
the NXT-to-database conversion is the extraction of
relevant information, which is done here by solving
NXT pointers and discarding NXT-specific markup
to group all information for a phenomenon in only
one structure or table.
The following criteria were important when defin-
ing the conversion procedure and database tables:
? Simplicity: the structure of the tables should
be easy to understand, and should be close to
the annotation dimensions?ideally one table
per annotation. Some information can be du-
plicated in several tables to make them more
intelligible. This makes the update of this in-
formation more difficult, but as this concerns a
recorded corpus, changes are less likely to oc-
cur; if such changes do occur, they would first
be input in the annotation files, from which a
new set of tables can easily be generated.
? Reusability: the tools allow anyone to recreate
the tables from the official distribution of the
annotation files. Therefore, if the format of the
annotation files or folders changes, or if a dif-
ferent format is desired for the tables, it is quite
easy to change the tools to generate a new ver-
sion of the database tables.
? Applicability: the tables are ready to be loaded
into any SQL database, so that they can be im-
mediately used by a meeting browser plugged
into the database.
Although we report one solution here, there are
other approaches to the same problem relying, for
example, on different database structures using more
or fewer tables to represent this information.
3 Annotations: Generation of Tables
The first goal is to convert the NXT files from the
AMI Corpus into a compact tabular representation
(tab-separated text files), using a simple, declarative
and easily updatable conversion procedure.
The conversion principle is the following: for
each type of annotation, which is generally stored
in a specific folder of the data distribution, an XSLT
stylesheet converts the NXT XML file into a tab-
separated text file, possibly using information from
one or more annotations. The stylesheets resolve
most of the NXT pointers, by including redundant
information into the tables, in order to speed up
queries by avoiding frequent joins. A Perl script
applies the respective XSLT stylesheet to each an-
notation file according to its type, and generates the
global tab-separated files for each annotation. The
script also generates an SQL script that creates a re-
lational annotation database and populates it with
data from the tab-separated files. The Perl script
also summarizes the results into a log file named
<timestamp>.log.
The conversion process can be summarized as fol-
lows and can be repeated at will, in particular if the
NXT source files are updated:
1. Start with the official NXT release (or other
XML-based format) of the AMI annotations as
a reference version.
2. Apply the table generation mechanism to
XML annotation files, using XSLT stylesheets
called by the script, in order to generate tab-
ular files (TSV) and a table-creation script
(db loader.sql).
3. Create and populate the annotation database.
4. Adapt the XSLT stylesheets as needed for vari-
ous annotations and/or table formats.
94
4 Metadata: Generation of Explicit Files
and Conversion to Tabular Format
As mentioned in Section 2, metadata denotes here
any static information about a meeting, not di-
rectly related to its content. The main metadata
items are: date, time, location, scenario, partic-
ipants, participant-related information (codename,
age, gender, knowledge of English and other lan-
guages), relations to media-files (participants vs. au-
dio channels vs. files), and relations to other docu-
ments produced during the meeting (slides, individ-
ual and whiteboard notes).
This important information is spread in many
places, and can be found as attributes of a meeting
in the annotation files (e.g. start time) or obtained
by parsing file names (e.g. audio channel, camera).
The relations to media files are gathered from differ-
ent resource files: mainly the meetings.xml and
participants.xml files. An additional prob-
lem in reconstructing such relations (e.g. files gen-
erated by a specific participant) is that information
about the media resources must be obtained directly
from the AMI Corpus distribution web site, since
the media resources are not listed explicitly in the
annotation files. This implies using different strate-
gies to extract the metadata: for example, stylesheets
are the best option to deal with the above-mentioned
XML files, while a crawler script is used for HTTP
access to the distribution site. However, the solution
adopted for annotations in Section 3 can be reused
with one major extension and applied to the con-
struction of the metadata database.
The standard chosen for the explicit meta-
data files is the IMDI format, proposed by
the ISLE Meta Data Initiative (Wittenburg
et al, 2002; Broeder et al, 2004a) (see
http://www.mpi.nl/IMDI/tools), which
is precisely intended to describe multimedia
recordings of dialogues. This standard provides a
flexible and extensive schema to store the defined
metadata either in specific IMDI elements or as
additional key/value pairs. The metadata generated
for the AMI Corpus can be explored with the IMDI
BC-Browser (Broeder et al, 2004b), a tool that
is freely available and has useful features such as
search or metadata editing.
The process of extracting, structuring and storing
the metadata is as follows:
1. Crawl the AMI Corpus website and store re-
sulting metadata (related to media files) into an
XML auxiliary file.
2. Apply an XSLT stylesheet to the aux-
iliary XML file, using also the dis-
tribution files meetings.xml and
participants.xml, to obtain one IMDI
file per meeting.
3. Apply the table generation mechanism to each
IMDI file in order to generate tabular files
(TSV) and a table-creation script.
4. Create and populate metadata tables within
database.
5. Adapt the XSLT stylesheet as needed for vari-
ous table formats.
5 Results: Current State and Distribution
The 16 annotation dimensions from the public AMI
Corpus were processed following the procedure
described in Section 3. The main Perl script,
anno-xml2db.pl, applied the 16 stylesheets cor-
responding to each annotation dimension, which
generated one large tab-separated file each. The
script also generated the table-creation SQL script
db loader.sql. The number of lines of each ta-
ble, hence the number of ?elementary annotations?,
is shown in Table 1.
The application of the metadata extraction tools
described in Section 4 generated a first version of
the explicit metadata for the AMI Corpus, consist-
ing of 171 automatically generated IMDI files (one
per meeting). In addition, 85 manual files were
created in order to organize the metadata files into
IMDI corpus nodes, which form the skeleton of the
corpus metadata and allow its browsing with the
BC-Browser. The resources and tools for annota-
tion/metadata processing will be made soon avail-
able on the AMI Corpus website, along with a demo
access to the BC-Browser.
6 Discussion and Perspectives
The proposed solution for annotation conversion is
easy to understand, as it can be summarized as ?one
table per annotation dimension?. The tables pre-
serve only the relevant information from the NXT
95
Annotation dimension Nb. of entries
words (transcript) 1,207,769
named entities 14,230
speech segments 69,258
topics 1,879
dialogue acts 117,043
adjacency pairs 26,825
abstractive summaries 2,578
extractive summaries 19,216
abs/ext links 22,101
participant summaries 3,409
focus 31,271
hand gesture 1,453
head gesture 36,257
argument structures 6,920
argumentation relations 4,759
discussions 8,637
Table 1: Results of annotation conversion; dimen-
sions are grouped by conceptual similarity.
annotation files, and search is accelerated by avoid-
ing repeated joins between tables.
The process of metadata extraction and genera-
tion is very flexible and the obtained data can be eas-
ily stored in different file formats (e.g. tab-separated,
IMDI, XML, etc.) with no need to repeatedly parse
file names or analyse folders. Moreover, the ad-
vantage of creating IMDI files is that the metadata
is compliant with a widely used standard accompa-
nied by freely available tools such as the metadata
browser. These results will also help disseminating
the AMI Corpus.
As a by-product of the development of annotation
and metadata conversion tools, we performed a con-
sistency checking and reported a number of to the
corpus administrators. The automatic processing of
the entire annotation and metadata set enabled us to
test initial hypotheses about annotation structure.
In the future we plan to include the AMI Cor-
pus metadata in public catalogues, through the Open
(Language) Archives Initiatives network (Bird and
Simons, 2001), as well as through the IMDI network
(Wittenburg et al, 2004). The metadata repository
will be harvested by answering the OAI-PMH pro-
tocol, and the AMI Corpus website could become
itself a metadata provider.
Acknowledgments
The work presented here has been supported by
the Swiss National Science Foundation through the
NCCR IM2 on Interactive Multimodal Information
Management (http://www.im2.ch). The au-
thors would like to thank Jean Carletta, Jonathan
Kilgour and Mae?l Guillemot for their help in access-
ing the AMI Corpus.
References
Steven Bird and Gary Simons. 2001. Extending Dublin
Core metadata to support the description and discovery
of language resources. Computers and the Humani-
ties, 37(4):375?388.
Daan Broeder, Thierry Declerck, Laurent Romary,
Markus Uneson, Sven Stro?mqvist, and Peter Witten-
burg. 2004a. A large metadata domain of language
resources. In LREC 2004 (4th Int. Conf. on Language
Resources and Evaluation), pages 369?372, Lisbon.
Daan Broeder, Peter Wittenburg, and Onno Crasborn.
2004b. Using profiles for IMDI metadata creation. In
LREC 2004 (4th Int. Conf. on Language Resources and
Evaluation), pages 1317?1320, Lisbon.
Jean Carletta and al. 2006. The AMI Meeting Corpus:
A pre-announcement. In Steve Renals and Samy Ben-
gio, editors, Machine Learning for Multimodal Inter-
action II, LNCS 3869, pages 28?39. Springer-Verlag,
Berlin/Heidelberg.
Jean Carletta and Jonathan Kilgour. 2005. The NITE
XML Toolkit meets the ICSI Meeting Corpus: Import,
annotation, and browsing. In Samy Bengio and Herve?
Bourlard, editors, Machine Learning for Multimodal
Interaction, LNCS 3361, pages 111?121. Springer-
Verlag, Berlin/Heidelberg.
Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan Kil-
gour, Judy Robertson, and Holger Voormann. 2003.
The NITE XML Toolkit: flexible annotation for multi-
modal language data. In Behavior Research Methods,
Instruments, and Computers, special issue on Measur-
ing Behavior, 35(3), pages 353?363.
Peter Wittenburg, Wim Peters, and Daan Broeder. 2002.
Metadata proposals for corpora and lexica. In LREC
2002 (3rd Int. Conf. on Language Resources and Eval-
uation), pages 1321?1326, Las Palmas.
Peter Wittenburg, Daan Broeder, and Paul Buitelaar.
2004. Towards metadata interoperability. In NLPXML
2004 (4th Workshop on NLP and XML at ACL 2004),
pages 9?16, Barcelona.
96
Reference Resolution over a Restricted Domain: References to Documents
Andrei POPESCU-BELIS
ISSCO/TIM/ETI
University of Geneva
Bd. du Pont d?Arve 40
Geneva, CH-1211, Switzerland
andrei.popescu-belis
@issco.unige.ch
Denis LALANNE
DIUF
University of Fribourg
Ch. du Muse?e 3
Fribourg, CH-1700, Switzerland
denis.lalanne@unifr.ch
Abstract
This article studies the resolution of references
made by speakers to documents discussed during
a meeting. The focus is on transcribed record-
ings of press review meetings, in French. After
an overview of the required framework for refer-
ence resolution?specification of the task, data an-
notation, and evaluation procedure?we propose,
analyze and evaluate an algorithm for the resolu-
tion of references to documents (ref2doc) based on
anaphora tracking and context matching. Appli-
cations to speech-to-document alignment and more
generally to meeting processing and retrieval are fi-
nally discussed.
1 Introduction
The references made by the speakers to the entities
that they talk about are one of the keys to the un-
derstanding of human dialogs. When speakers dis-
cuss one or more documents, as in a press review
meeting, the references to these documents consti-
tute a significant proportion of all the occurring ref-
erences.
A computer representation of the referents is
available in this case, unlike references to more
abstract objects, since here the documents can be
stored in electronic format. Reference resolution
amounts thus to the construction of links between
each referring expression (RE) and the correspond-
ing document element. For example, if someone
says: ?I do not agree with the title of our latest re-
port?, then ?our latest report? refers to a document
available as a computer file, and ?the title of our lat-
est report? refers precisely to its title, an element that
can be retrieved from the file.
We propose here an algorithm for the resolution
of references to documents, or ref2doc. Its imple-
mentation and evaluation require a computational
framework that includes several types of data?
documents, transcriptions, and links?and an eval-
uation measure.
We summarize our view of reference resolution
over a restricted domain in Section 2. Then, we
situate the present task in the overall speech-to-
document alignment process (Section 3). The an-
notated data and the evaluation metric are described
in Section 4, along with empirical results regarding
the patterns of the REs. The resolution algorithm
is presented in Section 5, and the results obtained
in various configurations are analyzed in Section 6,
with conclusions about their relevance. Section 7
outlines the applications of the ref2doc algorithm to
the exploitation of documents in meeting processing
and retrieval applications.
2 Challenges of Reference Resolution over
a Restricted Domain
From a cognitive point of view, the role of referring
expressions in discourse is to specify the entities
about which the speaker talks. It has long been ob-
served that a more accurate view is that REs rather
specify representations of entities in the speaker?s or
hearer?s mind, an abstraction called discourse enti-
ties or DEs (Sidner, 1983; Grosz et al, 1995).
Reference resolution can be defined as the con-
struction of the discourse entities specified by re-
ferring expressions, or rather, the construction of
computational representations of DEs. This diffi-
cult but important task in discourse understanding
by computers appears to be more tractable when
enough knowledge about a domain is available to
a system (Gaizauskas and Humphreys, 1997), or
when the representations are considerably simpli-
fied (Popescu-Belis et al, 1998).
The coreference and anaphoric links, that is,
links between REs only, are somewhat different as-
pects of the phenomenon of reference (Devitt and
Sterelny, 1999; Lycan, 2000). Coreference is the re-
lation between two REs that specify the same DE.
Anaphora is a relation between two REs, called an-
tecedent RE and anaphoric RE, where the DE spec-
ified by the latter is determined by knowledge of the
DE specified by the former. In other terms, the DE
specified by the anaphoric RE cannot be fully de-
termined without knowledge of the antecedent RE.
Depending on how the referent of the second RE is
determined by the referent of the first one, the two
REs may be coreferent, as in example (1) below, or
they can be related by other referring relations, e.g.
whole/part, function/value, etc., as in (2).
1. The first articlei is particularly relevant to our
company. Iti discusses . . .
2. The first articlei is particularly relevant to our
company. The titlej suggests that we. . .
In the present case, reference resolution over
a restricted domain differs significantly both from
anaphora resolution (Mitkov, 2002) and from coref-
erence resolution (Hirschman, 1997; van Deemter
and Kibble, 2000). The REs available in the dialog
transcript must be matched against the set of poten-
tial referents or DEs, which can be derived from the
document structure. Therefore a computational rep-
resentation of the referents is here available to serve
as DEs. This advantage results directly from our
present research goal and could be later extended to
DEs derived computationally from document con-
tent, such as the persons mentioned in an article.
Reference resolution in a restricted domain
presents similarities with problems in natural lan-
guage generation (NLG) and in command dialogs,
that is, when the sets of referents are known a pri-
ori to the system. In NLG, the problem is to gen-
erate REs from existing computational descriptions
of entities?see Paraboni and van Deemter (2002)
for an application to intra-document references. In
command dialogs, the problem is to match the REs
produced by the user against the objects managed
by the interface, again known formally to the sys-
tem (Huls et al, 1995; Skantze, 2002).
3 Components of a Fully Automated
Ref2doc System
3.1 Overview
Within the overall goal of a fully automated un-
derstanding of references to documents in meet-
ing dialogs, several related sub-tasks can be dis-
tinguished, most simply envisaged as separate pro-
cesses in a computational architecture:
1. Generate a transcript of the utterances pro-
duced by each speaker.
2. Detect the REs from the transcripts that make
references to the documents of the meeting.
3. Generate a formal representation of the docu-
ments: articles, titles, etc.
4. Connect or match each RE to the document el-
ement it refers to.
Each of these components can be further subdi-
vided. Our main focus here is task (4). For this task,
an evaluation procedure, an algorithm, and its eval-
uation are provided respectively in Sections 4.3, 5,
and 6. Task (3) is discussed below in Section 3.2.1.
Task (1), which amounts more or less to auto-
mated speech recognition, is of course a standard
one, for which the performance level, as measured
by the word error rate (WER), depends on the mi-
crophone used, the environment, the type of the
meeting, etc. To factor out these problems, which
are far beyond the scope of this paper, we use
manual transcripts of recorded meetings (see Sec-
tion 4.2.1).
The present separation between tasks (2) and (4)
needs further explanations?see also (van Deemter
and Kibble, 2000; Popescu-Belis, 2003) for more
details. Our interest here is the construction of ref-
erence links between REs and document elements
(from which coreference can be inferred), so we do
not focus on task (2). Instead, we use a set of REs
identified by humans.
Task (2) is not trivial, but could be carried out
using a repertoire of pattern matching rules. The
patterns of the manually detected REs shown in Ta-
ble 1 (Section 4.4) are a first step in this direction.
The difficulty is that sometimes task (2) proposes
candidate REs, for which only task (4) can decide
whether they can really be matched to a document
element or not. For instance, REs such as pronouns
(?it?) or deictics (?this?) that refer to document ele-
ments can only be detected using a combination of
(2) and (4). This is one of our future goals.
3.2 Construction of the Logical Structure of
Documents
Inferring the structure of a document from its graph-
ical aspect is a task that can be automated with good
performances, as explained elsewhere (Hadjar et al,
2004). Here, the documents are front pages of news-
papers, in French. We first define the template of
document structures, then summarize the construc-
tion method.
3.2.1 Targeted Document Structure
Many levels of abstraction are present in the lay-
out and content of a document. They are conveyed
by its various structures: thematic, physical, log-
ical, relational or even temporal. The form of a
document, i.e. its layout and its logical structure,
carries important (and often underestimated) clues
about the content, in particular for newspaper pages,
Newspaper -> Date, Name,
MasterArticle,
Highlight*, Article+,
Other*, Filename
MasterArticle -> Title, Subheading?,
Summary*, Author*,
Source?, Content?,
Reference?, Other*,
JournalArticle*
Article -> Title, Subtitle?,
Source?, Content,
Author*, Summary*,
Reference*, Other?
JournalArticle -> Title, Source?,
Summary*, Content?,
Reference+
Highlight -> Title, Subtitle,
Reference+
Figure 1: Logical structure of a newspaper front
page (in DTD style). Terminal nodes contain text.
where articles are organized by zones, and titles are
clearly marked.
We consider that newspaper front pages have
a hierarchical structure, which can be expressed
using a very simple ontology. This is summa-
rized in Figure 1 using a DTD-like declaration,
as the document structure is encoded in XML.
For instance, the first rule in Figure 1 states
that a Newspaper front page bears the newspa-
per?s Name, the Date, one Master Article,
zero, one or more Highlights, one or more
Articles, etc. Each content element has an ID
attribute bearing a unique index.
3.2.2 Document Structure Extraction
The document structure can be extracted automat-
ically from the PDF version of a document, along
with a logical representation of the layout. Our ap-
proach merges low level extraction methods applied
to PDF files with layout analysis of a synthetically
generated TIFF image (Hadjar et al, 2004). A seg-
mentation algorithm first extracts from the image
the threads, frames and text lines, then separates
image and text zones, and finally merges lines into
homogeneous blocks. In parallel, the objects con-
tained in the PDF file (text, images, and graphics)
are extracted and matched with the result of the lay-
out analysis; for instance, text is associated to phys-
ical (graphical) blocks. Finally, the cleaned PDF is
parsed into a unique tree, which can be transformed
<dialog>
<channel id="1">
...
<er id="12">The title</er>reads...
</channel>
...
<ref2doc>
...
<ref er-id="12"
doc-file="LeMonde030404.Logic.xml"
doc-id="//Article[@ID=?3?]/Author"/>
...
</ref2doc>
</dialog>
Figure 2: Sample annotation of a dialog transcrip-
tion with ref2doc information (er stands for RE).
either into SVG or into an XML document, and used
for various applications.
4 Evaluation Method and Data
Two important elements for testing are the avail-
able data (4.2), which must be specifically annotated
(4.1), and a scoring procedure (4.3), which is quite
straightforward, and provides several scores.
4.1 Annotation Model
The annotation model for the references to docu-
ments builds upon a shallow dialog analysis model
(Popescu-Belis et al, 2004), implemented in XML.
The main idea is to add external annotation blocks
that do not alter the master resource?here the timed
meeting transcription, divided into separate chan-
nels. However, REs are annotated on the dialog
transcription itself. A more principled solution, but
more complex to implement, would be to index the
master transcriptions by the number of words, then
externalize the annotation of REs as well (Salmon-
Alt and Romary, 2004).
As shown in Figure 2, the ref pointers from
the REs to the document elements are grouped in a
ref2doc block at the end of the document, using
as attributes the index of the RE (er-id), the docu-
ment filename (doc-file), and an XPath expres-
sion (doc-id) that refers to a document element
from the XML document representation.
4.2 Annotation Procedure and Results
4.2.1 Data Recording and Transcription
A document-centric meeting room has been set up
at the University of Fribourg to record different
types of meetings. Several modalities related to
documents are recorded, thanks to a dozen cam-
eras and eight microphones. These devices are con-
trolled and synchronized by a master computer run-
ning a meeting capture and archiving application,
which helps the users organize the numerous data
files (Lalanne et al, 2004).
At the time of writing, 22 press-review meet-
ings of ca. 15 minutes each were recorded, between
March and November 2003. In such meetings, par-
ticipants discuss (in French) the front pages of one
or more newspapers of the day. Each participant
presents a selection of the articles to his/her col-
leagues, for information purposes. In general, after
a monologue of 5-10 utterances that summarize an
article, a brief discussion ensues, made of questions,
answers and comments. Then, the chair of the meet-
ing shifts the focus of the meeting to another article.
The recordings of the 22 meetings were manu-
ally transcribed using Transcriber,1 then exported as
XML files. The structure of the documents was also
encoded as XML files using the procedure described
above (3.2.1) with manual correction to ensure near
100% accuracy.
4.2.2 Ref2doc Annotation
The annotation of the ground truth references was
done directly in the XML format described above
(Figure 2). We have annotated 15 meetings with
a total of 322 REs. In a first pass, the annotator
marked the REs (with <er>...</er> tags), if
they referred to an article or to one of its parts, for
instance its title or author. However, REs that corre-
sponded only to quotations of an article?s sentences
were not annotated, since they refer to entities men-
tioned in the documents, rather than to the document
elements. Table 1 synthesizes the observed patterns
of REs.
The REs were then automatically indexed, and
a template for the ref2doc block and an HTML
view were generated using XSLT. In a second pass,
the annotator filled in directly the attributes of the
ref2doc block in the template. The annotators
were instructed to fill in, for each RE (er-id),
the name of the journal file that the RE referred to
(doc-file), and the XPath to the respective doc-
ument element (doc-id), using its ID. Examples
were provided for XPath expressions. The follow-
ing separate windows are all required for the anno-
tation:
? text/XML editor for the ref2doc block of the
dialog annotation file;
? HTML browser for the serialized HTML tran-
script (with REs in boldface);
1www.etca.fr/CTA/gip/Projets/Transcriber
? XML browser for the document structure rep-
resentation (one per document);
? PDF viewer for the actual layout of the articles
(one per document).
4.2.3 Inter-Annotator Agreement
We tested the reliability of the annotators on the sec-
ond part of their task, viz., filling in the ref2doc
blocks. The experiment involved three annotators,
for the three meetings that discuss several docu-
ments at a time, with a total of 92 REs. In a first
stage, annotation was done without any communi-
cation between annotators, only using the annota-
tion guidelines. The result was on average 96%
agreement for document assignment (that is, 3 er-
rors for 92 REs), and 90% agreement on document
elements (that is, 9 errors).2
In a second stage, we analyzed and solved some
of the disagreements, thus reaching 100% agree-
ment on document assignment, and 97% agreement
on document elements, that is only two disagree-
ments. These resulted from different interpretations
of utterances?e.g., they in ?they say. . . ? could de-
note the author, the newspaper, etc.?and could not
be solved.
This experiment shows that ref2doc annotation is
a very reliable task: referents can be clearly identi-
fied in most cases. A perfect system would match
the human performance at more than 95%.3
4.3 Evaluation Metrics
Unlike intra-document coreference resolution, for
which evaluation is a complex task (Popescu-Belis,
2003), the evaluation of reference resolution over a
specific domain is quite straightforward. One must
compare for each RE the referent found by the sys-
tem with the correct one selected by the annotators.
If the two are the same, the system scores 1, oth-
erwise it scores 0. The total score is the number
of correctly solved REs out of the total number of
REs (100% means perfect). The automatic evalua-
tion measure we implemented using the XML anno-
tation described above provides in fact three scores:
1. The number of times the document an RE
refers to is correctly identified. This is infor-
mative only when a dialog deals with more
than one document.
2These numbers were found using the evaluation software
described below (Section 4.3). Document element agreement
means here that the elements had the same ID.
3As for the first part of the process, recognizing the REs
that refer to documents, we can only hypothesize that inter-
annotator agreement is lower than for the second part.
2. The number of times the document element,
characterized by its ID attribute, is cor-
rectly identified. Here, the possible types
of document elements are article: Master-
Article, JournalArticle, Article
or Highlight.
3. The number of times the specific part of an ar-
ticle is correctly identified (e.g., content, title,
author, image, as indicated by the XPath anno-
tation in the XML output format).
The third score is necessarily lower than the sec-
ond one, and the second one is necessarily lower
than the first one. The third score is not used for the
moment, since our ref2doc algorithms do not target
sub-article elements. To help adjust the resolution
algorithm, the scoring program also outputs a de-
tailed evaluation report for each meeting, so that a
human scorer can compare the system?s output and
the correct answer explicitly.
4.4 Empirical Analysis of Occurring REs
The patterns of the annotated REs are synthesized
in Table 1 according to the type of entity they re-
fer to. This analysis attempts to derive regular ex-
pressions that describe the range of variation of the
REs that refer to documents, but without general-
izing too much. Words in capital letters represent
classes of occurring words: NEWSP are newspa-
per names, SPEC is a specifier (one or more words,
e.g., an adjective or a relative sentence), DATE and
TITLE are obvious. Items in brackets are optional,
and | indicates an exclusive-or. The patterns derived
here could be used to recognize automatically such
REs, except for two categories?anaphors and (dis-
course) indexicals?that must be disambiguated.
5 Ref2doc Algorithms
5.1 Preliminary Study
The first resolution method we implemented uses
co-occurrences of words in the speech transcript and
in the documents. More precisely, for each RE an-
notated in the transcript as referring to documents,
the words it contains and the words surrounding it
in the same utterance are matched, using the cosine
metric, with the bag of words of each logical block
of the document: article, title, author, etc. To in-
crease the importance of the words within the REs,
their weight is double the weight of the surrounding
words. The most similar logical block is considered
to be the referent of the RE, provided the similarity
value exceeds a fixed threshold (confidence level).
Referent # RE
Journal 6 (le|du) NEWSP
2 le journal
Front 33 la une NEWSP
page 6 la une DATE+NEWSP
(une) 5 (la|une) une
Article 33 (l?|le premier|le dernier) article
31 cet article
15 [l?] article suivant
14 un [petit] article SPEC
11 [un] autre article [SPEC]
7 l?article SPEC
5 [l?article] ?TITLE?
Title 10 le [grand] titre [principal]
4 (premier|second|autre) titre
Other 12 [un] autre (point|sujet|
text fait) [SPEC]
elements 10 . . . (rubrique|encart|enque?te|
page|actualite?|highlight|
analyse) . . .
5 (premier|dernier) point
3 un [petit] point [SPEC]
3 les grands points de l?actualite?
3 (le|au) point de vue [SPEC]
Graphic 11 . . . (dessin|photo|sche?ma|
elements image|figure) . . .
Authors 6 l?auteur
5 le journaliste
Anaphors 27 ils
12 il
8 l?
4 (le|au) dernier
3 autre chose [SPEC]
2 on
Indexicals 5 la`
4 c?a
4 celui-la`
2 celui-ci
2 celui SPEC
Table 1: Patterns of REs that refer to documents, in
French, ordered by the type of the referent (9 REs
out of 322 did not follow these patterns).
5.2 Algorithm based on Anaphora Tracking
A more complex algorithm was designed, which is
based on the identification of anaphoric vs. non-
anaphoric REs, as well as co-occurrences of words.
The algorithm scans each meeting transcript lin-
early (not by channel/speaker), and stores as vari-
ables the ?current document? and the ?current docu-
ment element? or article. For each RE, the algorithm
assigns first the hypothesized document, from the
list of documents associated to the meeting. REs
that make use of a newspaper?s name are consid-
ered to refer to the respective newspaper; the other
ones are supposed to refer to the current newspaper,
i.e. they are anaphors. This simple method does not
handle complex references such as ?the other news-
paper?, but obtains nevertheless a sufficient score
(see Section 6 below).
The algorithm then attempts to assign a document
element to the current RE. First, it attempts to find
out whether the RE is anaphoric or not, by match-
ing it against a list of typical anaphors found in the
meetings: ?it?, ?the article? (bare definite), ?this arti-
cle?, ?the author? (equivalents in French). If the RE
is anaphoric, then it is associated to the current arti-
cle or document element?a very simple implemen-
tation of a focus stack (Grosz et al, 1995)?except
if the RE is the first one in the meeting, which is
never considered to be anaphoric.
If the RE is not considered to be anaphoric, then
the algorithm attempts to link it to a document el-
ement by comparing the content words of the RE
with those of each article. The words of the RE
are considered, as well as those of its left and right
contexts. A match with the title of the article, or
the author name, is weighted more than one with
the content. Finally, the article that scores the most
matches is considered to be the referent of the RE,
and becomes the current document element.
Several parameters govern the algorithm, in par-
ticular the weights of the various matches?the nine
pairs generated by {RE word, left context word,
right context word} ? {title or subtitle word, au-
thor word, contents word}?and the size of the left
and right context?the number of preceding and
following utterances, and the number of words re-
tained. Evaluation provides insights about the best
values for these parameters.
6 Results and Observations
6.1 Baseline and Best Scores
We provide first some baseline scores on the set of
15 meetings and 322 REs, that is, scores of very
simple methods against which our algorithms must
be compared (rather than against a 0% score). For
RE ? document association, always choosing the
most frequent newspaper leads to 82% accuracy
(265 REs out of 322). But some meetings deal
only with one document; if we look only at meet-
ings that involve more than one newspaper, then the
score of this baseline procedure is 50% (46/92), a
much lower value. Regarding RE ? document ele-
ment association, if the referent is always the front
page as a whole (/Newspaper), then accuracy
is 16%. If the referent is always the main article
(/MasterArticle[ID=?1?]), then accuracy is
18%?in both cases quite a low value.
The word co-occurrence algorithm (described in
Section 5.1) correctly solves more than 50% of the
selected REs, in a preliminary evaluation performed
on six meetings. This simple algorithm gives inter-
esting results especially when REs belong to an ut-
terance that is thematically close to the content of
a document?s logical block. However, the method
uses only thematic linking and, furthermore, does
not take advantage of all the various document
structures.4 The 50% score should thus be consid-
ered more as another baseline.
The second algorithm (described in Section 5.2)
reaches 98% accuracy for the identification of doc-
uments referred to by REs, or 93% if we take into
account only the meetings with several documents;
remember that baseline was 82%, respectively 50%.
The accuracy for document element identification
is 73% (237 REs out of 322). If we score only
REs for which the document was correctly identi-
fied, the accuracy is 74% (236 REs out of 316), a
little higher.
6.2 Score-based Analysis of the Algorithm
The best scores quoted above are obtained when
only the right context of the RE is considered for
matching (i.e. the words after the RE), not the left
one. Also, the optimal number of words to look for
in the right context is about ten. If the right context
is not considered either, the score drops at 40%.
Regarding the weights, a match between the RE
and the title of an article appears to be more im-
portant than one between the right context and the
title, and much more important than matches with
the content of the article: weights are about 15 vs.
10 vs. 1. All these values have been determined em-
pirically, by optimizing the score on the available
data. It is possible that they change slightly when
more data is available.
If anaphor tracking is disabled, the accuracy of
document element identification drops at 65%, i.e.
35% of the REs are linked to the wrong document
element. Anaphor tracking is thus useful, though
apparently not essential: dropping it leads to an al-
gorithm close to our first attempt (Section 5.1).
Since the automatic scorer provides a detailed
evaluation report for each meeting, we are in the
4For instance, it cannot solve references related to the doc-
ument topological information (e.g. ?the figure at the bottom?),
or related to the document logical structure (e.g. ?the author of
the first article?), which need a semantic analysis of the REs.
process of analyzing the errors to find systematic
patterns, which could help us improve the algo-
rithm. Rules depending on the lexical items in the
RE seem to be required.
7 Applications
7.1 Speech to Document Alignment
The resolution of references to documents is part
of a cross-channel process aimed at detecting links
between what was said during a meeting and the
documents related to the meeting. The process en-
hances dialog and document processing, as well as
the multi-media rendering of the results. Transcript-
to-document alignment allows the generation of an
enhanced transcript which is aligned also with the
relevant documents, thanks to hyperlinks from tran-
script to document zones. Such a mechanism is in-
tegrated in the query and browsing interfaces that
we are building.
Reference-based alignment is not the only way
to align documents with the speech transcript. We
have proposed two other techniques (Mekhaldi et
al., 2003; Lalanne et al, 2004). Citation-based
alignment is a pure lexicographic match between
terms in documents and terms in the speech tran-
scription. Thematic alignment is derived from se-
mantic similarity between sections of documents
(sentences, paragraphs, logical blocks, etc.) and
units of the dialog structure (utterances, turns, and
thematic episodes). We have implemented an al-
gorithm that uses various state-of-the-art similar-
ity metrics (cosine, Jaccard, Dice) between bags of
weighted words.
For matching spoken utterances with document
logical blocks, using cosine metric, recall is 0.84,
and precision is 0.77, which are encouraging re-
sults. And when matching speech turns with log-
ical blocks, recall stays at 0.84 and precision rises
to 0.85. On the other hand, alignment of spoken ut-
terances to document sentences is less precise but
is more promising since it relies on less processing.
Using Jaccard metric, recall is 0.83, and precision is
0.76 (Lalanne et al, 2004). Thematic units have not
been considered yet, for want of reliable automatic
segmentation.
Reference-based alignment is complementary to
other methods; these could be integrated in a com-
mon framework, so that they can be consolidated
and compared. Their fusion should allow for more
robust document-to-speech alignment.
7.2 Overall Application: Meeting Processing
and Retrieval
A promising use of human dialog understanding is
for the processing and retrieval of staff or business
meetings (Armstrong et al, 2003). When meetings
deal with one or several documents, it is important
to link in a precise manner each episode or even ut-
terance of the meeting to the sections of the doc-
uments that they refer to. Considering users who
have missed a meeting or want to review a meet-
ing that they attended, this alignment is required for
two types of queries that appear in recent studies of
user requirements (Lisowska et al, 2004). First, the
users could look for episodes of a meeting in which
a particular section of a given document was dis-
cussed, so that they can learn what was said about
that section. Second, the relevant documents could
automatically be displayed when the users browse a
given episode of a meeting?so that a rich, multi-
modal context of the meeting episode is presented.
8 Conclusion
This article described a framework and an algorithm
for solving references made to documents in meet-
ing recordings by linking referring expressions to
the document elements they denote. The imple-
mentation of the algorithm, together with test data
(annotated meeting documents and transcripts) and
an evaluation metric, show that the best results are
obtained when combining anaphora tracking with a
weighted lexical matching between RE plus right
context, against title plus article contents.
An extension of the present algorithm is under
study, in which REs are processed differently ac-
cording to their type: REs explicitly referring to an
article (?the article?, ?the section?), REs referring to
positions (?the article at the bottom left?), REs refer-
ring to the entities of the contents, etc. These could
be matched to various data categories from the doc-
ument representations.
Since printed documents and spoken interaction
are two important modalities in communication, this
article is also a step towards cross-modal appli-
cations. The reference-based alignment between
transcripts and documents generates enriched tran-
scripts, with explicit information about the contents
and the timing of document mentions; conversely, it
also helps document structuring. These in turn en-
hance browsing and searching capabilities for mul-
timodal meeting processing and retrieval.
Acknowledgements
This work is part of (IM)2, Interactive Mul-
timodal Information Management, a NCCR
supported by the FNS / Swiss Govern-
ment (www.im2.ch). The authors are in-
volved in two (IM)2 projects: IM2.MDM,
Multimodal Dialogue Management (see
http://www.issco.unige.ch/projects/
im2/mdm/) and IM2.DI, Document Integration
(see http://diuf.unifr.ch/im2/).
The data we used is available from
http://diuf.unifr.ch/im2/data.html.
We thank Emmanuel Palacio, intern at ISSCO, for
his contribution to the inter-annotator agreement
test. We are also grateful to the reviewers for their
helpful suggestions.
References
Susan Armstrong, Alexander Clark, Giovanni
Coray, Maria Georgescul, Vincenzo Pallotta, An-
drei Popescu-Belis, David Portabella, Martin Ra-
jman, and Marianne Starlander. 2003. Natural
language queries on natural language data: a
database of meeting dialogues. In NLDB 2003,
Burg/Cottbus, Germany.
Michael Devitt and Kim Sterelny. 1999. Language
and Reality: an Introduction to the Philosophy
of Language. The MIT Press, Cambridge, MA,
USA, 2nd edition.
Robert Gaizauskas and Kevin Humphreys. 1997.
Using a semantic network for information extrac-
tion. Natural Languge Engineering, 3(2-3):147?
169.
Barbara J. Grosz, Aravind K. Joshi, and Scott We-
instein. 1995. Centering: A framework for mod-
eling the local coherence of discourse. Computa-
tional Linguistics, 21(2):203?225.
Karim Hadjar, Maurizio Rigamonti, Denis Lalanne,
and Rolf Ingold. 2004. Xed: a new tool for ex-
tracting hidden structures from electronic docu-
ments. In Workshop on Document Image Analy-
sis for Libraries, Palo Alto, CA, USA.
Lynette Hirschman. 1997. MUC-7 coreference task
definition 3.0. Technical report, MITRE Corp.,
13 July 1997.
Carla Huls, Wim Claassen, and Edwin Bos. 1995.
Automatic referent resolution of deictic and
anaphoric expressions. Computational Linguis-
tics, 21(1):59?79.
Denis Lalanne, Dalila Mekhaldi, and Rolf Ingold.
2004. Talking about documents: revealing a
missing link to multimedia meeting archives.
In Document Recognition and Retrieval XI -
IS&T/SPIE?s Annual Symposium on Electronic
Imaging, San Jose, CA, USA.
Agnes Lisowska, Andrei Popescu-Belis, and Susan
Armstrong. 2004. User query analysis for the
specification and evaluation of a dialogue pro-
cessing and retrieval system. In LREC 2004, Lis-
bon, Portugal.
William G. Lycan. 2000. Philosophy of Language:
a Contemporary Introduction. Routledge, Lon-
don, UK.
Dalila Mekhaldi, Denis Lalanne, and Rolf Ingold.
2003. Thematic alignment of recorded speech
with documents. In ACM DocEng 2003, Greno-
ble, France.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London, UK.
Ivandre? Paraboni and Kees van Deemter. 2002. To-
wards the generation of document deictic refer-
ences. In Kees van Deemter and Rodger Kib-
ble, editors, Information Sharing: Reference and
Presupposition in Language Generation and In-
terpretation, pages 329?352. CSLI Publications,
Stanford, CA, USA.
Andrei Popescu-Belis, Isabelle Robba, and Ge?rard
Sabah. 1998. Reference resolution beyond coref-
erence: a conceptual frame and its application.
In Coling-ACL ?98, volume II, pages 1046?1052,
Montre?al, Canada. Universite? de Montre?al.
Andrei Popescu-Belis, Maria Georgescul, Alexan-
der Clark, and Susan Armstrong. 2004. Building
and using a corpus of shallow dialogue annotated
meetings. In LREC 2004, Lisbon, Portugal.
Andrei Popescu-Belis. 2003. Evaluation-driven de-
sign of a robust reference resolution system. Nat-
ural Language Engineering, 9(3):281?306.
Susanne Salmon-Alt and Laurent Romary. 2004.
RAF: Towards a reference annotation framework.
In LREC 2004), Lisbon, Portugal.
Candace Sidner. 1983. Focusing in the compre-
hension of definite anaphora. In M. Brady and
R. Berwick, editors, Computational Models of
Discourse, pages 267?330. MIT Press, Cam-
bridge, MA.
Gabriel Skantze. 2002. Coordination of referring
expressions in multimodal human-computer dia-
logue. In ICSLP 2002, Denver, CO, USA.
Kees van Deemter and Rodger Kibble. 2000. On
coreferring: Coreference in muc and related an-
notation schemes. Computational Linguistics,
26(4):629?637.
Work-in-Progress  project report : CESTA - Machine Translation Evaluation 
Campaign 
 
Widad Mustafa El Hadi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
mustafa@univ-lille3.fr  
Marianne Dabbadie 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
dabbadie@univ-lille3.fr 
Isma?l Timimi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
timimi@univ-lille3.fr  
Martin Rajman 
LIA 
Ecole 
Polytechnique 
F?d?rale de Lausanne 
B?t. INR 
CH-1015 Lausanne 
Switzerland 
martin.rajman@epfl.ch 
 
 
Philippe Langlais 
RALI / DIRO - 
Universit? de Montr?al 
C.P. 6128, 
succursale Centre-ville 
Montr?al (Qu?bec) - 
Canada, H3C 3J7 
felipe@IRO.UMontreal.
CA
Antony Hartley 
University of Leeds 
Centre for Translation 
Studies 
Woodhouse Lane 
LEEDS LS2 9JT 
UK 
a.hartley@leeds.ac.uk
Andrei Popescu Belis 
University of Geneva 40 
bvd du Pont d'Arve CH-
1211 Geneva 4 
Switzerland 
Andrei.Popescu-
Belis@issco.unige.ch  
 
Abstract 
CESTA, the first European Campaign 
dedicated to MT Evaluation, is a project 
labelled by the French Technolangue action. 
CESTA provides an evaluation of six 
commercial and academic MT systems using a 
protocol set by an international panel of 
experts. CESTA aims at producing reusable 
resources and information about reliability of 
the metrics. Two runs will be carried out: one 
using the system?s basic dictionary, another 
after terminological adaptation. Evaluation 
task, test material, resources, evaluation 
measures, metrics, will be detailed in the full 
paper. The protocol is the combination of a 
contrastive reference to: IBM ?BLEU? 
protocol (Papineni, K., S. Roukos, T. Ward 
and Z. Wei-Jing, 2001); ?BLANC? protocol 
derived from (Hartley, Rajman, 2002).; 
?ROUGE? protocol (Babych, Hartley, Atwell, 
2003). The results of the campaign will be 
published in a final report and be the object of 
two intermediary and final workshops. 
1 Introduction 
1.1 CESTA and the Technolangue Action in 
France 
 
This article is a collective paper written by the 
CESTA scientific committee that aims at 
presenting the CESTA evaluation campaign, a 
project labelled in 2002 by the French Ministry of 
Research and Education within the framework of 
the Technolangue call for projects and integrated 
to the EVALDA evaluation platform. It reports 
work in progress and therefore is the description of 
an on-going campaign for which system results are 
not yet available.  
 
In France, EVALDA is the new Evaluation 
platform, a joint venture between the French 
Ministry of Research and Technology and ELRA 
(European Language Resources and Evaluation 
Association, Paris, France). Within the framework 
of this initiative eight evaluation projets are being 
conducted:  ARCADE II: campagne d??valuation 
de l?alignement de corpus multilingues; CESART:
 campagne d'Evaluation de Syst?mes 
d?Acquisition de Ressources Terminologiques; 
CESTA : campagne d'Evaluation de Syst?mes de 
Traduction automatique; Easy: Evaluation des 
Analyseurs Syntaxiques du fran?ais; Campagne 
EQueR, Evaluation en question-r?ponse; 
Campagne ESTER, Evaluation de transcriptions 
d??missions radio; Campagne EvaSY, Evaluation 
en synth?se vocale; and Campagne MEDIA, 
Evaluation du dialogue hors et en contexte. 
 
Regarding evaluation, the objectives of the 
Action as Joseph Mariani pointed out in his 
presentation at the LREC 2002 conference are to: 
? Improve the present evaluation 
methodologies  
? Identify new (quantitative and qualitative) 
approaches for already evaluated 
technologies:  socio-technical and psycho-
cognitive aspects  
? Identify protocols for new technologies 
and applications  
? Identification of language resources 
relevant for evaluation (to promote the 
development of new linguistic resources 
for those languages and domains where 
they do not exist yet, or only exist in a 
prototype stage, or exist but cannot be 
made available to the interested users); 
 
The object of the CESTA campaign is twofold. 
It is on the one hand to provide an evaluation of 
commercial Machine Translation Systems and on 
the other hand, to work collectively on the setting 
of a new reusable Machine Translation Evaluation 
protocol that is both user oriented and accounts for 
the necessity to use semantic metrics in order to 
make available a high quality reusable machine 
translation protocol to system providers.  
 
1.2 Object of the campaign 
The object of the CESTA campaign is to 
evaluate technologies together with metrics, i.e. to 
contribute to the setting of a state of the art within 
the field of Machine Translation systems 
evaluation.  
1.3 CESTA user oriented protocol 
The campaign will last three years, starting 
from January 2003. A board of European 
experts are members of CESTA Scientific 
committee and have been working together in 
order to determine the protocol to use for the 
campaign. Six systems are being evaluated. 
Five of these systems are commercial MT 
systems and one is a prototype developed at 
the university of Montreal by the RALI 
research centre. Evaluation is carried out on 
text rather than sentences. Text approximate 
width will be 400 words. Two runs will be 
carried out. For industrial reasons, systems 
will be made anonymous. 
 
2 State-of-the-art in the field of Machine 
Translation evaluation 
 
In 1966, the ALPAC report draws light on the 
limits of Machine Translation systems. In 1979, 
the Van Slype report presented a study dedicated to 
Machine Translation metrics.  
 
In 1992, the JEIDA campaign puts the user at the 
center of evaluator?s preoccupation. JEIDA 
proposed to draw human measures on the basis of 
three questionnaires: 
? One destined to users (containing a 
hundred questions) 
? Other questionnaires are destined to 
system Machine translation systems 
editors (three different questionnaires),  
? And a set of other questionnaires reserved 
to Machine Translation systems 
developers.  
 
Scores are worked out on the background of 
fourteen categories of questions. From these 
scores, graphs are produced according to the 
answers obtained. A comparison of different 
graphs for each systems is used as a basis for 
systems classification. 
 
The first DARPA Machine Translation 
evaluation campaign (1992-1994) makes use of 
human judgments. It is a very expensive method 
but interesting however, as regards the reliability 
of the evaluation thus produced. This campaign is 
based on tests carried out from French, Spanish 
and Japanese as source languages and English as a 
target language. The measures used for each of the 
following criteria are:  
? Fidelity ? a proximity distance is worked 
out between a source sentence and a target 
sentence on a 1 to 5 scale. 
? Intelligibility, that corresponds to 
linguistic acceptability of a translation is 
measured on a 1 to 5 evaluation scale. 
? Informativeness: the test is carried out on 
reading of the target text alone. A 
questionnaire on text informative content 
is displayed allowing to work out a 
measure calculated on the basis of the 
percentage of good answers provided in 
system translation.  
 
In 1995, the OVUM report proposes to compare 
commercial Machine Translation systems on the 
basis of ten criteria. 
 
In 1996, the EAGLES report (EAGLES, 1999) 
sets new standards for Natural Language 
Processing software evaluation on the background 
of ISO 9126.  
 
Initiated in 1999, and coordinated by Pr Antonio 
Zampolli, the ISLE project is divided into three 
working groups, one being a Machine Translation 
group.  
 
Starting from ISO 9126 standard (King, 1999b), 
the aim of the project is to produce two taxonomies 
(c.f. section 3 of this article) and : 
? One defining quality subcriteria with the 
aim of refining the six criteria defined by 
ISO 9126 (i.e. functionality, reliability, 
user-friendliness, efficiency, maintenance 
portability) 
? The second one specifying use contexts 
that define the type of task induced the use 
of a by Machine Translation system, the 
types of users and input data. This 
taxonomy uses contextual parameters to 
select and order the quality criteria subject 
to evaluation. This taxonomy can be 
viewed and downloaded on the ISSCO 
website at the following address : 
http://www.issco.unige.ch/projects/isle/fe
mti/  
 
The second DARPA campaign (Papineni, K., S. 
Roukos, T. Ward and Z. Wei-Jing, 2001), making 
use of the IBM BLEU metric is mentioned in the 
CESTA protocol (c.f. section 8.1 of this article). 
 
3 User-oriented evaluations 
An emerging evaluation methodology in NLP 
technology focuses on quality requirements 
analysis. The needs and consequently the 
satisfaction of end-users, and this will depend on 
the tasks and expected results requirement 
domains, which we have identified as diagnostic 
quality dimensions. One of the most suitable 
methods in this type of evaluation is the adequacy 
evaluation that aims at finding out whether a 
system or product is adequate to someone?s needs 
(see Sparck-Jones & Gallier, 1996 and King, 1996 
among many others for a more detailed discussion 
of these issues). This approach encourages 
communication between users and developers. 
 
The definition of the CESTA evaluation 
protocol took into account the Framework for 
MT Evaluation in ISLE (FEMTI), available 
online. FEMTI offers the possibility to define 
evaluation requirements, then to select relevant 
'qualities', and the metrics commonly used to 
score them (cf. ISO/IEC 9126, 14598). The 
CESTA evaluation methodology is founded on 
a black box approach.  
 
CESTA evaluators considered a generic user, 
which is interested in general-purpose, ready-
to-use translations, preferably using an off-the-
shelf system. In addition, CESTA aims at 
producing reusable resources, and providing 
information about the reliability of the metrics 
(validation), while being cost-effective and 
fast.  
 
With these evaluation requirements in mind 
(FEMTI-1), it appears that the relevant 
qualities (FEMTI-2) are 'suitability', 'accuracy' 
and 'well-formedness'. Automated metrics best 
meet the CESTA needs for reusability, among 
which BLEU, X-score and D-score (chosen for 
internal reasons). Their validation requires the 
comparison of their scores with recognised 
human scores for the same qualities (e.g., 
human assessment of fidelity or fluency). 
'Efficiency', measured through post-editing 
time, was also discussed. For the evaluation, 
first a general-purpose dictionary could be 
used, then a domain-specific one. 
 
 
3.1 An approach based on use cases 
 
ISO 14598 directives for evaluators put forth as 
a prequisite for systems development the detailed 
identification of user needs that ought to be 
specified through the use case document. 
Moreover, conducting a full evaluation process 
involves going through the establishment of an 
evaluation requirements document. ISO 14598 
document specifies that quality requirements 
should be identified ?according to user needs, 
application area and experience, software integrity 
and experience, regulations, law, required 
standards, etc.?. 
 
The evaluation specification document is created 
using the Software Requirement Specifications 
(SRS) and the Use-Case document. The CESTA 
protocol relies on a use case that refers to a 
translation need grounded on basic syntactic 
correctness and simple understanding of a text, as 
required by information watch tasks for example, 
and excludes making a direct use of the text for 
post editing purposes.  
4 Two campaigns 
4.1 Specificities of the CESTA campaign 
Two campaigns are being organised : 
The first campaign is organised using a system?s 
default dictionary. After systems terminological 
adaptation a second campaign will be organised. 
Two studies previously carried out and presented 
respectively at the 2001 MT Summit (Mustafa El 
Hadi, Dabbadie, Timimi, 2001) and at the 2002 
LREC conference (Mustafa Mustafa El Hadi, 
Dabbadie, Timimi, 2002) allowed us to realise the 
gap in terms in terms of quality between results 
obtained on target text after terminological 
enrichment.  
 
4.2 First campaign 
The organisation of the campaign implies going 
through several steps : 
? Identification of potential participants 
? Original protocol readjustement, 
? The setting of a specific test tool that is 
currently being be implemented in 
conformity with protocol specifications 
validated by CESTA scientific 
committee. CESTA protocol 
specifications have been 
communicated to participants in 
particular as regards data formatting, 
test schedule, metrics and adaptation 
phase. For cost requirements, CESTA 
will not include a training phase. The 
first run will start during autumn 2004 
 
4.3 Second campaign 
The systems having already been tuned, an 
adaptation phase will not be carried out for the 
second campaign. However terminological 
adaptation will be necessary at this stage. The 
second series of tests being carried out on a 
thematically homogeneous corpus, the thematic 
domain only will be communicated to participants 
for terminological adaptation. For thematic  
adaptation, and in order to avoid system 
optimisation after the first series of tests, a new 
domain specific 200.000 word hiding corpus will 
be used.  
 
The terminological domain on which evaluation 
will be carried out will then have to be defined. 
This terminological domain will be communicated 
to participants but not the corpus used itself. On 
the other hand, participants will be asked to send 
organisers a written agreement by which they will 
commit themselves to provide organisers with any 
relevant information regarding system tuning and 
specific adaptations that have made on each of the 
participating MT systems, in order to allow the 
scientific committee to understand and analyse the 
origin of the potential system ranking changes. The 
second run will start during year 2005. 
 
Organisers have committed themselves not to 
publish the results between the two campaigns. 
 
After the training phase, the second campaign 
will take place. Participants will be given a fifteen 
days delay to send the results. An additional three 
months period will be necessary to carry out result 
analysis and prepare data publication and 
workshop organisation.  
 
CESTA scientific committee also decided in 
parallel with the two campaigns, to evaluate 
systems capacity to process formatted texts 
including images and HTML tags. Participants 
who do not wish to participate to this additional 
test have informed the scientific committee. Most 
of the time the reason is that their system is only 
capable of processing raw text. This is the case 
mainly for academic systems involved in the 
campaign, most of the commercial systems being 
nowadays able to process formatted text. 
 
5 Contrastive evaluation 
One of the particularities of the CESTA protocol 
is to provide a Meta evaluation of the automated 
metrics used for the campaign ? a kind of state of 
the art of evaluation metrics. The robustness of the 
metrics will be tested on minor language pairs 
through a contrastive evaluation against human 
judgement.  
 
The scientific committee has decided to use 
Arabic?French as a minor language pair. 
Evaluation on the minor language pair will be 
carried directly on two of the participating systems 
and using English as a pivotal language on the 
other systems. Translation through a pivotal 
language will then be the following : 
Arabic?English?French.  
 
Organiser are, of course, perfectly aware of the 
potential loss of quality provoked by the use of a 
pivotal language but recall however that, contrarily 
to the major language pair, evaluation carried out 
on the minor language pair through a pivotal 
system will not be used to evaluate these systems 
themselves, but metric robustness. Results of 
metric evaluation and systems evaluation will, of 
course, be obtained and disseminated separately. 
 
During the tests of the first campaign, the 
French?English system obtaining the best ranking 
will be selected to be used as a pivotal system for 
metrics robustness Meta evaluation.  
 
6 Test material 
The required material is a set of corpora as 
detailed in the following section and a test tool that 
will be implemented according to metrics 
requirements and under the responsibility of 
CESTA organisers. 
6.1 Corpus 
 
The evaluation corpus is composed of 50 texts, 
each text length is 400 words to be translated 
twice, considering that a translation already exists 
in the original corpus. The different corpora are 
provided by ELRA. The masking corpus has 
250.000 words and must be thematically 
homogeneous.  
 
 
For each language pair the following corpora 
will be used: 
 
Adaptation 
? This 200.000 ? 250.000 word corpus is a 
bilingual corpus. It is used to validate 
exchanges between organisers and 
participants and for system tuning.  
First Campaign 
? One 20.000 word evaluation corpus will be 
used (50 texts of 400 words each) 
? One 200.000 to 250.000 word masking 
corpus that hides the evaluation corpus. 
Second campaign 
? One new 20.000 word corpus will be used 
but it will have to be thematically 
homogeneous (on a specific domain that 
will be communicated to participants a few 
months before the run takes place) 
? One masking corpus similar to the 
previous one. 
 
Additional requirement 
The BLANC metric requires the use of a 
bilingual aligned corpus at document scale. 
 
Three human translations will be used for each 
of the evaluation source texts. Considering that the 
corpora used, already provide one official 
translations, only two additional human 
translations will be necessary. These translations 
will be carried out under the organisers 
responsibility. Within the framework of CESTA 
use cases, evaluation is not made in order to obtain 
a ready to publish target language translation, but 
rather to provide a foreign user a simple access to 
information within the limits of basic grammatical 
correctness, as already mentioned in this article.  
 
7 The BLEU, BLANC and ROUGE metrics 
 
Three types of metrics will be tested on the 
corpus, the CESTA protocol being the combination 
of a contrastive reference to three different 
protocols:  
 
7.1 The IBM ?BLEU? protocol (Papineni, K., 
S. Roukos, T. Ward and Z. Wei-Jing, 
2001). 
 
The IBM BLEU metric used by the DARPA for 
its 2001 evaluation campaign, uses co-occurrence 
measures based on N-Grams. The translation in 
English of 80 Chinese source documents by six 
different commercial Machine Translation 
systems, was submitted to evaluation. From a 
reference corpus of translations made by experts, 
this metric works out quality measures according 
to a distance calculated between an automatically 
produced translation and the reference translation 
corpus based on shared N-grams (n=1,2,3?). The 
results of this evaluation are then compared to 
human judgments. 
? NIST now offers an online evaluation of 
MT systems performance, i.e.:  
o A program that can be 
downloaded for research aims. 
The user then provides source 
texts and reference translations for 
a determined pair of languages. 
o An e-mail evaluation service, for 
more formal evaluations. Results 
can be obtained in a few minutes. 
 
7.2 The ?BLANC? protocol 
 
It is a metric derived from a study presented at 
the LREC 2002 conference (Hartley A., Rajman 
M., 2002). We only take into account a part of the 
protocol described in the referred paper, i.e. the X 
score, that corresponds to grammatical correctness.  
 
We will not give an exhaustive description of 
this experience and shall only detail the elements 
that are relevant to the CESTA evaluation protocol.  
 
The protocol has been tested on the following 
languages.  
? Source language: French 
? Target language: English 
? Source corpus : 100 texts ? domain : 
newspaper articles 
 
Human judgements for comparison referential: 
? 12 English monolingual students.  
? No human translation reference corpus. 
? Three criteria were tested: Fluency, 
Adequacy, Informativeness  
 
Six systems were submitted to evaluation : 
Candide (CD), Globalink (GL), MetalSystem 
(MS), Reverso (RV), Systran (SY), XS (XS) 
? Each of the systems is due to translate a 
hundred source texts ranging from 250 to 
300 words each. A corpus of 600 
translations is thus produced. 
? For each of the source texts, a corpus of 6 
translations is produced automatically. 
These translations are then regrouped by 
series of six texts.  
? According to the protocol initiated by 
(White & Forner, 2001) these series are 
then ranked by medium adequacy score. 
? Every 5 series, a series is extracted from 
the whole. Packs of twenty series of target 
translations are thus obtained and 
submitted to human evaluators.  
 
7.2.1 Evaluators? tasks 
? Each evaluator reads 10 series of 6 
translations i.e. 60 texts.  
? Each of these series is then read by six 
different evaluators 
? The evaluators must observe a ten minute 
compulsory break every two series.  
? The evaluators do not know that the texts 
have been translated automatically. 
 
The directive given to them is the following: 
? rank these six texts from best to worst. If 
you cannot manage to give a different ranking 
to two texts, regroup them under the same 
parenthesis and give them the same score, as in 
the following example : 4 [1 2] 6 [3 5].? 
The aim of this instruction is to produce 
rankings that are similar to the rankings attributed 
automatically.  
Human judgement that ranks from best to worse 
corresponds in reality to a set of the fluency, 
adequacy and Informativeness criteria that can be 
attributed to the texts translated automatically.  
 
7.2.2 Automatically generated scores 
? X-score : syntactic score 
? D-score : semantic score 
 
Within the framework of the CESTA 
evaluation campaign the scientific committee 
decided to make use of the X-score only, the 
semantic D-score having proved to be unstable 
and that it could be advantageously replaced 
by the a metric based on (Bogdan, B.; Hartley, 
A.; Atwell, 2003), a reformulation of the D-
score developed by (Rajman, M. and T. 
Hartley, 2001), and which we refer to as the 
ROUGE metric in this article. 
 
7.2.3 X-score: definition 
? This score corresponds to a grammaticality 
metric 
? Each of the texts is previously parsed with 
XELDA Xerox parser. 
? 22 types of syntactic dependencies 
identified through the corpus of automatic 
translations. 
? The syntactic profile of each source 
document is computed. This profile is then 
used to derive the X-score for each 
document, making use of the following 
formula: 
? X-score = (#RELSUBJ+#RELSUBJPASS-
#PADJ-#ADVADJ)  
 
 
7.3 The ?ROUGE? protocol  
 
This protocol, developed by Anthony Hartley in 
(Bogdan, B.; Hartley, A.; Atwell, 2003), is a 
semantic score. It is the result of a reformulation of 
the D-Score, the semantic score initiated through 
previous collaboration with Martin Rajman 
(Rajman, M. and T. Hartley, 2001), as explained in 
the previous section.  
 
The original idea on which this protocol is based 
relies on the fact that MT evaluation metrics that 
?are based on comparing the distribution of 
statistically significant words in corpora of MT 
output and in human reference translation 
corpora?.  
 
The method used to measure MT quality is the 
following:  a statistical model for MT output 
corpora and for a parallel corpus of human 
translations, each statistically significant word 
being highlighted in the corpus. On the other hand, 
a statistical significance score is given for each 
highlighted word. Then statistical models for MT 
target texts and human translations are compared, 
special attention being paid to words that are 
automatically marked as significant in MT outputs, 
whereas they do not appear to be marked as 
significant in human translations. These words are 
considered to be ?over generated?. The same 
operation is then carried out on ?under generated 
words?. At this stage, a third operation consists in 
the marking of the words equally marked as 
significant by the MT systems and the human 
translations. The overall difference is then 
calculated for each pair of texts in the corpora. 
Three measures specifying differences in statistical 
models for MT and human translations are then 
implemented : the first one aiming at avoiding 
?over generation?, the second one aiming at 
avoiding ?under generation? and the last one being 
a combination of these two measures. The average 
scores for each of the MT systems are then 
computed.  
 
As detailed in (Bogdan, B.; Hartley, A.; Atwell, 
2003): 
 
?1. The score of statistical significance is 
computed for each word (with absolute frequency 
? 2 in the particular text) for each text in the 
corpus, as follows: ( )
][
][][][
][ ln
corpallword
foundnottxtswordcorprestwordtextword
textword P
NPP
S
?
??? ??=
 
where: 
Sword[text] is the score of statistical significance for 
a particular word in a particular text 
Pword[text] is the relative frequency of the word in 
the text; 
Pword[rest-corp] is the relative frequency of the same 
word in the rest of the corpus, without this text; 
Nword[txt-not-found] is the proportion of texts in the 
corpus, where this word is not found (number of 
texts, where it is not found divided by number of 
texts in the corpus) 
Pword[all-corp] is the relative frequency of the word 
in the whole corpus, including this particular text 
 
2. In the second stage, the lists of statistically 
significant words for corresponding texts together 
with their Sword[text] scores are compared across 
different MT systems. Comparison is done in the 
following way: 
For all words which are present in lists of 
statistically significant words both in the human 
reference translation and in the MT output, we 
compute the sum of changes of their Sword[text] 
scores: ( )? ?= ].[].[. MTtextwordreferencetextworddifftext SSS  
The score Stext.diff is added to the scores of all 
"over-generated" words (words that do not appear 
in the list of statistically significant words for 
human reference translation, but are present in 
such list for MT output). The resulting score 
becomes the general "over-generation" score for 
this particular text: 
? ?? +=
textwords
textgeneratedoverworddifftexttextgenerationover SSS
.
][...
 
The opposite "under-generation" score for 
each text in the corpus is computed by adding 
Stext.dif and all Sword[text]  scores of "under-generated" 
words ? words present in the human reference 
translation, but absent from the MT output. 
?+=?
textwords
textatedundergenerworddifftexttextgenerationunder SSS
.
][...
 
It is more convenient to use inverted scores, 
which increases as the MT system improves. These 
scores, So.text and Su.text, could be interpreted as 
scores for ability to avoid "over-generation" and 
"under-generation" of statistically significant 
words. The combined (o&u) score is computed 
similarly to the F-measure, where Precision and 
Recall are equally important: 
textgenerationover
texto S
S
.
.
1
?
= ; 
 
textgenerationunder
textu S
S
.
.
1
?
= ;
 
textutexto
textutexto
textuo SS
SSS
..
..
.&
2
+=  
The number of statistically significant words 
could be different in each text, so in order to make 
the scores compatible across texts we compute the 
average over-generation and under-generation 
scores per each statistically significant word in a 
given text. For the otext score we divide So.text by the 
number of statistically significant words in the MT 
text, for the utext score we divide Su.text by the 
number of statistically significant words in the 
human (reference) translation: 
rdsInMTstatSignWo
texto
text n
So .= ;
 
rdsInHTstatSignWo
textu
text n
Su .= ; 
 
texttext
texttext
text uo
uoou +=
2&  
The general performance of an MT system for IE 
tasks could be characterised by the average o-
score, u-score and u&o-score for all texts in the 
corpus?. 
8 Time Schedule and result dissemination 
 
The CESTA evaluation campaign started in 
January 2003 after having been labeled by the 
French Ministry of Research. During year 2003 
CESTA scientific committee went through 
protocol detailed redefinition and specification and 
a time schedule was agreed upon.  
 
2004 first semester is being dedicated to corpus 
untagging and the programming of CESTA 
evaluation tool. Reference human translations will 
also have to be produced and the implemented 
evaluation tool submitted to trial and validation.  
 
After this preliminary work, the first run will 
start during autumn 2004. At the end of the first 
campaign, result analysis will be carried out. A 
workshop will then be organized for CESTA 
participants. Then the second campaign will take 
place at the end of Spring 2005, the terminological 
adaptation phase being scheduled on a five month 
scale. 
 
After carrying out result analysis and final report 
redaction, a public workshop will be organized and 
the results disseminated and subject to publication 
at the end of 2005.  
 
9 Conclusion 
 
CESTA is the first European Campaign 
dedicated to MT Evaluation. The results of the 
campaign will be published in a final report and be 
the object of an intermediary workshop between 
the two campaigns and a final workshop at the end 
of the campaign.  
 
It is a noticeable point that the CESTA campaign 
aims at providing a state of the art of automated 
metrics in order to ensure protocol reusability. The 
originality of the CESTA protocol lies in the 
combination and contrastive use of three different 
types of measures carried out in parallel with a 
Meta evaluation of the metrics. 
 
It is also important to note that CESTA aims at 
providing a black box evaluation of available 
Machine Translation technologies, rather than a 
comparison of systems and interfaces, that can be 
tuned to match a particular need. If systems had to 
be compared, the fact that these applications 
should be compared including all software lawyers 
and ergonomic properties, ought to be taken into 
consideration.  
 
Moreover apart from providing a state of the art 
through a Meta evaluation of the metrics used in its 
protocol, thanks to the setting of this original 
protocol that relies on the contrastive use of 
complementary metrics, CESTA aims at protocol 
reusability. One of the outputs of the campaign 
will be the creation of a Machine Translation 
evaluation toolkit that will be put at users and 
system developers? disposal.Acknowledgements 
References  
Besan?on, R. and Rajman, M., (2002). Evaluation 
of aVector Space similarity measure in a 
multilingual framework. Procs. 3rd 
International Conference on Language 
Resources and Evaluation, Las 
Palmas,Spain,.1252 
Bogdan, B.; Hartley, A.; Atwell E.; Statistical 
modelling of MT output corpora for 
Information Extraction Proceedings Corpus 
Linguistics 2003, Lancaster, UK, 28-31 
March 2003, pp. 62-70 
Chaudiron, S. Technolangue. In: 
http://www.apil.asso.fr/metil.htm, mars 2001 
Chaudiron, S. L??valuation des syst?mes de 
traitement de l?information textuelle : vers un 
changement de paradigmes, M?moire pour 
l?habilitation ? diriger des recherches en sciences 
de l?information, pr?sent? devant l?Universit? de 
Paris 10, Paris, novembre 2001 
Dabbadie, M, Mustafa El Hadi, W., Timimi, I. 
(2001). Setting a Methodology for Machine 
Translation Evaluation. In: Machine 
Translation Summit VIII, ISLE/EMTA, 
Santiago de Compestela, Spain, 18-23 
October 2001, pp. 49-54. 
Dabbadie, M., Mustafa El Hadi, W., Timimi, I., 
(2002). Terminological Enrichment for non-
Interactive MT Evaluation. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 6 ? 1878-
1884 
EAGLES-Evaluation-Workgroup. (1996). 
EAGLES evaluation of natural language 
processing systems. Final report, Center for 
Sprogteknologi, Denmark, October 1996. 
EAGLES (1999). EAGLES Reports (Expert 
Advisory Group on Language Engineering 
Standards)http://www.issco.unige.ch/project
s/eagles/ewg99. 
ISLE (2001). MT Evaluation Classification, 
Expanded Classification. 
http://www.isi.edu/natural-
language/mteval/2b-MT-classification.htm. 
ISO/IEC-9126. 1991. ISO/IEC 9126:1991 (E) ? 
Information Technology ? Software 
Product Evaluation ? Quality 
Characteristics and Guidelines for Their Use. 
ISO/IEC, Geneva. 
ISO (1999). Standard ISO/IEC 9126-1 Information 
Technology ? Software Engineering ? 
Quality characteristics and sub-
characteristics. Software Quality 
Characteristics and Metrics - Part 1 
ISO (1999). Standard ISO/IEC 9126-2 Information 
Technology ? Software Engineering ? 
Software products Quality : External Metrics 
- Part 2 
ISO/IEC-14598. 1998-2001. ISO/IEC 14598 ? 
Information technology ? Software product 
evaluation ? Part 1: General overview 
(1999), Part 2: Planning and management 
(2000), Part 3: Process for developers 
(2000), Part 4: Process for acquirers (1999), 
Part 5: Process for evaluators (1998), Part 6: 
Documentation of evaluation modules 
(2001). ISO/IEC, Geneva. 
ISSCO (2001) Machine Translation Evaluation : 
An Invitation to Get Your Hands Dirty!, 
ISSCO, University of Geneva, Workshop 
organised by M. King (ISSCO) & F. Reed, 
(Mitre Corporation), April 19-24 2001. 
King (1999a) EAGLES Evaluation Working 
Group, report,http://www.issco.unige.ch/ 
projects/eagles. 
King, M. (1999b). ?ISO Standards as a Point of 
Departure for EAGLES Work in EELS 
Conference (European Evaluation of 
Language Systems), 12-13 April 1999. 
Mariani, Joseph. ?Language Technologies : 
Technolangue Action ?. Presentation. In: 
LREC'2002 International Strategy Panel17, Las 
Palmas, May 2002. 
Nomura, H. and J. Isahara. (1992). The JEIDA 
report on MT. In Workshop on MT 
Evaluation: Basis for Future Directions, San 
Diego, CA. Association for Machine 
Translation in the Americas (AMTA). 
Popescu-Belis, A. S. Manzi, and M. King. (2001). 
Towards a two-stage taxonomy for MT 
evaluation. In Workshop on MT Evaluation 
?Who did what to whom?? at Mt Summit 
VIII, pages 1?8, Santiago de Compostela, 
Spain. 
Rajman, M. and T. Hartley, (2001). Automatically 
predicting MT systems rankings compatible 
with Fluency, Adequacy or Informativeness 
scores. Procs. 4th ISLE Workshop on MT 
Evaluation, MT Summit VIII, 29-34. 
Rajman, M. and T. Hartley, (2002). Automatic 
ranking of MT systems. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 4 ? 1247-
1253 
Reeder, F., K. Miller, J. Doyon, and J. White, J. 
(2001). The naming of things and the 
confusion of tongues: an MT metric. Procs. 
4th ISLE Workshop on MT Evaluation, MT 
Summit VIII, 55-59. 
Sparck-Jones K., Gallier, J.R. (1996). Evaluating 
Natural Language Processing Systems: An 
Analysis and Review, Springer, Berlin. 
TREC, NIST Website, last updated, August 1st, 
2000, visited by the authors, 23-03-2003  
Vanni, M. and K. Miller (2001). Scaling the ISLE 
framework: validating tests of machine 
translation quality for multi-dimensional 
measurement. Procs. 4th ISLE Workshop on 
MT Evaluation, MT Summit VIII, 21-27. 
VanSlype., G. (1979). Critical study of methods 
for evaluating the quality of MT. Technical 
Report BR 19142, European Commission / 
Directorate for General Scientific and 
Technical Information Management (DG 
XIII). 
V?ronis, J., Langlais, Ph. (2000). ARCADE: 
?valuation de syst?mes d'alignement de textes 
multilingues. In Chibout, K., Mariani, J., 
Masson, N., Neel, F. ?ds., (2000). Ressources et 
?valuation en ing?nierie de la langue, Duculot, 
Coll. Champs linguistiques, et Collection 
Universit?s Francophones (AUF). 
Towards Automatic Identification of Discourse Markers
in Dialogs: The Case of Like


Sandrine Zufferey
University of Geneva
School of Translation and Interpretation (ETI)
40, bd du Pont d?Arve
CH ? 1211 Geneva, Switzerland
sandrine.zufferey@eti.unige.ch
Andrei Popescu-Belis
University of Geneva
ISSCO / TIM / ETI
40, bd du Pont d?Arve
CH ? 1211 Geneva, Switzerland
andrei.popescu-belis@
issco.unige.ch

Abstract
This article discusses the detection of dis-
course markers (DM) in dialog transcriptions,
by human annotators and by automated
means. After a theoretical discussion of the
definition of DMs and their relevance to natu-
ral language processing, we focus on the role
of like as a DM. Results from experiments
with human annotators show that detection of
DMs is a difficult but reliable task, which re-
quires prosodic information from soundtracks.
Then, several types of features are defined for
automatic disambiguation of like: colloca-
tions, part-of-speech tags and duration-based
features. Decision-tree learning shows that for
like, nearly 70% precision can be reached,
with near 100% recall, mainly using colloca-
tion filters. Similar results hold for well, with
about 91% precision at 100% recall.
1 Introduction
The identification of discourse markers (DMs) is an es-
sential step in dialog understanding, since there is often
a prosodic, syntactic and functional distinction between
DMs and the rest of an utterance. For instance, the iden-
tification of DMs is relevant to lower-level analysis
processes such as POS tagging or parsing.
After a brief theoretical definition in relation to natu-
ral language processing, this article will focus on the
highly ambiguous discourse marker like  ? which besides
a DM can also be a verb, a preposition, etc. As a DM,
like mainly fulfills one function (to introduce an ap-
proximation with a variable scope), so the main problem
in NLP is to disambiguate occurrences of like  as a DM
from other occurrences. We describe in section 5 two
experiments that assess the performance of humans on
this task in terms of inter-annotator agreement, then
proceed to automate the identification of like as a DM,
using collocation filters (section 6), a POS tagger (sec-
tion 7), and decision-tree classification (section 8),
which is also extended to the identification of well. The
automated methods appear to be useful aids to manual
annotators, since they reach 70% precision for like  with
near 100% recall.
This article is related to a dialog processing and re-
trieval application, developed within the IM2 project1.
We make use of the ICSI Meeting Recording corpus of
transcribed and annotated dialog, which contains 75
one-hour recordings of staff meetings, each involving
up to eight speakers2. Each channel is manually tran-
scribed and timed. We use here an initial release of 50
dialogs, annotated with dialog acts, segmented into
about 65,000 prosodic utterances.
2 Role of Discourse Markers in Dialog
2.1 Definition
Despite the wide research interest raised by discourse
markers for many years, there is no generally agreed
upon definition of this term. The first difficulty arises
from the fuzzy terminology used to designate these ele-
ments. Even though in English they are most often re-
ferred to as discourse markers, a variety of other names
are also used, such as discourse particles, discourse
connectives, pragmatic markers, etc. But the main prob-
lem for the study of DMs is that there seems to be no
agreement regarding which elements should be included
                                                      
1
 Interactive Multimodal Information Management, a project
sponsored by the Swiss Government. This research is related
to the Multimodal Dialogue Management module, see
http://www.issco.unige.ch/projects/im2/mdm.
2
 See http://www.icsi.berkeley.edu/Speech/mr/. We are grateful
to the ICSI-MR group for sharing the data as part of the
IM2/ICSI agreement.
in  this  class.  For  instance,  in  English,  Fraser  (1990)  has 
proposed  a  list  of  32  DMs,  but  Schiffrin  (1987)  has  only 
23.  Moreover,  these  two  lists  have  only  five  common 
elements.  The  lack  of  agreement  on  what  counts  as  a 
DM  reflects  the  great  diversity  of  approaches  used  to 
investigate  them,  resulting  from  divergent  research  in-
terests,  methods  and  goals. 
At  a  very  general  level,  it  is  nevertheless  possible  to 
formulate  a  rather  consensual  definition  of  DMs.  Fol-
lowing  Andersen  (2001,  p.  39),  discourse  markers  are  ?a 
class  of  short,  recurrent  linguistic  items  that  generally 
have  little  lexical  import  but  serve  significant  pragmatic 
functions  in  conversation.?  Items  typically  featured  in 
this  class  include  (in  English):  actually,  and,  but,  I 
mean,  like,  so,  you  know,  and  well.   
Our  study  of  DMs  and  its  application  to  natural  lan-
guage  processing  is  related  to  a  wider-scope  investiga-
tion  of  DMs  which  is  grounded  in  relevance  theory 
(Sperber  &  Wilson  1986/1995).  In  this  framework,  DMs 
encode  a  procedure  whose  role  is  to  constrain  the  infer-
ential  part  of  communication,  by  restraining  the  number 
of  hypotheses  the  hearer  has  to  consider  in  order  to  un-
derstand  the  speaker?s  meaning3.   
2.2  Importance  of  Discourse  Markers  for  NLP 
The  analysis  of  DMs  for  language  processing  is  often 
inspired  by  discourse  analysis  theories  such  as  Rhetori-
cal  Structure  Theory  (Mann  &  Thompson  1988).  In  this 
context,  DMs  are  used  to  detect  coherence  relations 
automatically  (Marcu  2000).  For  example,  so,  therefore 
and  then  are  supposed  to  indicate  a  relation  of  conclu-
sion  between  two  segments.  However,  this  analysis  of 
DMs  is  not  fine-grained  enough:  for  instance,  if  the 
three  markers  above  imply  the  same  type  of  relation, 
why  can  they  not  be  interchanged  in  every  context?   
More  recently,  DMs  have  also  been  used  as  useful 
cues  to  detect  dialog  acts  and  conversational  moves.  For 
example,  oh  implies  a  response  to  a  new  piece  of  infor-
mation  and  well   implies  a  correction  (Heeman,  Byron  & 
Allen  1998).  However,  DMs  are  then  only  partial  cues, 
since  there  is  no  one-to-one  mapping  between  the  use  of 
a  marker  and  the  presence  of  a  given  relation  (see  for 
instance  Taboada  2003). 
In  order  to  provide  a  more  precise  and  comprehen-
sive  framework  for  the  use  of  DMs  in  natural  language 
processing,  we  derived  elsewhere  a  three-step  resolution 
procedure  from  a  relevance-theoretic  analysis  (Zufferey 
2004).  These  steps  can  be  summarized  as  follows: 
1.  detect  the  occurrences  of  DMs 
2.  attach  an  inferential  procedure  to  every  marker 
3.  determine  the  scope  of  each  procedure 
                                                                                                               
3
  For  a  more  detailed  explanation  of  the  role  of  DMs  in  rele-
vance  theory,  we  refer  the  interested  reader  to  Blakemore 
(2002)  for  a  recent  survey. 
In  the  remainder  of  this  paper,  we  will  focus  only  on  the 
first  step,  i.e.  the  detection  of  DMs.  The  difficulty  of 
this  task  comes  from  the  fact  that  DMs  are  very  am-
biguous  items.  Typically,  words  like  well,  now  or  like 
can  fulfill  multiple  functions.  The  first  step  towards  a 
correct  use  of  DMs  for  language  processing  is  therefore 
to  disambiguate  them,  i.e.  to  extract  only  the  occur-
rences  of  the  respective  lexical  item  functioning  as  a 
DM  ?  in  other  words  the  pragmatic  occurrences  (see 
their  definition  for  like   in  section  3  below).  Sections  6,  7 
and  8  below  will  describe  various  automatic  methods  to 
accomplish  this  task.  Note  that  even  if  we  have 
grounded  our  approach  in  relevance  theory,  this  first 
task  is  of  paramount  importance  to  any  theory  of  dis-
course.  For  instance,  in  an  RST  framework,  DMs  can  be 
used  to  infer  coherence  relations  only  if  their  pragmatic 
occurrences  have  previously  been  identified. 
2.3  Overview  of  DM  Frequencies 
The  manual  annotation  of  DMs  in  a  subpart  of  the  ICSI 
meeting  corpus  (ca.  6  hours  and  60,000  words)  shows  a 
big  difference  in  the  frequency  of  occurrence  for  various 
DMs.  The  most  frequent  ones  are  but  (543  times),  like 
(89),  and  well   (287).  Others  are  moderately  frequent, 
e.g.,  actually   (43),  basically   (21)  or  now  (19),  while 
other  are  very  rare:  furthermore  (2),  however  (1),  more-
over  (0).  The  frequency  of  each  DM  is  relatively  stable 
across  the  meetings. 
The  frequency  of  DMs  depends  a  lot  on  the  type  of 
discourse.  For  example,  the  DM  however  is  found  much 
more  frequently  in  written  than  in  spoken  language. 
There  are  about  50  occurrences  of  however  in  the  Lon-
don-Lund  Corpus  (500,000  words,  transcription  of  spo-
ken  language)  and  about  550  occurrences  in  the 
Lancaster-Oslo/Bergen  (LOB)  corpus  (1  million  words, 
written  texts).  However  ?  like  most  other  DMs  ?  is  also 
much  more  frequent  in  dialogs  as  opposed  to  monologs. 
Another  bias  comes  from  the  type  of  activity  recorded: 
however  is  more  frequent  in  formal  settings,  such  as  in-
terviews  vs.  telephone  conversations.  And  last,  the  re-
gional  variation  of  English,  e.g.  American  vs.  British, 
can  influence  the  results.  According  to  Lenk  (1998), 
?however  is  not  used  in  spoken  American  English?.   
The  conclusion  is  that  the  frequencies  above  cannot 
be  taken  to  be  universal.  But  in  the  type  of  data  we  are 
interested  in  ?  dialogs  ?  there  is  a  high  proportion  of 
DM  like.  Besides,  in  a  greater  part  of  the  ICSI-MR  cor-
pus  (ca.  50  hours),  37%  of  the  2,116  occurrences  of  like 
correspond  to  its  use  as  a  DM.  Hence  the  necessity  to 
disambiguate  it  correctly  becomes  quite  obvious,  not 
only  to  have  a  better  pragmatic  analysis  of  occurrences 
but  also  to  improve  parsing  and  POS  tagging4. 
                                                                                                               
4
  Sometimes,  the  POS  tagging  of  a  whole  utterance  can  be  ru-
ined  by  an  incorrect  tagging  of  the  DM  (cf.  section  7),  not  to 
mention  its  parsing. 
3  The  Case  of  Like
The  discourse  marker  like  is  probably  one  of  the  most 
difficult  to  detect  automatically  because  of  the  large 
number  of  functions  of  the  word  like.  Apart  from  a  DM, 
like   can  be  used  as  a  preposition,  as  in  example  (1)  be-
low,  an  adjective  (2),  a  conjunction  (3),  an  adverb  (4),  a 
noun  (5)  and  a  verb  (6)5: 
1.  He  was  like   a  son  to  me. 
2.  Cooking,  ironing  and  like  chores. 
3.  Nobody  can  sing  that  song  like  he  did. 
4.  It?s  nothing  like   as  nice  as  their  previous 
house! 
5.  Scenes  of  unrest  the  like(s)  of  which  had  never 
been  seen  before  in  the  city. 
6.  I  like   chocolate  very  much. 
  The  DM  like  is  sometimes  analyzed  simply  as  a 
?filler?,  a  hesitation  word  like  uhmm  that  has  no  contri-
bution  to  the  meaning  of  an  utterance6.  However,  other 
studies  have  shown  that  like  has  a  much  more  complex 
role  in  dialogue.   At  a  general  level,  like  can  be  de-
scribed  as  a  ?loose  talk?  marker  (Andersen  2001).  The 
function  of  like  is  to  make  explicit  to  the  hearer  that 
what  follows  the  marker  (for  instance  a  noun  phrase)  is 
in  fact  a  loose  interpretation  of  the  speaker?s  belief. 
Consider  the  following  examples  from  the  ICSI  corpus:
1.  It  took  like  twenty  minutes.   
2.    They  had  little  carvings  of  like  dead  people  on 
the  walls  or  something.   
In  the  first  example,  by  using  like,  the  speaker  intends  to 
communicate  that  the  duration  mentioned  is  an  ap-
proximation.  In  the  second  example,  the  approximation 
concerns  the  expression  that  was  used  (?dead  people?). 
By  using  like,  the  speaker  informs  the  audience  that  this 
term  doesn?t  exactly  match  what  she  has  in  mind.  But 
like   as  a  DM  has  also  other  functions,  for  example  in-
troducing  a  quotation  (reported  speech)  and  serving  as  a 
discourse  link  introducing  a  correction  or  a  reformula-
tion7.  We  will  not  elaborate  on  these  functions,  since  the 
remainder  of  this  paper  will  be  dedicated  to  the  identifi-
cation  of  DM  like,  regardless  of  its  precise  functions. 
4  Disambiguation  of  Like  by  Humans   
Before  trying  to  extract  automatically  the  pragmatic  oc-
currences  of  like,  we  have  designed  two  experiments 
involving  human  judges.  These  preliminary  experiments 
are  useful  indicators  of  the  difficulty  of  this  task,  and 
the  human  scores  will  be  used  to  assess  more  accurately 
the  scores  obtained  by  automatic  methods  systems.   
                                                                                                               
5
  Adapted  from  the  Dictionnaire  Hachette  Oxford.  Oxford: 
OUP,  1994,  1943p. 
6
  See  for  instance  the  Collins  Cobuild  English  Language 
Dictionary    (1987:  842). 
7
  For  a  detailed  analysis  of  like,  see  Andersen  (2001). 
4.1  Description  of  the  Experiments 
In  the  first  experiment,  human  judges  used  only  the 
written  transcription  of  utterances  containing  like.  In  the 
second  experiment,  we  explored  the  possibility  to  im-
prove  the  level  of  inter-annotator  agreement  by  using 
prosodic  information:  the  human  judges  were  also  able 
to  listen  to  the  meeting  recordings.        
4.2  First  Experiment:  Annotation  Based  on  Writ-
ten  Transcription  Only 
The  first  experiment  involved  6  human  judges,  3  men 
and  3  women  whose  age  ranged  from  25  to  40.    They 
were  divided  in  two  groups  of  equal  size:  one  of  native 
English  speakers,  and  one  of  French  speakers  with  a 
very  good  knowledge  of  English. 
Every  judge  was  asked  to  annotate  a  number  of  ut-
terances  containing  like,  taken  from  two  different 
sources:  26  occurrences  came  from  the  transcription  of 
movie  dialogs  (from  Pretty  Woman)  and  49  occurrences 
corresponded  to  one  ICSI-MR  meeting.   
The  participants  were  asked  to  decide  for  every  oc-
currence  of  like  whether  it  represented  a  DM  or  not. 
They  were  also  asked  to  specify  their  degree  of  certainty 
on  a  three-point  scale  (1  =  certain,  2  =  reasonably  sure, 
3  =  hesitating).  Answers  were  simply  written  on  paper. 
At  the  beginning,  participants  received  written  indica-
tions  concerning  the  role  of  like  as  a  DM  as  well  as  ex-
amples  of  pragmatic  and  non-pragmatic  uses. 
4.3  Second  Experiment:  Use  of  Prosodic  Cues 
In  the  second  experiment,  a  group  of  3  judges  (2  French 
speakers  and  1  English  speaker)  were  asked  to  perform 
the  same  type  of  task,  but  in  addition  to  the  written  tran-
scription,  they  were  also  allowed  to  listen  to  the  re-
cording  of  the  meeting  when  needed.  This  second 
experiment  did  not  include  dialogs  from  a  movie  but 
only  from  a  one-hour  ICSI-MR  meeting,  containing  55 
occurrences  of  like8.  The  participants  received  the  same 
set  of  instructions  as  in  the  first  experiment,  and  in  addi-
tion  some  explanation  about  the  prosody  of  like   as  a 
DM.  No  time  constraints  were  imposed,  so  the  subjects 
could  listen  to  the  recording  as  many  times  as  needed. 
On  average,  they  completed  the  task  in  a  half  an  hour. 
Access  to  the  recording  was  provided  through  a  hyper-
text  transcript  synchronized  to  the  sound  file  at  the  ut-
terance  level  (a  multimedia  solution  developed  for  the 
IM2  project).   
4.4  Results  and  Discussion 
Results  show  that  annotating  DMs  is  a  difficult  task 
even  for  human  judges.  In  the  first  experiment,  the  level 
                                                                                                               
8
  Two  of  the  participants  had  already  participated  in  the  first 
experiment,  but  the  meeting  was  not  the  one  used  in  the  previ-
ous  experiment. 
of  inter-annotator  agreement  measured  by  the  Kappa 
coefficient  is  quite  low  (?  =  0.40)  for  the  natural  dialogs 
of  the  ICSI-MR  corpus,  and  average  for  the  movie  tran-
scription  (?  =  0.65)  9.  In  the  second  experiment,  with  the 
help  of  prosodic  cues,  inter-annotator  agreement  in-
creases,  and  the  annotation  becomes  much  more  reliable 
(?  =  0.74).    Therefore,  the  identification  of  DM  like  is 
an  empirically  valid  task,  which  can  be  accomplished  at 
a  reasonable  performance  level  by  untrained  annotators. 
However,  access  to  the  prosodic  information  (from  re-
cordings)  appears  to  be  required.  The  inter-annotator 
agreement  scores  also  set  an  initial  boundary  on  auto-
matic  performances,  which  should  not  be  expected  to 
reach  much  higher  levels.  These  results  should  be  con-
firmed  by  experiments  on  longer  transcripts,  involving 
also  annotators  with  specific  training  for  DMs. 
The  results  obtained  in  these  experiments  shed  an  in-
teresting  empirical  light  on  a  number  of  predictions  that 
were  made  before  the  experiments. 
First,  it  appears  that  DMs  are  easier  to  annotate  in 
pre-planned  dialogs,  because  such  dialogs  are  less  am-
biguous  than  the  natural  ones.  Indeed,  the  level  of 
agreement  reached  for  the  movie  transcription  is  much 
higher  than  for  the  ICSI-MR  meeting  in  the  same  condi-
tions  (0.65  vs.  0.42).  This  result  confirms  that  even  if 
movie  dialogs  are  made  to  reproduce  the  naturalness  of 
naturally  occurring  dialogs,  they  are  never  as  ambigu-
ous,  mainly  because  they  only  reflect  the  global  com-
municative  intention  of  one  person  (the  author).   
The  second  hypothesis  we  tested  concerned  the  dif-
ference  between  native  and  non-native  speakers?  ability 
to  annotate  DMs.  We  believed  that  the  group  of  native 
English  speakers  would  have  a  better  level  of  agree-
ment.  This  prediction  has  not  been  confirmed:  the  group 
of  non-native  English  speakers  obtained  nearly  the  same 
level  of  agreement  as  the  native  English  speakers,  for 
both  types  of  corpora:  ?  =  0.67  vs.   ?  =  0.63  for  the 
movie  transcription  and  ?  =  0.4  vs.  ?  =  0.43  for  the 
meeting  corpus.  So  it  seems  that  non-native  English 
speakers  with  a  very  good  command  of  English  are  just 
as  reliable  as  native  English  speakers  to  annotate  DMs.     
The  third  prediction  we  have  tested  is  the  possible 
correlation  between  the  degree  of  certainty  of  annotators 
and  the  level  of  agreement.  We  haven?t  been  able  to 
find  any  significant  correlation  on  both  types  of  corpora 
and  in  both  experiments.  Thus,  the  capacity  of  human 
judges  to  evaluate  their  own  intuition  doesn?t  seem  to  be 
very  high  for  this  task.  However,  it  should  be  mentioned 
that  in  general,  the  subjects  have  been  much  more  con-
fident  in  the  second  experiment,  when  they  were  able  to 
use  prosodic  cues.  The  percentage  of  answers  given 
                                                                                                               
9
  We  use  Krippendorff?s  scale  to  assess  intercoder  agreement. 
This  scale  discounts  any  result  with  ?   <  0.67,  allows  tentative 
conclusions  when  0.67  <  ?   <  0.8  and  definite  conclusions 
when  ?   ?  0.8. 
with  maximal  certainty  by  the  two  annotators  who  took 
part  in  both  experiments  grew  from  45%  to  60%  and 
from  65%  to  87%  respectively. 
When  looking  more  closely  at  the  utterances  upon 
which  annotators  do  not  agree,  we  can  see  that  some 
types  of  occurrences  of  like  seem  to  be  much  more  dif-
ficult  to  annotate  in  both  experiments.  In  most  of  these 
cases,  like   had  the  function  of  a  preposition.  For  exam-
ple,  one  subject  was  mistaken  in  annotating  all  occur-
rences  of  the  type:  sounds  like,  seems  like,  feels  like,  as 
DMs.  This  observation  is  not  so  surprising  if  we  bear  in 
mind  that  the  pragmatic  uses  of  like  seem  to  have 
emerged  (historically)  in  a  grammaticalization  process. 
According  to  Andersen  (2001,  p.  294):  ?the  fundamen-
tal  assumption  here  is  that  the  pragmatic  marker  like 
originates  in  a  lexical  item,  that  is,  a  preposition  with 
the  inherent  meaning  ?similar  to??.  This  suggests  that 
more  detailed  explanations  regarding  the  role  of  the  DM 
like   as  well  as  some  more  training  would  probably  im-
prove  the  reliability  of  annotation.   
To  sum  up,  these  two  experiments  have  enabled  us 
to  quantify  the  level  of  agreement  between  human  anno-
tators  and  to  confirm  the  usefulness  of  prosodic  cues  in 
order  to  efficiently  detect  the  DM  like. 
5  Automatic  Detection  of  Like  as  a  DM 
5.1  A Priori  Cues 
We  have  defined  three  linguistic  criteria  to  be  used  for 
the  disambiguation  of  DMs  in  general,  which  we  will 
apply  to  the  disambiguation  of  like  in  section  6  below.   
The  first  criterion  is  the  presence  of  collocations. 
For  instance,  when  well   is  used  to  mark  a  change  of 
topic,  it  is  nearly  always  used  in  a  cluster  of  markers 
such  as:  well  you  know,  well   now,  well  I  think  or  oh 
well.  On  the  contrary,  when  used  to  close  a  topic,  well  
can  very  often  be  found  in  clusters  like  OK  well   or  well 
anyway/anyhow.  The  criterion  of  collocations  can  also 
be  applied  the  other  way  round,  to  establish  cases  where 
a  given  element  cannot   be  a  DM.  For  instance,  when 
like   is  used  in  collocations  such  as:  I/you  like, 
seems/feels  like,  just  like;  or  when  well  is  used  in  con-
structions  like:  very  well,  as  well,  quite  well,  etc.   
The  second  criterion  is  the  position  in  the  utterance. 
Again,  depending  on  the  word,  this  criterion  can  be  used 
to  ascertain  that  an  element  is  a  DM  or,  on  the  contrary, 
to  rule  out  this  possibility.  For  instance,  well   as  a  DM  is 
nearly  always  placed  at  the  beginning  of  an  utterance  or 
at  least,  at  the  beginning  of  a  prosodic  unit.  In  other 
cases,  the  use  of  this  criterion  implies  that  to  be  a  DM, 
an  element  must  not  commence  the  utterance.  Accord-
ing  to  Aijmer  (2002,  p.  30)  :  ?Some  of  the  discourse 
particles  [?]  (actually,  sort  of)  can,  for  instance,  be  in-
serted  parenthetically  or  finally,  often  with  little  differ-
ence  in  meaning,  after  a  sentence,  clause,  turn,  tone  unit 
as  a  post-end  field  constituent.? 
The  third  criterion  is  prosody.  According  to  Schif-
frin  (1987,  p.  328)  ?[a  discourse  particle]  has  to  have  a 
range  of  prosodic  contours  e.g.  tonic  stress  and  followed 
by  a  pause,  phonological  reduction?.   
However,  even  though  these  three  criteria  can  help  a 
human  annotator  to  extract  DMs  successfully  most  of 
the  time,  some  rare  occurrences  remain  ambiguous. 
Some  occurrences  are  at  the  boundary  between  a  prag-
matic  and  a  non-pragmatic  use.  In  these  rare  cases,  both 
interpretations  remain  equally  possible. 
5.2  Application  of  A Priori  Cues  to  NLP 
Some  of  the  criteria  we  propose  seem  relatively  easy  to 
automate.  For  instance,  it  is  rather  easy  to  extract  a  set  a 
collocations  once  a  list  is  made.  Although  some  colloca-
tions  imply  the  presence  of  a  DM,  and  some  other  its 
absence,  in  some  cases  this  criterion  is  in  fact  much 
more  efficient  in  its  second  form,  to  rule  out  the  pres-
ence  of  a  DM.  It  is  also  rather  easy  to  automate  the  cri-
terion  involving  a  certain  position  in  the  utterance, 
especially  when  the  position  is  strongly  constrained  (for 
instance,  at  the  beginning  or  end  of  the  utterance).  As 
far  as  prosody  is  concerned,  the  detection  of  pitch  varia-
tions  (for  instance  amounting  to  a  correct  transcription 
of  commas)  seems  feasible  for  good  quality  recordings. 
However,  used  independently  from  the  others,  none 
of  these  criteria  can  suffice  to  completely  automate  the 
extraction  of  DMs,  even  though  in  some  cases  a  single 
criterion  can  be  enough  to  get  good  results.  For  exam-
ple,  in  the  case  of  well,  the  position  in  the  utterance  can 
often  be  sufficient  to  correctly  extract  a  significant  pro-
portion  of  all  occurrences.  Nevertheless,  it  will  not  solve 
all  occurrences,  since  well   is  not  always  used  at  the  be-
ginning  of  an  utterance  but  also  at  the  beginning  of  a 
prosodic  phrase,  as  in:  ?And  I  said,  well   I  have  to  think 
about  it?.  In  these  cases,  the  use  of  prosody  to  detect 
prosodic  phrases  becomes  necessary.  Similarly,  the  ex-
clusion  of  some  collocations  like  very  well,  as  well,  etc. 
is  necessary  to  solve  the  last  problematic  cases. 
In  sum,  these  criteria  seem  to  be  sufficient  to  par-
tially  automate  the  disambiguation  of  DMs,  which  could 
serve  to  reduce  the  burden  of  human  annotators.   
5.3  Evaluation  of  NLP  Performance 
The  evaluation  of  DM  detection  requires  a  ?gold  stan-
dard?  (correct  annotation)  and  the  implementation  of 
comparison  metrics.  The  correct  annotation  of  DMs  was 
discussed  in  the  experiments  above,  in  the  case  of  like,  a 
highly  versatile  marker.  In  order  to  have  enough  data  for 
our  NLP  experiment,  one  of  the  authors  annotated 
manually  all  occurrences  of  like  in   50  one-hour  dialogs 
from  the  ICSI-MR  corpus,  generating  2,116  occurrences 
of  like,  of  which  792  are  DMs.  About  20  occurrences  of 
like   could  not  be  reliably  disambiguated  and  were  re-
moved  from  the  reference  annotation. 
We  have  already  compared  the  annotations  produced 
by  human  judges  using  the  kappa  metric.  This  metric 
can  be  used  as  well  to  score  the  performances  of  a  sys-
tem  at  distinguishing  pragmatic  from  non  pragmatic 
uses.  Note  that  kappa  compensates  the  scores  by  taking 
into  account  the  probability  of  agreement  by  chance.  A 
simpler  but  useful  metric  is  the  percentage  of  occur-
rences  correctly  identified,  or  accuracy.  Unlike  kappa, 
accuracy  does  not  factor  out  agreement  by  chance,  but 
provides  a  more  interpretable  score.10  
Furthermore,  if  the  task  to  be  evaluated  is  the  re-
trieval  of  pragmatic  uses  among  all  uses  of  the  lexical 
item  (which  are  trivial  to  detect),  then  recall  and  preci-
sion  are  also  relevant.  For  instance,  to  evaluate  tech-
niques  that  filter  out  non-DMs,  we  will  require  them  to 
reach  nearly  100%  recall,  and  a  reasonable  precision  ? 
say,  more  than  0.6  or  0.7  for  like,  i.e.  twice  the  baseline 
precision,  which  is  the  frequency  of  the  DM  use.   
6  Filters  for  the  Disambiguation  of  Like 
We  first  explore  the  possibility  to  use  a  list  of  colloca-
tions  in  order  to  identify  occurrences  of  like  as  a  DM  in 
two  different  corpora,  ICSI-MR  and  a  transcription  of 
Switchboard  telephone  conversations.  The  best  use  of 
this  criterion  is  to  maximize  precision  while  keeping 
recall  as  close  as  possible  to  100%,  i.e.  to  rule  out  a 
maximal  number  of  occurrences  that  are  not  pragmatic 
while  keeping  all   the  pragmatic  ones.  Such  a  partial 
identification  can  be  used  as  a  filter  to  reduce  the  num-
ber  of  occurrences  that  must  be  processed  manually.   
The  list  of  collocations  that  exclude  the  presence  of 
a  DM  contains  for  example  collocations  such  as:  some-
thing  like  that,  I  like,  looks  like,  etc.  The  full  list  con-
tains  26  collocations  and  was  tested  on  two  different 
corpora:  first,  on  a  subpart  of  the  ICSI-MR  corpus,  with 
6  hours  of  recording,  and  approximately  60,000  words; 
then  on  the  Switchboard  data,  transcribed  and  annotated 
with  DMs  (Meteer  1995),  with  ca.  2,500  conversations 
and  about  3  million  words. 
Our  method  reaches  0.75  precision  with  100%  recall 
on  the  ICSI-MR  corpus,  and  0.44  precision  with  0.99 
recall  on  Switchboard.  The  main  goal  of  the  filter  is  thus 
achieved:  recall  remains  very  high  on  both  corpora.  A 
precision  of  0.75  for  ICSI-MR  means  that  a  significant 
number  of  occurrences  are  correctly  ruled  out  ?  the  ini-
tial  proportion  of  pragmatic  uses  is  about  1/3,  while  af-
                                                                                                               
10
  Note  that  the  probability  of  agreement  by  chance  is  here 
close  to  0.5,  given  that  20?40%  of  the  occurrences  of  like  are 
DMs.  When  the  proportion  of  DM  occurrences  is  ?,  the  prob-
ability  of  agreement  by  chance  is  (?2  +  (1  ?  ?)2),  hence  0.68 
for  20%  and  0.52  for  40%. 
ter  the  application  of  the  filter  it  reaches  3/4,  and  none 
of  the  pragmatic  uses  was  missed  in  the  process. 
The  efficiency  of  the  filter  is  smaller  on  the 
Switchboard  data  (0.44  precision  vs.  0.75  for  ICSI).  In 
the  ICSI-MR  corpus,  the  precision  obtained  is  probably 
the  highest  possible  one  with  this  filter,  since  the  corpus 
was  used  as  a  development  corpus,  from  which  we  have 
extracted  our  set  of  collocations.  On  the  other  hand,  in 
the  Switchboard  corpus,  the  lower  precision  might  also 
be  due  to  the  incoherent  annotation.  We  used  indeed  the 
annotation  of  DMs  that  was  already  present  in 
Switchboard,  and  this  annotation  is  not  entirely  reliable. 
In  fact,  no  real  theoretical  assumptions  seem  to  underlie 
this  annotation  and  according  to  Meteer  (1995)  the  crite-
rion  to  decide  if  an  ambiguous  case  was  a  DM  was 
?[?]  if  the  speaker  is  a  heavy  discourse  like   user,  count 
ambiguous  cases  as  discourse  markers,  if  not,  assume 
they  are  not.?  In  such  circumstances,  we  can  expect  that 
the  low  precision  of  our  system  on  Switchboard  can  at 
least  be  partly  attributed  to  this  lack  of  reliability. 
Finally,  our  system  has  performed  the  same  task  as 
human  judges  in  the  first  experiment  (see  section  4)  on 
49  occurrences  of  like   in  one  ICSI-MR  meeting.  Inter-
estingly,  if  we  compare  the  average  kappa  obtained  be-
tween  humans  and  the  kappa  obtained  between  the 
system  and  all  human  judges,  we  get  the  same  value  (? 
=  0.42).  Even  though  the  results  obtained  by  this  pre-
liminary  system  are  quite  tentative,  this  comparison 
with  human  judges  seems  to  indicate  that  the  perform-
ance  is  quite  acceptable. 
7  Use  of  a  Part-of-speech  Tagger   
The  use  of  a  POS  tagger  for  disambiguating  pragmatic 
vs.  non-pragmatic  uses  of  like   is  a  straightforward  idea. 
Indeed,  if  the  accuracy  of  the  taggers  on  colloquial 
speech  transcripts  was  very  high,  this  would  help  filter-
ing  out  many  (if  not  all)  of  the  non-pragmatic  uses,  such 
as  cases  when  like   is  simply  a  verb. 
We  experimented  using  QTag,  a  freely  available 
probabilistic  POS  tagger  for  English  (Mason  2000)11. 
The  tagger  assigns  one  of  the  following  tags  to  occur-
rences  of  like:  preposition  (IN,  1,412  occurrences),  verb 
(VB,  509),  subordinative  conjunction  (CS,  134),  general 
adjective  (JJ,  52),  and  general  adverb  (RB,  9).   
These  tags  must  then  be  interpreted  in  terms  of  DM 
uses.  A  simple  attempt  is  to  use  the  tagger  as  a  filter,  to 
remove  verbal  occurrences.  Hence,  a  VB  tag  is  inter-
preted  as  non-DM,  and  all  the  other  tags  as  (possible) 
DMs.  Unfortunately,  the  evaluation  shows  that  such  a 
filter  is  unreliable:  recall  is  0.77,  precision  is  0.38,  accu-
racy  44%,  and  kappa  is  only  0.02,  i.e.  near  random  cor-
                                                                                                               
11QTag  uses  a  variant  of  the  Brown/UPenn  tagsets,  and  was 
trained  on  a  million-word  subset  of  the  BNC  (written 
material):  http://web.bham.ac.uk/o.mason/software/tagger/. 
relation.  As  expected,  other  interpretations  of  the  tags 
do  not  lead  to  better  overall  results.  The  most  significant 
figures  are  obtained  when  selecting  only  adjectival  uses 
of  like  (tagged  JJ)  as  potential  DMs:  the  recall  is  of 
course  very  low,  but  precision  is  0.74,  which  means  that 
the  JJ  tag  could  be  used  as  a  cue  for  the  presence  of  a 
DM  use.  
The  main  reason  that  explains  the  failure  of  the  tag-
ger  to  detect  DM  uses  of  like   is  that  it  was  not  trained  on 
speech  transcription,  where  like   is  quite  frequent.  A  tag-
ger  trained  on  speech  (supposing  annotated  data  is 
available)  could  use  some  punctuation  from  the  tran-
scription  to  improve  its  accuracy,  such  as  marks  for  in-
terruptions  and  pauses  that  sometimes  appear  around 
DM  uses  of  like.  This  could  help  it  to  avoid  marking 
some  of  those  occurrences  as  VB.  A  study  by  Heeman, 
Byron  and  Allen  (1997)  has  shown  that  when  specific 
tags  are  assigned  to  DMs  and  the  tagging  is  done  in  the 
process  of  speech  recognition,  both  the  quality  of  tag-
ging  and  the  correct  identification  of  DMs  are  signifi-
cantly  improved. 
8  Statistical  Training  of  DM  Classifiers 
The  relevance  of  machine  learning  techniques  to  detect 
DMs  and  to  improve  manually-derived  classification 
models  has  already  been  emphasized  by  Litman  (1996). 
We  have  conducted  machine  learning  experiments  with 
the  2,116-occurrence  data  set,  and  confirmed  the  rele-
vance  of  the  filters  defined  in  section  6  above,  and  the 
role  of  several  additional  features.  The  results  obtained 
with  like  are  also  compared,  at  the  end  of  this  section, 
with  an  analysis  on  well   as  a  DM. 
8.1  Features  for  the  Classification  of  Like  
For  each  occurrence  of  like,  we  extracted  the  following 
features  that  we  thought  relevant  to  the  DM/non-DM 
classification  problem: 
? presence  of  a  collocation  that  rules  out  the  occur-
rence  as  a  DM;  since  like   can  be  either  the  first 
word  or  the  second  word  in  the  collocation,  we 
separated  this  into  two  features; 
? duration  of  the  spoken  word  like   computed  from 
the  timing  provided  with  the  ICSI-MR  transcrip-
tions,  which  was  generated  automatically; 
? duration  of  the  pause  before  like:  0  or  more,  or 
?1  if  the  utterance  begins  with  like   (the  segmen-
tation  into  prosodic  utterances  was  also  provided 
with  the  transcription); 
? duration  of  the  pause  after  like:  0  or  more,  or  ?1 
if  the  utterance  ends  with  like. 
In  order  to  classify  each  of  the  occurrences  of  like  as 
either  a  DM  or  a  non-DM,  we  used  decision  trees  as 
provided  with  the  machine  learning  toolkit  WEKA 
(Witten  and  Frank  2000)12.  Since  not  all  the  features  are 
discrete,  we  used  the  C4.5  decision  tree  learner  (Quinlan 
1993),  or  J48  in  WEKA.  For  testing,  we  experimented 
both  with  separate  training  and  test  sets  derived  from 
the  data  (e.g.  1,500  vs.  616  instances),  and  by  using  10-
fold  cross-validation  of  classifiers  as  provided  by 
WEKA.  Results  being  similar,  we  report  below  the  lat-
ter  scores. 
8.2  Results  for  the  Classification  of  Like  
The  best  performance  obtained  by  a  C4.5  classifier  is 
0.95  recall  and  0.68  precision  for  the  DM  occurrences, 
corresponding  to  81%  correctly  classified  instances  and 
a  kappa  of  0.63.  This  is  a  significant  performance,  but  it 
appears  to  be  in  the  same  range  as  the  filter-based 
method  (tested  only  on  a  smaller  data  set).  And  indeed, 
the  classifier  tree  (see  Figure  1  in  the  Appendix)  exhib-
its  as  the  first  nodes  the  two  classes  of  collocation  filters 
defined  a  priori   in  section  6.  This  is  a  strong  empirical 
proof  of  the  relevance  of  these  filters.  Note  that  this  cri-
terion  has  not  been  used  by  Litman  (1996)  who  focuses 
on  a  much  more  detailed  analysis  of  the  prosody  along 
with  some  textual  features. 
Moreover,  the  next  feature  in  the  tree  is  the  duration 
of  the  pause  before  like  (?pause_avant?):  it  appears  that 
a  relatively  long  pause  before  like  (greater  than  240  ms) 
characterizes  a  DM  in  most  remaining  cases  (70  out  of 
78).  This  matches  our  intuitions  about  the  prosodic  be-
haviour  of  like   as  a  DM.  The  next  features  in  the  tree 
have  quite  a  low  precision,  and  may  not  generalize  to 
other  corpora.  Tentatively,  it  appears  that  a  very  short 
like   (shorter  than  120  ms)  is  not  a  DM. 
The  best  classifier  tends  to  show  that  apart  from  the 
collocation  filters,  the  other  features  do  not  play  an  im-
portant  role.  A  classifier  based  only  on  the  collocation 
filters  achieves  0.96  recall  and  0.67  precision  for  DM 
identification  (80%  correctly  classified  instances  and 
?  =  0.62),  which  is  only  slightly  below  the  best  classi-
fier.  Is  it  that  the  time-based  features  are  totally  irrele-
vant?  An  experiment  without  the  two  collocation  filters 
shows  that  temporal  features  are   relevant:  the  best  clas-
sifier  achieves  67%  correct  classification,  with  ?  =  0.23, 
that  is,  somewhat  above  chance.  Again,  among  the  first 
nodes  of  the  tree  are  the  interval  before  like  and  its  dura-
tion  (Figure  2  in  Appendix).  Also,  a  pause  after  like 
seems  to  signal  a  DM.  Temporal  features  are  therefore 
relevant  to  DM  detection,  but  they  are  in  reality  corre-
lated  with  collocation-based  features,  which  supersede 
them  when  they  can  be  detected. 
The  conclusions  of  this  experiment  with  like   are  that 
the  simple  features  designed  until  now,  though  particu-
                                                                                                               
12
  The  Waikato  Environment  for  Knowledge  Analysis 
(WEKA)  is  made  available  by  Ian  H.  Witten  and  Eibe  Frank  at 
http://www.cs.waikato.ac.nz/ml/weka. 
larly  efficient  given  their  simplicity,  do  not  allow  for 
more  than  70%  precision  (at  100%  recall)  for  the  detec-
tion  of  like  as  a  DM.  Time-based  features  do  not  outper-
form  collocation-based  filters  ?  though  the  former  could 
generalize  better  to  other  DMs.  This  result  is  also  par-
ticularly  interesting  considering  the  fact  that  human  an-
notators  performed  significantly  better  when  allowed  to 
use  sound  files.  The  results  suggest  that  prosodic  fea-
tures  other  than  duration  are  relevant  for  the  disam-
biguation  of  like.  Further  work  on  the  prosody  of  like 
(e.g.  pitch)  should  enable  us  to  refine  this  criterion.   
8.3  The  Classification  of  Well
Using  a  similar  procedure,  we  have  applied  C4.5  classi-
fication  to  the  detection  of  well   as  a  DM.  On  the  same 
dialogs  as  above,  we  annotated  the  occurrences  of  well  
as  a  DM  (579)  among  all  occurrences  of  well   (873). 
About  66%  of  all  occurrences  are  DMs,  which  gives  a 
baseline  classification  score  (all  occurrences  considered 
to  be  DMs). 
The  features  defined  for  well   are  similar  to  those 
used  for  like:  collocation-based  filters  (with  a  different 
content)  and  time-based  features.  In  addition,  we  de-
fined  a  collocation-based  feature  that  is  supposed  to  as-
certain  the  presence  of  a  DM,  namely  collocations  such 
as  oh  well   or  OK  well.  We  also  consider  the  occurrence 
of  well  at  the  end  of  an  interrupted  or  abandoned  utter-
ance  (ending  on  transcriptions  by  ?=  =?),  a  feature  we 
hypothesize  to  indicate  a  DM. 
The  highest  accuracy,  91%  and  ?  =  0.8,  is  obtained 
by  a  classifier  combining  the  collocation  filters  and  the 
duration  of  the  pause  after  well  (cf.  Figure  3  in  the  Ap-
pendix).  This  corresponds  to  91%  precision  and  97% 
recall  for  the  detection  of  DMs. 
The  use  of  the  collocation-based  filter  alone  ?  the 
one  that  rules  out  DM  occurrences  based  on  the  previ-
ous  word,  e.g.  as  well   ?  yields  only  slightly  lower  per-
formance  (90%  with  ?  =  0.79).  Again,  this  does  not 
mean  that  all  the  other  features  are  irrelevant.  Rather, 
the  time-based  filter  based  on  the  duration  of  the  pause 
after  well,  which  includes  the  detection  of  well   at  the 
end  of  completed  or  interrupted  utterances,  produces  a 
classification  accuracy  of  75%  (and  a  low  kappa,  0.45), 
with  77%  precision  and  96%  recall  on  the  identification 
of  DMs  only. 
These  results  suggest  that  time-based  features  could 
generalize  to  a  whole  class  of  DMs,  but  for  individual 
DMs,  such  features  are  outperformed  by  collocations 
filters  based  on  patterns  of  occurrences.  The  definition 
of  collocation  filters  for  a  set  of  DMs  seems  feasible, 
albeit  somehow  tedious. 
9  Conclusion 
This  paper  has  presented  several  computational  ap-
proaches  to  the  disambiguation  of  discourse  markers, 
with  a  focus  on  the  highly  ambiguous  word  like.  Ex-
periments  regarding  the  human  capacity  to  annotate  re-
liably  the  discourse  marker  like   show  that  relatively 
untrained  annotators  reach  a  kappa  agreement  of  about 
0.74,  producing  reliable,  though  not  perfect,  annotations 
?  provided  they  have  access  to  the  sound  files.  Auto-
matic  performance  of  the  identification  task,  using  a  set 
of  collocation  filters,  can  help  annotators  by  discarding 
some  of  the  non-pragmatic  occurrences.  However,  POS 
taggers  seem  unable  to  disambiguate  the  occurrences  of 
like  in  speech  transcripts.  Finally,  the  training  of  deci-
sion  trees  on  about  2,100  occurrences  of  like   confirms 
the  relevance  of  collocation  filters  as  the  main  features, 
followed  by  time-based  features,  while  correctly  classi-
fying  more  than  80%  of  the  occurrences  of  like,  and 
more  than  90%  of  those  of  well. 
Future  work  should  explore  the  relevance  of  other 
potential  features.  However,  given  the  strong  pragmatic 
function  of  DMs,  it  is  unlikely  that  low-level  features 
combined  with  machine  learning  will  entirely  solve  the 
problem.  As  we  have  seen,  POS  tagging  is  quite  unreli-
able  on  DMs,  but  POS  tags  from  the  surrounding  words 
could  serve  as  features  for  statistical  training.  More  data 
and  more  reliable  annotations  will  also  help.  Another 
promising  approach  is  the  generalization  of  classifica-
tion  features  across  several  DMs,  which  will  allow  the 
detection  of  an  entire  class  of  discourse  markers. 
References 
Aijmer,  K.  English  Discourse  Particles:  Evidence  from 
a  Corpus.  Amsterdam:  John  Benjamins,  2002. 
Andersen,  G.  Pragmatic  Markers  of  Sociolinguistic 
Variation:  a  Relevance-Theoretic  Approach  to  the 
Language  of  Adolescents.  Amsterdam:  John  Benja-
mins,  2001. 
Blakemore,  D.  Meaning  and  Relevance:  the  Semantics 
and  Pragmatics  of  Discourse  Markers.  Cambridge: 
CUP,  2002. 
Fraser,  B.  An  Approach  to  Discourse  Markers.  Journal 
of  Pragmatics.  1990,  vol.14,  pp.  383-395. 
Heeman,  P.,  Byron,  D.,  Allen,  J.  Identifying  Discourse 
Markers  in  Spoken  Dialog.  Proceedings  of  AAAI 
Spring  Symposium  on  Applying  Machine  Learning 
and  Discourse  Processing.  Stanford,  1998. 
Lenk,  U.  Marking  Discourse  Coherence:  Functions  of 
Discourse  Markers  in  Spoken  English.  T?bingen: 
Gunter  Narr  Verlag,  1998. 
Litman,  D.  Cue  Phrase  Classification  Using  Machine 
Learning.  Journal  of  Artificial  Intelligence  Research. 
1996,  vol.5,  pp.  53-94. 
Mann,  W.,  Thompson,  S.  Rhetorical  Structure  Theory: 
Toward  a  Functional  Theory  of  Text  Organisation. 
Text.  1988,  vol.8(3),  pp.  243-281. 
Marcu,  D.  The  Theory  and  Practice  of  Discourse  Pars-
ing  and  Summarization.  Cambridge:  MIT  Press, 
2000. 
Mason,  O.  Programming  for  Corpus  Linguistics:  How 
to  do  Text  Analysis  in  Java.   Edinburgh:  Edinburgh 
University  Press,  2000. 
Meteer,  M.  Dysfluency  Annotation  Stylebook  for  the 
Switchboard  Corpus.  LDC,  Working  Paper.  1995, 
28p.  http://www.ldc.upenn.edu/Catalog/CatalogList/ 
LDC99T42/DFLGUIDE.PS  (06/17/2003). 
Quinlan,  J.  R.  C4.5:  Programs  for  Machine  Learning. 
San  Francisco:  Morgan  Kaufman,  1993. 
Schiffrin,  D.  Discourse  Markers.  Cambridge:  CUP, 
1987. 
Sperber,  D.,  Wilson,  D.  Relevance:  Communication  and 
Cognition.  Oxford:  Blackwell,  1986/1995. 
Taboada,  M.  Discourse  Markers  as  Signals  (or  not)  of 
Rhetorical  Relations  in  Conversation.   Proceedings  of 
the  8th  International  Pragmatics  Conference.  To-
ronto,  2003. 
Witten,  I.,  Frank,  E.  Data  Mining:  Practical  Machine 
Learning  Tools  with  Java  Implementations.  San 
Francisco:  Morgan  Kaufmann,  2000. 
Zufferey,  S.  Une  Analyse  des  Connecteurs  Pragmati-
ques  Fond?e  sur  la  Th?orie  de  la  Pertinence  et  son 
Application  au  TALN.  Cahiers  de  linguistique  fran-
?aise.  2004,  vol.25,  pp.  257-272. 
 
Appendix 
The  classifiers  (C4.5  decision  trees)  built  for  like   and  for 
well   that  are  reproduced  here  must  be  interpreted  using 
the  following  rules.  Starting  with  the  root  of  the  tree, 
occurrences  of  the  respective  DM  (like  or  well)  are  clas-
sified  according  to  the  features  appearing  at  the  nodes 
(round  shapes).  Depending  on  the  values  of  the  features 
(branches  of  the  tree),  the  occurrences  are  classified  as  a 
DM  (1)  or  not  (0).  The  rectangular  boxes  contain  the 
class  (0/1)  and  the  number  of  instances  cor-
rectly/incorrectly  classified. 
 
 
Figure  1.  Best  classifier  for  like   as  a  DM.   
 
In  Figure  1,  for  like,  the  features  can  be  glossed  as  fol-
lows:  ?collokexclavant?  ?  collocation  filter  ruling  out 
the  presence  of  a  DM,  depending  on  the  word  before 
like;  ?collokexclapres?  ?  collocation  filter,  word  after 
like;  ?pauseavant?  ?  duration  of  the  silent  gap  before 
like;  ?dureelike?  ?  duration  of  like.  The  most  relevant 
feature,  after  the  collocation  filters,  is  the  gap  before 
like:  a  pause  signals  a  DM  in  91%  of  the  cases. 
In  Figure  2,  for  like,  when  collocation  filters  are  not 
used,  a  pause  before  (?pauseavant?)  and  a  pause  after 
(?pauseapres?)  are  the  most  reliable  indicators  of  a  DM 
(occurrences  classified  as  ?1?). 
 
 
Figure  2.  Best  classifier  for  like  as  a  DM,  without  the 
collocation  filters. 
 
In  Figure  3,  for  well,  the  features  are:  ?collokexclavant? 
?  collocation  filter  ruling  out  a  DM,  depending  on  the 
word  before  well;  ?collokinclavant?  ?  collocation  filter 
that  ascertains  a  DM,  based  on  the  word  before  well; 
?pauseapres?  ?  duration  of  the  gap  after  well:  ?2  means 
that  well  is  the  last  word  of  an  interrupted  utterance,  and 
?1  means  it  is  the  last  word  of  a  completed  utterance.   
 
 
Figure  3.  Best  classifier  for  well   as  a  DM. 
 
The  best  classifier  without  the  collocation  features  (not 
represented  here)  corresponds  to  the  following  rules:  (a) 
if  well   ends  an  interrupted  utterance,  then  it  is  a  DM 
(100%  accurate);  (b)  if  it  ends  a  completed  utterance, 
then  it  is  not  a  DM  (88%  acc.);  (c)  otherwise,  it  is  a  DM 
(81%  acc.). 
Multi-level Dialogue Act Tags
Alexander Clark and Andrei Popescu-Belis
ISSCO / TIM / ETI
University of Geneva
UNI-MAIL, Boulevard du Pont-d?Arve 40
CH-1211 Geneva 4
Switzerland
asc@aclark.demon.co.uk andrei.popescu-belis@issco.unige.ch
Abstract
In this paper we discuss the use of multi-
layered tagsets for dialogue acts, in the con-
text of dialogue understanding for multi-
party meeting recording and retrieval ap-
plications. We discuss some desiderata for
such tagsets and critically examine some
previous proposals. We then define MAL-
TUS, a new tagset based on the ICSI-MR
and Switchboard tagsets, which satisfies
these requirements. We present some ex-
periments using MALTUS which attempt
to compare the merits of integrated versus
multi-level classifiers for the detection of di-
alogue acts.
1 Introduction
The processing of dialogues by computers serves
two main applicative goals: understanding of hu-
man dialogues, for information extraction or sum-
marization, and human-computer dialogue manage-
ment, for language-based or multimodal interfaces.
Whether the computer takes part in a dialogue or
only attempts to monitor a recorded one, it is im-
portant to detect the functions of each of the human
utterances that constitute the dialogue. In addition,
when the computer must generate an utterance as
a reply, this must also bear some of the functions
expected by the hearer in return.
In this article, we focus on dialogue understand-
ing for a dialogue storage and retrieval application,
developed in the (IM)2 project1. The goal of the
application is the multimodal recording of meetings
(such as staff or business meetings), the processing
and storage of the recordings into a database, and the
1(IM)2 stands for Interactive Multimodal Information
Management, a project sponsored by the Swiss Govern-
ment (see http://www.im2.ch).
possibility of querying the dialogue database (Arm-
strong et al, 2003). The query interface and the
processing of the dialogue must therefore meet the
needs of the potential users of the system, who will
attempt to retrieve various types of information from
the meeting recordings. While the result of the query
is in general a chunk of recorded dialogue (prefer-
ably with multimedia rendering), the criteria used to
query the database can vary from trivial (?who at-
tended the meeting??) to very abstract (?what were
the main decisions??). Some form of understanding
of the dialogue structure is thus required for a sig-
nificant proportion of potential queries (more about
requirements in subsection 2.3).
The utterance functions with which we deal in this
paper are dialogue acts. Although dialogue acts (DA)
tags are commonly used as a simple representation
of the function of an utterance in dialogue, there is
little consensus amongst researchers about what set
of DA tags is appropriate in a particular situation.
Our own application domain, meeting recording, is
comparatively open-ended and we do not yet have
a clear understanding of precisely what features will
be most useful. In section 2, we will try to under-
stand the multiplicity of DA tagsets, then we will an-
alyze (section 3) the dialogue data and annotations
on which we work. These considerations prompted
us to abstract a new DA tagset, of which we explain
the merits in section 4. Experiments on the auto-
matic annotation of DAs using the MALTUS tagset
are described in section 5; the results (subsection 5.2)
are followed by a brief discussion.
2 Understanding Dialogue Structure:
Dialogue Acts
2.1 The Concepts behind Dialogue Acts
Dialogues are series of speaker turns. Utterances can
be defined as the atomic subparts of a turn that ac-
complish one or more ?functions? with respect to
speaker interaction. Utterances are in general sig-
nalled by syntactic and/or prosodic means, but the
specificity of their ?function? belongs to pragmat-
ics (Levinson, 1983, ch. 4). Linguists have identified
several dimensions for the role of sentences uttered
in a dialogue. These dimensions are not mutually ex-
clusive, and there are certainly correlations between
some of them (e.g. ?question? as a speech act and
as a member of an adjacency pair).
? Speech acts (Searle, 1969; Vanderveken, 1990):
(1) representatives, such as assertions or con-
clusions; (2) directives, such as requests, ques-
tions, suggestions; (3) commissives, such as
promises, threatenings, offers; (4) expressives
such as thanks, apologies, congratulations; (5)
declarations, such as excommunications, decla-
rations of war, christening, firing from employ-
ment, etc.
? Turn management: backchannel, floor holder,
floor grabber, hold;
? Adjacency pairs: utterances can be the first part
or the second part of exchange pairs such as re-
quest / accept (or refuse); offer / accept; assess
/ (dis)agree; question / answer; etc.
? Overall organization and topics: openings, clos-
ings, topic-changers, topic-continuers, etc.
? Politeness management: face-threatening, face-
saving, neutral;
? Rhetorical role: elaboration, purpose, restate-
ment, etc.
2.2 Dialogue Acts in Computational
Linguistics
There is not much agreement, within the CL/NLP
community, on the definition of a dialogue act. The
term denotes some function of an utterance in a dia-
logue, not reducible to its syntactic or semantic con-
tent. The function is selected, in general, among
a set of possible dialogue acts (a DA tagset) that
depends on the goals of its creator (Traum, 2000).
One of the main inspiration sources for DA tagsets
are speech acts, but the original repertoire (Searle,
1969; Vanderveken, 1990) has been gradually en-
riched with other possible functions. From the nu-
merous DA tagsets (Klein and Soria, 1998), the fol-
lowing are particularly relevant to a general-domain
meeting recording application.
The DA tags in DAMSL (Allen and Core, 1997)
are nearly all independent: the DAMSL guidelines
state that all tags (i.e. all ?functions?) that charac-
terize an utterance should be associated with it. The
DAMSL tags are grouped in four dimensions: com-
municative status, information level, forward-looking
function and backward-looking function. In fact,
several theories are conflated in DAMSL, which was
initially designed as a shared resource with a focus
primarily on task-oriented dialogs (Core and Allen,
1997). There are about 4 million possible combi-
nations of DAMSL tags, which make a huge search
space for automatic annotation.
The application of DAMSL to the Switchboard
data (two-party telephone conversations) lead to
SWBD-DAMSL (Jurafsky et al, 1997), a smaller
tagset than DAMSL. About 200,000 SWBD utter-
ances were first annotated with DAMSL tags: it was
observed that only 220 combinations of tags occurred
(Jurafsky et al, 1998). These 220 labels were then
clustered into 42 tags, such as: statement (36%),
opinion (13%), agree/accept (5%), yes-no-question
(2%). The resulting search space (42 mutually ex-
clusive tags) was well adapted to the initial goals,
viz., the automatic annotation of dialogue acts and
the use of dialogue act specific language models in
speech recognition (Stolcke et al, 2000).
2.3 Requirements for the Definition of a
DA Tagset
In this paper, our goal is to design a new DA tagset
for our application, with the following constraints in
mind (see also the analysis by D. Traum (2000)):
? Relation to one or more existing theories (de-
scriptive, explanatory, etc.).
? Compatibility with the observed functions of ac-
tual utterances in context, in a given domain.
? Empirical validation: reliability of human appli-
cation of the tagset to typical data (high inter-
annotator agreement, at least potentially).
? Possibility of automatic annotation (this re-
quirement is specific to NLP).
? Relevance to the targeted NLP application:
there are numerous possible functions of utter-
ances, but only some of them are really use-
ful to the application. Within our IM2.MDM
project, a study has been conducted on the rel-
evance of dialogue acts (in particular) to typical
user queries on meeting recordings (Lisowska,
2003)2.
? Mapping (at least partially) to existing tagsets,
so that useful insights are preserved, and data
can be reused.
2Many other potential uses of dialogue act informa-
tion have been hypothesized, such as their use to increase
ASR accuracy (Stolcke et al, 2000), or to locate ?hot
spots? in meetings (Wrede and Shriberg, 2003).
3 Available Data and Annotations:
ICSI Meeting Recorder
The volume of available annotated data suffers from
the diversity of DA tagsets (Klein and Soria, 1998).
One of the most significant resources is the Switch-
board corpus mentioned above, but telephone con-
versations have many differences with multi-party
meetings. Apart from the data recently available in
the IM2 project, results reported in this paper make
use of the ICSI Meeting Recording (MR) corpus of
transcribed and annotated dialogues (Morgan et al,
2003; Shriberg et al, 2004)3.
3.1 Overview of ICSI MR Corpus
The ICSI-MR corpus consists of 75 one-hour record-
ings of staff meetings, each involving up to eight
speakers on separate mike channels. Each channel
was manually transcribed and timed, then annotated
with dialogue act and adjacency pair information
(Shriberg et al, 2004). Following a preliminary re-
lease in November 2003 (sound files, transcriptions,
and annotations), the full corpus was released in
February 2004 to IM2 partners.
The dialogue act annotation makes use of the pre-
existing segmentation of each channel into (prosodic)
utterances, sometimes segmented further into func-
tional utterances, each of them bearing a separate di-
alogue act. There are about 112,000 prosodic utter-
ances, and about 7,200 are segmented into two func-
tional utterances (only one is segmented in three).
3.2 Discussion of the ICSI-MR DA Tagset
Each functional utterance from the ICSI-MR corpus
is marked with a dialogue label, composed of one
or more tags from the ICSI-MR tagset (Dhillon et
al., 2004). The tagset, which is well documented,
is based on SWBD-DAMSL, but unlike SWBD-
DAMSL, it allows one utterance to be marked with
multiple tags. Also, the SWBD-DAMSL tagset was
extended, for instance with disruption tags such as
?interrupted?, ?abandoned?, etc. Utterances can also
be marked as ?unintelligible? or ?non-speech?. An
ICSI-MR label is made of a general tag, followed
by zero or more specific tags, followed or not by a
disruption tag:
gen_tag [^spec_tag_1 ... ^spec_tag_n] [.d]
Our formalization of the guidelines using rewriting
rules (Popescu-Belis, 2003) shows that few tags are
mutually exclusive. The number of possible combi-
nations (DA labels) reaches several millions. For in-
stance, even when not considering disruption marks,
3See http://www.icsi.berkeley.edu/Speech/mr/
the labels are a combination of one general tag out
of 11, and one or more specific tags out of 39. If up
to five specific tags are allowed (as observed empir-
ically in the annotated data), there are more than
7,000,000 possible labels; if specific tags are limited
to four, there are about 1,000,000 possible labels.
Some studies acknowledge the difficulties of an-
notating precisely with ICSI-MR, but also the
fine-grained distinctions it allows for, e.g. be-
tween the possible functions of four related dis-
course particles (?yeah?, ?right?, ?okay?, and ?uhhuh?):
agreement/acceptance, acknowledgment, backchan-
nel, floor grabber (Bhagat et al, 2003). Conversely,
inter-annotator agreement on such fine-grained dis-
tinctions (specific tags) is lower than agreement on
major classes, though the kappa-statistic normally
used to measure agreement adjusts to a certain ex-
tent for this. In fact, ICSI-MR also provided a set
of five ?classmaps? that indicate how to group tags
into categories which reduce the number of possible
labels. For instance, the simplest one reduces all
DA labels to only five classes: statement, question,
backchannel, floor holder/grabber, disruption. Our
MALTUS proposal (see 4.1 below) could be viewed
as a classmap too: it preserves however more ICSI-
MR tags than the existing classmaps, and assigns in
addition conditions of mutual exclusiveness.
We also note that, while SWBD-DAMSL was an
attempt to reduce the dimensionality of the DAMSL
tagset (which had a clear theoretical base), the ICSI-
MR tagset alows SWBD tags to be combined again
instead of going back to DAMSL tags. Although
our proposal that we proceed to describe (MALTUS)
remains close to ICSI-MR for reusability reasons, we
are also working on a more principled DA tagset that
departs from ICSI-MR (Popescu-Belis, 2003).
3.3 Some Figures for the ICSI-MR Data
In the process of conversion to MALTUS (see 4.2
below), we validated the ICSI-MR data and made
several observations. Detected incoherent combina-
tions of tags (e.g., two general tags in a label) and
other remarks have also been sent back to ICSI.
We first separate prosodic utterances into func-
tional utterances, so that each utterance has one DA
label (and not two, separated by ?|?), thus obtaining
120,205 utterances. Also at this stage, we split ut-
terances that correspond to reported speech (marked
with ?:?). We then discard the disruption marks to fo-
cus on the DA labels only ? about 12,000 labels out of
ca. 120,000 are disruption marks, or contain one. We
are left with 113,560 utterances with DA labels, with
776 observed types of labels. An important param-
eter is the number of occurring vs. possible labels,
Nb. of Nb. of Nb. of Nb. of
tags in theoretical occurring tokens
label comb. comb.
1 11 11 68,213
2 429 129 37,889
3 8,151 402 5,054
4 100,529 176 2,064
5 904,761 49 326
6 6,333,327 9 14
7 . . . 0 0
Total: 7,347,208 776 113,560
Table 1: Number of possible labels (combinations of
tags): theoretical vs. actual.
Maximal nb. Maximal theoretical
of tags accuracy on ICSI-MR
1 0.601
2 0.934
3 0.979
4 0.997
5 0.999
6 1
Table 2: Maximal accuracy of DA tagging of the
ICSI-MR data that could be reached using a limited
number of tags per label.
which depends a lot on the number of specific tags
in a label, as summarized in table 1. The maximum
observed in the available data is five specific tags in
a label (hence six tags in all).
There is no guarantee that meaningful labels can-
not have more than six tags. However, such labels
are probably very infrequent, and a reasonable op-
tion for automatic tagging is to limit the number
of tag combinations, which is the main goal of the
MALTUS tagset. The maximal accuracies that could
be obtained on the available ICSI-MR data if the
number of tags in a label was limited to 1, 2, etc.
are shown in Table 2. In computing the accuracy we
consider here only perfect matches, but scores could
be higher if partial matches count too. Two or three
tags per label already allow very high accuracy, while
considerable reducing the search space.
4 The MALTUS DA Tagset
4.1 Definition
We defined MALTUS (Multidimensional Abstract
Layered Tagset for Utterances) in order to reduce
the number of possible combinations by assigning
exclusiveness constraints among tags, while remain-
ing compatible with ICSI-MR (Popescu-Belis, 2003).
MALTUS is more abstract than ICSI-MR, but can
be refined if needed. An utterance is either marked
U (undecipherable) or it has a general tag and zero
or more specific tags. It can also bear a disruption
mark. More formally (? means optional):
DA -> (U | (gen_tag (spec_tags)?)) (.D)?
gen_tag -> S | Q | B | H
spec_tags -> (RP | RN | RU)? AT? DO? PO?
The glosses of the tags, generally inspired from
ICSI-MR, are:
? U = undecipherable (unclear, noisy)
? S = statement
? Q = question
? B = backchannel
? H = hold (floor holder, floor grabber, hold)
? RP = positive answer (or positive response)
? RN = negative answer (or negative response)
? RU = other answer (or undecided answer or re-
sponse)
? RI = restated information
? DO = command or other performative (can be
refined into: command, commitment, sugges-
tion, open-option, explicit performative)
? AT = the utterance is related to attention man-
agement (can be refined into: acknowledgement,
rhetorical question backchannel, understanding
check, follow me, tag question)
? PO = the utterance is related to politeness (can
be refined into sympathy, apology, downplayer,
?thanks?, ?you?re welcome?)
? D = the utterance has been interrupted or aban-
doned
4.2 Conversion of ICSI-MR to MALTUS
There are only about 500 possible MALTUS labels,
but observations of the converted ICSI-MR data
show again that the probability distribution is very
skewed. An explicit correspondence table and con-
version procedure were designed to convert ICSI-MR
to MALTUS, so that the considerable ICSI-MR re-
source can be reused.
Correspondences between MALTUS and other
tagsets (Klein and Soria, 1998) were also provided
(Popescu-Belis, 2003). Such ?mappings? are imper-
fect for two reasons: first, they work only in one
direction, from the more specific tagset (ICSI-MR /
SWBD / DAMSL) to the more abstract one (MAL-
TUS). Second, a mapping is incomplete if one does
not state which tags must be mutually exclusive.
For MALTUS too, the idea to use at most three
tags per label in an automatic annotation program
might reduce the search space without decreasing the
accuracy too much. Another idea is to use only the
labels that appear in the data that is, only 50 labels.
An even smaller search space is provided by the 26
MALTUS labels that occur more than 10 times each.
If only these are used for tagging, then only 70 occur-
rences (only 0.061% of the total) would be incorrectly
tagged, on the ICSI-MR reference data. Occurring
labels ordered alphabetically and their frequencies
(when greater than 10) are listed below.
B (15180)
H (12288)
Q (5320)
Q^AT (3137)
Q^AT^RI (69)
Q^DO (239)
Q^RI (60)
Q^RN (19)
S (51304)
S^AT (8280)
S^AT^RI (273)
S^DO (3935)
S^DO^RI (32)
S^DO^RN (38)
S^DO^RP (41)
S^DO^RU (16)
S^PO (791)
S^PO^RI (13)
S^PO^RU (61)
S^RI (765)
S^RI^RN (46)
S^RI^RP (436)
S^RI^RU (18)
S^RN (2219)
S^RP (7612)
S^RU (1298)
Further analysis will tell whether this list should
be enriched with useful labels that are absent from
it. Also, a comparison of MALTUS to the SWBD
set (26 labels vs. 42) should determine whether the
loss in informativeness in MALTUS is compensated
by the gain in search space size and in theoretical
grounding.
5 Automatic Classification
As discussed above, one of the desiderata for a tagset
in this application domain is that the tags can be ap-
plied automatically. A requirement for annotations
that can only be applied manually is clearly unre-
alistic except for meetings of very high importance.
The ICSI-MR corpus on the other hand is concerned
with producing a body of annotated data that can
be used by researchers for a wide range of different
purposes: linguists who are interested in particular
forms of interaction, researchers in acoustics and so
on. It is by no means a criticism of their work that
some of the distinctions that they annotate or at-
tempt to annotate cannot be reliably automated.
Here we report some preliminary experiments on
the automatic annotation of meeting transcripts with
these tagsets. Our focus here is not so much on eval-
uating a classifier for this task but rather evaluating
the tagsets: we are interest in the extent to which
they can be predicted reliably from easily extracted
features of the utterance and its context. Addition-
ally we are interested in the multi-level nature of the
tagsets and exploring the extent to which the internal
structure of the tags allows other options for classi-
fiers. Therefore, our goal in these experiments is not
to build a high performance classifier; rather, it is to
explore the extent to which multi level tagsets can
be predicted by classifying each level separately ?
i.e. by having a set of ?orthogonal? classifiers ? as
opposed to classifying the entire structured object
in a single step using a single multi-class classifier
on a flattened representation. Accordingly there are
a number of areas in which our experimental setup
differs from that which would be appropriate when
performing experiments to evaluate a classifier.
Since in this paper we are not using prosodic or
acoustic information, but just the manual transcrip-
tions, there are two sources of information that can
be used to classify utterances. First, the sequence of
words that constitutes the utterance, and secondly
the surrounding utterances and their classification.
generally in prior research in this field, some form
of sequential inference algorithm has been used to
combine the local decisions about the DA of each ut-
terance into a classification of the whole utterance.
The common way of doing this has been to use a
hidden Markov model to model the sequence and to
use a standard decoding algorithm to find either the
sequence with maximum a posteriori (MAP) likeli-
hood or to select for each utterance the DA with
MAP likelihood. In the work here, we will ignore
this complexity and allow our classifier access to the
gold standard classification of the surrounding utter-
ances. This will make the task substantially easier,
since in a real application, there will be some noise
in the labels.
5.1 Feature selection
There are two sorts of features that we shall use here
? internal lexical features derived from the words in
the utterance, and contextual features derived from
the surrounding utterances. At our current state of
knowledge we have a very good idea about what the
lexically derived features should be, and how they
should be computed ? namely n-grams or gappy n-
grams including positional information. Addition-
ally, there are ways of computing these efficiently.
However, with regard to the contextually derived fea-
tures, our knowledge is much less complete. (Stolcke
et al, 2000) showed that in the Switchboard corpus
there was little dependence beyond the immediately
adjacent utterance, but whether this also applies in
this multi-party domain is unknown. Thus we find
ourselves in a rather asymmetric position with re-
gard to these two information sources. As we are
not here primarily interested in constructing a high
performance classifier, but rather identifying the pre-
dictable elements of the tag, we have resolved this
problem by deliberately selecting a rather limited set
of lexical features, together with a limited set of con-
textual features. Otherwise, we feel that our experi-
ments would be overly biased towards those elements
of the tag that are predictable from the internal lex-
ical evidence.
We used as lexical features the 1000 most frequent
words, together with additional features for these
words occurring at the beggining or end of the ut-
terance. This gives an upper bound of 3000 lexical
features. We experimented with a variety of simple
contextual features.
Preceding same label (SL) the immediately pre-
ceding utterance on the same channel has a par-
ticular DA tag.
Preceding label (PL) a preceding utterance on a
different channel has a particular DA tag. We
consider an utterance to be preceding if it starts
before the start of the current utterance.
Overlapping label (OL) an utterance on another
channel with a particular DA tag overlaps the
current utterance. We anticipate this being use-
ful for identifying backchannels.
Containing label (CL) an utterance on another
channel with a particular DA tag contains the
current channel ? i.e. the start is before the start
of the current utterance and the end is after the
end of the current utterance.
Figure 1 shows an artificial example in a multi-
party dialog with four channels. This illustrates the
features that will be defined for the classification of
the utterance that is shaded. In this example we
will have the following features SL:C1, PL:B1, PL:D1,
CL:D1, OL:A1, OL:B1, OL:B2, OL:D1. We have found
A
B
C
D
A1
B1 B2
C1
D1
Figure 1: Artificial example illustrating contextual
features defined for a particular utterance (shaded).
There are four channels labelled A to D; each box
represents an utterance, and the DA tag is repre-
sented by the characters inside each box.
that the overlapping label feature set does not help
the classifiers here, so we have used the remaining
three contextual feature sets. Note the absence of
contextual features corresponding to labels of utter-
ances that strictly follow the target utterance. We
felt that given the fact that we use the gold standard
tags this would be too powerful.
The data made available to us was preprocessed
in a number of ways. The most significant change
was to split utterances that had been labelled with a
sequence of DA labels (joined with pipes). We sep-
arated the utterances and the labels at the appro-
priate points and realigned. The data was provided
with individual time stamps for each word using a
speech recognizer in forced recognition mode: where
there were errors or mismatches we discarded the
words.
5.2 Results
We use a Maximum Entropy (ME) classifier (Man-
ning and Klein, 2003) which allows an efficient com-
bination of many overlapping features. We selected
5 meetings (6771 utterances after splitting) to use as
our test set and 40 as our training set leaving a fur-
ther five for possible later experiments. As a simple
baseline we use the classifier which just guesses the
most likely class. We first performed some experi-
ments on the original tag sets to see how predictable
they are.
We started by defining a simple six-way classifica-
tion task which classifies disruption forms, and unde-
cipherable forms as well as the four general tags de-
fined above. This is an empirically very well-founded
distinction: the ICSI-MR group have provided some
inter-annotator agreement figures(Carletta et al,
1997) for a very similar task and report a kappa
of 0.79. Our ME classifier scored 77.9% (baseline
54.0%).
We also tested a few simple binary classifications
to see how predictable they are. Utterances are anno-
tated for example with a tag J if they are a joke. As
would be expected, the Joke/Non-Joke classification
produced results not distinguishable from chance.
The performance of the classifiers on separating dis-
rupted utterances from non disrupted forms scored
slightly above chance at 89.9% (against baseline of
87.0%). We suspect that more sophisticated contex-
tual features could allow better performance here.
A more relevant performance criterion for our appli-
cation is the accuracy of classification into the four
general tags. In this case we removed disrupted and
undecipherable utterances, slightly reducing the size
of the test set, and achieved a score of 84.9% (base-
line 64.1%).
With regard to the larger sets of tags, since they
have some internal structure it should accordingly
be possible to identify the different parts separately,
and then combine the results. We have therefore per-
formed some preliminary experiments with classifiers
that classify each level separately. We again removed
the disruption tags since with out current framework
we are unable to predict them accurately. The base-
line for this task is again a classifier that chooses the
most likely tag (S) which gives 41.9% accuracy. Us-
ing a single classifier on this complex task gave an
accuracy of 73.2%.
We then constructed six classifiers as follows
Primary classifier S, H, Q or B
Politeness classifier PO or not PO
Attention classifier AT or not AT
Order classifier DO or not DO
Restatement classifier RI or not RI
Response classifier RP, RN, RU or no response
These were trained separately in the obvious way and
the results combined. This complex classifier gave an
accuracy 70.5%. This mild decrease in performance
is rather surprising ? one would expect the perfor-
mance to increase as the data sets for each distinction
get larger. This can be explained by dependences be-
tween the classifications. There are a number of ways
this could be treated ? for example, one could use a
sequence of classifiers, where each classifier can use
the output of the previous classifier as a feature in
the next. It is also possible that these dependencies
reflect idiosyncracies of the tagging process: tenden-
cies of the annotators for whatever reasons to favour
or avoid certain combinations of tags.
6 Conclusion
We have discussed some issues concerning the design
and use of dialogue act tagsets. It is too early to
draw firm conclusions from this preliminary study.
We can note the obvious point that simplified smaller
tagsets are easier to predict accurately than larger
ones. There appear to be non-trivial dependencies
between the tags for reasons that are not yet clear.
We expect the performance of a final, fully automatic
classifier to be substantially higher than the results
presented here, owing to the use of more powerful
classifiers and, more importantly, larger and richer
feature sets. Finally we note that an important point
of tagset design has not been addressed empirically
here: the question of whether particular distinctions
in the tagset are actually useful in our application.
Future studies will address this point by studying
the queries formulated by potential users of meeting
processing and retrieval systems.
Acknowledgments
We are grateful to the ICSI MR group for shar-
ing with us the data as part of the IM2/ICSI
agreement ? in particular to Barbara Peskin
and Liz Shriberg. This research is part of the
Multimodal Dialogue Management module (see
http://www.issco.unige.ch/projects/im2/mdm)
of the IM2 project.
References
James F. Allen and Mark G. Core. 1997. DAMSL:
Dialog act markup in several layers (draft 2.1).
Technical report, Multiparty Discourse Group,
Discourse Research Initiative, September/October
1997.
Susan Armstrong, Alexander Clark, Giovanni Coray,
Maria Georgescul, Vincenzo Pallotta, Andrei
Popescu-Belis, David Portabella, Martin Rajman,
and Marianne Starlander. 2003. Natural language
queries on natural language data: a database of
meeting dialogues. In NLDB?2003 (8th Inter-
national Conference on Applications of Natural
Language to Information Systems), Burg/Cottbus,
Germany.
Sonali Bhagat, Hannah Carvey, and Elizabeth
Shriberg. 2003. Automatically generated prosodic
cues to lexically ambiguous dialog acts in multi-
party meetings. In ICPhS 2003, Barcelona.
Jean Carletta, Amy Isard, Stephen Isard, Jacque-
line C. Kowtko, Gwyneth Doherty-Sneddon, and
Anne H. Anderson. 1997. The reliability of a di-
alogue structure coding scheme. Computational
Linguistics, 23:13?31.
Mark G. Core and James F. Allen. 1997. Coding
dialogues with the DAMSL annotation scheme. In
David Traum, editor, Working Notes: AAAI Fall
Symposium on Communicative Action in Humans
and Machines, pages 28?35, Menlo Park, CA.
American Association for Artificial Intelligence.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey,
and Elizabeth Shriberg. 2004. Meeting recorder
project: Dialog act labeling guide. Technical
Report TR-04-002, ICSI (International Computer
Science Institute), Berkeley, CA.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation (coders manual,
draft 13). Technical Report 97-02, University of
Colorado, Institute of Cognitive Science.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox,
and Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In ACL/COLING-98
Workshop on Discourse Relations and Discourse
Markers, pages 114?120.
Marion Klein and Claudia Soria. 1998. Dialogue
acts. In Marion Klein, Niels Ole Bernsen, Sarah
Davies, Laila Dybkjaer, Juanma Garrido, Hen-
rik Kasch, Andreas Mengel, Vito Pirrelli, Mas-
simo Poesio, Silvia Quazza, and Claudia Soria,
editors, MATE Deliverable 1.1: Supported Coding
Schemes, MATE (Multilevel Annotation, Tools
Engineering) European Project LE4-8370.
Stephen C. Levinson. 1983. Pragmatics. Cambridge
University Press, Cambridge, UK.
Agnes Lisowska. 2003. Multimodal interface design
for the multimodal meeting domain: Preliminary
indications from a query analysis study. Technical
report, IM2.MDM, 11/2003.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In Tutorial at HLT-NAACL
2003 and ACL 2003. ACL, Edmonton, Canada.
Nelson Morgan, Don Baron, Sonali Bhagat, Hannah
Carvey, Rajdip Dhillon, Jane A. Edwards, David
Gelbart, Adam Janin, Ashley Krupski, Barbara
Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas
Stolcke, and Chuck Wooters. 2003. Meetings
about meetings: research at ICSI on speech in
multiparty conversations. In ICASSP 2003 (In-
ternational Conference on Acoustics, Speech, and
Signal Processing), Hong Kong, China.
Andrei Popescu-Belis. 2003. Dialogue act tagsets for
meeting understanding: an abstraction based on
the DAMSL, Switchboard and ICSI-MR tagsets.
Technical report, IM2.MDM, v1.1, 09/2003.
John R. Searle. 1969. Speech Acts. Cambridge Uni-
versity Press, Cambridge, UK.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat,
Jeremy Ang, and Hannah Carvey. 2004. The ICSI
meeting recorder dialog act (MRDA) corpus. In
Proceedings of SIGDIAL ?04 (5th SIGdial Work-
shop on Discourse and Dialog), Cambridge, MA.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Paul Taylor, Rachel Martin, Marie Meteer, and
Carol Van Ess-Dykema. 2000. Dialogue act mod-
eling for automatic tagging and recognition of
conversational speech. Computational Linguistics,
26(3):339?371.
David R. Traum. 2000. 20 questions for dialogue act
taxonomies. Journal of Semantics, 17(1):7?30.
Daniel Vanderveken. 1990. Meaning and speech acts.
Cambridge University Press, Cambridge, UK.
Britta Wrede and Elizabeth Shriberg. 2003. The
relationship between dialogue acts and hot spots
in meetings. In IEEE Speech Recognition and Un-
derstanding Workshop, St. Thomas, U.S. Virgin
Islands.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 588?599, Dublin, Ireland, August 23-29 2014.
Enforcing Topic Diversity in a Document Recommender for Conversations
Maryam Habibi
Idiap Research Institute and EPFL
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
maryam.habibi@idiap.ch
Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
This paper addresses the problem of building concise, diverse and relevant lists of documents,
which can be recommended to the participants of a conversation to fulfill their information needs
without distracting them. These lists are retrieved periodically by submitting multiple implicit
queries derived from the pronounced words. Each query is related to one of the topics identified
in the conversation fragment preceding the recommendation, and is submitted to a search engine
over the English Wikipedia. We propose in this paper an algorithm for diverse merging of these
lists, using a submodular reward function that rewards the topical similarity of documents to
the conversation words as well as their diversity. We evaluate the proposed method through
crowdsourcing. The results show the superiority of the diverse merging technique over several
others which not enforce the diversity of topics.
1 Introduction
We present a diverse retrieval technique for ranking documents that are spontaneously retrieved and
recommended to people during a conversation. These documents represent potentially useful information
for the conversation participants. The information needs of the participants are represented by implicit
queries which are built in the background based on their current speech, specifically from keywords
obtained from the conversation transcripts. Since people usually mention several topics even during a
short conversation span, such keyword sets are made of content words related to different topics. When
juxtaposed in an implicit query, these topics may have noisy effects on the retrieval results (Bhogal et al.,
2007; Carpineto and Romano, 2012).
The purpose of this paper is to present a method for merging lists of documents retrieved through
multiple implicit queries prepared for short conversations spans. Several topically-separated queries are
constructed from keywords, and generate several lists of documents. The goal of the method proposed
here is to generate a unique and concise list of documents that can be recommended in real time to the
conversation participants. The list should cover the maximum number of implicit queries and therefore
topics. To merge the lists of documents according to these criteria, we use inspiration from extractive
text summarization (Lin and Bilmes, 2011; Li et al., 2012) and from our own previous work on diverse
keyword extraction (Habibi and Popescu-Belis, 2013). The method proposed here rewards at the same
time topic similarity ? to select the most relevant documents to the conversation fragment ? and topic
diversity ? to cover the maximum number of implicit queries and therefore topics in a concise and
relevant list of recommendations, if more than one topic is discussed in the conversation fragment.
Several studies have been previously carried out on merging lists of results in information retrieval.
Despite the superficial similarity, the problem here is in fact different from distributed information re-
trieval, where several lists of results from different search engines for the same query must be merged.
Moreover, many studies addressed the topic diversification approach for re-ranking the retrieved results
of a single query. However, these approaches are not directly applicable to multiple queries.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
588
The paper is organized as follows. In Section 2 we review existing techniques for merging and re-
ranking lists of search results which are applicable here. We then explain the general framework of our
document recommender system in Section 3. In Section 4 we describe the proposed algorithm for diverse
merging of lists of recommendations. Section 5 presents the data, the parameters setting, and evaluation
tasks for comparing document lists. In Section 6 we first demonstrate empirically the benefits, for just-in-
time document recommendation, of separating users? information needs into multiple topically-separated
queries rather than using a unique query. Then, we compare the proposed diverse merging technique with
several alternative ones, showing that it outperforms them according to human judgments of relevance,
and also exemplify the results on one conversation fragment given in the Appendix A.
2 Related Work
Just-in-time document retrieval systems have been designed to recommend to their users documents
which are potentially relevant to their activities, e.g. individual users authoring documents or browsing
various repositories, or small groups holding business or private meetings (Hart and Graham, 1997;
Rhodes and Maes, 2000; Popescu-Belis et al., 2008). When using a document recommender system,
people are generally unwilling to examine a large number of recommended documents, mainly because
this would distract them from their main activity. Several solutions to this problem have been proposed.
For instance, the Watson document recommender system (Budzik and Hammond, 2000), designed
for reading or writing activities, clustered the document results and selected from each cluster the best
representative to generate a list of recommendations. Clustering results is not suitable for our application
where the mixture of topics in a single query will degrade the document results aimed to be clustered
(Bhogal et al., 2007; Carpineto and Romano, 2012), and consequently may have a damaging effect on
the clusters? representatives. The second part of the method, which selected the best representative of
the clusters in the final document list can be helpful; however, its effectiveness relies on having clusters
with the same level of importance (Wu and McClean, 2007).
Many studies in information retrieval addressed the problem of diverse ranking, which can be stated as
a tradeoff between finding relevant versus diverse information (Robertson, 1997). The existing diverse
ranking proposals differ in their diversifying policies and definitions, which can be categorized into im-
plicit methods (Carbonell and Goldstein, 1998; Zhai et al., 2003; Radlinski and Dumais, 2006; Wang
and Zhu, 2009) or explicit ones (Agrawal et al., 2009; Carterette and Chandar, 2009; Santos et al., 2010;
Vargas et al., 2012). The implicit approaches assume that similar documents will cover similar aspects
of a query, and have to be demoted in the ranking to promote relative novelty and reduce overall redun-
dancy. In one of the earliest approaches, Carbonell and Goldstein (1998) introduced Maximal Marginal
Relevance (MMR) to re-rank documents based on a tradeoff between the relevance of document results
and relative novelty as a measure of diversity. MMR was also used by Radlinski and Dumais (2006) to
re-rank results from a query set which is generated for a user query and represents a variety of potential
user intents.
Instead of implicitly accounting for the aspects covered by each document, another option is to ex-
plicitly model these aspects within the diversification approach. Agrawal et al. (2009) introduced a
submodular objective function to minimize the probability of average user dissatisfaction by assuming
a taxonomy of information and modeling user query aspects at the topical level of this taxonomy. Al-
ternatively, Santos et al. (2010) proposed another submodular objective function to maximize coverage
and minimize redundancy with respect to query aspects modeled in a keyword-based representation form
instead of a predefined taxonomy.
In our case, the recommender system for conversational environments requires diversity in the results
of multiple topically-separated queries, rather than of a single ambiguous query. Therefore, a new ap-
proach will be proposed, and will be compared in particular to a version of the explicit diversification
approach (Santos et al., 2010) adapted to our problem.
3 Framework of our Document Recommender System
We have designed the Automatic Content Linking Device (ACLD), a speech-based just-in-time document
recommender system for business meetings (Popescu-Belis et al., 2008; Popescu-Belis et al., 2011).
589
Transcript of conversation fragment
Extract the best k keywords that cover all the 
main topics with high probability
Topical clustering of keywords to prepare M
multiple topic-aware queries 
},...,{
1 kccC 
Retrieval system
},...,{},,...,{
11 MM wwWqqQ 
Retrieval system
},...,{
1 MllL  l
Diverse ranking 
(DivS)
Similarity merging 
(SimM)
Round-robin
merging
Diverse merging 
(DivM)
},...,{
1 NddS },...,{ 1 NddS },...,{ 1 NddS },...,{ 1 NddS 
list of relevant documents,
(1)
(2)
(3)
(4)
Figure 1: The four stages of our document recommendation approach (shown vertically: 1?4) and the
four options considered in this paper (bottom line: SimM, Round-robin, DivM, and DivS).
The ACLD monitors the ongoing conversation, and formulates queries based on the words detected by
a real-time automatic speech recognition (ASR) system (Garner et al., 2009). The queries are fired
periodically to retrieve documents which are then recommended to users by displaying their titles along
with relevant excerpts. As these queries are built and triggered in the background, they are referred to as
?implicit queries?, as opposed to ?explicit? ones that could be formulated by users. Just-in-time document
recommendation in the ACLD system proceeds according to the steps shown in Figure 1, which displays
at step 4 the various options for merging lists of results that are the focus of this paper.
Prior to the first processing step outlined in Figure 1, the ACLD must decide when to make a recom-
mendation, and what portion of the conversation prior to that moment should be used. This question is
beyond the scope of this paper, and remains to be fully investigated, using verbal and non-verbal criteria.
Here, for the reasons explained in Section 5.2, the ACLD recommends documents every two minutes,
segmenting the conversation at the end of the nearest utterance and using the entire conversation frag-
ment since the previous recommendation. Although in practice the results of the current recommendation
process are merged with the previous ones (using a weighted mechanism that embodies the idea of ?per-
sistence? of documents over time), in this paper we will consider the recommendation for each fragment
independently of the previous one.
The recommendation process represented in Figure 1 starts by extracting a set of keywords, C, from
the words recognized by the ASR system from the users? conversations. The keywords are extracted
using the diverse keyword extraction technique that we proposed (Habibi and Popescu-Belis, 2013),
which maximizes the coverage of the topics of a text by the extracted keyword set, as we also target in
this paper. Then, implicit queries which express the users? information needs are formulated using the
keyword set, following two alternative approaches depicted in step 2 of Figure 1. In a baseline model
(right side of the figure), a single query is built for the conversation fragment using the entire keyword
list as an implicit query. In the approach we are advocating, multiple topically-separated queries are
produced for the conversation fragment (step 2, left side of the figure). This is described in a separate
document (Habibi and Popescu-Belis, submitted), but can be outlined as follows. The implicit queries are
obtained by clustering the above-mentioned keyword set into several topically-separated subsets, each
one corresponding to an abstract topic obtained using topic modeling techniques (similarly to the model
590
presented in Subsection 4.1). Each subset is an implicit query, and is weighted based on the importance
of the topic to which it is associated.
In step 3, we separately submit each implicit query to the Apache Lucene search engine over the
English Wikipedia and obtain several lists of relevant articles. Finally, we merge and re-rank these
lists before recommendation (step 4). One baseline alternative is the explicit diverse ranking technique
proposed by Santos et al. (2010) for diversifying the primary search results retrieved for a single query,
shown on the right side of the figure. To compare the methods, we adapted this latter method to make
it applicable to our system when a single implicit query is built for a conversation fragment, by defining
query aspects using the abstract topics employed for query and document representation. The method is
noted DivS as it diversifies documents from a single list.
Our proposal lies at step 4. As represented on the left side of Figure 1, in our system, we merge the lists
of documents retrieved for multiple implicit queries. We thus propose a new method noted DivM and we
compare it with two other merging techniques. The first one, noted SimM, ignores the diversity of topics
in the list of results and ranks documents only by considering their topic similarity to the conversation
fragment. The second one is the merging technique used by the above-mentioned Watson system (Budzik
and Hammond, 2000), which uses Round robin merging, hence it is noted Round-robin. In contrast, our
proposed method, DivM, is a diverse merging technique which we now proceed to define formally.
4 Diverse Merging of the Results of Multiple Queries
The diverse merging of retrieved document lists is the process of creating a short, diverse and relevant list
of recommended documents which covers the maximum number of topics of each conversation fragment.
The merging algorithm rewards diversity by decreasing the gain of selecting documents from a list as
the number of its previously selected documents increases. The method proceeds in two steps. First,
we represent queries and the corresponding list of candidate documents from the Apache Lucene search
engine using topic modeling techniques, and then we rank documents by using topical similarity and
rewarding the coverage of different lists.
4.1 Document and Query Representation
A topic model represents the abstract topics which occur in a collection of documents ? here, preferably,
a collection that is representative of the domain of the conversations. Once trained, topic models such
as Probabilistic Latent Semantic Analysis (PLSA) or Latent Dirichlet Allocation (LDA) can be used
to determine the distribution of abstract topics in each set of words composing either a conversation
fragment, or a query, or a document. LDA implemented in the Mallet toolkit (McCallum, 2002) is used
here to train topic models because it does not suffer from the over-fitting of PLSA (Blei et al., 2003).
We first learn a probability model for observing a word v in a document d through the set of abstract
topics T = {t
1
, ..., t
z
, ..., t
Z
}, where Z is the number of topics, using the Mallet toolkit:
p(v|d) =
Z
?
z=1
p(v|t
z
) ? p(t
z
|d) (1)
The topic-word distribution p(v|t
z
) and the document-topic distribution p(t
z
|d), which are obtained
using topic modeling, respectively show the contribution of the word v in the construction of the topic
t
z
, and the distribution of topic t
z
in the document d with respect to the other topics.
We represent each new text or fragment A (e.g. from a conversation or document) by a set of proba-
bility distributions over all abstract topics T noted as P (A) = {p(t
1
|A), ..., p(t
z
|A), ..., p(t
Z
|A)} where
p(t
z
|A) is inferred using the Gibbs sampling implemented by the Mallet toolkit given the topic models
previously learned. We associate to each new document d
i
and query q
j
a set of topic probabilities ac-
cording to the above definition noted respectively as P (d
i
) = {p(t
1
|d
i
), ..., p(t
z
|d
i
), ..., p(t
Z
|d
i
)} and
P (q
j
) = {p(t
1
|q
j
), ..., p(t
z
|q
j
), ..., p(t
Z
|q
j
)}.
4.2 Diverse Merging Problem
As stated above, our goal is to recommend a short ranked list of documents answering the users? informa-
tion needs hypothesized in a conversation fragment, which are modeled by multiple topic-aware implicit
591
queries as described in Section 3. We build the final list of recommended documents by merging the
document lists, one from each implicit query, with the objective of the maximum coverage of the topics
of the conversation fragment. Since each document list contains documents found by a search engine
given an implicit query, which was prepared for one of the main topics of the conversation fragment,
we merge the lists by selecting documents from the maximum number of lists in addition to maximizing
their topical similarity to the conversation fragment.
The problem of diverse merging of lists thus amounts to finding a ranked subset of documents S ?
?
M
i=1
l
i
, which are the most representative of all the result lists l
i
, and potentially the most informative with
respect to the conversation fragment and the information needs that are implicitly stated. This problem
is an instance of the maximum coverage problem, which is known to be NP-hard. Our formulation and
solution proceed as follows.
Let us consider a set of implicit queries Q = {q
1
, ..., q
M
}, and the corresponding set of document
lists L = {l
1
, ..., l
M
} resulting from each query. M is the number of implicit queries of the fragment,
and each l
i
is a list of documents {d
1
, ..., d
N
i
} which are retrieved for query q
i
. We define the weight
w
i
of each query q
i
as the importance within the conversation fragment of the topics represented in the
query q
i
, and compute it as the topical similarity of q
i
to the fragment, as shown in Equation 2. In this
equation, q is the query made from the whole keyword set, which we call a collective query, and includes
keywords for all the main topics of the conversation fragment in one query. In turn, we associate to q a
set of probabilities over abstract topics, P (q) = {p(t
1
|q), ..., p(t
Z
|q)}, similar to the representation of
implicit queries explained in Subsection 4.1.
w
i
=
Z
?
z=1
p(t
z
|q
i
) ? p(t
z
|q) (2)
4.3 Defining a Diverse Reward Function
Although the maximum coverage problem is NP-hard, it has been shown that a greedy algorithm can
find an approximate solution guaranteed to be within a factor of (1 ? 1/e) ' 0.63 of the optimal one if
the coverage function is submodular and monotone non-decreasing
1
(Nemhauser et al., 1978). Several
monotone submodular functions have been proposed in various domains for a similar underlying prob-
lem, such as explicit diverse re-ranking of retrieval results (Agrawal et al., 2009; Santos et al., 2010;
Vargas et al., 2012), extractive summarization of a text (Lin and Bilmes, 2011; Li et al., 2012), or our
own model of diverse keyword extraction from a text (Habibi and Popescu-Belis, 2013).
We define a monotone submodular function for diverse merging of document lists inspired by the latter
two applications, who proposed a power function with a scaling exponent between 0 and 1 for diverse
selection of sentences (or keywords) covering the maximum number of topics of a given document with a
fixed number of items. To adapt these techniques to the problem of diverse merging, from the perspective
of capturing users? information needs in the set of recommended documents, we define here a reward
function enforcing the diverse merging of the lists of document results.
We first estimate the topical similarity of the document subset S
i
= S ? l
i
to the collective query q
(see Subsection 4.2) as r
S
i
:
r
S
i
=
?
d?S
i
Z
?
z=1
p(t
z
|d) ? p(t
z
|q) (3)
We then propose the following reward function f for each S
i
containing relevant documents selected
from l
i
(results of implicit query q
i
), where w
i
is the topical similarity of q
i
to the conversation fragment
(see Equation 2), and ? is an exponent parameter between 0 and 1. This reward function is submodular
because it has the diminishing returns property when r
S
i
increases.
f : r
S
i
? w
i
? r
?
S
i
(4)
1
A function F is submodular if ?A ? B ? T \ t, F (A + t) ? F (A) ? F (B + t) ? F (B) (diminishing returns) and is
monotone non-decreasing if ?A ? B, F (A) ? F (B).
592
The set S is ultimately ranked by maximizing the cumulative reward function R(S) over all the lists,
written as follows:
R(S) =
M
?
i=1
w
i
? r
?
S
i
(5)
The probability of selecting documents from the list of results for q
i
thus depends on w
i
, the topical
similarity of the query to the conversation fragment. This is in contrast to choosing the best representative
document from the list of documents relevant to each query, like in the Watson system, which does not
select more documents for queries with higher weight before considering lower weight ones. Our model
rewards diversity to increase the chance of choosing documents from all the lists of results retrieved for
implicit queries.
4.4 Finding the Optimal Document List
Since R(S) is a monotone submodular function, we propose a greedy algorithm (Alg. 1) to maximize
R(S). If ? = 1, the reward function ignores the diversity constraint, because it does not penalize multiple
selections from the same list l
i
and ranks documents only depending on their similarity to the collective
query and on the weights of implicit queries. However, when 0<?<1, as soon as a document is selected
from the list of results of an implicit query, other documents from the same list start having diminishing
returns as competitors for selection. Decreasing the value of ? increases the impact of the diversity
constraint on ranking documents, which augments the chance of recommending documents from other
document lists.
Input : query set Q of size M with probabilities, set of weights W , set of lists of document results
L with probabilities, number of recommended documents k
Output: set of recommended documents S
S ? ?;
for i = 1 toM step 1 do
S
i
? ?;
end
while |S| ? k do
S ? S ? argmax
d?((?
M
i=1
l
i
)\S)
(g(d)) where g(d) =
?
M
i=1
w
i
? [r
{d}?l
i
+ r
S
i
]
?
;
for i = 1 toM step 1 do
S
i
= l
i
? S;
end
end
return S;
Algorithm 1: Diverse merging of document results for recommendation.
5 Data, Settings and Evaluation Method
The experiments were performed on conversational data from the ELEA Corpus (Emergent LEader Anal-
ysis, Sanchez-Cortes et al. (2012)). Implicit queries were formulated as presented above in Figure 1 using
keywords extracted from each conversation fragment, defined as below (Subsection 5.1). Each subset
of keywords obtained by topical clustering of the keyword set resulted in an implicit query. The lists of
document results for each implicit query were obtained by submitting the query to the Apache Lucene
search engine
2
over the English Wikipedia
3
. These initial lists of results were ultimately merged into
final recommendation lists of documents using the four alternative methods from Figure 1, including the
one we proposed. This section presents the data, system parameters, and evaluation methods used in our
experiments.
2
Available from http://lucene.apache.org.
3
A local copy was downloaded from http://dumps.wikimedia.org.
593
5.1 Conversational Corpus
The ELEA Corpus comprises nearly ten hours of recorded meetings in English and French. Each meeting
consists in a role play game in which participants play survivors of an airplane crash in a mountainous
region. They must rank a list of 12 items with respect to their utility for surviving until they are rescued.
We used from the ELEA corpus four English conversations of around fifteen minutes each, which have
been manually transcribed and segmented at the speaker turn level.
One of the most important issues for a just-in-time document recommender system is to determine
the appropriate timing of the recommendations, and the size of the context to use for computing them.
Here, awaiting future investigations
4
, we decided to make recommendations approximately every two
minutes, at the end of an ongoing speaker turn, and consider as input the words uttered since the previous
recommendation. A segment size of two minutes enables us to collect an appropriate number of words
(neither too small nor too large) in order to extract keywords, model the topics, and formulate implicit
queries. Based on our experience with the ACLD, it also corresponds to an acceptable frequency for
receiving suggestions.
Therefore, our test data comprises 26 two-minute segments, each of them ending at a speaker change.
On average, segments contain 278 words (including stop words). Once topic modeling is applied, the
average number of topics per fragment is 5, with an observed minimum of 3 and a maximum of 9.
5.2 Parameter Settings for Experimentation
As document search is performed over the English Wikipedia, we trained our topic models on this corpus
as well. We used only a subset of it for tractability reasons, i.e. about 125,000 articles as in other studies
(Hoffman et al., 2010). The subset is randomly selected from the entire English Wikipedia. As in
previous studies, we fixed the number of topics at 100 (Boyd-Graber et al., 2009; Hoffman et al., 2010).
The exponent of the submodular function was set to ? = 0.75, as in our diverse keyword extraction
study (Habibi and Popescu-Belis, 2013). This was found to be the best value for diverse merging of lists
of results, as it leads to a reasonable balance between relevance and diversity in the aggregated list of
documents. Of course, if sufficient training data were available, this could be used to optimize ?.
The number of recommended documents was fixed at five in our experiments. This value was selected
again based on user preferences observed with the ACLD. Moreover, this is also the value of the average
number of topics in a conversation fragment, which allows the system to cover on average one result per
topic. Experiments with other values were not carried out due to the cost of evaluation.
5.3 Evaluation Protocol and Metrics
We designed a task that measures the relevance of recommended document lists for each of the test
conversation fragment. Based on validation experiments in our previous work (Habibi and Popescu-
Belis, 2012), the task requires subjects to compare two lists obtained by two different methods. Using a
web browser, the subjects had to read the conversation transcript, answer several control questions about
its content, and then decide which of the two lists provides more relevant documents, with the following
options: the first list is better than the second one; the second is better than the first; both are equally
relevant; or both are equally irrelevant. The position of each system (first or second) was randomized
across the tasks.
The 26 comparison tasks (one for each ELEA fragment) were crowdsourced via Amazon?s Mechanical
Turk as ?human intelligence tasks? (HITs). For each HIT we recruited ten workers, only accepting
those with greater than 95% approval rate and more than 1000 previously approved HITs (qualification
control). We only kept answers from the workers who answered correctly our control questions about
each HIT. Each worker could answer the entire set of 26 HITs, or part of it. We observed that the average
time spent per HIT was around 90 seconds.
4
For instance, they could combine an analysis of non-verbal information to detect ?interruptibility? and of verbal information
to detect topic changes and perform online segmentation (Mohri et al., 2010). Topic changes, however, are not appropriate
moments to make recommendations because it would be useless to recommend documents about a topic that the users no
longer discuss (Jones and Brown, 2004).
594
To consolidate the comparative judgments over a large number of subjects and conversation fragments,
and compute an aggregated score, we applied a qualification control factor to the human judgments (to
reduce the effect of judgments which disagree with the majority vote) and another one to the HITs (to
reduce the impact of undecided HITs on the global scores). This was done by using the PCC-H metric,
defined and validated in our previous work (Habibi and Popescu-Belis, 2012), which provides two scores,
one for each document list, summing up to 100%; a higher value indicates a better list. In addition to
PCC-H, we also provide below (Table 1) the raw preference scores for each comparison, i.e. the number
of times a system was preferred over another one, although PCC-H was shown to be a more reliable
indicator of quality.
6 Experimental Results
We merged and re-ranked the document lists intended to be recommended during a conversation by the
four methods presented above in Section 3 and Figure 1. Three methods merge lists of results from
topically-separated queries: SimM only considers their similarity with the fragment; Round-robin picks
the best document in each list; and our proposal, DivM, considers the diversity and importance of topics.
A fourth method, DivS, uses one query made of all keywords extracted from the conversation fragment,
and ranks the documents using the diverse re-ranking technique proposed by Santos et al. (2010).
Binary comparisons were performed between pairs of techniques, using crowdsourcing over 26 con-
versation fragments of the ELEA Corpus, and aiming to minimize the number of binary comparisons
while still ordering completely the methods according to their perceived quality.
6.1 Diverse Re-ranking vs. Similarity Merging
We first performed a comparison between the top five documents generated by two recommendation
strategies, DivS and SimM, over 26 conversation fragments of the ELEA Corpus. The consolidated rele-
vance score (PCC-H) is 75% for SimM vs. 25% for DivS, as shown in Table 1. These scores indicate the
superiority of SimM over DivS. In other words, separating the mixture of topics of a fragment into mul-
tiple topically-separated queries mitigates the negative effect of the mixture of topics on the suggestions.
6.2 Comparison across Merging Techniques
Binary comparisons were then performed between pairs of merging techniques (SimM, Round-robin,
and DivM), using the same experimental settings. The PCC-H scores are 62% for DivM vs. 38% for
Round-robin, 59% for DivM vs. 41% for SimM, and 56% for Round-robin vs. 44% for SimM, as shown
in Table 1. The scores show that the diverse merging of lists of documents improves recommendations,
and indicate the following high to low ranking: DivM > Round-robin > SimM.
SimM ranks lowest in this ordering, likely because of the ignorance of diversity in the list of results.
Round-robin is second, likely because it disregards the major differences of importance among implicit
queries in a conversation fragment. The results of the comparisons confirm that the DivM technique,
which merges lists of documents by considering the diversity of topics in the list of recommendations,
in proportion to their importance in the conversation, is the most satisfying to the majority of human
subjects.
6.3 Impact of the Topical Diversity of Fragments
To further examine the benefits of our method, we studied its sensitivity to the number of topics in the
conversation fragments. For this purpose, we divided the set of test fragments into two subsets. The first
one (noted ?A? in Table 1) gathers the fragments for which fewer than or exactly five main topics (and
therefore implicit queries) have been computed. The other fragments, with more than five main topics,
form the second subset (noted ?B?). The value of five corresponds to the average number of main topics
per fragment as well as to the number of recommended documents in our experiments.
As shown in Table 1, although there is an improvement in the comparison scores of DivS over SimM
when the number of conveyed topics in the fragments is higher than the number of recommended doc-
uments (subset B), the comparison scores indicate the superiority of SimM over DivS in both cases, and
595
PCC-H relevance score (%) Raw preferences (%)
Compared methods A B A ? B A ? B
(m
1
vs.m
2
) m
1
m
2
m
1
m
2
m
1
m
2
m
1
m
2
SimM vs. DivS 80 20 70 30 75 25 70 30
Round-robin vs. SimM 33 67 68 32 56 44 52 48
DivM vs. Round-robin 64 36 60 40 62 38 58 42
DivM vs. SimM 54 46 60 40 59 41 58 42
Table 1: Comparative scores of the recommended document lists from four methods: DivS, SimM,
Round-robin, and DivM, evaluated by human judges over the ELEA Corpus. Subset A gathers frag-
ments with fewer than or exactly five topics, while subset B gathers all the other fragments. The results
imply the following ranking: DivM > Round-robin > SimM > DivS.
confirm the benefit of the diverse merging techniques. When comparing Round-robin versus SimM, the
scores show the superiority of the former method when the number of conveyed topics in fragments is
higher than the number of recommended documents, because it provides a diverse lists of documents
in which documents relevant to less important topics are not displayed. However, when the number of
topics is smaller than the number of recommendations, SimM provides better results. The reason of the
decrease in the scores of Round-robin is likely the ignorance of the actual importance of the main topics
when ranking documents. Overall, as shown in Table 1, regardless of the number of topics conveyed in
the fragments, DivM always outperforms Round-robin and SimM.
6.4 Example of Document Results
To illustrate how DivM surpasses the other techniques, we consider an example from one of the conver-
sation fragments of the ELEA Corpus. The manual transcript of this conversation fragment is given in
the Appendix A. As described in Section 5, the conversation participants had to select a list of 12 items
vital to survive in winter while waiting to be rescued. The keywords extracted from the manual transcript
of this fragment by our method (Habibi and Popescu-Belis, 2013) are: fire, lighter, cloth, shoe, cold, die,
igloo, walking. As our keyword extraction method was shown to be robust to ASR noise, we only use
here the reference transcripts (Habibi and Popescu-Belis, submitted).
We display the topically-aware implicit queries prepared by our method from this keyword list along
with their weights in Table 2. Then, in Table 3 we show the retrieval results (five highest-ranked
Wikipedia pages) obtained by the four methods using the reference transcript of this fragment.
As shown in Table 2, each implicit query corresponds to one of the main topics of the fragment with
a specific weight. In this example, the main topics spoken in the fragment are about making an igloo,
lightening a fire, having warm clothes, and suitable shoes for walking.
As shown in Table 3, DivS provides two irrelevant documents likely because the single (collective)
query does not separate the mixture of topics in the conversation fragment, and leads to some poor results
(Wikipedia pages) such as ?Cold Fire (Koontz novel)?. SimM slightly improves the results by separating
the discussed topics of the conversation fragment into multiple queries. However, it does not cover all the
Implicit queries Weights
q
1
= {fire, cold, igloo, lighter} w
1
= 0.110
q
2
= {shoe, lighter, walking} w
2
= 0.097
q
3
= {cloth} w
3
= 0.058
q
4
= {die} w
4
= 0.040
q
5
= {igloo} w
5
= 0.026
Table 2: Example of implicit queries built from the keyword list extracted from a sample fragment of the
ELEA Corpus. Each query covers one of the main topics of the fragment and has a different weight.
596
DivS SimM Round-robin DivM
Flint spark lighter Igloo Igloo Igloo
Extended Cold Flint spark lighter Shoe Shoe
Weather Clothing System
Cold Fire (Koontz novel) Lighter Jersey (clothing) Flint spark lighter
Igloo Lighter (barge) Die Hard Jersey (clothing)
Walking Worcester Cold Flint spark lighter Lighter
Storage Warehouse fire
Table 3: Example of retrieved Wikipedia pages from the four different methods tested in this paper.
Results of diverse merging (DivM) appear to cover more topics relevant to the conversation fragment
than other methods. The average ranking (DivM > Round-robin > SimM > DivS) is also observed in
this example.
topics mentioned in the fragment due to mostly focusing on the single topic represented by q
1
. Round-
robin further enhances the results by adding diversity, but as it gives the same level of importance to all
topics, it provides a poor result like ?Die Hard? from a topic of the conversation fragment with a small
weight. The results of DivM appear to be the most useful ones, as they include other articles relevant
to q
1
, q
2
, and q
3
before showing results relevant to the low weight queries q
4
and q
5
. Therefore, in this
example, DivM provides better ranking of documents by covering the largest number of main topics
mentioned in the fragment.
7 Conclusion
We proposed a diverse merging technique for combining lists of documents from multiple topically-
separated implicit queries, prepared using keyword lists obtained from the transcripts of conversation
fragments. Our diverse merging method DivM provides a short, diverse, and relevant list of recommen-
dations, which avoids distracting participants that would consider it during the conversation. We also
compared DivM to existing merging techniques, in terms of comprehensiveness and relevance of the
final recommended list of documents to the conversation fragment. The human judgments collected via
Amazon Mechanical Turk showed that DivM outperforms all other methods.
Moreover, these results emphasized the benefit of splitting the keyword set into multiple topically-
separated queries: the suggested lists of documents from DivS (which accounts for the diversity of results
by re-ranking the documents of a single list) were indeed found less relevant than those from SimM and
the other two methods, which merged results from multiple queries.
In the future, the diverse merging method DivM will be integrated in the ACLD just-in-time retrieval
system for conversational environments, with implicit queries that are prepared from the ASR transcript
of users? conversation. User-oriented evaluation experiments will be conducted. We will also enable the
system to answer explicit queries asked by users, considering contextual factors to improve the relevance
of the answers, which will complement the recommendation functionality based on implicit queries.
Acknowledgments
The authors are grateful to the Swiss National Science Foundation for its support through the IM2 NCCR
on Interactive Multimodal Information Management (2002-2013, see http://www.im2.ch), and to
the Hasler Foundation for its support through the REMUS project (Re-ranking Multiple Search Results
for Just-in-Time Document Recommendation, 2014). The authors also thank the anonymous reviewers
for their helpful suggestions.
References
Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying search results. In
Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 5?14. ACM.
597
Jagdev Bhogal, Andy Macfarlane, and Peter Smith. 2007. A review of ontology based query expansion. Informa-
tion and Processing Management, 43:866?886.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022.
Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves:
How humans interpret topic models. In Proceedings of the 23rd Annual Conference on Neural Information
Processing Systems (NIPS), pages 1?9.
Jay Budzik and Kristian J. Hammond. 2000. User interactions with everyday applications as context for just-
in-time information access. In Proceedings of the 5th International Conference on Intelligent User Interfaces
(IUI), pages 44?51. ACM.
Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research
and development in information retrieval, pages 335?336. ACM.
Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval.
ACM Computing Surveys (CSUR), 44(1):1?56.
Ben Carterette and Praveen Chandar. 2009. Probabilistic models of ranking novel documents for faceted topic
retrieval. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1287?
1296.
Philip N. Garner, John Dines, Thomas Hain, Asmaa El Hannani, Martin Karafiat, Danil Korchagin, Mike Lincoln,
Vincent Wan, and Le Zhang. 2009. Real-time ASR from meetings. In Proceedings of Interspeech 2009 (10th
Annual Conference of the International Speech Communication Association), pages 2119?2122.
Maryam Habibi and Andrei Popescu-Belis. 2012. Using crowdsourcing to compare document recommendation
strategies for conversations. In Workshop on Recommendation Utility Evaluation: Beyond RMSE (RUE 2011),
pages 15?20.
Maryam Habibi and Andrei Popescu-Belis. 2013. Diverse keyword extraction from conversations. In Proceedings
of the ACL 2013 (51th Annual Meeting of the Association for Computational Linguistics), pages 651?657.
Maryam Habibi and Andrei Popescu-Belis. submitted. Keyword extraction and clustering for document recom-
mendation in conversations. Manuscript submitted for publication.
Peter E. Hart and Jamey Graham. 1997. Query-free information retrieval. International Journal of Intelligent
Systems Technologies and Applications, 12(5):32?37.
Matthew D. Hoffman, David M. Blei, and Francis Bach. 2010. Online learning for Latent Dirichlet Allocation.
Proceedings of 24th Annual Conference on Neural Information Processing Systems, 23:856?864.
Gareth J.F. Jones and Peter J. Brown. 2004. Context-aware retrieval for ubiquitous computing environments. In
Mobile and ubiquitous information access, pages 227?243. Springer.
Jingxuan Li, Lei Li, and Tao Li. 2012. Multi-document summarization via submodularity. Applied Intelligence,
37(3):420?430.
Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of
the ACL 2011 (49th Annual Meeting of the Association for Computational Linguistics), pages 510?520.
Andrew K. McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.
Mehryar Mohri, Pedro Moreno, and Eugene Weinstein. 2010. Discriminative topic segmentation of text and
speech. In International Conference on Artificial Intelligence and Statistics, pages 533?540.
George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. 1978. An analysis of approximations for
maximizing submodular set functions. Mathematical Programming Journal, 14(1):265?294.
Andrei Popescu-Belis, Erik Boertjes, Jonathan Kilgour, Peter Poller, Sandro Castronovo, Theresa Wilson, Alejan-
dro Jaimes, and Jean Carletta. 2008. The AMIDA Automatic Content Linking Device: Just-in-time document
retrieval in meetings. In Proceedings of MLMI 2008 (Machine Learning for Multimodal Interaction), LNCS
5237, pages 272?283.
598
Andrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. Garner. 2011. A speech-based just-in-
time retrieval system using semantic search. In Proceedings of 49th Annual Meeting of the ACL, pages 80?85.
Filip Radlinski and Susan Dumais. 2006. Improving personalized web search using result diversification. In Pro-
ceedings of the 29th annual international ACM SIGIR conference on Research and development in information
retrieval, pages 691?692. ACM.
Bradley J. Rhodes and Pattie Maes. 2000. Just-in-time information retrieval agents. IBM Systems Journal,
39(3.4):685?704.
Stephen E. Robertson. 1997. The probability ranking principle in IR. In Karen Sparck Jones and Peter Willett,
editors, Readings in information retrieval, pages 281?286. Morgan Kaufmann Publishers Inc.
Dairazalia Sanchez-Cortes, Oya Aran, Marianne Schmid Mast, and Daniel Gatica-Perez. 2012. A nonverbal
behavior approach to identify emergent leaders in small groups. IEEE Trans. on Multimedia, 14(3):816?832.
Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting query reformulations for web search
result diversification. In Proceedings of the 19th Int. Conf. on the World Wide Web, pages 881?890. ACM.
Sa?ul Vargas, Pablo Castells, and David Vallet. 2012. Explicit relevance models in intent-oriented information
retrieval diversification. In Proceedings of the 35th international ACM SIGIR conference on Research and
development in information retrieval, pages 75?84. ACM.
Jun Wang and Jianhan Zhu. 2009. Portfolio theory of information retrieval. In Proceedings of the 32nd interna-
tional ACM SIGIR conference on Research and development in information retrieval, pages 115?122. ACM.
Shengli Wu and Sally McClean. 2007. Result merging methods in distributed information retrieval with overlap-
ping databases. Information Retrieval, 10(3):297?319.
Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond independent relevance: methods and
evaluation metrics for subtopic retrieval. In Proceedings of the 26th annual international ACM SIGIR confer-
ence on Research and development in information retrieval, pages 10?17. ACM.
Appendix A. Transcript of a Conversation Fragment from the ELEA Corpus
The following transcript of a conversation fragment (speakers noted A through C) was submitted to the
document recommender system and is exemplified in Section 6.4. The corresponding implicit queries
and recommendations are respectively shown in Tables 2 and 3.
A: okay I start.
B: how how do you want to proceed?
A: I guess -
C: yes what is the most important?
A: I guess fire light.
B: fire lighter?
A: fire, yes. I would say if we had something we can fire with -- I guess that
the lighter is useful in getting some sparks.
B: hopefully.
A: so we can use either newspaper or -- something like that.
C: but again - first it is more important to have enough err clothes.
A: and for me, more important to know where to go. I would say that the compass.
C: I mean -- if you don?t have enough clothes so -- at one point you can --
B: you can die.
C: yes you can -- you will die. so first issue, try to keep yourself alive and
then you can --
A: but -- but you already have some --
B: basics. you everything. you have enormous which is and so is no shoes here.
C: okay that we have shoes so -- okay.
B: because seventy kilometers will take you how many days? err in the snow --
what do you think?
A: two or three.
B: it can be two or three days?
C: yes, but okay you cannot always have fire with you -- but you need always
have clothes with you. I mean it is the only thing that protects you when you are
walking.
B: oh yes. and erm you can make an igloo during the evening. not that cold.
only about five degrees. so lighting a fire is not so important.
C: I guess fire is an extra. I mean it is important but err for me first it is
important that when you keep walking you should be protected.
599
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 455?466,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Explaining the Stars: Weighted Multiple-Instance Learning for
Aspect-Based Sentiment Analysis
Nikolaos Pappas
EPFL and Idiap Research Institute
Rue Marconi 19
CH-1920 Martigny, Switzerland
nikolaos.pappas@idiap.ch
Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19
CH-1920 Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
This paper introduces a model of multiple-
instance learning applied to the predic-
tion of aspect ratings or judgments of
specific properties of an item from user-
contributed texts such as product reviews.
Each variable-length text is represented by
several independent feature vectors; one
word vector per sentence or paragraph.
For learning from texts with known as-
pect ratings, the model performs multiple-
instance regression (MIR) and assigns im-
portance weights to each of the sentences
or paragraphs of a text, uncovering their
contribution to the aspect ratings. Next,
the model is used to predict aspect ratings
in previously unseen texts, demonstrating
interpretability and explanatory power for
its predictions. We evaluate the model on
seven multi-aspect sentiment analysis data
sets, improving over four MIR baselines
and two strong bag-of-words linear mod-
els, namely SVR and Lasso, by more than
10% relative in terms of MSE.
1 Introduction
Sentiment analysis of texts provides a coarse-
grained view of their overall attitude towards an
item, either positive or negative. The recent abun-
dance of user texts accompanied by real-valued la-
bels e.g. on a 5-star scale has contributed to the de-
velopment of automatic sentiment analysis of re-
views of items such as movies, books, music or
other products, with applications in social com-
puting, user modeling, and recommender systems.
The overall sentiment of a text towards an item
often results from the ratings of several specific
aspects of the item. For instance, the author of
a review might have a rather positive sentiment
about a movie, having particularly liked the plot
and the music, but not too much the actors. De-
termining the ratings of each aspect automatically
is a challenging task, which may seem to require
the engineering of a large number of features de-
signed to capture each aspect. Our goal is to put
forward a new feature-agnostic solution for ana-
lyzing aspect-related ratings expressed in a text,
thus aiming for a finer-grained, deeper analysis of
text meaning than overall sentiment analysis.
Current state-of-the-art approaches to sentiment
analysis and aspect-based sentiment analysis, at-
tempt to go beyond word-level features either by
using higher-level linguistic features such as POS
tagging, parsing, and knowledge infusion, or by
learning features that capture syntactic and seman-
tic dependencies between words. Once an appro-
priate feature space is found, the ratings are typi-
cally modeled using a linear model, such as Sup-
port Vector Regression (SVR) with `
2
norm for
regularization or Lasso Regression with `
1
norm.
By treating a text globally, these models ignore the
fact that the sentences of a text have diverse con-
tributions to the overall sentiment or to the attitude
towards a specific aspect of an item.
In this paper, we propose a new learning model
which answers the following question: ?To what
extent does each part of a text contribute to the
prediction of its overall sentiment or the rating of
a particular aspect?? The model uses multiple-
instance regression (MIR), based on the assump-
tion that not all the parts of a text have the same
contribution to the prediction of the rating. Specif-
ically, a text is seen as a bag of sentences (in-
stances), each of them modeled as a word vector.
The overall challenge is to learn which sentences
refer to a given aspect, and how they contribute to
the text?s attitude towards it, but the model applies
to overall sentiment analysis as well. For instance,
Figure 1 displays a positive global comment on a
TED talk and the weights assigned to two of its
sentences by MIR.
455
Figure 1: Analysis of a comment (bag of sentences
{s
1
, ..., s
j
}) annotated by humans with the maxi-
mal positive sentiment score (5 stars). The weights
assigned by MIR reveal that s
1
has the greatest rel-
evance to the overall sentiment.
Using regularized least squares, we formulate
an optimization objective to jointly assign instance
weights and regression hyperplane weights. Then,
an instance relevance estimation method is used
to predict aspect ratings, or global ones, in previ-
ously unseen texts. The parameters of the model
are learned using an alternating optimization pro-
cedure inspired by Wagstaff and Lane (2007). Our
model requires only text with ratings for training,
with no particular assumption on the word fea-
tures to be extracted, and provides interpretable
explanations of the predicted ratings through the
relevance weights assigned to sentences. We also
show that the model has reasonable computational
demands. The model is evaluated on aspect and
sentiment rating prediction over seven datasets:
five of them contain reviews with aspect labels
about beers, audiobooks and toys (McAuley et al.,
2012), and two contain TED talks with emotion la-
bels, and comments on them with sentiment labels
(Pappas and Popescu-Belis, 2013). Our model
outperforms previous MIR models and two strong
linear models for rating prediction, namely SVR
and Lasso by more than 10% relative in terms of
MSE. The improvement is observed even when the
sophistication of the feature space increases.
The paper is organized as follows. Section 2
shows how our model innovates with respect to
previous work on MIR and rating prediction. Sec-
tion 3 formulates the problem while Section 4 de-
scribes previous MIR models. Section 5 presents
our MIR model and learning procedure. Section 6
presents the datasets and evaluation methods. Sec-
tion 7 reports our results on rating prediction tasks,
and provides examples of rating explanation.
2 Related Work
2.1 Multiple-Instance Regression
Multiple-instance regression (MIR) belongs to the
class of multiple-instance learning (MIL) prob-
lems for real-valued output, and it is a variant
of multiple regression where each data point may
be described by more than one vectors of values.
Many MIL studies focused on classification (An-
drews et al., 2003; Bunescu and Mooney, 2007;
Settles et al., 2008; Foulds and Frank, 2010; Wang
et al., 2011) while fewer focused on regression
(Ray and Page, 2001; Davis and others, 2007;
Wagstaff et al., 2008; Wagstaff and Lane, 2007).
Related to document analysis, several MIR stud-
ies have focused on news categorization (Zhang
and Zhou, 2008; Zhou et al., 2009) or web-index
recommendation (Zhou et al., 2005) but, to our
knowledge, no study has attempted to use MIR for
aspect rating prediction or sentiment analysis with
real-valued labels.
MIR was firstly introduced by Ray et al. (2001),
proposing an EM algorithm which assumes that
one primary instance per bag is responsible for
its label. Wagstaff and Lane (2007) proposed to
simultaneously learn a regression model and es-
timate instance weights per bag for crop yield
modeling (not applicable to prediction). A simi-
lar method which learns the internal structure of
bags using clustering was proposed by Wagstaff et
al. (2008) for crop yield prediction, and we will
use it for comparison in the present study. Later,
the method was adapted to map bags into a single-
instance feature space by Zhang et al. (2009).
Wang et al. (2008) assumed that each bag is gener-
ated by random noise around a primary instance,
while Wang et al. (2012) represented bag labels
with a probabilistic mixture model. Foulds et
al. (2010) concluded that various assumptions are
differently suited to different tasks, and should be
stated clearly when describing an MIR model.
2.2 Rating Prediction from Text
Sentiment analysis aims at analyzing the polar-
ity of a given text, either with classification (for
discrete labels) or regression (for real-valued la-
bels). Early studies introduced machine learning
techniques for sentiment classification, e.g. Pang
et al. (2002), including unsupervised techniques
based on the notion of semantic orientation of
phrases, e.g. Turney et al. (2002). Other studies
focused on subjectivity detection, i.e. whether a
456
text span expresses opinions or not (Wiebe et al.,
2004). Rating inference was defined by Pang et
al. (2005) as multi-class classification or regres-
sion with respect to rating scales. Pang and Lee
(2008) discusses the large range of features engi-
neered for this task, though several recent stud-
ies focus on feature learning (Maas et al., 2011;
Socher et al., 2011), including the use of a deep
neural network (Socher et al., 2013). In contrast,
we do not make any assumption about the nature
or dimensionality of the feature space.
The fine-grained analysis of opinions regarding
specific aspects or features of items is known as
multi-aspect sentiment analysis. This task usu-
ally requires aspect-related text segmentation, fol-
lowed by prediction or summarization (Hu and
Liu, 2004; Zhuang et al., 2006). Most attempts to
perform this task have engineered various feature
sets, augmenting words with topic or content mod-
els (Mei et al., 2007; Titov and McDonald, 2008;
Sauper et al., 2010; Lu et al., 2011), or with lin-
guistic features (Pang and Lee, 2005; Baccianella
et al., 2009; Qu et al., 2010; Zhu et al., 2012).
Other studies have advocated joint modeling of
multiple aspects (Snyder and Barzilay, 2007) or
multiple reviews for the same product (Li et al.,
2011). McAuley et al. (2012) introduced new cor-
pora of multi-aspect reviews, which we also partly
use here, and proposed models for aspect detec-
tion, sentiment summarization and rating predic-
tion. Lastly, joint aspect identification and senti-
ment classification have been used for aggregating
product review snippets by Sauper at al. (2013).
None of the above studies considers the multiple-
instance property of text in their modeling.
3 MIR Definition
Let us consider a set B of m bags with
numerical labels Y as input data D =
{({b
1j
}
d
n
1
, y
1
), ..., ({b
mj
}
d
n
m
, y
m
)}, where b
ij
?
R
d
(for 1 ? j ? n
i
) and y
i
? R. Each bag
B
i
consists of n
i
data points (called ?instances?),
hence it is a matrix of n
i
d-dimensional vectors,
e.g. word vectors. The challenge is to infer the
label of the bag given a variable number of in-
stances n
i
. This requires finding a set of bag rep-
resentations X = {x
1
, . . . , x
m
} of size m where
x
i
? R
d
, from which the class labels can be com-
puted. The goal is then to find a mapping from
this representation, noted ? : R
d
? R, which is
able to predict the label of a given bag. Ideally,
assuming that X is the best bag representation for
our task, we look for the optimal regression hyper-
plane ? which minimizes a loss function L plus a
regularization term ? as follows:
? = arg min
?
(
L(Y,X,?)
? ?? ?
loss
+ ?(?)
? ?? ?
reg.
)
(1)
Since the best set of representationsX for a task is
generally unknown, one has to make assumptions
to define it or compute it jointly with the regres-
sion hyperplane ?. Thus, the main difficulty lies
in finding a good assumption for X , as we will
now discuss.
4 Previous MIR Assumptions
We describe here three assumptions frequently
made in past MIR studies, to which we will later
compare our model: aggregating all instances,
keeping them as separate examples, or choosing
the most representative one (Wang et al., 2012).
For each assumption, we will experiment with
two state-of-the-art regression models (noted ab-
stractly as f ), namely SVR (Drucker et al., 1996)
and Lasso (Tibshirani, 1996) with respectively the
`
2
and `
1
norms for regularization.
The Aggregated algorithm assumes that each
bag is represented as a single d-dimensional vec-
tor, which is the average of its instances (hence
x
i
? R
d
). Then, a regression model f is trained
on pairs of vectors and class labels, D
agg
=
{(x
i
, y
i
) | i = 1, . . . ,m}, and the predicted class
of an unlabeled bag B
i
= {b
ij
| j = 1, . . . , n
i
} is
computed as follows:
y?(B
i
) = f(mean({b
ij
| j = 1, . . . , n
i
})) (2)
In fact, a simple sum can also be used instead of
the mean, and we observed in practice that with an
appropriate regularization there is no difference on
the prediction performance between these options.
This baseline corresponds to the typical approach
for text regression tasks, where each text sample is
represented by a single vector in the feature space
(e.g. BOW with counts or TF-IDF weights).
The Instance algorithm considers each of the in-
stances in a bag as separate examples, by labeling
each of them with the bag?s label. A regression
model f is learned over the training set made of
all vectors of all bags, D
ins
= {(b
ij
, y
i
) | j =
1, . . . , n
i
; i = 1, . . . ,m}, assuming that there are
m labeled bags. To label a new bag B
i
, given that
457
there is no representation x
i
, the method simply
averages the predicted labels of its instances:
y?(B
i
) = mean({f(b
ij
) | j = 1, . . . , n
i
}) (3)
Instead of the average, the median value can also
be used, which is more appropriate when the bags
contain outlying instances.
The Prime algorithm assumes that a single in-
stance in each bag is responsible for its label (Ray
and Page, 2001). This instance is called the pri-
mary or prime one. The method is similar to the
previous one, except that only one instance per bag
is used as training data: D
pri
= {(b
p
i
, y
i
) | i =
1, . . . ,m}, where b
p
i
is the prime instance of the
i
th
bag B
i
and m is the number of bags. The
prime instances are discovered through an itera-
tive algorithm which refines the regression model
f . Given an initial model f , in each iteration the
algorithm selects from each bag a prime candidate
which is the instance with the lowest prediction er-
ror. Then, a new model is trained over the selected
prime candidates, until convergence. For a new
bag, the target class is computed as in Eq. 3.
5 Proposed MIR Model
We propose a new MIR model which assigns in-
dividual relevance values (weights) to each in-
stance of a bag, thus making fewer simplifying
assumptions than previous models. We extend
instance-relevance algorithms such as (Wagstaff
and Lane, 2007) by supporting high-dimensional
feature spaces, as required for text regression, and
by predicting both the class label and the con-
tent structure of previously unseen (hence unla-
beled) bags. The former is achieved by minimiz-
ing a regularized least squares loss (RLS) instead
of solving normal equations, which is prohibitive
in large spaces. The latter represents a significant
improvement over Aggregated and Instance algo-
rithms, which are unable to pinpoint the most rel-
evant instances with respect to the label of each
bag, being thus applicable only to bag label pre-
diction. Similarly, Prime only identifies the prime
instance when the bag is already labeled. Instead,
our model learns an optimal method to aggregate
instances, rather than a pre-defined one, and al-
lows more degrees of freedom in the regression
model than previous ones. Moreover, the weight
of an instance is interpreted as its relevance both
in training and prediction.
5.1 Instance Relevance Assumption
Each bag defines a bounded region of a hyper-
plane orthogonal to the y-axis (the envelope of all
its points). The goal is to find a regression hy-
perplane that passes through each bag B
i
and to
predict its label by using at least one data point
x
i
within that bounded region. Thus, the point x
i
is a convex combination of the points in the bag,
in other words B
i
is represented by the weighted
average of its instances b
ij
:
x
i
=
n
i
?
j=1
?
ij
b
ij
, ?
ij
? 0 and
n
i
?
j=1
?
ij
= 1 (4)
where ?
ij
is the weight of the j
th
instance of the
i
th
bag. Each weight ?
ij
indicates the relevance
of an instance j to the prediction of the class y
i
of
the i
th
bag. The constraint forces x
i
to fall within
the bounded region of the points in bag i and guar-
antees that the i
th
bag will influence the regressor.
5.2 Modeling Bag Structure and Labels
Let us consider a set ofm bags, where each bagB
i
is represented by its n
i
d-dimensional instances,
i.e. B
i
= {b
ij
}
d
n
i
along with the set of target class
labels for each bag, Y = {y
i
}
N
, y
i
? R. The
representation set of all B
i
in the feature space,
X = {x
1
, . . . , x
m
}, x
i
? R
d
, is obtained using
the n
i
instance weights associated to each bag B
i
,
?
i
= {?
ij
}
n
i
, ?
ij
? [0, 1] which are initially
unknown. Thus, we look for a linear regression
model f that is able to model the target values us-
ing the regression coefficients ? ? R
d
, where X
and Y are respectively the sets of training bags and
their labels: Y = f(X) = ?
T
X . We define a loss
function according to the least squares objective
dependent on X , Y , ? and the set of weight vec-
tors ? = {?
1
, . . . , ?
m
} using Eq. 4 as follows:
L(Y,X,?,?) = ||Y ? ?
T
X||
2
2
(4)
=
N
?
i=1
(
y
i
? ?
T
(
n
i
?
j=1
?
ij
b
ij
)
)
2
=
N
?
i=1
(
y
i
? ?
T
(B
i
?
i
)
)
2
(5)
Using the above loss function, accounting for the
constraints of our assumption in Eq. 4 and assum-
ing `
2
-norm for regularization with 
1
and 
2
terms
for each ?
i
? ? and ? respectively, we obtain the
458
following least squares objective from Eq. 1:
arg min
?
1
,...,?
m
,?
m
?
i=1
(
?
2
i
????
f
1
loss
+ 
1
||?
i
||
? ?? ?
f
1
reg.
)
? ?? ?
f
2
loss
+ 
2
||?||
2
? ?? ?
f
2
reg.
where ?
2
i
=
(
y
i
? ?
T
(B
i
?
i
)
)
2
, (6)
subject to ?
ij
? 0 ?i, j and
?
n
i
j=1
?
ij
= 1 ?i.
The selection of the `
2
-norm was based on prelim-
inary results showing that it outperforms `
1
-norm.
Other combinations of p-norm regularization can
be explored for f
1
and f
2
, e.g. to learn sparser in-
stance weights and denser regression coefficients
or vice versa.
The above objective is non-convex and difficult
to optimize because the minimization is with re-
spect to all ?
1
, . . . , ?
m
and ? at the same time. As
indicated in Eq. 6 above, we will note f
1
a model
that is learned from the minimization only with re-
spect to ?
1
, . . . , ?
m
and f
2
a model obtained from
the minimization with respect to ? only. In Eq. 6,
we can observe that if one of the two is known or
held fixed, then the other one is convex and can be
learned with the well-known least squares solving
techniques. In Section 5.3, we will describe an al-
gorithm that is able to exploit this observation.
Having computed ?
1
, . . . , ?
m
and ?, we could
predict a label for an unlabeled bag using Eq. 3,
but would not be able to compute the weights
of the instances. Moreover, information that has
been learned about the instances during the train-
ing phase would not be used during prediction.
For these reasons, we introduce a third regression
model f
3
with regression coefficients O ? R
d
as-
suming a `
2
-norm for the regularization with 
3
term, which is trained on the relevance weights
obtained from the Eq. 6, D
w
= {(b
ij
, ?
ij
) | i =
1, ...,m; j = 1, ..., n
i
}. The optimization objec-
tive for the f
3
model is the following:
arg min
O
N
?
i=1
n
i
?
j=1
(
?
ij
?O
T
b
ij
)
2
? ?? ?
f
3
loss function
+ 
3
||O||
2
? ?? ?
f
3
reg.
(7)
This minimization can be easily performed with
the well-known least squares solving techniques.
The learned model is able to estimate the weights
of the instances of an unlabeled bag during pre-
diction time as:
?
?
i
= f
3
(B
i
) = ?
T
B
i
. The
?
?
i
weights are estimations which are influenced by
the relevance weights learned in our minimization
objective of Eq. 6 but they are not constrained at
prediction time. To obtain interpretable weights,
we can convert the estimated scores to the [0, 1]
interval as follows:
?
?
i
=
?
?
i
/sum(
?
?
i
). Finally,
the prediction of the label for the i
th
bag using the
estimated instance weights
?
?
i
is done as follows:
y? = f
2
(B
i
) = ?
T
B
i
?
?
i
(8)
5.3 Learning with Alternating Projections
Algorithm 1 solves the non-convex optimization
problem of Eq. 6 by using a powerful class of
methods for finding the intersection of convex sets,
namely alternating projections (AP). The prob-
lem is firstly divided into two convex problems,
namely f
1
loss function and f
2
loss function,
which are then solved in an alternating fashion.
Like EM algorithms, AP algorithms do not have
general guarantees on their convergence rate, al-
though, in practice, we found it acceptable at gen-
erally fewer than 20 iterations.
Algorithm 1 APWeights(B, Y , 
1
, 
2
, 
3
)
1: Initialize(?
1
, . . . , ?
N
,?, X)
2: while not converged do
3: for B
i
in B do
4: ?
i
= cRLS(?
T
Bi, Y
i
, 
1
) # f
1
model
5: x
i
= B
i
?
T
i
6: end for
7: ? = RLS(X,Y, 
2
) # f
2
model
8: end while
9: ? = RLS({b
ij
?i, j}, {?
ij
?i, j}, 
3
) # f
3
model
Figure 2: Visual representation for the training and
testing procedure of Algorithm 1.
The algorithm takes as input the bags B
i
, their
target class labels Y and the regularization terms

1
, 
2
, 
3
and proceeds as follows. First, under a
fixed regression model (f
2
), it proceeds with f
1
to the optimal assignment of weights to the in-
stances of each bag (projection of ? vectors on
the ?
i
space which is a n
i
-simplex) and com-
putes its new representation set X . Second, given
the fixed instance weights, it trains a new regres-
sion model (f
2
) using X (projection back to the ?
459
Bags Instances Dimension Aspect ratings
Dataset Type Count Type Count Count Classes
BeerAdvocate
review
1,200
sentence
12,189 19,418 feel, look, smell, taste, overall
RateBeer (ES) 1,200 3,269 2,120 appearance, aroma, overall, palate, taste
RateBeer (FR) 1,200 4,472 903 appearance, aroma, overall, palate, taste
Audiobooks 1,200 4,886 3,971 performance, story, overall
Toys & Games 1,200 6,463 31,984 educational, durability, fun, overall
TED comments comment 1,200 sentence 3,814 957 sentiment (polarity)
TED talks comments
per talk
1,200 comment 11,993 5,000 unconvincing, fascinating, persuasive,
ingenious, longwinded, funny, inspir-
ing, jaw-dropping, courageous, beauti-
ful, confusing, obnoxious
Table 1: Description of the seven datasets used for aspect, sentiment and emotion rating prediction.
space). This procedure repeats until convergence,
i.e. when there is no more decrease on the training
error, or until a maximum number of iterations has
been reached. The regression model f
3
is trained
on the weights learned from the previous steps.
5.4 Complexity Analysis
The overall time complexity T of Algorithm 1 in
terms of the input variables, noted h = {m, n?, d},
with m being the number of bags, n? the average
size of the bags, and d the dimensionality of the
feature space (here, the size of word vectors), is
derived as follows:
T (h) = T
ap
(h) + T
f
3
(h)
= O
(
m(n?
2
+ d
2
)
)
+ O
(
mn?d
2
)
= O
(
m(n?
2
+ d
2
+ n?d
2
)
)
, (9)
where T
ap
and T
f
3
are respectively the time com-
plexity of the AP procedure and of training the f
3
model. Eq. 9 shows that when n?  m, the model
complexity is linear with the input bags m and al-
ways quadratic with the number of features d.
Previous works on relevance assignment for
MIR have prohibitive complexity for high-
dimensional feature spaces or numerous bags and
hence they are not most appropriate for text regres-
sion tasks. Wagstaff and Lane (2007) have cubic
time complexity with the average bag size n? and
number of features d; Zhou et al. (2009) use ker-
nels, thus their complexity is quadratic with the
number of bags m; and Wang et al. (2011) have
cubic time wrt. d. Our formulation is thus com-
petitive in terms of complexity.
6 Data, Protocol and Metrics
6.1 Aspect Rating Datasets
We use seven datasets summarized in Table 1.
Five publicly available datasets were built for as-
pect prediction by McAuley et al. (2012) ? Beer-
Advocate, Ratebeer (ES), RateBeer (FR), Audio-
books and Toys & Games ? and have aspect rat-
ings assigned by their creators on the respective
websites. On the set of comments on TED talks
from Pappas and Popescu-Belis (2013), we aim
to predict two things: talk-level emotion dimen-
sions assigned by viewers through voting, and
comment polarity scores assigned by crowdsourc-
ing. The distributions of aspect ratings per dataset
are shown in Figure 3. Five datasets are in En-
glish, one in Spanish (Ratebeer) and one in French
(RateBeer), so our results will also demonstrate
the language-independence of our method.
From every dataset we kept 1,200 texts as bags
of sentences, but we also used three full-size
datasets, namely Ratebeer ES (1,259 labeled re-
views), Ratebeer FR (17,998) and Audiobooks
(10,989). The features for each of them are word
vectors with binary attributes signaling word pres-
ence or absence, in a traditional bag-of-words
model (BOW). The word vectors are provided
with the first five datasets and we generated them
for the latter two, after lowercasing and stopword
removal. Moreover, for TED comments, we com-
puted TF-IDF scores using the same dimension-
ality as with BOW to experiment with a different
feature space. The target class labels were nor-
malized by the maximum rating in their scale, ex-
cept for TED talks where the votes were normal-
ized by the maximum number of votes over all the
emotion classes for each talk, and two emotions,
?informative? and ?ok?, were excluded as they are
neutral ones.
6.2 Evaluation Protocol
We compare the proposed model, noted AP-
Weights, with four baseline ones ? Aggre-
gated, Instance, Prime (Section 4) and Clus-
460
Figure 3: Distributions of rating values per aspect rating class for the seven datasets.
tering (from github.com/garydoranjr/
mcr), which is an instance relevance method pro-
posed by Wagstaff et al. (2008) for aspect rating
prediction. First, for each aspect class, we opti-
mize all methods on a development set of 25%
of the data (300 randomly selected bags). Then,
we perform 5-fold cross-validation for every as-
pect on each entire data set and report the average
error scores using the optimal hyper-parameters
per method. In addition, we report for compar-
ison the scores of AverageRating, which always
predicts the average rating over the training set.
We report standard error metrics for regression,
namely the Mean Absolute Error (MAE) and the
Mean Squared Error (MSE). The former measures
the average magnitude of errors in a set of predic-
tions while the latter measures the average of their
squares, which are defined over the test set of bags
B
i
respectively as MAE = (
?
k
i=1
|f(B
i
)?y
i
|)/k
and MSE = (
?
k
i=1
(f(B
i
) ? y
i
)
2
)/k. The cross-
validation scores are obtained by averaging the
MAE and MSE scores on each fold.
To find the optimal hyper-parameters for each
model, we perform 3-fold cross-validation on the
development set using exhaustive grid-search over
a fine-grained range of possible values and se-
lect the ones that perform best in terms of MAE.
The hyper-parameters to be optimized for the
baselines (except AverageRating) are the regular-
ization terms ?
2
, ?
1
of their possible regression
model f , namely SVR which uses the `
2
norm
and Lasso which uses the `
1
norm. As for AP-
Weights, it relies on three regularization terms,
namely 
1
, 
2
, 
3
of the `
2
-norm for f
1
, f
2
and
f
3
regression models. Lastly, for the Clustering
baseline, we use the f
2
regression model, which
relies on 
2
and the number of clusters k, opti-
mized over {5, ..., 50} with step 5, for its cluster-
ing algorithm, here k-Means. All the regulariza-
tion terms are optimized over the same range of
possible values, noted a ? 10
b
with a ? {1, . . . , 9}
and b ? {?4, . . . ,+4}, hence 81 values per term.
For the regression models and evaluation proto-
col, we use the scikit-learn machine learning li-
brary (Pedregosa et al., 2012). Our code and data
are available in the first author?s website.
7 Experimental Results
7.1 Aspect Rating Prediction
The results for aspect rating prediction are given
in Table 2. The proposed APWeights method
outperforms Aggregated (`
2
) and Aggregated (`
1
)
i.e. SVR and Lasso along with all other baselines
on each case. The SVR baseline has on average
11% lower performance than APWeights in terms
of MSE and about 6% in terms of MAE. Simi-
larly, the Lasso baseline has on average 13% lower
MSE and 8% MAE than APWeights. As shown
in Figure 4, APWeights also outperforms them for
each aspect in the five review datasets. The In-
stance method with `
1
performed well on BeerAd-
vocate and Toys & Games (for MSE), and with `
2
performed well on Ratebeer (ES), RateBeer (FR)
and Toys & Games (for MAE). Therefore, the
instance-as-example assumption is quite appropri-
ate for this task, however both options score be-
low APWeights ? by about 5% MAE, and 8%/9%
MSE, respectively. The Prime method with `
1
per-
formed well only on the BeerAdvocate dataset and
Prime with `
2
only on the Toys & Games dataset,
always with lower scores than APWeights, namely
about 9% MAE for both and 15%/18% MSE re-
spectively. This suggests that the primary-instance
461
REVIEW LABELS
BeerAdvocate RateBeer (ES) RateBeer (FR) Audiobooks Toys & Games
Model \ Error MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE
AverageRating 14.20 3.32 16.59 4.31 12.67 2.69 21.07 6.75 20.96 6.75
Aggregated (`
1
) 13.62 3.13 15.94 4.02 12.21 2.58 20.10 6.14 20.15 6.33
Aggregated (`
2
) 14.58 3.68 14.47 3.41 12.32 2.70 19.08 5.99 18.99 5.93
Instance (`
1
) 12.67 2.89 14.91 3.54 11.89 2.48 20.13 6.17 20.33 6.34
Instance (`
2
) 13.74 3.28 14.40 3.39 11.82 2.40 19.26 6.04 19.70 6.59
Prime (`
1
) 12.90 2.97 15.78 3.97 12.70 2.76 20.65 6.46 21.09 6.79
Prime (`
2
) 14.60 3.64 15.05 3.68 12.92 2.98 20.12 6.59 20.11 6.92
Clustering (`
2
) 13.95 3.26 15.06 3.64 12.23 2.60 20.50 6.48 20.59 6.52
APWeights (`
2
) 12.24 2.66 14.18 3.28 11.37 2.27 18.89 5.71 18.50 5.57
APW vs. SVR (%) +16.0 +27.7 +2.0 +3.8 +7.6 +15.6 +1.0 +4.5 +2.6 +6.0
APW vs. Lasso (%) +10.1 +15.1 +11.0 +18.4 +6.8 +11.8 +6.0 +6.9 +8.1 +11.9
APW vs. 2
nd
best (%) +3.3 +7.8 +1.5 +3.3 +3.7 +4.9 +1.0 +4.5 +2.6 +6.0
Table 2: Performance of aspect rating prediction (the lower the better) in terms of MAE and MSE (? 100)
with 5-fold cross-validation. All scores are averaged over all aspects in each dataset. The scores of the
best method are in bold and the second best ones are underlined. Significant improvements (paired t-test,
p < 0.05) are in italics. Fig. 4 shows MSE scores per aspect for three methods on five datasets.
assumption is not the most appropriate for this
task. Lastly, even though Clustering is an instance
relevance method, it has similar scores to Prime,
presumably because the relevances are assigned
according to the computed clusters and they are
not directly influenced by the task?s objective.
To compare with the state-of-the-art results ob-
tained by McAuley et al. (2012), we experimented
with three of their full-size datasets. Splitting each
dataset in half for training vs. testing, and using
the optimal settings from our experiments above,
we measured the average MSE over all aspects.
APWeights improved over Lasso by 10%, 26%
and 17% MSE respectively on each dataset ? the
absolute MSE scores are .038 for Lasso vs. .034
for APWeights on Ratebeer SP; .023 vs. .017 on
Ratebeer FR; .063 vs. .052 on Audiobooks. Sim-
ilarly, when compared to the best SVM baseline
provided by the McAuley et al., our method im-
proved by 32%, 43% and 35% respectively on
each dataset, though it did not use their rating
model. Moreover, the best model proposed by
McAuley et al., which uses a joint rating model
and an aspect-specific text segmenter trained on
hand-labeled data, reaches MSE scores of .03,
.02 and .03, which is comparable to our model
that does not use these features (.034, .017, .052),
though it could benefit from them in the future.
Lastly, as mentioned by the same authors, predic-
tors which use segmented text, for example with
topic models as in (Lu et al., 2011), do not neces-
sarly outperform SVR baselines; instead they have
marginal or even no improvements, therefore, we
did not further experiment with them. Interes-
SENT. LABELS EMO. LABELS
TED comm. TED talks
Model \ Error MAE MSE MAE MSE
AverageRating 19.47 5.05 17.86 6.06
Aggregated (`
1
) 17.08 4.17 15.98 5.03
Aggregated (`
2
) 16.88 4.47 15.24 4.97
Instance (`
1
) 17.69 4.37 16.48 5.30
Instance (`
2
) 16.93 4.24 16.10 5.57
Prime (`
1
) 17.39 4.37 15.98 5.78
Prime (`
2
) 18.03 4.91 16.74 5.94
Clustering (`
2
) 17.64 4.34 17.71 6.02
APWeights (`
2
) 15.91 3.95 15.02 4.89
APW vs SVR (%) +5.7 +11.5 +1.5 +1.6
APW vs Lasso (%) +6.8 +5.3 +6.0 +2.9
APW vs 2
nd
(%) +5.7 +5.3 +1.5 +1.6
Table 3: MAE and MSE (? 100) on sentiment
and emotion prediction with 5-fold c.-v. Scores
on TED talks are averaged over the 12 emotions.
The scores of the best method are in bold and the
second best ones are underlined. Significant im-
provements (paired t-test, p < 0.05) are in italics.
tignly, multiple-instance learning algorithms un-
der several assumptions go beyond SVR baselines
with BOW and even more sophisticated features
such as TF-IDF (see below).
7.2 Sentiment and Emotion Prediction
Our method is also competitive for sentiment pre-
diction over comments on TED talks, as well as
for talk-level emotion prediction with 12 dimen-
sions from subsets of 10 comments on each talk
(see Table 3). APWeights outperforms SVR and
Lasso, as well as all other methods for each task.
For sentiment prediction, SVR is outperformed by
11% MSE and Lasso by 5%. For emotion pre-
462
Figure 4: MSE scores of SVR, Lasso and APWeights for each aspect over the five review datasets.
diction (averaged over all 12 aspects), differences
are smaller, at 1.6% and 2.9% respectively. These
smaller differences could be explained by the fact
that among the 10 most recent comments for each
talk, many are not related to the emotion that the
system tries to predict.
As mentioned earlier, the proposed model does
not make any assumption about the feature space.
Thus, we examined whether the improvements it
brings remain present even with a different fea-
ture space, for instance based on TF-IDF instead
of BOW with counts. For sentiment prediction on
TED comments, we found that by changing the
feature space to TF-IDF, strong baselines such as
Aggregated (`
1
) and (`
2
), i.e. SVR and Lasso, im-
prove their performance (16.25 and 16.59 MAE;
4.16 and 3.97 MSE respectively). However, AP-
Weights still outperforms them on both MAE and
MSE scores (15.35 and 3.63), improving over
SVR by 5.5% on MAE and 12.5% on MSE, and
over Lasso by 7.4% on MAE and 8.5% on MSE.
These promising results suggest that improve-
ments with APWeights could be observed also on
more sophisticated feature spaces.
7.3 Interpreting the Relevance Weights
Apart from predicting ratings, the MIR scores as-
signed by our model reflect the contribution of
each sentence to these predictions.
To illustrate the explanatory power of our model
(until a dataset for quantitative analysis becomes
available), we provide examples of predictions
on test data taken from the cross-validation folds
above. Table 5 displays the most relevant com-
Sentences per comment
?
?
i
y?
i
y
i
?Very brilliant and witty, as well as
great improvisation.?
0.64
5.0 5.0
?I enjoyed this one a lot.? 0.36
?That?s great idea, I really like it!? 0.56
4.2 4.0
?I can?t wait to try it, but first thing,
I need a house with big windows,
next year, maybe I can do that.?
0.44
?Unfortunately countries are not led
by gifted children.?
0.48
2.4 2.0
?They are either dictated by the
most extreme personalities who
crave nothing but power or man-
aged by politicians who are voted in
by a far from gifted population.?
0.52
?I am very disappointed by this,
smug, cliched and missing so much
information as to be almost (...)??
0.43
1.8 1.0
?No mention of ship transport lets
say 50% of all material transport,
no mention of rail transport, (...)?
0.29
?I am sorry to be so negative, this
just sounds like a sales pitch that he
has given too many times (...).?
0.28
Table 4: Predicted sentiment for TED comments:
y
i
is the actual sentiment, y?
i
the predicted one, and
?
?
i
the estimated relevance of each sentence.
ment for two correctly predicted emotions on two
TED talks, based on the
?
?
i
relevance scores, along
with the
?
?
i
scores of the other comments, for
two emotion classes: ?beautiful? and ?courageous?.
These comments appear to reflect correctly the
fact that the respective emotion is the majority one
in each of the comments. As noted earlier, this
task is quite challenging since we use only the ten
most recent comments for each talk.
Table 4 displays four TED comments selected
463
Class Top comment per talk (according to weights ?
i
)
?
?
i
distribution
inspiring
?It seems to me that the idea worth spreading of this TED Talk is inspiring and key for
a full life. ?No-one else is the authority on your potential. You?re the only person that
decides how far you go and what you?re capable of.? It seems to me that teens actually
think that. As a child one is all knowing and all capable. How did we get to the (...)?
beautiful
?The beauty of the nature. It would be more interesting just integrates his thought and
idea into a mobile device, like a mobile, so we can just turn on the nature gallery in any
time. The paintings don?t look incidental but genuinely thought out, random perhaps, but
with a clear grand design behind the randomness. Drawing is an art where it doesn?t (...)?
funny
?Funny story, but not as funny as a good ?knock, knock? joke. My favorite knock-knock
joke of all time is Cheech & Chong?s ?Dave?s Not Here? gag from the early 1970s. I?m
still waiting for someone to top it after all these years. [Knock, knock] ?Who is it?? the
voice of an obviously stoned male answers from the other side of a door, (...)?
courageous
?I was a soldier in Iraq and part of the unit represented in this documentary. I would ques-
tion anyone that told you we went over there to kill Iraqi people. I spent the better part
of my time in Iraq protecting the Iraqi people from insurgents who came from countries
outside of Iraq to kill Iraqi people. We protected families men, women, and (...)?
Table 5: Two examples of top comments (according to weights ?
i
) for correctly predicted emotions in
four TED talks (score 1.0) and the distribution of weights over the 10 most recent comments in each talk.
Figure 5: Top words based on ? for predicting four emotions from comments on TED talks.
from the test set of a given fold, for the comment-
level sentiment prediction task. The table also
shows the
?
?
i
relevance scores assigned to each
of the composing sentences, the predicted polar-
ity scores y?
i
and the actual ones y
i
. We observe
that the sentences that convey the most sentiment
are assigned higher scores than sentences with less
sentiment, always with respect to the global polar-
ity level. These examples suggest that, given that
APWeights has more degrees of freedom for inter-
pretation, it is able to assign relevance to parts of
a text (here, sentences) and even to words, while
other models can only consider words. Hence, the
assigned weights might be useful for other NLP
tasks mentioned below.
8 Conclusion and Future Work
This paper introduced a novel MIR model for as-
pect rating prediction from text, which learns in-
stance relevance together with target labels. To the
best of our knowledge, this has not been consid-
ered before. Compared to previous work on MIR,
the proposed model is competitive and more effi-
cient in terms of complexity. Moreover, it is not
only able to assign instance relevances on labeled
bags, but also to predict them on unseen bags.
Compared to previous work on aspect rating
prediction, our model performs significantly bet-
ter than BOW regression baselines (SVR, Lasso)
without using additional knowledge or features.
The improvements persist even when the sophis-
tication of the features increases, suggesting that
our contribution may be orthogonal to feature en-
gineering or learning. Lastly, the qualitative eval-
uation on test examples demonstrates that the pa-
rameters learned by the model are not only useful
for prediction, but they are also interpretable.
In the future, we intend to test our model on sen-
timent classification at the sentence-level, based
only on document-level supervision (T?ackstr?om
and McDonald, 2011). Moreover, we will experi-
ment with other model settings, such as regulariza-
tion norms other than `
2
and feature spaces other
than BOW or TF-IDF. In the longer term, we plan
to investigate new methods to estimate instance
weights at prediction time, and to evaluate the im-
pact of assigned weights on sentence ranking, seg-
mentation or summarization.
Acknowledgments
The work described in this article was sup-
ported by the European Union through the inEvent
project FP7-ICT n. 287872 (see http://www.
inevent-project.eu).
464
References
Stuart Andrews, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Support vector machines for
multiple-instance learning. In Advances in Neu-
ral Information Processing Systems, pages 561?568,
Vancouver, British Columbia, Canada.
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2009. Multi-facet rating of product re-
views. In Mohand Boughanem, Catherine Berrut,
Josiane Mothe, and Chantal Soule-Dupuy, editors,
Advances in Information Retrieval, volume 5478 of
Lecture Notes in Computer Science, pages 461?472.
Springer Berlin Heidelberg.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Multiple instance learning for sparse positive bags.
In Proceedings of the 24th Annual International
Conference on Machine Learning, ICML ?07, Cor-
vallis, OR, USA.
Jesse Davis et al. 2007. Tightly integrating rela-
tional learning and multiple-instance regression for
real-valued drug activity prediction. In Proceedings
of the 24th International Conference on Machine
Learning, ICML ?07, pages 425?432, Corvallis, OR,
USA.
Harris Drucker, Chris J.C. Burges, Linda Kaufman,
Alex Smola, and Vladimir Vapnik. 1996. Support
vector regression machines. In Advances in Neu-
ral Information Processing systems, pages 155?161,
Denver, CO, USA.
James Foulds and Eibe Frank. 2010. A review of
multi-instance learning assumptions. The Knowl-
edge Engineering Review, 25:1:1?25.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD Int. Conf. on Knowledge discovery
and data mining, KDD ?04, pages 168?177, Seattle,
WA.
Fangtao Li, Nathan Liu, Hongwei Jin, Kai Zhao, Qiang
Yang, and Xiaoyan Zhu. 2011. Incorporating re-
viewer and product information for review rating
prediction. In Proceedings of the 22nd International
Joint Conference on Artificial Intelligence - Volume
3, IJCAI ?11, pages 1820?1825, Barcelona, Catalo-
nia, Spain.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin K.
Tsou. 2011. Multi-aspect sentiment analysis with
topic models. In Proceedings of the 11th IEEE In-
ternational Conference on Data Mining Workshops,
ICDMW ?11, pages 81?88, Washington, DC.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
142?150, Portland, OR.
J. McAuley, J. Leskovec, and D. Jurafsky. 2012.
Learning attitudes and attributes from multi-aspect
reviews. In Proceedings of the 12th IEEE Inter-
national Conference on Data Mining, ICDM ?12,
pages 1020?1025, Brussels, Belgium.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of the 16th Int. Conf. on the World Wide
Web, WWW ?07, pages 171?180, Banff, AB.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 115?124, Ann
Arbor, MI.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings
of the ACL Conf. on Empirical Methods in Natu-
ral Language Processing, EMNLP ?02, pages 79?
86, Philadelphia, PA.
Nikolaos Pappas and Andrei Popescu-Belis. 2013.
Sentiment analysis of user comments for one-class
collaborative filtering over TED talks. In 36th ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?13, Dublin, Ireland.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2012.
Scikit-learn: Machine learning in python. CoRR,
abs/1201.0490.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rating
prediction from sparse text patterns. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, COLING ?10, pages 913?921,
Beijing, China.
Soumya Ray and David Page. 2001. Multiple instance
regression. In Proceedings of the 18th International
Conference on Machine Learning, ICML ?01, pages
425?432.
Christina Sauper and Regina Barzilay. 2013. Auto-
matic aggregation by joint modeling of aspects and
values. Journal of Artificial Intelligence Research,
46(1):89?127.
Christina Sauper, Aria Haghighi, and Regina Barzi-
lay. 2010. Incorporating content structure into
text analysis applications. In Proceedings of the
2010 Conference on Empirical Methods in Natural
465
Language Processing, EMNLP ?10, pages 377?387,
Cambridge, MA.
Burr Settles, Mark Craven, and Soumya Ray. 2008.
Multiple-instance active learning. In Advances in
Neural Information Processing Systems, NIPS ?08,
pages 1289?1296, Vancouver, BC.
Benjamin Snyder and Regina Barzilay. 2007. Multi-
ple aspect ranking using the good grief algorithm.
In In Proceedings of the Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, HLT-
NAACL ?07, pages 300?307, Rochester, NY, USA.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 151?161, Ed-
inburgh, UK.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?13, pages 1631?1642, Portland, OR.
Oscar T?ackstr?om and Ryan McDonald. 2011. Dis-
covering fine-grained sentiment with latent variable
structured prediction models. In Proceedings of the
33rd European Conference on Advances in Infor-
mation Retrieval, ECIR?11, pages 368?374, Berlin,
Heidelberg. Springer-Verlag.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society (Series B), 58:267?288.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th international conference on
World Wide Web, WWW ?08, pages 111?120, Bei-
jing, China.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 417?424, Philadelphia, PA.
Kiri L. Wagstaff and Terran Lane. 2007. Salience as-
signment for multiple-instance regression. In ICML
2007 Workshop on Constrained Optimization and
Structured Output Spaces, Corvallis, Oregon, USA.
Kiri L. Wagstaff, Terran Lane, and Alex Roper. 2008.
Multiple-instance regression with structured data. In
Proceedings of the IEEE International Conference
on Data Mining Workshops, ICDMW ?08, pages
291?300.
Zhuang Wang, Vladan Radosavljevic, Bo Han, Zoran
Obradovic, and Slobodan Vucetic. 2008. Aerosol
optical depth prediction from satellite observations
by multiple instance regression. In Proceedings of
the SIAM Int. Conf. on Data Mining, SDM ?08,
pages 165?176, Atlanta, GA.
Hua Wang, Feiping Nie, and Heng Huang. 2011.
Learning instance specific distance for multi-
instance classification. In AAAI Conference on Arti-
ficial Intelligence.
Zhuang Wang, Liang Lan, and S. Vucetic. 2012. Mix-
ture model for multiple instance regression and ap-
plications in remote sensing. IEEE Transactions on
Geoscience and Remote Sensing, 50(6):2226?2237.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308, September.
Min-Ling Zhang and Zhi-Hua Zhou. 2008. M3MIML:
A maximum margin method for multi-instance
multi-label learning. In Data Mining, 2008. ICDM
?08. Eighth IEEE International Conference on,
pages 688?697, Dec.
Min-Ling Zhang and Zhi-Hua Zhou. 2009. Multi-
instance clustering with applications to multi-
instance prediction. Applied Intelligence, 31(1):47?
68.
Zhi-Hua Zhou, Kai Jiang, and Ming Li. 2005. Multi-
instance learning based web mining. Applied Intel-
ligence, 22(2):135?147.
Zhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. 2009.
Multi-instance learning by treating instances as non-
i.i.d. samples. In Proceedings of the 26th An-
nual International Conference on Machine Learn-
ing, ICML ?09, pages 1249?1256, Montreal, Que-
bec, Canada.
Jingbo Zhu, Chunliang Zhang, and Matthew Y. Ma.
2012. Multi-aspect rating inference with aspect-
based segmentation. IEEE Trans. on Affective Com-
puting, 3(4):469?481.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM International Conference
on Information and Knowledge Management, CIKM
?06, pages 43?50, Arlington, VA.
466
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 80?85,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
A Speech-based Just-in-Time Retrieval System using Semantic Search
Andrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. Garner
Idiap Research Institute
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
{apbelis,myazdani,ananchen,pgarner}@idiap.ch
Abstract
The Automatic Content Linking Device is a
just-in-time document retrieval system which
monitors an ongoing conversation or a mono-
logue and enriches it with potentially related
documents, including multimedia ones, from
local repositories or from the Internet. The
documents are found using keyword-based
search or using a semantic similarity measure
between documents and the words obtained
from automatic speech recognition. Results
are displayed in real time to meeting partici-
pants, or to users watching a recorded lecture
or conversation.
1 Introduction
Enriching a monologue or a conversation with re-
lated content, such as textual or audio-visual docu-
ments on the same topic, is a task with multiple ap-
plications in the field of computer-mediated human-
human communication. In this paper, we describe
the Automatic Content Linking Device (ACLD), a
system that analyzes spoken input from one or more
speakers using automatic speech recognition (ASR),
in order to retrieve related content, in real-time, from
a variety of repositories. These include local doc-
ument databases or archives of multimedia record-
ings, as well as websites. Local repositories are
queried using a keyword-based search engine, or us-
ing a semantic similarity measure, while websites
are queried using commercial search engines.
We will first describe the scenarios of use of the
ACLD in Section 2, and review previous systems for
just-in-time retrieval in Section 3. The ACLD com-
ponents will be outlined in Sections 4.1 to 4.5. Four
types of evaluation results obtained with our system
will finally be summarized in Sections 5.1 to 5.4.
2 Content Linking: Scenarios of Use
Just-in-time information retrieval, i.e. finding useful
documents without the need for a user to initiate a di-
rect search for them, is one of the ways in which the
large quantity of knowledge that is available in net-
worked environments can be efficiently put to use.
To perform this task, a system must consider ex-
plicit and implicit input from users, mainly speech
or typed input, and attempt to model their context,
in order to provide recommendations, which users
are free to consult if they feel the need for additional
information.
One of the main scenarios of use for the ACLD
involves people taking part in meetings, who often
mention documents containing facts under discus-
sion, but do not have the time to search for them
without interrupting the discussion flow. The ACLD
performs this search for them. Moreover, as the
ACLD was developed on meetings from the AMI
Corpus, it can also perform the same operations on
a replayed meeting, as a complement to a meet-
ing browser, for development or demonstration pur-
poses.
In a second scenario, content linking is performed
over live or recorded lectures, for instance in a
computer-assisted learning environment for individ-
ual students. The ACLD enriches the lectures with
related material drawn from various repositories,
through a search process that can be guided in real
80
time by its user. The advantage of real-time con-
tent linking over a more static enrichment, such as
the Feynman lectures at Microsoft Research,1 is that
users can tune search parameters at will while view-
ing the lecture.
3 Just-in-Time Retrieval Systems
The first precursors to the ACLD were the Fixit
query-free search system (Hart and Graham, 1997),
the Remembrance Agent for just-in-time retrieval
(Rhodes and Maes, 2000), and the Implicit Queries
(IQ) system (Dumais et al, 2004). Fixit monitored
the state of a user?s interaction with a diagnostic
system, and excerpts from maintenance manuals de-
pending on the interaction state. The Remembrance
Agent was integrated to the Emacs text editor, and
ran searches over emails or notes at regular time in-
tervals (every few seconds) using the latest 20?500
words typed by the user. The IQ system generated
context-sensitive searches based on a user?s ongoing
activities on their computer, such as writing email.
A version of the Remembrance Agent called Jim-
miny was conceived as a wearable assistant for tak-
ing notes, but ASR was only simulated for evalua-
tion (Rhodes, 1997).
The Watson system (Budzik and Hammond,
2000) monitored the user?s operations in a text ed-
itor, but proposed a more complex mechanism than
the Remembrance Agent for selecting terms for
queries, which were directed to a web search engine.
Another assistant for an authoring environment was
developed in the A-Propos project (Puerta Melguizo
et al, 2008). A query-free system was designed for
enriching television news with articles from the Web
(Henziker et al, 2005).
The FAME interactive space (Metze and al.,
2006), which provides multi-modal access to record-
ings of lectures via a table top interface, bears many
similarities to the ACLD. However, it requires the
use of specific voice commands by one user only,
and does not spontaneously follow a conversation.
More recently, several speech-based search en-
gines have become available, including as smart
phone applications. Conversely, many systems al-
low searching of spoken document archives.2 Inspi-
1See http://research.microsoft.com/apps/tools/tuva/.
2See workshops at http://www.searchingspeech.org.
ration from these approaches, which are not query-
free, can nevertheless be useful to just-in-time re-
trieval. Other related systems are the Speech Spot-
ter (Goto et al, 2004) and a personal assistant using
dual-purpose speech (Lyons et al, 2004), which en-
able users to search for information using commands
that are identified in the speech flow.
The ACLD improves over numerous past ones by
giving access to indexed multimedia recordings as
well as websites, with fully operational ASR and se-
mantic search, as we now explain.
4 Description of the ACLD
The architecture of the ACLD comprises the follow-
ing functions: document preparation, text extraction
and indexing; input sensing and query preparation;
search and integration of results; user interface to
display the results.
4.1 Document Preparation and Indexing
The preparation of the local database of documents
for content linking involves mainly the extraction of
text, and then the indexing of the documents, which
is done using Apache Lucene software. Text can be
extracted from a large variety of formats (includ-
ing MS Office, PDF, and HTML) and hierarchies
of directories are recursively scanned. The docu-
ment repository is generally prepared before using
the ACLD, but users can also add files at will. Be-
cause past discussions are relevant to subsequent
ones, they are passed through offline ASR and then
chunked into smaller units (e.g. of fixed length, or
based on a homogeneous topic). The resulting texts
are indexed along with the other documents.
The ACLD uses external search engines to search
in external repositories, for instance the Google Web
search API or the Google Desktop application to
search the user?s local drives.
4.2 Sensing the User?s Information Needs
We believe that the most useful cues about the in-
formation needs of participants in a conversation,
or of people viewing a lecture, are the words that
are spoken during the conversation or the lecture.
For the ACLD, we use the AMI real-time ASR sys-
tem (Garner et al, 2009). One of its main features
is the use of a pre-compiled grammar, which al-
lows it to retain accuracy even when running in real-
81
time on a low resource machine. Of course, when
content linking is done over past meetings, or for
text extraction from past recordings, the ASR sys-
tem runs slower than real-time to maximize accuracy
of recognition. However, the accuracy of real-time
ASR is only about 1% lower than the unconstrained
mode which takes several times real-time.
For the RT07 meeting data, when using signals
from individual headset microphones, the AMI ASR
system reaches about 38% word error rate. With
a microphone array, this increases to about 41%.
These values indicate that enough correct words are
sensed by the real-time ASR to make it applicable
to the ACLD, and that a robust search mechanism
could help avoiding retrieval errors due to spurious
words.
The words obtained from the ASR are filtered for
stopwords, so that only content words are used for
search; our list has about 80 words. Furthermore,
we believe that existing knowledge about the impor-
tant terminology of a domain or project can be used
to increase the impact of specific words on search. A
list of pre-specified keywords can be defined based
on such knowledge and can be modified while run-
ning the ACLD. For instance, for remote control de-
sign as in the AMI Corpus scenario, this list includes
about 30 words such as ?chip?, ?button?, or ?mate-
rial?. If any of them is detected in the ASR output,
then their importance is increased for searching, but
otherwise all the other words from the ASR (minus
the stopwords) are used for constructing the query.
4.3 Querying the Document Database
The Query Aggregator (QA) uses the ASR words
to retrieve the most relevant documents from one or
more databases. The current version of the ACLD
makes use of semantic search (see next subsection),
while previous versions used word-based search
from Apache Lucene for local documents, or from
the Google Web or Google Desktop APIs. ASR
words from the latest time frame are put together
(minus the stopwords) to form queries, and recog-
nized keywords are boosted in the Lucene query.
Queries are formulated at regular time intervals, typ-
ically every 15-30 seconds, or on demand. This du-
ration is a compromise between the need to gather
enough words for search, and the need to refresh the
search results reasonably often.
The results are integrated with those from the
previous time frame, using a persistence model to
smooth variations over time. The model keeps track
of the salience of each result, initialized from their
ranking among the search results, then decreasing
in time unless the document is again retrieved. The
rate of decrease (or its inverse, persistence) can be
tuned by the user, but in any case, all past results are
saved by the user interface and can be consulted at
any time.
4.4 Semantic Search over Wikipedia
The goal of our method for semantic search is to
improve the relevance of the retrieved documents,
and to make the mechanism more robust to noise
from the ASR. We have applied to document re-
trieval the graph-based model of semantic rela-
tedness that we recently developed (Yazdani and
Popescu-Belis, 2010), which is also related to other
proposals (Strube and Ponzetto, 2006; Gabrilovich
and Markovitch, 2007; Yeh et al, 2009).
The model is grounded in a measure of seman-
tic relatedness between text fragments, which is
computed using random walk over the network of
Wikipedia articles ? about 1.2 million articles from
the WEX data set (Metaweb Technologies, 2010).
The articles are linked through hyperlinks, and also
through lexical similarity links that are constructed
upon initialization. The random walk model allows
the computation of a visiting probability (VP) from
one article to another, and then a VP between sets of
articles, which has been shown to function as a mea-
sure of semantic relatedness, and has been applied
to various NLP problems. To compute relatedness
between two text fragments, these are first projected
represented into the network by the ten closest arti-
cles in terms of lexical similarity.
For the ACLD, the use of semantic relatedness for
document retrieval amounts to searching, in a very
large collection, the documents that are the most
closely related to the words from the ASR in a given
timeframe. Here, the document collection is (again)
the set of Wikipedia articles from WEX, and the goal
is to return the eight most related articles. Such a
search is hard to perform in real time; hence, the so-
lution that was found makes use of several approx-
imations to compute average VP between the ASR
fragment and all articles in the Wikipedia network.
82
Figure 1: Unobtrusive UI displaying document results.
Hovering the mouse over a result (here, the most relevant
one) displays a pop-up window with more information
about it.
4.5 The User Interface (UI)
The main goal of the UI is to make available all
information produced by the system, in a config-
urable way, allowing users to see a larger or smaller
amount of information according to their needs. A
modular architecture with a flexible layout has been
implemented, maximizing the accessibility but also
the understandability of the results, and displaying
also intermediary data such as ASR words and found
keywords. The UI displays up to five widgets, which
can be arranged at will:
1. ASR results with highlighted keywords.
2. Tag-cloud of keywords, coding for recency and
frequency of keywords.
3. Names of documents and past meeting snippets
found by the QA.
4. Names of web pages found via the Google API.
5. Names of local files found via the Google
Desktop API.
Two main arrangements are intended, though
many others are possible: an informative full-screen
UI, shown in Figure 2 with widgets 1?4; and an un-
obtrusive widget UI, with superposed tabs, shown in
Figure 1 with widget 3.
The document names displayed in widgets 3?5
function as hyperlinks to the documents, launching
appropriate external viewers when the user clicks on
them. Moreover, when hovering over a document
name, a pop-up window displays metadata and doc-
ument excerpts that match words from the query, as
an explanation of why the document was retrieved.
5 Evaluation Experiments
Four types of evidence for the relevance and utility
of the ACLD are summarized in this section.
5.1 Feedback from Potential Users
The ACLD was demonstrated to about 50 potential
users (industrial partners, focus groups, etc.) in a
series of sessions of about 30 minutes, starting with
a presentation of the ACLD and continuing with a
discussion and elicitation of feedback. The overall
concept was generally found useful, with positive
verbal evaluations. Feedback for smaller and larger
improvements was collected: e.g. the importance of
matching context, linking on demand, and the UI un-
obtrusive mode.
5.2 Pilot Task-based Experiments
A pilot experiment was conducted by a team at the
University of Edinburgh with an earlier version of
the unobtrusive UI. Four subjects had to complete a
task that was started in previous meetings (ES2008a-
b-c from the AMI Corpus). The goal was to compare
two conditions, with vs. without the ACLD, in terms
of satisfied constraints, overall efficiency, and satis-
faction. Two pilot runs have shown that the ACLD
was being consulted about five times per meeting.
Therefore, many more runs are required to reach sta-
tistical significance of observations, and remain to
be executed depending on future resources.
5.3 Usability Evaluation of the UI
The UI was submitted to a usability evaluation ex-
periment with nine non-technical subjects. The
subjects used the ACLD over a replayed meeting
recording, and were asked to perform several tasks
with it, such as adding a keyword to monitor, search-
ing for a word, or changing the layout. The subjects
then rated usability-related statements, leading to an
assessment on the System Usability Scale (Brooke,
1996).
The overall usability score was 68% (SD: 10),
which is considered as ?acceptable usability? for the
SUS. The average task-completion time was 45?
75 seconds. In free-form feedback, subjects found
the system helpful to review meetings but also lec-
tures, appreciated the availability of documents, but
also noted that search results (with keyword-based
83
Figure 2: Full screen UI with four widgets: ASR, keywords, document and website results.
search) were often irrelevant. They also suggested
simplifying the UI (menus, layout) and embedding
a media player for use in the meeting or lecture re-
play scenario.
5.4 Comparing the Relevance of
Keyword-based vs. Semantic Search
We compared the output of semantic search with
that of keyword-based search. The ASR transcript
of one AMI meeting (ES2008d) was passed to both
search methods, and ?evaluation snippets? contain-
ing the manual transcript for one-minute excerpts,
accompanied by the 8-best Wikipedia articles found
by each method were produced. Overall, 36 snip-
pets were generated. The manual transcript shown to
subjects was enriched with punctuation and speak-
ers? names, and the names of the Wikipedia pages
were placed on each side of the transcript frame.
Subjects were then asked to read each snippet,
and decide which of the two document sets was the
most relevant to the discussion taking place, i.e. the
most useful as a suggestion to the participants. They
could also answer ?none?, and could consult the re-
sult if necessary.
Results were obtained from 8 subjects, each see-
ing 9 snippets out of 36. Every snippet was thus
seen by two subjects. The subjects agreed on 23
(64%) snippets and disagreed on 13 (36%). In fact,
the number of true disagreements not including the
answer ?none? was only 7 out of 36.
Over the 23 snippets on which subjects agreed,
the result of semantic search was judged more rel-
evant than that of keyword search for 19 snippets
(53% of the total), and the reverse for 4 snippets
only (11%). Alternatively, if one counts the votes
cast by subjects in favor of each system, regardless
of agreement, then semantic search received 72%
of the votes and keyword-based only 28%. These
numbers show that semantic search quite clearly im-
proves relevance in comparison to keyword-based
one, but there is still room for improvement.
6 Conclusion
The ACLD is, to the best of our knowledge, the
first just-in-time retrieval system to use spontaneous
speech and to support access to multimedia docu-
ments and web pages, using a robust semantic search
method. Future work will aim at improving the rel-
evance of semantic search, at modeling context to
84
improve timing of results, and at inferring relevance
feedback from users. The ACLD should also be ap-
plied to specific use cases, and an experiment with
group work in a learning environment is under way.
Acknowledgments
The authors gratefully acknowledge the support
of the EU AMI and AMIDA Integrated Projects
(http://www.amiproject.org) and of the Swiss IM2
NCCR on Interactive Multimodal Information Man-
agement (http://www.im2.ch).
References
John Brooke. 1996. SUS: A ?quick and dirty? us-
ability scale. In Patrick W. Jordan, Bruce Thomas,
Bernard A. Weerdmeester, and Ian L. McClelland, ed-
itors, Usability evaluation in industry, pages 189?194.
Taylor and Francis, London, UK.
Jay Budzik and Kristian J. Hammond. 2000. User inter-
actions with everyday applications as context for just-
in-time information access. In IUI 2000 (5th Interna-
tional Conference on Intelligent User Interfaces), New
Orleans, LA.
Susan Dumais, Edward Cutrell, Raman Sarin, and Eric
Horvitz. 2004. Implicit Queries (IQ) for contextual-
ized search. In SIGIR 2004 (27th ACM SIGIR Confer-
ence) Demonstrations, page 534, Sheffield, UK.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In IJCAI 2007 (20th Inter-
national Joint Conference on Artificial Intelligence),
pages 6?12, Hyderabad, India.
Philip N. Garner, John Dines, Thomas Hain, Asmaa
El Hannani, Martin Karafiat, Danil Korchagin, Mike
Lincoln, Vincent Wan, and Le Zhang. 2009. Real-
time ASR from meetings. In Interspeech 2009 (10th
Annual Conference of the Intl. Speech Communication
Association), pages 2119?2122, Brighton, UK.
Masataka Goto, Koji Kitayama, Katsunobu Itou, and Tet-
sunori Kobayashi. 2004. Speech Spotter: On-demand
speech recognition in human-human conversation on
the telephone or in face-to-face situations. In ICSLP
2004 (8th International Conference on Spoken Lan-
guage Processing), pages 1533?1536, Jeju Island.
Peter E. Hart and Jamey Graham. 1997. Query-free in-
formation retrieval. IEEE Expert: Intelligent Systems
and Their Applications, 12(5):32?37.
Monika Henziker, Bay-Wei Chang, Brian Milch, and
Sergey Brin. 2005. Query-free news search. World
Wide Web: Internet and Web Information Systems,
8:101?126.
Kent Lyons, Christopher Skeels, Thad Starner, Cor-
nelis M. Snoeck, Benjamin A. Wong, and Daniel Ash-
brook. 2004. Augmenting conversations using dual-
purpose speech. In UIST 2004 (17th Annual ACM
Symposium on User Interface Software and Technol-
ogy), pages 237?246, Santa Fe, NM.
Metaweb Technologies. 2010. Freebase Wikipedia Ex-
traction (WEX). http://download.freebase.com/wex/.
Florian Metze and al. 2006. The ?Fame? interactive
space. In Machine Learning for Multimodal Interac-
tion II, LNCS 3869, pages 126?137. Springer, Berlin.
Maria Carmen Puerta Melguizo, Olga Monoz Ramos,
Lou Boves, Toine Bogers, and Antal van den Bosch.
2008. A personalized recommender system for writ-
ing in the Internet age. In LREC 2008 Workshop on
NLP Resources, Algorithms, and Tools for Authoring
Aids, pages 21?26, Marrakech, Morocco.
Bradley J. Rhodes and Pattie Maes. 2000. Just-in-time
information retrieval agents. IBM Systems Journal,
39(3-4):685?704.
Bradley J. Rhodes. 1997. The Wearable Remembrance
Agent: A system for augmented memory. Personal
Technologies: Special Issue on Wearable Computing,
1:218?224.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! Computing semantic relatedness using
Wikipedia. In AAAI 2006 (21st National Conference
on Artificial Intelligence), pages 1419?1424, Boston,
MA.
Majid Yazdani and Andrei Popescu-Belis. 2010. A ran-
dom walk framework to compute textual semantic sim-
ilarity: A unified model for three benchmark tasks. In
ICSC 2010 (4th IEEE International Conference on Se-
mantic Computing), pages 424?429, Pittsburgh, PA.
Eric Yeh, Daniel Ramage, Christopher D. Manning,
Eneko Agirre, and Aitor Soroa. 2009. WikiWalk: ran-
dom walks on Wikipedia for semantic relatedness. In
TextGraphs-4 (4th Workshop on Graph-based Methods
for NLP), pages 41?49, Singapore.
85
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651?657,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Diverse Keyword Extraction from Conversations
Maryam Habibi
Idiap Research Institute and EPFL
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
maryam.habibi@idiap.ch
Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
A new method for keyword extraction
from conversations is introduced, which
preserves the diversity of topics that are
mentioned. Inspired from summarization,
the method maximizes the coverage of
topics that are recognized automatically
in transcripts of conversation fragments.
The method is evaluated on excerpts of the
Fisher and AMI corpora, using a crowd-
sourcing platform to elicit comparative
relevance judgments. The results demon-
strate that the method outperforms two
competitive baselines.
1 Introduction
The goal of keyword extraction from texts is to
provide a set of words that are representative of
the semantic content of the texts. In the applica-
tion intended here, keywords are automatically ex-
tracted from transcripts of conversation fragments,
and are used to formulate queries to a just-in-time
document recommender system. It is thus impor-
tant that the keyword set preserves the diversity of
topics from the conversation. While the first key-
word extraction methods ignored topicality as they
were based on word frequencies, more recent me-
thods have considered topic modeling factors for
keyword extraction, but without specifically set-
ting a topic diversity constraint, which is impor-
tant for naturally-occurring conversations.
In this paper, we propose a new method for key-
word extraction that rewards both word similarity,
to extract the most representative words, and word
diversity, to cover several topics if necessary. The
paper is organized as follows. In Section 2 we re-
view existing methods for keyword extraction. In
Section 3 we describe our proposal, which relies
on topic modeling and a novel topic-aware diverse
keyword extraction algorithm. Section 4 presents
the data and tasks for comparing sets of keywords.
In Section 5 we show that our method outperforms
two existing ones.
2 State of the Art in Keyword Extraction
Numerous studies have been conducted to auto-
matically extract keywords from a text or a tran-
scribed conversation. The earliest techniques have
used word frequencies (Luhn, 1957), TFIDF val-
ues (Salton et al, 1975; Salton and Buckley,
1988), and pairwise word co-occurrence frequen-
cies (Matsuo and Ishizuka, 2004) to rank words
for extraction. These approaches do not con-
sider word meaning, so they may ignore low-
frequency words which together indicate a highly-
salient topic (Nenkova and McKeown, 2012).
To improve over frequency-based methods, se-
veral ways to use lexical semantic information
have been proposed. Semantic relations be-
tween words can be obtained from a manually-
constructed thesaurus such as WordNet, or from
Wikipedia, or from an automatically-built the-
saurus using latent topic modeling techniques.
Ye et al (2007) used the frequency of all words
belonging to the same WordNet concept set, while
the Wikifier system (Csomai and Mihalcea, 2007)
relied on Wikipedia links to compute a substitute
to word frequency. Harwath and Hazen (2012)
used topic modeling with PLSA to build a the-
saurus, which they used to rank words based on
topical similarity to the topics of a transcribed con-
versation. To consider dependencies among se-
lected words, word co-occurrence has been com-
bined with PageRank by Mihalcea and Tarau
(2004), and additionally with WordNet by Wang
et al (2007), or with topical information by Z. Liu
et al (2010). However, as shown empirically by
Mihalcea and Tarau (2004) and by Z. Liu et al
(2010) with various co-occurrence windows, such
approaches have difficulties modeling long-range
dependencies between words related to the same
651
topic. Z. Liu et al (2009b) used part-of-speech in-
formation and word clustering techniques, while
F. Liu et al (2009a) added this information to
the TFIDF method so as to consider both word
dependency and semantic information. However,
although they considered topical similarity, the
above methods did not explicitly reward diversity
and might miss secondary topics.
Supervised methods have been used to learn a
model for extracting keywords with various learn-
ing algorithms (Turney, 1999; Frank et al, 1999;
Hulth, 2003). These approaches, however, rely on
the availability of in-domain training data, and the
objective functions they use for learning do not
consider yet the diversity of keywords.
3 Diverse Keyword Extraction
We propose to build a topical representation of
a conversation fragment, and then to select key-
words using topical similarity while also reward-
ing the diversity of topic coverage, inspired by
recent summarization methods (Lin and Bilmes,
2011; Li et al, 2012).
3.1 Representing Topic Information
Topic models such as Probabilistic Latent Seman-
tic Analysis (PLSA) or Latent Dirichlet Allocation
(LDA) can be used to determine the distribution
over the topic z of a word w, noted p(z|w), from a
large amount of training documents. LDA imple-
mented in the Mallet toolkit (McCallum, 2002) is
used in this paper because it does not suffer from
the overfitting issue of PLSA (Blei et al, 2003).
The distribution of each topic z in a given con-
versation fragment t, noted p(z|t), can be com-
puted by summing over all probabilities p(z|w) of
the N words w spoken in the fragment:
p(z|t) = 1N
?
w?t
p(z|w).
3.2 Selecting Keywords
The problem of keyword extraction with maximal
topic coverage is formulated as follows. If a con-
versation fragment t mentions a set of topics Z,
and each word w from the fragment t can evoke a
subset of the topics in Z, then the goal is to find
a subset of unique words S ? t, with |S| ? k,
which maximzes the number of covered topics for
each number of keywords k.
This problem is an instance of the maximum
coverage problem, which isNP -hard. Nemhauser
et al (1978) showed that a greedy algorithm can
find an approximate solution guaranteed to be
within (1 ? 1e ) ' 0.63 of the optimal solutionif the coverage function is submodular and mono-
tone nondecreasing1.
To find a monotone submodular function for
keyword extraction, we used inspiration from re-
cent work on extractive summarization methods
(Lin and Bilmes, 2011; Li et al, 2012), which pro-
posed a square root function for diverse selection
of sentences to cover the maximum number of key
concepts of a given document. The function re-
wards diversity by increasing the gain of selecting
a sentence including a concept that was not yet
covered by a previously selected sentence. This
must be adapted for keyword extraction by defin-
ing an appropriate reward function.
We first introduce rS,z , the topical similarity
with respect to topic z of the keyword set S se-
lected from the fragment t, defined as follows:
rS,z =
?
w?S
p(z|w) ? p(z|t).
We then propose the following reward function
for each topic, where p(z|t) is the importance of
the topic and ? is a parameter between 0 and 1:
f : rS,z ? p(z|t) ? r?S,z .
This is clearly a submodular function with di-
minishing returns as rS,z increases.
Finally, the keywords S ? t, with |S| ? k,
are chosen by maximizing the cumulative reward
function over all the topics, formulated as follows:
R(S) =
?
z?Z
p(z|t) ? r?S,z .
Since R(S) is submodular, the greedy algo-
rithm for maximizing R(S) is shown as Algo-
rithm 1 on the next page, with r{w},z being similar
to rS,z with S = {w}. If ? = 1, the reward func-
tion is linear and only measures the topical simila-
rity of words with the main topics of t. However,
when 0 < ? < 1, as soon as a word is selected
from a topic, other words from the same topic start
having diminishing gains.
4 Data and Evaluation Method
The proposed keyword extraction method was
tested on two conversational corpora, the Fisher
1A function F is submodular if ?A ? B ? T \ t, F (A+
t) ? F (A) ? F (B + t) ? F (B) (diminishing returns) and
is monotone nondecreasing if ?A ? B, F (A) ? F (B).
652
(a) (b)
Please select one of the following options:
1. Image (a) represents the conversation fragment better than (b).
2. Image (b) represents the conversation fragment better than (a).
3. Both (a) and (b) offer a good representation of the conversation.
4. None of (a) and (b) offer a good representation of the conversation.
Figure 1: Example of a HIT based on an AMI discussion about the impact on sales of some features of
remote controls (the conversation transcript is given in the Appendix). The word cloud was generated
using WordleTM from the list produced by the diverse keyword extraction method with ? = 0.75 (noted
D(.75)) for image (a) and by a topic similarity method (TS) for image (b). TS over-represents the topic
?color? by selecting three words related to it, but misses other topics such as ?remote control?, ?losing a
device? and ?buying a device? which are also representative of the fragment.
Input : a given text t, a set of topics Z, the
number of keywords k
Output: a set of keywords S
S ? ?;
while |S| ? k do
S ? S ? {argmaxw?t\S(h(w))where
h(w) =
?
z?Z p(z|t)[r{w},z + rS,z]?};
end
return S;
Algorithm 1: Diverse keyword extraction.
Corpus (Cieri et al, 2004), and the AMI Meeting
Corpus (Carletta, 2007). The former corpus con-
tains about 11,000 topic-labeled telephone conver-
sations, on 40 pre-selected topics (one per con-
versation). We created a topic model using Mal-
let over two thirds of the Fisher Corpus, given its
large number of single-topic documents, with 40
topics. The remaining data is used to build 11
artificial ?conversations? (1-2 minutes long) for
testing, by concatenating 11 times three fragments
about three different topics.
The AMI Corpus contains 171 half-hour meet-
ings about remote control design, which include
several topics each ? so they cannot be directly
used for learning topic models. While selecting
for testing 8 conversation fragments of 2-3 min-
utes each, we trained topic models on a subset of
the English Wikipedia (10% or 124,684 articles).
Following several previous studies, the number of
topics was set to 100 (Boyd-Graber et al, 2009;
Hoffman et al, 2010).
To evaluate the relevance (or representative-
ness) of extracted keywords with respect to a
conversation fragment, we designed comparison
tasks. In each task, a fragment is shown, followed
by three control questions about its content, and
then by two lists of nine keywords each, from two
different extraction methods. To improve readabil-
ity, the keyword lists are presented to the judges
using a word cloud representation generated by
WordleTM (http://www.wordle.net), in which the
words ranked higher are emphasized in the word
cloud (see example in Figure 1). The judges had
to read the conversation transcript, answer the con-
trol questions, and then decide which word cloud
better represents the content of the conversation.
The tasks were crowdsourced via Amazon?s
Mechanical Turk (AMT) as ?human intelligence
tasks? (HITs). One of them is exemplified in Fig-
ure 1, without the control questions, and the re-
spective conversation transcript is given in the Ap-
pendix. Ten workers were recruited for each cor-
pus. An example of judgment counts for each of
the 8 AMI HITs comparing two methods is shown
in Table 1. After collecting judgments, the com-
parative relevance values were computed by first
applying a qualification control factor to the hu-
man judgments, and then averaging results over
all judgments (Habibi and Popescu-Belis, 2012).
Moreover, to verify the diversity of the key-
653
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
1.05
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
D(0.50)
D(0.75)
TS
WF
Ranking 
??NDCG val
ue
s 
Figure 2: Average ?-NDCG over the 11 conversations from the Fisher Corpus, for 1 to 15 extracted
keywords.
word set, we use the ?-NDCG measure (Clarke
et al, 2008) proposed for information retrieval,
which rewards a mixture of relevance and diver-
sity ? with equal weights when ? = .5 as set here.
We only apply ?-NDCG to the three-topic con-
versation fragments from the Fisher Corpus, rel-
evance of a keyword being set to 1 when it be-
longs to the fragment corresponding to the topic.
A higher value indicates that keywords are more
uniformly distributed across the three topics.
5 Experimental Results
We have compared several versions of the diverse
keyword extraction method, noted D(?), for ? ?
{.5, .75, 1}, with two other methods. The first
one uses only word frequency (not including stop-
words) and is noted WF. We did not use TFIDF
because it sets low weights on keywords that are
repeated in many fragments but which are never-
theless important to extract. The second method is
based on topical similarity (noted TS) but does not
specifically enforce diversity (Harwath and Hazen,
2012). In fact TS coincides with D(1), so it is
noted TS. As the relevance of keywords for D(.5)
was already quite low, we did not test lower values
of ?. Similarly, we did not test additional values
of ? above .5 because the resulting word lists were
very similar to tested values.
First of all, we compared the four methods with
respect to the diversity constraint over the con-
HIT A B C D E F G H
TS more relevant 4 1 1 1 2 2 1 1
D(.75) more rel. 4 1 8 9 6 6 6 8
Both relevant 2 5 1 0 2 2 3 1
Both irrelevant 0 3 0 0 0 0 0 0
Table 1: Number of answers for each of the four
options of the comparative evaluation task, from
ten human judges. The 8 HITs compare the D(.75)
and TS methods on 8 AMI HITs.
Corpus Compared methods Relevance (%)
(m1 vs. m2) m1 m2
Fisher D(.75) vs. TS 68 32
TS vs. WF 82 18
WF vs. D(.5) 95 5
AMI D(.75) vs. TS 78 22
TS vs. WF 60 40
WF vs. D(.5) 78 22
Table 2: Comparative relevance scores of keyword
extraction methods based on human judgments.
catenated fragments of the Fisher Corpus, by us-
ing ?-NDCG to measure how evenly the extracted
keywords were distributed across the three topics.
Figure 2 shows results averaged over 11 conversa-
tions for various sizes of the keyword set (1?15).
The average ?-NDCG values for D(.75) and D(.5)
are similar, and clearly higher than WF and TS
for all ranks (except, of course, for a single key-
word). The values for TS are quite low, and only
increase for a large number of keywords, demon-
strating that TS does not cope well with topic di-
versity, but on the contrary first selects keywords
from the dominant topic. The values for WF are
more uniform as it does not consider topics at all.
To measure the overall representativeness of
keywords, we performed binary comparisons be-
tween the outputs of each method, using crowd-
sourcing, over 11 fragments from the Fisher Cor-
pus and 8 fragments from AMI. The goal is to
rank the methods, so we only report here on
the comparisons required for complete ordering.
AMT workers compared two lists of nine key-
words each, with four options: X more represen-
tative or relevant than Y , or vice-versa, or both
relevant, or both irrelevant. Table 1 shows the
judgments collected when comparing the output of
D(.75) with TS on the AMI Corpus. Workers dis-
agreed for the first two HITs, but then found that
the keywords extracted by D(.75) were more rep-
resentative compared to TS. The consolidated rel-
654
evance (Habibi and Popescu-Belis, 2012) is 78%
for D(.75) vs. 22% for TS.
The averaged relevance values for all compar-
isons needed to rank the four methods are shown
in Table 2 separately for the Fisher and AMI Cor-
pora. Although the exact differences vary, the hu-
man judgments over the two corpora both indi-
cate the following ranking: D(.75) > TS > WF >
D(.5). The optimal value of ? is thus around .75,
and with this value, our diversity-aware method
extracts more representative keyword sets than TS
and WF. The differences between methods are
larger for the Fisher Corpus, due to the artificial
fragments that concatenate three topics, but they
are still visible on the natural fragments of the
AMI Corpus. The low scores of D(.5) are found
to be due, upon inspection, to the low relevance
of keywords. In particular, the comparative rele-
vance of D(.75) vs. D(.5) on the Fisher Corpus is
very large (96% vs. 4%).
6 Conclusion
The diverse keyword extraction method with ? =
.75 provides the keyword sets that are judged most
representative of the conversation fragments (two
conversational datasets) by a large number of hu-
man judges recruited via AMT, and has the high-
est ?-NDCG value. Therefore, enforcing both rel-
evance and diversity brings an effective improve-
ment to keyword extraction.
Setting ? for a new dataset remains an issue,
and requires a small development data set. How-
ever, preliminary experiments with a third dataset
showed that ? = .75 remains a good value.
In the future, we will use keywords to re-
trieve documents from a repository and recom-
mend them to conversation participants by formu-
lating topically-separate queries.
Appendix: Conversation transcript of
AMI ES2005a meeting (00:00:5-00:01:52)
The following transcript of a four-party conversa-
tions (speakers noted A through D) was submitted
to our keyword extraction method and a baseline
one, generating respectively the two word clouds
shown in Figure 1.
A: The only the only remote controls
I?ve used usually come with the
television, and they?re fairly basic.
So uh
D: Yeah. Yeah.
C: Mm-hmm.
D: Yeah, I was thinking that as well,
I think the the only ones that I?ve seen
that you buy are the sort of one for
all type things where they?re, yeah. So
presumably that might be an idea to
C: Yeah the universal ones. Yeah.
A: Mm. But but to sell it for twenty
five you need a lot of neat features.
For sure.
D: put into.
C: Yeah.
D: Yeah, yeah. Uh ?cause I mean, what
uh twenty five Euros, that?s about I
dunno, fifteen Pounds or so?
C: Mm-hmm, it?s about that.
D: And that?s quite a lot for a remote
control.
A: Yeah, yeah.
C: Mm. Um well my first thoughts
would be most remote controls are grey
or black. As you said they come with
the TV so it?s normally just your basic
grey black remote control functions, so
maybe we could think about colour? Make
that might make it a bit different from
the rest at least. Um, and as you say,
we need to have some kind of gimmick, so
um I thought maybe something like if you
lose it and you can whistle, you know
those things?
D: Uh-huh. Mm-hmm. Okay. The the
keyrings, yeah yeah. Okay, that?s cool.
C: Because we always lose our remote
control.
B: Uh yeah uh, being as a Marketing
Expert I will like to say like before
deciding the cost of this remote control
or any other things we must see the
market potential for this product like
what is the competition in the market?
What are the available prices of the
other remote controls in the prices?
What speciality other remote controls
are having and how complicated it is to
use these remote controls as compared to
other remote controls available in the
market.
D: Okay.
B: So before deciding or before
finalising this project, we must discuss
all these things, like and apart from
this, it should be having a good look
also, because people really uh like
to play with it when they are watching
movies or playing with or playing with
their CD player, MP three player like
any electronic devices. They really
want to have something good, having a
good design in their hands, so, yes, all
this.
Acknowledgments
The authors are grateful to the Swiss National Sci-
ence Foundation for its financial support through
the IM2 NCCR on Interactive Multimodal Infor-
mation Management (see www.im2.ch).
655
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jonathan Boyd-Graber, Jordan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of the 23rd Annual Conference on Neural
Information Processing Systems (NIPS).
Jean Carletta. 2007. Unleashing the killer corpus:
Experiences in creating the multi-everything AMI
Meeting Corpus. Language Resources and Evalu-
ation Journal, 41(2):181?190.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher Corpus: a resource for the next
generations of speech-to-text. In Proceedings of 4th
International Conference on Language Resources
and Evaluation (LREC), pages 69?71.
Charles L. A. Clarke, Maheedhar Kolla, Gordon V.
Cormack, Olga Vechtomova, Azin Ashkan, Stefan
Bu?ttcher, and Ian MacKinnon. 2008. Novelty and
diversity in information retrieval evaluation. In Pro-
ceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 659?666.
Andras Csomai and Rada Mihalcea. 2007. Linking
educational materials to encyclopedic knowledge.
Frontiers in Artificial Intelligence and Applications,
158:557.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence (IJCAI 1999), pages 668?673,
Stockholm, Sweden.
Maryam Habibi and Andrei Popescu-Belis. 2012. Us-
ing crowdsourcing to compare document recom-
mendation strategies for conversations. In Work-
shop on Recommendation Utility Evaluation: Be-
yond RMSE (RUE 2011), page 15.
David Harwath and Timothy J. Hazen. 2012. Topic
identification based extrinsic evaluation of summa-
rization techniques applied to conversational speech.
In Proceedings of International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 5073?5076. IEEE.
Matthew D. Hoffman, David M. Blei, and Francis
Bach. 2010. Online learning for Latent Dirichlet
Allocation. Proceedings of 24th Annual Conference
on Neural Information Processing Systems, 23:856?
864.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2003),
pages 216?223, Sapporo, Japan.
Jingxuan Li, Lei Li, and Tao Li. 2012. Multi-
document summarization via submodularity. Ap-
plied Intelligence, 37(3):420?430.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the ACL.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of the 2009 Annual Conference of the
North American Chapter of the ACL (HLT-NAACL),
pages 620?628.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2009b. Clustering to find exemplar
terms for keyphrase extraction. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
257?266.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2010), pages 366?
376.
Hans Peter Luhn. 1957. A statistical approach to
mechanized encoding and searching of literary in-
formation. IBM Journal of Research and Develop-
ment, 1(4):309?317.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157?
169.
Andrew K. McCallum. 2002. MALLET:
A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 404?411,
Barcelona.
George L. Nemhauser, Laurence A. Wolsey, and Mar-
shall L. Fisher. 1978. An analysis of approxi-
mations for maximizing submodular set functions.
Mathematical Programming Journal, 14(1):265?
294.
Ani Nenkova and Kathleen McKeown, 2012. A Survey
of Text Summarization Techniques, chapter 3, pages
43?76. Springer.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information Processing and Management Journal,
24(5):513?523.
656
Gerard Salton, Chung-Shu Yang, and Clement T. Yu.
1975. A theory of term importance in automatic text
analysis. Journal of the American Society for Infor-
mation Science, 26(1):33?44.
Peter Turney. 1999. Learning to extract keyphrases
from text. Technical Report ERB-1057, National
Research Council Canada (NRC).
Jinghua Wang, Jianyi Liu, and Cong Wang. 2007.
Keyword extraction based on PageRank. In Ad-
vances in Knowledge Discovery and Data Mining
(Proceedings of PAKDD 2007), LNAI 4426, pages
857?864. Springer-Verlag, Berlin.
Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long
Qiu. 2007. Document concept lattice for text un-
derstanding and summarization. Information Pro-
cessing and Management, 43(6):1643?1662.
657
Proceedings of the TextGraphs-6 Workshop, pages 29?36,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Using a Wikipedia-based Semantic Relatedness Measure for Document
Clustering
Majid Yazdani
Idiap Research Institute and EPFL
Centre du Parc, Rue Marconi 19
1920 Martigny, Switzerland
majid.yazdani@idiap.ch
Andrei Popescu-Belis
Idiap Research Institute
Centre du Parc, Rue Marconi 19
1920 Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
A graph-based distance between Wikipedia ar-
ticles is defined using a random walk model,
which estimates visiting probability (VP) be-
tween articles using two types of links: hy-
perlinks and lexical similarity relations. The
VP to and from a set of articles is then com-
puted, and approximations are proposed to
make tractable the computation of semantic
relatedness between every two texts in a large
data set. The model is applied to document
clustering on the 20 Newsgroups data set. Pre-
cision and recall are improved in comparison
with previous textual distance algorithms.
1 Introduction
Many approaches have been proposed to compute
similarity between texts, from lexical overlap mea-
sures to statistical topic models that are learned from
large corpora. In this paper, we propose a method for
using knowledge from a structured, collaborative re-
source ? the Wikipedia hypertext encyclopedia ? in
order to build a measure of semantic relatedness that
we test on a text clustering task.
The paper first describes the document graph de-
rived from Wikipedia (Section 2), and then defines
a network-based distance using visiting probability
(Section 3), along with algorithms for its applica-
tion to text clustering (Section 4). Results over the
20 Newsgroups dataset are shown to be competitive
(Section 5), and the relative contributions of cosine
lexical similarity and visiting probability are ana-
lyzed. Our proposal is discussed in the light of pre-
vious work in Section 6.
2 The Document Network
In the present proposal, knowledge about seman-
tic relatedness is embodied into a document net-
work, whose nodes are intended to represent con-
cepts, while the links between nodes stand for var-
ious relations between concepts. The nodes of the
network correspond to articles from the Wikipedia
hypertext encyclopedia, and are derived as follows.
The network was built from Wikipedia, using the
WEX dataset (Metaweb Technologies, 2010). All
articles from the following categories were removed,
as they do not correspond to proper concepts: Talk,
File, Image, Template, Category, Portal, and List.
Moreover, disambiguation pages and articles shorter
than 100 non-stopwords were filtered out as well.
Out of 4,327,482 articles in WEX, 1,264,611 articles
were kept, forming the nodes of our network.
The first type of links in our document network
are the hyperlinks between articles, because, in prin-
ciple, each link between two articles indicates some
form of relatedness between them. There are more
than 35 million such links in our network.
The second type of links is derived from the sim-
ilarity of lexical content between articles. This is
computed using cosine similarity between the lexi-
cal vectors corresponding to the articles? texts, after
stopword removal and stemming. Then, links are
created by connecting every article to the 10 arti-
cles that are most similar to it, each link receiving
a weight which is the normalized lexical similarity
score. The number 10 was chosen to ensure compu-
tational tractability, and is in the same range as the
average number of hyperlinks per node (30).
29
Computing semantic relatedness between two
texts requires: (1) to estimate relatedness between
two sets of nodes in the network, as described in
Sections 3 and 4; and (2) to project each text onto
a set of nodes, as we briefly explain here. The pro-
jection of a text onto the network is found by com-
puting the text?s lexical similarity with all articles,
again using cosine distance over stemmed words,
without stopwords. The text is mapped to the 10
closest articles, resulting in a probability distribution
over the 10 corresponding nodes. Again, this value
was chosen to be similar to the number of hyperlinks
and content links per node, and to keep computation
tractable. In fact, the numerous Wikipedia articles
are scattered in the space of words, therefore tuning
these values does not seem to bring crucial changes.
3 Computing Relatedness in the Network
Using Visiting Probability (VP)
We have previously defined a random walk model
(Yazdani and Popescu-Belis, 2010) to compute re-
latedness of sets of nodes as the visiting probability
(VP ) of a random walker from one set to another
one, and we will review the model in this section. In
the next section, we will explain how the model was
extended for application to document clustering.
3.1 Notations
Let S = {si|1 ? i ? n} be the set of n nodes
in the graph. Any two nodes si and sj can be con-
nected by one or more directed and weighted links,
which can be of L different types (L = 2 in our
case: hyperlinks and lexical similarity links). Links
between nodes can thus be represented by L matri-
ces Al (1 ? l ? L) of size n ? n, where Al(i, j)
is the weight of the link of type l between si and sj .
The transition matrix Cl gives the probability of a
direct transition between nodes si and sj , using only
links of type l. This matrix can be built from the Al
matrix as follows:
Cl(i, j) =
Al(i, j)
?n
k=1Al(i, k)
.
In the random walk process using all link types
(1 ? l ? L), let the weight wl denote the impor-
tance of link type l. Then, the overall transition ma-
trix C giving the transition probability Ci,j between
nodes si and sj is : C =
?L
l=1wlCl.
One of the main parameters in this computation
is the relative weight of the two types of links (lex-
ical similarity and hyperlinks) in the random walk
over the network. The settings for the experiments
on document clustering (0.6 vs. 0.4) are explained in
Section 5.1 below.
3.2 VP from a Set of Nodes to a Node
Let us consider a probability distribution ~r over
nodes, corresponding to the projection of a text frag-
ment onto the network of articles (Section 2). Given
a new node sj in the network, our model first esti-
mates the probability of visiting sj for the first time
when a random walker starts from ~r in the graph.
The model considers the state St of the random
walker (its position node) and provides a procedure
which, executed until termination, yields the value
of VP . Namely, the initial state is chosen at random
with probability P (S0 = si|~r) = ri (where the ri
are the components of ~r). Then, from state St?1, ei-
ther St?1 = sj and the procedure is finished, or the
next node is chosen using the transition matrix C.
Moreover, it is also possible to ?fail? the walk with
a small probability, called ?absorption probability?,
which makes longer paths less probable.
3.3 Differences between VP and PageRank or
Hitting Time
The VP of sj starting from the distribution ~r, as
computed here, is different from the probability as-
signed to sj after running Personalized PageRank
(Haveliwala, 2003) with a teleport vector equal to ~r.
In the computation of VP , the loops starting from sj
and ending to the same sj do not have any effect on
the final score, unlike for PPR, for which such loops
boost the probability of sj . If some pages have this
type of loops (typically, very ?popular? pages), then
after using PPR they will have high probability al-
though they might not be very close to the teleport
vector ~r.
The VP of sj is also different from the hitting
time to sj , defined as the average number of steps
a random walker would take to visit sj for the first
time in the graph starting from ~r. Hitting time is
more sensitive to long paths in comparison to VP ,
a fact that might introduce more noise.The perfor-
mance of these three algorithms in computing se-
30
mantic similarity has been compared in (Yazdani
and Popescu-Belis, 2010).
3.4 VP between Sets of Nodes
Generalizing now to the computation of VP from
a weighted set of nodes ~r1 (a probability distribu-
tion) to another set ~r2, the model first constructs a
virtual node representing ~r2 in the network, named
by convention sR, and then connects all nodes si to
sR according to their weights in ~r2. The transition
matrix for the random walk is updated accordingly.
To compute relatedness of two texts projected
onto the network as ~r1 and ~r2, the VP of ~r1 given
~r2 is averaged with the converse probability, of ~r2
given ~r1 ? a larger probability indicating closer se-
mantic relatedness.
3.5 Truncated VP
The computation of VP can be done iteratively and
can be truncated after a number of steps, as the im-
portance of longer paths grows smaller due to the
absorption probability, leading thus to a T -truncated
visiting probability noted VPT . Besides making
computation more tractable, truncation reduces the
effect of longer paths, which seem to be less reliable
indicators of relatedness.
We have computed an upper bound on the trun-
cation error, which helps to control and minimize
the number of steps actually computed in a random
walk. To compute the upper bound of the truncation
error we compute the probability of returning neither
success (reaching sj) nor failure (absorption) in first
t steps, which can be computed as
?n
i 6=j ?
t(~rC ?t)i.
This is in fact the probability mass at time t at all
nodes except sj , the targeted node. C ? is the transi-
tion matrix that gives the probability of a transition
between two nodes, modified to include the virtual
node sR in the network, and 1? ? is the absorption
probability.
If pt(success) denotes the probability of success
(reaching sj) considering paths of length at most t,
and ?t the error made by truncating after step t, then
we have:
?t = p(success)? pt(success) ?
?n
i 6=j ?
t(~rC ?t)i
So, if pt(success) is used as an approximation for
p(success) then an upper bound for this approxima-
tion error ?t is the right term of the above inequality.
4 Application of VP to Text Clustering
In this section, we describe the additional modeling
that was done so that semantic relatedness based on
VP could be applied efficiently to text clustering.
Indeed, it is not tractable to individually compute
the average VP between any two texts in the set of
documents to be clustered, because the numbers of
pairs is very large ? e.g., 20,000 documents in the
experiments in Section 5. Instead, we propose two
solutions for computing, respectively, VP to a set of
nodes (from all documents in the network), and re-
spectively VP from a set of nodes to all documents.
4.1 Computing VP from All Nodes to a Subset
To compute the T -truncated visiting probability
(noted VPT ) from all nodes in the network to a node
sR at the same time, the following recursive pro-
cedure is defined. Here, T is the number of steps
before truncation, and sR is a virtual node repre-
senting a probability distribution ~r from a text. The
procedure is based on the definition of VP between
nodes in Section 3 and uses the transition matrix C ?
that gives the probability of a transition between two
nodes, modified to include the virtual node sR in the
network. If 1? ? is the absorption probability, then
the recursive definition of VPT from a node si to
the virtual node sR is:
VPT (si, sR) = ?
?
k C
?(si, sk)VPT?1(sk, sR)
Using dynamic programming, it is possible to
compute VPT from all nodes to sR inO(ET ) steps,
where E is the number of links in the network.
The initialization of the procedure is done using
VPT (sR, sR) = 1 and VP0(si, sR) = 0 for any
i 6= R.
4.2 Computing VP from a Subset to All Nodes
To compute the truncated VP from ~r to all nodes
in the network, the total computation time using
the definition of VPT from Section 3 is O(ETN ),
where N is the number of nodes in the network, be-
cause VPT must be computed for each node sepa-
rately. For a large data set, this is not tractable.
The proposed solution is based on a sampling
method over the random walks to approximate
VPT . The sampling involves running M indepen-
dent random walks of length T from ~r. For a given
31
node sj and a sample walk m, the first time (if any)
when sj is visited on each random walk starting
from ~r is noted tjm . Then, VP
T can be estimated
by the following average over sample walks, where
1? ? is again the absorption probability:
?VPT (~r, sj) = (
?
m ?
tjm )/M.
As a result, the estimate of VPT can be computed
in O(MT ) steps, where M is the number of sample
paths.
Moreover, it is possible to compute a bound on the
error of the estimation, |VPT? ?VPT |, depending on
the number of sample paths M . It can be shown that
the error is lower than ?, with a probability larger
than 1 ? ?, on condition that the number of sample
paths is greater than ?2 ln(2/?)/2?2.
To prove this bound, we use inspiration from a
proof by Sarkar et al (2008). If the estimation of a
variableX is noted X? , let us suppose that concept sj
has been visited for the first time at {tj1 , ? ? ? , tjm}
time steps in the M sample walks. We define the
random variable X l by ?tjl/M , where tjl indicates
the time step at which sj was visited for the first
time in lth sampling. If sj was not visited at all,
then X l = 0 by convention. The l random variables
X l (j1 ? l ? jm) are independent and bounded by
0 and 1 (0 ? X l ? 1). We have:
?VPT (~r, sj) =
?
lX
l = (
?
l ?
tjl )/M and
E( ?VPT (~r, sj)) = VPT (~r, sj).
So, by applying Hoeffding?s inequality, we have:
P (| ?VPT ? E( ?VPT )| ? ) ? 2exp(?2M
2
?2 ).
If the probability of error must be at most ?, then
setting the right side lower than ? gives the bound
for M that is stated in our theorem.
As a consequence, we have the following lower
bound forM if we want ?-approximation for all pos-
sible sj with probability at least 1? ?. We use union
bound and Hoeffding?s inequality:
P (?j ? {1, . . . , n}, | ?VPT ? E( ?VPT )| ? ) ? 2n?
exp(?2M
2
?2 )
which gives the lower bound M ? ?
2 ln(2n/?)
22 .
5 Document Clustering
This section describes the experimental setting and
the results of applying the text relatedness measure
defined above to the problem of document cluster-
ing over the 20 Newsgroups dataset.1 The dataset
contains about 20,000 postings to 20 news groups,
hence 20 document classes, with about 1,000 docu-
ments per class. We aim here at finding these classes
automatically, using for testing the entire data set
without using any part of it as a training set. The
knowledge of our system comes entirely from the
document network and the techniques for comput-
ing distances between two texts projected onto it.
5.1 Setup of the Experiment
We first compute a similarity matrix for the entire 20
Newsgroups data set, with the relatedness score be-
tween any two documents being VPT . For tractabil-
ity, we fixed T = 5 that gives sufficient precision; a
larger value only increased computation time. In-
stead of computing VPT between all possible pairs
separately, we fill one row of the matrix at a time
using the approximations above.
We set the absorption probability of the random
walk 1 ? ? = 0.2 for this experiment. Given ?
and T by using the formula in section 3.5, it is pos-
sible to compute the error bound of the truncation,
and noting that for a smaller ?, fewer steps (T ) are
needed to achieve the same approximation precision
because of the penalty set to longer paths. Con-
versely, a larger ? decreases the penalty for longer
paths and requires more computation.2
For comparison purposes, four similarity matri-
ces were computed. Indeed, the theoretical appara-
tus described above can be applied to various types
of links in the document network. In Section 2, we
introduced two types of links, namely lexical simi-
larity and actual hyperlinks, and these can be used
separately in the model, or as a weighted combina-
tion. The following similarities will be compared:
1. VP over hyperlinks only (noted VPHyp);
2. VP over lexical similarity links (VPLex);
1Distributed at http://www.cs.cmu.edu/afs/cs.
cmu.edu/project/theo-20/www/data/news20.
html, see also (Mitchell, 1997, Chapter 6).
2Note that in the extreme case when ? = 0, similarity to all
nodes except the node itself is zero.
32
3. VP over a combination of hyperlinks (0.4) and
lexical links (0.6) (noted VPComb) ? these val-
ues gave the best results in our previous appli-
cations to word and document similarity tasks
(Yazdani and Popescu-Belis, 2010);
4. no random walk, only cosine similarity be-
tween the tf-idf vectors of the documents to be
clustered (noted LS , for lexical similarity).
5.2 Clustering Performance
Clustering is performed using a k-means algorithm
over each of the four similarity matrices.3 The qual-
ity of the clustering is first measured using the Rand
Index (RI), which counts the proportion of pairs of
documents that are similarly grouped, i.e. either in
the same, or in different clusters, in the reference
vs. candidate clusterings. Other methods exist (Pan-
tel and Lin, 2002), including a Rand Index adjusted
for chance (Vinh et al, 2009), but the RI suffices
for comparative judgments in this subsection. How-
ever, in Subsection 5.3, we will also look at preci-
sion and recall, and in Subsection 5.4 we will use
purity. As the clustering is performed over the entire
data set, because there is no training vs. test data,
confidence intervals are not available, though they
could be computed by splitting the data. As a result,
comparison with other scores on the same test set is
absolute.
The scores in terms of Rand Index are, in decreas-
ing order:
1. 90.8% for VPComb
2. 90.6% for VPHyp
3. 90.4% for VPLex
4. and only 86.1% for the LS cosine similarity.
The random walk model thus clearly outperforms
the baseline LS approach. If counting only wrongly
clustered document pairs, VPComb has 6.6% of such
pairs, while VPLex has 8.4%, confirming the lower
performance of the model using only lexical similar-
ity links, i.e. the utility of hyperlinks.
3The semantic relatedness measure proposed here could be
used with other clustering algorithms, such as the committee-
based method proposed by Pantel and Lin (2002).
5.3 Comparison to Other Methods
To obtain a better understanding of the performance
of the proposed method, we computed the clustering
precision and recall of several well-known methods
for statistical text representation, shown in Table 1.
For Latent Dirichlet Allocation (LDA) (Blei et al,
2003) and Latent Semantic Analysis (LSA) (Deer-
wester et al, 1990), we first mapped the documents
in the latent space and then computed the cosine
similarity between the documents in the latent space.
The number of topics for LSA and LDA is set to 100
to make the computation tractable. Precision and re-
call are used, rather than the Rand Index, to show in
more detail the performance of each method. The
use of VP over our document network clearly in-
creases both precision and recall in comparison to
other tested approaches.
Similarity method Precision Recall
LS 7.50 18.38
LSA 8.63 9.99
LDA 19.93 31.50
VPComb 23.81 35.32
Table 1: Precision and Recall for k-means clustering over
the 20 Newsgroups using several well-known methods to
compute text similarity, in comparison to the present pro-
posal.
5.4 Analysis of the Impact of VP with Respect
to Cosine Similarity
To find out in which cases the proposed method im-
proves over a simple cosine similarity measure, we
considered a linear combination of the cosine simi-
larity and VP , noted w?VPComb+(1?w)?LS ,
and varied the weight w from 0 to 1. Considering
the k-nearest neighbors of every document accord-
ing to this combined similarity, we define k-purity
as the number of documents with the correct label
over the total number of documents k in the com-
puted neighborhood. The variation of k-purity with
w, for several values of k, is shown in Figure 1.
The best purity appears to be obtained for a com-
bination of the two methods, for all values of k that
were tested. This shows that VPComb brings valu-
able additional information about document relat-
edness that cannot be found in LS only. Further-
33
Figure 1: Values of k-purity (vertical axis) averaged over all documents, for neighborhoods of different sizes k. The
horizontal axis indicates the weightw of visiting probability vs. cosine lexical similarity in the formula: w?VPComb+
(1? w)? LS .
more, when the size of the examined neighborhood
k increases (lower curves in Figure 1), the effect of
VPComb becomes more important, i.e. its weight in
the optimal combination increases. For very small
neighborhoods, LS is almost sufficient to ensure op-
timal purity, but for larger ones (k = 10 or 15),
VPComb used alone (w = 1) outperforms LS used
alone (w = 0). Their optimal combination leads to
scores that are higher than those obtained for each
of them used separately, and, as noted, the weight of
VPComb in the optimal combination increases for
larger neighborhoods.
These results can be explained as follows. For
very small neighborhoods, the cosine lexical simi-
larity score with the nearest 1?5 documents is very
high, as they have many words in common, so LS is
a good measure of text relatedness. However, when
looking at larger neighborhoods, for which related-
ness is less based on identical words, then VPComb
becomes more effective, and LS performs poorly.
Therefore, we can predict that VPComb will be most
relevant when looking for larger neighborhoods, or
in order to increase recall. VPComb should also be
relevant when there is low diversity among docu-
ment words, for instance when all documents are
very short.
6 Related Work
Many attempts have been made to improve the
overlap-based lexical similarity distance, for various
applications to HLT. One approach is to construct
a taxonomy of concepts and relations (manually or
automatically) and to map the text fragments to be
compared onto the taxonomy. For instance, Word-
net (Fellbaum, 1998) and Cyc (Lenat, 1995) are two
well-known knowledge bases that can be used for
enriching pure lexical matching. However, building
and maintaining such resources requires consider-
able effort, and they might cover only a fraction of
the vocabulary of a language, as they usually include
few proper names or technical terminology.
Another approach makes use of unsupervised
methods to construct a semantic representation of
34
documents by analyzing mainly co-occurrence rela-
tionships between words in a corpus. Latent Seman-
tic Analysis (Deerwester et al, 1990), Probabilistic
LSA (Hofmann, 1999) and Latent Dirichlet Alloca-
tion (Blei et al, 2003) are unsupervised methods that
construct a low-dimensional feature representation
or concept space, in which words are no longer sup-
posed to be independent.
Mihalcea et al (2006) compared knowledge-
based and corpus-based methods, using word sim-
ilarity and word specificity to define one general
measure of text semantic similarity. Because it com-
putes word similarity values between all word pairs,
the proposed method appears to be suitable mainly
to compute similarity between short fragments, oth-
erwise the computation becomes intractable.
WikiRelate! (Strube and Ponzetto, 2006) com-
putes semantic relatedness between two words by
using Wikipedia. Each word is mapped to the corre-
sponding Wikipedia article by using article titles. To
compute relatedness, several methods are proposed,
namely, using paths in the Wikipedia category struc-
ture or the articles? content. Our method, by compar-
ison, also uses the knowledge embedded in the hy-
perlinks between articles, as well as the entire con-
tents of articles, but unlike WikiRelate! it has been
extended to texts of arbitrary lengths.
Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), instead of mapping a text to a
node or a small group of nodes in a taxonomy, maps
the text to the entire collection of available con-
cepts, by computing the degree of affinity of each
concept to the input text. Similarity is measured
in the new concept space. ESA does not use the
link structure or other structured knowledge from
Wikipedia. Moreover, by walking over a content
similarity graph, our method benefits from a non-
linear distance measure according to the paths con-
sisting of small neighborhoods.
In the work of Yeh et al (2009), a graph of docu-
ments and hyperlinks is computed from Wikipedia,
then a Personalized PageRank (Haveliwala, 2003) is
computed for each text fragment, with the teleport
vector being the one resulting from the ESA algo-
rithm cited above. To compute semantic relatedness
between two texts, Yeh et al (2009) simply compare
their personalized page rank vectors. By compari-
son, in our method, we also consider in addition to
hyperlinks the effect of word co-occurrence between
article contents. The use of visiting probability also
gives different results over personalized page rank,
as it measures different properties of the network.
There are many studies on measuring distances
between vertices in a graph. Two measures that are
close to the visiting probability proposed here are
hitting time and Personalized PageRank mentioned
in Section 3.3. Hitting time has been used in var-
ious studies as a distance measure in graphs, e.g.
for dimensionality reduction (Saerens et al, 2004)
or for collaborative filtering in a recommender sys-
tem (Brand, 2005). Hitting time was also used for
link prediction in social networks along with other
distances (Liben-Nowell and Kleinberg, 2003), or
for semantic query suggestion using a query/URL
bipartite graph (Mei et al, 2008). As for Personal-
ized PageRank, it was used for word sense disam-
biguation (Agirre and Soroa, 2009), and for measur-
ing lexical relatedness of words in a graph built from
WordNet (Hughes and Ramage, 2007).
7 Conclusion
We proposed a model for measuring text seman-
tic relatedness based on knowledge embodied in
Wikipedia, seen here as document network with two
types of links ? hyperlinks and lexical similarity
ones. We have used visiting probability to mea-
sure proximity between weighted sets of nodes, and
have proposed approximation algorithms to make
computation efficient for large graphs (more than
one million nodes and 40 million links) and large
text clustering datasets (20,000 documents in 20
Newsgroups). Results on the document clustering
task showed an improvement using both word co-
occurrence information and user-defined hyperlinks
between articles over other methods for text repre-
sentation.
Acknowledgments
The work presented in this paper has been supported
by the IM2 NCCR (Interactive Multimodal Infor-
mation Management) of the Swiss National Science
Foundation (http://www.im2.ch).
35
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of EACL 2009 (12th Conference of the European
Chapter of the Association for Computational Linguis-
tics), pages 33?41, Athens, Greece.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Matthew Brand. 2005. A random walks perspective on
maximizing satisfaction and profit. In Proceedings of
the 2005 SIAM International Conference on Data Min-
ing, Newport Beach, CA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press, Cambridge, MA.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of IJCAI
2007 (20th International Joint Conference on Artifi-
cial Intelligence), pages 6?12, Hyderabad.
Taher H. Haveliwala. 2003. Topic-sensitive pagerank:
A context-sensitive ranking algorithm for web search.
IEEE Transactions on Knowledge and Data Engineer-
ing, 15:784?796.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proceedings of SIGIR 1999 (22nd ACM
SIGIR Conference on Research and Development in
Information Retrieval), pages 50?57, Berkeley, CA.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In
Proceedings of EMNLP-CoNLL 2007 (Conference on
Empirical Methods in Natural Language Processing
and Conference on Computational Natural Language
Learning), pages 581?589, Prague.
Douglas B. Lenat. 1995. CYC: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
David Liben-Nowell and Jon Kleinberg. 2003. The link
prediction problem for social networks. In Proceed-
ings of CIKM 2003 (12th ACM International Confer-
ence on Information and Knowledge Management),
pages 556?559, New Orleans, LA.
Qiaozhu Mei, Dengyong Zhou, and Kenneth Church.
2008. Query suggestion using hitting time. In Pro-
ceeding of CIKM 2008 (17th ACM International Con-
ference on Information and Knowledge Management),
pages 469?478, Napa Valley, CA.
Metaweb Technologies. 2010. Freebase Wikipedia Ex-
traction (WEX). http://download.freebase.com/wex/.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of AAAI
2006 (21st National Conference on Artificial Intelli-
gence), pages 775?782, Boston, MA.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Patrick Pantel and Dekang Lin. 2002. Document cluster-
ing with committees. In Proceedings of SIGIR 2002
(25th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval),
pages 199?206, Tampere.
Marco Saerens, Franois Fouss, Luh Yen, and Pierre
Dupont. 2004. The principal components analysis of
a graph, and its relationships to spectral clustering. In
Proceedings of ECML 2004 (15th European Confer-
ence on Machine Learning), pages 371?383, Pisa.
Purnamrita Sarkar, Andrew W. Moore, and Amit Prakash.
2008. Fast incremental proximity search in large
graphs. In Proceedings of ICML 2008 (25th Interna-
tional Conference on Machine Learning), pages 896?
903, Helsinki.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! Computing semantic relatedness using
Wikipedia. In Proceedings of AAAI 2006 (21st Na-
tional Conference on Artificial Intelligence), pages
1419?1424.
Nguyen Xuan Vinh, Julien Epps, and James Bailey.
2009. Information theoretic measures for clusterings
comparison: Is a correction for chance necessary? In
Proceedings of ICML 2009 (26th International Con-
ference on Machine Learning), Montreal.
Majid Yazdani and Andrei Popescu-Belis. 2010. A ran-
dom walk framework to compute textual semantic sim-
ilarity: a unified model for three benchmark tasks.
In Proceedings of IEEE ICSC 2010 (4th IEEE Inter-
national Conference on Semantic Computing), Pitts-
burgh, PA.
Eric Yeh, Daniel Ramage, Christopher D. Manning,
Eneko Agirre, and Aitor Soroa. 2009. WikiWalk:
random walks on Wikipedia for semantic relatedness.
In Proceedings of TextGraphs-4 (4th Workshop on
Graph-based Methods for Natural Language Process-
ing), pages 41?49, Singapore.
36
How Comparable are Parallel Corpora?  
Measuring the Distribution of General Vocabulary and Connectives  
 
Bruno Cartoni Sandrine Zufferey Thomas Meyer Andrei Popescu-Belis 
Linguistics Department 
University of Geneva 
2, rue de Candolle 
CH ? 1211 Geneva 4 
Linguistics Department 
University of Geneva 
2, rue de Candolle 
CH ? 1211 Geneva 4 
Idiap Research Institute 
Rue Marconi 19 
CH ? 1920 Martigny  
Idiap Research Institute 
Rue Marconi 19 
CH ? 1920 Martigny  
{bruno.cartoni|sandrine.zufferey}@unige.ch {thomas.meyer|andrei.popescu-
belis}@idiap.ch 
 
Abstract 
In this paper, we question the 
homogeneity of a large parallel corpus 
by measuring the similarity between 
various sub-parts. We compare results 
obtained using a general measure of 
lexical similarity based on ?2 and by 
counting the number of discourse 
connectives. We argue that discourse 
connectives provide a more sensitive 
measure, revealing differences that are 
not visible with the general measure. We 
also provide evidence for the existence 
of specific characteristics defining 
translated texts as opposed to non-
translated ones, due to a universal 
tendency for explicitation. 
1 Introduction 
Comparable corpora are often considered as a 
solution to compensate for the lack of parallel 
corpora. Indeed, parallel corpora are still 
perceived as the gold standard resource for many 
multilingual natural language processing 
applications, such as statistical machine 
translation.  
The aim of this paper is to assess the 
homogeneity of the widely used Europarl 
parallel corpus (Koehn 2005) by comparing a 
distributional measure of lexical similarity with 
results focused on a more specific measure, the 
frequency of use of discourse connectives. 
Various perspectives can be taken to assess the 
homogeneity of this corpus. First, we evaluate 
the (dis)similarities between translated and 
original language (Experiment 1) and then the 
(dis)similarities between texts translated from 
different source languages (Experiment 2). 
Analyzing the use of discourse connectives 
such as because and since in English highlights 
important differences between translated and 
original texts. The analysis also reveals 
important differences when comparing, for a 
given language, texts that have been translated 
from various source languages. The different 
distribution of connectives in original vs. 
translated French, as well as across varieties of 
French translated from various source languages 
(English, German, Italian and Spanish), are all 
the more intriguing that they are not matched by 
a distributional difference of the general 
vocabulary in these corpora. We will indeed 
show that a well-known method (Kilgarriff 
2001) designed to compare corpora finds that the 
original French and the various translated 
portions of Europarl are rather similar, 
regardless of their source language. 
The paper is structured as follows: we first 
present related work on the characterization of 
translated text (Section 2). In Section 3, we 
argue that analyzing discourse connectives sheds 
new light on text (dis)similarity. Section 4 
presents the Europarl parallel corpus and its sub-
parts that have been used in our studies, as well 
as the methodology and measures that have been 
applied to assess text similarities. Section 5 
presents our main findings and Section 6 
discusses our results, drawing methodological 
conclusions about the use of parallel corpora. 
78
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 78?86,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
2 Previous Work 
Existing studies on translated corpora are mainly 
designed to automatically identify the presence 
of so-called ?translationese? or ?third code?, in 
other words, a text style deemed to be specific to 
translated texts, as in (Baroni and Bernardini 
2005) or in (Ilisei et al 2010). In the literature, 
many possible characteristics of translationese 
have been identified, such as those listed in 
(Baker 1996): translations are simpler than 
original texts (Laviosa-Braithwaite 1996); 
translations are more explicit than original texts 
due to an increase of cohesion markers (Blum-
Kulka 1986); and the items that are unique in the 
target system (i.e. that do not have exact 
equivalents in the source language) are under-
represented in translations (Tirkkonen-Condit 
2000).  
In the field of natural language processing, 
several studies on parallel corpora have shown 
that when building a statistical machine 
translation system, knowing which texts have 
been originally written in a given language and 
which ones are translations has an impact on the 
quality of the system (Ozdowska 2009). A 
recent study using machine learning has 
confirmed the universal of simplification as a 
feature of translated texts (Ilisei et al 
2010).Corpora can be compared using similarity 
measures. Most of these measures are based on 
lexical frequency. Kilgariff (2001) provides a 
comprehensive review of the different methods 
for computing similarity. 
In this study, we chose to use the CBDF 
measure (Chi-by-degrees-of-freedom), as 
proposed in (Kilgariff 1997), to assess the 
similarity of our sub-corpora, as explained in 
Section 4.3. We compare this measure with 
another marker of text diversity (connectives), as 
explained in the following section. 
3 Discourse Connectives as Markers of 
Text Diversity 
Discourse connectives like but, because or while 
form a functional category of lexical items that 
are very frequently used to mark coherence 
relations such as explanation or contrast 
between units of text or discourse (e.g. Halliday 
& Hassan 1976; Mann & Thomson 1992; Knott 
& Dale 1994; Sanders 1997). One of the unique 
properties of discourse connectives is that the 
relation they convey can in many cases be 
inferred even when they are removed, as 
illustrated in (1) and (2):  
1 Max fell because Jack pushed him. 
2 Max fell. Jack pushed him.  
The causal relation conveyed by because in 
(1) is also inferable when the connective is 
absent by using world knowledge about the 
possible relation between the fact of pushing 
someone and this person?s fall in (2). In other 
words, contrary to most other lexical items, 
connectives can be used or left out without 
producing ungrammatical results or losing 
important aspects of meaning. At a macro-
textual level, it is however clear that a text 
containing no connective at all would become 
rather difficult to understand. Several psycho-
linguistic studies have indeed stressed the role of 
connectives for processing (Millis & Just 1994; 
Noordman & Blijzer 2000). But the point we 
want to make here is that in most texts or 
discourses, some coherence relations are 
conveyed by the use of connectives while others 
are not, depending on what the author/speaker 
feels necessary to mark explicitly. 
Another consequence of the fact that 
connectives are optional is that their use in 
translation can vary tremendously between the 
source and the target texts. Studies that have 
examined at the use of connectives in translation 
have indeed found that connectives were often 
removed or added in the target texts, and that the 
type of coherence relation conveyed was 
sometimes even modified due to the actual 
choice of connectives in the target system 
(Altenberg 1986; Baker 1993; Lamiroy 1994; 
Halverson 2004). For all these reasons, 
discourse connectives appear to be particularly 
interesting to investigate in relation to corpus 
homogeneity. 
In this study, we focus more particularly on 
the category of causal connectives, that is to say 
connectives such as because and since in 
English. This particular category seemed 
especially appropriate for our purposes for a 
number of reasons. First, causal connectives 
form a well-defined cluster in many languages 
and can be studied comprehensively. Second, 
causal relations are amongst the most basic ones 
79
for human cognition and in consequence causal 
connectives are widely used in almost all text 
types (Sanders & Sweetser 2009). Lastly, causal 
connectives have been found to be more volatile 
in translation than other categories, such as for 
example concessive connectives like but, 
however, etc. (Halverson 2004; Altenberg 1986). 
From a quantitative perspective, function 
words are usually very frequent whereas most 
content words tend to be in the tail of the 
distribution. This provides another reason to 
treat connectives as a key feature for assessing 
text similarities. 
4 Corpora and Methodology 
4.1 Corpora 
Our analysis is based on the Europarl corpus 
(Koehn 2005), a resource initially designed to 
train statistical machine translation systems. 
Europarl is a multilingual corpus that contains 
the minutes of the European Parliament. At the 
parliament, every deputy usually speaks in 
his/her own language, and all statements are 
transcribed, and then translated into the other 
official languages of the European Union (a total 
of 11 languages for this version of the corpus ? 
version 5). Based on this data, several parallel 
bilingual corpora can be extracted, but caution is 
necessary because the exact status of every text, 
original or translated, is not always clearly 
stated. However, for a number of statements, a 
specific tag provides this information.  
From this multilingual corpus, we extracted 
for our first experiment two parallel and 
?directional? corpora (En-Fr and Fr-En). By 
?directional? we mean that the original and 
translated texts are clearly identified in these 
corpora. Namely, in the English-French subset, 
the original speeches were made in English 
(presumably mostly by native speakers), and 
then translated into French, while the reverse is 
true for French-English. Still, for many 
applications, these would appear as two 
undifferentiated subsets of an English-French 
parallel corpus.  
Since language tags are scarcely present, we 
automatically gathered all the tag information in 
all the language-specific files, correcting all the 
tags and discarding texts with contradictory 
information. Therefore, these extracted 
directional corpora are made of discontinuous 
sentences, because of the very nature of this 
multilingual corpus. In one single debate, each 
speaker speaks in his/her own language, and 
when extracting statements of one particular 
language, discourse cohesion across speakers is 
lost. However, this has no incidence at the 
global level on the quantitative distribution of 
connectives.  
We have focused our investigation on the 
years 1996 to 1999 of the Europarl corpus. 
Indeed, statistical investigations and information 
gathered at the European Parliament revealed 
that the translation policy had changed over the 
years. The 1996-1999 period appeared to contain 
the most reliable translated data of the whole 
corpus. 
For Experiment 1, we extracted two parallel 
directional corpora made of two languages ? 
French and English ? in order to compare 
translated and original texts in both languages, 
as shown in Figure 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 1 gives the number of tokens in the 
English-French and in the French-English 
parallel directional corpora. 
 
Parallel corpus Token in ST Token in TT 
English-French (EF) 1,412,316 1,583,775 
French-English (FE) 1,257,879 1,188,923 
Table 1: Number of tokens in Source Texts (ST) 
and Translated Texts (TT) of the parallel 
directional corpora. 
 
Following the same methodology, we extracted 
for Experiment 2 other parallel directional 
Figure 1: Parallel and comparable corpora 
extracted from Europarl 
Parallel directional corpora 
Comparable corpora 
Original 
English 
Original 
French 
Translated 
French 
Translated 
English 
80
corpora, again with French as a target language 
(also from the 1996-1999 period), as shown in 
Figure 2. Table 2 presents the sizes of these four 
additional comparable corpora.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Parallel corpus Token in ST Token in TT 
German-French (DF) 1,254,531 1,516,634 
Italian-French (IF) 552,242 624,534 
Spanish-French (SF) 597,607 633,918 
Table 2: Number of tokens in Source Texts (ST) 
and Translated Texts (TT) of the three additional 
parallel directional corpora of translated French. 
 
These parallel directional corpora have been 
used as comparable corpora in our study because 
they are written in the same language and are of 
the same genre, but do not have the same 
?status?, since some are original texts while 
others are translations, as shown in . Moreover, 
for comparison purposes, we have also used a 
sub-part of Europarl which was originally 
produced in French (noted OF), corresponding 
to the French part of the French-English corpus 
described in Table 1 
All the experiments described below are 
based on these comparable corpora, i.e. on the 
translated vs. original corpus (for French and 
English) and on the different corpora of 
translated French (with Italian, English, Spanish 
and German as source languages).  
4.2 First Measure: CBDF Measure 
Following a proposal by Kilgarriff (2001), who 
criticizes a number of simpler techniques, we 
have measured corpus similarity by computing 
the ?2 statistic over the 500 most frequent words 
from the two corpora to be compared, which 
were limited to 200,000 words each, so that 
comparison with the values given by Kilgarriff 
was possible. The value was normalized by the 
number of degrees of freedom, which is (500?
1) ? (2?1) = 499, hence its name. As shown by 
Kilgarriff with artificially designed corpora, for 
which the similarity level was known in 
advance, the ?2 statistic is a reliable indicator of 
similarity. Moreover, Kilgarriff (2001: Table 10, 
page 260) provides a table with the ?2 values for 
all 66 pairs of 200,000-word corpora selected 
from 12 English corpora, which we will use for 
comparison below. The table also lists internal 
homogeneity values for each corpus, obtained by 
averaging the ?2 statistic over each 200,000-
word corpus split several times in half. In fact, 
as the same method is used for computing both 
similarity and homogeneity, only 100,000-word 
fragments are used for similarity, as stated by 
Kilgarriff. 
The CBDF similarity values between 
100,000-word subsets of Original French (OF), 
French translated from English (EF), from 
Italian (IF), from German (DF), and from 
Spanish (SF) are shown in Table 4 below. 
Taking OF vs. EF as an example, these values 
are computed by summing up, for all of the most 
frequent 500 words in OF+EF, the difference 
between the observed and the expected number 
of occurrences in each of OF and EF, more 
precisely (o ? e)2 / e, and then dividing the sum 
by 499. The expected number is simply the 
average of OF and EF occurrences, which is the 
best guess given the observations. The lower the 
result, the closer the two corpora are considered 
to be, in terms of lexical distribution, as shown 
by Kilgarriff (2001). 
For measuring homogeneity, we sliced each 
corpus in 10 equal parts, and computed the score 
by randomly building 10 different corpus 
configurations and calculating the average of the 
values.  
4.3 Second Measure: Counting Connectives 
As explained above, we focused our experiments 
on comparing frequencies of causal connectives. 
For French, our list of items included parce que, 
puisque, car, and ?tant donn? que. For English, 
Figure 2: Parallel and comparable corpora 
for Translated French 
Parallel directional corpora 
Comparable corpora 
Original 
English 
Original 
Italian 
Trans-
lated 
French 
Original 
German 
Original 
Spanish 
Trans-
lated 
French 
Transl-
lated 
French 
Trans-
lated 
French 
81
we included because, since, and given that1. In 
the case of since, we manually annotated its two 
meanings in order to distinguish its causal uses 
from its temporal ones, and retained only its 
causal uses in our counts. 
To count the number of occurrences for each 
causal connective in each sub-part of the corpus, 
we first pre-processed the corpora to transform 
each connective as one word-form (e.g. ?tant 
donn? que became ?tantdonn?que, and puisqu? 
became puisque.). Then, we counted each 
connective, and normalized the figures to obtain 
a ratio of connectives per 100,000 tokens. 
Moreover, when comparing French sub-
corpora translated from different source 
languages, we also computed the rank of each 
connective in the frequency list extracted from 
each corpus. Comparing these ranks provided 
important information about their respective 
frequencies.  
We have found that the frequency of each 
connective does not vary significantly 
throughout the corpus (years 1996-1999), which 
tends to prove that the use of connectives does 
not depend crucially on the style of a particular 
speaker or translator.  
5 Results  
This section presents the results of the CBDF 
measure for each corpus (Section 5.1), and 
shows how the frequencies of connectives reveal 
differences between translated and original texts 
(Section 5.2) and between texts translated from 
various source languages (Section 5.3).   
5.1 Text Similarity according to CBDF 
For Experiment 1, we have compared the 
differences between original and translated texts, 
for English and French. The values of CBDF 
similarity resulting from this comparison are 
shown in Table 3. Compared to the different 
scores computed by Kilgarriff, these scores 
indicate that the two pairs of corpora are both 
quite similar.  
                                                          
1
  The English causal connective for is more 
difficult to address because of its ambiguity with the 
homographic preposition. However, on a sample of 500 
tokens of for randomly extracted from Europarl, we found 
only two occurrences of the connective for, leading us to 
exclude this connective from our investigation. 
 CBDF 
Original English ? Translated English 13.28 
Original French ? Translated French 12.28 
Table 3: CBDF between original and translated 
texts 
 
The similarities between sub-corpora of 
French translated from different source 
languages (Experiment 2) are shown in Table 4. 
The values comparing the same portion (e.g. 
OF/OF) indicate the homogeneity score of the 
respective sub-corpus. 
 
 OF EF DF IF SF 
OF 2.64     
EF 6.00 3.34    
DF 5.11 4.83 2.74   
IF 4.88 6.30 4.99 2.86  
SF 5.34 5.43 5.36 4.43 2.22 
Table 4: Values of CBDF (?2 statistic 
normalized by degrees of freedom) for all pairs 
of source-specific 200,000-word subsets from 
Europarl. The lower the value, the more similar 
the subsets.   
 
Looking at the values in Table 4, we can see 
that the similarity score between OF and EF is 
6.00, which, compared to Kilgarriff?s values for 
British corpora, is lower than all but two of the 
66 pairs of corpora he compared. Most of the 
values observed by Kilgarriff are in fact between 
20 and 40, and the similarity we found for OF 
vs. EF is, for instance, in the same range as the 
one for the journal The Face vs. The Daily 
Mirror, a tabloid, and higher than the similarity 
of two broadsheet newspapers (i.e., they get a 
lower CBDF value). Therefore, we can conclude 
that OF and EF are very similar from a word 
distribution point of view. 
As for the other pairs, they are all in the same 
range of similarity, again much more similar 
than the corpora cited in Kilgarriff?s Table 10. 
Regarding internal comparisons, OF/EF appears 
as the second most dissimilar pair, preceded 
only by IF/EF (French translated from Italian vs. 
from English). The most similar pair is Original 
French vs. French translated from Italian, which 
is not surprising given that the two languages are 
closely related. Also similar to OF/IF are the 
IF/SF and EF/DF pairs, reflecting the similarity 
of translations from related languages. 
82
Homogeneity values are higher than similarity 
values (the ?2 scores are lower). These values 
are again comparable, albeit clearly lower, than 
those found by Kilgarriff, and presumably 
account for the lower variety of parliamentary 
discourse. Still, these values are similar to those 
of the most homogeneous subset used by 
Kilgarriff, the Dictionary of National Biography 
(1.86) or the Computergram (2.20). 
Figures on the distribution of connectives, 
presented in the next section, tend to show that 
these sub-corpora are however not as similar as 
they may seem at a first view.  
5.2 Text Similarities Measured with the 
Use of Causal Connectives: 
Experiment 1 
In Experiment 1, we highlight the differences in 
the use of causal connectives between original 
English and translated English. Figure 3 shows 
the discrepancy between the use of the same 
connectives in original and translated texts. 
Among these connectives, since is the only truly 
ambiguous word. We have therefore also 
evaluated the proportion of causal uses of since 
among all the uses of the word since. In original 
English, this proportion is 31.8% and doubles in 
translated English to reach 67.7%. 
 
 
Figure 3: Ratio connectives/100,000 tokens in 
original and translated English. 
 
These figures show that original and 
translated texts differ, at least in terms of the 
number of causal connectives they contain. 
While because seems equally used in original 
and translated English, since and given that are 
used three times more frequently in translated 
than in original texts. This variability is also 
noticeable when comparing original and 
translated uses of French connectives, as shown 
in Figure 4.  
 
Figure 4: Ratio connectives/100?000 tokens in 
original and translated French. 
 
For French, while car seems to be equally 
used in both sub-parts of the corpus, parce que 
is used twice less frequently in translated than in 
original texts. This discrepancy is even bigger in 
the case of puisque, which is used five times less 
frequently in translated than in original texts. 
The reverse phenomenon is observed for ?tant 
donn? que, which is used four times more 
frequently in translated than in original texts.  
By looking at the translation of every 
connective, we were able to count the number of 
connectives inserted in the target language, that 
is to say when there was a connective in the 
target system but no connective in the original 
text. Conversely, we have also counted the 
number of connectives removed in the target 
text, when a connective in the source language 
was not translated at all. Overall, we found that 
connectives were inserted much more often than 
removed during the process of translation. In the 
case of English as a target language, 65 
connectives were inserted while 35 were 
83
removed. In the case of French, 46 connectives 
were inserted while 11 were removed.  
5.3 Text similarities measured by the use of 
causal connectives: Experiment 2 
When comparing the number of occurrences of 
French causal connectives across texts translated 
from different languages, the differences are 
striking. Indeed, every source language seems to 
increase the use of one specific connective in the 
French translations.  
Figure 5 presents the ratio of connectives per 
100?000 token. The data compares the use of 
connectives in French translated from English, 
Italian, Spanish and German.  
 
 
Figure 5: Connectives per 100,000 tokens in 
French texts translated from various source 
languages (for each connective, from left to right 
OF, EF, IF, DF, SF) 
 
Table 5 provides the rank of every connective 
in the word frequency list (sorted by decreasing 
frequency) computed for each sub-corpus. Grey 
cells indicate the most frequent connective in 
each sub-corpus. 
 
 
 
 
 OF EF IF DF SF 
parce que 115 292 99 159 87 
car 136 172 201 82 85 
puisque 235 1070 601 886 790 
?tant donn? que 3882 1368 2104 1450 459 
Table 5: Rank of the connectives in word 
frequency list for each corpus. Note that the 
order varies with the source language. 
 
These figures show that the distribution of 
every connective differs radically according to 
the source language. Every source language 
seems to increase the use of one specific 
connective. When German is the source 
language, car is used twice more often than 
when English or Italian are the source 
languages. When Italian is the source language, 
parce que is used twice as often and when 
English is the source language, ?tant donn? que 
is again used twice as often. Overall, puisque is 
the only connective that does not seem to be 
enhanced by any of the source languages, which 
confirms some prior linguistic analyses of this 
item, showing that puisque does not have exact 
equivalents in other close languages (Degand 
2004; Zufferey to appear).  
6 Discussion 
We have compared the use of discourse 
connectives in different sub-parts of the 
Europarl parallel corpus with the use of general 
vocabulary, as computed by a measure of lexical 
homogeneity. Our main finding is that even 
though the lexical measure showed the similarity 
of these sub-parts, the use of discourse 
connectives varied tremendously between the 
various sub-parts of our corpus. 
One of the reasons why connectives show 
more variability than many other lexical items is 
that they are almost always optional. In other 
words, as argued in Section 3, for every 
individual use of a connective, the translator has 
the option to use another connective in the target 
language or to leave the coherence relation it 
conveys implicit. Coherence marking is 
therefore a global rather than a local textual 
strategy. 
Given that connectives can be used or left out 
without producing ungrammatical results, 
studying their variability between comparable 
corpora provides interesting indications about 
84
their global homogeneity. The significant 
variability that we report between comparable 
(monolingual) sub-parts of the Europarl corpus 
indicates that they are not as homogeneous as 
global lexical measures like the CBDF tend to 
indicate. In other words, the various sub-parts of 
the corpus are not equivalents of one another for 
all purposes, and should not be used as such 
without caution. These differences were 
noticeable both by the different number of every 
connective used in every sub-part of the corpus, 
but also by the rather different frequency rank 
that was measured for every one of them in these 
same sub-parts. 
From a translation perspective, our study also 
provides some further confirmation for the 
existence of specific characteristics that define 
translated texts (i.e. ?translationese? or ?third 
code?). More specifically, our study 
corroborates the explicitation hypothesis (Blum-
Kulka 1986), positing that translated texts are 
more explicit than original ones due to an 
increase of cohesion markers. Connectives are 
part of the lexical markers that contribute to 
textual coherence, and we found that they are 
indeed more numerous in translated than in 
original texts. For English as a target language, 
translators have inserted twice as many 
connectives as they have removed. For French, 
this proportion raises to four times more 
insertions than omissions.  
However, our data also indicates that the 
source language has an important influence on 
the nature of its translation. Indeed, for the use 
of connectives, we report important variations 
between texts translated into French from 
various source languages. More interestingly 
still, every source language triggered the use of 
one specific connective over the others. This 
connective was always specific to one particular 
source language. 
It is also noteworthy that the similarity 
between texts translated into French, as 
measured with the CBDF, is greater when the 
source languages are typologically related. In 
our corpora of translated French, we found that 
texts were more similar when comparing the 
portion translated from Spanish and Italian 
(Romance languages) and when comparing texts 
translated from English and German (Germanic 
languages). This result makes intuitive sense and 
provides further confirmation of the reliability of 
this measure to assess global similarity between 
portions of texts. 
7 Conclusion 
The Europarl corpus is mostly used in NLP 
research without taking into account the 
direction of translation, in other words, without 
knowing which texts were originally produced 
in one language and which ones are translations. 
The experiments reported in this paper show that 
this status has a crucial influence of the nature of 
texts and should therefore be considered. 
Moreover, we have shown that translated texts 
from different source languages are not 
homogeneous either, therefore there is no unique 
translationese, and we identified some 
characteristics that vary according to the source 
language. 
Our study also indicates that global measures 
of corpus similarity are not always sensitive 
enough to detect all forms of lexical variation, 
notably in the use of discourse connectives. 
However, the variability observed in the use of 
these items should not be discarded, both 
because of their rather frequent use and because 
they form an important aspect of textual 
strategies involving cohesion. 
Acknowledgments 
This study was partially funded by the Swiss 
National Science Foundation through the 
COMTIS Sinergia project 
(www.idiap.ch/comtis). The authors would 
particularly like to thank Adam Kilgarriff for his 
explanations regarding the CBDF measure.  
References  
Altenberg Bengt. 1986. Contrastive linking in spoken 
and written English. In Tottie G. & B?cklund U. 
(Eds.), English in Speech and writing: a 
symposium. Uppsala, 13-40. 
Baker Mona. 1993. In Other Words. A coursebook on 
translation. Routledge, London/New York. 
Baker Mona. 1996. Corpus-based translation studies: 
The challenges that lie ahead. In Somers H. (Ed.) 
Terminology, LSP and Translation. Studies in 
language engineering in honour of Juan C. Sager. 
John Benjamins, Amsterdam, 175-186. 
85
Baroni Marco and Bernardini Silvia. 2006. A new 
approach to the study of translationese: Machine-
learning the difference between original and 
translated text. Literary and Linguistic Computing 
21(3). 259-274 
Degand Liesbeth. 2004. Contrastive analyses, 
translation and speaker involvement: the case of 
puisque and aangezien. In Achard, M. & Kemmer, 
S. (Eds.), Language, Culture and Mind. The 
University of Chicago Press, Chicago, 251-270. 
Halliday Michael and Hasan Ruqaiya. 1976. Cohesion 
in English. Longman, London 
Halverson Sandra. 2004. Connectives as a translation 
problem. In Kittel, H. et al (Eds.) An International 
Encyclopedia of Translation Studies. Walter de 
Gruyter, Berlin/New York, 562-572. 
Ilisei Iustina, Inkpen Diana, Corpas Pastor Gloria and 
Mitkov Russlan. 2010 Identification of 
Translationese: A Machine Learning Approach. In 
Gelbukh, A. (Ed), Computational Linguistics and 
Intelligent Text Processing Lecture Notes in 
Computer Science. Springer, Berlin / Heidelberg, 
503-511 
Kilgarriff Adam. 2001. Comparing Corpora. Intl. 
Journal of Corpus Linguistics 6(1): 1-37. 
Kilgariff Adam. 1997. Using word frequency lists to 
measure corpus homogeneity and similarity 
between corpora. In Fifth ACL Workshop on Very 
Large Corpora, Beijing. 
Knott Alistair and Dale Robert. 1994. Using 
linguistic phenomena to motivate a set of 
coherence relations. Discourse processes 18(1), 
35-62. 
Koehn Philipp. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation, MT Summit 2005. 
Lamiroy Beatrice. 1994. Pragmatic connectives and 
L2 acquisition. The case of French and Dutch. 
Pragmatics 4(2), 183-201. 
Laviosa-Braithwaite Sara. 1996. The English 
Comparable Corpus (ECC): A Resource and a 
Methodology for the Empirical Study of 
Translation. PhD Thesis, Manchester, UMIST. 
 
Mann William and Thomson Sandra. 1992. 
Relational Discourse Structure: A Comparison of 
Approaches to Structuring Text by 'Contrast'. In 
Hwang S. & Merrifield W. (Eds.), Language in 
Context: Essays for Robert E. Longacre. SIL, 
Dallas, 19-45. 
Millis Keith & Just Marcel. 1994. The influence of 
connectives on sentence comprehension. Journal 
of Memory and Language 33 (1): 128-147. 
New Boris, Pallier Christophe, Brysbaert Marc, Ferr 
Ludovic and Holloway Royal. 2004. Lexique~2: A 
New French Lexical Database. Behavior Research 
Methods, Instruments, & Computers, 36 (3): 516-
524.  
Noordman Leo and de Blijzer Femke. 2000. On the 
processing of causal relations. In E. Couper-
Kuhlen & B. Kortmann (Eds.) Cause, Condition, 
Concession, Contrast. Mouton de Gruyter, Berlin. 
35-56. 
Ozdowska Sylvia. 2009. Donn?es bilingues pour la 
TAS fran?ais-anglais : impact de la langue source 
et direction de traduction originales sur la qualit? 
de la traduction. Proceedings of Traitement 
Automatique des Langues Naturelles, TALN'09, 
Senlis, France. 
Sanders Ted. 1997. Semantic and pragmatic sources 
of coherence: On the categorization of coherence 
relations in context. Discourse Processes 24: 119?
147. 
Sanders Ted and Sweetser Eve (Eds) 2009. Causal 
Categories in Discourse and Cognition. Mouton de 
Gruyter, Berlin. 
Tirkkonen-Condit Sonja. 2000. In search of 
translation universals: non-equivalence or 
? unique ? items in a corpus test. Paper presented 
at the UMIST/UCL Research Models in 
Translation Studies Conference, Manchester, UK, 
April 2000. 
Zufferey Sandrine to appear. ?Car, parce que, 
puisque? Revisited. Three empirical studies on 
French causal connectives. Journal of Pragmatics. 
 
 
 
86
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 194?203,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Multilingual Annotation and Disambiguation of Discourse Connectives for
Machine Translation
Thomas Meyer and Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, 1920 Martigny, Switzerland
Thomas.Meyer@idiap.ch, Andrei.Popescu-Belis@idiap.ch
Sandrine Zufferey and Bruno Cartoni
Department of Linguistics, University of Geneva
Rue de Candolle 2, 1211 Geneva 4, Switzerland
Sandrine.Zufferey@unige.ch, Bruno.Cartoni@unige.ch
Abstract
Many discourse connectives can signal several
types of relations between sentences. Their
automatic disambiguation, i.e. the labeling of
the correct sense of each occurrence, is impor-
tant for discourse parsing, but could also be
helpful to machine translation. We describe
new approaches for improving the accuracy
of manual annotation of three discourse con-
nectives (two English, one French) by using
parallel corpora. An appropriate set of labels
for each connective can be found using infor-
mation from their translations. Our results for
automatic disambiguation are state-of-the-art,
at up to 85% accuracy using surface features.
Using feature analysis, contextual features are
shown to be useful across languages and con-
nectives.
1 Introduction
Discourse connectives are generally considered as
indicators of discourse structure, relating two sen-
tences of a written or spoken text, and making ex-
plicit the rhetorical or coherence relation between
them. Leaving aside the cases when connectives are
only implicit, the presence of a connective does not
unambiguously signal a specific discourse relation.
In fact, many connectives can indicate several types
of relations between sentences, i.e. they have several
possible ?senses? in context.
This paper studies the manual and automated dis-
ambiguation of three ambiguous connectives in two
languages: alors que in French, since and while in
English. We will show how the multilingual per-
spective helps to improve the accuracy of annota-
tion, and how it helps to find appropriate labels for
automated processing and MT. Results from auto-
matic annotation experiments, which are close to the
state of the art, as well as feature analysis, help to as-
sess the usefulness of the proposed labels.
The paper is organized as follows. Section 2 ex-
plains the motivation of our experiments, and of-
fers a wider perspective on our research goals, illus-
trating them with examples of translation problems
which arise from ambiguous discourse connectives.
Current resources and methods for discourse anno-
tation are discussed in Section 3. Section 4 analyzes
our experiments in manual annotation and in partic-
ular the influence of the set of labels on the reliability
of annotation. The automatic disambiguation exper-
iments, the features used, the results and the analysis
of features are described in Section 5. Section 6 con-
cludes the paper and outlines future work.
2 Explicit Connectives and their
Translation
2.1 Three Multi-functional Connectives
Discourse connectives form a functional category of
lexical items that are used to mark coherence rela-
tions such as Cause or Contrast between units of
discourse. Along with other function words, many
connectives appear among the most frequent words,
as shown for instance by counts (Cartoni et al,
2011) over the Europarl corpus (Koehn, 2005). The
Penn Discourse Treebank (Prasad et al, 2008) (see
Section 3.1 below) includes around 100 connective
types, but the exact number varies across studies,
194
depending on the discourse theory used to classify
them. Among these types, Pitler et al(2008) have
shown that most of them are unambiguous and easy
to identify, but others, especially temporal ones, of-
ten signal multiple senses depending on their con-
text.
Following the terminology of Petukhova and
Bunt (2009, Section 2), we are interested here in
?sequential? multi-functionality, i.e. the fact that the
same connective can signal different relations in dif-
ferent contexts. We do not deal with ?simultane-
ous? multi-functionality, i.e. the possibility for a
single occurrence to signal several relations, which
has been less frequently studied for connectives (see
Petukhova and Bunt (2009) for the discourse usage
of and).
We identified the two English connectives while
and since, along with the French connective alors
que, as being particularly problematic because they
are highly multi-functional, i.e. they can signal mul-
tiple senses. For alors que, a French database of
connectives (LexConn (Roze et al, 2010), see Sec-
tion 3 below) contains examples of sentences where
alors que expresses either a Background or a Con-
trast relation. For the English connective since,
Miltsakaki et al (2005) identified three possible
meanings: Temporal, Causal, and simultaneously
Temporal/Causal. For while, even more senses are
observed: Comparison, Contrast, Concession, and
Opposition. In fact, in the Penn Discourse Tree-
bank, the connective while is annotated with more
than twenty different senses.
2.2 Wider Research Objectives
Our long-term goal is to identify automatically the
senses of connectives for an application to machine
translation (MT). Going beyond the labels provided
by discourse theories, the goal is thus to find the
most appropriate labels in a new multilingual, em-
pirical approach that makes use of parallel corpora to
annotate and then learn the various senses of connec-
tives. The disambiguation of such connectives in a
source text is crucial for its translation, because each
sense may be translated by a different connective
and/or syntactical construct in the target language.
More specifically, we hypothesize that correctly
labeled connectives are easier to learn and to trans-
late by statistical MT systems than unlabeled ones.
To support this hypothesis, we set up an experiment
(Meyer, 2011) in which we constrained the transla-
tion of the three senses of the discourse connective
while that were previously annotated as Temporal,
Contrast and Concession. The system was forced to
use predefined French translations known to be cor-
rect, by directly modifying the phrase table of the
trained MT system. This modification noticeably
helped to improve translation quality and rose the
BLEU score by 0.8 for a preliminary test set of 20
sentences.
2.3 Illustration of Mistranslations
Among the connectives that we plan to process in or-
der to improve MT, the three connectives we focus
on in this paper are frequent, ambiguous and there-
fore difficult to translate correctly by MT systems,
as illustrated in the following examples.
A first reason why machine translation of connec-
tives can be difficult is that there may be no direct
lexical correspondence for the explicit source lan-
guage connective in the target language, as shown
in the reference translation of the first example in
Table 1, taken from the Europarl corpus (Koehn,
2005).
EN It is also important that we should not leave these indica-
tors floating in the air while congratulating ourselves on
the fact that we have produced them.
FR Il est e?galement important de ne pas laisser ces indicateurs
flotter, en nous fe?licitant de les avoir instaure?s.
EN Finally, and in conclusion, Mr President, with the expiry of
the ECSC Treaty, the regulations will have to be reviewed
since [causal] I think that the aid system will have to con-
tinue beyond 2002 . . .
FR *Enfin, et en conclusion, Monsieur le pre?sident, a`
l?expiration du traite? ceca, la re?glementation devra e?tre
revu depuis que [temporal] je pense que le syste`me d?aides
devront continuer au-dela` de 2002 . . .
FR Oui, bien entendu, sauf que le de?veloppement ne se ne?gocie
pas, alors que [contrast] le commerce, lui, se ne?gocie.
EN *Yes, of course, but development cannot be negotiated, so
[causal] that trade can.
EN Between 1998 and 1999, loyalists assaulted and shot 123
people, while [contrast] republicans assaulted and shot 93
people.
FR *Entre 1998 et 1999, les loyalistes ont attaque? et abattu
123 personnes, ? 93 pour les re?publicains.
Table 1: Translation examples from Europarl. Discourse
connectives, their translations, and their senses are indi-
cated in bold. The first example is a reference transla-
tion from EN into FR, while the others are wrong transla-
tions generated by MT (EN/FR and respectively FR/EN),
hence marked with an asterisk.
195
When an ambiguous connective is explicitly
translated by another connective, the incorrect ren-
dering of its sense can lead to erroneous translations,
as in the second and third examples in Table 1, which
are translated by the Moses SMT decoder (Koehn et
al., 2007) trained on the Europarl corpus. The ref-
erence translation for the second example uses the
French connective car with a correct causal sense,
instead of the wrong depuis que generated by SMT,
which expresses a temporal relation. In the third ex-
ample, the French connective alors que, in its con-
trastive usage, is wrongly translated into the English
connective so, which has a causal meaning (the ref-
erence translation uses whereas to express contrast).
It may even occur that the system fails to translate a
connective at all, as in the fourth example where the
discourse information provided by while, namely a
Contrast relation, is lost in the French translation,
which is hardly coherent any longer.
3 Related Work
3.1 Annotated Resources
One of the very few available discourse annotated
corpora is the Penn Discourse Treebank (PDTB) in
English (Prasad et al, 2008). For this resource, one
hundred types of explicit discourse connectives were
manually annotated, as well as implicit relations not
signaled by a connective. The sense hierarchy used
for annotation consists of three levels, from four top-
level senses (Temporal, Contingency, Comparison,
and Expansion), to 16 subsenses on the second level,
and 23 further ones on the third level. The annota-
tors were allowed to assign more than one sense to
each occurrence, so 129 simple or complex labels
are observed, over more than 18,000 explicit con-
nectives. For French, the ANNODIS project (Pe?ry-
Woodley et al, 2009) will provide annotation of dis-
course on an original corpus. Resources for Czech
are also becoming available (Zika?nova? et al, 2010).
For German, a lexicon of discourse markers
named DiMLex exists since the 1990s (Stede and
Umbach, 1998). An equivalent, more recent
database for French is the LexConn lexicon of con-
nectives (Roze et al, 2010) containing a list of 328
explicit connectives. For each of them, LexConn
indicates and exemplifies the possible senses, cho-
sen from a list of 30 labels inspired from Rhetorical
Structure Theory (Mann and Thompson, 1988).
3.2 Automatic Disambiguation of Connectives
The release of the PDTB had quite an impact on
automatic disambiguation experiments. The state-
of-the-art for recognizing all types of explicit con-
nectives in English is therefore already high, at
97% accuracy for disambiguating discourse vs. non-
discourse uses (Lin et al, 2010) and 94% for disam-
biguating the four main senses from the PDTB hier-
archy (Pitler and Nenkova, 2009). Lin et al (2010)
recently built the first end-to-end PDTB discourse
parser, which is able to parse unrestricted text with
an F1 score of 38.18% for senses on the second level
of the PDTB hierarchy. Other important contribu-
tions to automatic discourse connective classifica-
tion and feature analysis has been provided by Well-
ner et al (2006) and Elwell and Baldrige (2008).
Fewer studies focus on the detailed analysis of
specific discourse connectives. In Section 5.3, we
will compare our results to Miltsakaki et al (2005)
who report classification results for the connectives
since, while and when. In their study, as in the
present one, the goal is to disambiguate senses from
the second level of the PDTB hierarchy, a level
which, as we will show, is appropriate for the trans-
lation of these connectives as well.
4 Connective Annotation in Parallel
Corpora
The resources mentioned above are either monolin-
gual only (PDTB, LexConn) and/or not yet publicly
available (ANNODIS, DiMLex). Moreover, our
overall goal is related to multilingualism and trans-
lation, as explained in Section 2.2 above. There-
fore, we performed manual annotation of connec-
tives in a multilingual, aligned resource: the Eu-
roparl corpus (Koehn, 2005). We extracted from Eu-
roparl two subcorpora for each translation direction,
EN/FR and FR/EN, to take into account the varying
distribution of connectives in translated vs. original
language, as explained in Cartoni et al (2011).
As the full PDTB hierarchy seemed too fine-
grained given current capabilities for automatic la-
beling and the needs for translating connectives,
we defined a simplified set of labels for the senses
of connectives, by considering their usefulness and
196
granularity with respect to translation, focusing on
those that may lead to different connectives or syn-
tactical constructs in the target language.
4.1 Method
There are two major ways to annotate explicit dis-
course connectives. The first approach is to label
each occurrence of a connective with a label for
its sense, similar to the PDTB or LexConn hierar-
chies of senses. However, as shown among others
by Zikanova et al (2010), this is a difficult and time-
consuming task even when the annotators are trained
over a long period of time. This is confirmed by the
rather low kappa scores resulting from the manual
sense annotations as can be seen for each connective
in detail below.
The second approach to annotation, which is the
one put forward in this paper, is based on translation
spotting. In a first step, human annotators work on
bilingual sentence pairs, and annotate the translation
of each connective in the target language. The trans-
lations are either a target language connective (sig-
naling in principle the same sense(s) as the source
one), or a reformulation, or a construct with no con-
nective at all. In a second step of the annotation,
all translations of a connective are manually clus-
tered by the experimenters to derive sense labels, by
grouping together similar translations.
As demonstrated in the following subsections, for
the three connectives under study, the second ap-
proach to connective annotation not only facilitates
the annotation task, but also helps to derive the ap-
propriate level of granularity for the sense labels.
4.2 Annotation of alors que
This first manual annotation involved two experi-
enced annotators who annotated alors que in 423
original French sentences. The two main senses
identified for alors que are Background (labeled B)
Contrast (labeled C), as in the LexConn database.
Annotators were also allowed to use the J label if
they did not know which label to assign, and a
D label for discarded sentences ? due to a non-
connective use of the two words which could not be
filtered out automatically (e.g. Alors, que fera-t-on?
). The annotators found 20 sentences labeled with
D, which were removed from the data. 15 sentences
were labeled with J by one annotator (but none by
both), and it was decided to assign to them the label
(either B or C) provided by the other annotator.
The inter-annotator agreement on the B vs. C la-
bels was quite low, showing the difficulty of the task:
kappa reached 0.43, quite below the 0.7 mark often
considered as indicating reliability. The following
example from Europarl illustrates the difficulty of
choosing between B and C. In particular, the refer-
ence translation into English also uses an ambiguous
connective, namely while.
FR La monnaie unique va entrer en vigueur au milieu
de la tourmente financie`re, alors que de nombreux
comple?ments, logiques, mais que les E?tats ne sem-
blaient pas avoir pre?vus, n?ont pas encore e?te? ap-
porte?s.
EN The single currency is going to come into force in the
midst of financial turmoil, while a great many ad-
ditional factors which were only to be expected, but
which the states do not seem to have anticipated, have
not been taken into consideration.
Two methods were applied to deal with diverg-
ing manual annotations. To prepare the datasets for
the automated disambiguation experiments, one so-
lution (named A1, see Table 2) is to use the double-
sense label B/C for sentences labeled differently by
annotators (B vs. C). This label reflects the diffi-
culty of manual annotation and preserves the am-
biguity which might be genuinely present in each
occurrence. The relevance of the B/C label is also
supported by results from automatic labeling in Sec-
tion 5.3 below.
For comparison purposes, a second dataset named
A2 was derived from translation spotting on the
same French sentences aligned to English ones, as
explained in Section 4.1. Alors que appeared to be
mainly translated by the following English equiv-
alents and constructs: although, whereas, while,
whilst, when, at a time when. Through this opera-
tion, inter-annotator disagreement can sometimes be
solved: when the translation is a clearly contrastive
English connective (whereas or although), then the
C label was assigned instead of B/C. Conversely,
when the English translation was still ambiguous
(while, whilst, or when), the experimenters made a
decision in favor of either B or C by re-examining
source and target sentences.
4.3 Annotation of since
For since, 30 sentences were annotated by four ex-
perimenters in a preliminary round, with a kappa
197
ID Connective Sent. Labels (nb. of occ.)
A1 alors que 403 B (92), C (191), B/C (120)
A2 alors que 403 B (126), C (277)
B1 since 727 T (375), C (341), T/C (11)
B2 since 727 T (375), C (352)
C1 while 299 T/C (92), CONC (134), C (43)
T/CAUSAL (19), T/DUR (7)
T/PUNCT (4)
C2 while 299 T (30), C (135), CONC (134)
Table 2: The six datasets resulting from the manual anno-
tation of the three connectives, with total number of sen-
tences, possible labels and their number of occurrences.
The explanations of the labels are given in Sections 4.2
through 4.4.
score of 0.77, indicating good agreement. Then,
each half of the entire dataset (727 sentences) was
annotated by another person with three possible
sense labels: T for Temporal, C for Causal and
T/C for a simultaneously Temporal/Causal meaning.
Two datasets were again derived from this manual
annotation. To study the effects of a supplementary
label, we kept the label T/C for dataset B1, but con-
densed it under label C in dataset B2, as shown in
Table 2.
4.4 Annotation of while
The English connective while is highly ambiguous.
In the PDTB, occurrences of while are annotated
with no less than 21 possible senses, ranging from
Conjunction to Contrast, Concession, or Synchrony.
We performed a pilot annotation of 30 sentences
containing while with five different experimenters,
resulting in a quite low inter-annotator agreement,
? = 0.56. We therefore decided to perform a
translation spotting task only, with two experienced
annotators fluent in English and French. The ob-
served translations into French confirm the ambigu-
ity of while, as they include several connectives and
constructs, quite evenly distributed in terms of fre-
quency: alors que, gerundive reformulations, other
reformulations, si, tandis que, me?me si, bien que,
etc.
The translations were manually clustered to de-
rive senses for while, in an empirical manner.
For example, alors que signals Temporal/Contrast,
which is also true for tandis que. Similarly, me?me si
and bien que are clustered under the label Conces-
sion, and so forth. The translation spotting shows
that at least Contrast, Concession, and several tem-
poral senses are necessary to account for a correct
translation. These distinctions are comparable to the
semantic granularity of the second PDTB hierarchy
level.
To generate training sets for automated classifica-
tion out of a total of 500 sentences, we discarded 201
sentences labeled by annotators with G (gerundive
constructions), P (reformulations) or Z (no transla-
tion at all) ? these cases could be reconsidered in fur-
ther work, as they represent valid translation prob-
lems. For the remaining 299 sentences, we created
the following six labels by clustering the spotted
translations: T/C (Temporal/Contrast), T/PUNCT
(Temporal/Punctual), T/DUR (Temporal/Duration),
T/CAUSAL (Temporal/Causal), CONC (Conces-
sion) and C (Contrast). These were used to tag the
remaining 299 sentences, forming dataset C1. A
second dataset (C2) with fewer senses was obtained
from C1 by merging T/C to C (Contrast only) and
all T/x to T (Temporal only).
5 Disambiguation Experiments
The features for connective classification, the re-
sults obtained and a detailed feature analysis are dis-
cussed in this section. We show that an automated
disambiguation system can be used to determine the
most appropriate set of labels, and thus to corrob-
orate the selection we made using translation spot-
ting.
5.1 Features
For feature extraction, all the datasets described in
Section 4 were processed as follows. The English
texts were parsed and POS-tagged by Charniak and
Johnson?s (2005) reranking parser. The French texts
were POS-tagged with the MElt tagger (Denis and
Sagot, 2009) and parsed with MaltParser (Nivre,
2003). As the English parser provides constituency
trees, and the parser for French generates depen-
dency trees, the features are slightly different in the
two languages. The other features below were ex-
tracted using elementary pre-processing of the sen-
tences.
For English sentences, we used the following fea-
tures: the sentence-initial character of the connec-
198
tive (yes/no); the POS tag of the first verb in the
sentence; the type of first auxiliary verb in the sen-
tence (if any); the word preceding the connective;
the word following the connective; the POS tag of
the first verb following the connective; the type of
the first auxiliary verb after the connective (if any).
For French sentences, the features were the fol-
lowing: the sentence-initial character of the connec-
tive (yes/no); the dependency tag of the connective;
the first verb in the sentence; its dependency tag; the
word preceding the connective; its POS tag; its de-
pendency tag; the word following the connective; its
POS tag; its dependency tag; the first verb after the
connective; its dependency tag.
The cased connective word forms from the cor-
pus were not lower-cased, thus keeping the implicit
indication of the sentence-initial character of the oc-
currence, i.e. whether it starts a sentence or not. The
output of the POS taggers was used for neighboring
words, but not for the connectives, which almost al-
ways received the same tag. Charniak?s parser for
English provides POS tags which differentiate the
verb tenses, such as VBD (past), VBG (gerund), and
so on. These were considered for the verb directly
preceding and the one directly following the connec-
tive. Tense was believed to be potentially relevant
because since and while can have temporal mean-
ings.
The occurrence of auxiliary verbs (be, have, do,
or need) may give additional indications about tem-
poral relations in the sentence. We therefore used
the types of auxiliary verbs as features, including
the elementary conjugations, represented for to be
as: be present, be past, be part, be inf, be gerund
? and similarly for the other auxiliary verbs, as in
(Miltsakaki et al, 2005).
As shown by Lin et al (2010), duVerle and
Prendinger (2009) or Wellner et al (2006), the con-
text of a connective is very important. We there-
fore extracted the words preceding and following
each connective, the verbs and the first and the last
word of the sentences. These may include numbers,
sometimes indicating a numerical comparison, time
expressions, or antonyms, which could indicate con-
trastive relations, such as rise vs. fall (e.g. It is inter-
esting to see the fundamental stock pickers scream
?foul? on program trading when the markets de-
cline, while hailing the great values still abounding
as the markets rise.).
For French, we likewise extracted the words im-
mediately preceding and following each connective,
supplemented by their POS tags. In contrast to con-
stituents, dependency structures contain information
about the grammatical function of each word (heads)
and link the dependents belonging to the same head.
However, as the dependency parser provides no dif-
ferentiated verb tags, we extracted the verb word
forms themselves and added their dependency tags.
The same applies to the connective itself, and pre-
ceding and following words and their dependency
tags.
The dependency tag of the non-connectives varies
between subj (subject), det (determiner), mod (mod-
ifier) and obj (object). The first verb in the sentence
often belongs to the root dependency while the verb
following the connective most often belongs to the
obj dependency. For alors que, the most frequent
dependency tags were mod mod and mod obj, indi-
cating the connective?s main function as a modifier
of its argument.
5.2 Experimental Setting
Our classification experiments made use of the
WEKA machine learning toolkit (Hall et al, 2009)
to run and compare several classification algorithms:
Random Forest (sets of decision trees), Naive Bayes,
and Support Vector Machine. The results are re-
ported with 10-fold cross validation on the entire
data for each connective, using all features.
Table 3 lists for each method ? including the ma-
jority classifier as a baseline ? the percentage of cor-
rectly classified instances (or accuracy, noted Acc.),
and the kappa values. Significance above the base-
line is computed using paired t-tests at 95% confi-
dence. When a score is significantly above the base-
line, it is shown in italics in Table 3. The best scores
for each dataset, across classifiers, are indicated in
boldface. When these scores were not significantly
above the baseline, at least they were never signifi-
cantly below either.
5.3 Results and Discussion
Overall, the SVM classifier performed best, which
may be due to the large number of textual features
(3 for EN data and 5 for FR data), as SVMs are
known to handle them well (Joachims, 1998; du-
199
ID Connective # Labels Baseline R. Forest N. Bayes SVM
Acc. Acc. ? Acc. ? Acc. ?
A1 alors que 403 B, C, B/C 46.9 53.1 0.2 55.7 0.3 54.2 0.3
A2 alors que B, C 68.7 69.2 0.1 68.3 0.2 64.7 0.1
B1 since 727 T, C, T/C 51.6 79.8 0.6 82.3 0.7 85.4 0.7
B2 since T, C 51.6 80.7 0.6 84.0 0.7 85.7 0.7
C1 while 299 T/C, T/PUNCT, T/DUR,
T/CAUSAL, CONC, C
44.8 43.2 0.1 49.9 0.2 52.2 0.2
C2 while T, C, CONC 43.5 60.5 0.3 59.9 0.3 60.9 0.3
Table 3: Disambiguation scores for three connectives (number of occurrences in the training sets), with two sets of
labels each, for various classification algorithms. Accuracy (Acc.) is in percentage (%), and kappa is zero for the
baseline method (majority class). The best scores for each data set are in boldface, and scores significantly above the
baseline (95% t-test) are in italics.
Verle and Prendinger, 2009). The maximum accu-
racy for alors que is 55.7%, for since it is 85.7%, and
for while it is 60.9%. While close to other reported
values, there is still potential for improvement in the
future.
The analysis of results for each data sets leads
to observations that are specific to each connective.
The high improvement of over the baseline for A1,
as opposed to no improvement for A2, confirms the
usefulness of the double-sense B/C label for alors
que, showing that in this case the three-way classi-
fication is probably better adapted to the linguistic
properties of alors que than a two-way classifica-
tion. Indeed, alors que, just as its frequently spot-
ted translation while, is linguistically ambiguous in
some contexts (see for instance the example in Sec-
tion 4.2), in which the temporal and the contrastive
meaning are likely to co-exist. In the case of A2,
where the labels were forced to B or C only, auto-
matic classifiers do not significantly outperform the
baseline. While more elaborate features might help,
these low scores can be related to the difficulties of
human annotators (Section 4.2), and make a strong
case against using a two-label schema for alors que.
The features used so far lead to high scores for
since in datasets B1 and B2. The results are com-
parable to those from Miltsakaki et al (2005), who
used similar features and labels, though with a Max-
imum Entropy classifier. Moreover, they provide re-
sults for individual connectives, and not, as most of
the related work for the PDTB, on the whole set
of ca. 100 discourse connective types. However,
Miltsakaki et al (2005) used their own datasets for
each connective, which are different from the PDTB,
because the PDTB was not available at that time.
Our SVM classifier outperforms considerably the
Maximum Entropy classifier on the three-way clas-
sification task (with T, C, T/C), with an accuracy
of 85.4% vs. 75.5%, obtained however on differ-
ent datasets. For the two-way classification (T, C),
again on different datasets, our accuracy of 85.7% is
slightly lower than the 89.5% given in Miltsakaki et
al. (2005).1
For while, when comparing C1 to C2, it appears
that reducing the number of labels from six to three
increases accuracy by 8-10%. This is probably
due to the small number of training instances for
the labels T/PUNCT and T/DUR in C1 for exam-
ple. However, even for the larger set of labels, the
scores are significantly above baseline (52.2% vs.
44.8%), which indicates that such a classifier might
still be useful as input to an MT system, possibly
improved thanks to a larger training set. The perfor-
mance obtained by Miltsakaki et al (2005) on while
is markedly better than ours, with an accuracy of
71.8% compared to ours of 60.9% with three labels.
5.4 Feature Analysis
The relevance of features can be measured using
WEKA by computing the information gain (IG)
brought by each feature to the classification task,
1In another experiment (Meyer, 2011), we also applied our
classifiers to the PDTB data, with less features however. The
results were in the same range as those from Miltsakaki et
al. (2005), i.e. 75.3% accuracy for since and 59.6% for while.
200
R Feature IG
A1 A2
1 preceding word 1.12 0.64
2 following verb 0.81 0.51
3 first verb 0.74 0.42
4 following word 0.68 0.23
5 preceding word?s POS tag 0.15 0.05
5 first verb?s dep. tag 0.14 0.06
5 following word?s POS tag 0.19 0.03
8 preceding word?s dep. tag 0.10 0.03
8 connective?s dep. tag 0.09 0.04
10 following word?s dep. tag 0.13 0.013
10 following verb?s dep. tag 0.04 0.03
12 sentence initial 0.05 0.001
Table 4: Information gain (IG) of features for French con-
nective alors que, ordered by decreasing average ranking
(R) in experiments A1 and A2. Features 1?4 are consid-
erably more relevant than the following ones.
R Feature IG
B1 B2
1 preceding word 0.83 0.75
2 following word 0.56 0.52
3 following verb?s POS tag 0.24 0.21
4 type of following aux. verb 0.13 0.12
5 type of first aux. verb 0.11 0.11
6 first verb?s POS tag 0.02 0.01
7 sentence initial 0.00 0.00
Table 5: Information gain (IG) of features for EN con-
nective since, ordered by decreasing average ranking (R)
in experiments B1 and B2.
i.e. the reduction in entropy with respect to desired
classes (Hall et al, 2009) ? the higher the IG, the
more relevant the feature. Features can be ranked
by decreasing IG, as shown in Tables 4, 5 and 6, in
which ranks were averaged over the first and the sec-
ond data set in each series.
The tables show that across all three connectives
and the two languages, the contextual features are
always in the first positions, thus confirming the im-
portance of the context of a connective. Following
these are verbal features, which are, for these con-
nectives, of importance because the temporal mean-
ings are additionally established by verbal tenses.
POS and dependency features seem the least help-
R Feature IG
C1 C2
1 preceding word 1.02 0.65
2 following word 0.83 0.55
3 type of first aux. verb 0.12 0.07
4 following verb?s POS tag 0.16 0.04
5 first verb?s POS tag 0.07 0.09
5 type of following aux. verb 0.12 0.05
7 sentence initial 0.08 0.07
Table 6: Information gain (IG) of features for EN con-
nective while, ordered by decreasing average ranking (R)
in experiments C1 and C2. The first two features are con-
siderably more relevant than the remaining ones.
ful for disambiguation.
6 Conclusion and Future Work
We have described a translation-oriented approach
to the manual and automatic annotation of discourse
connectives, with the goal of identifying their senses
automatically, prior to machine translation. The
manual annotation of the senses of connectives has
been enhanced through parallel corpora and transla-
tion spotting. This has lead to tag sets that improved
both inter-annotator agreement and automatic label-
ing, which reached state-of-the-art scores. The ana-
lysis of relevant features has shown the utility of
contextual information.
To improve over these initial results, we will use
more semantic information, such as relations found
in WordNet between words in the neighborhood of
connectives ? e.g. word similarity measures and se-
mantic relations such as antonymy. To generate
more training instances of the labels found, man-
ual annotation will continue in order to see whether
the senses found through translation spotting can im-
prove automatic disambiguation of many more con-
nectives. The annotation of a large parallel corpus
will then help to train disambiguation tools along
with statistical MT systems that use their output.
Acknowledgments
We are grateful for the funding of this work by the
Swiss National Science Foundation (SNSF) under
the COMTIS Sinergia Project, n. CRSI22 127510
(see www.idiap.ch/comtis/).
201
References
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How comparable are
parallel corpora? Measuring the distribution of gen-
eral vocabulary and connectives. In Proceedings of 4th
Workshop on Building and Using Comparable Cor-
pora, Portland, OR.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL 2005 (43rd Annual Meet-
ing of the ACL), pages 173?180, Ann Arbor, MI.
Pascal Denis and Beno??t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art POS tagging with less human effort. In
Proceedings of PACLIC 2009 (23rd Pacific Asia Con-
ference on Language, Information and Computation),
pages 110?119, Hong Kong, China.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine clas-
sification. In Proceedings of ACL-IJCNLP 2009 (47th
Annual Meeting of the ACL and 4th International Joint
Conference on NLP of the AFNLP), pages 665?673,
Singapore.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identification with connective
specific rankers. In Proceedings of ICSC 2008 (2nd
IEEE International Conference on Semantic Comput-
ing), pages 198?205, Santa Clara, CA.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
ACM SIGKDD Explorations Newsletter, 11:10?18.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of ECML 1998 (10th Euro-
pean Conference on Machine Learning), pages 137?
142, Chemnitz, Germany.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbs. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007 (45th Annual Meeting of the
ACL), Demonstration Session, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of MT
Summit X, pages 79?86, Phuket, Thailand.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
Report TRB8/10, School of Computing, National Uni-
versity of Singapore, Singapore.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: towards a functional the-
ory of text organization. Text, 8(3):243?281.
Thomas Meyer. 2011. Disambiguating temporal-
contrastive discourse connectives for machine transla-
tion. In Proceedings of ACL-HLT 2011 (49th Annual
Meeting of the ACL: Human Language Technologies),
Student Session, Portland, OR.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Aravind
Joshi, and Bonnie Webber. 2005. Experiments on
sense annotations and sense disambiguation of dis-
course connectives. In Proceedings of the TLT 2005
(4th Workshop on Treebanks and Linguistic Theories),
Barcelona, Spain.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of IWPT
2008 (8th International Workshop on Parsing Tech-
nologies), pages 149?160, Tokyo, Japan.
Marie-Paule Pe?ry-Woodley, Nicholas Asher, Patrice
Enjalbert, Farah Benamara, Myriam Bras, Ce?cile
Fabre, Ste?phane Ferrari, Lydia-Mai Ho-Dac, Anne
Le Draoulec, Yann Mathet, Philippe Muller, Laurent
Pre?vot, Josette Rebeyrolle, Ludovic Tanguy, Marianne
Vergez-Couret, Laure Vieu, and Antoine Widlo?cher.
2009. Annodis: une approche outille?e de l?annotation
de structures discursives. In Proceedings of TALN
2009 (16e`me Confe?rence sur le Traitement Automa-
tique des Langues Naturelles), Paris, France.
Volha Petukhova and Harry Bunt. 2009. Towards a
multidimensional semantics of discourse markers in
spoken dialogue. In Proceedings of IWCS-8 (8th In-
ternational Conference on Computational Semantics),
pages 157?168, Tilburg, The Netherlands.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
Proceedings of ACL-IJCNLP 2009 (47th Annual Meet-
ing of the ACL and 4th International Joint Conference
on NLP of the AFNLP), Short Papers, pages 13?16,
Singapore.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceedings of
Coling 2008 (22nd International Conference on Com-
putational Linguistics), Companion Volume: Posters,
pages 87?90, Manchester, UK.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0. In
Proceedings of LREC 2008 (6th International Confer-
ence on Language Resources and Evaluation), pages
2961?2968, Marrakech, Morocco.
Charlotte Roze, Laurence Danlos, and Phillippe Muller.
2010. LEXCONN: a French lexicon of discourse con-
nectives. In Proceedings of MAD 2010 (Multidis-
202
ciplinary Approaches to Discourse), pages 114?125,
Moissac, France.
Manfred Stede and Carla Umbach. 1998. DiMLex: a
lexicon of discourse markers for text generation and
understanding. In Proceedings of ACL 1998 (36th An-
nual Meeting of the ACL), pages 1238?1242, Mon-
treal, Canada.
Ben Wellner, James Pustejovsky, Catherine Havasi,
Roser Sauri, and Anna Rumshisky. 2006. Classifica-
tion of discourse coherence relations: An exploratory
study using multiple knowledge sources. In Proceed-
ings of 7th SIGDIAL Workshop on Discourse and Di-
alogue, pages 117?125, Sydney, Australia.
Sa?rka Zika?nova?, Lucie Mladova?, Jir??? M??rovsky?, and
Pavlina J??nova?. 2010. Typical cases of annotators?
disagreement in discourse annotations in Prague De-
pendency Treebank. In Proceedings of LREC 2010
(7th International Conference on Language Resources
and Evaluation), pages 2002?2006, Valletta, Malta.
203
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 350?352,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Just-in-Time Document Retrieval System for Dialogues or Monologues
Andrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. Garner
Idiap Research Institute
Rue Marconi 19, Case Postale 592
1920 Martigny, Switzerland
{apbelis,myazdani,ananchen,pgarner}@idiap.ch
Abstract
The Automatic Content Linking Device is a
just-in-time document retrieval system that
monitors an ongoing dialogue or monologue
and enriches it with potentially related docu-
ments from local repositories or from the Web.
The documents are found using queries that
are built from the dialogue words, obtained
through automatic speech recognition. Re-
sults are displayed in real time to the dialogue
participants, or to people watching a recorded
dialogue or a talk. The system can be demon-
strated in both settings.
1 Introduction
The Automatic Content Linking Device (ACLD) is
a system that analyzes speech input from one or
more speakers using automatic speech recognition
(ASR), in order to retrieve related content, in real
time, from a variety of repositories. This paper de-
scribes the main components of the system and sum-
marizes evaluation results. The remainder of this
section introduces scenarios of use and previous sys-
tems with similar goals.
The first scenario of use involves people taking
part in meetings, who often mention documents con-
taining facts that are relevant to the current discus-
sion, but cannot search for them without interrupt-
ing the discussion flow. Our goal is to perform such
searches automatically. In a second scenario, search
is performed for live or recorded lectures, for in-
stance in a computer-assisted learning environment.
The ACLD enriches the lectures with related course
material, receiving real-time feedback from the user.
The ACLD improves over past systems by using
speech, by giving access to multimedia documents,
and by using semantic search. Its first precursors
were the Fixit query-free search system (Hart and
Graham, 1997), the Remembrance Agent for just-
in-time retrieval (Rhodes and Maes, 2000), and the
Implicit Queries system (Dumais et al, 2004). A
version of the Remembrance Agent called Jimminy
was conceived as a wearable assistant for taking
notes, but ASR was only simulated (Rhodes, 1997).
Watson monitored the user?s operations in a text
editor, and selected terms for web search (Budzik
and Hammond, 2000). Another authoring assistant
was developed in the A-Propos project (Puerta Mel-
guizo and al., 2008). Recently, several speech-
based search engines have been proposed, as well as
systems for searching spoken documents. For hu-
man dialogues in meetings, the FAME interactive
space (Metze and al., 2006) provided multi-modal
access to recordings of lectures via a table top in-
terface, but required specific voice commands from
one user only, and did not spontaneously follow a
conversation as the ACLD does.
2 Description of the ACLD
The architecture of the ACLD comprises modules
for: (1) document preparation and indexing; (2) in-
put sensing and query construction; (3) search and
integration of results; (4) user interaction.
2.1 Document Preparation and Indexing
The preparation of the local database of documents
available for search requires text extraction from
various file formats (like MS Office or PDF), and
350
document indexing, here using Apache Lucene. Past
meetings, when available, are automatically tran-
scribed, then chunked into smaller units, and in-
dexed along with the other documents. For search-
ing the Web, the system does not build indexes but
uses the Google Search API.
2.2 Sensing the User?s Information Needs
The ACLD uses the AMI real-time ASR system for
English (Garner and al., 2009), which has an ac-
ceptable accuracy for use with conversational speech
in the ACLD. When processing past recordings, the
ASR system can run slower than real-time to maxi-
mize its accuracy. If one or more pre-specified key-
words (based on domain knowledge) are detected in
the ASR output, then their importance is increased
for searching. Otherwise, all the words from the
ASR (except stopwords) are used for constructing
the query.
2.3 Querying the Document Database
The Query Aggregator component uses the ASR
words in order to retrieve the most relevant docu-
ments from a given database. The latest version
of the ACLD makes use of semantic search (see
below), but earlier versions used keyword-based
search from Apache Lucene for local documents.
Queries are formulated and launched at regular time
intervals, typically every 15-30 seconds, or on de-
mand. The search results are integrated with previ-
ous ones, using a persistence model that smoothes
variations in time by keeping track of the salience of
each result. Salience is initialized from the ranking
of search results, then decreases in time, or increases
if the document appears again among results. A his-
tory of all results is also accessible.
2.4 Semantic Search over Wikipedia
The goal of semantic search is to improve the rel-
evance of results with respect to the spoken words,
and to make search more robust to noise from ASR.
The method used here is adapted from a graph-based
measure of semantic relatedness between text frag-
ments (Yazdani and Popescu-Belis, 2010). Related-
ness is computed using random walk in a large net-
work of documents, here about 1.2 milion Wikipedia
articles from the WEX data set (Metaweb Technolo-
gies, 2010). These are linked by directional hy-
Figure 1: Unobtrusive UI of the ACLD displaying docu-
ment results. The pop-up window shows more details for
the first results.
perlinks, and also by lexical similarity links that
we construct upon initialization. The random walk
model allows the computation of the visiting proba-
bility (VP) from one document to another, and then
of the VP between sets of documents. This functions
as a measure of semantic relatedness, and has been
applied to several NLP problems by projecting the
text fragments to be compared onto the documents
in the network (Yazdani and Popescu-Belis, 2010).
For the ACLD, the use of semantic relatedness for
document retrieval amounts to searching, in a very
large collection, the documents that are the most
closely related to the words obtained from the ASR
in a given time frame. Here, we set the document
collection to Wikipedia (WEX). As the search is
hard to perform in real time, we made a series of
justified approximations to make it tractable.
2.5 The User Interface
The goal of the UI is to make ACLD information
available in a configurable way, allowing users to
see more or less information according to their own
needs. The UI displays up to four widgets, which
can be arranged at will, and contain: (1) ASR words
with highlighted keywords; (2) tag-cloud of key-
words, coding for recency and frequency; (3) links
to the current results from the local repository; (4)
links to the current Web search results.
Two main arrangements are intended: an infor-
mative full-screen UI (not shown here from lack of
space) and an unobtrusive UI, with superposed tabs,
shown in Figure 1 with the document result widget.
When hovering over a document name, a pop-up
window displays metadata and document excerpts
that match words from the query, as an explanation
for why the document was retrieved.
351
3 Evaluation of the ACLD
Four types of evidence for the relevance and util-
ity of the ACLD are summarized here. Firstly, the
ACLD was demonstrated to about 50 potential users
(industrial partners, focus groups, etc.), who found
the concept useful, and offered positive verbal eval-
uation, along with suggestions for smaller and larger
improvements.
Secondly, a pilot experiment was conducted with
a group using an earlier version of the UI. Two pilot
runs have shown that the ACLD was consulted about
five times per meeting, but many more runs are (still)
needed for statistical significance of observations.
Thirdly, the UI was tested in a usability evaluation
experiment with nine non-technical subjects, who
rated it as ?acceptable? (68%) on the System Usabil-
ity Scale, following a series of tasks they had to per-
form using it. Additional suggestions for changes
were received.
Finally, we compared offline the results of seman-
tic search with the keyword-based ones. We asked
eight subjects to read a series of nine meeting frag-
ments, and to decide which of the two results was
the most useful one (they could also answer ?none?).
Of a total of 36 snippets, each seen by two subjects,
there was agreement on 23 (64%) snippets and dis-
agreement on 13 (36%). In fact, if ?none? is ex-
cluded, there were only 7 true disagreements. Over
the 23 snippets on which the subjects agreed, the
result of semantic search was judged more relevant
than that of keyword search for 19 (53% of the to-
tal), and the reverse for 4 only (11%). Alternatively,
if one counts the votes cast by subjects in favor of
each system, regardless of agreement, then semantic
search received 72% of the votes and keyword-based
only 28%. Hence, semantic search already outper-
forms keyword based one.
4 Conclusion
The ACLD is, to the best of our knowledge, the
first just-in-time retrieval system to use spontaneous
speech and to support access to multimedia doc-
uments and to websites, using a robust semantic
search method. Future work should aim at improv-
ing the relevance of semantic search, at modeling
context to improve the timing of results, and at in-
ferring relevance feedback from users. The ACLD
should also be applied to specific use cases, and an
experiment with group discussions in a learning en-
vironment is under way.
Acknowledgments
We are grateful to the EU AMI and AMIDA Inte-
grated Projects and to the Swiss IM2 NCCR (In-
teractive Multimodal Information Management) for
supporting the development of the ACLD.
References
Jay Budzik and Kristian J. Hammond. 2000. User inter-
actions with everyday applications as context for just-
in-time information access. In IUI 2000 (5th Interna-
tional Conference on Intelligent User Interfaces), New
Orleans, LA.
Susan Dumais, Edward Cutrell, Raman Sarin, and Eric
Horvitz. 2004. Implicit Queries (IQ) for contextual-
ized search. In SIGIR 2004 (27th Annual ACM SIGIR
Conference) Demonstrations, page 534, Sheffield.
Philip N. Garner and al. 2009. Real-time ASR from
meetings. In Interspeech 2009 (10th Annual Confer-
ence of the International Speech Communication As-
sociation), pages 2119?2122, Brighton.
Peter E. Hart and Jamey Graham. 1997. Query-free in-
formation retrieval. IEEE Expert: Intelligent Systems
and Their Applications, 12(5):32?37.
Metaweb Technologies. 2010. Freebase Wikipedia Ex-
traction (WEX). http://download.freebase.com/wex/.
Florian Metze and al. 2006. The ?Fame? interactive
space. In Machine Learning for Multimodal Interac-
tion II, LNCS 3869, pages 126?137. Springer, Berlin.
Maria Carmen Puerta Melguizo and al. 2008. A person-
alized recommender system for writing in the Internet
age. In LREC 2008 Workshop on NLP Resources, Al-
gorithms, and Tools for Authoring Aids, pages 21?26,
Marrakech.
Bradley J. Rhodes and Pattie Maes. 2000. Just-in-time
information retrieval agents. IBM Systems Journal,
39(3-4):685?704.
Bradley J. Rhodes. 1997. The Wearable Remembrance
Agent: A system for augmented memory. Personal
Technologies: Special Issue on Wearable Computing,
1:218?224.
Majid Yazdani and Andrei Popescu-Belis. 2010. A ran-
dom walk framework to compute textual semantic sim-
ilarity: a unified model for three benchmark tasks. In
ICSC 2010 (4th IEEE International Conference on Se-
mantic Computing), pages 424?429, Pittsburgh, PA.
352
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 129?138,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Using Sense-labeled Discourse Connectives
for Statistical Machine Translation
Thomas Meyer and Andrei Popescu-Belis
Idiap Research Institute
Rue Marconi 19, 1920 Martigny, Switzerland
{thomas.meyer, andrei.popescu-belis}@idiap.ch
Abstract
This article shows how the automatic dis-
ambiguation of discourse connectives can
improve Statistical Machine Translation
(SMT) from English to French. Connec-
tives are firstly disambiguated in terms of
the discourse relation they signal between
segments. Several classifiers trained using
syntactic and semantic features reach state-
of-the-art performance, with F1 scores of
0.6 to 0.8 over thirteen ambiguous English
connectives. Labeled connectives are then
used into SMT systems either by mod-
ifying their phrase table, or by training
them on labeled corpora. The best modi-
fied SMT systems improve the translation
of connectives without degrading BLEU
scores. A threshold-based SMT system us-
ing only high-confidence labels improves
BLEU scores by 0.2?0.4 points.
1 Introduction
Current approaches to Statistical Machine Trans-
lation (SMT) have difficulties in modeling long-
range dependencies between words, including
those that are due to discourse-level phenomena.
Among these, discourse connectives are words
that signal rhetorical relations between clauses or
sentences. Their translation often depends on the
exact relation signaled in context, a feature that
current SMT systems were not designed to cap-
ture, hence their frequent mistranslations of con-
nectives (see Section 2 below).
In this paper, we present a series of experiments
that aim to use, in SMT systems, data with au-
tomatically labeled discourse connectives. Sec-
tion 3 first presents the data sets used in our ex-
periments. We designed classifiers that attempt to
assign sense labels to ambiguous discourse con-
nectives, and their scores compare favorably with
the state-of-the-art for this task, as shown in Sec-
tion 4. In particular, we consider WordNet rela-
tions and temporal expressions as well as candi-
date translations of connectives as additional fea-
tures (Section 4.2).
However, our main goal is not the disambigua-
tion of connectives per se, but the use of the labels
assigned to connectives as additional input to an
SMT system. To the best of our knowledge, our
experiments are the first attempts to combine con-
nective disambiguation and SMT. Three solutions
to this combination are compared in Section 5:
modifying phrase tables, and training on data la-
beled manually, or automatically, with senses of
connectives. We further show that a modified
SMT system is best used when the confidence for
a given label is high (Section 6). The paper con-
cludes with a comparison to related work (Sec-
tion 7) and an outline of future work (Section 8).
2 Discourse Connectives in Translation
Discourse connectives such as although, however,
since or while form a functional category of
lexical items that are frequently used to mark
coherence or discourse relations such as expla-
nation, synchrony or contrast between units of
text or discourse. For example, in the Europarl
corpus from years 199x (Koehn, 2005), the
following nine lexical items, which are often
(though not always) discourse connectives, are
among the 400 most frequent tokens over a
total of 12,846,003 (in parentheses, rank and
number of occurrences): after (244th/6485),
although (375th/4062), however (110th/12,857),
indeed (334th/4486), rather (316th/4688),
129
since (190th/8263), still (168th/9195), while
(390th/3938), yet (331st/4532) ? see also (Car-
toni et al, 2011). Discourse connectives can
be difficult to translate, because many of them
can signal different relations between clauses in
different contexts. Moreover, if a wrong connec-
tive is used in translation, then a text becomes
incoherent, as in the two examples below, taken
from Europarl and translated (EN/FR) with
Moses (Koehn et al, 2007) trained on the entire
corpus:
1. EN: This tax, though [contrast], does not come
without its problems.
FR-SMT: *Cette taxe, me?me si [concession],
ne se pre?sente pas sans ses proble`mes.
2. EN: Finally, and in conclusion, Mr President,
with the expiry of the ECSC Treaty, the
regulations will have to be reviewed since
[causal] I think that the aid system will have
to continue beyond 2002 . . .
FR-SMT: *Enfin, et en conclusion, Monsieur
le pre?sident, a` l?expiration du traite? CECA,
la re?glementation devra e?tre revu depuis que
[temporal] je pense que le syste`me d?aides
devront continuer au-dela` de 2002 . . .
In the first example, the connective generated
by SMT (me?me si, literally ?even if?) signals a
concession and not a contrast, for which the con-
nective mais should have been used (as in the ref-
erence). In the second example, the connective
depuis que (literally ?from the time?) generated
by SMT expresses a temporal relation and not a
causal one, which should have been conveyed e.g.
by the French car.
Such examples suggest that the disambiguation
of connectives prior to translation could help SMT
systems to generate a correct connective in the tar-
get language. Of course, depending on the lan-
guage pair, some ambiguities can be carried over
from the source to the target language, so they
need not be solved. Still, improving the over-
all translation of discourse connectives should in-
crease the overall coherence of MT output, with a
potential large impact on perceived quality.
3 Data Used in Our Experiments
For both tasks, the disambiguation of connectives
and SMT, different training and testing data sets
are available. This section shows how we made
use of these resources and how we augmented
them by manual and automated annotation of the
senses of discourse connectives.
3.1 Data for the Disambiguation of
Discourse Connectives
One of the most important resources for discourse
connectives in English is the Penn Discourse
Treebank (Prasad et al, 2008). The PDTB pro-
vides a discourse-layer annotation over the Wall
Street Journal Corpus (WSJ) and the Penn Tree-
bank syntactic annotation. The discourse anno-
tation consists of manually annotated senses for
about 100 types of explicit connectives, for im-
plicit ones, and their clause spans. For the en-
tire WSJ corpus of about 1,000,000 tokens there
are 18,459 instances of annotated explicit connec-
tives. The senses that discourse connectives can
signal are organized in a hierarchy with 4 toplevel
senses, followed by 16 subtypes on the second
level and 23 detailed subsenses on the third level.
Studies making use of the PDTB to build classi-
fiers usually split the WSJ corpus into Sections
02?21 for training and Section 23 for testing (as
we did for our disambiguation experiments, see
Section 4).
From the PDTB, we extracted the 13 most fre-
quent and most ambiguous connectives: after, al-
though, however, indeed, meanwhile, neverthe-
less, nonetheless, rather, since, still, then, while,
and yet. This set shows in particular that connec-
tives signaling contrastive or temporal senses are
the most ambiguous ones, hence they are also po-
tentially difficult to translate, as this ambiguity is
often not preserved across languages (Danlos and
Roze, 2011). We used the senses from the sec-
ond PDTB hierarchy level (as the third level is too
fine-grained for EN/FR translation) and generated
the training and testing sets listed with statistics in
Table 1 (Section 4).
In principle, classifiers trained on PDTB data
can be applied directly to label connectives over
the English side of the Europarl corpus (Koehn,
2005) used for training and testing SMT. How-
ever, to control the difference in register from
newswire texts to formal political speech, and to
allow for future studies of other languages, we
also performed manual annotation (Cartoni et al,
2011) of five connectives over the Europarl corpus
(although, even though, since, though and while).
130
The manual annotation was performed on sub-
sets of Europarl v5 (years 199x) for the first few
hundred occurrences of each connective. Instead
of a potentially difficult and costly annotation of
senses, as in the PDTB, we performed translation
spotting, asking annotators to highlight the trans-
lation of each of the five connectives in the French
side of the corpus. From the list of all observed
translations one can then cluster the necessary
sense labels, as some target language connectives
clearly signal only one sense or, in cases where
ambiguity is preserved, one can group the equally
ambiguous connectives under one composite la-
bel. For example, while is sometimes translated
to the French discourse connectives tandis que or
alors que which both preserve the ambiguity of
while signaling a temporal or contrastive sense.
With this method we built the data sets listed with
statistics in Table 2 below (Section 4).
3.2 Data for Statistical Machine Translation
The translation data for our SMT experiments has
been often used in other MT research work and is
freely distributed for the shared tasks of the Work-
shop on Machine Translation (WMT)1.
For training our SMT systems, the EN/FR Eu-
roparl corpus v5 was used in three ways to inte-
grate data with labeled discourse connectives into
SMT: no changes (for MT phrase table modifica-
tions), integration of manually annotated data and
integration of automatically labeled data. These
methods are described below in Section 5 ? here,
we gather descriptions of the corresponding data.
a: Modification of the phrase table: Europarl
(346,803 sentences), labeling the translation
model after training.
b: Integration of manual annotation: Europarl
(346,803 sentences), minus all 8,901 sen-
tences containing one of the above 5 connec-
tive types, plus 1,147 sentences with manu-
ally sense-labeled connectives.
c: Integration of automated annotation: Europarl
? years 199x (58,673 sentences), all occur-
rences of the 13 PDTB subset connective
types have been labeled by classifiers (in
6,961 sentences).
For Minimum Error Rate tuning (MERT) (Och,
2003) of the SMT systems, we used the 2009
1statmt.org/wmt10/translation-task.html
News Commentary (NC) EN/FR development set
with the following modifications:
d: Phrase table: NC 2009 (2,051 sentences), no
modifications.
e: Manual annotation: NC 2009 (2,051 sen-
tences), minus all 123 sentences containing
one of the above 5 connective types, plus 102
sentences with manually sense-labeled con-
nectives.
f: Automated annotation: NC 2009 (2,051 sen-
tences), all occurrences of the 13 PDTB sub-
set connective types have been labeled by
classifiers (in 340 sentences).
For testing our modified SMT systems, three
test sets were extracted in the following way:
g: 35 sentences from NC 2007, with 7 occur-
rences for each of the 5 connective types
above, manually labeled.
h: 62 sentences from NC 2007 and 2006 with oc-
currences for the 13 PDTB connective types,
automatically labeled with classifiers.
i: 10,311 sentences from the EN/FR UN corpus,
all occurrences of the five Europarl connec-
tive types, automatically labeled with classi-
fiers.
These test sets might appear small compared to
the amount of data normally used for SMT system
testing. In our system evaluation however, apart
from automated scoring, we also had to perform
manual counts of improved translations, which is
why we could not evaluate more than a hundred
sentences (Section 5). When counting manually
for test set (i), it was downsampled to the same
amount of 35 and 62 sentences as for sets (g)
and (h), by extracting the first occurrences of each
connective.
In all experiments, we use the Moses Phrase-
based SMT decoder (Koehn et al, 2007) and a 5-
gram language model built over the entire French
part of the Europarl corpus v5.
4 Automatically Disambiguating
Discourse Connectives
4.1 Classifier PT: Trained on PDTB Data
A first classifier (?PT?) for ambiguous discourse
connectives and their senses was built by using
the PDTB subset of 13 ambiguous connectives as
training material. For each connective we built a
131
Connective Number of occurrences and senses F1 Scores
Training set: total and per sense Test set: total and per sense PT PT+
after 507 456 As, 51 As/Ca 25 22 As, 3 As/Ca 0.66 1.00
although 267 135 Cs, 118 Ct, 14 Cp 16 9 Ct, 7 Cs 0.60 0.66
however 176 121 Ct, 32 Cs, 23 Cp 14 13 Ct, 1 Cs 0.33 1.00
indeed 69 37 Cd, 24 R, 3 Ca, 3 E, 2 I *2 2 R *0.50 *0.50
meanwhile 117 66 Cj/S, 16 Cd, 16 S, 14
Ct/S, 5 Ct
10 5 S, 5 Ct/S 0.32 0.53
nevertheless 26 15 Ct, 11 Cs 6 4 Cs, 2 Ct 0.44 0.66
nonetheless 12 7 Cs, 3 Ct, 2 Cp *1 1 Cs *1.00 *1.00
rather 10 6 R, 2 Al, 1 Ca, 1 Ct *1 1 Al *0.00 *0.00
since 166 75 As, 83 Ca, 8 As/Ca 9 4 As, 3 Ca, 2
As/Ca
0.78 0.78
still 114 56 Cs, 51 Ct, 7 Cp 13 9 Ct, 4 Cs 0.60 0.66
then 145 136 As, 6 Cd, 3 As/Ca 6 5 As, 1 Cd 0.83 1.00
while 631 317 Ct, 140 S, 79 Cs, 41
Ct/S, 36 Cd, 18 Cp
37 19 Ct, 10 S, 4 Cs,
4 Ct/S
0.93 0.96
yet 80 46 Ct, 25 Cs, 9 Cp *2 2 Ct *0.5 *1.00
Total 2,320 ? 142 ? 0.57 0.75
Table 1: Performance of MaxEnt connective sense classifiers: Classifier PT (initial feature set) and Classifier
PT+ (with candidate translation features) for 13 temporal and contrastive connectives in the PDTB. The sense
labels are coded as follows. Al: alternative, As: asynchronous, Ca: cause, Cd: condition, Cj: conjunction, Cp:
comparison, Cs: concession, Ct: contrast, E: expansion, I: instantiation, R: restatement, S: synchrony. In some
cases marked with ?*?, the test sets are too small to provide meaningful scores.
specialized classifier, by using the Stanford Max-
imum Entropy classifier package (Manning and
Klein, 2003). Maximum Entropy is known to han-
dle discrete features well and has been applied
successfully to connective disambiguation before
(see Section 7).
An initial set of features can directly be ob-
tained from the PDTB (and must hence be con-
sidered as oracle features): the (capitalized) con-
nective token, its POS tag, first word of clause 1,
last word of clause 1, first word of clause 2 (the
one containing the explicit connective), last word
of clause 2, POS tag of the first word of clause 2,
type of first word of clause 2, parent syntactical
categories of the connective, punctuation pattern
of the sentences. Apart from these standard fea-
tures in discourse connective disambiguation we
used WordNet (Miller, 1995) to compute lexical
similarity scores with the lesk metric (Baner-
jee and Pedersen, 2002) for all the possible com-
binations of nouns, verbs and adjectives in the
two clauses, as well as antonyms found for these
word groups. In addition, we used features that
are likely to help detecting temporal relations and
were obtained from the Tarsqi Toolkit (Verhagen
and Pustejovsky, 2008), which annotates English
sentences automatically with the TimeML anno-
tation language for temporal expressions. For ex-
ample, in the sentence The crimes may appear
small, but the prices can be huge (PDTB Sec-
tion 2, WSJ file 0290), for example, our features
would indicate the antonyms small vs. huge that
signal the contrast, along with a temporal order-
ing of the event appear before the event can.
We report the classifier performances as F1
scores for each connective (weighting precision
and recall equally) in Table 1, testing on Section
23 of the PDTB. This sense classifier will be re-
ferred to as Classifier PT in the rest of the paper,
in particular when used for the SMT experiments.
4.2 Classifier PT+: With Candidate
Translations as Features
In an attempt to improve Classifier PT, we added
a new type of feature, resulting in Classifier PT+.
Namely, we used candidate translations of dis-
course connectives from a baseline SMT system
(not adapted to connectives). To find these values,
a Moses baseline decoder was used to translate the
PDTB data, which was then word-aligned (En-
132
Connective Number of occurrences and senses F1
Size of training set: total and per sense Test set: total and per sense Score
although 173 155 Cs, 18 Ct 10 5 Cs, 5 Ct 0.67
even though 179 165 Cs, 14 Ct 10 5 Cs, 5 Ct 1.00
since 413 274 S, 131 Ca, 8 S/Ca 10 5 Ca, 3 S, 2 S/Ca 0.80
though 150 80 Cs, 70 Ct 10 5 Cs, 5 Ct 1.00
while 280 130 Cs, 41 Ct, 89 S/Ct, 13 S/Ca, 7 S 14 4 Cs, 2 Ct, 2 S/Ct, 2
S/Ca, 4 S
0.64
Total 1,195 ? 54 ? 0.82
Table 2: Performance of a MaxEnt connective sense classifier (Classifier EU) for 5 connectives in the Europarl
corpus. The sense labels are coded as follows. Cs: Concession, Ct: Contrast, S: Synchrony, Ca: Cause.
glish source with target French) by using GIZA++
(Och and Ney, 2003). In this alignment, we
searched for the translation equivalents of the 13
PDTB connectives by using a hand-crafted dic-
tionary of possible French translations. When the
translation candidate is not ambiguous ? e.g. bien
que as a translation for while clearly signals a con-
cession ? its specific sense label was added as the
value of an additional feature. In some cases,
however, the values of the features are not de-
termined (and are set to NONE): either when the
SMT system or GIZA++ failed in translating or
aligning a connective, or when the target connec-
tive was just as ambiguous as the source one (e.g.
while translated as tandis que, which can be la-
beled both temporal or contrast). Overall, this
procedure led to an accuracy gain of Classifier
PT+ with respect to Classifier PT of about 0.1 to
0.6 F1 score for some of the connectives, as can
be seen in the last column of Table 1.
4.3 Classifier EU: Trained on Europarl Data
As explained in Section 3.1, we performed man-
ual annotation of connective senses in Europarl
as well, to provide labeled instances directly in
the data used for SMT training and to account for
the register change. For the Europarl data sets,
we built a new MaxEnt classifier (called Classi-
fier EU) using the same feature set as Classifier
PT. However, all features were this time extracted
automatically (no oracle). In particular, we used
Charniak and Johnson?s (2005) parser to then ex-
tract the syntactic features. In Table 2, we re-
port the results of Classifier EU, again in terms
of F1 scores. For all three classifiers, PT, PT+
and EU, the F1 scores are in a range of 0.6 and
0.8, thus comparing favorably to the state-of-the-
art for discourse connective disambiguation with
detailed senses (Section 7). Classifier EU also
compares favorably to PT and PT+, as seen for in-
stance for since (0.80 vs. 0.78) or although (0.67
vs. 0.60?0.66).
5 Use of Labeled Connectives for SMT
In this section, we report on experiments that
study the effect of discourse connective labeling
on SMT. The experiments differ with respect to
the method used for taking advantage of the la-
bels, but also with respect to the data sets and the
sense classifiers that are used.
5.1 Evaluation Metrics for MT
The variation in MT quality can be estimated in
several ways. On the one hand, we use the BLEU
metric (Papineni et al, 2002) with one reference
translation as is most often done in current SMT
research2. To improve confidence in the BLEU
scores, especially when test sets are small, we
also compute BLEU scores using bootstrapping
of data sets (Zhang and Vogel, 2010); the test
sets are re-sampled a thousand times and the av-
erage BLEU score is computed from individual
sample scores. The BLEU approach is not likely,
however, to be sensitive enough to the small dif-
ferences due to the correction of discourse con-
nectives (less than one word per sentence). We
therefore additionally resort to a manual evalua-
tion metric, referred to as ?Connectives, which
counts the occurrences of connectives that are bet-
ter translated by our modified systems compared
to the baseline ones.
2The scores are generated by the NIST MTeval script
version 11b, available from www.itl.nist.gov/iad/
mig/tools/.
133
MT system N. Connectives in MT test data ?Conn. (%) BLEU scores
Occ. Types Labeling + = ? Standard Bootstrap
Modified phrase table 1 35 5 manual 29 51 20 39.92 40.54
2 10,311 5 Cl. EU 34 46 20 22.13 23.63
Trained on manual 3 35 5 manual 32 57 11 41.58 42.38
annotations 4 10,311 5 Cl. EU 26 66 8 22.43 24.00
Trained on automatic 5 62 13 Cl. PT 16 60 24 14.88 15.96
annotations (Cl. PT) 6 10,311 5 Cl. EU 16 66 18 19.78 21.17
Trained on automatic 7 62 13 Cl. PT+ 11 70 19 15.67 16.73
annotations (Cl. PT+) 8 10,311 5 Cl. EU 18 68 14 20.14 21.55
Table 3: MT systems dealing with manually and automatically (PT, PT+, EU) sense-labeled connectives: BLEU
scores (including bootstrapped ones) and variation in the translation of individual connectives (?Connectives,
as a percentage). The description of each condition and the baseline BLEU scores are in the text of the article.
5.2 Phrase Table Modification
A first way of using labeled connectives is to
modify the phrase table of an SMT system previ-
ously trained/tuned on data sets (a)/(d) from Sec-
tion 3.2, in order to force it to translate each spe-
cific sense of a discourse connective (as indicated
by its label) with an acceptable equivalent se-
lected among those learned from the training data.
Of course, this only handles cases when connec-
tives are translated by explicit lexical items (typ-
ically, target connectives) and not by more com-
plex grammatical constructs.
The phrase table modification is done as fol-
lows. Based on a small dictionary of the five con-
nective types of Table 2, their acceptable French
equivalents and the possible senses, the initial
phrase table is searched for phrases containing a
connective and each occurrence is inspected to
find out which sense is reflected in the transla-
tion. If the sense is non-ambiguous, then the ta-
ble entry is modified to include the label, and the
probability score is set to 1 in order to maximize
the chance that the respective translation is found
during decoding. For instance, for every phrase
table entry where while is translated as alors que,
this corresponds to a contrastive use and while is
changed into while CONTRAST. Or, for the en-
tries where while is translated as bien que, the
lexical entry is changed into while CONCESSION.
However, when the source entry is as ambiguous
as the target one, no modification is made. This
means that during decoding (testing) with labeled
sentences, these entries will never be used.
The results of the SMT system are shown in
experiments 1 and 2 in Table 3, respectively test-
ing over data set (g) (7 manually annotated sen-
tences for each of the 5 connectives) and over
set (i), in which the 5 connectives were automat-
ically labeled with Classifier EU. In the first test,
the translations of 29% of the connectives are im-
proved by the modified system, while 20% are
degraded and 51% remain unchanged ? thus re-
flecting an overall 10% improvement in the trans-
lations of connectives (?Connectives). How-
ever, for this test set, the BLEU score is about 3
points below the baseline SMT system that used
the same phrase table without modification of la-
bels and scores (not shown in Table 3). In exper-
iment 2, however, the BLEU score of the modi-
fied system is in the same range as the baseline
one (22.13 vs. 22.76). As for ?Connectives,
as it was not possible to score manually all the
10,311 connectives, we sampled 35 sentences and
found that 34% of the connectives are improved,
20% are degraded and 46% remain unchanged,
again reflecting an improvement in the translation
of connectives. This shows that piping automatic
labeling and SMT with a modified phrase table
does not degrade the overall BLEU score, while
increasing ?Connectives.
5.3 Training on Tagged Corpora
We explored a more principled way to integrate
external labels into SMT, by using labeled data
(manually or automatically) for training, so that
the system directly learns a modified phrase table
which allows the translation of labeled data (auto-
matically) when testing.
134
5.3.1 Manual Gold Annotation
We report first two experiments using the man-
ual gold annotation for the five connective types
over Europarl excerpts, used for training. When
used also for testing (experiment 3 in Table 3),
this can be seen as an oracle experiment, measur-
ing the translation improvement when connective
sense labeling is perfect. However, in experiment
4, the SMT system uses the output of an auto-
matic labeler. For training/tuning we used data
sets (b)/(e), Section 3.2.
In experiment 3, for test set (g), 32% of the
connectives were translated better by the modi-
fied system, 57% remained the same, and 11%
were degraded. In experiment 4, over a 35 sen-
tence sample of the bigger test set (i), 26% were
improved, 66% remained the same, and only
8% were degraded. The baseline SMT system
(not shown in Table 3) was built with the same
amounts of unlabeled training and tuning data.
Overall, the BLEU scores of our modified systems
are similar to the baseline ones, though still lower
? 41.58 vs. 42.77 for experiment 3, and 22.43
vs. 22.76 for experiment 4, also confirmed by the
bootstrapped scores.
Another comparison shows that the system
trained on manual annotations (exp. 4) outper-
forms the system using a modified phrase ta-
ble (exp. 2) in terms of BLEU scores (22.43 vs.
22.13) and bootstrapped ones (24.00 vs. 23.63).
5.3.2 Automated Annotation
We evaluated an SMT system trained on data
that was automatically labeled using the classi-
fiers in Section 4. This method provides a large
amount of imperfect training data, and uses no
manual annotations at all, except for the initial
training of the classifiers. For these experiments
(5 and 6 in Table 3), the BLEU scores as well
as the manual counts of improved connectives are
lower than in the preceding experiments because,
overall, less training/tuning data was used ? about
15% of Europarl, data sets (c) and (f) in Sec-
tion 3.2. The baseline system was built over the
same amount of data, with no labels.
Testing here was performed over the slightly
bigger test set (h) with 62 sentences (13 connec-
tive types). The occurrences were tagged with
Classifier PT prior to translation (exp. 5). Com-
pared to the baseline system, the translations of
16% of the connectives were improved, while
60% remained the same and 24% were degraded.
In experiment 6, the 10,311 UN occurrences for 5
connective types were first tagged with Classifier
EU. Evaluated on a sample of 62 sentences, 16%
of the connectives were improved, while 66% re-
mained the same and 18% were degraded. De-
spite less training data, in terms of BLEU, the dif-
ference to the respective baseline system (scores
not shown in Table 3) is similar in both experi-
mental settings: 19.78 vs. 20.11 for experiment
6 (automated annotation), compared to 22.43 vs.
22.76 for experiment 4 (manual annotation).
Finally, we carried out two experiments (7
and 8) with Classifier PT+, which uses as addi-
tional features the translation candidates and has a
higher accuracy than PT (Section 4.2). As a result,
the translation of connectives (?Connectives) is
indeed improved compared (respectively) to ex-
periments 5 and 6, as it appears from lines 7?8
of Table 3. Also, the BLEU scores of the corre-
sponding SMT systems are increased in experi-
ments 7 vs. 5 and in 8 vs. 6, and are now equal
to the baseline ones (for experiment 8: 20.14 vs.
20.11, or, bootstrapped, 21.55 vs. 21.55).
The results of experiments 7/8 vs. 5/6 in-
dicate that improved classifiers for connec-
tives also improve SMT output as measured by
?Connectives, with BLEU remaining fairly
constant, and therefore are worth investigating
in more depth in the future. When compar-
ing manual (experiments 3/4) vs. automated an-
notation (experiments 5/6/7/8) and their use in
SMT, the differences in the scores (BLEU and
?Connectives) highlight a trade-off: manually
annotated data used for training leads to better
scores, but noisier and larger training data that is
annotated automatically is an acceptable solution
when manual annotations are not available.
6 Classifier Confidence Scores
As shown with the above experiments, the accu-
racy of the connective classifiers influences SMT
quality. We therefore hypothesize that an SMT
system dealing with labeled connectives would
best be used when the confidence of the classi-
fier is high, while a generic SMT system could be
used for lower confidence values.
We experimented with the confidence scores of
Classifier EU, which assigns a score between 0
and 1 to each of its decisions on the connectives?
labels. (All processing is automatic in these ex-
135
(a) although (b) since
Figure 1: Use of a combined system (COMB) that directs the input sentences either to a system trained on a sense-
labeled corpus (TTC) or to a baseline one (BASE), depending on the confidence of the connective classifier. The
x-axis shows the threshold above which TTC is used ? BASE being used below it ? and the y-axis shows the
BLEU scores of COMB with respect to TTC and BASE. Figure (a) is for although and (b) for since.
periments, and the evaluation is done solely in
terms of BLEU). We defined a threshold-based
procedure to combine SMT systems: if the con-
fidence for a sense label is above a certain thresh-
old, then the sentence is translated by an SMT
system trained on labeled data from experiment
4 (or ?tagged corpus?, hence noted TTC), and if it
is below the threshold, it is sent to a baseline sys-
tem (noted BASE). The resulting BLEU scores of
the combined system (COMB) obtained for vari-
ous threshold values are shown in Figure 1 for two
connectives.
Firstly, we considered all the 1,572 sentences
from the UN corpus which contained the connec-
tive although, labeled either as contrast or con-
cession. We show BLEU scores of the COMB
system for several thresholds in the interval of ob-
served confidence scores, along with the scores of
BASE and TTC, in Figure 1(a). The results show
that the scores of COMB increase with the value
of the threshold, and that for at least one value
of the threshold (0.95) COMB outperforms both
TTC and BASE by 0.20 BLEU points.
To confirm this finding with another connec-
tive, we took the first 1,572 sentences containing
the connective since from the UN corpus. The
BLEU scores for COMB are shown for the range
of observed confidence values (0.4?1.0) in Fig-
ure 1(b). For several values of the threshold,
COMB outperforms both BASE and TTC, in par-
ticular for 0.85, with a difference of 0.39 BLEU
points.
The significance of the observed improvement
was tested as follows. For each of the two con-
nectives, we split the test sets of 1,572 sentences
each in five folds, and compared for each fold the
scores of COMB for the best performing thresh-
old (0.95 or 0.85) with the highest of BASE or
TTC (i.e. BASE for although and TTC for since).
We performed a paired t-test to compute the sig-
nificance of the difference, and found p = 0.12 for
although. This value, although slightly above the
conventional boundary of 0.1, shows that the five
pairs of scores reflect a significant difference in
quality. Similarly, when performing a t-test for
since, the difference in scores is found significant
at the 0.01 level (p = 0.005). Of course, COMB
is always significantly better than the lower of
BASE or TTC (p < 0.05). In the future, the sys-
tem combination will be tested for all connectives,
and the respective values of the thresholds will be
set on tuning, not on test data.
7 Related Work
Discourse parsing (Marcu, 2000) has proven to
be a difficult task, even when complex models
(CRFs, SVMs) are used (Wellner, 2009; Her-
nault et al, 2010). The performance of discourse
parsers is in a range of 0.4 to 0.6 F1 score.
136
With the release of the PDTB, recent research
focused on the disambiguation of discourse con-
nectives as a task in its own right. For the disam-
biguation of explicit connectives, the state-of-the-
art performance for labeling all types of connec-
tives in English is quite high. In the PDTB data,
the disambiguation of discourse vs. non-discourse
uses of connectives reaches 97% accuracy (Lin et
al., 2010). The labeling of the four main senses
from the PDTB sense hierarchy (temporal, contin-
gency, comparison, expansion) reaches 94% ac-
curacy (Pitler and Nenkova, 2009) ? however, the
baseline accuracy is already around 85% when us-
ing only the connective token as a feature. Vari-
ous methods for classification and feature analy-
sis have been proposed (Wellner et al, 2006; El-
well and Baldridge, 2008). Other studies have
focused on the analysis of highly ambiguous dis-
course connectives only. Miltsakaki et al (2005)
report classification results for the connectives
since, while and when. Using a Maximum En-
tropy classifier, they reach 75.5% accuracy for
since, 71.8% for while and 61.6% for when. As
the PDTB was not completed at that time, the data
sets and labels are not exactly identical to the ones
that we used above (see Section 4).
The disambiguation of senses signaled by dis-
course connectives can be seen as a word sense
disambiguation (WSD) problem for functional
words (as opposed to WSD for content words,
which is more frequently studied). The integra-
tion of WSD into SMT has especially been stud-
ied by Carpuat and Wu (2007), who used the
translation candidates output by a baseline SMT
system as word sense labels. This is similar to
our use of translation candidates as an additional
feature for classification in Section 4.2. Then,
the output of several classifiers based on linguis-
tic features was weighed against the translation
candidates output by the baseline SMT system.
With this procedure, their WSD+SMT system im-
proved the BLEU scores by 0.4?0.5 for the En-
glish/Chinese pair.
Chang et al (2009) use a LogLinear classi-
fier with linguistic features in order to disam-
biguate the Chinese particle ?DE? that has five dif-
ferent context-dependent uses (modifier, preposi-
tion, relative clause etc.). When the classifier is
used to annotate the particle prior to SMT, the
output of the translation system improves by up
to 1.49 BLEU score for phrase-based Chinese to
English translation. Ma et al (2011) use a Maxi-
mum Entropy model to POS tag English colloca-
tional particles (e.g. come down/by, turn against,
inform of ) more specifically than a usual POS tag-
ger does (where only one label is given to all par-
ticles). The authors claim the usefulness of such
a particle tagger for English/Chinese translation,
but do not show its actual integration into an MT
system.
These approaches, as well as ours, show that
integrating discourse information into SMT is
promising and deserves future examination. The
disambiguation of word senses, including func-
tion words, can improve SMT output when the
senses are annotated in a pre-processing step that
uses classifiers based on linguistic features at
the semantic and discourse levels, which are not
available to a state-of-the-art SMT systems.
8 Conclusion and Future Work
This paper has presented methods and results for
the disambiguation of temporal and contrastive
discourse connectives using MaxEnt classifiers
with syntactic and semantic features, in English
texts, in terms of senses intended to help SMT.
These classifiers have been used to perform exper-
iments with connective-annotated data applied to
EN/FR SMT systems. The results have shown an
improvement in the translation of connectives for
fully automatic systems trained on either hand-
labeled or automatically-labeled data. Moreover,
BLEU scores were significantly improved by 0.2?
0.4 when such systems were only used for con-
nectives that had been disambiguated with high
confidence.
In future work we plan to improve the sense
classifiers using additional features, to improve
their integration with SMT, and to unify our data
sets through additional manual annotations over
Europarl. The applicability of the method to other
languages will also be demonstrated experimen-
tally.
Acknowledgments
We are grateful for the funding of this
work to the Swiss National Science Foun-
dation (SNSF), under the COMTIS Sin-
ergia Project n. CRSI22 127510 (see
www.idiap.ch/comtis/).
137
References
Satanjeev Banerjee and Ted Pedersen. 2002. An
Adapted Lesk Algorithm for Word Sense Disam-
biguation Using WordNet. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, LNCS 2276, pages 117?171.
Springer, Berlin/Heidelberg.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. Proc. of EMNLP-CoNLL, pages 61?
72, Prague.
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How Comparable
are Parallel Corpora? Measuring the Distribution of
General Vocabulary and Connectives. Proc. of the
4th Workshop on Building and Using Comparable
Corpora (BUCC), pages 78?86, Portland, OR.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D.
Manning. 2009. Disambiguating ?DE? for Chinese-
English Machine Translation. Proc. of the Fourth
Workshop on Statistical Machine Translation at
EACL-2009, Athens.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best Parsing and MaxEnt Discriminative
Reranking. Proc. of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor, MI.
Laurence Danlos and Charlotte Roze. 2011. Traduc-
tion (Automatique) des Connecteurs de Discours.
Actes 18e Confe?rence sur le Traitement Automa-
tique des Langues Naturelles (TALN), Montpellier.
Robert Elwell and Jason Baldridge. 2008. Discourse
Connective Argument Identification with Connec-
tive Specific Rankers. Proc. of the 2nd IEEE
International Conference on Semantic Computing
(ICSC), pages 198?205, Santa Clara, CA.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser using Support Vector Machine classification.
Dialogue and Discourse, 3(1):1?33.
Philipp Koehn, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. Proc.
of 45th Annual Meeting of the ACL, Demonstration
Session, pages 177?180, Prague.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proc. of MT Sum-
mit X, pages 79?86, Phuket.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A PDTB-styled End-to-end Discourse Parser. Tech-
nical Report TRB8/10, School of Computing, Na-
tional University of Singapore.
Jianjun Ma, Degen Huang, Haixia Liu, and Wenfeng
Sheng. 2011. POS Tagging of English Particles
for Machine Translation. Proc. of MT Summit XIII,
pages 57?63, Xiamen.
Christopher Manning and Dan Klein. 2003. Opti-
mization, MaxEnt Models, and Conditional Estima-
tion without Magic. Tutorial at HLT-NAACL and
41st ACL conferences, Edmonton, AB and Sapporo.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. A Bradford
Book. The MIT Press, Cambridge, MA.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38(11):39?41.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Exper-
iments on Sense Annotations and Sense Disam-
biguation of Discourse Connectives. Proc. of the
4th Workshop on Treebanks and Linguistic Theories
(TLT), Barcelona.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. Proc. of the
41st Annual Meeting of the ACL, pages 160?167,
Sapporo.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A method for Au-
tomatic Evaluation of Machine Translation. Proc.
of 40th Annual Meeting of the ACL, pages 311?318,
Philadelphia, PA.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. Proc. of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference of the
AFNLP (ACL-IJCNLP), Short Papers, pages 13?16,
Singapore.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Treebank
2.0. Proc. of the 6th International Conference on
Language Resources and Evaluation (LREC), pages
2961?2968, Marrakech.
Marc Verhagen and James Pustejovsky. 2008. Tem-
poral Processing with the TARSQI Toolkit. Proc.
of the 22nd International Conference on Com-
putational Linguistics (COLING), Demonstrations,
pages 189?192, Manchester, UK.
Ben Wellner, James Pustejovsky, Catherine Havasi,
Roser Sauri, and Anna Rumshisky. 2006. Classi-
fication of Discourse Coherence Relations: An Ex-
ploratory Study using Multiple Knowledge Sources.
Proc. of the 7th SIGdial Meeting on Discourse and
Dialog, pages 117?125, Sydney.
Ben Wellner. 2009. Sequence Models and Ranking
Methods for Discourse Parsing. PhD thesis, Bran-
deis University, Waltham, MA.
Ying Zhang and Stefan Vogel. 2010. Significance
Tests of Automatic Machine Translation Evaluation
Metrics. Machine Translation, 24(1):51?65.
138
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 33?42,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Detecting Narrativity to Improve English to French
Translation of Simple Past Verbs
Thomas Meyer
Idiap Research Institute and EPFL
Martigny and Lausanne, Switzerland
thomas.meyer@idiap.ch
Cristina Grisot
University of Geneva
Switzerland
cristina.grisot@unige.ch
Andrei Popescu-Belis
Idiap Research Institute
Martigny, Switzerland
andrei.popescu-belis@idiap.ch
Abstract
The correct translation of verb tenses en-
sures that the temporal ordering of events
in the source text is maintained in the tar-
get text. This paper assesses the utility
of automatically labeling English Simple
Past verbs with a binary discursive fea-
ture, narrative vs. non-narrative, for sta-
tistical machine translation (SMT) into
French. The narrativity feature, which
helps deciding which of the French past
tenses is a correct translation of the En-
glish Simple Past, can be assigned with
about 70% accuracy (F1). The narrativity
feature improves SMT by about 0.2 BLEU
points when a factored SMT system is
trained and tested on automatically labeled
English-French data. More importantly,
manual evaluation shows that verb tense
translation and verb choice are improved
by respectively 9.7% and 3.4% (absolute),
leading to an overall improvement of verb
translation of 17% (relative).
1 Introduction
The correct rendering of verbal tenses is an im-
portant aspect of translation. Translating to a
wrong verbal tense in the target language does not
convey the same meaning as the source text, for
instance by distorting the temporal order of the
events described in a text. Current statistical ma-
chine translation (SMT) systems may have diffi-
culties in choosing the correct verb tense transla-
tions, in some language pairs, because these de-
pend on a wider-range context than SMT systems
consider. Indeed, decoding for SMT is still at
the phrase or sentence level only, thus missing
information from previously translated sentences
(which is also detrimental to lexical cohesion and
co-reference).
In this paper, we explore the merits of a dis-
course feature called narrativity in helping SMT
systems to improve their translation choices for
English verbs in the Simple Past tense (hence-
forth, SP) into one of the three possible French
past tenses. The narrativity feature characterizes
each occurrence of an SP verb, either as narrative
(for ordered events that happened in the past) or
non-narrative (for past states of affairs). Narra-
tivity is potentially relevant to EN/FR translation
because three French past tenses can potentially
translate an English Simple Past (SP), namely the
Passe? Compose? (PC), Passe? Simple (PS) or Impar-
fait (IMP). All of them can be correct translations
of an EN SP verb, depending on its narrative or
non-narrative role.
The narrativity feature can be of use to SMT
only if it can be assigned with sufficient preci-
sion over a source text by entirely automatic meth-
ods. Moreover, a narrativity-aware SMT model is
likely to make a difference with respect to base-
line SMT only if it is based on additional features
that are not captured by, e.g., a phrase-based SMT
model. In this study, we use a small amount of
manually labeled instances to train a narrativity
classifier for English texts. The (imperfect) out-
put of this classifier over the English side of a
large parallel corpus will then be used to train a
narrativity-aware SMT system. In testing mode,
the narrativity classifier provides input to the SMT
system, resulting (as we will show below) in im-
proved tense and lexical choices for verbs, and
a modest but statistically significant increase in
BLEU and TER scores. Overall, the method is
similar in substance to our previous work on the
33
combination of a classifier for discourse connec-
tives with an SMT system (Meyer and Popescu-
Belis, 2012; Meyer et al, 2012).
The paper is organized as follows. Section 2 ex-
emplifies the hypothesized relation between nar-
rativity and the translations of the English Sim-
ple Past into French, along with related work on
modeling tense for MT. The automatic labeling
experiments are presented in Section 3. Exper-
iments with SMT systems are presented in Sec-
tion 4, with results from both automatic (4.3) and
manual translation scoring (4.4), followed by a
discussion of results and suggestions on improv-
ing them (Section 5).
2 English Simple Past in Translation
2.1 Role of Narrativity: an Example
The text in Figure 1 is an example taken from the
?newstest 2010? data described in Section 4 below.
In this four-sentence discourse, the English verbs,
all in Simple Past, express a series of events hav-
ing occurred in the past, which no longer affect
the present. As shown in the French translation by
a baseline SMT system (not aware of narrativity),
the English SP verbs are translated into the most
frequent tense in French, as learned from the par-
allel data the SMT was trained on.
When looking more closely, however, it ap-
pears that the Simple Past actually conveys dif-
ferent temporal and aspectual information. The
verbs offered and found describe actual events that
were ordered in time and took place subsequently,
whereas were and was describe states of general
nature, not indicating any temporal ordering.
The difference between narrative and non-
narrative uses of the English Simple Past is not al-
ways captured correctly by the baseline SMT out-
put in this example. The verbs in the first and third
sentences are correctly translated into the French
PC (one of the two tenses for past narratives in
French along with the PS). The verb in the sec-
ond sentence is also correctly rendered as IMP,
in a non-narrative use. However, the verb was in
the fourth sentence should also have been trans-
lated as an IMP, but from lack of sufficient infor-
mation, it was incorrectly translated as a PC. A
non-narrative label could have helped to find the
correct verb tense, if it would have been annotated
prior to translation.
EN: (1) After a party, I offered [Narrative] to
throw out a few glass and plastic bottles. (2) But,
on Kounicova Ulice, there were [Non-narrative]
no colored bins to be seen. (3) Luckily, on the
way to the tram, I found [Narrative] the right
place. (4) But it was [Non-narrative] overflow-
ing with garbage.
FR from BASELINE MT system: (1) Apre`s
un parti, j?ai propose? pour rejeter un peu de
verre et les bouteilles en plastique. (2) Mais, sur
Kounicova Ulice, il n?y avait pas de colored bins
a` voir. (3) Heureusement, sur la manie`re de le
tramway, j?ai trouve? la bonne place. (4) Mais il
*a e?te? de?borde?s avec des ramasseurs.
Figure 1: Example English text from ?newstest
2010? data with narrativity labels and a translation
into French from a baseline SMT. The tenses gen-
erated in French are, respectively: (1) PC, (2) IMP,
(3) PC, (4) PC. The mistake on the fourth one is
explained in the text.
2.2 Modeling Past Tenses
The classical view on verb tenses that express past
tense in French (PC, PS and IMP) is that both the
PC and PS are perfective, indicating that the event
they refer to is completed and finished (Martin,
1971). Such events are thus single points in time
without internal structure. However, on the one
hand, the PC signals an accomplished event (from
the aspectual point of view) and thus conveys as
its meaning the possible consequence of the event.
The PS on the other hand is considered as aspectu-
ally unaccomplished and is used in contexts where
time progresses and events are temporally ordered,
such as narratives.
The IMP is imperfective (as its name suggests),
i.e. it indicates that the event is in its preparatory
phrase and is thus incomplete. In terms of aspect,
the IMP is unaccomplished and provides back-
ground information, for instance ongoing state of
affairs, or situations that are repeated in time, with
an internal structure.
Conversely, in English, the SP is described as
having as its main meaning the reference to past
tense, and as specific meanings the reference to
present or future tenses identified under certain
contextual conditions (Quirk et al, 1986). Cor-
blin and de Swart (2004) argue that the SP is as-
pectually ?transparent?, meaning that it applies to
34
all types of events and it preserves their aspectual
class.
The difficulty for the MT systems is thus
to choose correctly among the three above-
mentioned tenses in French, which are all valid
possibilities of translating the English SP. When
MT systems fail to generate the correct tense in
French, several levels of incorrectness may oc-
cur, exemplified in Figure 2 with sentences taken
from the data used in this paper (see Section 3 and
Grisot and Cartoni (2012)).
1. In certain contexts, tenses may be quite inter-
changeable, which is the unproblematic case
for machine translation, depending also on
the evaluation measure. In Example 1 from
Figure 2, the verb e?taient conside?re?es (were
seen) in IMP has a focus on temporal length
which is preserved even if the translated tense
is a PC (ont e?te? conside?re?es, i.e. have been
seen) thanks to the adverb toujours (always)).
2. In other contexts, the tense proposed by the
MT system can sound strange but remains ac-
ceptable. For instance, in Example 2, there
is a focus on temporal length with the IMP
translation (voyait, viewed) but this meaning
is not preserved if a PC is used (a vu, has
viewed) though it can be recovered by the
reader.
3. The tense output by an MT system may be
grammatically wrong. In Example 3, the PC
a renouvele? (has renewed) cannot replace the
IMP renouvelaient (renewed) because of the
conflict with the imperfective meaning con-
veyed by the adverbial sans cesse (again and
again).
4. Finally, a wrong tense in the MT output can
be misleading, if it does not convey the mean-
ing of the source text but remains unnoticed
by the reader. In Example 4, using the PC
a e?te? leads to the interpretation that the per-
son was no longer involved when he died,
whereas using IMP e?tait implies that he was
still involved, which may trigger very differ-
ent expectations in the mind of the reader
(e.g. on the possible cause of the death, or its
importance to the peace process).
1. EN: Although the US viewed Musharraf as an agent
of change, he has never achieved domestic political legiti-
macy, and his policies were seen as rife with contradictions.
FR: Si les Etats-Unis voient Moucharraf comme un agent
de changement, ce dernier n?est jamais parvenu a` avoir
une le?gitimite? dans son propre pays, ou` ses politiques ont
toujours e?te? conside?re?es (PC) / e?taient conside?re?es (IMP)
comme un tissu de contradictions.
2. EN: Indeed, she even persuaded other important
political leaders to participate in the planned January 8
election, which she viewed as an opportunity to challenge
religious extremist forces in the public square.
FR: Benazir Bhutto a me?me convaincu d?autres dirigeants
de participer aux e?lections pre?vues le 8 janvier, qu?elle voy-
ait (IMP) / ?a vu (PC) comme une occasion de s?opposer
aux extre?mistes religieux sur la place publique.
3. EN: The agony of grief which overpowered them
at first, was voluntarily renewed, was sought for, was
created again and again...
FR: Elles s?encourage`rent l?une l?autre dans leur affliction,
la renouvelaient (IMP) / l?*a renouvele? (PC) volontaire-
ment, et sans cesse...
4. EN: Last week a person who was at the heart of
the peace process passed away.
FR: La semaine passe?e une personne qui e?tait (IMP) / a
e?te? (PC) au c?ur du processus de paix est de?ce?de?e.
Figure 2: Examples of translations of the English
SP by an MT system, differing from the refer-
ence translation: (1) unproblematic, (2) strange
but acceptable, (3) grammatically wrong (*), and
(4) misleading.
2.3 Verb Tenses in SMT
Modeling verb tenses for SMT has only recently
been addressed. For Chinese/English translation,
Gong et al (2012) built an n-gram-like sequence
model that passes information from previously
translated main verbs onto the next verb so that
its tense can be more correctly rendered. Tense is
morphologically not marked in Chinese, unlike in
English, where the verbs forms are modified ac-
cording to tense (among other factors). With such
a model, the authors improved translation by up to
0.8 BLEU points.
Conversely, in view of English/Chinese trans-
lation but without implementing an actual trans-
lation system, Ye et al (2007) used a classifier
to generate and insert appropriate Chinese aspect
markers that in certain contexts have to follow the
Chinese verbs but are not present in the English
source texts.
For translation from English to German, Gojun
and Fraser (2012) reordered verbs in the English
source to positions where they normally occur in
35
German, which usually amounts to a long-distance
movement towards the end of clauses. Reordering
was implemented as rules on syntax trees and im-
proved the translation by up to 0.61 BLEU points.
In this paper, as SMT training needs a large
amount of data, we use an automatic classifier to
tag instances of English SP verbs with narrativity
labels. The labels output by this classifier are then
modeled when training the SMT system.
3 Automatic Labeling of Narrativity
3.1 Data
A training set of 458 and a test set of 118 English
SP verbs that were manually annotated with narra-
tivity labels (narrative or non-narrative) was pro-
vided by Grisot and Cartoni (2012) (see their ar-
ticle for more details about the data). The train-
ing set consists of 230 narrative and 228 non-
narrative instances, the test set has 75 narrative in-
stances and 43 non-narrative ones. The sentences
come from parallel EN/FR corpora of four dif-
ferent genres: literature, news, parliamentary de-
bates and legislation. For each instance, the En-
glish sentence with the SP verb that must be clas-
sified, as well as the previous and following sen-
tences, had been given to two human annotators,
who assigned a narrative or non-narrative label. To
avoid interference with the translation into French,
which could have provided clues about the label,
the translations were not shown to annotators1.
Annotators agreed over only 71% of the in-
stances, corresponding to a kappa value of only
0.44. As this is at the lower end of the accept-
able spectrum for discourse annotation (Carletta,
1996), one of the important questions we ask in
this paper is: what can be achieved with this qual-
ity of human annotation, in terms of an automatic
narrativity classifier (intrinsic performance) and of
its use for improving verb translation by SMT (ex-
trinsic evaluation)? It must be noted that instances
on which the two annotators had disagreed were
resolved (to either narrative or non-narrative) by
looking at the French human translation (an ac-
ceptable method given that our purpose here is
translation into French), thus increasing the qual-
ity of the annotation.
1The goal was to focus on the narrativity property, regard-
less of its translation. However, annotations were adjudicated
also by looking at the FR translation. For a different ap-
proach, considering exclusively the tense in translation, see
the discussion in Section 5.
Model Recall Prec. F1 ?
MaxEnt 0.71 0.72 0.71 +0.43
CRF 0.30 0.44 0.36 ?0.44
Table 1: Performance of MaxEnt and CRF clas-
sifiers on narrativity. We report recall, precision,
their mean (F1), and the kappa value for class
agreement.
3.2 Features for Narrativity
The manually annotated instances were used for
training and testing a Maximum Entropy classi-
fier using the Stanford Classifier package (Man-
ning and Klein, 2003). We extracted the following
features from the sentence containing the verb to
classify and the preceding sentence as well, thus
modeling a wider context than the one modeled by
phrase-based SMT systems. For each verb form,
we considered its POS tag and syntactical cate-
gory, including parents up to the first verbal phrase
(VP) parent node, as generated by Charniak and
Johnson?s constituent parser (2005). This parser
also assigns special tags to auxiliary (AUX) and
modal verbs (MD), which we include in the fea-
tures.
We further used a TimeML parser, the Tarsqi
Toolkit (Verhagen et al, 2005; Verhagen and
Pustejovsky, 2008), which automatically outputs
an XML-like structure of the sentence, with a hy-
pothesis on the temporal ordering of the events
mentioned. From this structure we extract event
markers such as PAST-OCCURRENCE and aspec-
tual information such as STATE.
Temporal ordering is often also signaled by
other markers such as adverbials (e.g., three weeks
before). We manually gathered a list of 66 such
temporal markers and assigned them, as an addi-
tional feature, a label indicating whether they sig-
nal synchrony (e.g., meanwhile, at the same time)
or asynchrony (e.g., before, after).
3.3 Results of Narrativity Labeling
With the above features, we obtained the classi-
fication performance indicated in Table 1. The
MaxEnt classifier reached 0.71 F1 score, which is
similar to the human annotator?s agreement level.
Moreover, the kappa value for inter-class agree-
ment was 0.43 between the classifier and the hu-
man annotation, a value which is also close to the
kappa value for the two human annotators. In a
sense, the classifier thus reaches the highest scores
36
that are still meaningful, i.e. those of inter-coder
agreement. As a baseline for comparison, the ma-
jority class in the test set (the ?narrative? label)
would account for 63.56% of correctly classified
instances, whereas the classifier correctly labeled
72.88% of all test instances.
For further comparison we built a CRF
model (Lafferty et al, 2001) in order to label nar-
rativity in sequence of other tags, such as POS.
The CRF uses as features the two preceding POS
tags to label the next POS tag in a sequence of
words. The same training set of 458 sentences
as used above was POS-tagged using the Stan-
ford POS tagger (Toutanova et al, 2003), with the
left3words-distsim model. We replaced
the instances of ?VBD? (the POS tag for SP verbs)
with the narrativity labels from the manual annota-
tion. The same procedure was then applied to the
118 sentences of the test set on which CRF was
evaluated.
Overall, the CRF model only labeled narrativity
correctly at an F1 score of 0.36, while kappa had
a negative value signaling a weak inverse correla-
tion. Therefore, it appears that the temporal and
semantic features used for the MaxEnt classifier
are useful and account for the much higher per-
formance of MaxEnt, which is used in the SMT
experiments described below.
We further evaluate the MaxEnt classifier by
providing in Table 2 the confusion matrix of the
automatically obtained narrativity labels over the
test set. Labeling non-narrative uses is slightly
more prone to errors (32.6% error rate) than nar-
rative ones (24% errors), likely due to the larger
number of narratives vs. non-narratives in the
training and the test data.
System
Reference Narr. Non-narr. Total
Narrative 57 18 75
Non-narr. 14 29 43
Total 71 47 118
Table 2: Confusion matrix for the labels output
by the MaxEnt classifier (System) versus the gold
standard labels (Reference).
4 SMT with Narrativity Labels
4.1 Method
Two methods to use labels conveying to SMT in-
formation about narrativity were explored (though
more exist). First, as in our initial studies ap-
plied to discourse connectives, the narrativity la-
bels were simply concatenated with the SP verb
form in EN (Meyer and Popescu-Belis, 2012) ?
see Example 2 in Figure 3. Second, we used
factored translation models (Koehn and Hoang,
2007), which allow for any linguistic annotation
to be considered as additional weighted feature
vectors, as in our later studies with connectives
(Meyer et al, 2012). These factors are log-linearly
combined with the basic features of phrase-based
SMT models (phrase translation, lexical and lan-
guage model probabilities).
To assess the performance gain of narrativity-
augmented systems, we built three different SMT
systems, with the following names and configura-
tions:
? BASELINE: plain text, no verbal labels.
? TAGGED: plain text, all SP verb forms con-
catenated with a narrativity label.
? FACTORED: all SP verbs have narrativity
labels as source-side translation factors (all
other words labeled ?null?).
1. BASELINE SMT: on wednesday the c?ssd de-
clared the approval of next year?s budget to be a
success. the people?s party was also satisfied.
2. TAGGED SMT: on wednesday the c?ssd
declared-Narrative the approval of next year?s
budget to be a success. the people?s party was-
Non-narrative also satisfied.
3. FACTORED SMT: on wednesday the c?ssd
declared|Narrative the approval of next year?s
budget to be a success. the people?s party
was|Non-narrative also satisfied.
Figure 3: Example input sentence from ?newstest
2010? data for three translation models: (1) plain
text; (2) concatenated narrativity labels; (3) narra-
tivity as translation factors (the ?|null? factors on
other words were omitted for readability).
Figure 3 shows an example input sentence for
these configurations. For the FACTORED SMT
model, both the EN source word and the factor
37
information are used to generate the FR surface
target word forms. The tagged or factored annota-
tions are respectively used for the training, tuning
and test data as well.
For labeling the SMT data, no manual annota-
tion is used. In a first step, the actual EN SP verbs
to be labeled are identified using the Stanford POS
tagger, which assigns a ?VBD? tag to each SP verb.
These tags are replaced, after feature extraction
and execution of the MaxEnt classifier, by the nar-
rativity labels output by the latter. Of course, the
POS tagger and (especially) our narrativity clas-
sifier may generate erroneous labels which in the
end lead to translation errors. The challenge is
thus to test the improvement of SMT with respect
to the baseline, in spite of the noisy training and
test data.
4.2 Data
In all experiments, we made use of parallel En-
glish/French training, tuning and testing data from
the translation task of the Workshop on Machine
Translation (www.statmt.org/wmt12/).
? For training, we used Europarl v6 (Koehn,
2005), original EN2 to translated FR
(321,577 sentences), with 66,143 instances
of SP verbs labeled automatically: 30,452
are narrative and 35,691 are non-narrative.
? For tuning, we used the ?newstest 2011? tun-
ing set (3,003 sentences), with 1,401 auto-
matically labeled SP verbs, of which 807 are
narrative and 594 non-narrative.
? For testing, we used the ?newstest 2010? data
(2,489 sentences), with 1,156 automatically
labeled SP verbs (621 narrative and 535 non-
narrative).
We built a 5-gram language model with SRILM
(Stolcke et al, 2011) over the entire FR part of Eu-
roparl. Tuning was performed by Minimum Error
Rate Training (MERT) (Och, 2003). All transla-
tion models were phrase-based using either plain
text (possibly with concatenated labels) or fac-
tored training as implemented in the Moses SMT
toolkit (Koehn et al, 2007).
2We only considered texts that were originally authored
in English, not translated into it from French or a third-party
language, to ensure only proper tenses uses are observed. The
relevance of this constraint is discussed for connectives by
Cartoni et al (2011).
4.3 Results: Automatic Evaluation
In order to obtain reliable automatic evaluation
scores, we executed three runs of MERT tuning for
each type of translation model. With MERT being
a randomized, non-deterministic optimization pro-
cess, each run leads to different feature weights
and, as a consequence, to different BLEU scores
when translating unseen data.
Table 3 shows the average BLEU and TER
scores on the ?newstest 2010? data for the three
systems. The scores are averages over the three
tuning runs, with resampling of the test set,
both provided in the evaluation tool by Clark
et al (2011) (www.github.com/jhclark/
multeval). BLEU is computed using jBLEU
V0.1.1 (an exact reimplementation of NIST?s
?mteval-v13.pl? script without tokenization). The
Translation Error Rate (TER) is computed with
version 0.8.0 of the software (Snover et al, 2006).
A t-test was used to compute p values that indicate
the significance of differences in scores.
Translation model BLEU TER
BASELINE 21.4 61.9
TAGGED 21.3 61.8
FACTORED 21.6* 61.7*
Table 3: Average values of BLEU (the higher the
better) and TER (the lower the better) over three
tuning runs for each model on ?newstest 2010?.
The starred values are significantly better (p <
0.05) than the baseline.
In terms of overall BLEU and TER scores, the
FACTORED model improves performance over the
BASELINE by +0.2 BLEU and -0.2 TER (as lower
is better), and these differences are statistically
significant at the 95% level. On the contrary,
the concatenated-label model (noted TAGGED)
slightly decreases the global translation perfor-
mance compared to the BASELINE. A similar
behavior was observed when using labeled con-
nectives in combination with SMT (Meyer et al,
2012).
The lower scores of the TAGGED model may
be due to the scarcity of data (by a factor of 0.5)
when verb word-forms are altered by concatenat-
ing them with the narrativity labels. The small
improvement by the FACTORED model of over-
all scores (such as BLEU) is also related to the
scarcity of SP verbs: although their translation is
38
improved, as we will now show, the translation of
all other words is not changed by our method, so
only a small fraction of the words in the test data
are changed.
4.4 Results: Human Evaluation
To assess the improvement specifically due to the
narrativity labels, we manually evaluated the FR
translations by the FACTORED model for the 207
first SP verbs in the test set against the transla-
tions from the BASELINE model. As the TAGGED
model did not result in good scores, we did not fur-
ther consider it for evaluation. Manual scoring was
performed along the following criteria for each oc-
currence of an SP verb, by bilingual judges look-
ing both at the source sentence and its reference
translation.
? Is the narrativity label correct? (?correct? or
?incorrect?) ? this is a direct evaluation of the
narrativity classifier from Section 3
? Is the verb tense of the FACTORED model
more accurate than the BASELINE one?
(noted ?+? if improved, ?=? if similar, ??? if
degraded)
? Is the lexical choice of the FACTORED model
more accurate than the BASELINE one, re-
gardless of the tense? (again noted ?+? or ?=?
or ???)
? Is the BASELINE translation of the verb
phrase globally correct? (?correct? or ?incor-
rect?)
? Is the FACTORED translation of the verb
phrase globally correct? (?correct? or ?incor-
rect?)
Tables 4 and 5 summarize the counts and per-
centages of improvements and/or degradations of
translation quality with the systems FACTORED
and BASELINE. The correctness of the labels, as
evaluated by the human judges on SMT test data,
is similar to the values given in Section 3 when
evaluated against the test sentences of the narra-
tivity classifier. As shown in Table 4, the narrativ-
ity information clearly helps the FACTORED sys-
tem to generate more accurate French verb tenses
in almost 10% of the cases, and also helps to find
more accurate vocabulary for verbs in 3.4% of the
cases. Overall, as shown in Table 5, the FAC-
TORED model yields more correct translations of
the verb phrases than the BASELINE in 9% of the
cases ? a small but non-negligible improvement.
Criterion Rating N. % ?
Labeling correct 147 71.0
incorrect 60 29.0
Verb + 35 17.0
tense = 157 75.8 +9.7
? 15 7.2
Lexical + 19 9.2
choice = 176 85.0 +3.4
? 12 5.8
Table 4: Human evaluation of verb translations
into French, comparing the FACTORED model
against the BASELINE. The ? values show the
clear improvement of the narrativity-aware fac-
tored translation model.
System Rating Number %
BASELINE correct 94 45.5
incorrect 113 54.5
FACTORED correct 113 54.5
incorrect 94 45.5
Table 5: Human evaluation of the global cor-
rectness of 207 translations of EN SP verbs into
French. The FACTORED model yields 9% more
correct translations than the BASELINE one.
An example from the test data shown in Fig-
ure 4 illustrates the improved verb translation. The
BASELINE system translates the SP verb looked
incorrectly into the verb conside?rer (consider), in
wrong number and its past participle only (con-
side?re?s, plural). The FACTORED model generates
the correct tense and number (IMP, semblait, sin-
gular) and the better verb sembler (look, appear).
This example is scored as follows: the labeling is
correct (?yes?), the tense was improved (?+?), the
lexical choice was improved too (?+?), the BASE-
LINE was incorrect while the FACTORED model
was correct.
5 Discussion and Future Work
When looking in detail through the translations
that were degraded by the FACTORED model,
some were due to the POS tagging used to find
the EN SP verbs to label. For verb phrases made
of an auxiliary verb in SP and a past participle
(e.g. was born), the POS tagger outputs was/VBD
born/VBN. As a consequence, our classifier only
considers was, as non-narrative, although was
39
EN: tawa hallae looked|Non-narrative like
many other carnivorous dinosaurs.
FR BASELINE: tawa hallae *conside?re?s
comme de nombreuses autres carnivores di-
nosaures.
FR FACTORED: tawa hallae semblait comme
de nombreux autres carnivores dinosaures.
Figure 4: Example comparison of a baseline and
improved factored translation. The ?|null? factors
in EN were omitted for readability. See the text
for a discussion.
born as a whole is a narrative event. This can
then result in wrong FR tense translations. For
instance, the fragment nelson mandela was|Non-
narrative born on . . . is translated as: nelson man-
dela *e?tait ne? en . . . , which in FR is pluperfect
tense instead of the correct Passe? Compose? est ne?
as in the reference translation. A method to con-
catenate such verb phrases to avoid such errors is
under work.
A further reason for the small improvements in
translation quality might be that factored transla-
tion models still operate on rather local context,
even when the narrativity information is present.
To widen the context captured by the translation
model, labeling entire verbal phrase nodes in hi-
erarchical or tree-based syntactical models will be
considered in the future. Moreover, it has been
shown that it is difficult to choose the optimal pa-
rameters for a factored translation model (Tam-
chyna and Bojar, 2013).
In an alternative approach currently under work,
a more direct way to label verb tense is imple-
mented, where a classifier can make use of the
same features as those extracted here (in Sec-
tion 3.2), but its classes are those that directly
indicate which target verb tense should be out-
put by the SMT. Thus, not only SP verbs can
be considered and no intermediate category such
as narrativity (that is more difficult to learn) is
needed. The classifier will predict which FR tense
should be used depending on the context of the
EN verbs, for which the FR tense label can be
annotated as above, within a factored translation
model. Through word alignment and POS tag-
ging, this method has the additional advantage of
providing much more training data, extracted from
word alignment of the verb phrases, and can be
applied to all tenses, not only SP. Moreover, the
approach is likely to learn which verbs are prefer-
ably translated with which tense: for instance, the
verb started is much more likely to become a com-
mence? (PC) in FR than to commenc?ait (IMP), due
to its meaning of a punctual event in time, rather
than a continuous or repetitive one.
6 Conclusion
The paper presented a method to automatically la-
bel English verbs in Simple Past tense with a bi-
nary pragmatic feature, narrativity, which helps
to distinguish temporally ordered events that hap-
pened in the past (?narrative?) from past states of
affairs (?non-narrative?). A small amount of man-
ually annotated data, combined with the extraction
of temporal semantic features, allowed us to train a
classifier that reached 70% correctly classified in-
stances. The classifier was used to automatically
label the English SP verbs in a large parallel train-
ing corpus for SMT systems. When implement-
ing the labels in a factored SMT model, translation
into French of the English SP verbs was improved
by about 10%, accompanied by a statistically sig-
nificant gain of +0.2 BLEU points for the overall
quality score. In the future, we will improve the
processing of verb phrases, and study a classifier
with labels that are directly based on the target lan-
guage tenses.
Acknowledgments
We are grateful for the funding of this work to the
Swiss National Science Foundation (SNSF) under
the COMTIS Sinergia Project, n. CRSI22 127510
(see www.idiap.ch/comtis/). We would
also like to thank the anonymous reviewers for
their helpful suggestions.
References
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22:249?254.
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How Comparable
are Parallel Corpora? Measuring the Distribution of
General Vocabulary and Connectives. In Proceed-
ings of 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), pages 78?86, Portland, OR.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best Parsing and MaxEnt Discriminative
40
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 173?180, Ann Arbor, MI.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation: Controlling for Optimizer
Instability. In Proceedings of ACL-HLT 2011 (46th
Annual Meeting of the ACL: Human Language Tech-
nologies), Portland, OR.
Francis Corblin and Henrie?tte de Swart. 2004. Hand-
book of French Semantics. CSLI Publications, Stan-
ford, CA.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the Placement of German Verbs in English-to-
German SMT. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 726?735,
Avignon, France.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-Gram-Based Tense
Models for Statistical Machine Translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing (EMNLP)
and Computational Natural Language Learning
(CoNLL), pages 276?285, Jeju Island, Korea.
Cristina Grisot and Bruno Cartoni. 2012. Une de-
scription bilingue des temps verbaux: e?tude con-
trastive en corpus. Nouveaux cahiers de linguistique
franc?aise, 30:101?117.
Philipp Koehn and Hieu Hoang. 2007. Factored
Translation Models. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) and Computational
Natural Language Learning (CoNLL), pages 868?
876, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbs. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of 45th Annual Meeting of the
Association for Computational Linguistics (ACL),
Demonstration Session, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. The Journal of Machine Learning Re-
search, 8:693?723.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, MaxEnt Models, and Conditional Estimation
without Magic. In Tutorial at HLT-NAACL and 41st
ACL conferences, Edmonton, Canada and Sapporo,
Japan.
Robert Martin. 1971. Temps et aspect: essai sur
l?emploi des temps narratifs en moyen franc?ais.
Klincksieck, Paris, France.
Thomas Meyer and Andrei Popescu-Belis. 2012. Us-
ing Sense-labeled Discourse Connectives for Statis-
tical Machine Translation. In Proceedings of the
EACL 2012 Joint Workshop on Exploiting Synergies
between IR and MT, and Hybrid Approaches to MT
(ESIRMT-HyTra), pages 129?138, Avignon, FR.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine Translation
of Labeled Discourse Connectives. In Proceedings
of the Tenth Biennial Conference of the Association
for Machine Translation in the Americas (AMTA),
San Diego, CA.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1986. A Comprehensive Gram-
mar of the English Language. Pearson Longman,
Harlow, UK.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the Tenth Biennial Con-
ference of the Association for Machine Translation
in the Americas (AMTA), Cambridge, MA.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and
Outlook. In Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop,
Waikoloa, Hawaii.
Ales? Tamchyna and Ondr?ej Bojar. 2013. No Free
Lunch in Factored Phrase-Based Machine Transla-
tion. In Proceedings of the 14th International Con-
ference on Computational Linguistics and Intelli-
gent Text Processing (CICLING), Samos, Greece.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Tech-
nology Conference and the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL), pages 252?259, Edmonton, CA.
Marc Verhagen and James Pustejovsky. 2008. Tempo-
ral Processing with the TARSQI Toolkit. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (COLING), Companion
volume: Demonstrations, pages 189?192, Manch-
ester, UK.
41
Marc Verhagen, Inderjeet Mani, Roser Sauri, Jes-
sica Littman, Robert Knippen, Seok Bae Jang,
Anna Rumshisky, John Phillips, and James Puste-
jovsky. 2005. Automating Temporal Annotation
with TARSQI. In Proceedings of the 43th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Demo Session, pages 81?84, Ann
Arbor, USA.
Yang Ye, Karl-Michael Schneider, and Steven Abney.
2007. Aspect Marker Generation for English-to-
Chinese Machine Translation. In Proceedings of MT
Summit XI, pages 521?527, Copenhagen, Danmark.
42
