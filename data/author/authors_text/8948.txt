Semi-Supervised Training of a Kernel PCA-Based Model
for Word Sense Disambiguation
Weifeng SU Marine CARPUAT Dekai WU1
weifeng@cs.ust.hk marine@cs.ust.hk dekai@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology, Clear Water Bay, Hong Kong
Abstract
In this paper, we introduce a new semi-supervised learning
model for word sense disambiguation based on Kernel Prin-
cipal Component Analysis (KPCA), with experiments showing
that it can further improve accuracy over supervised KPCA
models that have achieved WSD accuracy superior to the best
published individual models. Although empirical results with
supervised KPCA models demonstrate significantly better ac-
curacy compared to the state-of-the-art achieved by either na??ve
Bayes or maximum entropy models on Senseval-2 data, we
identify specific sparse data conditions under which supervised
KPCA models deteriorate to essentially a most-frequent-sense
predictor. We discuss the potential of KPCA for leveraging
unannotated data for partially-unsupervised training to address
these issues, leading to a composite model that combines both
the supervised and semi-supervised models.
1 Introduction
Wu et al (2004) propose an efficient and accurate new
supervised learning model for word sense disambigua-
tion (WSD), that exploits a nonlinear Kernel Principal
Component Analysis (KPCA) technique to make pre-
dictions implicitly based on generalizations over feature
combinations. Experiments performed on the Senseval-
2 English lexical sample data show that KPCA-based
word sense disambiguation method is capable of outper-
forming other widely used WSD models including na??ve
Bayes, maximum entropy, and SVM models.
Despite the excellent performance of the supervised
KPCA-based WSD model on average, though, our fur-
ther error analysis investigations have suggested certain
limitations. In particular, the supervised KPCA-based
model often appears to perform poorly when it encoun-
ters target words whose contexts are highly dissimilar
to those of any previously seen instances in the train-
ing set. Empirically, the supervised KPCA-based model
nearly always disambiguates target words of this kind
to the most frequent sense. As a result, for this partic-
ular subset of test instances, the precision achieved by
the KPCA-based model is essentially no higher than the
precision achieved by the most-frequent-sense baseline
model (which simply always selects the most frequent
sense for the target word). The work reported in this pa-
per stems from a hypothesis that the most-frequent-sense
1The author would like to thank the Hong Kong Research Grants
Council (RGC) for supporting this research in part through grants
RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.
strategy can be bettered for this category of errors.
This is a case of data sparseness, so the observation
should not be very surprising. Such behavior is to be ex-
pected from classifiers in general, and not just the KPCA-
based model. Put another way, even though KPCA is
able to generalize over combinations of dependent fea-
tures, there must be a sufficient number of training in-
stances from which to generalize.
The nature of KPCA, however, suggests a strategy that
is not applicable to many of the other conventional WSD
models. We propose a model in this paper that takes ad-
vantage of unsupervised training using large quantities of
unannotated corpora, to help compensate for sparse data.
Note that although we are using the WSD task to ex-
plain the model, in fact the proposed model is not lim-
ited to WSD applications. We have hypothesized that
the KPCA-based method is likely to be widely applica-
ble to other NLP tasks; since data sparseness is a com-
mon problem in many NLP tasks, a weakly-supervised
approach allowing the KPCA-based method to compen-
sate for data sparseness is highly desirable. The general
technique we describe here is applicable to any similar
classification task where insufficient labeled training data
is available.
The paper is organized as follows. After a brief look
at related work, we review the baseline supervised WSD
model, which is based on Kernel PCA. We then discuss
how data sparseness affects the model, and propose a
new semi-supervised model that takes advantage of un-
labeled data, along with a composite model that com-
bines both the supervised and semi-supervised models.
Finally, details of the experimental setup and compara-
tive results are given.
2 Related work
The long history of WSD research includes numerous
statistically trained methods; space only permits us to
summarize a few key points here. Na??ve Bayes models
(e.g., Mooney (1996), Chodorow et al (1999), Pedersen
(2001), Yarowsky and Florian (2002)) as well as max-
imum entropy models (e.g., Dang and Palmer (2002),
Klein and Manning (2002)) in particular have shown a
large degree of success for WSD, and have established
challenging state-of-the-art benchmarks. The Senseval
series of evaluations facilitates comparing the strengths
and weaknesses of various WSD models on common
data sets, with Senseval-1 (Kilgarriff and Rosenzweig,
1999), Senseval-2 (Kilgarriff, 2001), and Senseval-3 held
in 1998, 2001, and 2004 respectively.
3 Supervised KPCA baseline model
Our baseline WSD model is a supervised learning model
that also makes use of Kernel Principal Component
Analysis (KPCA), proposed by (Scho?lkopf et al, 1998)
as a generalization of PCA. KPCA has been successfully
applied in many areas such as de-noising of images of
hand-written digits (Mika et al, 1999) and modeling the
distribution of non-linear data sets in the context of shape
modelling for real objects (Active Shape Models) (Twin-
ing and Taylor, 2001). In this section, we first review the
theory of KPCA and explanation of why it is suited for
WSD applications.
3.1 Kernel Principal Component Analysis
The Kernel Principal Component Analysis technique, or
KPCA, is a method of nonlinear principal component ex-
traction. A nonlinear function maps the n-dimensional
input vectors from their original space Rn to a high-
dimensional feature space F where linear PCA is per-
formed. In real applications, the nonlinear function is
usually not explicitly provided. Instead we use a kernel
function to implicitly define the nonlinear mapping; in
this respect KPCA is similar to Support Vector Machines
(Scho?lkopf et al, 1998).
Compared with other common analysis techniques,
KPCA has several advantages:
? As with other kernel methods it inherently takes
combinations of predictive features into account
when optimizing dimensionality reduction. For nat-
ural language problems in general, of course, it is
widely recognized that significant accuracy gains
can often be achieved by generalizing over relevant
feature combinations (e.g., Kudo and Matsumoto
(2003)).
? We can select suitable kernel function according to
the task we are dealing with and the knowledge we
have about the task.
? Another advantage of KPCA is that it is good at
dealing with input data with very high dimension-
ality, a condition where kernel methods excel.
Nonlinear principal components (Diamantaras and
Kung, 1996) may be defined as follows. Suppose we
are given a training set of M pairs (xt, ct) where the
observed vectors xt ? Rn in an n-dimensional input
space X represent the context of the target word being
disambiguated, and the correct class ct represents the
sense of the word, for t = 1, ..,M . Suppose ? is a
nonlinear mapping from the input space Rn to the fea-
ture space F . Without loss of generality we assume the
M vectors are centered vectors in the feature space, i.e.,
?M
t=1 ? (xt) = 0; uncentered vectors can easily be con-
verted to centered vectors (Scho?lkopf et al, 1998). We
wish to diagonalize the covariance matrix in F :
C =
1
M
M?
j=1
? (xj) ?
T (xj) (1)
To do this requires solving the equation ?v = Cv for
eigenvalues ? ? 0 and eigenvectors v ? F . Because
Cv =
1
M
M?
j=1
(?(xj) ? v)? (xj) (2)
we can derive the following two useful results. First,
? (?(xt) ? v) = ? (xt) ? Cv (3)
for t = 1, ..,M . Second, there exist ?i for i = 1, ...,M
such that
v =
M?
i=1
?i? (xi) (4)
Combining (1), (3), and (4), we obtain
M?
M?
i=1
?i (?(xt) ? ?(xi ))
=
M?
i=1
?i(? (xt) ?
M?
j=1
? (xj)) (?(xj) ? ?(xi ))
for t = 1, ..,M . Let K? be the M ?M matrix such that
K?ij = ?(xi) ? ? (xj) (5)
and let ??1 ? ??2 ? . . . ? ??M denote the eigenvalues
of K? and ??1 ,..., ??M denote the corresponding complete
set of normalized eigenvectors, such that ??t(??t ? ??t) = 1
when ??t > 0. Then the lth nonlinear principal compo-
nent of any test vector xt is defined as
ylt =
M?
i=1
??li (?(xi) ? ?(xt )) (6)
where ??li is the lth element of ??l .
3.2 Why is KPCA suited to WSD?
The potential of nonlinear principal components for
WSD can be illustrated by a simplified disambiguation
example for the ambiguous target word ?art?, with the
two senses shown in Table 1. Assume a training cor-
pus of the eight sentences as shown in Table 2, adapted
from Senseval-2 English lexical sample corpus. For each
sentence, we show the feature set associated with that
occurrence of ?art? and the correct sense class. These
eight occurrences of ?art? can be transformed to a binary
vector representation containing one dimension for each
feature, as shown in Table 3.
Extracting nonlinear principal components for the vec-
tors in this simple corpus results in nonlinear generaliza-
tion, reflecting an implicit consideration of combinations
of features. Table 2 shows the first three dimensions of
the principal component vectors obtained by transform-
ing each of the eight training vectors xt into (a) principal
component vectors zt using the linear transform obtained
via PCA, and (b) nonlinear principal component vectors
yt using the nonlinear transform obtained via KPCA as
described below.
Table 1: A tiny corpus for the target word ?art?, adapted from the Senseval-2 English lexical sample corpus (Kilgarriff
2001), together with a tiny example set of features. The training and testing examples can be represented as a set of
binary vectors: each row shows the correct class c for an observed vector x of five dimensions.
TRAINING design/N media/N the/DT entertainment/N world/N Class
x1 He studies art in London. 1
x2 Punch?s weekly guide to the
world of the arts, entertain-
ment, media and more.
1 1 1 1
x3 All such studies have influ-
enced every form of art, de-
sign, and entertainment in
some way.
1 1 1
x4 Among the technical arts cul-
tivated in some continental
schools that began to affect
England soon after the Nor-
man Conquest were those
of measurement and calcula-
tion.
1 2
x5 The Art of Love. 1 2
x6 Indeed, the art of doctor-
ing does contribute to bet-
ter health results and discour-
ages unwarranted malprac-
tice litigation.
1 2
x7 Countless books and classes
teach the art of asserting
oneself.
1 2
x8 Pop art is an example. 1
TESTING
x9 In the world of de-
sign arts particularly, this led
to appointments made for
political rather than academic
reasons.
1 1 1 1
Table 2: The original observed training vectors (showing only the first three dimensions) and their first three principal
components as transformed via PCA and KPCA.
Observed vectors PCA-transformed vectors KPCA-transformed vectors Class
t (x1t , x
2
t , x
3
t ) (z
1
t , z
2
t , z
3
t ) (y
1
t , y
2
t , y
3
t ) ct
1 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
2 (0, 1, 1) (1.675, -1.132, 0.1049) (1.149, 0.02934, 0.322) 1
3 (1, 0, 0) (-0.367, 1.697, -0.2391) (0.8209, 0.7722, -0.2015) 1
4 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
5 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
6 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
7 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
8 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
Similarly, for the test vector x9, Table 3 shows the
first three dimensions of the principal component vec-
tors obtained by transforming it into (a) a principal com-
ponent vector z9 using the linear PCA transform ob-
tained from training, and (b) a nonlinear principal com-
ponent vector y9 using the nonlinear KPCA transform
obtained obtained from training. The vector similarities
in the KPCA-transformed space can be quite different
from those in the PCA-transformed space. This causes
the KPCA-based model to be able to make the correct
Table 3: Testing vector (showing only the first three dimensions) and its first three principal components as transformed
via the trained PCA and KPCA parameters. The PCA-based and KPCA-based sense class predictions disagree.
Observed
vectors
PCA-transformed vectors KPCA-transformed vectors Predicted
Class
Correct
Class
t (x1t , x
2
t , x
3
t ) (z
1
t , z
2
t , z
3
t ) (y
1
t , y
2
t , y
3
t ) c?t ct
9 (1, 0, 1) (-0.3671, -0.5658, -0.2392) 2 1
9 (1, 0, 1) (4e-06, 8e-07, 1.111e-18) 1 1
class prediction, whereas the PCA-based model makes
the wrong class prediction.
What permits KPCA to apply stronger generalization
biases is its implicit consideration of combinations of
feature information in the data distribution from the high-
dimensional training vectors. In this simplified illustra-
tive example, there are just five input dimensions; the
effect is stronger in more realistic high dimensional vec-
tor spaces. Since the KPCA transform is computed from
unsupervised training vector data, and extracts general-
izations that are subsequently utilized during supervised
classification, it is possible to combine large amounts of
unsupervised data with reasonable smaller amounts of
supervised data.
Interpreting this example graphically can be illuminat-
ing even though the interpretation in three dimensions is
severely limiting. Figure 1(a) depicts the eight original
observed training vectors xt in the first three of the five
dimensions; note that among these eight vectors, there
happen to be only four unique points when restricting
our view to these three dimensions. Ordinary linear PCA
can be straightforwardly seen as projecting the original
points onto the principal axis, as can be seen for the case
of the first principal axis in Figure 1(b). Note that in this
space, the sense 2 instances are surrounded by sense 1
instances. We can traverse each of the projections onto
the principal axis in linear order, simply by visiting each
of the first principal components z1t along the principle
axis in order of their values, i.e., such that
z11 ? z
1
8 ? z
1
4 ? z
1
5 ? z
1
6 ? z
1
7 ? z
1
2 ? z
1
3 ? z
1
9
It is significantly more difficult to visualize the non-
linear principal components case, however. Note that
in general, there may not exist any principal axis in X ,
since an inverse mapping from F may not exist. If we
attempt to follow the same procedure to traverse each of
the projections onto the first principal axis as in the case
of linear PCA, by considering each of the first principal
components y1t in order of their value, i.e., such that
y14 ? y
1
5 ? y
1
6 ? y
1
7 ? y
1
9 ? y
1
1 ? y
1
8 ? y
1
3 ? y
1
2
then we must arbitrarily select a ?quasi-projection? di-
rection for each y1t since there is no actual principal axis
toward which to project. This results in a ?quasi-axis?
roughly as shown in Figure 1(c) which, though not pre-
cisely accurate, provides some idea as to how the non-
linear generalization capability allows the data points to
be grouped by principal components reflecting nonlin-
ear patterns in the data distribution, in ways that linear
Figure 1: Original vectors, PCA projections, and KPCA
?quasi-projections? (see text).
PCA cannot do. Note that in this space, the sense 1 in-
stances are already better separated from sense 2 data
points. Moreover, unlike linear PCA, there may be up
to M of the ?quasi-axes?, which may number far more
than five. Such effects can become pronounced in the
high dimensional spaces are actually used for real word
sense disambiguation tasks.
3.3 Algorithm
To extract nonlinear principal components efficiently,
note that in both Equations (5) and (6) the explicit form
of ? (xi) is required only in the form of (? (xi) ?? (xj)),
i.e., the dot product of vectors in F . This means that we
can calculate the nonlinear principal components by sub-
stituting a kernel function k(xi, xj) for (?(xi) ??(xj ))
in Equations (5) and (6) without knowing the mapping ?
explicitly; instead, the mapping ? is implicitly defined
by the kernel function. It is always possible to construct
a mapping into a space where k acts as a dot product
so long as k is a continuous kernel of a positive integral
operator (Scho?lkopf et al, 1998).
Thus we train the KPCA model using the following
algorithm:
1. Compute an M ?M matrix K? such that
K?ij = k(xi, xj) (7)
2. Compute the eigenvalues and eigenvectors of matrix
K? and normalize the eigenvectors. Let ??1 ? ??2 ?
. . . ? ??M denote the eigenvalues and ??1,..., ??M de-
note the corresponding complete set of normalized
eigenvectors.
To obtain the sense predictions for test instances, we
need only transform the corresponding vectors using the
trained KPCA model and classify the resultant vectors
using nearest neighbors. For a given test instance vector
x, its lth nonlinear principal component is
ylt =
M?
i=1
??lik(xi, xt) (8)
where ??li is the ith element of ??l.
For our disambiguation experiments we employ a
polynomial kernel function of the form k(xi, xj) =
(xi ? xj)
d
, although other kernel functions such as gaus-
sians could be used as well. Note that the degenerate
case of d = 1 yields the dot product kernel k(xi, xj) =
(xi?xj) which covers linear PCA as a special case, which
may explain why KPCA always outperforms PCA.
4 Semi-supervised KPCA model
4.1 Utilizing unlabeled data
In WSD, as with many NLP tasks, features are often in-
terdependent. For example, the features that represent
words that frequently co-occur are typically highly in-
terdependent. Similarly, the features that represent syn-
onyms tend to be highly interdependent.
It is a strength of the KPCA-based model that it gen-
eralizes over combinations of interdependent features.
This enables the model to predict the correct sense even
when the context surrounding a target word has not been
previously seen, by exploiting the similarity to feature
combinations that have been seen.
However, in practice the labeled training corpus for
WSD is typically relatively small, and does not yield
enough training instances to reliably extract dependen-
cies between features. For example, in the Senseval-
2 English lexical sample data, for each target word
there are only about 120 training instances on average,
whereas on the other hand we typically have thousands
of features for each target word.
The KPCA model can fail when it encounters a target
word whose context contains a combination of features
that may in fact be interdependent, but are not similar to
any combinations that occurred in the limited amounts
of labeled training data. Because of the sparse data, the
KPCA model wrongly considers the context of the tar-
get word to be dissimilar to those previously seen?even
though the contexts may in truth be similar. In the ab-
sence of any contexts it believes to be similar, the model
therefore tends simply to predict the most frequent sense.
The potential solution we propose to this problem is
to add much larger quantities of unannotated data, with
which the KPCA model can first be trained in unsu-
pervised fashion. This provides a significantly broader
dataset from which to generalize over combinations of
dependent features. One of the advantages of our WSD
model is that during KPCA training, the sense class is not
taken into consideration. Thus we can take advantage of
the vast amounts of cheap unannotated corpora, in addi-
tion to the relatively small amounts of labeled training
data. Adding a large quantity of unlabeled data makes
it much likelier that dependent features can be identified
during KPCA training.
4.2 Algorithm
The primary difference of the semi-supervised KPCA
model from the supervised KPCA baseline model de-
scribed above lies in the eigenvector calculation step. As
we mentioned earlier, in KPCA-based model, we need
to calculate the eigenvectors of matrix K, where Kij =
(?(xi) ? ?(xj )). In the supervised KPCA model, train-
ing vectors such as xi and xj are only drawn from the
labeled training corpus. In the semi-supervised KPCA
model, training vectors are drawn from both the labeled
training corpus and a much larger unlabeled training cor-
pus. As a consequence, the maximum number of eigen-
vectors in the supervised KPCA model is the minimum
of the number of features and the number of vectors from
the labeled training corpus, while the maximum number
of eigenvectors for the semi-supervised KPCA model is
the minimum of the number of features and total num-
ber of vectors from the combined labeled and unlabeled
training corpora.
However, one would not want to apply the semi-
supervised KPCA model indiscriminately. While it can
be expected to be valuable in cases where the data was
too sparse for reliable training of the supervised KPCA
model, at the same time it is important to note that the un-
labeled data is typically drawn from quite different dis-
tributions than the labeled data, and may therefore be ex-
pected to introduce a new source of noise.
We therefore define a composite semi-supervised
KPCA model based on the following assumption. If we
are sufficiently confident about the prediction made by
the supervised KPCA model as to the predicted sense
for the target word, we need not resort to the semi-
supervised KPCA method. On the other hand, if we
are not confident about the supervised KPCA model?s
prediction, we then turn to the semi-supervised KPCA
model and take its classification as the predicted sense.
Specifically, the composite model uses the following
algorithm to combine the sense predictions of the super-
vised and semi-supervised KPCA models in order to dis-
ambiguate the target word in a given test instance x:
1. let s1 be the predicted sense of x using the super-
vised KPCA baseline model
2. let c be the similarity between x and its most similar
training instance
3. if c ? t or s1 6= smf (where t is a preset thresh-
old, and smf is the most frequent sense of the target
word):
? then predict the sense of the target word of x
to be s1
? else predict the sense of the target word of
x to be s2, the sense predicted by the semi-
supervised KPCA model
The two conditions checked in step 3 serve to fil-
ter those instances where the supervised KPCA baseline
model is confident enough to skip the semi-supervised
KPCA model. In particular:
? The threshold t specifies a minimum level of the
supervised KPCA baseline model?s confidence, in
terms of similarity. If c ? t, then there were training
instances that were of sufficient similarity to the test
instance so that the model can be confident that a
correct disambiguation can be predicted based only
on those similar training instances. In this case the
semi-supervised KPCA model is not needed.
? If s1 is not the most frequent sense smf of the
target word, then there is strong evidence that the
test instance should be disambiguated as s1 because
this is overriding an otherwise strong tendency to
disambiguate the target word to the most frequent
sense. Again, in this case the semi-supervised
KPCA model should be avoided.
The threshold t is defined to rise as the relative fre-
quency of the most frequent sense falls. Specifically,
t = 1? P (smf) + c where P (smf) is the probability of
most frequent sense in the training corpus and c is a small
constant. This reflects the assumption that the higher the
probability of the most frequent sense, the less likely that
a test instance disambiguated as the most frequent sense
is wrong.
5 Experimental setup
We evaluated the composite semi-supervised KPCA
model using data from the Senseval-2 English lexical
sample task (Kilgarriff, 2001)(Palmer et al, 2001). We
chose to focus on verbs, which have proven particularly
difficult to disambiguate. Our task consists in disam-
biguating several instances of 16 different target verbs.
Table 4: The semi-supervised KPCA model outperforms
supervised na??ve Bayes and maximum entropy models,
as well as the most-frequent-sense and supervised KPCA
baseline models.
Fine-grained
accuracy
Coarse-
grained
accuracy
Most frequent
sense
41.4% 51.7%
Na??ve Bayes 55.4% 64.2%
Maximum entropy 54.9% 64.1%
Supervised KPCA 57.0% 66.6%
Composite semi-
supervised KPCA
57.4% 67.2%
For each target word, training and test instances manu-
ally tagged with WordNet senses are available. There are
an average of about 10.5 senses per target word, rang-
ing from 4 to 19. All our models are evaluated on the
Senseval-2 test data, but trained on different training sets.
We report accuracy, the number of correct predictions
over the total number of test instances, at two different
levels of sense granularity.
The supervised models are trained on the Senseval-
2 training data. On average, 137 annotated training in-
stances per target word are available.
In addition to the small annotated Senseval-2 data
set, the semi-supervised KPCA model can make use of
large amounts of unannotated data. Since most of the
Senseval-2 verb data comes from the Wall Street Journal,
we choose to augment the Senseval-2 data by collecting
additional training instances from the Wall Street Jour-
nal Tipster corpus. In order to minimize the noise during
KPCA learning, we only extract the sentences in which
the target word occurs. For each target word, up to 1500
additional training instances were extracted. The result-
ing training corpus for the semi-supervised KPCA model
is more than 10 times larger than the Senseval-2 training
set, with an average of 1637 training instances per target
word.
The set of features used is as described by Yarowsky
and Florian (2002) in their ?feature-enhanced na??ve
Bayes model?, with position-sensitive, syntactic, and lo-
cal collocational features.
6 Results
Table 4 shows that the composite semi-supervised KPCA
model improves on the high-performance supervised
KPCA model, for both coarse-grained and fined-grained
sense distinctions. The supervised KPCA model signif-
icantly outperforms a na??ve Bayes model, and a max-
imum entropy model, which are among the top per-
forming models for WSD. Note that these results are
consistent with the larger study of supervised models
conducted by Wu et al (2004). The composite semi-
supervised KPCA model outperforms all of the three su-
pervised models, and in particular, it further improves the
Table 5: Semi-supervised KPCA is not necessary when
supervised KPCA is very confident.
Fine-grained
accuracy
Coarse-
grained
accuracy
Supervised KPCA 62.1% 71.3%
Semi-supervised
KPCA
57.1% 67.1%
Table 6: Semi-supervised KPCA outperforms supervised
KPCA when supervised KPCA is not confident: adding
training data helps when there are no similar instances in
the training set.
Fine-grained
accuracy
Coarse-
grained
accuracy
Supervised KPCA 30.8% 44.11%
Semi-supervised
KPCA
38.3% 51.47%
accuracy of the supervised KPCA model.
Overall, with the addition of the semi-supervised
model, the accuracy for disambiguating the verbs in-
creases from 57% to 57.4% on the fine-grained task, and
from 66.6% to 67.2% on the coarse-grained task.
In our composite model, the supervised KPCA model
predicts senses with high confidence for more than 94%
of the test instances. The predictions of the semi-
supervised model are used for the remaining 6% of the
test instances. Table 5 shows that it is not necessary to
use the semi-supervised training model for all the train-
ing instances. In fact, when the supervised model is con-
fident, its predictions are significantly more accurate than
those of the semi-supervised model alone.
When the predictions of the supervised KPCA model
are not accurate, the semi-supervised KPCA model out-
performs the supervised model. This happens when (1)
there is no training instance that is very similar to the test
instance considered and when (2) in the absence of rele-
vant features to learn from in the small annotated train-
ing set, the supervised KPCA model can only predict the
most frequent sense for the current target. In these condi-
tions, our experiment results in Table 6 confirm that the
semi-supervised KPCA model benefits from the large ad-
ditional training data, suggesting it is able to learn useful
feature conjunctions, which help to give better predic-
tions.
The composite semi-supervised KPCA model there-
fore chooses the best model depending on the degree
of confidence of the supervised model. All the KPCA
weights, for both the supervised and the semi-supervised
model, have been pre-computed during training, and it
is therefore inexpensive to switch from one model to the
other at testing time.
7 Conclusion
We have proposed a new composite semi-supervised
WSD model based on the Kernel PCA technique, that
employs both supervised and semi-supervised compo-
nents. This strategy allows us to combine large amounts
of cheap unlabeled data with smaller amounts of labeled
data. Experiments on the hard-to-disambiguate verbs
from the Senseval-2 English lexical sample task confirm
that when the supervised KPCA model is insufficiently
confident in its sense predictions, taking advantage of the
semi-supervised KPCA model trained with the unlabeled
data can help to give a better prediction. The composite
semi-supervised KPCA model exploits this to improve
upon the accuracy of the supervised KPCA model intro-
duced by Wu et al (2004).
References
Martin Chodorow, Claudia Leacock, and George A. Miller. A topical/local clas-
sifier for word sense identification. Computers and the Humanities, 34(1-
2):115?120, 1999. Special issue on SENSEVAL.
Hoa Trang Dang and Martha Palmer. Combining contextual features for word
sense disambiguation. In Proceedings of the SIGLEX/SENSEVAL Workshop
on Word Sense Disambiguation: Recent Successes and Future Directions,
pages 88?94, Philadelphia, July 2002. SIGLEX, Association for Computa-
tional Linguistics.
Konstantinos I. Diamantaras and Sun Yuan Kung. Principal Component Neural
Networks. Wiley, New York, 1996.
Adam Kilgarriff and Joseph Rosenzweig. Framework and results for English
Senseval. Computers and the Humanities, 34(1):15?48, 1999. Special issue
on SENSEVAL.
Adam Kilgarriff. English lexical sample task description. In Proceedings of
Senseval-2, Second International Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 17?20, Toulouse, France, July 2001. SIGLEX,
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. Conditional structure versus conditional
estimation in NLP models. In Proceedings of EMNLP-2002, Conference on
Empirical Methods in Natural Language Processing, pages 9?16, Philadel-
phia, July 2002. SIGDAT, Association for Computational Linguistics.
Taku Kudo and Yuji Matsumoto. Fast methods for kernel-based text analysis.
In Proceedings of the 41set Annual Meeting of the Asoociation for Computa-
tional Linguistics, pages 24?31, 2003.
S. Mika, B. Scho?lkopf, A. Smola, K.-R. Mu?ller, M. Scholz, and G. Ra?tsch. Ker-
nel PCA and de-noising in feature spaces. Advances in Neural Information
Processing Systems, 1999.
Raymond J. Mooney. Comparative experiments on disambiguating word senses:
An illustration of the role of bias in machine learning. In Proceedings of the
Conference on Empirical Methods in Natural Language Processing, Philadel-
phia, May 1996. SIGDAT, Association for Computational Linguistics.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang
Dang. English tasks: All-words and verb lexical sample. In Proceedings of
Senseval-2, Second International Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 21?24, Toulouse, France, July 2001. SIGLEX,
Association for Computational Linguistics.
Ted Pedersen. Machine learning with lexical features: The Duluth approach to
SENSEVAL-2. In Proceedings of Senseval-2, Second International Work-
shop on Evaluating Word Sense Disambiguation Systems, pages 139?142,
Toulouse, France, July 2001. SIGLEX, Association for Computational Lin-
guistics.
Bernhard Scho?lkopf, Alexander Smola, and Klaus-Rober Mu?ller. Nonlinear com-
ponent analysis as a kernel eigenvalue problem. Neural Computation, 10(5),
1998.
C. J. Twining and C. J. Taylor. Kernel principal component analysis and the con-
struction of non-linear active shape models. In Proceedings of BMVC20001,
2001.
Dekai Wu, Weifeng Su, and Marine Carpuat. A Kernel PCA method for superior
word sense disambiguation. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics, Barcelona, July 2004.
David Yarowsky and Radu Florian. Evaluating sense disambiguation across di-
verse parameter spaces. Natural Language Engineering, 8(4):293?310, 2002.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 61?72, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Statistical Machine Translation using
Word Sense Disambiguation
Marine CARPUAT Dekai WU?
marine@cs.ust.hk dekai@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science and Engineering
University of Science and Technology, Clear Water Bay, Hong Kong
Abstract
We show for the first time that incorporating
the predictions of a word sense disambigua-
tion system within a typical phrase-based
statistical machine translation (SMT) model
consistently improves translation quality
across all three different IWSLT Chinese-
English test sets, as well as producing sta-
tistically significant improvements on the
larger NIST Chinese-English MT task?
and moreover never hurts performance on
any test set, according not only to BLEU
but to all eight most commonly used au-
tomatic evaluation metrics. Recent work
has challenged the assumption that word
sense disambiguation (WSD) systems are
useful for SMT. Yet SMT translation qual-
ity still obviously suffers from inaccurate
lexical choice. In this paper, we address
this problem by investigating a new strat-
egy for integrating WSD into an SMT sys-
tem, that performs fully phrasal multi-word
disambiguation. Instead of directly incor-
porating a Senseval-style WSD system, we
redefine the WSD task to match the ex-
act same phrasal translation disambiguation
task faced by phrase-based SMT systems.
Our results provide the first known empir-
ical evidence that lexical semantics are in-
deed useful for SMT, despite claims to the
contrary.
?This material is based upon work supported in part by
the Defense Advanced Research Projects Agency (DARPA)
under GALE Contract No. HR0011-06-C-0023, and by the
Hong Kong Research Grants Council (RGC) research grants
1 Introduction
Common assumptions about the role and useful-
ness of word sense disambiguation (WSD) models
in full-scale statistical machine translation (SMT)
systems have recently been challenged.
On the one hand, in previous work (Carpuat and
Wu, 2005b) we obtained disappointing results when
using the predictions of a Senseval WSD system in
conjunction with a standard word-based SMT sys-
tem: we reported slightly lower BLEU scores de-
spite trying to incorporate WSD using a number
of apparently sensible methods. These results cast
doubt on the assumption that sophisticated dedicated
WSD systems that were developed independently
from any particular NLP application can easily be
integrated into a SMT system so as to improve trans-
lation quality through stronger models of context
and rich linguistic information. Rather, it has been
argued, SMT systems have managed to achieve sig-
nificant improvements in translation quality without
directly addressing translation disambiguation as a
WSD task. Instead, translation disambiguation deci-
sions are made indirectly, typically using only word
surface forms and very local contextual information,
forgoing the much richer linguistic information that
WSD systems typically take advantage of.
On the other hand, error analysis reveals that the
performance of SMT systems still suffers from inac-
curate lexical choice. In subsequent empirical stud-
ies, we have shown that SMT systems perform much
worse than dedicated WSD models, both supervised
RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any
opinions, findings and conclusions or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the views of the Defense Advanced Research
Projects Agency.
61
and unsupervised, on a Senseval WSD task (Carpuat
and Wu, 2005a), and therefore suggest that WSD
should have a role to play in state-of-the-art SMT
systems. In addition to the Senseval shared tasks,
which have provided standard sense inventories and
data sets, WSD research has also turned increasingly
to designing specific models for a particular applica-
tion. For instance, Vickrey et al (2005) and Specia
(2006) proposed WSD systems designed for French
to English, and Portuguese to English translation re-
spectively, and present a more optimistic outlook for
the use of WSD in MT, although these WSD sys-
tems have not yet been integrated nor evaluated in
full-scale machine translation systems.
Taken together, these seemingly contradictory re-
sults suggest that improving SMT lexical choice ac-
curacy remains a key challenge to improve current
SMT quality, and that it is still unclear what is
the most appropriate integration framework for the
WSD models in SMT.
In this paper, we present first results with a
new architecture that integrates a state-of-the-art
WSD model into phrase-based SMT so as to per-
form multi-word phrasal lexical disambiguation,
and show that this new WSD approach not only
produces gains across all available Chinese-English
IWSLT06 test sets for all eight commonly used au-
tomated MT evaluation metrics, but also produces
statistically significant gains on the much larger
NIST Chinese-English task. The main difference
between this approach and several of our earlier ap-
proaches as described in Carpuat and Wu (2005b)
and subsequently Carpuat et al (2006) lies in the
fact that we focus on repurposing the WSD system
for multi-word phrase-based SMT. Rather than us-
ing a generic Senseval WSD model as we did in
Carpuat and Wu (2005b), here both the WSD train-
ing and the WSD predictions are integrated into the
phrase-based SMT framework. Furthermore, rather
than using a single word based WSD approach to
augment a phrase-based SMT model as we did in
Carpuat et al (2006) to improve BLEU and NIST
scores, here the WSD training and predictions oper-
ate on full multi-word phrasal units, resulting in sig-
nificantly more reliable and consistent gains as eva-
luted by many other translation accuracy metrics as
well. Specifically:
? Instead of using a Senseval system, we redefine
the WSD task to be exactly the same as lexi-
cal choice task faced by the multi-word phrasal
translation disambiguation task faced by the
phrase-based SMT system.
? Instead of using predefined senses drawn from
manually constructed sense inventories such as
HowNet (Dong, 1998), our WSD for SMT sys-
tem directly disambiguates between all phrasal
translation candidates seen during SMT train-
ing.
? Instead of learning from manually annotated
training data, our WSD system is trained on the
same corpora as the SMT system.
However, despite these adaptations to the SMT
task, the core sense disambiguation task remains
pure WSD:
? The rich context features are typical of WSD
and almost never used in SMT.
? The dynamic integration of context-sensitive
translation probabilities is not typical of SMT.
? Although it is embedded in a real SMT sys-
tem, the WSD task is exactly the same as in
recent and coming Senseval Multilingual Lexi-
cal Sample tasks (e.g., Chklovski et al (2004)),
where sense inventories represent the semantic
distinctions made by another language.
We begin by presenting the WSD module and
the SMT integration technique. We then show that
incorporating it into a standard phrase-based SMT
baseline system consistently improves translation
quality across all three different test sets from the
Chinese-English IWSLT text translation evaluation,
as well as on the larger NIST Chinese-English trans-
lation task. Depending on the metric, the individual
gains are sometimes modest, but remarkably, incor-
porating WSD never hurts, and helps enough to al-
ways make it a worthwile additional component in
an SMT system. Finally, we analyze the reasons for
the improvement.
62
2 Problems in context-sensitive lexical
choice for SMT
To the best of our knowledge, there has been no pre-
vious attempt at integrating a state-of-the-art WSD
system for fully phrasal multi-word lexical choice
into phrase-based SMT, with evaluation of the re-
sulting system on a translation task. While there
are many evaluations of WSD quality, in particular
the Senseval series of shared tasks (Kilgarriff and
Rosenzweig (1999), Kilgarriff (2001), Mihalcea et
al. (2004)), very little work has been done to address
the actual integration of WSD in realistic SMT ap-
plications.
To fully integrate WSD into phrase-based SMT,
it is necessary to perform lexical disambiguation
on multi-word phrasal lexical units; in contrast,
the model reported in Cabezas and Resnik (2005)
can only perform lexical disambiguation on sin-
gle words. Like the model proposed in this paper,
Cabezas and Resnik attempted to integrate phrase-
based WSD models into decoding. However, al-
though they reported that incorporating these predic-
tions via the Pharaoh XML markup scheme yielded
a small improvement in BLEU score over a Pharaoh
baseline on a single Spanish-English translation data
set, we have determined empirically that applying
their single-word based model to several Chinese-
English datasets does not yield systematic improve-
ments on most MT evaluation metrics (Carpuat and
Wu, 2007). The single-word model has the disad-
vantage of forcing the decoder to choose between
the baseline phrasal translation probabilities versus
the WSD model predictions for single words. In ad-
dition, the single-word model does not generalize
to WSD for phrasal lexical choice, as overlapping
spans cannot be specified with the XML markup
scheme. Providing WSD predictions for phrases
would require committing to a phrase segmenta-
tion of the input sentence before decoding, which
is likely to hurt translation quality.
It is also necessary to focus directly on translation
accuracy rather than other measures such as align-
ment error rate, which may not actually lead to im-
proved translation quality; in contrast, for example,
Garcia-Varea et al (2001) and Garcia-Varea et al
(2002) show improved alignment error rate with a
maximum entropy based context-dependent lexical
choice model, but not improved translation accu-
racy. In contrast, our evaluation in this paper is con-
ducted on the actual decoding task, rather than in-
termediate tasks such as word alignment. Moreover,
in the present work, all commonly available auto-
mated MT evaluation metrics are used, rather than
only BLEU score, so as to maintain a more balanced
perspective.
Another problem in the context-sensitive lexical
choice in SMT models of Garcia Varea et al is that
their feature set is insufficiently rich to make much
better predictions than the SMT model itself. In
contrast, our WSD-based lexical choice models are
designed to directly model the lexical choice in the
actual translation direction, and take full advantage
of not residing strictly within the Bayesian source-
channel model in order to benefit from the much
richer Senseval-style feature set this facilitates.
Garcia Varea et al found that the best results are
obtained when the training of the context-dependent
translation model is fully incorporated with the EM
training of the SMT system. As described below,
the training of our new WSD model, though not in-
corporated within the EM training, is also far more
closely tied to the SMT model than is the case with
traditional standalone WSD models.
In contrast with Brown et al (1991), our ap-
proach incorporates the predictions of state-of-the-
art WSD models that use rich contextual features for
any phrase in the input vocabulary. In Brown et al?s
early study of WSD impact on SMT performance,
the authors reported improved translation quality on
a French to English task, by choosing an English
translation for a French word based on the single
contextual feature which is reliably discriminative.
However, this was a pilot study, which is limited to
words with exactly two translation candidates, and it
is not clear that the conclusions would generalize to
more recent SMT architectures.
3 Problems in translation-oriented WSD
The close relationship between WSD and SMT has
been emphasized since the emergence of WSD as
an independent task. However, most of previous re-
search has focused on using multilingual resources
typically used in SMT systems to improve WSD ac-
curacy, e.g., Dagan and Itai (1994), Li and Li (2002),
63
Diab (2004). In contrast, this paper focuses on the
converse goal of using WSD models to improve ac-
tual translation quality.
Recently, several researchers have focused on de-
signing WSD systems for the specific purpose of
translation. Vickrey et al (2005) train a logistic re-
gression WSD model on data extracted from auto-
matically word aligned parallel corpora, but evaluate
on a blank filling task, which is essentially an eval-
uation of WSD accuracy. Specia (2006) describes
an inductive logic programming-based WSD sys-
tem, which was specifically designed for the purpose
of Portuguese to English translation, but this system
was also only evaluated on WSD accuracy, and not
integrated in a full-scale machine translation system.
Ng et al (2003) show that it is possible to use
automatically word aligned parallel corpora to train
accurate supervised WSD models. The purpose of
the study was to lower the annotation cost for su-
pervised WSD, as suggested earlier by Resnik and
Yarowsky (1999). However this result is also en-
couraging for the integration of WSD in SMT, since
it suggests that accurate WSD can be achieved using
training data of the kind needed for SMT.
4 Building WSD models for phrase-based
SMT
4.1 WSD models for every phrase in the input
vocabulary
Just like for the baseline phrase translation model,
WSD models are defined for every phrase in the in-
put vocabulary. Lexical choice in SMT is naturally
framed as a WSD problem, so the first step of inte-
gration consists of defining a WSD model for every
phrase in the SMT input vocabulary.
This differs from traditional WSD tasks, where
the WSD target is a single content word. Sense-
val for instance has either lexical sample or all word
tasks. The target words for both categories of Sen-
seval WSD tasks are typically only content words?
primarily nouns, verbs, and adjectives?while in the
context of SMT, we need to translate entire sen-
tences, and therefore have a WSD model not only
for every word in the input sentences, regardless of
their POS tag, but for every phrase, including tokens
such as articles, prepositions and even punctuation.
Further empirical studies have suggested that includ-
ing WSD predictions for those longer phrases is a
key factor to help the decoder produce better trans-
lations (Carpuat and Wu, 2007).
4.2 WSD uses the same sense definitions as the
SMT system
Instead of using pre-defined sense inventories, the
WSD models disambiguate between the SMT trans-
lation candidates. In order to closely integrate WSD
predictions into the SMT system, we need to formu-
late WSD models so that they produce features that
can directly be used in translation decisions taken
by the SMT system. It is therefore necessary for the
WSD and SMT systems to consider exactly the same
translation candidates for a given word in the input
language.
Assuming a standard phrase-based SMT system
(e.g., Koehn et al (2003)), WSD senses are thus ei-
ther words or phrases, as learned in the SMT phrasal
translation lexicon. Those ?sense? candidates are
very different from those typically used even in ded-
icated WSD tasks, even in the multilingual Senseval
tasks. Each candidate is a phrase that is not neces-
sarily a syntactic noun or verb phrase as in manually
compiled dictionaries. It is quite possible that dis-
tinct ?senses? in our WSD for SMT system could be
considered synonyms in a traditional WSD frame-
work, especially in monolingual WSD.
In addition to the consistency requirements for in-
tegration, this requirement is also motivated by em-
pirical studies, which show that predefined trans-
lations derived from sense distinctions defined in
monolingual ontologies do not match translation
distinction made by human translators (Specia et al,
2006).
4.3 WSD uses the same training data as the
SMT system
WSD training does not require any other resources
than SMT training, nor any manual sense annota-
tion. We employ supervised WSD systems, since
Senseval results have amply demonstrated that su-
pervised models significantly outperform unsuper-
vised approaches (see for instance the English lexi-
cal sample tasks results described by Mihalcea et al
(2004)).
Training examples are annotated using the phrase
alignments learned during SMT training. Every in-
64
put language phrase is sense-tagged with its aligned
output language phrase in the parallel corpus. The
phrase alignment method used to extract the WSD
training data therefore depends on the one used by
the SMT system. This presents the advantage of
training WSD and SMT models on exactly the same
data, thus eliminating domain mismatches between
Senseval data and parallel corpora. But most impor-
tantly, this allows WSD training data to be gener-
ated entirely automatically, since the parallel corpus
is automatically phrase-aligned in order to learn the
SMT phrase bilexicon.
4.4 The WSD system
The word sense disambiguation subsystem is mod-
eled after the best performing WSD system in the
Chinese lexical sample task at Senseval-3 (Carpuat
et al, 2004).
The features employed are typical of WSD and
are therefore far richer than those used in most
SMT systems. The feature set consists of position-
sensitive, syntactic, and local collocational fea-
tures, since these features yielded the best results
when combined in a na??ve Bayes model on several
Senseval-2 lexical sample tasks (Yarowsky and Flo-
rian, 2002). These features scale easily to the bigger
vocabulary and sense candidates to be considered in
a SMT task.
The Senseval system consists of an ensemble of
four combined WSD models:
The first model is a na??ve Bayes model, since
Yarowsky and Florian (2002) found this model to be
the most accurate classifier in a comparative study
on a subset of Senseval-2 English lexical sample
data.
The second model is a maximum entropy model
(Jaynes, 1978), since Klein and Manning (Klein
and Manning, 2002) found that this model yielded
higher accuracy than na??ve Bayes in a subsequent
comparison of WSD performance.
The third model is a boosting model (Freund
and Schapire, 1997), since boosting has consistently
turned in very competitive scores on related tasks
such as named entity classification. We also use the
Adaboost.MH algorithm.
The fourth model is a Kernel PCA-based model
(Wu et al, 2004). Kernel Principal Component
Analysis or KPCA is a nonlinear kernel method for
extracting nonlinear principal components from vec-
tor sets where, conceptually, the n-dimensional in-
put vectors are nonlinearly mapped from their origi-
nal space Rn to a high-dimensional feature space F
where linear PCA is performed, yielding a transform
by which the input vectors can be mapped nonlin-
early to a new set of vectors (Scho?lkopf et al, 1998).
WSD can be performed by a Nearest Neighbor Clas-
sifier in the high-dimensional KPCA feature space.
All these classifiers have the ability to handle
large numbers of sparse features, many of which
may be irrelevant. Moreover, the maximum entropy
and boosting models are known to be well suited to
handling features that are highly interdependent.
4.5 Integrating WSD predictions in
phrase-based SMT architectures
It is non-trivial to incorporate WSD into an existing
phrase-based architecture such as Pharaoh (Koehn,
2004), since the decoder is not set up to easily ac-
cept multiple translation probabilities that are dy-
namically computed in context-sensitive fashion.
For every phrase in a given SMT input sentence,
the WSD probabilities can be used as additional fea-
ture in a loglinear translation model, in combina-
tion with typical context-independent SMT bilexi-
con probabilities.
We overcome this obstacle by devising a calling
architecture that reinitializes the decoder with dy-
namically generated lexicons on a per-sentence ba-
sis.
Unlike a n-best reranking approach, which is lim-
ited by the lexical choices made by the decoder us-
ing only the baseline context-independent transla-
tion probabilities, our method allows the system to
make full use of WSD information for all competing
phrases at all decoding stages.
5 Experimental setup
The evaluation is conducted on two standard Chi-
nese to English translation tasks. We follow stan-
dard machine translation evaluation procedure us-
ing automatic evaluation metrics. Since our goal is
to evaluate translation quality, we use standard MT
evaluation methodology and do not evaluate the ac-
curacy of the WSD model independently.
65
Table 1: Evaluation results on the IWSLT06 dataset: integrating the WSD translation predictions improves
BLEU, NIST, METEOR, WER, PER, CDER and TER across all 3 different available test sets.
Test
Set
Exper. BLEU NIST METEOR METEOR
(no syn)
TER WER PER CDER
Test 1 SMT 42.21 7.888 65.40 63.24 40.45 45.58 37.80 40.09
SMT+WSD 42.38 7.902 65.73 63.64 39.98 45.30 37.60 39.91
Test 2 SMT 41.49 8.167 66.25 63.85 40.95 46.42 37.52 40.35
SMT+WSD 41.97 8.244 66.35 63.86 40.63 46.14 37.25 40.10
Test 3 SMT 49.91 9.016 73.36 70.70 35.60 40.60 32.30 35.46
SMT+WSD 51.05 9.142 74.13 71.44 34.68 39.75 31.71 34.58
Table 2: Evaluation results on the NIST test set: integrating the WSD translation predictions improves
BLEU, NIST, METEOR, WER, PER, CDER and TER
Exper. BLEU NIST METEOR METEOR
(no syn)
TER WER PER CDER
SMT 20.41 7.155 60.21 56.15 76.76 88.26 61.71 70.32
SMT+WSD 20.92 7.468 60.30 56.79 71.34 83.87 57.29 67.38
5.1 Data set
Preliminary experiments are conducted using train-
ing and evaluation data drawn from the multilin-
gual BTEC corpus, which contains sentences used in
conversations in the travel domain, and their transla-
tions in several languages. A subset of this data was
made available for the IWSLT06 evaluation cam-
paign (Paul, 2006); the training set consists of 40000
sentence pairs, and each test set contains around 500
sentences. We used only the pure text data, and not
the speech transcriptions, so that speech-specific is-
sues would not interfere with our primary goal of un-
derstanding the effect of integrating WSD in a full-
scale phrase-based model.
A larger scale evaluation is conducted on the stan-
dard NIST Chinese-English test set (MT-04), which
contains 1788 sentences drawn from newswire cor-
pora, and therefore of a much wider domain than the
IWSLT data set. The training set consists of about 1
million sentence pairs in the news domain.
Basic preprocessing was applied to the corpus.
The English side was simply tokenized and case-
normalized. The Chinese side was word segmented
using the LDC segmenter.
5.2 Baseline SMT system
Since our focus is not on a specific SMT architec-
ture, we use the off-the-shelf phrase-based decoder
Pharaoh (Koehn, 2004) trained on the IWSLT train-
ing set. Pharaoh implements a beam search decoder
for phrase-based statistical models, and presents
the advantages of being freely available and widely
used.
The phrase bilexicon is derived from the inter-
section of bidirectional IBM Model 4 alignments,
obtained with GIZA++ (Och and Ney, 2003), aug-
mented to improve recall using the grow-diag-final
heuristic. The language model is trained on the Eng-
lish side of the corpus using the SRI language mod-
eling toolkit (Stolcke, 2002).
The loglinear model weights are learned using
Chiang?s implementation of the maximum BLEU
training algorithm (Och, 2003), both for the base-
line, and the WSD-augmented system. Due to
time constraints, this optimization was only con-
ducted on the IWSLT task. The weights used in the
WSD-augmented NIST model are based on the best
IWSLT model. Given that the two tasks are quite
different, we expect further improvements on the
WSD-augmented system after running maximum
BLEU optimization for the NIST task.
6 Results and discussion
Using WSD predictions in SMT yields better trans-
lation quality on all test sets, as measured by all
eight commonly used automatic evaluation metrics.
66
Table 3: Translation examples with and without WSD for SMT, drawn from IWSLT data sets.
Input ?lX-. ?
Ref. Please transfer to the Chuo train line.
SMT Please turn to the Central Line.
SMT+WSD Please transfer to Central Line.
Input fh(f
p
Ref. Do I pay on the bus?
SMT Please get on the bus?
SMT+WSD I buy a ticket on the bus?
Input  ???
Ref. Do I need a reservation?
SMT I need a reservation?
SMT+WSD Do I need a reservation?
Input ??n? ? h???
Ref. I want to reconfirm this ticket.
SMT I would like to reconfirm a flight for this ticket.
SMT+WSD I would like to reconfirm my reservation for this ticket.
Input eL??0??
Ref. Can I get there on foot?
SMT Is there on foot?
SMT+WSD Can I get there on foot?
Input 	? *?Inversion Transduction Grammar Constraints
for Mining Parallel Sentences from
Quasi-Comparable Corpora
Dekai Wu1 and Pascale Fung2
1 Human Language Technology Center, HKUST,
Department of Computer Science
2 Department of Electrical and Electronic Engineering,
University of Science and Technology, Clear Water Bay, Hong Kong
dekai@cs.ust.hk, pascale@ee.ust.hk
Abstract. We present a new implication of Wu?s (1997) Inversion
Transduction Grammar (ITG) Hypothesis, on the problem of retriev-
ing truly parallel sentence translations from large collections of highly
non-parallel documents. Our approach leverages a strong language uni-
versal constraint posited by the ITG Hypothesis, that can serve as a
strong inductive bias for various language learning problems, resulting
in both efficiency and accuracy gains. The task we attack is highly prac-
tical since non-parallel multilingual data exists in far greater quantities
than parallel corpora, but parallel sentences are a much more useful re-
source. Our aim here is to mine truly parallel sentences, as opposed to
comparable sentence pairs or loose translations as in most previous work.
The method we introduce exploits Bracketing ITGs to produce the first
known results for this problem. Experiments show that it obtains large
accuracy gains on this task compared to the expected performance of
state-of-the-art models that were developed for the less stringent task of
mining comparable sentence pairs.
1 Introduction
Parallel sentences are a relatively scarce but extremely useful resource for many
applications including cross-lingual retrieval and statistical machine translation.
Parallel sentences, or bi-sentences for short, can be exploited for a wealth of
applications ranging from mining term translations for cross-lingual applications,
to training paraphrase models and inducing structured terms for indexing, query
processing, and retrieval.
Unfortunately, far more is available in the way of monolingual data. High-
quality parallel corpora are currently largely limited to specialized collections
of government (especially UN) and certain newswire collections, and even then
relatively few bi-sentences are available in tight sentence-by-sentence translation.
 This work was supported in part by the Hong Kong Research Grants Council through
grants RGC6083/99E, RGC6256/00E, DAG03/04.EG09, and RGC6206/03E.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 257?268, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
258 D. Wu and P. Fung
Increasingly sophisticated methods for extracting loose translations from non-
parallel monolingual corpora?and in particular, what have been called com-
parable sentence pairs?have also recently become available. But while loose
translations by themselves already have numerous applications, truly parallel
sentence translations provide invaluable types of information for the aforemen-
tioned types of mining and induction, which cannot easily be obtained from
merely loose translations or comparable sentence pairs. In particular, truly par-
allel bi-sentences are especially useful for extracting more precise syntactic and
semantic relations within word sequences.
We present a new method that exploits a novel application of Inversion
Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu
1997 [2]) for mining monolingual data to obtain tight sentence translation pairs,
yielding accuracy significantly higher than previous known methods. We focus
here on very non-parallel quasi-comparable monolingual corpora, which are avail-
able in far larger quantities but are significantly more difficult to mine than either
noisy parallel corpora or comparable corpora. The majority of previous work has
concerned noisy parallel corpora (sometimes imprecisely also called ?compara-
ble corpora?), which contain non-aligned sentences that are nevertheless mostly
bilingual translations of the same document. More recent work has examined
comparable corpora, which contain non-sentence-aligned, non-translated bilin-
gual documents that are topic-aligned. Still relatively few methods attempt to
mine quasi-comparable corpora, which contain far more heterogeneous, very non-
parallel bilingual documents that could be either on the same topic (in-topic) or
not (off-topic).
Our approach is motivated by a number of desirable characteristics of ITGs,
which historically were developed for translation and alignment purposes, rather
than mining applications of the kind discussed in this paper. The ITG Hypothesis
posits a strong language universal constraint that can act as a strong inductive
bias for various language learning problems, resulting in both efficiency and accu-
racy gains. Specifically, the hypothesis asserts that sentence translation between
any two natural languages can be accomplished within ITG expressiveness (sub-
ject to certain conditions). So-called Bracketing ITGs (BITG) are particularly
interesting in certain applications such as the problem we consider here, because
they impose ITG constraints in language-independent fashion, and do not re-
quire any language-specific linguistic grammar. (As discussed below, Bracketing
ITGs are the simplest form of ITGs, where the grammar uses only a single,
undifferentiated non-terminal.)
The key modeling property of bracketing ITGs that is most relevant to the
task of identifying parallel bi-sentences is that they assign strong preference to
candidate sentence pairs in which nested constituent subtrees can be recursively
aligned with a minimum of constituent boundary violations. Unlike language-
specific linguistic approaches, however, the shape of the trees are driven in un-
supervised fashion by the data. One way to view this is that the trees are hid-
den explanatory variables. This not only provides significantly higher robustness
than more highly constrained manually constructed grammars, but also makes
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 259
the model widely applicable across languages in economical fashion without a
large investment in manually constructed resources.
Moreover, for reasons discussed by Wu [2], ITGs possess an interesting in-
trinsic combinatorial property of permitting roughly up to four arguments of any
frame to be transposed freely, but not more. This matches suprisingly closely the
preponderance of linguistic verb frame theories from diverse linguistic traditions
that all allow up to four arguments per frame. Again, this property falls naturally
out of ITGs in language-independent fashion, without any hardcoded language-
specific knowledge. This further suggests that ITGs should do well at picking out
translation pairs where the order of up to four arguments per frame may vary
freely between the two languages. Conversely, ITGs should do well at rejecting
candidates where (1) too many words in one sentence find no correspondence in
the other, (2) frames do not nest in similar ways in the candidate sentence pair,
or (3) too many arguments must be transposed to achieve an alignment?all of
which would suggest that the sentences probably express different ideas.
Various forms of empirical confirmation for the ITG Hypothesis have emerged
recently, which quantitatively support the qualitative cross-linguistic character-
istics just described across a variety of language pairs and tasks. Zens and Ney
(2003) [3] show that ITG constraints yield significantly better alignment coverage
than the constraints used in IBM statistical machine translation models on both
German-English (Verbmobil corpus) and French-English (Canadian Hansards
corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using
Bracketing ITGs produces significantly lower Chinese-English alignment error
rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea
(2005) [6] show that lexicalized ITGs can further improve alignment accuracy.
With regard to translation rather than alignment accuracy, Zens et al (2004)
[7] show that decoding under ITG constraints yields significantly lower word
error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] ob-
tains significant BLEU score improvements via unsupervised induction of hi-
erarchical phrasal bracketing ITGs. Such results partly motivate the work we
discuss here.
We will begin by surveying recent related work and reviewing the formal
properties of ITGs. Subsequently we describe the architecture of our new
method, which relies on multiple stages so as to balance efficiency and accuracy
considerations. Finally we discuss experimental results on a quasi-comparable
corpus of Chinese and English from the topic detection task.
2 Recent Approaches to Mining Non-parallel Corpora
Recent work (Fung and Cheung 2004 [9]; Munteanu et al 2004 [10]; Zhao and
Vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely
based on finding on-topic documents first through similarity matching and time
alignment.
However, Zhao and Vogel used a corpus of Chinese and English versions of
news stories from the Xinhua News agency, with ?roughly similar sentence order
260 D. Wu and P. Fung
of content?. This corpus can be more accurately described as a noisy parallel cor-
pus. Munteanu et al used comparable corpora of news articles published within
the same 5-day window. In both cases, the corpora contain documents on the
same matching topics; unlike our present objective of mining quasi-comparable
corpora, these other methods assume corpora of on-topic documents.
Munteanu et al first identify on-topic document pairs by looking at publication
date and word overlap, then classify all sentence pairs as being parallel or not par-
allel, using a maximum entropy classifier trained on parallel corpora. In contrast,
the method we will propose identifies candidate sentence pairs without assuming
that publication date information is available, and then uses the ITG constraints
to automatically find parallel sentence pairs without requiring any training.
It is also difficult to relate Munteanu et al?s work to our present objective
because they do not directly evaluate the quality of the extracted bi-sentences
(they instead look at performance of their machine translation application);
however, as with Fung and Cheung, they noted that the sentences extracted
were not truly parallel on the whole.
In this work, we aim to find parallel sentences from much more heterogenous,
very non-parallel quasi-comparable corpora. Since many more multilingual text
collections available today contain documents that do not match documents
in the other language, we propose finding more parallel sentences from off-topic
documents, as well as on-topic documents. An example is the TDT corpus, which
is an aggregation of multiple news sources from different time periods.
3 Inversion Transduction Grammars
Formally, within the expressiveness hierarchy of transduction grammars, the
ITG level of expressiveness has highly unusual intrinsic properties as seen in
Figure 1. Wu [2] showed that the ITG class is an equivalence class of subsets
of syntax-directed transduction grammars or SDTGs (Lewis and Stearns 1968
[12]), equivalently defined by meeting any of the following three conditions: (1) all
rules are of rank 2, (2) all rules are of rank 3, or (3) all rules are either of straight
or inverted orientation (and may have any rank). Ordinary unrestricted SDTGs
allow any permutation of the symbols on the right-hand side to be specified when
translating from the input language to the output language. In contrast, ITGs
only allow two out of the possible permutations. If a rule is straight, the order of
its right-hand symbols must be the same for both languages (just as in a simple
SDTG or SSDTG). On the other hand, if a rule is inverted, then the order is left-
to-right for the input language and right-to-left for the output language. Since
inversion is permitted at any level of rule expansion, a derivation may intermix
productions of either orientation within the parse tree. The ability to compose
multiple levels of straight and inverted constituents gives ITGs much greater
expressiveness than might seem at first blush, as indicated by the growing body
of empirical results mentioned earlier.
A simple example may be useful to fix ideas. Consider the following pair of
parse trees for sentence translations:
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 261
Fig. 1. The ITG level of expressiveness constitutes a surprisingly broad equivalence class
within the expressiveness hierarchy of transduction grammars. The simple monolingual
notion of ?context-free? is too coarse to adequately categorize the bilingual case of trans-
duction grammars. The expressiveness of a transduction grammar depends on the max-
imum rank k of rules, i.e., the maximum number of nonterminals on the right-hand-side.
SDTG-k is always more expressive than SDTG-(k-1), except for the special case of the
ITG class which includes both SDTG-2 and SDTG-3. In contrast, for monolingual CFGs,
expressiveness is not affected by rank, as shown by the existence of a binary Chomsky
normal form for any CFG. A binary normal form exists for ITGs but not SDTGs.
[[[The Authority]
NP
[will [[be accountable]
VV
[to [the [[Financial Secretary]
NN
]
NNN
]
NP
]
PP
]
VP
]
VP
]
SP
.]
S
[[[]
NP
[ [[ [[[ ]
NN
]
NNN
]
NP
]
PP
[]
VV
]
VP
]
VP
]
SP
]
S
Even though the order of constituents under the inner VP is inverted between
the languages, an ITG can capture the common structure of the two sentences.
This is compactly shown by writing the parse tree together for both sentences
with the aid of an ?? angle bracket notation marking parse tree nodes that
instantiate rules of inverted orientation:
[[[The/?Authority/]NP [will/ ?[be/?accountable/]VV [to/ [the/?
[[Financial/Secretary/]NN ]NNN ]NP ]PP ?VP ]VP ]SP./]S
In a weighted or stochastic ITG (SITG), a weight or a probability is associ-
ated with each rewrite rule. Following the standard convention, we use a and b
to denote probabilities for syntactic and lexical rules, respectively. For example,
the probability of the rule NN
0.4? [A N] is aNN?[A N] = 0.4. The probability of a
lexical rule A
0.001? x/y is bA(x, y) = 0.001. Let W1, W2 be the vocabulary sizes
of the two languages, and N = {A1, . . . , AN} be the set of nonterminals with
indices 1, . . . , N .
Polynomial-time algorithms are possible for various tasks including transla-
tion using ITGs, as well as bilingual parsing or biparsing, where the task is to
build the highest-scored parse tree given an input bi-sentence.
262 D. Wu and P. Fung
For present purposes we can employ the special case of Bracketing ITGs,
where the grammar employs only one single, undistinguished ?dummy? nonter-
minal category for any non-lexical rule. Designating this category A, a Bracketing
ITG has the following form (where, as usual, lexical transductions of the form
A ? e/f may possibly be singletons of the form A ? e/ or A ? /f).
A ? [AA]
A ? ?AA?
A ? , 
A ? e1/f1
. . .
A ? ei/fj
Broadly speaking, Bracketing ITGs are useful when we wish to make use of
the structural properties of ITGs discussed above, without requiring any addi-
tional linguistic information as constraints. Since they lack differentiated syn-
tactic categories, Bracketing ITGs merely constrain the shape of the trees that
align various nested portions of a sentence pair. The only linguistic knowledge
used in Bracketing ITGs is the purely lexical set of collocation translations.
Nevertheless, the ITG Hypothesis implies that biparsing truly parallel sentence
pairs with a Bracketing ITG should typically yield high scores. Conversely, some
non-parallel sentence pairs could be ITG-alignable, but any significant departure
violating constituent boundaries will be downgraded.
As an illustrative example, in the models employed by most previous work
on mining bi-sentences from non-parallel corpora, the following pair of sentences
(found in actual data arising in our experiments below) would receive an inap-
propriately high score, because of the high lexical similarity between the two
sentences:
Chinese president Jiang Zemin arrived in Japan today for a landmark state visit .
            
(Jiang Zemin will be the first Chinese national president to pay a state vist to
Japan.)
However, the ITG based model is sensitive enough to the differences in the
constituent structure (reflecting underlying differences in the predicate argument
structure) so that our experiments show that it assigns a low score. On the
other hand, the experiments also show that it successfully assigns a high score
to other candidate bi-sentences representing a true Chinese translation of the
same English sentence, as well as a true English translation of the same Chinese
sentence.
4 Candidate Generation
An extremely large set of pairs of monolingual sentences from the quasi-
comparable monolingual corpora will need to be scanned to obtain a useful
M
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 263
Fig. 2. Candidate generation overview. The iterative bootstrapping algorithm first
mines loosely parallel sentence pairs from quasi-comparable corpora that contain both
on-topic and off-topic documents. In a preprocessing step, documents that are believed
to be on the same topic according to their similarity score are extracted, then ?parallel?
pairs are mined from these matched documents. The extracted sentences are used to
bootstrap the entire process iteratively in two ways: (1) they are used to update a
bilingual lexicon, which is then used again to reprocess the documents to be matched
again; (2) any document pairs that are found to contain at least one ?parallel? sentence
pairs are considered to be on-topic, and added to the matched document set. Note
that step (2) adds to the on-topic document set certain document pairs that are not
considered to be on-topic by document matching scores.
number of parallel sentences, since obviously, the overwhelming majority of the
n2 possible sentence pairs will not be parallel. It is infeasible to run the ITG
biparsing algorithm on n2 candidate sentence pairs. Therefore a multi-stage al-
gorithm is needed that first generates likely candidates using faster heuristics,
and then biparses the candidates to obtain the final high-precision results.
We base our candidate generation on a method that Fung and Cheung (2004)
developed for extracting loose translations (comparable sentence pairs) from
quasi-comparable corpora [9], as shown in Figure 2. We selected this model
because it produces the highest known accuracy on that task.
Figure 3 outlines the algorithm in greater detail. In the following sections, we
describe the document pre-processing step followed by each of the subsequent
iterative steps of the algorithm.
264 D. Wu and P. Fung
1. Initial document matching
For all documents in the comparable corpus D:
? Gloss Chinese documents using the bilingual lexicon (Bilex)
? For every pair of glossed Chinese document and English documents:
? compute document similarity => S(i,j)
? Obtain all matched bilingual document pairs whose S(i,j) > threshold1 => D2
2. Sentence matching
For each document pair in D2:
? For every pair of glossed Chinese sentence and English sentence:
? compute sentence similarity => S2(i,j)
? Obtain all matched bilingual sentence pairs whose S2(i,j) > threshold2 => C1
3. EM learning of new word translations
For all bilingual sentences pairs in C1, do:
? Compute translation lexicon probabilities of all bilingual word pairs =>S3(i,j)
? Obtain all bilingual word pairs previously unseen in Bilex and whose S3(i,j) > threshold3
=> L1, and update Bilex
? Compute sentence alignment scores => S4; if S4 does not change then return C1 and L1,
otherwise continue
4. Document re-matching
? Find all pairs of glossed Chinese and English documents which contain parallel sentences
(anchor sentences) from C1 => D3
? Expand D2 by finding documents similar to each of the document in D2
? D2 := D3
5. Goto 2 if termination criterion not met
Fig. 3. Candidate generation algorithm
Document preprocessing. The documents are word segmented with the Linguistic
Data Consortium (LDC) Chinese-English dictionary 2.0. The Chinese document
is then glossed using all the dictionary entries. When a Chinese word has multiple
possible translations in English, it is disambiguated using an extension of Fung
et al?s (1999) method [13].
Initial document matching. The aim of this step is to roughly match the Chinese-
English documents pairs that are on-topic, in order to extract parallel sentences
from them. Following previous work, cosine similarity between document vectors
is used to judge whether a bilingual document pair is on-topic or off-topic.
Both the glossed Chinese document and English are represented in word
vectors, with term weights. Pair-wise similarities are calculated for all possible
Chinese-English document pairs, and bilingual documents with similarities above
a certain threshold are considered to be comparable. Comparable documents are
often on-topic.
Sentence matching. All sentence pair combinations within the on-topic docu-
ments are considered next in the selection process. Each sentence is again rep-
resented as word vectors. For each extracted document pair, pair-wise cosine
similarities are calculated for all possible Chinese-English sentence pairs. Sen-
tence pairs above a set threshold are considered parallel and extracted from the
documents. Since cosine similarity is computed on translated word pairs within
the sentence pairs, the better our bilingual lexicon is, the more accurate the
sentence similarity will be. In the following section, we discuss how to find new
word translations.
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 265
EM lexical learning from matched sentence pairs. This step updates the bilingual
lexicon according to the intermediate results of parallel sentence extraction. New
bilingual word pairs are learned from the extracted sentence pairs based on an
EM learning method. In our experience any common method can be used for this
purpose; for the experiments below we used the GIZA++ [14] implementation
of the IBM statistical translation lexicon Model 4 of Brown et al (1993) [15].
This model is based on the conditional probability of a source word being
generated by the target word in the other language, based on EM estimation
from aligned sentences. Zhao and Vogel (2002) showed that this model lends
itself to adaptation and can provide better vocabulary coverage and better sen-
tence alignment probability estimation [11]. In our work, we use this model on
the intermediate results of parallel sentence extraction, i.e., on a set of aligned
sentence pairs that may or may not truly correspond to each other.
We found that sentence pairs with high alignment scores are not necessarily
more similar than others. This might be due to the fact that EM estimation
at each intermediate step is not reliable, since we only have a small amount of
aligned sentences that are truly parallel. The EM learner is therefore weak when
applied to bilingual sentences from very non-parallel quasi-comparable corpora.
Document re-matching. This step implements a ?find-one-get-more? principle,
by augmenting the earlier matched documents with document pairs that are
found to contain at least one parallel sentence pair. We further find other doc-
uments that are similar to each of the monolingual documents found. The algo-
rithm then iterates to refine document matching and parallel sentence extraction.
Convergence. The IBM model parameters, including sentence alignment score
and word alignment scores, are computed in each iteration. The parameter values
eventually stay unchanged and the set of extracted bi-sentence candidates also
converges to a fixed size. The iteration then terminates and returns the last set
of bilingual sentence pairs as the generated candidate sentences.
5 ITG Scoring
The ITG model computes scores upon the set of candidates generated in the
preceding stage. A variant of the approach used by Leusch et al (2003) [16]
allows us to forego training to estimate true probabilities; instead, rules are
simply given unit weights. This allows the scores computed by ITG biparsing to
be interpreted as a generalization of classical Levenshtein string edit distance,
where inverted block transpositions are also allowed. Even without probability
estimation, Leusch et al found excellent correlation with human judgment of
similarity between translated paraphrases.
As mentioned earlier, biparsing for ITGs can be accomplished efficiently in
polynomial time, rather than the exponential time required for classical SDTGs.
The biparsing algorithm employs a dynamic programming approach described
by Wu [2]. The time complexity of the algorithm in the general case is ?
(
T 3V 3
)
where T and V are the lengths of the two sentences. This is a factor of V 3 more
266 D. Wu and P. Fung
than monolingual chart parsing, but has turned out to remain quite practical
for corpus analysis, where parsing need not be real-time.
6 Experiments
Method. For our experiments we extracted the bi-sentences from a very non-
parallel, quasi-comparable corpus of TDT3 data which consists of transcriptions
of news stories from radio and TV broadcasts in both English and Chinese chan-
nels during the period 1998-2000. This corpus contained approximately 290,000
English sentences and 110,000 Chinese sentences. This yields over 30 billion
possible sentence pairs, so a multi-stage approach is clearly necessary.
Experience showed that the lexicon learned in the candidate generation stage,
while adequate for candidate generation, is not of sufficient quality for biparsing
due to the non-parallel nature of the training data. However, any translation
lexicon of reasonable accuracy can be used. For these experiments we employed
the LDC Chinese-English dictionary 2.0.
To conduct as blind an evaluation as possible, an independent annotator
separately produced gold standard labels for a random sample of approximately
300 of the top 2,500 candidate sentence pairs proposed by the generation stage.
The annotator was instructed to accept any semantically equivalent translations,
including non-literal ones. Inspection had shown that sentence pair candidates
longer than about 15 words were practically never truly parallel translations,
so these were a priori excluded by the sampling in order to ensure that preci-
sion/recall scores would be more meaningful.
Results. Under our method any desired tradeoff between precision and recall
can be obtained. Therefore, rather than arbitrarily setting a threshold, we are
interested in evaluation metrics that can show whether the ITG model is highly
effective at any desired tradeoff points. Thus, we assess the contribution of ITG
ranking by computing standard uninterpolated average precision scores used to
evaluate the effectiveness of ranking methods. Specifically, in this case, this is
the expected value of precision over the rank positions of the correctly identified
truly parallel bi-sentences:
uninterpolated average precision =
1
| T |
?
i?T
precision at rank (i) (1)
where T is the set of correctly identified bi-sentences.
Our method yielded an uninterpolated average precision of 64.7%. No di-
rect comparison of this figure is possible since previous work has focused on
the rather different objectives of mining noisy parallel or comparable corpora
to extract comparable sentence pairs and loose translations. However, we can
understand the improvement by comparing against scores obtained using the
cosine-based lexical similarity metric which is typical of the majority of previ-
ous methods for mining non-parallel corpora, including that of Fung and Cheung
(2004)[9]. Evaluating the ranking produced under this more typical score yielded
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 267
Fig. 4. Precision-recall curves for the ITG model (upper curve) versus traditional cosine
model (lower curve); see text
an uninterpolated average precision of 24.6%. This suggests that the ITG based
method could produce significant accuracy gains if applied to many of the ex-
isting non-parallel corpus mining methods.
Figure 4 compares precision versus recall curves obtained with rankings from
the ITG model compared with the more traditional cosine lexical similarity
model. The graph reveals that at all levels, much higher precision can be obtained
using the ITG model. Up to 20% recall, the ITG ranking produces bi-sentences
with perfect precision; in contrast, the cosine model produces 30% precision.
Even at 50% recall, the ITG ranked bi-sentences have above 65% precision, as
compared with 21% for the cosine model.
As can be seen from the following examples of extracted bi-sentences (shown
with rough word glosses), the ITG constraints are able to accommodate nested
inversions accounting for the cross-linguistic differences in constituent order:
7 Conclusion
We have introduced a new method that exploits generic bracketing Inversion
Transduction Grammars giving the first known results for the new task of min-
ing truly parallel sentences from very non-parallel quasi-comparable corpora.
It is time to break the silence.
?( b , / S4 ?? ? ? ? 
(Now topical , is break silence genitive time aspectual .)
I think that?s what people were saying tonight.
 ?: ? / ?? ?Z @ ? ?? 
(I think this is people today by say genitive words .)
If the suspects are convicted, they will serve their time in Scotland.
?? $  ?? ? ? $ 	j , 1 ? ( ?<p  
(If two classifier suspected person bei-particle sentence guilty, then must in Scot-
land serve time .)
268 D. Wu and P. Fung
The method takes the strong language universal constraint posited by the ITG
Hypothesis as an inductive bias on the bi-sentence extraction task which we
anticipate will become a key stage in unsupervised learning for numerous more
specific models. Experiments show that the method obtains large accuracy gains
on this task compared to the performance that could be expected if state-of-the-
art models for the less stringent task of mining comparable sentence pairs were
applied to this task instead. From a practical standpoint, the method has the
dual advantages of neither requiring expensive training nor requiring language-
specific grammatical resources, while producing high accuracy results.
References
1. Wu, D.: An algorithm for simultaneously bracketing parallel texts by aligning
words. In: ACL-95, Cambridge, MA (1995)
2. Wu, D.: Stochastic inversion transduction grammars and bilingual parsing of par-
allel corpora. Computational Linguistics 23 (1997)
3. Zens, R., Ney, H.: A comparative study on reordering constraints in statistical
machine translation. In: ACL-03, Sapporo (2003) 192?202
4. Zhang, H., Gildea, D.: Syntax-based alignment: Supervised or unsupervised? In:
COLING-04, Geneva (2004)
5. Yamada, K., Knight, K.: A syntax-based statistical translation model. In: ACL-01,
Toulouse, France (2001)
6. Zhang, H., Gildea, D.: Stochastic lexicalized inversion transduction grammar for
alignment. In: ACL-05, Ann Arbor (2005) 475?482
7. Zens, R., Ney, H., Watanabe, T., Sumita, E.: Reordering constraints for phrase-
based statistical machine translation. In: COLING-04, Geneva (2004)
8. Chiang, D.: A hierarchical phrase-based model for statistical machine translation.
In: ACL-05, Ann Arbor (2005) 263?270
9. Fung, P., Cheung, P.: Mining very-non-parallel corpora: Parallel sentence and
lexicon extraction via bootstrapping and em. In: EMNLP-2004, Barcelona (2004)
10. Munteanu, D.S., Fraser, A., Marcu, D.: Improved machine translation performance
via parallel sentence extraction from comparable corpora. In: NAACL-04. (2004)
11. Zhao, B., Vogel, S.: Adaptive parallel sentences mining from web bilingual news
collections. In: IEEE Workshop on Data Mining. (2002)
12. Lewis, P.M., Stearns, R.E.: Syntax-directed transduction. Journal of the Associa-
tion for Computing Machinery 15 (1968) 465?488
13. Fung, P., Liu, X., Cheung, C.S.: Mixed-language query disambiguation. In: ACL-
99, Maryland (1999)
14. Och, F.J., Ney, H.: Improved statistical alignment models. In: ACL-2000, Hong
Kong (2000)
15. Brown, P.F., DellaPietra, S.A., DellaPietra, V.J., Mercer, R.L.: The mathematics
of statistical machine translation. Computational Linguistics 19 (1993) 263?311
16. Leusch, G., Ueffing, N., Ney, H.: A novel string-to-string distance measure with
applications to machine translation evaluation. In: MT Summit IX. (2003)
Evaluating the Word Sense Disambiguation Performance of Statistical
Machine Translation
Marine CARPUAT Dekai WU
 
marine@cs.ust.hk dekai@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
Abstract
We present the first known empirical test of
an increasingly common speculative claim,
by evaluating a representative Chinese-to-
English SMT model directly on word sense
disambiguation performance, using standard
WSD evaluation methodology and datasets
from the Senseval-3 Chinese lexical sam-
ple task. Much effort has been put in de-
signing and evaluating dedicated word sense
disambiguation (WSD) models, in particu-
lar with the Senseval series of workshops.
At the same time, the recent improvements
in the BLEU scores of statistical machine
translation (SMT) suggests that SMT mod-
els are good at predicting the right transla-
tion of the words in source language sen-
tences. Surprisingly however, the WSD ac-
curacy of SMT models has never been eval-
uated and compared with that of the dedi-
cated WSD models. We present controlled
experiments showing the WSD accuracy of
current typical SMT models to be signifi-
cantly lower than that of all the dedicated
WSD models considered. This tends to sup-
port the view that despite recent speculative
claims to the contrary, current SMT models
do have limitations in comparison with ded-
icated WSD models, and that SMT should
benefit from the better predictions made by
the WSD models.
1The authors would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research
in part through grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
1 Introduction
Word sense disambiguation or WSD, the task of iden-
tifying the correct sense of a word in context, is a cen-
tral problem for all natural language processing ap-
plications, and in particular machine translation: dif-
ferent senses of a word translate differently in other
languages, and resolving sense ambiguity is needed to
identify the right translation of a word.
Much work has been done in building dedicated
WSD models. The recent Senseval series of work-
shop promoted controlled comparison of very differ-
ent WSD models with common accuracy metrics and
common data sets. These efforts yielded steady im-
provements in WSD accuracy, but for WSD evalu-
ated as a standalone task. Senseval focuses on the
evaluation of standalone, generic WSD models, even
though many application-specific systems?machine
translation, information retrieval, and so on?all per-
form WSD either explicitly or implicitly.
Since the Senseval models have been built and op-
timized specifically to address the WSD problems,
they typically use richer disambiguating information
than SMT systems. This, however, raises the question
of whether the sophisticated WSD models are in fact
needed in practice.
In many machine translation architectures, in partic-
ular most current statistical machine translation (SMT)
models, the WSD problem is typically not explicitly
addressed. However, recent progress in machine trans-
lation and the continuous improvement on evaluation
metrics such as BLEU (Papineni et al, 2002) suggest
that SMT systems are already very good at choosing
correct word translations. BLEU score with low order
n-grams can be seen as an evaluation of the transla-
tion adequacy, which suggests that as SMT systems
achieve higher BLEU score, their ability to disam-
biguate word translations improves.
In other work, we have been conducting compara-
tive studies testing whether state-of-the-art WSD mod-
120
els can improve SMT translation quality (Carpuat and
Wu, 2005). Using a state-of-the-art Chinese word
sense disambiguation model to choose translation can-
didates for a typical IBM statistical MT system, we
found that word sense disambiguation does not yield
significantly better translation quality than the statis-
tical machine translation system alone. The surpris-
ing difficulty of this challenge might suggest that SMT
models are sufficiently strong at word level disam-
biguation on their own, and has recently encouraged
speculation that SMT performs WSD as well as the
dedicated WSD models.
The studies described in this paper are aimed at di-
rectly testing this increasingly common speculation.
The comparison of SMT and WSD strengths is not ob-
vious; there are strong arguments in support of both
the WSD and the SMT models. A controlled empiri-
cal comparison is therefore needed to better assess the
strengths and weaknesses of each type of model on the
WSD task.
We therefore propose to evaluate statistical machine
translation models on a WSD task, in terms of standard
WSD accuracy metrics. This addresses the inverse,
complementary question to the other study mentioned
above (of whether WSD models can help SMT sys-
tems in terms of machine translation quality metrics).
Senseval provides a good framework for this evalu-
ation, and allows a direct comparison of the perfor-
mance of the SMT model with state-of-the-art WSD
models on a common dataset. We built a Chinese-
to-English SMT system using freely available toolkits,
and show that it does not perform as well as the WSD
models specifically built for this task.
2 Statistical machine translation vs.
word sense disambiguation
We begin by examining the respective strengths and
weaknesses of full SMT models versus dedicated
WSD models, which might be expected to be relevant
to the WSD task.
2.1 Features unique to SMT
Unlike lexical sample WSD models, SMT models si-
multaneously translate complete sentences rather than
isolated target words. The lexical choices are made in
a way that heavily prefers phrasal cohesion in the out-
put target sentence, as scored by the language model.
That is, the predictions benefit from the sentential con-
text of the target language. This has the general effect
of improving translation fluency.
Another major difference with most lexical sample
WSD models is that SMT models are always unsu-
pervised. SMT models learn from large sets of bisen-
tences but the correct word alignment between the two
sentences is unknown. SMT models cannot therefore
directly exploit sense-annotated data, or at least not as
easily as classification-based WSD models do.
2.2 Features unique to WSD
Dedicated WSD is typically cast as a classification
task with a predefined sense inventory. Sense distinc-
tions and granularity are often manually predefined,
which means that they can be adapted to the task at
hand, but also that the translation candidates are lim-
ited to an existing set.
To improve accuracy, dedicated WSD models typ-
ically employ features that are not limited to the lo-
cal context, and that include more linguistic informa-
tion than the surface form of words. For example, a
typical dedicated WSD model might employ features
as described by Yarowsky and Florian (2002) in their
?feature-enhanced naive Bayes model?, with position-
sensitive, syntactic, and local collocational features.
The feature set made available to the WSD model to
predict lexical choices is therefore much richer than
that used by a statistical MT model.
Also, dedicated WSD models can be supervised,
which yields significantly higher accuracies than unsu-
pervised. For the experiments described in this study
we employed supervised training, exploiting the an-
notated corpus that was produced for the Senseval-3
evaluation.
Again, this brief comparison shows that both mod-
els have important and very different strengths, which
motivates our controlled empirical comparison of their
WSD performance.
3 The SMT system
To build a representative baseline SMT system, we
restricted ourselves to making use of freely available
tools.
3.1 Alignment model
The alignment model was trained with GIZA++ (Och
and Ney, 2003), which implements the most typical
IBM and HMM alignment models. Translation qual-
ity could be improved using more advanced hybrid
phrasal or tree models, but this would interfere with
the questions being investigated here. The alignment
model used is IBM-4, as required by our decoder. The
training scheme is IBM-1, HMM, IBM-3 and IBM-4,
as specified in (Och and Ney, 2003).
The training corpus consists of about 1 million sen-
tences from the United Nations Chinese-English par-
allel corpus from LDC. This corpus was automatically
sentence-aligned, so the training data does not require
as much manual annotation as for the WSD model.
121
3.2 Language model
The English language model is a trigram model trained
on the Gigaword newswire data and on the English
side of the UN and Xinhua parallel corpora. The lan-
guage model is also trained using a publicly available
software, the CMU-Cambridge Statistical Language
Modeling Toolkit (Clarkson and Rosenfeld, 1997).
3.3 Decoding
The ISI ReWrite decoder (Germann, 2003), which
implements an efficient greedy decoding algorithm,
is used to translate the Chinese sentences, using the
alignment model and language model previously de-
scribed.
Notice that very little contextual information is
available to the IBM SMT models. Lexical choice
during decoding essentially depends on the translation
probabilities learned for the target word, and on the
English language model scores.
4 The WSD system
The WSD system used here is based on the model that
achieved the best performance on the Senseval-3 Chi-
nese lexical sample task, outperforming other systems
by a large margin (Carpuat et al, 2004).
The model consists of an ensemble of four highly
accurate classifiers combined by majority vote: a naive
Bayes classifier, a maximum entropy model (Jaynes,
1978), a boosting model (Freund and Schapire, 1997),
and a Kernel PCA-based model (Wu et al, 2004),
which has the advantage of having a signficantly dif-
ferent bias. All these classifiers have the ability to han-
dle large numbers of sparse features, many of which
may be irrelevant. Moreover, the maximum entropy
and boosting models are known to be well suited to
handling features that are highly interdependent.
The feature set used consists of position-sensitive,
syntactic, and local collocational features, as described
by Yarowsky and Florian (2002).
5 Experimental method
5.1 Senseval-3 Chinese lexical sample task
The Senseval-3 Chinese lexical sample task includes
20 target word types. For each word type, several
senses are defined using the HowNet knowledge base.
There are an average of 3.95 senses per target word
type, ranging from 2 to 8. Only about 37 training in-
stances per target word are available.
The dedicated WSD models described in Section 4
are trained to predict HowNet senses for a set of new
occurrences of the target word in context.
We use the SMT sytem described in Section 3 to
translate the Chinese sentences of the Senseval evalua-
tion test set, and extract the translation chosen for each
of the target word occurrences. In order to evaluate
the predictions of the SMT model just like any WSD
model, we need to map the English translations to
HowNet senses. This mapping is done using HowNet,
which provides English glosses for each of the senses
of every Chinese word.
Note that Senseval-3 also defined a translation or
multilingual lexical sample task (Chklovski et al,
2004), which is just like the English lexical sample
task, except that the WSD systems are expected to
predict Hindi translations instead of WordNet senses.
This translation task might seem to be a more natural
evaluation framework for SMT than the monolingual
Chinese lexical sample task. However, in practice,
there is very little data available to train an English-
to-Hindi SMT model, which would significantly hin-
der its performance and bias the study in favor of the
dedicated WSD models.
5.2 Allowing the SMT model to exploit the
Senseval data
Comparing the Senseval WSD models with a regu-
lar SMT model is not entirely fair, since, unlike the
SMT model, the dedicated WSD models are trained
and evaluated on similar data. We address this prob-
lem by adapting our SMT model to the lexical sample
task domain in two ways.
First, we augment the training set of the SMT model
with the Senseval training data. Since the training set
consists of sense-annotated Chinese sentences, and not
of Chinese-English bisentences, we artificially create
sentence pairs for each training instance, where the
Chinese sentence consists of the Chinese target word,
and the English sense is the English gloss given by
HowNet for that particular target word and HowNet
sense.
Second, we restrict the translation candidates con-
sidered by the decoder for the target words to the set
of all the English glosses given by HowNet for all the
senses of the target word considered. With this mod-
ification, the degree of ambiguity faced by the SMT
model is closer to that of the WSD model.
Table 1 shows that each of these modifications help
the accuracy, overall yielding a 28.9% relative im-
provement over the regular SMT system.
6 Results
Table 2 summarizes the results of the SMT and WSD
models on the Senseval-3 Chinese lexical sample task.
Note that the accuracy of the most frequent sense
baseline is extremely low, which shows that the eval-
uation set contains instances that are particularly diffi-
cult to disambiguate. All our SMT and WSD models
significantly outperform this baseline.
122
Table 1: Evaluation of the different variations of the SMT model on the Senseval-3 Chinese Lexical Sample task.
System Accuracy
SMT 25.6
SMT with augmented training set 26.9
SMT with augmented training set and restricted translation candidates 33.0
Table 2: Accuracy of all our SMT and WSD models on the Senseval-3 Chinese Lexical Sample task.
System Accuracy
Most Frequent Sense Baseline 7.7
Best SMT system 33.0
UMD unsupervised system 44.5
WSD naive Bayes 60.4
WSD KPCA 63.6
WSD Boosting 64.1
WSD Maximum Entropy 64.4
WSD Ensemble (current best Senseval-3 model) 66.2
6.1 The dedicated supervised WSD models all
significantly outperform SMT
Table 2 clearly shows that even the best of the SMT
model considered performs significantly worse than
any of the dedicated WSD models considered. The
accuracy of the best Senseval-3 system is double the
accuracy of the best SMT model.
Since the best Senseval-3 system is a classifier en-
semble that benefits from the predictions of four indi-
vidual WSD models which have very different biases,
we also compare the performance of the SMT model
with that of the individual WSD models. All the in-
dividual component WSD models, including the sim-
plest naive Bayes model, also significantly outperform
the SMT model.
6.2 The SMT model prefers phrasal cohesion in
the output sentences to WSD accuracy
Inspection of the output reveals that the main cause of
errors is that the SMT model tends to prefer phrasal co-
hesion to word translation adequacy: lexical choice is
essentially performed by the English language model,
therefore translations are primarily chosen to preserve
phrasal cohesion in the output sentence, and only local
context is used. We will give three different examples
to illustrate this effect.
The Chinese word ?
 
?(huodong) has the senses
?move/exercise? vs. ?act?. A Chinese sentence is in-
correctly translated as ?the party leadership which de-
velop the constitution and laws and in constitutional
and legal framework exercise?. Here, ?exercise? is
not the right translation, ?act? should be used instead.
However, the language model prefers the use of the
phrase ?legal framework exercise?, where the word
?exercise? is used in a different sense than the one
meant in the ?move/exercise? category. Note that
choosing the wrong translation for this word not only
affects the adequacy, but also the grammaticality and
fluency of the translated sentence.
In one of the target sentences, the SMT model has to
choose between two translations for the Chinese word
?  ? (cailiao): ?data? or ?material?. The two clos-
est left neighbors can be translated as ?provide proof?,
and the SMT incorrectly picks the ?material? sense,
because the phrase ?provide proof of material...? is
more frequent than ?provide proof of data?. In con-
trast, the WSD model has access to a wider context to
correctly pick the ?data? translation.
Similarly, in a test sentence where the Chinese
word ?  ? (fengzi) is used in the sense ?ele-
ment/component? vs. ?member?, the SMT system in-
correctly chooses the ?member? translation because
the neighboring word translates to ?active?, and the
language model prefers the phrase ?active member? to
?active element? or ?active component?.
6.3 WSD models are consistently better than
SMT models for all target word types
Computing accuracies per target word type shows that
the previous observations hold for each target word.
Table 3 compares the accuracies of the best SMT
vs. the best WSD system per target word type and
shows that the WSD system always yields significantly
higher scores for the set of target words considered.
Also this breakdown reveals, interestingly, that the
123
Table 3: Accuracy of the best SMT and best WSD models for each target word type in the Senseval-3 Chinese
Lexical Sample task.
Target word SMT accuracy WSD accuracy
 
38.5 53.8
 62.5 81.2

13.8 66.6

29.4 64.7

16.7 58.3

15.8 89.5
	
40.0 66.7


 28.6 61.9
 40.0 60.0

66.7 60.0
  
43.7 62.5

53.3 80.0

28.6 57.1

40.0 73.3

26.9 57.7

20.8 62.5

25.0 55.0
Statistical Machine Translation Part II: Tree-Based SMT 
 Dekai WU 
Human Language Technology Center 
Hong Kong University of Science and Technology (HKUST) 
Clear Water Bay, Hong Kong 
dekai@cs.ust.hk 
 
 
 
Abstract 
One of the most active and promising areas of 
statistical machine translation (SMT) research 
are tree-based SMT approaches. Tree-based 
SMT has the potential to overcome the 
weaknesses of early SMT architectures which (a) 
do not handle long-distance dependencies well, 
and (b) are underconstrained in that they allow 
too much flexibility in word reordering. 
In this tutorial, we will review the various 
possible approaches to tree-based SMT, ranging 
from the original Inversion Transduction 
Grammar (ITG) models to later models such as 
alignment templates, dependency models, tree-
to-string models, tree-to-tree models, and also 
probabilistic EBMT models. We will discuss the 
theoretical relationships between approaches, 
with critical analysis of their strengths and 
weaknesses. Within this framework we will 
survey the emerging comparative results from 
intriguing new large-scale empirical studies 
across various language pairs. We will consider 
what kind of constraints and biases can or 
should be imposed by models on the variation 
between unrelated human languages, and how 
this can facilitate efficient algorithms for a wide 
range of tasks in machine learning and 
processing of language. We will consider both 
scientific and engineering implications, and 
investigate the potential relationships to cross-
language universals. 
Biography 
Prof. Wu received his PhD in Computer Science 
from the University of California at Berkeley, 
and was a postdoctoral fellow at the University 
of Toronto (Ontario, Canada) prior to joining 
HKUST in 1992. He received a BS in Computer 
Engineering from the University of California at 
San Diego (Revelle College departmental award, 
cum laude, Phi Beta Kappa) in 1984 and an 
Executive MBA from Kellogg and HKUST in 
2002. He has been a visiting researcher at 
Columbia University in 1995-96, Bell 
Laboratories in 1995, and the Technische 
Universit?t M?nchen (Munich, Germany) during 
1986-87. Prof. Wu serves as Associate Editor of 
ACM Transactions on Speech and Language 
Processing, Machine Translation, Journal of 
Natural Language Engineering, and 
Communications of COLIPS. He has also served 
as Co-Chair for EMNLP-2004, on the Editorial 
Board of Computational Linguistics, the 
Organizing Committee of ACL-2000 and 
WVLC-5 (SIGDAT 1997), and the Executive 
Committee of the Association for Computational 
Linguistics (ACL). 
 
 
 
276
Using N-best Lists for Named Entity Recognition from Chinese Speech 
Lufeng ZHAI*, Pascale FUNG*, Richard SCHWARTZ?, Marine CARPUAT?, Dekai WU? 
 
* HKUST 
Human Language Technology Center 
Electrical & Electronic Engineering 
University of Science and Technology 
Clear Water Bay, Hong Kong 
{lfzhai,pascale}@ee.ust.hk 
? BBN Technologies 
9861 Broken Land Parkway  
Columbia, MD 21046 
                     U.S.A 
schwartz@bbn.com 
? HKUST 
Human Language Technology Center 
Department of Computer Science 
University of Science and Technology 
Clear Water Bay, Hong Kong 
{marine,dekai}@cs.ust.hk 
 
Abstract 
We present the first known result for named 
entity recognition (NER) in realistic large-
vocabulary spoken Chinese.  We establish this 
result by applying a maximum entropy model, 
currently the single best known approach for 
textual Chinese NER, to the recognition 
output of the BBN LVCSR system on Chinese 
Broadcast News utterances. Our results 
support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese 
than for English.  We propose re-segmenting 
the ASR hypotheses as well as applying post-
classification to improve the performance. 
Finally, we introduce a method of using n-best 
hypotheses that yields a small but nevertheless 
useful improvement NER accuracy.  We use 
acoustic, phonetic, language model, NER and 
other scores as confidence measure.  
Experimental results show an average of 6.7% 
relative improvement in precision and 1.7% 
relative improvement in F-measure. 
1. Introduction 
Named Entity Recognition (NER) is the first step for 
many tasks in the fields of natural language processing 
and information retrieval. It is a designated task in a 
number of conferences, including the Message 
Understanding Conference (MUC), the Information 
Retrieval and Extraction Conference (IREX), the 
Conferences on Natural Language Learning (CoNLL) 
and the recent Automatic Content Extraction 
Conference (ACE).   
 
There has been a considerable amount of work on 
English NER yielding good performance (Tjong Kim 
Sang et al 2002, 2003; Cucerzan & Yarowsky 1999; 
Wu et al 2003). However, Chinese NER is more 
difficult, especially on speech output, due to two 
reasons. First, Chinese has a large number of homonyms 
and the vocabulary used in Chinese person names is an 
open set so more characters/words are unseen in the 
training data. Second, there is no standard definition of 
Chinese words. Word segmentation errors made by 
recognizers may lead to NER errors. Previous work on 
Chinese textual NER includes Jing et al (2003) and Sun 
et al (2003) but there has been no published work on 
NER in spoken Chinese.  
 
Named Entity Recognition for speech is more difficult 
than for text, since the most reliable features for textual 
NER (punctuation, capitalization, and syntactic 
patterns) are often not available in speech output. NER 
on automatically recognized broadcast news was first 
conducted by MITRE in 1997, and was subsequently 
added to Hub-4 evaluation as a task. Palmer et al 
(1999) used error modeling, and Horlock & King (2003) 
proposed discriminative training to handle NER errors; 
both used a hidden Markov model (HMM). Miller et al 
(1999) also reported results in English speech NER 
using an HMM model. In a NIST 1999 evaluation, it 
was found that NER errors on speech arise from a 
combination of ASR errors and errors of the underlying 
NER system. 
 
In this work, we investigate whether the NIST finding 
holds for Chinese speech NER as well. We present the 
first known result for recognizing named entities in 
realistic large-vocabulary spoken Chinese. We propose 
to use the best-known model for Chinese textual NER?
a maximum entropy model?on Chinese speech NER. 
We also propose using re-segmentation and post-
classification to improve this model. Finally, we 
propose to integrate the ASR and NER components to 
optimize NER performance by making use of the n-best 
ASR output.  
2. A Spoken Chinese NER Model 
2.1 LVCSR output  
We use the ASR output from BBN?s Byblos system on  
broadcast news data from the Xinhua News Agency, 
which has 1046 sentences. This system has a character 
error rate of 7%. We had manually annotated them with 
named entities as an evaluation set according to the PFR 
corpus annotation guideline (PFR 2001). 
2.2 A maximum-entropy NER model with post- 
classification  
To establish a baseline spoken Chinese NER model, we 
selected a maximum entropy (MaxEnt) approach since 
this is currently the single most accurate approach 
known for recognizing named entities in text (Tjong 
Kim Sang et al, 2002, 2003, Jing et al, 2003)1. In the 
CoNLL 2003 NER evaluation, 5 out of 16 systems use 
MaxEnt models and the top 3 results for English and top 
2 results for German were obtained by systems that use 
MaxEnt.  
 
Natural language can be viewed as a stochastic process. 
We can use p(y|x) to denote the probability distribution 
of what we try to predict y (.e.g. part-of-speech tag, 
Named Entity tag) conditioned on what we observe x 
(e.g. previous POS or the actual word). The Maximum 
Entropy principle can be stated as follows: given some 
set of constrains from observations, find the most 
uniform probability distribution (Maximum Entropy) 
p(y|x) that satisfies these constrains:  
0
0 0
* arg max ( | )
1( | ) exp( ( , ))
( )
( ) exp( ( , ))
yi i i
m
i i j j i i
ji
l m
i j j i k
k j
y P y x
P y x f x y
Z x
Z x f x y
?
?
=
= =
=
= ?
= ?
?
? ?
 
In the above equations, fj(xi,yk) is a binary valued feature 
function, and ?j is a weight that indicates how important 
feature fj is for the model. Z(xi) is a normalization factor. 
We estimate the weights using the improved iterative 
scaling (IIS) algorithm.  
 
For our task, we first compare a character-based 
MaxEnt model to a word-based model. Since 
recognition errors also lead to segmentation errors 
which in turn have an adverse effect on the NER 
performance, we experiment with disregarding the word 
boundaries in the ASR hypothesis and instead re-
segment using a MaxEnt segmenter. We also compare 
an approach of one-pass identification/classification to a 
two-pass approach where the identified NE candidates 
are classified later. In addition, we propose a hybrid 
approach of using one-pass identification/classification 
results, discarding the extracted NE tags, and re-
classifying the extracted NE in a second pass.  
                                                 
1 We exclude from the present focus the slight improvements 
that are usually possible to obtain by combination of multiple 
models, usually through ad hoc methods such as voting. 
2.3 Experimental setup  
We use two annotated corpora for training. One is a 
corpus of People?s Daily newspaper from January 1998, 
annotated by the Institute of Computational Linguistics 
of Beijing University (the ?PFR? corpus). This corpus 
consists of about 20k sentences, annotated with word 
segmentation, part-of-speech tags and three named-
entity tags including person (PER), location (LOC) and 
organization (ORG) . We use the first 6k sentences to 
train our NER system. Our system is then evaluated on 
2k sentences from People?s Daily and 1k sentences from 
the BBN ASR output. The results are shown in Tables 1 
and 3. 
  
To compare our system to the IBM baseline described 
in (Jing et al 2003), we need to evaluate our system on 
the same corpus as they used. Among the data they used, 
the only publicly available corpus is a human-generated 
transcription of broadcast news, provided by NIST for 
the Information Extraction ? Entity Recognition 
evaluation (the ?IEER? corpus). This corpus consists of 
10 hours of training data and 1 hour of test data. Ten 
categories of NEs were annotated, including person 
names, location, organization, date, duration, and 
measure. A comparison of results is shown in Table 2. 
2.4 Results and discussion 
From text to speech 
Table 1 compares the NER performances of the same 
MaxEnt model on the Chinese textual PFR test data and 
the one-best BBN ASR hypotheses. We can see a 
significant drop in performance in the latter. These 
results support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese than for 
English.  We argue that this is due to the combination of 
different factors specific to spoken Chinese. First, 
Chinese has a large number of homonyms that leads to a 
degradation in speech recognition accuracy which in 
turn leads to low NER accuracy. Second, the vocabulary 
used in Chinese person names is an open set so many 
characters/words are unseen in the training data.  
 
Comparison to IBM baseline 
Table 2 compares results on IEER data from our 
baseline word-based MaxEnt model compared with that 
of IBM?s HMM word-based model. These two models 
achieved almost the same results, which show that our 
NER system based on MaxEnt is state-of-the-art.  
 
Re-segmentation effect 
Table 3 shows that by discarding word boundaries from 
the ASR hypothesis, and then re-segmenting using our 
MaxEnt segmenter, we obtained a better performance in 
most cases. We believe that some reduction in 
segmentation errors due to recognition errors is obtained 
this way; for example, in the ASR output, two words 
??  ? ? in ???  ?  ?  ???   ?  ? ? are 
misrecognized as one word ????, which can be 
corrected by re-segmentation. 
 
Post-classification effect 
Table 3 also shows that the one-pass 
identification/classification method yields better results 
than the two-pass method. However, there are still 
errors in the one-pass output where the bracketing is 
correct, but the NE classification is wrong. In particular, 
the type ORG is easily confusable with LOC in Chinese. 
Both types of NEs tend to be rather long. We propose a 
hybrid approach by first using the one-pass method to 
extract NEs, and then removing all type information, 
combining words of one NE to a whole NE-word and 
post-classifying all the NE-words again. Our results in 
Figure 1 show that the post-classification combined 
with the one-pass approach performs much better on all 
types. 
Table 1. NER results on Chinese speech data are worse 
than on Chinese text data. 
Table 2. Our NER baseline is comparable to the IBM 
baseline. 
Table 3. The character model is better than the word 
model, and one-pass NER is better than two-pass. 
3. Using N-Best Lists to Improve NER 
Miller et al (1999) performed NER on the one-best 
hypothesis of English Broadcast News data.  Palmer & 
Ostendorf (2001) and Horlock & King (2003) carried 
out English NER on word lattices. We are interested in 
investigating how to best utilize the n-best hypothesis 
from the ASR system to improve NER performances. 
From Figure 1, we can see that recall increases as the 
number of hypotheses increases. Thus it would appear 
possible to find a way to make use of the n-best ASR 
output, in order to improve the NER performance. 
However, we can expect it to be difficult to get 
significant improvement since the same figure (Figure 1) 
shows that precision drops much more quickly than 
recall. This is because the nth hypothesis tends to have 
more character errors than the (n-1)th hypothesis, which 
may lead to more NER errors. Therefore the question is, 
given n NE-tagged hypotheses, what is the best way to 
use them to obtain a better NER overall performance 
than by using the one-best hypothesis alone? 
                                                                                                                
One simple approach is to allow all the hypotheses to 
vote on a possible NE output. In simple voting, a 
recognized named-entity is considered correct only 
when it appears in more than 30 percent of the total 
number of all the hypotheses for one utterance. The 
result of this simple voting is shown in Table 4. Next, 
we propose a mechanism of weighted voting using 
confidence measure for each hypothesis. In one 
experiment, we use the MaxEnt NER score as 
confidence measure. In another experiment, we use all 
the six scores (acoustic, language model, number of 
words, number of phones, number of silence, or NER 
score) provided by the BBN ASR system as confidence 
measure. During implementation, an optimizer based on 
Powell?s algorithm is used to find the 6 weights (?k) for 
each score (Sk). For any given hypothesis, confidence 
measure is given by: 
PER LOC ORG  P R F P R F P R F 
Newspaper 
text .86 .76 .81 .87 .75 .81 .83 .83 .83 
1-best ASR 
hypothesis  .22 .18 .20 .75 .79 .77 .43 .35 .39 
Model Precision Recall F-measure
IBM HMM 77.51% 65.22% 70.83% 
MaxEnt 77.3% 65.4% 70.9% 
6
1
k k
k
W S ?
=
= ??  
The above confidence measure is then normalized into a 
final confidence measure for each hypothesis: 
1
exp( )?
exp( )
i
i N
l
l
WW
W
=
=
?
 
Finally, an NE output is considered valid if  
1
? ( ) 0.
N
i i
i
W NE?
=
? >? 3  
1,
( )
0,i
NE occurs in the i th hypothesis
NE
Otherwise
? ??= ??
 
PER LOC ORG  
P R F P R F P R F 
2-pass, 
word  .23  .18  .20  .75  .79  .77  .43 .35 .39  
1-pass, 
word .25  .20  .21  .76  .84  .80  .70 .25 .36  
2-pass, 
character  .53  .43  .48  .67  .70  .68  .75 .59 .66 
1-pass, 
character  .60  .45  .52  .56  .69  .62  .55 .35 .43 
3.1 Experimental setup 
We use the n-best hypothesis of 1,046 Broadcast News 
Chinese utterances from the BBN LVCSR system. n 
ranges from one to 300, averaging at 68. Each utterance 
has a reference transcription with no recognition error. 
3.2 Results and discussion 
Table 4 presents the NER results for the reference 
sentence, one best hypothesis, and different n-best 
voting methods. Results for the reference sentences 
show the upper bound performance (68% F-measure) of 
applying a MaxEnt NER system trained from the 
Chinese text corpus (e.g., PFR) to Chinese speech 
output (e.g., Broadcast News). From Table 4, we can 
conclude that it is possible to improve NER precision by 
using n-best hypothesis by finding the optimized 
combination of different acoustic, language model, 
NER, and other scores. In particular, since most errors 
in Chinese ASR seem to be for person names, using 
NER score on the n-best hypotheses can improve 
recognition results by a relative 6.7% in precision and 
1.7% in F-measure.   
 
PER LOC ORG Results 
F P F P F P 
Reference 
sentence 
0.71 0.75 0.78 0.77 0.56 0.72 
One best 0.46 0.50 0.75 0.74 0.54 0.69 
n-best  
simple vote 
0.45 0.59 0.76 0.75 0.56 0.71 
n-best 
weighted vote 
(NE score) 
0.46 0.59 0.77 0.76 0.55 0.71 
n-best 
weighted vote 
(all scores) 
0.48 0.53 0.75 0.73 0.55 0.69 
 
Table 4. n-best weighted voting with NE score gives the 
best performance.                                                                                                   
Recall 
 
Precision 
 
F-measure 
 
Figure 1. Post-classification improves NER performance. 
4. Conclusion 
We present the first known result for named entity 
recognition (NER) in realistic large-vocabulary spoken 
Chinese.  We apply a maximum entropy (MaxEnt) 
based system to the n-best output of the BBN LVCSR 
system on Chinese Broadcast News utterances. Our 
results support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese than for 
English.  We show that re-segmenting the ASR 
hypotheses improves the NER performance by 24%. We 
also show that applying post-classification improves the 
NER performance by 13%. Finally, we introduce a 
method of using n-best hypotheses that yields a useful 
6.7% relative improvement in NER precision, and 1.7% 
relative improvement in F-measure, by weighted voting.  
 
Institute of Computational Linguistics, Beijing University. 2001. The PFR 
corpus.  http://icl.pku.edu.cn/research/corpus/shengming.htm. 
Hongyan JING, Radu FLORIAN, Xiaoqiang LUO, Tong ZHANG and Abraham 
ITTYCHERIAH. 2003. HowtogetaChineseName(Entity): Segmentation and 
combination issues. Proceedings of EMNLP. Sapporo, Japan: July 2003. 
 
David MILLER, Richard SCHWARTZ, Ralph WEISCHEDEL and Rebecca STONE. 
1999. Named entity extraction from broadcast news. Proceedings of the 
DARPA Broadcast News Workshop. Herndon, Virginia: 1999. 37-40. 
 David D. PALMER, Mari OSTENDORF and John D. BURGER. 1999. Robust 
information extraction from spoken language data. Proceedings of 
Eurospeech 1999. Sep 1999. 
Acknowledgements. We would like to thank the Hong Kong 
Research Grants Council (RGC) for supporting this research 
in part via grants HKUST6206/03E, HKUST6256/00E, 
HKUST6083/99E, DAG03/04.EG30, and DAG03/04.EG09.  
Jian SUN, Ming ZHOU and Jianfeng GAO. 2003. A class-based language model 
approach to Chinese named entity identification. Computational Linguistics 
and Chinese Language Processing. 2003. 
References Erik F. TJONG KIM SANG. 2002. Introduction to the CoNLL-2002 Shared Task: Language-independent named entity recognition. Proceedings of CoNLL-
2002. Taipei, Taiwan: 2002. 155-158.  
Silviu CUCERZAN and David YAROWSKY. 1999. Language independent named 
entity recognition combining morphological and contextual evidence. 
Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC. 
University of Maryland, MD. 
Erik F. TJONG KIM SANG and Fien DE MEULDER. 2003. Introduction to the 
CoNLL-2003 Shared Task: Language-Independent Named Entity 
Recognition. Proceedings of CoNLL-2003. Edmonton, Canada. 142-147.  
James HORLOCK and Simon KING. 2003. Discriminative Methods for Improving 
Named Entity Extraction on Speech Data. Proceedings of Eurospeech 2003. 
Geneva. 
Dekai WU, Grace NGAI and Marine CARPUAT. 2003. A Stacked, Voted, Stacked 
Model for Named Entity Recognition. Proceedings of CoNLL-2003. 
Edmonton, Canada: 2003. 200-203.  
Proceedings of NAACL HLT 2009: Short Papers, pages 13?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semantic Roles for SMT: A Hybrid Two-Pass Model Dekai WU1          Pascale FUNG2 Human Language Technology Center HKUST 1Department of Computer Science and Engineering 2Department of Electronic and Computer Engineering University of Science and Technology, Clear Water Bay, Hong Kong dekai@cs.ust.hk    pascale@ee.ust.hk  Abstract We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation.  The approach avoids major complexity limitations via a two-pass architecture.  The first pass is per-formed using a conventional phrase-based SMT model.  The second pass is performed by a re-ordering strategy guided by shallow se-mantic parsers that produce both semantic frame and role labels.  Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline ? to our knowledge, the first successful application of semantic role labeling to SMT. 1 Introduction Many of the most glaring errors made by to-day?s statistical machine translation systems are those resulting from confusion of semantic roles.  Translation errors of this type frequently result in critical misunderstandings of the essential meaning of the original input language sentences ? who did what to whom, for whom or what, how, where, when, and why. Semantic role confusions are errors of adequacy rather than fluency.  It has often been noted that the dominance of lexically-oriented, precision-based metrics such as BLEU (Papineni et al 2002) tend to reward fluency more than adequacy.  The length penalty in the BLEU metric, in particular, is only an indirect and weak indicator of adequacy.  As a result, SMT work has been driven to optimize 
systems such that they often produce translations that contain significant role confusion errors de-spite reading fluently. The present work is inspired by the question of whether we can improve translation utility via a strategy of favoring semantic adequacy slightly more ? possibly at the expense of slight degrada-tions in lexical fluency. Shallow semantic parsing models have attained increasing levels of accuracy in recent years (Gildea and Jurafsky 2000; Sun and Jurafsky 2004; Pradhan et al 2004, 2005; Pradhan 2005; Fung et al 2006, 2007; Gim?nez and M?rquez 2007a, 2008).  Such models, which identify semantic frames within input sentences by marking its predicates, and labeling their arguments with the semantic roles that they fill. Evidence has begun to accumulate that semantic frames ? predicates and semantic roles ? tend to preserve consistency across translations better than syntactic roles do.  This is, of course, by design; it follows from the definition of semantic roles, which are less language-dependent than syntactic roles.  Across Chinese and English, for example, it has been reported that approximately 84% of se-mantic roles are preserved consistently (Fung et al 2006).  Of these, roughly 15% do not preserve syn-tactic roles consistently. Since this directly targets the task of determin-ing semantic correctness, we believe that the ade-quacy of MT output could be improved by leveraging the predictions of semantic parsers.  We would like to exploit automatic semantic parsers to identify inconsistent semantic frame and role map-pings between the input source sentences and their output translations. However, we take note of the difficult experi-ence in making syntactic and semantic models con-
13
tribute to improving SMT accuracy.  On the one hand, there is reason to be optimistic.  Over the past decade, we have seen an accumulation of evi-dence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accu-racy, in the form of techniques adapted from word sense disambiguation models (Chan et al 2007; Gim?nez and M?rquez  2007b; Carpuat and Wu 2007). On the other hand, both directions saw un-expected disappointments along the way (e.g., Och et al 2003; Carpuat and Wu 2005).  We are there-fore forewarned that it is likely to be at least as difficult to successfully adapt the even more com-plex types of lexical semantics modeling from se-mantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, success-fully applies semantic parsing technology to the challenge of improving the quality of Chinese-English statistical machine translation.  The model makes use of a typical representative SMT system based on Moses, plus shallow semantic parsers for both English and Chinese. 2 Hybrid two-pass semantic SMT While the accuracy of shallow semantic parsers has been approaching reasonably high levels in recent years for well-studied languages like Eng-lish, and to a lesser extent, Chinese, the problem of excessive computational complexity is one of the primary challenges in adapting semantic parsing technology to the translation task. Semantic parses, by definition, are less likely than syntactic parses to obey clearly nested hierar-chical composition rules.  Moreover, the semantic parses are less likely to share an exactly isomor-phic structure across the input and output lan-guages, since the raison d??tre of semantic parsing is to capture semantic frame and role regularities independent of syntactic variation ? monolingually and cross-lingually. This makes it difficult to incorporate semantic parsing into SMT merely by applying the sort of dynamic programming techniques found in current syntactic and tree-structured SMT models, most of which rely on being able to factor the computation 
into independent computations on the subtrees.  In other words, the key computational obstacle is that the semantic parse of a larger string (or string pair, in the case of translation) is not in general strictly mechanically composable from the semantic parses of its smaller substrings (or substring pairs). In fact, the lack of easy compositionality is the reason that today?s most accurate shallow semantic parsers rely not primarily on compositional parsing techniques, but rather on ensembles of predictors that independently rate/rank a wide variety of fac-tors supporting the role assignments given a broad sentence-wide range of context features.  But while this improves semantic parsing accuracy, it poses a major obstacle for efficient tight integration into the sub-hypothesis construction and maintenance loops within SMT decoders. To circumvent this computational obstacle, the hybrid two-pass model defers application of the non-compositional semantic parsing information until a second error-correcting pass.  This imposes a division of labor between the two passes.   1. Apply a semantic parser for the input language to the input source sentence.  2. Apply a semantic parser for the output language to the baseline translation that was output by the first pass.  Note: this also pro-duces a shallow syntactic parse as a byproduct.  3. If the semantic frames (target predicates and their associated semantic roles) are all consistent between the input and output sentences, and are aligned to each other by the phrase alignments from the first pass, then finish immediately and output the base-line translation.  4. Segment the baseline translation by introducing segment boundaries around every constituent phrase whose shallow syn-tactic parse category (from step 2) was V, NP, or PP.  This breaks the baseline translation into a small number of coarse chunks to consider during re-ordering, instead of a large number of individual words.  5. Generate a set of candidate re-ordered translation hypotheses by iteratively moving constituent phrases whose predicate or se-mantic role label was mismatched to the input sentence.  Each new candidate generated may in turn spawn a further set of can-didates (especially since moving one constituent phrase may cause another?s predicate or semantic role label to change from matched to mismatched).  This search is performed breadth-first to favor fewer re-orderings (in case the hypothesis generation grows beyond allotted time).  6. Apply a semantic parser for the output language to each candi-date re-ordered translation hypothesis as it is generated.   7. Return the re-ordered translation hypothesis with the maximum match of semantic predicates and arguments.  Figure 1.  Algorithm for second pass. 
14
 Figure 2.  Example, showing translations after SMT first pass and after re-ordering second pass.  The first pass is performed using a conventional phrase-based SMT model. The phrase-based SMT model is assigned to the tasks of (a) providing an initial baseline hypothesis translation, and (b) fix-ing the lexical choice decisions.  Note that the lexi-cal choice decisions are not only at the single-word level, but are in general at the phrasal level. The second pass takes the output of the first pass, and re-orders constituent phrases correspond-ing to semantic predicates and arguments, seeking to maximize the cross-lingual match of the seman-tic parse of the re-ordered translation to that of the original input sentence.  The second pass algorithm performs the error correction shown in Figure 1. The design decision to allow the first pass to fix all lexical choices follows an insight inspired by an empirical observation from our error analyses:  the lexical choice decisions being made by today?s SMT models have attained fairly reasonable levels, and are not where the major problems of adequacy lie.  Rather, the ordering of arguments in relation to their predicates is often where the main failures of adequacy occur.  By avoiding lexical choice variations while considering re-ordering hypothe-ses, a significantly larger amount of re-ordering can be done without further increasing computa-tional complexity.  So we sacrifice a small amount of fluency by allowing re-ordering without com-pensating lexical choice ? in exchange for gaining potentially a larger amount of fluency by getting the predicate-argument structure right. The model has a similar rationale for employing a re-ordering pass instead of re-ranking n-best lists or lattices.  Oracle analysis of n-best lists and lat-tices show that they often focus on lexical choice alternatives rather than re-ordering / role variations which are more important to semantic adequacy. 
3 Experiment A Chinese-English experiment was conducted on the two-pass hybrid model. A phrase-based SMT baseline model was built by augmenting the open source statistical machine translation decoder Moses (Koehn et al 2007) with additional pre-processors.  English and Chinese shallow semantic parsers followed those discussed in Section 1. The model was trained on LDC newswire paral-lel text consisting of 3.42 million sentence pairs, containing 64.1 million English words and 56.9 million Chinese words. The English was tokenized and case-normalized; the Chinese was tokenized via a maximum-entropy model (Fung et al 2004). Phrase translations were extracted via the grow-diag-final heuristic. The language model is a 6-gram model trained with Kneser-Ney smoothing using the SRI lan-guage modeling toolkit (Stolcke 2002). The test set of Wall Street Journal newswire sentences was randomly extracted from the Chi-nese-English Bilingual Propbank.  Although we did not make use of the Propbank annotations, this would potentially allow other types of analyses in the future. The phrase-based SMT model used for the first pass achieves a BLEU score of 42.99, establishing a fairly strong baseline to begin with. In comparison, the automatically error-corrected translations that are output by the second pass achieve a BLEU score of 43.51.  This repre-sents approximately half a point improvement over the strong baseline. An example is seen in Figure 2.  The SMT first pass translation has an ARG0 National Develop-ment Bank of Japan in the capital market which is badly mismatched to both the input sentence?s 
15
ARG0 ?? ?? ?? and ARGM-LOC ? ?? ?? ??.  The second pass ends up re-ordering the constituent phrase corresponding to the mis-matched ARGM-LOC, of Japan in the capital market, to follow the PRED issued, where the new English semantic parse now assigns most of its words the correctly matched ARGM-LOC seman-tic role label.  Similarly, samurai bonds 30 billion yen is re-ordered to 30 billion yen samurai bonds. 4 Discussion and conclusion To our knowledge, this is a first result demonstrat-ing that shallow semantic parsing can improve translation accuracy of SMT models.  We note that accuracy here was measured via BLEU, and it has been widely observed that the negative impacts of semantic predicate-argument errors on the utility of the translation are underestimated by evaluation metrics based on lexical criteria such as BLEU. We conjecture that more expensive manual evalua-tion techniques which directly measure translation utility could even more strongly reveal improve-ment in role confusion errors. The hybrid two-pass approach can be compared with the greedy re-ordering based strategy of the ReWrite decoder (Germann et al 2001), although our search is breadth-first rather than purely greedy.  Whereas ReWrite was based on word-level re-ordering, however, our approach is based on constituent phrase re-ordering, and the phrases to be re-ordered are more selectively chosen via the semantic parse labels.  Moreover, the objective function being maximized by ReWrite is still the SMT model score; whereas in our case the new objective function is cross-lingual semantic predi-cate-argument match (plus an implicit search bias toward fewer re-orderings). The hybrid two-pass approach can also be com-pared with serial combination architectures for hy-brid MT (e.g., Ueffing et al 2008).  But whereas Ueffing et al take the output from a first-pass rule-based MT system, and then correct it using a sec-ond-pass SMT system, our two-pass semantic SMT model does the reverse: it takes the output from a first-pass SMT system, and then corrects it with the aid of semantic analyzers.  Acknowledgments.  Thanks to Chi-kiu Lo and Zhaojun Wu.  This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants GRF621008, GRF612806, DAG03/04.EG09, RGC6256/00E, and RGC6083/99E. 
References Marine Carpuat and Dekai Wu. 2005. Word sense disambiguation vs. statistical machine translation. 43rd Annual Meeting of the Association for Computa-tional Linguistics (ACL-2005). Ann Arbor, MI: Jun 2005. Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007). Prague: Jun 2007. 61-72. Yee Seng Chan, Hwee Tou Ng and David Chiang 2007. Word sense disam-biguation improves statistical machine translation. 45th Annual Meeting of the Association for Computational Linguistics (ACL-07), Prague: Jun 2007. Pascale Fung, Grace Ngai, Yongsheng Yang and Benfeng Chen. 2004. A maxi-mum-entropy Chinese parser augmented by transformation-based learning. ACM Transactions on Asian Language Information Processing (TALIP) 3(2): 159-168. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu. 2006. Automatic learning of Chinese/English semantic structure mapping. IEEE/ACL 2006 Workshop on Spoken Language Technology (SLT 2006). Aruba: Dec 2006. 230-233. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu. 2007. Learning Bilingual Semantic Frames: Shallow Semantic Parsing vs. Semantic Role Projection.  11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007). Sk?vde, Sweden: Sep 2007. 75-84. Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu and Kenji Yamada. Fast Decoding and Optimal Decoding for Machine Translation. 2001. 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001). Toulouse: July 2001. Daniel Gildea and Daniel Jurafsky. 2000. Automatic Labeling of Semantic Roles. 38th Annual Conference of the Association for Computational Lin-guistics (ACL-2000). 512?520, Hong Kong: Oct 2000. Jes?s Gim?nez and Llu?s M?rquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. WMT 2007 (ACL'07). Jes?s Gim?nez and Llu?s M?rquez. 2007. Discriminative Phrase Selection for Statistical Machine Translation. Learning Machine Translation. NIPS Workshop Series. MIT Press.  Jes?s Gim?nez and Llu?s M?rquez. 2008. A Smorgasbord of Features for Auto-matic MT Evaluation. 3rd ACL Workshop on Statistical Machine Transla-tion (shared evaluation task). Pages 195-198, Columbus, Ohio: Jun 2008. Alessandro Moschitti and Roberto Basili. 2005. Verb subcategorization kernels for automatic semantic labeling. ACL-SIGLEX Workshop on Deep Lexical Acquisition. Ann Arbor: Jun 2005. 10-17. Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. Human Language Technology Conference of the North American Chapter of the Association for Computa-tional Linguistics (HLT/NAACL-2004). Boston: May 2004. Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002). Sameer Pradhan. 2005. ASSERT: Automatic Statistical SEmantic Role Tagger. http://oak.colorado.edu/assert/. Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin and Daniel Jurafsky. 2005. Support Vector Learning for Semantic Argument Classification. Machine Learning 60(1-3): 11-39. Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin and Daniel Juraf-sky. 2004. Shallow Semantic Parsing using Support Vector Machines. Hu-man Language Technology/North American Chapter of the Association for Computational Linguistics (HLT/NAACL-2004). Boston: May 2004. Andreas Stolcke. 2002. SRILM ? An Extensible Language Modeling Toolkit. International Conference on Spoken Language Processing (ICSLP-2002). Denver, Colorado: Sep 2002. Honglin Sun and Daniel Jurafsky. 2004. Shallow Semantic Parsing of Chinese. Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL-2004). 249-256. Boston: May 2004.  Nicola Ueffing, Jens Stephan, Evgeny Matusov, Lo?c Dugast, George Foster, Roland Kuhn, Jean Senellart and Jin Yang. 2008. Tighter Integration of Rule-based and Statistical MT in Serial System Combination. 22nd Interna-tional Conference on Computational Linguistics (COLING 2008). Manches-ter: Aug 2008. Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual parsing of parallel corpora. Computational Linguistics 23(3): 377-404. Dekai Wu and David Chiang (eds). 2009. Proceedings of SSST-3, Third Work-shop on Syntax and Structure in Statistical Translation (NAACL-HLT 2009). Boulder, CO: Jun 2009. Nianwen Xue and Martha Palmer. 2005. Automatic Semantic Role Labeling for Chinese Verbs. 19th International Joint Conference on Artificial Intelli-gence. Edinburgh, Scotland. 
16
An Information-Theory-Based Feature Type Analysis for the
Modelling of Statistical Parsing
SUI Zhifang ??, ZHAO Jun ?, Dekai WU ?
? Hong Kong University of Science & Technology
Department of Computer Science
Human Language Technology Center
Clear Water Bay, Hong Kong
? Peking University
Department of Computer Science & Technology
Institute of Computational Linguistics
Beijing, China
suizf@icl.pku.edu.cn, zhaojun@cs.ust.hk, dekai@cs.ust.hk
Abstract
The paper proposes an information-theory-
based method for feature types analysis in
probabilistic evaluation modelling for
statistical parsing. The basic idea is that we
use entropy and conditional entropy to
measure whether a feature type grasps some
of the information for syntactic structure
prediction. Our experiment quantitatively
analyzes several feature types? power for
syntactic structure prediction and draws a
series of interesting conclusions.
1  Introduction
In the field of statistical parsing, various
probabilistic evaluation models have been
proposed where different models use different
feature types [Black, 1992] [Briscoe, 1993]
[Brown, 1991] [Charniak, 1997] [Collins, 1996]
[Collins, 1997] [Magerman, 1991] [Magerman,
1992] [Magerman, 1995] [Eisner, 1996]. How to
evaluate the different feature types? effects for
syntactic parsing? The paper proposes an
information-theory-based feature types analysis
model, which uses the measures of predictive
information quantity, predictive information
gain, predictive information redundancy and
predictive information summation to
quantitatively analyse the different contextual
feature types? or feature types combination?s
predictive power for syntactic structure.
  In the following, Section 2 describes the
probabilistic evaluation model for syntactic trees;
Section 3 proposes an information-theory-based
feature type analysis model; Section 4
introduces several experimental issues; Section 5
quantitatively analyses the different contextual
feature types or feature types combination in the
view of information theory and draws a series of
conclusion on their predictive powers for
syntactic structures.
2  The probabilistic evaluation model
for statistical syntactic parsing
Given a sentence, the task of statistical syntactic
parsing is to assign a probability to each
candidate parsing tree that conforms to the
grammar and select the one with highest
probability as the final analysis result. That is:
)|(maxarg STPT
T
best =  (1)
where S denotes the given sentence, T denotes
the set of all the candidate parsing trees that
conform to the grammar, P(T|S) denotes the
probability of parsing tree T for the given
sentence S.
  The task of probabilistic evaluation model in
syntactic parsing is the estimation of P(T|S). In
the syntactic parsing model which uses rule-
based grammar, the probability of a parsing tree
can be defined as the probability of the
derivation which generates the current parsing
tree for the given sentence. That is,
?
?
=
=
?
=
=
=
n
i
ii
n
i
ii
n
ShrP
SrrrrP
SrrrPSTP
1
1
121
21
),|(
),,,,|(
)|,,,()|(


(2)
Where, 121 ,,, ?irrr   denotes a derivation rule
sequence, hi denotes the partial parsing tree
derived from 121 ,,, ?irrr  .
  In order to accurately estimate the parameters,
we need to select some feature types
mFFF ,,, 21  , depending on which we can
divide the contextual condition Shi ,  for
predicting rule ri into some equivalence classes,
that is, ],[, ,,, 21 ShSh iFFFi m???? ??  , so that
??
==
?
n
i
ii
n
i
ii ShrPShrP
11
]),[|(),|(  (3)
According to the equation of (2) and (3), we
have the following equation:
?
=
?
n
i
ii ShrPSTP
1
]),[|()|(  (4)
  In this way, we can get a unite expression of
probabilistic evaluation model for statistical
syntactic parsing. The difference among the
different parsing models lies mainly in that they
use different feature types or feature type
combination to divide the contextual condition
into equivalent classes. Our ultimate aim is to
determine which combination of feature types is
optimal for the probabilistic evaluation model of
statistical syntactic parsing. Unfortunately, the
state of knowledge in this regard is very limited.
Many probabilistic evaluation models have been
published inspired by one or more of these
feature types [Black, 1992] [Briscoe, 1993]
[Charniak, 1997] [Collins, 1996] [Collins, 1997]
[Magerman, 1995] [Eisner, 1996], but
discrepancies between training sets, algorithms,
and hardware environments make it difficult, if
not impossible, to compare the models
objectively. In the paper, we propose an
information-theory-based feature type analysis
model by which we can quantitatively analyse
the predictive power of different feature types or
feature type combinations for syntactic structure
in a systematic way. The conclusion is expected
to provide reliable reference for feature type
selection in the probabilistic evaluation
modelling for statistical syntactic parsing.
3 The information-theory-based
feature type analysis model for statistical
syntactic parsing
In the prediction of stochastic events, entropy
and conditional entropy can be used to evaluate
the predictive power of different feature types.
To predict a stochastic event, if the entropy of
the event is much larger than its conditional
entropy on condition that a feature type is
known, it indicates that the feature type grasps
some of the important information for the
predicted event.
  According to the above idea, we build the
information-theory-based feature type analysis
model, which is composed of four concepts:
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation.
z Predictive Information Quantity (PIQ)
);( RFPIQ , the predictive information quantity
of feature type F to predict derivation rule R, is
defined as the difference between the entropy of
R and the conditional entropy of R on condition
that the feature type F is known.
?
?? ?
=
?=
RrFf rPfP
rfP
rfP
FRHRHRFPIQ
,
)()(
),(log),(
)|()();(
   (5)
  Predictive information quantity can be used to
measure the predictive power of a feature type in
feature type analysis.
z Predictive Information Gain (PIG)
For the prediction of rule R,
PIG(Fx;R|F1,F2,...,Fi), the predictive information
gain of taking Fx as a variant model on top of a
baseline model employing F1,F2,...,Fi as feature
type combination, is defined as the difference
between the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi
and the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi,Fx.
)6(),,,(
),,(
),,,(
),,,,(log),,,,(
),,,|(),,|(),,|;(
1
1
1
1
1
111
11
rffP
ffP
fffP
rfffP
rfffP
FFFRHFFRHFFRFPIG
i
i
xi
xi
Rr
Ff
Ff
Ff
xi
xiiix
xx
ii







?=
?=
?
?
?
?
?
 If ),,,|;(),,,|;( 2121 iyix FFFRFPIGFFFRFPIG  > ,
then Fx is deemed to be more informative than
Fy for predicting R on top of F1,F2,...,Fi, no
matter whether PIQ(Fx;R) is larger than
PIQ(Fy;R) or not.
z Predictive Information Redundancy(PIR)
Based on the above two definitions, we can
further draw the definition of predictive
information redundancy as follows.
PIR(Fx,{F1,F2,...,Fi};R) denotes the redundant
information between feature type Fx and feature
type combination {F1,F2,...,Fi} in predicting R,
which is defined as the difference between
PIQ(Fx;R) and PIG(Fx;R|F1,F2,...,Fi). That is,
),,,|;();(
)};,,,{,(
21
21
ixx
ix
FFFRFPIGRFPIQ
RFFFFPIR


?=
 (7)
  Predictive information redundancy can be
used as a measure of the redundancy between
the predictive information of a feature type and
that of a feature type combination.
z Predictive Information Summation (PIS)
PIS(F1,F2,...,Fm;R), the predictive information
summation of feature type combination
F1,F2,...,Fm, is defined as the total information
that F1,F2,...,Fm can provide for the prediction of
a derivation rule. Exactly,
?
=
?
+=
m
i
ii
m
FFRFPIGRFPIQ
RFFFPIS
2
111
21
),,|;();(
);,,,(


 (8)
4 Experimental Issues
4.1 The classification of the feature
types
The predicted event of our experiment is the
derivation rule to extend the current non-
terminal node. The feature types for prediction
can be classified into two classes, history feature
types and objective feature types. In the
following, we will take the parsing tree shown in
Figure-1 as the example to explain the
classification of the feature types.
In Figure-1, the current predicted event is the
derivation rule to extend the framed non-
terminal node VP, the part connected by the
solid line belongs to history feature types, which
is the already derived partial parsing tree,
representing the structural environment of the
current non-terminal node. The part framed by
the larger rectangle belongs to the objective
feature types, which is the word sequence
containing the leaf nodes of the partial parsing
tree rooted by the current node, representing the
final objectives to be derived from the current
node.
4.2 The corpus used in the experiment
The experimental corpus is derived from Penn
TreeBank[Marcus,1993]. We semi-
automatically assign a headword and a POS tag
to each non-terminal node. 80% of the corpus
(979,767 words) is taken as the training set, used
for estimating the various co-occurrence
probabilities, 10% of the corpus (133,814 words)
is taken as the testing set, used to calculate
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation. The other 10% of the corpus
(133,814 words) is taken as the held-out set. The
grammar rule set is composed of 8,126 CFG
rules extracted from Penn TreeBank.
S
V P
V P
N N P
Pierre
N N P
Vinken
M D
will
V B
join
D T
the
N N
board
IN
as
D T
a
JJ
nonexecut ive
N N
director
N N P
Nov.
C D
29
.
.
N P N P N P
P P
N P
Figure-1: The classification of feature types
4.3 The smoothing method used in the
experiment
In the information-theory-based feature type
analysis model, we need to estimate joint
probability ),,,,( 21 rfffP i . Let F1,F2,...,Fi be
the feature type series selected till now,
RrFfFfFf ii ???? ,,,, 2211  , we use a
blended probability ),,,,(~ 21 rfffP i  to
approximate probability ),,,,( 21 rfffP i  in
order to solve the sparse data problem[Bell,
1992].
  
?
=
??
++=
i
j
jj
i
rfffPwrPwrPw
rfffP
1
210011
21
),,,,()()(
),,,,(~


   (9)
In the above formula,
         
?
?
?
=
Rr
rc
rP
?
1 )?(
1)(
              (10)
  
?
?
=
Rr
rc
rc
rP
?
0 )?(
)()(                (11)
where )(rc is the total number of time that r has
been seen in the corpus.
  According to the escape mechanism in [Bell,
1992], we define the weights wk )1( ik ?<?  in
the formula (9) as follows.
   
ii
i
ks
skk
ew
ikeew
?=
????= ?
+=
1
1,)1(
1       (12)
where ek denotes the escape probability of
context ),,,( 21 kfff   , that is, the probability
in which (f1 , f2 , ... , fk , r) is unseen in the corpus.
In such case, the blending model has to escape
to the lower contexts to approximate
),,,,( 21 rfffP k . Exactly, escape probability is
defined as
  
???
???
?
?=
??
= ?
?
?
?
1,0
0,)?,,...,,(
)?,,...,,(
?
21
?
21
k
ik
rfffc
rfffd
e
Rr
k
Rr
k
k    (13)
where
   
??
?
=
>
=
0)?,,...,,(,0
0)?,,...,,(,1)?,,...,,(
21
21
21
rfffcif
rfffcif
rfffd
k
k
k  (14)
In the above blending model, a special
probability ?
?
?
=
Rr
rc
rP
?
1 )?(
1)(  is used, where all
derivation rules are given an equal probability.
As a result, 0),,,,(~ 21 >rfffP i  as long as
0)?(
?
>?
?Rr
rc .
5 The information-theory-based
feature type analysis
The experiments led to a number of interesting
conclusions on the predictive power of various
feature types and feature type combinations,
which is expected to provide reliable reference
for the modelling of probabilistic parsing.
5.1 The analysis to the predictive
information quantities of lexical feature
types, part-of-speech feature types and
constituent label feature types
z Goal
One of the most important variation in statistical
parsing over the last few years is that statistical
lexical information is incorporated into the
probabilistic evaluation model. Some statistical
parsing systems show that the performance is
improved after the lexical information is added.
Our research aims at a quantitative analysis of
the differences among the predictive information
quantities provided by the lexical feature types,
part-of-speech feature types and constituent
label feature types from the view of information
theory.
z Data
The experiment is conducted on the history
feature types of the nodes whose structural
distance to the current node is within 2.
  In Table-1, ?Y? in PIQ(X of Y; R) represents
the node, ?X? represents the constitute label, the
headword or POS of the headword of the node.
In the following, the units of PIQ are bits.
z Conclusion
Among the feature types in the same structural
position of the parsing tree, the predictive
information quantity of lexical feature type is
larger than that of part-of-speech feature type,
and the predictive information quantity of part-
of-speech feature type is larger than that of the
constituent label feature type.
Table-1: The predictive information quantity of the history feature type candidates
PIQ(X of Y; R) X= constituent label X= headword X= POS of
the headword
Y= the current node 2.3609 3.7333 2.7708
Y= the parent 1.1598 2.3253 1.1784
Y= the grandpa 0.6483 1.6808 0.6612
Y= the first right brother of the current node 0.4730 1.1525 0.7502
Y= the first left brother of the current node 0.5832 2.1511 1.2186
Y= the second right brother of the current node 0.1066 0.5044 0.2525
Y= the second left brother of the current node 0.0949 0.6171 0.2697
Y= the first right brother of the parent 0.1068 0.3717 0.2133
Y= the first left brother of the parent 0.2505 1.5603 0.6145
5.2 The analysis to the influence of the
structural relation and the structural
distance to the predictive information
quantities of the history feature types
z Goal:
In this experiment, we wish to find out the
influence of the structural relation and structural
distance between the current node and the node
that the given feature type related to has to the
predictive information quantities of these feature
types.
z Data:
In Table-2, SR represents the structural relation
between the current node and the node that the
given feature type related to. SD represents the
structural distance between the current node and
the node that the given feature type related to.
Table-2: The predictive information quantity of the selected history feature types
PIQ(constituent label
of Y; R)
SR= parent relation SR= brother relation SR= mixed parent and
brother relation
0.5832
(Y= the first left brother)
SD=1 1.1598
(Y= the parent)
0.4730
(Y= the first right brother)
0.2505
(Y= the first left brother
of the parent)
0.0949
(Y= the second left brother)
SD=2 0.6483
(Y= the grandpa)
0.1066
(Y= the second right brother)
0.1068
(Y= the first right
brother of the parent)
z Conclusion
Among the history feature types which have the
same structural relation with the current node
(the relations are both parent-child relation, or
both brother relation, etc), the one which has
closer structural distance to the current node will
provide larger predictive information quantity;
Among the history feature types which have the
same structural distance to the current node, the
one which has parent relation with the current
node will provide larger predictive information
quantity than the one that has brother relation or
mixed parent and brother relation to the current
node (such as the parent's brother node).
5.3 The analysis to the predictive
information quantities of the history
feature types and the objective feature
types
z Goal
Many of the existing probabilistic evaluation
models prefer to use history feature types other
than objective feature types. We select some of
history feature types and objective feature types,
and quantitatively compare their predictive
information quantities.
z Data
The history feature type we use here is the
headword of the parent, which has the largest
predictive information quantity among all the
history feature types. The objective feature types
are selected stochastically, which are the first
word and the second word in the objective word
sequence of the current node (Please see 4.1 and
Figure-1 for detailed descriptions on the selected
feature types).
Table-3: The predictive information quantity of the selected history and objective feature types
Class Feature type PIQ(Y;R)
History feature type Y= headword of the parent 2.3253
Y= the first word in the objective word sequence 3.2398Objective feature type
Y= the second word in the objective word sequence 3.0071
z Conclusion
Either of the predictive information quantity of
the first word and the second word in the
objective word sequence is larger than that of
the headword of the parent node which has the
largest predictive information quantity among all
of the history feature type candidates. That is to
say, objective feature types may have larger
predictive power than that of the history feature
type.
5.4 The analysis to the predictive
information quantities of the objective
features types selected respectively on the
physical position information, the
heuristic information of headword and
modifier, and the exact headword
information
z Goal
Not alike the structural history feature types, the
objective feature types are sequential. Generally,
the candidates of the objective feature types are
selected according to the physical position.
However, from the linguistic viewpoint, the
physical position information can hardly grasp
the relations between the linguistic structures.
Therefore, besides the physical position
information, our research try to select the
objective feature types respectively according to
the exact headword information and the heuristic
information of headword and modifier. Through
the experiment, we hope to find out what
influence the exact headword information, the
heuristic information of headword and modifier,
and the physical position information have
respectively to the predictive information
quantities of the feature types.
z Data:
  Table-4 gives the evidence for the claim.
Table-4: the predictive information quantity of the selected objective feature types
the information used to select the objective
feature types
PIQ(Y;R)
the physical position information 3.2398
(Y= the first word in the objective word sequence)
Heuristic information 1: determine whether a
word has the possibility to act as the headword of
the current constitute according to its POS
3.1401
(Y= the first word in the objective word sequence
which has the possibility to act as the headword of
the current constitute)
Heuristic information 2: determine whether a
word has the possibility to act as the modifier of
the current constitute according to its POS
3.1374
(Y= the first word in the objective word sequence
which has the possibility to act as the modifier of the
current constitute)
Heuristic information 3: given the current
headword, determine whether a word has the
possibility to modify the headword
2.8757
(Y= the first word in the objective word sequence
which has the possibility to modify the headword)
the exact headword information 3.7333
(Y= the headword of the current constitute)
z Conclusion
The predictive information quantity of the
headword of the current node is larger than that
of a feature type selected according to the
selected heuristic information of headword or
modifier, and larger than that of a feature type
selected according to the physical positions; The
predictive information quantity of a feature type
selected according to the physical positions is
larger than that of a feature types selected
according to the selected heuristic information
of headword or modifier.
5.5 The selection of the feature type
combination which has the optimal
predictive information summation
z Goal:
We aim at proposing a method to select the
feature types combination that has the optimal
predictive information summation for prediction.
z Approach
We use the following greedy algorithm to select
the optimal feature type combination.
  In building a model, the first feature type to
be selected is the feature type which has the
largest predictive information quantity for the
prediction of the derivation rule among all of the
feature type candidates, that is,
);(maxarg1 RFPIQF i
Fi ??
=    (15)
Where ?  is the set of candidate feature types.
  Given that the model has selected feature type
combination jFFF ,,, 21  , the next feature
type to be added into the model is the feature
type which has the largest predictive information
gain in all of the feature type candidate except
jFFF ,,, 21  , on condition that jFFF ,,, 21 
is known. That is,
)16(),,,|;( 21
},,2,1{
1 maxarg ji
jFFFiF
iF
j FFFRFPIGF 
?
??
+ =
z Data:
Among the feature types mentioned above, the
optimal feature type combination (i.e. the feature
type combination with the largest predictive
information summation) which is composed of 6
feature types is, the headword of the current
node (type1), the headword of the parent node
(type2), the headword of the grandpa node
(type3), the first word in the objective word
sequence(type4), the first word in the objective
word sequence which have the possibility to act
as the headword of the current constitute(type5),
the headword of the right brother node(type6).
The cumulative predictive information
summation is showed in Figure-2
0
1
2
3
4
5
6
7
type1 type2 type3 type4 type5 type6
feature type
cu
m
m
u
la
tiv
e 
pr
ed
ic
tin
g 
in
fo
rm
at
io
n
su
m
m
at
io
n
Figure-2: The cumulative predictive information summation of the feature type combinations
6 Conclusion
The paper proposes an information-theory-based
feature type analysis method, which not only
presents a series of heuristic conclusion on the
predictive power of the different feature types
and feature type combination for syntactic
parsing, but also provides a guide for the
modeling of syntactic parsing in the view of
methodology, that is, we can quantitatively
analyse the different contextual feature types or
feature types combination's effect for syntactic
structure prediction in advance. Based on these
analysis, we can select the feature type or feature
types combination that has the optimal
predictive information summation to build the
probabilistic parsing model.
  However, there are still some questions to be
answered in this paper. For example, what is the
beneficial improvement in the performance after
using this method in a real parser? Whether the
improvements in PIQ will lead to the
improvement of parsing accuracy or not? In the
following research, we will incorporate these
conclusions into a real parser to see whether the
parsing accuracy can be improved or not.
Another work we will do is to do some
experimental analysis to find the impact of data
sparseness on feature type analysis, which is
critical to the performance of real systems.
  The proposed feature type analysis method
can be used in not only the probabilistic
modelling for statistical syntactic parsing, but
also language modelling in more general fields
[WU, 1999a] [WU, 1999b].
References
Bell, T.C., Cleary, J.G., Witten,I.H. 1992. Text
Compression, PRENTICE HALL, Englewood
Cliffs, New Jersey 07632, 1992
Black, E., Jelinek, F.,Lafferty, J.,Magerman, D.M.,
Mercer, R. and Roukos, S. 1992.  Towards
history-based grammars: using richer models of
context in probabilistic parsing. In Proceedings of
the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
Brown, P., Jelinek, F., & Mercer, R. 1991. Basic
method of probabilistic context-free grammars.
IBM internal Report, Yorktown Heights, NY.
T.Briscoe and J. Carroll. 1993. Generalized LR
parsing of natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1): 25-60
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statics. In
Proceedings of the Fourteenth National Conference
on Artificial Intelligence, AAAI Press/MIT Press,
Menlo Park.
Stanley F. Chen and Joshua Goodman. 1999.  An
Empirical Study of Smoothing Techniques for
Language Modeling. Computer Speech and
Language, Vol.13, 1999
Michael John Collins. 1996.  A new statistical
parser based on bigram lexical dependencies. In
Proceedings of the 34th Annual Meeting of the
ACL.
Michael John Collins. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
ACL.
J.Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In
Proceedings of COLING-96, pages 340-345
Joshua Goodman. 1998. Parsing Inside-Out. PhD.
Thesis, Harvard University, 1998
Magerman, D.M. and Marcus, M.P. 1991. Pearl: a
probabilistic chart parser. In Proceedings of the
European ACL Conference, Berlin, Germany.
Magerman, D.M. and Weir, C. 1992. Probabilistic
prediction and Picky chart parsing. In Proceedings
of the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33th
Annual Meeting of the ACL.
Mitchell P. Marcus, Beatrice Santorini & Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank.
Computational Linguistics 19, pages 313-330
C. E. Shannon. 1951. Prediction and Entropy of
Printed English. Bell System Technical Journal,
1951
Dekai,Wu, Sui Zhifang, Zhao Jun. 1999a. An
Information-Based Method for Selecting Feature
Types for Word Prediction. Proceedings of
Eurospeech'99, Budapest Hungary
Dekai, Wu, Zhao Jun, Sui Zhifang. 1999b. An
Information-Theoretic Empirical Analysis of
Dependency-Based Feature Types for Word
Prediction Models. Proceedings of EMNLP'99,
University of Maryland, USA
A Kernel PCA Method for Superior Word Sense Disambiguation
Dekai WU1 Weifeng SU Marine CARPUAT
dekai@cs.ust.hk weifeng@cs.ust.hk marine@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
Abstract
We introduce a new method for disambiguating
word senses that exploits a nonlinear Kernel Prin-
cipal Component Analysis (KPCA) technique to
achieve accuracy superior to the best published indi-
vidual models. We present empirical results demon-
strating significantly better accuracy compared to
the state-of-the-art achieved by either na??ve Bayes
or maximum entropy models, on Senseval-2 data.
We also contrast against another type of kernel
method, the support vector machine (SVM) model,
and show that our KPCA-based model outperforms
the SVM-based model. It is hoped that these highly
encouraging first results on KPCA for natural lan-
guage processing tasks will inspire further develop-
ment of these directions.
1 Introduction
Achieving higher precision in supervised word
sense disambiguation (WSD) tasks without resort-
ing to ad hoc voting or similar ensemble techniques
has become somewhat daunting in recent years,
given the challenging benchmarks set by na??ve
Bayes models (e.g., Mooney (1996), Chodorow et
al. (1999), Pedersen (2001), Yarowsky and Flo-
rian (2002)) as well as maximum entropy models
(e.g., Dang and Palmer (2002), Klein and Man-
ning (2002)). A good foundation for comparative
studies has been established by the Senseval data
and evaluations; of particular relevance here are
the lexical sample tasks from Senseval-1 (Kilgarriff
and Rosenzweig, 1999) and Senseval-2 (Kilgarriff,
2001).
We therefore chose this problem to introduce
an efficient and accurate new word sense disam-
biguation approach that exploits a nonlinear Kernel
PCA technique to make predictions implicitly based
on generalizations over feature combinations. The
1The author would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research
in part through grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
technique is applicable whenever vector represen-
tations of a disambiguation task can be generated;
thus many properties of our technique can be ex-
pected to be highly attractive from the standpoint of
natural language processing in general.
In the following sections, we first analyze the po-
tential of nonlinear principal components with re-
spect to the task of disambiguating word senses.
Based on this, we describe a full model for WSD
built on KPCA. We then discuss experimental re-
sults confirming that this model outperforms state-
of-the-art published models for Senseval-related
lexical sample tasks as represented by (1) na??ve
Bayes models, as well as (2) maximum entropy
models. We then consider whether other kernel
methods?in particular, the popular SVM model?
are equally competitive, and discover experimen-
tally that KPCA achieves higher accuracy than the
SVM model.
2 Nonlinear principal components and
WSD
The Kernel Principal Component Analysis tech-
nique, or KPCA, is a nonlinear kernel method
for extraction of nonlinear principal components
from vector sets in which, conceptually, the n-
dimensional input vectors are nonlinearly mapped
from their original space Rn to a high-dimensional
feature space F where linear PCA is performed,
yielding a transform by which the input vectors
can be mapped nonlinearly to a new set of vectors
(Scho?lkopf et al, 1998).
A major advantage of KPCA is that, unlike other
common analysis techniques, as with other kernel
methods it inherently takes combinations of pre-
dictive features into account when optimizing di-
mensionality reduction. For natural language prob-
lems in general, of course, it is widely recognized
that significant accuracy gains can often be achieved
by generalizing over relevant feature combinations
(e.g., Kudo and Matsumoto (2003)). Another ad-
vantage of KPCA for the WSD task is that the
dimensionality of the input data is generally very
Table 1: Two of the Senseval-2 sense classes for the target word ?art?, from WordNet 1.7 (Fellbaum 1998).
Class Sense
1 the creation of beautiful or significant things
2 a superior skill
large, a condition where kernel methods excel.
Nonlinear principal components (Diamantaras
and Kung, 1996) may be defined as follows. Sup-
pose we are given a training set of M pairs (xt, ct)
where the observed vectors xt ? Rn in an n-
dimensional input space X represent the context of
the target word being disambiguated, and the cor-
rect class ct represents the sense of the word, for
t = 1, ..,M . Suppose ? is a nonlinear mapping
from the input space Rn to the feature space F .
Without loss of generality we assume the M vec-
tors are centered vectors in the feature space, i.e.,
?M
t=1 ?(xt) = 0; uncentered vectors can easily
be converted to centered vectors (Scho?lkopf et al,
1998). We wish to diagonalize the covariance ma-
trix in F :
C = 1M
M
?
j=1
?(xj) ?T (xj) (1)
To do this requires solving the equation ?v = Cv
for eigenvalues ? ? 0 and eigenvectors v ? F . Be-
cause
Cv = 1M
M
?
j=1
(?(xj) ? v)? (xj) (2)
we can derive the following two useful results. First,
? (?(xt) ? v) = ? (xt) ? Cv (3)
for t = 1, ..,M . Second, there exist ?i for i =
1, ...,M such that
v =
M
?
i=1
?i?(xi) (4)
Combining (1), (3), and (4), we obtain
M?
M
?
i=1
?i (?(xt) ? ?(xi ))
=
M
?
i=1
?i(? (xt) ?
M
?
j=1
?(xj)) (?(xj) ? ?(xi ))
for t = 1, ..,M . Let K? be the M ? M matrix such
that
K?ij = ?(xi) ? ?(xj) (5)
and let ??1 ? ??2 ? . . . ? ??M denote the eigenval-
ues of K? and ??1 ,..., ??M denote the corresponding
complete set of normalized eigenvectors, such that
??t(??t ? ??t) = 1 when ??t > 0. Then the lth nonlinear
principal component of any test vector xt is defined
as
ylt =
M
?
i=1
??li (?(xi) ? ?(xt )) (6)
where ??li is the lth element of ??l .
To illustrate the potential of nonlinear principal
components for WSD, consider a simplified disam-
biguation example for the ambiguous target word
?art?, with the two senses shown in Table 1. Assume
a training corpus of the eight sentences as shown
in Table 2, adapted from Senseval-2 English lexical
sample corpus. For each sentence, we show the fea-
ture set associated with that occurrence of ?art? and
the correct sense class. These eight occurrences of
?art? can be transformed to a binary vector represen-
tation containing one dimension for each feature, as
shown in Table 3.
Extracting nonlinear principal components for
the vectors in this simple corpus results in nonlinear
generalization, reflecting an implicit consideration
of combinations of features. Table 3 shows the first
three dimensions of the principal component vectors
obtained by transforming each of the eight training
vectors xt into (a) principal component vectors zt
using the linear transform obtained via PCA, and
(b) nonlinear principal component vectors yt using
the nonlinear transform obtained via KPCA as de-
scribed below.
Similarly, for the test vector x9, Table 4 shows the
first three dimensions of the principal component
vectors obtained by transforming it into (a) a princi-
pal component vector z9 using the linear PCA trans-
form obtained from training, and (b) a nonlinear
principal component vector y9 using the nonlinear
KPCA transform obtained obtained from training.
The vector similarities in the KPCA-transformed
space can be quite different from those in the PCA-
transformed space. This causes the KPCA-based
model to be able to make the correct class pre-
diction, whereas the PCA-based model makes the
Table 2: A tiny corpus for the target word ?art?, adapted from the Senseval-2 English lexical sample corpus
(Kilgarriff 2001), together with a tiny example set of features. The training and testing examples can be
represented as a set of binary vectors: each row shows the correct class c for an observed vector x of five
dimensions.
TRAINING design/N media/N the/DT entertainment/N world/N Class
x1 He studies art in London. 1
x2 Punch?s weekly guide to
the world of the arts,
entertainment, media and
more.
1 1 1 1
x3 All such studies have in-
fluenced every form of art,
design, and entertainment
in some way.
1 1 1
x4 Among the techni-
cal arts cultivated in
some continental schools
that began to affect
England soon after the
Norman Conquest were
those of measurement
and calculation.
1 2
x5 The Art of Love. 1 2
x6 Indeed, the art of doc-
toring does contribute to
better health results and
discourages unwarranted
malpractice litigation.
1 2
x7 Countless books and
classes teach the art of
asserting oneself.
1 2
x8 Pop art is an example. 1
TESTING
x9 In the world of de-
sign arts particularly, this
led to appointments made
for political rather than
academic reasons.
1 1 1 1
wrong class prediction.
What permits KPCA to apply stronger general-
ization biases is its implicit consideration of com-
binations of feature information in the data dis-
tribution from the high-dimensional training vec-
tors. In this simplified illustrative example, there
are just five input dimensions; the effect is stronger
in more realistic high dimensional vector spaces.
Since the KPCA transform is computed from unsu-
pervised training vector data, and extracts general-
izations that are subsequently utilized during super-
vised classification, it is quite possible to combine
large amounts of unsupervised data with reasonable
smaller amounts of supervised data.
It can be instructive to attempt to interpret this
example graphically, as follows, even though the
interpretation in three dimensions is severely limit-
ing. Figure 1(a) depicts the eight original observed
training vectors xt in the first three of the five di-
mensions; note that among these eight vectors, there
happen to be only four unique points when restrict-
ing our view to these three dimensions. Ordinary
linear PCA can be straightforwardly seen as pro-
jecting the original points onto the principal axis,
Table 3: The original observed training vectors (showing only the first three dimensions) and their first three
principal components as transformed via PCA and KPCA.
Observed vectors PCA-transformed vectors KPCA-transformed vectors Class
t (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) ct
1 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
2 (0, 1, 1) (1.675, -1.132, 0.1049) (1.149, 0.02934, 0.322) 1
3 (1, 0, 0) (-0.367, 1.697, -0.2391) (0.8209, 0.7722, -0.2015) 1
4 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
5 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
6 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
7 (0, 0, 1) (-1.675, -1.132, -0.1049) (-1.774, -0.1216, 0.03258) 2
8 (0, 0, 0) (-1.961, 0.2829, 0.2014) (0.2801, -1.005, -0.06861) 1
Table 4: Testing vector (showing only the first three dimensions) and its first three principal components
as transformed via the trained PCA and KPCA parameters. The PCA-based and KPCA-based sense class
predictions disagree.
Observed
vectors
PCA-transformed vectors KPCA-transformed vec-
tors
Predicted
Class
Correct
Class
t (x1t , x2t , x3t ) (z1t , z2t , z3t ) (y1t , y2t , y3t ) c?t ct
9 (1, 0, 1) (-0.3671, -0.5658, -0.2392) 2 1
9 (1, 0, 1) (4e-06, 8e-07, 1.111e-18) 1 1
as can be seen for the case of the first principal axis
in Figure 1(b). Note that in this space, the sense 2
instances are surrounded by sense 1 instances. We
can traverse each of the projections onto the prin-
cipal axis in linear order, simply by visiting each of
the first principal components z1t along the principle
axis in order of their values, i.e., such that
z11 ? z18 ? z14 ? z15 ? z16 ? z17 ? z12 ? z13 ? z19
It is significantly more difficult to visualize
the nonlinear principal components case, however.
Note that in general, there may not exist any prin-
cipal axis in X , since an inverse mapping from F
may not exist. If we attempt to follow the same pro-
cedure to traverse each of the projections onto the
first principal axis as in the case of linear PCA, by
considering each of the first principal components
y1t in order of their value, i.e., such that
y14 ? y15 ? y16 ? y17 ? y19 ? y11 ? y18 ? y13 ? y12
then we must arbitrarily select a ?quasi-projection?
direction for each y1t since there is no actual prin-
cipal axis toward which to project. This results in a
?quasi-axis? roughly as shown in Figure 1(c) which,
though not precisely accurate, provides some idea
as to how the nonlinear generalization capability al-
lows the data points to be grouped by principal com-
ponents reflecting nonlinear patterns in the data dis-
tribution, in ways that linear PCA cannot do. Note
that in this space, the sense 1 instances are already
better separated from sense 2 data points. More-
over, unlike linear PCA, there may be up to M of
the ?quasi-axes?, which may number far more than
five. Such effects can become pronounced in the
high dimensional spaces are actually used for real
word sense disambiguation tasks.
3 A KPCA-based WSD model
To extract nonlinear principal components effi-
ciently, note that in both Equations (5) and (6) the
explicit form of ?(xi) is required only in the form
of (?(xi) ??(xj)), i.e., the dot product of vectors in
F . This means that we can calculate the nonlinear
principal components by substituting a kernel func-
tion k(xi, xj) for (?(xi) ? ?(xj )) in Equations (5)
and (6) without knowing the mapping ? explicitly;
instead, the mapping ? is implicitly defined by the
kernel function. It is always possible to construct
a mapping into a space where k acts as a dot prod-
uct so long as k is a continuous kernel of a positive
integral operator (Scho?lkopf et al, 1998).
the/DT
4, 5, 6, 7
1, 8
3
2
design/N
media/N
(a)
9
the/DT
4, 5, 6, 7
1, 8 3
2
design/N
media/N
(b)
9
the/DT
4, 5, 6, 7
1, 8 3
2
design/N
media/N
(c)
9
first principal
axis
: training example with sense class 1
: training example with sense class 2
: test example with unknown sense class
: test example with predicted sense
first principal
? quasi-axis?
class 2 (correct sense class=1)
: test example with predicted sense
class 1 (correct sense class=1)
Figure 1: Original vectors, PCA projections, and
KPCA ?quasi-projections? (see text).
Table 5: Experimental results showing that the
KPCA-based model performs significantly better
than na??ve Bayes and maximum entropy models.
Significance intervals are computed via bootstrap
resampling.
WSD Model Accuracy Sig. Int.
na??ve Bayes 63.3% +/-0.91%
maximum entropy 63.8% +/-0.79%
KPCA-based model 65.8% +/-0.79%
Thus we train the KPCA model using the follow-
ing algorithm:
1. Compute an M ? M matrix K? such that
K?ij = k(xi, xj) (7)
2. Compute the eigenvalues and eigenvectors of
matrix K? and normalize the eigenvectors. Let
??1 ? ??2 ? . . . ? ??M denote the eigenvalues
and ??1,..., ??M denote the corresponding com-
plete set of normalized eigenvectors.
To obtain the sense predictions for test instances,
we need only transform the corresponding vectors
using the trained KPCA model and classify the re-
sultant vectors using nearest neighbors. For a given
test instance vector x, its lth nonlinear principal
component is
ylt =
M
?
i=1
??lik(xi, xt) (8)
where ??li is the ith element of ??l.
For our disambiguation experiments we employ a
polynomial kernel function of the form k(xi, xj) =
(xi ? xj)d, although other kernel functions such as
gaussians could be used as well. Note that the de-
generate case of d = 1 yields the dot product kernel
k(xi, xj) = (xi?xj) which covers linear PCA as a
special case, which may explain why KPCA always
outperforms PCA.
4 Experiments
4.1 KPCA versus na??ve Bayes and maximum
entropy models
We established two baseline models to represent
the state-of-the-art for individual WSD models: (1)
na??ve Bayes, and (2) maximum entropy models.
The na??ve Bayes model was found to be the most
accurate classifier in a comparative study using a
subset of Senseval-2 English lexical sample data
by Yarowsky and Florian (2002). However, the
maximum entropy (Jaynes, 1978) was found to
yield higher accuracy than na??ve Bayes in a sub-
sequent comparison by Klein and Manning (2002),
who used a different subset of either Senseval-1 or
Senseval-2 English lexical sample data. To control
for data variation, we built and tuned models of both
kinds. Note that our objective in these experiments
is to understand the performance and characteristics
of KPCA relative to other individual methods. It
is not our objective here to compare against voting
or other ensemble methods which, though known to
be useful in practice (e.g., Yarowsky et al (2001)),
would not add to our understanding.
To compare as evenly as possible, we em-
ployed features approximating those of the ?feature-
enhanced na??ve Bayes model? of Yarowsky and Flo-
rian (2002), which included position-sensitive, syn-
tactic, and local collocational features. The mod-
els in the comparative study by Klein and Man-
ning (2002) did not include such features, and so,
again for consistency of comparison, we experi-
mentally verified that our maximum entropy model
(a) consistently yielded higher scores than when
the features were not used, and (b) consistently
yielded higher scores than na??ve Bayes using the
same features, in agreement with Klein and Man-
ning (2002). We also verified the maximum en-
tropy results against several different implementa-
tions, using various smoothing criteria, to ensure
that the comparison was even.
Evaluation was done on the Senseval 2 English
lexical sample task. It includes 73 target words,
among which nouns, adjectives, adverbs and verbs.
For each word, training and test instances tagged
with WordNet senses are provided. There are an av-
erage of 7.8 senses per target word type. On average
109 training instances per target word are available.
Note that we used the set of sense classes from Sen-
seval?s ?fine-grained? rather than ?coarse-grained?
classification task.
The KPCA-based model achieves the highest ac-
curacy, as shown in Table 5, followed by the max-
imum entropy model, with na??ve Bayes doing the
poorest. Bear in mind that all of these models are
significantly more accurate than any of the other re-
ported models on Senseval. ?Accuracy? here refers
to both precision and recall since disambiguation of
all target words in the test set is attempted. Results
are statistically significant at the 0.10 level, using
bootstrap resampling (Efron and Tibshirani, 1993);
moreover, we consistently witnessed the same level
of accuracy gains from the KPCA-based model over
Table 6: Experimental results comparing the
KPCA-based model versus the SVM model.
WSD Model Accuracy Sig. Int.
SVM-based model 65.2% +/-1.00%
KPCA-based model 65.8% +/-0.79%
many variations of the experiments.
4.2 KPCA versus SVM models
Support vector machines (e.g., Vapnik (1995),
Joachims (1998)) are a different kind of ker-
nel method that, unlike KPCA methods, have al-
ready gained high popularity for NLP applications
(e.g., Takamura and Matsumoto (2001), Isozaki and
Kazawa (2002), Mayfield et al (2003)) including
the word sense disambiguation task (e.g., Cabezas
et al (2001)). Given that SVM and KPCA are both
kernel methods, we are frequently asked whether
SVM-based WSD could achieve similar results.
To explore this question, we trained and tuned
an SVM model, providing the same rich set of fea-
tures and also varying the feature representations to
optimize for SVM biases. As shown in Table 6,
the highest-achieving SVM model is also able to
obtain higher accuracies than the na??ve Bayes and
maximum entropy models. However, in all our ex-
periments the KPCA-based model consistently out-
performs the SVM model (though the margin falls
within the statistical significance interval as com-
puted by bootstrap resampling for this single exper-
iment). The difference in KPCA and SVM perfor-
mance is not surprising given that, aside from the
use of kernels, the two models share little structural
resemblance.
4.3 Running times
Training and testing times for the various model im-
plementations are given in Table 7, as reported by
the Unix time command. Implementations of all
models are in C++, but the level of optimization is
not controlled. For example, no attempt was made
to reduce the training time for na??ve Bayes, or to re-
duce the testing time for the KPCA-based model.
Nevertheless, we can note that in the operating
range of the Senseval lexical sample task, the run-
ning times of the KPCA-based model are roughly
within the same order of magnitude as for na??ve
Bayes or maximum entropy. On the other hand,
training is much faster than the alternative kernel
method based on SVMs. However, the KPCA-
based model?s times could be expected to suffer
in situations where significantly larger amounts of
Table 7: Comparison of training and testing times for the different WSD model implementations.
WSD Model Training time [CPU sec] Testing time [CPU sec]
na??ve Bayes 103.41 16.84
maximum entropy 104.62 59.02
SVM-based model 5024.34 16.21
KPCA-based model 216.50 128.51
training data are available.
5 Conclusion
This work represents, to the best of our knowl-
edge, the first application of Kernel PCA to a
true natural language processing task. We have
shown that a KPCA-based model can significantly
outperform state-of-the-art results from both na??ve
Bayes as well as maximum entropy models, for
supervised word sense disambiguation. The fact
that our KPCA-based model outperforms the SVM-
based model indicates that kernel methods other
than SVMs deserve more attention. Given the theo-
retical advantages of KPCA, it is our hope that this
work will encourage broader recognition, and fur-
ther exploration, of the potential of KPCA modeling
within NLP research.
Given the positive results, we plan next to com-
bine large amounts of unsupervised data with rea-
sonable smaller amounts of supervised data such as
the Senseval lexical sample. Earlier we mentioned
that one of the promising advantages of KPCA is
that it computes the transform purely from unsuper-
vised training vector data. We can thus make use of
the vast amounts of cheap unannotated data to aug-
ment the model presented in this paper.
References
Clara Cabezas, Philip Resnik, and Jessica Stevens.
Supervised sense tagging using support vector
machines. In Proceedings of Senseval-2, Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 59?62,
Toulouse, France, July 2001. SIGLEX, Associ-
ation for Computational Linguistics.
Martin Chodorow, Claudia Leacock, and George A.
Miller. A topical/local classifier for word sense
identification. Computers and the Humanities,
34(1-2):115?120, 1999. Special issue on SEN-
SEVAL.
Hoa Trang Dang and Martha Palmer. Combining
contextual features for word sense disambigua-
tion. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 88?
94, Philadelphia, July 2002. SIGLEX, Associa-
tion for Computational Linguistics.
Konstantinos I. Diamantaras and Sun Yuan Kung.
Principal Component Neural Networks. Wiley,
New York, 1996.
Bradley Efron and Robert J. Tibshirani. An Intro-
duction to the Bootstrap. Chapman and Hall,
1993.
Hideki Isozaki and Hideto Kazawa. Efficient sup-
port vector classifiers for named entity recogni-
tion. In Proceedings of COLING-2002, pages
390?396, Taipei, 2002.
E.T. Jaynes. Where do we Stand on Maximum En-
tropy? MIT Press, Cambridge MA, 1978.
Thorsten Joachims. Text categorization with sup-
port vector machines: Learning with many rel-
evant features. In Proceedings of ECML-98,
10th European Conference on Machine Learning,
pages 137?142, 1998.
Adam Kilgarriff and Joseph Rosenzweig. Frame-
work and results for English Senseval. Comput-
ers and the Humanities, 34(1):15?48, 1999. Spe-
cial issue on SENSEVAL.
Adam Kilgarriff. English lexical sample task de-
scription. In Proceedings of Senseval-2, Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 17?20,
Toulouse, France, July 2001. SIGLEX, Associ-
ation for Computational Linguistics.
Dan Klein and Christopher D. Manning. Con-
ditional structure versus conditional estimation
in NLP models. In Proceedings of EMNLP-
2002, Conference on Empirical Methods in Nat-
ural Language Processing, pages 9?16, Philadel-
phia, July 2002. SIGDAT, Association for Com-
putational Linguistics.
Taku Kudo and Yuji Matsumoto. Fast methods
for kernel-based text analysis. In Proceedings of
the 41set Annual Meeting of the Asoociation for
Computational Linguistics, pages 24?31, 2003.
James Mayfield, Paul McNamee, and Christine Pi-
atko. Named entity recognition using hundreds of
thousands of features. In Walter Daelemans and
Miles Osborne, editors, Proceedings of CoNLL-
2003, pages 184?187, Edmonton, Canada, 2003.
Raymond J. Mooney. Comparative experiments on
disambiguating word senses: An illustration of
the role of bias in machine learning. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, Philadelphia, May
1996. SIGDAT, Association for Computational
Linguistics.
Ted Pedersen. Machine learning with lexical fea-
tures: The Duluth approach to SENSEVAL-2.
In Proceedings of Senseval-2, Second Interna-
tional Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 139?142, Toulouse,
France, July 2001. SIGLEX, Association for
Computational Linguistics.
Bernhard Scho?lkopf, Alexander Smola, and Klaus-
Rober Mu?ller. Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation,
10(5), 1998.
Hiroya Takamura and Yuji Matsumoto. Feature
space restructuring for SVMs with application to
text categorization. In Proceedings of EMNLP-
2001, Conference on Empirical Methods in Nat-
ural Language Processing, pages 51?57, 2001.
Vladimir N. Vapnik. The Nature of Statistical
Learning Theory. Springer-Verlag, New York,
1995.
David Yarowsky and Radu Florian. Evaluat-
ing sense disambiguation across diverse param-
eter spaces. Natural Language Engineering,
8(4):293?310, 2002.
David Yarowsky, Silviu Cucerzan, Radu Florian,
Charles Schafer, and Richard Wicentowski. The
Johns Hopkins SENSEVAL2 system descrip-
tions. In Proceedings of Senseval-2, Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 163?166,
Toulouse, France, July 2001. SIGLEX, Associa-
tion for Computational Linguistics.
Proceedings of the 43rd Annual Meeting of the ACL, pages 387?394,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Word Sense Disambiguation vs. Statistical Machine Translation
Marine CARPUAT Dekai WU1
marine@cs.ust.hk dekai@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
Abstract
We directly investigate a subject of much
recent debate: do word sense disambiga-
tion models help statistical machine trans-
lation quality? We present empirical re-
sults casting doubt on this common, but
unproved, assumption. Using a state-of-
the-art Chinese word sense disambigua-
tion model to choose translation candi-
dates for a typical IBM statistical MT
system, we find that word sense disam-
biguation does not yield significantly bet-
ter translation quality than the statistical
machine translation system alone. Error
analysis suggests several key factors be-
hind this surprising finding, including in-
herent limitations of current statistical MT
architectures.
1 Introduction
Word sense disambiguation or WSD, the task of de-
termining the correct sense of a word in context, is
a much studied problem area with a long and hon-
orable history. Recent years have seen steady ac-
curacy gains in WSD models, driven in particular
by controlled evaluations such as the Senseval series
of workshops. Word sense disambiguation is often
assumed to be an intermediate task, which should
then help higher level applications such as machine
1The authors would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research
in part through grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09, and several anonymous reviewers for in-
sights and suggestions.
translation or information retrieval. However, WSD
is usually performed and evaluated as a standalone
task, and to date there have been very few efforts to
integrate the learned WSD models into full statisti-
cal MT systems.
An energetically debated question at conferences
over the past year is whether even the new state-
of-the-art word sense disambiguation models actu-
ally have anything to offer to full statistical machine
translation systems. Among WSD circles, this can
sometimes elicit responses that border on implying
that even asking the question is heretical. In efforts
such as Senseval we tend to regard the construction
of WSD models as an obviously correct, if necessar-
ily simplified, approach that will eventually lead to
essential disambiguation components within larger
applications like machine translation.
There is no question that the word sense disam-
biguation perspective has led to numerous insights in
machine translation, even of the statistical variety. It
is often simply an unstated assumption that any full
translation system, to achieve full performance, will
sooner or later have to incorporate individual WSD
components.
However, in some translation architectures and
particularly in statistical machine translation (SMT),
the translation engine already implicitly factors in
many contextual features into lexical choice. From
this standpoint, SMT models can be seen as WSD
models in their own right, albeit with several major
caveats.
But typical statistical machine translation models
only rely on a local context to choose among lexical
translation candidates, as discussed in greater detail
later. It is therefore often assumed that dedicated
WSD-based lexical choice models, which can incor-
387
porate a wider variety of context features, can make
better predictions than the ?weaker? models implicit
in statistical MT, and that these predictions will help
the translation quality.
Nevertheless, this assumption has not been em-
pirically verified, and we should not simply assume
that WSD models can contribute more than what the
SMT models perform. It may behoove us to take
note of the sobering fact that, perhaps analogously,
WSD has yet to be conclusively shown to help in-
formation retrieval systems after many years of at-
tempts.
In this work, we propose to directly investigate
whether word sense disambiguation?at least as it is
typically currently formulated?is useful for statis-
tical machine translation. We tackle a real Chinese
to English translation task using a state-of-the-art su-
pervised WSD system and a typical SMT model. We
show that the unsupervised SMT model, trained on
parallel data without any manual sense annotation,
yields higher BLEU scores than the case where the
SMT model makes use of the lexical choice predic-
tions from the supervised WSD model, which are
more expensive to create. The reasons for the sur-
prising difficulty of improving over the translation
quality of the SMT model are then discussed and
analyzed.
2 Word sense disambiguation vs.
statistical machine translation
We begin by examining the respective strengths and
weaknesses of dedicated WSD models versus full
SMT models, that could be expected to be relevant
to improving lexical choice.
2.1 Features Unique to WSD
Dedicated WSD is typically cast as a classification
task with a predefined sense inventory. Sense dis-
tinctions and granularity are often manually prede-
fined, which means that they can be adapted to the
task at hand, but also that the translation candidates
are limited to an existing set.
To improve accuracy, dedicated WSD models typ-
ically employ features that are not limited to the lo-
cal context, and that include more linguistic infor-
mation than the surface form of words. This of-
ten requires several stages of preprocessing, such
as part-of-speech tagging and/or parsing. (Prepro-
cessor domain can be an issue, since WSD accu-
racy may suffer from domain mismatches between
the data the preprocessors were trained on, and the
data they are applied to.) For example, a typi-
cal dedicated WSD model might employ features
as described by Yarowsky and Florian (2002) in
their ?feature-enhanced naive Bayes model?, with
position-sensitive, syntactic, and local collocational
features. The feature set made available to the WSD
model to predict lexical choices is therefore much
richer than that used by a statistical MT model.
Also, dedicated WSD models can be supervised,
which yields significantly higher accuracies than un-
supervised. For the experiments described in this
study we employed supervised training, exploit-
ing the annotated corpus that was produced for the
Senseval-3 evaluation.
2.2 Features Unique to SMT
Unlike lexical sample WSD models, SMT models
simultaneously translate complete sentences rather
than isolated target words. The lexical choices are
made in a way that heavily prefers phrasal cohesion
in the output target sentence, as scored by the lan-
guage model. That is, the predictions benefit from
the sentential context of the target language. This
has the general effect of improving translation flu-
ency.
The WSD accuracy of the SMT model depends
critically on the phrasal cohesion of the target lan-
guage. As we shall see, this phrasal cohesion prop-
erty has strong implications for the utility of WSD.
In other work (forthcoming), we investigated
the inverse question of evaluating the Chinese-to-
English SMT model on word sense disambigua-
tion performance, using standard WSD evaluation
methodology and datasets from the Senseval-3 Chi-
nese lexical sample task. We showed the accuracy of
the SMT model to be significantly lower than that of
all the dedicated WSD models considered, even af-
ter adding the lexical sample data to the training set
for SMT to allow for a fair comparison. These re-
sults highlight the relative strength, and the potential
hoped-for advantage of dedicated supervised WSD
models.
388
3 The WSD system
The WSD system used for the experiments is based
on the model that achieved the best performance, by
a large margin, on the Senseval-3 Chinese lexical
sample task (Carpuat et al, 2004).
3.1 Classification model
The model consists of an ensemble of four voting
models combined by majority vote.
The first voting model is a naive Bayes model,
since Yarowsky and Florian (2002) found this model
to be the most accurate classifier in a comparative
study on a subset of Senseval-2 English lexical sam-
ple data.
The second voting model is a maximum entropy
model (Jaynes, 1978), since Klein and Manning
(2002) found that this model yielded higher accu-
racy than naive Bayes in a subsequent comparison
of WSD performance. (Note, however, that a differ-
ent subset of either Senseval-1 or Senseval-2 English
lexical sample data was used for their comparison.)
The third voting model is a boosting model (Fre-
und and Schapire, 1997), since has consistently
turned in very competitive scores on related tasks
such as named entity classification (Carreras et al,
2002) . Specifically, an AdaBoost.MH model was
used (Schapire and Singer, 2000), which is a multi-
class generalization of the original boosting algo-
rithm, with boosting on top of decision stump clas-
sifiers (i.e., decision trees of depth one).
The fourth voting model is a Kernel PCA-based
model (Wu et al, 2004). Kernel Principal Compo-
nent Analysis (KPCA) is a nonlinear kernel method
for extracting nonlinear principal components from
vector sets where, conceptually, the n-dimensional
input vectors are nonlinearly mapped from their
original space Rn to a high-dimensional feature
space F where linear PCA is performed, yielding a
transform by which the input vectors can be mapped
nonlinearly to a new set of vectors (Scho?lkopf et al,
1998). WSD can be performed by a Nearest Neigh-
bor Classifier in the high-dimensional KPCA feature
space. (Carpuat et al, 2004) showed that KPCA-
based WSD models achieve close accuracies to the
best individual WSD models, while having a signif-
icantly different bias.
All these classifiers have the ability to handle
large numbers of sparse features, many of which
may be irrelevant. Moreover, the maximum entropy
and boosting models are known to be well suited to
handling features that are highly interdependent.
The feature set used consists of position-sensitive,
syntactic, and local collocational features, as de-
scribed by Yarowsky and Florian (2002).
3.2 Lexical choice mapping model
Ideally, we would like the WSD model to predict En-
glish translations given Chinese target words in con-
text. Such a model requires Chinese training data
annotated with English senses, but such data is not
available. Instead, the WSD system was trained us-
ing the Senseval-3 Chinese lexical sample task data.
(This is suboptimal, but reflects the difficulties that
arise when considering a real translation task; we
cannot assume that sense-annotated data will always
be available for all language pairs.)
The Chinese lexical sample task includes 20 tar-
get words. For each word, several senses are defined
using the HowNet knowledge base. There are an av-
erage of 3.95 senses per target word type, ranging
from 2 to 8. Only about 37 training instances per
target word are available.
For the purpose of Chinese to English translation,
the WSD model should predict English translations
instead of HowNet senses. Fortunately, HowNet
provides English glosses. This allows us to map
each HowNet sense candidate to a set of English
translations, converting the monolingual Chinese
WSD system into a translation lexical choice model.
We further extended the mapping to include any sig-
nificant translation choice considered by the SMT
system but not in HowNet.
4 The SMT system
To build a representative baseline statistical machine
translation system, we restricted ourselves to mak-
ing use of freely available tools, since the potential
contribution of WSD should be easier to see against
this baseline. Note that our focus here is not on the
SMT model itself; our aim is to evaluate the impact
of WSD on a real Chinese to English statistical ma-
chine translation task.
389
Table 1: Example of the translation candidates before and after mapping for the target word ?4? (lu)
HowNet Sense ID HowNet glosses HowNet glosses + improved transla-
tions
56520 distance distance
56521 sort sort
56524 Lu Lu
56525, 56526, 56527, 56528 path, road, route, way path, road, route, way, circuit, roads
56530, 56531, 56532 line, means, sequence line, means, sequence, lines
56533, 56534 district, region district, region
4.1 Alignment model
The alignment model was trained with GIZA++
(Och and Ney, 2003), which implements the most
typical IBM and HMM alignment models. Transla-
tion quality could be improved using more advanced
hybrid phrasal or tree models, but this would inter-
fere with the questions being investigated here. The
alignment model used is IBM-4, as required by our
decoder. The training scheme consists of IBM-1,
HMM, IBM-3 and IBM-4, following (Och and Ney,
2003).
The training corpus consists of about 1 million
sentences from the United Nations Chinese-English
parallel corpus from LDC. This corpus was automat-
ically sentence-aligned, so the training data does not
require as much manual annotation as for the WSD
model.
4.2 Language model
The English language model is a trigram model
trained on the Gigaword newswire data and on the
English side of the UN and Xinhua parallel corpora.
The language model is also trained using a publicly
available software, the CMU-Cambridge Statistical
Language Modeling Toolkit (Clarkson and Rosen-
feld, 1997).
4.3 Decoding
The ISI ReWrite decoder (Germann, 2003), which
implements an efficient greedy decoding algorithm,
is used to translate the Chinese sentences, using the
alignment model and language model previously de-
scribed.
Notice that very little contextual information is
available to the SMT models. Lexical choice dur-
ing decoding essentially depends on the translation
probabilities learned for the target word, and on the
English language model scores.
5 Experimental method
5.1 Test set selection
We extracted the Chinese sentences from the NIST
MTEval-04 test set that contain any of the 20 target
words from the Senseval-3 Chinese lexical sample
target set. For a couple of targets, no instances were
available from the test set. The resulting test set con-
tains a total of 175 sentences, which is smaller than
typical MT evaluation test sets, but slightly larger
than the one used for the Senseval Chinese lexical
sample task.
5.2 Integrating the WSD system predictions
with the SMT model
There are numerous possible ways to integrate the
WSD system predictions with the SMT model. We
choose two different straightforward approaches,
which will help analyze the effect of the different
components of the SMT system, as we will see in
Section 6.5.
5.2.1 Using WSD predictions for decoding
In the first approach, we use the WSD sense pre-
dictions to constrain the set of English sense candi-
dates considered by the decoder for each of the tar-
get words. Instead of allowing all the word transla-
tion candidates from the translation model, when we
use the WSD predictions we override the translation
model and force the decoder to choose the best trans-
lation from the predefined set of glosses that maps to
the HowNet sense predicted by the WSD model.
390
Table 2: Translation quality with and without the WSD model
Translation System BLEU score
SMT 0.1310
SMT + WSD for postprocessing 0.1253
SMT + WSD for decoding 0.1239
SMT + WSD for decoding with improved translation candidates 0.1232
5.2.2 Using WSD predictions for
postprocessing
In the second approach, we use the WSD predic-
tions to postprocess the output of the SMT system:
in each output sentence, the translation of the target
word chosen by the SMT model is directly replaced
by the WSD prediction. When the WSD system pre-
dicts more than one candidate, a unique translation
is randomly chosen among them. As discussed later,
this approach can be used to analyze the effect of the
language model on the output.
It would also be interesting to use the gold stan-
dard or correct sense of the target words instead of
the WSD model predictions in these experiments.
This would give an upper-bound on performance
and would quantify the effect of WSD errors. How-
ever, we do not have a corpus which contains both
sense annotation and multiple reference translations:
the MT evaluation corpus is not annotated with the
correct senses of Senseval target words, and the Sen-
seval corpus does not include English translations of
the sentences.
6 Results
6.1 Even state-of-the-art WSD does not help
BLEU score
Table 2 summarizes the translation quality scores
obtained with and without the WSD model. Using
our WSD model to constrain the translation candi-
dates given to the decoder hurts translation quality,
as measured by the automated BLEU metric (Pap-
ineni et al, 2002).
Note that we are evaluating on only difficult sen-
tences containing the problematic target words from
the lexical sample task, so BLEU scores can be ex-
pected to be on the low side.
6.2 WSD still does not help BLEU score with
improved translation candidates
One could argue that the translation candidates cho-
sen by the WSD models do not help because they
are only glosses obtained from the HowNet dictio-
nary. They consist of the root form of words only,
while the SMT model can learn many more transla-
tions for each target word, including inflected forms
and synonyms.
In order to avoid artificially penalizing the WSD
system by limiting its translation candidates to the
HowNet glosses, we expand the translation set us-
ing the bilexicon learned during translation model
training. For each target word, we consider the En-
glish words that are given a high translation prob-
ability, and manually map each of these English
words to the sense categories defined for the Sen-
seval model. At decoding time, the set of transla-
tion candidates considered by the language model is
therefore larger, and closer to that considered by the
pure SMT system.
The results in Table 2 show that the improved
translation candidates do not help BLEU score. The
translation quality obtained with SMT alone is still
better than when the improved WSD Model is used.
The simpler approach of using WSD predictions in
postprocessing yields better BLEU score than the
decoding approach, but still does not outperform the
SMT model.
6.3 WSD helps translation quality for very few
target words
If we break down the test set and evaluate the effect
of the WSD per target word, we find that for all but
two of the target words WSD either hurts the BLEU
score or does not help it, which shows that the de-
crease in BLEU is not only due to a few isolated tar-
get words for which the Senseval sense distinctions
391
are not helpful.
6.4 The ?language model effect?
Error analysis revealed some surprising effects. One
particularly dismaying effect is that even in cases
where the WSD model is able to predict a better tar-
get word translation than the SMT model, to use the
better target word translation surprisingly often still
leads to a lower BLEU score.
The phrasal coherence property can help explain
this surprising effect we observed. The translation
chosen by the SMT model will tend to be more likely
than the WSD prediction according to the language
model; otherwise, it would also have been predicted
by SMT. The translation with the higher language
model probability influences the translation of its
neighbors, thus potentially improving BLEU score,
while the WSD prediction may not have been seen
occurring within phrases often enough, thereby low-
ering BLEU score.
For example, we observe that the WSD model
sometimes correctly predicts ?impact? as a better
translation for ???? (chongji), where the SMT
model selects ?shock?. In these cases, some of
the reference translations also use ?impact?. How-
ever, even when the WSD model constrains the de-
coder to select ?impact? rather than ?shock?, the
resulting sentence translation yields a lower BLEU
score. This happens because the SMT model does
not know how to use ?impact? correctly (if it did, it
would likely have chosen ?impact? itself). Forcing
the lexical choice ?impact? simply causes the SMT
model to generate phrases such as ?against Japan for
peace constitution impact? instead of ?against Japan
for peace constitution shocks?. This actually lowers
BLEU score, because of the n-gram effects.
6.5 Using WSD predictions in postprocessing
does not help BLEU score either
In the postprocessing approach, decoding is done
before knowing the WSD predictions, which elim-
inates the ?language model effect?. Even in these
conditions, the SMT model alone is still the best per-
forming system.
The postprocessing approach also outperforms
the integrated decoding approach, which shows that
the language model is not able to make use of the
WSD predictions. One could expect that letting the
Table 3: BLEU scores per target word: WSD helps
for very few target words
Target word SMT SMT +
WSD
?? bawo 0.1482 0.1484
? bao 0.1891 0.1891
a? cailiao 0.0863 0.0863
?? chongji 0.1396 0.1491
?0 difang 0.1233 0.1083
I fengzi 0.1404 0.1402
?? huodong 0.1365 0.1465
? lao 0.1153 0.1136
4 lu 0.1322 0.1208
?u qilai 0.1104 0.1082
 qian 0.1948 0.1814
B? tuchu 0.0975 0.0989
?? yanjiu 0.1089 0.1089
?? zhengdong 0.1267 0.1251
 zhou 0.0825 0.0808
decoder choose among the WSD translations also
yields a better translation of the context. This is
indeed the case, but for very few examples only:
for instance the target word ??0? (difang) is bet-
ter used in the integrated decoding ouput ?the place
of local employment? , than in the postprocessing
output ?the place employment situation?. Instead,
the majority of cases follow the pattern illustrated
by the following example where the target word is
??? (lao): the SMT system produces the best output
(?the newly elected President will still face old prob-
lems?), the postprocessed output uses the fluent sen-
tence with a different translation (?the newly elected
President will still face outdated problems?), while
the translation is not used correctly with the decod-
ing approach (?the newly elected President will face
problems still to be outdated?).
6.6 BLEU score bias
The ?language model effect? highlights one of the
potential weaknesses of the BLEU score. BLEU pe-
nalizes for phrasal incoherence, which in the present
study means that it can sometimes sacrifice ade-
quacy for fluency.
However, the characteristics of BLEU are by
392
no means solely responsible for the problems with
WSD that we observed. To doublecheck that n-gram
effects were not unduly impacting our study, we also
evaluated using BLEU-1, which gave largely simi-
lar results as the standard BLEU-4 scores reported
above.
7 Related work
Most translation disambiguation tasks are defined
similarly to the Senseval Multilingual lexical sam-
ple tasks. In Senseval-3, the English to Hindi trans-
lation disambigation task was defined identically to
the English lexical sample task, except that the WSD
models are expected to predict Hindi translations in-
stead of WordNet senses. This differs from our ap-
proach which consists of producing the translation
of complete sentences, and not only of a predefined
set of target words.
Brown et al (1991) proposed a WSD algorithm to
disambiguate English translations of French target
words based on the single most informative context
feature. In a pilot study, they found that using this
WSD method in their French-English SMT system
helped translation quality, manually evaluated using
the number of acceptable translations. However, this
study is limited to the unrealistic case of words that
have exactly two senses in the other language.
Most previous work has focused on the distinct
problem of exploiting various bilingual resources
(e.g., parallel or comparable corpora, or even MT
systems) to help WSD. The goal is to achieve accu-
rate WSD with minimum amounts of annotated data.
Again, this differs from our objective which consists
of using WSD to improve performance on a full ma-
chine translation task, and is measured in terms of
translation quality.
For instance, Ng et al (2003) showed that it is
possible to use word aligned parallel corpora to train
accurate supervised WSD models. The objective is
different; it is not possible for us to use this method
to train our WSD model without undermining the
question we aim to investigate: we would need to
use the SMT model to word-align the parallel sen-
tences, which could too strongly bias the predic-
tions of the WSD model towards those of the SMT
model, instead of combining predictive information
from independent sources as we aim to study here.
Other work includes Li and Li (2002) who pro-
pose a bilingual bootstrapping method to learn a
translation disambiguation WSD model, and Diab
(2004) who exploited large amounts of automati-
cally generated noisy parallel data to learn WSD
models in an unsupervised bootstrapping scheme.
8 Conclusion
The empirical study presented here argues that we
can expect that it will be quite difficult, at the least,
to use standard WSD models to obtain significant
improvements to statistical MT systems, even when
supervised WSD models are used. This casts signif-
icant doubt on a commonly-held, but unproven, as-
sumption to the contrary. We have presented empiri-
cally based analysis of the reasons for this surprising
finding.
We have seen that one major factor is that the
statistical MT model is sufficiently accurate so that
within the training domain, even the state-of-the-art
dedicated WSD model is only able to improve on its
lexical choice predictions in a relatively small pro-
portion of cases.
A second major factor is that even when the ded-
icated WSD model makes better predictions, cur-
rent statistical MT models are unable to exploit this.
Under this interpretation of our results, the depen-
dence on the language model in current SMT ar-
chitectures is excessive. One could of course ar-
gue that drastically increasing the amount of train-
ing data for the language model might overcome the
problems from the language model effect. Given
combinatorial problems, however, there is no way at
present of telling whether the amount of data needed
to achieve this is realistic, particularly for translation
across many different domains. On the other hand, if
the SMT architecture cannot make use of WSD pre-
dictions, even when they are in fact better than the
SMT?s lexical choices, then perhaps some alterna-
tive model striking a different balance of adequacy
and fluency is called for. Ultimately, after all, WSD
is a method of compensating for sparse data. Thus
it may be that the present inability of WSD models
to help improve accuracy of SMT systems stems not
from an inherent weakness of dedicated WSD mod-
els, but rather from limitations of present-day SMT
architectures.
393
To further test this, our experiments could be
tried on other statistical MT models. For exam-
ple, the WSD model?s predictions could be em-
ployed in a Bracketing ITG translation model such
as Wu (1996) or Zens et al (2004), or alternatively
they could be incorporated as features for rerank-
ing in a maximum-entropy SMT model (Och and
Ney, 2002), instead of using them to constrain the
sentence translation hypotheses as done here. How-
ever, the preceding discussion argues that it is doubt-
ful that this would produce significantly different re-
sults, since the inherent problem from the ?language
model effect? would largely remain, causing sen-
tence translations that include the WSD?s preferred
lexical choices to be discounted. For similar rea-
sons, we suspect our findings may also hold even for
more sophisticated statistical MT models that rely
heavily on n-gram language models. A more gram-
matically structured statistical MT model that less n-
gram oriented, such as the ITG based ?grammatical
channel? translation model (Wu and Wong, 1998),
might make more effective use of the WSD model?s
predictions.
References
Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and
Robert Mercer. Word-sense disambiguation using statistical
methods. In Proceedings of 29th meeting of the Associa-
tion for Computational Linguistics, pages 264?270, Berke-
ley, California, 1991.
Marine Carpuat, Weifeng Su, and Dekai Wu. Augmenting en-
semble classification for word sense disambiguation with a
Kernel PCA model. In Proceedings of Senseval-3, Third
International Workshop on Evaluating Word Sense Disam-
biguation Systems, Barcelona, July 2004. SIGLEX, Associ-
ation for Computational Linguistics.
Xavier Carreras, Llu??s Ma`rques, and Llu??s Padro?. Named en-
tity extraction using AdaBoost. In Dan Roth and Antal van
den Bosch, editors, Proceedings of CoNLL-2002, pages 167?
170, Taipei, Taiwan, 2002.
Philip Clarkson and Ronald Rosenfeld. Statistical language
modeling using the CMU-Cambridge toolkit. In Proceed-
ings of Eurospeech ?97, pages 2707?2710, Rhodes, Greece,
1997.
Mona Diab. Relieving the data acquisition bottleneck in word
sense disambiguation. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguistics,
2004.
Yoram Freund and Robert E. Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting. In Journal of Computer and System Sciences,
55(1), pages 119?139, 1997.
Ulrich Germann. Greeedy decoding for statistical machine
translation in almost linear time. In Proceedings of HLT-
NAACL-2003. Edmonton, AB, Canada, 2003.
E.T. Jaynes. Where do we Stand on Maximum Entropy? MIT
Press, Cambridge MA, 1978.
Dan Klein and Christopher D. Manning. Conditional structure
versus conditional estimation in NLP models. In Proceed-
ings of EMNLP-2002, Conference on Empirical Methods
in Natural Language Processing, pages 9?16, Philadelphia,
July 2002. SIGDAT, Association for Computational Linguis-
tics.
Cong Li and Hang Li. Word translation disambiguation using
bilingual bootstrapping. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguistics,
pages 343?351, 2002.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. Exploiting paral-
lel texts for word sense disambiguation: An empirical study.
In Proceedings of ACL-03, Sapporo, Japan, pages 455?462,
2003.
Franz Och and Hermann Ney. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proceedings of ACL-02, Philadelphia, 2002.
Franz Josef Och and Hermann Ney. A systematic comparison
of various statistical alignment models. Computational Lin-
guistics, 29(1):19?52, 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, 2002.
Robert E. Schapire and Yoram Singer. BoosTexter: A boosting-
based system for text categorization. Machine Learning,
39(2):135?168, 2000.
Bernhard Scho?lkopf, Alexander Smola, and Klaus-Rober
Mu?ller. Nonlinear component analysis as a kernel eigenvalue
problem. Neural Computation, 10(5), 1998.
Dekai Wu and Hongsing Wong. Machine translation with a
stochastic grammatical channel. In Proceedings of COLING-
ACL?98, Montreal,Canada, August 1998.
Dekai Wu, Weifeng Su, and Marine Carpuat. A Kernel PCA
method for superior word sense disambiguation. In Proceed-
ings of the 42nd Annual Meeting of the Association for Com-
putational Linguistics, Barcelona, July 2004.
Dekai Wu. A polynomial-time algorithm for statistical machine
translation. In Proceedings of 34th Annual Meeting of the
Association for Computational Linguistics, Santa Cruz, Cal-
ifornia, June 1996.
David Yarowsky and Radu Florian. Evaluating sense disam-
biguation across diverse parameter spaces. Natural Lan-
guage Engineering, 8(4):293?310, 2002.
Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro
Sumita. Reordering constraints for phrase-based statisti-
cal machine translation. In Proceedings of COLING-2004,
Geneva,Switzerland, August 2004.
394
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 905?912,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Grammatical Approach to Understanding Textual Tables using
Two-Dimensional SCFGs
Dekai WU1 Ken Wing Kuen LEE
Human Language Technology Center
HKUST
Department of Computer Science and Engineering
University of Science and Technology
Clear Water Bay, Hong Kong
{dekai,cswkl}@cs.ust.hk
Abstract
We present an elegant and extensible
model that is capable of providing seman-
tic interpretations for an unusually wide
range of textual tables in documents. Un-
like the few existing table analysis mod-
els, which largely rely on relatively ad hoc
heuristics, our linguistically-oriented ap-
proach is systematic and grammar based,
which allows our model (1) to be concise
and yet (2) recognize a wider range of data
models than others, and (3) disambiguate
to a significantly finer extent the under-
lying semantic interpretation of the table
in terms of data models drawn from rela-
tion database theory. To accomplish this,
the model introduces Viterbi parsing under
two-dimensional stochastic CFGs. The
cleaner grammatical approach facilitates
not only greater coverage, but also gram-
mar extension and maintenance, as well as
a more direct and declarative link to se-
mantic interpretation, for which we also
introduce a new, cleaner data model. In
disambiguation experiments on recogniz-
ing relevant data models of unseen web ta-
bles from different domains, a blind evalu-
ation of the model showed 60% precision
and 80% recall.
1 Introduction
Natural language processing has historically
tended to emphasize understanding of linear
strings?sentences, paragraphs, discourse struc-
ture. The vast body of work that focuses on text
understanding is often seen as an approximation of
1The authors would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research
in part through grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
spoken language understanding. Yet real-life text
is actually heavily dependent on visual layout and
formatting, which compensate for cues normally
found in spoken language but are absent in text.
As Scott (2003) reiterated in the opening ACL?03
invited talk: ?The overlay of graphics on text is in
many ways equivalent to the overlay of prosody on
speech... Just as prosody undoubtedly contributes
to the meaning of utterances, so too does a text?s
graphical presentation contribute to its meaning.
However... few natural language understanding
systems use graphical presentational features to
aid interpretation...? (Power et al, 2003).
Nowhere is this more evident than in the wide-
spread use of tables in real-world, unsimplified
text documents. Tables have a comparable or
greater complexity as other elements of text. Un-
fortunately, in mainstream NLP it is not uncom-
mon for tables to be regarded as a somehow ?de-
generate? form of text, unworthy of the same de-
gree of attention as the rest of the text. But as
we will discuss, the degree of ambiguity in ta-
ble understanding is at least as great as for many
sense and attachment problems. Many of the same
mechanisms used for understanding linear text are
also required for table understanding. The same
division of surface syntax and underlying seman-
tics is found.
Indeed, to perceive the limitations of existing
table understanding models, we may distinguish
several very different levels of table analysis tasks.
In table classification, the table is classified into
one of several coarse categories (in the extreme
case, some models simply predict whether the pur-
pose of the table is for page layout versus tabular
data). In table synactic recognition, the surface
types of individual cells or block regions are la-
beled (e.g., as heading or data) but the underlying
semantic relationships between the table elements
remain unrecognized and usually highly ambigu-
ous (i.e., no logical relations between the elements
905
in the table are assigned). In contrast, in table se-
mantic interpretation, the exact logical relations
between the elements in the table must be recog-
nized (e.g., by associating the table and/or subre-
gions thereof with precise table schemas in rela-
tional database style).
Existing table understanding work largely lies at
the level of superficial table classification or syn-
tactic recognition. Rarely, if ever, are precise logi-
cal relations assigned between the elements in the
table. Ad hoc heuristic approaches tend to rule,
rather than linguistic approaches.
On the other hand, in the linguistic approach ad-
vocated by Scott (2003) and (Power et al, 2003),
tables were not considered. The various physical
presentation elements discussed included head-
ings, captions, and bulleted lists?all of which
exhibit numerous similarities to tabular elements.
Possibly, tables were not considered because they
are difficult to describe adequately within the ex-
pressiveness of common linguistic formalisms like
CFGs.
The work presented here aims to address this
problem. Our model provides an enabling foun-
dation toward a linguistic approach by first shift-
ing to a two-dimensional CFG framework. This
permits us to construct a grammar where all the
rules are meaningfully discriminative, such that?
unlike existing table understanding models?any
analysis of a table includes a full parse tree that
assigns precise data model labels to all its regions
(including nested subregions) thereby specifying
the logical relations between the table?s elements.
Additionally, probabilities on the production rules
support thresholding (or ranking) of the alternative
candidate table interpretation hypotheses.
As with many natural language phenomena, a
full model of disambiguation must ultimately inte-
grate lexical semantics. However, in this research
step we focus on the question of howmuch seman-
tic interpretation can be performed on the basis of
other features, in the absence of a lexical or on-
tological model. Just as syntax and morphology
and prosody alone already permit much recogni-
tion and disambiguation of semantic roles and ar-
gument structure to be done for sentence, the same
can be done for tables. At the same time, we be-
lieve future integration of lexical semantics will be
facilitated by the grammatical framework of our
model.
One way to think about this is that we wish to
Table 1: Example ?Martian? table (see text).
Pbje Kwe Zxc Amc
Hoer 15 - 18 17 - 20 19 - 23
NQ 85 - 95% 70 - 90% 75 - 95%
Ncowifl Djhi Djhi Rubzlx
model what you might be able to recognize from a
?Martian? table such as that in Table 1. The non-
Martian reader relies solely on knowledge of al-
phabets and numbers, can spot font and formatting
clues, and is familiar with the conventions (i.e.,
grammars) of tables in general.
You might reasonably interpret this table as a
collection of vertical records with an attributes
header column (Pbje, Hoer, NQ, Ncowifl) on the
left. You might additionally interpret it as a ta-
ble that contains an record key header row (Kwe,
Zxc, Amc) along with the attributes header col-
umn (Pbje, Hoer, NQ, Ncowifl). You might as-
sign the latter interpretation a slightly higher prob-
ability, noticing the slightly longer form of Pbje
compared to Kwe, Zxc, and Amc. On the other
hand, even without reading English, you could re-
ject the interpretation as a collection of horizon-
tal records under the header attributes row (Pbje,
Kwe, Zxc, Amc), since each row contains differ-
ent forms and types, in a pattern that is consistent
across columns. Other interpretations are also pos-
sible, but unlikely given the regularity of the pat-
terns.
Thus by analyzing the structure of a table, the
reader would form a hypothesis about its data
model, providing a semantic interpretation that al-
lows the reader to extract information from the ta-
ble. As can be seen from the restored original
English version of the same example in Table 2,
the most likely interpretation was predicted even
without access to specific lexical knowledge. We
aim to show that a fairly useful baseline level of
semantic interpretation accuracy can already be
achieved, even with relatively little lexical and on-
tological knowledge.
We model these alternative hypotheses for the
interpretation of ambiguous tables as competing
parses. Just as with ordinary parsing and seman-
tic interpretation, the reader often builds multiple
competing interpretations of the same table.
Note that many previous models do not even
distinguish between the alternative possible inter-
pretations in the Martian example. Existing mod-
906
els such as Hurst (2000) and Yang (2002) inter-
pret tables with the same structural layout simply
by assigning them same data model, which stops
short of recognizing that it is necessary to rank
multiple competing interpretations that entail dif-
ferent sets of logical relations.
In contrast, our proposed model is capable of
producing multiple competing parses indicating
different semantic interpretations of tables having
the same structural layout, by selecting specific
data models for the table and its subregions.
2 Data Models for Specifying Semantic
Interpretations
To begin, some formal basis is needed to facilitate
precise specification of the alternative semantic in-
terpretations of a table, such that the exact logical
relations between its elements are unambiguously
specified. This will enable us to then design a ta-
ble understanding model that attempts to map any
given table (and recursively, its subregions) to al-
ternative data models depending on which is most
appropriate.
The set of data models we define below is a
more comprehensive and precise inventory than
found in the previous table analysis models dis-
cussed in this paper. It describes all the common
conventional patterns of logical relations we have
found in the course of empirically analyzing tables
from corpora. One advantage of this inventory of
data models arises from our appropriation of re-
lational database theory wherever possible to help
describe the form of the data models (Silberschatz
et al, 2002), allowing broad coverage of different
table types without sacrificing precision as to the
logical relations between entities.
Each data model assigns a clear semantics in
terms of logical relations between the table ele-
ments, thereby allowing extraction of relational
facts. In contrast, previous work on table analy-
sis tends to either classify a table using only one
single limited data model (e.g., Hurst (2000)), or
using data models which essentially are merely
surface layout types whose semantics are vague
and ambiguous (e.g., Yang (2002), Yang and Luk
(2002), Wang et al (2000), Yoshida et al (2001)).
A table is a logical view of a collection of inter-
related items usually presented as a row-column
structure such that the reader?s ability to access
and compare information can be enhanced, as also
noted by Wang (1996). From a database manage-
Table 2: Example from Table 1 in its original ver-
sion, with the English words restored.
Date Thu Fri Sat
Temp 15 - 18 17 - 20 19 - 23
RH 85 - 95% 70 - 90% 75 - 95%
Weather Cool Cool Cloudy
ment system perspective, each table can be con-
sidered as a (tiny) database. Like a program, the
reader accesses the data. As a result, we consider
that every table must correspond to a data model,
and this model determines how the reader extracts
information from the table.
Each data model has a schema which, as we
shall see below, may or may not surface (partially
or completely) as a subset of cells in the table that
describe attributes. Recognizing the data models
of a table correctly therefore also implies that both
attribute-value pairings and table structures have
been recognized.
At the top level, we categorize the data models
into three broad types:
? Flat model: A table is interpreted as a
database table in non-1NF normal relational
model.
? Nested model: A table is interpreted as a
database table in an object-relational model,
which allow complex types such as nested re-
lations and concept hierarchy.
? Dimensional data model: A table (usually
cross-tabular) is interpreted as a data cube
(multidimensional table) in a multidimen-
sional data model.
We now consider each of these types of data
models in turn.
2.1 Flat model
A flat model is used for the semantic interpretation
of any table as a relational database table in non-
1NF. For example, tables such as Tables 2 and 3
are often interpreted by humans in terms of flat
models. It is obvious that Table 3 can be viewed
as a relational database table with a schema (Pos,
Teams, Pld, Pts) and three records, because the
table?s surface form resembles how records are
stored in a relational database tables. Similarly,
Table 2 resembles a relational database table, but
transposed to a vertical orientation, with the first
907
Table 3: Example of a ranking table, which is typ-
ically laid out in a flat relational model.
Pos Teams Pld Pts
1. Chelsea 38 95
2. Arsenal 38 83
3. Man United 38 77
column as the schema (Date, Temp, RH, Weather)
and other columns as data records.
The flat model is closest to the 1-dimensional
table approach used by the majority of previous
models, but our approach designates the flat model
as a semantic representation, in contrast to the
previous models which see 1-dimensional tables
merely as a syntactic surface form (e.g., Yang
(2002), Yang and Luk (2002), Wang et al (2000),
Yoshida et al (2001)). While such previous mod-
els only recognize tables that are physically laid
out in this form, our approach clearly delineates an
explicit separation of syntax and semantics, which
provides greater flexibility allowing any table to be
interpreted as a flat model, regardless of its surface
form (though the flat model interpretation is more
common for some surface forms than others).
As an example showing that any kind of
table can be categorized as flat model, consider
Table 6. Even such a table can be semantically
interpreted as a flat model because related at-
tributes can join together to form a composite
attribute, though humans would less naturally
choose this semantic interpretation. Certainly
there are hierarchical relationship between
attributes; for example, Ass1 is a subtype of
Assignments. However, it is also valid to consider
the attributes along a hierarchical path as one
composite attribute. For example, ?Mark -> As-
signments -> Ass1? becomes the single attribute
?Mark-Assignments-Ass1?. Then the complete
flat model schema is (Year, Team, Mark-
Assignments-Ass1, Mark-Assignments-Ass2,
Mark-Assignments-Ass3, Mark-Examinations-
Midterm, Mark-Examinations-Final), and the first
record is (1991, Winter, 85, 80, 75, 60, 75, 75).
2.2 Nested model
With the exception of Hurst (2000), previous work
has not generally considered nested models in ex-
plicit fashion. Hurst (2000)?s model is based on
Wang (1996)?s abstract table model, in which at-
tributes may be related in a hierarchical way. On
the other hand, Wang et al (2000) oversimplis-
tically considers nested models as 1-dimensional,
thus missing the correct relationships between at-
tributes and values.
A nested model can be seen as a generalization
of the flat model, in which attributes may be re-
lated through composition or inheritance. Table 6
is naturally interpreted as a nested data model be-
cause the attributes have an inheritance relation-
ship. The corresponding schema is (Year, Team,
Mark (Assignments (Ass1, Ass2, Ass3), Exami-
nations (Midterm, Final, Grade)).
A nested model is not appropriate for tables
without hierarchical structure, such as Table 2 and
Table 3.
2.3 Dimensional model
Our approach also nicely handles dimensional
models, which are generally handled quite weakly
in previous models. A dimensional model refers
to a table, such as the table in Table 4, that resem-
bles a view of collection of data stored in multi-
dimensional data model. A multidimensional data
cube, as described in the database literature (e.g.,
Han and Kamber (2000), Chaudhuri and Dayal
(1997)), consists of a set of numeric measures
(though in fact the data need not be numeric), each
of which is determined by a set of dimensions.
Each dimension is described by a set of attributes.
For example, Table 5 can be semantically inter-
preted using the multidimensional data model de-
picted in Figure 1. Likewise, the cross-tabular ta-
ble in Table 4 can also be semantically interpreted
using the same multidimensional data model in
Figure 1. The value of the first three columns in
Table 5 are the dimension attributes and the rev-
enue values are the measures.
In contrast, among previous models, Yang
(2002) produces a semantically incorrect recogni-
tion of a multidimensional table that inappropri-
ately presents the attributes in hierarchical struc-
ture. Yang and Luk (2002) and Wang et al
(2000) only recognize the simplest 2-dimensional
case and apparently cannot handle 3 or more di-
mensions. Yoshida et al (2001) only handle 1-
dimensional cases.
A dimensional model is an inappropriate inter-
pretation for non-cross-tabular tables, such as Ta-
ble 2 and Table 3. A dimensional model is also not
valid for tables such as Table 6. Semantically, it
is not possible for ?Assignments? and ?Midterm?
908
Table 4: Example table showing revenue accord-
ing to Location = {Vancouver, Victoria}, Type =
{Phone, Computer} and Time = {2001, 2002}, us-
ing a tabular view of a 3-dimensional data cube.
Vancouver Victoria
Phone Computer Phone Computer
2001 845 1078 818 968
2002 943 1130 894 1024
1
Table 5: Example relational database table con-
taining the same logical information as Table 4.
Location Type Time Revenue
Vancouver Phone 2001 845
Vancouver Phone 2002 943
Vancouver Computer 2001 1078
Vancouver Computer 2002 1130
Victoria Phone 2001 818
Victoria Phone 2002 894
Victoria Computer 2001 968
Victoria Computer 2002 1024
Location
Type
Time
Vancouver Victoria
Phone
Computer
845
1078
2001 2002
Figure 1: Multidimensional data cube corre-
sponding to Tables 4 and 5.
to belong to different dimensions because it is in-
correct to determine the score by both ?Assign-
ments? and ?Midterm?. Syntactically, the texts
in the last attribute row of Table 6 are all unique;
however, the last attribute row of the table in Ta-
ble 4 is a repeating sequence of (?Phone?, ?Com-
puter?). Therefore, to a non-English reader, an
English cross-tabular table which possess repeat-
ing sequences in the attribute rows is likely to be
semantically interpreted as a dimensional model,
while a cross-tabular table which does not have
this property is likely to be interpreted as a nested
Table 6: Example table of grades.
Mark
Assignments Examinations
Ass1 Ass2 Ass3 Midterm Final Grade
Winter 85 80 75 60 75 75
Spring 80 65 75 60 70 70
Fall 80 85 75 55 80 75
Winter 85 80 70 70 75 75
Spring 80 80 70 70 75 75
Fall 75 70 65 60 80 70
Year Team
1991
1992
1
model.
3 A 2D SCFG Model for Table Analysis
In this section, we will present our two-
dimensional SCFG parsing model for table analy-
sis which has several advantages over the ad hoc
approaches. First, the probabilistic grammar ap-
proach permits a cleaner encapsulation and gen-
eralization of the kind of knowledge that previ-
ous models attempted to capture within their ad
hoc heuristics. Most previous works (e.g. Yang
(2002), Yang and Luk (2002), Hurst (2000), Hurst
(2002)) gradually built up their ad hoc heuristics
manually by inspecting some set of training sam-
ples. This approach may work if tables are from
limited domains of similar nature. However, like
text documents, the syntactic layout of textual ta-
bles may be determined by its context as well as its
language. For instance, it is natural for an Arabic
reader to read an Arabic table taking the rightmost
column as the attribute column, instead of the left-
most column. Yoshida et al (2001) use machine-
learning techniques to analyze nine types of table
structures, all 1-dimensional. Our grammar-based
approach allows the model to be readily adapted
to different situations by applying different sets of
grammar rules.
Another advantage is that grammatical ap-
proach can make more accurate decisions while
being simpler to implement, because it requires
only a single integrated parsing process to com-
plete the entire table analysis. This includes clas-
sifying the functions of each cell (as attribute or
value), pairing attributes and values, and identify-
ing the structure and the data model of a table. In
contrast, previous works require several stages to
complete the entire analysis, introducing complex
909
problems that are difficult to resolve, such as pre-
mature commitment to incorrect early-stage deci-
sions.
To our knowledgeWang et al (2000) is the only
textual table analysis model that uses a grammar
to describe table structures. However, in that case,
only a simple template matching analyzer is used.
Their grammar notation is unable to show both
physical structure and the semantics of a table at
the same time in a hierarchical manner. In con-
trast, information such as ?a data block contains
three rows of data cell? can be stored in the parse
tree constructed by our parsing model.
Outside of the table understanding literature,
there exists a different 2D parsing technique called
PLEX (Feder, 1971), (Costagliola et al, 1994)
which allows an object to have finite sets of attach-
ing points. PLEX is used to generate 2D diagrams
such as molecular structures, circuit diagrams and
flow charts in a grammatical way. However, we
consider it too complex and computationally ex-
pensive for our application because it does not ex-
ploit that fact that a textual table cell only has at
most four attaching points in fixed directions.
Our parser is a two-dimensional extension of
the conventional probabilistic chart parsing algo-
rithm (Lari and Young, 1990), (Goodman, 1998).
Intuitively, consider a sentence as a vector of to-
kens that will be parsed horizontally; then a ta-
ble is a matrix of tokens (like a crossword puzzle)
that will be parsed both horizontally and vertically.
Because of this, our parser must run in both direc-
tions. We achieve this by employing a grammar
notation that specifies the direction of parsing.
The two-dimensional grammar notation in-
cludes of a set of nonterminals, terminals, and two
generation operators ??>? and ?|->?. Let X be a
nonterminal and Y, Z, be two symbols which may
be either nonterminals or terminals. Then:
? X ?> Y Z denotes a horizontal production
rule saying that the nonterminal X horizon-
tally generates two symbols Y and Z.
? X |-> Y Z denotes a vertical production rule
saying that the nonterminal X vertically gen-
erates two symbols Y and Z.
? X ?> Y or X |-> Y equivalently denote a
unary production rule saying that the nonter-
minal X generates a symbol Y.
We assume that all rules are binary without loss
of generality, since any grammar can be mechan-
ically binarized without materially changing the
parse tree structure, just as in the case of ordinary
1D grammars.
The operators ??>? and ?|->? control the gen-
eration direction. In term of table analysis, a non-
terminal represents a matrix of tokens and a termi-
nal represents a single token. Sub-matrices gen-
erated by a horizontal rule will have same height
but not necessarily same width; similarly, sub-
matrices generated by a vertical rule will have
same width but not necessarily same height. In
other words, a matrix is partitioned into two halves
by the binary production rule.
Probabilities are placed on each rule, as in ordi-
nary 1D SCFGs. They are used to eliminate parses
falling below a threshold, which also helps to re-
duce the time complexity in practice.
Parsing with two-dimensional grammars can be
conceptualized most easily via parse tree exam-
ples. Figure 2 shows a complete parse tree for
parsing the table in Table 7 into a flat model. Fig-
ure 3 is a portion of a parse tree for parsing the
table in Table 8 into a nested model, while Figure
4 is a portion of parse trees for parsing Table 7 into
a dimensional model. The following is the gram-
mar fragment that gives the parse tree as Figure
2:
T1-1H |-> FlatModel
FlatModel |-> FlatSchema Records
FlatSchema --> CompositeAttribute FlatSchema
FlatSchema --> CompositeAttribute
Records |-> Record Records
Records |-> Record
Record --> Data Record
Record --> Record
Note that the internal nodes of the parse trees
serve to label subregions with data models, thus
assigning a semantic interpretation specifying the
exact logical relations between table elements.
None of the previous models construct declara-
tive parse trees like these, which are necessary for
many types of subsequent analysis, including in-
formation extraction applications.
4 Experimental Method
To the best of our knowledge, unfortunately none
of the table corpora mentioned in previous work
are available to the public. Thus, it was neces-
sary to construct a corpus for our experiments.
We collected a large sample of tables by issuing
Google searches with a list of random keywords,
for example, census age, confusion matrix, data
table, movie ranking, MSFT, school ranking, tele-
phone plan, tsunami numbers, weather report, and
910
D:\ust\ner\docs\emnlp\parse_flat.html
T1-1H |->
FlatModel |->
FlatSchema -->
Composite
Attribute |->
Attribute |->
VA
Composite
Attribute |->
Attribute
P
FlatSchema -->
Composite
Attribute |->
Attribute |->
VA
Composite
Attribute |->
Attribute
C
FlatSchema -->
Composite
Attribute |->
Attribute |->
VB
Composite
Attribute |->
Attribute |->
P
FlatSchema -->
Composite
Attribute |->
Attribute |->
VB
Composite
Attribute |->
Attribute
C
Records |->
Record -->
Data |->
11
Record -->
Data |->
12
Record -->
Data |->
13
Data |->
14
Records |->
Record -->
Data |->
21
Record -->
Data |->
22
Record -->
Data |->
23
Data |->
24
1
Figure 2: A parse tree for a flat model.
NestedModelSchema -->
NestedAttribute |->
Base |->
VA
Final -->
Attribute |->
P
Final -->
Attribute |->
C
NestedAttribute |->
Base |->
VB
Final -->
Attribute |->
X
Final -->
Attribute |->
Y
1
Figure 3: A partial parse for a nested model.
DimensionalModelSchema |->
Dimension -->
DimAttribute |->
DimAttribute |->
VA
Dimension -->
Dim
Attribute |->
P
Dimension
-->
Dim
Attribute |->
C
Dimension -->
DimAttribute |->
DimAttribute |->
VB
Dimension -->
Dim
Attribute |->
P
Dimension
-->
Dim
Attribute |->
C
1
Figure 4: A partial parse for a dimensional model.
so on. Tables were extracted from the collected
sample, automatically cleaned, and tokenized into
two-dimensional array of tokens.
Table 7: Example table for Figures 2 and 4.
VA VB
P C P C
11 12 13 14
21 22 23 24
1
Table 8: Example table for Figure 3.
VA VB
P C X Y
11 12 13 14
21 22 23 24
1
Table 9: Example table showing a floor legend.
6 School of Business & Management
5 Department of Biochemistry
4 Classrooms 4202 - 4205
3 Department of Computer Science
3 Department of Mathematics
For the blind evaluation, a human annotator in-
dependently manually annotated a randomly cho-
sen sample of 45 tables from the collection. All ta-
bles in the evaluation sample were previously un-
seen test cases, never inspected prior to the con-
struction of the two-dimensional grammar.
Each tokenized table was tagged by the human
judge with a list of types T relevant to the table.
The relevance is defined as follows: a data model
is relevant to a table if and only if the human
would agree that such a data model would natu-
rally be hypothesized as an interpretation for that
table (analogously to the way that word senses are
manually annotated for WSD evaluations). Each
type is a tuple of the form (R, O, S), where R is
the relevant data model, O is the reading orienta-
tion of R, and S is a boolean saying if a schema
(i.e. attributes) exist in the table. Thus, Table 2
would be tagged as {(flat, vertical, true)} while
the table in Table 4 would be tagged as {(flat, hor-
izontal, true), (flat, vertical, true), (dimensional, ,
true)}. But Table 9 may be tagged as {(flat, hor-
izontal, false)}. The exceptions are that both the
nested model and the dimensional model always
have a schema, while the dimensional model does
not have orientation. In cases where multiple legit-
imate readings were possible, the table was tagged
911
Table 10: Experimental results.
Precision Recall
0.60 0.80
with multiple types. A total of 92 relevant types
were generated from the tokenized tables.
We processed the tokenized tables with the two-
dimensional SCFG parser, and computed the pre-
cision and the recall rates against the judge?s lists
of tags for all the test cases.
5 Results and Discussion
The experimental results are summarized in Table
10. All tables could be parsed; in general, it is very
rare for any table to be rejected by the parser, since
the grammar permits so many different configura-
tions that can be recursively composed.
Unfortunately it is impossible to compare re-
sults directly against previous models, since nei-
ther those models nor the data they evaluated on
are available.
Moreover, it is difficult to compare with pre-
vious models as our evaluation criteria are more
stringent than in earlier work. Most previous work
evaluated the performance in terms of the (vaguer
and less demanding) criteria of number of correct
attribute-value pairings. Such an evaluation ap-
proach gives unduly high weight to large repetitive
tables, and neglects structural errors in the analysis
of the table. In contrast, our approach gives equal
weight to all tables regardless of how many entries
they contain, requires semantically valid structural
analyses, and yet still accepts any parse that yields
the correct attribute-value pairings (since the tag-
ging of the test set includes all legitimate types
when there are multiple valid alternatives).
The fact that precision was lower than recall is
due to the fact that many tables were wrongly in-
terpreted as tables without schema or in wrong ori-
entations. The current grammar has difficulty dis-
tinguishing attributes from values. Significant im-
provement can be obtained by using constraints to
limit the number of incorrect parses, a strategy we
are currently implementing.
6 Conclusion
We have introduced a framework to support a
more linguistically-oriented approach to finer in-
terpretation of tables, using two-dimensional sto-
chastic CFGs with Viterbi parsing to find appro-
priate semantic interpretations of textual tables in
terms of different data models. This approach
yields a concise model that at the same time fa-
cilitates broader coverage than existing models,
and is more easily scalable and maintainable. We
also introduce a cleaner and richer data model to
represent semantic interpretations, and illustrate
how it systematically captures a wider range of ta-
ble types. Without such a data model, the right
attribute-value relations caanot be extracted from
a table, even if surface elements like ?header? and
?data? are correctly labeled as previous models at-
tempted to do. Our experiments show that even
without other ontological and linguistic knowl-
edge, excellent semantic interpretation accuracy
can be obtained by parsing with a two-dimensional
grammar based on these data models, by using
a wide variety of surface features in the terminal
symbols. We plan next to extend the model by in-
corporating ontological and linguistic knowledge
for additional disambiguation leverage.
References
Surajit Chaudhuri and Umesh Dayal. An overview of data warehousing and
OLAP technology. ACM SIGMOD Record, 26(1), March 1997.
Gennaro Costagliola, Andrea De Lucia, and Sergio Orefice. Towards efficient
parsing of diagrammatic languages. In AVI ?94: Proceedings of the work-
shop on Advanced visual interfaces, pages 162?171, New York, NY, USA,
1994. ACM Press.
Jerome Feder. Plex languages. Information Sciences, 3:225?241, 1971.
Joshua T. Goodman. Parsing Inside-Out. PhD thesis, Harvard University,
1998.
Jiawei Han and Micheline Kamber. Data Mining: Concepts and Techniques.
Morgan Kaufmann, 1 edition, 2000.
Matthew Francis Hurst. The Interpretation of Tables in Texts. PhD thesis, The
University of Edinburgh, 2000.
Matthew Hurst. Classifying table elements in html. In The 11th International
World Wide Web Conference, Hawaii, USA, 2002.
K. Lari and S. J. Young. The estimation of stochastic context-free grammars
using the inside-outside algorithm. Computer Speech and Language, 4:35?
36, 1990.
Richard Power, Donia Scott, and Nadjet Bouayad-Agha. Document structure.
Computational Linguistics, 29(4):211?260, Dec 2003.
Donia Scott. Layout in NLP: The case for document structure (invited talk).
In 41st Annual Meeting of the Association for Computational Linguistics
(ACL-2003), Aug 2003.
Abraham Silberschatz, Henry F. Korth, and S. Sudarshan. Database System
Concepts. McGraw-Hill, 4th edition, 2002.
H. L. Wang, S. H. Wu, I. C. Wang, C. L. Sung, W. L. Hsu, and W. K. Shih.
Semantic search on internet tabular information extraction for answering
queries. In CIKM ?00: Proceedings of the ninth international conference
on Information and knowledge management, pages 243?249, New York,
NY, USA, 2000. ACM Press.
Xinxin Wang. Tabular Abstraction, Editing, and Formatting. PhD thesis, The
University of Waterloo, Waterloo, Ontario, Canada, 1996.
Yingchen Yang and Wo-Shun Luk. A framework for web table mining. In
WIDM ?02: Proceedings of the 4th international workshop on Web infor-
mation and data management, pages 36?42, New York, NY, USA, 2002.
ACM Press.
Yingchen Yang. Web table mining and database discovery. Master?s thesis,
Simon Fraser University, August 2002.
M. Yoshida, K. Torisawa, and J. Tsujii. A method to integrate tables of the
world wide web, 2001.
912
 	
ffA Stacked, Voted, Stacked Model for Named Entity Recognition
Dekai Wu?? and Grace Ngai? and Marine Carpuat?
? Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
{dekai,marine}@cs.ust.hk
? Hong Kong Polytechnic University
Department of Computing
Kowloon
Hong Kong
csgngai@polyu.edu.hk
Abstract
This paper investigates stacking and voting
methods for combining strong classifiers like
boosting, SVM, and TBL, on the named-entity
recognition task. We demonstrate several ef-
fective approaches, culminating in a model that
achieves error rate reductions on the develop-
ment and test sets of 63.6% and 55.0% (En-
glish) and 47.0% and 51.7% (German) over the
CoNLL-2003 standard baseline respectively,
and 19.7% over a strong AdaBoost baseline
model from CoNLL-2002.
1 Introduction
We describe multiple stacking and voting methods that
effectively combine strong classifiers such as boosting,
SVM, and TBL, for the named-entity recognition (NER)
task. NER has emerged as an important step for many
natural language applications, including machine trans-
lation, information retrieval and information extraction.
Much of the research in this field was pioneered in
the Message Understanding Conference (MUC) (Sund-
heim, 1995), which performed detailed entity extraction
and identification on English documents. As a result,
most current NER systems which have impressive per-
formances have been specially constructed and tuned for
English MUC-style documents. It is unclear how well
they would perform when applied to another language.
Our system was designed for the CoNLL-2003 shared
task, the goal of which is to identify and classify four
types of named entities: PERSON, LOCATION, ORGA-
NIZATION and MISCELLANEOUS. The task specifi-
cations were that two languages would be involved. We
*The author would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research in
part through two research grants (RGC 6083/99E and RGC
6256/00E).
were given about a month to develop our system on the
first language, which was English, but only two weeks to
adapt it to the surprise language, which was German.
Given the goal of the shared task, we designed our
system to achieve a high performance without relying
too heavily on knowledge that is very specific for a par-
ticular language or domain. In the spirit of language-
independence, we avoided using features and information
which would not be easily obtainable for almost any ma-
jor language.
2 Classification Methods
To carry out the stacking and voting experiments, we con-
structed a number of relatively strong individual compo-
nent models of the following kinds.
2.1 Boosting
The main idea behind boosting algorithms is that a set
of many weak classifiers can be effectively combined to
yield a single strong classifier. Each weak classifier is
trained sequentially, increasingly focusing more heavily
on the instances that the previous classifiers found diffi-
cult to classify.
For the boosting framework, our system uses Ada-
Boost.MH (Freund and Schapire, 1997), an n-ary classifi-
cation variant of the original binary AdaBoost algorithm.
It performs well on a number of natural language process-
ing problems, including text categorization (Schapire and
Singer, 2000) and word sense disambiguation (Escudero
et al, 2000). In particular, it has also been demonstrated
that boosting can be used to build language-independent
NER models that perform exceptionally well (Wu et al,
2002; Carreras et al, 2002).
2.2 Support Vector Machines
Support Vector Machines (SVMs) have gained a con-
siderable following in recent years (Boser et al, 1992),
particularly in dealing with high-dimensional spaces
such as commonly found in natural language prob-
lems like text categorization (Joachims, 1998). SVMs
have shown promise when applied to chunking (Kudo
and Matsumoto, 2001) and named entity recognition
(Sassano and Utsuro, 2000; McNamee and Mayfield,
2002), though performance is quite sensitive to param-
eter choices.
2.3 Transformation-based Learning
Transformation-based learning (Brill, 1995) (TBL) is a
rule-based machine learning algorithm that was first in-
troduced by Brill and used for part-of-speech tagging.
The central idea of transformation-based learning is to
learn an ordered list of rules which progressively improve
upon the current state of the training set. An initial as-
signment is made based on simple statistics, and then
rules are greedily learned to correct the mistakes, until
no net improvement can be made.
The experiments presented in this paper were per-
formed using the fnTBL toolkit (Ngai and Florian, 2001),
which implements several optimizations in rule learning
to drastically speed up the time needed for training.
3 Data Resources
3.1 Preprocessing the Data
The data that was provided by the CoNLL organizers was
sentence-delimited and tokenized, and hand-annotated
with named entity chunks. The English data was au-
tomatically labeled with part-of-speech and chunk tags
from the memory-based tagger and chunker (Daelemans
et al, 1996), and the German data was labelled with the
decision-tree-based TreeTagger (Schmidt, 1994). We re-
placed the English part-of-speech tags with those gener-
ated by a transformation-based learner (Ngai and Florian,
2001). The chunk tags did not appear to help in either
case and were discarded.
As we did not want to overly rely on characteristics
which were specific to the Indo-European language fam-
ily, we did not perform detailed morphological analysis;
but instead, an approximation was made by simply ex-
tracting the prefixes and suffixes of up to 4 characters
from all the words.
In order to let the system generalize over word types,
we normalized the case information of all the words in
the corpus by converting them to uniform lower case. To
recapture the lost information, each word was annotated
with a tag that specified if it was in all lower case, all
upper case, or was of mixed case.
3.2 Gazetteers
Apart from the training and test data, the CoNLL orga-
nizers also provided two lists of named entities, one in
English and one in Dutch. Part of the challenge for this
year?s shared task was to find ways of using this resource
in the system.
To supplement the provided gazetteers, a large col-
lection of names and words was downloaded from var-
ious web sources. This collection was used to compile
a gazetteer of 120k uncategorized English proper names
and a lexicon of 500k common English words. As there
were no supplied gazetteers for German, we also com-
piled a gazetteer of 8000 German names, which were
mostly personal first and last names and geographical lo-
cations, and a lexicon of 32k common German words.
Named entities in the corpus which appeared in the
gazetteers were identified lexically or using a maximum
forward match algorithm similar to that used in Chinese
word segmentation. Once named entities have been iden-
tified in this preprocessing step, each word can then be
annotated with an NE chunk tag corresponding to the out-
put from the system. The learner can view the NE chunk
tag as an additional feature.
The variations in this approach come from resolving
conflicts between different possible type information for
the same NE. The different ways that we dealt with the
problem were: (1) Rank all the NE types by frequency in
the training corpus. In the case of a conflict, default to
the more common NE. (2) Give all the possible NEs to
the boosting learner as a set of possible NE chunk tags.
(3) Discard the NE type information and annotate each
word with a tag indicating whether it is inside an NE.
4 Classifier Combination
It is a well-known fact that if several classifiers are avail-
able, they can be combined in various ways to create
a system that outperforms the best individual classifier.
Since we had several classifiers available to us, it was rea-
sonable to investigate combining them in different ways.
4.1 Stacking
Like voting, stacking is a learning paradigm that con-
structs a combined model from several classifiers. The
basic concept behind stacking is to train two or more clas-
sifiers sequentially, with each successive classifier incor-
porating the results of the previous ones in some fashion.
4.1.1 Integration of External Resources
As mentioned above, at the most basic level, lexicon
and gazetteer information was integrated into our classi-
fiers by including them as additional features. However
we also experimented with several different ways of in-
corporating this information via stacking?one possible
approach was to view the gazetteers as a separate system
that would produce an output and then implement stack-
ing to combine their outputs.
4.1.2 Division into Subtasks
One of the most straightforward approaches to stack-
ing can be applied to tasks that are naturally divisible into
hierarchically ordered subtasks. An example approach,
which was taken by several of the participating teams in
the CoNLL-2002 shared task, is to split the NER task into
the identification phase, where named entities are identi-
fied in the text; and the classification phase, where the
identified named entities are categorized into the various
subtypes. Provided that the performance of the individ-
ual classifier is fairly high (otherwise errors made in the
earlier stages could propagate down the chain), this has
the advantage of reducing the complexity of the task for
each individual classifier.
To construct such a system, we trained a stacked Ada-
Boost.MH classifier to perform NE reclassification on
boundaries identified in the base model. The output of the
initial models are postprocessed to remove all NE type
information and then passed to this stacked classifier. As
Table 1 shows, stacking the boosting models yields a sig-
nificant gain in performance.
English devel. Precision Recall F?=1
(Boost) Base 88.64% 87.68% 88.16
(Boost) Base + Stacked 89.26% 88.29% 88.77
Table 1: Improving classification of NE types via stacked
AdaBoost.MH.
4.1.3 Error Correction
Another approach to stacking that we investigated in
this work involves a closer interaction between the mod-
els. The general overview of this approach is for a given
model to use the output of another trained model as its
initial state, and to improve beyond it. The idea is that
the second model, with a different learning and represen-
tation bias, will be able to move out of the local maxima
that the previous model has settled into.
To accomplish this we introduced Stacked TBL
(STBL), a variant of TBL tuned for this purpose (Wu et
al., 2003). We found TBL to be an appropriate point of
departure since it starts from an initial state of classifi-
cation and learns rules to iteratively correct the current
labeling. We aimed to use STBL to improve the base
model from the preceding section.
STBL proved quite effective; in fact it yielded the best
base model performance among all our models. Table 2
shows the result of stacking STBL on the boosting base
model.
4.2 Voting
The simplest approach to combining classifiers is through
voting, which examines the outputs of the various mod-
English devel. Precision Recall F?=1
(Boost) Base 88.64% 87.68% 88.16
(Boost + STBL) Base 87.83% 88.79% 88.31
Table 2: Improving the above AdaBoost.MH base model,
via Stacked TBL (STBL).
els and selects the classifications which have a weight
exceeding some threshold, where the weight is depen-
dent upon the models that proposed this particular clas-
sification. The variations in this approach stem from the
method by which weights are attached to the models. It
is possible to assign varying weights to the models, in ef-
fect giving one model more ?say? than the others. In our
system, however, we simply assigned each model equal
weight, and selected classifications which were proposed
by a majority of models.
Voting was thus used to further improve the base
model. Four models chosen for heterogeneity partici-
pated in the voting: two variants of the AdaBoost.MH
model, the SVM model, and the Stacked TBL model.
As before, the stacked AdaBoost.MH reclassifier was
applied to the voted result, yielding a final stacked voted
stacked model.
This model gave the best overall results on the task as
a whole. Table 3 shows the results of our system.
English devel. Precision Recall F?=1
Boost1 + Stacked 89.26% 88.29% 88.77
Boost2 + Stacked 82.98% 85.62% 84.28
SVM + Stacked 84.41% 85.71% 85.05
Boost + STBL + Stacked 89.09% 88.07% 88.57
Voted + Stacked 90.18% 88.86% 89.51
Table 3: Improving classification of NE types, via stacked
voted stacked AdaBoost.MH, STBL, and SVM models.
5 Overall Results
Complete results on the development and test sets, for
both English and German, are shown in Table 4.
6 Conclusion
This paper has presented an overview of our entry to
the CoNLL-2003 shared task. As individual component
models, we constructed strong AdaBoost.MH models,
SVM models, and Stacked TBL models, and provided
them with detailed features on the data.
We then demonstrated several stacking and voting
models that proved capable of improving performance
further. This was non-trivial since the individual compo-
nent models were all quite strong to begin with. Because
of this the vast majority of classifier combination models
we tested actually turned out to degrade performance, or
showed zero improvement. The models presented here
worked well because they were each motivated by de-
tailed analyses.
We did investigate a number of ways in which
gazetteers could be incorporated. The gazetteer supplied
for the shared task was found not to improve perfor-
mance significantly, because our models were already ad-
equately powerful to correctly identify most of the named
entities supplied by the gazetteer. However, minimal ef-
fort to augment the gazetteers did result in a performance
boost. Moreover, performance was further improved by
the inclusion of a common word lexicon not containing
any named entities.
Inspection revealed that some errors found in the
output of the system stemmed from either erroneous
sentence boundaries in the test data, or difficult-to-avoid
inconsistencies in the the gold standard annotations. For
example, in the following:
1. . . . [ Panamanian ]MISC boxing legend . . .
2. . . . [ U.S. ]LOC collegiate national champion . . .
both ?Panamanian? and ?U.S.? are used as modifiers, but
one is annotated as a MISC-type NE while the other is
considered a LOC-type.
The stacked voted stacked model obtained an improve-
ment of 4.83 F-Measure points on the English devel-
opment set over our best model from the CoNLL-2002
shared task which we took as our baseline, resulting in
a substantial 19.7% error rate reduction. The system
achieves this respectable performance using very little
in the way of outside resources?only a part-of-speech
tagger and some common wordlists?which can be ob-
tained easily for almost any major language. Most fea-
tures we used can also be used for uninflected and non-
Indo-European languages such as Chinese, where the pre-
fixes and suffixes can be replaced by decomposing the
words at the character level. This is in keeping with the
the language-independent spirit of the shared task.
References
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik. 1992. A training al-
gorithm for optimal margin classifiers. In David Haussler, editor, Proceedings
of the 4th Workshop on Computational Learning Theory, pages 144?152, San
Mateo, CA. ACM Press.
Eric Brill. 1995. Transformation-based error-driven learning and natural lan-
guage processing: A case study in part of speech tagging. Computational
Linguistics, 21(4):543?565.
Xavier Carreras, Llu?is Ma`rquez, and Llu?is Padro?. 2002. Named entity extrac-
tion using adaboost. In Proceedings of CoNLL-2002, pages 167?170. Taipei,
Taiwan.
Walter Daelemans, Jakub Zavrel, and Peter Berck. 1996. MBT: A memory-based
part of speech tagger-generator.
Gerard Escudero, Llu?is Ma`rquez, and German Rigau. 2000. Boosting applied to
word sense disambiguation. In European Conference on Machine Learning,
pages 129?141.
English devel. Precision Recall F?=1
LOC 91.88% 92.98% 92.42
MISC 91.53% 83.19% 87.16
ORG 86.43% 80.76% 83.50
PER 90.39% 93.49% 91.91
Overall 90.18% 88.86% 89.51
English test Precision Recall F?=1
LOC 86.27% 85.91% 86.09
MISC 75.88% 77.07% 76.47
ORG 78.19% 72.97% 75.49
PER 83.94% 87.26% 85.57
Overall 82.02% 81.39% 81.70
German devel. Precision Recall F?=1
LOC 75.13% 61.39% 67.57
MISC 75.46% 45.05% 56.42
ORG 80.05% 47.86% 59.91
PER 74.19% 61.96% 67.52
Overall 75.92% 54.67% 63.56
German test Precision Recall F?=1
LOC 74.94% 60.97% 67.23
MISC 72.77% 51.04% 60.00
ORG 61.22% 42.69% 50.30
PER 83.68% 73.39% 78.20
Overall 75.20% 59.35% 66.34
Table 4: Overall results of stacked voted stacked model
on development and test sets.
Yoav Freund and Rob E. Schapire. 1997. A decision-theoretic generalization of
on-line learning and an application to boosting. In Journal of Computer and
System Sciences, 55(1), pages 119?139.
Thorsten Joachims. 1998. Text categorization with support vector machines:
learning with many relevant features. In Claire Ne?dellec and Ce?line Rou-
veirol, editors, Proceedings of ECML-98, 10th European Conference on Ma-
chine Learning, number 1398, pages 137?142, Chemnitz, DE. Springer Ver-
lag, Heidelberg, DE.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of NAACL-2001.
Paul McNamee and James Mayfield. 2002. Entity extraction without language-
specific resources. In Proceedings of CoNLL-2002, pages 183?186. Taipei,
Taiwan.
Grace Ngai and Radu Florian. 2001. Transformation-based learning in the fast
lane. In Proceedings of NAACL?01, pages 40?47, Pittsburgh, PA. ACL.
Manabu Sassano and Takehito Utsuro. 2000. Named entity chunking techniques
in supervised learning for Japanese named entity recognition. In Proceedings
of COLING-2000, pages 705?711.
Rob E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system
for text categorization. In Machine Learning, 39(2/3, pages 135?168.
Helmut Schmidt. 1994. Probabilistic part-of-speech tagging using decision trees.
In International Conference on New Methods in Natural Language Process-
ing, pages 44?49, Manchester, U.K.
Beth Sundheim. 1995. MUC6 named entity task defnition, version 2.1. In Pro-
ceedings of the Sixth Message Understanding Conference (MUC6).
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang.
2002. Boosting for named entity recognition. In Proceedings of CoNLL-2002,
pages 195?198. Taipei, Taiwan.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. Forthcoming.
An Efficient Algorithm to Induce Minimum Average Lookahead Grammars
for Incremental LR Parsing
Dekai WU1 Yihai SHEN
dekai@cs.ust.hk shenyh@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology, Clear Water Bay, Hong Kong
Abstract
We define a new learning task, minimum average
lookahead grammar induction, with strong poten-
tial implications for incremental parsing in NLP and
cognitive models. Our thesis is that a suitable learn-
ing bias for grammar induction is to minimize the
degree of lookahead required, on the underlying
tenet that language evolution drove grammars to be
efficiently parsable in incremental fashion. The in-
put to the task is an unannotated corpus, plus a non-
deterministic constraining grammar that serves as
an abstract model of environmental constraints con-
firming or rejecting potential parses. The constrain-
ing grammar typically allows ambiguity and is it-
self poorly suited for an incremental parsing model,
since it gives rise to a high degree of nondetermin-
ism in parsing. The learning task, then, is to in-
duce a deterministic LR (k) grammar under which
it is possible to incrementally construct one of the
correct parses for each sentence in the corpus, such
that the average degree of lookahead needed to do
so is minimized. This is a significantly more dif-
ficult optimization problem than merely compiling
LR (k) grammars, since k is not specified in ad-
vance. Clearly, na??ve approaches to this optimiza-
tion can easily be computationally infeasible. How-
ever, by making combined use of GLR ancestor ta-
bles and incremental LR table construction meth-
ods, we obtain an O(n3 + 2m) greedy approxima-
tion algorithm for this task that is quite efficient in
practice.
1 Introduction
Marcus? (1980) Determinism Hypothesis proposed
that natural language can be parsed by a mechanism
that operates ?strictly deterministically? in that it
does not simulate a nondeterministic machine. Al-
though the structural details of the deterministic LR
1The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
parsing model we employ in this paper diverge from
those of Marcus, fundamentally we adhere to his
constraints that (1) all syntactic substructures cre-
ated are permanent, which prohibits simulating de-
terminism by backtracking, (2) all syntactic sub-
structures created for a given input must be part of
the output structure assigned to that input, which
prohibits memoizing intermediate results as in dy-
namic programming or beam search, and (3) no
temporary syntactic structures are encoded within
the internal state of the machine, which prohibits
the moving of temporary structures into procedural
codes.
A key issue is that, to give the Determinism Hy-
pothesis teeth, it is necessary to limit the size of the
decision window. Otherwise, it is always possible
to circumvent the constraints simply by increasing
the degree of lookahead or, equivalently, increasing
the buffer size (which we might call the degree of
?look-behind?); either way, increasing the decision
window essentially delays decisions until enough
disambiguating information is seen. In the limit, a
decision window equal to the sentence length ren-
ders the claim of incremental parsing meaningless.
Marcus simply postulated that a maximum buffer
size of three was sufficient. In contrast, our ap-
proach permits greater flexibility and finer grada-
tions, where the average degree of lookahead re-
quired can be minimized with the aim of assisting
grammar induction.
Since Marcus? work, a significant body of work
on incremental parsing has developed in the sen-
tence processing community, but much of this work
has actually suggested models with an increased
amount of nondeterminism, often with probabilistic
weights (e.g., Narayanan & Jurafsky (1998); Hale
(2001)).
Meanwhile, in the way of formal methods,
Tomita (1986) introduced Generalized LR parsing,
which offers an interesting hybrid of nondetermin-
istic dynamic programming surrounding LR parsing
methods that were originally deterministic.
Additionally, methods for determinizing and
minimizing finite-state machines are well known
(e.g., Mohri (2000), B   al & Carton (1968)). How-
ever, such methods (a) do not operate at the context-
free level, (b) do not directly minimize lookahead,
and (c) do not induce grammars under environmen-
tal constraints.
Unfortunately, there has still been relatively lit-
tle work on automatic learning of grammars for de-
terministic parsers to date. Hermjakob & Mooney
(1997) describe a semi-automatic procedure for
learning a deterministic parser from a treebank,
which requires the intervention of a human expert
in the loop to determine appropriate derivation or-
der, to resolve parsing conflicts between certain ac-
tions such as ?merge? and ?add-into?, and to iden-
tify specific features for disambiguating actions. In
our earlier work we described a deterministic parser
with a fully automatically learned decision algo-
rithm (Wong and Wu, 1999). But unlike our present
work, the decision algorithms in both Hermjakob &
Mooney (1997) and Wong & Wu (1999) are pro-
cedural; there is no explicit representation of the
grammar that can be meaningfully inspected.
Finally, we observe that there are also trainable
stochastic shift-reduce parser models (Briscoe and
Carroll, 1993), which are theoretically related to
shift-reduce parsing, but operate in a highly nonde-
terministic fashion during parsing.
We believe the shortage of learning models for
deterministic parsing is in no small part due to the
difficulty of overcoming computational complexity
barriers in the optimization problems this would in-
volve. Many types of factors need to be optimized
in learning, because deterministic parsing is much
more sensitive to incorrect choice of structural fea-
tures (e.g., categories, rules) than nondeterministic
parsing that employ robustness mechanisms such as
weighted charts.
Consequently, we suggest shifting attention to the
development of new methods that directly address
the problem of optimizing criteria associated with
deterministic parsing, in computationally feasible
ways. In particular, we aim in this paper to develop
a method that efficiently searches for a parser under
a minimum average lookahead cost function.
It should be emphasized that we view the role of
a deterministic parser as one component in a larger
model. A deterministic parsing stage can be ex-
pected to handle most input sentences, but not all.
Other nondeterministic mechanisms will clearly be
needed to handle a minority of cases, the most ob-
vious being garden-path sentences.
In the sections that follow, we first formalize the
learning problem. We then describe an efficient ap-
proximate algorithm for this task. The operation of
this algorithm is illustrated with an example. Fi-
nally, we give an analysis of the complexity charac-
teristics of the algorithm.
2 The minimum average lookahead
(MAL) grammar problem
We formulate the learning task as follows:
Definition Given an unannotated corpus
S = {S
1
, . . . , S
|S|
} plus a constraining gram-
mar G
C
, the minimum average lookahead grammar
GMAL(S,GC) is defined as
arg min
G?G
C
??i:parse
G
(S
i
) =?
?k (G)
where the average lookahead objective function
?k (G) is the average (over S) amount of lookahead
that an LR parser for G needs in order to determinis-
tically parse the sample S without any shift-reduce
or reduce-reduce conflicts. If G is ambiguous in the
sense that it generates more than one parse for any
sentence in S, then ?k (G) = ? since a conflict is
unavoidable no matter how much lookahead is used.
In other words, GMAL is the subset of rules of
G
C
that requires the smallest number of lookaheads
on average so as to make parsing S using this subset
of G deterministic.
Note that the constraining grammar G
C
by na-
ture is not deterministic. The constraining gram-
mar serves merely as an abstract model of envi-
ronmental constraints that confirm or reject poten-
tial parses. Since such feedback is typically quite
permissive, the constraining grammar typically al-
lows a great deal of ambiguity. This of course ren-
ders the constraining grammar itself poorly suited
for an incremental parsing model, since it gives rise
to a high degree of nondeterminism in parsing. In
other words, we should not expect the constraining
grammar alone to contain sufficient information to
choose a deterministically parsable grammar.
For expository simplicity we assume all gram-
mars are in standard context-free form in the dis-
cussion that follows, although numerous notational
variations, generalizations, and restricted cases are
certainly possible. We note also that, although the
formalization is couched in terms of standard syn-
tactic phrase structures, there is no reason why one
could not employ categories and/or attributes on
parse nodes representing semantic features. Do-
ing so would permit the framework to accommodate
some semantic information in minimizing looka-
head for deterministic parsing, which would be
more realistic from a cognitive modeling stand-
point. (Of course, further extensions to integrate
more complex incremental semantic interpretation
mechanisms into this framework could be explored
as well.)
Finding the minimum average lookahead gram-
mar is clearly a difficult optimization problem. To
compute the value of ?k (G), one needs the LR table
for that particular G, which is expensive to compute.
Computing the LR table for all G ? G
C
would be
infeasible. It is a natural conjecture, in fact, that
the problem of learning MAL grammars is NP-hard.
We therefore seek an approximation algorithm with
good performance, as discussed next.
3 An efficient approximate algorithm for
learning incremental MAL parsers
We now describe an approximate method for effi-
ciently learning a MAL grammar. During learning,
the MAL grammar is represented simultaneously as
both a set of standard production rules as well as an
LR parsing table. Thus the learning algorithm out-
puts an explicit declarative rule set together with a
corresponding compiled LR table.
3.1 Approximating assumptions
To overcome the obstacles mentioned in the previ-
ous section, we make the following approximations:
1. Incremental approximation for MAL rule set
computation. We assume that the MAL gram-
mar for a given corpus is approximately equal
to the sequential union of the MAL grammar
rules for each sentence in the corpus, where the
set of MAL grammar rules for each sentence is
determined relative to the set of all rules se-
lected from preceding sentences in the corpus.
2. Incremental approximation for LR state set
computation. We assume that the correct set
of LR states for a given set of rules is approx-
imately equal to that obtained by incremen-
tally modifying the LR table and states from
a slightly smaller subset of the rules. (Our ap-
proach exploits the fact that the correct set of
states for LR (k) parsers is always independent
of k.)
Combining these approximation assumptions en-
ables us to utilize a sentence-by-sentence greedy ap-
proach to seeking a MAL grammar. Specifically,
the algorithm iteratively computes a minimum aver-
age lookahead set of rules for each sentence in the
training corpus, accumulating all rules found, while
keeping the LR state set and table updated. The full
algorithm is fairly complex; we discuss its key as-
pects here.
3.2 Structure of the iteration
As shown in Figure 1, find MAL parser accepts
as input an unannotated corpus S = {S
1
, . . . , S
|S|
}
plus a constraining grammar G
C
, and outputs the
LR table for a parser that can deterministically parse
the entire training corpus using a minimum average
lookahead.
The algorithm consists of an initialization step
followed by an iterative loop. In the initialization
step in lines 1?3, we create an empty LR table T ,
along with an empty set A of parsing action se-
quences defined as follows. A parsing action se-
quence A (P ) for a given parse P is the sequence
of triples (state, input, action) that a shift-reduce
parser follows in order to construct P . At any given
point, T will hold the LR table computed from the
MAL parse of all sentences already processed, and
A will hold the corresponding parsing sequences for
the MAL parses.
Entering the loop, we iteratively augment T by
adding the states arising from the MAL parse F ? of
each successive sentence in the training corpus and,
in addition, cache the corresponding parsing action
sequence A (F ?) into the set A. This is done by
first computing a chart for the sentence, in line 4, by
parsing S
i
under the constraining grammar G
C
us-
ing the standard Earley (1970) procedure. We then
call find MAL parse in line 5, to compute the parse
that requires minimum average lookahead to resolve
ambiguity. The items and states produced by the
rules in F ?are added to the LR table T by calling
incremental update LR in line 6, and the parsing
action sequence of F ? is appended to A in line 7.
Note that the indices of the original states in T are
not changed and only items are added into them if
need be so that A is not changed by adding items and
states to T, and there might be new states introduced
which are also indexed.
By definition, the true MAL grammar does not
depend on the order of the sentences the learning al-
gorithm inspects. However, find MAL parser pro-
cesses the example sentences in order, and attempts
to find the MAL grammar sentence by sentence.
The order of the sentences impacts the grammar
produced by the learning algorithm, so it is not guar-
anteed to find the true MAL grammar. However the
approximation is well motivated particularly when
we have large numbers of example sentences.
3.3 Incrementally updating the LR table
Given the structure of the loop, it can be seen that
efficient learning of the set of MAL rules cannot be
achieved without a component that can update the
LR table incrementally as each rule is added into the
find MAL parser(S,G
C
)
1. T ? ?
2. A ? ?
3. i ? 0
4. C ? chart parse(S
i
, G
C
)
5. F ? ? find MAL parse(C,A,R)
6. T ? incremental update LR(T, F ?)
7. append(A,A(F ? ))
8. if i < |S| then i ? i + 1; goto 4
Figure 1: Main algorithm find MAL parser.
current MAL grammar. Otherwise, every time a rule
is found to be capable of reducing average looka-
head and therefore is added into the MAL gram-
mar, the LR table must be recomputed from scratch,
which is sufficiently time consuming as to be infea-
sible when trying to learn a MAL grammar with a
realistically large input corpus and/or constraining
grammar.
The incremental update LR function incre-
mentally updates the LR table in an efficient fash-
ion that avoids recomputing the entire table. The
inputs to incremental update LR are a pre-existing
LR table T , and a set of new rules R to be added.
This algorithm is derived from the incremental LR
parser generator algorithm ilalr and is relatively
complex; see Horspool (1988) for details. Histor-
ically, work on incremental parser generators first
concentrated on LL(1) parsers. Fischer (1980) was
first to describe a method for incrementally updat-
ing an LR(1) table. Heering et al(1990) use the
principle of lazy evaluation to attack the same prob-
lem. Our design of incremental update LR is more
closely related to ilalr for the following reasons:
? ilalr has the property that when updating the
LR table to contain the newly added rules, it
does not change the index of each already ex-
isting state. This is important for our task as
the slightest change in the states might affect
significantly the parsing sequences of the sen-
tences that have already been processed.
? Although worst case complexity for ilalr is ex-
ponential in the number of rules in the gram-
mar, empirically it is quite efficient in practical
use. Heuristics are used to improve the speed
of the algorithm, and as we do not need to com-
pute lookahead sets, the speed of the algorithm
can be further improved.
compute average lookahead(r,A)
1. h ? lookahead(r,A)
2. if ?v
1
= (i, s, a
1
, k
1
, r
1
, d
1
)
? then k = k
1
= (m
1
, l
1
)
? else k = (0,?)
3. // note v? = (i?, s?, a?, k?, r?, d?) and k? =
(m?, l?)
4. l?? = m?l?+h
m
?
+1
5. if l?? < l
? then l = l??
? else m = m? + 1
Figure 2: Algorithm compute average lookahead.
The method is approximate, and may yield slight,
acceptable deviations from the optimal table. ilalr
is not an exact LR table generator in the sense that
it may create states that should not exist and may
miss some states that should exist. The algorithm
is based on the assumption that most states in the
original LR table occur with the same kernel items
in the updated LR table. Empirically, the assump-
tion is valid as the proportion of superfluous states
is typically only in the 0.1% to 1.3% range.
3.4 Finding minimum average lookahead
parses
The function find MAL parse selects the full parse
F? of a given sentence that requires the least av-
erage lookahead ?k (A( F )) to resolve any shift-
reduce or reduce-reduce conflicts with a set A of
parsing action sequences, such that F? is a sub-
set of a chart C . The inputs to find MAL parse,
more specifically, are a chart C containing all the
partial parses in the input sentence, and the set A
containing the parsing action sequences of the MAL
parse of all sentences processed so far. The algo-
rithm operates by constructing a graph-structured
stack of the same form as in GLR parsing (Tomita,
1986)(Tomita and Ng, 1991) while simultaneously
computing the minimum average lookahead. Note
that Tomita?s original method for constructing the
graph-structured stack has exponential time com-
plexity O
(
n?+1
)
, in which n and ? are the length of
the sentence and the length of the longest rhs of any
rule in the grammar. As a result, Tomita?s algorithm
achieves O
(
n3
)
for grammars in Chomsky normal
form but is potentially exponential when produc-
tions are of unrestricted length, which in practice
is the case with most parsing problems. We fol-
low Kipps (1991) in modifying Tomita?s algorithm
to allow it to run in time proportional to n3 for
grammars with productions of arbitrary length. The
most time consuming part in Tomita?s algorithm is
when reduce actions are executed in which the an-
cestors of the current node have to be found in-
curring time complexity n?. To avoid this we em-
ploy an ancestor table to keep the ancestors of each
node in the GLR forest which is updated dynam-
ically as the GLR forest is being constructed. This
modification brings down the time complexity of re-
duce actions to n2 in the worst case, and allows the
function build GLR forest to construct the graph-
structured stack in O
(
n3
)
. Aside from constructing
the graph-structured stack, we compute the average
lookahead for each LR state transition taken during
the construction. Whenever there is a shift or re-
duce action in the algorithm, a new vertex for the
graph-structured stack is generated, and the func-
tion compute average lookahead is called to ascer-
tain the average lookahead of the new vertex. Fi-
nally, reconstruct MAL parse is called to recover
the full parse F? for the MAL parsing action se-
quence.
Figure 2 shows the com-
pute average lookahead function, which es-
timates the average lookahead of a vertex v
generated by an LR state transition r. To facilitate
computations involving average lookahead, we
use a 6-tuple (i, s, a, k, r, d) instead of the more
common triple form (i, s, a) to represent each
vertex in the graph-structured stack, where:
? i: The index of the right side of the coverage
of the vertex. The vertices with the same right
side i will be kept in U
i
.
? s: The state in which the vertex is in.
? a: The ancestor table of the vertex.
? k: The average lookahead information, in the
form of a pair (m, l) where l is the minimum
average lookahead of all paths leading from the
root to this vertex and m is the number of state
transitions in that MAL path.
? r: The parsing action that generates the ver-
tex along the path that needs minimum average
lookahead. r is a triple (d
1
, d
2
, f) denoting ap-
plying the action f on the vertex d
1
to generate
the vertex d
2
.
? d: The unique index of the vertex.
The inputs to compute average lookahead are an
LR state transition r = (d?, d, f) taken in the con-
struction of the graph-structured stack where d? and
Table 1: Example constraining grammar.
(1) S ? NP VP
(2) VP ? v NP
(3) VP ? v PP
(4) VP ? v
(5) VP ? v p
(6) VP ? v det
(7) PP ? p NP
(8) NP ? NP PP
(9) NP ? n
(10) NP ? det n
(11) VP ? VP n
d are the index of vertices and f is an action, and the
set A containing the parsing action sequences of the
MAL parse of all sentences processed so far. Let
v = (i, s, a, k, r, d) be the new vertex with index d,
and let v? = (i?, s?, a?, k?, r?, d?) be the vertex with
index d?. The function proceeds by first computing
the lookahead needed to resolve conflicts between r
and A. Next we check whether v is a packed node
and initialize k in v; if not, k is initialized to (0, 0),
and otherwise it is copied from the packed node. We
then compute the average lookahead needed to go
from v? to v and check whether it provides a more
economical way to resolve conflicts. The average
lookahead of a vertex v generated by applying an
action f on vertex v? can be computed from k? of v?
and the lookahead needed to generate v from v ?. v
can be generated by applying different actions on
different vertices and k keeps the one that needs
minimum average lookahead and f keeps that ac-
tion.
Finally, the reconstruct MAL parse function
is called after construction of the entire graph-
structured stack is complete in order to recover the
full minimum average lookahead parse tree. We as-
sume the grammar has only one start symbol and
rebuild the parse tree from the state that is labelled
with the start symbol.
4 An example
We now walk through a simplified example so as
to fix ideas and illustrate the operation of the algo-
rithm. Table 1 shows a simple constraining gram-
mar G
C
which we will use for this example.
Now consider the small training corpus:
1. I did.
2. He went to Africa.
3. I bought a ticket.
Table 2: LR state transitions and lookaheads for
sentence 1.
[S [NP In ] [VP didv ] ] ?k
(0, 1, sh, 0) (1, 2, re9, 0) (2, 4, sh, 0)
(4, 5, re4, 0) (5, 3, re1, 0) (3, acc) 0*
To begin with, find MAL parser considers sen-
tence 1. In this particular case, chart parse(S
1
, G
C
)
finds only one valid parse. The GLR forest is built,
giving the LR state transitions and parsing actions
shown in Table 2, where each tuple (d?, d, f, k)
gives the state prior to the action, the state resulting
from the action, the action, and the average looka-
head. Here compute average lookahead determines
that the average lookahead ?k is 0. From this parse
tree, incremental update LR accepts rules (1), (4),
and (9) and updates the previously empty LR table
T .
Next, find MAL parser considers sentence 2.
Here, chart parse(S
1
, G
C
) finds two possible
parses, leading to the LR state transitions and pars-
ing actions shown in Table 3. This time, the average
lookahead calculation is sensitive to the what was
already entered into the LR table T during the pre-
vious step of processing sentence 1. For example,
in the first parse, the fourth transition (4, 6, sh, 1)
requires a lookahead of 1 in order to avoid a shift-
reduce conflict with (4, 5, re4, 0) from sentence 1.
The sixth transition (1, 9, re9, 2) requires a looka-
head of 2. It turns out that the first parse has an aver-
age lookahead of 0.20,while the second parse has an
average lookahead of 0.33. We thus prefer the first
parse tree, calling incremental update LR to further
update the LR table T using rules (3) and (7).
Finally, find MAL parser considers sentence
3. Again, chart parse(S
1
, G
C
) finds two possible
parses, leading this time to the LR state transi-
tions and parsing actions shown in Table 4. Vari-
ous lookaheads are again needed to avoid conflicts
with the existing rules in T . The first parse has an
average lookahead of 0.22, and is selected in pref-
erence to the second parse which has an average
lookahead of 0.33. From the first parse tree, in-
cremental update LR accepts rules (2) and (10) to
again update the LR table T .
Thus the final output MAL grammar, requiring a
lookahead of 1, is shown in Table 5.
Table 3: LR state transitions and lookaheads for
sentence 2.
[S [NP Hen ] [VP wentv [PP top [NP African ] ] ] ] ?k
(0, 1, sh, 0) (1, 2, re9, 0) (2, 4, sh, 0)
(4, 6, sh, 1) (6, 1, sh, 0) (1, 9, re9, 1)
(9, 7, re7, 0) (7, 5, re3, 0) (5, 3, re1, 0)
(3, acc) 2/10
[S [NP Hen ] [VP [VP wentv top ] African ] ]
(0, 1, sh, 0) (1, 2, re9, 0) (2, 4, sh, 0)
(4, 6, sh, 1) (6, 5, re5, 0) (5, 8, sh, 1)
(8, 5, re11, 0) (5, 3, re1, 1) (3, acc) 3/9
Table 4: LR state transitions and lookaheads for
sentence 3.
[S [NP In ] [VP boughtv [NP adet ticketn ] ] ] ?k
(0, 1, sh, 0) (1, 2, re9, 1) (2, 4, sh, 0)
(4, 8, sh, 1) (8, 11, sh, 0) (11, 12, re10, 0)
(12, 5, re12, 0) (5, 3, re1, 0) (3, acc) 2/9
[S [NP In ] [VP [VP boughtv adet ] ticketn ] ]
(0, 1, sh, 0) (1, 2, re9, 1) (2, 4, sh, 0)
(4, 8, sh, 1) (8, 5, re6, 0) (5, 10, sh, 1)
(10, 5, re11, 0) (5, 3, re1, 0) (3, acc) 3/9
Table 5: Output MAL grammar.
(1) S ? NP VP
(2) VP ? v NP
(3) VP ? v PP
(4) VP ? v
(7) PP ? p NP
(9) NP ? n
(10) NP ? det n
5 Complexity analysis
5.1 Time complexity
Since the algorithm executes each of its five main
steps once for each sentence in the corpus, the time
complexity of the algorithm is upper bounded by
the sum of the time complexities of those five steps.
Suppose n is the maximum length of any sentence
in the corpus, and m is the number of rules in the
grammar. Then for each of the five steps:
1. chart parse is O
(
n3
)
.
2. build GLR forest is O
(
n3
)
. As discussed
previously, the use of an ancestor table allows
the graph-structured stack to be built in O(n3)
in the worst case.
3. compute average lookahead is O
(
n2
)
. As
the number of lookaheads needed by each pars-
ing action is computed by comparing the pars-
ing action with the MAL parsing action se-
quences for all previous sentences, the time
complexity of this function depends on the
maximum length of any sentence that has al-
ready been processed, which is bounded by n.
The dynamic programming method used to lo-
cate the most economical parse in terms of av-
erage lookahead, described above, can be seen
to be quadratic in n.
4. reconstruct MAL parse is O
(
n2
)
. This is
bounded by the number of LR state transi-
tions in each full parse of the sentence, which
is is O
(
n2
)
. Note, however, that Tanaka et
al. (1992) propose an enhancement that can re-
construct the parse trees in time linear to n; this
is a direction for future improvement of our al-
gorithm.
5. incremental update LR is O (2m). As with
ilalr, theoretically the worst time complexity is
exponential in the number of rules in the exist-
ing grammar. However, various heuristics can
be employed to make the algorithm quicker,
and in practical experiments the algorithm is
quite fast and precise in producing LR tables,
particularly since m is very small relative to
|S|.
The time complexity of the algorithm for each
sentence is thus O
(
n3
)
+ O
(
n3
)
+ O
(
n2
)
+
O
(
n2
)
+ O (2m) which is O(n3 + 2m). Given |S|
sentences in the corpus, the total training time is
O (( n3 + 2m) ? |S |).
5.2 Space complexity
As with time complexity, an upper bound on the
space complexity can be obtained from the five
main steps:
1. chart parse is O
(
n3
)
.
2. build GLR forest is O
(
n2
)
. The space com-
plexity of both Tomita?s original algorithm and
the modified algorithm is n2.
3. compute average lookahead is O
(
n2
)
. The
space usage of compute average lookahead di-
rectly corresponds to the dynamic program-
ming structure, like the time complexity.
4. reconstruct MAL parse is O (n). This is
bounded by the number of vertices in the
graph-structured stack, which is is O (n).
5. incremental update LR is O (2m). As with
time complexity, although the worst time com-
plexity is exponential in the number of rules in
the existing grammar, in practice this is not the
major bottleneck.
The space complexity of the algorithm is thus
O
(
n3
)
+O
(
n2
)
+O
(
n2
)
+O (n)+O (2m) which
is again O(n3 + 2m).
6 Conclusion
We have defined a new grammar learning task based
on the concept of a minimum average lookahead
(MAL) objective criterion. This approach provides
an alternative direction for modeling of incremen-
tal parsing: it emphatically avoids increasing the
amount of nondeterminism in the parsing models,
as has been done in across a wide range of recent
models, including probabilized dynamic program-
ming parsers as well as GLR approaches. In con-
trast, the objective here is to learn completely de-
terministic parsers from unannotated corpora, with
loose environmental guidance from nondeterminis-
tic constraining grammars.
Within this context, we have presented a greedy
algorithm for the difficult task of learning approx-
imately MAL grammars for deterministic incre-
mental LR(k) parsers, with a time complexity of
O (( n3 + 2m) ? |S |) and a space complexity of
O(n3 + 2m). This algorithm is efficient in prac-
tice, and thus enables a broad range of applications
where degree of lookahead serves as a grammar in-
duction bias.
Numerous future directions are suggested by this
model. One obvious line of work involves experi-
ments varying the types of corpora as well as the nu-
merous parameters within the MAL grammar learn-
ing algorithm, to test predictions against various
modeling criteria. More efficient algorithms and
heuristics could help further increase the applicabil-
ity of the model. In addition, the accuracy of the
model could be strengthened by reducing sensitiv-
ity to some of the approximating assumptions.
References
Marie-Pierre Beal and Olivier Carton. Determiniza-
tion of transducers over finite and infinite words.
Theoretical Computer Science, 289(1), 1968.
Ted Briscoe and John Carroll. Generalised prob-
abilistic LR parsing for unification-based gram-
mars. Computational Linguistics, 19(1):25?60,
1993.
Jay Earley. An efficient context-free parsing algo-
rithm. Communications of the Association for
Computing Machinery, 13(2), 1970.
G. Fischer. Incremental LR(1) parser construction
as an aid to syntactical extensibility. Technical
report, Department of Computer Science, Univer-
sity of Dortmund, Federal Republic of Germany,
1980. PhD Dissertation, Tech. Report 102.
John Hale. A probabilistic Earley parser as a psy-
cholinguistic model. In NAACL-2001: Second
Meeting of the North American Chapter of the
Association for Computational Linguistics, 2001.
Jan Heering, Paul Klint, and Jan Rekers. Incremen-
tal generation of parsers. IEEE Transactions on
Software Engineering, 16(12):1344?1351, 1990.
Ulf Hermjakob and Raymond J. Mooney. Learn-
ing parse and translation decisions from examples
with rich context. In ACL/EACL?97: Proceed-
ings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics and 8th Con-
ference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 482?
489, 1997.
R. Nigel Horspool. Incremental generation of LR
parsers. Technical report, University of Victoria,
Victoria, B.C., Canada, 1988. Report DCS-79-
IR.
James R. Kipps. GLR parsing in time O(n3). In
M. Tomita, editor, Generalized LR Parsing, pages
43?60. Kluwer, Boston, 1991.
Mitchell P. Marcus. A Theory of Syntactic Recog-
nition for Natural Language. MIT Press, Cam-
bridge, MA, 1980.
Mehryar Mohri. Minimization algorithms for se-
quential transducers. Theoretical Computer Sci-
ence, 234(1?2):177?201, 2000.
Srini Narayanan and Daniel Jurafsky. Bayesian
models of human sentence processing. In Pro-
ceedings of CogSci-98, 1998.
Hozumi Tanaka, K.G. Suresh, and Koiti Yamada. A
family of generalized LR parsing algorithms us-
ing ancestors table. Technical report, Department
of Computer Science, Tokyo Institute of Technol-
ogy, Tokyo, Japan, 1992. TR92-0019.
Masaru Tomita and See-Kiong Ng. The Generalized
LR parsing algorithm. In Masaru Tomita, edi-
tor, Generalized LR Parsing, pages 1?16. Kluwer,
Boston, 1991.
Masaru Tomita. Efficient Parsing for Natural Lan-
guage. Kluwer, Boston, 1986.
Aboy Wong and Dekai Wu. Learning a
lightweight robust deterministic parser. In EU-
ROSPEECH?99: Sixth European Conference on
Speech Communication and Technology, Bu-
dapest, Sep 1999.
Augmenting Ensemble Classification for Word Sense Disambiguation with a
Kernel PCA Model
Marine CARPUAT Weifeng SU Dekai WU1
marine@cs.ust.hk weifeng@cs.ust.hk dekai@cs.ust.hk
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology, Clear Water Bay, Hong Kong
Abstract
The HKUST word sense disambiguation systems
benefit from a new nonlinear Kernel Principal
Component Analysis (KPCA) based disambigua-
tion technique. We discuss and analyze results
from the Senseval-3 English, Chinese, and Multi-
lingual Lexical Sample data sets. Among an en-
semble of four different kinds of voted models, the
KPCA-based model, along with the maximum en-
tropy model, outperforms the boosting model and
na??ve Bayes model. Interestingly, while the KPCA-
based model typically achieves close or better ac-
curacy than the maximum entropy model, neverthe-
less a comparison of predicted classifications shows
that it has a significantly different bias. This char-
acteristic makes it an excellent voter, as confirmed
by results showing that removing the KPCA-based
model from the ensemble generally degrades per-
formance.
1 Introduction
Classifier combination has become a standard ar-
chitecture for shared task evaluations in word sense
disambiguation (WSD), named entity recognition,
and similar problems that can naturally be cast as
classification problems. Voting is the most com-
mon method of combination, having proven to be
remarkably effective yet simple.
A key problem in improving the accuracy of such
ensemble classification systems is to find new vot-
ing models that (1) exhibit significantly different
prediction biases from the models already voting,
and yet (2) attain stand-alone classification accura-
cies that are as good or better. When either of these
conditions is not met, adding the new voting model
typically degrades the accuracy of the ensemble in-
stead of helping it.
In this work, we investigate the potential of one
promising new disambiguation model with respect
1The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
to augmenting our existing ensemble combining a
maximum entropy model, a boosting model, and
a na??ve Bayes model?a combination representing
some of the best stand-alone WSD models cur-
rently known. The new WSD model, proposed
by Wu et al (2004), is a method for disambiguat-
ing word senses that exploits a nonlinear Kernel
Principal Component Analysis (KPCA) technique.
That the KPCA-based model could potentially be
a good candidate for a new voting model is sug-
gested by Wu et al?s empirical results showing that
it yielded higher accuracies on Senseval-2 data sets
than other models that included maximum entropy,
na??ve Bayes, and SVM based models.
In the following sections, we begin with a de-
scription of the experimental setup, which utilizes
a number of individual classifiers in a voting en-
semble. We then describe the KPCA-based model
to be added to the baseline ensemble. The accuracy
results of the three submitted models are examined,
and also the individual voting models are compared.
Subsequently, we analyze the degree of difference
in voting bias of the KPCA-based model from the
others, and finally show that this does indeed usu-
ally lead to accuracy gains in the voting ensemble.
2 Experimental setup
2.1 Tasks evaluated
We performed experiments on the following lexical
sample tasks from Senseval-3:
English (fine). The English lexical sample task
includes 57 target words (32 verbs, 20 nouns and
5 adjectives). For each word, training and test in-
stances tagged with WordNet senses are provided.
There are an average of 8.5 senses per target word
type, ranging from 3 to 23. On average, 138 training
instances per target word are available.
English (coarse). This modified evaluation of the
preceding task employs a sense map that groups
fine-grained sense distinctions into the same coarse-
grained sense.
Chinese. The Chinese lexical sample task in-
cludes 21 target words. For each word, several
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
senses are defined using the HowNet knowledge
base. There are an average of 3.95 senses per tar-
get word type, ranging from 2 to 8. Only about 37
training instances per target word are available.
Multilingual (t). The Multilingual (t) task is de-
fined similarly to the English lexical sample task,
except that the word senses are the translations into
Hindi, rather than WordNet senses. The Multilin-
gual (t) task requires finding the Hindi sense for 31
English target word types. There are an average of
7.54 senses per target word type, ranging from 3 to
16. A relatively large training set is provided (more
than 260 training instances per word on average).
Multilingual (ts). The Multilingual (ts) task uses
a different data set of 10 target words and provides
the correct English sense of the target word for both
training and testing. There are an average of 6.2
senses per target word type, ranging from 3 to 11.
The training set for this subtask was smaller, with
about 150 training instances per target word.
2.2 Ensemble classification
The WSD models presented here consist of ensem-
bles utilizing various combinations of four voting
models, as follows. Some of these component mod-
els were also evaluated on other Senseval-3 tasks:
the Basque, Catalan, Italian, and Romanian Lexical
Sample tasks (Wicentowski et al, 2004), as well as
Semantic Role Labeling (Ngai et al, 2004).
The first voting model, a na??ve Bayes model, was
built as Yarowsky and Florian (2002) found this
model to be the most accurate classifier in a compar-
ative study on a subset of Senseval-2 English lexical
sample data.
The second voting model, a maximum entropy
model (Jaynes, 1978), was built as Klein and Man-
ning (2002) found that it yielded higher accuracy
than na??ve Bayes in a subsequent comparison of
WSD performance. However, note that a different
subset of either Senseval-1 or Senseval-2 English
lexical sample data was used.
The third voting model, a boosting model (Fre-
und and Schapire, 1997), was built as boosting has
consistently turned in very competitive scores on re-
lated tasks such as named entity classification (Car-
reras et al, 2002)(Wu et al, 2002). Specifically, we
employed an AdaBoost.MH model (Schapire and
Singer, 2000), which is a multi-class generalization
of the original boosting algorithm, with boosting on
top of decision stump classifiers (decision trees of
depth one).
The fourth voting model, the KPCA-based
model, is described below.
All classifier models were selected for their abil-
ity to able to handle large numbers of sparse fea-
tures, many of which may be irrelevant. More-
over, the maximum entropy and boosting models are
known to be well suited to handling features that are
highly interdependent.
2.3 Controlled feature set
In order to facilitate a controlled comparison across
the individual voting models, the same feature set
was employed for all classifiers. The features are
as described by Yarowsky and Florian (2002) in
their ?feature-enhanced na??ve Bayes model?, with
position-sensitive, syntactic, and local collocational
features.
2.4 The KPCA-based WSD model
We briefly summarize the KPCA-based model here;
for full details including illustrative examples and
graphical interpretation, please refer to Wu et al
(2004).
Kernel PCA Kernel Principal Component Analy-
sis is a nonlinear kernel method for extracting non-
linear principal components from vector sets where,
conceptually, the n-dimensional input vectors are
nonlinearly mapped from their original space Rn
to a high-dimensional feature space F where linear
PCA is performed, yielding a transform by which
the input vectors can be mapped nonlinearly to a
new set of vectors (Scho?lkopf et al, 1998).
As with other kernel methods, a major advantage
of KPCA over other common analysis techniques is
that it can inherently take combinations of predic-
tive features into account when optimizing dimen-
sionality reduction. For WSD and indeed many nat-
ural language tasks, significant accuracy gains can
often be achieved by generalizing over relevant fea-
ture combinations (see, e.g., Kudo and Matsumoto
(2003)). A further advantage of KPCA in the con-
text of the WSD problem is that the dimensionality
of the input data is generally very large, a condition
where kernel methods excel.
Nonlinear principal components (Diamantaras
and Kung, 1996) are defined as follows. Suppose
we are given a training set of M pairs (xt, ct) where
the observed vectors xt ? Rn in an n-dimensional
input space X represent the context of the target
word being disambiguated, and the correct class ct
represents the sense of the word, for t = 1, ..,M .
Suppose ? is a nonlinear mapping from the input
space Rn to the feature space F . Without loss of
generality we assume the M vectors are centered
vectors in the feature space, i.e.,
?M
t=1 ?(xt) = 0;
uncentered vectors can easily be converted to cen-
tered vectors (Scho?lkopf et al, 1998). We wish to
diagonalize the covariance matrix in F :
C = 1M
M
?
j=1
?(xj) ?T (xj) (1)
To do this requires solving the equation ?v =
Cv for eigenvalues ? ? 0 and eigenvectors v ?
Rn\ {0}. Because
Cv = 1M
M
?
j=1
(?(xj) ? v)? (xj) (2)
we can derive the following two useful results. First,
? (?(xt) ? v) = ? (xt) ? Cv (3)
for t = 1, ..,M . Second, there exist ?i for i =
1, ...,M such that
v =
M
?
i=1
?i?(xi) (4)
Combining (1), (3), and (4), we obtain
M?
M
?
i=1
?i (?(xt) ? ?(xi ))
=
M
?
i=1
?i(? (xt) ?
M
?
j=1
?(xj)) (?(xj) ? ?(xi ))
for t = 1, ..,M . Let K? be the M ? M matrix such
that
K?ij = ?(xi) ? ?(xj) (5)
and let ??1 ? ??2 ? . . . ? ??M denote the eigenval-
ues of K? and ??1 ,..., ??M denote the corresponding
complete set of normalized eigenvectors, such that
??t(??t ? ??t) = 1 when ??t > 0. Then the lth nonlinear
principal component of any test vector xt is defined
as
ylt =
M
?
i=1
??li (?(xi) ? ?(xt )) (6)
where ??li is the lth element of ??l .
See Wu et al (2004) for a possible geometric in-
terpretation of the power of the nonlinearity.
WSD using KPCA In order to extract nonlin-
ear principal components efficiently, first note that
in both Equations (5) and (6) the explicit form of
?(xi) is required only in the form of (?(xi) ?
?(xj)), i.e., the dot product of vectors in F . This
means that we can calculate the nonlinear princi-
pal components by substituting a kernel function
k(xi, xj) for (?(xi) ? ?(xj )) in Equations (5) and
(6) without knowing the mapping ? explicitly; in-
stead, the mapping ? is implicitly defined by the
kernel function. It is always possible to construct
a mapping into a space where k acts as a dot prod-
uct so long as k is a continuous kernel of a positive
integral operator (Scho?lkopf et al, 1998).
Thus we train the KPCA model using the follow-
ing algorithm:
1. Compute an M ? M matrix K? such that
K?ij = k(xi, xj) (7)
2. Compute the eigenvalues and eigenvectors of
matrix K? and normalize the eigenvectors. Let
??1 ? ??2 ? . . . ? ??M denote the eigenvalues
and ??1,..., ??M denote the corresponding com-
plete set of normalized eigenvectors.
To obtain the sense predictions for test instances,
we need only transform the corresponding vectors
using the trained KPCA model and classify the re-
sultant vectors using nearest neighbors. For a given
test instance vector x, its lth nonlinear principal
component is
ylt =
M
?
i=1
??lik(xi, xt) (8)
where ??li is the ith element of ??l.
For our disambiguation experiments we employ a
polynomial kernel function of the form k(xi, xj) =
(xi ? xj)d, although other kernel functions such as
gaussians could be used as well. Note that the de-
generate case of d = 1 yields the dot product kernel
k(xi, xj) = (xi?xj) which covers linear PCA as a
special case, which may explain why KPCA always
outperforms PCA.
3 Results and discussion
3.1 Accuracy
Table 1 summarizes the results of the submitted sys-
tems along with the individual voting models. Since
our models attempted to disambiguate all test in-
stances, we report accuracy (precision and recall be-
ing equal). Earlier experiments on Senseval-2 data
showed that the KPCA-based model significantly
outperformed both na??ve Bayes and maximum en-
tropy models (Wu et al, 2004). On the Senseval-
3 data, the maximum entropy model fares slightly
better: it remains significantly worse on the Multi-
lingual (ts) task, but achieves statistically the same
accuracy on the English (fine) task and is slightly
Table 1: Comparison of accuracy results for various HKUST ensemble and individual models on Senseval-
3 Lexical Sample tasks, confirming the high accuracy of the KPCA-based model. All test instances were
attempted. (Bold model names were the systems entered.)
English
(fine)
English
(coarse)
Chinese Multilingual
(t)
Multilingual
(ts)
HKUST comb2 (me, boost, nb, kpca) 71.4 78.6 66.2 62.0 63.8
HKUST comb (me, boost, kpca) 70.9 78.1 66.5 61.4 63.8
HKUST me 69.3 76.4 64.4 60.6 60.8
HKUST kpca 69.2 - 63.6 60.0 63.3
HKUST boost 67.0 - 64.1 57.3 60.3
HKUST nb 64.3 - 60.4 57.3 56.8
Table 2: Confusion matrices showing that the KPCA-based model votes very differently from the other
models on the Senseval-3 Lexical Sample tasks. Percentages representing disagreement between KPCA and
other voting models are shown in bold.
kpca vs: me boost nb
task incorrect correct incorrect correct incorrect correct
English incorrect 24.14% 6.62% 21.60% 9.15% 21.04% 9.71%
(fine) correct 6.59% 62.65% 11.38% 57.86% 14.63% 54.61%
Chinese incorrect 24.01% 12.40% 22.96% 13.46% 26.65% 9.76%
correct 11.61% 51.98% 12.93% 50.66% 12.93% 50.66%
Multilingual incorrect 32.71% 7.33% 32.04% 8.01% 30.54% 9.51%
(t) correct 6.74% 53.22% 10.63% 49.33% 12.20% 47.75%
Multilingual incorrect 33.17% 3.52% 31.66% 5.03% 30.15% 6.53%
(ts) correct 6.03% 57.29% 8.04% 55.28% 13.07% 50.25%
more accurate on the Multilingual (t) task. For un-
known reasons?possibly the very small number of
training instances per Chinese target word, as men-
tioned earlier?there is an exception on the Chinese
task, where boosting outperforms the KPCA-based
model. We are investigating the possible causes.
The na??ve Bayes model remains significantly worse
under all conditions.
3.2 Differentiated voting bias
For a new voting model to raise the accuracy of an
existing classifier ensemble, it is not only important
that the new voting model achieve accuracy compa-
rable to the other voters, as shown above, but also
that it provides a significantly differentiated predic-
tion bias than the other voters. Otherwise, the accu-
racy is typically hurt rather than helped by the new
voting model.
To examine whether the KPCA-based model sat-
isfies this requirement, we compared its predictions
against each of the other classifiers (for those tasks
where we have been given the answer key). Table 2
shows nine confusion matrices revealing the per-
centage of instances where the KPCA-based model
votes differently from one of the other voters. The
disagreement between KPCA and the other voting
models ranges from 6.03% to 14.63%, as shown
by the bold entries in the confusion matrices. Note
that where there is disagreement, the KPCA-based
model predicts the correct sense with significantly
higher accuracy, in nearly all cases.
3.3 Voting effectiveness
The KPCA-based model exhibits the accuracy and
differentiation characteristics requisite for an effec-
tive additional voter, as shown in the foregoing sec-
Table 3: Comparison of the accuracies for the voting ensembles with and without the KPCA voter, confirm-
ing that adding the KPCA-based model to the voting ensemble always helps on Senseval-3 Lexical Sample
tasks.
English
(fine)
English
(coarse)
Chinese Multilingual
(t)
Multilingual
(ts)
HKUST comb3 (me, boost, nb) 71.2 - 67.5 60.6 60.8
HKUST comb2 (me, boost, nb, kpca) 71.4 78.6 66.2 62.0 63.8
tions. To verify that adding the KPCA-based model
to the voting ensemble indeed improves accuracy,
we compared our voting ensemble?s accuracies to
that obtained with KPCA removed. The results,
shown in Table 3, confirm that the KPCA-based
model generally helps on Senseval-3 Lexical Sam-
ple tasks. The only exception is on Chinese, due
to the aforementioned anomaly of boosting outper-
forming KPCA on that task. In the Multilingual (t)
and (ts) cases, the improvement in accuracy is sig-
nificant.
4 Conclusion
We have described our word sense disambiguation
system and its performance on the Senseval-3 En-
glish, Chinese, and Multilingual Lexical Sample
tasks. The system consists of an ensemble clas-
sifier utilizing combinations of maximum entropy,
boosting, na??ve Bayes, and a new Kernel PCA based
model.
We have demonstrated that our new model based
on Kernel PCA is, along with maximum entropy,
one of the most accurate stand-alone models vot-
ing in the ensemble, as evaluated under carefully
controlled to ensure the same optimized feature set
across all models being compared. Moreover, we
have shown that the KPCA model exhibits a signif-
icantly different classification bias, a characteristic
that makes it a valuable voter in an ensemble. The
results confirm that accuracy is generally improved
by the addition of the KPCA-based model.
References
Xavier Carreras, Llu??s Ma`rques, and Llu??s Padro?. Named entity
extraction using AdaBoost. In Dan Roth and Antal van den
Bosch, editors, Proceedings of CoNLL-2002, pages 167?
170, Taipei, Taiwan, 2002.
Konstantinos I. Diamantaras and Sun Yuan Kung. Principal
Component Neural Networks. Wiley, New York, 1996.
Yoram Freund and Robert E. Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting. In Journal of Computer and System Sciences,
55(1), pages 119?139, 1997.
E.T. Jaynes. Where do we Stand on Maximum Entropy? MIT
Press, Cambridge MA, 1978.
Dan Klein and Christopher D. Manning. Conditional struc-
ture versus conditional estimation in NLP models. In Pro-
ceedings of EMNLP-2002, Conference on Empirical Meth-
ods in Natural Language Processing, pages 9?16, Philadel-
phia, July 2002. SIGDAT, Association for Computational
Linguistics.
Taku Kudo and Yuji Matsumoto. Fast methods for kernel-based
text analysis. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics, pages 24?31,
2003.
Grace Ngai, Dekai Wu, Marine Carpuat, Chi-Shing Wang,
and Chi-Yung Wang. Semantic role labeling with boost-
ing, SVMs, maximum entropy, SNOW, and decision lists.
In Proceedings of Senseval-3, Third International Work-
shop on Evaluating Word Sense Disambiguation Systems,
Barcelona, July 2004. SIGLEX, Association for Computa-
tional Linguistics.
Robert E. Schapire and Yoram Singer. Boostexter: A boosting-
based system for text categorization. In Machine Learning,
39(2/3), pages 135?168, 2000.
Bernhard Sch o?lkopf, Alexander Smola, and Klaus-Rober
M u?ller. Nonlinear component analysis as a kernel eigen-
value problem. Neural Computation, 10(5), 1998.
Richard Wicentowski, Grace Ngai, Dekai Wu, Marine Carpuat,
Emily Thomforde, and Adrian Packel. Joining forces to
resolve lexical ambiguity: East meets West in Barcelona.
In Proceedings of Senseval-3, Third International Work-
shop on Evaluating Word Sense Disambiguation Systems,
Barcelona, July 2004. SIGLEX, Association for Computa-
tional Linguistics.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and
Yongsheng Yang. Boosting for named entity recognition.
In Dan Roth and Antal van den Bosch, editors, Proceedings
of CoNLL-2002, pages 195?198. Taipei, Taiwan, 2002.
Dekai Wu, Weifeng Su, and Marine Carpuat. A Kernel PCA
method for superior word sense disambiguation. In Pro-
ceedings of the 42nd Annual Meeting of the Association for
Computational Linguistics, Barcelona, July 2004.
David Yarowsky and Radu Florian. Evaluating sense disam-
biguation across diverse parameter spaces. Natural Lan-
guage Engineering, 8(4):293?310, 2002.
Semantic Role Labeling with
Boosting, SVMs, Maximum Entropy, SNOW, and Decision Lists
Grace NGAI?1 , Dekai WU?2
Marine CARPUAT? , Chi-Shing WANG? , Chi-Yung WANG?
? Dept. of Computing
HK Polytechnic University
Hong Kong
? HKUST, Dept of Computer Science
Human Language Technology Center
Hong Kong
csgngai@polyu.edu.hk, dekai@cs.ust.hk
marine@cs.ust.hk, wcsshing@netvigator.com, cscywang@comp.polyu.edu.hk
Abstract
This paper describes the HKPolyU-HKUST sys-
tems which were entered into the Semantic Role La-
beling task in Senseval-3. Results show that these
systems, which are based upon common machine
learning algorithms, all manage to achieve good
performances on the non-restricted Semantic Role
Labeling task.
1 Introduction
This paper describes the HKPolyU-HKUST sys-
tems which participated in the Senseval-3 Semantic
Role Labeling task. The systems represent a diverse
array of machine learning algorithms, from decision
lists to SVMs to Winnow-type networks.
Semantic Role Labeling (SRL) is a task that
has recently received a lot of attention in the NLP
community. The SRL task in Senseval-3 used
the Framenet (Baker et al, 1998) corpus: given a
sentence instance from the corpus, a system?s job
would be to identify the phrase constituents and
their corresponding role.
The Senseval-3 task was divided into restricted
and non-restricted subtasks. In the non-restricted
subtask, any and all of the gold standard annotations
contained in the FrameNet corpus could be used.
Since this includes information on the boundaries
of the parse constituents which correspond to some
frame element, this effectively maps the SRL task
to that of a role-labeling classification task: given a
constituent parse, identify the frame element that it
belongs to.
Due to the lack of time and resources, we chose to
participate only in the non-restricted subtask. This
enabled our systems to take the classification ap-
proach mentioned in the previous paragraph.
1The author would like to thank the Hong Kong Polytechnic
University for supporting this research in part through research
grants A-PE37 and 4-Z03S.
2The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
2 Experimental Features
This section describes the features that were used
for the SRL task. Since the non-restricted SRL task
is essentially a classification task, each parse con-
stituent that was known to correspond to a frame
element was considered to be a sample.
The features that we used for each sample have
been previously shown to be helpful for the SRL
task (Gildea and Jurafsky, 2002). Some of these
features can be obtained directly from the Framenet
annotations:
? The name of the frame.
? The lexical unit of the sentence ? i.e. the lex-
ical identity of the target word in the sentence.
? The general part-of-speech tag of the target
word.
? The ?phrase type? of the constituent ? i.e. the
syntactic category (e.g. NP, VP) that the con-
stituent falls into.
? The ?grammatical function? (e.g. subject, ob-
ject, modifier, etc) of the constituent, with re-
spect to the target word.
? The position (e.g. before, after) of the con-
stituent, with respect to the target word.
In addition to the above features, we also ex-
tracted a set of features which required the use of
some statistical NLP tools:
? Transitivity and voice of the target word ?
The sentence was first part-of-speech tagged
and chunked with the fnTBL transformation-
based learning tools (Ngai and Florian, 2001).
Simple heuristics were then used to deduce the
transitivity voice of the target word.
? Head word (and its part-of-speech tag) of the
constituent ? After POS tagging, a syntactic
parser (Collins, 1997) was then used to ob-
tain the parse tree for the sentence. The head
word (and the POS tag of the head word) of
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the syntactic parse constituent whose span cor-
responded most closely to the candidate con-
stituent was then assumed to be the head word
of the candidate constituent.
The resulting training data set consisted of 51,366
constituent samples with a total of 151 frame ele-
ment types. These ranged from ?Descriptor? (3520
constituents) to ?Baggage? and ?Carrier? (1 con-
stituent each). This training data was randomly par-
titioned into a 80/20 ?development training? and
?validation? set.
3 Methodology
The previous section described the features that
were extracted for each constituent. This section
will describe the experiment methodology as well
as the learning systems used to construct the mod-
els.
Our systems had originally been trained on the
entire development training (devtrain) set, gener-
ating one global model per system. However, on
closer examination of the task, it quickly became
evident that distinguishing between 151 possible
outcomes was a difficult task for any system. It
was also not clear that there was going to be a
lot of information that could be generalized across
frame types. We therefore partitioned the data by
frame, so that one model would be trained for each
frame. (This was also the approach taken by (Gildea
and Jurafsky, 2002).) Some of our individual sys-
tems tried both approaches; the results are com-
pared in the following subsections. For compar-
ison purposes, a baseline model was constructed
by simply classifying all constituents with the most
frequently-seen (in the training set) frame element
for the frame.
In total, five individual systems were trained for
the SRL task, and four ensemble models were gen-
erated by using various combinations of the indi-
vidual systems. With one exception, all of the indi-
vidual systems were constructed using off-the-shelf
machine learning software. The following subsec-
tions describe each system; however, it should be
noted that some of the individual systems were not
officially entered as competing systems; therefore,
their scores are not listed in the final rankings.
3.1 Boosting
The most successful of our individual systems is
based on boosting, a powerful machine learning
algorithm which has been shown to achieve good
results on NLP problems in the past. Our sys-
tem was constructed around the Boostexter soft-
ware (Schapire and Singer, 2000), which imple-
Model Prec. Recall Attempted
Single Model 0.891 0.795 89.2%
Frame Separated 0.894 0.798 89.2%
Baseline 0.444 0.396 89.2%
Table 1: Boosting Models: Validation Set Results
ments boosting on top of decision stumps (decision
trees of one level), and was originally designed for
text classification. The same system also partici-
pated in the Senseval-3 lexical sample tasks for Chi-
nese and English, as well as the Multilingual lexical
sample task (Carpuat et al, 2004).
Table 1 compares the results of training one sin-
gle overall boosting model (Single) versus training
separate models for each frame (Frame). It can be
seen that training frame-specific models produces
a small improvement over the single model. The
frame-specific model was used in all of the ensem-
ble systems, and was also entered into the competi-
tion as an individual system (hkpust-boost).
3.2 Support Vector Machines
The second of our individual systems was based
on support vector machines, and implemented using
the TinySVM software package (Boser et al, 1992).
Since SVMs are binary classifiers, we used a
one-against-all method to reduce the SRL task to
a binary classification problem. One model is con-
structed for each possible frame element and the
task of the model is to decide, for a given con-
stituent, whether it should be classified with that
frame element. Since it is possible for all the bi-
nary classifiers to decide on ?NOT-<element>?, the
model is effectively allowed to pass on samples that
it is not confident about. This results in a very pre-
cise model, but unfortunately at a significant hit to
recall.
A number of kernel parameter settings were in-
vestigated, and the best performance was achieved
with a polynomial kernel of degree 4. The rest of
the parameters were left at the default values. Table
2 shows the results of the best SVM model on the
validation set. This model participated in the all of
the ensemble systems, and was also entered into the
competition as an individual system.
System Prec. Recall Attempted
SVM 0.945 0.669 70.8%
Baseline 0.444 0.396 89.2%
Table 2: SVM Models: Validation Set Results
3.3 Maximum Entropy
The third of our individual systems was based on
the maximum entropy model, and implemented on
top of the YASMET package (Och, 2002). Like the
boosting model, the maximum entropy system also
participated in the Senseval-3 lexical sample tasks
for Chinese and English, as well as the Multilingual
lexical sample task (Carpuat et al, 2004).
Our maximum entropy models can be classi-
fied into two main approaches. Both approaches
used the frame-partitioned data. The more conven-
tional approach (?multi?) then trained one model
per frame; that model would be responsible for clas-
sifying a constituent belonging to that frame with
one of several possible frame elements. The second
approach (binary) used the same approach as the
SVM models, and trained one binary one-against-
all classifier for each frame type-frame element
combination. (Unlike the boosting models, a single
maximum entropy model could not be trained for all
possible frame types and elements, since YASMET
crashed on the sheer size of the feature space.)
System Prec. Recall Attempted
multi 0.856 0.764 89.2%
binary 0.956 0.539 56.4%
Baseline 0.444 0.396 89.2%
Table 3: Maximum Entropy Models: Validation Set
Results
Table 3 shows the results for the maximum en-
tropy models. As would have been expected, the
binary model achieves very high levels of precision,
but at considerable expense of recall. Both systems
were eventually used in the some of the ensemble
models but were not submitted as individual contes-
tants.
3.4 SNOW
The fourth of our individual systems is based on
SNOW ? Sparse Network Of Winnows (Mun?oz et
al., 1999).
The development approach for the SNOW mod-
els was similar to that of the boosting models. Two
main model types were generated: one which gener-
ated a single overall model for all the possible frame
elements, and one which generated one model per
frame type. Due to a bug in the coding which was
not discovered until the last minute, however, the
results for the frame-separated model were invali-
dated. The single model system was eventually used
in some of the ensemble systems, but not entered as
an official contestant. Table 4 shows the results.
System Prec. Recall Attempted
Single Model 0.764 0.682 89.2%
Baseline 0.444 0.396 89.2%
Table 4: SNOW Models: Validation Set Results
3.5 Decision Lists
The final individual system was a decision list im-
plementation contributed from the Swarthmore Col-
lege team (Wicentowski et al, 2004), which partic-
ipated in some of the lexical sample tasks.
The Swarthmore team followed the frame-
separated approach in building the decision list
models. Table 5 shows the result on the validation
set. This system participated in some of the final
ensemble systems as well as being an official par-
ticipant (hkpust-swat-dl).
System Prec. Recall Attempted
DL 0.837 0.747 89.2%
Baseline 0.444 0.396 89.2%
Table 5: Decision List Models: Validation Set Re-
sults
3.6 Ensemble Systems
Classifier combination, where the results of differ-
ent models are combined in some way to make a
new model, has been well studied in the literature.
A successful combined classifier can result in the
combined model outperforming the best base mod-
els, as the advantages of one model make up for the
shortcomings of another.
Classifier combination is most successful when
the base models are biased differently. That condi-
tion applies to our set of base models, and it was
reasonable to make an attempt at combining them.
Since the performances of our systems spanned
a large range, we did not want to use a simple ma-
jority vote in creating the combined system. Rather,
we used a set of heuristics which trusted the most
precise systems (the SVM and the binary maximum
entropy) when they made a prediction, or a combi-
nation of the others when they did not.
Table 6 shows the results of the top-scoring com-
bined systems which were entered as official con-
testants. As expected, the best of our combined sys-
tems outperformed the best base model.
4 Test Set Results
Table 7 shows the test set results for all systems
which participated in some way in the official com-
petition, either as part of a combined system or as
an individual contestant.
Model Prec. Recall Attempted
svm, boosting, maxent (binary) (hkpolyust-all(a)) 0.874 0.867 99.2%
boosting (hkpolyust-boost) 0.859 0.852 0.846%
svm, boosting, maxent (binary), DL (hkpolyust-swat(a)) 0.902 0.849 94.1%
svm, boosting, maxent (binary), DL, snow (hkpolyust-swat(b)) 0.908 0.846 93.2%
svm, boosting, maxent (multi), DL, snow (hkpolyust-all(b)) 0.905 0.846 93.5%
decision list (hkpolyust-swat-dl) 0.819 0.812 99.2%
maxent (multi) 0.827 0.735 88.8%
svm (hkpolyust-svm) 0.926 0.725 76.1%
snow 0.713 0.499 70.0%
maxent (binary) 0.935 0.454 48.6%
Baseline 0.438 0.388 88.6%
Table 7: Test set results for all our official systems, as well as the base models used in the ensemble system.
Base Models Prec. Recall Attempted
svm, boosting,
maxent (bin)
0.901 0.803 89.2%
svm, boosting,
maxent (bin), snow
0.938 0.8 85.2%
svm, boosting,
maxent (bin), DL
0.926 0.783 84.6%
svm, boosting,
maxent (multi),
DL, snow
0.935 0.797 85.2%
Baseline 0.444 0.396 89.2%
Table 6: Combined Models: Validation Set Results
The top-performing system is the combined sys-
tem that uses the SVM, boosting and the binary im-
plementation of maximum entropy. Of the individ-
ual systems, boosting performs the best, even out-
performing 3 of the combined systems. The SVM
suffers from its high-precision approach, as does the
binary implementation of maximum entropy. The
rest of the systems fall somewhere in between.
5 Conclusion
This paper presented the HKPolyU-HKUST sys-
tems for the non-restricted Semantic Role Labeling
task for Senseval-3. We mapped the task to that
of a simple classification task, and used features
and systems which were easily extracted and con-
structed. Our systems achieved good performance
on the SRL task, easily beating the baseline.
6 Acknowledgments
The ?hkpolyust-swat-*? systems are the result of
joint work between our team and Richard Wicen-
towski?s team at Swarthmore College. The authors
would like to express their immense gratitude to the
Swarthmore team for providing their decision list
system as one of our models.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Chris-
tian Boitet and Pete Whitelock, editors, Proceedings
of the Thirty-Sixth Annual Meeting of the Associa-
tion for Computational Linguistics and Seventeenth
International Conference on Computational Linguis-
tics, pages 86?90, San Francisco, California. Morgan
Kaufmann Publishers.
Bernhard E. Boser, Isabelle Guyon, and Vladimir Vap-
nik. 1992. A training algorithm for optimal margin
classifiers. In Computational Learing Theory, pages
144?152.
Marine Carpuat, Weifeng Su, and Dekai Wu. 2004.
Augmenting Ensemble Classification for Word Sense
Disambiguation with a Kernel PCA Model. In Pro-
ceedings of Senseval-3, Barcelona.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the ACL (jointly with the 8th
Conference of the EACL), Madrid.
Daniel Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):256?288.
Marcia Mun?oz, Vasin Punyakanok, Dan Roth, and Dav
Zimak. 1999. A learning approach to shallow pars-
ing. In Proceedings of EMNLP-WVLC?99, pages
168?178, College Park. Association for Computa-
tional Linguistics.
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proceedings of the 39th
Conference of the Association for Comp utational Lin-
guistics, Pittsburgh, PA.
Franz Josef Och. 2002. Yet Another Small Max-
ent Toolkit: Yasmet. http://www-i6.informatik.rwth-
aachen.de/Colleagues/och.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Richard Wicentowski, Emily Thomforde, and Adrian
Packel. 2004. The Swarthmore College Senseval-3
system. In Proceedings of Senseval-3, Barcelona.
Joining forces to resolve lexical ambiguity:
East meets West in Barcelona
Richard WICENTOWSKI*, Grace NGAI?1 , Dekai WU?2
Marine CARPUAT? , Emily THOMFORDE*, Adrian PACKEL*
*Swarthmore College
Swarthmore, PA
USA
? Dept. of Computing
? HK Polytechnic University
Hong Kong
? HKUST, Dept of Computer Science
Human Language Technology Center
Hong Kong
richardw@cs.swarthmore.edu, csgngai@polyu.edu.hk, dekai@cs.ust.hk
marine@cs.ust.hk, ethomfo1@cs.swarthmore.edu, packel@cs.swarthmore.edu
Abstract
This paper describes the component models and
combination model built as a joint effort be-
tween Swarthmore College, Hong Kong PolyU, and
HKUST. Though other models described elsewhere
contributed to the final combination model, this pa-
per focuses solely on the joint contributions to the
?Swat-HK? effort.
1 Introduction
This paper describes the two joint component mod-
els of the Swat-HK systems entered into four of
the word sense disambiguation lexical sample tasks
in Senseval-3: Basque, Catalan, Italian and Roma-
nian, as well as a combination model for each lan-
guage. The feature engineering (and construction of
three other component models which are described
in (Wicentowski et al, 2004)) was performed at
Swarthmore College, while the Hong Kong team
constructed two component models based on well-
known machine learning algorithms. The combina-
tion model, which was constructed at Swarthmore,
uses voting to combine all five models.
2 Experimental Features
A full description of the experimental features for
all four tasks can be found in the report submitted
by the Swarthmore College Senseval team (Wicen-
towski et al, 2004). Briefly, the systems used lexi-
cal and syntactic features in the context of the target
word:
? The ?bag of words (and lemmas)? in the con-
text of the ambiguous word.
? Bigrams and trigrams of words (and lemmas,
1The author would like to thank the Hong Kong Polytechnic
University for supporting this research in part through research
grants A-PE37 and 4-Z03S.
2The author would like to thank the Hong Kong Research
Grants Council (RGC) for supporting this research in part
through research grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09.
part-of-speech tags, and, for Basque, case in-
formation) surrounding the ambiguous word.
? The topic (or code) of the document containing
the current instance of the word was extracted.
(Basque and Catalan only.)
These features have been shown to be effective
in previous WSD research. Since our systems were
all supervised, all the data used was provided by the
Senseval organizers; no additional (unlabeled) data
was included.
3 Methodology
The systems that were constructed by this team in-
cluded two component models: a boosting model
and a maximum entropy model as well as a com-
bination system. The component models were also
used in other Senseval-3 tasks: Semantic Role La-
beling (Ngai et al, 2004) and the lexical sample
tasks for Chinese and English, as well as the Multi-
lingual task (Carpuat et al, 2004).
To perform parameter tuning for the two compo-
nent models, 20% of the samples from the training
set were held out into a validation set. Since we
did not expect the senses of different words to share
any information, the training data was partitioned by
the ambiguous word in question. A model was then
trained for each ambiguous word type. In total, we
had 40 models for Basque, 27 models for Catalan,
45 models for Italian and 39 models for Romanian.
3.1 Boosting
Boosting is a powerful machine learning algorithm
which has been shown to achieve good results on
a variety of NLP problems. One known property
of boosting is its ability to handle large numbers of
features. For this reason, we felt that it would be
well suited to the WSD task, which is known to be
highly lexicalized with a large number of possible
word types.
Our system was constructed around the Boostex-
ter software (Schapire and Singer, 2000), which im-
plements boosting on top of decision stumps (deci-
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
sion trees of one level), and was originally designed
for text classification.
Tuning a boosting system mainly lies in modify-
ing the number of iterations, or the number of base
models it would learn. Larger number of iterations
contribute to the boosting model?s power. However,
they also make it more prone to overfitting and in-
crease the training time. The latter, a simple dis-
advantage in another problem, becomes a real issue
for Senseval, since large numbers of models (one for
each word type) need to be trained in a short period
of time.
Since the available features differed from lan-
guage to language, the optimal number of iterations
also varied. Table 1 shows the performance of the
model on the validation set with respect to the num-
ber of iterations per language.
Accuracy
Number of iterations
Language 500 1000 2000
Basque 66.12% 67.07% 67.08%
Catalan 84.77% 84.89% 85.02%
Italian 51.11% 50.93%
Romanian 64.68% 64.52%
Table 1: Boosting models on the validation sets.
The final systems for the languages used 2000 it-
erations for Basque and Catalan and 500 iterations
for Italian and Romanian. The test set results are
shown in Table 4
3.2 Maximum Entropy
The other individual system was based on the maxi-
mum entropy model, another machine learning al-
gorithm which has been successfully applied to
many NLP problems. Our system was implemented
on top of the YASMET package (Och, 2002).
Due to lack of time, we did not manage to fine-
tune the maximum entropy model. The YASMET
package does provide a number of easily variable
parameters, but we were only able to try varying the
feature selection count threshold and the smoothing
parameter, and only on the Basque data.
Experimentally, however, smoothing did not
seem to make a difference. The only change in per-
formance was caused by varying the feature selec-
tion count threshold, which controls the number of
times a feature has to be seen in the training set in
order to be considered. Table 2 shows the perfor-
mances of the system on the Basque validation set,
with count thresholds of 0, 1 and 2.
Since word sense disambiguation is known to be
Threshold
0 1 2
Accuracy 55.62% 66.13% 65.68%
Table 2: Maximum Entropy Models on Basque val-
idation set.
a highly lexicalized task involving many feature val-
ues and sparse data, it is not too surprising that set-
ting a low threshold of 1 proves to be the most effec-
tive. The final system kept this threshold, smooth-
ing was not done and the GIS iterations allowed to
proceed until it converged on its own. These param-
eters were used for all four languages.
The maximum entropy model was not entered
into the competition as an official contestant; how-
ever, it did participate in the combined system.
3.3 Combined System
Ensemble methods have been widely studied in
NLP research, and it is well-known that a set of
systems will often combine to produce better re-
sults than those achieved by the best individual sys-
tem alone. The final system contributed by the
Swarthmore-Hong Kong team was such an ensem-
ble. In addition to the boosting and maximum en-
tropy models mentioned earlier, three other models
were included: a nearest-neighbor clustering model,
a decision list, and a Na??ve Bayes model. The five
models were then combined by a simple weighted
majority vote, with an ad-hoc weight of 1.1 given
to the boosting and decision lists systems, and 1.0
otherwise, with ties broken arbitrarily.
Due to an unfortunate error with the input data of
the voting algorithm (Wicentowski et al, 2004), the
official submitted results for the combined system
were poorer than they should have been. Table 3
compares the official (submitted) results to the cor-
rected results on the test set. The decrease in per-
formance caused by the error ranged from 0.9% to
3.3%.
Language official corrected net gain
Basque 67.0% 67.9% 0.9%
Catalan 79.5% 80.4% 0.9%
Italian 51.4% 54.7% 3.3%
Romanian 72.4% 73.3% 0.9%
Table 3: Ensemble system results on the test set.
Both official and corrected results are included.
System
Description Name Acc. (%)
Basque
Boosting basque-swat hk-bo 71.1
Combined swat-hk-basque 67.0 (67.9)
NNC 66.0
DL 64.6
Maxent 62.1
NB 60.4
Baseline 55.8
Catalan
Boosting catalan-swat hk-bo 79.6
DL 80.6
Combined swat-hk-catalan 79.5 (80.4)
NNC 77.5
NB 71.3
Maxent 70.9
Baseline 66.4
Italian
Combined swat-hk-italian 51.4 (54.7)
DL 50.3
Boosting italian-swat hk 48.3
Maxent 46.9
NNC 44.9
NB 42.1
Baseline 23.7
Romanian
Boosting romanian-swat hk-bo 72.7
Combined swat-hk-romanian 72.4 (73.3)
DL 70.9
NNC 67.9
Maxent 66.5
NB 62.8
Baseline 58.4
Table 4: Test set results on 4 languages. Offi-
cial contestants are in bold; corrected voting results
are in parentheses. Key: NB: Na??ve Bayes, NNC:
Nearest-Neighbor Clustering, DL: Decision List
4 Test Set Results
Final results from all the systems are shown in Ta-
ble 4. As a reference, the results of a simple base-
line system which assigns the most frequent sense
as seen in the training set is also provided.
Due to the error in the voting system, the offi-
cial results for the combination system were lower
than they should have been ? as a result, boosting
was officially the top ranked system for 3 of the 4
languages. With the corrected results, however, the
combined system outperforms the individual mod-
els, as expected. The only exception is Basque,
where the booster had an exceptionally strong per-
formance. This is probably due to the fact that
Basque has a much richer feature set than the other
languages, which boosting was better able to take
advantage of.
The poor performance of the maximum entropy
model was also unexpected at first; however, it is
perhaps not too surprising, given the lack of time
spent on fine-tuning the model. As a result, most of
the parameters were left at their default values.
One thing worth noting is the fact that the sys-
tems were combined as ?closed systems? ? i.e. all
that was known about them was the output result,
and nothing else. The result was that no confidence
measures from the boosting and maximum entropy
could be used in the combined system. It is likely
that the performance could have been further im-
proved if more information had been available.
5 Conclusions and Discussion
This paper describes the ?Swat-HK? systems which
were the result of collaborative work between
Swarthmore College, Hong Kong Polytechnic Uni-
versity and HKUST. Several base systems were con-
structed on the same feature set, and a weighed ma-
jority voting system was used to combine the re-
sults. The individual systems all achieve good re-
sults, easily beating the baseline. As expected, the
combined system outperforms the best individual
system for the majority of the tasks.
References
Marine Carpuat, Weifeng Su, and Dekai Wu. 2004.
Augmenting Ensemble Classification for Word
Sense Disambiguation with a Kernel PCA Model.
In Proceedings of Senseval-3, Barcelona.
Grace Ngai, Dekai Wu, Marine Carpuat, Chi-Shing
Wang, and Chi-Yung Wang. 2004. Semantic
Role Labeling with Boosting, SVMs, Maximum
Entropy, SNOW, and Decision Lists. In Proceed-
ings of Senseval-3, Barcelona.
Franz Josef Och. 2002. Yet Another Small
Maxent Toolkit: Yasmet. http://www-
i6.informatik.rwth-aachen.de/Colleagues/och.
Robert E. Schapire and Yoram Singer. 2000. Boos-
texter: A boosting-based system for text catego-
rization. Machine Learning, 39(2/3):135?168.
Richard Wicentowski, Emily Thomforde, and
Adrian Packel. 2004. The Swarthmore College
Senseval-3 system. In Proceedings of Senseval-
3, Barcelona.
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 25?30,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Recognizing Paraphrases and Textual Entailment using
Inversion Transduction Grammars
Dekai Wu1
Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology, Clear Water Bay, Hong Kong
dekai@cs.ust.hk
Abstract
We present first results using paraphrase as well as
textual entailment data to test the language univer-
sal constraint posited by Wu?s (1995, 1997) Inver-
sion Transduction Grammar (ITG) hypothesis. In
machine translation and alignment, the ITG Hypoth-
esis provides a strong inductive bias, and has been
shown empirically across numerous language pairs
and corpora to yield both efficiency and accuracy
gains for various language acquisition tasks. Mono-
lingual paraphrase and textual entailment recogni-
tion datasets, however, potentially facilitate closer
tests of certain aspects of the hypothesis than bilin-
gual parallel corpora, which simultaneously exhibit
many irrelevant dimensions of cross-lingual varia-
tion. We investigate this using simple generic Brack-
eting ITGs containing no language-specific linguis-
tic knowledge. Experimental results on the MSR
Paraphrase Corpus show that, even in the absence
of any thesaurus to accommodate lexical variation
between the paraphrases, an uninterpolated aver-
age precision of at least 76% is obtainable from
the Bracketing ITG?s structure matching bias alone.
This is consistent with experimental results on the
Pascal Recognising Textual Entailment Challenge
Corpus, which show surpisingly strong results for a
number of the task subsets.
1 Introduction
The Inversion Transduction Grammar or ITG formalism,
which historically was developed in the context of trans-
lation and alignment, hypothesizes strong expressiveness
restrictions that constrain paraphrases to vary word or-
der only in certain allowable nested permutations of ar-
guments (Wu, 1997). The ITG Hypothesis has been more
extensively studied across different languages, but newly
available paraphrase datasets provide intriguing opportu-
1The author would like to thank the Hong Kong Re-
search Grants Council (RGC) for supporting this research
in part through grants RGC6083/99E, RGC6256/00E, and
DAG03/04.EG09, and Marine Carpuat and Yihai Shen for in-
valuable assistance in preparing the datasets and stoplist.
nities for meaningful analysis of the ITG Hypothesis in a
monolingual setting.
The strong inductive bias imposed by the ITG Hypoth-
esis has been repeatedly shown empirically to yield both
efficiency and accuracy gains for numerous language ac-
quisition tasks, across a variety of language pairs and
tasks. For example, Zens and Ney (2003) show that
ITG constraints yield significantly better alignment cov-
erage than the constraints used in IBM statistical ma-
chine translation models on both German-English (Verb-
mobil corpus) and French-English (Canadian Hansards
corpus). Zhang and Gildea (2004) find that unsuper-
vised alignment using Bracketing ITGs produces signif-
icantly lower Chinese-English alignment error rates than
a syntactically supervised tree-to-string model (Yamada
and Knight, 2001). With regard to translation rather than
alignment accuracy, Zens et al (2004) show that decod-
ing under ITG constraints yields significantly lower word
error rates and BLEU scores than the IBM constraints.
We are conducting a series of investigations motivated
by the following observation: the empirically demon-
strated suitability of ITG paraphrasing constraints across
languages should hold, if anything, even more strongly
in the monolingual case. The monolingual case allows in
some sense closer testing of various implications of the
ITG hypothesis, without irrelevant dimensions of varia-
tion arising from other cross-lingual phenomena.
Asymmetric textual entailment recognition (RTE)
datasets, in particular the Pascal Recognising Textual En-
tailment Challenge Corpus (Dagan et al, 2005), provide
testbeds that abstract over many tasks, including infor-
mation retrieval, comparable documents, reading com-
prehension, question answering, information extraction,
machine translation, and paraphrase acquisition.
At the same time, the emergence of paraphrasing
datasets presents an opportunity for complementary ex-
periments on the task of recognizing symmetric bidirec-
tional entailment rather than asymmetric directional en-
tailment. In particular, for this study we employ the MSR
Paraphrase Corpus (Quirk et al, 2004).
25
2 Inversion Transduction Grammars
Formally, ITGs can be defined as the restricted subset of
syntax-directed transduction grammars or SDTGs Lewis
and Stearns (1968) where all of the rules are either of
straight or inverted orientation. Ordinary SDTGs allow
any permutation of the symbols on the right-hand side to
be specified when translating from the input language to
the output language. In contrast, ITGs only allow two out
of the possible permutations. If a rule is straight, the or-
der of its right-hand symbols must be the same for both
language. On the other hand, if a rule is inverted, then the
order is left-to-right for the input language and right-to-
left for the output language. Since inversion is permitted
at any level of rule expansion, a derivation may intermix
productions of either orientation within the parse tree.
The ability to compose multiple levels of straight and in-
verted constituents gives ITGs much greater expressive-
ness than might seem at first blush.
A simple example may be useful to fix ideas. Consider
the following pair of parse trees for sentence translations:
[[[The Authority]NP [will [[be accountable]VV [to
[the [[Financial Secretary]NN ]NNN ]NP ]PP ]VP
]VP ]SP .]S
[[[??]NP [R? [[5 [[[cu?]NN ]NNN ]NP ]PP
[?	]VV ]VP ]VP ]SP ]S
Even though the order of constituents under the inner
VP is inverted between the languages, an ITG can cap-
ture the common structure of the two sentences. This is
compactly shown by writing the parse tree together for
both sentences with the aid of an ?? angle bracket no-
tation marking parse tree nodes that instantiate rules of
inverted orientation:
[[[The/?Authority/ ? ?]NP [will/R ?
?[be/?accountable/? 	]VV [to/5 [the/?
[[Financial/cuSecretary/?]NN ]NNN ]NP ]PP
?VP ]VP ]SP./]S
In a weighted or stochastic ITG (SITG), a weight or a
probability is associated with each rewrite rule. Follow-
ing the standard convention, we use a and b to denote
probabilities for syntactic and lexical rules, respectively.
For example, the probability of the rule NN 0.4? [A N] is
aNN?[A N] = 0.4. The probability of a lexical rule A
0.001
?
x/y is bA(x, y) = 0.001. Let W1,W2 be the vocabulary
sizes of the two languages, and N = {A1, . . . , AN} be
the set of nonterminals with indices 1, . . . , N .
Wu (1997) also showed that ITGs can be equivalently
be defined in two other ways. First, ITGs can be defined
as the restricted subset of SDTGs where all rules are of
rank 2. Second, ITGs can also be defined as the restricted
subset of SDTGs where all rules are of rank 3.
Polynomial-time algorithms are possible for various
tasks including translation using ITGs, as well as bilin-
gual parsing or biparsing, where the task is to build the
highest-scored parse tree given an input bi-sentence.
For present purposes we can employ the special case of
Bracketing ITGs, where the grammar employs only one
single, undistinguished ?dummy? nonterminal category
for any non-lexical rule. Designating this category A, a
Bracketing ITG has the following form (where, as usual,
lexical transductions of the form A ? e/f may possibly
be singletons of the form A ? e/ or A ? /f ).
A ? [AA]
A ? ?AA?
A ? , 
A ? e1/f1
. . .
A ? ei/fj
The simplest class of ITGs, Bracketing ITGs, are
particularly interesting in applications like paraphras-
ing, because they impose ITG constraints in language-
independent fashion, and in the simplest case do not re-
quire any language-specific linguistic grammar or train-
ing. In Bracketing ITGs, the grammar uses only a
single, undifferentiated non-terminal (Wu, 1995). The
key modeling property of Bracketing ITGs that is most
relevant to paraphrase recognition is that they assign
strong preference to candidate paraphrase pairs in which
nested constituent subtrees can be recursively aligned
with a minimum of constituent boundary violations. Un-
like language-specific linguistic approaches, however, the
shape of the trees are driven in unsupervised fashion by
the data. One way to view this is that the trees are
hidden explanatory variables. This not only provides
significantly higher robustness than more highly con-
strained manually constructed grammars, but also makes
the model widely applicable across languages in econom-
ical fashion without a large investment in manually con-
structed resources.
Moreover, for reasons discussed by Wu (1997), ITGs
possess an interesting intrinsic combinatorial property of
permitting roughly up to four arguments of any frame to
be transposed freely, but not more. This matches supris-
ingly closely the preponderance of linguistic verb frame
theories from diverse linguistic traditions that all allow
up to four arguments per frame. Again, this property
emerges naturally from ITGs in language-independent
fashion, without any hardcoded language-specific knowl-
edge. This further suggests that ITGs should do well
at picking out paraphrase pairs where the order of up
to four arguments per frame may vary freely between
the two strings. Conversely, ITGs should do well at re-
jecting pairs where (1) too many words in one sentence
26
find no correspondence in the other, (2) frames do not
nest in similar ways in the candidate sentence pair, or
(3) too many arguments must be transposed to achieve an
alignment?all of which would suggest that the sentences
probably express different ideas.
As an illustrative example, in common similarity mod-
els, the following pair of sentences (found in actual data
arising in our experiments below) would receive an inap-
propriately high score, because of the high lexical simi-
larity between the two sentences:
Chinese president Jiang Zemin arrived in Japan
today for a landmark state visit .
T? R 4 t ?? ) )/6? { D? ?)
)ff?? .
(Jiang Zemin will be the first Chinese national
president to pay a state vist to Japan.)
However, the ITG based model is sensitive enough
to the differences in the constituent structure (reflecting
underlying differences in the predicate argument struc-
ture) so that our experiments show that it assigns a low
score. On the other hand, the experiments also show that
it successfully assigns a high score to other candidate bi-
sentences representing a true Chinese translation of the
same English sentence, as well as a true English transla-
tion of the same Chinese sentence.
We investigate a model for the paraphrase recognition
problem that employ simple generic Bracketing ITGs.
The experimental results show that, even in the absence
of any thesaurus to accommodate lexical variation be-
tween the two strings, the Bracketing ITG?s structure
matching bias alone produces a significant improvement
in average precision.
3 Scoring Method
All words of the vocabulary are included among the lex-
ical transductions, allowing exact word matches between
the two strings of any candidate paraphrase pair.
Each candidate pair of the test set was scored via the
ITG biparsing algorithm, which employs a dynamic pro-
gramming approach as follows.Let the input English sen-
tence be e1, . . . , eT and the corresponding input Chinese
sentence be c1, . . . , cV . As an abbreviation we write es..t
for the sequence of words es+1, es+2, . . . , et, and simi-
larly for cu..v; also, es..s =  is the empty string. It is
convenient to use a 4-tuple of the form q = (s, t, u, v)
to identify each node of the parse tree, where the sub-
strings es..t and cu..v both derive from the node q. De-
note the nonterminal label on q by `(q). Then for any
node q = (s, t, u, v), define
?q(i) = ?stuv(i) = max
subtrees ofq
P [subtree ofq, `(q) = i, i ?? es..t/cu..v]
as the maximum probability of any derivation from i that
successfully parses both es..t and cu..v . Then the best
parse of the sentence pair has probability ?0,T,0,V (S).
The algorithm computes ?0,T,0,V (S) using the follow-
ing recurrences. Note that we generalize argmax to the
case where maximization ranges over multiple indices,
by making it vector-valued. Also note that [ ] and ?? are
simply constants, written mnemonically. The condition
(S? s)(t?S)+ (U ?u)(v?U) 6= 0 is a way to specify
that the substring in one but not both languages may be
split into an empty string  and the substring itself; this
ensures that the recursion terminates, but permits words
that have no match in the other language to map to an 
instead.
1. Initialization
?t?1,t,v?1,v(i) = bi(et/cv),
1 ? t ? T
1 ? v ? V
?t?1,t,v,v(i) = bi(et/),
1 ? t ? T
0 ? v ? V
?t,t,v?1,v(i) = bi(/cv),
0 ? t ? T
1 ? v ? V
2. Recursion For all i, s, t, u, v such that
{
1?i?N
0?s<t?T
0?u<v?V
t?s+v?u>2
?stuv(i) = max[?
[ ]
stuv(i), ?
??
stuv(i)]
?stuv(i) =
{
[ ] if ?[ ]stuv(i) ? ?
??
stuv(i)
?? otherwise
where
?[ ]stuv(i) = max
1?j?N
1?k?N
s?S?t
u?U?v
(S?s)(t?S)+(U?u)(v?U) 6=0
ai?[jk] ?sSuU (j) ?StUv(k)
?
?
?
?
?
?[ ]stuv(i)
?[ ]stuv(i)
?[ ]stuv(i)
?[ ]stuv(i)
?
?
?
?
?
= argmax
1?j?N
1?k?N
s?S?t
u?U?v
(S?s)(t?S)+(U?u)(v?U) 6=0
ai?[jk] ?sSuU (j) ?StUv(k)
???stuv(i) = max
1?j?N
1?k?N
s?S?t
u?U?v
(S?s)(t?S)+(U?u)(v?U) 6=0
ai??jk? ?sSUv(j) ?StuU (k)
?
?
?
?
?
???stuv(i)
???stuv(i)
???stuv(i)
???stuv(i)
?
?
?
?
?
= argmax
1?j?N
1?k?N
s?S?t
u?U?v
(S?s)(t?S)+(U?u)(v?U) 6=0
ai??jk? ?sSUv(j) ?StuU (k)
27
3. Reconstruction Initialize by setting the root of the
parse tree to q1 = (0, T, 0, V ) and its nonterminal la-
bel to `(q1) = S. The remaining descendants in the
optimal parse tree are then given recursively for any
q = (s, t, u, v) by:
LEFT(q) =
?
??
??
NIL if t?s+v?u?2
(s, ?[ ]q (`(q)), u, ?
[ ]
q (`(q))) if ?q(`(q)) = [ ]
(s, ???q (`(q)), ?
??
q (`(q))) if ?q(`(q)) = ??
RIGHT(q) =
?
??
??
NIL if t?s+v?u?2
(?[ ]q (`(q)), t, ?
[ ]
q (`(q)), v) if ?q(`(q)) = [ ]
(???q (`(q)), t, u, ?
??
q (`(q))) if ?q(`(q)) = ??
`(LEFT(q)) = ??q(`(q))q (`(q))
`(RIGHT(q)) = ??q(`(q))q (`(q))
As mentioned earlier, biparsing for ITGs can be ac-
complished efficiently in polynomial time, rather than the
exponential time required for classical SDTGs. The re-
sult in Wu (1997) implies that for the special case of
Bracketing ITGs, the time complexity of the algorithm
is ?
(
T 3V 3
)
where T and V are the lengths of the two
sentences. This is a factor of V 3 more than monolingual
chart parsing, but has turned out to remain quite practical
for corpus analysis, where parsing need not be real-time.
The ITG scoring model can also be seen as a variant
of the approach described by Leusch et al (2003), which
allows us to forego training to estimate true probabilities;
instead, rules are simply given unit weights. The ITG
scores can be interpreted as a generalization of classi-
cal Levenshtein string edit distance, where inverted block
transpositions are also allowed. Even without probability
estimation, Leusch et al found excellent correlation with
human judgment of similarity between translated para-
phrases.
4 Experimental Results?Paraphrase
Recognition
Our objective here was to isolate the effect of the ITG
constraint bias. No training was performed with the avail-
able development sets. Rather, the aim was to establish
foundational baseline results, to see in this first round of
paraphrase recognition experiments what results could be
obtained with the simplest versions of the ITG models.
The MSR Paraphrase Corpus test set consists of 1725
candidate paraphrase string pairs, each annotated for se-
mantic equivalence by two or three human collectors.
Within the test set, 66.5% of the examples were annotated
as being semantically equivalent. The corpus was origi-
nally generated via a combination of automatic filtering
methods, making it difficult to make specific claims about
distributional neutrality, due to the arbitrary nature of the
example selection process.
The ITG scoring model produced an uninterpolated
average precision (also known as confidence weighted
score) of 76.1%. This represents an improvement of
roughly 10% over the random baseline. Note that this
improvement can be achieved with no thesaurus or lexi-
cal similarity model, and no parameter training.
5 Experimental Results?Textual
Entailment Recognition
The experimental procedure for the monolingual textual
entailment recognition task is the same as for paraphrase
recognition, except that one string serves as the Text and
the other serves as the Hypothesis.
Results on the textual entailment recognition task are
consistent with the above paraphrase recognition results.
For the PASCAL RTE challenge datasets, across all sub-
sets overall, the model produced a confidence-weighted
score of 54.97% (better than chance at the 0.05 level). All
examples were labeled, so precision, recall, and f-score
are equivalent; the accuracy was 51.25%.
For the RTE task we also investigated a second variant
of the model, in which a list of 172 words from a stoplist
was excluded from the lexical transductions. The moti-
vation for this model was to discount the effect of words
such as ?the? or ?of? since, more often than not, they
could be irrelevant to the RTE task.
Surprisingly, the stoplisted model produced worse
results. The overall confidence-weighted score was
53.61%, and the accuracy was 50.50%. We discuss the
reasons below in the context of specific subsets.
As one might expect, the Bracketing ITG models per-
formed better on the subsets more closely approximat-
ing the tasks for which Bracketing ITGs were designed:
comparable documents (CD), paraphrasing (PP), and in-
formation extraction (IE). We will discuss some impor-
tant caveats on the machine translation (MT) and reading
comprehension (RC) subsets. The subsets least close to
the Bracketing ITG models are information retrieval (IR)
and question answering (QA).
5.1 Comparable Documents (CD)
The CD task definition can essentially be characterized as
recognition of noisy word-aligned sentence pairs. Among
all subsets, CD is perhaps closest to the noisy word align-
ment task for which Bracketing ITGs were originally de-
veloped, and indeed produced the best results for both
of the Bracketing ITG models. The basic model pro-
duced a confidence-weighted score of 79.88% (accuracy
71.33%), while the stoplisted model produced an essen-
tially unchanged confidence-weighted score of 79.83%
28
(accuracy 70.00%).
The results on the RTE Challenge datasets closely re-
flect the larger-scale findings of Wu and Fung (2005),
who demonstrate that an ITG based model yields far
more accurate extraction of parallel sentences from quasi-
comparable non-parallel corpora than previous state-of-
the-art methods. Wu and Fung?s results also use the eval-
uation metric of uninterpolated average precision (i.e.,
confidence-weighted score).
Note also that we believe the results here are artificially
lowered by the absence of any thesaurus, and that signifi-
cantly further improvements would be seen with the addi-
tion of a suitable thesaurus, for reasons discussed below
under the MT subsection.
5.2 Paraphrase Acquisition (PP)
The PP task is also close to the task for which Brack-
eting ITGs were originally developed. For the PP task,
the basic model produced a confidence-weighted score of
57.26% (accuracy 56.00%), while the stoplisted model
produced a lower confidence-weighted score of 51.65%
(accuracy 52.00%). Unlike the CD task, the greater
importance of function words in determining equivalent
meaning between paraphrases appears to cause the degra-
dation in the stoplisted model.
The effect of the absence of a thesaurus is much
stronger for the PP task as opposed to the CD task. In-
spection of the datasets reveals much more lexical vari-
ation between paraphrases, and shows that cases where
lexis does not vary are generally handled accurately by
the Bracketing ITG models. The MT subsection below
discusses why a thesaurus should produce significant im-
provement.
5.3 Information Extraction (IE)
The IE task presents a slight issue of misfit for the
Bracketing ITG models, but yielded good results any-
how. The basic Bracketing ITG model attempts to align
all words/collocations between the two strings. However,
for the IE task in general, only a substring of the Text
should be aligned to the Hypothesis, and the rest should
be disregarded as ?noise?. We approximated this by al-
lowing words to be discarded from the Text at little cost,
by using parameters that impose only a small penalty on
null-aligned words from the Text. (As a reasonable first
approximation, this characterization of the IE task ig-
nores the possibility of modals, negation, quotation, and
the like in the Text.)
Despite the slight modeling misfit, the Bracketing ITG
models produced good results for the IE subset. The basic
model produced a confidence-weighted score of 59.92%
(accuracy 55.00%), while the stoplisted model produced
a lower confidence-weighted score of 53.63% (accuracy
51.67%). Again, the lower score of the stoplisted model
appears to arise from the greater importance of function
words in ensuring correct information extraction, as com-
pared with the CD task.
5.4 Machine Translation (MT)
One exception to expectations is the machine translation
subset, a task for which Bracketing ITGs were devel-
oped. The basic model produced a confidence-weighted
score of 34.30% (accuracy 40.00%), while the stoplisted
model produced a comparable confidence-weighted score
of 35.96% (accuracy 39.17%).
However, the performance here on the machine trans-
lation subset cannot be directly interpreted, for two rea-
sons.
First, the task as defined in the RTE Challenge datasets
is not actually crosslingual machine translation, but rather
evaluation of monolingual comparability between an au-
tomatic translation and a gold standard human transla-
tion. This is in fact closer to the problem of defining a
good MT evaluation metric, rather than MT itself. Leusch
et al (2003 and personal communication) found that
Bracketing ITGs as an MT evaluation metric show ex-
cellent correlation with human judgments.
Second, no translation lexicon or equivalent was used
in our model. Normally in translation models, includ-
ing ITG models, the translation lexicon accommodates
lexical ambiguity, by providing multiple possible lexi-
cal choices for each word or collocation being translated.
Here, there is no second language, so some substitute
mechanism to accommodate lexical ambiguity would be
needed.
The most obvious substitute for a translation lexicon
would be a monolingual thesaurus. This would allow
matching synonomous words or collocations between the
Text and the Hypothesis. Our original thought was to in-
corporate such a thesaurus in collaboration with teams fo-
cusing on creating suitable thesauri, but time limitations
prevented completion of these experiments. Based on our
own prior experiments and also on Leusch et al?s expe-
riences, we believe this would bring performance on the
MT subset to excellent levels as well.
5.5 Reading Comprehension (RC)
The reading comprehension task is similar to the infor-
mation extraction task. As such, the Bracketing ITG
model could be expected to perform well for the RC sub-
set. However, the basic model produced a confidence-
weighted score of just 49.37% (accuracy 47.14%), and
the stoplisted model produced a comparable confidence-
weighted score of 47.11% (accuracy 45.00%).
The primary reason for the performance gap between
the RC and IE domains appears to be that RC is less
news-oriented, so there is less emphasis on exact lexical
choices such as named entities. This puts more weight on
29
the importance of a good thesaurus to recognize lexical
variation. For this reason, we believe the addition of a
thesaurus would bring performance improvements simi-
lar to the case of MT.
5.6 Information Retrieval (IR)
The IR task diverges significantly from the tasks for
which Bracketing ITGs were developed. The basic model
produced a confidence-weighted score of 43.14% (ac-
curacy 46.67%), while the stoplisted model produced a
comparable confidence-weighted score of 44.81% (accu-
racy 47.78%).
Bracketing ITGs seek structurally parallelizable sub-
strings, where there is reason to expect some degree of
generalization between the frames (heads and arguments)
of the two substrings from a lexical semantics standpoint.
In contrast, the IR task relies on unordered keywords, so
the effect of argument-head binding cannot be expected
to be strong.
5.7 Question Answering (QA)
The QA task is extremely free in the sense that ques-
tions can differ significantly from the answers in both
syntactic structure and lexis, and can also require a
significant degree of indirect complex inference us-
ing real-world knowledge. The basic model pro-
duced a confidence-weighted score of 33.20% (accuracy
40.77%), while the stoplisted model produced a signifi-
cantly better confidence-weighted score of 38.26% (ac-
curacy 44.62%).
Aside from adding a thesaurus, to properly model the
QA task, at the very least the Bracketing ITG models
would need to be augmented with somewhat more lin-
guistic rules that include a proper model for wh- words in
the Hypothesis, which otherwise cannot be aligned to the
Text. In the Bracketing ITG models, the stoplist appears
to help by normalizing out the effect of the wh- words.
6 Conclusion
The most serious omission in our experiments with
Bracketing ITG models was the absence of any thesaurus
model, allowing zero lexical variation between the two
strings of a candidate paraphrase pair (or Text and Hy-
pothesis, in the case of textual entailment recognition).
This forced the models to rely entirely on the Bracketing
ITG?s inherent tendency to optimize structural match be-
tween hypothesized nested argument-head substructures.
What we find highly interesting is the perhaps surpris-
ingly large effect obtainable from this structure matching
bias alone, which already produces good results on para-
phrasing as well as a number of the RTE subsets.
We plan to remedy the absence of a thesaurus as the
obvious next step. This can be expected to raise perfor-
mance significantly on all subsets.
Wu and Fung (2005) also discuss how to obtain any
desired tradeoff between precision and recall. This would
be another interesting direction to pursue in the context of
recognizing paraphrases or textual entailment.
Finally, using the development sets to train the param-
eters of the Bracketing ITG model would improve per-
formance. It would only be feasible to tune a few basic
parameters, however, given the small size of the develop-
ment sets.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal
recognising textual entailment challenge. In PASCAL Pro-
ceedings of the First Challenge Workshop?Recognizing Tex-
tual Entailment, pages 1?8, Southampton, UK, April 2005.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. A novel
string-to-string distance measure with applications to ma-
chine translation evaluation. In Machine Translation Summit,
New Orleans, 2003.
P. M. Lewis and R. E. Stearns. Syntax-directed transduc-
tion. Journal of the Association for Computing Machinery,
15:465?488, 1968.
C. Quirk, C. Brockett, and W. B. Dolan. Monolingual ma-
chine translation for paraphrase generation. In Proceed-
ings of the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2004), Barcelona, June
2004. SIGDAT, Association for Computational Linguistics.
Dekai Wu and Pascale Fung. Inversion Transduction Gram-
mar constraints for mining parallel sentences from quasi-
comparable corpora. In Forthcoming, 2005.
Dekai Wu. An algorithm for simultaneously bracketing parallel
texts by aligning words. In 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics Conference (ACL-95),
Cambridge, MA, Jun 1995. Association for Computational
Linguistics.
Dekai Wu. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Linguis-
tics, 23(3), Sep 1997.
Kenji Yamada and Kevin Knight. A syntax-based statistical
translation model. In 39th Annual Meeting of the Associ-
ation for Computational Linguistics Conference (ACL-01),
Toulouse, France, 2001. Association for Computational Lin-
guistics.
Richard Zens and Hermann Ney. A comparative study on re-
ordering constraints in statistical machine translation. pages
192?202, Hong Kong, August 2003.
Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro
Sumita. Reordering constraints for phrase-based statistical
machine translation. In Proceedings of COLING, Geneva,
August 2004.
Hao Zhang and Daniel Gildea. Syntax-based alignment: Super-
vised or unsupervised? In Proceedings of COLING, Geneva,
August 2004.
30
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 150?153,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Boosting for Chinese Named Entity Recognition
Xiaofeng YU Marine CARPUAT Dekai WU*
Human Language Technology Center
HKUST
Department of Computer Science and Engineering
University of Science and Technology
Clear Water Bay, Hong Kong
{xfyu,marine,dekai}@cs.ust.hk
Abstract
We report an experiment in which a high-
performance boosting based NER model
originally designed for multiple European
languages is instead applied to the Chi-
nese named entity recognition task of the
third SIGHAN Chinese language process-
ing bakeoff. Using a simple character-
based model along with a set of features
that are easily obtained from the Chi-
nese input strings, the system described
employs boosting, a promising and the-
oretically well-founded machine learning
method to combine a set of weak classi-
fiers together into a final system. Even
though we did no other Chinese-specific
tuning, and used only one-third of the
MSRA and CityU corpora to train the
system, reasonable results are obtained.
Our evaluation results show that 75.07 and
80.51 overall F-measures were obtained
on MSRA and CityU test sets respectively.
1 Introduction
Named entity recognition (NER), which includes
the identification and classification of certain
proper nouns, such as person names, organiza-
tions, locations, temporal, numerical and mon-
etary phrases, plays an important part in many
natural language processing applications, such
as machine translation, information retrieval, in-
formation extraction and question answering.
Much of the NER research was pioneered in
the MUC/DUC and Multilingual Entity Task
(MET) evaluations, as a result of which signif-
icant progress has been made and many NER
?This work was supported in part by DARPA GALE
contract HR0011-06-C-0023, and by the Hong Kong Re-
search Grants Council (RGC) research grants RGC6083/99E,
RGC6256/00E, and DAG03/04.EG09.
systems of fairly high accuracy have been con-
structed. In addition, the shared tasks of CoNLL-
2002 and CoNLL-2003 helped spur the devel-
opment toward more language-independent NER
systems, by evaluating four types of entities (peo-
ple, locations, organizations and names of miscel-
laneous entities) in English, German, Dutch and
Spanish.
However, these are all European languages, and
Chinese NER appears to be significantly more
challenging in a number of important respects.
We believe some of the main reasons to be as
follows: (1) Unlike European languages, Chi-
nese lacks capitalization information which plays
a very important role in identifying named enti-
ties. (2) There is no space between words in Chi-
nese, so ambiguous segmentation interacts with
NER decisions. Consequently, segmentation er-
rors will affect the NER performance, and vice
versa. (3) Unlike European languages, Chinese al-
lows an open vocabulary for proper names of per-
sons, eliminating another major source of explicit
clues used by European language NER models.
This paper presents a system that introduces
boosting to Chinese named entity identification
and classification. Our primary aim was to con-
duct a controlled experiment to test how well
the boosting based models we designed for Eu-
ropean languages would fare on Chinese, without
major modeling alterations to accommodate Chi-
nese. We evaluated the system using data from
the third SIGHAN Chinese language processing
bakeoff, the goal of which was to perform NER
on three types of named entities: PERSON, LO-
CATION and ORGANIZATION.1 Three training
corpora from MSRA, CityU and LDC were given.
TheMSRA and LDC corpora were simplified Chi-
nese texts while the CityU corpus was traditional
1Except in the LDC corpus, which contains four types
of entities: PERSON, LOCATION, ORGANIZATION and
GEOPOLITICAL.
150
Chinese. In addition, the competition also spec-
ified open and closed tests. In the open test, the
participants may use any other material including
material from other training corpora, proprietary
dictionaries, and material from the Web besides
the given training corpora. In the closed test, the
participants can only use the three training cor-
pora. No other material or knowledge is allowed,
including part-of-speech (POS) information, ex-
ternally generated word-frequency counts, Arabic
and Chinese numbers, feature characters for place
names, common Chinese surnames, and so on.
The approach we used is based on selecting a
number of features, which are used to train several
weak classifiers. Using boosting, which has been
shown to perform well on other NLP problems and
is a theoretically well-founded method, the weak
classifiers are then combined to perform a strong
classifier.
2 Boosting
The main idea behind the boosting algorithm is
that a set of many simple and moderately accu-
rate weak classifiers (also called weak hypothe-
ses) can be effectively combined to yield a sin-
gle strong classifier (also called the final hypoth-
esis). The algorithm works by training weak clas-
sifiers sequentially whose classification accuracy
is slightly better than random guessing and finally
combining them into a highly accurate classifier.
Each weak classifier searches for the hypothesis in
the hypotheses space that can best classify the cur-
rent set of training examples. Based on the eval-
uation of each iteration, the algorithm reweights
the training examples, forcing the newly generated
weak classifier to give higher weights to the exam-
ples that are misclassified in the previous iteration.
The boosting algorithm was originally created to
deal with binary classification in supervised learn-
ing. The boosting algorithm is simple to imple-
ment, does feature selection resulting in a rela-
tively simple classifier, and has fairly good gen-
eralization.
Based on the boosting framework, our system
uses the AdaBoost.MH algorithm (Schapire and
Singer, 1999) as shown in Figure 1, an n-ary clas-
sification variant of the original well-known bi-
nary AdaBoost algorithm (Freund and Schapire,
1997). The original AdaBoost algorithm was de-
signed for the binary classification problem but did
not fulfill the requirements of the Chinese NER
Input: A training set Tr = {< d1, C1 >, . . . , < dg, Cg >}
where Cj ? C = {c1, ..., cm} for all j = 1, . . . , g.
Output: A final hypothesis ?(d, c) =
?S
s=1 ?s?s(d, c).
Algorithm: LetD1(dj , ci) = 1mg for all j = 1, . . . , g and
for all i = 1, . . . ,m. For s = 1, . . . , S do:
? pass distribution Ds(dj , ci)to the weak classifier;
? derive the weak hypothesis ?s from the weak
classifier;
? choose ?s ? R;
? set Ds+1(dj , ci) =
Ds(dj ,ci)exp(??sCj [ci]?s(dj ,ci))
Zs
where
Zs =?m
i=1
?g
j=1 Ds(dj , ci )exp( ? ?sCj [ci] ?s(dj , ci))
is a normalization factor chosen so that?m
i=1
?g
j=1 Ds+1(dj , ci) = 1.
Figure 1: The AdaBoost.MH algorithm.
task. AdaBoost.MH has shown its usefulness on
standard machine learning tasks through exten-
sive theoretical and empirical studies, where dif-
ferent standard machine learning methods have
been used as the weak classifier (e.g., Bauer and
Kohavi (1999), Opitz and Maclin (1999), Schapire
(2002)). It also performs well on a number of nat-
ural language processing problems, including text
categorization (e.g., Schapire and Singer (2000),
Sebastiani et al (2000)) and word sense disam-
biguation (e.g., Escudero et al (2000)). In partic-
ular, it has also been demonstrated that boosting
can be used to build language-independent NER
models that perform exceptionally well (Wu et al
(2002), Wu et al (2004), Carreras et al (2002)).
The weak classifiers used in the boosting algo-
rithm come from a wide range of machine learning
methods. We have chosen to use a simple classifier
called a decision stump in the algorithm. A deci-
sion stump is basically a one-level decision tree
where the split at the root level is based on a spe-
cific attribute/value pair. For example, a possible
attribute/value pair could beW2 =?/.
3 Experiment Details
In order to implement the boosting/decision
stumps, we used the publicly available software
AT&T BoosTexter (Schapire and Singer, 2000),
which implements boosting on top of decision
stumps. For preprocessing we used an off-the-
shelf Chinese lexical analysis system, the open
source ICTCLAS (Zhang et al, 2003), to segment
and POS tag the training and test corpora.
151
3.1 Data Preprocessing
The training corpora provided by the SIGHAN
bakeoff organizers were in the CoNLL two col-
umn format, with one Chinese character per line
and hand-annotated named entity chunks in the
second column.
In order to provide basic features for training
the decision stumps, the training corpora were seg-
mented and POS tagged by ICTCLAS, which la-
bels Chinese words using a set of 39 tags. This
module employs a hierarchical hidden Markov
model (HHMM) and provides word segmentation,
POS tagging and unknown word recognition. It
performs reasonably well, with segmentation pre-
cision recently evaluated at 97.58%.2 The recall
rate of unknownwords using role tagging was over
90%.
We note that about 200 words in each train-
ing corpora remained untagged. For these words
we simply assigned the most frequently occurring
tags in each training corpora.
3.2 Feature Set
The boosting/decision stumps were able to accom-
modate a large number of features. The primitive
features we used were:
? The current character and its POS tag.
? The characters within a window of 2 charac-
ters before and after the current character.
? The POS tags within a window of 2 charac-
ters before and after the current character.
? The chunk tags (gold standard named entity
label during the training) of the previous two
characters.
The chunk tag is the BIO representation, which
was employed in the CoNLL-2002 and CoNLL-
2003 evaluations. In this representation, each
character is tagged as either the beginning of a
named entity (B tag), a character inside a named
entity (I tag), or a character outside a named entity
(O tag).
When we used conjunction features, we found
that they helped the NER performance signifi-
cantly. The conjunction features used are basi-
cally conjunctions of 2 consecutive characters and
2 consecutive POS tags. We also found that a
2Results from the recent official evaluation in the national
973 project.
Table 1: Dev set results on MSRA and CityU.
Precision Recall F?=1
MSRA
LOC 82.00% 85.93% 83.92
ORG 76.99% 61.44% 68.34
PER 89.33% 74.47% 81.22
Overall 82.62% 76.45% 79.41
CityU
LOC 88.62% 81.69% 85.02
ORG 82.50% 66.44% 73.61
PER 84.05% 84.58% 84.31
Overall 86.46% 79.26% 82.71
Table 2: Test set results on MSRA, CityU, LDC.
Precision Recall F?=1
MSRA
LOC 84.98% 80.94% 82.91
ORG 72.82% 57.78% 64.43
PER 82.89% 59.91% 69.55
Overall 81.95% 69.26% 75.07
CityU
LOC 88.65% 83.58% 86.04
ORG 83.75% 57.25% 68.01
PER 86.11% 76.42% 80.98
Overall 86.92% 74.98% 80.51
LDC
LOC 65.84% 76.51% 70.78
ORG 53.69% 39.52% 45.53
PER 80.29% 68.97% 74.20
Overall 67.20% 65.54% 66.36
LDC (w/GPE)
GPE 0.00% 0.00% 0.00
LOC 1.94% 37.74% 3.70
ORG 53.69% 39.52% 45.53
PER 80.29% 68.97% 74.20
Overall 30.58% 29.82% 30.19
larger context window (3 characters instead of 2
before and after the current character) to be quite
helpful to performance.
Apart from the training and test corpora, we
considered the gazetteers from LDC which con-
tain about 540K persons, 242K locations and 98K
organization names. Named entities in the train-
ing corpora which appeared in the gazetteers were
identified lexically or by using a maximum for-
ward match algorithm. Once named entities have
been identified, each character can then be anno-
tated with an NE chunk tag. The boosting learner
152
can view the NE chunk tag as an additional fea-
ture. Here we used binary gazetteer features. If
the character was annotated with an NE chunk
tag, its gazetteer feature was set to 1; otherwise
it was set to 0. However we found that adding bi-
nary gazetteer features does not significantly help
the performance when conjunction features were
used. In fact, it actually hurt the performance
slightly.
The features used in the final experiments were:
? The current character and its POS tag.
? The characters within a window of 3 charac-
ters before and after the current character.
? The POS tags within a window of 3 charac-
ters before and after the current character.
? A small set of conjunctions of POS tags and
characters within a window of 3 characters of
the current character.
? The BIO chunk tags of the previous 3 charac-
ters.
4 Results
Table 1 presents the results obtained on the MSRA
and CityU development test set. Table 2 presents
the results obtained on theMSRA, CityU and LDC
test sets. These numbers greatly underrepresent
what could be expected from the boosting model,
since we only used one-third of MSRA and CityU
training corpora due to limitations of the boost-
ing software. Another problem for the LDC cor-
pus was training/testing mismatch: we did not
train any models at all with the LDC training cor-
pus, which was the only training set annontated
with geopolitical entities (GPE). Instead, for the
LDC test set, we simply used the system trained
on the MSRA corpus. Thus, when we consider
the geopolitical entity (GPE), our low overall F-
measure on the LDC test set cannot be interpreted
meaningfully.3 Even so, using only one-third of
the training data, the results on the MSRA and
CityU test sets are reasonable: 75.07 and 80.51
overall F-measures were obtained on the MSRA
and CityU test sets, respectively.
5 Conclusion
We have described an experiment applying a
boosting based NER model originally designed
3Our LDC test result was scored twice by the organizer.
for multiple European languages instead to the
Chinese named entity recognition task. Even
though we only used one-third of the MSRA and
CityU corpora to train the system, the model
produced reasonable results, obtaining 75.07 and
80.51 overall F-measures on MSRA and CityU
test sets respectively.
Having established this baseline for compari-
son against our multilingual European language
boosting based NER models, our next step will be
to incorporate Chinese-specific attributes into the
model to compare with.
References
Eric Bauer and Ron Kohavi. An empirical comparison of
voting classification algorithms: Bagging, boosting, and
variants. Machine Learning, 36:105?142, 1999.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?. Named en-
tity extraction using AdaBoost. In Computational Natural
Language Learning (CoNLL-2002), at COLING-2002,
pages 171?174, Taipei, Sep 2002.
Gerard Escudero, Llu??s Ma`rquez, and German Rigau. Boost-
ing applied to word sense disambiguation. In 11th Euro-
pean Conference on Machine Learning (ECML-00), pages
129?141, 2000.
Yoav Freund and Robert E. Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting. Computer and System Sciences, 55(1):119?139,
1997.
David Opitz and Richard Maclin. Popular ensemble meth-
ods: An empirical study. Journal of Artificial Intelligence
Research, 11:169?198, 1999.
Robert E. Schapire and Yoram Singer. Improved boosting
algorithms using confidence-rated predictions. Machine
Learning, 37(3):297?336, 1999.
Robert E. Schapire and Yoram Singer. Boostexter: A
boosting-based system for text categorization. Machine
Learning, 39(2-3):135?168, 2000.
Robert E. Schapire. The boosting approach to machine learn-
ing: An overview. In MSRI workshop on Nonlinear Esti-
mation and Classification, 2002.
Fabrizio Sebastiani, Alessandro Sperduti, and Nicola Val-
dambrini. An improved boosting algorithm and its appli-
cation to automated text categorization. In Proceedings
of 9th ACM International Conference on Information and
Knowledge Management, pages 78?85, 2000.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and
Yongsheng Yang. Boosting for named entity recognition.
In Computational Natural Language Learning (CoNLL-
2002), at COLING-2002, pages 195?198, Taipei, Sep
2002.
Dekai Wu, Grace Ngai, and Marine Carpuat. Why nitpicking
works: Evidence for Occam?s razor in error correctors. In
20th International Conference on Computational Linguis-
tics (COLING-2004), Geneva, 2004.
Hua Ping Zhang, Qun Liu, Xue-Qi Cheng, Hao Zhang, and
Hong Kui Yu. Chinese lexical analysis using Hierarchi-
cal Hidden Markov Model. In Proceedings of the second
SIGHAN workshop on Chinese language processing, vol-
ume 17, pages 63?70, 2003.
153
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Phrase-Based Translation via Word Alignments from Stochastic Inversion Transduction Grammars  Markus SAERS   Dept. of Linguistics and Philology  Uppsala University Sweden   markus.saers@lingfil.uu.se Dekai WU   Human Language Technology Center Dept. of Computer Science & Engineering HKUST Hong Kong dekai@cs.ust.hk  Abstract We argue that learning word alignments through a compositionally-structured, joint process yields higher phrase-based transla-tion accuracy than the conventional heuris-tic of intersecting conditional models. Flawed word alignments can lead to flawed phrase translations that damage translation accuracy. Yet the IBM word alignments usually used today are known to be flawed, in large part because IBM models (1) model reordering by allowing unrestricted movement of words, rather than con-strained movement of compositional units, and therefore must (2) attempt to compen-sate via directed, asymmetric distortion and fertility models. The conventional heuris-tics for attempting to recover from the re-sulting alignment errors involve estimating two directed models in opposite directions and then intersecting their alignments ? to make up for the fact that, in reality, word alignment is an inherently joint relation. A natural alternative is provided by Inversion Transduction Grammars, which estimate the joint word alignment relation directly, eliminating the need for any of the conven-tional heuristics. We show that this align-ment ultimately produces superior translation accuracy on BLEU, NIST, and METEOR metrics over three distinct lan-guage pairs. 1 Introduction In this paper we argue that word alignments learned through a compositionally-structured, joint 
process are able to significantly improve the train-ing of phrase-based translation systems, leading to higher translation accuracy than the conventional heuristic of intersecting conditional models. To-day, statistical machine translation (SMT) systems perform at state-of-the-art levels; their ability to weigh different translation hypotheses against each other to find an optimal solution has proven to be a great asset. What sets various SMT systems apart are the models employed to determine what to con-sider optimal. The most common systems today consist of phrase-based models, where chunks of texts are substituted and rearranged to produce the output sentence. Our premise is that certain flawed word align-ments can lead to flawed phrase translations that in turn damage translation accuracy, since word alignment is the basis for learning phrase transla-tions in phrase-based SMT systems. A critical part of such systems is the word-level translation model, which is estimated from aligned data. Cur-rently, the standard way of computing a word alignment is to estimate a function linking words in one of the languages to words in the other. Func-tions can only define many-to-one relations, but word alignment is a many-to-many relation. The solution is to combine two functions, one in each direction, and harmonize them by means of some heuristic. After that, phrases can be extracted from the word alignments. The problem is that the starting point for word alignments is usually the IBM models (Brown et al, 1993), which are known to produce flawed alignments, in large part because they (1) model reordering by allowing unrestricted movement of words, rather than constrained movement of com-positional units, and therefore must (2) attempt to compensate via directed, asymmetric distortion and fertility models. 
28
The conventional heuristics for attempting to re-cover from the resulting alignment errors is to es-timate two directed models in opposite directions and then intersect their alignments ? to make up for the fact that, in reality, word alignment is an inherently joint relation. It is unfortunate that such a critical stage in the training process of an SMT system relies on inaccurate heuristics, which have been largely motivated by historical implementa-tion factors, rather than principles explaining lan-guage phenomena. Inversion Transduction Grammar (ITG) models provide a natural, alternative approach, by estimat-ing the joint word alignment relation directly, eliminating the need for any of the conventional heuristics. A transduction grammar is a grammar that generates sentences in two languages (L0 and L1) simultaneously; i.e., one start symbol expands into two strings, as for example in Figure 1(b).  A transduction grammar explains two languages si-multaneously.  ITGs model a class of transductions (sets of sentence translations) with expressive power and computational complexity falling be-tween (a) finite-state transducers or FSTs and (b) syntax-directed transduction grammars1 or SDTGs.  An ITG produces both a common structural form for a sentence pairs, as well as relating the words ? aligning them.  This could actually work as the joint word alignment that is usually constructed by heuristic function combination. Yet despite the substantial body of literature on word alignment, ITG based models, and phrase-based SMT, the existing work has not assessed the potential for improving phrase-based translation quality by using joint ITG based word alignments to replace the error-prone conditional IBM model based word alignments and associated heuristics for intersecting bidirectional IBM alignments. On one hand, word alignment work is usually evaluated not on actual translation quality, but rather on artificial metrics like alignment error rate (AER, Och & Ney, 2003), which relies on a manu-ally annotated gold standard word alignment. There are some indications that ITG produces bet-ter alignment then the standard method (Zhao & Vogel, 2003, Zhang & Gildea 2005, Chao & Li, 2007). There is, however, little inherent utility in alignments ? their value is determined by the SMT systems one can build from them. In fact, recent                                                            1 Which ?synchronous CFGs? are essentially identical to. 
studies have discredited the earlier assumption that lower AER is correlated with improved translation quality ? the opposite can very well occur (Ayan & Dorr, 2006). Therefore it is essential to evaluate the quality of the word alignment not in terms of AER, but rather in terms of actual translation qual-ity in a system built from it. On the other hand, ITG models have been em-ployed to improve translation quality as measured by BLEU (Papineni et al, 2002), but still without directly addressing the problem of dependence on inaccurate IBM alignments. S?nchez & Bened? (2006) construct an ITG from word alignments computed by the conventional IBM model, which does little to alleviate the problems. Sima?an & Mylonakis (2008) use an ITG to structure a prior distribution to a phrase extraction system, which is an altogether different approach. Cherry & Lin (2007) do use ITG to build word alignments, but blur the lines by still mixing in the conventional IBM method, and focus on phrase extraction. The present work clearly demonstrates, for the first time to our knowledge, that replacing the widely-used heuristic of intersecting IBM word alignments from two directed conditional models instead with a single ITG alignment from a joint model produces superior translation accuracy.  The experiments are performed on three distinct lan-guage pairs: German?English, Spanish?English, and French?English. Translation accuracy is re-ported in terms of BLEU, NIST, and METEOR metrics. 2 Background Statistical Machine Translation is a paradigm where translation is considered as a code-breaking problem. The goal is to find the most likely output sentence (clear text message) given the supplied input sentence (coded message), according to some model. To get a probabilistic model, large amounts of training data are used. These data have to be aligned so that an understanding of correspon-dences between the languages is there to be learnt from. Even if the data is assumed to be aligned at sentence level, sub-sentence alignment is also needed. This is usually carried out by training some statistical model of a word-to-word function (Brown et al, 1993), or a hidden Markov model consuming input words and emitting output words 
29
(Vogel, Ney & Tillmann, 1996). The toolkit GIZA++ (Och & Ney, 2000) is freely available and widely used to compute such word alignments. All these models learn a directed translation function that maps input words to output words. Since these functions focus solely on surface phe-nomena, they have no mechanisms for dealing with the kind of structured reordering between lan-guages that could account for, e.g., the difference between SVO languages and SOV languages. What emerges is in fact a rather flawed model of how one language is rewritten into another. The conventional way to alleviate this flaw is to train an equally flawed model in the other direction, and then intersect the two. This practice certainly alle-viates some of the problems, but far from all. To build a phrase-based SMT system, the word alignment is used as a starting point to try to ac-count for the entire sentence. This means that the word alignment is gradually expanded, so that all words in both sentences are accounted for, either by words in the other language, or by the null empty word ?. This process is called grow-diag-final (Koehn, Och & Marcu, 2003). The grow-diag-final process does smooth over some of the flaws still left in the word alignment, but error analysis gives reason to doubt that it re-pairs enough of the errors to avoid damaging trans-lation accuracy. Thus, we are motivated to investigate a completely different approach that attempts to avoid the noisy directed alignments in the first place. 2.1 Inversion Transduction Grammars A transduction is a set of sentence translation pairs ? just as a language is a set of sentences.  The set defines a relation between the input and output languages. In the generative view, a transduction gram-mar generates a transduction, i.e., a set of sentence translation pairs or bisentences ? just as an ordi-nary (monolingual) language grammar generates a language, i.e., a set of sentences.  In the recogni-tion view, alternatively, a transduction grammar biparses or accepts all sentence pairs of a trans-duction ? just as a language grammar parses or accepts all sentences of a language.  And in the transduction view, a transduction grammar trans-duces (translates) input sentences to output sen-tences. 
Two familiar classes of transductions have been in widespread use for decades in many areas of computer science and linguistics:  A syntax-directed transduction is a set of bisen-tences generated by some syntax-directed transduc-tion grammar or SDTG (Lewis & Stearns, 1968; Aho & Ullman, 1969, 1972).  A ?synchronous CFG? is equivalent to an SDTG.  A finite-state transduction is a set of bisentences generated by some finite-state transducer or FST.  It is possible to describe finite-state transductions us-ing SDTGs (or synchronous CFGs) by restricting them alternatively to the special cases of either ?right regular SDTGs? or ?left regular SDTGs?.  However, such characterizations rather misleadingly overlook the key point ? by severely limiting expressive power, finite-state transductions are orders of magni-tude cheaper to biparse, train, and induce than syn-tax-directed transductions ? and are often even more accurate to induce.  More recently, an intermediate equivalence class of transductions whose generative capacity and computational complexity falls in between these two has become widely used in state-of-the-art MT systems ? due to numerous empirical results indi-cating significantly better fit to modeling transla-tion between many human language pairs:  An inversion transduction is a set of bisentences generated by some inversion transduction gram-mar or ITG (Wu, 1995a, 1995b, 1997).  As above with finite-state transductions, it is possible to de-scribe inversion transductions using SDTGs (or syn-chronous CFGs) by restricting them alternatively to the special cases of ?binary SDTGs?, ?ternary SDTGs?, or ?SDTGs whose transduction rules are restricted to straight and inverted permutations only?.  Again however, as above, such characterizations rather misleadingly overlook the key point ? by se-verely limiting expressive power, inversion transduc-tions are orders of magnitude cheaper to biparse, train, and induce than syntax-directed transductions ? and are often even more accurate to induce.  Any SDTG (or synchronous CFG) of binary rank ? i.e., that has at most two nonterminals on the right-hand-side of any rule ? is an ITG.  (Simi-larly, any SDTG (or synchronous CFG) that is right regular is a finite-state transduction gram-mar.)  Thus, for example, any grammar computed by the binarization algorithm of Zhang et al 
30
monolingual bilingual  regular or finite-state languages FSA or CFG that is right regular or left regular      O(n2)  regular or finite-state transductions FST or  SDTG (or syn-chronous CFG) that is right regular or left regular  
    O(n4) context-free languages CFG   O(n3) inversion transductions ITG or  SDTG (or syn-chronous CFG) that is binary or ternary or inverting   O(n6)    syntax-directed transductions SDTG (or synchro-nous CFG)     O(n2n+2)  Table 1: Summary comparison of computational complexity for Viterbi and chart (bi)parsing, and EM training algorithms for both monolingual and bilingual hierarchies.   
(2006) is an ITG.  Similarly, any grammar induced following the hierarchical phrase-based translation method, which always yields a binary transduction grammar (Chiang 2005), is an ITG. Moreover, any SDTG (or synchronous CFG) of ternary rank ? i.e., that has at most three nontermi-nals on the right-hand-side of any rule ? is still equivalent to an ITG.  Of course, this does not hold for SDTGs (or synchronous CFGs) in general, which allow arbitrary rank (possibly exceeding three) at the price of exponential complexity, as summarized in Table 1. 
Without loss of generality, any ITG can be con-veniently written in a 2-normal form (Wu, 1995a, 1997).  This cannot be done for SDTGs (or syn-chronous CFGs) ? unlike the monolingual case of CFGs, which form an equivalence class of context-free languages that can all be written in Chomsky?s 2-normal form.  In the bilingual case, only ITGs 
form an equivalence class of inversion transduc-tions that can all be written in a 2-normal form. Formally, an ITG in this 2-normal form, which segregates syntactic versus lexical rules, consists of a tuple  where N is a set of non-terminal symbols, V0 and V1 are the vocabularies of L0 and L1 respectively, R is a set of transduction rules, and  is the start symbol.  Each trans-duction rule takes one of the following forms:  S ? X X ? [Y Z] X ? <Y Z> X ? segmentL0/? X ? ?/segmentL1 X ? segmentL0/segmentL1  where X, Y and Z may be any nonterminal. Aside from the start rule, there are two kinds of syntactic transduction rules, namely straight and inverted.  In the above notation, straight transduc-tion rules X ? [Y Z] use square brackets, whereas inverted rules X ? <Y Z> use angled brackets.  The transductions generated by straight nodes have the same order in both languages, whereas the transduction generated by the inverted nodes are inverted in one of the languages, mean-ing that the children are read left-to-right in L0 and right-to-left in L1. In Figure 1(b) for example, the parse tree node instantiating an inverted transduc-tion rule is marked with a horizontal bar.  This mechanism allows for a minimal amount of reor-dering, while keeping the complexity down. The last three forms are for lexical transduction rules.  Each segment comes from the vocabulary of one of the languages, indicated by the subscript.  In the simplest case, the two ?-rule forms define singletons, which insert ?spurious? segments into either language.  Spurious segments lack any cor-respondence in the other language ? they are ?aligned to null? ? and singletons are lexical rules that associate a null-aligned segment in one of the languages with an empty segment (?) in the other. On the other hand, the last rule form defines a lexical translation pair that aligns the word/phrase segmentL0 to its translation segmentL1.  Such rules can also be written com-positionally as a pair of singletons, although it reads less transparently:  X ? segmentL0/? ?/segmentL1 
31
Note that segments typically consist of multiple tokens. Common examples include: ? Chinese word/phrase segments consisting of multiple unsegmented character tokens ? Chinese word/phrase segments consisting of multiple smaller, presegmented multi-character word/phrase tokens ? English phrase/collocation segments consist-ing of multiple word tokens (roller coaster) ITGs inherently model phrasal translation ? lin-guistically speaking, ITGs assume the set of lexical translation pairs constitutes a phrasal lexicon (just as lexicographers assume in building ordinary eve-ryday dictionaries). An advantage of this is that the ITG biparsing and decoding algorithms perform integrated translation-driven segmentation si-multaneously with optimizing the parse (Wu, 1997; Wu & Wong, 1998). These properties allow an ITG to (1) insert and delete words/phrases, which matches the ability of the conventional methods for word alignment as well as phrase alignment, and (2) account for the reordering in a more principled and restricted way than conventional alignment methods. A stochastic ITG or SITG is an ITG where every rule is associated with a probability. As with a stochastic CFG (SCFG), the probabilities are conditioned on the left-hand-side symbol, so that the probability of rule X ? ? is p(?|X). A bracketing ITG or BITG or BTG (Wu, 1995a) contains only one nonterminal symbol, with syntactic transduction rules X ? [X X] and X ? <X X>, which means that it produces a bracketing rather than a labeled tree. With a sto-chastic BITG (SBITG or SBTG) it is still possible to determine an optimal tree, since inversion and alignment are coupled: where inversions are needed is decided by the translations, and vice versa. In Wu (1995b) algorithms for training a SITG using expectation maximization, as well as finding the optimal parse of a sentence pair given a SITG are presented. These are polynomial time O(n6), as seen in Table 1. Further pruning methods can also be added, especially for longer sentences. 2.2 Previous uses of ITG in alignment There have been several attempts to use various forms of ITGs in an alignment setting. 
Zhao & Vogel (2003) and S?nchez & Bened? (2006) both use GIZA++ to establish their SITG. Since they use GIZA++ to create their ITG, little light is shed on the question of whether an ITG produces better alignments than GIZA++. Zhang & Gildea (2005) compare lexicalized and standard ITGs on an alignment task, and conclude that both are superior to IBM models 1 and 4, and that lexicalization helps. They also employ some pruning techniques to speed up training. Chao & Li (2007) incorporate the reordering constraints im-posed by an ITG to their discriminative word aligner, and also note a lower alignment error rate in their system. Since neither work evaluates re-sults on a translation task, it is hard to know whether better AER would translate into improved translation quality, in light of Ayan & Dorr (2006). Sima?an & Mylonakis (2008) use an ITG as the basis of a prior distribution in their system that ex-tracts all possible phrases rather than employing a length cut-off, and report an increase in translation quality as measured by the BLEU score (Papineni et al, 2002). In this paper, it is not primarily pure ITG that is being evaluated, but it lends some credibility to our assumption that the ITG structure helps when aligning. Cherry & Lin (2007) use an ITG to produce phrase tables that are then used in a translation sys-tem. However, to make their system outperform GIZA++, they blend in a non-compositionality constraint that is still based on GIZA++ word alignments. We would very much like to clearly see and understand the difference between ITG and GIZA++ alignments, and the lines are somewhat blurred in their work. 3 Model First, the lexicon of the SBITG is initialized, by extracting lexical transduction rules from cooccur-rence data from the corpus. Each pair of tokens in each sentence pair is initially considered equally likely to be a lexical translation pair. Each token is also considered to be a possible singleton. The two syntactic transduction rules X ? [X X] and X ? <X X> are initially assumed to be equally likely. Then full expectation-maximization training (Wu, 1995b) is carried out on the training data. Instead of waiting for full convergence, the process is halted when the increase in the training data?s probability starts to decline. 
32
 sentence pairs tokens de-en 115,323 1,602,781 es-en 108,073 1,466,132 fr-en 95,990 1,340,718 Table 2: Summary of training data.   
  (a)   (a)   (a) 
  (b)   (b)   (b)  Figure 1: (a) Bidirectional IBM alignments and their intersection and (b) ITG alignments.  Figure 2: (a) Bidirectional IBM alignments and their intersection and (b) ITG alignments.  Figure 3: (a) Bidirectional IBM alignments and their intersec-tion and (b) ITG alignments.    At this point, we extract the optimal parses from the training data, and use the word alignment im-posed by the ITG instead of the one computed by GIZA++ (Och & Ney, 2000). Training after this point is carried out according to the guidelines for the WSMT08 baseline system (see section 4.2). In Figure 1(a) is an example of a sentence aligned with GIZA++, and in Figure 1(b) is the same sen-tence, aligned with ITG. In this case it is clearly visible how the structured reordering constraints that the ITG enforces results in a clear alignment, whereas GIZA++ is unable to sort it out. 
4 Experimental setup 4.1 Data We used a subset of the data provided for the Sec-ond Workshop on Statistical Machine Translation2, which consists mainly of texts from the Europarl corpus (Koehn, 2005). We used the Europarl part for the translation tasks: German?English (de-en), Spanish?English (es-en), and French?English (fr-en). Table 2 summarizes the datasets used for training. For tuning and testing, the tuning and de-velopment test sets provided for the workshop were used ? each measuring 2,000 sentence pairs. 4.2 Baseline system For baseline system we trained phrase-based SMT models with GIZA++ (Och & Ney, 2000), the training scripts supplied with Moses (Koehn et al,                                                            2 www.statmt.org/wmt08 
33
2007), and minimum error rate training (MERT, Och, 2003), all according to the WSMT08-guidelines for baseline systems. This means that 5 iterations are carried out with IBM model 1 train-ing, 5 iterations with HMM training, 3 iterations of IBM model 3 training, and finally 3 iterations of IBM model 4 training. After GIZA++ training, the Moses training script extracts and scores phrases, and establishes a lexicalized reordering model. The WSMT08 guidelines call for the combina-tion heuristic ?grow-diag-final-and? (GDFA). We also tried the ?intersect? combination heuristic, which simply calculates the intersection of align-ment points in the two directed alignments pro-vided by GIZA++. 4.3 SBITG system Since imposing an SBITG biparse on a sentence pair forces a word alignment on the sentence pair, word alignment under SBITG models is identical to biparsing. Expectation-maximization training was used to induce a SBITG from the training data. Training is halted when the EM-process started to converge. In our experience, convergence typically requires no more than 3 iterations or so. When EM training is finished, we extracted the optimal biparses from the training data, which then constitute the optimal alignment given the grammar. This alignment was then output in GIZA++ format. All singletons from the SBITG alignment were converted to be null-alignments in the GIZA++ formatted file. These files could then be used instead of GIZA++ in the remainder of the training process for the phrase-based translation system. Although the results from the ITG are inter-preted as two directed alignments, they are identi-cal, both with each other and the intersection. Trying different combination heuristics for these results always yields the same results. The training process was identical save for the fact that the word alignments were produced by SBITGs rather than by GIZA++. 5 Experimental results We trained a total of nine systems (three tasks and three different alignments), which we evaluated with three different measures: BLEU (Papineni et al, 2002), NIST (Doddington, 2002), and METEOR (Lavie & Agarwal 2007). 
Figure 2 shows a sentence pair as it was aligned with the two different models. Figure 2(a) shows the GIZA++ alignment in both directions, and the intersection between them, whereas Figure 2(b) shows the SBITG alignment with its common structure. The asymmetric reordering mechanism of the IBM models is simply unable to relate the two halves to one another. The segment zur kenntnis genommen could certainly be said to mean note, but as a verb, and not as a noun, which is the current usage of the word. This is an inherent problem of the asymmetry of the IBM models, which is rectified by simultaneous alignment. Figure 3 shows another sentence pair. Again, Figure 3(a) was aligned with GIZA++ and Figure 3(b) with the SITG model. This shows a case with perhaps even more structured reordering, where a notion of constituency is definitely needed to get it right. SITG handles constituency, and gets this is-sue right. The IBM models do not, resulting in the error of aligning either to aufgerufen. As mentioned before, the GDFA heuristic is ap-plied after the word alignment process, and it does fix some of these problems. Therefore we opted to evaluate this, not on alignments, but rather on translation quality of phrase based SMT systems derived from the alignments. Our empirical results confirm that SBITG alignments do indeed lead to better translation quality, as shown in Table 2. We also tried the intersect combination heuris-tic, and depending on language pair and evaluation metric, the GDFA and intersect heuristics come out on top. The ITG approach is, however, consistently better than either of the heuristics applied to GIZA++ output. 6 Discussion There are of course fundamental differences be-tween ITG and IBM models. The main difference is that IBM models are directed and surface ori-ented, whereas the ITG model is joint and struc-tured. The directedness means that the IBM models are unable to produce a word alignment that is op-timal for a sentence pair; they can only produce word alignments that are optimal when translating from one language into the other. An ITG on the other hand is capable of producing the optimal alignment that explains both sentences in the pair. We see this phenomenon clearly in Figures 1?3. 
34
BLEU NIST METEOR GIZA++ GIZA++ GIZA++  GDFA inters. SBITG GDFA inters. SBITG GDFA inters. SBITG de-en 20.59 20.69 21.13 5.8668 5.8623 5.9380 0.4969 0.4953 0.5029 es-en 25.97 26.33 26.63 6.6352 6.6793 6.7407 0.5599 0.5582 0.5612 fr-en 26.03 26.17 26.63 6.6907 6.7071 6.8151 0.5544 0.5560 0.5635  Table 2: Results. The best result on each task/metric combination is in bold digits. (The identical results for SBITG on Spanish?English and French?English are not typos.)  IBM models are also built to allow for fairly ?whimsical? reorderings, which are not modeled very well to begin with. This allows for far too many degrees of freedom to fit the model to the data. Because natural languages are inherently structural, this excess degree of freedom could hurt performance. Some restraints are needed. ITGs on the other hand only allow for compositionally structured reordering, which corresponds better to the reorderings between natural languages. There are some issues with ITG as well, one of them be-ing that all permutations are actually not allowed, even if structured. This has led to some problems when an a prior alignment or structure is forced upon a sentence pair, but using unrestricted expec-tation-maximization means that the sentence pair is fitted to the grammar, and what the grammar can-not express is not applied to the data. Even if ITG proves to be too restrictive in the future, the fact that it bases reordering on structure, rather than unrestricted lexical movement, gives it an edge over the IBM models. The benefits of structured reordering as opposed to unrestricted are clearly visible in Figures 1?3. An argument to continue using IBM models is that two directed alignments can be intersected and heuristically grown to build a joint alignment, thus compensating for the flaws in the original models. But as we have seen in Figure 3, even the combi-nation of two models contains errors that should have been avoided. This approach is not able to smooth over the flaws of the IBM models. The results in this paper give credibility to the claim that these limitations of the IBM models are so serious that they hurt translation quality of sys-tems built upon them; even after the phrase build-ing heuristic has been applied. Systems built on ITG alignment on the other hand fare better, on all three evaluation metrics. 
There is still more to be done. So far we have only employed bracketing SITGs, which are not able to distinguish one structure form another. The structural changes that the SBITG is capable of are dictated by the alignment of the leaves in the tree. This seems impressive, given the information at hand, but is really a logical conclusion of the fact that the grammar can leverage different alignment probabilities against each other, and as the align-ment is coupled to the structure of the ITG parse, the structure is constrained to the alignment. The reverse is also true: the alignment is constrained by the structure. This coupling is essential to the train-ing of SITGs. For a SBITG, there is very little in-formation in the structure, only the decision to read the node as straight or inverted. This is not an in-herent property of ITGs in general; more informa-tion can be carried higher up in the tree by labeling the nonterminals. There is great hope that adding more information to the structuring, even better alignments could be gained. In this paper we have extracted the word align-ments from ITG biparses, and inserted them into the conventional phrase-based SMT pipeline. It is feasible to extract phrases directly from the gram-mar, as demonstrated by Cherry & Lin (2007). Our results suggest that augmenting other portions of the phrase-based SMT framework with ITG struc-tures might also be worth exploring, in particular decoding. Recall that in the transduction view of transduction grammars (as opposed to generative or recognition views), an output translation can be determined by parsing an input sentence with a transduction grammar (Wu 1996; Wu & Wong 1998). This kind of translation would also entail the notion of structure that we have just witnessed helping alignment. Phrase-based SMT currently relies on unrestricted phrasal movement, which is a lot better than unrestricted lexical movement, but could probably use some structure as well. 
35
7 Conclusion We have shown that learning word alignments through a compositionally-structured, joint process yields higher phrase-based translation accuracy than the conventional heuristic of intersecting con-ditional models. The conventional method with IBM-models suf-fers from their directionality. The asymmetry causes bad alignments. We have instead introduced an automatically induced ITG alignment that does not suffer from this asymmetry, and is able to ex-plain the two sentences simultaneously rather than one in terms of the other. The IBM-models also suffers from a simplified reordering model, which relies on moving individual words. The hierarchi-cal structure of ITGs means that even a BITG has enough structural information to outperform the IBM models. Previous work shows that these ad-vantages translate into better alignments as meas-ured against a manually annotated gold standard using alignment error rate (AER). Previous work also shows that AER is a poor indicator of whether translation quality is increased. We have showed that the increase in alignment quality actually translates into an increase in translation quality in this case, as measured by BLEU, NIST and METEOR across three different language pairs. Acknowledgments This material is based upon work supported in part by the Swedish National Graduate School of Language Technology, the Olof Gjerd-mans Travel Grant, the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and the Hong Kong Research Grants Council (RGC) under research grants GRF621008, DAG03/04.EG09, RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or recommendations ex-pressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. References AHO, Alfred V. & Jeffrey D. ULLMAN (1969) ?Syntax-directed trans-lations and the pushdown assembler? in Journal of Computer and System Sciences 3: 37?56. AHO, Alfred V. & Jeffrey D. ULLMAN (1972) The Theory of Parsing, Translation, and Compiling (Volumes 1 and 2). Englewood Cliffs, NJ: Prentice-Hall. AYAN, Necip Fazil & Bonnie J. DORR (2006) ?Going Beyond AER: An Extensive Analysis of Word Alignments and Their Impact on MT? in COLING-ACL?06, pp. 9?16, Sydney, Australia, July 2006. BROWN, Peter F., Stephen A. DELLA PIETRA, Vincent J. DELLA PIETRA & Robert L. MERCER (1993) ?The Mathematics of Statis-tical Machine Translation? in Computational Linguistics 19(2): 263?311.  CHAO, Wen-Han & Zhou-Jun LI (2007) ?Incorporating Constituent Structure Constraint into Discriminative Word Alignment? in MT Summit XI, pp. 97?103, Copenhagen, Denmark. 
CHERRY, Colin & Dekang LIN (2007) ?Inversion Transduction Grammar for Joint Phrasal Translation Modeling? in Proceedings of SSST, pp. 17?24, Rochester, New York, April 2007. CHIANG, David (2005) ?A Hierarchical Phrase-Based Model for Sta-tistical Machine Translation? in ACL-2005, pp. 263?270, Ann Ar-bor, MI, June 2005. KOEHN, Philipp, Franz Josef OCH & Daniel MARCU (2003) ?Statisti-cal Phrase-based Translation? in HLT-NAACL?03, pp. 127?133. KOEHN, Philipp (2005) ?Europarl: A Parallel Corpus for Statistical Machine Translation? in MT Summit X, Phuket, Thailand, Septem-ber 2005. DODDINGTON, George (2002) ?Automatic Evaluation of Machine Translation Quality using n-gram Co-occurrence Statistics? in HLT-2002. San Diego, California. KOEHN, Philipp, Hieu HOANG, Alexandra BIRCH, Chris CALLISON-BURCH, Marcello FEDERICO, Nicola BERTOLDI, Brooke COWAN, Wade SHEN, Christine MORAN, Richard ZENS, Chris DYER, On-drej BOJAR, Alexandra CONSTANTIN & Evan HERBST (2007) ?Moses: Open Source Toolkit for Statistical Machine Translation? in ACL?07, Prague, Czech Republic, June 2007.  LAVIE, Alon & Abhaya AGARWAL (2007) ?METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgment? in WSMT. Prague, Czech Republic, June 2007. LEWIS, Philip M. & Richard E. STEARNS. (1968) ?Syntax-directed transduction? in Journal of the ACM 15: 465?488. OCH, Franz Josef & Hermann NEY (2000) ?Improved Statistical Alignment Models? in ACL-2000, pp. 440?447, Hong Kong, Oc-tober 2000. OCH, Franz Josef (2003) ?Minimum error rate training in statistical machine translation? in ACL?03. OCH, Franz Josef & Hermann NEY (2003) ?A Systematic Comparison of Various Statistical Alignment Models? in Computational Lin-guistics 29(1), pp. 19?52. PAPINENI, Kishore, Salim ROUKOS, Todd WARD & Wei-Jing ZHU (2002) ?BLEU: a Method for Automatic Evaluation of Machine Translation? in ACL?02, pp 311-318. Philadelphia, Pennsylvania. S?NCHEZ, J. A., J.M. BENED? (2006) ?Stochastic Inversion Transduc-tion Grammars for Obtaining Word Phrases for Phrase-based Sta-tistical Machine Translation? in WSMT, pp. 130?133, New York City, June 2006. SIMA?AN, Khalil & Markos MYLONAKIS (2008) ?Better Statistical Estimation Can Benefit all Phrases in Phrase-based Statistical Ma-chine Translation? in SLT 2008, pp. 237?240, Goa, India, Decem-ber 2008. VOGEL, Stephan, Hermann NEY & Christoph TILLMANN (1996) ?HMM-based Word Alignment in Statistical Translation? in COLING?96, pp. 836?841. WU, Dekai (1995a) ?An Algorithm for Simultaneously Bracketing Parallel Texts by Aligning Words? in ACL?95, pp. 244?251, Cam-bridge, Massachusetts, June 1995. WU, Dekai (1995b) ?Trainable Coarse Bilingual Grammars for Paral-lel Text Bracketing? in WVLC-3, pp. 69?82, Cambridge, Massa-chusetts, June 1995. WU, Dekai (1996) ?A polynomial-time algorithm for statistical ma-chine translation? in ACL-96, Santa Cruz, CA: June 1996. WU, Dekai (1997) ?Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora? in Computational Linguis-tics 23(3), pp 377?403. WU, Dekai & Hongsing WONG (1998) ?Machine Translation with a Stochastic Grammatical Channel? in COLING-ACL'98, Montreal, August 1998.  ZHANG, Hao & Daniel GILDEA (2005) ?Stochastic Lexicalized Inver-sion Transduction Grammar for Alignment? in ACL?05, pp. 475?482, Ann Arbor, June 2005. ZHANG, Hao, Liang HUANG, Dan GILDEA & Kevin KNIGHT (2006) ?Synchronous Binarization for Machine Translation? in HLT/NAACL-2006, pp. 256?263, New York, June 2006. ZHAO, Bing & Stephan VOGEL (2003) ?Word Alignment Based on Bilingual Bracketing? in HLT-NAACL Workshop: Building and Using Parallel Texts, pp. 15?18, Edmonton, May?June 2003. 
36
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 29?32,
Paris, October 2009. c?2009 Association for Computational Linguistics
Learning Stochastic Bracketing Inversion Transduction Grammars
with a Cubic Time Biparsing Algorithm
Markus SAERS Joakim NIVRE
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
We present a biparsing algorithm for
Stochastic Bracketing Inversion Transduc-
tion Grammars that runs in O(bn3) time
instead of O(n6). Transduction gram-
mars learned via an EM estimation proce-
dure based on this biparsing algorithm are
evaluated directly on the translation task,
by building a phrase-based statistical MT
system on top of the alignments dictated
by Viterbi parses under the induced bi-
grammars. Translation quality at different
levels of pruning are compared, showing
improvements over a conventional word
aligner even at heavy pruning levels.
1 Introduction
As demonstrated by Saers & Wu (2009) there
is something to be gained by applying structural
models such as Inversion Transduction Grammars
(ITG) to the problem of word alignment. One is-
sue is that na??ve methods for inducing ITGs from
parallel data can be very time consuming. We in-
troduce a parsing algorithm for inducing Stochas-
tic Bracketing ITGs from parallel data in O(bn3)
time instead ofO(n6), where b is a pruning param-
eter (lower = tighter pruning). We try out different
values for b, and evaluate the results on a transla-
tion tasks.
In section 2 we summarize the ITG framework;
in section 3 we present our algorithm, whose time
complexity is analyzed in section 4. In section 5
we describe how the algorithm is evaluated, and in
section 6, the empirical results are given.
2 Inversion Transduction Grammars
Inversion transductions are a theoretically inter-
esting and empirically useful equivalence class of
transductions, with expressiveness and computa-
tional complexity characteristics lying intermedi-
ate between finite-state transductions and syntax-
directed transductions. An Inversion Transduc-
tion Grammar (ITG) can be used to synchronously
generate sentence pairs, synchronously parse sen-
tence pairs, or transduce from a sentence in one
language to a sentence in another.1
The equivalence class of inversion transduc-
tions can be described by restricting Syntax-
Directed Transduction Grammars (SDTG)2 in var-
ious equivalent ways to the special cases of (a) bi-
nary SDTGs, (b) ternary SDTGs, or (c) SDTGs
whose transduction rules are restricted to straight
and inverted permutations only.
Thus on one hand, any binary or ternary SDTG
is an ITG. Conversely, any ITG can be stated in
binary two-normal form (Wu, 1997). Only three
kinds of rules are present in the normal form:
A? [BC]
A? ?BC?
A? e/f
On the other hand, under characterization (c),
what distinguishes ITGs is that the permutation of
constituents is restricted in such a way that all chil-
dren of a node must be read either left-to-right, or
right-to-left. The movement only applies to one of
the languages, the other is fixed. Formally, an ITG
is a tuple ?N,V,?, S?, where N is a set of nonter-
minal symbols, ? is a set of rewrite rules, S ? N
is the start symbol and V ? VE ? VF is a set of
biterminal symbols, where VE is the vocabulary of
E and VF is the vocabulary of F . We will write a
biterminal as e/f , where e ? VE and f ? VF . A
sentence pair will be written as e/f , and a bispan
as es..t/fu..v.
Each rule ? ? ? is a tuple ?X, ?, ?? where
X ? N is the right hand side of the rule, ? ?
1All transduction grammars (a.k.a. synchronous gram-
mars, or simply bigrammars) can be interpreted as models
for generation, recognition, or transduction.
2SDTGs (Lewis & Stearns (1968); Aho & Ullman (1969),
(1972)) are also recently called synchronous CFGs.
29
{N ? V }? is a series of nonterminal and biter-
minal symbols representing the production of the
rule and ? ? {?, [], ??} denotes the orientation (ax-
iomatic, straight or inverted) of the rule. Straight
rules are read left-to-right in both languages, while
inverted rules are read left-to-right in E and right-
to-left in F . The direction of the axiomatic rules is
undefined, as they must be completely made up of
terminals. For notational convenience, the orien-
tation of the rule is written as surrounding the pro-
duction, like so: X ? ?, X ? [?] and X ? ???.
The vocabularies of the languages may both in-
clude the empty token , allowing for deletions
and insertions. The empty biterminal, / is not
allowed.
2.1 Stochastic ITGs
In a Stochastic ITG (SITG), each rule is also asso-
ciated with a probability, such that
?
?
Pr(X ? ?) = 1
for all X ? N . The probability of a deriva-
tion S ?? e/f is defined as the production of
the probabilities of all rules used. As shown by
Wu (1995), it is possible to fit the parameters of
a SITG to a parallel corpus via EM (expectation-
maximization) estimation.
2.2 Bracketing ITGs
An ITG where there is only one nonterminal (other
than the start symbol) is called a bracketing ITG
(BITG). Since the one nonterminal is devoid of
information, it can only be used to group its chil-
dren together, imposing a bracketing on the sen-
tence pairs.
3 Parsing SBITGs
In this section we present a biparsing algorithm
for Stochastic Bracketing Inversion Transduction
Grammars (SBITGs) in normal form which incor-
porates a pruning parameter b. The algorithm is
basically an agenda-based bottom-up chart parser,
where the pruning parameter controls the number
of active items of a given length.
To parse a sentence pair e/f , the parser needs
a chart C and a series of T + V agendas
A1, A2, . . . , AT+V , where T = |e| and V = |f |.
An item is defined as a nonterminal symbol (we
use X to denote the anonymous nonterminal sym-
bol of the bracketing ITG) and one span in each
language, written as Xstuv where 0 ? s ? t ? T
corresponds to the span es..t and 0 ? u ? v ? V
corresponds to the span fu..v. The length of an
item is defined as |Xstuv| = (t?s)+(v?u). Since
items are grouped by their length, highly skewed
links (eg. 6:1) will be competing with very even
links (eg. 4:3). Skewed links are generally bad
(and should be pruned), or have a high probability
(which means they are likely to survive pruning).
An item may be active or passive, the active items
are present in the agendas and the chart, whereas
the passive items are only present in the chart.
The parser starts by asserting items from all lex-
ical rules (X ? e/f ), and placing them on their
respective agendas. After the initial seeding, the
agendas are processed in order. When an agenda
is processed, it is first pruned, so that only the b
best items are kept active. After pruning, the re-
maining active items are allowed to be extended.
When extended, the item combines with an adja-
cent item in the chart to form a larger item. The
newly created item is considered active, and added
to both the chart and the appropriate agenda. Once
an item has been processed it goes from being ac-
tive to being passive. The process is halted when
the goal item S0T0V is reached, or when no active
items remain. To build the forest corresponding to
the parse process, back-pointers are used.
3.1 Initialization
In the initial step, the set of lexical items L is built.
All lexical items i ? L are then activated by plac-
ing them on their corresponding agenda A|i|.
L =
?
?
?Xstuv
??????
0?s? t?T,
0?u?v?V,
X ? es..t/fu..v ? ?
?
?
?
By limiting the length of phrasal terminals to some
threshold ?, the variables t and v can be limited to
s+? and u+? respectively, limiting the complexity
of the initialization step from O(n4) to O(n2).
3.2 Recursion
In the recursive step we build a set of extensions
E(i) for all active items i. All items in E(i)
are then activated by placing them on their cor-
responding agenda (i ? A|i|).
E(Xstuv) =
{XStUv|0?S?s, 0?U?u,XSsUu ? C} ?
{XsSuU |t?S?T, v?U?V,XtSvU ? C} ?
{XsSUv|t?S?T, 0?U?u,XtSUu ? C} ?
{XStuU |0?S?s, v?U?V,XSsvU ? C}
30
Since we are processing the agendas in order, any
item in the chart will be as long as or shorter than
the item being extended. This fact can be exploited
to limit the number of possible siblings explored,
but has no impact on time complexity.
3.3 Viterbi parsing
When doing Viterbi parsing, all derivations but
the most probable are discarded. This gives an
unambiguous parse, which dictates exactly one
alignment between e and f . The alignment of
the Viterbi parse can be used to substitute that of
other word aligners (Saers and Wu, 2009) such as
GIZA++ (Och and Ney, 2003).
4 Analysis
Looking at the algorithm, it is clear that there will
be a total of T + V = O(n) agendas, each con-
taining items of a certain length. The items in an
agenda can start anywhere in the alignment space:
O(n2) possible starting points, but once the end
point in one language is set, the end point in the
other follows from that, adding a factor O(n).
This means that each agenda contains O(n3) ac-
tive items. Each active item has to go through all
possible siblings in the recursive step. Since the
start point of the sibling is determined by the item
itself (it has to be adjacent), only the O(n2) pos-
sible end points have to be explored. This means
that each active item takes O(n2) time to process.
The total time is thus O(n6): O(n) agendas,
containing O(n3) active items, requiring O(n2)
time to process. This is also the time complex-
ity reported for ITGs in previous work (Wu, 1995;
Wu, 1997).
The pruning works by limiting the number of
active items in an agenda to a constant b, meaning
that there are O(n) agendas, containing O(b) ac-
tive items, requiring O(n2) time to process. This
gives a total time complexity of O(bn3).
5 Evaluation
We evaluate the parser on a translation task
(WMT?08 shared task3). In order to evaluate on
a translation task, a translation system has to be
built. We use the alignments from the Viterbi
parses of the training corpus to substitute the
alignments of GIZA++. This is the same approach
as taken in Saers & Wu (2009). We will evalu-
ate the resulting translations with two automatic
3http://www.statmt.org/wmt08/
metrics: BLEU (Papineni et al, 2002) and NIST
(Doddington, 2002).
6 Empirical results
In this section we describe the experimental setup
as well as the outcomes.
6.1 Setup
We use the Moses Toolkit (Koehn et al, 2007) to
train our phrase-based SMT models. The toolkit
also includes scripts for applying GIZA++ (Och
and Ney, 2003) as a word aligner. We have
trained several systems, one using GIZA++ (our
baseline system), one with no pruning at all, and
6 different values of b (1, 10, 25, 50, 75 and
100). We used the grow-diag-final-and
method to extract phrases from the word align-
ment, and MERT (Och, 2003) to optimize the re-
sulting model. We trained a 5-gram SRI language
model (Stolcke, 2002) using the corpus supplied
for this purpose by the shared task organizers. All
of the above is consistent with the guidelines for
building a baseline system for the WMT?08 shared
task.
The translation tasks we applied the above
procedure to are all taken from the Europarl
corpus (Koehn, 2005). We selected the tasks
German-English, French-English and Spanish-
English. Furthermore, we restricted the training
sentence pairs so that none of the sentences ex-
ceeded length 10. This was necessary to be able to
carry out exhaustive search. The total amount of
training data was roughly 100,000 sentence pairs
in each language pair, which is a relatively small
corpus, but by no means a toy example.
6.2 Grammar induction
It is possible to set the parameters of a SBITG
by applying EM to an initial guess (Wu, 1995).
As our initial guess, we used word co-occurrence
counts, assuming that there was one empty token
in each sentence. This gave an estimate of the lex-
ical rules. The probability mass was divided so
that the lexical rules could share half of it, while
the other half was shared equally by the two struc-
tural rules (X ? [XX] and X ? ?XX?).
Several training runs were made with different
pruning parameters. The EM process was halted
when a relative improvement in log-likelihood of
10?3 was no longer achieved over the previous it-
eration.
31
Baseline Different values of b for SBITGs
Metric (GIZA++) ? 100 75 50 25 10 1
Spanish-English
BLEU 0.2597 0.2663 0.2671 0.2661 0.2653 0.2655 0.2608 0.1234
NIST 6.6352 6.7407 6.7445 6.7329 6.7101 6.7312 6.6439 3.9705
time 03:20:00 02:40:00 02:00:00 01:20:00 00:38:00 00:17:00 00:03:10
German-English
BLEU 0.2059 0.2113 0.2094 0.2091 0.2090 0.2091 0.2050 0.0926
NIST 5.8668 5.9380 5.9086 5.8955 5.8947 5.9292 5.8743 3.4297
time 03:40:00 02:45:00 02:10:00 01:25:00 00:41:00 00:17:00 00:03:20
French-English
BLEU 0.2603 0.2663 0.2655 0.2668 0.2669 0.2654 0.2632 0.1268
NIST 6.6907 6.8151 6.8068 6.8068 6.8065 6.7013 6.7136 4.0849
time 03:10:00 02:45:00 02:10:00 01:25:00 00:42:00 00:17:00 00:03:25
Table 1: Results. Time measures are approximate time per iteration.
Once the EM process terminated, Viterbi parses
were calculated for the training corpus, and the
alignments from them outputted in the same for-
mat produced by GIZA++.
6.3 Results
The results are presented in Table 1. GIZA++
generally terminates within minutes (6?7) on the
training corpora used, making it faster than any
of the SBITGs (they generally required 4?6 iter-
ations to terminate, making even the fastest ones
slower than GIZA++). To put the times in per-
spective, about 6 iterations were needed to get
the ITGs to converge, making the longest training
time about 16?17 hours. The time it takes to ex-
tract the phrases and tune the model using MERT
is about 14 hours for these data sets.
Looking at translation quality, we see a sharp
initial rise as b grows to 10. At this point the
SBITG system is on par with GIZA++. It con-
tinues to rise up to b = 25, but after that is more or
less levels out. From this we conclude that the pos-
itive results reported in Saers & Wu (2009) hold
under harsh pruning.
7 Conclusions
We have presented a SBITG biparsing algorithm
that uses a novel form of pruning to cut the com-
plexity of EM-estimation from O(n6) to O(bn3).
Translation quality using the resulting learned
SBITG models is improved over using conven-
tional word alignments, even under harsh levels of
pruning.
Acknowledgments
The authors are grateful for the comments made by the two anonymous review-
ers. This work was funded by the Swedish National Graduate School of Lan-
guage Technology, the Defense Advanced Research Projects Agency (DARPA)
under GALE Contract No. HR0011-06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the authors and do
not necessarily reflect the views of the Defense Advanced Research Projects
Agency.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax-directed translations
and the pushdown assembler. Journal of Computer and System Sciences,
3(1):37?56.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Transla-
tion, and Compiling (Volumes 1 and 2). Prentice-Halll, Englewood Cliffs,
NJ.
George Doddington. 2002. Automatic evaluation of machine translation qual-
ity using n-gram co-occurrence statistics. In Human Language Technology
conference (HLT-2002), San Diego, CA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine trans-
lation. In ACL-2007 Demo and Poster Sessions, pages 177?180, Prague,
Jun.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine trans-
lation. In Machine Translation Summit X, Phuket, Thailand, September.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-directed transduction.
Journal of the Association for Computing Machinery, 15(3):465?488.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training in statistical machine
translation. In 41st Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of machine translations. In 40th Annual
Meeting of the Association for Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, Jul.
Markus Saers and Dekai Wu. 2009. Improving phrase-based translation via
word alignments from Stochastic Inversion Transduction Grammars. In
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statis-
tical Translation (at NAACL HLT 2009), pages 28?36, Boulder, CO, Jun.
Andreas Stolcke. 2002. SRILM ? an extensible language modeling toolkit.
In International Conference on Spoken Language Processing, Denver, CO,
Sep.
Dekai Wu. 1995. Trainable coarse bilingual grammars for parallel text brack-
eting. In Third Annual Workshop on Very Large Corpora (WVLC-3), pages
69?81, Cambridge, MA, Jun.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual
parsing of parallel corpora. Computational Linguistics, 23(3):377?404,
Sep.
32
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 33?36,
Paris, October 2009. c?2009 Association for Computational Linguistics
Empirical lower bounds on translation unit error rate for the full class of
inversion transduction grammars
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Dekai Wu
Human Language Technology Center
Hong Kong Univ. of Science and Technology
dekai@cs.ust.hk
Abstract
Empirical lower bounds studies in which
the frequency of alignment configurations
that cannot be induced by a particular for-
malism is estimated, have been important
for the development of syntax-based ma-
chine translation formalisms. The for-
malism that has received most attention
has been inversion transduction grammars
(ITGs) (Wu, 1997). All previous work
on the coverage of ITGs, however, con-
cerns parse failure rates (PFRs) or sen-
tence level coverage, which is not di-
rectly related to any of the evaluation mea-
sures used in machine translation. S?gaard
and Kuhn (2009) induce lower bounds on
translation unit error rates (TUERs) for a
number of formalisms, incl. normal form
ITGs, but not for the full class of ITGs.
Many of the alignment configurations that
cannot be induced by normal form ITGs
can be induced by unrestricted ITGs, how-
ever. This paper estimates the difference
and shows that the average reduction in
lower bounds on TUER is 2.48 in absolute
difference (16.01 in average parse failure
rate).
1 Introduction
The first stage in training a machine translation
system is typically that of aligning bilingual text.
The quality of alignments is in that case of vi-
tal importance to the quality of the induced trans-
lation rules used by the system in subsequent
stages. In string-based statistical machine trans-
lation, the alignment space is typically restricted
by the n-grams considered in the underlying lan-
guage model, but in syntax-based machine trans-
lation the alignment space is restricted by very
different and less transparent structural contraints.
While it is easy to estimate the consequences of
restrictions to n-grams of limited size, it is less
trivial to estimate the consequences of the struc-
tural constraints imposed by syntax-based ma-
chine translation formalisms. Consequently, much
work has been devoted to this task (Wu, 1997;
Zens and Ney, 2003; Wellington et al, 2006;
Macken, 2007; S?gaard and Kuhn, 2009).
The task of estimating the consequences of
the structural constraints imposed by a particular
syntax-based formalism consists in finding what is
often called ?empirical lower bounds? on the cov-
erage of the formalism (Wellington et al, 2006;
S?gaard and Kuhn, 2009). Gold standard align-
ments are constructed and queried in some way
as to identify complex alignment configurations,
or they are parsed by an all-accepting grammar
such that a parse failure indicates that no align-
ment could be induced by the formalism.
The assumption in this and related work that en-
ables us to introduce a meaningful notion of align-
ment capacity is that simultaneously recognized
words are aligned (Wu, 1997; Zhang and Gildea,
2004; Wellington et al, 2006; S?gaard and Kuhn,
2009). As noted by S?gaard (2009), this defi-
nition of alignment has the advantageous conse-
quence that candidate alignments can be singled
out by mere inspection of the grammar rules. It
also has the consequence that alignments are tran-
sitive (Goutte et al, 2004), since simultaneity is
transitive.
While previous work (S?gaard and Kuhn, 2009)
has estimated empirical lower bounds for normal
form ITGs at the level of translation units (TUER),
or cepts (Goutte et al, 2004), defined as maxi-
mally connected subgraphs in alignments, nobody
has done this for the full class of ITGs. What
is important to understand is that while normal
form ITGs can induce the same class of transla-
tions as the full class of ITGs, they do not induce
the same class of alignments. They do not, for ex-
33
ample, induce discontinuous translation units (see
Sect. 3). Sect. 2 briefly presents some related re-
sults in the literature. Some knowledge about for-
malisms used in machine translation is assumed.
2 Related work
Aho and Ullman (1972) showed that 4-ary syn-
chronous context-free grammars (SCFGs) could
not be binarized, and Satta and Peserico (2005)
showed that the hiearchy of SCFGs beyond ternary
ones does not collapse; they also showed that the
complexity of the universal recognition problem
for SCFGs is NP-complete. ITGs on the other
hand has a O(|G|n6) solvable universal recog-
nition problem, which coincides with the unre-
stricted alignment problem (S?gaard, 2009). The
result extends to decoding in conjunction with a
bigram language model (Huang et al, 2005).
Wu (1997) introduced ITGs and normal form
ITGs. ITGs are a notational variant of the sub-
class of SCFGs such that all indexed nonterminals
in the source side of the RHS occur in the same
order or exactly in the inverse order in the target
side of the RHS. It turns out that this subclass of
SCFGs defines the same set of translations that can
be defined by binary SCFGs. The different forms
of production rules are listed below with the more
restricted normal form production rules in the right
column, with ? ? (N ?{e/f | e ? T ?, f ? T ?})?
(N nonterminals and T terminals, as usual). The
RHS operator [ ] preserves source language con-
stituent order in the target language, while ? ? re-
verses it.1
A ? [?] A ? [BC]
A ? ??? A ? ?BC?
A ? e/f
Several studies have adressed the alignment ca-
pacity of ITGs and normal form ITGs. Zens and
Ney (2003) induce lower bounds on PRFs for
normal form ITGs. Wellington et al (2006) in-
duce lower bounds on PRFs for ITGs. S?gaard
and Kuhn (2009) induce lower bounds on TUER
for normal form ITGs and more expressive for-
malisms for syntax-based machine translation. No
one has, however, to the best our knowledge in-
duced lower bounds on TUER for ITGs.
1One reviewer argues that our definition of full ITGs is
not equivalent to the definition in Wu (1997), which, in the
reviewer?s words, allows ?at most one lexical item from each
language?. Sect. 6 of Wu (1997), however, explicitly encour-
ages lexical elements in rules to have more than one lexical
item in many cases.
3 Experiments
As already mentioned empirical lower bounds
studies differ in four important respects, namely
wrt.: (i) whether they use hand-aligned or auto-
matically aligned gold standards, (ii) the level at
which they count failures, e.g. sentence, align-
ment or translation unit level, (iii) whether they
interpret translation units disjunctively or conjunc-
tively, and (iv) whether they induce the lower
bounds (a) by running an all-accepting grammar
on the gold standard data, (b) by logical charac-
terization of the structures that can be induced by
a formalism, or (c) by counting the frequency of
complex alignment configurations. The advantage
of (a) and (b) is that they are guaranteed to find the
highest possible lower bound on the gold standard
data, whereas (c) is more modular (formalism-
independent) and actually tells us what configu-
rations cause trouble.
(i) In this study we use hand-aligned gold stan-
dard data. It should be obvious why this is prefer-
able to automatically aligned data. The only rea-
son that some previous studies used automatically
aligned data is that hand-aligned data are hard to
come by. This study uses the data also used by
S?gaard and Kuhn (2009), which to the best of
our knowledge uses the largest collection of hand-
aligned parallel corpora used in any of these stud-
ies. (ii) Failures are counted at the level of trans-
lation units as argued for in the above, but sup-
plemented by parse failure rates for completeness.
(iii) Since we count failures at the level of transla-
tion units, it is natural to interpret them conjunc-
tively. Otherwise we would in reality count fail-
ures at the level of alignments. (iv) We use (c).
The conjunctive interpretation of translation
units was also adopted by Fox (2002) and is mo-
tivated by the importance of translation units and
discontinuous ones in particular to machine trans-
lation in general (Simard and colleagues, 2005;
Ayan and Dorr, 2006; Macken, 2007; Shieber,
2007). In brief,
TUER = 1 ? 2|SU ?GU ||SU |+ |GU |
where GU are the translation units in the gold stan-
dard, and SU the translation units produced by
the system. This evaluation measure is related to
consistent phrase error rate (CPER) introduced in
Ayan and Dorr (2006), except that it does not only
consider contiguous phrases.
34
3.1 Data
The characteristics of the hand-aligned gold stan-
dard parallel corpora used are presented in Fig-
ure 1. The Danish-Spanish text is part of
the Copenhagen Dependency Treebank (Parole),
English-German is from Pado and Lapata (2006)
(Europarl), and the six combinations of English,
French, Portuguese and Spanish are documented
in Graca et al (2008) (Europarl).
3.2 Alignment configurations
The full class of ITGs induces many alignment
configurations that normal form ITGs do not in-
duce, incl. discontinuous translation units (DTUs),
i.e. translation units with at least one gap, double-
sided DTUs, i.e. DTUs with both a gap in the
source side and a gap in the target side, and multi-
gap DTUs with arbitrarily many gaps (as long as
the contents in the gap are either respect the linear
order of the source side or the inverted order).
ITGs do not induce (i) inside-out alignments,
(ii) cross-serial DTUs, (iii) what is called the ?bon-
bon? configuration below, and (iv) multigap DTUs
with mixed order in the target side. The reader is
referred to Wu (1997) for discussion of inside-out
alignments. (ii) and (iii) are explained below.
3.2.1 Induced configurations
DTUs are easily induced by unrestricted ITG pro-
ductions, while they cannot be induced by pro-
ductions in normal form. The combination of the
production rules A ? [?/ne B nothing/pas] and
B ? [change/modifie], for example, induces a
DTU with a gap in the French side for the pair of
substrings ?change nothing, ne modifie pas?.
Multigap DTUs with up to three gaps are fre-
quent (S?gaard and Kuhn, 2009) and have shown
to be important for translation quality (Simard and
colleagues, 2005). While normal form ITGs do
not induce multigap DTUs, ITGs induce a partic-
ular subclass of multigap DTUs, namely those that
are constructed by linear or inverse interpolation.
3.2.2 Non-induced configurations
Inside-out alignments were first described by
Wu (1997), and their frequency has been a mat-
ter of some debate (Lepage and Denoual, 2005;
Wellington et al, 2006; S?gaard and Kuhn, 2009).
Cross-serial DTUs are made of two DTUs non-
contiguous to the same side such that both have
material in the gap of each other. Bonbons are
similar, except the DTUs are non-contiguous to
different sides, i.e. D has a gap in the source side
that contains at least one token in E, and E has
a gap in the target side that contains at least one
token in D. Here?s an example of a bonbon con-
figuration from Simard et al (2005):
Pierre ne mange pas
Pierre does not eat
Multigap DTUs with mixed transfer are, as al-
ready mentioned multigap DTUs with crossing
alignments from material in two distinct gaps.
3.3 Results
The lower bounds on TUER for the full class of
ITGs are obtained by summing the ratios of inside-
out alignments, cross-serial DTUs, bonbons and
mixed order multigap DTUs, subtracting any over-
lap between these classes of configurations. The
lower bounds on TUER for normal form ITGs
sum ratios of inside-out aligments and DTUs sub-
tracting any overlap. Figure 1 presents the ratio
(?100), and Figure 2 presents the induced lower
bounds on the full class of ITGs and normal form
ITGs. Any two configurations differ on all trans-
lation units in order to count as two distinct con-
figurations in these statistics. Otherwise a single
translation unit could be removed to simplify two
or more configurations.
4 Discussion
The usefulness of alignment error rate (AER) (Och
and Ney, 2000) has been questioned lately (Fraser
and Marcu, 2007); most importantly, AER does
not always seem to correlate with translation qual-
ity. TUER is likely to correlate better with transla-
tion quality, since it by definition correlates with
CPER (Ayan and Dorr, 2006). No large-scale
experiment has been done yet to estimate the
strength of this correlation.
Our study also relies on the assumption that
simulatenously recognized words are aligned in
bilingual parsing. The relationship between pars-
ing and alignment can of course be complicated in
ways that will alter the alignment capacity of ITG
and its normal form; on some definitions the two
formalisms may even become equally expressive.
5 Conclusion
It was shown that the absolute reduction in average
lower bound on TUER is 2.48 for the full class of
ITGs over its canonical normal form. For PRF, it
is 16.01.
35
Snts TUs IOAs DTUs CDTUs Bonbons MIX-DTUs
Da-Sp 926 6441 0.56 9.16 0.81 0.16 0.23
En-Fr 100 869 0.23 2.99 0.12 0.23 0.23
En-Ge 987 17354 1.75 5.55 0.45 0.05 0.79
En-Po 100 783 0.26 2.17 0.00 0.00 0.38
En-Sp 100 831 0.48 1.32 0.00 0.00 0.36
Po-Fr 100 862 0.23 3.13 0.58 0.00 0.46
Po-Sp 100 882 0.11 0.90 0.00 0.00 0.00
Sp-Fr 100 914 0.11 2.95 0.55 0.00 0.22
Figure 1: Characteristics of the parallel corpora and frequency of configurations ( nTUs ? 100).
ITGs NF-ITGs
LB-TUER LB-PFR Ovlp(TUs) Ovlp(Snts) LB-TUER PFR Ovlp(TUs) Ovlp(Snts)
Da-Sp 1.58 10.37 11 10 8.54 40.50 76 32
En-Fr 0.69 6.00 1 1 2.88 22.00 3 2
En-Ge 2.75 47.32 49 42 5.24 69.30 357 236
En-Po 0.64 5.00 0 0 2.43 19.00 0 0
En-Sp 0.84 7.00 0 0 1.80 15.00 0 0
Po-Fr 1.04 9.00 2 2 3.36 24.00 0 0
Po-Sp 0.11 1.00 1 1 0.90 8.00 1 1
Sp-Fr 0.77 7.00 1 1 3.06 23.00 0 0
AV 1.05 11.59 3.53 27.60
Figure 2: Induced lower bounds for ITGs and normal form ITGs (NF-ITGs). LB-TUER lists the lower bounds on TUER.
LB-PFR lists the lower bounds on parse failure rates. Finally, the third and fourth columns list configuration overlaps at the
level of translation units, resp. sentences.
References
Alfred Aho and Jeffrey Ullman. 1972. The theory of parsing,
translation and compiling. Prentice-Hall.
Necip Ayan and Bonnie Dorr. 2006. Going beyond AER. In
COLING-ACL?06, Sydney, Australia.
Heidi Fox. 2002. Phrasal cohesion and statistical machine
translation. In EMNLP?02, Philadelphia, PA.
Alexander Fraser and Daniel Marcu. 2007. Measuring word
alignment quality for statistical machine translation. Com-
putational Linguistics, 33(3):293?303.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In ACL?04,
Barcelona, Spain.
Joao Graca, Joana Pardal, Lu??sa Coheur, and Diamantino
Caseiro. 2008. Building a golden collection of paral-
lel multi-language word alignments. In LREC?08, Mar-
rakech, Morocco.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
IWPT?05, pages 65?73, Vancouver, BC.
Yves Lepage and Etienne Denoual. 2005. Purest ever
example-based machine translation. Machine Translation,
19(3?4):251?282.
Lieve Macken. 2007. Analysis of translational correspon-
dence in view of sub-sentential alignment. In METIS-II,
pages 9?18, Leuven, Belgium.
Franz Och and Hermann Ney. 2000. A comparison of align-
ment models for statistical machine translation. In COL-
ING?00, Saarbru?cken, Germany.
Sebastian Pado? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic projec-
tion. In ACL-COLING?06, Sydney, Australia.
Giorgio Satta and Enoch Peserico. 2005. Some compu-
tational complexity results for synchronous context-free
grammars. In HLT-EMNLP?05, Vancouver, BC.
Stuart Shieber. 2007. Probabilistic synchronous tree-
adjoining grammars for machine translation. In SSST?07,
pages 88?95, Rochester, NY.
Michel Simard and colleagues. 2005. Translating with non-
contiguous phrases. In HLT-EMNLP?05, Vancouver, BC.
Anders S?gaard and Jonas Kuhn. 2009. Empirical lower
bounds on alignment error rates in syntax-based machine
translation. In NAACL-HLT?09, SSST-3, Boulder, CO.
Anders S?gaard. 2009. On the complexity of alignment
problems in two synchronous grammar formalisms. In
NAACL-HLT?09, SSST-3, Boulder, CO.
Benjamin Wellington, Sonjia Waxmonsky, and Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translational equivalence. In ACL?06, pages
977?984, Sydney, Australia.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative study
on reordering constraints in statistical machine translation.
In ACL?03, Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2004. Syntax-based align-
ment: supervised or unsupervised? In COLING?04, pages
418?424, Geneva, Switzerland.
36
Why Nitpicking Works: Evidence for Occam?s Razor in Error Correctors
Dekai WU?1 Grace NGAI?2 Marine CARPUAT?
dekai@cs.ust.hk csgngai@polyu.edu.hk marine@cs.ust.hk
? Human Language Technology Center
HKUST
Department of Computer Science
University of Science and Technology
Clear Water Bay, Hong Kong
? Hong Kong Polytechnic University
Department of Computing
Kowloon
Hong Kong
Abstract
Empirical experience and observations have shown us when
powerful and highly tunable classifiers such as maximum en-
tropy classifiers, boosting and SVMs are applied to language
processing tasks, it is possible to achieve high accuracies, but
eventually their performances all tend to plateau out at around
the same point. To further improve performance, various error
correction mechanisms have been developed, but in practice,
most of them cannot be relied on to predictably improve per-
formance on unseen data; indeed, depending upon the test set,
they are as likely to degrade accuracy as to improve it. This
problem is especially severe if the base classifier has already
been finely tuned.
In recent work, we introduced N-fold Templated Piped Cor-
rection, or NTPC (?nitpick?), an intriguing error corrector that
is designed to work in these extreme operating conditions. De-
spite its simplicity, it consistently and robustly improves the ac-
curacy of existing highly accurate base models. This paper in-
vestigates some of the more surprising claims made by NTPC,
and presents experiments supporting an Occam?s Razor argu-
ment that more complex models are damaging or unnecessary
in practice.
1 Introduction
The investigation we describe here arose from a very
commonly discussed experience, apparently triggered by
the recent popularity of shared task evaluations that have
opened opportunities for researchers to informally com-
pare their experiences ?with a common denominator?, so
to speak.
Among the perennial observations which are made
during the analysis of the results is that (1) methods de-
signed to ?fine-tune? the high-accuracy base classifiers
behave unpredictably, their success or failure often ap-
pearing far more sensitive to where the test set was drawn
from, rather than on any true quality of the ?fine-tuning?,
and consequently, (2) the resulting system rankings are
often unpredictable, especially as they are typically con-
ducted only on a single new test set, often drawn from
a single arbitrary new source of a significantly different
nature than the training sets. One could argue that such
evaluations do not constitute a fair test, but in fact, this is
1The author would like to thank the Hong Kong Research Grants
Council (RGC) for supporting this research in part through research
grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09.
2The author would like to thank the Hong Kong Polytechnic Univer-
sity for supporting this research in part through research grants A-PE37
and 4-Z03S.
where computational linguistics modeling diverges from
machine learning theory, since for any serious NLP ap-
plication, such evaluations constitute a much more accu-
rate representation of the real world.
We believe one primary reason for this common ex-
perience is that the models involved are typically al-
ready operating well beyond the limits of accuracy of
the models? assumptions about the nature of distributions
from which testing samples will be drawn. For this rea-
son, even ?sophisticated? discriminative training crite-
ria, such as maximum entropy, minimum error rate, and
minimum Bayes risk, are susceptible to these stability
problems. There has been much theoretical work done
on error correction, but in practice, any error correction
usually lowers the performance of the combined system
on unseen data, rather than improving it. Unfortunately,
most existing theory simply does not apply.
This is especially true if the base model has been
highly tuned. For the majority of tasks, the performance
of the trained models, after much fine tuning, tend to
plateau out at around the same point, regardless of the
theoretical basis of the underlying model. This holds
true with most highly accurate classifiers, including max-
imum entropy classifiers, SVMs, and boosting models.
In addition, even though data analysis gives us some gen-
eral idea as to what kinds of feature conjunctions might
help, the classifiers are not able to incorporate those
into their model (usually because the computational cost
would be infeasible), and any further post-processing
tends to degrade accuracy on unseen data. The common
practice of further improving accuracy at this point is to
resort to ad hoc classifier combination methods, which
are usually not theoretically well justified and, again, un-
predictably improve or degrade performance?thus con-
suming vast amounts of experimental resources with rel-
atively low expected payoff, much like a lottery.
There are a variety of reasons for this, ranging from
the aforementioned validity of the assumptions about the
distribution between the training and test corpora, to the
absence of a well justified stopping point for error cor-
rection. The latter problem is much more serious than it
seems at first blush, since without a well-justified stop-
ping criterion, the performance of the combined model
will be much more dependent upon the distribution of
the test set, than on any feature engineering. Empirical
evidence for this argument can be seen from the result of
the CoNLL shared tasks (Tjong Kim Sang, 2002)(Tjong
Kim Sang and Meulder, 2003), where the ranking of the
participating systems changes with the test corpora.
Inspired by the repeated observations of this phe-
nomenon by many participants, we decided to stop
?sweeping the issue under the rug?, and undertook to
confront it head-on. Accordingly, we challenged our-
selves to design an error corrector satisfying the follow-
ing criteria, which few if any existing models actually
meet: (1) it would leverage off existing base models,
while targeting their errors; (2) it would consistently im-
prove accuracy, even on top of base models that already
deliver high accuracy; (3) it would be robust and con-
servative, so as to almost never accidentally degrade ac-
curacy; (4) it would be broadly applicable to any classi-
fication or recognition task, especially high-dimensional
ones such as named-entity recognition and word-sense
disambiguation; and (5) it would be template-driven and
easily customizable, which would enable it to target er-
ror patterns beyond the base models? representation and
computational complexity limitations.
Our goal in this undertaking was to invent as little as
possible. We expected to make use of relatively sophis-
ticated error-minimization techniques. Thus the results
were surprising: the simplest models kept outperform-
ing the ?sophisticated? models. This paper attempts to
investigate some of the key reasons why.
To avoid reinventing the wheel, we originally
considered adapting an existing error-driven method,
transformation-based learning (TBL) for this purpose.
TBL seems well suited to the problem as it is inherently
an error corrector and, on its own, has been shown to
achieve high accuracies on a variety of problems (see
Section 4). Our original goal was to adapt TBL for
error correction of high-performing models (Wu et al,
2004a), with two main principles: (1) since it is not clear
that the usual assumptions made about the distribution
of the training/test data are valid in such extreme oper-
ating ranges, empirical observations would take prece-
dence over theoretical models, which implies that (2) any
model would have to be empirically justified by testing
on a diverse range of data. Experimental observations,
however, increasingly drove us toward different goals.
Our resulting error corrector, NTPC, was instead con-
structed on the principle of making as few assumptions as
possible in order to robustly generalize over diverse situ-
ations and problems. One observation made in the course
of experimentation, after many attempts at fine-tuning
model parameters, was that many of the complex theo-
retical models for error correction often do not perform
consistently. This is perhaps not too surprising upon fur-
ther reflection, since the principle of Occam?s Razor does
prefer simpler hypotheses over more complex ones.
NTPC was introduced in (Wu et al, 2004b), where the
controversial issues it raised generated a number of in-
teresting questions, many of which were were directed at
NTPC?s seeming simplicity, which seems in opposition
to the theory behind many other error correcting models.
In this paper, we investigate the most commonly-asked
questions. We illuminate these questions by contrasting
NTPC against the more powerful TBL, presenting ex-
periments that show that NTPC?s simple model is indeed
Figure 1: Piped architecture with n-fold partitioning.
key to its robustness and reliability.
The rest of the paper is laid out as follows: Section 2
presents an introduction to NTPC, including an overview
of its architecture. Section 3 addresses key questions re-
lated to NTPC?s architecture and presents empirical re-
sults justifying its simplicity.
2 N-fold Templated Piped Correction
N-fold Templated Piped Correction or NTPC, is a
model that is designed to robustly improve the accuracy
of existing base models in a diverse range of operating
conditions. As was described above, the most challeng-
ing situations for any error corrector is when the base
model has been finely tuned and the performance has
reached a plateau. Most of the time, any further feature
engineering or error correction after that point will end
up hurting performance rather than improving it.
2.1 The architecture of NTPC is surprisingly
simple
One of the most surprising things about NTPC lies in
the fact that despite its simplicity, it outperforms math-
ematically much more ?sophisticated? methods at error
correcting. Architecturally, it relies on a simple rule-
learning mechanism and cross-partitioning of the train-
ing data to learn very conservative, cautious rules that
make only a few corrections at a time.
Figure 1 illustrates the NTPC architecture. Prior to
learning, NTPC is given (1) a set of rule templates which
describe the types of rules that it is allowed to hypothe-
size, (2) a single base learning model, and (3) an anno-
tated training set.
The NTPC architecture is essentially a sequen-
tially chained piped ensemble that incorporates cross-
validation style n-fold partition sets generated from the
base model. The training set is partitioned n times in
order to train n base models. Subsequently the n held-
out validation sets are classified by the respective trained
base models, with the results combined into a ?reconsti-
tuted? training set. The reconstituted training set is used
by Error Corrector Learner, which learns a set of rules.
Rule hypotheses are generated according to the given set
of allowable templates:
R = {r| r ? H ? ? (r) > ?min ?  (r) = 0} (1)
?(r) =
?X
j=1
?
r(xj ,y?j) 6=?
?(r(xj , y?j), yj) (2)
(r) =
?X
j=1
?
r(xj ,y?j) 6=?
1? ?(r(xj , y?j), yj)(3)
where X is a sequence of X training examples xi, Y
is a sequence of reference labels yi for each example
respectively, Y? is a sequence of labels y?i as predicted
by the base model for each example respectively, H is
the hypothesis space of valid rules implied by the tem-
plates, and ?min is a confidence threshold. Setting ?min
to a relatively high value (say 15) implements the re-
quirement of high reliability. R is subsequently sorted
by the ?i value of each rule ri into an ordered list of rules
R? = (r?0, . . . , r
?
i?1).
During the evaluation phase, depicted in the lower por-
tion of Figure 1, the test set is first labeled by the base
model. The error corrector?s rules r?i are then applied in
the order of R? to the evaluation set. The final classifica-
tion of a sample is then the classification attained when
all the rules have been applied.
2.2 NTPC consistently and robustly improves
accuracy of highly-accurate base models
In previous work (Wu et al, 2004b), we presented ex-
periments on named-entity identification and classifica-
tion across four diverse languages, using Adaboost.MH
as the base learner, which showed that NTPC was ca-
pable of robustly and consistently improving upon the
accuracy of the already-highly-accurate boosting model;
correcting the errors committed by the base model but
not introducing any of its own.
Table 1 compares results obtained with the base Ad-
aboost.MH model (Schapire and Singer, 2000) and the
NTPC-enhanced model for a total of eight different
named-entity recognition (NER) models. These experi-
ments were performed on the CoNLL-2002 and CoNLL-
2003 shared task data sets. It can be seen that the Ad-
aboost.MH base models clearly already achieve high ac-
curacy, setting the bar very high for NTPC to improve
upon. However, it can also be seen that NTPC yields fur-
ther F-Measure gains on every combination of task and
language, including English NE bracketing (Model M2)
for which the base F-Measure is the highest.
An examination of the rules (shown in the Appendix)
can give an idea as to why NTPC manages to identify
and correct errors which were overlooked by the highly
tuned base model. NTPC?s advantage comes from two
aspects: (1) its ability to handle complex conjunctions
of features, which often reflect structured, linguistically
motivated expectations, in the form of rule templates;
and (2) its ability to ?look forward? at classifications
from the right context, even when processing the sen-
tence in a left-to-right direction. The base classifier is
unable to incorporate these two aspects, because (1) in-
cluding complex conjunctions of features would raise the
computational cost of searching the feature space to a
point where it would be infeasible, and (2) most classi-
fiers process a sentence from left-to-right, deciding on
the class label for each word before moving on to the
next one. Rules that exploit these advantages are eas-
ily picked out in the table; many of the rules (especially
those in the top 5 for both English and Spanish) consist of
complex conjunctions of features; and rules that consider
the right context classifications can be identified by the
string ?ne <num>?, where <num> is a positive integer
(indicating how many words to the right).
3 Experiments
The most commonly-raised issues about NTPC relate
to the differences between NTPC and TBL (though the
conceptual issues are much the same as for other error-
minimization criteria, such as minimum error rate or
minimum Bayes risk). This is expected, since it was
one of our goals to reinvent as little as possible. As
a result, NTPC does bear a superficial resemblance to
TBL, both of them being error-driven learning methods
that seek to incrementally correct errors in a corpus by
learning rules that are determined by a set of templates.
One of the most frequently asked questions is whether
the Error Corrector Learner portion of NTPC could be
replaced by a transformation-based learner. This section
will investigate the differences between NTPC and TBL,
and show the necessity of the changes that were incorpo-
rated into NTPC.
The experiments run in this section were performed
on the data sets used in the CoNLL-2002 and CoNLL-
2003 Named Entity Recognition shared tasks. The
high-performing base model is based on AdaBoost.MH
(Schapire and Singer, 2000), the multi-class generaliza-
tion of the original boosting algorithm, which imple-
ments boosting on top of decision stump classifiers (de-
cision trees of depth one).
3.1 Any Error is Bad
The first main difference between NTPC and TBL, and
also what seems to be an extreme design decision on
the part of NTPC, is the objective scoring function. To
be maximally certain of not introducing any new errors
with its rules, the first requirement that NTPC?s objective
function places onto any candidate rules is that they must
not introduce any new errors ( (r) = 0). This is called
the zero error tolerance principle.
To those who are used to learners such as
transformation-based learning and decision lists, which
allow for some degree of error tolerance, this design prin-
ciple seems overly harsh and inflexible. Indeed, for al-
Table 1: NTPC consistently yields improvements on all eight different high-accuracy NER base models, across every
combination of task and language.
Model Task Language Model Precision Recall F-Measure1
M1 Bracketing Dutch Base 87.27 91.48 89.33
Base w/ NTPC 87.44 92.04 89.68
M2 Bracketing English Base 95.01 93.98 94.49
Base w/ NTPC 95.23 94.05 94.64
M3 Bracketing German Base 83.44 65.86 73.62
Base w/ NTPC 83.43 65.91 73.64
M4 Bracketing Spanish Base 89.46 87.57 88.50
Base w/ NTPC 89.77 88.07 88.91
M5 Classification + Bracketing Dutch Base 70.26 73.64 71.91
Base w/ NTPC 70.27 73.97 72.07
M6 Classification + Bracketing English Base 88.64 87.68 88.16
Base w/ NTPC 88.93 87.83 88.37
M7 Classification + Bracketing German Base 75.20 59.35 66.34
Base w/ NTPC 75.19 59.41 66.37
M8 Classification + Bracketing Spanish Base 74.11 72.54 73.32
Base w/ NTPC 74.43 73.02 73.72
most all models, there is an implicit assumption that the
scoring function will be based on the difference between
the positive and negative applications, rather than on an
absolute number of corrections or mistakes.
Results for eight experiments are shown in Figures 2
and 3. Each experiment compares NTPC against other
variants that allow relaxed  (r) ? max conditions for
various max ? {1, 2, 3, 4,?}. The worst curve in each
case is for max = ?? in other words, the system that
only considers net performance improvement, as TBL
and many other rule-based models do. The results con-
firm empirically that the  (r) = 0 condition (1) gives the
most consistent results, and (2) generally yields accura-
cies among the highest, regardless of how long training is
allowed to continue. In other words, the presence of any
negative application during the training phase will cause
the error corrector to behave unpredictably, and the more
complex model of greater error tolerance is unnecessary
in practice.
3.2 Rule Interaction is Unreliable
Another key difference between NTPC and TBL is the
process of rule interaction. Since TBL allows a rule to
use the current classification of a sample and its neigh-
bours as features, and a rule updates the current state of
the corpus when it applies to a sample, the application
of one rule could end up changing the applicability (or
not) of another rule. From the point of view of a sam-
ple, its classification could depend on the classification
of ?nearby? samples. Typically, these ?nearby? samples
are those found in the immediately preceding or succeed-
ing words of the same sentence. This rule interaction is
permitted in both training and testing.
NTPC, however, does not allow for this kind of rule in-
teraction. Rule applications only update the output clas-
sification of a sample, and do not update the current state
of the corpus. In other words, the feature values for a
Figure 2: NTPC?s zero tolerance condition yields less
fluctuation and generally higher accuracy than the re-
laxed tolerance variations, in bracketing experiments.
(bold = NTPC, dashed = relaxed tolerance)
sample are initialized once, at the beginning of the pro-
gram, and not changed again thereafter. The rationale for
making this decision is the hypothesis that rule interac-
tion is in nature unreliable, since the high-accuracy base
model provides sparse opportunities for rule application
and thus much sparser opportunities for rule interaction,
making any rule that relies on rule interaction suspect.
As a matter of fact, by considering only rules that make
no mistake during the learning phase, NTPC?s zero error
tolerance already eliminates any correction of labels that
results from rule interaction?since a label correction on
a sample that results from the application of more than
one rule necessarily implies that at least one of the rules
made a mistake.
Since TBL is a widely used error-correcting method,
Figure 3: NTPC?s zero tolerance condition yields less
fluctuation and generally higher accuracy than the re-
laxed tolerance variations, in bracketing + classification
experiments. (bold = NTPC, dashed = relaxed tolerance)
Figure 4: Unpredictable fluctuations on the bracket-
ing task show that allowing TBL-style rule interaction
does not yield reliable improvement over NTPC. (bold =
NTPC, dashed = rule interaction)
Figure 5: Unpredictable fluctuations on the bracket-
ing + classification task show that allowing TBL-style
rule interaction does not yield reliable improvement over
NTPC. (bold = NTPC, dashed = rule interaction)
it is natural to speculate that NTPC?s omission of rule in-
teraction is a weakness. In order to test this question, we
implemented an iterative variation of NTPC that allows
rule interaction, where each iteration targets the residual
error from previous iterations as follows:
1. i? 0,X0 ? X
2. r?i ? null, s?i ? 0
3. foreach r ? H such that i (r) = 0
? if ?i (r) > ??i then r?i ? r, ??i ? ?i (r)
4. if ??i < ?min then return
5. Xi+1 ? result of applying r?i to Xi
6. i? i + 1
7. goto Step 3
where
?i(r) =
?X
j=1
?
r(xij ,y?j) 6=?
?(r(xij , y?j), yj)
i(r) =
?X
j=1
?
r(xij ,y?j) 6=?
1? ?(r(xij , y?j), yj)
Here, incremental rule interaction is a natural conse-
quence of arranging the structure of the algorithm to ob-
serve the right context features coming from the base
model, as with transformation-based learning. In Step
5 of the algorithm, the current state of the corpus is up-
dated with the latest rule on each iteration. That is, in
each given iteration of the outer loop, the learner consid-
ers the corrected training data obtained by applying rules
learned in the previous iterations, so the learner has ac-
cess to the labels that result from applying the previous
rules. Since these rules may apply anywhere in the cor-
pus, the learner is not restricted to using only labels from
the left context.
The time complexity of this variation is an order of
magnitude more expensive than NTPC, due to the need
to allow rule interaction using nested loops. The ordered
list of output rules r?0 , . . . , r?i?1is learned in a greedy
fashion, to progressively improve upon the performance
of the learning algorithm on the training set.
Results for eight experiments on this variation, shown
in Figures 4 and 5, demonstrate that this expensive extra
capability is rarely useful in practice and does not reli-
ably guarantee that accuracy will not be degraded. This
is yet another illustration of the principle that, in high-
accuracy error correction problems, at least, more simple
modes of operation should be preferred over more com-
plex arrangements.
3.3 NTPC vs. N-fold TBL
Another question on NTPC that is frequently raised is
whether or not ordinary TBL, which is after all, intrinsi-
cally an error-correcting model, can be used in place of
NTPC to perform better error correction. Figure 6 shows
the results of four sets of experiments evaluating this ap-
proach on top of boosting. As might be expected from
extrapolation from the foregoing experiments that inves-
tigated their individual differences, NTPC outperforms
the more complex TBL in all cases, regardless of how
long training is allowed to continue.
Figure 6: NTPC consistently outperforms error correc-
tion using TBL even when n-fold partitioning is used.
(bold = NTPC, dashed = TBL with n-fold partitioning)
Table 2: The more complex partition-based voted er-
ror corrector degrades performance, while NTPC helps
(bracketing + classification, English).
Model Precision Recall F-Measure1
Base 95.01 93.98 94.49
Partition-Based
Voting
95.07 93.79 94.43
Base w/ NTPC 95.14 94.05 94.59
3.4 NTPC vs. Partition-Based Voting
Another valid question would be to ask if the way that
NTPC combines the results of the n-fold partitioning is
oversimplistic and could be improved upon. As was pre-
viously stated, the training corpus for the error correc-
tor in NTPC is the ?reconstituted training set? gener-
ated by combining the held-out validation sets after they
have labeled with initial classifications by their respec-
tive trained base models. To investigate if NTPC could
benefit from a more complex model, we employed vot-
ing, a commonly-used technique in machine learning and
natural language processing. As before, the training set
was partitioned and multiple base learners were trained
and evaluated on the multiple training and validation sets,
respectively. However, instead of recombining the vali-
dation sets into a reconstituted training set, multiple er-
ror corrector models were trained on the n partition sets.
During the evaluation phase, all n error correctors were
evaluated on the evaluation set after it had been labeled
by the base model, and they voted on the final output.
Table 2 shows the results of using such an approach
for the bracketing + classification task on English. The
empirical results clearly show that the more complex and
time-consuming voting model not only does not outper-
fom NTPC, but in fact again degrades the performance
from the base boosting-only model.
3.5 Experiment Summary
In our experiments, we set out to investigate whether
NTPC?s operating parameters were overly simple, and
whether more complex arrangements were necessary or
desirable. However, empirical evidence points to the fact
that, in this problem of error correction in high accuracy
ranges, at least, simple mechanisms will suffice to pro-
duce good results?in fact, the more complex operations
end up degrading rather than improving accuracy.
A valid question is to ask why methods such
as decision list learning (Rivest, 1987) as well as
transformation-based learning benefit from these more
complex mechanisms. Though structurally similar to
NTPC, these models operate in a very different environ-
ment, where many initially poorly labeled examples are
available to drive rule learning with. Hence, it is pos-
sibly advantageous to trade off some corrections with
some mistakes, provided that there is an overall posi-
tive change in accuracy. However, in an error-correcting
situation, most of the samples are already correctly la-
beled, errors are few and far in between and the sparse
data problem is exacerbated. In addition, the idea of er-
ror correction implies that we should, at the very least,
not do any worse than the original algorithm, and hence
it makes sense to err on the side of caution and minimize
any errors created, rather than hoping that a later rule ap-
plication will undo mistakes made by an earlier one.
Finally, note that the same point applies to many other
models where training criteria like minimum error rate
are used, since such criteria are functions of the trade-
off between correctly and incorrectly labeled examples,
without zero error tolerance to compensate for the sparse
data problem.
4 Previous Work
4.1 Boosting and NER
Boosting (Freund and Schapire, 1997) has been success-
fully applied to several NLP problems. In these NLP
systems boosting is typically used as the ultimate stage
in a learned system. For example, Shapire and Singer
(2000) applied it to Text Categorization while Escud-
ero et al(2000) used it to obtain good results on Word
Sense Disambiguation. More closely relevant to the ex-
periments described here in, two of the best-performing
three teams in the CoNLL-2002 Named Entity Recog-
nition shared task evaluation used boosting as their base
system (Carreras et al, 2002)(Wu et al, 2002).
However, precedents for improving performance af-
ter boosting are few. At the CoNLL-2002 shared task
session, Tjong Kim Sang (unpublished) described an ex-
periment using voting to combine the NER outputs from
the shared task participants which, predictably, produced
better results than the individual systems. A couple of
the individual systems were boosting models, so in some
sense this could be regarded as an example.
Tsukamoto et al(2002) used piped AdaBoost.MH
models for NER. Their experimental results were some-
what disappointing, but this could perhaps be attributable
to various reasons including the feature engineering or
not using cross-validation sampling in the stacking.
Appendix
The following examples show the top 10 rules learned for English and Spanish on the bracketing + classification task.
(Models M6 and M8)
English
ne -2=ZZZ ne -1=ZZZ word:[1,3]=21 nonnevocab 0=inNonNeVocab nevocab 0=inNeVocab captype 0=firstword-firstupper => ne=I-ORG
ne 1=O ne 2=O word -1=ZZZ nonnevocab 0=inNonNeVocab nevocab 0=not-inNeVocab captype 0=firstword-firstupper => ne=O
captype 0=notfirstword-firstupper captype -1=firstword-firstupper captype 1=number nonnevocab 0=inNonNeVocab nevocab 0=inNeVocab ne 0=I-LOC => ne=I-
ORG
ne -1=ZZZ ne 0=I-ORG word 1=, nonnevocab 0=not-inNonNeVocab nevocab 0=not-inNeVocab captype 0=allupper => ne=I-LOC
ne 0=I-PER word:[1,3]=0 nonnevocab 0=not-inNonNeVocab nevocab 0=not-inNeVocab captype 0=notfirstword-firstupper => ne=I-ORG
ne 0=I-ORG ne 1=O ne 2=O nonnevocab 0=inNonNeVocab nevocab 0=not-inNeVocab captype 0=alllower => ne=O
ne 0=I-PER ne 1=I-ORG => ne=I-ORG
ne -1=ZZZ ne 0=I-PER word:[-3,-1]=ZZZ word:[1,3]=1 => ne=I-ORG
ne 0=I-ORG word:[-3,-1]=spd => ne=B-ORG
ne -1=I-ORG ne 0=I-PER word:[1,3]=1 => ne=I-ORG
Spanish
wcaptype 0=alllower ne -1=I-ORG ne 0=I-ORG ne 1=O => ne=O
captypeLex -1=inLex captypeGaz -1=not-inGaz wcaptype -1=alllower ne -1=O ne 0=O captypeLex 0=not-inLex captypeGaz 0=not-inGaz wcaptype 0=noneed-
firstupper => ne=I-ORG
wcaptype 0=noneed-firstupper wcaptype -1=noneed-firstupper wcaptype 1=alllower captypeLex 0=not-inLex captypeGaz 0=not-inGaz ne 0=O => ne=I-ORG
ne 0=O word 0=efe => ne=I-ORG
ne -1=O ne 0=O word 1=Num word 2=. captypeLex 0=not-inLex captypeGaz 0=not-inGaz wcaptype 0=allupper => ne=I-MISC
pos -1=ART pos 0=NCF wcaptype 0=noneed-firstupper ne -1=O ne 0=O => ne=I-ORG
wcaptype 0=alllower ne 0=I-PER ne 1=O ne 2=O => ne=O
ne 0=O ne 1=I-MISC word 2=Num captypeLex 0=not-inLex captypeGaz 0=not-inGaz wcaptype 0=allupper => ne=I-MISC
ne 0=I-LOC word:[-3,-1]=universidad => ne=I-ORG
ne 1=O ne 2=O word 0=de captypeLex 0=not-inLex captypeGaz 0=inGaz wcaptype 0=alllower => ne=O
The AdaBoost.MH base model?s high accuracy sets
a high bar for error correction. Aside from brute-force
en masse voting of the sort at CoNLL-2002 described
above, we do not know of any existing post-boosting
models that improve rather than degrade accuracy. We
aim to further improve performance, and propose using
a piped error corrector.
4.2 Transformation-based Learning
Transformation-based learning (Brill, 1995), or TBL, is
one of the most successful rule-based machine learning
algorithms. The central idea of TBL is to learn an or-
dered list of rules, each of which evaluates on the re-
sults of those preceding it. An initial assignment is made
based on simple statistics, and then rules are greedily
learned to correct the mistakes, until no net improvement
can be made.
Transformation-based learning has been used to tackle
a wide range of NLP problems, ranging from part-of-
speech tagging (Brill, 1995) to parsing (Brill, 1996) to
segmentation and message understanding (Day et al,
1997). In general, it achieves state-of-the-art perfor-
mances and is fairly resistant to overtraining.
5 Conclusion
We have investigated frequently raised questions about
N-fold Templated Piped Correction (NTPC), a general-
purpose, conservative error correcting model, which has
been shown to reliably deliver small but consistent gains
on the accuracy of even high-performing base models
on high-dimensional NLP tasks, with little risk of acci-
dental degradation. Experimental evidence shows that
when error-correcting high-accuracy base models, sim-
ple models and hypotheses are more beneficial than com-
plex ones, while the more complex and powerful models
are surprisingly unreliable or damaging in practice.
References
Eric Brill. Transformation-based error-driven learning and natural language pro-
cessing: A case study in part of speech tagging. Computational Linguistics,
21(4):543?565, 1995.
Eric Brill. Recent Advances in Parsing Technology, chapter Learning to Parse
with Transformations. Kluwer, 1996.
Xavier Carreras, Llu??s Ma`rques, and Llu??s Padro?. Named entity extraction using
adaboost. In Dan Roth and Antal van den Bosch, editors, Proceedings of
CoNLL-2002, pages 167?170. Taipei, Taiwan, 2002.
David Day, John Aberdeen, Lynette Hirshman, Robyn Kozierok, Patricia Robin-
son, and Marc Vilain. Mixed initiative development of language processing
systems. In Proceedings of the Fifth Conference on Applied Natural Language
Processing, Washington, D.C., March 1997. Association of Computational
Linguistics.
Gerard Escudero, Lluis Marquez, and German Rigau. Boosting applied to word
sense disambiguation. In European Conference on Machine Learning, pages
129?141, 2000.
Yoram Freund and Robert E. Schapire. A decision-theoretic generalization of
on-line learning and an application to boosting. In Journal of Computer and
System Sciences, 55(1), pages 119?139, 1997.
Ronald L. Rivest. Learning decision lists. Machine Learning, 2(3):229?246,
1987.
Robert E. Schapire and Yoram Singer. Boostexter: A boosting-based system for
text categorization. Machine Learning, 2(3):135?168, 2000.
Erik Tjong Kim Sang and Fien Meulder. Introduction to the conll-2003 shared
task: Language-independent named entity recognition. In Walter Daelemans
and Miles Osborne, editors, Proceedings of CoNLL-2003. Edmonton, Canada,
2003.
Erik Tjong Kim Sang. Introduction to the conll-2002 shared task: Language-
independent named entity recognition. In Dan Roth and Antal van den Bosch,
editors, Proceedings of CoNLL-2002, pages 155?158. Taipei, Taiwan, 2002.
Koji Tsukamoto, Yutaka Mitsuishi, and Manabu Sassano. Learning with multiple
stacking for named entity recognition. In Dan Roth and Antal van den Bosch,
editors, Proceedings of CoNLL-2002, pages 191?194. Taipei, Taiwan, 2002.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe Larsen, and Yongsheng Yang.
Boosting for named entity recognition. In Dan Roth and Antal van den Bosch,
editors, Proceedings of CoNLL-2002, pages 195?198. Taipei, Taiwan, 2002.
Dekai Wu, Grace Ngai, and Marine Carpuat. N-fold templated piped correc-
tion. In First International Joint Conference on Natural Language Processing
(IJCNLP-2004), pages 632?637. Hainan Island, China, March 2004.
Dekai Wu, Grace Ngai, and Marine Carpuat. Raising the bar: Stacked conserva-
tive error correction beyond boosting. In Fourth International Conference on
Language Resources and Evaluation (LREC-2004). Lisbon, May 2004.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 102?112,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning to Freestyle: Hip Hop Challenge-Response Induction via
Transduction Rule Segmentation
Dekai Wu Karteek Addanki Markus Saers Meriem Beloucif
Human Language Technology Center
Department of Computer Science
HKUST, Clear Water Bay, Hong Kong
{dekai|vskaddanki|masaers|mbeloucif}@cs.ust.hk
Abstract
We present a novel model, Freestyle, that
learns to improvise rhyming and fluent re-
sponses upon being challenged with a line of
hip hop lyrics, by combining both bottom-
up token based rule induction and top-down
rule segmentation strategies to learn a stochas-
tic transduction grammar that simultaneously
learns both phrasing and rhyming associations.
In this attack on the woefully under-explored
natural language genre of music lyrics, we
exploit a strictly unsupervised transduction
grammar induction approach. Our task is par-
ticularly ambitious in that no use of any a pri-
ori linguistic or phonetic information is al-
lowed, even though the domain of hip hop
lyrics is particularly noisy and unstructured.
We evaluate the performance of the learned
model against a model learned only using
the more conventional bottom-up token based
rule induction, and demonstrate the superi-
ority of our combined token based and rule
segmentation induction method toward gen-
erating higher quality improvised responses,
measured on fluency and rhyming criteria as
judged by human evaluators. To highlight
some of the inherent challenges in adapting
other algorithms to this novel task, we also
compare the quality of the responses generated
by our model to those generated by an out-of-
the-box phrase based SMT system. We tackle
the challenge of selecting appropriate training
data for our task via a dedicated rhyme scheme
detection module, which is also acquired via
unsupervised learning and report improved
quality of the generated responses. Finally,
we report results with Maghrebi French hip
hop lyrics indicating that our model performs
surprisingly well with no special adaptation to
other languages.
1 Introduction
The genre of lyrics in music has been severely under-
studied from the perspective of computational lin-
guistics despite being a form of language that has
perhaps had the most impact across almost all human
cultures. With the motivation of spurring further re-
search in this genre, we apply stochastic transduc-
tion grammar induction algorithms to address some
of the modeling issues in song lyrics. An ideal start-
ing point for this investigation is hip hop, a genre
that emphasizes rapping, spoken or chanted rhyming
lyrics against strong beats or simple melodies. Hip
hop lyrics, in contrast to poetry and other genres of
music, present a significant number of challenges for
learning as it lacks well-defined structure in terms of
rhyme scheme, meter, or overall meaning making it
an interesting genre to bring to light some of the less
studied modeling issues.
The domain of hip hop lyrics is particularly un-
structured when compared to classical poetry, a do-
main on which statistical methods have been applied
in the past. Hip hop lyrics are unstructured in the
sense that a very high degree of variation is permit-
ted in the meter of the lyrics, and large amounts of
colloquial vocabulary and slang from the subculture
are employed. The variance in the permitted me-
ter makes it hard to make any assumptions about
the stress patterns of verses in order to identify the
rhyming words used when generating output. The
broad range of unorthodox vocabulary used in hip
hop make it difficult to use off-the-shelf NLP tools
for doing phonological and/or morphological analy-
sis. These problems are further exacerbated by dif-
ferences in intonation of the same word and lack of
robust transcription (Liberman, 2010).
102
We argue that stochastic transduction grammars,1
given their success in the area of machine transla-
tion and efficient unsupervised learning algorithms,
are ideal for capturing the structural relationship be-
tween lyrics. Hence, our Freestyle system mod-
els the problem of improvising a rhyming response
given any hip hop lyric challenge as transducing
a challenge line into a rhyming response. We
use a stochastic transduction grammar induced in
a completely unsupervised fashion using a combi-
nation of token based rule induction and segment-
ing (Saers et al, 2013) as the underlying model to
fully-automatically learn a challenge-response sys-
tem and compare its performance against a simpler
token based transduction grammar model. Both our
models are completely unsupervised and use no prior
phonetic or linguistic knowledge whatsoever despite
the highly unstructured and noisy domain.
We believe that the challenge-response system
based on an interpolated combination of token based
rule induction and rule segmenting transduction
grammars will generate more fluent and rhyming re-
sponses compared to one based on token based trans-
duction grammars models. This is based on the ob-
servation that token based transduction grammars
suffer from a lack of fluency; a consequence of the
degree of expressivity they permit. Therefore, as a
principal part of our investigation we compare the
quality of responses generated using a combination
of token based rule induction and top-down rule seg-
menting transduction grammars to those generated
by pure token based transduction grammars.
We also hypothesize that in order to generate flu-
ent and rhyming responses, it is not sufficient to train
the transduction grammars on all adjacent lines of a
hip hop verse. Therefore, we propose a data selec-
tion scheme using a rhyme scheme detector acquired
through unsupervised learning to generate the train-
ing data for the challenge-response systems. The
rhyme scheme detector segments each verse of a hip
hop song into stanzas and identifies the lines in each
stanza that rhyme with each other which are then
added as training instances. We demonstrate the su-
periority of our training data selection method by
comparing the quality of the responses generated by
the models trained on data selected with and without
1Also known in SMT as ?synchronous grammars?.
using the rhyme scheme detector.
Unlike conventional spoken and written language,
disfluencies and backing vocals2 occur very fre-
quently in the domain of hip hop lyrics which af-
fect the performance of NLP models designed for
processing well-formed sentences. We propose two
strategies to mitigate the effect of disfluencies on our
model performance and compare their efficacy using
human evaluations. Finally, in order to illustrate the
challenges faced by other NLP algorithms, we con-
trast the performance of our model against a conven-
tional, widely used phrase-based SMT model.
A brief terminological note: ?stanza? and ?verse?
are frequently confused and sometimes conflated.
Worse yet, their usage for song lyrics is often con-
tradictory to that for poetry. To avoid ambiguity
we consistently follow these technical definitions for
segments in decreasing size of granularity:
verse a large unit of a song?s lyrics. A song typi-
cally contains several verses interspersed with
choruses. In the present work, we do not differ-
entiate choruses from verses. In song lyrics, a
verse is most commonly represented as a sepa-
rate paragraph.
stanza a segment within a verse which has a me-
ter and rhyme scheme. Stanzas often consist of
2, 3, or 4 lines, but stanzas of more lines are
also common. Particularly in hip hop, a single
verse often contains many stanzas with differ-
ent rhyme schemes and meters.
line a segment within a stanza consisting of a single
line. In poetry, strictly speaking this would be
called a ?verse?, which however conflicts with
the conventional use of ?verse? in song lyrics.
In Section 2, we discuss some of the previous
work that applies statistical NLP methods to less
conventional domains and problems. We describe
our experimental conditions in Section 3. We com-
pare the performance of token and segment based
transduction grammar models in Section 4. We com-
pare our data selection schemes and disfluency han-
dling strategies in Sections 5 and 6. Finally, in
2Particularly the repetitive chants, exclamations, and inter-
jections in hip hop ?hype man? style backing vocals.
103
Section 7 we describe some preliminary results ob-
tained using our approach on improvising hip hop
responses in French and conclude in Section 8.
2 Related work
Although a few attempts have been made to apply
statistical NLP learning methods to unconventional
domains, Freestyle is among the first to tackle the
genre of hip hop lyrics (Addanki and Wu, 2013; Wu
et al, 2013a,b). Our preliminary work suggested the
need for further research to identify models that cap-
ture the correct generalizations to be able to gener-
ate fluent and rhyming responses. As a step towards
this direction, we contrast the performance of inter-
polated bottom-up token based rule induction and
top-down segmenting transduction grammar models
and token based transduction grammar models. We
briefly describe some of the past work in statistical
NLP on unconventional domains below.
Most of the past work either uses some form of
prior linguistic knowledge or enforces harsher con-
straints such as set number of words in a line, or a set
meter which are warranted by more structured do-
mains such as poetry. However, in hip hop lyrics it
is hard to make any linguistic or structural assump-
tions. For example, words such as sho, flo, holla
which frequently appear in the lyrics are not part of
any standard lexicon and hip hop does not require a
set number of syllables in a line, unlike poems. Also,
surprising and unlikely rhymes in hip hop are fre-
quently achieved via intonation and assonance, mak-
ing it hard to apply prior phonological constraints.
A phrase based SMT systemwas trained to ?trans-
late? the first line of a Chinese couplet or duilian
into the second by Jiang and Zhou (2008). The most
suitable next line was selected by applying linguistic
constraints to the n best output of the SMT system.
However in contrast to Chinese couplets, which ad-
here to strict rules requiring, for example, an identi-
cal number of characters in each line and one-to-one
correspondence in their metrical length, the domain
of hip hop lyrics is far more unstructured and there
exists no clear constraint that would ensure fluent
and rhyming responses to hip hop challenge lyrics.
Barbieri et al (2012) use controlled Markov pro-
cesses to semi-automatically generate lyrics that sat-
isfy the structural constraints of rhyme and meter.
Tamil lyrics were automatically generated given a
melody using conditional random fields by A. et al
(2009). The lyrics were represented as a sequence
of labels using the KNM system where K, N and M
represented the long vowels, short vowels and con-
sonants respectively.
Genzel et al (2010) used SMT in conjunction
with stress patterns and rhymes found in a pronun-
ciation dictionary to produce translations of poems.
Although many constraints were applied in translat-
ing full verses of poems, it was challenging to sat-
isfy all the constraints. Stress patterns were assigned
to words given the meter of a line in Shakespeare?s
sonnets by Greene et al (2010), which were then
combined with a language model to generate poems.
Sonderegger (2011) attempted to infer the pronun-
ciation of words in old English by identifying the
rhyming patterns using graph theory. However, their
heuristic of clustering words with similar IPA end-
ings resulted in large clusters of false positives such
as bloom and numb. A language-independent gener-
ative model for stanzas in poetry was proposed by
Reddy and Knight (2011) via which they could dis-
cover rhyme schemes in French and English poetry.
3 Experimental conditions
Before introducing our Freestyle models, we first
detail our experimental assumptions and the evalua-
tion scheme under which the responses generated by
different models are compared against one another.
We describe our training data as well as a phrase-
based SMT (PBSMT) contrastive baseline. We also
define the evaluation scheme used to compare the re-
sponses of different systems on criteria of fluency
and rhyming.
3.1 Training data
We used freely available user generated hip hop
lyrics on the Internet to provide training data for our
experiments. We collected approximately 52,000
English hip hop song lyrics amounting to approxi-
mately 800Mb of raw HTML content. The data was
cleaned by stripping HTML tags, metadata and nor-
malized for special characters and case differences.
The processed corpus contained 22 million tokens
with 260,000 verses and 2.7 million lines of hip hop
lyrics. As human evaluation using expert hip hop
104
listeners is expensive, a small subset of 85 lines was
chosen as the test set to provide challenges for com-
paring the quality of responses generated by different
systems.
3.2 Evaluation scheme
The performance of various Freestyle versions
was evaluated on the task of generating a improvised
fluent and rhyming response given a single line of a
hip hop verse as a challenge. The output of all the
systems on the test set was given to three indepen-
dent frequent hip hop listeners for manual evalua-
tion. They were asked to evaluate the system out-
puts according to fluency and the degree of rhyming.
They were free to choose the tune to make the lyrics
rhyme as the beats of the song were not used in the
training data. Each evaluator was asked to score the
response of each system on the criterion of fluency
and rhyming as being good, acceptable or bad.
3.3 Phrase-based SMT baseline
In order to evaluate the performance of an out-of-
the-box phrase-based SMT (PBSMT) system toward
this novel task of generating rhyming and fluent re-
sponses, a standard Moses baseline (Koehn et al,
2007) was also trained in order to compare its per-
formance with our transduction grammar induction
model. A 4-gram language model which was trained
on the entire training corpus using SRILM (Stolcke,
2002) was used to generate responses in conjunction
with the phrase-based translation model. As no au-
tomatic quality evaluation metrics exist for hip hop
responses analogous to BLEU for SMT, the model
weights cannot be tuned in conventional ways such
asMERT (Och, 2003). Instead, a slightly higher than
typical language model weight was empirically cho-
sen using a small development set to produce fluent
outputs.
4 Interpolated segmenting model vs. token
based model
We compare the performance of transduction gram-
mars induced via interpolated token based and rule
segmenting (ISTG) versus token based transduction
grammars (TG) on the task of generating a rhyming
and fluent response to hip hop challenges. We use
the framework of stochastic transduction grammars,
specifically bracketing ITGs (inversion transduction
grammars) (Wu, 1997), as our translation model for
?transducing? any given challenge into a rhyming
and fluent response. Our choice is motivated by
the significant amount of empirical evidence for the
representational capacity of transduction grammars
across a spectrum of natural language tasks such as
textual entailment (Wu, 2006), mining parallel sen-
tences (Wu and Fung, 2005) and machine translation
(Zens and Ney, 2003). Further, existence of effi-
cient learning algorithms (Saers et al, 2012; Saers
and Wu, 2011) that make no language specific as-
sumptions, make inversion transduction grammars a
suitable framework for our modeling needs. Exam-
ples of lexical transduction rules can be seen in Ta-
bles 3 and 5. In addition, the grammar also includes
structural transduction rules for the straight case
A? [A A] and also the inverted case A? <A A>.
4.1 Token based vs. segmental ITGs
The degenerate case of ITGs are token based ITGs
wherein each translation rule contains at most one
token in input and output languages. Efficient induc-
tion algorithmswith polynomial run time exist for to-
ken based ITGs and the expressivity they permit has
been empirically determined to capture most of the
word alignments that occur across natural languages.
The parameters of the token based ITGs can be es-
timated using expectation maximization through an
efficient dynamic programming algorithm in con-
junction with beam pruning (Saers and Wu, 2011).
In contrast to token based ITGs, each rule in a seg-
mental ITG grammar can contain more than one to-
ken in both input and output languages. In machine
translation applications, segmental models produce
translations that are more fluent as they can capture
lexical knowledge at a phrasal level. However, only
a handful of purely unsupervised algorithms exist
for learning segmental ITGs under matched training
and testing assumptions. Most other approaches in
SMT use a variety of ad hoc heuristics for extracting
segments from token alignments, justified purely by
short term improvements in automatic MT evalua-
tion metrics such as BLEU (Papineni et al, 2002)
which cannot be transferred to our current task. In-
stead, we use a completely unsupervised learning al-
gorithm for segmental ITGs that stays strictly within
the transduction grammar optimization framework
for both training and testing as proposed in Saers
105
et al (2013).
Saers et al (2013) induce a phrasal inversion
transduction grammar via interpolating the bottom-
up rule chunking approach proposed in Saers et al
(2012) with a top-down rule segmenting approach
driven by a minimum description length objective
function (Solomonoff, 1959; Rissanen, 1983) that
trades off the maximum likelihood against model
size. Saers et al (2013) report improvements in
BLEU score (Papineni et al, 2002) on their transla-
tion task. In our current approach instead of using a
bottom-up rule chunking approach we use a simpler
token based grammar instead. Given two grammars
(Ga and Gb) and an interpolation parameter ? the
probability function of the interpolated grammar is
given by:
pa+b (r) = ?pa (r) + (1? ?)pb (r)
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. The
pseudocode for the top-down rule segmenting algo-
rithm is shown in 1. The algorithm uses the methods
collect_biaffixes, eval_dl, sort_by_delta and
make_segmentations. These methods collect all the
biaffixes in an ITG, evaluate the difference in de-
scription length, sort candidates by these differences,
and commit to a given set of candidates, respectively.
The suitable interpolation parameter is chosen em-
pirically based on the responses generated on a small
development set.
We compare the performance of inducing a token
based ITG versus inducing a segmental ITG using in-
terpolated bottom-up token based rule induction and
top-down rule segmentation. To highlight some of
the inherent challenges in adapting other algorithms
to this novel task, we also compare the quality of the
responses generated by our model to those generated
by an off-the-shelf phrase based SMT system.
4.2 Decoding heuristics
We use our in-house ITG decoder implemented ac-
cording to the algorithm mentioned in Wu (1996)
for the generating responses to challenges by decod-
ing with the trained transduction grammars. The de-
coder uses a CKY-style parsing algorithm (Cocke,
Algorithm 1 Iterative rule segmenting learning
driven by minimum description length.
1: ? ? The ITG being induced
2: repeat
3: ?sum ? 0
4: bs? collect_biaffixes(?)
5: b? ? []
6: for all b ? bs do
7: ? ? eval_dl(b,?)
8: if ? < 0 then
9: b? ? [b?, ?b, ??]
10: sort_by_delta(b?)
11: for all ?b, ?? ? b? do
12: ?? ? eval_dl(b,?)
13: if ?? < 0 then
14: ?? make_segmentations(b,?)
15: ?sum ? ?sum + ??
16: until ?sum ? 0
17: return ?
1969) with cube pruning (Chiang, 2007). The de-
coder builds an efficient hypergraph structure which
is then scored using the induced grammar. The
trained transduction grammar model was decoded
using the 4-gram language model and the model
weights determined as described in 3.3.
In our decoding algorithm, we restrict the reorder-
ing to only be monotonic as we want to produce out-
put that follows the same rhyming order of the chal-
lenge. Interleaved rhyming order is harder to evalu-
ate without the larger context of the song and we do
not address that problem in our current model. We
also penalize singleton rules to produce responses of
similar length as successive lines in a stanza are typ-
ically of similar length. Finally, we add a penalty to
reflexive translation rules that map the same surface
form to itself such as A ? yo/yo. We obtain these
rules with a high probability due to the presence of
sentence pairs where both the input and output are
identical strings as many stanzas in our data contain
repeated chorus lines.
4.3 Results: Rule segmentation improves
responses
Results in Table 1 indicate that the ISTG outperforms
the TG model towards the task of generating fluent
and rhyming responses. On the criterion of fluency,
106
Table 1: Percentage of ?good and ?acceptable (i.e., either good or acceptable) responses on fluency and rhyming
criteria. PBSMT, TG and ISTG models trained using corpus generated from all adjacent lines in a verse. PBSMT+RS,
TG+RS, ISTG+RS are models trained on rhyme scheme based corpus selection strategy. Disfluency correction strategy
was used in all cases.
model fluency (?good) fluency (?acceptable) rhyming (?good) rhyming (?acceptable)
PBSMT 3.14% 4.70% 1.57% 4.31%
TG 21.18% 54.51% 23.53% 39.21%
ISTG 26.27% 57.64% 27.45% 48.23%
PBSMT+RS 30.59% 43.53% 1.96% 9.02%
TG+RS 34.12% 60.39% 20.00% 42.74%
ISTG+RS 30.98% 61.18% 30.98% 53.72%
Table 2: Transduction rules learned by ISTG model.
transduction grammar rule log prob.
A? long/wrong -11.6747
A? rhyme/time -11.6604
A? felt bad/couldn't see what i really had -11.3196
A? matter what you say/leaving anyway -11.8792
A? arhythamatic/this rhythm is sick -12.3492
the ISTGmodel produces a significantly higher frac-
tion of sentences rated good (26.27% vs. 21.18%)
and ?acceptable (57.64% vs. 54.51%). Higher frac-
tion of responses generated by the ISTG model are
rated as good (27.45% vs. 23.53%) and ?acceptable
(57.64% vs. 54.51%) compared to the TG model.
Both TG and ISTG model perform significantly bet-
ter than the PBSMT baseline. Upon inspecting the
learned rules, we noticed that the ISTG models cap-
ture rhyming correspondences both at the token and
segmental levels. Table 2 shows some examples
of the transduction rules learned by ISTG grammar
trained using rhyme scheme detection.
5 Data selection via rhyme scheme
detection vs. adjacent lines
We now compare two data selection approaches
for generating the training data for transduction
grammar induction via a rhyme scheme detection
module and choosing all adjacent lines in a verse.
We also briefly describe the training of the rhyme
scheme detection module and determine the efficacy
of our data selection scheme by training the ISTG
model, TG model and the PBSMT baseline on train-
ing data generated with and without employing the
rhyme scheme detection module. As the rule seg-
menting approach was intended to improve the flu-
ency as opposed to the rhyming nature of the re-
sponses, we only train the rule segmenting model
on the randomly chosen subset of all adjacent lines
in the verse. Further, adding adjacent lines as the
training data to the segmenting model maintains the
context of the responses generated thereby produc-
ing higher quality responses. The segmental trans-
duction grammar model was combined with the to-
ken based transduction grammar model trained on
data selected with and without using rhyme scheme
detection model.
5.1 Rhyme scheme detection
Although our approach adapts a transduction gram-
mar induction model toward the problem of generat-
ing fluent and rhyming hip hop responses, it would
be undesirable to train the model directly on all the
successive lines of the verses?as done by Jiang and
Zhou (2008)?due to variance in hip hop rhyming
patterns. For example, adding successive lines of a
stanza which follows ABAB rhyme scheme as train-
ing instances to the transduction grammar causes in-
correct rhyme correspondences to be learned. The
fact that a verse (which is usually represented as
a separate paragraph) may contain multiple stanzas
of varying length and rhyme schemes worsens this
problem. Adding all possible pairs of lines in a verse
as training examples not only introduces a lot of
noise but also explodes the size of the training data
due to the large size of the verse.
We employ a rhyme scheme detection model (Ad-
danki and Wu, 2013) in order to select training in-
stances that are likely to rhyme. Lines belonging to
107
the same stanza and marked as rhyming according
to the rhyme scheme detection model are added to
the training corpus. We believe that this data selec-
tion scheme will improve the rhyming associations
learned during the transduction grammar induction
thereby biasing the model towards producing fluent
and rhyming output.
The rhyme scheme detection model proposes a
HMM based generative model for a verse of hip hop
lyrics similar to Reddy and Knight (2011). However,
owing to the lack of well-defined verse structure in
hip hop, a number of hidden states corresponding to
stanzas of varying length are used to automatically
obtain a soft-segmentation of the verse. Each state
in the HMM corresponds to a stanza with a particu-
lar rhyme scheme such asAA,ABAB,AAAAwhile
the emissions correspond to the final words in the
stanza. We restrict the maximum length of a stanza
to be four to maintain a tractable number of states
and further only use states to represent stanzas whose
rhyme schemes could not be partitioned into smaller
schemes without losing a rhyme correspondence.
The parameters of the HMM are estimated using
the EM algorithm (Devijer, 1985) using the corpus
generated by taking the final word of each line in the
hip hop lyrics. The lines from each stanza that rhyme
with each other according to the Viterbi parse using
the trained model are added as training instances for
transduction grammar induction. As the source and
target languages are identical, each selected pair gen-
erates two training instances: a challenge-response
and a response-challenge pair.
The training data for the rhyme scheme detector
was obtained by extracting the end-of-line tokens
from each verse. However, upon data inspection we
noticed that shorter lines in hip hop stanzas are typi-
cally joined with a comma and represented as a sin-
gle line of text and hence all the tokens before the
commas were also added to the training corpus. We
obtained a corpus containing 4.2 million tokens cor-
responding to potential rhyming candidates compris-
ing of around 153,000 unique token types.
We evaluated the performance of our rhyme
scheme detector on the task of correctly labeling a
given verse with rhyme schemes. As our model is
completely unsupervised, we chose a random sam-
ple of 75 verses from our training data as our test set.
Two native English speakers who were frequent hip
hop listeners were asked to partition the verse into
stanzas and assign them with a gold standard rhyme
scheme. Precision and recall were aggregated for the
Viterbi parse of each verse against this gold standard
and f-score was calculated. The rhyme scheme de-
tection module employed in our data selection ob-
tained a precision of 35.81% and a recall of 57.25%,
giving an f-score of 44.06%.
5.2 Training data selection via rhyme scheme
detection
We obtained around 600,000 training instances upon
extracting a training corpus using rhyme scheme de-
tection module as described in Section 5.1. We
added those lines that were adjacent and labeled as
rhyming by the rhyme scheme detector as training in-
stances resulting in a training corpus of size 200,000.
5.3 Training data selection via adjacent lines
Considering all adjacent lines in a verse resulted in
a corpus with over 5 million training instances. In
order to ensure fair comparison of models trained
on data selected using rhyme scheme detection, we
randomly chose 200,000 training instances from the
generated corpus. The training corpus thus gener-
ated shared around 15% of training instances with
the corpus generated through our proposed data se-
lection scheme.
5.4 Results: Rhyme scheme detection helps
Results in Table 1 indicate that using the rhyme
scheme detector for training data selection helps
produce significantly more fluent responses com-
pared to using adjacent lines. A possible explana-
tion for this could be that adding all adjacent lines as
training instances introduces a lot of noise into the
model which hurts the fluency of the responses gen-
erated. Also, the cumulative fraction of sentences
that were labeled good or ?acceptable on the crite-
rion of rhyming is larger when rhyme scheme detec-
tion was used to generate the training data (although
the TG model trained on the corpus generated us-
ing adjacent lines produces a higher percentage of
rhyming responses that were rated good). Given the
significantly higher rate of response fluency when
using rhyme scheme detection, we argue that using
rhyme scheme detector for data selection is benefi-
cial.
108
Table 3: English hip hop challenge-response examples.
challenge man i die to see em all thun i just don't care
tg+rs in the sky and me the in polla and the you there
tg and the ride the me the and white the i the air
pbsmt+rs man i live to see em all i just don't care
challenge did a twelve year bid in the streets and held it down
tg+rs to the girls here kid the and to the thought the now
tg to the p's here did the a the i was the the
pbsmt+rs did a year in the streets and it down
challenge oh i believe in yesterday
tg+rs can you see the day
tg now you see the way
pbsmt+rs oh i believe in tomorrow
challenge what would i do
tg+rs just me and you
tg and you and you
pbsmt+rs what would you do
challenge cause you ain't going home till the early morn
tg+rs and the you this alone i i gotta on
tg and i you my on the a home we
pbsmt+rs cause you and your friends aint nothing but
It is also interesting to note from Table 1 that
ISTG+RS performs better than TG+RS indicating
that transduction grammar induced via interpolating
token based grammar and rule segmenting produces
better responses than token based transduction gram-
mar on both data selection schemes. Although the
average fraction of responses rated good on fluency
are slightly lower for ISTG+RS compared to TG+RS
(34.12% vs. 30.98%), the fraction of responses rated
?acceptable are higher (61.18% vs. 57.64%). It is
important to note that the fraction of sentences rated
good and ?acceptable on rhyming are much larger
for ISTG+RS model. Although the fluency of the
responses generated by PBSMT+RS drastically im-
proves compared to PBSMT it still lags behind the
TG+RS and ISTG+RS models on both fluency and
rhyming. The results in Table 1 confirm our hypoth-
esis that off-the-shelf SMT systems are not guaran-
teed to be effective on our novel task.
5.5 Challenge-response examples
Table 3 shows some of the challenges and the cor-
responding responses of PBSMT+RS, TG+RS and
TG model. While PBSMT+RS and TG+RS mod-
els generate responses reflecting a high degree of
fluency, the output of the TG contains a lot of ar-
ticles. It is interesting to note that TG+RS produces
responses comparable to PBSMT+RS despite being
a token based transduction grammar. However, PB-
SMT tends to produce responses that are too simi-
lar to the challenge. Moreover, TG models produce
responses that indeed rhyme better (shown in bold-
face). In fact, TG tries to rhymewords not only at the
end but also in middle of the lines, as our transduc-
tion grammar model captures structural associations
more effectively than the phrase-based model.
6 Disfluency handling via disfluency
correction and filtering
In this section, we compare the effect of two dis-
fluency mitigating strategies on the quality of the re-
sponses generated by the PBSMT baseline and token
based transduction grammar model with and without
using rhyme scheme detection.
6.1 Correction vs. filtering
Error analysis of our initial runs showed a dis-
turbingly high proportion of responses generated by
our system that contained disfluencies with succes-
sive repetitions of words such as the and I. Upon in-
spection of data we noticed that the training lyrics
actually did contain such disfluencies and backing
vocal lines, amounting to 10% of our training data.
We therefore compared two alternative strategies to
tackle this problem. The first strategy involved fil-
tering out all lines from our training corpus which
contained such disfluencies. In the second strategy,
we implemented a disfluency detection and correc-
tion algorithm (for example, the the the, which fre-
quently occurred in the training corpus, was cor-
rected to simply the). The PBSMT baseline and the
TG model were trained on both the filtered and cor-
rected versions of the training corpus and the quality
of the responses were compared.
6.2 Results: Disfluency correction helps
The results in Table 4 indicate that the disfluency
correction strategy outperforms the filtering strategy
for both TG and TG+RS models. For the model
TG+RS, disfluency correction generated 34.12%
good responses in terms of fluency, while the filter-
ing strategy produced only 28.63% good responses.
Similarly for the model TG, disfluency correction
produced 21.8% of responses with good fluency and
the filtering strategy produced only 17.25%. Dis-
fluency correction strategy produces higher fraction
of responses with ?acceptable fluency compared to
the filtering strategy for both TG and TG+RS mod-
els. This result is not surprising, as harshly pruning
109
Table 4: Effect of the disfluency correction strategies on fluency of the responses generated for the TG induction
models vs PBSMT baselines using both rhyme scheme detection and adjacent lines as the corpus selection method.
model+disfluency strat. fluency (good) fluency (?acceptable) rhyming (good) rhyming (?acceptable)
PBSMT+filtering 4.3% 13.72% 3.53% 7.06%
PBSMT+correction 3.14% 4.70% 1.57% 4.31%
PBSMT+RS+filtering 31.76% 43.91% 12.15% 21.17%
PBSMT+RS+correction 30.59% 43.53% 1.96% 9.02%
TG+filtering 17.25% 46.27% 18.04% 33.33%
TG+correction 21.18% 54.51% 23.53% 39.21%
TG+RS+filtering 28.63% 56.86% 14.90% 34.51%
TG+RS+correction 34.12% 60.39% 20.00% 42.74%
the training corpus causes useful word association
information necessary for rhyming to be lost. Sur-
prisingly, for both PBSMT and PBSMT+RSmodels,
the disfluency correction has a negative effect on the
fluency level of the response but still falls behind TG
and TG+RS models. As disfluency correction yields
more fluent responses for TG and TG+RS models,
the results for ISTG and ISTG+RS models in Table
1 were obtained using disfluency correction strategy.
7 Maghrebi French hip hop
We have begun to apply Freestyle to rap in lan-
guages other than English, taking advantage of
the language independence and linguistics-light ap-
proach of our unsupervised transduction grammar
induction methods. With no special adaption our
transduction grammar based model performs sur-
prisingly well, even with significantly smaller train-
ing data size and noisier data. These results across
different languages are encouraging as they can be
used to discover truly language independence as-
sumptions. We briefly describe our initial experi-
ments on Maghrebi French hip hop lyrics below.
7.1 Dataset
We collected freely available French hip hop lyrics
of approximately 1300 songs. About 85% of the
songs were by Maghrebi French artists of Alge-
rian, Moroccan, or Tunisian cultural backgrounds,
while the remaining were by artists from the rest
of Francophonie. As the large majority of songs
are in Maghrebi French, the lyrics are sometimes
interspersed with romanized Arabic such as ?De la
travers?e du d?sert au bon couscous de Y?ma? (Y?ma
meansMy mother). Some songs also contain Berber
phrases, for instance ?a yemmi ino, a thizizwith?
(which means my son, a bee). Furthermore, some
songs also contained English phrases in the style of
gangster rap such as ?T'es game over, game over... Le
son de Chicken wings?. As mentioned earlier, it is
complexity like this which dissuaded us from mak-
ing language specific assumptions in our model.
We extracted the end-of-line words and obtained
a corpus containing 120,000 tokens corresponding
to potential rhyming candidates with around 29,000
unique token types which was used as the training
data for the rhyme scheme detector module. For the
transduction grammar induction, the training data
contained about 47,000 sentence pairs selected us-
ing rhyme scheme detection.
7.2 Results
After human evaluation by native French speak-
ers and frequent hip hop listeners, our transduction
grammar based model generates about 9.2% and
14.5% of the responses that are rated good by the
human evaluators on the criterion of fluency and
rhyming respectively. About 30.2% and 38% of
the responses are rated as ?acceptable. These num-
bers are encouraging given the noisy lyrics andmuch
smaller amount of training data. Some examples of
the challenge-response pairs and learned transduc-
tion rules in French are shown in Tables 5 and 6.
From Table 5, we can see that responses gener-
ated by the system rhyme with the challenges. The
first response is fluent and not only perfectly rhymes
with the challenge but also semantically valid. In the
second example, the model realizes a less common
AABA rhyme scheme through the response. The re-
110
Table 5: French hip hop challenge-response examples.
challenge Si je me trompe
response faut que je raconte
challenge Un jour je suis un livre
response et ce que je de vivre
challenge Pacha mama ils ne voient pas ta souffrance
response Combat ni leur de voulait de la d?cadence
challenge la palestine n'etait pas une terre sans peuple.
response le darfour d'autre de la guerre on est
challenge Une banlieue qui meut
response les yeux et
Table 6: Transduction rules for Maghrebi French hip hop.
transduction grammar rule log prob.
A? terre/la guerre -9.4837
A? haine/peine -9.77056
A? mal/pays natal -10.6877
A? je frissonne/mi corazon -11.0931
A? gratteurs/rappeurs -11.7306
sponse in the third example, exhibits strong rhyming
with the challenge and both the challenge and the
response contain words like souffrance, combat and
d?cadence which are related. Similarly in the fourth
example, the challenge and response also contain se-
mantically related tokens which also rhyme. These
examples illustrate that our transduction grammar
formalism coupled with our rhyme scheme detection
module does capture the necessary correspondences
between lines of hip hop lyrics without assuming any
language specific resources.
8 Conclusion
We presented a new machine learning approach for
improvising hip hop responses to challenge lyrics
by inducing stochastic transduction grammars, and
demonstrated that inducing the transduction rules by
interpolating bottom-up token based rule induction
and rule segmentation strategies outperforms a token
based baseline. We compared the performance of
our Freestylemodel against a widely used off-the-
shelf phrase-based SMT model, showing that PB-
SMT falls short in tackling the noisy and highly un-
structured domain of hip hop lyrics. We showed that
the quality of responses improves when the training
data for the transduction grammar induction is se-
lected using a rhyme scheme detector. Several do-
main related oddities such as disfluencies and back-
ing vocals have been identified and some strategies
for alleviating their effects have been compared. We
also reported results on Maghrebi French hip hop
lyrics which indicate that our model works surpris-
ingly well with no special adaptation for languages
other than English. In the future, we plan to inves-
tigate alternative training data selection techniques,
disfluency handling strategies, search heuristics, and
novel transduction grammar induction models.
Acknowledgements
This material is based upon work supported in part by
the Hong Kong Research Grants Council (RGC) research
grants GRF620811, GRF621008, GRF612806; by the
Defense Advanced Research Projects Agency (DARPA)
under BOLT contract no. HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-
0023; and by the European Union under the FP7 grant
agreement no. 287658. Any opinions, findings and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the
views of the RGC, EU, or DARPA.
References
Ananth Ramakrishnan A., Sankar Kuppan, and
Lalitha Devi Sobha. ?Automatic generation of
Tamil lyrics for melodies.? Workshop on Computa-
tional Approaches to Linguistic Creativity (CALC-09).
2009.
Karteek Addanki and DekaiWu. ?Unsupervised rhyme
scheme identification in hip hop lyrics using hidden
Markov models.? 1st International Conference on Sta-
tistical Language and Speech Processing (SLSP 2013).
2013.
Gabriele Barbieri, Fran?ois Pachet, Pierre Roy, and
Mirko Degli Esposti. ?Markov constraints for gen-
erating lyrics with style.? 20th European Conference
on Artificial Intelligence, (ECAI 2012). 2012.
David Chiang. ?Hierarchical phrase-based translation.?
Computational Linguistics, 33(2), 2007.
John Cocke. Programming languages and their compil-
ers: Preliminary notes. Courant Institute ofMathemat-
ical Sciences, New York University, 1969.
P.A. Devijer. ?Baum?s forward-backward algorithm re-
visited.? Pattern Recognition Letters, 3(6), 1985.
D. Genzel, J. Uszkoreit, and F. Och. ?Poetic statisti-
cal machine translation: rhyme and meter.? 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010). Association for Computa-
tional Linguistics, 2010.
E. Greene, T. Bodrumlu, and K. Knight. ?Auto-
matic analysis of rhythmic poetry with applications
111
to generation and translation.? 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010). Association for Computational Lin-
guistics, 2010.
Long Jiang and Ming Zhou. ?Generating Chinese
couplets using a statistical MT approach.? 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008). 2008.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. ?Moses:
Open source toolkit for statistical machine translation.?
Interactive Poster and Demonstration Sessions of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2007). June 2007.
MarkLiberman. ?Rap scholarship, rapmeter, and the an-
thology of mondegreens.? http://languagelog.ldc.
upenn.edu/nll/?p=2824, December 2010. Accessed:
2013-06-30.
Franz Josef Och. ?Minimum error rate training in sta-
tistical machine translation.? 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
2003). July 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. ?BLEU: a method for automatic evalu-
ation of machine translation.? 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02). July 2002.
S. Reddy and K. Knight. ?Unsupervised discovery of
rhyme schemes.? 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL HLT 2011), vol. 2. Association for
Computational Linguistics, 2011.
Jorma Rissanen. ?A universal prior for integers and es-
timation by minimum description length.? The Annals
of Statistics, 11(2), June 1983.
Markus Saers, KarteekAddanki, andDekaiWu. ?From
finite-state to inversion transductions: Toward un-
supervised bilingual grammar induction.? 24th In-
ternational Conference on Computational Linguistics
(COLING 2012). December 2012.
Markus Saers, Karteek Addanki, and Dekai Wu.
?Combining top-down and bottom-up search for un-
supervised induction of transduction grammars.? Sev-
enth Workshop on Syntax, Semantics and Structure in
Statistical Translation (SSST-7). June 2013.
Markus Saers and Dekai Wu. ?Reestimation of reified
rules in semiring parsing and biparsing.? Fifth Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (SSST-5). Association for Computational
Linguistics, June 2011.
Ray J. Solomonoff. ?A new method for discov-
ering the grammars of phrase structure languages.?
International Federation for Information Processing
Congress (IFIP). 1959.
M. Sonderegger. ?Applications of graph theory to an
English rhyming corpus.? Computer Speech & Lan-
guage, 25(3), 2011.
Andreas Stolcke. ?SRILM ? an extensible language
modeling toolkit.? 7th International Conference on
Spoken Language Processing (ICSLP2002 - INTER-
SPEECH 2002). September 2002.
Dekai Wu. ?A polynomial-time algorithm for statisti-
cal machine translation.? 34th Annual Meeting of the
Association for Computational Linguistics (ACL96).
1996.
DekaiWu. ?Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora.? Computa-
tional Linguistics, 23(3), 1997.
Dekai Wu. ?Textual entailment recognition using inver-
sion transduction grammars.? Joaquin Qui?onero-
Candela, Ido Dagan, Bernardo Magnini, and Flo-
rence d?Alch? Buc (eds.),Machine Learning Chal-
lenges, Evaluating Predictive Uncertainty, Visual Ob-
ject Classification and Recognizing Textual Entail-
ment, First PASCAL Machine Learning Challenges
Workshop (MLCW 2005), vol. 3944 of Lecture Notes
in Computer Science. Springer, 2006.
Dekai Wu, Karteek Addanki, and Markus Saers.
?FREESTYLE: A challenge-response system for hip
hop lyrics via unsupervised induction of stochastic
transduction grammars.? 14th Annual Conference of
the International Speech Communication Association
(Interspeech 2013). 2013a.
Dekai Wu, Karteek Addanki, and Markus Saers.
?Modeling hip hop challenge-response lyrics as ma-
chine translation.? 14th Machine Translation Summit
(MT Summit XIV). 2013b.
Dekai Wu and Pascale Fung. ?Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora.? Second Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP 2005). Springer, 2005.
Richard Zens and HermannNey. ?A comparative study
on reordering constraints in statistical machine trans-
lation.? 41st Annual Meeting of the Association for
Computational Linguistics (ACL-2003). Association
for Computational Linguistics, 2003.
112
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341?344,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Word Alignment with
Stochastic Bracketing Linear Inversion Transduction Grammar
Markus SAERS and Joakim NIVRE
Computational Linguistics Group
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
The class of Linear Inversion Transduction
Grammars (LITGs) is introduced, and used to
induce a word alignment over a parallel cor-
pus. We show that alignment via Stochas-
tic Bracketing LITGs is considerably faster
than Stochastic Bracketing ITGs, while still
yielding alignments superior to the widely-
used heuristic of intersecting bidirectional
IBM alignments. Performance is measured as
the translation quality of a phrase-based ma-
chine translation system built upon the word
alignments, and an improvement of 2.85 BLEU
points over baseline is noted for French?
English.
1 Introduction
Machine translation relies heavily on word align-
ments, which are usually produced by training IBM-
models (Brown et al, 1993) in both directions and
combining the resulting alignments via some heuris-
tic. Automatically training an Inversion Transduc-
tion Grammar (ITG) has been suggested as a viable
way of producing superior alignments (Saers and
Wu, 2009). The main problem of using Bracket-
ing ITGs for alignment is that exhaustive biparsing
runs in O(n6) time. Several ways to lower the com-
plexity of ITGs has been suggested, but in this paper,
a different approach is taken. Instead of using full
ITGs, we explore the possibility of subjecting the
grammar to a linear constraint, making exhaustive
biparsing of a sentence pair in O(n4) time possible.
This can be further improved by applying pruning.
2 Background
A transduction is the bilingual version of a language.
A language (Ll) can be formally viewed as a set of
sentences, sequences of tokens taken from a speci-
fied vocabulary (Vl). A transduction (Te,f ) between
two languages (Le and Lf ) is then a set of sentence
pairs, sequences of bitokens from the cross produc-
tion of the vocabularies of the two languages being
transduced (Ve,f = Ve ? Vf ). This adds an extra
layer of complexity to finding transductions from
raw bitexts, as an alignment has to be imposed.
Simple (STG) and Syntax Directed (SDTG) Trans-
duction Grammars (Aho and Ullman, 1972) can
be used to parse transductions between context-free
languages. Both work fine as long as a grammar is
given and parsing is done as transduction, that is: a
sentence in one language is rewritten into the other
language. In NLP, interest has shifted away from
hand-crafted grammars, towards stochastic gram-
mars induced from corpora. To induce a stochas-
tic grammar from a parallel corpus, expectations of
all possible parses over a sentence pair are typically
needed. STGs can biparse sentence pairs in polyno-
mial time, but are unable to account for the complex-
ities typically found in natural languages. SDTGs do
account for the complexities in natural languages,
but are intractable for biparsing.
Inversion transductions (Wu, 1995; Wu, 1997) are
a special case of transductions that are not mono-
tone, but where permutations are severely limited.
By limiting the possible permutations, biparsing be-
comes tractable. This in turn means that ITGs can be
induced from parallel corpora in polynomial time,
341
as well as account for most of the reorderings found
between natural languages.
An Inversion transduction is limited so that it
must be expressible as non-overlapping groups, in-
ternally permuted either by the identity permuta-
tion or the inversion permutation (hence the name).
This requirement also means that the grammar is bi-
narizable, yielding a two-normal form. A produc-
tion with the identity permutation is written inside
square brackets, while productions with the inver-
sion permutation is written inside angled brackets.
This gives us a two-normal form that looks like this
(where e/f is a biterminal):
A ? [B C]
A ? ?B C?
A ? e/f
The time complexity for exhaustive ITG biparsing is
O(Gn6), which is typically too large to be applica-
ble to large grammars and long sentence. The gram-
mar constant G can be eliminated by limiting the
grammar to a bracketing ITG (BITG), which only has
one nonterminal symbol. Saers & Wu (2009) show
that it is possible to apply exhaustive biparsing to a
large parallel corpus (? 100, 000 sentence pairs) of
short sentences (?10 tokens in both language). The
word alignments read off the Viterbi parse also in-
creased translation quality when used instead of the
alignments from bidirectional IBM alignments.
The O(n6) time complexity is somewhat pro-
hibitive for large corpora, so pruning in some form is
needed. Saers, Nivre & Wu (2009) introduce a beam
pruning scheme, which reduces time complexity to
O(bn3). They also show that severe pruning is pos-
sible without significant deterioration in alignment
quality. Haghighi et. al (2009) use a simpler aligner
as guidance for pruning, which reduce the time com-
plexity by two orders of magnitude, and also intro-
duce block ITG, which gives many-to-one instead of
one-to-one alignments. Zhang et. al (2008) present
a method for evaluating spans in the sentence pair to
determine whether they should be excluded or not.
The algorithm has a best case time complexity of
O(n3).
In this paper we introduce Linear ITG (LITG), and
apply it to a word-alignment task which is evaluated
by the phrase-based statistical machine translation
(PBSMT) system that can be built from that.
3 Stochastic Bracketing Linear Inversion
Transduction Grammar
A Bracketing Linear Inversion Transduction Gram-
mar (BLITG) is a BITG where rules may have at most
one nonterminal symbol in their production. This
gives us a normal form that is somewhat different
from the usual ITG:
X ? [Xe/f ]
X ? [e/fX]
X ? ?Xe/f?
X ? ?e/fX?
X ? ?/?
where one but not both of the tokens in the bitermi-
nal may be the empty string ?, if a nonterminal is
produced. By associating each rule with a probabil-
ity, we get a Stochastic BLITG (SBLITG).
3.1 Biparsing Algorithm
The sentence pair to be biparsed consists of two vec-
tors of tokens (e and f ). An item is represented
as a nonterminal (X), and one span in each of the
languages (es..t and fu..v). For notational conve-
nience, an item will be written as the nonterminal
with the spans as subscripts (Xs,t,u,v). The length of
an item is defined as the sum of the length of the two
spans: |Xs,t,u,v| = t ? s + v ? u. Items are gath-
ered in buckets, Bn, according to their length so that
Xs,t,u,v ? B|Xs,t,u,v|. The algorithm is initialized
with the item spanning the entire sentence pair:
X0,|e|,0,|f | ? B|X0,|e|,0,|f||
Starting from this top bucket, buckets are processed
in larger to smaller order: Bn, Bn?1, . . . , B1. While
processing a bucket, only smaller items are added,
meaning that B0 is fully constructed by the time B1
has been processed. Each item in B0 can have the
rule X ? ?/? applied to it, eliminating the nonter-
minal and halting processing. If there are no items
in B0, parsing has failed.
To process a bucket, each item is extended by all
applicable rules, and the nonterminals in the produc-
tions are added to their respective buckets.
342
System BLEU NIST Phrases
GIZA++ (intersect) 0.2629 6.7968 146,581,109
GIZA++ (grow-diag-final) 0.2632 6.7410 1,298,566
GIZA++ (grow-diag-final-and) 0.2742 6.9228 7,340,369
SBLITG (b = 25) 0.3027 7.3664 13,551,915
SBLITG (b = ?) 0.3008 7.3303 12,673,361
Table 1: Results for French?English.
Xs,t,u,v ?
[es,s+1/fu,u+1 Xs+1,t,u+1,v]
| [Xs,t?1,u,v?1 et?1,t/fv?1,v ]
| ?es,s+1/fv?1,v Xs+1,t,u,v?1?
| ?Xs,t?1,u+1,v et?1,t/fu,u+1?
| [es,s+1/? Xs+1,t,u,v] | ?es,s+1/? Xs+1,t,u,v?
| [?/fu,u+1 Xs,t,u+1,v] | ?Xs,t,u+1,v ?/fu,u+1?
| [Xs,t?1,u,v et?1,t/?] | ?Xs,t?1,u,v et?1,t/??
| [Xs,t,u,v?1 ?/fv?1,v] | ??/fv?1,v Xs,t,u,v?1?
Note that there are two productions on each of the
four last rows. These are distinct rules, but the
symbols in the productions are identical. This phe-
nomenon is due to the fact that the empty sym-
bols can be ?read? off either end of the span. In
our experiments, such rules were merged into their
non-inverting form, effectively eliminating the last
four inverted rules (productions enclosed in angled
brackets) above.
3.2 Analysis
Let n be the length of the longer sentence in the
pair. The number of buckets will be O(n), since
the longest item will be at most 2n long. Within a
bucket, there can be O(n2) starting points for items,
but once the length of one of the spans is fixed, the
length of the other follows, adding a factor O(n),
making the total number of items in a bucket O(n3).
Each item in a bucket can be analyzed in 8 possible
ways, requiring O(1) time. In summary, we have:
O(n)?O(n3)?O(1) = O(n4)
The pruning scheme works by limiting the num-
ber of items that are processed from each bucket, re-
ducing the cost of processing a bucket from O(n3)
to O(b), where b is the beam width. This gives time
complexity O(n) ?O(b) ?O(1) = O(bn).
4 Experiments
We used the guidelines of the shared task of
WMT?081 to train our baseline system as well as
our experimental system. This includes induction of
word alignments with GIZA++ (Och and Ney, 2003),
induction of a Phrase-based SMT system (Koehn et
al., 2007), and tuning with minimum error rate train-
ing (Och, 2003), as well as applying some utility
scripts provided for the workshop. The translation
model is combined with a 5-gram language model
(Stolcke, 2002).
Our experimental system uses alignments from
the Viterbi parses, extracted during EM training of an
SBLITG on the training corpus, instead of GIZA++.
Since EM will converge fairly slowly, it was limited
to 10 iterations, after which it was halted.
We used the French?English part of the WMT?08
shared task, but limited the training set to sentence
pairs where both sentences were of length 20 or less.
This was necessary in order to carry out exhaustive
search in the SBLITG algorithm. In total, we had
381,780 sentence pairs for training, and 2,000 sen-
tence pairs each for tuning and testing. The language
model was trained with the entire training set.
To evaluate the systems we used BLEU (Papineni
et al, 2002) and NIST (Doddington, 2002)
Results are presented in Table 1. It is interesting
to note that there is no correlation between the num-
ber of phrases extracted and translation quality. The
only explanation for the results we are seeing is that
the SBLITGs find better phrases. Since the only dif-
ference is the word alignment strategy, this suggests
that the word alignments from SBLITGs are better
suited for phrase extraction than those from bidirec-
tional IBM-models. The fact that SBLITGs extract
more phrases than bidirectional IBM-models under
1http://www.statmt.org/wmt08/
343
the grow-diag-x heuristics is significant, since
more phrases means that more translation possibil-
ities are extracted. The fact that SBLITGs extract
fewer phrases than bidirectional IBM-models under
the intersect heuristic is also significant, since
it implies that simply adding more phrases is a bad
strategy. Combined, the two observations leads us
to believe that there are some alignments missed by
the bidirectional IBM-models that are found by the
SBLITG-models. It is also interesting to see that the
pruned version outperforms the exhaustive version.
We believe this to be because the pruned version ap-
proaches the correct grammar faster than the exhaus-
tive. That would mean that the exhaustive SBLITG
would be better in the limit, but the experiment was
limited to 10 iterations.
5 Conclusion
In this paper we have focused on the benefits of ap-
plying SBLITGs to the task of inducing word align-
ments, which leads to a 2.85 BLEU points improve-
ment compared to the standard model (heuristically
combined bidirectional IBM-models). In the future,
we hope that LITGs will be a spring board towards
full ITGs, with more interesting nonterminals than
the BITGs seen in the literature so far. With the pos-
sibility of inducing full ITG from parallel corpora it
becomes viable to use ITG decoders directly as ma-
chine translation systems.
Acknowledgments
This work was funded by the Swedish National Gradu-
ate School of Language Technology (GSLT), the Defense
Advanced Research Projects Agency (DARPA) under
GALE Contract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under research
grants GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of DARPA.The computations were performed on
UPPMAX resources under project p2007020.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice-Halll, En-
glewood Cliffs, New Jersey.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology
conference (HLT-2002), San Diego, California.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of ACL/IJCNLP 2009, pages 923?931,
Singapore.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
ACL 2007 Demo and Poster Session, pages 177?180,
Prague, Czech Republic.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL 2003,
pages 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311?
318, Philadelphia, Pennsylvania.
M. Saers and D. Wu. 2009. Improving phrase-based
translation via word alignments from Stochastic In-
version Transduction Grammars. In Proceedings of
SSST-3 at NAACL HLT 2009, pages 28?36, Boulder,
Colorado.
M. Saers, J. Nivre, and D. Wu. 2009. Learning stochas-
tic bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings of
IWPT?09, pages 29?32, Paris, France.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In International Conference on Spoken
Language Processing, Denver, Colorado.
D. Wu. 1995. An algorithm for simultaneously bracket-
ing parallel texts by aligning words. In Proceedings of
WVLC-3, pages 69?82, Cambridge, Massachusetts.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL/HLT
2008, pages 97?105, Columbus, Ohio.
344
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 220?229,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
MEANT: An inexpensive, high-accuracy, semi-automatic metric for
evaluating translation utility via semantic frames
Chi-kiu Lo and DekaiWu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo,dekai}@cs.ust.hk
Abstract
We introduce a novel semi-automated metric,
MEANT, that assesses translation utility by match-
ing semantic role fillers, producing scores that cor-
relate with human judgment as well as HTER but
at much lower labor cost. As machine transla-
tion systems improve in lexical choice and flu-
ency, the shortcomings of widespread n-gram based,
fluency-oriented MT evaluation metrics such as
BLEU, which fail to properly evaluate adequacy,
become more apparent. But more accurate, non-
automatic adequacy-oriented MT evaluation metrics
like HTER are highly labor-intensive, which bottle-
necks the evaluation cycle. We first show that when
using untrained monolingual readers to annotate se-
mantic roles in MT output, the non-automatic ver-
sion of the metric HMEANT achieves a 0.43 corre-
lation coefficient with human adequacy judgments at
the sentence level, far superior to BLEU at only 0.20,
and equal to the far more expensive HTER. We then
replace the human semantic role annotators with au-
tomatic shallow semantic parsing to further automate
the evaluation metric, and show that even the semi-
automated evaluation metric achieves a 0.34 corre-
lation coefficient with human adequacy judgment,
which is still about 80% as closely correlated as
HTER despite an even lower labor cost for the evalu-
ation procedure. The results show that our proposed
metric is significantly better correlated with human
judgment on adequacy than current widespread au-
tomatic evaluation metrics, while being much more
cost effective than HTER.
1 Introduction
In this paper we show that evaluating machine trans-
lation by assessing the translation accuracy of each argu-
ment in the semantic role framework correlates with hu-
man judgment on translation adequacy as well as HTER,
at a significantly lower labor cost. The correlation of this
new metric, MEANT, with human judgment is far supe-
rior to BLEU and other automatic n-gram based evalua-
tion metrics.
We argue that BLEU (Papineni et al, 2002) and other
automatic n-gram basedMT evaluation metrics do not ad-
equately capture the similarity in meaning between the
machine translation and the reference translation?which,
ultimately, is essential for MT output to be useful. N-
gram based metrics assume that ?good? translations tend
to share the same lexical choices as the reference trans-
lations. While BLEU score performs well in captur-
ing the translation fluency, Callison-Burch et al (2006)
and Koehn and Monz (2006) report cases where BLEU
strongly disagree with human judgment on translation
quality. The underlying reason is that lexical similarity
does not adequately reflect the similarity in meaning. As
MT systems improve, the shortcomings of the n-gram
based evaluation metrics are becoming more apparent.
State-of-the-art MT systems are often able to output flu-
ent translations that are nearly grammatical and contain
roughly the correct words, but still fail to express mean-
ing that is close to the input.
At the same time, although HTER (Snover et al, 2006)
is more adequacy-oriented, it is only employed in very
large scale MT system evaluation instead of day-to-day
research activities. The underlying reason is that it re-
quires rigorously trained human experts to make difficult
combinatorial decisions on the minimal number of edits
so as to make the MT output convey the same meaning as
the reference translation?a highly labor-intensive, costly
process that bottlenecks the evaluation cycle.
Instead, with MEANT, we adopt at the outset the
principle that a good translation is one that is useful,
in the sense that human readers may successfully un-
derstand at least the basic event structure??who did
what to whom, when, where and why? (Pradhan et al,
2004)?representing the central meaning of the source ut-
terances. It is true that limited tasks might exist for which
inadequate translations are still useful. But for meaning-
ful tasks, generally speaking, for a translation to be use-
ful, at least the basic event structure must be correctly un-
derstood. Therefore, our objective is to evaluate trans-
lation utility: from a user?s point of view, how well is220
the most essential semantic information being captured
by machine translation systems?
In this paper, we detail the methodology that underlies
MEANT, which extends and implements preliminary di-
rections proposed in (Lo andWu, 2010a) and (Lo andWu,
2010b). We present the results of evaluating translation
utility by measuring the accuracy within a semantic role
labeling (SRL) framework. We show empirically that our
proposed SRL based evaluation metric, which uses un-
trained monolingual humans to annotate semantic frames
inMT output, correlates with human adequacy judgments
as well as HTER, and far better than BLEU and other
commonly used metrics. Finally, we show that replacing
the human semantic role labelers with an automatic shal-
low semantic parser in our proposed metric yields an ap-
proximation that is about 80% as closely correlated with
human judgment as HTER, at an even lower cost?and
is still far better correlated than n-gram based evaluation
metrics.
2 Related work
Lexical similarity based metrics BLEU (Papineni et
al., 2002) is the most widely used MT evaluation met-
ric despite the fact that a number of large scale meta-
evaluations (Callison-Burch et al, 2006; Koehn and
Monz, 2006) report cases where it strongly disagree with
human judgment on translation accuracy. Other lexi-
cal similarity based automatic MT evaluation metrics,
like NIST (Doddington, 2002), METEOR (Banerjee and
Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch
et al, 2006) and WER (Nie?en et al, 2000), also per-
form well in capturing translation fluency, but share the
same problem that although evaluation with these metrics
can be done very quickly at low cost, their underlying as-
sumption?that a ?good? translation is one that shares the
same lexical choices as the reference translation?is not
justified semantically. Lexical similarity does not ade-
quately reflect similarity in meaning. State-of-the-art MT
systems are often able to output translations containing
roughly the correct words, yet expressing meaning that is
not close to that of the input.
We argue that a translation metric that reflects meaning
similarity is better based on similarity in semantic struc-
ture, rather than simply flat lexical similarity.
HTER (non-automatic) Despite the fact that Human-
targeted Translation Edit Rate (HTER) as proposed by
Snover et al (2006) shows a high correlation with human
judgment on translation adequacy, it is not widely used in
day-to-day machine translation evaluation because of its
high labor cost. HTER not only requires human experts
to understand the meaning expressed in both the refer-
ence translation and the machine translation, but also re-
quires them to propose the minimum number of edits to
the MT output such that the post-edited MT output con-
veys the same meaning as the reference translation. Re-
quiring such heavy manual decision making greatly in-
creases the cost of evaluation, bottlenecking the evalua-
tion cycle.
To reduce the cost of evaluation, we aim to reduce any
human decisions in the evaluation cycle to be as simple
as possible, such that even untrained humans can quickly
complete the evaluation. The human decisions should
also be defined in a way that can be closely approximated
by automatic methods, so that similar objective functions
might potentially be used for tuning in MT system devel-
opment cycles.
Task based metrics (non-automatic) Voss and Tate
(2006) proposed a task-based approach to MT evaluation
that is in some ways similar in spirit to ours, but rather
than evaluating how well people understand the mean-
ing as a whole conveyed by a sentence translation, they
measured the recall with which humans can extract one of
the who, when, or where elements from MT output?and
without attaching them to any predicate or frame. A
large number of human subjects were instructed to extract
only one particular type of wh-item from each sentence.
They evaluated only whether the role fillers were cor-
rectly identified, without checking whether the roles were
appropriately attached to the correct predicate. Also, the
actor, experiencer, and patient were all conflated into the
undistinguished who role, while other crucial elements,
like the action, purpose, manner, were ignored.
Instead, we argue, evaluating meaning similarity
should be done by evaluating the semantic structure as
a whole: (a) all core semantic roles should be checked,
and (b) not only should we evaluate the presence of se-
mantic role fillers in isolation, but also their relations to
the frames? predicates.
Syntax based metrics Unlike Voss and Tate, Liu and
Gildea (2005) proposed a structural approach, but it was
based on syntactic rather than semantic structure, and fo-
cused on checking the correctness of the role structure
without checking the correctness of the role fillers. Their
subtree metric (STM) and headword chain metric (HWC)
address the failure of BLEU to evaluate translation gram-
maticality; however, the problem remains that a gram-
matical translation can achieve a high syntax-based score
even if contains meaning errors arising from confusion of
semantic roles.
STM was the first proposed metric to incorporate syn-
tactic features in MT evaluation, and STM underlies most
other recently proposed syntactic MT evaluation met-
rics, for example the evaluation metric based on lexical-
functional grammar of Owczarzak et al (2008). STM is
a precision-based metric that measures what fraction of
subtree structures are shared between the parse trees of221
machine translations and reference translations (averag-
ing over subtrees up to some depth threshold). Unlike
Voss and Tate, however, STM does not check whether the
role fillers are correctly translated.
HWC is similar, but is based on dependency trees con-
taining lexical as well as syntactic information. HWC
measures what fraction of headword chains (a sequence
of words corresponding to a path in the dependency tree)
also appear in the reference dependency tree. This can be
seen as a similarity measure on n-grams of dependency
chains. Note that the HWC?s notion of lexical similarity
still requires exact word match.
Although STM-like syntax-based metrics are an im-
provement over flat lexical similarity metrics like BLEU,
they are still more fluency-oriented than adequacy-
oriented. Similarity of syntactic rather than semantic
structure still inadequately reflects meaning preservation.
Moreover, properly measuring translation utility requires
verifying whether role fillers have been correctly trans-
lated?verifying only the abstract structures fails to pe-
nalize when role fillers are confused.
Semantic roles as features in aggregate metrics
Gime?nez and Ma`rquez (2007, 2008) introduced ULC, an
automatic MT evaluation metric that aggregates many
types of features, including several shallow semantic sim-
ilarity features: semantic role overlapping, semantic role
matching, and semantic structure overlapping. Unlike Liu
and Gildea (2007) who use discriminative training to tune
the weight on each feature, ULC uses uniform weights.
Although the metric shows an improved correlation with
human judgment of translation quality (Callison-Burch et
al., 2007; Gime?nez and Ma`rquez, 2007; Callison-Burch
et al, 2008; Gime?nez and Ma`rquez, 2008), it is not com-
monly used in large-scale MT evaluation campaigns, per-
haps due to its high time cost and/or the difficulty of in-
terpreting its score because of its highly complex combi-
nation of many heterogenous types of features.
Specifically, note that the feature based representations
of semantic roles used in these aggregate metrics do not
actually capture the structural predicate-argument rela-
tions. ?Semantic structure overlapping? can be seen as
the shallow semantic version of STM: it only measures
the similarity of the tree structure of the semantic roles,
without considering the lexical realization. ?Semantic
role overlapping? calculates the degree of lexical overlap
between semantic roles of the same type in the machine
translation and its reference translation, using simple bag-
of-words counting; this is then aggregated into an average
over all semantic role types. ?Semantic role matching?
is just like ?semantic role overlapping?, except that bag-
of-words degree of similarity is replaced (rather harshly)
by a boolean indicating whether the role fillers are an ex-
act string match. It is important to note that ?semantic
role overlapping? and ?semantic role matching? both use
flat feature based representations which do not capture the
structural relations in semantic frames, i.e., the predicate-
argument relations.
Like system combination approaches, ULC is a vastly
more complex aggregate metric compared to widely used
metrics like BLEU or STM. We believe it is important
to retain a focus on developing simpler metrics which
not only correlate well with human adequacy judgments,
but nevertheless still directly provide representational
transparency via simple, clear, and transparent scoring
schemes that are (a) easily human readable to support er-
ror analysis, and (b) potentially directly usable for auto-
matic credit/blame assignment in tuning tree-structured
SMT systems. We also believe that to provide a foun-
dation for better design of efficient automated metrics,
making use of humans for annotating semantic roles and
judging the role translation accuracy in MT output is an
essential step that should not be bypassed, in order to ade-
quately understand the upper bounds of such techniques.
We agree with Przybocki et al (2010), who observe
in the NIST MetricsMaTr 2008 report that ?human [ade-
quacy] assessments only pertain to the translations evalu-
ated, and are of no use even to updated translations from
the same systems?. Instead, we aim for MT evaluation
metrics that provide fine-grained scores in a way that also
directly reflects interpretable insights on the strengths and
weaknesses of MT systems rather than simply replicating
human assessments.
3 MEANT: SRL for MT evaluation
A good translation is one from which human readers
may successfully understand at least the basic event struc-
ture??who did what to whom, when, where and why?
(Pradhan et al, 2004)?which represents the most essen-
tial meaning of the source utterances.
MEANT measures this as follows. First, semantic role
labeling is performed (either manually or automatically)
on both the reference translation and the machine transla-
tion. The semantic frame structures thus obtained for the
MT output are compared to those in the reference transla-
tions, frame by frame, argument by argument. The frame
translation accuracy is a weighted sum of the number of
correctly translated arguments. Conceptually, MEANT
is defined in terms of f-score, with respect to the preci-
sion/recall for sentence translation accuracy as calculated
by averaging the translation accuracy for all frames in the
MT output across the number of frames in the MT out-
put/reference translations. Details are given below.
3.1 Annotating semantic frames
In designing a semantic MT evaluation metric, one im-
portant issue that should be addressed is how to evaluate
the similarity of meaning objectively and systematically222
Figure 1: Example of source sentence and reference translation with reconstructed semantic frames in Propbank format and MT
output with reconstructed semantic frames by minimal trained human annotators. Following Propbank, there are no semantic frames
for MT3 because there is no predicate.
using fine-grained measures. We adopted the Propbank
SRL style predicate-argument framework, which captures
the basic event structure in a sentence in a way that clearly
indicates many strengths and weaknesses of MT. Figure 1
shows the reference translationwith reconstructed seman-
tic frames in Propbank format and the corresponding MT
output with reconstructed semantic frames by minimal
trained human annotators.
3.2 Comparing semantic frames
After annotating the semantic frames, we must deter-
mine the translation accuracy for each semantic role filler
in the reference and machine translations. Although ulti-
mately it would be nice to do this automatically, it is es-
sential to first understand extremely well the upper bound
of accuracy for MT evaluation via semantic frame theory.
Thus, instead of resorting to excessively permissive bag-
of-words matching or excessively restrictive exact string
matching, for the experiments reported here we employed
a group of human judges to evaluate the correctness of
each role filler translation between the reference and ma-
chine translations.
In order to facilitate a finer-grained measurement of
utility, the human judges were not only allowed to mark
each role filler translation as ?correct? or ?incorrect?, but
also ?partial?. Translations of role fillers are judged ?cor-
rect? if they express the same meaning as that of the refer-
ence translations (or the original source input, in the bilin-
guals experiment discussed later). Translations may also
be judged ?partial? if only part of the meaning is correctly
translated. Extra meaning in a role filler is not penalized
unless it belongs in another role. We also assume that a
wrongly translated predicate means that the entire seman-
tic frame is incorrect; therefore, the ?correct? and ?par-
tial? argument counts are collected only if their associated
predicate is correctly translated in the first place.
Table 1 shows an example of SRL annotation of MT1
in Figure 1 by one of the annotators, along with the human
judgment on translation accuracy of each argument. The
predicate ceased in the reference translation did not match
with any predicate annotated in MT1, while the predicate
resumed matched with the predicate resume annotated in
MT1. All arguments of the untranslated ceased are auto-
matically considered incorrect (with no need to consider
each argument individually), under our assumption that a
wrongly translated predicate causes the entire event frame
to be considered mistranslated. The ARGM-TMP argu-
ment, Until after their sales had ceased in mainland China for
almost two months, in the reference translation is partially
translated to ARGM-TMP argument, So far , nearly two
months, inMT1. Similar decisions are made for the ARG1
argument and the other ARGM-TMP argument; now in
the reference translation is missing in MT1.
3.3 Quantifying semantic frame match
To quantify the above in a summary metric, we define
MEANT in terms of an f-score that balances the precision
and recall analysis of the comparative matrices collected
from the human judges, as follows.
Ci,j = # correct fillers of ARG j for PRED i in MT
Pi,j = # partial fillers of ARG j for PRED i in MT
Mi,j = total # fillers of ARG j for PRED i in MT
Ri,j = total # fillers of ARG j of PRED i in REF223
Table 1: SRL annotation of MT1 in Figure 1 and the human judgment of translation accuracy for each argument (see text).
SRL REF MT1 Decision
PRED (Action) ceased ? no match
PRED (Action) resumed resume match
ARG0 (Agent) ? sk - ii the sale of products in
the mainland of China
incorrect
ARG1 (Experiencer) sales of complete range of SK - II products sales partial
ARGM-TMP (Temporal) Until after , their sales had ceased in mainland
China for almost two months
So far , nearly two months partial
ARGM-TMP (Temporal) now ? incorrect
Cprecision =
?
matched i
wpred +
?
j wjCi,j
wpred +
?
j wjMi,j
Crecall =
?
matched i
wpred +
?
j wjCi,j
wpred +
?
j wjRi,j
Pprecision =
?
matched i
?
j wjPi,j
wpred +
?
j wjMi,j
Precall =
?
matched i
?
j wjPi,j
wpred +
?
j wjRi,j
precision = Cprecision + (wpartial ? Pprecision)total # predicates in MT
recall = Crecall + (wpartial ? Precall)total # predicates in REF
f-score = 2 ? precision ? recallprecision+ recall
Cprecision, Pprecision, Crecall and Precall are the sum of the
fractional counts of correctly or partially translated se-
mantic frames in theMT output and the reference, respec-
tively, which can be viewed as the true positive for pre-
cision and recall of the whole semantic structure in one
source utterence. Therefore, the SRL based MT evalua-
tion metric is equivalent to the f-score, i.e., the translation
accuracy for the whole predicate-argument structure.
Note that wpred, wj and wpartial are the weights for the
matched predicate, arguments of type j, and partial trans-
lations. These weights can be viewed as the importance
of meaning preservation for each different category of se-
mantic roles, and the penalty for partial translations. We
will describe below how these weights are estimated.
If all the reconstructed semantic frames in the MT out-
put are completely identical to those annotated in the ref-
erence translation, and all the arguments in the recon-
structed frames express the same meaning as the corre-
sponding arguments in the reference translations, then the
f-score will be equal to 1.
For instance, consider MT1 in Figure 1. The number
of frames in MT1 and the reference translation are 1 and
2, respectively. The total number of participants (includ-
ing both predicates and arguments) of the resume frame
in both MT1 and the reference translation is 4 (one pred-
icate and three arguments), with 2 of the arguments (one
ARG1/experiencer and one ARGM-TMP/temporal) only
partially translated. Assuming for now that the metric ag-
gregates ten types of semantic roles with uniform weight
for each role (optimization of weights will be discussed
later), then wpred = wj = 0.1, and so Cprecision and Crecall
are both zero while Pprecision and Precallare both 0.5. If we
further assume thatwpartial = 0.5, then precison and recall
are 0.25 and 0.125 respectively. Thus the f-score for this
example is 0.17.
Both human and semi-automatic variants of the
MEANT translation evaluation metric were meta-
evaluated, as described next.
4 Meta-evaluation methodology
4.1 Evaluation Corpus
We leverage work from Phase 2.5 of the DARPA
GALE program in which both a subset of the Chinese
source sentences, as well as their English reference, are
being annotated with semantic role labels in Propbank
style. The corpus also includes three participating state-
of-the-art MT systems? output. For present purposes, we
randomly drew 40 sentences from the newswire genre of
the corpus to form a meta-evaluation corpus. To maintain
a controlled environment for experiments and consistent
comparison, the evaluation corpus is fixed throughout this
work.
4.2 Correlation with human judgements on
adequacy
We followed the benchmark assessment procedure in
WMT and NIST MetricsMaTr (Callison-Burch et al,
2008, 2010), assessing the performance of the proposed
evaluation metric at the sentence level using ranking pref-
erence consistency, which also known as Kendall?s ? rank
correlation coefficient, to evaluate the correlation of the
proposed metric with human judgments on translation ad-
equacy ranking. A higher value for ? indicates more simi-
larity to the ranking by the evaluation metric to the human
judgment. The range of possible values of correlation co-
efficient is [-1,1], where 1 means the systems are ranked224
Table 2: List of semantic roles that human judges are requested
to label.
Label Event Label Event
Agent who Location where
Action did Purpose why
Experiencer what Manner how
Patient whom Degree or Extent how
Temporal when Other adverbial arg. how
in the same order as the human judgment and -1 means
the systems are ranked in the reverse order as the human
judgment.
5 Experiment: Using human SRL
The first experiment aims to provide a more concrete
understanding of one of the key questions as to the upper
bounds of the proposed evaluation metric: how well can
human annotators perform in reconstructing the semantic
frames in MT output? This is important since MT out-
put is still not close to perfectly grammatical for a good
syntactic parsing?applying automatic shallow semantic
parsers, which are trained on grammatical input and valid
syntactic parse trees, on MT output may significantly un-
derestimate translation utility.
5.1 Experimental setup
We thus introduce HMEANT, a variant of MEANT
based on the idea that semantic role labeling can be sim-
plified into a task that is easy and fast even for untrained
humans. The human annotators are given only very sim-
ple instructions of less than half a page, along with two
examples. Table 2 shows the list of labels annotators are
requested to annotate, where the semantic role labeling
instructions are given in the intuitive terms of ?who did
what to whom, when, where, why and how?. To facili-
tate the inter-annotator agreement experiments discussed
later, each sentence is independently assigned to at least
two annotators.
After calculating the SRL scores based on the confu-
sion matrix collected from the annotation and evaluation,
we estimate the weights using grid search to optimize cor-
relation with human adequacy judgments.
5.2 Results: Correlation with human judgement
Table 3 shows results indicating that HMEANT corre-
lates with human judgment on adequacy as well as HTER
does (0.432), and is far superior to BLEU (0.198) or other
surface-oriented metrics.
Inspection of the cross validation results shown in Ta-
ble 4 indicates that the estimated weights are not over-
fitting. Recall that the weights used in HMEANT are
globally estimated (by grid search) using the evaluation
Table 3: Sentence-level correlation with human adequacy judg-
ments, across the evaluation metrics.
Metrics Kendall ?
HMEANT 0.4324
HTER 0.4324
NIST 0.2883
BLEU 0.1982
METEOR 0.1982
TER 0.1982
PER 0.1982
CDER 0.1171
WER 0.0991
Table 4: Analysis of stability for HMEANT?s weight settings,
withRHMEANT rank and Kendall?s ? correlation scores (see text).
Fold 0 Fold 1 Fold 2 Fold 3
RHMEANT 3 1 3 5
distinct R 16 29 19 17
?HMEANT 0.33 0.48 0.48 0.40
?HTER 0.59 0.41 0.44 0.30
?CV train 0.45 0.42 0.40 0.43
?CV test 0.33 0.37 0.48 0.40
corpus. To analyze stability, the corpus is also parti-
tioned randomly into four folds of equal size. For each
fold, another grid search is also run. RHMEANT is the
rank at which the Kendall?s correlation for HMEANT
is found, if the Kendall?s correlations for all points in
the grid search space are sorted. Many similar weight-
vectors produce the same Kendall?s correlation score, so
?distinct R? shows how many distinct Kendall?s corre-
lation scores exist in each case?between 16 and 29.
HMEANT?s weight settings always produce Kendall?s
correlation scores among the top 5, regardless of which
fold is chosen, indicating good stability of HMEANT?s
weight-vector.
Next, Kendall?s ? correlation scores are shown for
HMEANT on each fold. They vary from 0.33 to 0.48,
and are at least as stable as those shown for HTER, where
? varies from 0.30 to 0.59.
Finally, ?CV shows Kendall?s correlations if the weight-
vector is instead subjected to full cross-validation training
and testing, again demonstrating good stability. In fact,
the correlations for the training set in three of the folds (0,
2, and 3) are identical to those for HMEANT.
5.3 Results: Cost of evaluating
The time needed for training non-expert humans to
carry out our annotation protocol is significantly less than
HTER and gold standard Propbank annotation. The half-
page instructions given to annotators required only be-
tween 5 to 15 minutes for all annotators, including time225
for asking questions if necessary. Aside from providing
two annotated examples, no further training was given.
Similarly, the time needed for running the evaluation
metric is also significantly less than HTER?under at
most 5 minutes per sentence, even for non-expert humans
using no computer-assisted UI tools. The average time
used for annotating each sentence was lower bounded by
2 minutes and upper bounded by 3 minutes, and the time
used for determing the translation accuracy of role fillers
averaged under 2 minutes.
Note that these figures are for unskilled non-experts.
These times tend to diminish significantly after annotators
acquire experience.
6 Experiment: Monolinguals vs. bilinguals
We now show that using monolingual annotators is es-
sentially just as effective as using more expensive bilin-
gual annotators. We study the cost/benefit trade-off of
using human annotators from different language back-
grounds for the proposed evaluation metric, and compare
whether providing the original source text helps. Note
that this experiment focuses on the SRL annotation step,
rather than the judgments of role filler paraphrasing accu-
racy, because the latter is only a simple three-way deci-
sion between ?correct?, ?partial?, and ?incorrect? that is
far less sensitive to the annotators? language backgrounds.
MT output is typically poor. Therefore, readers of
MT output often guess the original meaning in the source
input using their own language background knowledge.
Readers? language background thus affects their under-
standing of the translation, which could affect the accu-
racy of capturing the key semantic roles in the translation.
6.1 Experimental Setup
Both English monolinguals and Chinese-English bilin-
guals (Chinese as first language and English as second
language) were employed to annotate the semantic roles.
For bilinguals, we also experimented with the difference
in guessing constraints by optionally providing the origi-
nal source input together with the translation. Therefore,
there are three variations in the experiment setup: mono-
linguals seeing translation output only; bilinguals seeing
translation output only; and bilinguals seeing both input
and output.
The aim here is to do a rough sanity check on the effect
of the variation of language background of the annotators;
thus for these experiments we have not run the weight es-
timation step after SRL based f-score calculation. Instead,
we simply assigned a uniform weight to all the seman-
tic elements, and evaluated the variation under the same
weight settings. (The correlation scores reported in this
section are thus expected to be lower than that reported in
the last section.)
Table 5: Sentence-level correlation with human adequacy judg-
ments, for monolinguals vs. bilinguals. Uniform rather than op-
timized weights are used.
Metrics Kendall ?
HMEANT - bilinguals 0.3514
HMEANT - monolinguals 0.3153
HMEANT - bilinguals with input 0.3153
6.2 Results
Table 5 of our results shows that using more expen-
sive bilinguals for SRL annotation instead of monolin-
guals improves the correlation only slightly. The cor-
relation coefficient of the SRL based evaluation metric
driven by bilingual human annotators (0.351) is slightly
better than that driven by monolingual human annotators
(0.315); however, using bilinguals in the evaluation pro-
cess is more costly than using monolinguals.
The results show that even allowing the bilinguals to
see the input as well as the translation output for SRL
annotation does not help the correlation. The correlation
coefficient of the SRL based evaluation metric driven by
bilingual human annotators who see also the source in-
put sentences is 0.315 which is the same as that driven by
monolingual human annotators. We find that the correla-
tion coefficient of the proposed with human judgment on
adequacy drops when bilinguals are shown to the source
input sentence during annotation. Error analyses lead
us to believe that annotators will drop some parts of the
meaning in the translations when trying to align them to
the source input.
This suggests that HMEANT requires only monolin-
gual English annotators, who can be employed at low
cost.
7 Inter-annotator agreement
One of the concerns of the proposed metric is that,
given only minimal training on the task, humans would
annotate the semantic roles so inconsistently as to reduce
the reliability of the evaluation metric. Inter-annotator
agreement (IAA) measures the consistency of human in
performing the annotation task. A high IAA suggests that
the annotation is consistent and the evaluation results are
reliable and reproducible.
To obtain a clear analysis on where any inconsistency
might lie, we measured IAA in two steps: role identifica-
tion and role classification.
7.1 Experimental setup
Role identification Since annotators are not consistent
in handling articles or punctuation at the beginning or
the end of the annotated arguments, the agreement of se-
mantic role identification is counted over the matching of226
Table 6: Inter-annotator agreement rate on role identification
(matching of word span)
Experiments REF MT
bilinguals working on output only 76% 72%
monolinguals working on output only 93% 75%
bilinguals working on input-output 75% 73%
Table 7: Inter-annotator agreement rate on role classification
(matching of role label associated with matched word span)
Experiments Ref MT
bilinguals working on output only 69% 65%
monolinguals working on output only 88% 70%
bilinguals working on input-output 70% 69%
word span in the annotated role fillers with a tolerance
of ?1 word in mismatch. The inter-annotator agreement
rate (IAA) on the role identification task is calculated as
follows. A1 andA2 denote the number of annotated pred-
icates and arguments by annotator 1 and annotator 2 re-
spectively. Mspan denotes the number of annotated pred-
icates and arguments with matching word span between
annotators.
Pidentification =
Mspan
A1
Ridentification =
Mspan
A2
IAAidentification =
2 ? Pidentification ?Ridentification
Pidentification +Ridentification
Role classification The agreement of classified roles
is counted over the matching of the semantic role labels
within two aligned word spans. The IAA on the role clas-
sification task is calculated as follows. Mlabel denotes
the number of annotated predicates and arguments with
matching role label between annotators.
Pclassification =
Mlabel
A1
Rclassification =
Mlabel
A2
IAAclassification =
2 ? Pclassification ?Rclassification
Pclassification +Rclassification
7.2 Results
The high inter-annotator agreement suggests that the
annotation instructions provided to the annotators are in
general sufficient and the evaluation is repeatable and
could be automated in the future. Table 6 and 7 show the
annotators reconstructed the semantic frames quite con-
sistently, even they were given only simple and minimal
training.
We have noticed that the agreement on role identifica-
tion is higher than that on role classification. This sug-
gests that there are role confusion errors among the an-
notators. We expect a slightly more detailed instructions
and explanations on different roles will further improve
the IAA on role classification.
The results also show that monolinguals seeing output
only have the highest IAA in semantic frame reconstruc-
tion. Data analyses lead us to believe the monolinguals
are the most constrained group in the experiments. The
monolingual annotators can only guess the meaning in
the MT output using their English language knowledge.
Therefore, they all understand the translation almost the
same way, even if the translation is incorrect.
On the other hand, bilinguals seeing both the input and
output discover the mistranslated portions, and often un-
consciously try to compensate by re-interpreting the MT
output with information not necessarily appearing in the
translation, in order to better annotate what they think
it should have conveyed. Since there are many degrees
of freedom in this sort of compensatory re-interpretation,
this group achieved a lower IAA than the monolinguals.
Bilinguals seeing only output appear to take this even a
step further: confrontedwith a poor translation, they often
unconsciously try to guess what the original input might
have been. Consequently, they agree the least, because
they have the most freedom in applying their own knowl-
edge of the unseen input language, when compensating
for poor translations.
8 Experiment: Using automatic SRL
In the previous experiment, we showed that the pro-
posed evaluation metric driven by human semantic role
annotators performed as well as HTER. It is now worth
asking a deeper question: can we further reduce the la-
bor cost of MEANT by using automatic shallow semantic
parsing instead of humans for semantic role labeling?
Note that this experiment focuses on understanding the
cost/benefit trade-off for the semantic frame reconstruc-
tion step. For SRL annotation, we replace humans with
automatic shallow semantic parsing. We decouple this
from the ternary judgments of role filler accuracy, which
are still made by humans. However, we believe the eval-
uation of role filler accuracy will also be automatable.
8.1 Experimental setup
We performed three variations of the experiments to
assess the performance degradation from the automatic
approximation of semantic frame reconstruction in each
translation (reference translation and MT output): we ap-
plied automatic shallow semantic parsing on the MT out-
put only; on the reference translation only; and on both
reference translation and MT output. For the semantic227
Table 8: Sentence-level correlation with human adequacy judg-
ments. *The weights for individual roles in the metric are tuned
by optimizing the correlation.
Metrics Kendall ?
HTER 0.4324
HMEANT gold - monolinguals * 0.4324
HMEANT auto - monolinguals * 0.3964
MEANT gold - auto * 0.3694
MEANT auto - auto * 0.3423
NIST 0.2883
BLEU / METEOR / TER / PER 0.1982
CDER 0.1171
WER 0.0991
parser, we used ASSERT (Pradhan et al, 2004) which
achieves roughly 87% semantic role labeling accuracy.
8.2 Results
Table 8 shows that the proposed SRL based evaluation
metric correlates slightly worse than HTER with a much
lower labor cost. The correlation with human judgment
on adequacy of the fully automated SRL annotation ver-
sion, i.e., applying ASSERT on both the reference transla-
tion and the MT output, of the SRL based evaluation met-
ric is about 80% of that of HTER. The results also show
that the correlation with human judgment on adequacy of
either one side of translation using automatic SRL is in
the 85% to 95% range of that HTER.
9 Conclusion
We have presented MEANT, a novel semantic MT
evaluation metric that assesses the translation accuracy
via Propbank-style semantic predicates, roles, and fillers.
MEANT provides an intuitive picture on how much in-
formation is correctly translated in the MT output.
MEANT can be run using inexpensive untrainedmono-
linguals and yet correlates with human judgments on ad-
equacy as well as HTER with a lower labor cost. In con-
trast to HTER, which requires rigorous training of human
experts to find aminimum edit of the translation (an expo-
nentially large search space), MEANT requires untrained
humans to make well-defined, bounded decisions on an-
notating semantic roles and judging translation correct-
ness. The process by which MEANT reconstructs the se-
mantic frames in a translation and then judges translation
correctness of the role fillers conceptually models how
humans read and understand translation output.
We also showed that using automatic shallow seman-
tic parser to further reduce the labor cost of the pro-
posed metric successfully approximates roughly 80% of
the correlation with human judgment on adequacy. The
results suggest future potential for a fully automatic vari-
ant of MEANT that could out-perform current automatic
MT evaluation metrics and still perform near the level of
HTER.
Numerous intriguing questions arise from this work. A
further investigation into the correlation of each of the in-
dividual roles to human adequacy judgments is detailed
elsewhere, along with additional improvements to the
MEANT family of metrics (Lo and Wu, 2011). Another
interesting investigation would then be to similarly repli-
cate this analysis of the impact of each individual role, but
using automatically rather than manually labeled seman-
tic roles, in order to ascertain whether the more difficult
semantic roles for automatic semantic parsers might also
correspond to the less important aspects of end-to-endMT
utility.
Acknowledgments
This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under GALE Contract Nos. HR0011-06-
C-0022 and HR0011-06-C-0023 and by the Hong
Kong Research Grants Council (RGC) research
grants GRF621008, GRF612806, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the views of the Defense Advanced
Research Projects Agency.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An Au-
tomatic Metric for MT Evaluation with Improved Cor-
relation with Human Judgments. In Proceedings of the
43th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in Machine Transla-
tion Research. In Proceedings of the 13th Conference
of the European Chapter of the Association for Compu-
tational Linguistics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. (Meta-) evalua-
tion of Machine Translation. In Proceedings of the 2nd
Workshop on Statistical Machine Translation, pages
136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. Further Meta-
evaluation of Machine Translation. In Proceedings of
the 3rd Workshop on Statistical Machine Translation,
pages 70?106, 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Pryzbocki, and Omar Zaidan.228
Findings of the 2010 Joint Workshop on Statistical Ma-
chine Translation andMetrics for Machine Translation.
In Proceedings of the Joint 5th Workshop on Statistical
Machine Translation and MetricsMATR, pages 17?53,
Uppsala, Sweden, 15-16 July 2010.
G. Doddington. Automatic Evaluation of Machine Trans-
lation Quality using N-gram Co-occurrence Statistics.
In Proceedings of the 2nd International Conference
on Human Language Technology Research (HLT-02),
pages 138?145, San Francisco, CA, USA, 2002. Mor-
gan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu?is Ma`rquez. Linguistic Features
for Automatic Evaluation of Heterogenous MT Sys-
tems. In Proceedings of the 2nd Workshop on Sta-
tistical Machine Translation, pages 256?264, Prague,
Czech Republic, June 2007. Association for Computa-
tional Linguistics.
Jesu?s Gime?nez and Llu?is Ma`rquez. A Smorgasbord of
Features for Automatic MT Evaluation. In Proceed-
ings of the 3rd Workshop on Statistical Machine Trans-
lation, pages 195?198, Columbus, OH, June 2008. As-
sociation for Computational Linguistics.
Philipp Koehn and Christof Monz. Manual and Auto-
matic Evaluation of Machine Translation between Eu-
ropean Languages. In Proceedings of the Workshop on
Statistical Machine Translation, pages 102?121, 2006.
Gregor Leusch, Nicola Ueffing, andHermannNey. CDer:
Efficient MT Evaluation Using Block Movements. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), 2006.
Ding Liu and Daniel Gildea. Syntactic Features for Eval-
uation of Machine Translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, page 25, 2005.
Ding Liu and Daniel Gildea. Source-Language Fea-
tures and Maximum Correlation Training for Machine
Translation Evaluation. In Proceedings of the 2007
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (NAACL-07),
2007.
Chi-kiu Lo and Dekai Wu. Evaluating machine transla-
tion utility via semantic role labels. In Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2010), pages 2873?2877, Malta, May
2010.
Chi-kiu Lo and Dekai Wu. Semantic vs. syntactic vs.
n-gram structure for machine translation evaluation.
In Dekai Wu, editor, Proceedings of SSST-4, Fourth
Workshop on Syntax and Structure in Statistical Trans-
lation (at COLING 2010), pages 52?60, Beijing, Aug
2010.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
22nd International Joint Conference on Artificial In-
telligence (IJCAI-11), Barcelona, Jul 2011. To appear.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. A Evaluation Tool forMachine Translation:
Fast Evaluation forMTResearch. InProceedings of the
2nd International Conference on Language Resources
and Evaluation (LREC-2000), 2000.
Karolina Owczarzak, Josef van Genabith, and AndyWay.
Evaluating machine translation with LFG dependen-
cies. Machine Translation, 21:95?119, 2008.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: A Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th An-
nualMeeting of the Association for Computational Lin-
guistics (ACL-02), pages 311?318, 2002.
Sameer Pradhan, WayneWard, Kadri Hacioglu, James H.
Martin, and Dan Jurafsky. Shallow Semantic Parsing
Using Support Vector Machines. In Proceedings of
the 2004 Conference on Human Language Technology
and the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-04), 2004.
Mark Przybocki, Kay Peterson, Se?bastien Bronsart, and
Gregory Sanders. The NIST 2008Metrics for Machine
Translation Challenge - Overview, Methodology, Met-
rics, and Results. Machine Tr, 23:71?103, 2010.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A Study of Trans-
lation Edit Rate with Targeted Human Annotation. In
Proceedings of the 7th Conference of the Association
for Machine Translation in the Americas (AMTA-06),
pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Arkaitz Zubiaga, and Hassan Sawaf. Accelerated
DP Based Search For Statistical Translation. In Pro-
ceedings of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH-97),
1997.
Clare R. Voss and Calandra R. Tate. Task-based Evalua-
tion of Machine Translation (MT) Engines: Measuring
HowWell People ExtractWho,When,Where-Type El-
ements in MT Output. In Proceedings of the 11th An-
nual Conference of the European Association for Ma-
chine Translation (EAMT-2006), pages 203?212, Oslo,
Norway, June 2006.
229
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375?381,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving machine translation by training against
an automatic semantic frame based evaluation metric
Chi-kiu Lo and Karteek Addanki and Markus Saers and Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|vskaddanki|masaers|dekai}@cs.ust.hk
Abstract
We present the first ever results show-
ing that tuning a machine translation sys-
tem against a semantic frame based ob-
jective function, MEANT, produces more
robustly adequate translations than tun-
ing against BLEU or TER as measured
across commonly used metrics and human
subjective evaluation. Moreover, for in-
formal web forum data, human evalua-
tors preferredMEANT-tuned systems over
BLEU- or TER-tuned systems by a sig-
nificantly wider margin than that for for-
mal newswire?even though automatic se-
mantic parsing might be expected to fare
worse on informal language. We argue
that by preserving themeaning of the trans-
lations as captured by semantic frames
right in the training process, an MT sys-
tem is constrained to make more accu-
rate choices of both lexical and reorder-
ing rules. As a result, MT systems tuned
against semantic frame based MT evalu-
ation metrics produce output that is more
adequate. Tuning a machine translation
system against a semantic frame based ob-
jective function is independent of the trans-
lation model paradigm, so, any transla-
tion model can benefit from the semantic
knowledge incorporated to improve trans-
lation adequacy through our approach.
1 Introduction
We present the first ever results of tuning a statis-
tical machine translation (SMT) system against a
semantic frame based objective function in order
to produce a more adequate output. We compare
the performance of our system with that of two
baseline SMT systems tuned against BLEU and
TER, the commonly used n-gram and edit distance
based metrics. Our system performs better than
the baseline across seven commonly used evalu-
ation metrics and subjective human evaluation on
adequacy. Surprisingly, tuning against a seman-
tic MT evaluation metric also significantly out-
performs the baseline on the domain of informal
web forum data wherein automatic semantic pars-
ing might be expected to fare worse. These results
strongly indicate that using a semantic frame based
objective function for tuning would drive develop-
ment of MT towards direction of higher utility.
Glaring errors caused by semantic role confu-
sion that plague the state-of-the-art MT systems
are a consequence of using fast and cheap lexi-
cal n-gram based objective functions like BLEU
to drive their development. Despite enforcing flu-
ency it has been established that these metrics do
not enforce translation utility adequately and often
fail to preservemeaning closely (Callison-Burch et
al., 2006; Koehn and Monz, 2006).
We argue that instead of BLEU, a metric that fo-
cuses on getting the meaning right should be used
as an objective function for tuning SMT so as to
drive continuing progress towards higher utility.
MEANT (Lo et al, 2012), is an automatic seman-
tic MT evaluation metric that measures similarity
between the MT output and the reference transla-
tion via semantic frames. It correlates better with
human adequacy judgment than other automatic
MT evaluation metrics. Since a high MEANT
score is contingent on correct lexical choices as
well as syntactic and semantic structures, we be-
lieve that tuning against MEANT would improve
both translation adequacy and fluency.
Incorporating semantic structures into SMT by
tuning against a semantic frame based evaluation
metric is independent of the MT paradigm. There-
fore, systems from different MT paradigms (such
as hierarchical, phrase based, transduction gram-
mar based) can benefit from the semantic informa-
tion incorporated through our approach.
375
2 Related Work
Relatively little work has been done towards bi-
asing the translation decisions of an SMT system
to produce adequate translations that correctly pre-
servewho did what to whom, when, where and why
(Pradhan et al, 2004). This is because the devel-
opment of SMT systemswas predominantly driven
by tuning against n-gram based evaluation met-
rics such as BLEU or edit distance based metrics
such as TER which do not sufficiently bias SMT
system?s decisions to produce adequate transla-
tions. Although there has been a recent surge of
work aimed towards incorporating semantics into
the SMT pipeline, none attempt to tune against a
semantic objective function. Below, we describe
some of the attempts to incorporate semantic in-
formation into the SMT and present a brief survey
on evaluation metrics that focus on rewarding se-
mantically valid translations.
Utilizing semantics in SMT In the past few
years, there has been a surge of work aimed at in-
corporating semantics into various stages of the
SMT. Wu and Fung (2009) propose a two-pass
model that reorders the MT output to match the
SRL of the input, which is too late to affect the
translation decisions made by the MT system dur-
ing decoding. In contrast, training against a se-
mantic objective function attempts to improve the
decoding search strategy by incorporating a bias
towards meaningful translations into the model in-
stead of postprocessing its results.
Komachi et al (2006) and Wu et al (2011) pre-
process the input sentence to match the verb frame
alternations in the output side. Liu and Gildea
(2010) and Aziz et al (2011) use input side SRL
to train a tree-to-string SMT system. Xiong et al
(2012) trained a discriminative model to predict
the position of the semantic roles in the output.
All these approaches are orthogonal to the present
question of whether to train toward a semantic ob-
jective function. Any of the above models could
potentially benefit from tuning with semantic met-
rics.
MT evaluation metrics As mentioned previ-
ously, tuning against n-gram based metrics such
as BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie,
2005) does not sufficiently drive SMT into mak-
ing decisions to produce adequate translations
that correctly preserve ?who did what to whom,
when, where and why?. In fact, a number of
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) report cases
where BLEU strongly disagrees with human judg-
ments of translation accuracy. Tuning against edit
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al, 2000), and TER
(Snover et al, 2006) also fails to sufficiently bias
SMT systems towards producing translations that
preserve semantic information.
We argue that an SMT system tuned against an
adequacy-oriented metric that correlates well with
human adequacy judgement produces more ade-
quate translations. For this purpose, we choose
MEANT, an automatic semantic MT evaluation
metric that focuses on getting the meaning right by
comparing the semantic structures of the MT out-
put and the reference. We briefly describe some
of the alternative semantic metrics below to justify
our choice.
ULC (Gim?nez and M?rquez, 2007, 2008) is
an aggregated metric that incorporates several se-
mantic similarity features and shows improved
correlation with human judgement on translation
quality (Callison-Burch et al, 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al, 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an MT system against
ULC perhaps due to its expensive running time.
Lambert et al (2006) did tune on QUEEN, a sim-
plified version of ULC that discards the seman-
tic features and is based on pure lexical features.
Although tuning on QUEEN produced slightly
more preferable translations than solely tuning on
BLEU, themetric does not make use of any seman-
tic features and thus fails to exploit any potential
gains from tuning to semantic objectives.
Although TINE (Rios et al, 2011) is an recall-
oriented automatic evaluation metric which aims
to preserve the basic event structure, no work has
been done towards tuning an SMT system against
it. TINE performs comparably to BLEU andworse
than METEOR on correlation with human ade-
quacy judgment.
In contrast to TINE, MEANT (Lo et al, 2012),
which is the weighted f-score over the matched se-
mantic role labels of the automatically aligned se-
mantic frames and role fillers, outperforms BLEU,
NIST, METEOR, WER, CDER and TER. This
makes it more suitable for tuning SMT systems to
produce much adequate translations.
376
newswire BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 29.85 8.84 52.10 55.42 67.88 55.67 58.40 0.1667
TER-tuned 25.37 6.56 48.26 51.24 66.18 52.58 56.96 0.1578
MEANT-tuned 25.91 7.81 50.15 53.60 67.76 54.56 58.61 0.1676
Table 1: Translation quality of MT system tuned against MEANT, BLEU and TER on newswire data
forum BLEU NIST METEOR no_syn METEOR WER CDER TER MEANT
BLEU-tuned 9.58 4.10 31.77 34.63 80.09 64.54 76.12 0.1711
TER-tuned 6.94 2.21 28.55 30.85 76.15 57.96 74.73 0.1539
MEANT-tuned 7.92 3.11 30.40 33.08 77.32 61.01 74.64 0.1727
Table 2: Translation quality of MT system tuned against MEANT, BLEU and TER on forum data
3 Tuning SMT against MEANT
We now show that using MEANT as an objec-
tive function to drive minimum error rate training
(MERT) of state-of-the-art MT systems improves
MT utility not only on formal newswire text, but
even on informal forum text, where automatic se-
mantic parsing is difficult.
Toward improving translation utility of state-of-
the-art MT systems, we chose to use a strong and
competitive system in the DARPA BOLT program
as our baseline. The baseline system is a Moses
hierarchical model trained on a collection of LDC
newswire and a small portion of Chinese-English
parallel web forum data, together with a 5-gram
language model. For the newswire experiment, we
used a collection of NIST 02-06 test sets as our de-
velopment set and NIST 08 test set for evaluation.
The development and test sets contain 6,331 and
1,357 sentences respectively with four references.
For the forum data experiment, the development
and test sets were a held-out subset of the BOLT
phase 1 training data. The development and test
sets contain 2,000 sentences and 1,697 sentences
with one reference.
We use ZMERT (Zaidan, 2009) to tune the base-
line because it is a widely used, highly competi-
tive, robust, and reliable implementation of MERT
that is also fully configurable and extensible with
regard to incorporating new evaluation metrics. In
this experiment, we use aMEANT implementation
along the lines described in Lo et al (2012).
In each experiment, we tune two contrastive
conventional 100-best MERT tuned baseline sys-
tems on both newswire and forum data genres; one
tuned against BLEU, an n-gram based evaluation
metric and the other using TER, an edit distance
based metric. As semantic role labeling is expen-
sive we only tuned using 10-best list for MEANT-
tuned system. Tuning against BLEU and TER took
around 1.5 hours and 5 hours per iteration respec-
tively whereas tuning against MEANT took about
1.6 hours per iteration.
4 Results
Of course, tuning against any metric would maxi-
mize the performance of the SMT system on that
particular metric, but would be overfitting. For
example, something would be seriously wrong
if tuning against BLEU did not yield the best
BLEU scores. A far more worthwhile goal would
be to bias the SMT system to produce adequate
translations while achieving the best scores across
all the metrics. With this as our objective, we
present the results of comparing MEANT-tuned
systems against the baselines as evaluated on com-
monly used automatic metrics and human ade-
quacy judgement.
Cross-evaluation using automatic metrics Ta-
bles 1 and 2 show that MEANT-tuned systems
achieve the best scores across all other metrics in
both newswire and forum data genres, when avoid-
ing comparison of the overfit metrics too similar to
the one the system was tuned on (the cells shaded
in grey in the table: NIST and METEOR are n-
gram based metrics, similar to BLEU while WER
and CDER are edit distance based metrics, similar
to TER). In the newswire domain, however, our
system achieves marginally lower TER score than
BLEU-tuned system.
Figure 1 shows an example where the MEANT-
tuned system produced a more adequate transla-
tion that accurately preserves the semantic struc-
ture of the input sentence than the two baseline
systems. The MEANT scores for the MT output
from the BLEU-, TER- and MEANT-tuned sys-
tems are 0.0635, 0.1131 and 0.2426 respectively.
Both the MEANT score and the human evaluators
rank the MT output from the MEANT-tuned sys-
377
Figure 1: Examples of machine translation output and the corresponding semantic parses from the [B]
BLEU-, [T] TER-and [M]MEANT-tuned systems together with [IN] the input sentence and [REF] the
reference translation. Note that the MT output of the BLEU-tuned system has no semantic parse output
by the automatic shallow semantic parser.
tem as the most adequate translation. In this exam-
ple, the MEANT-tuned system has translated the
two predicates ???? and ???? in the input sen-
tence into the correct form of the predicates ?at-
tack? and ?adopted? in theMT output, whereas the
BLEU-tuned system has translated both of them
incorrectly (translates the predicates into nouns)
and the TER-tuned system has correctly translated
only the first predicate (into ?seized?) and dropped
the second predicate. Moreover, for the frame ??
?? in the input sentence, the MEANT-tuned sys-
tem has correctly translated the ARG0 ??????
??? into ?Hamas militants? and the ARG1 ??
???? into ?Gaza?. However, the TER-tuned
system has dropped the predicate ???? so that
the corresponding arguments ?The Palestinian Au-
thority? and ?into a state of emergency? have all
been incorrectly associated with the predicate ??
? /seized?. This example shows that the transla-
tion adequacy of SMT has been improved by tun-
ing against MEANT because the MEANT-tuned
system is more accurately preserving the semantic
structure of the input sentence.
Our results show that MEANT-tuned system
maintains a balance between lexical choices and
word order because it performs well on n-gram
based metrics that reward lexical matching and
edit distance metrics that penalize incorrect word
order. This is not surprising as a high MEANT
score relies on a high degree of semantic structure
matching, which is contingent upon correct lexi-
cal choices as well as syntactic and semantic struc-
tures.
Human subjective evaluation In line with our
original objective of biasing SMT systems towards
producing adequate translations, we conduct a hu-
man evaluation to judge the translation utility of
the outputs produced by MEANT-, BLEU- and
TER-tuned systems. Following the manual eval-
uation protocol of Lambert et al (2006), we ran-
domly draw 150 sentences from the test set in each
domain to form the manual evaluation set. Table
3 shows the MEANT scores of the two manual
evaluation sets. In both evaluation sets, like in the
test sets, the output from the MEANT-tuned sys-
tem score slightly higher inMEANT than that from
the BLEU-tuned system and significantly higher
than that from the TER-tuned system. The output
of each tuned MT system along the input sentence
and the reference were presented to human evalu-
ators. Each evaluation set is ranked by two evalu-
ators for measuring inter-evaluator agreement.
Table 4 indicates that output of the MEANT-
tuned system is ranked adequate more frequently
compared to BLEU- and TER-tuned baselines for
both newswire and web forum genres. The inter-
378
newswire forum
BLEU-tuned 0.1564 0.1663
TER-tuned 0.1203 0.1453
MEANT-tuned 0.1633 0.1737
Table 3: MEANT scores of each system in the 150-
sentence manual evaluation set.
newswire forum
Eval 1 Eval 2 Eval 1 Eval 2
BLEU-tuned (B) 37 42 47 42
TER-tuned (T) 22 24 28 23
MEANT-tuned (M) 55 56 59 68
B=T 14 12 0 0
M=B 5 4 8 9
M=T 4 4 4 4
M=B=T 13 9 4 4
Table 4: No. of sentences ranked the most ade-
quate by human evaluators for each system.
H1 newswire forum
MEANT-tuned > BLEU-tuned 80% 95%
MEANT-tuned > TER-tuned 99% 99%
Table 5: Significance level of accepting the alter-
native hypothesis.
evaluator agreement is 84% and 70% for newswire
and forum data genres respectively.
We performed the right-tailed two proportion
significance test on human evaluation of the SMT
system outputs for both the genres. Table 5 shows
that the MEANT-tuned system generates more ad-
equate translations than the TER-tuned system at
the 99% significance level for both newswire and
web forum genres. The MEANT-tuned system is
ranked more adequate than the BLEU-tuned sys-
tem at the 95% significance level on the web fo-
rum genre and for the newswire genre the hypoth-
esis is accepted at a significance level of 80%.
The high inter-evaluator agreement and the signif-
icance tests confirm that MEANT-tuned system is
better at producing adequate translations compared
to BLEU- or TER-tuned systems.
Informal vs. formal text The results of table
4 and 5 also show that?surprisingly?the human
evaluators preferred MEANT-tuned system out-
put over BLEU-tuned and TER-tuned system out-
put by a far wider margin on the informal forum
text compared to the formal newswire text. The
MEANT-tuned system is better than both base-
lines at the 80% significance level for the formal
text genre. For the informal text genre, it per-
forms the two baselines at the 95% significance
level. Although one might expect an semantic
frame dependent metric such as MEANT to per-
form poorly on the domain of informal text, sur-
prisingly, it nonetheless significantly outperforms
the baselines at the task of generating adequate out-
put. This indicates that the design of the MEANT
evaluation metric is robust enough to tune an SMT
system towards adequate output on informal text
domains despite the shortcomings of automatic
shallow semantic parsing.
5 Conclusion
We presented the first ever results to demon-
strate that tuning an SMT system against MEANT
produces much adequate translation than tuning
against BLEU or TER, as measured across all
other commonly used metrics and human subjec-
tive evaluation. We also observed that tuning
against MEANT succeeds in producing adequate
output significantly more frequently even on the
informal text such as web forum data. By pre-
serving the meaning of the translations as captured
by semantic frames right in the training process,
an MT system is constrained to make more accu-
rate choices of both lexical and reordering rules.
The performance of our system as measured across
all commonly used metrics indicate that tuning
against a semantic MT evaluation metric does pro-
duce output which is adequate and fluent.
We believe that tuning onMEANTwould prove
equally useful for MT systems based on any
paradigm, especially where the model does not
incorporate semantic information to improve the
adequacy of the translations produced and using
MEANT as an objective function to tune SMT
would drive sustainable development of MT to-
wards the direction of higher utility.
Acknowledgment
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
ment no. 287658; and by the Hong Kong
Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
379
References
Wilker Aziz, Miguel Rios, and Lucia Specia. Shal-
low semantic trees for SMT. In Proceedings
of the Sixth Workshop on Statistical Machine
Translation (WMT2011), 2011.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?
72, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational Linguis-
tics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation. In
Proceedings of the 2nd Workshop on Statistical
Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Transla-
tion. In Proceedings of the 3rd Workshop on
Statistical Machine Translation, pages 70?106,
2008.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the
2nd International Conference on Human Lan-
guage Technology Research, pages 138?145,
San Diego, California, 2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic
features for automatic evaluation of heteroge-
nous MT systems. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 256?264, Prague, Czech Republic,
June 2007.
Jes?s Gim?nez and Llu?sM?rquez. A smorgasbord
of features for automaticMT evaluation. In Pro-
ceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198, Colum-
bus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation
between European Languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation (WMT-06), pages 102?121, 2006.
Mamoru Komachi, Yuji Matsumoto, and Masaaki
Nagata. Phrase reordering for statistical ma-
chine translation based on predicate-argument
structure. In Proceedings of the 3rd Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2006), 2006.
Patrik Lambert, Jes?s Gim?nez, Marta R Costa-
juss?, Enrique Amig?, Rafael E Banchs, Llu?s
M?rquez, and JAR Fonollosa. Machine Transla-
tion system development based on human like-
ness. In Spoken Language Technology Work-
shop, 2006. IEEE, pages 246?249. IEEE, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT Evaluation Using
Block Movements. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
06), 2006.
Ding Liu and Daniel Gildea. Semantic role fea-
tures for machine translation. In Proceedings of
the 23rd international conference on Computa-
tional Linguistics (COLING-10), 2010.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic MT Evaluation.
In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation (WMT2012), 2012.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International
Conference on Language Resources and Evalu-
ation (LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H.Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
In Proceedings of the 2004 Conference on Hu-
man Language Technology and the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL-04), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine:
A metric to assess mt adequacy. In Proceed-
380
ings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 116?122. Association
for Computational Linguistics, 2011.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In Proceedings of the 7th Conference
of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, Cam-
bridge, Massachusetts, August 2006.
Dekai Wu and Pascale Fung. Semantic Roles for
SMT: A Hybrid Two-Pass Model. In Proceed-
ings of the 2009 Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics - Human Language Technolo-
gies (NAACL-HLT-09), pages 13?16, 2009.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Ha-
jime Tsukada, and Masaaki Nagata. Extract-
ing preordering rules from predicate-argument
structures. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP-11), 2011.
Deyi Xiong, Min Zhang, and Haizhou Li. Mod-
eling the Translation of Predicate-Argument
Structure for SMT. In Proceedings of the Joint
conference of the 50th AnnualMeeting of the As-
sociation for Computational Linguistics (ACL-
12), 2012.
Omar F. Zaidan. Z-MERT: A Fully Config-
urable Open Source Tool for Minimum Error
Rate Training of Machine Translation Systems.
The Prague Bulletin of Mathematical Linguis-
tics, 91:79?88, 2009.
381
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765?771,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
XMEANT: Better semantic MT evaluation without reference translations
Lo, Chi-kiu Beloucif, Meriem Saers, Markus Wu, Dekai
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce XMEANT?a new cross-lingual
version of the semantic frame based MT
evaluation metric MEANT?which can cor-
relate even more closely with human ade-
quacy judgments than monolingual MEANT
and eliminates the need for expensive hu-
man references. Previous work established
that MEANT reflects translation adequacy
with state-of-the-art accuracy, and optimiz-
ing MT systems against MEANT robustly im-
proves translation quality. However, to go
beyond tuning weights in the loglinear SMT
model, a cross-lingual objective function that
can deeply integrate semantic frame crite-
ria into the MT training pipeline is needed.
We show that cross-lingual XMEANT out-
performs monolingual MEANT by (1) replac-
ing the monolingual context vector model in
MEANT with simple translation probabilities,
and (2) incorporating bracketing ITG con-
straints.
1 Introduction
We show that XMEANT, a new cross-lingual ver-
sion of MEANT (Lo et al, 2012), correlates with
human judgment even more closely than MEANT
for evaluating MT adequacy via semantic frames,
despite discarding the need for expensive human
reference translations. XMEANT is obtained by
(1) using simple lexical translation probabilities,
instead of the monolingual context vector model
used in MEANT for computing the semantic role
fillers similarities, and (2) incorporating bracket-
ing ITG constrains for word alignment within the
semantic role fillers. We conjecture that the rea-
son that XMEANT correlates more closely with
human adequacy judgement than MEANT is that
on the one hand, the semantic structure of the
MT output is closer to that of the input sentence
than that of the reference translation, and on the
other hand, the BITG constraints the word align-
ment more accurately than the heuristic bag-of-
word aggregation used in MEANT. Our results
suggest that MT translation adequacy is more ac-
curately evaluated via the cross-lingual semantic
frame similarities of the input and the MT output
which may obviate the need for expensive human
reference translations.
The MEANT family of metrics (Lo and Wu,
2011a, 2012; Lo et al, 2012) adopt the princi-
ple that a good translation is one where a human
can successfully understand the central meaning
of the foreign sentence as captured by the basic
event structure: ?who did what to whom, when,
where and why? (Pradhan et al, 2004). MEANT
measures similarity between the MT output and
the reference translations by comparing the simi-
larities between the semantic frame structures of
output and reference translations. It is well estab-
lished that the MEANT family of metrics corre-
lates better with human adequacy judgments than
commonly used MT evaluation metrics (Lo and
Wu, 2011a, 2012; Lo et al, 2012; Lo and Wu,
2013b; Mach?a?cek and Bojar, 2013). In addition,
the translation adequacy across different genres
(ranging from formal news to informal web fo-
rum and public speech) and different languages
(English and Chinese) is improved by replacing
BLEU or TER with MEANT during parameter
tuning (Lo et al, 2013a; Lo and Wu, 2013a; Lo
et al, 2013b).
In order to continue driving MT towards better
translation adequacy by deeply integrating seman-
tic frame criteria into theMT training pipeline, it is
necessary to have a cross-lingual semantic objec-
tive function that assesses the semantic frame sim-
ilarities of input and output sentences. We there-
fore propose XMEANT, a cross-lingual MT evalu-
ation metric, that modifies MEANT using (1) sim-
ple translation probabilities (in our experiments,
765
from quick IBM-1 training), to replace the mono-
lingual context vector model in MEANT, and (2)
constraints from BITGs (bracketing ITGs). We
show that XMEANT assesses MT adequacy more
accurately than MEANT (as measured by correla-
tion with human adequacy judgement) without the
need for expensive human reference translations in
the output language.
2 Related Work
2.1 MT evaluation metrics
Surface-form oriented metrics such as BLEU (Pa-
pineni et al, 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), CDER
(Leusch et al, 2006), WER (Nie?en et al, 2000),
and TER (Snover et al, 2006) do not correctly re-
flect the meaning similarities of the input sentence.
In fact, a number of large scale meta-evaluations
(Callison-Burch et al, 2006; Koehn and Monz,
2006) report cases where BLEU strongly dis-
agrees with human judgments of translation ade-
quacy.
This has caused a recent surge of work to de-
velop better ways to automatically measure MT
adequacy. Owczarzak et al (2007a,b) improved
correlation with human fluency judgments by us-
ing LFG to extend the approach of evaluating syn-
tactic dependency structure similarity proposed by
Liu and Gildea (2005), but did not achieve higher
correlation with human adequacy judgments than
metrics like METEOR. TINE (Rios et al, 2011) is
a recall-oriented metric which aims to preserve the
basic event structure but it performs comparably
to BLEU and worse than METEOR on correlation
with human adequacy judgments. ULC (Gim?enez
and M`arquez, 2007, 2008) incorporates several
semantic features and shows improved correla-
tion with human judgement on translation quality
(Callison-Burch et al, 2007, 2008) but no work
has been done towards tuning an SMT system us-
ing a pure form of ULC perhaps due to its expen-
sive run time. Similarly, SPEDE (Wang and Man-
ning, 2012) predicts the edit sequence for match-
ing the MT output to the reference via an inte-
grated probabilistic FSM and PDA model. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps, con-
tain several dozens of parameters to tune, and em-
ploy expensive linguistic resources like WordNet
Figure 1: Monolingual MEANT algorithm.
or paraphrase tables; the expensive training, tun-
ing, and/or running time makes them hard to in-
corporate into the MT development cycle.
2.2 The MEANT family of metrics
MEANT (Lo et al, 2012), which is the weighted f-
score over the matched semantic role labels of the
automatically aligned semantic frames and role
fillers, that outperforms BLEU, NIST, METEOR,
WER, CDER and TER in correlation with human
adequacy judgments. MEANT is easily portable
to other languages, requiring only an automatic se-
mantic parser and a large monolingual corpus in
the output language for identifying the semantic
structures and the lexical similarity between the
semantic role fillers of the reference and transla-
tion.
Figure 1 shows the algorithm and equations for
computing MEANT. q
0
i,j
and q
1
i,j
are the argument
of type j in frame i in MT and REF respectively.
w
0
i
and w
1
i
are the weights for frame i in MT/REF
respectively. These weights estimate the degree of
contribution of each frame to the overall meaning
of the sentence. w
pred
and w
j
are the weights of
the lexical similarities of the predicates and role
fillers of the arguments of type j of all frame be-
tween the reference translations and the MT out-
766
Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow
semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate.
put. There is a total of 12 weights for the set
of semantic role labels in MEANT as defined in
Lo and Wu (2011b). For MEANT, they are de-
termined using supervised estimation via a sim-
ple grid search to optimize the correlation with
human adequacy judgments (Lo and Wu, 2011a).
For UMEANT (Lo and Wu, 2012), they are es-
timated in an unsupervised manner using relative
frequency of each semantic role label in the refer-
ences and thus UMEANT is useful when human
judgments on adequacy of the development set are
unavailable.
s
i,pred
and s
i,j
are the lexical similarities based
on a context vector model of the predicates and
role fillers of the arguments of type j between the
reference translations and the MT output. Lo et al
(2012) and Tumuluru et al (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al, 2013a; Lo
and Wu, 2013a; Lo et al, 2013b). In this paper,
we employ a newer version of MEANT that uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of seman-
tic role fillers, as our experiments indicate this is
more accurate than the previously used aggrega-
tion functions.
Recent studies (Lo et al, 2013a; Lo and Wu,
2013a; Lo et al, 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
2.3 MT quality estimation
Evaluating cross-lingual MT quality is similar to
the work of MT quality estimation (QE). Broadly
speaking, there are two different approaches to
QE: surface-based and feature-based.
Token-based QE models, such as those in Gan-
drabur et al (2006) and Ueffing and Ney (2005)
fail to assess the overall MT quality because trans-
lation goodness is not a compositional property. In
contrast, Blatz et al (2004) introduced a sentence-
level QE system where an arbitrary threshold is
used to classify the MT output as good or bad.
The fundamental problem of this approach is that
it defines QE as a binary classification task rather
than attempting to measure the degree of goodness
of the MT output. To address this problem, Quirk
(2004) related the sentence-level correctness of the
QE model to human judgment and achieved a high
correlation with human judgement for a small an-
notated corpus; however, the proposed model does
not scale well to larger data sets.
Feature-based QE models (Xiong et al, 2010;
He et al, 2011; Ma et al, 2011; Specia, 2011;
Avramidis, 2012; Mehdad et al, 2012; Almaghout
and Specia, 2013; Avramidis and Popovi?c, 2013;
Shah et al, 2013) throw a wide range of linguis-
tic and non-linguistic features into machine learn-
767
Figure 3: Cross-lingual XMEANT algorithm.
ing algorithms for predicting MT quality. Al-
though the feature-based QE system of Avramidis
and Popovi?c (2013) slightly outperformed ME-
TEOR on correlation with human adequacy judg-
ment, these ?black box? approaches typically lack
representational transparency, require expensive
running time, and/or must be discriminatively re-
trained for each language and text type.
3 XMEANT: a cross-lingual MEANT
Like MEANT, XMEANT aims to evaluate how
well MT preserves the core semantics, while
maintaining full representational transparency.
But whereas MEANT measures lexical similar-
ity using a monolingual context vector model,
XMEANT instead substitutes simple cross-lingual
lexical translation probabilities.
XMEANT differs only minimally from
MEANT, as underlined in figure 3. The same
weights obtained by optimizing MEANT against
human adequacy judgement were used for
XMEANT. The weights can also be estimated in
unsupervised fashion using the relative frequency
of each semantic role label in the foreign input, as
in UMEANT.
To aggregate individual lexical translation prob-
abilities into phrasal similarities between cross-
lingual semantic role fillers, we compared two nat-
ural approaches to generalizing MEANT?s method
of comparing semantic parses, as described below.
3.1 Applying MEANT?s f-score within
semantic role fillers
The first natural approach is to extend MEANT?s
f-score based method of aggregating semantic
parse accuracy, so as to also apply to aggregat-
ing lexical translation probabilities within seman-
tic role filler phrases. However, since we are miss-
ing structure information within the flat role filler
phrases, we can no longer assume an injective
mapping for aligning the tokens of the role fillers
between the foreign input and the MT output. We
therefore relax the assumption and thus for cross-
lingual phrasal precision/recall, we align each to-
ken of the role fillers in the output/input string
to the token of the role fillers in the input/output
string that has the maximum lexical translation
probability. The precise definition of the cross-
lingual phrasal similarities is as follows:
e
i,pred
? the output side of the pred of aligned frame i
f
i,pred
? the input side of the pred of aligned frame i
e
i,j
? the output side of the ARG j of aligned frame i
f
i,j
? the input side of the ARG j of aligned frame i
p(e, f) =
?
t (e|f) t (f |e)
prec
e,f
=
?
e?e
max
f?f
p(e, f)
|e|
rec
e,f
=
?
f?f
max
e?e
p(e, f)
|f|
s
i,pred
=
2 ? prec
e
i,pred
,f
i,pred
? rec
e
i,pred
,f
i,pred
prec
e
i,pred
,f
i,pred
+ rec
e
i,pred
,f
i,pred
s
i,j
=
2 ? prec
e
i,j
,f
i,j
? rec
e
i,j
,f
i,j
prec
e
i,j
,f
i,j
+ rec
e
i,j
,f
i,j
where the joint probability p is defined as the har-
monized the two directions of the translation table
t trained using IBM model 1 (Brown et al, 1993).
prec
e,f
is the precision and rec
e,f
is the recall of
the phrasal similarities of the role fillers. s
i,pred
and s
i,j
are the f-scores of the phrasal similarities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
3.2 Applying MEANT?s ITG bias within
semantic role fillers
The second natural approach is to extend
MEANT?s ITG bias on compositional reorder-
ing, so as to also apply to aggregating lexical
translation probabilities within semantic role filler
phrases. Addanki et al (2012) showed empiri-
cally that cross-lingual semantic role reordering of
the kind that MEANT is based upon is fully cov-
ered within ITG constraints. In Wu et al (2014),
we extend ITG constraints into aligning the tokens
within the semantic role fillers within monolingual
MEANT, thus replacing its previous monolingual
phrasal aggregation heuristic. Here we borrow the
768
idea for the cross-lingual case, using the length-
normalized inside probability at the root of a BITG
biparse (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009) as follows:
G ? ?{A} ,W
0
,W
1
,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 0.25
p (e/f |A) =
1
2
?
t (e|f) t (f |e)
s
i,pred
=
1
1?
ln
(
P
(
A
?
?e
i,pred
/f
i,pred
|G
))
max(|e
i,pred
|,|f
i,pred
|)
s
i,j
=
1
1?
ln
(
P
(
A
?
?e
i,j
/f
i,j
|G
))
max(|e
i,j
|,|f
i,j
|)
where G is a bracketing ITG, whose only nonter-
minal is A, and where R is a set of transduction
rules where e ? W
0
? {?} is an output token
(or the null token), and f ? W
1
? {?} is an in-
put token (or the null token). The rule probabil-
ity function p is defined using fixed probabilities
for the structural rules, and a translation table t
trained using IBM model 1 in both directions. To
calculate the inside probability of a pair of seg-
ments, P
(
A
?
? e/f|G
)
, we use the algorithm de-
scribed in Saers et al (2009). s
i,pred
and s
i,j
are
the length normalized BITG parsing probabilities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
4 Results
Table 1 shows that for human adequacy judgments
at the sentence level, the f-score based XMEANT
(1) correlates significantly more closely than other
commonly used monolingual automatic MT eval-
uation metrics, and (2) even correlates nearly as
well as monolingual MEANT. This suggests that
the semantic structure of the MT output is indeed
closer to that of the input sentence than that of the
reference translation.
Furthermore, the ITG-based XMEANT (1) sig-
nificantly outperforms MEANT, and (2) is an au-
tomatic metric that is nearly as accurate as the
HMEANT human subjective version. This indi-
cates that BITG constraints indeed provide a more
robust token alignment compared to the heuris-
tics previously employed in MEANT. It is also
consistent with results observed while estimating
word alignment probabilities, where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Table 1: Sentence-level correlation with HAJ
(GALE phase 2.5 evaluation data)
Metric Kendall
HMEANT 0.53
XMEANT (BITG) 0.51
MEANT (f-score) 0.48
XMEANT (f-score) 0.46
MEANT (2013) 0.46
NIST 0.29
BLEU/METEOR/TER/PER 0.20
CDER 0.12
WER 0.10
5 Conclusion
We have presented XMEANT, a new cross-lingual
variant of MEANT, that correlates even more
closely with human translation adequacy judg-
ments thanMEANT, without the expensive human
references. This is (1) accomplished by replacing
monolingual MEANT?s context vector model with
simple translation probabilities when computing
similarities of semantic role fillers, and (2) fur-
ther improved by incorporating BITG constraints
for aligning the tokens in semantic role fillers.
While monolingual MEANT alone accurately re-
flects adequacy via semantic frames and optimiz-
ing SMT against MEANT improves translation,
the new cross-lingual XMEANT semantic objec-
tive function moves closer toward deep integration
of semantics into the MT training pipeline.
The phrasal similarity scoring has only been
minimally adapted to cross-lingual semantic role
fillers in this first study of XMEANT. We expect
further improvements to XMEANT, but these first
results already demonstrate XMEANT?s potential
to drive research progress toward semantic SMT.
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT con-
tract nos. HR0011-12-C-0014 and HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of DARPA, the EU, or
RGC.
769
References
Karteek Addanki, Chi-Kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-lingual
verb frame alternations. In 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT-2012), Trento, Italy, May 2012.
Hala Almaghout and Lucia Specia. A CCG-based qual-
ity estimation metric for statistical machine transla-
tion. In Machine Translation Summit XIV (MT Sum-
mit 2013), Nice, France, 2013.
Eleftherios Avramidis and Maja Popovi?c. Machine
learning methods for comparative and time-oriented
quality estimation of machine translation output. In
8th Workshop on Statistical Machine Translation
(WMT 2013), 2013.
Eleftherios Avramidis. Quality estimation for machine
translation output using linguistic analysis and de-
coding features. In 7th Workshop on Statistical Ma-
chine Translation (WMT 2012), 2012.
Satanjeev Banerjee and Alon Lavie. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. Confidence estimation
for machine translation. In 20th international con-
ference on Computational Linguistics, 2004.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311, 1993.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. (meta-)
evaluation of machine translation. In Second Work-
shop on Statistical Machine Translation (WMT-07),
2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third
Workshop on Statistical Machine Translation (WMT-
08), 2008.
Julio Castillo and Paula Estrella. Semantic textual sim-
ilarity for MT evaluation. In 7th Workshop on Sta-
tistical Machine Translation (WMT 2012), 2012.
George Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In The second international conference on
Human Language Technology Research (HLT ?02),
San Diego, California, 2002.
Simona Gandrabur, George Foster, and Guy Lapalme.
Confidence estimation for nlp applications. ACM
Transactions on Speech and Language Processing,
2006.
Jes?us Gim?enez and Llu??s M`arquez. Linguistic features
for automatic evaluation of heterogenous MT sys-
tems. In Second Workshop on Statistical Machine
Translation (WMT-07), pages 256?264, Prague,
Czech Republic, June 2007.
Jes?us Gim?enez and Llu??s M`arquez. A smorgasbord
of features for automatic MT evaluation. In Third
Workshop on Statistical Machine Translation (WMT-
08), Columbus, Ohio, June 2008.
Yifan He, Yanjun Ma, Andy Way, and Josef van
Genabith. Rich linguistic features for translation
memory-inspired consistent translation. In 13th Ma-
chine Translation Summit (MT Summit XIII), 2011.
Philipp Koehn and Christof Monz. Manual and auto-
matic evaluation of machine translation between eu-
ropean languages. In Workshop on Statistical Ma-
chine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), 2006.
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on
Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. super-
vised weight estimation for semantic MT evalua-
tion metrics. In Sixth Workshop on Syntax, Seman-
tics and Structure in Statistical Translation (SSST-
6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT
Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013:
A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Sta-
tistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT
2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by train-
ing against an automatic semantic frame based eval-
770
uation metric. In 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2013),
2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by tun-
ing against Chinese MEANT. In International
Workshop on Spoken Language Translation (IWSLT
2013), 2013.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. Consistent translation using discriminative
learning: a translation memory-inspired approach.
In 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL HLT 2011). Association for Computa-
tional Linguistics, 2011.
Matou?s Mach?a?cek and Ond?rej Bojar. Results of the
WMT13 metrics shared task. In Eighth Workshop on
Statistical Machine Translation (WMT 2013), Sofia,
Bulgaria, August 2013.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
Match without a referee: evaluating mt adequacy
without reference translations. In 7th Workshop on
Statistical Machine Translation (WMT 2012), 2012.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first Na-
tional Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. A evaluation tool for machine trans-
lation: Fast evaluation for MT research. In The
Second International Conference on Language Re-
sources and Evaluation (LREC 2000), 2000.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Dependency-based automatic evaluation for
machine translation. In Syntax and Structure in Sta-
tistical Translation (SSST), 2007.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-02), pages 311?318, Philadelphia, Pennsylva-
nia, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow seman-
tic parsing using support vector machines. In Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2004), 2004.
Christopher Quirk. Training a sentence-level machine
translation confidence measure. In Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal, May 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine: A
metric to assess MT adequacy. In Sixth Workshop on
Statistical Machine Translation (WMT 2011), 2011.
Markus Saers and Dekai Wu. Improving phrase-based
translation via word alignments from stochastic in-
version transduction grammars. In Third Workshop
on Syntax and Structure in Statistical Translation
(SSST-3), Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu. Learning
stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th
International Conference on Parsing Technologies
(IWPT?09), Paris, France, October 2009.
Kashif Shah, Trevor Cohn, and Lucia Specia. An in-
vestigation on the effectiveness of features for trans-
lation quality estimation. In Machine Translation
Summit XIV (MT Summit 2013), Nice, France, 2013.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A study of trans-
lation edit rate with targeted human annotation. In
7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages
223?231, Cambridge, Massachusetts, August 2006.
Lucia Specia. Exploiting objective annotations for
measuring translation post-editing effort. In 15th
Conference of the European Association for Ma-
chine Translation, pages 73?80, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu.
Accuracy and robustness in measuring the lexical
similarity of semantic role fillers for automatic se-
mantic MT evaluation. In 26th Pacific Asia Confer-
ence on Language, Information, and Computation
(PACLIC 26), 2012.
Nicola Ueffing and Hermann Ney. Word-level con-
fidence estimation for machine translation using
phrase-based translation models. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 763?770, 2005.
Mengqiu Wang and Christopher D. Manning. SPEDE:
Probabilistic edit distance metrics for MT evalua-
tion. In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Dekai Wu, Chi-kiu Lo, Meriem Beloucif, and Markus
Saers. IMEANT: Improving semantic frame based
MT evaluation via inversion transduction grammars.
Forthcoming, 2014.
Dekai Wu. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403, 1997.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detec-
tion for statistical machine translation using linguis-
tic features. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2010),
2010.
Richard Zens and Hermann Ney. A comparative
study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2003),
pages 144?151, Stroudsburg, Pennsylvania, 2003.
771
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167?171,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Linear Inversion Transduction Grammar Alignments
as a Second Translation Path
Markus SAERS and Joakim NIVRE
Computational Linguistics Group
Dept. of Linguistics and Philology
Uppsala University
Sweden
first.last@lingfil.uu.se
Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
Hong Kong
dekai@cs.ust.hk
Abstract
We explore the possibility of using
Stochastic Bracketing Linear Inversion
Transduction Grammars for a full-scale
German?English translation task, both on
their own and in conjunction with align-
ments induced with GIZA++. The ratio-
nale for transduction grammars, the details
of the system and some results are pre-
sented.
1 Introduction
Lately, there has been some interest in using In-
version Transduction Grammars (ITGs) for align-
ment purposes. The main problem with ITGs is the
time complexity, O(Gn6) doesn?t scale well. By
limiting the grammar to a bracketing ITG (BITG),
the grammar constant (G) can be eliminated, but
O(n6) is still prohibitive for large data sets.
There has been some work on approximate in-
ference of ITGs. Zhang et al (2008) present a
method for evaluating spans in the sentence pair
to determine whether they should be excluded or
not. The algorithm has a best case time com-
plexity of O(n3). Saers, Nivre & Wu (2009) in-
troduce a beam pruning scheme, which reduces
time complexity to O(bn3). They also show
that severe pruning is possible without significant
deterioration in alignment quality (as measured
by downstream translation quality). Haghighi et
al. (2009) use a simpler aligner as guidance for
pruning, which reduces the time complexity by
two orders of magnitude. Their work also par-
tially implements the phrasal ITGs for translation-
driven segmentation introduced in Wu (1997), al-
though they only allow for one-to-many align-
ments, rather than many-to-many alignments. A
more extreme approach is taken in Saers, Nivre
& Wu (2010). Not only is the search severely
pruned, but the grammar itself is limited to a lin-
earized form, getting rid of branching within a sin-
gle parse. Although a small deterioration in down-
stream translation quality is noted (compared to
harshly pruned SBITGs), the grammar can be in-
duced in linear time.
In this paper we apply SBLITGs to a full size
German?English WMT?10 translation task. We
also use differentiated translation paths to com-
bine SBLITG translation models with a standard
GIZA++ translation model.
2 Background
A transduction grammar is a grammar that gener-
ates a pair of languages. In a transduction gram-
mar, the terminal symbols consist of pairs of to-
kens where the first is taken from the vocabulary
of one of the languages, and the second from the
vocabulary of the other. Transduction grammars
have to our knowledge been restricted to trans-
duce between languages no more complex than
context-free languages (CFLs). Transduction be-
tween CFLs was first described in Lewis & Stearns
(1968), and then further explored in Aho & Ull-
man (1972). The main motivation for explor-
ing this was to build programming language com-
pilers, which essentially translate between source
code and machine code. There are two types of
transduction grammars between CFLs described in
the computer science literature: simple transduc-
tion grammars (STGs) and syntax-directed trans-
duction grammars (SDTGs). The difference be-
tween them is that STGs are monotone, whereas
SDTGs allow unlimited reordering in rule produc-
tions. Both allow the use of singletons to insert
and delete tokens from either language. A sin-
gleton is a biterminal where one of the tokens is
the empty string (). Neither STGs nor SDTGs
are intuitively useful in translating natural lan-
guages, since STGs have no way to model reorder-
ing, and SDTGs require exponential time to be in-
duced from examples (parallel corpora). Since
167
compilers in general work on well defined, manu-
ally specified programming languages, there is no
need to induce them from examples, so the expo-
nential complexity is not a problem in this setting
? SDTGs can transduce in O(n3) time, so once the
grammar is known they can be used to translate
efficiently.
In natural language translation, the grammar is
generally not known, in fact, state-of-the art trans-
lation systems rely heavily on machine learning.
For transduction grammars, this means that they
have to be induced from parallel corpora.
An inversion transduction grammar (ITG)
strikes a good balance between STGs and SDTGs,
as it allows some reordering, while requiring only
polynomial time to be induced from parallel cor-
pora. The allowed reordering is either the iden-
tity permutation of the production, or the inver-
sion permutation. Restricting the permutations in
this way ensures that an ITG can be expressed in
two-normal form, which is the key property for
avoiding exponential time complexity in biparsing
(parsing of a sentence pair).
An ITG in two-normal form (representing the
transduction between L1 and L2) is written with
identity productions in square brackets, and in-
verted productions in angle brackets. Each such
rule can be construed to represent two (one L1 and
one L2) synchronized CFG rules:
ITGL1,L2 CFGL1 CFGL2
A? [ B C ] A? B C A? B C
A? ? B C ? A? B C A? C B
A? e/f A? e A? f
Inducing an ITG from a parallel corpus is still slow,
as the time complexity is O(Gn6). Several ways
to get around this has been proposed (Zhang et al,
2008; Haghighi et al, 2009; Saers et al, 2009;
Saers et al, 2010).
Taking a closer look at the linear ITGs (Saers et
al., 2010), there are five rules in normal form. De-
composing these five rule types into monolingual
rule types reveals that the monolingual grammars
are linear grammars (LGs):
LITGL1,L2 LGL1 LGL2
A? [ e/f C ] A? e C A? f C
A? [ B e/f ] A? B e A? B f
A? ? e/f C ? A? e C A? C f
A? ? B e/f ? A? B e A? f B
A? / A?  A? 
This means that LITGs are transduction grammars
that transduce between linear languages.
There is also a nice parallel in search time com-
plexities between CFGs and ITGs on the one hand,
and LGs and LITGs on the other. Searching for
all possible parses given a sentence is O(n3) for
CFGs, and O(n2) for LGs. Searching for all possi-
ble biparses given a bisentence is O(n6) for ITGs,
and O(n4) for LITGs. This is consistent with
thinking of biparsing as finding every L2 parse for
every L1 parse. Biparsing consists of assigning a
joint structure to a sentence pair, rather than as-
signing a structure to a sentence.
In this paper, only stochastic bracketing gram-
mars (SBITGs and SBLITGs) were used. A brack-
eting grammar has only one nonterminal symbol,
denoted X . A stochastic grammar is one where
each rule is associated with a probability, such that
?X
?
?
?
?
p(X ? ?) = 1
?
?
While training a Stochastic Bracketing ITG
(SBITG) or LITG (SBLITG) with EM, expectations
of probabilities over the biparse-forest are calcu-
lated. These expectations approach the true prob-
abilities, and can be used as approximations. The
probabilities over the biparse-forest can be used
to select the one-best parse-tree, which in turn
forces an alignment over the sentence pair. The
alignments given by SBITGs and SBLITGs has been
shown to give better translation quality than bidi-
rectional IBM-models, when applied to short sen-
tence corpora (Saers and Wu, 2009; Saers et al,
2009; Saers et al, 2010). In this paper we ex-
plore whether this hold for SBLITGs on standard
sentence corpora.
3 Setup
The baseline system for the shared task was a
phrase based translation model based on bidi-
rectional IBM- (Brown et al, 1993) and HMM-
models (Vogel et al, 1996) combined with the
grow-diag-final-and heuristic. This is
computed with the GIZA++ tool (Och and Ney,
2003) and the Moses toolkit (Koehn et al, 2007).
The language model was a 5-gram SRILM (Stol-
cke, 2002). Parameters in the final translation sys-
tem were determined with Minimum Error-Rate
Training (Och, 2003), and translation quality was
assessed with the automatic measures BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002).
168
Corpus Type Size
German?English Europarl out of domain 1,219,343 sentence pairs
German?English news commentary in-domain 86,941 sentence pairs
English news commentary in-domain 48,653,884 sentences
German?English news commentary in-domain tuning data 2,051 sentence pairs
German?English news commentary in-domain test data 2,489 sentence pairs
Table 1: Corpora available for the German?English translation task after baseline cleaning.
System BLEU NIST
GIZA++ 17.88 5.9748
SBLITG 17.61 5.8846
SBLITG (only Europarl) 17.46 5.8491
SBLITG (only news) 15.49 5.4987
GIZA++ and SBLITG 17.66 5.9650
GIZA++ and SBLITG (only Europarl) 17.58 5.9819
GIZA++ and SBLITG (only news) 17.48 5.9693
Table 2: Results for the German?English translation task.
We chose to focus on the German?English
translation task. The corpora resources available
for that task is summarized in Table 1. We used the
entire news commentary monolingual data con-
catenated with the English side of the Europarl
bilingual data to train the language model. In ret-
rospect, this was probably a bad choice, as others
seem to prefer the use of two language models in-
stead.
We contrasted the baseline system with pure
SBLITG systems trained on different parts of the
training data, as well as combined systems, where
the SBLITG systems were combined with the base-
line system. The combination was done by adding
the SBLITG translation model as a second transla-
tion path to the base line system.
To train our SBLITG systems, we used the algo-
rithm described in Saers et al (2010). We set the
beam size parameter to 50, and ran expectation-
maximization for 10 iterations or until the log-
probability of the training corpus started deterio-
rating. After the grammar was induced we ob-
tained the one-best parse for each sentence pair,
which also dictates a word alignment over that
sentence pair, which we used instead of the word
alignments provided by GIZA++. From that point,
training did not differ from the baseline procedure.
We trained a total of three pure SBLITG system,
one with only the news commentary part of the
corpus, one with only the Europarl part, and one
with both. We also combined all three SBLITG
systems with the baseline system to see whether
the additional translation paths would help.
The system we submitted corresponds to the
?GIZA++ and SBLITG (only news)? system, but
with RandLM (Talbot and Osborne, 2007) as lan-
guage model rather than SRILM. This was because
we lacked the necessary RAM resources to calcu-
late the full SRILM model before the system sub-
mission deadline.
4 Results
The results for the development test set are sum-
marized in Table 2. The submitted system
achieved a BLEU score of 0.1759 and a NIST
score of 5.9579 for cased output on this year?s test
set (these numbers are not comparable to those
in Table 2). To our surprise, adding the addi-
tional phrases as a second translation path does
not seem to help. Instead a small deterioration
in BLEU is noted (0.22?0.40 points), whereas the
differences in NIST are mixed (-0.0098?+0.0071
points). Over all the variations were very small.
The pure SBLITG systems perform consistently
below baseline, which could indicate that the
grammar class is unable to capture the reorderings
found in longer sentence pairs adequately in one
parse. The variation between the pure SBLITG sys-
tems can be explained by the size of the training
data: more data ? better quality.
169
5 Conclusions
We tried to use SBLITGs as word aligners on full
size sentences, which has not been done to date,
and noted that the formalism seems unable to ac-
count for the full complexity of longer sentence
pairs. We also tried combining the translation
models acquired with SBLITG alignments to the
baseline system, and noted very small differences,
tending to a deterioration in quality. The fact that
SBLITGs seem unable to capture the complex re-
lationship between an English and a German sen-
tence in one parse means that we need to find ei-
ther some more complex model or some way to
use the entire parse forest to arrive at the align-
ment.
Acknowledgments
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under
research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency. The com-
putations were performed on UPPMAX resources
under project p2007020.
References
Jeffrey D. Aho, Alfred V. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Englewood Cliffs, NJ.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of Human
Language Technology conference (HLT-2002), San
Diego, California.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 923?
931, Suntec, Singapore, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-
directed transduction. Journal of the Association for
Computing Machinery, 15(3):465?488.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160?167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, July.
Markus Saers and Dekai Wu. 2009. Improving phrase-
based translation via word alignments from Stochas-
tic Inversion Transduction Grammars. In Proceed-
ings of the Third Workshop on Syntax and Structure
in Statistical Translation (SSST-3) at NAACL HLT
2009, pages 28?36, Boulder, Colorado, June.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Transduc-
tion Grammars with a cubic time biparsing algo-
rithm. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT?09), pages
29?32, Paris, France, October.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings of
Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Los An-
geles, California, June.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In International Confer-
ence on Spoken Language Processing, Denver, Col-
orado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
512?519, Prague, Czech Republic, June.
170
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics, pages 836?841, Mor-
ristown, New Jersey.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June.
171
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10?18,
COLING 2010, Beijing, August 2010.
A Systematic Comparison between Inversion Transduction Grammar
and Linear Transduction Grammar for Word Alignment
Markus Saers and Joakim Nivre
Dept. of Linguistics & Philology
Uppsala University
first.last@lingfil.uu.se
Dekai Wu
HKUST
Human Language Technology Center
Dept. of Computer Science & Engineering
Hong Kong Univ. of Science & Technology
dekai@cs.ust.hk
Abstract
We present two contributions to gram-
mar driven translation. First, since both
Inversion Transduction Grammar and
Linear Inversion Transduction Gram-
mars have been shown to produce bet-
ter alignments then the standard word
alignment tool, we investigate how the
trade-off between speed and end-to-end
translation quality extends to the choice
of grammar formalism. Second, we
prove that Linear Transduction Gram-
mars (LTGs) generate the same transduc-
tions as Linear Inversion Transduction
Grammars, and present a scheme for ar-
riving at LTGs by bilingualizing Linear
Grammars. We also present a method for
obtaining Inversion Transduction Gram-
mars from Linear (Inversion) Transduc-
tion Grammars, which can speed up
grammar induction from parallel corpora
dramatically.
1 Introduction
In this paper we introduce Linear Transduction
Grammars (LTGs), which are the bilingual case
of Linear Grammars (LGs). We also show that
LTGs are equal to Linear Inversion Transduction
Grammars (Saers et al, 2010). To be able to in-
duce transduction grammars directly from par-
allel corpora an approximate search for parses is
needed. The trade-off between speed and end-to-
end translation quality is investigated and com-
pared to Inversion Transduction Grammars (Wu,
1997) and the standard tool for word alignment,
GIZA++ (Brown et al, 1993; Vogel et al, 1996;
Och and Ney, 2003). A heuristic for converting
stochastic bracketing LTGs into stochastic brack-
eting ITGs is presented, and fitted into the speed?
quality trade-off.
In section 3 we give an overview of transduc-
tion grammars, introduce LTGs and show that
they are equal to LITGs. In section 4 we give
a short description of the rational for the trans-
duction grammar pruning used. In section 5 we
describe a way of seeding a stochastic bracketing
ITG with the rules and probabilities of a stochas-
tic bracketing LTG. Section 6 describes the setup,
and results are given in section 7. Finally, some
conclusions are offered in section 8
2 Background
Any form of automatic translation that relies on
generalizations of observed translations needs to
align these translations on a sub-sentential level.
The standard way of doing this is by aligning
words, which works well for languages that use
white space separators between words. The stan-
dard method is a combination of the family of
IBM-models (Brown et al, 1993) and Hidden
Markov Models (Vogel et al, 1996). These
methods all arrive at a function (A) from lan-
guage 1 (F ) to language 2 (E). By running the
process in both directions, two functions can be
estimated and then combined to form an align-
ment. The simplest of these combinations are in-
tersection and union, but usually, the intersection
is heuristically extended. Transduction gram-
mars on the other hand, impose a shared struc-
ture on the sentence pairs, thus forcing a consis-
tent alignment in both directions. This method
10
has proved successful in the settings it has been
tried (Zhang et al, 2008; Saers and Wu, 2009;
Haghighi et al, 2009; Saers et al, 2009; Saers
et al, 2010). Most efforts focus on cutting down
time complexity so that larger data sets than toy-
examples can be processed.
3 Transduction Grammars
Transduction grammars were first introduced in
Lewis and Stearns (1968), and further devel-
oped in Aho and Ullman (1972). The origi-
nal notation called for regular CFG-rules in lan-
guage F with rephrased E productions, either in
curly brackets, or comma separated. The bilin-
gual version of CFGs is called Syntax-Directed
Transduction Grammars (SDTGs). To differenti-
ate identical nonterminal symbols, indices were
used (the bag of nonterminals for the two pro-
ductions are equal by definition).
A ? B(1) a B(2) {x B(1) B(2)}
= A ? B(1) a B(2), x B(1) B(2)
The semantics of the rules is that one nontermi-
nal rewrites into a bag of nonterminals that is dis-
tributed independently in the two languages, and
interspersed with any number of terminal sym-
bols in the respective languages. As with CFGs,
the terminal symbols can be factored out into
preterminals with the added twist that they are
shared between the two languages, since preter-
minals are formally nonterminals. The above
rule can thus be rephrased as
A ? B(1) Xa/x B(2), Xa/x B(1) B(2)
Xa/x ? a, x
In this way, rules producing nonterminals and
rules producing terminals can be separated.
Since only nonterminals are allowed to move,
their movement can be represented as the orig-
inal sequence of nonterminals and a permutation
vector as follows:
A ? B Xa/x B ; 1, 0, 2
Xa/x ? a, x
To keep the reordering as monotone as possible,
the terminals a and x can be produced separately,
but doing so eliminates any possibility of param-
eterizing their lexical relationship. Instead, the
individual terminals are pair up with the empty
string (?).
A ? Xx B Xa B ; 0, 1, 2, 3
Xa ? a, ?
Xx ? ?, x
Lexical rules involving the empty string are re-
ferred to as singletons. Whenever a preterminal
is used to pair up two terminal symbols, we refer
to that pair of terminals as a biterminal, which
will be written as e/f .
Any SDTG can be rephrased to contain per-
muted nonterminal productions and biterminal
productions only, and we will call this the nor-
mal form of SDTGs. Note that it is not possi-
ble to produce a two-normal form for SDTGs,
as there are some rules that are not binarizable
(Wu, 1997; Huang et al, 2009). This is an
important point to make, since efficient parsing
for CFGs is based on either restricting parsing
to only handle binary grammars (Cocke, 1969;
Kasami, 1965; Younger, 1967), or rely on on-
the-fly binarization (Earley, 1970). When trans-
lating with a grammar, parsing only has to be
done in F , which is binarizable (since it is a
CFG), and can therefor be computed in polyno-
mial time (O(n3)). Once there is a parse tree
for F , the corresponding tree for E can be eas-
ily constructed. When inducing a grammar from
examples, however, biparsing (finding an anal-
ysis that is consistent across a sentence pair) is
needed. The time complexity for biparsing with
SDTGs is O(n2n+2), which is clearly intractable.
Inversion Transduction Grammars or ITGs
(Wu, 1997) are transduction grammars that have
a two-normal form, thus guaranteeing binariz-
ability. Defining the rank of a rule as the number
of nonterminals in the production, and the rank
of a grammar as the highest ranking rule in the
rule set, ITGs are a) any SDTG of rank two, b)
any SDTG of rank three or c) any SDTG where no
rule has a permutation vector other than identity
permutation or inversion permutation. It follows
from this definition that ITGs have a two-normal
form, which is usually expressed as SDTG rules,
11
with brackets around the production to distin-
guish the different kinds of rules from each other.
A ? B C ; 0, 1 = A ? [ B C ]
A ? B C ; 1, 0 = A ? ? B C ?
A ? e/f = A ? e/f
By guaranteeing binarizability, biparsing time
complexity becomes O(n6).
There is an even more restricted version of
SDTGs called Simple Transduction Grammar
(STG), where no permutation at all is allowed,
which can also biparse a sentence pair in O(n6)
time.
A Linear Transduction Grammar (LTG) is a
bilingual version of a Linear Grammar (LG).
Definition 1. An LG in normal form is a tuple
GL = ?N,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols, R is a finite
set of rules and S ? N is the designated start
symbol. The rule set is constrained so that
R ? N ? (? ? {?})N(? ? {?}) ? {?}
Where ? is the empty string.
To bilingualize a linear grammar, we will take
the same approach as taken when a finite-state
automaton is bilingualized into a finite-state
transducer. That is: to replace all terminal sym-
bols with biterminal symbols.
Definition 2. An LTG in normal form is a tuple
T GL = ?N,?,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols in language
E, ? is a finite set of terminal symbols in lan-
guage F , R is a finite set of linear transduction
rules and S ? N is the designated start symbol.
The rule set is constrained so that
R ? N ??N? ? {??, ??}
Where ? = ??{?}???{?} and ? is the empty
string.
Graphically, we will represent LTG rules as pro-
duction rules with biterminals:
?A, ?x, p?B?y, q?? = A ? x/p B y/q
?A, ??, ??? = B ? ?/?
Like STGs, LTGs do not allow any reordering,
and are monotone, but because they are linear,
this has no impact on expressiveness, as we shall
see later.
Linear Inversion Transduction Grammars
(LITGs) were introduced in Saers et al (2010),
and represent ITGs that are allowed to have at
most one nonterminal symbol in each produc-
tion. These are attractive because they can bi-
parse a sentence pair in O(n4) time, which can
be further reduced to linear time by severely
pruning the search space. This makes them
tractable for large parallel corpora, and a viable
way to induce transduction grammars from large
parallel corpora.
Definition 3. An LITG in normal form is a tuple
T GLI = ?N,?,?, R, S?
Where N is a finite set of nonterminal symbols,
? is a finite set of terminal symbols from lan-
guage E, ? is a finite set of terminal symbols
from language F , R is a set of rules and S ? N
is the designated start symbol. The rule set is
constrained so that
R ? N ? {[], ??} ??N ?N? ? {??, ??}
Where [] represents identity permutation and ??
represents inversion permutation, ? = ??{?}?
? ? {?} is a possibly empty biterminal, and ? is
the empty string.
Graphically, a rule will be represented as an ITG
rule:
?A, [], B?e, f?? = A ? [ B e/f ]
?A, ??, ?e, f?B? = A ? ? e/f B ?
?A, [], ??, ??? = A ? ?/?
As with ITGs, productions with only biterminals
will be represented without their permutation, as
any such rule can be trivially rewritten into in-
verted or identity form.
12
Definition 4. An ?-free LITG is an LITG where
no rule may rewrite one nonterminal into another
nonterminal only. Formally, the rule set is con-
strained so that
R ?N ? {[], ??} ? ({??, ??}B ?B{??, ??}) = ?
The LITG presented in Saers et al (2010) is
thus an ?-free LITG in normal form, since it has
the following thirteen rule forms (of which 8 are
meaningful, 1 is only used to terminate genera-
tion and 4 are redundant):
A ? [ e/f B ]
A ? ? e/f B ?
A ? [ B e/f ]
A ? ? B e/f ?
A ? [ e/? B ] | A ? ? e/? B ?
A ? [ B e/? ] | A ? ? B e/? ?
A ? [ ?/f B ] | A ? ? B ?/f ?
A ? [ B ?/f ] | A ? ? ?/f B ?
A ? ?/?
All the singleton rules can be expressed either in
straight or inverted form, but the result of apply-
ing the two rules are the same.
Lemma 1. Any LITG in normal form can be ex-
pressed as an LTG in normal form.
Proof. The above LITG can be rewritten in LTG
form as follows:
A ? [ e/f B ] = A ? e/f B
A ? ? e/f B ? = A ? e/? B ?/f
A ? [ B e/f ] = A ? B e/f
A ? ? B e/f ? = A ? ?/f B e/?
A ? [ e/? B ] = A ? e/? B
A ? [ B e/? ] = A ? B e/?
A ? [ ?/f B ] = A ? ?/f B
A ? [ B ?/f ] = A ? B ?/f
A ? ?/? = A ? ?/?
To account for all LITGs in normal form, the fol-
lowing two non-?-free rules also needs to be ac-
counted for:
A ? [ B ] = A ? B
A ? ? B ? = A ? B
Lemma 2. Any LTG in normal form can be ex-
pressed as an LITG in normal form.
Proof. An LTG in normal form has two rules,
which can be rewritten in LITG form, either as
straight or inverted rules as follows
A ? x/p B y/q = A ? [ x/p B? ]
B? ? [ B y/q ]
= A ? ? x/q B? ?
B? ? ? B y/p ?
A ? ?/? = A ? ?/?
Theorem 1. LTGs in normal form and LITGs in
normal form express the same class of transduc-
tions.
Proof. Follows from lemmas 1 and 2.
By theorem 1 everything concerning LTGs is also
applicable to LITGs, and an LTG can be expressed
in LITG form when convenient, and vice versa.
4 Pruning the Alignment Space
The alignment space for a transduction grammar
is the combinations of the parse spaces of the
sentence pair. Let e be the E sentence, and f
be the F sentence. The parse spaces would be
O(|e|2) and O(|f |2) respectively, and the com-
bination of these spaces would be O(|e|2?|f |2),
or O(n4) if we assume n to be proportional
to the sentence lengths. In the case of LTGs,
this space is searched linearly, giving time com-
plexity O(n4), and in the case of ITGs there
is branching within both parse spaces, adding
an order of magnitude each, giving a total time
complexity of O(n6). There is, in other words,
a tight connection between the alignment space
and the time complexity of the biparsing al-
gorithm. Furthermore, most of this alignment
space is clearly useless. Consider the case where
the entire F sentence is deleted, and the entire E
sentence is simply inserted. Although it is pos-
sible that it is allowed by the grammar, it should
have a negligible probability (since it is clearly a
translation strategy that generalize poorly), and
could, for all practical reasons, be ignored.
13
Language pair Bisentences Tokens
Spanish?English 108,073 1,466,132
French?English 95,990 1,340,718
German?English 115,323 1,602,781
Table 1: Size of training data.
Saers et al (2009) present a scheme for prun-
ing away most of the points in the alignment
space. Parse items are binned according to cov-
erage (the total number of words covered), and
each bin is restricted to carry a maximum of b
items. Any items that do not fit in the bins are
excluded from further analysis. To decide which
items to keep, inside probability is used. This
pruning scheme effectively linearizes the align-
ment space, as is will be of size O(nb), regard-
less of what type grammar is used. An ITG can
thus be biparsed in cubic time, and an LTG in lin-
ear time.
5 Seeding an ITG with an LTG
Since LTGs are a subclass of ITGs, it would be
possible to convert an LTG to a ITG. This could
save a lot of time, since LTGs are much faster to
induce from corpora than ITGs.
Converting a BLTG to a BITG is fairly straight
forward. Consider the BLTG rule
X ? [ e/f X ]
To convert it to BITG in two-normal form, the
biterminal has to be factored out. Replacing
the biterminal with a temporary symbol X? , and
introducing a rule that rewrites this temporary
symbol to the replaced biterminal produces two
rules:
X ? [ X? X ]
X? ? e/f
This is no longer a bracketing grammar since
there are two nonterminals, but equating X? to X
restores this property. An analogous procedure
can be applied in the case where the nonterminal
comes before the biterminal, as well as for the
inverting cases.
When converting stochastic LTGs, the proba-
bility mass of the SLTG rule has to be distributed
to two SITG rules. The fact that the LTG rule
X ? ?/? lacks correspondence in ITGs has to
be weighted in as well. In this paper we took the
maximum entropy approach and distributed the
probability mass uniformly. This means defin-
ing the probability mass function p? for the new
SBITG from the probability mass function p of
the original SBLTG such that:
p?(X ? [ X X ]) =
?
e/f
?
???
?
p(X?[ e/f X ])
1?p(X??/?)
+?
p(X?[ X e/f ])
1?p(X??/?)
?
???
p?(X ? ? X X ?) =
?
e/f
?
???
?
p(X?? e/f X ?)
1?p(X??/?)
+?
p(X?? X e/f ?)
1?p(X??/?)
?
???
p?(X ? e/f) =
?
?????????????
?
p(X?[ e/f X ])
1?p(X??/?)
+?
p(X?[ X e/f ])
1?p(X??/?)
+?
p(X?? e/f X ?)
1?p(X??/?)
+?
p(X?? X e/f ?)
1?p(X??/?)
?
?????????????
6 Setup
The aim of this paper is to compare the align-
ments from SBITG and SBLTG to those from
GIZA++, and to study the impact of pruning
on efficiency and translation quality. Initial
grammars will be estimated by counting cooc-
currences in the training corpus, after which
expectation-maximization (EM) will be used to
refine the initial estimate. At the last iteration,
the one-best parse of each sentence will be con-
sidered as the word alignment of that sentence.
In order to keep the experiments comparable,
relatively small corpora will be used. If larger
corpora were used, it would not be possible to get
any results for unpruned SBITGs because of the
prohibitive time complexity. The Europarl cor-
pus (Koehn, 2005) was used as a starting point,
and then all sentence pairs where one of the sen-
tences were longer than 10 tokens were filtered
14
Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (in
seconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination.
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.1234 0.2608 0.2655 0.2653 0.2661 0.2671 0.2663
SBLTG 0.2574 0.2645 0.2631 0.2624 0.2625 0.2633 0.2628
GIZA++ 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597
NIST
SBITG 3.9705 6.6439 6.7312 6.7101 6.7329 6.7445 6.6793
SBLTG 6.6023 6.6800 6.6657 6.6637 6.6714 6.6863 6.6765
GIZA++ 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464
Training times
SBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00
SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59
Table 2: Results for the Spanish?English translation task.
out (see table 1). The GIZA++ system was built
according to the instructions for creating a base-
line system for the Fifth Workshop on Statistical
Machine Translation (WMT?10),1 but the above
corpora were used instead of those supplied by
the workshop. This includes word alignment
with GIZA++, a 5-gram language model built
with SRILM (Stolcke, 2002) and parameter tun-
ing with MERT (Och, 2003). To carry out the ac-
tual translations, Moses (Koehn et al, 2007) was
used. The SBITG and SBLTG systems were built
in exactly the same way, except that the align-
ments from GIZA++ were replaced by those from
the respective grammars.
In addition to trying out exhaustive biparsing
1http://www.statmt.org/wmt10/
for SBITGs and SBLTGs on three different trans-
lation tasks, several different levels of pruning
were tried (1, 10, 25, 50, 75 and 100). We also
used the grammar induced from SBLTGs with a
beam size of 25 to seed SBITGs (see section 5),
which were then run for an additional iteration
of EM, also with beam size 25.
All systems are evaluated with BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002).
7 Results
The results for the three different translation
tasks are presented in Tables 2, 3 and 4. It is
interesting to note that the trend they portray is
quite similar. When the beam is very narrow,
GIZA++ is better, but already at beam size 10,
both transduction grammars are superior. Con-
15
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.1268 0.2632 0.2654 0.2669 0.2668 0.2655 0.2663
SBLTG 0.2600 0.2638 0.2651 0.2668 0.2672 0.2662 0.2649
GIZA++ 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603
NIST
SBITG 4.0849 6.7136 6.7913 6.8065 6.8068 6.8088 6.8151
SBLTG 6.6814 6.7608 6.7656 6.7992 6.8020 6.7925 6.7784
GIZA++ 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907
Training times
SBITG 03:25 17:00 42:00 1:25:00 2:10:00 2:45:00 3:10:00
SBLTG 31 1:41 3:25 7:06 9:35 13:56 10:52
Table 3: Results for the French?English translation task.
Beam size
System 1 10 25 50 75 100 ?
BLEU
SBITG 0.0926 0.2050 0.2091 0.2090 0.2091 0.2094 0.2113
SBLTG 0.2015 0.2067 0.2066 0.2073 0.2080 0.2066 0.2088
GIZA++ 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059
NIST
SBITG 3.4297 5.8743 5.9292 5.8947 5.8955 5.9086 5.9380
SBLTG 5.7799 5.8819 5.8882 5.8963 5.9252 5.8757 5.9311
GIZA++ 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668
Training times
SBITG 03:20 17:00 41:00 1:25:00 2:10:00 2:45:00 3:40:00
SBLTG 38 1:58 4:52 8:08 11:42 16:05 13:32
Table 4: Results for the German?English translation task.
sistent with Saers et al (2009), SBITG has a sharp
rise in quality going from beam size 1 to 10,
and then a gentle slope up to beam size 25, af-
ter which it levels out. SBLTG, on the other hand
start out at a respectable level, and goes up a gen-
tle slope from beam size 1 to 10, after which is
level out. This is an interesting observation, as it
suggests that SBLTG reaches its optimum with a
lower beam size (although that optimum is lower
than that of SBITG). The trade-off between qual-
ity and time can now be extended beyond beam
size to include grammar choice. In Figure 1, run
times are plotted against BLEU scores to illus-
trate this trade-off. It is clear that SBLTGs are
indeed much faster than SBITGs, the only excep-
tion is when SBITGs are run with b = 1, but then
the BLEU score is so low that is is not worth con-
sidering.
The time may seem inconsistent between b =
100 and b = ? for SBLTG, but the extra time
for the tighter beam is because of beam manage-
ment, which the exhaustive search doesn?t bother
with.
In table 5 we compare the pure approaches
to one where an LTG was trained during 10 it-
erations of EM and then used to seed (see sec-
16
Translation task System BLEU NIST Total time
SBLTG 0.2631 6.6657 36:40
Spanish?English SBITG 0.2655 6.7312 6:20:00
Both 0.2660 6.7124 1:14:40
SBLTG 0.2651 6.7656 34:10
French?English SBITG 0.2654 6.7913 7:00:00
Both 0.2625 6.7609 1:16:10
SBLTG 0.2066 5.8882 48:52
German?English SBITG 0.2091 5.9292 6:50:00
Both 0.2095 5.9224 1:29:40
Table 5: Results for seeding an SBITG with an SBLTG (Both) compared to the pure approach. Total
time refers to 10 iterations of EM training for SBITG and SBLTG respectively, and 10 iterations of
SBLTG and one iteration of SBITG training for the combined system.
tion 5) an SBITG, which was then trained for
one iteration of EM. Although the differences
are fairly small, German?English and Spanish?
English seem to reach the level of SBITG,
whereas French?English is actually hurt. The
big difference is in time, since the combined sys-
tem needs about a fifth of the time the SBITG-
based system needs. This phenomenon needs to
be more thoroughly examined.
It is also worth noting that GIZA++ was beaten
by an aligner that used less than 20 minutes (less
than 2 minutes per iteration and at most 10 itera-
tions) to align the corpus.
8 Conclusions
In this paper we have introduced the bilingual
version of linear grammar: Linear Transduc-
tion Grammars, and found that they generate the
same class of transductions as Linear Inversion
Transduction Grammars. We have also com-
pared Stochastic Bracketing versions of ITGs and
LTGs to GIZA++ on three word alignment tasks.
The efficiency issues with transduction gram-
mars have been addressed by pruning, and the
conclusion is that there is a trade-off between
run time and translation quality. A part of the
trade-off is choosing which grammar framework
to use, as LTGs are faster but not as good as ITGs.
It also seems possible to take a short-cut in this
trade-off by starting out with an LTG and convert-
ing it to an ITG. We have also showed that it is
possible to beat the translation quality of GIZA++
with a quite fast transduction grammar.
Acknowledgments
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tracts No. HR0011-06-C-0022 and No. HR0011-
06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants
GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views ofthe Defense Ad-
vanced Research Projects Agency. The computa-
tions were performed on UPPMAX resources un-
der project p2007020.
References
Aho, Alfred V. Ullman, Jeffrey D. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Inc., Upper Saddle River, NJ.
Brown, Peter F., Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Cocke, John. 1969. Programming languages and
their compilers: Preliminary notes. Courant Insti-
17
tute of Mathematical Sciences, New York Univer-
sity.
Doddington, George. 2002. Automatic eval-
uation of machine translation quality using n-
gram co-occurrence statistics. In Proceedings of
Human Language Technology conference (HLT-
2002), San Diego, California.
Earley, Jay. 1970. An efficient context-free parsing
algorithm. Communications of the Association for
Comuter Machinery, 13(2):94?102.
Haghighi, Aria, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
923?931, Suntec, Singapore, August.
Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguis-
tics, 35(4):559?595.
Kasami, Tadao. 1965. An efficient recognition
and syntax analysis algorithm for context-free lan-
guages. Technical Report AFCRL-65-00143, Air
Force Cambridge Research Laboratory.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic, June.
Koehn, Philipp. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, Phuket, Thailand, Septem-
ber.
Lewis, Philip M. and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the Asso-
ciation for Computing Machinery, 15(3):465?488.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160?167, Sapporo, Japan, July.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July.
Saers, Markus and Dekai Wu. 2009. Improving
phrase-based translation via word alignments from
Stochastic Inversion Transduction Grammars. In
Proceedings of the Third Workshop on Syntax
and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009, pages 28?36, Boulder, Col-
orado, June.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Trans-
duction Grammars with a cubic time biparsing al-
gorithm. In Proceedings of the 11th International
Conference on Parsing Technologies (IWPT?09),
pages 29?32, Paris, France, October.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings
of Human Language Technologies: The 11th An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
Los Angeles, California, June.
Stolcke, Andreas. 2002. SRILM ? an extensible
language modeling toolkit. In International Con-
ference on Spoken Language Processing, Denver,
Colorado, September.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in sta-
tistical translation. In Proceedings of the 16th con-
ference on Computational linguistics, pages 836?
841, Morristown, New Jersey.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Younger, Daniel H. 1967. Recognition and parsing
of context-free languages in time n3. Information
and Control, 10(2):189?208.
Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June.
18
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 52?60,
COLING 2010, Beijing, August 2010.
7IQERXMG ZW 7]RXEGXMG ZW 2KVEQ 7XVYGXYVI
JSV 1EGLMRI 8VERWPEXMSR )ZEPYEXMSR
'LMOMY 03 ERH (IOEM ;9
,/978
,YQER 0ERKYEKI 8IGLRSPSK] 'IRXIV
(ITEVXQIRX SJ 'SQTYXIV 7GMIRGI ERH )RKMRIIVMRK
,SRK /SRK 9RMZIVWMX] SJ 7GMIRGI ERH 8IGLRSPSK]
NEGOMIPSHIOEM $GWYWXLO
%FWXVEGX
;I TVIWIRX VIWYPXW SJ ER IQTMVMGEP WXYH]
SR IZEPYEXMRK XLI YXMPMX] SJ XLI QEGLMRI
XVERWPEXMSR SYXTYX F] EWWIWWMRK XLI EGGY
VEG] [MXL [LMGL LYQER VIEHIVW EVI EFPI
XS GSQTPIXI XLI WIQERXMG VSPI ERRSXEXMSR
XIQTPEXIW 9RPMOI XLI [MHIP]YWIH PI\M
GEP ERH RKVEQ FEWIH SV W]RXEGXMG FEWIH
18 IZEPYEXMSR QIXVMGW [LMGL EVI xYIRG]
SVMIRXIH SYV VIWYPXW WLS[ XLEX YWMRK WI
QERXMG VSPI PEFIPW XS IZEPYEXI XLI YXMPMX]
SJ 18 SYXTYX EGLMIZI LMKLIV GSVVIPEXMSR
[MXL LYQER NYHKQIRXW SR EHIUYEG] -R
XLMW WXYH] LYQER VIEHIVW [IVI IQTPS]IH
XS MHIRXMJ] XLI WIQERXMG VSPI PEFIPW MR XLI
XVERWPEXMSR *SV IEGL VSPI XLI wPPIV MW
GSRWMHIVIH ER EGGYVEXI XVERWPEXMSR MJ MX I\
TVIWWIW XLI WEQI QIERMRK EW XLEX ERRS
XEXIH MR XLI KSPH WXERHEVH VIJIVIRGI XVERW
PEXMSR 3YV 760 FEWIH JWGSVI IZEPYEXMSR
QIXVMG LEW E  GSVVIPEXMSR GSIJwGMIRX
[MXL XLI LYQER NYHKIQIRX SR EHIUYEG]
[LMPI MR GSRXVEWX &0)9 LEW SRP] E 
GSVVIPEXMSR GSIJwGMIRX ERH XLI W]RXEGXMG
FEWIH18 IZEPYEXMSR QIXVMG 781LEW SRP]
 GSVVIPEXMSR GSIJwGMIRX [MXL XLI LY
QER NYHKIQIRX SR EHIUYEG] 3YV VIWYPXW
WXVSRKP] MRHMGEXI XLEX YWMRK WIQERXMG VSPI
PEFIPW JSV 18 IZEPYEXMSR GER FI WMKRMw
GERXP] QSVI IJJIGXMZI ERH FIXXIV GSVVIPEXIH
[MXL LYQER NYHKIQIRX SR EHIUYEG] XLER
&0)9 ERH 781
 -RXVSHYGXMSR
-R XLMW TETIV [I WLS[ XLEX IZEPYEXMRK QEGLMRI
XVERWPEXMSR UYEPMX] F] EWWIWWMRK XLI EGGYVEG] SJ
LYQER TIVJSVQERGI MR VIGSRWXVYGXMRK XLI WIQER
XMG JVEQIW JVSQ XLI 18 SYXTYX LEW E LMKLIV GSV
VIPEXMSR [MXL LYQER NYHKQIRX SR XVERWPEXMSR EH
IUYEG] XLER  XLI [MHIP]YWIH PI\MGEP RKVEQ
TVIGMWMSR FEWIH 18 IZEPYEXMSR QIXVMG &0)9 4E
TMRIRM IX EP  EW [IPP EW  XLI FIWXORS[R
W]RXEGXMG XVII TVIGMWMSR FEWIH 18 IZEPYEXMSR QIX
VMG 781 0MY ERH +MPHIE  %X XLI WEQI
XMQI YRPMOI WSQI LMKLP] PEFSV MRXIRWMZI IZEPYE
XMSR QIXVMGW WYGL EW ,8)6 7RSZIV IX EP Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10?20,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Structured vs. Flat Semantic Role Representations
for Machine Translation Evaluation
Chi-kiu Lo and DekaiWu
HKUST
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|dekai}@cs.ust.hk
Abstract
We argue that failing to capture the degree of
contribution of each semantic frame in a sen-
tence explains puzzling results in recent work
on the MEANT family of semantic MT eval-
uation metrics, which have disturbingly in-
dicated that dissociating semantic roles and
fillers from their predicates actually improves
correlation with human adequacy judgments
even though, intuitively, properly segregat-
ing event frames should more accurately re-
flect the preservation of meaning. Our anal-
ysis finds that both properly structured and
flattened representations fail to adequately ac-
count for the contribution of each seman-
tic frame to the overall sentence. We then
show that the correlation of HMEANT, the hu-
man variant of MEANT, can be greatly im-
proved by introducing a simple length-based
weighting scheme that approximates the de-
gree of contribution of each semantic frame
to the overall sentence. The new results
also show that, without flattening the struc-
ture of semantic frames, weighting the degree
of each frame?s contribution gives HMEANT
higher correlations than the previously best-
performing flattened model, as well as HTER.
1 Introduction
In this paper we provide a more concrete answer
to the question: what would be a better represen-
tation, structured or flat, of the roles in semantic
frames to be used in a semantic machine transla-
tion (MT) evaluation metric? We compare recent
studies on the MEANT family of semantic role la-
beling (SRL) based MT evaluation metrics (Lo and
Wu, 2010a,b, 2011a,b) by (1) contrasting their vari-
ations in semantic role representation and observing
disturbing comparative results indicating that segre-
gating the event frames in structured role representa-
tion actually damages correlation against human ad-
equacy judgments and (2) showing how SRL based
MT evaluation can be improved beyond the current
state-of-the-art compared to previous MEANT vari-
ants as well as HTER, through the introduction of
a simple weighting scheme that reflects the degree
of contribution of each semantic frame to the overall
sentence. The weighting scheme we propose uses
a simple length-based heuristic that reflects the as-
sumption that a semantic frame that covers more to-
kens contributes more to the overall sentence transla-
tion. We demonstrate empirically that when the de-
gree of each frame?s contribution to its sentence is
taken into account, the properly structured role rep-
resentation is more accurate and intuitive than the
flattened role representation for SRL MT evaluation
metrics.
For years, the task of measuring the performance
of MT systems has been dominated by lexical n-
gram based machine translation evaluation met-
rics, such as BLEU (Papineni et al, 2002), NIST
(Doddington, 2002), METEOR (Banerjee and Lavie,
2005), PER (Tillmann et al, 1997), CDER (Leusch
et al, 2006) and WER (Nie?en et al, 2000). These
metrics are excellent at ranking overall systems by
averaging their scores over entire documents. How-
ever, as MT systems improve, the shortcomings of
such metrics are becoming more apparent. Though
containing roughly the correct words, MT output at
the sentence remains often quite incomprehensible,
and fails to preserve the meaning of the input. This
results from the fact that n-gram based metrics are
not as reliable at ranking the adequacy of transla-
tions of individual sentences, and are particularly
10
poor at reflecting translation quality improvements
involving more meaningful word sense or semantic
frame decisions?which human judges have no trou-
ble distinguishing. Callison-Burch et al (2006) and
Koehn and Monz (2006), for example, study situ-
ations where BLEU strongly disagrees with human
judgment of translation quality.
Newer avenues of research seek substitutes for
n-gram based MT evaluation metrics that are bet-
ter at evaluating translation adequacy, particularly at
the sentence level. One line of research emphasizes
more the structural correctness of translation. Liu
and Gildea (2005) propose STM, a metric based on
syntactic structure, that addresses the failure of lex-
ical similarity based metrics to evaluate translation
grammaticality. However, the problem remains that
a grammatical translation can achieve a high syntax-
based score yet still make significant errors arising
from confusion of semantic roles. On the other hand,
despite the fact that non-automatic, manually evalu-
ated metrics, such as HTER (Snover et al, 2006), are
more adequacy oriented exhibit much higher correla-
tion with human adequacy judgment, their high labor
cost prohibits widespread use. There has also been
work on explicitly evaluating MT adequacy by ag-
gregating over a very large set of linguistic features
(Gime?nez and Ma`rquez, 2007, 2008) and textual en-
tailment (Pado et al, 2009).
2 SRL based MT evaluation metrics
A blueprint for more direct assessment of mean-
ing preservation across translation was outlined by
Lo and Wu (2010a), in which translation utility is
manually evaluated with respect to the accuracy of
semantic role labels. A good translation is one from
which human readers may successfully understand
at least the basic event structure??who did what
to whom, when, where and why? (Pradhan et al,
2004)?which represents the most essential meaning
of the source utterances. Adopting this principle,
the MEANT family of metrics compare the seman-
tic frames in reference translations against those that
can be reconstructed from machine translation out-
put.
Preliminary results reported in (Lo and Wu,
2010b) confirm that the blueprint model outper-
forms BLEU and similar n-gram oriented evalu-
ation metrics in correlation against human ade-
quacy judgments, but does not fare as well as
HTER. The more complete study of Lo and Wu
(2011a) introduces MEANT and its human variants
HMEANT, which implement an extended version of
blueprint methodology. Experimental results show
that HMEANT correlates against human adequacy
judgments as well as the more expensive HTER,
even though HMEANT can be evaluated using low-
cost untrained monolingual semantic role annotators
while still maintaining high inter-annotator agree-
ment (both are far superior to BLEU or other sur-
face oriented evaluation metrics). The study also
shows that replacing the human semantic role la-
belers with an automatic shallow semantic parser
yields an approximation that is still vastly superior
to BLEU while remaining about 80% as closely cor-
related with human adequacy judgments as HTER.
Along with additional improvements to the accu-
racy of the MEANT family of metrics, Lo and Wu
(2011b) study the impact of each individual seman-
tic role to themetric?s correlation against human ade-
quacy judgments, as well as the time cost for humans
to reconstruct the semantic frames and compare the
translation accuracy of the role fillers.
In general, the MEANT family of SRL MT eval-
uation metrics (Lo and Wu, 2011a,b) evaluate the
translation utility as follows. First, semantic role
labeling is performed (either manually or automat-
ically) on both the reference translation (REF) and
the machine translation output (MT) to obtain the
semantic frame structure. Then, the semantic pred-
icates, roles and fillers reconstructed from the MT
output are compared to those in the reference trans-
lations. The number of correctly and partially cor-
rectly annotated arguments of each type in each
frame of the MT output are collected in this step:
Ci,j ? # correct ARG i of PRED i in MT
Pi,j ? # partially correct ARG j of PRED i in MT
Mi,j ? total # ARG j of PRED i in MT
Ri,j ? total # ARG j of PRED i in REF
In the following three subsections, we describe
how the translation utility is calculated using these
counts in (a) the original blueprint model, (b) the
first version of HMEANT and MEANT using struc-
tured role representations, and (c) the more accu-
11
Figure 1: The structured role representation for the
blueprint SRL-based MT evaluation metric as proposed
in Lo and Wu (2010a,b), with arguments aggregated into
core and adjunct classes.
rate flattened-role implementation of HMEANT and
MEANT.
2.1 Structured core vs. adjunct role
representation
Figure 1 depicts the semantic role representation
in the blueprint model of SRL MT evaluation metric
proposed by Lo and Wu (2010a,b). Each sentence
consists of a number of frames, and each frame con-
sists of a predicate and two classes of arguments, ei-
ther core or adjunct. The frame precision/recall is
the weighted sum of the number of correctly trans-
lated roles (where arguments are grouped into the
core and adjunct classes) in a frame normalized by
the weighted sum of the total number of all roles in
that frame in the MT/REF respectively. The sen-
tence precision/recall is the sum of the frame preci-
sion/recall for all frames averaged by the total num-
ber of frames in the MT/REF respectively. The SRL
evaluation metric is then defined in terms of f-score
in order to balance the sentence precision and recall.
More precisely, assuming the above definitions of
Ci,j , Pi,j , Mi,j and Ri,j , the sentence precision and
recall are defined as follows.
precision =
?
i
wpred+
?
t
wt
(
?
j?t
(Ci,j+wpartialPi,j)
)
wpred+
?
t
wt
(
?
j?t
Mi,j
)
# frames in MT
recall =
?
i
wpred+
?
t
wt
(
?
j?t
(Ci,j+wpartialPi,j)
)
wpred+
?
t
wt
(
?
j?t
Ri,j
)
# frames in REF
Figure 2: The structured role representation for the
MEANT family of metrics as proposed in Lo and Wu
(2011a).
where wpred is the weight for predicates, and wt
where t ? {core, adj} is the weight for core argu-
ments and adjunct arguments. These weights rep-
resent the degree of contribution of the predicate
and different classes of arguments (either core or ad-
junct) to the overall meaning of the semantic frame
they attach to. In addition,wpartial is a weight control-
ling the degree to which ?partially correct? transla-
tions are penalized. All the weights can be automat-
ically estimated by optimizing the correlation with
human adequacy judgments.
We conjecture that the reason for the low correla-
tion with human adequacy judgments of this model
as reported in Lo and Wu (2010b) is that the ab-
straction of arguments actually reduces the repre-
sentational power of the original predicate-argument
structure in SRL. Under this representation, all the
arguments in the same class, e.g. all adjunct argu-
ments, are weighted uniformly. The assumption that
all types of arguments in the same class have the
same degree of contribution to their frame is obvi-
ously wrong, and the empirical results confirm that
the assumption is too coarse.
2.2 Structured role representation
Figure 2 shows the structured role representation
used in the MEANT family of metrics as proposed
in Lo and Wu (2011a), which avoids aggregating ar-
guments into core and adjunct classes. The design
of the MEANT family of metrics addresses the in-
correct assumption in the blueprint model by assum-
ing each type of argument has a unique weight repre-
senting its degree of contribution to the overall sen-
tence translation. Thus, the number of dimensions of
12
the weight vector is increased to allow an indepen-
dent weight to be assigned to each type of argument.
Unlike the previous representation in the blueprint
model, there is no aggregation of arguments into
core and adjunct classes. Each sentence consists of a
number of frames, and each frame consists of a pred-
icate and a number of arguments of type j.
Under the new approach, the frame preci-
sion/recall is the weighted sum of the number of cor-
rectly translated roles in a frame normalized by the
weighted sum of the total number of all roles in that
frame in the MT/REF respectively. Similar to the
previous blueprint representation, the sentence pre-
cision/recall is the sum of the frame precision/recall
for all frames averaged by the total number of frames
in the MT/REF respectively. More precisely, fol-
lowing the previous definitions of Ci,j , Pi,j , Mi,j ,
Ri,j ,wpred andwpartial, the sentence precision and re-
call are redefined as follows.
precision =
?
i
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjMi,j
#frames in MT
recall =
?
i
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjRi,j
# frames in REF
where wj is the weight for the arguments of type j.
Theseweights represent the degree of contribution of
different types of arguments to the overall meaning
of their semantic frame.
2.3 Flat role representation
Figure 3 depicts the flat role representation used in
themore accurate variants ofMEANT as proposed in
Lo andWu (2011b). This representation ismotivated
by the studies of the impact of individual seman-
tic role. The highly significant difference between
this flat representation and both of the previous two
structured role representations is that the semantic
frames in the sentence are no longer segregated.
The flat role representation desegregates the frame
structure, resulting in a flat, single level structure.
Therefore, there is no frame precision/recall. The
sentence precision/recall is the weighted sum of the
number of correctly translated roles in all frames nor-
malized by the weighted sum of the total number of
roles in all frames in theMT/REF respectively. More
precisely, again assuming the previous definitions of
Ci,j , Pi,j , Mi,j , Ri,j and wpartial, the sentence preci-
sion and recall are redefined as follows.
Cpred ? total # correctly translated predicates
Mpred ? total # predicates in MT
Rpred ? total # predicates in REF
precision =
wpredCpred +
?
j wj
(
?
i(Ci,j + wpartialPi,j)
)
wpredMpred +
?
j wj
(
?
i Mi,j
)
recall =
wpredCpred +
?
j wj
(
?
i(Ci,j + wpartialPi,j)
)
wpredRpred +
?
j wj
(
?
i Ri,j
)
Note that there is a small modification of the defini-
tion of wpred and wj . Instead of the degree of contri-
bution to the overall meaning of the semantic frame
that the roles attached to, wpredand wj now represent
the degree of contribution of the predicate and the ar-
guments of type j to the overall meaning of the entire
sentence.
It is worth noting that the semantic role features in
the ULC metric proposed by Gime?nez and Ma`rquez
(2008) also employ a flat feature-based represen-
tation of semantic roles. However, the definition
of those semantic role features adopts a different
methodology for determining the role fillers? transla-
tion accuracy, which prevents a controlled consistent
environment for the comparative experiments that
the present work focuses on.
3 Experimental setup
The evaluation data for our experiments consists
of 40 sentences randomly drawn from the DARPA
GALE program Phase 2.5 newswire evaluation cor-
pus containing Chinese input sentence, English ref-
erence translations, and themachine translation from
three different state-of-the-art GALE systems. The
Chinese and the English reference translation have
both been annotated with gold standard PropBank
(Palmer et al, 2005) semantic role labels. The
weightswpred, wcore, wadj, wj and wpartial can be esti-
mated by optimizing correlation against human ade-
quacy judgments, using any of themany standard op-
timization search techniques. In the work of Lo and
13
Figure 3: The flat role representation for the MEANT family of metrics as proposed in Lo and Wu (2011b) .
Wu (2011b), the correlations of all individual roles
with the human adequacy judgments were found to
be non-negative, therefore we found grid search to
be quite adequate for estimating the weights. We use
linear weighting because we would like to keep the
metric?s interpretation simple and intuitive.
Following the benchmark assessment in NIST
MetricsMaTr 2010 (Callison-Burch et al, 2010), we
assess the performance of the semantic MT evalua-
tion metric at the sentence level using the summed-
diagonal-of-confusion-matrix score. The human ad-
equacy judgments were obtained by showing all
three MT outputs together with the Chinese source
input to a human reader. The human reader was in-
structed to order the sentences from the three MT
systems according to the accuracy of meaning in
the translations. For the MT output, we ranked the
sentences from the three MT systems according to
their evaluation metric scores. By comparing the
two sets of rankings, a confusion matrix is formed.
The summed diagonal of confusion matrix is the per-
centage of the total count when a particular rank by
the metric?s score exactly matches the human judg-
ments. The range of possible values of summed di-
agonal of confusion matrix is [0,1], where 1 means
all the systems? ranks determined by the metric are
identical with that of the human judgments and 0
means all the systems? ranks determined by the met-
ric are different from that of the human judgment.
Since the summed diagonal of confusion matrix
scores only assess the absolute ranking accuracy,
we also report the Kendall?s ? rank correlation co-
efficients, which measure the correlation of the pro-
posed metric against human judgments with respect
to their relative ranking of translation adequacy. A
higher the value for ? indicates the more similar the
ranking by the evaluation metric to the human judg-
ment. The range of possible values of correlation
Table 1: Sentence-level correlations against human ade-
quacy judgments as measured by Kendall?s ? and summed
diagonal of confusion matrix as used in MetricsMaTr
2010. ?SRL - blueprint? is the blueprint model described
in section 2.1. ?HMEANT (structured)? is HMEANT us-
ing the structured role representation described in sec-
tion 2.2. ?HMEANT (flat)? is HMEANT using the flat
role representation described in section 2.3.
Metric Kendall MetricsMaTr
HMEANT (flat) 0.4685 0.5583
HMEANT (structured) 0.4324 0.5083
SRL - blueprint 0.3784 0.4667
coefficient is [-1,1], where 1 means the systems are
ranked in the same order as the human judgment and
-1 means the systems are ranked in the reverse order
as the human judgment.
4 Round 1: Flat beats structured
Our first round of comparative results quantita-
tively assess whether a structured role representation
(that properly preserves the semantic frame struc-
ture, which is typically hierarchically nested in com-
positional fashion) outperforms the simpler (but less
intuitive, and certainly less linguistically satisfying)
flat role representation.
As shown in table 1, disturbingly, HMEANT us-
ing flat role representations yields higher correla-
tions against human adequacy judgments than us-
ing structured role representations, regardless of
whether role types are aggregated into core and
adjunct classes. The results are consistent for
both Kendall?s tau correlation coefficient and Met-
ricsMaTr?s summed diagonal of confusion matrix.
HMEANT using a flat role representation achieved
a Kendall?s tau correlation coefficient and summed
diagonal of confusion matrix score of 0.4685 and
0.5583 respectively, which is superior to both
14
Figure 4: The new proposed structured role representa-
tion, incorporating a weighting scheme reflecting the de-
gree of contribution of each semantic frame to the overall
sentence.
HMEANT using a structured role representation
(0.4324 and 0.5083 respectively) and the blueprint
model (0.3784 and 0.4667 respectively).
Error analysis, in light of these surprising results,
strongly suggests that the problem lies in the design
which uniformly averages the frame precision/recall
over all frames in a sentence when computing the
sentence precision/recall. This essentially assumes
that each frame in a sentence contributes equally
to the overall meaning in the sentence translation.
Such an assumption is trivially wrong and could well
hugely degrade the advantages of using a structured
role representation for semanticMT evaluation. This
suggests that the structured role representation could
be improved by also capturing the degree of contri-
bution of each frame to the overall sentence transla-
tion.
5 Capturing the importance of each frame
To address the problem in the previousmodels, we
introduce a weighting scheme to reflect the degree
of contribution of each semantic frame to the overall
sentence. However, unlike the contribution of each
role to a frame, the contribution of each frame to
the overall sentence cannot be estimated across sen-
tences. This is because unlike semantic roles, which
can be identified by their types, frames do not neces-
sarily have easily defined types, and their construc-
tion is also different from sentence to sentence so that
the positions of their predicates in the sentence are
the only way to identify the frames. However, the
degree of contribution of each frame does not depend
on the position of the predicate in the sentence. For
example, the two sentences I met Tom when I was go-
ing home andWhen I was walking home, I saw Tom have
similar meanings. The verbs met and saw are the
predicates of the key event frames which contribute
more to the overall sentences, whereas going and
walking are the predicates of the minor nested event
frames (in locative manner roles of the key event
frames) and contribute less to the overall sentences.
However, the two sentences are realized with differ-
ent surface constructions, and the two key frames are
in different positions. Therefore, the weights learned
from one sentence cannot directly be applied to the
other sentence.
Instead of estimating the weight of each frame us-
ing optimization techniques, wemake an assumption
that a semantic frame filled with more word tokens
expresses more concepts and thus contributes more
to the overall sentence. Following this assumption,
we determine the weights of each semantic frame by
its span coverage in the sentence. In other words,
the weight of each frame is the percentage of word
tokens it covers in the sentence.
Figure 4 depicts the structured role representa-
tion with the proposed new frame weighting scheme.
The significant difference between this representa-
tion and the structured role representation in the
MEANT variants proposed in Lo and Wu (2011a)
is that each frame is now assigned an independent
weight, which is its span coverage in the MT/REF
when obtaining the frame precision/recall respec-
tively.
As in Lo and Wu (2011a), each sentence consists
of a number of frames, and each frame consists of
a predicate and a number of arguments of type j.
Each type of argument is assigned an independent
weight to represent its degree of contribution to the
overall meaning of the semantic frame they attached
to. The frame precision/recall is the weighted sum
of the number of correctly translated roles in a frame
normalized by the weighted sum of the number of all
roles in that frame in the MT/REF. The sentence pre-
cision/recall is the weighted sum of the frame preci-
sion/recall for all frames normalized by the weighted
sum of the total number of frames in MT/REF re-
spectively. More precisely, again assuming the ear-
15
lier definitions of Ci,j , Pi,j , Mi,j , Ri,j , wpred and
wpartial in section 2, the sentence precision and recall
are redefined as follows.
mi ?
# tokens filled in frame i of MT
total # tokens in MT
ri ?
# tokens filled in frame i of REF
total # tokens in REF
precision =
?
imi
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjMi,j
?
imi
recall =
?
i ri
wpred+
?
j
wj(Ci,j+wpartialPi,j)
wpred+
?
j
wjRi,j
?
i ri
where mi and ri are the weights for frame i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence.
6 Round 2: Structured beats flat
We now assess the performance of the new pro-
posed structured role representation, by comparing
it with the previous models under the same experi-
mental setup as in section 4. We have also run con-
trastive experiments against BLEU and HTER un-
der the same experimental conditions. In addition,
to investigate the consistency of results for the au-
tomated variants of MEANT, we also include com-
parative experiments where shallow semantic pars-
ing (ASSERT) replaces human semantic role label-
ers for each model of role representation.
Figure 5 shows an example where HMEANTwith
the frame weighting scheme outperforms HMEANT
using other role representations in correlation against
human adequacy judgments. IN is the Chinese
source input. REF is the corresponding refer-
ence translation. MT1, MT2 and MT3 are the
three corresponding MT output. The human ade-
quacy judgments for this set of translation are that
MT1>MT3>MT2. HMEANT with the proposed
frame weighting predicts the same ranking order
as the human adequacy judgment, while HMEANT
with the flat role representation and HMEANT
with the structured role representation without frame
weighting both predict MT3>MT1>MT2. There
are four semantic frames in IN while there are only
three semantic frames in the REF. This is because
the predicate ?? in IN is translated in REF as had
which is not a predicate. However, for the same
frame, both MT1 and MT2 translated ARG1???
? into the predicate affect, while MT3 did not trans-
late the predicate ?? and translated the ARG1 ?
??? into the noun phrase adverse impact. There-
fore, using the flat role representation or the previ-
ous structured role representation which assume all
frames have an identical degree of contribution to the
overall sentence translation, MT1?s and MT2?s sen-
tence precision is greatly penalized for having one
more extra frame than the reference. In contrast, ap-
plying the frame weighting scheme, the degree of
contribution of each frame is adjusted by its token
coverage. Therefore, the negative effect of the less
important extra frames is minimized, allowing the
positive effect of correctly translating more roles in
more important frames to be more appropriately re-
flected.
Table 2 shows that HMEANT with the proposed
new frameweighting scheme correlatesmore closely
with human adequacy judgments than HMEANT
using the previous alternative role representations.
The results from Kendall?s tau correlation coeffi-
cient and MetricsMaTr?s summed diagonal of con-
fusion matrix analysis are consistent. HMEANT
using the frame-weighted structured role represen-
tation achieved a Kendall?s tau correlation coef-
ficient and summed diagonal of confusion matrix
score of 0.2865 and 0.575 respectively, bettering
both HMEANT using the flat role representation
(0.4685 and 0.5583) and HMEANT using the pre-
vious un-frame-weighted structured role representa-
tion (0.4324 and 0.5083).
HMEANT using the improved structured role rep-
resentation also outperforms other commonly used
MT evaluation metrics. It correlates with human ad-
equacy judgments more closely than HTER (0.4324
and 0.425 in Kendall?s tau correlation coefficient and
summed diagonal of confusionmatrix, respectively).
It also correlates with human adequacy judgments
significantly more closely than BLEU (0.1982 and
0.425).
Turning to the variants that replace human SRL
with automated SRL, table 2 shows that MEANT
16
Figure 5: Example input sentence along with reference and machine translations, annotated with semantic frames in
Propbank format. The MT output is annotated with semantic frames by minimally trained humans. HMEANT with
the new frame-weighted structured role representation successfully ranks the MT output in an order that matches with
human adequacy judgments (MT1>MT3>MT2), whereas HMEANT with a flat role representation or the previous
un-frame-weighted structured role representation fails to rank MT1 and MT3 in an order that matches with human
adequacy judgments. See section 6 for details.
17
Table 2: Sentence-level correlations against human ade-
quacy judgments as measured by Kendall?s ? and summed
diagonal of confusion matrix as used in MetricsMaTr
2010. ?SRL - blueprint?, ?HMEANT (structured)? and
?HMEANT (flat)? are the same as in table 1. ?MEANT
(structured)? and ?MEANT (flat)? use automatic rather
than human SRL. ?MEANT (frame)? and ?HMEANT
(frame)? are MEANT/HMEANT using the structured
role representation with the frame weighting scheme de-
scribed in section 5.
Metric Kendall MetricsMaTr
HMEANT (frame) 0.4865 0.575
HMEANT (flat) 0.4685 0.5583
HMEANT (structured) 0.4324 0.5083
HTER 0.4324 0.425
SRL - blueprint 0.3784 0.4667
MEANT (frame) 0.3514 0.4333
MEANT (structured) 0.3423 0.425
MEANT (flat) 0.3333 0.425
BLEU 0.1982 0.425
using the new frame-weighted structured role repre-
sentation yields an approximation that is about 81%
as closely correlated with human adequacy judgment
as HTER, and is better than all previous MEANT
variants using alternative role representations. All
results consistently confirm that using a structured
role representation with the new frame weighting
scheme, which captures the event structure and an
approximate degree of contribution of each frame to
the overall sentence, outperforms using a flat role
representation for SRL based MT evaluation met-
rics.
7 Conclusion
We have shown how the MEANT family of SRL
based MT evaluation metrics is significantly im-
proved beyond the state-of-the-art for both HTER
and previous variants of MEANT, through the in-
troduction of a simple but well-motivated weight-
ing scheme to reflect the degree of contribution of
each semantic frame to the overall sentence trans-
lation. Following the assumption that a semantic
frame filled with more word tokens tends to express
more concepts, the new model weight each frame
by its span coverage. Consistent experimental re-
sults have been demonstrated under conditions uti-
lizing both human and automatic SRL. Under the
new frame weighted representation, properly nested
structured semantic frame representations regain an
empirically preferred position over the less intuitive
and linguistically unsatisfying flat role representa-
tions.
One future direction of this work will be to com-
pare MEANT against the feature based and string
based representations of semantic relations in ULC.
Such a comparison could yield a more complete
credit/blame perspective on the representationmodel
when operating under the condition of using auto-
matic SRL.
Another interesting extension of this work would
be to investigate the discriminative power of the
MEANT family of metrics to distinguish distances
in translation adequacy. In this paper we confirmed
that the MEANT family of metrics are stable in cor-
relation with human ranking judgments of transla-
tion adequacy. Further studies could focus on the
correlation of the MEANT family of metrics against
human scoring. We also plan to experiment on meta-
evaluating MEANT on a larger scale in other genres
and for other language pairs.
Acknowledgments
This material is based uponwork supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under GALE Contract Nos. HR0011-06-
C-0022 and HR0011-06-C-0023 and by the Hong
Kong Research Grants Council (RGC) research
grants GRF621008, GRF612806, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In
43th AnnualMeeting of the Association of Compu-
tational Linguistics (ACL-05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In 13th Confer-
18
ence of the European Chapter of the Association
for Computational Linguistics (EACL-06), pages
249?256, 2006.
Chris Callison-Burch, Philipp Koehn, Christof
Monz, Kay Peterson, Mark Pryzbocki, and Omar
Zaidan. Findings of the 2010 Joint Workshop
on Statistical Machine Translation and Metrics
for Machine Translation. In Joint 5th Workshop
on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, 15-16
July 2010.
G. Doddington. Automatic Evaluation of Machine
Translation Quality using N-gram Co-occurrence
Statistics. In 2nd International Conference on Hu-
man Language Technology Research (HLT-02),
pages 138?145, San Francisco, CA, USA, 2002.
Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu?is Ma`rquez. Linguistic Fea-
tures for Automatic Evaluation of Heterogenous
MT Systems. In 2nd Workshop on Statistical Ma-
chine Translation, pages 256?264, Prague, Czech
Republic, June 2007. Association for Computa-
tional Linguistics.
Jesu?s Gime?nez and Llu?is Ma`rquez. A Smorgas-
bord of Features for Automatic MT Evaluation. In
3rd Workshop on Statistical Machine Translation,
pages 195?198, Columbus, OH, June 2008. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation be-
tween European Languages. In Workshop on
Statistical Machine Translation, pages 102?121,
2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Efficient MT Evaluation Using Block
Movements. In 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL-06), 2006.
Ding Liu and Daniel Gildea. Syntactic Features for
Evaluation of Machine Translation. In ACLWork-
shop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summariza-
tion, page 25, 2005.
Chi-Kiu Lo and Dekai Wu. Evaluating Machine
Translation Utility via Semantic Role Labels. In
7th International Conference on Language Re-
sources and Evaluation (LREC-2010), 2010.
Chi-Kiu Lo and Dekai Wu. Semantic vs. Syntac-
tic vs. N-gram Structure for Machine Translation
Evaluation. In Proceedings of the 4th Workshop
on Syntax and Structure in Statistical Translation
(SSST-4), 2010.
Chi-Kiu Lo and Dekai Wu. MEANT: An Inexpen-
sive, High-Accuracy, Semi-Automatic Metric for
Evaluating Translation Utility based on Seman-
tic Roles. In Joint conference of the 49th Annual
Meeting of the Association for Computational Lin-
guistics : Human Language Technologies (ACL
HLT 2011), 2011.
Chi-Kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
To appear in 22nd International Joint Conference
on Artificial Intelligence, 2011.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. A Evaluation Tool for Machine
Translation: Fast Evaluation for MT Research. In
2nd International Conference on Language Re-
sources and Evaluation (LREC-2000), 2000.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Chris Manning. Robust Machine Translation
Evaluation with Entailment Features. In Joint
conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics and the
4th International Joint Conference on Natural
Language Processing of the Asian Federation of
Natural Language Processing (ACL-IJCNLP-09),
2009.
Martha Palmer, Daniel Gildea, and Paul Kings-
bury. The Proposition Bank: an Annotated Cor-
pus of Semantic Roles. Computational Linguis-
tics, 31(1):71?106, 2005.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-02), pages 311?318,
2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
19
In 2004 Conference on Human Language Tech-
nology and the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL-04), 2004.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A Study of
Translation Edit Rate with Targeted Human An-
notation. In 7th Conference of the Association for
Machine Translation in the Americas (AMTA-06),
pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Arkaitz Zubiaga, and Hassan Sawaf. Accelerated
DP Based Search For Statistical Translation. In
5th European Conference on Speech Communica-
tion and Technology (EUROSPEECH-97), 1997.
20
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 70?78,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Reestimation of Reified Rules in Semiring Parsing and Biparsing
Markus Saers and Dekai Wu
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|dekai}@cs.ust.hk
Abstract
We show that reifying the rules from hyper-
edge weights to first-class graph nodes au-
tomatically gives us rule expectations in any
kind of grammar expressible as a deductive
system, without any explicit algorithm for cal-
culating rule expectations (such as the inside-
outside algorithm). This gives us expecta-
tion maximization training for any grammar
class with a parsing algorithm that can be
stated as a deductive system, for free. Having
such a framework in place accelerates turn-
over time for experimenting with new gram-
mar classes and parsing algorithms?to imple-
ment a grammar learner, only the parse forest
construction has to be implemented.
1 Introduction
We propose contextual probability as a quantity that
measures how often something has been used in
a corpus, and when calculated for rules, it gives
us everything needed to calculate rule expectations
for expectation maximization. For labeled spans in
context-free parses, this quantity is called outside
probability, and in semiring (bi-) parsing, it is called
reverse value. The inside-outside algorithm for rees-
timating context-free grammar rules uses this quan-
tity for the symbols occurring in the parse forest.
Generally, the contextual probability is:
The contextual probability of something
is the sum of the probabilities
of all contexts where it was used.
For symbols participating in a parse, we could state
it like this:
The contextual probability of an item
is the sum of the probabilities
of all contexts where it was used.
. . . which is exactly what we mean with outside
probability. In semiring (bi-) parsing, this quantity
is called reverse value, but in this framework it is
also defined for rules, which means that we could
restate our boxed statement as:
The contextual probability of a rule
is the sum of the probabilities
of all contexts where it was used.
This opens up an interesting line of inquiry into what
this quantity might represent. In this paper we show
that the contextual probabilities of the rules contain
precisely the new information needed in order to cal-
culate the expectations needed to reestimate the rule
probabilities. This line of inquiry was discovered
while working on a preterminalized version of lin-
ear inversion transduction grammars (LITGs), so we
will use these preterminalized LITGs (Saers and Wu,
2011) as an example throughout this paper.
We will start by examining semiring parsing
(parsing as deductive systems over semirings, Sec-
tion 3), followed by a section on how this relates to
weighted hypergraphs, a common representation of
parse forests (Section 4). This reveals a disparity be-
tween weighted hypergraphs and semiring parsing.
It seems like we are forced to choose between the
inside-outside algorithm for context-free grammars
70
on the one side, and the flexibility of grammar for-
malism and parsing algorithm development afforded
by semiring (bi-) parsing. It is, however, possible to
have both, which we will show in Section 5. An
integral part of this unification is the concept of con-
textual probability. Finally, we will offer some con-
clusions in Section 6.
2 Background
A common view on probabilistic parsing?be it
bilingual or monolingual?is that it involves the
construction of a weighted hypergraph (Billot and
Lang, 1989; Manning and Klein, 2001; Huang,
2008). This is an appealing conceptualization, as it
separates the construction of the parse forest (the ac-
tual hypergraph) from the probabilistic calculations
that need to be carried out. The calculations are,
in fact, given by the hypergraph itself. To get the
probability of the sentence (pair) being parsed, one
simply have to query the hypergraph for the value
of the goal node. It is furthermore possible to ab-
stract away the calculations themselves, by defining
the hypergraph over an arbitrary semiring. When the
Boolean semiring is used, the value of the goal node
will be true if the sentence (pair) is a member of the
language (or transduction) defined by the grammar,
and false otherwise. When the probabilistic semir-
ing is used, the probability of the sentence (pair) is
attained, and with the tropical semiring, the proba-
bility of the most likely tree is attained. To further
generalize the building of the hypergraph?the pars-
ing algorithm?a deductive system can be used. By
defining a hand-full of deductive rules that describe
how items can be constructed, the full complexi-
ties of a parsing algorithm can be very succinctly
summarized. Deductive systems to represent parsers
and semirings to calculate the desired values for the
parses were introduced in Goodman (1999).
In this paper we will reify the grammar rules
by moving them from the meta level to the object
level?effectively making them first-class citizens of
the parse trees, which are no longer weighted hyper-
graphs, but mul/add-graphs. This move allows us
to calculate rule expectations for expectation maxi-
mization (Dempster et al, 1977) as part of the pars-
ing process, which significantly shortens turn-over
time for experimenting with different grammar for-
malisms.
Another approach which achieve a similar goal is
to use a expectation semiring (Eisner, 2001; Eisner,
2002; Li and Eisner, 2009). In this semiring, all val-
ues are pairs of probabilities and expectations. The
inside-outside algorithm with the expectation semir-
ing requires the usual inside and outside calcula-
tions over the probability part of the semiring val-
ues, followed by a third traversal over the parse for-
est to populate the expectation part of the semiring
values. The approach taken in this paper also re-
quires the usual inside and outside calculations, but
o third traversal of the parse forest. Instead, the pro-
posed approach requires two passes over the rules
of the grammar per EM iteration. The asymptotic
time complexities are thus equivalent for the two ap-
proaches.
2.1 Notation
We will use w to mean a monolingual sentence,
and index the individual tokens from 0 to |w| ? 1.
This means that w = w0, . . . , w|w|?1. We will fre-
quently use spans from this sentence, and denote
them wi..j , which is to be interpreted as array slices,
that is: including the token at position i, but ex-
cluding the token at position j (the interval [i, j)
over w, or wi, . . . , wj?1). A sentence w thus cor-
responds to the span w0..|w|. We will also assume
that there exists a grammar G = ?N,?, S,R? or a
transduction grammar (over languages L0 and L1)
G = ?N,?,?, S,R? (depending on the context),
where N is the set of nonterminal symbols, ? is a
set of (L0) terminal symbols, ? is a set of (L1) ter-
minal symbols, S ? N is the dedicated start symbol
and R is a set of rules appropriate to the grammar.
A stochastic grammar is further assumed to have a
parameterization function ?, that assigns probabili-
ties to all the rules in R. For general L0 tokens we
will use lower case letters from the beginning of the
alphabet, and for L1 from the end of the alphabet.
For specific sentences we will use e = e0..|e| to rep-
resent an L0 sentence and f = f0..|f | to represent an
L1 sentence.
3 Semiring parsing
Semiring parsing was introduced in Goodman
(1999), as a unifying approach to parsing. The gen-
71
eral idea is that any parsing algorithm can be ex-
pressed as a deductive system. The same algorithm
can then be used for both traditional grammars and
stochastic grammars by changing the semiring used
in the deductive system. This approach thus sepa-
rates the algorithm from the specific calculations it
is used for.
Definition 1. A semiring is a tuple ?A,?,?,0,1?,
where A is the set the semiring is defined over, ? is
an associative, commutative operator over A, with
identity element 0 and ? is an associative operator
over A distributed over ?, with identity element 1.
Semirings can be intuitively understood by consid-
ering the probabilistic semiring: ?R+,+,?, 0, 1?,
that is: the common meaning of addition and
multiplication over the positive real numbers (in-
cluding zero). Although this paper will have a
heavy focus on the probabilistic semiring, sev-
eral other exists. Among the more popular are
the Boolean semiring ?{>,?},?,?,?,>? and the
tropical semiring ?R+ ? {?},min,+,?, 0? (or
?R? ? {??},max,+,??, 0? which can be used
for probabilities in the logarithmic domain).
The deductive systems used in semiring parsing
have three components: an item representation, a
goal item and a set of deductive rules. Taking
CKY parsing (Cocke, 1969; Kasami and Torii, 1969;
Younger, 1967) as an example, the items would have
the form Ai,j , which is to be interpreted as the span
wi..j of the sentence being parsed, labeled with the
nonterminal symbol A. The goal item would be
S0,|w|: the whole sentence labeled with the start
symbol of the grammar. Since the CKY algorithm
is a very simple parsing algorithm, it only has two
deductive rules:
A? a, Ia(wi..j)
Ai,j
0?i?j?|w| (1)
Bi,k, Ck,j , A? BC
Ai,j
(2)
Where Ia(?) is the terminal indicator function for the
semiring. The general form of a deductive rule is
that the conditions (entities over the line) yield the
consequence (the entity under the line) given that
the side conditions (to the right of the line) are satis-
fied. We will make a distinction between conditions
that are themselves items, and conditions that are
not. The non-item conditions will be called axioms,
and are exemplified above by the indicator function
(Ia(wi..j) which has a value that depends only on the
sentence) and the rules (A? a andA? BC which
have values that depends only on the grammar).
The indicator function might seem unnecessary,
but allows us to reason under uncertainty regarding
the input. In this paper, we will assume that we have
perfect knowledge of the input (but for generality,
we will not place it as a side condition). The func-
tion is defined such that:
?a ? ?? : Ia(w) =
{
1 if a = w
0 otherwise
An important concept of semiring parsing is that
the deductive rules also specify how to arrive at the
value of the consequence. Since it is the first value
computed for a node, we will call it ?, and the gen-
eral way to calculate it given a deductive rule and the
?-values of the conditions is:
?(b) =
n?
i=1
?(ai) iff
a1, . . . , an
b
c1,...,cm
If the same consequence can be produced in several
ways, the values are summed using the ? operator:
?(b) =
?
n,a1,...,an
such that
a1,...,an
b
n?
i=1
?(ai)
The ?-values of axioms depend on what kind of ax-
iom it is. For the indicator function, the ?-value is
the value of the function, and for grammar rules, the
?-value is the value assigned to the rule by the pa-
rameterization function ? of the grammar.
The ?-value of a consequence corresponds to the
value of everything leading up to that consequence.
If we are parsing with a context-free grammar and
the probabilistic semiring, this corresponds to the in-
side probability.
3.1 Reverse values
When we want to reestimate rule probabilities, it is
not enough to know the probabilities of arriving at
different consequences, we also need to know how
likely we are to need the consequences as a condi-
tion for other deductions. These values are called
72
S ? A
A0,|e|,0,|f |
,
As,s,u,u, A? /
G
,
Bs?,t,u?,v, B ? [XA], X ? a/x , Ia/x (
es..s?/fu..u? )
As,t,u,v
0?s?s?,
0?u?u?,
Bs,t?,u,v? , B ? [AX], X ? a/x , Ia/x (
et?..t/fv?..v )
As,t,u,v
t??t?|e|,
v??v?|f |,
Bs?,t,u,v? , B ? ?XA?, X ? a/x , Ia/x (
es..s?/fv?..v )
As,t,u,v
0?s?s?,
v??v?|f |,
Bs,t?,u?,v, B ? ?AX?, X ? a/x , Ia/x (
et?..t/fu..u? )
As,t,u,v
t??t?|e|,
0?u?u?
Figure 2: Deductive system describing a PLITG parser. The symbols A, B and S are nonterminal symbols, while X
represents a preterminal symbol.
S ? A
A0,|e|,0,|f |
,
As,s,u,u, A? /
G
,
Bs?,t,u?,v, B ? [a/x A], Ia/x (
es..s?/fu..u? )
As,t,u,v
0?s?s?,
0?u?u?,
Bs,t?,u,v? , B ? [A a/x ], Ia/x (
et?..t/fv?..v )
As,t,u,v
t??t?|e|,
v??v?|f |,
Bs?,t,u,v? , B ? ?a/x A?, Ia/x (
es..s?/fv?..v )
As,t,u,v
0?s?s?,
v??v?|f |,
Bs,t?,u?,v, B ? ?A a/x ?, Ia/x (
et?..t/fu..u? )
As,t,u,v
t??t?|e|,
0?u?u?
Figure 1: Deductive system describing an LITG parser.
reverse values in Goodman (1999), and outside
probabilities in the inside-outside algorithm (Baker,
1979). In this paper we will call them contextual
values, or ?-values (since they are the second value
we calculate).
The way to calculate the reverse values is to start
with the goal node and work your way back to the
axioms. The reverse value is calculated to be:
?(x) =
?
n,i,b,a1,...,an
such that
a1,...,an
b ?x=ai
?(b)?
?
{j|1?j?n,j 6=i}
?(aj)
That is: the reverse value of the consequence com-
bined with the values of all sibling conditions is cal-
culated and summed for all deductive rules where
the item is a condition.
3.2 SPLITG
After we introduced stochastic preterminalized
LITGs (Saers, 2011, SPLITG), the idea of express-
ing them in term of semiring parsing occurred. This
is relatively straight forward, producing a compact
set of deductive rules similar to that of LITGs. For
LITGs, the items take the form of bispans labeled
with a symbol. We will represent these bispans as
As,t,u,v, where A is the label, and the two spans be-
ing labeled are es..t and fu..v. Since we usually do
top-down parsing, the goal item is a virtual item (G)
than can only be reached by rewriting a nontermi-
nal to the empty bistring ( / ). Figure 1 shows the
deductive rules for LITG parsing.
A preterminalized LITG promote preterminal
symbols to a distinct class of symbols in the gram-
mar, which is only allowed to rewrite into bitermi-
nals. Factoring out the terminal productions in this
fashion allows the grammar to define one probability
distribution over all the biterminals, which is useful
for bilexica induction. It also means that the LITG
rules that produce biterminals have to be replaced
by two rules in a PLITG, resulting in the deductive
rules in Figure 2.
4 Weighted hypergraphs
A hypergraph is a graph where the nodes are con-
nected with hyperedges. A hyperedge is an edge
that can connect several nodes with one node?it has
73
Figure 3: A weighted hyperedge between three nodes,
based on the rule A ? BC. The tip of the arrow points
to the head of the edge, and the two ends are the tails. The
dashed line idicates where the weight of the edge comes
from.
one head, but may have any number of tails. Intu-
itively, this is a good match to context-free gram-
mars, since each rule connects one symbol on the
left hand side (the head of the hyperedge) with any
number of symbols on the right hand side (the tails
of the hyperedge). During parsing, one node is con-
structed for each labeled (bi-) span, and the nodes
are connected with hyperedges based on the valid
applications of rules. A hyperedge will be repre-
sented as [h : t1, . . . , tn] where h is the head and ti
are the tails.
When this is applied to weighted grammar, each
hyperedge can be associated with a weight, making
the hypergraph weighted. Every time an edge is tra-
versed, its weight is combined with the value travel-
ling through the edge. Weights are assigned to hy-
peredges via a weighting function w(?).
Figure 3 contains an illustration of a weighted
hyperedge. The arrow indicates the edge itself,
whereas the dotted line indicates where the weight
comes from. Since each hyperedge corresponds to
exactly one rule from a stochastic context-free gram-
mar, we can use the inside-outside algorithm (Baker,
1979) to calculate inside and outside probabilities as
well as to reestimate the probabilities of the rules.
What we cannot easily do, however, is to change the
parsing algorithm or grammar formalism.
If the weighted hyperedge approach was a one-to-
one mapping to the semiring parsing approach, we
could, but it is not. The main difference is that rules
are part of the object level in semiring parsing, but
Figure 4: The same hyperedge as in Figure 3, where the
rule has been promoted to first-class citizen. The hyper-
edge is no longer weighted.
part of the meta level in weighted hypergraphs. To
address this disparity, we will reify the rules in the
weighted hypergraph to make them nodes. Figure 4
shows the same hyperedge as Figure 3, but with the
rule as a proper node rather than a weight associ-
ated with the hyperedge. These hyperedges are ag-
nostic to what the tail nodes represent, so we can no
longer use the inside-outside algorithm to reestimate
the rule probabilities. We can, however, still calcu-
late inside probabilities. In the weighted hyperedge
approach, the inside probability of a node is:
?(p) =
?
n,q1,...,qn
such that
[p:q1,...qn]
w([p : q1, . . . , qn])?
n?
i=1
?(qi)
Whereas with the rules reified, the weight simply
moved into the tail product:
?(p)
?
n,q1,...,qn
such that
[p:q1,...qn]
n?
i=1
?(qi)
By virtue of the deductive system used to build the
hypergraph, we also have the reverse values, which
correspond to outside probability:
?(x) =
?
i,p,n,q1,...,qn
such that
[p:q1,...qn]?x=qi
?(p)?
?
{j|1?j?n,j 6=i}
?(qj)
This means that we have the inside and outside prob-
abilities of the nodes, and we could shoe-horn it into
the reestimation part of the inside-outside algorithm.
74
It also means that we have ?-values for the rules,
which we are calculating as a side-effect of moving
them into the object level. In Section 5, we will take
a closer look at the semantics of the contextual prob-
abilities that we are in fact calculating for the reified
rules, and see how they can be used in reestimation
of the rules.
4.1 SPLITG
Using the hypergraph parsing framework for
SPLITGs turns out to be non-trivial. Where the stan-
dard LITG uses one rule to rewrite a nonterminal into
another nonterminal and a biterminal, the SPLITG
rewrites a nonterminal to a preterminal and a non-
terminal, and rewrites the preterminal into a biter-
minal. This causes problems within the hypergraph
framework, where each rule application should cor-
respond to one hyperedge. As it stands we have two
options:
1. Let each rule correspond to one hyperedge,
which means that we need to introduce preter-
minal nodes into the hypergraph. This has
a clear drawback for bracketing grammars,1
since it is now necessary to keep different sym-
bols apart. It also produces larger hypergraphs,
since the number of nodes is inflated.
2. Let hypergraphs be associated with one or two
rules, which means that we need to redefine hy-
peredges so that there are two different weight-
ing functions: one for the nonterminal weight
and one for the preterminal weight. Although
all hyperedges are associated with one nonter-
minal rule, some hyperedges are not associated
with any preterminal rule, making the pretermi-
nal weighting function partly defined.
Both of these approaches work in practice, but nei-
ther is completely satisfactory since they both rep-
resent work-arounds to shoe-horn the parsing algo-
rithm (as stated in the deductive system) into a for-
malism that is not completely compatible. By reify-
ing the rules into the object level, we rid ourselves
of this inconvenience, as we no longer differentiate
between different types of conditions.
1A bracketing grammar is a grammar where |N | = 1.
5 Reestimation of reified rules
As has been amply hinted at, the contextual prob-
abilities (outside probabilities, reverse values or ?-
values) contain all new information we need about
the rules to reestimate their probability in an expec-
tation maximization (Dempster et al, 1977) frame-
work. To show that this is indeed the case, we
will rewrite the reestimation formulas of the inside-
outside algorithm (Baker, 1979) so that they are
stated in terms of contextual probability for the
rules.
In general, a stochastic context-free grammar can
be estimated from examples of trees generated by
the grammar by means of relative frequency. This
is also true for expectation maximization with the
caveat that we have multiple hypotheses over each
sentence (pair), and therefore calculate expectations
rather than discrete frequency counts. We thus com-
pute the updated parameterization function ?? based
on expectations from the current parameterization
function:
?? (?|p) =
E? [p? ?]
E? [p]
Where p ? N and ? ? {? ? N}+ (or ? ?
{(?????)?N}+ for transduction grammars). The
expectations are calculated from the sentences in a
corpus C:
E? [x] =
?
w?C
E? [x|w]
The exact way of calculating the expectation on x
given a sentence depends on what x is. For nonter-
minal symbols, the expectations are given by:
E? [p|w] =
E? [p,w]
E? [w]
=
?
0?i?j?|w| Pr (pi,j ,w|G)
Pr (w|G)
=
?
0?i?j?|w| ?(pi,j)?(pi,j)
?(S0,|w|)?(S0,|w|)
For nonterminal rules, the expectations are shown in
Figure 5. The most noteworthy step is the last one,
where we use the fact that the summation is over
the equivalence of the rule?s reverse value. Each
75
E? [p? qr|w] =
E? [p? qr,w]
E? [w]
=
?
0?i?k?j?|w| Pr
(
w0..i, pi,j , wj..|w|
?
?G
)
Pr (wi..k|qi,k, G) Pr (wk..j |rk,j , G) ? (qr|p)
Pr (w|G)
=
?
0?i?k?j?|w| ?(pi,j)?(qi,k)?(rk,j)? (qr|p)
?(S0,|w|)?(S0,|w|)
=
? (qr|p)
?
0?i?k?j?|w| ?(pi,j)?(qi,k)?(rk,j)
?(S0,|w|)?(S0,|w|)
=
?(p? qr)?(p? qr)
?(S0,|w|)?(S0,|w|)
Figure 5: Expected values for nonterminal rules in a specific sentence.
E? [p? a|w] =
E? [p? a,w]
E? [w]
=
?
0?i?j?|w| Pr
(
w0..i, pi,j , wj..|w|
?
?G
)
Ia(wi..j)? (a|p)
Pr (w|G)
=
?
0?i?j?|w| ?(pi,j)Ia(wi..j)? (a|p)
?(S0,|w|)?(S0,|w|)
=
? (a|p)
?
0?i?j?|w| ?(pi,j)Ia(wi..j)
?(S0,|w|)?(S0,|w|)
=
?(p? a)?(p? a)
?(S0,|w|)?(S0,|w|)
Figure 6: Expected values of terminal rules in a specific sentence.
?(pi,j)?(qi,k)?(rk,j) term of the summation corre-
sponds to one instance where the rule was used in
the parse. Furthermore, the ? value is the outside
probability of the consequence of the deductive rule
applied, and the two ? values are the inside prob-
abilities of the sibling conditions of that deductive
rule. The entire summation thus corresponds to our
definition of the reverse value of a rule, or its outside
probability.
In Figure 6, the same process is carried out for ter-
minal rules. Again, the summation is over all possi-
ble ways that we can combine the inside probability
of the sibling conditions of the rule with the outside
probability of the consequence.
Since the expected values of both terminal and
nonterminal rules have the same form, we can gen-
eralize the formula for any production ?:
E? [p? ?|w] =
?(p? ?)?(p? ?)
?(S0,|w|)?(S0,|w|)
Finally, plugging it all into the original rule estima-
tion formula, we have:
?? (?|p) =
E? [p? ?]
E? [p]
=
?
w?C
?(p??)?(p??)
?(S0,|w|)?(S0,|w|)
?
w?C
?
0?i?j?|w|
?(pi,j)?(pi,j)
?(S0,|w|)?(S0,|w|)
= ?(p? ?)
?
w?C
?(p??)
?(S0,|w|)?(S0,|w|)
?
w?C
?
0?i?j?|w|
?(pi,j)?(pi,j)
?(S0,|w|)?(S0,|w|)
Rather than keeping track of the expectations of non-
terminals, they can be calculated from the rule ex-
pectations by marginalizing the productions:
E? [p] =
?
?
E? [p? ?]
76
Figure 7: The same hyperedge as in Figures 3 and 4, rep-
resented as a mul/add-subgraph.
5.1 SPLITG
Since this view of EM and parsing generalizes to de-
ductive systems with multiple rules as conditions,
we can apply it to the deductive system of SPLITGs.
It is, however, also interesting to note how the hy-
pergraph view of parsing is changed by this. We
effectively removed the weights from the edges, but
kept the feature that values of nodes depend entirely
on the values connected by incoming hyperedges. If
we assume the values to be from the Boolean semir-
ing, the hypergraphs we ended up with are in fact
and/or-graphs. That is: each node in the hypergraph
corresponds to an or-node, and each hyperedge cor-
responds to an and-node. We note that this can be
generalized to any semiring, since or is equivalent to
? and and is equivalent to ? for the Boolean semir-
ing, we can express a hypergraph over an arbitrary
semiring as a mul/add-graph.2 Figure 7 shows how
a hyperedge looks in this new graph form. The ?-
value of a node is calculated by combining the val-
ues of all incoming edges using the operator of the
node. The ?-values are also calculated using the op-
erator of the node, but with the edges reversed. For
this to work properly, the mul-nodes need to behave
somewhat different from add-nodes: each incoming
edge has to be reversed one at a time, as illustrated
in Figure 8.
6 Conclusions
We have shown that the reification of rules into the
parse forest graphs allows for a unified framework
where all calculations are performed the same way,
2Because it is much easier to pronounce than ?/?-graph.
Figure 8: Reverse values (?) are calculated by track-
ing backwards through all possible paths. This produces
three different paths for the mul/add-subgraph from Fig-
ure 7. Arrows pointing downward propagate ?-values
while arrows pointing upward propagate ?-values.
and where the calculations for the rules encompass
all information needed to reestimate them using ex-
pectation maximization. The contextual probability
of a rule?its outside probability?holds all infor-
mation needed to calculate expectations, which can
be exploited by promoting the rules to first-class cit-
izens of the parse forest. We have also seen how this
reification of the rules helped solve a real transla-
tion problem?induction of stochastic preterminal-
ized linear inversion transduction grammars using
expectation maximization.
Acknowledgments
This work was funded by the Defense Advanced
Research Projects Agency (DARPA) under GALE
Contract Nos. HR0011-06-C-0023 and HR0011-
06-C-0023, and the Hong Kong Research Grants
Council (RGC) under research grants GRF621008,
GRF612806, DAG03/04.EG09, RGC6256/00E, and
RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily re-
flect the views of the Defense Advanced Research
Projects Agency. We would also like to thank the
three anonymous reviewers, whose feedback made
this a better paper.
References
James K. Baker. 1979. Trainable grammars for speech
recognition. In Speech Communication Papers for the
97th Meeting of the Acoustical Society of America,
pages 547?550, Cambridge, Massachusetts.
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
77
of the 27th annual meeting on Association for Compu-
tational Linguistics, ACL?89, pages 143?151, Strouds-
burg, Pennsylvania, USA.
John Cocke. 1969. Programming languages and their
compilers: Preliminary notes. Courant Institute of
Mathematical Sciences, New York University.
Arthur Pentland Dempster, Nan M. Laird, and Don-
ald Bruce Rubin. 1977. Maximum likelihood from
incomplete data via the em algorithm. Journal of the
Royal Statistical Society. Series B (Methodological),
39(1):1?38.
Jason Eisner. 2001. Expectation semirings: Flexible
EM for finite-state transducers. In Gertjan van No-
ord, editor, Proceedings of the ESSLLI Workshop on
Finite-State Methods in Natural Language Processing
(FSMNLP). Extended abstract (5 pages).
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 1?8, Philadelphia, July.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Liang Huang. 2008. Forest-based Algorithms in Natu-
ral Language Processing. Ph.D. thesis, University of
Pennsylvania.
Tadao Kasami and Koji Torii. 1969. A syntax-analysis
procedure for unambiguous context-free grammars.
Journal of the Association for Computing Machinery,
16(3):423?431.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 40?51, Singapore,
August.
Christopher D. Manning and Dan Klein. 2001. Parsing
and hypergraphs. In Proceedings of the 2001 Interna-
tional Workshop on Parsing Technologies.
Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.
Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
78
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 243?252,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Fully Automatic Semantic MT Evaluation
Chi-kiu LO, Anand Karthik TUMULURU and Dekai WU
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo,aktumuluru,dekai}@cs.ust.hk
Abstract
We introduce the first fully automatic, fully seman-
tic frame based MT evaluation metric, MEANT,
that outperforms all other commonly used auto-
matic metrics in correlating with human judgment
on translation adequacy. Recent work on HMEANT,
which is a human metric, indicates that machine
translation can be better evaluated via semantic
frames than other evaluation paradigms, requiring
only minimal effort from monolingual humans to an-
notate and align semantic frames in the reference and
machine translations. We propose a surprisingly ef-
fective Occam?s razor automation of HMEANT that
combines standard shallow semantic parsing with
a simple maximum weighted bipartite matching al-
gorithm for aligning semantic frames. The match-
ing criterion is based on lexical similarity scoring
of the semantic role fillers through a simple con-
text vector model which can readily be trained us-
ing any publicly available large monolingual cor-
pus. Sentence level correlation analysis, following
standard NIST MetricsMATR protocol, shows that
this fully automated version of HMEANT achieves
significantly higher Kendall correlation with hu-
man adequacy judgments than BLEU, NIST, ME-
TEOR, PER, CDER, WER, or TER. Furthermore,
we demonstrate that performing the semantic frame
alignment automatically actually tends to be just as
good as performing it manually. Despite its high
performance, fully automated MEANT is still able
to preserve HMEANT?s virtues of simplicity, repre-
sentational transparency, and inexpensiveness.
1 Introduction
We introduce the first fully automatic semantic-frame-
based MT evaluation metric capable of outperforming
all other commonly used automatic metrics like BLEU,
NIST, METEOR, PER, CDER, WER, and TER for eval-
uating translation adequacy. This work, MEANT, can be
seen as a fully automated version of HMEANT, which is
a human metric, introduced by Lo and Wu (2011b). De-
spite its high performance, MEANT is still able to pre-
serve HMEANT?s virtues of Occam?s razor simplicity,
representational transparency, and inexpensiveness.
For the past decade, MT evaluation has relied heavily
on inexpensive automatic metrics such as BLEU (Pap-
ineni et al, 2002), NIST (Doddington, 2002), METEOR
(Banerjee and Lavie, 2005), PER (Tillmann et al, 1997),
CDER (Leusch et al, 2006), WER (Nie?en et al, 2000),
and TER (Snover et al, 2006). In large part, this is be-
cause automatic metrics significantly shorten the evalua-
tion cycle by providing a fast, easy and cheap quantita-
tive evaluation which can be effectively incorporated into
modern SMT training methods.
Despite the fact that HMEANT, a human metric re-
cently proposed by Lo and Wu (2011b,c,d), was shown
to reflect translation adequacy more accurately than all
of these automatic metrics, it is unfortunately infeasible
to incorporate the HMEANT metrics directly into SMT
training methods, due to the non-automatic processes of
(1) semantic parsing and (2) aligning semantic frames.
In this paper we introduce an automatic metric in which
both the semantic parsing and the alignment of semantic
frames are fully automated. Our aim is to show that even
with full automation, this new metric still outperforms all
the previous automatic metrics mentioned, thus provid-
ing a foundation for future incorporation into the training
of SMT to drive system improvements in providing more
adequate translation output.
N-gram oriented automatic MT evaluation metrics like
BLEU perform well at capturing translation fluency, and
ranking overall systems with respect to each other when
their scores are averaged over entire documents or cor-
pora. However, they do not fare so well in ranking trans-
lations of individual sentences. As MT systems improve,
the n-gram based evaluation metrics have begun to show
their limits. State-of-the-art MT systems are often able to
output translations containing roughly the correct words,
while failing to convey important aspects of the meaning
of the input sentence. Cases where BLEU strongly dis-
agrees with human judgment of translation quality were
243
reported in large scale MT evaluation tasks by Callison-
Burch et al (2006) and Koehn and Monz (2006).
Motivated by the goal of addressing the weaknesses
of n-gram oriented automatic MT evaluation metrics at
evaluating translation adequacy, the HMEANT metric
assesses translation utility by matching the basic event
structure??who did what to whom, when, where and
why? (Pradhan et al, 2004)?representing the central
meaning conveyed by sentences. As mentioned above,
however, HMEANT requires humans to manually anno-
tate semantic frames in the reference and machine trans-
lations, and then to align the semantic frames?making
it difficult to incorporate HMEANT as an objective func-
tion in the MT system training, evaluating, and optimiz-
ing cycle.
We argue in this paper that both the human seman-
tic parsing and the semantic frame alignment tasks per-
formed within HMEANT can be successfully automated
to produce a state-of-the-art automatic metric. Moreover,
we show that the spirit of Occam?s razor can be preserved
even for the semantic frame alignment, by demonstrating
the effectiveness of a simple maximum weighted bipar-
tite matching algorithm based on the lexical similarity be-
tween semantic frames. In addition, we show empirically
that performing this semantic frame alignment automati-
cally tends to be just as good as performing it manually.
Our results indicate that MEANT, the fully automatic
version of HMEANT, achieves levels of correlation with
human adequacy judgment (in our experiments, approx-
imately 0.37) which significantly outperforms the com-
monly used automatic metrics BLEU, NIST, METEOR,
PER, CDER, WER, and TER (in our experiments, rang-
ing between 0.20 and 0.29).
2 Related Work
2.1 Automatic lexical similarity based metrics
BLEU (Papineni et al, 2002) remains the most widely
used MT evaluation metric despite the fact that a num-
ber of large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) report cases where it
strongly disagrees with human judgments of translation
accuracy. Other lexical similarity based automatic MT
evaluation metrics, like NIST (Doddington, 2002), ME-
TEOR (Banerjee and Lavie, 2005), PER (Tillmann et al,
1997), CDER (Leusch et al, 2006), WER (Nie?en et al,
2000), and TER (Snover et al, 2006), also perform well
in capturing translation fluency, but share the same prob-
lem that although evaluation with these metrics can be
done very quickly at low cost, their underlying assump-
tionthat a good translation is one that shares the
same lexical choices as the reference translationis not
justified semantically. Lexical similarity does not ade-
quately reflect similarity in meaning.
Generating a translation that contains roughly the cor-
rect words may be necessary?but is far from sufficient?
to preserve the essence of the meaning. We argue that a
translation metric that reflects meaning similarity needs
to be based on similarity of semantic structure, and not
merely flat lexical similarity.
2.2 HMEANT (human SRL based metric)
As mentioned above, despite the fact that the semi-
automatic HMEANT metric recently proposed by Lo and
Wu (2011b,c,d) shows a higher correlation with human
adequacy judgments than all commonly used automatic
MT evaluation metrics, as with other human metrics like
HTER (Snover et al, 2006), it is unfortunately infeasible
to incorporate the HMEANT metrics directly into SMT
training methods. HMEANT requires non-automatic
manual steps of (1) semantic parsing and (2) aligning
semantic frames. Monolingual (or bilingual) annotators
must label the semantic roles in both the reference and
machine translations, and then to align the semantic pred-
icates and role fillers in the MT output to the reference
translations. These annotations allow HMEANT to then
look at the aligned role fillers, and aggregate the trans-
lation accuracy for each role. In the spirit of Occam?s
razor and representational transparency, the HMEANT
score is defined simply in terms of a weighted f-score
over these aligned predicates and role fillers. More pre-
cisely, HMEANT is defined as follows:
1. Human annotators annotate the shallow semantic
structures of both the references and MT output.
2. Human judges align the semantic frames between
the references and MT output by judging the cor-
rectness of the predicates.
3. For each pair of aligned semantic frames,
(a) Human judges determine the translation cor-
rectness of the semantic role fillers.
(b) Human judges align the semantic role fillers
between the reference and MT output accord-
ing to the correctness of the semantic role
fillers.
4. Compute the weighted f-score over the matching
role labels of these aligned predicates and role
fillers.
mi ?
#tokens filled in aligned frame i of MT
total #tokens in MT
ri ?
#tokens filled in aligned frame i of REF
total #tokens in REF
Mi,j ? total # ARG j of aligned frame i in MT
Ri,j ? total # ARG j of aligned frame i in REF
Ci,j ? # correct ARG j of aligned frame i in MT
Pi,j ? # partially correct ARG j of aligned frame i in MT
244
Figure 1: Examples of human semantic frame annotation. Semantic parses of the Chinese input and the English reference trans-
lation are from the Propbank gold standard. The MT output is semantically parsed by monolingual lay annotators according to the
HMEANT guidelines. There are no semantic frames for MT3 because there is no predicate.
precision =
?
i mi
wpred+
?
j wj(Ci,j+wpartialPi,j)
wpred+
?
j wjMi,j
?
i mi
recall =
?
i ri
wpred+
?
j wj(Ci,j+wpartialPi,j)
wpred+
?
j wjRi,j
?
i ri
where mi and ri are the weights for frame, i, in the
MT/REF respectively. These weights estimate the degree
of contribution of each frame to the overall meaning of
the sentence. Mi,j and Ri,j are the total counts of argu-
ment of type j in frame i in the MT and REF respec-
tively. Ci,j and Pi,j are the count of the correctly and
partial correctly translated argument of type j in frame i
in the MT output. Figure 1 shows examples of human se-
mantic frame annotation on reference and machine trans-
lations as used in HMEANT. Table 1 shows examples of
human judges? decisions for semantic frame alignment
and translation correctness for each semantic roles, for
the ?MT2? output from Figure 1.
Unlike HMEANT, MEANT is fully automatic; but
nevertheless, it adheres to HMEANT?s principles of Oc-
cam?s razor simplicity and representational transparency.
These properties crucially facilitate error analysis and
credit/blame assignment that are invaluable for MT sys-
tem modeling.
Furthermore, being fully automatic, MEANT is even
less expensive than HMEANT, which was already shown
by Lo and Wu (2011b,c,d) to be significantly less ex-
pensive than HTER. This makes MEANT a much bet-
ter candidate than HMEANT for future incorporation into
the automatic training of SMT systems to drive improve-
ments in translation adequacy.
2.3 Semantic role labels as features in aggregate
metrics
Gime?nez and Ma`rquez (2007, 2008) introduced ULC, an
automatic MT evaluation metric that aggregates many
types of features, including several shallow semantic sim-
ilarity features. However, unlike Lo and Wu (2011b),
245
Table 1: Example of SRL annotation for the MT2 output from figure 1 along with the human judgements of translation correctness
for each argument. *Notice that although the decision made by the human judge for ?in mainland China? in the reference translation
and ?the mainland of China? in MT2 is ?correct?, nevertheless the HMEANT computation will not count this as a match since their
role labels do not match.
REF roles REF MT2 roles MT2 decision
PRED ceased Action stop match
ARG0 their sale ? ? incorrect
ARGM-LOC in mainland China Agent the mainland of China correct*
ARGM-TMP for almost two months Temporal nearly two months correct
? ? Experiencer SK - 2 products incorrect
PRED resumed Action resume match
ARG0 sales of complete range of SK
- II products
Experiencer in the mainland of China to
stop selling nearly two months
of SK - 2 products sales
incorrect
ARGM-TMP Until after , their sales had
ceased in mainland China for
almost two months
Temporal So far partial
ARGM-TMP now ? ? incorrect
the ULC representation is based on flat semantic role
label features that do not capture the structural rela-
tions in semantic frames, i.e., the predicate-argument re-
lations. Also unlike HMEANT, which weights each se-
mantic role type according to its empirically determined
relative importance to the adequate preservation of mean-
ing, ULC uses uniform weights. Although the automatic
ULC metric shows an improved correlation with human
judgment of translation quality (Callison-Burch et al,
2007; Gime?nez and Ma`rquez, 2007; Callison-Burch et
al., 2008; Gime?nez and Ma`rquez, 2008), it is not com-
monly used in large-scale MT evaluation campaigns, per-
haps due to its high time cost and/or the difficulty of in-
terpreting its score because of its highly complex combi-
nation of many heterogeneous types of features.
Like system combination approaches, ULC is a vastly
more complex aggregate metric compared to widely used
metrics like BLEU. We believe it is important for auto-
matic semantic MT evaluation metrics to provide rep-
resentational transparency via simple, clear, and trans-
parent scoring schemes that are (a) easily human read-
able to support error analysis, and (b) potentially directly
usable for automatic credit/blame assignment in tuning
tree-structured SMT systems.
3 MEANT: A fully automatic semantic
MT evaluation metric
Like HMEANT, our guiding principle is that a good
translation is one that is useful, in the sense that hu-
man readers may successfully understand at least the ba-
sic event structurewho did what to whom, when, where
and why (Pradhan et al, 2004)representing the central
meaning of the source utterances. Whereas HMEANT
measures this using a f-score of correctly translated
semantic roles in MT output that are annotated and
compared by monolingual human annotators, MEANT
automates HMEANT as follows (the differences from
HMEANT are italicized):
1. Apply an automatic shallow semantic parser on both
the references and MT output.
2. Apply maximum weighted bipartite matching algo-
rithm to align the semantic frames between the ref-
erences and MT output by the lexical similarity of
the predicates.
3. For each pair of aligned semantic frames,
(a) Lexical similarity scores determine the similar-
ity of the semantic role fillers.
(b) Apply maximum weighted bipartite matching
algorithm to align the semantic role fillers be-
tween the reference and MT output according
to their lexical similarity.
4. Compute the weighted f-score over the matching
role labels of these aligned predicates and role
fillers.
3.1 Automatic semantic parsing
To automate the process of human semantic role label-
ing, we apply an automatic shallow semantic parser on
both the reference and MT output that takes the raw trans-
lation as input and outputs the corresponding predicate-
argument structure. We choose to semantically parse the
translation independently, instead of inducing the parses
246
Figure 2: Examples of automatic shallow semantic parses. The Chinese input is parsed by a Chinese automatic shallow semantic
parser. The English reference and machine translations are parsed by an English automatic shallow semantic parser. There are no
semantic frames for mt3 since there is no predicate.
from the input, because it captures the raw meaning con-
veyed in the translation rather than predicting the mean-
ing conveyed in the translation from the input. Figure 2
shows examples of automatic shallow semantic parses on
both reference and machine translations.
3.2 Automatic semantic frame alignment
After reconstructing the shallow semantic parse, the man-
ual semantic frame alignment process is automated by
applying the maximum weighted bipartite matching algo-
rithm where the weights of the edges represent the lexical
similarity of the predicates. A wide range of lexical sim-
ilarity measures are available to us, including for exam-
ple BLEU, METEOR, cosine similarity based on context
vector models (Dagan, 2000), and so forth. In Section
4, we will show the performance of the fully automatic
semantic MT evaluation metric, MEANT ,couple with
different lexical similarity metrics and other commonly
used automatic MT evaluation metrics. In Section 6, we
will discuss aligning the semantic frames according to all
semantic role fillers, instead of the predicates only.
Then, for each pair of aligned semantic frames, we es-
timate the similarity of the semantic role fillers by sum-
ming all the lexical similarity of all the pairwise combi-
nation of tokens between the references and MT output.
After obtaining the similarity of the semantic role fillers,
we again apply the maximum weighted bipartite match-
ing algorithm to align the semantic role fillers between
the references and MT output. Table 2 shows examples
of the human judges? decisions on semantic frame align-
ment and translation correctness for each semantic role in
the ?MT2? output from Figure 2.
3.3 Scoring the semantic similarity
After aligning the semantic frames automatically, the
computation of the MEANT score is largely the same as
stated in Lo and Wu (2011d), except that we now replace
the counts of correctly and partially correctly translated
semantic role fillers by the similarity scores of the predi-
cates and arguments between the references and MT out-
put.
mi ?
#tokens filled in aligned frame i of MT
total #tokens in MT
ri ?
#tokens filled in aligned frame i of REF
total #tokens in REF
Mi,j ? total # ARG j of aligned frame i in MT
Ri,j ? total # ARG j of aligned frame i in REF
Si,pred ? sim. of pred of REF and MT in aligned frame i
Si,j ? sim. of ARG j of REF and MT in aligned frame i
precision =
?
i mi
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjMi,j
?
i mi
recall =
?
i ri
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjRi,j
?
i ri
247
Table 2: Automatic semantic frame alignment of the MT2 output from figure 2, along with the automatic lexical similarity scoring
on translation correctness for each argument.
REF roles REF MT2 roles MT2 similarity
PRED ceased PRED stop 0.0377
ARG0 their sales ? ? ?
ARGM-LOC in mainland China ? ? ?
ARGM-TMP for almost two months ? ? ?
? ? PRED selling ?
? ? ARG1 nearly two months of SK ?
PRED resumed PRED resumed 1.0
ARG1 sales of complete range of SK
- II products
ARG1 2 products sales 0.0836
ARGM-TMP now ARGM-TMP So far 0.0459
where mi, ri, Mi,j, Ri,j are defined the same as in
HMEANT, and Si,pred and Si,j are the lexical similarities
(BLEU, METEOR, cosine similarity based on a context
vector model, and so on, as discussed in the following
section) of the predicates and arguments of type j be-
tween the reference translations and the MT output.
4 MEANT outperforms all automatic
metrics
We will first show that the fully automatic semantic MT
evaluation metric, MEANT, outperforms all the other
commonly used automatic metrics.
4.1 Experimental setup
For assessing lexical similarity, a wide range of lexi-
cal similarity scoring models are available. We describe
a representative subset of a wide range of experiments
we have performed using all the most typical and com-
monly used measures. On one hand, we report experi-
ments with integrating two commonly used MT evalua-
tion metrics, BLEU and METEOR, as the lexical simi-
larity. On the other hand, we also report experiments
on integrating two common similarity measures?cosine
similarity measure and min/max with mutual information
(Dagan, 2000)?that are based on context vector models,
and trained from the Gigaword corpus with window sizes
of 3 and 5.
The cosine similarity between two sequences of word
tokens, ??u and ??v , is defined as follows:
??wx = context vector of word token x
wxi = attribute i of context vector?
??wx
f(x,wxi) =
count(x,wxi)
count (wxi)
cosine(x, y) =
?
i
f(x,wxi)? f(y, wyi)
?
?
i
f(x,wxi)
2
?
?
i
f(y, wyi)
2
cosine(??u ,??v ) = ?
i
?
j
cosine(ui, vj)
Using the same definition of wxi, the min/max with
mutual information similarity between two sequences of
word tokens, ??u and ??v , is defined as follows:
P (wxi ? x) =
count(x,wxi)?
i count(x,wxi)
P (wxi) =
?
y count(y, wxi)
?
y
?
j count(y, wxj)
MI(x,wxi) = log
(
P (wxi ? x)
P (wxi)
)
MinMax-MI(x, y) =
?
i
min (MI(x,wxi),MI(y, wyi ))
?
i
max (MI(x,wxi),MI(y, wyi ))
MinMax-MI(??u ,??v ) = ?
i
?
j
MinMax-MI(ui, vj)
For our benchmark comparison, the evaluation data
for our experiments is the same two sets of sentences,
GALE-A and GALE-B that were used in Lo and Wu
(2011d), where GALE-A is used for estimating the
weight parameters of the metric by optimizing the cor-
relation with human adequacy judgment, and then the
learned weights are applied to testing on GALE-B.
For the automatic semantic role labeling, we used the
publicly available off-the-shelf shallow semantic parser,
ASSERT (Pradhan et al, 2004).
The correlation with human adequacy judgments on
sentence-level system ranking is assessed by the stan-
dard NIST MetricsMaTr procedure (Callison-Burch et
al., 2010) using Kendall correlation coefficients.
248
Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing all
commonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metric
integrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and
(d) MinMax with mutual information.
GALE-A (training) GALE-B (testing)
Human metrics
HMEANT 0.49 0.27
HTER 0.43 0.20
Automatic metrics
MEANT ? ?
- with MinMax-MI on context vector model of window size 3 0.37 0.19
- with MinMax-MI on context vector model of window size 5 0.37 0.17
- with Cosine on context vector model of window size 3 0.32 0.13
- with Cosine on context vector model of window size 5 0.30 0.08
- with METEOR 0.17 ?
- with BLEU 0.00 ?
METEOR 0.20 0.21
NIST 0.29 0.09
TER 0.20 0.10
BLEU 0.20 0.12
PER 0.20 0.07
WER 0.10 0.11
CDER 0.12 0.10
4.2 Results
Table 3 shows that MEANT significantly outperforms all
the other automatic MT evaluation metrics when inte-
grated with a simple similarity measure based on word
context vectors trained from a large monolingual corpus.
We can also see that using min/max with mutual infor-
mation is significantly better than using cosine similarity.
Furthermore, context vector models using a window size
of 3 appear to be as good or better than those using a
window size of 5.
Although the human metrics, HMEANT and HTER,
obviously remain superior, MEANT performs far better
than almost all other automatic metrics. The only excep-
tion is the GALE-B dataset, where METEOR performs
marginally better than MEANT and even HTER. Data
analysis shows that the marginally higher correlation of
METEOR on the GALE-B dataset is a statistical outlier;
it is quite rare for a lexically based automatic metric to
outperform even the human-driven HTER metric.
Interestingly and somewhat surprisingly, using the n-
gram based MT evaluation metrics BLEU and METEOR
as lexical similarity scores does not work well at all for
this purpose, even on the training data (thus obviating the
need to obtain results on the testing data). Analysis in-
dicates that the reason for this is that variation between
alternative paraphrasing of the role fillers makes the num-
ber of matching n-grams quite small, since there are many
synonyms and few exact consecutive n-gram matches.
Table 4: Sentence-level correlation with human adequacy judg-
ment on GALE-A (training) and GALE-B (testing) for aligning
sematnic frame automatically and manually.
Semantic frame alignment GALE-A GALE-B
Automatic 0.37 0.19
Manual 0.35 0.17
In the following sections, we turn to considering sev-
eral questions that naturally arise following these strong
results.
5 Don?t align semantic frames manually
One obvious question is whether the automatic alignment
of semantic frames degrades MEANT?s accuracy, and if
so, the extent to which it hurts.
5.1 Experimental setup
To test this question, we compare the best fully automatic
results of the previous section against a semi-automatic
variant of our proposed metric. In the semi-automatic
variant, the semantic parsing is still performed automati-
cally. However, the semantic frame alignment is instead
done manually by human annotators.
The rest of the experimental setup is the same as that
used in Section 4.
249
5.2 Results
Table 4 shows that performing the alignment of semantic
frames automatically is as good?or even better than?
doing the alignment manually. We believe the success of
automatic semantic frame alignment reflects the high de-
gree of reliability of our chosen lexical similarity metric,
when the candidates for role fillers are restricted to the
fairly small set defined by the sentence pairs.
6 Look only at predicates when aligning
semantic frames
Given the positive results of the previous sections, it is
worth asking a deeper question: would it further improve
the correlation with human adequacy judgment of the
metric if the semantic frames were aligned not only by
matching predicates (as HMEANT did), but in addition
by trying to also maximize the match of the semantic role
fillers?
The reason to revisit this question is that even though
Lo and Wu (2011a) showed that in the case of HMEANT
it is effective for human annotators to align semantic
frames according to the predicates only, this could eas-
ily be due to the mental challenge for lay annotators to
compare and keep in mind all the semantic role fillers at
the same time. But in the case of a fully automatic metric,
on the other hand, it is easy for an algorithm to compute
the individual similarities between all the semantic role
fillers and consider the aggregate similarity when opti-
mizing the alignment of semantic frames.
Surprisingly, however, the results will show that even
in the automated case, this still does not help improve the
correlation with human adequacy judgments.
6.1 Experimental setup
To align semantic frames using all semantic roles, we
aggregate the lexical similarity of all the semantic role
fillers into a semantic frame similarity score. We exper-
iment on two variations of the aggregation function (1)
simple linear average of the lexical similarity over the
number of aligned semantic roles in the frames; or (2) the
inverse of the sum of the negative log of the role fillers
similarity.
The rest of the experimental setup is the same as that
used in Section 4.
6.2 Results
Table 5 shows that to align semantic frames, using only
the lexical similarity of the predicates between the frames
in the reference translations and the MT output (0.37
Kendall in GALE-A and 0.19 Kendall in GALE-B) is
more robust than either of the two natural ways of ag-
gregating the lexical similarity of the aligned semantic
role fillers. Aggregating by linear average yields a lower
Table 5: Sentence-level correlation with human adequacy judg-
ments on GALE-A (training set) and GALE-B (testing set) for
aligning semantic frames using predicate only vs. using all se-
mantic role fillers aggregated by (1) the linear average of the
lexical similarity vs. (2) the inverse of the sum of negative log
of the lexical similarity.
Frame alignment GALE-A GALE-B
Predicate only 0.37 0.19
Linear average 0.35 0.10
Inverse of sum of neg. log 0.30 0.17
0.35 Kendall in GALE-A and 0.10 Kendall in GALE-B.
Aggregating by the inverse of the sum of negative logs
yields a lower 0.30 Kendall in GALE-A and 0.17 Kendall
in GALE-B.
What might explain this perhaps surprising result? Our
conjecture is that aggregating the lexical similarities of
the semantic role fillers fails to help find better seman-
tic frame alignments because the lexical similarities are
aggregated with uniform weight across different types of
role fillers. Therefore, the aggregation ignores the fact
that different types of role types contribute to a widely
varying degree to the meaning of an entire semantic
frame?in reality, some role types are much more impor-
tant than others. However, the complexity of the met-
ric would be greatly increased if we added weights for
each semantic roles type for semantic frame alignment
process, and this would not be likely to be worthwhile
given that automatic alignment is already performing as
well as human alignment of semantic frames.
7 Don?t word align semantic role fillers
Another question that naturally arises from the positive
results above is: when aligning the semantic frames,
would word-aligning the tokens within role fillers help?
Specifically, if we had word alignments for every candi-
date pair of role filler strings, we could sum the lexical
similarities only between the aligned tokens?instead of
what we did above, which was to sum the lexical similar-
ities of all pairwise combinations of tokens.
However, experimental results will show that, surpris-
ingly, to judge the similarity of semantic role fillers,
summing the lexical similarities over only word-aligned
tokens?instead of all pairwise combinations of tokens?
does not help to improve the correlation of the semantic
MT evaluation with human adequacy judgment.
7.1 Experimental setup
To avoid the danger of aligning a token in one segment
to excessive numbers of tokens in the other segment,
we adopt a variant of competitive linking by Melamed
(1996). Competitive linking is a greedy best-first word
alignment algorithm.
250
Table 6: Sentence-level correlation with human adequacy judg-
ments on GALE-A (training set) and GALE-B (testing set) for
judging semantic role fillers similarity using pairwise tokens vs.
only aligned tokens.
Semantic role filler similarity GALE-A GALE-B
All pairwise tokens 0.37 0.19
Only aligned tokens 0.36 0.17
The rest of the experimental setup is the same as that
used in Section 4.
7.2 Results
Table 6 shows that, surprisingly, judging semantic role
filler similarity using only the aligned tokens (selected
by competitive linking word alignment algorithm) does
not help the correlation with human adequacy judgment.
This is surprising as, intuitively, using only the aligned
tokens should avoid the introduction of noise in judg-
ing the similarity between semantic role fillers because
it avoids adding in similarities for words within semantic
role fillers whose meanings are not close to each other.
How might this outcome be explained? We conjecture
that the word alignments over-constrain the calculation
of segment similarities. The individual lexical similari-
ties are already weighted fairly accurately, so the lexical
similarities between words that do not correspond do not
hurt since they are already close to zero. On the other
hand, in cases where the word alignment is ambiguous,
it is better to aggregate over different possible pairwise
alignments?strictly obeying a hard word alignment un-
desirably forces dropping of some individual lexical sim-
ilarity scores that are actually relevant.
8 Conclusion
We have introduced a new fully automatic semantic MT
evaluation metric, MEANT, that is fundamentally based
on semantic frames, that is the first such metric to out-
perform all other commonly used automatic MT evalu-
ation metrics. Experimental results following the stan-
dard NIST MetricsMATR protocol indicate that our pro-
posed metric achieves levels of correlation with human
adequacy judgment (in our experiments, approximately
0.37) that significantly outperform BLEU, NIST, ME-
TEOR, PER, CDER, WER, and TER (in our experiments,
ranging between 0.20 and 0.29).
We have also shown in this paper that the spirit of Oc-
cam?s razor of HMEANT can be preserved even under
full automation by (1) replacing human semantic role an-
notation with automatic shallow semantic parsing and (2)
replacing human semantic frame alignment with a simple
maximum weighted bipartite matching algorithm based
on the lexical similarity between semantic frames. Under
analysis, we have further shown empirically that perform-
ing this semantic frame alignment automatically tends to
be just as good as performing it manually. Furthermore,
we have shown surprisingly that (1) for aligning seman-
tic frames, using only the similarity of predicates is more
accurate than also taking into account the similarity of se-
mantic role fillers, and (2) to judge similarity between se-
mantic role fillers, aggregating similarity of all pairwise
combination of word tokens is more accurate than con-
sidering only the similarity of the tokens that obey word
alignments.
Papineni et al (2002) stated in their conclusion that
?We believe that BLEU will accelerate the MT R&D cy-
cle by allowing researchers to rapidly home in on effec-
tive modeling ideas.? since fully automatic metrics allow
inexpensive training and tuning of SMT systems. Devel-
opments in the past decade have more than borne witness
to this statement. However, SMT has progressed to the
stage where simple metrics like BLEU are no longer ca-
pable of driving progress toward preservation of meaning
with respect to proper event structure. We believe that
MEANT that rapidly and accurately reflects the transla-
tion adequacy of MT output by directly assessing who did
what to whom, when, where and why is needed to bring
MT R&D to a new level of improvement in generating
more meaningful MT output.
Acknowledgments
This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-12-C-
0016, and GALE contract nos. HR0011-06-C-0022
and HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by the
Hong Kong Research Grants Council (RGC) research
grants GRF621008, GRF612806, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions, find-
ings and conclusions or recommendations expressed in
this material are those of the authors and do not necessar-
ily reflect the views of the RGC, EU, or DARPA.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An Automatic
Metric for MT Evaluation with Improved Correlation with
Human Judgments. In Proceedings of the 43th Annual Meet-
ing of the Association of Computational Linguistics (ACL-
05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-
evaluating the role of BLEU in Machine Translation Re-
search. In Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. (Meta-) evaluation of
251
Machine Translation. In Proceedings of the 2nd Workshop
on Statistical Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. Further Meta-evaluation
of Machine Translation. In Proceedings of the 3rd Workshop
on Statistical Machine Translation, pages 70?106, 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Pe-
terson, Mark Pryzbocki, and Omar Zaidan. Findings of the
2010 Joint Workshop on Statistical Machine Translation and
Metrics for Machine Translation. In Proceedings of the Joint
5th Workshop on Statistical Machine Translation and Met-
ricsMATR, pages 17?53, Uppsala, Sweden, 15-16 July 2010.
Ido Dagan. Contextual word similarity. In Robert Dale, Her-
man Moisl, and Harold Somers, editors, Handbook of Nat-
ural Language Processing, pages 459?476. Marcel Dekker,
New York, 2000.
G. Doddington. Automatic Evaluation of Machine Translation
Quality using N-gram Co-occurrence Statistics. In Proceed-
ings of the 2nd International Conference on Human Lan-
guage Technology Research (HLT-02), pages 138?145, San
Francisco, CA, USA, 2002. Morgan Kaufmann Publishers
Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. Linguistic Features for Au-
tomatic Evaluation of Heterogenous MT Systems. In Pro-
ceedings of the 2nd Workshop on Statistical Machine Trans-
lation, pages 256?264, Prague, Czech Republic, June 2007.
Association for Computational Linguistics.
Jesu?s Gime?nez and Llu??s Ma`rquez. A Smorgasbord of Features
for Automatic MT Evaluation. In Proceedings of the 3rd
Workshop on Statistical Machine Translation, pages 195?
198, Columbus, OH, June 2008. Association for Computa-
tional Linguistics.
Philipp Koehn and Christof Monz. Manual and Automatic
Evaluation of Machine Translation between European Lan-
guages. In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 102?121, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Effi-
cient MT Evaluation Using Block Movements. In Proceed-
ings of the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-06), 2006.
Chi-kiu Lo and Dekai Wu. A Radically Simple, Effective An-
notation and Alignment Methodology for Semantic Frame
Based SMT and MT Evaluation. In Proceedings of Interna-
tional Workshop on Using Linguistic Information for Hybrid
Machine Translation (LiHMT 2011), organized by OpenMT-
2., 2011.
Chi-kiu Lo and Dekai Wu. MEANT: An Inexpensive, High-
Accuracy, Semi-Automatic Metric for Evaluating Transla-
tion Utility based on Semantic Roles. In Proceedings of the
Joint conference of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics : Human Language Tech-
nologies (ACL-HLT-11), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How seman-
tic frames evaluate MT more accurately. In Proceedings of
the 22nd International Joint Conference on Artificial Intelli-
gence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Structured vs. Flat Semantic Role
Representations for Machine Translation Evaluation. In Pro-
ceedings of the 5th Workshop on Syntax and Structure in Sta-
tistical Translation (SSST-5), 2011.
I. Dan Melamed. Automatic construction of clean broad-
coverage translation lexicons. In Proceedings of the 2nd
Conference of the Association for Machine Translation in the
Americas (AMTA-1996), 1996.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Hermann
Ney. A Evaluation Tool for Machine Translation: Fast Eval-
uation for MT Research. In Proceedings of the 2nd Inter-
national Conference on Language Resources and Evaluation
(LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics (ACL-
02), pages 311?318, 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Mar-
tin, and Dan Jurafsky. Shallow Semantic Parsing Using Sup-
port Vector Machines. In Proceedings of the 2004 Confer-
ence on Human Language Technology and the North Amer-
ican Chapter of the Association for Computational Linguis-
tics (HLT-NAACL-04), 2004.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. A Study of Translation Edit
Rate with Targeted Human Annotation. In Proceedings of the
7th Conference of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zu-
biaga, and Hassan Sawaf. Accelerated DP Based Search
For Statistical Translation. In Proceedings of the 5th Euro-
pean Conference on Speech Communication and Technology
(EUROSPEECH-97), 1997.
252
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 30?38,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Towards a Predicate-Argument Evaluation for MT
Ondr?ej Bojar?, Dekai Wu?
? Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
? HKUST, Human Language Technology Center,
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
bojar@ufal.mff.cuni.cz, dekai@cs.ust.hk
Abstract
HMEANT (Lo and Wu, 2011a) is a man-
ual MT evaluation technique that focuses on
predicate-argument structure of the sentence.
We relate HMEANT to an established lin-
guistic theory, highlighting the possibilities of
reusing existing knowledge and resources for
interpreting and automating HMEANT. We
apply HMEANT to a new language, Czech
in particular, by evaluating a set of English-
to-Czech MT systems. HMEANT proves to
correlate with manual rankings at the sentence
level better than a range of automatic met-
rics. However, the main contribution of this
paper is the identification of several issues
of HMEANT annotation and our proposal on
how to resolve them.
1 Introduction
Manual evaluation of machine translation output is
a tricky enterprise. It has been long recognized
that different evaluation techniques lead to different
outcomes, e.g. Blanchon et al (2004) mention an
evaluation carried out in 1972 where the very same
Russian-to-English MT outputs were scored 4.5 out
of the maximum 5 points by prospective users of
the system but only 1 out of 5 by teachers of En-
glish. Throughout the years, many techniques were
explored with more or less of a success.
The two-scale scoring for adequacy and fluency
used in NIST evaluation has been abandoned by
some evaluation campaigns, most notably the WMT
shared task series, see Koehn and Monz (2006)
through Callison-Burch et al (2012)1. Since 2008,
WMT uses a simple relative ranking of MT out-
puts as its primary manual evaluation technique:
the annotator is presented with up to 5 MT out-
puts for a given input sentence and the task is to
rank them from best to worst (ties allowed) on what-
ever criteria he or she deems appropriate. While this
single-scale relative ranking is perhaps faster to an-
notate and reaches a higher inter- and intra-annotator
agreement than the (absolute) fluency and adequacy
(Callison-Burch et al, 2007), the technique and its
evaluation are still far from satisfactory. Bojar et
al. (2011) observe several discrepancies in the in-
terpretation of the rankings, partly due to the high
load on human annotators (the comparison of sev-
eral long sentences at once, among other issues) but
partly also due to technicalities of the calculation.
Lo and Wu (2011a) present an interesting evalua-
tion technique called MEANT (or HMEANT if car-
ried out by humans), the core of which lies in as-
sessing whether the key elements in the predicate-
argument structure of the sentence have been pre-
served. In other words, lay annotators are check-
ing, if they recognize who did what to whom, when,
where and why from the MT outputs and whether
the respective role fillers convey the same meaning
as in the reference translation. HMEANT has been
shown to correlate reasonably well with manual ad-
equacy and ranking evaluations. It is relatively fast
and should lend itself to full automatization. On
the other hand, HMEANT was so far tested only on
translation into English and with just three compet-
ing MT systems.
1http://www.statmt.org/wmt06 till wmt12
30
In this work, we extend the application of
HMEANT to evaluating MT into Czech, a mor-
phologically rich language with relatively free word
order. The paper is structured as follows: Sec-
tion 2 presents the technical details of HMEANT
and relates HMEANT to an established linguistic
theory that underlies the Prague dependency tree-
banks (Hajic? et al, 2006; Hajic? et al, 2012) and
several other works. We also suggest possible ben-
efits of this coupling such as the reuse of tools. In
Section 3, we describe the setup and results of our
HMEANT experiment. Since this is the first time
HMEANT is applied to a new language, Section 4
constitutes the main contribution of this work. We
point out at several problems of HMEANT and pro-
pose a remedy, the empirical evaluation of which
however remains for future work. Section 5 con-
cludes our observations.
2 Relating HMEANT and Valency Theory
of FGD
2.1 HMEANT Annotation Procedure
HMEANT is designed to be simple and fast. The
annotation consists of two steps: (1) semantic role
labelling, SRL in the sequel, and (2) alignment of
roles between the hypothesis and the reference.
The annotation guidelines are deliberately mini-
malistic, so that even inexpert people can learn them
quickly. The complete guidelines for SRL are given
in Figure 1 and it takes less than 15 minutes to train
an unskilled person.
In the alignment task, the annotators first indicate
which frames in the reference and the hypothesis
correspond to each other. In the second step, they
align all matching role fillers to each other and also
mark the translation as ?Correct? or ?Partial?.
The HMEANT calculation then evaluates the f-
score of the predicates and their role fillers in a given
sentence. An important aspect of the calculation is
that unmatched predicates with all their role fillers
are excluded from the calculation.
2.2 Functional Generative Description
The core ideas of HMEANT follow the case gram-
mar (Fillmore, 1968) or PropBank (Palmer et al,
2005) and can be also directly related to an estab-
lished linguistic theory which was primarily devel-
Semantic frames summarize a sentence using a simple event
structure that captures essential parts of the meaning like
?who did what to whom, when, where, why and how?.
Phrases or clauses that express meanings can be identified as
playing a particular semantic role in the sentence. In other
words, semantic frames are the systematic abstraction of the
meanings in a sentence.
The following is the list of the semantic roles to be used in
HMEANT evaluation:
Agent (who) Action (did)
Experiencer or Patient (what) Benefactive (whom)
Temporal (when) Locative (where)
Purpose (why) Manner (how)
Degree or Extent (how) Modal (how) [may, should, ...]
Negation (how) [not] Other adverbial argument (how)
You may consider the Action predicate to be the central
event, while the other roles modify the Action to give a more
detailed description of the event. Each semantic frame con-
tains exactly one Action and any number of other roles.
Please note that the Action predicate must be exactly ONE
single word.
There may be multiple semantic frames in one sentence, be-
cause a sentence may be constructed to describe multiple
events and each semantic frame captures only one event.
Figure 1: Semantic role labeling guidelines of HMEANT.
oped for Czech, namely the Functional Generative
Description (Sgall, 1967; Sgall et al, 1986). The
theory defines so-called ?tectogrammatical? layer (t-
layer). At the t-layer, each sentence is represented as
a dependency tree with just content words as sepa-
rate nodes. All auxiliary words are ?hidden? into
attributes of the corresponding t-nodes. Moreover,
ellipsis is restored to some extent, so e.g. dropped
subject pronouns do have a corresponding t-node.
An important element of FGD is the valency the-
ory (Panevova?, 1980) which introduces empirical
linguistic tests to distinguish between what other
theories would call complements vs. adjuncts and
postulates the relationship between the set of verb
modifiers as observed in the sentence and the set of
valency slots that should be listed in a valency dic-
tionary. This aspect could provide a further refine-
ment of HMEANT, e.g. weighing complements and
adjuncts differently.
FGD has been thoroughly tested and refined dur-
ing the development of the Prague Dependency
Treebank (Hajic? et al, 2006)2 and the parallel
Prague Czech-English Dependency Treebank (Hajic?
2http://ufal.mff.cuni.cz/pdt2.0/
31
et al, 2012)3. Note that the latter is a translation
of all the 49k sentences of the Penn Treebank WSJ
section. Both English and Czech sentences are man-
ually annotated at the tectogrammatical layer, where
the English layer is based on the Penn annotation
and manually adapted for t-layer. Both languages in-
clude their respective valency lexicons and the work
on a bilingual valency lexicon is being developed
(S?indlerova? and Bojar, 2010).
A range of automatic tools to convert plain text up
to the t-layer exist for both English and Czech. Most
of them are now part of the Treex platform (Popel
and Z?abokrtsky?, 2010)4 and they were successfully
used in automatic annotation of 15 million parallel
sentences (Bojar et al, 2012)5 as well as other NLP
tasks including English-to-Czech MT. Recently, sig-
nificant effort was also invested in parsing not quite
correct output of MT systems into Czech for the
purposes of rule-based grammar correction (Rosa et
al., 2012). Establishing the automatic pipeline for
MEANT should be relatively easy with these tools
at hand.
2.3 HMEANT vs. FGD Valency
The formulation of HMEANT in terms of FGD is
straightforward: it is the f-score of matched t-nodes
for predicates and the subtrees of their immediate
dependents in the t-trees of the hypothesis and the
reference.
HMEANT uses a simple web-based annotation
interface which operates on the surface form of the
sentence. Annotators mark the predicate and their
complementations as contiguous spans in the sen-
tence. While this seems natural when we want
lay people to annotate, it brings some problems,
see Section 4. A linguistically adequate interface
would allow to mark tectogrammatical nodes and
subtrees in the t-layer, however, the customizable
editor TrEd6 used for manual annotation of t-layer
is too heavy for our purposes both in terms of speed
and complexity of user interface.
Perhaps the best option we plan to investigate in
future research is a mixed approach: the interface
would display only the text version of the sentence
3http://ufal.mff.cuni.cz/pcedt2.0/
4http://ufal.mff.cuni.cz/treex/
5http://ufal.mff.cuni.cz/czeng/
6http://ufal.mff.cuni.cz/tred/
HMEANT 0.2833
METEOR 0.2167
WER 0.1708
CDER 0.1375
NIST 0.1167
TER 0.1167
PER 0.0208
BLEU 0.0125
Table 1: Kendall?s ? for sentence-level correlation with
human rankings.
but it would internally know the (automatic) t-layer
structure. Selecting any word that corresponds to
the t-node of a verb would automatically extend the
selection to all other belongings of the t-node, i.e.
all auxiliaries of the verb. For role fillers, select-
ing any word from the role filler would select the
whole t-layer subtree. In order to handle errors in
the automatic t-layer annotation, the interface would
certainly need to allow manual selection and de-
selection of words, providing valuable feedback to
the automatic tools.
3 An Experiment in English-Czech MT
Evaluation
In this first study, we selected 50 sentences from the
English-to-Czech WMT12 manual evaluation. The
sentences were chosen to overlap with the standard
WMT ranking procedure (see Section 3.1) as much
as possible.
In total, 13 MT systems participated in this trans-
lation direction. We allocated 14 annotators (one
annotator for the SRL of the reference) so that no-
body saw the same sentence translated by more sys-
tems. The hypotheses were shuffled so every annota-
tor got samples from all systems as well as the refer-
ence. Unfortunately, time constraints and the large
number of MT systems prevented us from collect-
ing overlapping annotations, so we cannot evaluate
inter-annotator agreement.
Following Lo and Wu (2011a) and Callison-
Burch et al (2012), we report Kendall?s ? rank cor-
relation coefficients for sentence-level rankings as
provided by a range of automatic metrics and our
HMEANT. The gold standard are the manual WMT
rankings. See Table 1.
32
We see that HMEANT achieves a better correla-
tion than all the tested automatic metrics, although
in absolute terms, the correlation is not very high.
Lo and Wu (2011b) report ? for HMEANT of up to
0.49 and Lo and Wu (2011a) observe ? in the range
0.33 to 0.43. These figures are not comparable to our
result for several reasons: we evaluated 13 and not
just 3 MT systems, the gold standard for us are over-
all system rankings, not just adequacy judgments as
for Lo and Wu (2011b), and we evaluate translation
to Czech, not English. Callison-Burch et al (2012)
report ? for several automatic metrics on the whole
WMT12 English-to-Czech dataset, the best of which
correlates at ? = 0.18. The only common metric is
METEOR and it reaches 0.16 on the whole WMT12
set.7 In line with our observation, Czech-to-English
correlations reported by Callison-Burch et al (2012)
are higher: the best metric achieves 0.28 and aver-
ages 0.25 across four source languages.
The overall low sentence-level correlation of
our HMEANT and WMT12 rankings is obviously
caused to some extent by the problems we identi-
fied, see Section 4 below. On the other hand, it is
quite possible that the WMT-style rankings taken as
the gold standard are of a disputable quality them-
selves, see Section 3.1 or the detailed report on inter-
annotator agreement and a long discussion on inter-
preting the rankings in Callison-Burch et al (2012).
Last but not least, it is likely that HMEANT and
manual ranking simply measure different properties
of MT outputs. The Kendall?s ? is thus not an ulti-
mate meta-evaluation metric for us.
3.1 WMT-Style Rankings
This section illustrates some issues with the WMT
rankings when used for system-level evaluation. Ob-
viously, at the sentence level, the rankings can be-
have differently but the system-level evaluation ben-
efits from a large number of manual labels.
In the WMT-style rankings, humans are provided
with no more than 5 system outputs for a given sen-
tence at once. The task is to rank these 5 systems
relatively to each other, ties allowed.
Following Bojar et al (2011), we report three
possible evaluation regimes (or ?interpretations?) of
7It is possible that Callison-Burch et al (2012) use some-
what different METEOR settings apart from the different subset
of the data.
these 5-fold rankings to obtain system-level scores.
The first step is shared: all pairwise comparisons
implied by the 5-fold ranking are extracted. For
each system, we then report the percentage of cases
where the system won the pairwise comparison. Our
default interpretation is to exclude all ties from the
calculation, labelled ?Ties Ignored?, i.e. winswins + losses .
The former WMT interpretation (up to 2011) was to
include ties in both the numerator and the denomi-
nator, i.e. wins + tieswins+ties+losses denoted ?? Others?. WMT
summary paper also reports ?> Others? where the
ties are included in the denominator only, thus giv-
ing credit to systems that are different.
As we see in Table 2, each of the interpretations
leads to different rankings of the systems. More im-
portantly, the underlying set of sentences also affects
the result. For instance, the system ONLINEA jumps
to the second position in ?Ties Ignored? if we con-
sider only the 50 sentences used in our HMEANT
evaluation. To some extent, the differences are
caused by the lower number of observations. While
?All-No Ties? is based on 2893?134 pairwise com-
parisons per system, ?50-No Ties? is based on just
186?30 observations. Moreover, not all systems
came up among the 5 ranked systems for a given
sentence. In our 50 sentences, only 7.3?2.1 systems
were compared per sentence. On the full set of sen-
tences, this figure drops to 5.9?1.7.
4 Problems of HMEANT Annotation
We asked our annotators to take notes and report
any problems. On the positive side, some annota-
tors familiar with the WMT ranking evaluation felt
that in both phases of HMEANT, they ?knew what
they were doing and why?. In the ranking task, it
is unfortunately quite common that the annotator is
asked to rank incomparably bad hypotheses. In such
cases, the annotator probably tries to follow some
subjective and unspoken criteria, which often leads
to a lower in inter- and intra-annotator agreement.
On the negative side, we observed many problems
of the current version of HMEANT, and we propose
a remedy for all of them. We disregard minor tech-
nical issues of the annotation interface and focus on
the design decisions. The only technical limitation
worth mentioning was the inability to return to pre-
vious sentences. In some cases, this even caused the
33
Interpretation Ties Ignored ? Others > Others
Sentences All 50 All 50 All 50
cu-depfix 66.4 72.5 73.0 77.5 53.3 59.4
onlineB 63.0 61.4 70.5 69.3 50.3 49.0
uedin-wmt12 55.8 60.3 63.6 66.3 46.0 o 51.1
cu-tamch-boj 55.6 54.6 o 64.7 62.1 44.2 45.7
cu-bojar 2012 54.3 53.2 o 64.1 o 62.2 42.6 43.0
CU TectoMT 53.1 o 54.9 60.5 59.8 o 44.6 o 49.0
onlineA 52.9 o 61.4 o 60.8 o 66.7 o 44.0 o 53.0
pctrans2010 47.7 o 54.1 55.1 o 60.1 40.9 o 47.1
commercial2 46.0 51.3 54.6 59.5 38.7 42.7
cu-poor-comb 44.1 41.6 o 54.7 50.5 35.7 35.2
uk-dan-moses 43.5 33.2 53.4 44.2 o 35.9 27.7
SFU 36.1 31.0 46.8 43.0 30.0 25.6
jhu-hiero 32.2 26.7 43.2 36.0 27.0 23.3
Table 2: WMT12 system-level ranking results in three different evaluation regimes evaluated either on all sentences
or just the 50 sentences that were subject to our HMEANT annotation. The table is sorted along the first column and
the symbol ?o? in other columns marks items out of sequence.
annotators to skip parts of the annotation altogether,
because they clicked Next Sentence instead of the
Next Frame button.
Note that the impact of the problems on the final
HMEANT reliability varies. What causes just minor
hesitations in the SRL phase can lead to complete
annotation failures in the Alignment phase and vice
versa. We list the problems in decreasing severity,
based on our observations as well as the number of
annotators who complained about the given issue.
4.1 Vague SRL Guidelines
The first group of problems is caused by the SRL
guidelines being (deliberately) too succinct and de-
veloped primarily for English.
Complex predicates. Out of the many possible
cases where predicates are described using several
words, SRL guidelines mention just modal verbs and
reserve a label for them (assuming that the main verb
will be chosen as the Action, i.e. the predicate it-
self). This goes against the syntactic properties of
Czech and other languages, where the modal verb is
the one that conjugates and it is only complemented
by the content verb in infinitive. Some annotators
thus decided to mark such cases as a pair of nested
frames.
The problem becomes more apparent for other
classes of verbs, such as phasic verbs (e.g. ?to be-
gin?), which naturally lead to nested frames.
A specific problem for Czech mentioned by al-
most all annotators, was the copula verb ?to be?.
Here, the meaning-bearing element is actually the
adjective that follows (e.g. ?to be glad to . . . ?).
HMEANT forced the annotators to use e.g. the Ex-
periencer slot for the non-verbal part of this complex
predicate. In the negated form, ?nen?? (is not)?, some
annotators even marked the copula as Negation and
the non-verbal part as the Action.
No verb at all. HMEANT does not permit to an-
notate frames with no predicate. There are however
at least two frequent cases that deserve this option:
(1) the whole sentence can be a nominal construc-
tion such as the title of a section, and (2) an MT
system may erroneously omit the verb, while the re-
maining slot fillers are understandable and the whole
meaning of the sentence can be also guessed. Giving
no credit to such a sentence at all seems too strict. In
some cases, it was possible for the annotators to find
a substitute word for the Action role, e.g. a noun that
should have been translated as the verb.
A related issue was caused by the uncertainty to
what extent the frame annotation should go. There
are many nouns derived from verbs that also bear va-
lency. FGD acknowledges this and valency lexicons
for Czech do include also many of such nouns. If the
34
Reference Oblec?ky mus??me vystr???hat z c?asopisu?
Gloss clothes we-must cut from magazines
Roles Experiencer Modal Action Locative
Meaning We must cut the clothes (assuming paper toys) from magazines
Hypothesis Mus??me vyr???znout oblec?en?? z c?asopisu?
Gloss We-must cut clothes from magazines
Roles Modal Action Experiencer
Figure 2: An example of PP-attachment mismatch. While it is (almost) obvious from the word order of the reference
that the preposition phrase ?z c?asopisu?? is a separate filler, it was marked as part of the Experiencer role in the
hypothesis. In the alignment phase, there is no way to align the single Experiencer slot of the hypothesis onto the two
slots (Experiencer, Locative) if the reference.
instructions are not clear in this respect, it is quite
possible that one annotator creates frames for such
nouns and the other does not, causing a mismatch in
the Alignment phase.
PP-attachment. The problem of attaching prepo-
sitional phrases to verbs or to other noun phrases
is well acknowledged in many languages including
English and Czech. See an example in Figure 2.
A complete solution of the problem in the SRL
phase will never be possible, because there are nat-
urally ambiguous cases where each annotator can
prefer a different reading. However, the Align-
ment phase should be somehow prepared for the in-
evitable mismatches.
Unclear role labels. Insufficient role labels.
The set of role labels of HMEANT is very simple
compared to the set of edge labels (called ?func-
tors?) in the tectogrammatical annotation. Several
annotators mentioned that the HMEANT roleset is
hard to use especially for passive constructions or
verbs with a secondary object.
Because the final HMEANT calculation requires
aligned fillers to match in their role labels, the agree-
ment on role labels is important. We suggest experi-
menting also with a variant of HMEANT that would
disregard the labels altogether.
Other problematic cases are sentences where sev-
eral role fillers appear to belong to the same type,
e.g. Locative: ?Byl pr?evezen (He was transported)
| do nemocnice (to the hospital) | v za?chranne?m vr-
tuln??ku (in a helicopter)?. While it is semantically
obvious that the hospital is not in the helicopter, so
this is not a PP-attachment problem, some annota-
tors still mark both Locatives jointly as a single slot,
causing the same slot mismatch. It is also possible
that the annotator has actually assigned the Locative
label twice but the annotation interface interpreted
all the words as belonging to one filler only.
Coreference. The SRL guidelines are not specific
on handling of slot fillers realized as pronouns (or
even dropped pronouns). If we consider a sentence
like ?It is the man who wins?, it is not clear which
words should be marked as the Agent of the Action
?wins?. There are three candidates, all equally cor-
rect from the purely semantic point of view: ?it?,
?the man? and ?who?.
A natural choice would be to select the closest
word referring to the respective object, however, in
constructions of complex verbs or in pro-drop lan-
guages the object may not be explicitly stated in
the syntactically closest position. Depending on the
annotators? decisions, this can lead to a mismatch
in the number of slots in the subsequent Alignment
phase.
Other problems. Some annotators mentioned a
few other problems. One of them were paratactic
constructions: the frame-labelling procedure does
not allow to distinguish between sentences like ?It
is windy and it rains? vs. ?It is windy but it rains?,
because neither ?and? nor ?but? are a slot filler. Sim-
ilarly, expressions like ?for example? do not seem to
constitute a slot filler but still somehow refine the
meaning of the sentence and should be preserved in
the translation.
One annotator suggested that the importance of
the SRL phase should be emphasized and the anno-
tators should be pushed towards annotating as much
as they can, e.g. also by highlighting all verbs in
the sentence, in order to provide enough frames and
fillers to align in the second phase.
35
Reference Opily? r?idic? te?z?ce zrane?n
Gloss A drunken driver seriously injured
Roles Agent Extent Action
Meaning A drunken driver is seriously injured.
Hypothesis Opily? r?idic? va?z?ne? zranil
Gloss A drunken driver seriously injured (active form)
Roles Agent Extent Action
Meaning A drunken driver seriously injured (someone).
Figure 3: A mismatch of the meanings of the predicates. Other roles in the frames match perfectly.
The following sections describe problems of the
Alignment phase.
4.2 Correctness of the Predicate
HMEANT alignment phase allows the annotators to
either align or not align a pair of frames. There is
no option to indicate that the match of the predicates
themselves is somewhat incorrect. Once the predi-
cates are aligned, the user can only match individual
fillers, possibly penalizing partial mismatches.
Figure 3 illustrates this issue on a real example
from our data. Once the annotator decides to align
the frames, there is no way to indicate that the mean-
ing was reversed by the translation.
What native speakers of Czech also feel is that
the MT output in Figure 3 is incomplete, an Ex-
periencer is missing. A similar example from the
data is the hypothesis ?Sve?dek ozna?mil policii. (The
witness informed/announced the police.)? The verb
?ozna?mit (inform/announce)? in Czech requires the
message (perhaps the Experiencer in the HMEANT
terminology), similarly to the English ?announce?
but unlike ?inform?. The valency theory of FGD for-
mally describes the problem as a missing slot filler
and given a valency dictionary, such errors can be
even identified automatically.
On the other hand, it should be noted that a mis-
match in the predicate alone does not mean that the
translation is incorrect. An example in our data was
the phrase ?dokud se souc?asne? ume?n?? nedoc?kalo ve
V??dni nove?ho sta?nku? vs. ?nez? souc?asne? ume?n?? ve
V??dni dostalo novy? domov?. Both versions mean
?until contemporary art in Vienna was given a new
home? but due to the different conjunction chosen
(?dokud/nez?, till/until?), one of the verbs has to be
negated.
4.3 Need for M:N Frame Alignment
The majority of our annotators complained that
complex predicates such as phasal verbs or copula
constructions as well as muddled MT output with
no verb often render the frame matching impossi-
ble. If the reference and the hypothesis differ in the
number of frames, then it is also almost certain that
the role fillers observed in the two sentences will be
distributed differently among the frames, prohibiting
filler alignment.
A viable solution would be allow merging of
frames during the Alignment phase, which is equiva-
lent to allowing many-to-many alignment of frames.
The sets of role fillers would be simply unioned, im-
proving the chance for filler alignment.
4.4 Need for M:N Slot Alignment
Inherent ambiguities like PP-attachment or spuri-
ous differences in SRL prevent from 1-1 slot align-
ment rather frequently. A solution would be to allow
many-to-many alignments of slot fillers.
4.5 Partial Adequacy vs. Partial Fluency
The original HMEANT Alignment guidelines say to
mark an aligned slot pair as Correct or Partial match.
(Mismatching slots should not be aligned at all.) A
Partial match is described as:
Role fillers in MT express part of the
meaning of the aligned role fillers in the
reference translation. Do NOT penalize
extra meaning unless it belongs in other
role fillers in the reference translation.
The second sentence of the instructions is prob-
ably aimed at cases where the MT expresses more
than the reference does, which is possible because
36
the translator may have removed part of the content
or because the source and the reference are both not
quite literal translations from a third language. A
clarifying example of this case in the instructions is
highly desirable.
What our annotators noticed were cases where the
translation was semantically adequate but contained
e.g. an agreement mismatch or another grammar er-
ror. The instructions should exemplify, if this is to
be treated as a Correct or Partial match. Optionally,
the Partial match could be split into three separate
cases: partially inadequate, partially disfluent, and
partially inadequate and disfluent.
4.6 Summary of Suggested HMEANT Fixes
To summarize the observations above, our experi-
ence with HMEANT was overall positive, but we
propose several changes in the design to improve the
reliability of the annotations:
SRL Phase:
? The SRL guidelines should be kept as simple as
they are, but more examples and especially ex-
amples of incorrect MT output should be pro-
vided.
? The Action should be allowed to consist of sev-
eral words, including non-adjacent ones.
? The possibility of using automatic t-layer anno-
tation tools should be explored, at least to pre-
annotate which words form a multi-word pred-
icate or role filler.
Alignment Phase:
? The annotator must be able to indicate a partial
or incorrect match of the predicates themselves.
? Both frames as well as fillers should support
M:N alignment to overcome a range of natu-
rally appearing as well as spurious mismatches
in the two SRL annotations.
? Examples of anaphoric expressions should be
included in the guidelines, stressing that any el-
ement of the anaphora chain should be treated
as an appropriate representant of the role filler.
? The Partial match could distinguish between
an error in adequacy or fluency, or rather, the
Alignment guidelines should explicitly provide
examples of both types and ask the annotators
to disregard the difference.
Technical Changes:
? The annotators need to be able to go back
within each phase. (The division between
the SRL and Alignment phases should be pre-
served.)
We do not expect any of the proposed changes to
negatively impact annotation time. Actually, some
speedup may be obtained from the suggested pre-
annotation and also from a reduced hesitation of the
annotators in the alignment phase thanks to the M:N
alignment possibility.
5 Conclusion
We applied HMEANT, a technique for manual eval-
uation of MT quality based on predicate-argument
structure, to a new language, Czech. The experiment
confirmed that HMEANT is applicable in this set-
ting, outperforming automatic metrics in sentence-
level correlation with manual rankings.
During our annotation, we identified a range of
problems in the current HMEANT design. We thus
propose a few modifications to the technique and
also suggest backing HMEANT with a linguistic
theory of deep syntax, opening the avenue to au-
tomating the metric using available tools.
Acknowledgments
We would like to thank our annotators for all the com-
ments and also Chi-kiu Lo, Karteek Addanki, Anand
Karthik Tumuluru, and Avishek Kumar Thakur for ad-
ministering the annotation interface. This work was sup-
ported by the project EuroMatrixPlus (FP7-ICT-2007-
3-231720 of the EU and 7E09003+7E11051 of the
Czech Republic) and the Czech Science Foundation
grants P406/11/1499 and P406/10/P259 (Ondr?ej Bo-
jar); and by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no. HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-0022
and HR0011-06-C-0023; by the European Union under
the FP7 grant agreement no. 287658; and by the Hong
Kong Research Grants Council (RGC) research grant
GRF621008 (Dekai Wu). Any opinions, findings and
conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect
the views of the RGC, EU, or DARPA.
37
References
Herve? Blanchon, Christian Boitet, and Laurent Besacier.
2004. Spoken Dialogue Translation Systems Evalua-
tion: Results, New Trends, Problems and Proposals.
In Proceedings of International Conference on Spoken
Language Processing ICSLP 2004, Jeju Island, Korea,
October.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 1?11,
Edinburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012. The Joy of Parallelism with CzEng 1.0.
In Proceedings of the Eighth International Language
Resources and Evaluation Conference (LREC?12), Is-
tanbul, Turkey, May. ELRA, European Language Re-
sources Association. In print.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
Evaluation of Machine Translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Charles J. Fillmore. 1968. The Case for Case. In E. Bach
and R. Harms, editors, Universals in Linguistic The-
ory, pages 1?90. New York.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Ondr?ej Bojar, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing Prague Czech-English Dependency Treebank 2.0.
In Proceedings of the Eighth International Language
Resources and Evaluation Conference (LREC?12), Is-
tanbul, Turkey, May. ELRA, European Language Re-
sources Association. In print.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, Zdene?k Z?abokrtsky?, and Magda S?evc???kova?
Raz??mova?. 2006. Prague Dependency Treebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, New
York City, June. Association for Computational Lin-
guistics.
Chi-kiu Lo and Dekai Wu. 2011a. Meant: An inexpen-
sive, high-accuracy, semi-automatic metric for evalu-
ating translation utility based on semantic roles. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 220?229, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Chi-kiu Lo and Dekai Wu. 2011b. Structured vs. flat
semantic role representations for machine translation
evaluation. In Proceedings of the Fifth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, SSST-5, pages 10?20, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Jarmila Panevova?. 1980. Formy a funkce ve stavbe? c?eske?
ve?ty [Forms and functions in the structure of the Czech
sentence]. Academia, Prague, Czech Republic.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
Modular NLP Framework. In Hrafn Loftsson, Eirikur
Ro?gnvaldsson, and Sigrun Helgadottir, editors, Lec-
ture Notes in Artificial Intelligence, Proceedings of the
7th International Conference on Advances in Natu-
ral Language Processing (IceTAL 2010), volume 6233
of Lecture Notes in Computer Science, pages 293?
304, Berlin / Heidelberg. Iceland Centre for Language
Technology (ICLT), Springer.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek. 2012.
DEPFIX: A System for Automatic Correction of
Czech MT Outputs. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics. Submitted.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic and
Pragmatic Aspects. Academia/Reidel Publishing
Company, Prague, Czech Republic/Dordrecht, Nether-
lands.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague, Czech Republic.
Jana S?indlerova? and Ondr?ej Bojar. 2010. Building
a Bilingual ValLex Using Treebank Token Align-
ment: First Observations. In Proceedings of the Sev-
enth International Language Resources and Evalua-
tion (LREC?10), pages 304?309, Valletta, Malta, May.
ELRA, European Language Resources Association.
38
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49?56,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Unsupervised vs. supervised weight estimation
for semantic MT evaluation metrics
Chi-kiu LO and Dekai WU
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo,dekai}@cs.ust.hk
Abstract
We present an unsupervised approach to esti-
mate the appropriate degree of contribution of
each semantic role type for semantic transla-
tion evaluation, yielding a semantic MT eval-
uation metric whose correlation with human
adequacy judgments is comparable to that of
recent supervised approaches but without the
high cost of a human-ranked training corpus.
Our new unsupervised estimation approach
is motivated by an analysis showing that the
weights learned from supervised training are
distributed in a similar fashion to the relative
frequencies of the semantic roles. Empiri-
cal results show that even without a training
corpus of human adequacy rankings against
which to optimize correlation, using instead
our relative frequency weighting scheme to
approximate the importance of each semantic
role type leads to a semantic MT evaluation
metric that correlates comparable with human
adequacy judgments to previous metrics that
require far more expensive human rankings of
adequacy over a training corpus. As a result,
the cost of semantic MT evaluation is greatly
reduced.
1 Introduction
In this paper we investigate an unsupervised ap-
proach to estimate the degree of contribution of each
semantic role type in semantic translation evalua-
tion in low cost without using a human-ranked train-
ing corpus but still yields a evaluation metric that
correlates comparably with human adequacy judg-
ments to that of recent supervised approaches as in
Lo and Wu (2011a, b, c). The new approach is
motivated by an analysis showing that the distri-
bution of the weights learned from the supervised
training is similar to the relative frequencies of the
occurrences of each semantic role in the reference
translation. We then introduce a relative frequency
weighting scheme to approximate the importance of
each semantic role type. With such simple weight-
ing scheme, the cost of evaluating translation of lan-
guages with fewer resources available is greatly re-
duced.
For the past decade, the task of measuring the per-
formance of MT systems has relied heavily on lex-
ical n-gram based MT evaluation metrics, such as
BLEU (Papineni et al, 2002), NIST (Doddington,
2002), METEOR (Banerjee and Lavie, 2005), PER
(Tillmann et al, 1997), CDER (Leusch et al, 2006)
and WER (Nie?en et al, 2000) because of their sup-
port on fast and inexpensive evaluation. These met-
rics are good at ranking overall systems by averaging
their scores over the entire document. As MT sys-
tems improve, the focus of MT evaluation changes
from generally reflecting the quality of each system
to assisting error analysis on each MT output in de-
tail. The failure of such metrics in evaluating trans-
lation quality on sentence level are becoming more
apparent. Though containing roughly the correct
words, the MT output as a whole sentence is still
quite incomprehensible and fails to express mean-
ing that is close to the input. Lexical n-gram based
evaluation metrics are surface-oriented and do not
do so well at ranking translations according to ad-
equacy and are particularly poor at reflecting sig-
nificant translation quality improvements on more
meaningful word sense or semantic frame choices
which human judges can indicate clearly. Callison-
Burch et al (2006) and Koehn and Monz (2006)
even reported cases where BLEU strongly disagrees
with human judgment on translation quality.
49
Liu and Gildea (2005) proposed STM, a struc-
tural approach based on syntax to addresses the fail-
ure of lexical similarity based metrics in evaluating
translation grammaticality. However, a grammatical
translation can achieve a high syntax-based score but
still contains meaning errors arising from confusion
of semantic roles. On the other hand, despite the
fact that non-automatic, manually evaluations, such
as HTER (Snover et al, 2006), are more adequacy
oriented and show a high correlation with human ad-
equacy judgment, the high labor cost prohibits their
widespread use. There was also work on explicitly
evaluating MT adequacy with aggregated linguistic
features (Gime?nez and Ma`rquez, 2007, 2008) and
textual entailment (Pado et al, 2009).
In the work of Lo and Wu (2011a), MEANT
and its human variants HMEANT were introduced
and empirical experimental results showed that
HMEANT, which can be driven by low-cost mono-
lingual semantic roles annotators with high inter-
annotator agreement, correlates as well as HTER
and far superior than BLEU and other surfaced ori-
ented evaluation metrics. Along with additional im-
provements to the MEANT family of metrics, Lo
and Wu (2011b) detailed the studies of the impact of
each individual semantic role to the metric?s corre-
lation with human adequacy judgments. Lo and Wu
(2011c) further discussed that with a proper weight-
ing scheme of semantic frame in a sentence, struc-
tured semantic role representation is more accurate
and intuitive than flattened role representation for se-
mantic MT evaluation metrics.
The recent trend of incorporating more linguistic
features into MT evaluation metrics raise the dis-
cussion on the appropriate approach in weighting
and combining them. ULC (Gime?nez and Ma`rquez,
2007, 2008) uses uniform weights to aggregate lin-
guistic features. This approach does not capture the
importance of each feature to the overall translation
quality to the MT output. One obvious example of
different semantic roles contribute differently to the
overall meaning is that readers usually accept trans-
lations with errors in adjunct arguments as a valid
translation but not those with errors in core argu-
ments. Unlike ULC, Liu and Gildea (2007); Lo and
Wu (2011a) approach the weight estimation prob-
lem by maximum correlation training which directly
optimize the correlation with human adequacy judg-
Figure 1: HMEANT structured role representation with a
weighting scheme reflecting the degree of contribution of
each semantic role type to the semantic frame. (Lo and
Wu, 2011a,b,c).
ments. However, the shortcomings of this approach
is that it requires a human-ranked training corpus
which is expensive, especially for languages with
limited resource.
We argue in this paper that for semantic MT eval-
uation, the importance of each semantic role type
can easily be estimated using a simple unsupervised
approach which leverage the relative frequencies of
the semantic roles appeared in the reference transla-
tion. Our proposed weighting scheme is motivated
by an analysis showing that the weights learned
from supervised training are distributed in a similar
fashion to the relative frequencies of the semantic
roles. Our results show that the semantic MT eval-
uation metric using the relative frequency weight-
ing scheme to approximate the importance of each
semantic role type correlates comparably with hu-
man adequacy judgments to previous metrics that
use maximum correlation training, which requires
expensive human rankings of adequacy over a train-
ing corpus. Therefore, the cost of semantic MT eval-
uation is greatly reduced.
2 Semantic MT evaluation metrics
Adopting the principle that a good translation is one
from which human readers may successfully un-
derstand at least the basic event structure-?who did
what to whom, when, where and why? (Pradhan et
al., 2004)-which represents the most essential mean-
ing of the source utterances, Lo and Wu (2011a,b,c)
50
proposed HMEANT to evaluate translation utility
based on semantic frames reconstructed by human
reader of machine translation output. Monolingual
(or bilingual) annotators must label the semantic
roles in both the reference and machine translations,
and then to align the semantic predicates and role
fillers in the MT output to the reference translations.
These annotations allow HMEANT to then look at
the aligned role fillers, and aggregate the transla-
tion accuracy for each role. In the spirit of Oc-
cam?s razor and representational transparency, the
HMEANT score is defined simply in terms of a
weighted f-score over these aligned predicates and
role fillers. More precisely, HMEANT is defined as
follows:
1. Human annotators annotate the shallow seman-
tic structures of both the references and MT
output.
2. Human judges align the semantic frames be-
tween the references and MT output by judging
the correctness of the predicates.
3. For each pair of aligned semantic frames,
(a) Human judges determine the translation
correctness of the semantic role fillers.
(b) Human judges align the semantic role
fillers between the reference and MT out-
put according to the correctness of the se-
mantic role fillers.
4. Compute the weighted f-score over the match-
ing role labels of these aligned predicates and
role fillers.
mi ?
#tokens filled in frame i of MT
total #tokens in MT
ri ?
#tokens filled in frame i of REF
total #tokens in REF
Mi, j ? total # ARG j of PRED i in MT
Ri, j ? total # ARG j of PRED i in REF
Ci, j ? # correct ARG j of PRED i in MT
Pi, j ? # partially correct ARG j of PRED i in MT
precision =
?i mi
wpred+? j w j(Ci, j+wpartialPi, j)
wpred+? j w jMi, j
?i mi
recall =
?i ri
wpred+? j w j(Ci, j+wpartialPi, j)
wpred+? j w jRi, j
?i ri
HMEANT =
2?precision? recall
precision+ recall
where mi and ri are the weights for frame,i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence. Mi, j and Ri, j are the to-
tal counts of argument of type j in frame i in the
MT/REF respectively. Ci, j and Pi, j are the count of
the correctly and partial correctly translated argu-
ment of type j in frame i in the MT. wpred is the
weight for the predicate and wj is the weights for the
arguments of type j. These weights estimate the de-
gree of contribution of different types of semantic
roles to the overall meaning of the semantic frame
they attached to. The frame precision/recall is the
weighted sum of the number of correctly translated
roles in a frame normalized by the weighted sum
of the total number of all roles in that frame in the
MT/REF respectively. The sentence precision/recall
is the weighted sum of the frame precision/recall for
all frames normalized by the weighted sum of the to-
tal number of frames in MT/REF respectively. Fig-
ure 1 shows the internal structure of HMEANT.
In the work of Lo and Wu (2011b), the correla-
tion of all individual roles with the human adequacy
judgments were found to be non-negative. There-
fore, grid search was used to estimate the weights
of each roles by optimizing the correlation with hu-
man adequacy judgments. This approach requires
an expensive human-ranked training corpus which
may not be available for languages with sparse re-
sources.Unlike the supervised training approach, our
proposed relative frequency weighting scheme does
not require additional resource other than the SRL
annotated reference translation.
3 Which roles contribute more in the
semantic MT evaluation metric?
We begin with an investigation that suggests that the
relative frequency of each semantic role (which can
be estimated in unsupervised fashion without human
rankings) approximates fairly closely its importance
as determined by previous supervised optimization
approaches. Since there is no ground truth on which
51
Role Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)
Agent -0.09 -0.05 0.03
Experiencer 0.23 0.05 0.02
Benefactive 0.02 0.04 -0.01
Temporal 0.11 0.08 0.03
Locative -0.05 -0.05 -0.07
Purpose -0.01 0.03 -0.01
Manner -0.01 0.00 -0.01
Extent -0.02 0.00 -0.01
Modal ? 0.04 0.01
Negation ? 0.01 -0.01
Other -0.12 0.05 -0.01
Table 1: Deviation of relative frequency from optimized weight of each semantic role in GALE-A, GALE-B and
WMT12
semantic role contribute more to the overall meaning
in a sentence for semantic MT evaluation, we first
show that the unsupervised estimation are close to
the weights obtained from the supervised maximum
correlation training on a human-ranked MT evalua-
tion corpus. More precisely, the weight estimation
function is defined as follows:
c j ? # count of ARG j in REF of the test set
w j =
c j
? j c j
3.1 Experimental setup
For our benchmark comparison, the evaluation data
for our experiment is the same two sets of sentences,
GALE-A and GALE-B that were used in Lo and Wu
(2011b). The translation in GALE-A is SRL an-
notated with 9 semantic role types, while those in
GALE-B are SRL annotated with 11 semantic role
types (segregating the modal and the negation roles
from the other role).
To validate whether or not our hypothesis is lan-
guage independent, we also construct an evalua-
tion data set by randomly selecting 50 sentences
from WMT12 English to Czech (WMT12) transla-
tion task test corpus, in which 5 systems (out of
13 participating systems) were randomly picked for
translation adequacy ranking by human readers. In
total, 85 sets of translations (with translations from
some source sentences appear more than once in dif-
ferent sets) were ranked. The translation in WMT12
are also SRL annotated with the tag set as GALE-B,
i.e., 11 semantic role types.
The weights wpred, w j and wpartial were estimated
using grid search to optimize the correlation against
human adequacy judgments.
3.2 Results
Inspecting the distribution of the trained weights and
the relative frequencies from all three data sets, as
shown in table 1, we see that the overall pattern of
weights from unsupervised estimation has a fairly
small deviation from the those learned via super-
vised optimization. To visualize more clearly the
overall pattern of the weights from the two estima-
tion methods, we show the deviation of the unsuper-
vised estimation from the supervised estimation. A
deviation of 0 for all roles would mean that unsu-
pervised and supervised estimation produce exactly
identical weights. If the unsupervised estimation is
higher than the supervised estimation, the deviation
will be positive and vice versa.
What we see is that in almost all cases, the de-
viation between the trained weight and the relative
frequency of each role is always within the range [-
0.1, 0.1].
Closer inspection also reveals the following more
detailed patterns:
? The weight of the less frequent adjunct argu-
ments (e.g. purpose, manner, extent, modal and
negation) from the unsupervised estimation is
highly similar to that learned from the super-
52
PRED estimation Deviation (GALE-A) Deviation (GALE-B) Deviation (WMT12)
Method (i) 0.16 0.16 0.31
Method (ii) 0.02 0.01 0.01
Table 2: Deviation from optimized weight in GALE-A, GALE-B and WMT12 of the predicate?s weight as estimated
by (i) frequency of predicates in frames, relative to predicates and arguments; and (ii) one-fourth of agent?s weight.
vised maximum correlation training.
? The unsupervised estimation usually gives a
higher weight to the temporal role than the su-
pervised training would.
? The unsupervised estimation usually gives a
lower weight to the locative role than the super-
vised training would but the two weights from
the two approach are still high similar to each
other, yielding a deviation within the range of
[-0.07, 0.07].
? There is an obvious outlier found in GALE-A
where the deviation of the relative frequency
from the optimized weight is unusually high.
This suggests that the optimized weights in
GALE-A may be at the risk of over-fitting the
training data.
4 Estimating the weight for the predicate
The remaining question left to be investigated
is how we are to estimate the importance of the
predicate in an unsupervised approach. One obvious
approach is to treat the predicate the same way as
the arguments. That is, just like with arguments,
we could weight predicates by the relative fre-
quency of how often predicates occur in semantic
frames. However, this does not seem well motivated
since predicates are fundamentally different from
arguments: by definition, every semantic frame is
defined by one predicate, and arguments are defined
relative to the predicate.
On the other hand, inspecting the weights on the
predicate obtained from the supervised maximum
correlation training, we find that the weight of the
predicate is usually around one-fourth of the weight
of the agent role. More precisely, the two weight
estimation functions are defined as follows:
cpred ? # count of PRED in REF of the test set
Method (i) =
cpred
cpred +? j c j
Method (ii) = 0.25 ?wagent
We now show that the supervised estimation of
the predicate?s weight is closely approximated by
unsupervised estimation.
4.1 Experimental setup
The experimental setup is the same as that used in
section 3.
4.2 Results
The results in table 2 show that the trained weight
of the predicate and its unsupervised estimation of
one-fourth of the agent role?s weight are highly sim-
ilar to each other. In all three data sets, the devia-
tion between the trained weight and the heuristic of
one-fourth of the agent?s weight is always within the
range [0.1, 0.2].
On the other hand, treating the predicate the same
as arguments by estimating the unsupervised weight
using relative frequency largely over-estimates and
has a large deviation from the weight learned from
supervised estimation.
5 Semantic MT evaluation using
unsupervised weight estimates
Having seen that the weights of the predicate and
semantic roles estimated by the unsupervised ap-
proach fairly closely approximate those learned
from the supervised approach, we now show that the
unsupervised approach leads to a semantic MT eval-
uation metric that correlates comparably with hu-
man adequacy judgments to one that is trained on
a far more expensive human-ranked training corpus.
5.1 Experimental setup
Following the benchmark assessment in NIST Met-
ricsMaTr 2010 (Callison-Burch et al, 2010), we as-
sess the performance of the semantic MT evaluation
53
Metrics GALE-A GALE-B WMT12
HMEANT (supervised) 0.49 0.27 0.29
HMEANT (unsupervised) 0.42 0.23 0.20
NIST 0.29 0.09 0.12
METEOR 0.20 0.21 0.22
TER 0.20 0.10 0.12
PER 0.20 0.07 0.02
BLEU 0.20 0.12 0.01
CDER 0.12 0.10 0.14
WER 0.10 0.11 0.17
Table 3: Average sentence-level correlation with human adequacy judgments of HMEANT using supervised and
unsupervised weight scheme on GALE-A, GALE-B and WMT12, (with baseline comparison of commonly used
automatic MT evaluation metric.
metric at the sentence level using Kendall?s rank
correlation coefficient which evaluate the correla-
tion of the proposed metric with human judgments
on translation adequacy ranking. A higher the value
for indicates a higher similarity to the ranking by
the evaluation metric to the human judgment. The
range of possible values of correlation coefficient is
[-1,1], where 1 means the systems are ranked in the
same order as the human judgment and -1 means the
systems are ranked in the reverse order as the hu-
man judgment. For GALE-A and GALE-B, the hu-
man judgment on adequacy was obtained by show-
ing all three MT outputs together with the Chinese
source input to a human reader. The human reader
was instructed to order the sentences from the three
MT systems according to the accuracy of meaning in
the translations. For WMT12, the human adequacy
judgments are provided by the organizers.
The rest of the experimental setup is the same as
that used in section 3.
5.2 Results
Table 3 shows that HMEANT with the proposed un-
supervised semantic role weighting scheme corre-
late comparably with human adequacy judgments to
that optimized with a more expensive human-ranked
training corpus, and, outperforms all other com-
monly used automatic metrics (except for METEOR
in Czech). The results from GALE-A, GALE-B and
WMT12 are consistent. These encouraging results
show that semantic MT evaluation metric could be
widely applicable to languages other than English.
6 Conclusion
We presented a simple, easy to implement yet well-
motivated weighting scheme for HMEANT to esti-
mate the importance of each semantic role in eval-
uating the translation adequacy. Unlike the previ-
ous metrics, the proposed metric does not require
an expensive human-ranked training corpus and still
outperforms all other commonly used automatic MT
evaluation metrics. Interestingly, the distribution of
the optimal weights obtained by maximum correla-
tion training, is similar to the relative frequency of
occurrence of each semantic role type in the refer-
ence translation. HMEANT with the new weight-
ing scheme showed consistent results across differ-
ent language pairs and across different corpora in
the same language pair. With the proposed weight-
ing scheme, the semantic MT evaluation metric is
ready to be used off-the-shelf without depending on
a human-ranked training corpus. We believe that our
current work reduces the barrier for semantic MT
evaluation for resource scarce languages sufficiently
so that semantic MT evaluation can be applied to
most other languages.
Acknowledgments
We would like to thank Ondr?ej Bojar and all the
annotators from the Charles University in Prague
for participating in the experiments. This ma-
terial is based upon work supported in part by
the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-
12-C-0016, and GALE contract nos. HR0011-
06-C-0022 and HR0011-06-C-0023; by the Eu-
54
ropean Union under the FP7 grant agreement
no. 287658; and by the Hong Kong Research
Grants Council (RGC) research grants GRF621008,
GRF612806, DAG03/04.EG09, RGC6256/00E, and
RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the RGC, EU, or DARPA.
References
Satanjeev Banerjee and Alon Lavie. METEOR: An Auto-
matic Metric for MT Evaluation with Improved Corre-
lation with Human Judgments. In Proceedings of the
43th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-05), pages 65?72, 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Machine
Translation Research. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL-06), pages 249?
256, 2006.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Pryzbocki, and Omar Zaidan.
Findings of the 2010 Joint Workshop on Statistical
Machine Translation and Metrics for Machine Transla-
tion. In Proceedings of the Joint 5th Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
17?53, Uppsala, Sweden, 15-16 July 2010.
G. Doddington. Automatic Evaluation of Machine Trans-
lation Quality using N-gram Co-occurrence Statistics.
In Proceedings of the 2nd International Conference
on Human Language Technology Research (HLT-02),
pages 138?145, San Francisco, CA, USA, 2002. Mor-
gan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. Linguistic Features
for Automatic Evaluation of Heterogenous MT Sys-
tems. In Proceedings of the 2nd Workshop on Sta-
tistical Machine Translation, pages 256?264, Prague,
Czech Republic, June 2007. Association for Computa-
tional Linguistics.
Jesu?s Gime?nez and Llu??s Ma`rquez. A Smorgasbord of
Features for Automatic MT Evaluation. In Proceed-
ings of the 3rd Workshop on Statistical Machine Trans-
lation, pages 195?198, Columbus, OH, June 2008. As-
sociation for Computational Linguistics.
Philipp Koehn and Christof Monz. Manual and Auto-
matic Evaluation of Machine Translation between Eu-
ropean Languages. In Proceedings of the Workshop on
Statistical Machine Translation, pages 102?121, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer:
Efficient MT Evaluation Using Block Movements. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), 2006.
Ding Liu and Daniel Gildea. Syntactic Features for Eval-
uation of Machine Translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, page 25, 2005.
Ding Liu and Daniel Gildea. Source-Language Fea-
tures and Maximum Correlation Training for Machine
Translation Evaluation. In Proceedings of the 2007
Conference of the North American Chapter of the As-
sociation of Computational Linguistics (NAACL-07),
2007.
Chi-kiu Lo and Dekai Wu. MEANT: An Inexpensive,
High-Accuracy, Semi-Automatic Metric for Evaluat-
ing Translation Utility based on Semantic Roles. In
Proceedings of the Joint conference of the 49th Annual
Meeting of the Association for Computational Linguis-
tics : Human Language Technologies (ACL-HLT-11),
2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How se-
mantic frames evaluate MT more accurately. In Pro-
ceedings of the 22nd International Joint Conference
on Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Structured vs. Flat Semantic
Role Representations for Machine Translation Evalu-
ation. In Proceedings of the 5th Workshop on Syn-
tax and Structure in Statistical Translation (SSST-5),
2011.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. A Evaluation Tool for Machine Transla-
tion: Fast Evaluation for MT Research. In Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation (LREC-2000), 2000.
Sebastian Pado, Michel Galley, Dan Jurafsky, and Chris
Manning. Robust Machine Translation Evaluation
with Entailment Features. In Proceedings of the Joint
conference of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP-09), 2009.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: A Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-02), pages 311?318, 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H.
Martin, and Dan Jurafsky. Shallow Semantic Parsing
Using Support Vector Machines. In Proceedings of
55
the 2004 Conference on Human Language Technology
and the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-04), 2004.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A Study of Trans-
lation Edit Rate with Targeted Human Annotation. In
Proceedings of the 7th Conference of the Association
for Machine Translation in the Americas (AMTA-06),
pages 223?231, 2006.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Arkaitz Zubiaga, and Hassan Sawaf. Accelerated
DP Based Search For Statistical Translation. In Pro-
ceedings of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH-97),
1997.
56
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48?57,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Combining Top-down and Bottom-up Search
for Unsupervised Induction of Transduction Grammars
Markus SAERS and Karteek ADDANKI and Dekai WU
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|vskaddanki|dekai}@cs.ust.hk
Abstract
We show that combining both bottom-up rule
chunking and top-down rule segmentation
search strategies in purely unsupervised learn-
ing of phrasal inversion transduction gram-
mars yields significantly better translation ac-
curacy than either strategy alone. Previous ap-
proaches have relied on incrementally building
larger rules by chunking smaller rules bottom-
up; we introduce a complementary top-down
model that incrementally builds shorter rules
by segmenting larger rules. Specifically, we
combine iteratively chunked rules from Saers
et al (2012) with our new iteratively seg-
mented rules. These integrate seamlessly be-
cause both stay strictly within a pure trans-
duction grammar framework inducing under
matching models during both training and
testing?instead of decoding under a com-
pletely different model architecture than what
is assumed during the training phases, which
violates an elementary principle of machine
learning and statistics. To be able to drive in-
duction top-down, we introduce a minimum
description length objective that trades off
maximum likelihood against model size. We
show empirically that combining the more lib-
eral rule chunking model with a more conser-
vative rule segmentation model results in sig-
nificantly better translations than either strat-
egy in isolation.
1 Introduction
In this paper we combine both bottom-up chunking
and top-down segmentation as search directions in
the unsupervised pursuit of an inversion transduc-
tion grammar (ITG); we also show that the combi-
nation of the resulting grammars is superior to ei-
ther of them in isolation. For the bottom-up chunk-
ing approach we use the method reported in Saers
et al (2012), and for the top-down segmentation ap-
proach, we introduce a minimum description length
(MDL) learning objective. The new learning objec-
tive is similar to the Bayesian maximum a poste-
riori objective, and makes it possible to learn top-
down, which is impossible using maximum likeli-
hood, as the initial grammar that rewrites the start
symbol to all sentence pairs in the training data al-
ready maximizes the likelihood of the training data.
Since both approaches result in stochastic ITGs, they
can be easily combined into a single stochastic ITG
which allows for seamless combination. The point
of our present work is that the two different search
strategies result in very different grammars so that
the combination of them is superior in terms of trans-
lation accuracy to either of them in isolation.
The transduction grammar approach has the ad-
vantage that induction, tuning and testing are op-
timized on the exact same underlying model?this
used to be a given in machine learning and statistical
prediction, but has been largely ignored in the statis-
tical machine translation (SMT) community, where
most current SMT approaches to learning phrase
translations that (a) require enormous amounts of
run-time memory, and (b) contain a high degree of
redundancy. In particular, phrase-based SMT mod-
els such as Koehn et al (2003) and Chiang (2007)
often search for candidate translation segments and
transduction rules by committing to a word align-
ment that is completely alien to the grammar, as it
is learned with very different models (Brown et al
(1993), Vogel et al (1996)), whose output is then
combined heuristically to form the alignment actu-
ally used to extract lexical segment translations (Och
48
and Ney, 2003). The fact that it is even possible
to improve the performance of a phrase-based di-
rect translation system by tossing away most of the
learned segmental translations (Johnson et al, 2007)
illustrates the above points well.
Transduction grammars can also be induced from
treebanks instead of unannotated corpora, which cuts
down the vast search space by enforcing additional,
external constraints. This approach was pioneered
by Galley et al (2006), and there has been a lot of re-
search since, usually referred to as tree-to-tree, tree-
to-string and string-to-tree, depending on where
the analyses are found in the training data. This com-
plicates the learning process by adding external con-
straints that are bound to match the translation model
poorly; grammarians of English should not be ex-
pected to care about its relationship to Chinese. It
does, however, constitute a way to borrow nonter-
minal categories that help the translation model.
It is also possible for the word alignments leading
to phrase-based SMT models to be learned through
transduction grammars (see for example Cherry and
Lin (2007), Zhang et al (2008), Blunsom et al
(2008), Saers andWu (2009), Haghighi et al (2009),
Blunsom et al (2009), Saers et al (2010), Blunsom
and Cohn (2010), Saers and Wu (2011), Neubig et
al. (2011), Neubig et al (2012)). Even when the
SMT model is hierarchical, most of the information
encoded in the grammar is tossed away, when the
learned model is reduced to a word alignment. A
word alignment can only encode the lexical relation-
ships that exist between a sentence pair according to
a single parse tree, which means that the rest of the
model: the alternative parses and the syntactic struc-
ture, is ignored.
Theminimumdescription length (MDL) objective
that we will be using to drive the learning will pro-
vide a way to escape the maximum-likelihood-of-
the-data-given-the-model optimum that we start out
with. However, going only by MDL will also lead to
a degenerate case, where the size of the grammar is
allowed to shrink regardless of how unlikely the cor-
pus becomes. Instead, we will balance the length of
the grammar with the probability of the corpus given
the grammar. This has a natural Bayesian interpreta-
tion where the length of the grammar acts as a prior
over the structure of the grammar.
Similar approaches have been used before, but to
induce monolingual grammars. Stolcke and Omo-
hundro (1994) use a method similar to MDL called
Bayesianmodel merging to learn the structure of hid-
den Markov models as well as stochastic context-
free grammars. The SCFGs are induced by allowing
sequences of nonterminals to be replaced with a sin-
gle nonterminal (chunking) as well as allowing two
nonterminals to merge into one. Gr?nwald (1996)
uses it to learn nonterminal categories in a context-
free grammar. It has also been used to interpret vi-
sual scenes by classifying the activity that goes on in
a video sequences (Si et al, 2011). Our work in this
paper is markedly different to even the previous NLP
work in that (a) we induce an inversion transduc-
tion grammar (Wu, 1997) rather than a monolingual
grammar, and (b) we focus on learning the terminal
segments rather than the nonterminal categories.
The similar Bayesian approaches to finding the
model structure of ITGs have been tried before, but
only to generate alignments that mismatched trans-
lation models are then trained on, rather than using
the ITG directly as translation model, which we do.
Zhang et al (2008) use variational Bayes with a spar-
sity prior over the parameters to prevent the size of
the grammar to explode when allowing for adjacent
terminals in the Viterbi biparses to chunk together.
Blunsom et al (2008), Blunsom et al (2009) and
Blunsom and Cohn (2010) use Gibbs sampling to
find good phrasal translations. Neubig et al (2011)
and Neubig et al (2012) use a method more similar
to ours, but with a Pitman-Yor process as prior over
the structures.
The idea of iteratively segmenting the existing
sentence pairs to find good phrasal translations has
also been tried before; Vilar and Vidal (2005) intro-
duces the Recursive Alignment Model, which recur-
sively determines whether a bispan is a good enough
translation on its own (using IBM model 1), or if it
should be split into two bispans (either in straight or
inverted order). The model uses length of the input
sentence to determine whether to split or not, and
uses very limited local information about the split
point to determine where to split. Training the pa-
rameters is done with a maximum likelihood objec-
tive. In contrast, our model is one single genera-
tive model (as opposed to an ad hoc model), trained
with a minimum description length objective (rather
than trying to maximize the probability of the train-
49
ing data).
The rest of the paper is structured so that we first
take a closer look at the minimum description length
principle that will be used to drive the top-down
search (Section 2). We then show how the top-down
grammar is learned (Sections 3 and 4), before show-
ing how we combine the new grammar with that of
Saers et al (2012) (Section 5). We then detail the
experimental setup that will substantiate our claims
empirically (Section 6) before interpreting the results
of those experiments (Section 7). Finally, we offer
some conclusions (Section 8).
2 Minimum description length
The minimum description length principle is about
finding the optimal balance between the size of a
model and the size of some data given the model
(Solomonoff (1959), Rissanen (1983)). Consider the
information theoretical problem of encoding some
datawith amodel, and then sending both the encoded
data and the information needed to decode the data
(the model) over a channel; the minimum descrip-
tion length would be the minimum number of bits
sent over the channel. The encoded data can be inter-
preted as carrying the information necessary to dis-
ambiguate the ambiguities or uncertainties that the
model has about the data. Theoretically, the model
can grow in size and become more certain about the
data, and it can shrink in size and become more un-
certain about the data. An intuitive interpretation of
this is that the exceptions, which are a part of the en-
coded data, can be moved into the model itself. By
doing so, the size of the model increases, but there
is no longer an exception that needs to be conveyed
about the data. Some ?exceptions? occur frequently
enough that it is a good idea to incorporate them into
the model, and some do not; finding the optimal bal-
ance minimizes the total description length.
Formally, the description length (DL) is:
DL (M,D) = DL (D|M) + DL (M) (1)
Where M is the model and D is the data. Note the
clear parallel to probabilities that have been moved
into the logarithmic domain.
In natural language processing, we never have
complete data to train on, so we need our models to
generalize to unseen data. A model that is very cer-
tain about the training data runs the risk of not being
able to generalize to new data: it is over-fitting. It
is bad enough when estimating the parameters of a
transduction grammar, and catastrophic when induc-
ing the structure of the grammar. The key concept
that we want to capture when learning the structure
of a transduction grammar is generalization. This is
the property that allow it to translate new, unseen,
input. The challenge is to pin down what general-
ization actually is, and how to measure it.
One property of generalization for grammars is
that it will lower the probability of the training data.
This may seem counterintuitive, but can be under-
stood as moving some of the probability mass away
from the training data and putting it in unseen data.
A second property is that rules that are specific to
the training data can be eliminated from the gram-
mar (or replaced with less specific rules that generate
the same thing). The second property would shorten
the description of the grammar, and the first would
make the description of the corpus given the gram-
mar longer. That is: generalization raises the first
term and lowers the second in Equation 1. A good
generalization will lower the total MDL, whereas a
poor onewill raise it; a good generalizationwill trade
a little data certainty for more model parsimony.
2.1 Measuring the length of a corpus
The information-theoretic view of the problem also
gives a hint at the operationalization of length. Shan-
non (1948) stipulates that the number of bits it takes
to encode that a probabilistic variable has taken a cer-
tain value can be encoded using as little as the nega-
tive logarithmic probability of that outcome.
Following this, the parallel corpus given the trans-
duction grammar gives the number of bits required
to encode it: DL (C|G) = ?log2 (P (C|G)), where
C is the corpus and G is the grammar.
2.2 Measuring the length of an ITG
Since information theory deals with encoding se-
quences of symbols, we need some way to serialize
an inversion transduction grammar (ITG) into a mes-
sage whose length can be measured.
To serialize an ITG, we first need to determine
the alphabet that the message will be written in. We
need one symbol for every nonterminal, L0-terminal
and L1-terminal. We will also make the assump-
tion that all these symbols are used in at least one
50
rule, so that it is sufficient to serialize the rules in
order to express the entire grammar. To serialize
the rules, we need some kind of delimiter to know
where one rule starts and the next ends; we will ex-
ploit the fact that we also need to specify whether the
rule is straight or inverted (unary rules are assumed
to be straight), and merge these two functions into
one symbol. This gives the union of the symbols of
the grammar and the set {[], ??}, where [] signals the
beginning of a straight rule, and ?? signals the be-
ginning of an inverted rule. The serialized format
of a rule will be: rule type/start marker, followed by
the left-hand side nonterminal, followed by all right-
hand side symbols. The symbols on the right-hand
sides are either nonterminals or biterminals?pairs
ofL0-terminals andL1-terminals that model transla-
tion equivalences. The serialized form of a grammar
is the serialized form of all rules concatenated.
Consider the following toy grammar:
S ? A, A ? ?AA?, A ? [AA] ,
A ? have/?, A ? yes/?, A ? yes/?
Its serialized form would be:
[]SA??AAA[]AAA[]Ahave?[]Ayes?[]Ayes?
Now we can, again turn to information theory to ar-
rive at an encoding for this message. Assuming a
uniform distribution over the symbols, each symbol
will require ?log2
(
1
N
)
bits to encode (where N is
the number of different symbols?the type count).
The above example has 8 symbols, meaning that
each symbol requires 3 bits. The entire message is
23 symbols long, which means that we need 69 bits
to encode it.
3 Model initialization
Rather than starting out with a general transduction
grammar and fitting it to the training data, we do the
exact opposite: we start with a transduction gram-
mar that fits the training data as well as possible, and
generalize from there. The transduction grammar
that fits the training data the best is the one where
the start symbol rewrites to the full sentence pairs
that it has to generate. It is also possible to add any
number of nonterminal symbols in the layer between
the start symbol and the bisentences without altering
the probability of the training data. We take advan-
tage of this by allowing for one intermediate sym-
bol so that the start symbol conforms to the normal
form and always rewrites to precisely one nontermi-
nal symbol. This violate the MDL principle, as the
introduction of new symbols, by definition, makes
the description of the model longer, but conforming
to the normal form of ITGs was deemedmore impor-
tant than strictly minimizing the description length.
Our initial grammar thus looks like this:
S ? A,
A ? e0..T0/f0..V0 ,
A ? e0..T1/f0..V1 ,
...,
A ? e0..TN /f0..VN
Where S is the start symbol, A is the nonterminal,
N is the number of sentence pairs in the training cor-
pus, Ti is the length of the ith output sentence (which
makes e0..Ti the ith output sentence), and Vi is the
length of the ith input sentence (which makes f0..Vi
the ith input sentence).
4 Model generalization
To generalize the initial inversion transduction gram-
mar we need to identify parts of the existing biter-
minals that could be validly used in isolation, and
allow them to combine with other segments. This
is the very feature that allows a finite transduction
grammar to generate an infinite set of sentence pairs.
Doing this moves some of the probability mass,
which was concentrated in the training data, to un-
seen data?the very definition of generalization. Our
general strategy is to propose a number of sets of
biterminal rules and a place to segment them, eval-
uate how the description length would change if we
were to apply one of these sets of segmentations to
the grammar, and commit to the best set. That is:
we do a greedy search over the power set of possi-
ble segmentations of the rule set. As we will see, this
intractable problem can be reasonable efficiently ap-
proximated, which is what we have implemented and
tested.
The key component in the approach is the ability
to evaluate how the description length would change
if a specific segmentation was made in the grammar.
51
This can then be extended to a set of segmentations,
which only leaves the problem of generating suitable
sets of segmentations.
The key to a successful segmentation is to maxi-
mize the potential for reuse. Any segment that can
be reused saves model size. Consider the terminal
rule:
A ? five thousand yen is my limit/
????????
(Chinese gloss: ?w? z?i d?o ch? w? q?an r? y?an?).
This rule can be split into three rules:
A ? ?AA?,
A ? five thousand yen/????,
A ? is my limit/????
Note that the original rule consists of 16 symbols (in
our encoding scheme), whereas the new three rules
consists of 4 + 9 + 9 = 22 symbols. It is reason-
able to believe that the bracketing inverted rule is in
the grammar already, but this still leaves 18 symbols,
which is decidedly longer than 16 symbols?and we
need to get the length to be shorter if we want to see
a net gain, since the length of the corpus given the
grammar is likely to be longer with the segmented
rules. What we really need to do is find a way to
reuse the lexical rules that came out of the segmen-
tation. Now suppose the grammar also contained this
terminal rule:
A ? the total fare is five thousand yen/
??????????
(Chinese gloss: ?z?ng g?ng de f?i y?ng sh? w? q?an
r? y?an?). This rule can also be split into three rules:
A ? [AA] ,
A ? the total fare is/??????,
A ? five thousand yen/????
Again, we will assume that the structural rule is al-
ready present in the grammar, the old rule was 19
symbols long, and the two new terminal rules are
12+9 = 21 symbols long. Again we are out of luck,
as the new rules are longer than the old one, and three
rules are likely to be less probable than one rule dur-
ing parsing. The way to make this work is to realize
that the two existing rules share a bilingual affix?a
biaffix: ?five thousand dollars? translating into ??
????. If we make the two changes at the same
time, we get rid of 16 + 19 = 35 symbols worth of
rules, and introduce a mere 9 + 9 + 12 = 30 sym-
bols worth of rules (assuming the structural rules are
already in the grammar). Making these two changes
at the same time is essential, as the length of the five
saved symbols can be used to offset the likely in-
crease in the length of the corpus given the data. And
of course: the more rules we can find with shared bi-
affixes, the more likely we are to find a good set of
segmentations.
Our algorithm takes advantage of the above obser-
vation by focusing on the biaffixes found in the train-
ing data. Each biaffix defines a set of lexical rules
paired up with a possible segmentation. We evaluate
the biaffixes by estimating the change in description
length associated with committing to all the segmen-
tations defined by a biaffix. This allows us to find
the best set of segmentations, but rather than com-
mitting only to the one best set of segmentations, we
will collect all sets which would improve descrip-
tion length, and try to commit to as many of them
as possible. The pseudocode for our algorithm is as
follows:
G // The grammar
biaffixes_to_rules // Maps biaffixes to the
// rules they occur in
biaffixes_delta = [] // A list of biaffixes and
// their DL impact on G
for each biaffix b :
delta = eval_dl(b, biaffixes_to_rules[b], G)
if (delta < 0)
biaffixes_delta.push(b, delta)
sort_by_delta(biaffixes_delta)
for each b:delta pair in biaffixes_delta :
real_delta = eval_dl(b, biaffixes_to_rules[b], G)
if (real_delta < 0)
G = make_segmentations(b, biaffixes_to_rules[b], G)
The methods eval_dl, sort_by_delta and
make_segmentations evaluates the impact on de-
scription length that committing to a biaffix would
cause, sorts a list of biaffixes according to this delta,
and applies all the changes associated with a biaffix
to the grammar, respectively.
Evaluating the impact on description length
breaks down into two parts: the difference in de-
scription length of the grammar DL (G?) ? DL (G)
(where G? is the grammar that results from applying
all the changes that committing to a biaffix dictates),
52
and the difference in description length of the corpus
given the grammar DL (C|G?) ? DL (C|G). These
two quantities are simply added up to get the total
change in description length.
The difference in grammar length is calculated
as described in Section 2.2. The difference in de-
scription length of the corpus given the grammar
can be calculated by biparsing the corpus, since
DL (C|G?) = ?log2 (P (C|p?)) and DL (C|G) =
?log2 (P (C|p)) where p? and p are the rule prob-
ability functions of G? and G respectively. Bipars-
ing is, however, a very costly process that we do not
want to have inside a loop. Instead, we assume that
we have the original corpus probability (through bi-
parsing outside the loop), and estimate the new cor-
pus probability from it (in closed form). Given that
we are splitting the rule r0 into the three rules r1,
r2 and r3, and that the probability mass of r0 is dis-
tributed uniformly over the new rules, the new rule
probability function p? will be identical to p, except
that:
p? (r0) = 0,
p? (r1) = p (r1) +
1
3
p (r0) ,
p? (r2) = p (r2) +
1
3
p (r0) ,
p? (r3) = p (r3) +
1
3
p (r0)
Since we have eliminated all the occurrences of r0
and replaced them with combinations of r1, r2 and
r3, the probability of the corpus given this new rule
probability function will be:
P
(
C|p?
)
= P (C|p) p
? (r1) p? (r2) p? (r3)
p (r0)
To make this into a description length, we need to
take the negative logarithm of the above, which re-
sults in:
DL
(
C|G?
)
=
DL (C|G) ? log2
(p? (r1) p? (r2) p? (r3)
p (r0)
)
The difference in description length of the corpus
given the grammar can now be expressed as:
DL (C|G?) ? DL (C|G) =
?log2
(
p?(r1)p?(r2)p?(r3)
p(r0)
)
To calculate the impact of a set of segmentations, we
need to take all the changes into account in one go.
We do this in a two-pass fashion, first calculating
the new probability function (p?) and the change in
grammar description length (taking care not to count
the same rule twice), and then, in the second pass,
calculating the change in corpus description length.
5 Model combination
Themodel we learn by iteratively subsegmenting the
training data is guaranteed to be parsimonious while
retaining a decent fit to the training data; these are
desirable qualities, but there is a real risk that we
failed to make some generalization that we should
have made; to counter this risk, we can use a model
trained under more liberal conditions. We chose the
approach taken by Saers et al (2012) for two rea-
sons: (a) the model has the same form as our model,
which means that we can integrate it seamlessly, and
(b) their aims are similar to ours but their method
differs significantly; specifically, they let the model
grow in size as long as the data reduces in size. Both
these qualities make it a suitable complement for our
model.
Assuming we have two grammars (Ga and Gb)
that we want to combine, the interpolation param-
eter ? will determine the probability function of the
combined grammar such that:
pa+b (r) = ?pa (r) + (1 ? ?)pb (r)
for all rules r in the union of the two rule sets, and
where pa+b is the rule probability function of the
combined grammar and pa and pb are the rule prob-
ability functions of Ga and Gb respectively. Some
initial experiments indicated that an ? value of about
0.4 was reasonable (when Ga was the grammar ob-
tained through the training scheme outlined above,
andGb was the grammar obtained through the train-
ing scheme outlined in Saers et al (2012)), so we
used 0.4 in this paper.
6 Experimental setup
We have made the claim that iterative top-down seg-
mentation guided by the objective of minimizing the
description length gives a better precision grammar
than iterative bottom-up chunking, and that the com-
bination of the two gives superior results to either
53
0
2
4
6
8
10
12
14
 0  1  2  3  4  5  6  7Pro
bab
ility
 in 
log
 do
ma
in (M
bit)
Iterations
Figure 1: Description length in bits over the different it-
erations of top-down search. The lower portion represents
DL (G) and the upper portion represents DL (C|G).
approach in isolation. We have outlined how this
can be done in practice, and we now substantiate that
claim empirically.
We will initialize a stochastic bracketing inver-
sion transduction grammar (BITG) to rewrite it?s
one nonterminal symbol directly into all the sen-
tence pairs of the training data (iteration 0). We will
then segment the grammar iteratively a total of seven
times (iterations 1?7). For each iteration we will
record the change in description length and test the
grammar. Each iteration requires us to biparse the
training data, which we do with the cubic time algo-
rithm described in Saers et al (2009), with a beam
width of 100.
As training data, we use the IWSLT07 Chinese?
English data set (Fordyce, 2007), which contains
46,867 sentence pairs of training data, 506 Chinese
sentences of development data with 16 English ref-
erence translations, and 489 Chinese sentences with
6 English reference translations each as test data; all
the sentences are taken from the traveling domain.
Since the Chinese is written without whitespace, we
use a tool that tries to clump characters together into
more ?word like? sequences (Wu, 1999).
As the bottom-up grammar, we will reuse the
grammar learned in Saers et al (2012), specifically,
we will use the BITG that was bootstrapped from
a bracketing finite-state transduction grammar (BF-
STG) that has been chunked twice, giving bitermi-
nals where the monolingual segments are 0?4 tokens
long. The bottom-up grammar is trained on the same
0
10
20
30
40
50
60
 0  1  2  3  4  5  6  7N
um
ber
 of 
rule
s (th
ous
and
s)
Iterations
Figure 2: Number of rules learned during top-down
search over the different iterations.
data as our model.
To test the learned grammars as translation mod-
els, we first tune the grammar parameters to the train-
ing data using expectation maximization (Dempster
et al, 1977) and parse forests acquired with the
above mentioned biparser, again with a beam width
of 100. To do the actual decoding, we use our
in-house ITG decoder. The decoder uses a CKY-
style parsing algorithm (Cocke, 1969; Kasami, 1965;
Younger, 1967) and cube pruning (Chiang, 2007) to
integrate the language model scores. The decoder
builds an efficient hypergraph structure which is then
scored using both the induced grammar and the lan-
guage model. The weights for the language model
and the grammar, are tuned towards BLEU (Papineni
et al, 2002) using MERT (Och, 2003). We use the
ZMERT (Zaidan, 2009) implementation ofMERT as
it is a robust and flexible implementation of MERT,
while being loosely coupled with the decoder. We
use SRILM (Stolcke, 2002) for training a trigram
language model on the English side of the training
data. To evaluate the quality of the resulting transla-
tions, we use BLEU, and NIST (Doddington, 2002).
7 Experimental results
The results from running the experiments detailed
in the previous section can be summarized in four
graphs. Figures 1 and 2 show the size of our new,
segmenting model during induction, in terms of de-
scription length and in terms of rule count. The ini-
tial ITG is at iteration 0, where the vast majority
54
0.00
0.05
0.10
0.15
0.20
 0  1  2  3  4  5  6  7
BL
EU
Iterations
Figure 3: Variations in BLEU score over different iter-
ations. The thin line represents the baseline bottom-up
search (Saers et al, 2012), the dotted line represents the
top-down search, and the thick line represents the com-
bined results.
of the size is taken up by the model (DL (G)), and
very little by the data (DL (C|G))?just as we pre-
dicted. The trend over the induction phase is a sharp
decrease in model size, and a moderate increase in
data size, with the overall size constantly decreas-
ing. Note that, although the number of rules rises,
the total description length decreases. Again, this is
precisely what we expected. The size of the model
learned according to Saers et al (2012) is close to 30
Mbits?far off the chart. This shows that our new
top-down approach is indeed learning a more parsi-
monious grammar than the bottom-up approach.
Figures 3 and 4 shows the translation quality of
the learned model. The thin flat lines show the qual-
ity of the bottom-up approach (Saers et al, 2012),
whereas the thick curves shows the quality of the
new, top-down model presented in this paper with-
out (dotted line), and without the bottom-up model
(solid line). Although the MDL-based model is bet-
ter than the old model, the combination of the two
is still superior. It is particularly encouraging to see
that the over-fitting that seems to take place after iter-
ation 3 with the MDL-based approach is ameliorated
with the bottom-up model.
8 Conclusions
We have introduced a purely unsupervised learning
scheme for phrasal stochastic inversion transduction
grammars that is the first to combine two oppos-
0.0
1.0
2.0
3.0
4.0
5.0
 0  1  2  3  4  5  6  7
NIS
T
Iterations
Figure 4: Variations in NIST score over different iter-
ations. The thin line represents the baseline bottom-up
search (Saers et al, 2012), the dotted line represents the
top-down search, and the thick line represents the com-
bined results.
ing ways of searching for the phrasal translations: a
bottom-up rule chunking approach driven by a maxi-
mum likelihood (ML) objective and a top-down rule
segmenting approach driven by a minimum descrip-
tion length (MDL) objective. The combination ap-
proach takes advantage of the fact that the conser-
vative top-down MDL-driven rule segmenting ap-
proach learns a very parsimonious, yet competitive,
model when compared to a liberal bottom-up ML-
driven approach. Results show that the combination
of the two opposing approaches is significantly su-
perior to either of them in isolation.
9 Acknowledgements
This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under BOLT contract no. HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
55
References
P. Blunsom and T. Cohn. Inducing syn-
chronous grammars with slice sampling. In
HLT/NAACL2010, pages 238?241, Los Angeles,
California, June 2010.
P. Blunsom, T. Cohn, and M. Osborne. Bayesian
synchronous grammar induction. In Proceedings
of NIPS 21, Vancouver, Canada, December 2008.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. A
gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of ACL/IJCNLP, pages
782?790, Suntec, Singapore, August 2009.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L.Mercer. TheMathematics ofMachine Trans-
lation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311, 1993.
C. Cherry and D. Lin. Inversion transduction gram-
mar for joint phrasal translation modeling. In Pro-
ceedings of SSST, pages 17?24, Rochester, New
York, April 2007.
D. Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228, 2007.
J. Cocke. Programming languages and their compil-
ers: Preliminary notes. Courant Institute ofMath-
ematical Sciences, New York University, 1969.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38, 1977.
G. Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In Proceedings of the 2nd International
Conference on Human Language Technology Re-
search, pages 138?145, San Diego, California,
2002.
C. S. Fordyce. Overview of the IWSLT 2007 evalu-
ation campaign. In Proceedings of IWSLT, pages
1?12, 2007.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thayer. Scalable infer-
ence and training of context-rich syntactic trans-
lation models. In Proceedings of COLING/ACL-
2006, pages 961?968, Sydney, Australia, July
2006.
Peter Gr?nwald. A minimum description length ap-
proach to grammar inference in symbolic. Lecture
Notes in Artificial Intelligence, (1040):203?216,
1996.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein.
Better word alignments with supervised itg mod-
els. In Proceedings of ACL/IJCNLP-2009, pages
923?931, Suntec, Singapore, August 2009.
H. Johnson, J. Martin, G. Foster, and R. Kuhn.
Improving translation quality by discarding most
of the phrasetable. In Proceedings of EMNLP-
CoNLL-2007, pages 967?975, Prague, Czech Re-
public, June 2007.
T. Kasami. An efficient recognition and syntax anal-
ysis algorithm for context-free languages. Tech-
nical Report AFCRL-65-00143, Air Force Cam-
bridge Research Laboratory, 1965.
P. Koehn, F. J. Och, and D. Marcu. Statistical
Phrase-Based Translation. In Proceedings of
HLT/NAACL-2003, volume 1, pages 48?54, Ed-
monton, Canada, May/June 2003.
G. Neubig, T. Watanabe, E. Sumita, S. Mori, and
T. Kawahara. An unsupervised model for joint
phrase alignment and extraction. In Proceedings
of ACL/HLT-2011, pages 632?641, Portland, Ore-
gon, June 2011.
G. Neubig, T. Watanabe, S. Mori, and T. Kawahara.
Machine translation without words through sub-
string alignment. In Proceedings of ACL-2012,
pages 165?174, Jeju Island, Korea, July 2012.
F. J. Och and H. Ney. A Systematic Comparison of
Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51, 2003.
F. J. Och. Minimum error rate training in statistical
machine translation. InProceedings of ACL-2003,
pages 160?167, Sapporo, Japan, July 2003.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL-2002,
pages 311?318, Philadelphia, Pennsylvania, July
2002.
J. Rissanen. A universal prior for integers and esti-
mation by minimum description length. The An-
nals of Statistics, 11(2):416?431, June 1983.
56
M. Saers and D. Wu. Improving phrase-based
translation via word alignments from Stochastic
Inversion Transduction Grammars. In Proceed-
ings of SSST-3, pages 28?36, Boulder, Colorado,
June 2009.
M. Saers and D. Wu. Principled induction of phrasal
bilexica. In Proceedings of EAMT-2011, pages
313?320, Leuven, Belgium, May 2011.
M. Saers, J. Nivre, and D. Wu. Learning stochastic
bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings
of IWPT?09, pages 29?32, Paris, France, October
2009.
M. Saers, J. Nivre, and D. Wu. Word alignment with
stochastic bracketing linear inversion transduc-
tion grammar. In Proceedings of HLT/NAACL-
2010, pages 341?344, Los Angeles, California,
June 2010.
M. Saers, K. Addanki, and D. Wu. From finite-state
to inversion transductions: Toward unsupervised
bilingual grammar induction. In Proceedings of
COLING 2012: Technical Papers, pages 2325?
2340, Mumbai, India, December 2012.
C. E. Shannon. A mathematical theory of com-
munication. The Bell System Technical Journal,
27:379?423, 623?, July, October 1948.
Z. Si, M. Pei, B. Yao, and S. Zhu. Unsuper-
vised learning of event and-or grammar and se-
mantics from video. In Proceedings of the 2011
IEEE International Conference on Computer Vi-
sion (ICCV), pages 41?48, November 2011.
R. J. Solomonoff. A new method for discovering the
grammars of phrase structure languages. In IFIP
Congress, pages 285?289, 1959.
A. Stolcke and S. Omohundro. Inducing proba-
bilistic grammars by bayesian model merging. In
R. C. Carrasco and J. Oncina, editors, Grammat-
ical Inference and Applications, pages 106?118.
Springer, 1994.
A. Stolcke. SRILM ? an extensible language model-
ing toolkit. In Proceedings of ICSLP-2002, pages
901?904, Denver, Colorado, September 2002.
J. M. Vilar and E. Vidal. A recursive statistical trans-
lation model. In ACL-2005 Workshop on Building
andUsing Parallel Texts, pages 199?207, AnnAr-
bor, Jun 2005.
S. Vogel, H. Ney, and C. Tillmann. HMM-based
Word Alignment in Statistical Translation. In Pro-
ceedings of COLING-96, volume 2, pages 836?
841, 1996.
D. Wu. Stochastic Inversion Transduction Gram-
mars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403, 1997.
Z. Wu. LDC Chinese segmenter, 1999.
D. H. Younger. Recognition and parsing of context-
free languages in time n3. Information and Con-
trol, 10(2):189?208, 1967.
O. F. Zaidan. Z-MERT: A Fully Configurable Open
Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague
Bulletin of Mathematical Linguistics, 91:79?88,
2009.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea.
Bayesian learning of non-compositional phrases
with synchronous parsing. In Proceedings of
ACL-08: HLT, pages 97?105, Columbus, Ohio,
June 2008.
57
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 422?428,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
MEANT at WMT 2013: A tunable, accurate yet inexpensive
semantic frame based MT evaluation metric
Chi-kiu LO and Dekai WU
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|dekai}@cs.ust.hk
Abstract
The linguistically transparentMEANT and
UMEANT metrics are tunable, simple
yet highly effective, fully automatic ap-
proximation to the human HMEANT MT
evaluation metric which measures seman-
tic frame similarity between MT output
and reference translations. In this pa-
per, we describe HKUST?s submission
to the WMT 2013 metrics evaluation
task, MEANT and UMEANT. MEANT
is optimized by tuning a small number
of weights?one for each semantic role
label?so as to maximize correlation with
human adequacy judgment on a devel-
opment set. UMEANT is an unsuper-
vised version where weights for each se-
mantic role label are estimated via an in-
expensive unsupervised approach, as op-
posed to MEANT?s supervised method re-
lying on more expensive grid search. In
this paper, we present a battery of exper-
iments for optimizing MEANT on differ-
ent development sets to determine the set
of weights that maximize MEANT?s accu-
racy and stability. Evaluated on test sets
from the WMT 2012/2011 metrics evalua-
tion, bothMEANT and UMEANT achieve
competitive correlations with human judg-
ments using nothing more than a monolin-
gual corpus and an automatic shallow se-
mantic parser.
1 Introduction
We evaluate in the context of WMT 2013 the
MEANT (Lo et al, 2012) and UMEANT (Lo
andWu, 2012) semantic machine translation (MT)
evaluation metrics?tunable, simple yet highly ef-
fective, fully-automatic semantic frame based ob-
jective functions that score the degree of similarity
between the MT output and the reference transla-
tions via semantic role labels (SRL). Recent stud-
ies (Lo et al, 2013; Lo and Wu, 2013) show that
tuningMT systems againstMEANTmore robustly
improves translation adequacy, compared to tun-
ing against BLEU or TER.
In the past decade, the progress of machine
translation (MT) research is predominantly driven
by the fast and cheap n-gram based MT eval-
uation metrics, such as BLEU (Papineni et al,
2002), which assume that a good translation is one
that shares the same lexical choices as the ref-
erence translation. Despite enforcing fluency, it
has been established that these metrics do not en-
force translation utility adequately and often fail to
preserve meaning closely (Callison-Burch et al,
2006; Koehn and Monz, 2006). Unlike BLEU,
or other n-gram based MT evaluation metrics,
MEANT adopts at outset the principle that a good
translation is one from which the human readers
may successfully understand at least the central
meaning of the input sentence as captured by the
basic event structure??who did what to whom,
when, where and why?(Pradhan et al, 2004).
Lo et al (2012) show that MEANT correlates
better with human adequacy judgment than other
commonly used automatic MT evaluation metrics,
such as BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie,
2005), CDER (Leusch et al, 2006), WER (Nie?en
et al, 2000), and TER (Snover et al, 2006). Re-
cent studies (Lo et al, 2013; Lo andWu, 2013) also
show that tuning MT system against MEANT pro-
duces more robustly adequate translations on both
formal news text genre and informal web forum
or public speech genre compared to tuning against
BLEU or TER. These studies show thatMEANT is
a tunable and highly-accurate MT evaluation met-
ric that drives MT system development towards
higher utility.
As described in Lo and Wu (2011a), the pa-
422
rameters in MEANT, i.e. the weight for each se-
mantic role label, could be estimated using simple
grid search to optimize the correlation with human
adequacy judgments. Later, Lo and Wu (2012)
described an unsupervised approach for estimat-
ing the parameters of MEANT using relative fre-
quency of each semantic role label in the reference
translations under the situation when the human
judgments for the development set are unavailable.
In this paper, we refer the version of MEANT us-
ing the unsupervised approach of weight estima-
tion as UMEANT.
In this paper, we present a battery of exper-
iments for optimizing MEANT on different de-
velopment sets to determine the set of weights
that maximizes MEANT?s accuracy and stability.
Evaluated on the test sets ofWMT 2012/2011 met-
rics evaluation, MEANT and UMEANT achieve
a competitive correlation score with human judg-
ments by nothing more than a monolingual corpus
and an automatic shallow semantic parser.
2 Related work
2.1 Lexical similarity based metrics
N-gram or edit distance based metrics such as
BLEU (Papineni et al, 2002), NIST (Dodding-
ton, 2002), METEOR (Banerjee and Lavie, 2005),
CDER (Leusch et al, 2006), WER (Nie?en et
al., 2000), and TER (Snover et al, 2006) do not
correctly reflect the similarity of the basic event
structure? ?who did what to whom, when, where
and why?? of the input sentence. In fact, a
number of large scale meta-evaluations (Callison-
Burch et al, 2006; Koehn and Monz, 2006) report
cases where BLEU strongly disagrees with human
judgments of translation adequacy.
Although AMBER (Chen et al, 2012) shows a
high correlation with human adequacy judgment
(Callison-Burch et al, 2012) and claims to pre-
serve the simplicity of BLEU, the modifications it
incurred on BLEU through four different n-gram
matching strategies and several different penalties
makes it very hard to interpret and indicate what
errors the MT systems are making.
2.2 Linguistic feature based metrics
ULC (Gim?nez and M?rquez, 2007, 2008) is
an automatic metric that incorporates several se-
mantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al, 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al, 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an SMT system using a
pure form of ULC perhaps due to its expensive run
time. Lambert et al (2006) did tune on QUEEN,
a simplified version of ULC that discards the se-
mantic features of ULC and is based on pure lexi-
cal similarity. Therefore, QUEEN suffers from the
problem of failing to reflect translation adequacy
similar to other n-gram based metrics.
Similarly, SPEDE (Wang andManning, 2012) is
an integrated probabilistic FSM and probabilistic
PDA model that predicts the edit sequence needed
for the MT output to match the reference. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps; con-
tain several dozens of parameters to tune and em-
ploy expensive linguistic resources, like WordNet
and paraphrase table. Like ULC, these matrices
are not useful in the MT system development cy-
cle for tuning due to expensive running time. The
metrics themselves are also expensive in training
and tuning due to the large number of parameters
to be estimated. Although ROSE (Song and Cohn,
2011) is a weighted linear model of shallow lin-
guistic features which is cheaper in run time but it
still contains several dozens of weights that need to
be tuned which affects the portability of the metric
for evaluating translations across domains.
Rios et al (2011) introduced TINE, an auto-
matic recall-oriented evaluationmetric which aims
to preserve the basic event structure, but no work
has been done toward tuning an SMT system
against it. TINE performs comparably to BLEU
and worse than METEOR on correlation with hu-
man adequacy judgment.
3 MEANT and UMEANT
MEANT (Lo et al, 2012), which is the weighted
f-measure over the matched semantic role labels
of the automatically aligned semantic frames and
role fillers, outperforms BLEU, NIST, METEOR,
WER, CDER and TER. Recent studies (Lo et al,
2013; Lo andWu, 2013) also show that tuning MT
system against MEANT produces more robustly
adequate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech. Pre-
423
Figure 1: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow se-
mantic parser. There are no semantic frames for MT3 since there is no predicate.
cisely, MEANT is computed as follows:
1. Apply an automatic shallow semantic parser
on both the references and MT output. (Fig-
ure 1 shows examples of automatic shallow
semantic parses on both reference and MT
output.)
2. Applymaximumweighted bipartite matching
algorithm to align the semantic frames be-
tween the references and MT output by the
lexical similarity of the predicates.
3. For each pair of aligned semantic frames,
(a) Lexical similarity scores determine the
similarity of the semantic role fillers.
(b) Apply maximum weighted bipartite
matching algorithm to align the seman-
tic role fillers between the reference and
MT output according to their lexical
similarity.
4. Compute the weighted f-measure over the
matching role labels of these aligned predi-
cates and role fillers.
Mi,j ? total # ARG j of aligned frame i in MT
Ri,j ? total # ARG j of aligned frame i in REF
Si,pred ? similarity of predicate in aligned frame i
Si,j ? similarity of ARG j in aligned frame i
wpred ? weight of similarity of predicates
wj ? weight of similarity of ARG j
mi ? #tokens filled in aligned frame i of MTtotal #tokens in MT
ri ? #tokens filled in aligned frame i of REFtotal #tokens in REF
precision =
?
imi
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjMi,j?
imi
recall =
?
i ri
wpredSi,pred+
?
j wjSi,j
wpred+
?
j wjRi,j?
i ri
wheremi and ri are the weights for frame, i, in the
MT/REF respectively. These weights estimate the
degree of contribution of each frame to the overall
meaning of the sentence. Mi,j and Ri,j are the to-
tal counts of argument of type j in frame i in the
MT and REF respectively. Si,pred and Si,j are the
lexical similarities of the predicates and role fillers
of the arguments of type j between the reference
translations and the MT output. wpred and wj are
the weights of the lexical similarities of the predi-
cates and role fillers of the arguments of typej be-
tween the reference translations and the MT out-
put. There are in total 12 weights for the set of
424
semantic role labels in MEANT as defined in Lo
and Wu (2011b).
For MEANT, wpred and wj are determined us-
ing supervised estimation via a simple grid search
to optimize the correlation with human adequacy
judgments (Lo and Wu, 2011a). For UMEANT,
wpred and wj are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the reference translations when the hu-
man judgments on adequacy of the development
set were unavailable (Lo and Wu, 2012).
In this experiment, we use a MEANT /
UMEANT implementation along the lines de-
scribed in Lo et al (2012) and Tumuluru et al
(2012) but we incorporate a variant of the aggre-
gation function proposed in Mihalcea et al (2006)
for phrasal similarity of role fillers as it normal-
izes the phrase length better than geometric mean
as described in Tumuluru et al (2012). In case
there is no semantic frame in the sentence, we treat
the whole sentence as a phrase and calculate the
phrasal similarity, like the role fillers in step 3.1,
as the MEANT score.
4 Experimental setup
We tune the 12 weights for the set of semantic role
labels in MEANT using grid search to maximize
the correlationwith human judgment on 6 develop-
ment sets. Following the protocol inWMT12 met-
rics evaluation task (Callison-Burch et al, 2012),
we use Kendall?s correlation coefficient for the
sentence-level correlation with human judgments.
The GALE development set consists of 40 sen-
tences randomly drawn from the DARPA GALE
P2.5 Chinese-English evaluation set alng with the
outputs from 3 participating MT systems and the
corresponding human adequacy judgments. The
WMT12-A development set consists of 800 sen-
tences randomly drawn from the Czech-English
test set in WMT12 metrics evaluation task along
with the output from 5 participating systems and
the corresponding human judgments. Similarly,
each of theWMT12-B,WMT12-C andWMT12-D
development sets consists of 800 randomly drawn
sentences from the WMT12 metrics evaluation
test set on German-English, Spanish-English and
French-English respectively. The WMT12-E de-
velopment set consists of 800 sentences out of
which 200 sentences were randomly drawn from
each of WMT12-A, WMT12-B, WMT12-C and
WMT12-D data set.
We evaluated MEANT and UMEANT on 3
groups of test sets. The first group is the original
(without partition) test data for each language pair
(translated in English) in WMT12. This group of
test sets is used for comparing MEANT?s perfor-
mance with the reported results from other partic-
ipants of WMT12. The second group is the held
out subset of the test data for each language pair in
WMT12. The third group is the original set of test
data for each language pair in WMT11. The lat-
ter 2 groups are used for determining which set of
tuned weights maximize the accuracy and stability
of MEANT.
5 Results
Table 1 shows that the best and the worst sentence-
level correlations reported in Callison-Burch et al
(2012) on the original WMT12 test sets (without
partitioning) for translations into English, together
the sentence-level correlation of MEANT tuned
on different development sets and UMEANT. The
grey boxes mark the results of experiments in
which there was an overlap between parts of the
development data and the test data. A study of the
values for the 12 weights associated with the se-
mantic role labels show that a general trend of the
importance of different labels in MEANT: ?who?
is always the most important; ?did?, ?what?,
?where?, ?why?, ?extent?, ?modal? and ?other?
are quite important too; ?when?, ?manner? and
?negation? fluctuate where they are quite impor-
tant in some development sets but not quite im-
portant in some development sets; ?whom? is usu-
ally not important. Given the fact that MEANT
employs significantly less expensive linguistic re-
sources and less sophisticated machine learning al-
gorithm in tuning the parameters, the performance
of MEANT is very competitive with other partici-
pants last year.
Table 2 shows the sentence-level correlation on
the WMT12 held-out test sets and the original
WMT11 test sets of MEANT tuned on different
development sets and UMEANT together with the
average sentence-level correlation on all test sets.
The results show that MEANT tuning onWMT12-
C development set achieve the highest sentence-
level correlation with human judgments on aver-
age. UMEANT, the unsupervised wight estimated
version of MEANT, achieves a very competitive
correlation score when compared with MEANT
tuned on different development sets. As a result,
425
Table 1: The best and the worst sentence-level correlation reported in Callison-Burch et al (2012) on the
original WMT12 test sets (without partitioning) for translations into English together the sentence-level
correlation of MEANT tuned on different development sets and UMEANT. The grey box marked results
of experiments in which parts of the development data and the test data are overlapped.
WMT12 cz-en WMT12 de-en WMT12 es-en WMT12 fr-en
Best reported 0.21 0.28 0.26 0.26
MEANT (GALE) 0.13 0.16 0.15 0.15
MEANT (WMT12-A) 0.12 0.17 0.16 0.15
MEANT (WMT12-B) 0.11 0.18 0.15 0.14
MEANT (WMT12-C) 0.12 0.17 0.17 0.15
MEANT (WMT12-D) 0.12 0.17 0.16 0.16
MEANT (WMT12-E) 0.12 0.17 0.17 0.15
UMEANT 0.12 0.17 0.16 0.14
Worst reported 0.06 0.08 0.08 0.07
Table 2: Sentence-level correlation on the WMT12 held-out test sets and the original WMT11 test sets
of MEANT tuned on different development sets and UMEANT together with the average sentence-level
correlation on all test sets.
WMT12 held-out WMT11 Average
cz-en de-en es-en fr-en cz-en de-en es-en fr-en -
MEANT (GALE) 0.0657 0.1251 0.1762 0.1719 0.3460 0.1123 0.2416 0.1913 0.1788
MEANT (WMT12-A) 0.0652 0.1117 0.1663 0.1540 0.3764 0.1101 0.2314 0.1944 0.1762
MEANT (WMT12-B) 0.0458 0.1294 0.1556 0.1548 0.3992 0.1479 0.2571 0.2037 0.1867
MEANT (WMT12-C) 0.0746 0.1278 0.1833 0.1592 0.3764 0.1324 0.2674 0.1882 0.1887
MEANT (WMT12-D) 0.0628 0.1164 0.1826 0.1655 0.3802 0.1168 0.2339 0.1975 0.1820
MEANT (WMT12-E) 0.0496 0.1353 0.1791 0.1619 0.3840 0.1101 0.2596 0.1851 0.1831
UMEANT 0.0477 0.1333 0.1606 0.1548 0.3764 0.1257 0.2828 0.1913 0.1841
we submitted two metrics to WMT 2013 metrics
evaluation task. One is MEANT with weights
learned from tuning on WMT12-C development
sets and the other submission is UMEANT.
6 Conclusion
In this paper, we have evaluated in the context of
WMT2013 the MEANT and UMEANT metrics,
which are tunable, accurate yet inexpensive fully
automatic machine translation evaluation metrics
that measure similarity between theMT output and
the reference via semantic frames. Recent stud-
ies show that tuning MT system against MEANT
produces more robustly adequate translations than
the common practice of tuning against BLEU or
TER across different data genres, such as formal
newswire text, informal web forum text and infor-
mal public speech. The weight for each seman-
tic role label in MEANT is estimated by maximiz-
ing the correlation with human adequacy judgment
on a development set. UMEANT is a version of
MEANT in which weight for each semantic role
label is estimated in an unsupervised fashion us-
ing the relative frequency of the semantic role la-
bels in the reference. We present the experimen-
tal results for determining the set of weights that
maximize MEANT?s accuracy and stability by op-
timizing MEANT on different development sets.
We disagree with the notion ?a good evaluation
metric is not necessarily a good tuning metric, and
vice versa? (Chen et al, 2012). Instead, we be-
lieve that a good evaluation metric should be one
that is a good objective function to drive the devel-
opment of MT systems towards higher utility. In
other words, a good evaluation metric should cor-
relate well with human adequacy judgment and at
the same time, be inexpensive in running time so as
to fit into the MT pipeline to improve MT quality.
Our results shows that MEANT is a good evalu-
ation/tuning metric because it achieves a competi-
tive correlation scorewith human judgments by us-
ing less expensive linguistic resources and training
algorithms making it possible to tune MT system
against MEANT to improve MT quality.
7 Acknowledgment
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
426
ment no. 287658; and by the Hong Kong Re-
search Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
References
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65?
72, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in Ma-
chine Translation Research. In Proceedings of
the 13th Conference of the European Chapter
of the Association for Computational Linguis-
tics (EACL-06), pages 249?256, 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(Meta-) evaluation of Machine Translation. In
Proceedings of the 2nd Workshop on Statistical
Machine Translation, pages 136?158, 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further Meta-evaluation of Machine Transla-
tion. In Proceedings of the 3rd Workshop on
Statistical Machine Translation, pages 70?106,
2008.
Chris Callison-Burch, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Spe-
cia. Findings of the 2012 Workshop on Statisti-
cal Machine Translation. In Proceedings of the
7th Workshop on Statistical Machine Transla-
tion (WMT 2012), pages 10?51, 2012.
Julio Castillo and Paula Estrella. Semantic Textual
Similarity for MT evaluation. In Proceedings of
the 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), pages 52?58, 2012.
Boxing Chen, Roland Kuhn, and George Foster.
Improving AMBER, an MT Evaluation Metric.
In Proceedings of the 7th Workshop on Statis-
tical Machine Translation (WMT 2012), pages
59?63, 2012.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the
2nd International Conference on Human Lan-
guage Technology Research, pages 138?145,
San Diego, California, 2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic
features for automatic evaluation of heteroge-
nous MT systems. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 256?264, Prague, Czech Republic,
June 2007.
Jes?s Gim?nez and Llu?sM?rquez. A smorgasbord
of features for automaticMT evaluation. In Pro-
ceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198, Colum-
bus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
Automatic Evaluation of Machine Translation
between European Languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation (WMT-06), pages 102?121, 2006.
Patrik Lambert, Jes?s Gim?nez, Marta R Costa-
juss?, Enrique Amig?, Rafael E Banchs, Llu?s
M?rquez, and JAR Fonollosa. Machine Transla-
tion system development based on human like-
ness. In Spoken Language Technology Work-
shop, 2006. IEEE, pages 246?249. IEEE, 2006.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT Evaluation Using
Block Movements. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-
06), 2006.
Chi-kiu Lo and Dekai Wu. MEANT: An Inexpen-
sive, High-Accuracy, Semi-Automatic Metric
for Evaluating Translation Utility based on Se-
mantic Roles. In Proceedings of the Joint con-
ference of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics : Human
Language Technologies (ACL-HLT-11), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelli-
gence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs.
supervised weight estimation for semantic MT
evaluation metrics. In Proceedings of the 6th
Workshop on Syntax and Structure in Statistical
Translation (SSST-6), 2012.
427
Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic seman-
tic metrics? In Proceedings of the 14th Machine
Translation Summit (MTSummit-XIV), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully Automatic Semantic MT Evaluation.
In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation (WMT2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics (ACL-13), 2013.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In Pro-
ceedings of the national conference on artificial
intelligence, volume 21, page 775. Menlo Park,
CA; Cambridge, MA; London; AAAI Press;
MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International
Conference on Language Resources and Evalu-
ation (LREC-2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H.Martin, and Dan Jurafsky. Shallow Se-
mantic Parsing Using Support Vector Machines.
In Proceedings of the 2004 Conference on Hu-
man Language Technology and the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL-04), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine:
A metric to assess MT adequacy. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation (WMT-2011), pages 116?122,
2011.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In Proceedings of the 7th Conference
of the Association for Machine Translation in
the Americas (AMTA-06), pages 223?231, Cam-
bridge, Massachusetts, August 2006.
Xingyi Song and Trevor Cohn. Regression and
Ranking based Optimisation for Sentence Level
Machine Translation Evaluation. In Proceed-
ings of the 6th Workshop on Statistical Machine
Translation (WMT 2011), pages 123?129, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring
the lexical similarity of semantic role fillers
for automatic semantic mt evaluation. In Pro-
ceeding of the 26th Pacific Asia Conference
on Language, Information, and Computation
(PACLIC-26), 2012.
Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic Edit Distance Metrics for
MT Evaluation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation (WMT
2012), pages 76?83, 2012.
428
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 67?73,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Unsupervised Transduction Grammar Induction
via Minimum Description Length
Markus Saers and Karteek Addanki and Dekai Wu
Human Language Technology Center
Dept. of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|vskaddanki|dekai}@cs.ust.hk
Abstract
We present a minimalist, unsupervised
learning model that induces relatively
clean phrasal inversion transduction gram-
mars by employing the minimum descrip-
tion length principle to drive search over
a space defined by two opposing ex-
treme types of ITGs. In comparison to
most current SMT approaches, the model
learns a very parsimonious phrase trans-
lation lexicons that provide an obvious
basis for generalization to abstract trans-
lation schemas. To do this, the model
maintains internal consistency by avoid-
ing use of mismatched or unrelated mod-
els, such as word alignments or probabil-
ities from IBM models. The model in-
troduces a novel strategy for avoiding the
pitfalls of premature pruning in chunking
approaches, by incrementally splitting an
ITGwhile using a second ITG to guide this
search.
1 Introduction
We introduce an unsupervised approach to induc-
ing parsimonious, relatively clean phrasal inver-
sion transduction grammars or ITGs (Wu, 1997)
that employs a theoretically well-founded mini-
mum description length (MDL) objective to ex-
plicitly drive two opposing, extreme ITGs to-
wards one minimal ITG. This represents a new
attack on the problem suffered by most current
SMT approaches of learning phrase translations
that require enormous amounts of run-time mem-
ory, contain a high degree of redundancy, and fails
to provide an obvious basis for generalization to
abstract translation schemas. In particular, phrasal
SMT models such as Koehn et al (2003) and Chi-
ang (2005) often search for candidate translation
segments and transduction rules by committing
to a word alignment based on very different as-
sumptions (Brown et al, 1993; Vogel et al, 1996),
and heuristically derive lexical segment transla-
tions (Och and Ney, 2003). In fact, it is possible
to improve the performance by tossing away most
of the learned segmental translations (Johnson et
al., 2007). In addition to preventing such waste-
fulness, our work aims to also provide an obvi-
ous basis for generalization to abstract translation
schemas by driving the search for phrasal rules by
simultaneously using two opposing types of ITG
constraints that have both individually been empir-
ically proven to match phrase reordering patterns
across translations well.
We adopt a more ?pure? methodology for eval-
uating transduction grammar induction than typ-
ical system building papers. Instead of embed-
ding our learned ITG in the midst of many other
heuristic components for the sake of a short term
boost in BLEU, we focus on scientifically under-
standing the behavior of pure MDL-based search
for phrasal translations, divorced from the effect
of other variables, even though BLEU is naturally
much lower this way. The common practice of
plugging some aspect of a learned ITG into ei-
ther (a) a long pipeline of training heuristics and/or
(b) an existing decoder that has been patched up
to compensate for earlier modeling mistakes, as
we and others have done before?see for example
Cherry and Lin (2007); Zhang et al (2008); Blun-
som et al (2008, 2009); Haghighi et al (2009);
Saers and Wu (2009, 2011); Blunsom and Cohn
(2010); Burkett et al (2010); Riesa and Marcu
(2010); Saers et al (2010); Neubig et al (2011,
2012)?obscures the specific traits of the induced
grammar. Instead, we directly use our learned
ITG in translation mode (any transduction gram-
mar also represents a decoder when parsing with
the input sentence as a hard constraint) which al-
lows us to see exactly which aspects of correct
translation the transduction rules have captured.
67
When the structure of an ITG is induced without
supervision, it has so far been assumed that smaller
rules get clumped together into larger rules. This
is a natural way to search, since maximum like-
lihood (ML) tends to improve with longer rules,
which is typically balanced with Bayesian priors
(Zhang et al, 2008). Bayesian priors are also used
in Gibbs sampling (Blunsom et al, 2008, 2009;
Blunsom and Cohn, 2010), as well as other non-
parametric learning methods (Neubig et al, 2011,
2012). All of the above evaluate their models by
feeding them into mismatched decoders, making it
hard to evaluate how accurate the learned models
themselves were. In this work we take a radically
different approach, and start with the longest rules
possible and attempt to segment them into shorter
rules iteratively. This makes ML useless, since our
initial model maximizes it. Instead, we balance the
ML objective with a minimum description length
(MDL) objective, which let us escape the initial
ML optimum by rewarding model parsimony.
Transduction grammars can also be induced
with supervision from treebanks, which cuts down
the search space by enforcing external constraints
(Galley et al, 2006). This complicates the learn-
ing process by adding external constraints that are
bound to match the translation model poorly. It
does, however, constitute a way to borrow nonter-
minal categories that help the translation model.
MDL has been used before in monolingual
grammar induction (Gr?nwald, 1996; Stolcke and
Omohundro, 1994), as well as to interpret visual
scenes (Si et al, 2011). Our work is markedly dif-
ferent in that we (a) induce an ITG rather than a
monolingual grammar, and (b) focus on learning
the terminal segments rather than the nonterminal
categories. Iterative segmentation has also been
used before, but only to derive a word alignment
as part of a larger pipeline (Vilar and Vidal, 2005).
The paper is structured as follows: we start by
describing theMDL principle (Section 2). We then
describe the initial ITGs (Section 3), followed by
the algorithm that induces an MDL-optimal ITG
from them (Section 4). After that we describe the
experiments (Section 5), and the results (Section
6). Finally, we offer some conclusions (Section 7).
2 Minimum description length
The minimum description length principle is about
finding the optimal balance between the size of a
model and the size of some data given the model
(Solomonoff, 1959; Rissanen, 1983). Consider the
information theoretical problem of encoding some
data with a model, and then sending both the en-
coded data and the information needed to decode
the data (the model) over a channel; the minimum
description length is the minimum number of bits
sent over the channel. The encoded data can be in-
terpreted as carrying the information necessary to
disambiguate the uncertainties that the model has
about the data. The model can grow in size and be-
comemore certain about the data, and it can shrink
in size and become more uncertain about the data.
Formally, description length (DL) is:
DL (?, D) = DL (D|?) + DL (?)
where ? is the model and D is the data.
In practice, we rarely have complete data to train
on, so we need our models to generalize to unseen
data. Amodel that is very certain about the training
data runs the risk of not being able to generalize to
new data: it is over-fitting. It is bad enough when
estimating the parameters of a transduction gram-
mar, and catastrophic when inducing the structure.
The information-theoretic view of the problem
gives a hint at the operationalization of descrip-
tion length of a corpus given a grammar. Shannon
(1948) stipulates that we can get a lower bound on
the number of bits required to encode a specific
outcome of a random variable. We thus define de-
scription length of the corpus given the grammar
to be: DL (D|?) = ?lgP (D|?)
Information theory is also useful for the descrip-
tion length of the grammar: if we can find a way
to serialize the grammar into a sequence of tokens,
we can figure out how that sequence can be opti-
mally encoded. To serialize an ITG, we first need
to determine the alphabet that the message will be
written in. We need one symbol for every nonter-
minal, L0- and L1-terminal. We will also make
the assumption that all these symbols are used in
at least one rule, so that it is sufficient to serial-
ize the rules in order to express the entire ITG.
We serialize a rule with a type marker, followed
by the left-hand side nonterminal, followed by all
the right-hand side symbols. The type marker is
either [] denoting the start of a straight rule, or
?? denoting the start of an inverted rule. Unary
rules are considered to be straight. We serialize
the ITG by concatenating the serialized form of all
the rules, assuming that each symbol can be serial-
ized into?lgc bits where c is the symbol?s relative
frequency in the serialized form of the ITG.
68
3 Initial ITGs
To tackle the exponential problem of searching for
an ITG that minimizes description length, it is use-
ful to contrast two extreme forms of ITGs. De-
scription length has two components, model length
and data length. We call an ITG that minimizes
the data at the expense of the model a long ITG;
we call an ITG that minimizes the model at the ex-
pense of the data a short ITG.1 The long ITG sim-
ply has all the sentence pairs as biterminals:
S ? A
A ? e0..T0/f0..V0
A ? e0..T1/f0..V1
...
A ? e0..TN /f0..VN
where S is the start symbol, A is the nonterminal,
N is the number of sentence pairs, Ti is the length
of the ith output sentence (making e0..Ti the ith out-
put sentence), and Vi is the length of the ith input
sentence (making f0..Vi the ith input sentence). The
short ITG is a token-based bracketing ITG:
S ? A, A? [AA] , A? ?AA?,
A? e/f, A? e/?, A? ?/f
where, S is the start symbol, A is the nonterminal
symbol, e is an L0-token, f is an L1-token, and ?
is the empty sequence of tokens.
4 Shortening the long ITG
To shorten the long ITG,wewill identify good split
candidates in the terminal rules by parsing them
with the short ITG, and commit to split candidates
that give a net gain. A split candidate is an exist-
ing long terminal rule, information about where to
split its right-hand side, and whether to invert the
resulting two rules or not. Consider the terminal
rule A ? es..t/fu..v; it can be split at any point S
in L0 and any point U in L1, giving the three rules
A ? [AA], A ? es..S/fu..U and A ? eS..t/fU..v
when it is split in straight order, and the three rules
A? ?AA?, A? es..S/fU..v and A? eS..t/fu..U
when it is split in inverted order. We will refer to
the original long rule as r0, and the resulting three
rules as r1, r2 and r3.
To identify the split candidates and to figure out
how the probability mass of r0 is to be distributed
1Long and short ITGs correspond well to ad-hoc and
promiscuous grammars in Gr?nwald (1996).
Algorithm 1 Rule shortening.
Gl ? The long ITG
Gs ? The short ITG
repeat
cands? collect_candidates(Gl, Gs)
? ? 0
removed? {}
repeat
score(cands)
sort_by_delta(cands)
for all c ? cands do
r ? original_rule(c)
if r /? removed and ?c ? 0 then
Gl ? update_grammar(Gl, c)
removed? {r} ? removed
? ? ? + ?c
end if
end for
until ? ? 0
until ? ? 0
return Gl
to the new rules, we use the short ITG to biparse the
right-hand side of r0. The distribution is derived
from the inside probability of the bispans that the
new rules are covering in the chart, and we refer
to them as ?1, ?2 and ?3, where the index indi-
cates which new rule they apply to. This has the
effect of preferring to split a rule into parts that are
roughly equally probable, as the size of the data is
minimized when the weights are equal.
To choose which split candidates to commit to,
we need a way to estimate their impact on the to-
tal MDL score of the model. This breaks down
into two parts: the difference in description length
of the grammar: DL (??) ? DL (?) (where ?? is
? after committing to the split candidate), and the
difference in description length of the corpus given
the grammar: DL (D|??) ? DL (D|?). The two
are added up to get the total change in description
length.The difference in grammar length is calcu-
lated as described in Section 2. The difference in
description length of the corpus given the grammar
can be calculated by biparsing the corpus, since
DL (D|??) = ?lgP (D|p?) and DL (D|?) =
?lgP (D|p) where p? and p are the rule probabil-
ity functions of ?? and ? respectively. Bipars-
ing is, however, a very costly process that we do
not want to carry out for every candidate. Instead,
we assume that we have the original corpus proba-
bility (through biparsing when generating the can-
69
Table 1: The results of decoding. NIST and BLEU are the translation scores at each iteration, followed
by the number of rules in the grammar, followed by the average (as measured by mean and mode) number
of English tokens in the rules.
Iteration NIST BLEU Rules Mean Mode
1 2.7015 11.97 43,704 7.20 6
2 4.0116 14.04 42,823 6.30 6
3 4.1654 16.58 41,867 5.68 2
4 4.3723 17.43 40,953 5.23 1
5 4.2032 18.78 40,217 4.97 1
6 4.1329 17.28 39,799 4.84 1
7 4.0710 17.31 39,587 4.79 1
8 4.0437 17.10 39,470 4.75 1
didates), and estimate the new corpus probability
from it (in closed form). The new rule probability
function p? is identical to p, except that:
p? (r0) = 0
p? (r1) = p (r1) + ?1p (r0)
p? (r2) = p (r2) + ?2p (r0)
p? (r3) = p (r3) + ?3p (r0)
We assume the probability of the corpus given this
new rule probability function to be:
P
(
D|p?
)
= P (D|p) p
? (r1) p? (r2) p? (r3)
p (r0)
This gives the following description length differ-
ence:
DL (D|??)? DL (D|?) =
?lgp
?(r1)p?(r2)p?(r3)
p(r0)
We will commit to all split candidates that are es-
timated to lower the DL, restricting it so that any
original rule is split only in the best way (Algo-
rithm 1).
5 Experimental setup
To test whether minimum description length is a
good driver for unsupervised inversion transduc-
tion induction, we implemented and executed the
method described above. We start by initializing
one long and one short ITG. The parameters of the
long ITG cannot be adjusted to fit the data better,
but the parameters of the short ITG can be tuned to
the right-hand sides of the long ITG.We do so with
an implementation of the cubic time algorithm de-
scribed in Saers et al (2009), with a beam width
of 100. We then run the introduced algorithm.
As training data, we use the IWSLT07 Chinese?
English data set (Fordyce, 2007), which contains
46,867 sentence pairs of training data, and 489
Chinese sentences with 6 English reference trans-
lations each as test data; all the sentences are taken
from the traveling domain. Since the Chinese is
written without whitespace, we use a tool that tries
to clump characters together into more ?word like?
sequences (Wu, 1999).
After each iteration, we use the long ITG to
translate the held out test set with our in-house ITG
decoder. The decoder uses a CKY-style parsing
algorithm (Cocke, 1969; Kasami, 1965; Younger,
1967) and cube pruning (Chiang, 2007) to inte-
grate the language model scores. The decoder
builds an efficient hypergraph structure which is
scored using both the induced grammar and a lan-
guage model. We use SRILM (Stolcke, 2002) for
training a trigram language model on the English
side of the training corpus. To evaluate the re-
sulting translations, we use BLEU (Papineni et al,
2002) and NIST (Doddington, 2002).
We also perform a combination experiment,
where the grammar at different stages of the learn-
ing process (iterations) are interpolated with each
other. This is a straight-forward linear interpola-
tion, where the probabilities of the rules are added
up and the grammar is renormalized. Although
it makes little sense from an MDL point of view
to increase the size of the grammar so indiscrim-
inately, it does make sense from an engineering
point of view, since more rules typically means
better coverage, which in turn typically means bet-
ter translations of unknown data.
6 Results
As discussed at the outset, rather than burying our
learned ITG in many layers of unrelated heuristics
just to push up the BLEU score, we think it is more
70
Table 2: The results of decoding with combined grammars. NIST and BLEU are the translation scores for
each combination, followed by the number of rules in the grammar, followed by the average (as measured
by mean and mode) number of English tokens in the rules.
Combination NIST BLEU Rules Mean Mode
1?2 (2 grammars) 4.2426 15.28 74,969 6.69 6
3?4 (2 grammars) 4.5087 18.75 54,533 5.41 3
5?6 (2 grammars) 4.1897 18.19 44,264 4.86 1
7?8 (2 grammars) 4.0953 17.40 40,785 4.79 1
1?4 (4 grammars) 4.9234 19.98 109,183 6.19 5
5?8 (4 grammars) 4.1089 17.86 47,504 4.84 1
1?8 (8 grammars) 4.8649 20.41 124,423 5.92 3
important to illuminate scientific understanding of
the behavior of pure MDL-driven rule induction
without interference from other variables. Directly
evaluating solely the ITG in translation mode?
instead of (a) deriving word alignments from it by
committing to only the one-best parse, but then dis-
carding any trace of structure and/or (b) evaluating
it through a decoder that has been patched up to
compensate for deficiencies in disparate aspects of
translation?allows us to see exactly how accurate
the learned transduction rules are.
The results from the individual iterations (Table
1) show that we learn very parsimonious models
that far outperforms the only other result we are
aware of where an ITG is tested exactly as it was
learned without altering the model itself: Saers et
al. (2012) induce a pure ITG by iteratively chunk-
ing rules, but they report significantly lower trans-
lation quality (8.30 BLEU and 0.8554 NIST) de-
spite a significantly larger ITG (251,947 rules).
The average rule length also decreases as smaller
reusable spans are found. The English side of the
training data has amean of 8.45 and amode of 7 to-
kens per sentence, and these averages drop steadily
during training. It is very encouraging to see the
mode drop to one so quickly, as this indicates that
the learning algorithm finds translations of individ-
ual English words. Not only are the rules getting
fewer, but they are also getting shorter.
The results from the combination experiments
(Table 2) corroborate the engineering intuition that
more rules give better translations at the expense of
a larger model. Using all eight grammars gives a
BLEU score of 20.41, at the expense of approxi-
mately tripling the size of the grammar. All indi-
vidual iterations benefit from being combined with
other iterations?but for the very best iterations
more additional data is needed to get this improve-
ment; the fifth iteration, which excelled at BLEU
score needs to be combinedwith all other iterations
to see an improvement, whereas the first and sec-
ond iterations only need each other to see an im-
provement.
7 Conclusions
We have presented a minimalist, unsupervised
learning model that induces relatively clean
phrasal ITGs by iteratively splitting existing rules
into smaller rules using a theoretically well-
founded minimum description length objective.
The resulting translation model is very parsimo-
nious and provide an obvious foundation for gen-
eralization tomore abstract transduction grammars
with informative nonterminals.
8 Acknowledgements
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by
the European Union under the FP7 grant agree-
ment no. 287658; and by the Hong Kong
Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any
opinions, findings and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA, the EU, or RGC.
References
Phil Blunsom and Trevor Cohn. Inducing syn-
chronous grammars with slice sampling. In
HLT/NAACL2010, pages 238?241, Los Ange-
les, California, June 2010.
71
Phil Blunsom, Trevor Cohn, and Miles Osborne.
Bayesian synchronous grammar induction. In
Proceedings of NIPS 21, Vancouver, Canada,
December 2008.
Phil Blunsom, Trevor Cohn, Chris Dyer, andMiles
Osborne. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of
the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, pages 782?790, Suntec, Singapore,
August 2009.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The Mathe-
matics of Machine Translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, 1993.
David Burkett, John Blitzer, and Dan Klein. Joint
parsing and alignment with weakly synchro-
nized grammars. In HLT/NAACL?10, pages
127?135, Los Angeles, California, June 2010.
Colin Cherry and Dekang Lin. Inversion transduc-
tion grammar for joint phrasal translation mod-
eling. In Proceedings of SSST?07, pages 17?24,
Rochester, New York, April 2007.
David Chiang. A hierarchical phrase-based model
for statistical machine translation. In Proceed-
ings of ACL?05, pages 263?270, Ann Arbor,
Michigan, June 2005.
David Chiang. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?
228, 2007.
John Cocke. Programming languages and their
compilers: Preliminary notes. Courant Institute
of Mathematical Sciences, New York Univer-
sity, 1969.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. InProceedings of HLT?02,
pages 138?145, San Diego, California, 2002.
C. S. Fordyce. Overview of the IWSLT 2007 eval-
uation campaign. In Proceedings IWSLT?07,
pages 1?12, 2007.
Michel Galley, Jonathan Graehl, Kevin Knight,
Daniel Marcu, Steve DeNeefe, Wei Wang, and
Ignacio Thayer. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings COLING/ACL?06, pages 961?968,
Sydney, Australia, July 2006.
Peter Gr?nwald. A minimum description
length approach to grammar inference in sym-
bolic. Lecture Notes in Artificial Intelligence,
(1040):203?216, 1996.
Aria Haghighi, John Blitzer, John DeNero, and
Dan Klein. Better word alignments with
supervised itg models. In Proceedings of
ACL/IJCNLP?09, pages 923?931, Suntec, Sin-
gapore, August 2009.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. Improving translation quality
by discarding most of the phrasetable. In Pro-
ceedings EMNLP/CoNLL?07, pages 967?975,
Prague, Czech Republic, June 2007.
TadaoKasami. An efficient recognition and syntax
analysis algorithm for context-free languages.
Technical Report AFCRL-65-00143, Air Force
Cambridge Research Laboratory, 1965.
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. Statistical Phrase-Based Translation.
In Proceedings of HLT/NAACL?03, volume 1,
pages 48?54, Edmonton, Canada, May/June
2003.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. An
unsupervised model for joint phrase alignment
and extraction. In Proceedings of ACL/HLT?11,
pages 632?641, Portland, Oregon, June 2011.
Graham Neubig, Taro Watanabe, Shinsuke Mori,
and Tatsuya Kawahara. Machine translation
without words through substring alignment. In
Proceedings of ACL?12, pages 165?174, Jeju Is-
land, Korea, July 2012.
Franz Josef Och and Hermann Ney. A Systematic
Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?
51, 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In Proceed-
ings of ACL?02, pages 311?318, Philadelphia,
Pennsylvania, July 2002.
Jason Riesa andDanielMarcu. Hierarchical search
for word alignment. In Proceedings of ACL?10,
pages 157?166, Uppsala, Sweden, July 2010.
Jorma Rissanen. A universal prior for integers and
estimation by minimum description length. The
Annals of Statistics, 11(2):416?431, June 1983.
72
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
Stochastic Inversion Transduction Grammars.
In Proceedings of SSST?09, pages 28?36, Boul-
der, Colorado, June 2009.
Markus Saers and Dekai Wu. Principled induction
of phrasal bilexica. In Proceedings of EAMT?11,
pages 313?320, Leuven, Belgium, May 2011.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In Proceedings of IWPT?09, pages
29?32, Paris, France, October 2009.
Markus Saers, JoakimNivre, and DekaiWu. Word
alignment with stochastic bracketing linear in-
version transduction grammar. In Proceedings
of HLT/NAACL?10, pages 341?344, Los Ange-
les, California, June 2010.
Markus Saers, Karteek Addanki, and Dekai Wu.
From finite-state to inversion transductions: To-
ward unsupervised bilingual grammar induc-
tion. In Proceedings of COLING 2012: Techni-
cal Papers, pages 2325?2340, Mumbai, India,
December 2012.
Claude Elwood Shannon. A mathematical theory
of communication. The Bell System Technical
Journal, 27:379?423, 623?656, July, October
1948.
Zhangzhang Si, Mingtao Pei, Benjamin Yao,
and Song-Chun Zhu. Unsupervised learning
of event and-or grammar and semantics from
video. In Proceedings of the 2011 IEEE ICCV,
pages 41?48, November 2011.
Ray J. Solomonoff. A new method for discovering
the grammars of phrase structure languages. In
IFIP Congress, pages 285?289, 1959.
Andreas Stolcke and Stephen Omohundro. Induc-
ing probabilistic grammars by bayesian model
merging. In R. C. Carrasco and J. Oncina, ed-
itors, Grammatical Inference and Applications,
pages 106?118. Springer, 1994.
Andreas Stolcke. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the
International Conference on Spoken Language
Processing, pages 901?904, Denver, Colorado,
September 2002.
Juan Miguel Vilar and Enrique Vidal. A recur-
sive statistical translation model. In ACL-2005
Workshop on Building and Using Parallel Texts,
pages 199?207, Ann Arbor, Jun 2005.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. HMM-basedWord Alignment in Statisti-
cal Translation. In Proceedings of COLING-96,
volume 2, pages 836?841, 1996.
Dekai Wu. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Par-
allel Corpora. Computational Linguistics,
23(3):377?403, 1997.
Zhibiao Wu. LDC Chinese segmenter, 1999.
Daniel H. Younger. Recognition and parsing of
context-free languages in time n3. Information
and Control, 10(2):189?208, 1967.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In Proceedings of ACL/HLT?08, pages 97?
105, Columbus, Ohio, June 2008.
73
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22?33,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Better Semantic Frame Based MT Evaluation
via Inversion Transduction Grammars
Dekai Wu Lo Chi-kiu Meriem Beloucif Markus Saers
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
Abstract
We introduce an inversion transduc-
tion grammar based restructuring of
the MEANT automatic semantic frame
based MT evaluation metric, which,
by leveraging ITG language biases, is
able to further improve upon MEANT?s
already-high correlation with human
adequacy judgments. The new metric,
called IMEANT, uses bracketing ITGs to
biparse the reference and machine transla-
tions, but subject to obeying the semantic
frames in both. Resulting improvements
support the presumption that ITGs, which
constrain the allowable permutations
between compositional segments across
the reference and MT output, score the
phrasal similarity of the semantic role
fillers more accurately than the simple
word alignment heuristics (bag-of-word
alignment or maximum alignment) used
in previous version of MEANT. The
approach successfully integrates (1) the
previously demonstrated extremely high
coverage of cross-lingual semantic frame
alternations by ITGs, with (2) the high
accuracy of evaluating MT via weighted
f-scores on the degree of semantic frame
preservation.
1 Introduction
There has been to date relatively little use of in-
version transduction grammars (Wu, 1997) to im-
prove the accuracy of MT evaluation metrics, de-
spite long empirical evidence the vast majority of
translation patterns between human languages can
be accommodated within ITG constraints (and the
observation that most current state-of-the-art SMT
systems employ ITG decoders). We show that
ITGs can be used to redesign the MEANT seman-
tic frame based MT evaluation metric (Lo et al.,
2012) to produce improvements in accuracy and
reliability. This work is driven by the motiva-
tion that especially when considering semanticMT
metrics, ITGs would be seem to be a natural basis
for several reasons.
To begin with, it is quite natural to think of
sentences as having been generated from an ab-
stract concept using a rewriting system: a stochas-
tic grammar predicts how frequently any particu-
lar realization of the abstract concept will be gen-
erated. The bilingual analogy is a transduction
grammar generating a pair of possible realizations
of the same underlying concept. Stochastic trans-
duction grammars predict how frequently a partic-
ular pair of realizations will be generated, and thus
represent a good way to evaluate how well a pair
of sentences correspond to each other.
The particular class of transduction gram-
mars known as ITGs tackle the problem that
the (bi)parsing complexity for general syntax-
directed transductions (Aho and Ullman, 1972)
is exponential. By constraining a syntax-directed
transduction grammar to allow only monotonic
straight and inverted reorderings, or equivalently
permitting only binary or ternary rank rules, it is
possible to isolate the low end of that hierarchy into
a single equivalence class of inversion transduc-
tions. ITGs are guaranteed to have a two-normal
form similar to context-free grammars, and can
be biparsed in polynomial time and space (O
(
n
6
)
time andO
(
n
4
)
space). It is also possible to do ap-
proximate biparsing in O
(
n
3
)
time (Saers et al.,
2009). These polynomial complexities makes it
feasible to estimate the parameters of an ITG us-
ing standard machine learning techniques such as
expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have
also been directly shown to be more than sufficient
to account for the reordering that occur within se-
mantic frame alternations (Addanki et al., 2012).
This makes ITGs an appealing alternative for eval-
22
uating the possible links between both semantic
role fillers in different languages as well as the
predicates, and how these parts fit together to form
entire semantic frames. We believe that ITGs are
not only capable of generating the desired struc-
tural correspondences between the semantic struc-
tures of two languages, but also provide meaning-
ful constraints to prevent alignments from wander-
ing off in the wrong direction.
In this paper we show that IMEANT, a newmet-
ric drawing from the strengths of both MEANT
and inversion transduction grammars, is able to
exploit bracketing ITGs (also known as BITGs
or BTGs) which are ITGs containing only a sin-
gle non-differentiated non terminal category (Wu,
1995a), so as to produce even higher correlation
with human adequacy judgments than any auto-
matic MEANT variants, or other common auto-
matic metrics. We argue that the constraints pro-
vided by BITGs over the semantic frames and ar-
guments of the reference and MT output sentences
are essential for accurate evaluation of the phrasal
similarity of the semantic role fillers.
In common with the various MEANT semantic
MT evaluation metrics (Lo and Wu, 2011a, 2012;
Lo et al., 2012; Lo and Wu, 2013b), our proposed
IMEANT metric measures the degree to which
the basic semantic event structure is preserved
by translation?the ?who did what to whom, for
whom, when, where, how and why? (Pradhan et
al., 2004)?emphasizing that a good translation
is one that can successfully be understood by a
human. In the other versions of MEANT, sim-
ilarity between the MT output and the reference
translations is computed as a modified weighted f-
score over the semantic predicates and role fillers.
Across a variety of language pairs and genres, it
has been shown thatMEANT correlates better with
human adequacy judgment than both n-gram based
MT evaluation metrics such as BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Banerjee and Lavie, 2005), as well as edit-
distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al., 2000), and TER
(Snover et al., 2006) when evaluating MT output
(Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and
Wu, 2013b; Mach??ek and Bojar, 2013). Further-
more, tuning the parameters of MT systems with
MEANT instead of BLEU or TER robustly im-
proves translation adequacy across different gen-
res and different languages (English and Chinese)
(Lo et al., 2013a; Lo and Wu, 2013a; Lo et al.,
2013b). This has motivated our choice of MEANT
as the basis on which to experiment with deploying
ITGs into semantic MT evaluation.
2 Related Work
2.1 ITGs and MT evaluation
Relatively little investigation into the potential
benefits of ITGs is found in previous MT eval-
uation work. One exception is invWER, pro-
posed by Leusch et al. (2003) and Leusch and Ney
(2008). The invWER metric interprets weighted
BITGs as a generalization of the Levenshtein edit
distance, in which entire segments (blocks) can be
inverted, as long as this is done strictly compo-
sitionally so as not to violate legal ITG biparse
tree structures. The input and output languages
are considered to be those of the reference and ma-
chine translations, and thus are over the same vo-
cabulary (say,English). At the sentence level, cor-
relation of invWER with human adequacy judg-
ments was found to be among the best.
Our current approach differs in several key
respects from invWER. First,invWER operates
purely at the surface level of exact token match,
IMEANT mediates between segments of refer-
ence translation andMT output using lexical BITG
probabilities.
Secondly, there is no explicit semantic model-
ing in invWER. Providing they meet the BITG
constraints, the biparse trees in invWER are com-
pletely unconstrained. In contrast, IMEANT em-
ploys the same explicit, strong semantic frame
modeling as MEANT, on both the reference and
machine translations. In IMEANT, the semantic
frames always take precedence over pure BITG
biases. Compared to invWER, this strongly con-
strains the space of biparses that IMEANT permits
to be considered.
2.2 MT evaluation metrics
Like invWER, other common surface-form ori-
ented metrics like BLEU (Papineni et al., 2002),
NIST (Doddington, 2002), METEOR (Banerjee
and Lavie, 2005; Denkowski and Lavie, 2014),
CDER (Leusch et al., 2006), WER (Nie?en et
al., 2000), and TER (Snover et al., 2006) do
not correctly reflect the meaning similarities of
the input sentence. There are in fact several
large scale meta-evaluations (Callison-Burch et
al., 2006; Koehn and Monz, 2006) reporting cases
23
where BLEU strongly disagrees with human judg-
ments of translation adequacy.
Such observations have generated a recent surge
of work on developing MT evaluation metrics that
would outperform BLEU in correlation with hu-
man adequacy judgment (HAJ). Like MEANT, the
TINE automatic recall-oriented evaluation metric
(Rios et al., 2011) aims to preserve basic event
structure. However, its correlation with human ad-
equacy judgment is comparable to that of BLEU
and not as high as that of METEOR. Owczarzak
et al. (2007a,b) improved correlation with human
fluency judgments by using LFG to extend the ap-
proach of evaluating syntactic dependency struc-
ture similarity proposed by Liu and Gildea (2005),
but did not achieve higher correlation with hu-
man adequacy judgments than metrics like ME-
TEOR. Another automatic metric, ULC (Gim?nez
and M?rquez, 2007, 2008), incorporates several
semantic similarity features and shows improved
correlation with human judgement of translation
quality (Callison-Burch et al., 2007; Gim?nez
and M?rquez, 2007; Callison-Burch et al., 2008;
Gim?nez and M?rquez, 2008) but no work has
been done towards tuning an SMT system using
a pure form of ULC perhaps due to its expensive
run time. Likewise, SPEDE (Wang and Manning,
2012) predicts the edit sequence needed to match
the machine translation to the reference translation
via an integrated probabilistic FSM and probabilis-
tic PDA model. The semantic textual similarity
metric Sagan (Castillo and Estrella, 2012) is based
on a complex textual entailment pipeline. These
aggregated metrics require sophisticated feature
extraction steps, contain many parameters that
need to be tuned, and employ expensive linguis-
tic resources such asWordNet or paraphrase tables.
The expensive training, tuning and/or running time
renders these metrics difficult to use in the SMT
training cycle.
3 IMEANT
In this section we give a contrastive description
of IMEANT: we first summarize the MEANT ap-
proach, and then explain how IMEANT differs.
3.1 Variants of MEANT
MEANT and its variants (Lo et al., 2012) measure
weighted f-scores over corresponding semantic
frames and role fillers in the reference andmachine
translations. The automatic versions of MEANT
replace humans with automatic SRL and align-
ment algorithms. MEANT typically outperforms
BLEU, NIST, METEOR, WER, CDER and TER
in correlation with human adequacy judgment, and
is relatively easy to port to other languages, re-
quiring only an automatic semantic parser and a
monolingual corpus of the output language, which
is used to gauge lexical similarity between the se-
mantic role fillers of the reference and translation.
MEANT is computed as follows:
1. Apply an automatic shallow semantic parser
to both the reference and machine transla-
tions. (Figure 1 shows examples of auto-
matic shallow semantic parses on both refer-
ence and MT.)
2. Apply the maximum weighted bipartite
matching algorithm to align the semantic
frames between the reference and machine
translations according to the lexical similari-
ties of the predicates. (Lo and Wu (2013a)
proposed a backoff algorithm that evaluates
the entire sentence of theMT output using the
lexical similarity based on the context vector
model, if the automatic shallow semantic
parser fails to parse the reference or machine
translations.)
3. For each pair of the aligned frames, apply the
maximum weighted bipartite matching algo-
rithm to align the arguments between the ref-
erence andMT output according to the lexical
similarity of role fillers.
4. Compute the weighted f-score over the
matching role labels of these aligned predi-
cates and role fillers according to the follow-
ing definitions:
q
0
i,j
? ARG j of aligned frame i in MT
q
1
i,j
? ARG j of aligned frame i in REF
w
0
i
?
#tokens filled in aligned frame i of MT
total #tokens in MT
w
1
i
?
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ? weight of similarity of predicates
w
j
? weight of similarity of ARG j
e
i,pred ? the pred string of the aligned frame i of MT
f
i,pred ? the pred string of the aligned frame i of REF
e
i,j
? the role fillers of ARG j of the aligned frame i of MT
f
i,j
? the role fillers of ARG j of the aligned frame i of REF
s(e, f) = lexical similarity of token e and f
24
[IN] ?? ? ? ?? ?? ?? ? ? ? ? ? ? ????? ?? ?? ?? ?? ?  
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed . 
ARGM-TMP PRED ARGM-LOC PRED ARG1 
ARGM-LOC PRED ARG1 PRED ARG1 
ARG0 ARGM-TMP 
[MT1] So far , nearly two months sk - ii the sale of products in the mainland of China to resume sales .  
PRED ARG0 ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 
[MT3] So far , the sale in the mainland of China for nearly two months of SK - II line of products .  
PRED 
PRED ARG0 
ARG1 ARGM-TMP 
ARGM-ADV 
ARG0 
ARGM-EXT 
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames forMT3 since there is no predicate
in the MT output.
prece,f =
?
e?e max
f?f
s(e, f)
| e |
rece,f =
?
f?f max
e?e
s(e, f)
| f |
s
i,pred =
2 ? prece
i,pred,fi,pred ? recei,pred,fi,pred
prece
i,pred,fi,pred + recei,pred,fi,pred
s
i,j
=
2 ? prece
i,j
,f
i,j
? rece
i,j
,f
i,j
prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
precision =
?
i
w
0
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
0
i,j
|
?
i
w
0
i
recall =
?
i
w
1
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
1
i,j
|
?
i
w
1
i
MEANT = 2 ? precision ? recallprecision + recall
where q0
i,j
and q1
i,j
are the argument of type j in
frame i inMT andREF respectively.w0
i
andw1
i
are
the weights for frame i in MT/REF respectively.
These weights estimate the degree of contribution
of each frame to the overall meaning of the sen-
tence. wpred and wj are the weights of the lexical
similarities of the predicates and role fillers of the
arguments of type j of all frame between the ref-
erence translations and the MT output.There is a
total of 12 weights for the set of semantic role la-
bels in MEANT as defined in Lo and Wu (2011b).
For MEANT, they are determined using super-
vised estimation via a simple grid search to opti-
mize the correlation with human adequacy judg-
ments (Lo andWu, 2011a). For UMEANT (Lo and
Wu, 2012), they are estimated in an unsupervised
manner using relative frequency of each semantic
role label in the references and thus UMEANT is
useful when human judgments on adequacy of the
development set are unavailable.
s
i,pred and si,j are the lexical similarities based
on a context vectormodel of the predicates and role
fillers of the arguments of type j between the ref-
erence translations and the MT output. Lo et al.
(2012) and Tumuluru et al. (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al., 2013a; Lo and
Wu, 2013a; Lo et al., 2013b). In this paper, we
will assess IMEANT against the latest version of
MEANT (Lo et al., 2014) which, as shown, uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of semantic
role fillers,since this has been shown to bemore ac-
curate than the previously used aggregation func-
tions.
Recent studies (Lo et al., 2013a; Lo and Wu,
2013a; Lo et al., 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
25
In an alternative quality-estimation oriented line
of research, Lo et al. (2014) describe a cross-
lingual variant called XMEANT capable of eval-
uating translation quality without the need for ex-
pensive human reference translations, by utiliz-
ing semantic parses of the original foreign in-
put sentence instead of a reference translation.
Since XMEANT?s results could have been due
to either (1) more accurate evaluation of phrasal
similarity via cross-lingual translation probabili-
ties, or (2) better match of semantic frames with-
out reference translations, there is no direct evi-
dencewhether ITGs contribute to the improvement
in MEANT?s correlation with human adequacy
judgment. For the sake of better understanding
whether ITGs improve semantic MT evaluation,
we will also assess IMEANT against cross-lingual
XMEANT.
3.2 The IMEANT metric
Although MEANT was previously shown to pro-
duce higher correlation with human adequacy
judgments compared to other automatic metrics,
our error analyses suggest that it still suffers from a
common weakness among metrics employing lex-
ical similarity, namely that word/token alignments
between the reference and machine translations
are severely under constrained. No bijectivity or
permutation restrictions are applied, even between
compositional segments where this should be nat-
ural. This can cause role fillers to be aligned even
when they should not be. IMEANT, in contrast,
uses a bracketing inversion transduction grammar
to constrain permissible token alignment patterns
between aligned role filler phrases. The semantic
frames above the token level also fits ITG com-
positional structure, consistent with the aforemen-
tioned semantic frame alternation coverage study
of Addanki et al. (2012). Figure 2 illustrates how
the ITG constraints are consistent with the needed
permutations between semantic role fillers across
the reference and machine translations for a sam-
ple sentence from our evaluation data, which as
we will see leads to higher HAJ correlations than
MEANT.
Subject to the structural ITG constraints,
IMEANT scores sentence translations in a spirit
similar to the way MEANT scores them: it utilizes
an aggregated score over the matched semantic
role labels of the automatically aligned semantic
frames and their role fillers between the reference
and machine translations. Despite the structural
differences, like MEANT, at the conceptual level
IMEANT still aims to evaluate MT output in
terms of the degree to which the translation has
preserved the essential ?who did what to whom,for
whom, when, where, how and why? of the foreign
input sentence.
Unlike MEANT, however, IMEANT aligns and
scores under ITG assumptions. MEANT uses a
maximum alignment algorithm to align the tokens
in the role fillers between the reference and ma-
chine translations, and then scores by aggregating
the lexical similarities into a phrasal similarity us-
ing an f-measure. In contrast, IMEANT aligns and
scores by utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009; Addanki et al., 2012). To be precise in
this regard, we can see IMEANT as differing from
the foregoing description of MEANT in the defi-
nition of s
i,pred and si,j , as follows.
G ? ?{A} ,W0,W1,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 1
p (e/f |A) = s(e, f)
s
i,pred = lg?1
?
?
lg
(
P
(
A ?? e
i,pred/fi,pred|G
))
max(| e
i,pred |, | fi,pred |)
?
?
s
i,j
= lg?1
?
?
lg
(
P
(
A ?? e
i,j
/f
i,j
|G
))
max(| e
i,j
|, | f
i,j
|)
?
?
where G is a bracketing ITG whose only non ter-
minal is A, andR is a set of transduction rules with
e ? W
0
?{?} denoting a token in theMToutput (or
the null token) and f ? W1?{?} denoting a token
in the reference translation (or the null token). The
rule probability (or more accurately, rule weight)
function p is set to be 1 for structural transduction
rules, and for lexical transduction rules it is de-
fined using MEANT?s context vector model based
lexical similarity measure. To calculate the inside
probability (or more accurately, inside score) of a
pair of segments, P
(
A ?? e/f|G
)
, we use the al-
gorithm described in Saers et al. (2009). Given
this, s
i,pred and si,j now represent the length nor-
malized BITG parse scores of the predicates and
role fillers of the arguments of type j between the
reference and machine translations.
4 Experiments
In this section we discuss experiments indicating
that IMEANT further improves upon MEANT?s
26
[REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work .  
[MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency .  
ARG0 ARG1 PRED 
ARG0 PRED ARG1 
The 
level 
of 
reduction 
is 
conducive 
to 
raising 
the 
inspection 
and 
supervision 
work 
efficiency 
. 
Th
e 
red
uct
ion
 in 
hie
rar
chy
 
hel
ps rais
e the
 
effi
cie
ncy
 of 
ins
pec
tion
 
and
 
sup
erv
iso
ry . 
wo
rk 
pred 
ARG0 
ARG1 
pr
ed
 
AR
G0
 
AR
G1
 
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence ????????????
????????. Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
already-high correlation with human adequacy
judgments.
4.1 Experimental setup
We perform the meta-evaluation upon two differ-
ent partitions of the DARPA GALE P2.5 Chinese-
English translation test set. The corpus includes
the Chinese input sentences, each accompanied by
one English reference translation and three partic-
ipating state-of-the-art MT systems? output.
For the sake of consistent comparison, the first
evaluation partition, GALE-A, is the same as the
one used in Lo and Wu (2011a), and the second
evaluation partition, GALE-B, is the same as the
one used in Lo and Wu (2011b).
For both reference and machine translations, the
ASSERT (Pradhan et al., 2004) semantic role la-
beler was used to automatically predict semantic
parses.
27
Table 1: Sentence-level correlation with human
adequacy judgements on different partitions of
GALE P2.5 data. IMEANT always yields top
correlations, and is more consistent than either
MEANT or its recent cross-lingual XMEANT
quality estimation variant. For reference, the hu-
man HMEANT upper bound is 0.53 for GALE-A
and 0.37 for GALE-B?thus, the fully automated
IMEANT approximation is not far from closing the
gap.
metric GALE-A GALE-B
IMEANT 0.51 0.33
XMEANT 0.51 0.20
MEANT 0.48 0.33
METEOR 1.5 (2014) 0.43 0.10
NIST 0.29 0.16
METEOR 0.4.3 (2005) 0.20 0.29
BLEU 0.20 0.27
TER 0.20 0.19
PER 0.20 0.18
CDER 0.12 0.16
WER 0.10 0.26
4.2 Results
The sentence-level correlations in Table 1 show
that IMEANT outperforms other automatic met-
rics in correlation with human adequacy judgment.
Note that this was achieved with no tuning what-
soever of the default rule weights (suggesting that
the performance of IMEANT could be further im-
proved in the future by slightly optimizing the ITG
weights).
On the GALE-A partition, IMEANT shows 3
points improvement over MEANT, and is tied
with the cross-lingual XMEANT quality estimator
discussed earlier.IMEANT produces much higher
HAJ correlations than any of the other metrics.
On the GALE-B partition, IMEANT is tied with
MEANT, and is significantly better correlated with
HAJ than the XMEANT quality estimator. Again,
IMEANT produces much higher HAJ correlations
than any of the other metrics.
We note that we have also observed this pattern
consistently in smaller-scale experiments?while
the monolingual MEANT metric and its cross-
lingual XMEANT cousin vie with each other on
different data sets, IMEANT robustly and consis-
tently produces top HAJ correlations.
In both the GALE-A and GALE-B partitions,
IMEANT comes within a few points of the human
upper bound benchmark HAJ correlations com-
puted using the human labeled semantic frames
and alignments used in the HMEANT.
Data analysis reveals two reasons that IMEANT
correlates with human adequacy judgement more
closely than MEANT. First, BITG constraints in-
deed provide more accurate phrasal similarity ag-
gregation, compared to the naive bag-of-words
based heuristics employed in MEANT. Similar re-
sults have been observed while trying to estimate
word alignment probabilities where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
Secondly, the permutation and bijectivity con-
straints enforced by the ITG provide better lever-
age to reject token alignments when they are not
appropriate, compared with the maximal align-
ment approach which tends to be rather promiscu-
ous. A case of this can be seen in Figure 3, which
shows the result on the same example sentence as
in Figure 1. Disregarding the semantic parsing er-
rors arising from the current limitations of auto-
matic SRL tools, the ITG tends to provide clean,
sparse alignments for role fillers like the ARG1
of the resumed PRED, preferring to leave tokens
like complete and range unaligned instead of aligning
them anyway as MEANT?s maximal alignment al-
gorithm tends to do. Note that it is not simply a
matter of lowering thresholds for accepting token
alignments: Tumuluru et al. (2012) showed that
the competitive linking approach (Melamed, 1996)
which also generally produces sparser alignments
does not work as well inMEANT, whereas the ITG
appears to be selective about the token alignments
in a manner that better fits the semantic structure.
For contrast, Figure 4 shows a case where
IMEANT appropriately accepts dense alignments.
5 Conclusion
We have presented IMEANT, an inversion trans-
duction grammar based rethinking of the MEANT
semantic frame based MT evaluation approach,
that achieves higher correlation with human ad-
equacy judgments of MT output quality than
MEANT and its variants, as well as other com-
mon evaluation metrics. Our results improve upon
previous research showing that MEANT?s explicit
use of semantic frames leads to state-of-the-art au-
tomatic MT evaluation. IMEANT achieves this
by aligning and scoring semantic frames under a
simple, consistent ITG that provides empirically
28
[REF] Until after their sales had ceased in mainland China for almost two months , sales of the complete range of SK ? II products have now been resumed .  
ARG0 PRED ARGM-LOC PRED ARG1 
[MT2] So far , in the mainland of China to stop selling nearly two months of SK - 2 products sales resumed .  
ARGM-TMP ARG1 PRED PRED ARG1 PRED 
ARGM-TMP ARGM-TMP 
So 
far 
, 
in 
the 
mainland 
of 
China 
to 
stop 
selling 
nearly 
two 
months 
of 
SK 
- 
2 
products 
sales 
resumed 
. 
Un
til 
afte
r 
the
ir 
sal
e had
 
cea
sed
 in 
ma
inla
nd 
Ch
ina
 for 
alm
ost
 
two
 , 
sal
es of the
 
com
ple
te 
PRED 
PRED 
ARG1 
ARGM-TMP 
ARG1 
PRED 
PR
ED
 
PR
ED
 
AR
G1
 
AR
GM
-L
OC
 
AR
G0
 
ran
ge of SK
 - II 
pro
duc
ts 
hav
e 
now
 
bee
n 
res
um
ed . 
mo
nth
s 
AR
GM
-T
MP
 
AR
GM
-T
MP
 
Figure 3: An example where the ITG helps produce correctly sparse alignments by rejecting inappro-
priate token alignments in the ARG1 of the resumed PRED, instead of wrongly aligning tokens like the,
complete, and range as MEANT tends to do. (The semantic parse errors are due to limitations of automatic
SRL.)
informative permutation and bijectivity biases, in-
stead of the maximal alignment and bag-of-words
assumptions used by MEANT. At the same time,
IMEANT retains the Occam?s Razor style simplic-
ity and representational transparency characteris-
tics of MEANT.
Given the absence of any tuning of ITG weights
in this first version of IMEANT, we speculate that
29
[REF] Australian Prime Minister Howard said the government could cancel AWB 's monopoly in the wheat business next week . 
[MT2] Australian Prime Minister John Howard said that the Government might cancel the AWB company wheat monopoly next week . 
ARG0 
ARG0 
PRED 
ARG0 PRED ARG1 PRED 
ARG0 
ARGM-MOD ARGM-TMP 
ARG1 
PRED ARGM-MOD ARG1 ARGM-TMP 
ARG1 
Australian 
Prime 
Minister 
John 
Howard 
said 
the 
Government 
might 
cancel 
the 
AWB 
company 
wheat 
monopoly 
Au
str
alia
n 
Pri
me
 
Min
iste
r 
Ho
wa
rd sai
d the
 
go
ver
nm
en
t 
cou
ld 
can
cel
 
AW
B 's 
mo
no
po
ly the
 in 
next 
week 
that 
. 
wh
ea
t 
bu
sin
ess
 
ne
xt 
we
ek . 
pr
ed
 
pr
ed
 
AR
G0
 
AR
G0
 
AR
GM
-M
OD
 
AR
G1
 
AR
GM
-T
MP
 
AR
G1
 
pred 
pred 
ARG0 
ARG0 
ARGM-MOD 
ARGM-TMP 
ARG1 
ARG1 
Figure 4: An example of dense alignments in IMEANT, for the Chinese input sentence ???????
?????????????? AWB?????????? (The semantic parse errors are due to limitations
of automatic SRL.)
IMEANT could perform even better than it already
does here.We plan to investigate simple hyperpa-
rameter optimizations in the near future.
30
6 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessar-
ily reflect the views of DARPA, the EU, or RGC.
Thanks to Karteek Addanki for supporting work,
and to Pascale Fung, Yongsheng Yang and Zhao-
jun Wu for sharing the maximum entropy Chinese
segmenter and C-ASSERT, the Chinese semantic
parser.
References
Karteek Addanki, Chi-kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-
lingual verb frame alternations. In 16th An-
nual Conference of the European Association
for Machine Translation (EAMT-2012), Trento,
Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Satanjeev Banerjee and Alon Lavie. METEOR:
An automatic metric forMT evaluation with im-
proved correlation with human judgments. In
Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in ma-
chine translation research. In 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
(meta-) evaluation of machine translation. In
Second Workshop on Statistical Machine Trans-
lation (WMT-07), 2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
Further meta-evaluation of machine transla-
tion. In Third Workshop on Statistical Machine
Translation (WMT-08), 2008.
Julio Castillo and Paula Estrella. Semantic tex-
tual similarity for MT evaluation. In 7th Work-
shop on Statistical Machine Translation (WMT
2012), 2012.
Michael Denkowski and Alon Lavie. METEOR
universal: Language specific translation eval-
uation for any target language. In 9th Work-
shop on Statistical Machine Translation (WMT
2014), 2014.
George Doddington. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In The second interna-
tional conference on Human Language Technol-
ogy Research (HLT ?02), San Diego, California,
2002.
Jes?s Gim?nez and Llu?s M?rquez. Linguistic fea-
tures for automatic evaluation of heterogenous
MT systems. In Second Workshop on Statisti-
cal Machine Translation (WMT-07), pages 256?
264, Prague, Czech Republic, June 2007.
Jes?s Gim?nez and Llu?s M?rquez. A smorgas-
bord of features for automaticMT evaluation. In
Third Workshop on Statistical Machine Transla-
tion (WMT-08), Columbus, Ohio, June 2008.
Philipp Koehn and Christof Monz. Manual and
automatic evaluation of machine translation be-
tween european languages. InWorkshop on Sta-
tistical Machine Translation (WMT-06), 2006.
Gregor Leusch and Hermann Ney. Bleusp, invwer,
cder: Three improved mt evaluation measures.
In NIST Metrics for Machine Translation Chal-
lenge (MetricsMATR), at Eighth Conference of
the Association for Machine Translation in the
Americas (AMTA 2008), Waikiki, Hawaii, Oct
2008.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. A novel string-to-string distance measure
with applications to machine translation evalu-
ation. In Machine Translation Summit IX (MT
Summit IX), New Orleans, Sep 2003.
Gregor Leusch, Nicola Ueffing, and Hermann
Ney. CDer: Efficient MT evaluation using
block movements. In 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL-2006), 2006.
31
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. InWorkshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
Ann Arbor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpen-
sive, high-accuracy, semi-automatic metric for
evaluating translation utility based on seman-
tic roles. In 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux:
How semantic frames evaluate MT more ac-
curately. In Twenty-second International Joint
Conference on Artificial Intelligence (IJCAI-
11), 2011.
Chi-kiu Lo andDekaiWu. Unsupervised vs. super-
vised weight estimation for semantic MT evalu-
ation metrics. In Sixth Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres
be better translated by tuning on automatic se-
mantic metrics? In 14th Machine Translation
Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT
2013: A tunable, accurate yet inexpensive se-
mantic frame based mt evaluation metric. In
8th Workshop on Statistical Machine Transla-
tion (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai
Wu. Fully automatic semantic MT evaluation.
In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by
training against an automatic semantic frame
based evaluation metric. In 51st Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by
tuning against Chinese MEANT. In Interna-
tional Workshop on Spoken Language Transla-
tion (IWSLT 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, Markus Saers, and
Dekai Wu. XMEANT: Better semantic MT
evaluation without reference translations. In
52nd Annual Meeting of the Association for
Computational Linguistics (ACL 2014), 2014.
Matou?Mach??ek andOnd?ej Bojar. Results of the
WMT13 metrics shared task. In Eighth Work-
shop on Statistical Machine Translation (WMT
2013), Sofia, Bulgaria, August 2013.
I. Dan Melamed. Automatic construction of
clean broad-coverage translation lexicons. In
2nd Conference of the Association for Ma-
chine Translation in the Americas (AMTA-
1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strap-
parava. Corpus-based and knowledge-based
measures of text semantic similarity. In The
Twenty-first National Conference on Artificial
Intelligence (AAAI-06), volume 21, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch,
and Hermann Ney. A evaluation tool for ma-
chine translation: Fast evaluation for MT re-
search. In The Second International Conference
on Language Resources and Evaluation (LREC
2000), 2000.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Dependency-based automatic eval-
uation for machine translation. In Syntax
and Structure in Statistical Translation (SSST),
2007.
Karolina Owczarzak, Josef van Genabith, and
Andy Way. Evaluating machine translation
with LFG dependencies. Machine Translation,
21:95?119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. BLEU: a method for automatic
evaluation of machine translation. In 40th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL-02), pages 311?318,
Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow se-
mantic parsing using support vector machines.
In Human Language Technology Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-
NAACL 2004), 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia.
TINE: A metric to assess MT adequacy. In
Sixth Workshop on Statistical Machine Transla-
tion (WMT 2011), 2011.
32
Markus Saers and Dekai Wu. Improving phrase-
based translation via word alignments from
stochastic inversion transduction grammars. In
Third Workshop on Syntax and Structure in
Statistical Translation (SSST-3), pages 28?36,
Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT?09), pages 29?32,
Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. A study
of translation edit rate with targeted human an-
notation. In 7th Biennial Conference Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 2006), pages 223?231, Cambridge,
Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai
Wu. Accuracy and robustness in measuring the
lexical similarity of semantic role fillers for au-
tomatic semantic MT evaluation. In 26th Pa-
cific Asia Conference on Language, Informa-
tion, and Computation (PACLIC 26), 2012.
Mengqiu Wang and Christopher D. Manning.
SPEDE: Probabilistic edit distance metrics for
MT evaluation. In 7th Workshop on Statistical
Machine Translation (WMT 2012), 2012.
DekaiWu. An algorithm for simultaneously brack-
eting parallel texts by aligning words. In 33rd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL 95), pages 244?251,
Cambridge, Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars
for parallel text bracketing. In Third Annual
Workshop on Very Large Corpora (WVLC-3),
pages 69?81, Cambridge, Massachusetts, June
1995.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Richard Zens and Hermann Ney. A compara-
tive study on reordering constraints in statisti-
cal machine translation. In 41st Annual Meeting
of the Association for Computational Linguis-
tics (ACL-2003), pages 144?151, Stroudsburg,
Pennsylvania, 2003.
33
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86?93,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Ternary Segmentation for Improving Search
in Top-down Induction of Segmental ITGs
Markus Saers Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{masaers|dekai}@cs.ust.hk
Abstract
We show that there are situations where
iteratively segmenting sentence pairs top-
down will fail to reach valid segments and
propose a method for alleviating the prob-
lem. Due to the enormity of the search
space, error analysis has indicated that it is
often impossible to get to a desired embed-
ded segment purely through binary seg-
mentation that divides existing segmental
rules in half ? the strategy typically em-
ployed by existing search strategies ? as
it requires two steps. We propose a new
method to hypothesize ternary segmenta-
tions in a single step, making the embed-
ded segments immediately discoverable.
1 Introduction
One of the most important improvements to sta-
tistical machine translation to date was the move
from token-basedmodel to segmental models (also
called phrasal). This move accomplishes two
things: it allows a flat surface-based model to
memorize some relationships between word real-
izations, but more importantly, it allows the model
to capture multi-word concepts or chunks. These
chunks are necessary in order to translate fixed ex-
pressions, or other multi-word units that do not
have a compositional meaning. If a sequence in
one language can be broken down into smaller
pieces which are then translated individually and
reassembled in another language, the meaning of
the sequence is compositional; if not, the only way
to translate it accurately is to treat it as a single unit
? a chunk. Existing surface-based models (Och et
al., 1999) have high recall in capturing the chunks,
but tend to over-generate, which leads to big mod-
els and low precision. Surface-based models have
no concept of hierarchical composition, instead
they make the assumption that a sentence consists
of a sequence of segments that can be individually
translated and reordered to form the translation.
This is counter-intuitive, as the who-did-what-to-
whoms of a sentence tends to be translated and re-
ordered as units, rather than have their components
mixed together. Transduction grammars (Aho and
Ullman, 1972; Wu, 1997), also called hierarchical
translation models (Chiang, 2007) or synchronous
grammars, address this through a mechanism sim-
ilar to context-free grammars. Inducing a segmen-
tal transduction grammar is hard, so the standard
practice is to use a similar method as the surface-
based models use to learn the chunks, which is
problematic, since that method mostly relies on
memorizing the relationships that the mechanics
of a compositional model is designed to general-
ize. A compositional translation model would be
able to translate lexical chunks, as well as gener-
alize different kinds of compositions; a segmen-
tal transduction grammar captures this by having
segmental lexical rules and different nonterminal
symbols for different categories of compositions.
In this paper, we focus on inducing the former:
segmental lexical rules in inversion transduction
grammars (ITGs).
One natural way would be to start with a token-
based grammar and chunk adjacent tokens to form
segments. The main problemwith chunking is that
the data becomes more and more likely as the seg-
ments get larger, with the degenerate end point of
all sentence pairs being memorized lexical items.
Zhang et al. (2008) combat this tendency by intro-
ducing a sparsity prior over the rule probabilities,
and variational Bayes to maximize the posterior
probability of the data subject to this symmetric
Dirichlet prior. To hypothesize possible chunks,
they examine the Viterbi biparse of the existing
model. Saers et al. (2012) use the entire parse for-
est to generate the hypotheses. They also boot-
strap the ITG from linear and finite-state transduc-
tion grammars (LTGs, Saers (2011), and FSTGs),
86
rather than initialize the lexical probabilities from
IBM models.
Another way to arrive at a segmental ITG is to
start with the degenerate chunking case: each sen-
tence pair as a lexical item, and segment the exist-
ing lexical rules into shorter rules. Since the start
point is the degenerate case when optimizing for
data likelihood, this approach requires a different
objective function to optimize against. Saers et al.
(2013c) proposes to use description length of the
model and the data given the model, which is sub-
sequently expressed in a Bayesian form with the
addition of a prior over the rule probabilities (Saers
andWu, 2013). The way they generate hypotheses
is restricted to segmenting an existing lexical item
into two parts, which is problematic, because em-
bedded lexical items are potentially overlooked.
There is also the option of implicitly defining
all possible grammars, and sample from that dis-
tribution. Blunsom et al. (2009) do exactly that;
they induce with collapsed Gibbs sampling which
keeps one derivation for each training sentence
that is altered and then resampled. The operations
to change the derivations are split, join, delete
and insert. The split-operator corresponds to bi-
nary segmentation, the join-operator corresponds
to chunking; the delete-operator removes an inter-
nal node, resulting in its parent having three chil-
dren, and the insert-operator allows a parent with
three children to be normalized to have only two.
The existence of ternary nodes in the derivation
means that the learned grammar contains ternary
rules. Note that it still takes three operations: two
split-operations and one delete-operation for their
model to do what we propose to do in a single
ternary segmentation. Also, although we allow for
single-step ternary segmentations, our grammar
does not contain ternary rules; instead the results of
a ternary segmentation is immediately normalized
to the 2-normal form. Although their model can
theoretically sample from the entire model space,
the split-operation alone is enough to do so; the
other operations were added to get the model to do
so in practice. Similarly, we propose ternary seg-
mentation to be able to reach areas of the model
space that we failed to reach with binary segmen-
tation.
To illustrate the problem with embedded lexi-
cal items, we will introduce a small example cor-
pus. Although Swedish and English are relatively
similar, with the structure of basic sentences being
identical, they already illustrate the common prob-
lem of rare embedded correspondences. Imagine
a really simple corpus of three sentence pairs with
identical structure:
he has a red book / han har en r?d bok
she has a biology book / hon har en biologibok
it has begun / det har b?rjat
The main difference is that Swedish concate-
nates rather than juxtaposes compounds such as
biologibok instead of biology book. A bilingual
person looking at this corpus would produce bilin-
gual parse trees like those in Figure 1. Inducing
this relatively simple segmental ITG from the data
is, however, quite a challenge.
The example above illustrates a problem with
the chunking approach, as one of the most com-
mon chunks is has a/har en, whereas the linguis-
tically motivated chunk biology book/biologibok
occurs only once. There is very little in this data
that would lead the chunking approach towards the
desired ITG. It also illustrates a problem with the
binary segmentation approach, as all the bilingual
prefixes and suffixes, the biaffixes, are unique;
there is no way of discovering that all the above
sentences have the exact same verb.
In this paper, we propose a method to al-
low bilingual infixes to be hypothesized and used
to drive the minimization of description length,
which would be able to induce the desired ITG
from the above corpus.
The paper is structured so that we start by giv-
ing a definition of the grammar formalism we use:
ITGs (Section 2). We then describe the notion
of description length that we use (Section 3), and
how ternary segmentation differs from and com-
plements binary segmentation (Section 4). We
then present our induction algorithm (Section 5)
and give an example of a run through (Section 6).
Finally we offer some concluding remarks (Sec-
tion 7).
2 Inversion transduction grammars
Inversion transduction grammars, or ITGs (Wu,
1997), are an expressive yet efficient way to
model translation. Much like context-free gram-
mars (CFGs), they allow for sentences to be ex-
plained through composition of smaller units into
larger units, but where CFGs are restricted to gen-
erate monolingual sentences, ITGs generate sets
of sentence pairs ? transductions ? rather than
languages. Naturally, the components of differ-
87
hasshe a biology book
har en biologibokhon
hasit begun
har b?rjatdet
hashe a red
har en r?dhan
book
bok
Figure 1: Possible inversion transduction trees over the example sentence pairs.
ent languages may have to be ordered differently,
which means that transduction grammars need to
handle these differences in order. Rather than al-
lowing arbitrary reordering and pay the price of ex-
ponential time complexity, ITGs allow only mono-
tonically straight or inverted order of the produc-
tions, which cuts the time complexity down to a
manageable polynomial.
Formally, an ITG is a tuple ?N,?,?, R, S?,
where N is a finite nonempty set of nonterminal
symbols, ? is a finite set of terminal symbols in
L
0
, ? is a finite set of terminal symbols in L
1
, R
is a finite nonempty set of inversion transduction
rules and S ? N is a designated start symbol. An
inversion transduction rule is restricted to take one
of the following forms:
S ? [A] , A?
[
?
+
]
, A? ??
+
?
where S ? N is the start symbol, A ? N is a non-
terminal symbol, and ?+ is a nonempty sequence
of nonterminals and biterminals. A biterminal is
a pair of symbol strings: ?? ???, where at least
one of the strings have to be nonempty. The square
and angled brackets signal straight and inverted or-
der respectively. With straight order, both the L
0
and the L
1
productions are generated left-to-right,
but with inverted order, theL
1
production is gener-
ated right-to-left. The brackets are frequently left
out when there is only one element on the right-
hand side, which means that S ? [A] is shortened
to S ? A.
Like CFGs, ITGs also have a 2-normal form,
analogous to the Chomsky normal form for CFGs,
where the rules are further restricted to only the
following four forms:
S ? A, A? [BC] , A? ?BC?, A? e/f
where S ? N is the start symbol, A,B,C ? N
are nonterminal symbols and e/f is a biterminal
string.
A bracketing ITG, or BITG, has only one non-
terminal symbol (other than the dedicated start
symbol), which means that the nonterminals carry
no information at all other than the fact that their
yields are discrete unit. Rather than make a proper
analysis of the sentence pair they only produce a
bracketing, hence the name.
A transduction grammar such as ITG can be
used in three modes: generation, transduction
and biparsing. Generation derives a bisentence, a
sentence pair, from the start symbol. Transduction
derives a sentence in one language from a sentence
in the other language and the start symbol. Bipars-
ing verifies that a given bisentence can be derived
from the start symbol. Biparsing is an integral part
of any learning that requires expected counts such
as expectation maximization, and transduction is
the actual translation process.
3 Description length
We follow the definition of description length from
Saers et al. (2013b,c,d,a); Saers and Wu (2013),
that is: the size of the model is determined by
counting the number of symbols needed to encode
the rules, and the size of the data given the model
is determined by biparsing the data with the model.
Formally, given a grammar? its description length
DL (?) is the sum of the length of the symbols
needed to serialize the rule set. For convenience
later on, the symbols are assumed to be uniformly
distributed with a length of?lg 1
N
bits each (where
N is the number of different symbols). The de-
scription length of the data D given the model is
defined as DL (D|?) = ?lgP (D|?).
88
Figure 2: The four different kinds of binary seg-
mentation hypotheses.
Figure 3: The two different hypotheses that can
be made from an infix-to-infix link.
4 Segmenting lexical items
With a background in computer science it is tempt-
ing to draw the conclusion that any segmentation
can be made as a sequence of binary segmenta-
tions. This is true, but only relevant if the entire
search space can be exhaustively explored. When
inducing transduction grammars, the search space
is prohibitively large; in fact, we are typically af-
forded only an estimate of a single step forward
in the search process. In such circumstances, the
kinds of steps you can take start to matter greatly,
and adding ternary segmentation to the typically
used binary segmentation adds expressive power.
Figure 2 contains a schematic illustration of bi-
nary segmentation: To the left is a lexical item
where a good biaffix (anL
0
prefix or suffix associ-
ated with anL
1
prefix or suffix) has been found, as
illustrated with the solid connectors. To the right is
the segmentation that can be inferred. For binary
segmentation, there is no uncertainty in this step.
When adding ternary segmentation, there are
five more situations: one situation where an in-
Figure 4: The eight different hypotheses that can
bemade from the four different infix-to-affix links.
fix is linked to an infix, and four situations where
an infix is linked to an affix. Figure 3 shows the
infix-to-infix situation, where there is one addi-
tional piece of information to be decided: are the
surroundings linked straight or inverted? Figure 4
shows the situations where one infix is linked to an
affix. In these situations, there are twomore pieces
of information that needs to be inferred: (a) where
the sibling of the affix needs to be segmented, and
(b) how the two pieces of the sibling of the affix
link to the siblings of the infix. The infix-to-affix
situations require a second monolingual segmen-
tations decision to be made. As this is beyond the
scope of this paper, we will limit ourselves to the
infix-to-infix situation.
5 Finding segmentation hypotheses
Previous work on binary hypothesis generation
makes assumptions that do not hold with ternary
segmentation; this section explains why that is and
how we get around it. The basic problem with bi-
nary segmentation is that any bisegment hypothe-
sized to be good on its own has to be anchored to
either the beginning or the end of an existing biseg-
ment. An infix, by definition, does not.
While recording all affixes is possible, even for
non-toy corpora (Saers and Wu, 2013; Saers et al.,
2013b,c), recording all bilingual infixes is not, so
collecting them all is not an option (while there are
89
Algorithm 1 Pseudo code for segmenting an ITG.
? ? The ITG being induced.
? ? The token-based ITG used to evaluate lexical rules.
h
max
? The maximum number of hypotheses to keep from a single lexical rule.
repeat
? ? 0
H
?
? Initial hypotheses
for all lexical rules A? e/f do
p? parse(?, e/f)
c ? Fractional counts of bispans
for all bispans s, t, u, v ? e/f do
c(s, t, u, v)? 0
H
??
? []
for all items B
s,t,u,v
? p do
c(s, t, u, v)? c(s, t, u, v) + ?(B
s,t,u,v
)?(B
s,t,u,v
)/?(S
0,T,0,V
)
H
??
? [H
??
, ?s, t, u, v, c(s, t, u, v)?]
sort H ?? on c(s, t, u, v)
for all ?s, t, u, v, c(s, t, u, v)? ? H ??[0..h
max
] do
H
?
(e
s..t
/f
u..v
)? [H
?
(e
s..t
/f
u..v
), ?s, t, u, v, A? e/f?]
H ? Evaluated hypotheses
for all bisegments e
s..t
/f
u..v
? keys(H
?
) do
?
?
? ?
R? []
for all bispan-rule pairs ?s, t, u, v, A? e/f? ? H ?(e
s..t
/f
u..v
) do
?
?
? make_grammar_change(??, e/f, s, t, u, v)
R? [R,A? e/f ]
?
?
? DL(?
?
)?DL(?) +DL(D|?
?
)?DL(D|?)
if ?? < 0 then
H ? [H, ?e
s..t
/f
u..v
, R, ?
?
?]
sort H on ??
for all ?e
s..t
/f
u..v
, R, ?
?
? ? H do
?
?
? ?
for all rules A? e/f ? R ?R
?
? do
?
?
? make_grammar_change(??, e/f, s, t, u, v)
?
?
? DL(?
?
)?DL(?) +DL(D|?
?
)?DL(D|?)
if ?? < 0 then
?? ?
?
? ? ? + ?
?
until ? ? 0
return ?
only O
(
n
2
)
possible biaffixes for a parallel sen-
tence of average length n, there are O
(
n
4
)
possi-
ble bilingual infixes). A way to prioritize, within
the scope of a single bisegment, which infixes and
affixes to consider as hypotheses is crucial. In this
paper we use an approach similar to Saers et al.
(2013d), in which we use a token-based ITG to
evaluate the lexical rules in the ITG that is be-
ing induced. Using a transduction grammar has
the advantage of calculating fractional counts for
hypotheses, which allows both long and short hy-
potheses to compete on a level playing field.
In Algorithm 1, we start by parsing all the lex-
ical rules in the grammar ? being learned using a
token-based ITG ?. For each rule, we only keep
the best hmax bispans. In the second part, all col-
lected bispans are evaluated as if they were the
only hypothesis being considered for changing ?.
Any hypothesis with a positive effect is kept for
further processing. These hypotheses are sorted
90
and applied. Since the grammarmay have changed
since the effect of the hypothesis was estimated,
we have to check that the hypothesis would have
a positive effect on the updated grammar before
committing to it. All this is repeated as long as
there are improvements that can be made.
Themake_grammar_changemethod deletes the
old rule, and distributes its probability mass to
the rules replacing it. For ternary segmentation,
this will be three lexical rules, and two structural
rules (which happens to be identical in a bracket-
ing grammar, giving that one rule two shares of
the probability mass being distributed). For binary
segmentation it is two lexical rules and one struc-
tural rule.
Rather than calculating DL (D|?)? DL (D|?)
explicitly by biparsing the entire corpus, we es-
timate the change. For binary rules, we use the
same estimate as Saers and Wu (2013): multiply-
ing in the new rule probabilities and dividing out
the old. For ternary rules, we make the assump-
tion that the three new lexical rules are combined
using structural rules the way they would during
parsing, which means two binary structural rules
being applied. The infix-to-infix situation must be
generated either by two straight combinations or
by two inverted combinations, so for a bracketing
grammar it is always two applications of a single
structural rule. We thus multiply in the three new
lexical rules and the structural rule twice, and di-
vide out the old rule. In essence, both these meth-
ods are recreating the situations in which the parser
would have used the old rule, but now uses the new
rules.
Having exhausted all the hypotheses, we also
run expectation maximization to stabilize the pa-
rameters. This step is not shown in the pseudo
code.
Examining the pseudocode closer reveals that
the outer loop will continue as long as the grammar
changes; since the only way the grammar changes
is by making lexical rules shorter, this loop is guar-
anteed to terminate. Inside the outer loop there are
three inner loops: one over the rule set, one over
the set of initial hypothesesH ? and one over the set
of evaluated hypothesesH . The sets of hypotheses
are related such that |H| ? |H ?|, which means that
the size of the initial set of hypotheses will dom-
inate the time complexity. The size of this initial
set of hypotheses is itself limited so that it cannot
contain more than hmax hypotheses from any one
rule. The dominating factor is thus the size of the
rule set, which we will further analyze.
The first thing we do is to parse the right-hand
side of the rule, which requires O
(
n
3
)
with the
Saers et al. (2009) algorithm, where n is the av-
erage length of the lexical items. We then initial-
ize the counts, which does not actually require a
specific step in implementation. We then iterate
over all bispans in the parse, which has the same
upper bound as the parsing process, since the ap-
proximate parsing algorithm avoids exploring the
entire search space. We then sort the set of hy-
potheses derived from the current rule only, which
is asymptotically bound byO
(
n
3lgn
)
, since there
is exactly one hypothesis per parse item. Finally,
there is a selection being made from the set of hy-
potheses derived from the current rule. In prac-
tice, the parsing is more complicated than the sort-
ing, making the time complexity of the whole inner
loop be dominated by the time it takes to parse the
rules.
6 Example
In this section we will trace through how the ex-
ample from the introduction fails to go through
binary segmentation, but succeeds when infix-to-
infix segmentations are an option.
The initial grammar consists of all the sentence
pairs as segmental lexical rules:
S ? A 1
A?
he has a red book
han har en r?d bok 0.3
A?
she has a biology book
hon har en biologibok 0.3
A?
it has begun
det har b?rjat 0.3
As noted before, there are no shared biaffixes
among the three lexical rules, so binary segmen-
tation cannot break this grammar down further.
There are, however, three shared bisegments rep-
resenting three different segmentation hypotheses:
has a/har en, has/har and a/en. In this example it
does not matter which hypothesis you choose, so
we will go with the first one, since that is the one
our implementation chose. Breaking out all occur-
rences of has a/har en gives the following gram-
91
mar:
S ? A 1
A? [AA] 0.36
A? it has begun/det har b?rjat 0.09
A? has a/har en 0.18
A? he/han 0.09
A? red book/r?d bok 0.09
A? she/hon 0.09
A? biology book/biologibok 0.09
At this point there are two bisegments that occur
in more than one rule: has/har and a/en. Again,
it does not matter for the final outcome which of
the hypotheses we choose, so we will chose the
first one, again because that is the one our imple-
mentation chose. Breaking out all occurrences of
has/har gives the following grammar:
S ? A 1
A? [AA] 0.421
A? he/han 0.053
A? red book/r?d bok 0.053
A? she/hon 0.053
A? biology book/biologibok 0.053
A? has/har 0.158
A? it/det 0.053
A? begun/b?rjat 0.053
A? a/en 0.105
There are no shared bisegments left in the gram-
mar now, so no more segmentations can be done.
Obviously, the probability of the data given this
new grammar is much smaller, but the grammar
itself has generalized far beyond the training data,
to the point where it largely agrees with the pro-
posed trees in Figure 1 (except that this grammar
binarizes the constituents, and treats red book/r?d
bok as a segment).
7 Conclusions
We have shown that there are situations in which
a top-down segmenting approach that relies solely
on binary segmentation will fail to generalize, de-
spite there being ample evidence to a human that
a generalization is warranted. We have proposed
ternary segmentation as a solution to provide hy-
potheses that are considered good under a mini-
mum description length objective. And we have
shown that the proposed method could indeed per-
form generalizations that are clear to the human
eye, but not discoverable through binary segmen-
tation. The algorithm is comparable to previ-
ous segmentation approaches in terms of time and
space complexity, so scaling up to non-toy training
corpora is likely to work when the time comes.
Acknowledgements
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
References
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Phil Blunsom, Trevor Cohn, Chris Dyer, andMiles
Osborne. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Joint Confer-
ence of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and 4th
International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP
2009), pp. 782?790, Suntec, Singapore, August
2009.
David Chiang. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?
228, 2007.
Frans Josef Och, Christoph Tillmann, and Her-
mann Ney. Improved alignment models for sta-
tistical machine translation. In 1999 Joint SIG-
DAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora, pp. 20?28, University of Maryland, Col-
lege Park, Maryland, June 1999.
Markus Saers and Dekai Wu. Bayesian induc-
tion of bracketing inversion transduction gram-
mars. In Sixth International Joint Conference on
Natural Language Processing (IJCNLP2013),
pp. 1158?1166, Nagoya, Japan, October 2013.
Asian Federation of Natural Language Process-
ing.
92
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference
on Parsing Technologies (IWPT?09), pp. 29?32,
Paris, France, October 2009.
Markus Saers, Karteek Addanki, and Dekai Wu.
From finite-state to inversion transductions: To-
ward unsupervised bilingual grammar induc-
tion. In 24th International Conference on
Computational Linguistics (COLING 2012), pp.
2325?2340, Mumbai, India, December 2012.
Markus Saers, Karteek Addanki, and Dekai Wu.
Augmenting a bottom-up ITG with top-down
rules by minimizing conditional description
length. In Recent Advances in Natural Lan-
guage Processing (RANLP 2013), Hissar, Bul-
garia, September 2013.
Markus Saers, Karteek Addanki, and Dekai Wu.
Combining top-down and bottom-up search for
unsupervised induction of transduction gram-
mars. In Seventh Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-7), pp. 48?57, Atlanta, Georgia, June
2013.
Markus Saers, Karteek Addanki, and Dekai Wu.
Iterative rule segmentation under minimum
description length for unsupervised transduc-
tion grammar induction. In Adrian-Horia
Dediu, Carlos Mart?n-Vide, Ruslan Mitkov, and
Bianca Truthe, editors, Statistical Language and
Speech Processing, First International Confer-
ence, SLSP 2013, Lecture Notes in Artificial In-
telligence (LNAI). Springer, Tarragona, Spain,
July 2013.
Markus Saers, Karteek Addanki, and Dekai Wu.
Unsupervised transduction grammar induction
via minimum description length. In Second
Workshop on Hybrid Approaches to Transla-
tion (HyTra), pp. 67?73, Sofia, Bulgaria, Au-
gust 2013.
Markus Saers. Translation as Linear Trans-
duction: Models and Algorithms for Efficient
Learning in Statistical Machine Translation.
PhD thesis, Uppsala University, Department of
Linguistics and Philology, 2011.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In 46th Annual Meeting of the Association
for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pp. 97?
105, Columbus, Ohio, June 2008.
93
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 112?121,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Transduction Recursive Auto-Associative Memory:
Learning Bilingual Compositional Distributed Vector Representations of
Inversion Transduction Grammars
Karteek Addanki Dekai Wu
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{vskaddanki|dekai}@cs.ust.hk
Abstract
We introduce TRAAM, or Transduction
RAAM, a fully bilingual generalization
of Pollack?s (1990) monolingual Recur-
sive Auto-AssociativeMemory neural net-
workmodel, in which each distributed vec-
tor represents a bilingual constituent?i.e.,
an instance of a transduction rule, which
specifies a relation between two monolin-
gual constituents and how their subcon-
stituents should be permuted. Bilingual
terminals are special cases of bilingual
constituents, where a vector represents ei-
ther (1) a bilingual token?a token-to-
token or ?word-to-word? translation rule
?or (2) a bilingual segment?a segment-
to-segment or ?phrase-to-phrase? transla-
tion rule. TRAAMs have properties that
appear attractive for bilingual grammar in-
duction and statistical machine translation
applications. Training of TRAAM drives
both the autoencoder weights and the vec-
tor representations to evolve, such that
similar bilingual constituents tend to have
more similar vectors.
1 Introduction
We introduce Transduction RAAM?or TRAAM
for short?a recurrent neural network model that
generalizes the monolingual RAAMmodel of Pol-
lack (1990) to a distributed vector representation
of compositionally structured transduction gram-
mars (Aho andUllman, 1972) that is fully bilingual
from top to bottom. In RAAM, which stands for
Recursive Auto-Associative Memory, using fea-
ture vectors to characterize constituents at every
level of a parse tree has the advantages that (1)
the entire context of all subtrees inside the con-
stituent can be efficiently captured in the feature
vectors, (2) the learned representations generalize
well because similar feature vectors represent sim-
ilar constituents or segments, and (3) representa-
tions can be automatically learned so as to max-
imize prediction accuracy for various tasks using
semi-supervised learning. We argue that different,
but analogous, properties are desirable for bilin-
gual structured translation models.
Unlike RAAM, where each distributed vector
represents a monolingual token or constituent,
each distributed vector in TRAAM represents a
bilingual constituent or biconstituent?that is, an
instance of a transduction rule, which asserts a re-
lation between two monolingual constituents, as
well as specifying how to permute their subcon-
stituents in translation. Bilingual terminals, or
biterminals, are special cases of biconstituents
where a vector represents either (1) a bitoken?a
token-to-token or ?word-to-word? translation rule
?or (2) a bisegment?a segment-to-segment or
?phrase-to-phrase? translation rule.
The properties of TRAAMs are attractive for
machine translation applications. As with RAAM,
TRAAMs can be trained via backpropagation
training, which simultaneously evolves both the
autoencoder weights and the biconstituent vector
representations. As with RAAM, the evolution
of the vector representations within the hidden
layer performs automatic feature induction, and for
many applications can obviate the need for man-
ual feature engineering. However, the result is
that similar vectors tend to represent similar bicon-
stituents, rather than monolingual constituents.
The learned vector representations thus tend to
form clusters of similar translation relations in-
stead of merely similar strings. That is, TRAAM
clusters represent soft nonterminal categories of
cross-lingual relations and translation patterns, as
opposed to soft nonterminal categories of mono-
lingual strings as in RAAM.
Also, TRAAMs inherently make full simulta-
neous use of both input and output language fea-
112
tures, recursively, in an elegant integrated fash-
ion. TRAAM does not make restrictive a pri-
ori assumptions of conditional independence be-
tween input and output language features. When
evolving the biconstituent vector representations,
generalization occurs over similar input and out-
put structural characteristics simultaneously. In
most recurrent neural network applications to ma-
chine translation to date, only input side features
or only output language features are used. Even in
the few previous cases where recurrent neural net-
works have employed both input and output lan-
guage features for machine translation, the models
have typically been factored so that their recursive
portion is applied only to either the input or output
language, but not both.
As with RAAM, the objective criteria for train-
ing can be adjusted to reflect accuracy on nu-
merous different kinds of tasks, biasing the di-
rection that vector representations evolve toward.
But again, TRAAM?s learned vector representa-
tions support making predictions that simultane-
ously make use of both input and output struc-
tural characteristics. For example, TRAAM has
the ability to take into account the structure of
both input and output subtree characteristics while
making predictions on reordering them. Similarly,
for specific cross-lingual tasks such as word align-
ment, sense disambiguation, or machine transla-
tion, classifiers can simultaneously be trained in
conjunction with evolving the vector representa-
tions to optimize task-specific accuracy (Chris-
man, 1991).
In this paper we use as examples binary bi-
parse trees consistent with transduction grammars
in a 2-normal form, which by definition are in-
version transduction grammars (Wu, 1997) since
they are binary rank. This is not a requirement
for TRAAM, which in general can be formed for
transduction grammars of any rank. Moreover,
with distributed vector representations, the notion
of nonterminal categories in TRAAM is that of soft
membership, unlike in symbolically represented
transduction grammars. We start with bracketed
training data that contains no bilingual category
labels (like training data for Bracketing ITGs or
BITGs). Training results in self-organizing clus-
ters that have been automatically induced, repre-
senting soft nonterminal categories (unlike BITGs,
which do not have differentiated nonterminal cat-
egories).
2 Related work
TRAAM builds on different aspects of a spec-
trum of previous work. A large body of work ex-
ists on various different types of self-organizing
recurrent neural network approaches to model-
ing recursive structure, but mostly in monolin-
gual modeling. Even in applications to ma-
chine translation or cross-lingual modeling, the
typical practice has been to insert neural net-
work scoring components while still maintain-
ing older SMT modeling assumptions like bags-
of-words/phrases, ?shake?n?bake? translation that
relies heavily on strong monolingual language
models, and log-linear models?in contrast to
TRAAM?s fully integrated bilingual approach.
Here we survey representative work across the
spectrum.
2.1 Monolingual related work
Distributed vector representations have long
been used for n-gram language modeling; these
continuous-valued models exploit the general-
ization capabilities of neural networks, although
there is no hidden contextual or hierarchical
structure as in RAAM. Schwenk (2010) applies
one such language model within an SMT system.
In the simple recurrent neural networks (RNNs
or SRNs) of Elman (1990), hidden layer represen-
tations are fed back to the input to dynamically rep-
resent an aggregate of the immediate contextual
history. More recently, the probabilistic NNLMs
of Bengio et al. (2003) and Bengio et al. (2009)
follow in this vein.
To represent hierarchical tree structure using
vector representations, one simple family of ap-
proaches employs convolutional networks, as in
Lee et al. (2009) for example. Collobert and We-
ston (2008) use a convolution neural network layer
quite effectively to learn vector representations for
words which are then used in a host of NLP tasks
such as POS tagging, chunking, and semantic role
labeling.
RAAM approaches, and related recursive au-
toencoder approaches, can be more flexible than
convolutional networks. Like SRNs, they can be
extended in numerous ways. The URAAM (Uni-
fication RAAM) model of Stolcke and Wu (1992)
extended RAAM to demonstrate the possibility of
using neural networks to perform more sophisti-
cated operations like unification directly upon the
distributed vector representations of hierarchical
113
feature structures. Socher et al. (2011) used mono-
lingual recursive autoencoders for sentiment pre-
diction, with or without parse tree information; this
was perhaps the first use of a RAAM style ap-
proach on a large scale NLP task, albeit mono-
lingual. Scheible and Sch?tze (2013) automat-
ically simplified the monolingual tree structures
generated by recursive autoencoders, validated the
simplified structures via manual evaluation, and
showed that sentiment classification accuracy is
not affected.
2.2 Bilingual related work
The majority of work on learning bilingual dis-
tributed vector representations has not made use of
recursive approaches or hidden contextual or com-
positional structure, as in the bilingual word em-
bedding learning of Klementiev et al. (2012) or the
bilingual phrase embedding learning of Gao et al.
(2014). Schwenk (2012) uses a non-recursive neu-
ral network to predict phrase translation probabil-
ities in conventional phrase-based SMT.
Attempts have been made to generalize the dis-
tributed vector representations of monolingual n-
gram language models, avoiding any hidden con-
textual or hierarchical structure. Working within
the framework of n-gram translation models, Son
et al. (2012) generalize left-to-right monolingual
n-gram models to bilingual n-grams, and study
bilingual variants of class-based n-grams. How-
ever, their model does not allow tackling the chal-
lenge of modeling cross-lingual constituent order,
as TRAAM does; instead it relies on the assump-
tion that some other preprocessor has already man-
aged to accurately re-order the words of the input
sentence into exactly the order of words in the out-
put sentence.
Similarly, generalizations of monolingual SRNs
to the bilingual case have been studied. Zou
et al. (2013) generalize the monolingual recur-
rent NNLM model of Bengio et al. (2009) to
learn bilingual word embeddings using conven-
tional SMTword alignments, and demonstrate that
the resulting embeddings outperform the baselines
in word semantic similarity. They also add a sin-
gle semantic similarity feature induced with bilin-
gual embeddings to a phrase-based SMT log-linear
model, and report improvements in BLEU. Com-
pared to TRAAM, however, they only learn non-
compositional features, with distributed vectors
only representing biterminals (as opposed to bi-
constituents or bilingual subtrees), and so other
mechanisms for combining biterminal scores still
need to be used to handle hierarchical structure,
as opposed to seamlessly being integrated into
the distributed vector representation model. De-
vlin et al. (2014) obtain translation accuracy im-
provements by extending the probabilistic NNLMs
of Bengio et al. (2003), which are used for the
output language, by adding input language con-
text features. Unlike TRAAM, neither of these
approaches symmetrically models the recursive
structure of both the input and output language
sides.
For convolutional network approaches, Kalch-
brenner and Blunsom (2013) use a recurrent prob-
abilistic model to generate a representation of the
source sentence and then generate the target sen-
tence from this representation. This use of in-
put language context to bias translation choices
is in some sense a neural network analogy to
the PSD (phrase sense disambiguation) approach
for context-dependent translation probabilities of
Carpuat and Wu (2007). Unlike TRAAM, the
model does not contain structural constraints, and
permutation of phrases must still be done in con-
ventional PBSMT ?shake?n?bake? style by rely-
ing mostly on a language model (in their case, a
NNLM).
A few applications ofmonolingual RAAM-style
recursive autoencoders to bilingual tasks have also
appeared. For cross-lingual document classifica-
tion, Hermann and Blunsom (2014) use two sep-
arate monolingual fixed vector composition net-
works, one for each language. One provides the
training signal for the other, and training is only
on the embeddings.
Li et al. (2013) described a use of monolingual
recursive autoencoders within maximum entropy
ITGs. They replace their earlier model for pre-
dicting reordering based on the first and the last
tokens in a constituent, by instead using the con-
text vector generated using the recursive autoen-
coder. Only input language context is used, unlike
TRAAM which can use the input and output lan-
guage contexts equally.
Autoencoders have also been applied to SMT in
a very different way by Zhao et al. (2014) but with-
out recursion and not for learning distributed vec-
tor representations of words; rather, they used non-
recursive autoencoders to compress very high-
dimensional bilingual sparse features down to low-
dimensional feature vectors, so that MIRA or PRO
114
could be used to optimize the log-linear model
weights.
3 Representing transduction grammars
with TRAAM
As a recurrent neural network representation of a
transduction grammar, TRAAM learns bilingual
distributed representations that parallel the struc-
tural composition of a transduction grammar. As
with transduction grammars, the learned represen-
tations are symmetric and model structured rela-
tional correlations between the input and output
languages. The induced feature vectors in effect
represent soft categories of cross-lingual relations
and translations. The TRAAM model integrates
elegantly with the transduction grammar formal-
ism and aims to model the compositional struc-
ture of the transduction grammar as opposed to
incorporating external alignment information. It
is straightforward to formulate TRAAMs for arbi-
trary syntax directed transduction grammars; here
we shall describe an example of a TRAAM model
for an inversion transduction grammar (ITG).
Formally, an ITG is a tuple ?N,?,?, R, S?,
where N is a finite nonempty set of nonterminal
symbols,? is a finite set of terminal symbols inL
0
,
? is a finite set of terminal symbols in  L
1
, R is a
finite nonempty set of inversion transduction rules
andS ? N is a designated start symbol. A normal-
form ITG consists of rules in one of the following
four forms:
S ? A, A ? [BC] , A ? ?BC?, A ? e/f
where S ? N is the start symbol, A,B,C ?
N  are nonterminal symbols and e/f  is a biter-
minal. A biterminal is a pair of symbol strings:
?
?
??
?, where at least one of the strings have to
be nonempty. The square and angled brackets sig-
nal straight and inverted order respectively. With
straight order, both the L
0
and the L
1
productions
are generated left-to-right, but with inverted order,
the L
1
production is generated right-to-left.
In the distributed TRAAM representation of the
ITG, we represent each bispan, using a feature vec-
tor v of dimension d that represents a fuzzy encod-
ing of all the nonterminals that could generate it.
This is in contrast to the ITG model where each
nonterminal that generates a bispan has to be enu-
merated separately. Feature vectors correspond-
ing to larger bispans are compositionally generated
from smaller bispans using a compressor network
which takes two feature vectors of dimension d,
corresponding to the smaller bispans and gener-
ates the feature vector of dimension d correspond-
ing to the larger bispan. A single bit correspond-
ing to straight or inverted order is also fed as an
input to the compressor network. The compres-
sor network in TRAAM serves a similar role as
the syntactic rules in the symbolic ITG, but keeps
the encoding fuzzy. Figure 2 shows the straight
and inverted syntactic rules and the correspond-
ing inputs to the compressor network. Modeling
of unary rules (with start symbol on the left hand
side) although similar, is beyond the scope of this
paper.
It is easy to demonstrate that TRAAM mod-
els are capable of representing any symbolic ITG
model. All the nonterminals representing a bispan
can be encoded as a bit vector in the feature vector
of the bispan. Using the universal approximation
theorem of neural networks (Hornik et al., 1989),
an encoder with a single hidden layer can represent
any set of syntactic rules. Similarly, all TRAAM
models can be represented using a symbolic ITG
by assuming a unique nonterminal label for every
feature vector. Therefore, TRAAM and ITGs rep-
resent two equivalent classes of models for repre-
senting compositional bilingual relations.
It is important to note that although both
TRAAM and ITG models might be equivalent, the
fuzzy encoding of nonterminals in TRAAM is suit-
able for modeling the generalizations in bilingual
relationswithout exploding the search space unlike
the symbolic models. This property of TRAAM
makes it attractive for bilingual category learning
and machine translation applications as long as ap-
propriate language bias and objective functions are
determined.
Given our objective of inducing categories of
bilingual relations in an unsupervised manner, we
bias our TRAAM model by using a simple non-
linear activation function to be our compressor,
similar to the monolingual recursive autoencoder
model proposed by Socher et al. (2011). Having a
single layer in our compressor provides the neces-
sary language bias by forcing the network to cap-
ture the generalizations while reducing the dimen-
sions of the input vectors. We use tanh as the non-
linear activation function and the compressor ac-
cepts two vectors c
1
and c
2
of dimension d corre-
sponding to the nonterminals of the smaller bis-
pans and a single bit o corresponding to the in-
115
Figure 1: Example of English-Telugu biparse trees where inversion depends on output language sense.
Compressor 
Compressor 
Reconstructor 
Reconstructor 
o1#=#1# c1# c2# c3#
o2#=#(1#
p1#
o1'# c1'# c2'#
p2#
o2'# p1'# c3'#
Figure 2: Architecture of TRAAM.
version order and generates a vector p of dimen-
sion d corresponding to the larger bispan generated
by combining the two smaller bispans as shown in
Figure 2. The vector p then serves as the input for
the successive combinations of the larger bispan
with other bispans.
p = tanh(W
1
[o; c
1
; c
2
] + b
1
) (1)
whereW
1
and b
1
are the weight matrix and the bias
vector of the encoder network.
To ensure that the computed vector p captures
the fuzzy encodings of its children and the inver-
sion order, we use a reconstructor network which
attempts to reconstruct the inversion order and the
feature vectors corresponding of its children. We
use the error in reconstruction as our objective
function and train our model to minimize the re-
construction error over all the nodes in the biparse
tree. The reconstructor network in our TRAAM
model can be replaced by any other network that
enables the computed feature vector representa-
tions to be optimized for the given task. In our
current implementation, we reconstruct the inver-
sion order o? and the child vectors c?
1
and c?
2
using
another nonlinear activation function as follows:
[o
?
; c
?
1
; c
?
2
] = tanh(W
2
p+ b
2
) (2)
whereW
2
and b
2
are the weight matrix and the bias
vector of the reconstructor network.
4 Bilingual training
4.1 Initialization
The weights and the biases of the compressor and
the reconstructor networks of the TRAAM model
are randomly initialized. Bisegment embeddings
116
corresponding to the leaf nodes (biterminals in the
symbolic ITG notation) in the biparse trees are also
initialized randomly. These constitute the model
parameters and are optimized to minimize our ob-
jective function of reconstruction error. The parse
trees for providing the structural constraints are
generated by a bracketing inversion transduction
grammar (BITG) induced in a purely unsupervised
fashion, according to the algorithm in Saers et al.
(2009). Due to constraints on the training time, we
consider only the Viterbi biparse trees according
to the BITG instead of all the biparse trees in the
forest.
4.2 Computing feature vectors
We compute the feature vectors at each internal
node in the biparse tree, similar to the feedforward
pass in a neural network. We topologically sort all
the nodes in the biparse tree and set the feature vec-
tor of each node in the topologically sorted order
as follows:
? If the node is a leaf node, the feature vector is
the corresponding bisegment embedding.
? Else, the biconstituent embedding corre-
sponding to the internal node is generated us-
ing the feature vectors of the children and the
inversion order using Equation 1. We also
normalize the length of the computed fea-
ture vector so as to prevent the network from
making the biconstituent embedding arbitrar-
ily small in magnitude (Socher et al., 2011).
4.3 Feature vector optimization
We train our current implementation of TRAAM,
by optimizing the model parameters to minimize
an objective function based on the reconstruction
error over all the nodes in the biparse trees. The
objective function is defined as a linear combina-
tion of the l2 norm of the reconstruction error of
the children and the cross-entropy loss of recon-
structing the inversion order. We define the error
at each internal node n as follows:
E
n
=
?
2
|[c
1
; c
2
]? [c
?
1
; c
?
2
]|
2
? (1? ?)
[(1? o) log(1? o?) + (1 + o) log(1 + o?)]
where c
1
, c
2
, o correspond to the left child, right
child and inversion order, c?
1
, c
?
2
, o
? are the respec-
tive reconstructions and ? is the linear weighting
factor. The global objective function J is the sum
of the error function at all internal nodesn in the bi-
parse trees averaged over the total number of sen-
tences T in the corpus. A regularization parameter
? is used on the norm of the model parameters ? to
avoid overfitting.
J =
1
T
?
n
E
n
+ ?||?||
2 (3)
As the bisegment embeddings are also a part of
the model parameters, the optimization objective
is similar to a moving target training objective Ro-
hwer (1990). We use backpropagation with struc-
ture Goller and Kuchler (1996) to compute the gra-
dients efficiently. L-BFGS algorithm Liu and No-
cedal (1989) is used in order to minimize the loss
function.
5 Bilingual representation learning
We expect the TRAAM model to generate clus-
ters over cross-lingual relations similar to RAAM
models on monolingual data. We test this hypoth-
esis by bilingually training our model using a par-
allel English-Telugu blocks world dataset. The
dataset is kept simple to better understand the na-
ture of clusters. Our dataset comprises of com-
mands which involves manipulating different col-
ored objects over different shapes.
5.1 Example
Figure 1 shows the biparse trees for two English-
Telugu sentence pairs. The preposition on in En-
glish translates to ???????(pinunna) and ????(pina) re-
spectively in the first and second sentence pairs be-
cause in the first sentence block is described by its
position on the square, whereas in the second sen-
tence block is the subject and square is the object.
Since Telugu is a language with an SOV structure,
the verbs ????(vunchu) and ?????(teesuko) occur at
the end for both sentences.
The sentences in 1 illustrate the importance of
modeling bilingual relations simultaneously in-
stead of focusing only on the input or output lan-
guage as the cross-lingual structural relations are
sensitive to both the input and output language
context. For example, the constituent whose input
side is block on the square, the corresponding output
language tree structure is determined by whether
or not on is translated to ??????? (pinunna) or ????(pina).
In symbolic frameworks such as ITGs, such
relations are encoded using different nontermi-
nal categories. However, inducing such cate-
117
Figure 3: Clustering of biconstituents in the Telugu-English data.
gories within a symbolic framework in an un-
supervised manner creates extremely challenging
combinatorial scaling issues. TRAAM models
are a promising approach for tackling this prob-
lem, since the vector representations learned us-
ing the TRAAM model inherently yield soft syn-
tactic category membership properties, despite be-
ing trained only with the unlabeled structural con-
straints of simple BITG-style data.
5.2 Biconstituent clustering
The soft membership properties of learned dis-
tributed vector representations can be explored
via cluster analysis. To illustrate, we trained a
TRAAM network bilingually using the algorithm
in Section 4, and obtained feature vector represen-
tations for each unique biconstituent. Clustering
the obtained feature vectors reveals emergence of
fuzzy nonterminal categories, as shown in Figure
3. It is important to note that each point in the
vector space corresponds to a tree-structured bi-
constituent as opposed to merely a flat bilingual
phrase, as same surface forms with different tree
structures will have different vectors.
As the full cluster tree is too unwieldy, Figure
4 zooms in to shows an enlarged version of a por-
tion of the clustering, alongwith the corresponding
bracketed bilingual structures. One can observe
that the cluster represents the biconstituents that
describe the object by its position on another ob-
ject. We can deduce this from the fact that only a
single sense of on/???????(pinnuna) seems to be occur-
ing in all the biconstituents of the cluster. Manual
inspection of other clusters reveals such similari-
ties despite noise expected to be introduced by the
sparsity of our dataset.
6 Conclusion
We have introduced a fully bilingual generaliza-
tion of Pollack?s (1990) monolingual Recursive
Auto-Associative Memory neural network model,
TRAAM, in which each distributed vector repre-
sents a bilingual constituent?i.e., an instance of
a transduction rule, which specifies a relation be-
tween two monolingual constituents and how their
subconstituents should be permuted. Bilingual ter-
minals are special cases of bilingual constituents,
where a vector represents either (1) a bilingual to-
ken?a token-to-token or ?word-to-word? transla-
tion rule?or (2) a bilingual segment?a segment-
to-segment or ?phrase-to-phrase? translation rule.
TRAAMs can be used for arbitrary rank SDTGs
(syntax-directed transduction grammars, a.k.a.
synchronous context-free grammars). Although
our discussions in this paper have focused on bi-
parse trees from SDTGs in a 2-normal form, which
by definition are ITGs due to the binary rank,
nothing prevents TRAAMs from being applied to
higher-rank transduction grammars.
We believe TRAAMs are worth detailed ex-
ploration as their intrinsic properties address key
problems in bilingual grammar induction and sta-
118
Figure 4: Typical zoomed view into the Telugu-English biconstituent clusters from Figure 3.
tistical machine translation?their sensitivity to
both input and output language context means that
the learned vector representations tend to reflect
the similarity of bilingual rather than monolingual
constituents, which is what is needed to induce dif-
ferentiated bilingual nonterminal categories.
7 Acknowledgments
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
References
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Yoshua Bengio, R?jean Ducharme, Pascal Vin-
cent, and Christian Jauvin. A neural probabilis-
tic language model. Journal of Machine Learn-
ing Research, 3:1137?1155, 2003.
Yoshua Bengio, J?r?me Louradour, Ronan Col-
lobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th annual international
conference on machine learning, pages 41?48.
ACM, 2009.
Marine Carpuat and Dekai Wu. Context-
dependent phrasal translation lexicons for sta-
tistical machine translation. In 11th Machine
Translation Summit (MT Summit XI), pages 73?
80, 2007.
Lonnie Chrisman. Learning recursive distributed
representations for holistic computation. Con-
nection Science, 3(4):345?366, 1991.
Ronan Collobert and Jason Weston. A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
In Proceedings of the 25th International Con-
ference on Machine Learning, ICML ?08, pages
160?167, New York, NY, USA, 2008. ACM.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John
Makhoul. Fast and robust neural network joint
models for statistical machine translation. In
119
52nd Annual Meeting of the Association for
Computational Linguistics, 2014.
Jeffrey L Elman. Finding structure in time. Cog-
nitive science, 14(2):179?211, 1990.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. Learning continuous phrase represen-
tations for translation modeling. In 52nd Annual
Meeting of the Association for Computational
Linguistics (Short Papers), 2014.
Christoph Goller and Andreas Kuchler. Learn-
ing task-dependent distributed representations
by backpropagation through structure. In Neu-
ral Networks, 1996., IEEE International Con-
ference on, volume 1, pages 347?352. IEEE,
1996.
Karl Moritz Hermann and Phil Blunsom. Multi-
lingual models for compositional distributed se-
mantics. In 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, volume
abs/1404.4641, 2014.
Kurt Hornik, Maxwell Stinchcombe, and Hal-
bert White. Multilayer feedforward networks
are universal approximators. Neural networks,
2(5):359?366, 1989.
Nal Kalchbrenner and Phil Blunsom. Recurrent
continuous translation models. In EMNLP,
pages 1700?1709, 2013.
Alexandre Klementiev, Ivan Titov, and Binod
Bhattarai. Inducing crosslingual distributed
representations of words. In 24th Interna-
tional Conference on Computational Linguistics
(COLING 2012). Citeseer, 2012.
Honglak Lee, Roger Grosse, Rajesh Ranganath,
and Andrew Y Ng. Convolutional deep be-
lief networks for scalable unsupervised learning
of hierarchical representations. In Proceedings
of the 26th Annual International Conference
on Machine Learning, pages 609?616. ACM,
2009.
Peng Li, Yang Liu, and Maosong Sun. Recur-
sive autoencoders for itg-based translation. In
EMNLP, pages 567?577, 2013.
Dong C Liu and Jorge Nocedal. On the limited
memory bfgs method for large scale optimiza-
tion. Mathematical programming, 45(1-3):503?
528, 1989.
Jordan B Pollack. Recursive distributed represen-
tations. Artificial Intelligence, 46(1):77?105,
1990.
Richard Rohwer. The ?moving targets?training al-
gorithm. In Neural Networks, pages 100?109.
Springer, 1990.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT?09), pages 29?32,
Paris, France, October 2009.
Christian Scheible and Hinrich Sch?tze. Cutting
recursive autoencoder trees. In 1st International
Conference on Learning Representations (ICLR
2013), Scottsdale, Arizona, May 2013.
Holger Schwenk. Continuous-space language
models for statistical machine translation. In
The Prague Bulletin of Mathematical Linguis-
tics, volume 93, pages 137?146, 2010.
Holger Schwenk. Continuous space transla-
tion models for phrase-based statistical machine
translation. In Proceedings of COLING 2012:
Posters, pages 1071??1080. Citeseer, 2012.
Richard Socher, Jeffrey Pennington, Eric H
Huang, Andrew Y Ng, and Christopher D Man-
ning. Semi-supervised recursive autoencoders
for predicting sentiment distributions. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages
151?161. Association for Computational Lin-
guistics, 2011.
Le Hai Son, Alexandre Allauzen, and Fran?ois
Yvon. Continuous space translationmodels with
neural networks. In Proceedings of the 2012
conference of the north american chapter of the
association for computational linguistics: Hu-
man language technologies, pages 39?48. As-
sociation for Computational Linguistics, 2012.
Andreas Stolcke and Dekai Wu. Tree match-
ing with recursive distributed representations.
In AAAI 1992 Workshop on Integrating Neu-
ral and Symbolic Processes?The Cognitive Di-
mension, 1992.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, 1997.
Bing Zhao, Yik-Cheung Tam, and Jing Zheng.
An autoencoder with bilingual sparse features
for improved statistical machine translation. In
120
IEEE International Conference on Acoustic,
Speech and Signal Processing (ICASSP), 2014.
Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher DManning. Bilingual word embed-
dings for phrase-based machine translation. In
EMNLP, pages 1393?1398, 2013.
121
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144?153,
Dublin, Ireland, August 23, 2014.
Lexical Access Preference and Constraint Strategies for Improving
Multiword Expression Association within Semantic MT Evaluation
Dekai Wu Lo Chi-kiu Markus Saers
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{dekai|jackielo|masaers|dekai}@cs.ust.hk
Abstract
We examine lexical access preferences and constraints in computing multiword expression asso-
ciations from the standpoint of a high-impact extrinsic task-based performance measure, namely
semantic machine translation evaluation. In automated MT evaluation metrics, machine transla-
tions are compared against human reference translations, which are almost never worded exactly
the sameway except in the most trivial of cases. Because of this, one of the most important factors
in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative
lexical realizations of the same multiword expressions in semantic role fillers. Our results com-
paring bag-of-words, maximum alignment, and inversion transduction grammars indicate that
cognitively motivated ITGs provide superior lexical access characteristics for multiword expres-
sion associations, leading to state-of-the-art improvements in correlation with human adequacy
judgments.
1 Introduction
We investigate lexical access strategies in the context of computing multiword expression associations
within automatic semantic MT evaluation metrics?a high-impact real-world extrinsic task-based per-
formance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious
issues in machine translation and automatic MT evaluation; there are simply too many forms to enumer-
ate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a
decade and a half, but until recently little has been done to use lexical semantics as the main foundation
for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nie?en et al.,
2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference
and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and
Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation
adequacy.
Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et
al., 2012; Lo andWu, 2013b), have instead directly couchedMT evaluation in the more cognitive terms of
semantic frames, by measuring the degree to which the basic event structure is preserved by translation?
the ?who did what to whom, for whom, when, where, how and why? (Pradhan et al., 2004)?emphasizing
that a good translation is one that can successfully be understood by a human. Across a variety of language
pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both n-
gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and
METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et
al., 2006), WER (Nie?en et al., 2000), and TER (Snover et al., 2006) when evaluatingMT output (Lo and
Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach??ek and Bojar, 2013). Furthermore, tuning
the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/
144
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL
decided to drop the predicate.
adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English
and Chinese) and different genres (formal newswire text, informal web forum text and informal public
speech).
Because of this, we have chosen to run our lexical association experiments in the context of the neces-
sity of recognizingmatching semantic role fillers, approximately 85%ofwhich aremultiword expressions
in our data, the overwhelming majority of which would not be enumerated within conventional lexicons.
We compare four common lexical access approaches to aggregation, preferences, and constraints: bag-
of-words, two different types of maximal alignment, and inversion transduction grammar based methods.
2 Background
The MEANT metric measures weighted f-scores over corresponding semantic frames and role fillers
in the reference and machine translations. Whereas HMEANT uses human annotation, the automatic
versions of MEANT instead replace humans with automatic SRL and alignment algorithms. MEANT
typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human ade-
quacy judgment, and is relatively easy to port to other languages, requiring only an automatic semantic
parser and a monolingual corpus of the output language, which is used to gauge lexical similarity between
the semantic role fillers of the reference and translation. More precisely, MEANT computes scores as
follows:
1. Apply an automatic shallow semantic parser to both the references and MT output. (Figure 1 shows
examples of automatic shallow semantic parses on both reference and MT.)
2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between
the references and MT output according to the lexical similarities of the predicates.
3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to
align the arguments between the reference and MT output according to the lexical similarity of role
fillers.
4. Compute the weighted f-score over the matching role labels of these aligned predicates and role
fillers according to the following definitions:
145
q0
i,j
? ARG j of aligned frame i in MT
q
1
i,j
? ARG j of aligned frame i in REF
w
0
i
?
#tokens filled in aligned frame i of MT
total #tokens in MT
w
1
i
?
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ? weight of similarity of predicates
w
j
? weight of similarity of ARG j
e
i,pred ? the pred of the aligned frame i of the machine translation
f
i,pred ? the pred of the aligned frame i of the reference translation
e
i,j
? the ARG j of the aligned frame i of the machine translation
f
i,j
? the ARG j of the aligned frame i of the reference translation
s(e, f) = lexical similarity of token e and f
prece,f =
?
e?e max
f?f
s(e, f)
| e |
rece,f =
?
f?f max
e?e
s(e, f)
| f |
precision =
?
i
w
0
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
0
i,j
|
?
i
w
0
i
recall =
?
i
w
1
i
wpredsi,pred+
?
j
w
j
s
i,j
wpred+
?
j
w
j
|q
1
i,j
|
?
i
w
1
i
MEANT = 2 ? precision ? recallprecision + recall
where the possible approaches to defining the lexical associations s
i,pred and si,j are discussed in the
following section. q0
i,j
and q1
i,j
are the argument of type j in frame i in MT and REF, respectively. w0
i
and w1
i
are the weights for frame i in MT and REF, respectively. These weights estimate the degree
of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of
the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between
the reference translations and the MT output. There is a total of 12 weights for the set of semantic role
labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised
estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and
Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using
relative frequency of each semantic role label in the references and thus UMEANT is useful when human
judgments on adequacy of the development set are unavailable.
3 Comparison of multiword expression association approaches
To assess alternative lexical access preferences and constraints for computing multiword expression
associations, we now consider four alternative approaches to defining the lexical similarities s
i,pred and
s
i,j
, all of which employ a standard context vector model of the individual words/tokens in the multiword
expression arguments between the reference and machine translations, as descibed by Lo et al. (2012)
and Tumuluru et al. (2012).
3.1 Bag of words (geometric mean)
The original MEANT approaches employed standard a bag-of-words strategy for lexical association.
This baseline approach applies no alignment constraints on multiword expressions:
s
i,pred = e
?
e?e
i,pred
?
f?f
i,pred
lg(s(e,f))
|e
i,pred|?|fi,pred|
s
i,j
= e
?
e?e
i,j
?
f?f
i,j
lg(s(e,f))
|e
i,j
|?|f
i,j
|
146
3.2 Maximum alignment (precision-recall average)
In the first maximum alignment based approach we will consider, the definitions of s
i,pred and si,j are
inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length.
s
i,pred =
1
2
(prece
i,pred,fi,pred + recei,pred,fi,pred)
s
i,j
=
1
2
(prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
)
3.3 Maximum alignment (f-score)
The second of the maximum alignment based approaches replaces the above linear averaging of pre-
cision and recall with a proper f-score. Although this is less consistent with the previous literature, such
as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT,
and thus we include it in our comparison as a variant of the maximum alignment strategy.
s
i,pred =
2 ? prece
i,pred,fi,pred ? recei,pred,fi,pred
prece
i,pred,fi,pred + recei,pred,fi,pred
s
i,j
=
2 ? prece
i,j
,f
i,j
? rece
i,j
,f
i,j
prece
i,j
,f
i,j
+ rece
i,j
,f
i,j
3.4 Inversion transduction grammar based
There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve
the accuracy of MT evaluation metrics?despite (1) long empirical evidence the vast majority of transla-
tion patterns between human languages can be accommodated within ITG constraints, and (2) the obser-
vation thatmost current state-of-the-art SMT systems employ ITG decoders. Especially when considering
semanticMTmetrics, ITGs would seem to be a natural strategy for multiword expression association for
several cognitively motivated reasons, having to do with language universal properties of cross-linguistic
semantic frame structure.
To begin with, it is quite natural to think of sentences as having been generated from an abstract concept
using a rewriting system: a stochastic grammar predicts how frequently any particular realization of the
abstract concept will be generated. The bilingual analogy is a transduction grammar generating a pair
of possible realizations of the same underlying concept. Stochastic transduction grammars predict how
frequently a particular pair of realizations will be generated, and thus represent a good way to evaluate
how well a pair of sentences correspond to each other.
The particular class of transduction grammars known as ITGs tackle the problem that the (bi)parsing
complexity for general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By
constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted
reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low
end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to
have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and
space (O
(
n
6
)
time and O
(
n
4
)
space). It is also possible to do approximate biparsing in O
(
n
3
)
time
(Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an
ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have also been directly shown to be more than sufficient
to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This
language universal property has an evolutionary explanation in terms of computational efficiency and
cognitive load for language learnability and interpretability (Wu, 2014).
ITGs are thus an appealing alternative for evaluating the possible links between both semantic role
fillers in different languages as well as the predicates, and how these parts fit together to form entire
semantic frames. We believe that ITGs are not only capable of generating the desired structural corre-
spondences between the semantic structures of two languages, but also provide meaningful constraints
to prevent alignments from wandering off in the wrong direction.
Following this reasoning, alternate definitions of s
i,pred and si,j can be constructed in terms of brack-
eting ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated
147
nonterminal category (Wu, 1995a). The idea is to attack a potential weakness of the foregoing three
lexical association strategies, namely that word/token alignments between the reference and machine
translations are severely underconstrained. No bijectivity or permutation restrictions are applied, even
between compositional segments where this should be natural. This can cause multiword expressions of
semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inver-
sion transduction grammar can potentially better constrain permissible token alignment patterns between
aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed
permutations between semantic role fillers across the reference and machine translations for a sample
sentence from the evaluation data.
In this approach, both alignment and scoring are performed utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define s
i,pred and
s
i,j
as follows.
s
i,pred = lg?1
?
?
lg
(
P
(
A ?? e
i,pred/fi,pred|G
))
max(| e
i,pred |, | fi,pred |)
?
?
s
i,j
= lg?1
?
?
lg
(
P
(
A ?? e
i,j
/f
i,j
|G
))
max(| e
i,j
|, | f
i,j
|)
?
?
where
G ? ?{A} ,W0,W1,R,A?
R ? {A ? [AA] ,A ? ?AA?,A ? e/f}
p ([AA] |A) = p (?AA?|A) = 1
p (e/f |A) = s(e, f)
Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with
e ? W
0
? {?} denoting a token in the MT output (or the null token) and f ? W1 ? {?} denoting
a token in the reference translation (or the null token). The rule probability (or more accurately, rule
weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is
defined by MEANT?s lexical similarity measure on English Gigaword context vectors. To calculate the
inside probability (or more accurately, inside score) of a pair of segments, P
(
A ?? e/f|G
)
, we use the
algorithm described in Saers et al. (2009). Given this, s
i,pred and si,j now represent the length normalized
BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and
machine translations.
4 Experiments
In this section we discuss experiments comparing the four alternative lexical access preference and
constraint strategies.
4.1 Experimental setup
We compared using the DARPA GALE P2.5 Chinese-English translation test set, as used in Lo and
Wu (2011a). The corpus includes the Chinese input sentences, each accompanied by an English reference
translation and three participating state-of-the-art MT systems? output.
We computed sentence-level correlations following the benchmark assessment procedure used by
WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; Mach??ek and Bojar,
2013), which use Kendall?s ? correlation coefficient, to evaluate the correlation of evaluation metrics
against human judgment on ranking the translation adequacy of the three systems? output. A higher
value for Kendall?s ? indicates more similarity to the human adequacy rankings by the evaluation met-
rics. The range of possible values of Kendall?s ? correlation coefficient is [-1, 1], where 1 means the
148
Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE
P2.5 data. For reference, the human HMEANT upper bound is 0.53?so the fully automatic ITG based
MEANT approximation is not far from closing the gap.
Kendall correlation
MEANT + ITG based 0.51
MEANT + maximum alignment (f-score) 0.48
MEANT + maximum alignment (average of precision & recall) 0.46
MEANT + bag of words (geometric mean) 0.38
NIST 0.29
METEOR 0.20
BLEU 0.20
TER 0.20
PER 0.20
CDER 0.12
WER 0.10
systems are ranked in the same order as the human judgment by the evaluation metric; and -1 means the
systems are ranked in the reverse order as human judgment by the evaluation metric.
For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler
was used to automatically predict semantic parses.
4.2 Results and discussion
The sentence-level correlations in Table 1 show that the ITG based strategy outperforms other auto-
matic metrics in correlation with human adequacy judgment. Note that this was achieved with no tuning
whatsoever of the rule weights (suggesting that the performance could be further improved in the future
by slightly optimizing the ITG weights).
The ITG based strategy shows 3 points improvement over the next best strategy, which is maximal
alignment under f-score aggregation. The ITG based approach produces much higher HAJ correlations
than any of the other metrics.
In fact, the ITG based strategy even comes within a few points of the human upper bound bench-
mark HAJ correlations computed using the human labeled semantic frames and alignments used in the
HMEANT.
Data analysis reveals two reasons that the ITG based strategy correlates with human adequacy judge-
ment more closely than the other approaches. First, BITG constraints indeed provide more accurate
phrasal similarity aggregation, compared to the naive bag-of-words based heuristics. Similar results
have been observed while trying to estimate word alignment probabilities where BITG constraints out-
performed alignments from GIZA++ (Saers and Wu, 2009). Secondly, the permutation and bijectivity
constraints enforced by the ITG provide better leverage to reject token alignments when they are not
appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The
ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave
tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not
simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed
that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered
in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better
fits the semantic structure.
5 Conclusion
We have compared four alternative lexical access strategies for aggregation, preferences, and con-
straints in scoringmultiword expression associations that are far too numerous to be explicitly enumerated
in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words,
149
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
biparse tree and alignment matrix depictions, for the Chinese input sentence ????????????
???????? Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
150
two maximum alignment based approaches, and an inversion transduction grammar based approach.
Controlled experiments within the MEANT semantic MT evaluation framework shows that the cog-
nitively motivated ITG based strategy achieves significantly higher correlation with human adequacy
judgments of MT output quality than the more typically used lexical association approaches. The results
show how to improve upon previous research showing that MEANT?s explicit use of semantic frames
leads to state-of-the-art automatic MT evaluation, by aligning and scoring semantic frames under a sim-
ple, consistent ITG that provides empirically informative permutation and bijectivity biases, instead of
more naive maximal alignment or bag-of-words assumptions.
Cognitive studies of the lexicon are often described using intrinsic measures of quality. Our exper-
iments complement this by situating the empirical comparisons within extrinsic real-world task-based
performance measures. We believe that progress can be accelerated via a combination of intrinsic and
extrinsic measures of lexicon acquisition and access models.
Acknowledgments
This material is based upon work supported in part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract nos. HR0011-12-C-0014 and HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-0023; by the European Union under the FP7
grant agreement no. 287658; and by the Hong Kong Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA,
the EU, or RGC.
References
Karteek Addanki, Chi-kiu Lo, Markus Saers, and Dekai Wu. LTG vs. ITG coverage of cross-lingual verb
frame alternations. In 16th Annual Conference of the European Association for Machine Translation
(EAMT-2012), Trento, Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation, and Compiling. Prentice-Halll,
Englewood Cliffs, New Jersey, 1972.
Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the European Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third Workshop on Statistical Machine Translation (WMT-
08), 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Pryzbocki, and Omar Zaidan.
Findings of the 2010 joint workshop on statistical machine translation and metrics for machine trans-
lation. In Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (WMT10), pages
17?53, Uppsala, Sweden, 15-16 July 2010.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan. Findings of the 2011
Workshop on Statistical Machine Translation. In 6th Workshop on Statistical Machine Translation
(WMT 2011), 2011.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. Find-
ings of the 2012 workshop on statistical machine translation. In 7th Workshop on Statistical Machine
Translation (WMT 2012), pages 10?51, 2012.
151
George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence
statistics. In The second international conference on Human Language Technology Research
(HLT ?02), San Diego, California, 2002.
Philipp Koehn and Christof Monz. Manual and automatic evaluation of machine translation between
european languages. InWorkshop on Statistical Machine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL-2006), 2006.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluat-
ing translation utility based on semantic roles. In 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. supervised weight estimation for semantic MT evaluation
metrics. In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6),
2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and Dekai Wu. Improving machine translation by training
against an automatic semantic frame based evaluationmetric. In 51st AnnualMeeting of the Association
for Computational Linguistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Improving machine translation into Chinese by tuning
against Chinese MEANT. In International Workshop on Spoken Language Translation (IWSLT 2013),
2013.
Matou? Mach??ek and Ond?ej Bojar. Results of the WMT13 metrics shared task. In Eighth Workshop
on Statistical Machine Translation (WMT 2013), Sofia, Bulgaria, August 2013.
I. DanMelamed. Automatic construction of clean broad-coverage translation lexicons. In 2nd Conference
of the Association for Machine Translation in the Americas (AMTA-1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first National Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Hermann Ney. A evaluation tool for machine transla-
tion: Fast evaluation forMT research. In The Second International Conference on Language Resources
and Evaluation (LREC 2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics
(ACL-02), pages 311?318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow semantic
parsing using support vector machines. In Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), 2004.
Markus Saers and Dekai Wu. Improving phrase-based translation via word alignments from stochastic
inversion transduction grammars. In Third Workshop on Syntax and Structure in Statistical Translation
(SSST-3), pages 28?36, Boulder, Colorado, June 2009.
152
Markus Saers, JoakimNivre, and DekaiWu. Learning stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies
(IWPT?09), pages 29?32, Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and JohnMakhoul. A study of trans-
lation edit rate with targeted human annotation. In 7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages 223?231, Cambridge, Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lex-
ical similarity of semantic role fillers for automatic semantic MT evaluation. In 26th Pacific Asia
Conference on Language, Information, and Computation (PACLIC 26), 2012.
Dekai Wu. An algorithm for simultaneously bracketing parallel texts by aligning words. In 33rd An-
nual Meeting of the Association for Computational Linguistics (ACL 95), pages 244?251, Cambridge,
Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In Third Annual Workshop
on Very Large Corpora (WVLC-3), pages 69?81, Cambridge, Massachusetts, June 1995.
Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, 1997.
Dekai Wu. The magic number 4: Evolutionary pressures on semantic frame structure. In 10th Interna-
tional Conference on the Evolution of Language (Evolang X), Vienna, Apr 2014.
Richard Zens and Hermann Ney. A comparative study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),
pages 144?151, Stroudsburg, Pennsylvania, 2003.
153
