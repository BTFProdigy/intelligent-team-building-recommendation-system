Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 656?664,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Conundrums in Noun Phrase Coreference Resolution:
Making Sense of the State-of-the-Art
Veselin Stoyanov
Cornell University
Ithaca, NY
ves@cs.cornell.edu
Nathan Gilbert
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
Claire Cardie
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Ellen Riloff
University of Utah
Salt Lake City, UT
riloff@cs.utah.edu
Abstract
We aim to shed light on the state-of-the-art in NP
coreference resolution by teasing apart the differ-
ences in the MUC and ACE task definitions, the as-
sumptions made in evaluation methodologies, and
inherent differences in text corpora. First, we exam-
ine three subproblems that play a role in coreference
resolution: named entity recognition, anaphoric-
ity determination, and coreference element detec-
tion. We measure the impact of each subproblem on
coreference resolution and confirm that certain as-
sumptions regarding these subproblems in the eval-
uation methodology can dramatically simplify the
overall task. Second, we measure the performance
of a state-of-the-art coreference resolver on several
classes of anaphora and use these results to develop
a quantitative measure for estimating coreference
resolution performance on new data sets.
1 Introduction
As is common for many natural language process-
ing problems, the state-of-the-art in noun phrase
(NP) coreference resolution is typically quantified
based on system performance on manually anno-
tated text corpora. In spite of the availability of
several benchmark data sets (e.g. MUC-6 (1995),
ACE NIST (2004)) and their use in many formal
evaluations, as a field we can make surprisingly
few conclusive statements about the state-of-the-
art in NP coreference resolution.
In particular, it remains difficult to assess the ef-
fectiveness of different coreference resolution ap-
proaches, even in relative terms. For example, the
91.5 F-measure reported by McCallum and Well-
ner (2004) was produced by a system using perfect
information for several linguistic subproblems. In
contrast, the 71.3 F-measure reported by Yang et
al. (2003) represents a fully automatic end-to-end
resolver. It is impossible to assess which approach
truly performs best because of the dramatically
different assumptions of each evaluation.
Results vary widely across data sets. Corefer-
ence resolution scores range from 85-90% on the
ACE 2004 and 2005 data sets to a much lower 60-
70% on the MUC 6 and 7 data sets (e.g. Soon et al
(2001) and Yang et al (2003)). What accounts for
these differences? Are they due to properties of
the documents or domains? Or do differences in
the coreference task definitions account for the dif-
ferences in performance? Given a new text collec-
tion and domain, what level of performance should
we expect?
We have little understanding of which aspects
of the coreference resolution problem are handled
well or poorly by state-of-the-art systems. Ex-
cept for some fairly general statements, for exam-
ple that proper names are easier to resolve than
pronouns, which are easier than common nouns,
there has been little analysis of which aspects of
the problem have achieved success and which re-
main elusive.
The goal of this paper is to take initial steps to-
ward making sense of the disparate performance
results reported for NP coreference resolution. For
our investigations, we employ a state-of-the-art
classification-based NP coreference resolver and
focus on the widely used MUC and ACE corefer-
ence resolution data sets.
We hypothesize that performance variation
within and across coreference resolvers is, at least
in part, a function of (1) the (sometimes unstated)
assumptions in evaluation methodologies, and (2)
the relative difficulty of the benchmark text cor-
pora. With these in mind, Section 3 first examines
three subproblems that play an important role in
coreference resolution: named entity recognition,
anaphoricity determination, and coreference ele-
ment detection. We quantitatively measure the im-
pact of each of these subproblems on coreference
resolution performance as a whole. Our results
suggest that the availability of accurate detectors
for anaphoricity or coreference elements could
substantially improve the performance of state-of-
the-art resolvers, while improvements to named
entity recognition likely offer little gains. Our re-
sults also confirm that the assumptions adopted in
656
MUC ACE
Relative Pronouns no yes
Gerunds no yes
Nested non-NP nouns yes no
Nested NEs no GPE & LOC premod
Semantic Types all 7 classes only
Singletons no yes
Table 1: Coreference Definition Differences for MUC and
ACE. (GPE refers to geo-political entities.)
some evaluations dramatically simplify the resolu-
tion task, rendering it an unrealistic surrogate for
the original problem.
In Section 4, we quantify the difficulty of a
text corpus with respect to coreference resolution
by analyzing performance on different resolution
classes. Our goals are twofold: to measure the
level of performance of state-of-the-art corefer-
ence resolvers on different types of anaphora, and
to develop a quantitative measure for estimating
coreference resolution performance on new data
sets. We introduce a coreference performance pre-
diction (CPP) measure and show that it accurately
predicts the performance of our coreference re-
solver. As a side effect of our research, we pro-
vide a new set of much-needed benchmark results
for coreference resolution under common sets of
fully-specified evaluation assumptions.
2 Coreference Task Definitions
This paper studies the six most commonly used
coreference resolution data sets. Two of those are
from the MUC conferences (MUC-6, 1995; MUC-
7, 1997) and four are from the Automatic Con-
tent Evaluation (ACE) Program (NIST, 2004). In
this section, we outline the differences between the
MUC and ACE coreference resolution tasks, and
define terminology for the rest of the paper.
Noun phrase coreference resolution is the pro-
cess of determining whether two noun phrases
(NPs) refer to the same real-world entity or con-
cept. It is related to anaphora resolution: a NP is
said to be anaphoric if it depends on another NP
for interpretation. Consider the following:
John Hall is the new CEO. He starts on Monday.
Here, he is anaphoric because it depends on its an-
tecedent, John Hall, for interpretation. The two
NPs also corefer because each refers to the same
person, JOHN HALL.
As discussed in depth elsewhere (e.g. van
Deemter and Kibble (2000)), the notions of coref-
erence and anaphora are difficult to define pre-
cisely and to operationalize consistently. Further-
more, the connections between them are extremely
complex and go beyond the scope of this paper.
Given these complexities, it is not surprising that
the annotation instructions for the MUC and ACE
data sets reflect different interpretations and sim-
plifications of the general coreference relation. We
outline some of these differences below.
Syntactic Types. To avoid ambiguity, we will
use the term coreference element (CE) to refer
to the set of linguistic expressions that participate
in the coreference relation, as defined for each of
the MUC and ACE tasks.1 At times, it will be im-
portant to distinguish between the CEs that are in-
cluded in the gold standard ? the annotated CEs
? from those that are generated by the corefer-
ence resolution system ? the extracted CEs.
At a high level, both the MUC and ACE eval-
uations define CEs as nouns, pronouns, and noun
phrases. However, the MUC definition excludes
(1) ?nested? named entities (NEs) (e.g. ?Amer-
ica? in ?Bank of America?), (2) relative pronouns,
and (3) gerunds, but allows (4) nested nouns (e.g.
?union? in ?union members?). The ACE defini-
tion, on the other hand, includes relative pronouns
and gerunds, excludes all nested nouns that are not
themselves NPs, and allows premodifier NE men-
tions of geo-political entities and locations, such
as ?Russian? in ?Russian politicians?.
Semantic Types. ACE restricts CEs to entities
that belong to one of seven semantic classes: per-
son, organization, geo-political entity, location, fa-
cility, vehicle, and weapon. MUC has no semantic
restrictions.
Singletons. The MUC data sets include annota-
tions only for CEs that are coreferent with at least
one other CE. ACE, on the other hand, permits
?singleton? CEs, which are not coreferent with
any other CE in the document.
These substantial differences in the task defini-
tions (summarized in Table 1) make it extremely
difficult to compare performance across the MUC
and ACE data sets. In the next section, we take a
closer look at the coreference resolution task, ana-
lyzing the impact of various subtasks irrespective
of the data set differences.
1We define the term CE to be roughly equivalent to (a)
the notion of markable in the MUC coreference resolution
definition and (b) the structures that can be mentions in the
descriptions of ACE.
657
3 Coreference Subtask Analysis
Coreference resolution is a complex task that
requires solving numerous non-trivial subtasks
such as syntactic analysis, semantic class tagging,
pleonastic pronoun identification and antecedent
identification to name a few. This section exam-
ines the role of three such subtasks ? named en-
tity recognition, anaphoricity determination, and
coreference element detection ? in the perfor-
mance of an end-to-end coreference resolution
system. First, however, we describe the corefer-
ence resolver that we use for our study.
3.1 The RECONCILEACL09 Coreference
Resolver
We use the RECONCILE coreference resolution
platform (Stoyanov et al, 2009) to configure a
coreference resolver that performs comparably to
state-of-the-art systems (when evaluated on the
MUC and ACE data sets under comparable as-
sumptions). This system is a classification-based
coreference resolver, modeled after the systems of
Ng and Cardie (2002b) and Bengtson and Roth
(2008). First it classifies pairs of CEs as coreferent
or not coreferent, pairing each identified CE with
all preceding CEs. The CEs are then clustered
into coreference chains2 based on the pairwise de-
cisions. RECONCILE has a pipeline architecture
with four main steps: preprocessing, feature ex-
traction, classification, and clustering. We will
refer to the specific configuration of RECONCILE
used for this paper as RECONCILEACL09.
Preprocessing. The RECONCILEACL09 prepro-
cessor applies a series of language analysis tools
(mostly publicly available software packages) to
the source texts. The OpenNLP toolkit (Baldridge,
J., 2005) performs tokenization, sentence splitting,
and part-of-speech tagging. The Berkeley parser
(Petrov and Klein, 2007) generates phrase struc-
ture parse trees, and the de Marneffe et al (2006)
system produces dependency relations. We em-
ploy the Stanford CRF-based Named Entity Rec-
ognizer (Finkel et al, 2004) for named entity
tagging. With these preprocessing components,
RECONCILEACL09 uses heuristics to correctly ex-
tract approximately 90% of the annotated CEs for
the MUC and ACE data sets.
Feature Set. To achieve roughly state-of-the-
art performance, RECONCILEACL09 employs a
2A coreference chain refers to the set of CEs that refer to
a particular entity.
dataset docs CEs chains CEs/ch tr/tst split
MUC6 60 4232 960 4.4 30/30 (st)
MUC7 50 4297 1081 3.9 30/20 (st)
ACE-2 159 2630 1148 2.3 130/29 (st)
ACE03 105 3106 1340 2.3 74/31
ACE04 128 3037 1332 2.3 90/38
ACE05 81 1991 775 2.6 57/24
Table 2: Dataset characteristics including the number of
documents, annotated CEs, coreference chains, annotated
CEs per chain (average), and number of documents in the
train/test split. We use st to indicate a standard train/test split.
fairly comprehensive set of 61 features introduced
in previous coreference resolution systems (see
Bengtson and Roth (2008)). We briefly summarize
the features here and refer the reader to Stoyanov
et al (2009) for more details.
Lexical (9): String-based comparisons of the two
CEs, such as exact string matching and head noun
matching.
Proximity (5): Sentence and paragraph-based
measures of the distance between two CEs.
Grammatical (28): A wide variety of syntactic
properties of the CEs, either individually or as a
pair. These features are based on part-of-speech
tags, parse trees, or dependency relations. For ex-
ample: one feature indicates whether both CEs are
syntactic subjects; another indicates whether the
CEs are in an appositive construction.
Semantic (19): Capture semantic information
about one or both NPs such as tests for gender and
animacy, semantic compatibility based on Word-
Net, and semantic comparisons of NE types.
Classification and Clustering. We configure
RECONCILEACL09 to use the Averaged Percep-
tron learning algorithm (Freund and Schapire,
1999) and to employ single-link clustering (i.e.
transitive closure) to generate the final partition-
ing.3
3.2 Baseline System Results
Our experiments rely on the MUC and ACE cor-
pora. For ACE, we use only the newswire portion
because it is closest in composition to the MUC
corpora. Statistics for each of the data sets are
shown in Table 2. When available, we use the
standard test/train split. Otherwise, we randomly
split the data into a training and test set following
a 70/30 ratio.
3In trial runs, we investigated alternative classification
and clustering models (e.g. C4.5 decision trees and SVMs;
best-first clustering). The results were comparable.
658
Scoring Algorithms. We evaluate using two
common scoring algorithms4 ? MUC and B3.
The MUC scoring algorithm (Vilain et al, 1995)
computes the F1 score (harmonic mean) of preci-
sion and recall based on the identifcation of unique
coreference links. We use the official MUC scorer
implementation for the two MUC corpora and an
equivalent implementation for ACE.
The B3 algorithm (Bagga and Baldwin, 1998)
computes a precision and recall score for each CE:
precision(ce) = |Rce ?Kce|/|Rce|
recall(ce) = |Rce ?Kce|/|Kce|,
where Rce is the coreference chain to which ce is
assigned in the response (i.e. the system-generated
output) and Kce is the coreference chain that con-
tains ce in the key (i.e. the gold standard). Pre-
cision and recall for a set of documents are com-
puted as the mean over all CEs in the documents
and the F1 score of precision and recall is reported.
B3 Complications. Unlike the MUC score,
which counts links between CEs, B3 presumes
that the gold standard and the system response are
clusterings over the same set of CEs. This, of
course, is not the case when the system automat-
ically identifies the CEs, so the scoring algorithm
requires a mapping between extracted and anno-
tated CEs. We will use the term twin(ce) to refer
to the unique annotated/extracted CE to which the
extracted/annotated CE is matched. We say that
a CE is twinless (has no twin) if no corresponding
CE is identified. A twinless extracted CE signals
that the resolver extracted a spurious CE, while an
annotated CE is twinless when the resolver fails to
extract it.
Unfortunately, it is unclear how the B3 score
should be computed for twinless CEs. Bengtson
and Roth (2008) simply discard twinless CEs, but
this solution is likely too lenient ? it doles no pun-
ishment for mistakes on twinless annotated or ex-
tracted CEs and it would be tricked, for example,
by a system that extracts only the CEs about which
it is most confident.
We propose two different ways to deal with
twinless CEs for B3. One option, B3all, retains
all twinless extracted CEs. It computes the preci-
4We also experimented with the CEAF score (Luo, 2005),
but excluded it due to difficulties dealing with the extracted,
rather than annotated, CEs. CEAF assigns a zero score to
each twinless extracted CE and weights all coreference chains
equally, irrespective of their size. As a result, runs with ex-
tracted CEs exhibit very low CEAF precision, leading to un-
reliable scores.
sion as above when ce has a twin, and computes
the precision as 1/|Rce| if ce is twinless. (Simi-
larly, recall(ce) = 1/|Kce| if ce is twinless.)
The second option, B30, discards twinless
extracted CEs, but penalizes recall by setting
recall(ce) = 0 for all twinless annotated CEs.
Thus, B30 presumes that all twinless extracted
CEs are spurious.
Results. Table 3, box 1 shows the performance
of RECONCILEACL09 using a default (0.5) coref-
erence classifier threshold. The MUC score is
highest for the MUC6 data set, while the four ACE
data sets show much higher B3 scores as com-
pared to the two MUC data sets. The latter occurs
because the ACE data sets include singletons.
The classification threshold, however, can be
gainfully employed to control the trade-off be-
tween precision and recall. This has not tradi-
tionally been done in learning-based coreference
resolution research ? possibly because there is
not much training data available to sacrifice as a
validation set. Nonetheless, we hypothesized that
estimating a threshold from just the training data
might be effective. Our results (BASELINE box
in Table 3) indicate that this indeed works well.5
With the exception of MUC6, results on all data
sets and for all scoring algorithms improve; more-
over, the scores approach those for runs using an
optimal threshold (box 3) for the experiment as de-
termined by using the test set. In all remaining ex-
periments, we learn the threshold from the training
set as in the BASELINE system.
Below, we resume our investigation of the role
of three coreference resolution subtasks and mea-
sure the impact of each on overall performance.
3.3 Named Entities
Previous work has shown that resolving corefer-
ence between proper names is relatively easy (e.g.
Kameyama (1997)) because string matching func-
tions specialized to the type of proper name (e.g.
person vs. location) are quite accurate. Thus, we
would expect a coreference resolution system to
depend critically on its Named Entity (NE) extrac-
tor. On the other hand, state-of-the-art NE taggers
are already quite good, so improving this compo-
nent may not provide much additional gain.
To study the influence of NE recognition,
we replace the system-generated NEs of
5All experiments sample uniformly from 1000 threshold
values.
659
ReconcileACL09 MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
1. DEFAULT THRESHOLD (0.5)
MUC 70.40 58.20 65.76 66.73 56.75 64.30
B3all 69.91 62.88 77.25 77.56 73.03 72.82
B30 68.55 62.80 76.59 77.27 72.99 72.43
2. BASELINE
MUC 68.50 62.80 65.99 67.87 62.03 67.41
= THRESHOLD ESTIMATION
B3all 70.88 65.86 78.29 79.39 76.50 73.71
B30 68.43 64.57 76.63 77.88 75.41 72.47
3. OPTIMAL THRESHOLD
MUC 71.20 62.90 66.83 68.35 62.11 67.41
B3all 72.31 66.52 78.50 79.41 76.53 74.25
B30 69.49 64.64 76.83 78.27 75.51 72.94
4. BASELINE with
MUC 69.90 - 66.37 70.35 62.88 67.72
perfect NEs
B3all 72.31 - 78.06 80.22 77.01 73.92
B30 67.91 - 76.55 78.35 75.22 72.90
5. BASELINE with
MUC 85.80* 81.10* 76.39 79.68 76.18 79.42
perfect CEs
B3all 76.14 75.88 78.65 80.58 77.79 76.49
B30 76.14 75.88 78.65 80.58 77.79 76.49
6. BASELINE with
MUC 82.20* 71.90* 86.63 85.58 83.33 82.84
anaphoric CEs
B3all 72.52 69.26 80.29 79.71 76.05 74.33
B30 72.52 69.26 80.29 79.71 76.05 74.33
Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5
threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e. one that
places all CEs into a single coreference chain.
RECONCILEACL09 with gold-standard NEs
and retrain the coreference classifier. Results
for each of the data sets are shown in box 4 of
Table 3. (No gold standard NEs are available for
MUC7.) Comparison to the BASELINE system
(box 2) shows that using gold standard NEs
leads to improvements on all data sets with the
exception of ACE2 and ACE05, on which perfor-
mance is virtually unchanged. The improvements
tend to be small, however, between 0.5 to 3
performance points. We attribute this to two
factors. First, as noted above, although far from
perfect, NE taggers generally perform reasonably
well. Second, only 20 to 25% of the coreference
element resolutions required for these data sets
involve a proper name (see Section 4).
Conclusion #1: Improving the performance of NE tag-
gers is not likely to have a large impact on the performance
of state-of-the-art coreference resolution systems.
3.4 Coreference Element Detection
We expect CE detection to be an important sub-
problem for an end-to-end coreference system.
Results for a system that assumes perfect CEs
are shown in box 5 of Table 3. For these runs,
RECONCILEACL09 uses only the annotated CEs
for both training and testing. Using perfect CEs
solves a large part of the coreference resolution
task: the annotated CEs divulge anaphoricity in-
formation, perfect NP boundaries, and perfect in-
formation regarding the coreference relation de-
fined for the data set.
We see that focusing attention on all and only
the annotated CEs leads to (often substantial) im-
provements in performance on all metrics over
all data sets, especially when measured using the
MUC score.
Conclusion #2: Improving the ability of coreference re-
solvers to identify coreference elements would likely improve
the state-of-the-art immensely ? by 10-20 points in MUC F1
score and from 2-12 F1 points for B3.
This finding explains previously published re-
sults that exhibit striking variability when run with
annotated CEs vs. system-extracted CEs. On the
MUC6 data set, for example, the best published
MUC score using extracted CEs is approximately
71 (Yang et al, 2003), while multiple systems
have produced MUC scores of approximately 85
when using annotated CEs (e.g. Luo et al (2004),
McCallum and Wellner (2004)).
We argue that providing a resolver with the an-
notated CEs is a rather unrealistic evaluation: de-
termining whether an NP is part of an annotated
coreference chain is precisely the job of a corefer-
ence resolver!
Conclusion #3: Assuming the availability of CEs unre-
alistically simplifies the coreference resolution task.
3.5 Anaphoricity Determination
Finally, several coreference systems have suc-
cessfully incorporated anaphoricity determination
660
modules (e.g. Ng and Cardie (2002a) and Bean
and Riloff (2004)). The goal of the module is to
determine whether or not an NP is anaphoric. For
example, pleonastic pronouns (e.g. it is raining)
are special cases that do not require coreference
resolution.
Unfortunately, neither the MUC nor the ACE
data sets include anaphoricity information for all
NPs. Rather, they encode anaphoricity informa-
tion implicitly for annotated CEs: a CE is consid-
ered anaphoric if is not a singleton.6
To study the utility of anaphoricity informa-
tion, we train and test only on the ?anaphoric? ex-
tracted CEs, i.e. the extracted CEs that have an
annotated twin that is not a singleton. Note that
for the MUC datasets all extracted CEs that have
twins are considered anaphoric.
Results for this experiment (box 6 in Table 3)
are similar to the previous experiment using per-
fect CEs: we observe big improvements across the
board. This should not be surprising since the ex-
perimental setting is quite close to that for perfect
CEs: this experiment also presumes knowledge
of when a CE is part of an annotated coreference
chain. Nevertheless, we see that anaphoricity info-
mation is important. First, good anaphoricity iden-
tification should reduce the set of extracted CEs
making it closer to the set of annotated CEs. Sec-
ond, further improvements in MUC score for the
ACE data sets over the runs using perfect CEs (box
5) reveal that accurately determining anaphoric-
ity can lead to substantial improvements in MUC
score. ACE data includes annotations for single-
ton CEs, so knowling whether an annotated CE is
anaphoric divulges additional information.
Conclusion #4: An accurate anaphoricity determina-
tion component can lead to substantial improvement in coref-
erence resolution performance.
4 Resolution Complexity
Different types of anaphora that have to be han-
dled by coreference resolution systems exhibit dif-
ferent properties. In linguistic theory, binding
mechanisms vary for different kinds of syntactic
constituents and structures. And in practice, em-
pirical results have confirmed intuitions that differ-
ent types of anaphora benefit from different clas-
sifier features and exhibit varying degrees of diffi-
culty (Kameyama, 1997). However, performance
6Also, the first element of a coreference chain is usually
non-anaphoric, but we do not consider that issue here.
evaluations rarely include analysis of where state-
of-the-art coreference resolvers perform best and
worst, aside from general conclusions.
In this section, we analyze the behavior of
our coreference resolver on different types of
anaphoric expressions with two goals in mind.
First, we want to deduce the strengths and weak-
nesses of state-of-the-art systems to help direct
future research. Second, we aim to understand
why current coreference resolvers behave so in-
consistently across data sets. Our hypothesis is
that the distribution of different types of anaphoric
expressions in a corpus is a major factor for coref-
erence resolution performance. Our experiments
confirm this hypothesis and we use our empirical
results to create a coreference performance predic-
tion (CPP) measure that successfully estimates the
expected level of performance on novel data sets.
4.1 Resolution Classes
We study the resolution complexity of a text cor-
pus by defining resolution classes. Resolution
classes partition the set of anaphoric CEs accord-
ing to properties of the anaphor and (in some
cases) the antecedent. Previous work has stud-
ied performance differences between pronominal
anaphora, proper names, and common nouns, but
we aim to dig deeper into subclasses of each of
these groups. In particular, we distinguish be-
tween proper and common nouns that can be re-
solved via string matching, versus those that have
no antecedent with a matching string. Intuitively,
we expect that it is easier to resolve the cases
that involve string matching. Similarly, we par-
tition pronominal anaphora into several subcate-
gories that we expect may behave differently. We
define the following nine resolution classes:
Proper Names: Three resolution classes cover
CEs that are named entities (e.g. the PER-
SON, LOCATION, ORGANIZATION and DATE
classes for MUC and ACE) and have a prior ref-
erent7 in the text. These three classes are distin-
guished by the type of antecedent that can be re-
solved against the proper name.
(1) PN-e: a proper name is assigned to this exact string match
class if there is at least one preceding CE in its gold standard
coreference chain that exactly matches it.
(2) PN-p: a proper name is assigned to this partial string
match class if there is at least one preceding CE in its gold
standard chain that has some content words in common.
(3) PN-n: a proper name is assigned to this no string match
7We make a rough, but rarely inaccurate, assumption that
there are no cataphoric expressions in the data.
661
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05 Avg
# % scr # % scr # % scr # % scr # % scr # % scr % scr
PN-e 273 17 .87 249 19 .79 346 24 .94 435 25 .93 267 16 .88 373 31 .92 22 .89
PN-p 157 10 .68 79 6 .59 116 8 .86 178 10 .87 194 11 .71 125 10 .71 9 .74
PN-n 18 1 .18 18 1 .28 85 6 .19 79 4 .15 66 4 .21 89 7 .27 4 .21
CN-e 292 18 .82 276 21 .65 84 6 .40 186 11 .68 165 10 .68 134 11 .79 13 .67
CN-p 229 14 .53 239 18 .49 147 10 .26 168 10 .24 147 9 .40 147 12 .43 12 .39
CN-n 194 12 .27 148 11 .15 152 10 .50 148 8 .90 266 16 .32 121 10 .20 11 .18
1+2Pr 48 3 .70 65 5 .66 122 8 .73 76 4 .73 158 9 .77 51 4 .61 6 .70
G3Pr 160 10 .73 50 4 .79 181 12 .83 237 13 .82 246 14 .84 69 60 .81 10 .80
U3Pr 175 11 .49 142 11 .49 163 11 .45 122 7 .48 153 9 .49 91 7 .49 9 .48
Table 4: Frequencies and scores for each resolution class.
class if no preceding CE in its gold standard chain has any
content words in common with it.
Common NPs: Three analogous string match
classes cover CEs that have a common noun as a
head: (4) CN-e (5) CN-p (6) CN-n.
Pronouns: Three classes cover pronouns:
(7) 1+2Pr: The anaphor is a 1st or 2nd person pronoun.
(8) G3Pr: The anaphor is a gendered 3rd person pronoun
(e.g. ?she?, ?him?).
(9) U3Pr: The anaphor is an ungendered 3rd person pro-
noun.
As noted above, resolution classes are defined for
annotated CEs. We use the twin relationship to
match extracted CEs to annotated CEs and to eval-
uate performance on each resolution class.
4.2 Scoring Resolution Classes
To score each resolution class separately, we de-
fine a new variant of the MUC scorer. We compute
a MUC-RC score (for MUC Resolution Class) for
class C as follows: we assume that all CEs that do
not belong to class C are resolved correctly by tak-
ing the correct clustering for them from the gold
standard. Starting with this correct partial cluster-
ing, we run our classifier on all ordered pairs of
CEs for which the second CE is of class C, es-
sentially asking our coreference resolver to deter-
mine whether each member of class C is corefer-
ent with each of its preceding CEs. We then count
the number of unique correct/incorrect links that
the system introduced on top of the correct par-
tial clustering and compute precision, recall, and
F1 score. This scoring function directly measures
the impact of each resolution class on the overall
MUC score.
4.3 Results
Table 4 shows the results of our resolution class
analysis on the test portions of the six data sets.
The # columns show the frequency counts for each
resolution class, and the % columns show the dis-
tributions of the classes in each corpus (i.e. 17%
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
0.92 0.95 0.91 0.98 0.97 0.96
Table 5: Correlations of resolution class scores with respect
to the average.
of all resolutions in the MUC6 corpus were in the
PN-e class). The scr columns show the MUC-
RC score for each resolution class. The right-hand
side of Table 4 shows the average distribution and
scores across all data sets.
These scores confirm our expectations about the
relative difficulty of different types of resolutions.
For example, it appears that proper names are eas-
ier to resolve than common nouns; gendered pro-
nouns are easier than 1st and 2nd person pronouns,
which, in turn, are easier than ungendered 3rd per-
son pronouns. Similarly, our intuition is confirmed
that many CEs can be accurately resolved based on
exact string matching, whereas resolving against
antecedents that do not have overlapping strings is
much more difficult. The average scores in Table 4
show that performance varies dramatically across
the resolution classes, but, on the surface, appears
to be relatively consistent across data sets.
None of the data sets performs exactly the same,
of course, so we statistically analyze whether the
behavior of each resolution class is similar across
the data sets. For each data set, we compute the
correlation between the vector of MUC-RC scores
over the resolution classes and the average vec-
tor of MUC-RC scores for the remaining five data
sets. Table 5 contains the results, which show high
correlations (over .90) for all six data sets. These
results indicate that the relative performance of the
resolution classes is consistent across corpora.
4.4 Coreference Performance Prediction
Next, we hypothesize that the distribution of res-
olution classes in a corpus explains (at least par-
tially) why performance varies so much from cor-
662
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
P 0.59 0.59 0.62 0.65 0.59 0.62
O 0.67 0.61 0.66 0.68 0.62 0.67
Table 6: Predicted (P) vs Observed (O) scores.
pus to corpus. To explore this issue, we create a
Coreference Performance Prediction (CPP) mea-
sure to predict the performance on new data sets.
The CPP measure uses the empirical performance
of each resolution class observed on previous data
sets and forms a predicton based on the make-up
of resolution classes in a new corpus. The distribu-
tion of resolution classes for a new corpus can be
easily determined because the classes can be rec-
ognized superficially by looking only at the strings
that represent each NP.
We compute the CPP score for each of our six
data sets based on the average resolution class per-
formance measured on the other five data sets.
The predicted score for each class is computed as
a weighted sum of the observed scores for each
resolution class (i.e. the mean for the class mea-
sured on the other five data sets) weighted by the
proportion of CEs that belong to the class. The
predicted scores are shown in Table 6 and com-
pared with the MUC scores that are produced by
RECONCILEACL09.8
Our results show that the CPP measure is a
good predictor of coreference resolution perfor-
mance on unseen data sets, with the exception
of one outlier ? the MUC6 data set. In fact,
the correlation between predicted and observed
scores is 0.731 for all data sets and 0.913 exclud-
ing MUC6. RECONCILEACL09?s performance on
MUC6 is better than predicted due to the higher
than average scores for the common noun classes.
We attribute this to the fact that MUC6 includes
annotations for nested nouns, which almost al-
ways fall in the CN-e and CN-p classes. In ad-
dition, many of the features were first created for
the MUC6 data set, so the feature extractors are
likely more accurate than for other data sets.
Overall, results indicate that coreference perfor-
mance is substantially influenced by the mix of
resolution classes found in the data set. Our CPP
measure can be used to produce a good estimate
of the level of performance on a new corpus.
8Observed scores for MUC6 and 7 differ slightly from Ta-
ble 3 because this part of the work did not use the OPTIONAL
field of the key, employed by the official MUC scorer.
5 Related Work
The bulk of the relevant related work is described
in earlier sections, as appropriate. This paper stud-
ies complexity issues for NP coreference resolu-
tion using a ?good?, i.e. near state-of-the-art, sys-
tem. For state-of-the-art performance on the MUC
data sets see, e.g. Yang et al (2003); for state-of-
the-art performance on the ACE data sets see, e.g.
Bengtson and Roth (2008) and Luo (2007). While
other researchers have evaluated NP coreference
resolvers with respect to pronouns vs. proper
nouns vs. common nouns (Ng and Cardie, 2002b),
our analysis focuses on measuring the complexity
of data sets, predicting the performance of coref-
erence systems on new data sets, and quantify-
ing the effect of coreference system subcompo-
nents on overall performance. In the related area
of anaphora resolution, researchers have studied
the influence of subsystems on the overall per-
formance (Mitkov, 2002) as well as defined and
evaluated performance on different classes of pro-
nouns (e.g. Mitkov (2002) and Byron (2001)).
However, due to the significant differences in task
definition, available datasets, and evaluation met-
rics, their conclusions are not directly applicable
to the full coreference task.
Previous work has developed methods to predict
system performance on NLP tasks given data set
characteristics, e.g. Birch et al (2008) does this for
machine translation. Our work looks for the first
time at predicting the performance of NP corefer-
ence resolvers.
6 Conclusions
We examine the state-of-the-art in NP coreference
resolution. We show the relative impact of perfect
NE recognition, perfect anaphoricity information
for coreference elements, and knowledge of all
and only the annotated CEs. We also measure the
performance of state-of-the-art resolvers on sev-
eral classes of anaphora and use these results to
develop a measure that can accurately estimate a
resolver?s performance on new data sets.
Acknowledgments. We gratefully acknowledge
technical contributions from David Buttler and
David Hysom in creating the Reconcile corefer-
ence resolution platform. This research was sup-
ported in part by the Department of Homeland
Security under ONR Grant N0014-07-1-0152 and
Lawrence Livermore National Laboratory subcon-
tract B573245.
663
References
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. In In Linguistic Corefer-
ence Workshop at LREC 1998.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
D. Bean and E. Riloff. 2004. Unsupervised Learn-
ing of Contextual Role Knowledge for Coreference
Resolution. In Proceedings of the Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL 2004).
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294?303. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting Success in Machine Translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
745?754. Association for Computational Linguis-
tics.
Donna Byron. 2001. The Uncommon Denomina-
tor: A Proposal for Consistent Reporting of Pro-
noun Resolution Results. Computational Linguis-
tics, 27(4):569?578.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and
C. Manning. 2004. Exploiting Context for Biomed-
ical Entity Recognition: From Syntax to the Web. In
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications at COLING 2004.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. In Machine Learning, pages 277?296.
Megumi Kameyama. 1997. Recognizing Referential
Links: An Information Extraction Perspective. In
Workshop On Operational Factors In Practical Ro-
bust Anaphora Resolution For Unrestricted Texts.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics.
X. Luo. 2005. On Coreference Resolution Perfor-
mance Metrics. In Proceedings of the 2005 Human
Language Technology Conference / Conference on
Empirical Methods in Natural Language Process-
ing.
Xiaoqiang Luo. 2007. Coreference or Not: A Twin
Model for Coreference Resolution. In Proceedings
of the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2007).
A. McCallum and B. Wellner. 2004. Conditional Mod-
els of Identity Uncertainty with Application to Noun
Coreference. In 18th Annual Conference on Neural
Information Processing Systems.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
MUC-6. 1995. Coreference Task Definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335?344.
MUC-7. 1997. Coreference Task Definition. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7).
V. Ng and C. Cardie. 2002a. Identifying Anaphoric
and Non-Anaphoric Noun Phrases to Improve
Coreference Resolution. In Proceedings of the 19th
International Conference on Computational Lin-
guistics (COLING 2002).
V. Ng and C. Cardie. 2002b. Improving Machine
Learning Approaches to Coreference Resolution. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
NIST. 2004. The ACE Evaluation Plan.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL
2007).
W. Soon, H. Ng, and D. Lim. 2001. A Machine
Learning Approach to Coreference of Noun Phrases.
Computational Linguistics, 27(4):521?541.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff, David Buttler, and David Hysom. 2009.
Reconcile: A Coreference Resolution Research Plat-
form. Computer Science Technical Report, Cornell
University, Ithaca, NY.
Kees van Deemter and Rodger Kibble. 2000. On
Coreferring: Coreference in MUC and Related
Annotation Schemes. Computational Linguistics,
26(4):629?637.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Corefer-
ence Scoring Theme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6).
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference Resolution Us-
ing Competition Learning Approach. In ACL ?03:
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 176?183.
664
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 704?714,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Sarcasm as Contrast between a Positive Sentiment and Negative Situation
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva,
Nathan Gilbert, Ruihong Huang
School Of Computing
University of Utah
Salt Lake City, UT 84112
{riloff,asheq,alnds,ngilbert,huangrh}@cs.utah.edu, prafulla.surve@gmail.com
Abstract
A common form of sarcasm on Twitter con-
sists of a positive sentiment contrasted with a
negative situation. For example, many sarcas-
tic tweets include a positive sentiment, such as
?love? or ?enjoy?, followed by an expression
that describes an undesirable activity or state
(e.g., ?taking exams? or ?being ignored?). We
have developed a sarcasm recognizer to iden-
tify this type of sarcasm in tweets. We present
a novel bootstrapping algorithm that automati-
cally learns lists of positive sentiment phrases
and negative situation phrases from sarcastic
tweets. We show that identifying contrast-
ing contexts using the phrases learned through
bootstrapping yields improved recall for sar-
casm recognition.
1 Introduction
Sarcasm is generally characterized as ironic or satir-
ical wit that is intended to insult, mock, or amuse.
Sarcasm can be manifested in many different ways,
but recognizing sarcasm is important for natural lan-
guage processing to avoid misinterpreting sarcastic
statements as literal. For example, sentiment anal-
ysis can be easily misled by the presence of words
that have a strong polarity but are used sarcastically,
which means that the opposite polarity was intended.
Consider the following tweet on Twitter, which in-
cludes the words ?yay? and ?thrilled? but actually
expresses a negative sentiment: ?yay! it?s a holi-
day weekend and i?m on call for work! couldn?t be
more thrilled! #sarcasm.? In this case, the hashtag
#sarcasm reveals the intended sarcasm, but we don?t
always have the benefit of an explicit sarcasm label.
In the realm of Twitter, we observed that many
sarcastic tweets have a common structure that
creates a positive/negative contrast between a senti-
ment and a situation. Specifically, sarcastic tweets
often express a positive sentiment in reference to a
negative activity or state. For example, consider the
tweets below, where the positive sentiment terms
are underlined and the negative activity/state terms
are italicized.
(a) Oh how I love being ignored. #sarcasm
(b) Thoroughly enjoyed shoveling the driveway
today! :) #sarcasm
(c) Absolutely adore it when my bus is late
#sarcasm
(d) I?m so pleased mom woke me up with
vacuuming my room this morning. :) #sarcasm
The sarcasm in these tweets arises from the jux-
taposition of a positive sentiment word (e.g., love,
enjoyed, adore, pleased) with a negative activity or
state (e.g., being ignored, bus is late, shoveling, and
being woken up).
The goal of our research is to identify sarcasm
that arises from the contrast between a positive sen-
timent referring to a negative situation. A key chal-
lenge is to automatically recognize the stereotypi-
cally negative ?situations?, which are activities and
states that most people consider to be unenjoyable or
undesirable. For example, stereotypically unenjoy-
able activities include going to the dentist, taking an
exam, and having to work on holidays. Stereotypi-
cally undesirable states include being ignored, hav-
ing no friends, and feeling sick. People recognize
704
these situations as being negative through cultural
norms and stereotypes, so they are rarely accompa-
nied by an explicit negative sentiment. For example,
?I feel sick? is universally understood to be a nega-
tive situation, even without an explicit expression of
negative sentiment. Consequently, we must learn to
recognize phrases that correspond to stereotypically
negative situations.
We present a bootstrapping algorithm that auto-
matically learns phrases corresponding to positive
sentiments and phrases corresponding to negative
situations. We use tweets that contain a sarcasm
hashtag as positive instances for the learning pro-
cess. The bootstrapping algorithm begins with a sin-
gle seed word, ?love?, and a large set of sarcastic
tweets. First, we learn negative situation phrases
that follow a positive sentiment (initially, the seed
word ?love?). Second, we learn positive sentiment
phrases that occur near a negative situation phrase.
The bootstrapping process iterates, alternately learn-
ing new negative situations and new positive sen-
timent phrases. Finally, we use the learned lists
of sentiment and situation phrases to recognize sar-
casm in new tweets by identifying contexts that con-
tain a positive sentiment in close proximity to a neg-
ative situation phrase.
2 Related Work
Researchers have investigated the use of lexical
and syntactic features to recognize sarcasm in text.
Kreuz and Caucci (2007) studied the role that dif-
ferent lexical factors play, such as interjections (e.g.,
?gee? or ?gosh?) and punctuation symbols (e.g., ???)
in recognizing sarcasm in narratives. Lukin and
Walker (2013) explored the potential of a bootstrap-
ping method for sarcasm classification in social di-
alogue to learn lexical N-gram cues associated with
sarcasm (e.g., ?oh really?, ?I get it?, ?no way?, etc.)
as well as lexico-syntactic patterns.
In opinionated user posts, Carvalho et al (2009)
found oral or gestural expressions, represented us-
ing punctuation and other keyboard characters, to
be more predictive of irony1 in contrast to features
representing structured linguistic knowledge in Por-
1They adopted the term ?irony? instead of ?sarcasm? to re-
fer to the case when a word or expression with prior positive
polarity is figuratively used to express a negative opinion.
tuguese. Filatova (2012) presented a detailed de-
scription of sarcasm corpus creation with sarcasm
annotations of Amazon product reviews. Their an-
notations capture sarcasm both at the document level
and the text utterance level. Tsur et al (2010) pre-
sented a semi-supervised learning framework that
exploits syntactic and pattern based features in sar-
castic sentences of Amazon product reviews. They
observed correlated sentiment words such as ?yay!?
or ?great!? often occurring in their most useful pat-
terns.
Davidov et al (2010) used sarcastic tweets and
sarcastic Amazon product reviews to train a sarcasm
classifier with syntactic and pattern-based features.
They examined whether tweets with a sarcasm hash-
tag are reliable enough indicators of sarcasm to be
used as a gold standard for evaluation, but found that
sarcasm hashtags are noisy and possibly biased to-
wards the hardest form of sarcasm (where even hu-
mans have difficulty). Gonza?lez-Iba?n?ez et al (2011)
explored the usefulness of lexical and pragmatic fea-
tures for sarcasm detection in tweets. They used sar-
casm hashtags as gold labels. They found positive
and negative emotions in tweets, determined through
fixed word dictionaries, to have a strong correlation
with sarcasm. Liebrecht et al (2013) explored N-
gram features from 1 to 3-grams to build a classifier
to recognize sarcasm in Dutch tweets. They made an
interesting observation from their most effective N-
gram features that people tend to be more sarcastic
towards specific topics such as school, homework,
weather, returning from vacation, public transport,
the church, the dentist, etc. This observation has
some overlap with our observation that stereotypi-
cally negative situations often occur in sarcasm.
The cues for recognizing sarcasm may come from
a variety of sources. There exists a line of work
that tries to identify facial and vocal cues in speech
(e.g., (Gina M. Caucci, 2012; Rankin et al, 2009)).
Cheang and Pell (2009) and Cheang and Pell (2008)
performed studies to identify acoustic cues in sarcas-
tic utterances by analyzing speech features such as
speech rate, mean amplitude, amplitude range, etc.
Tepperman et al (2006) worked on sarcasm recog-
nition in spoken dialogue using prosodic and spec-
tral cues (e.g., average pitch, pitch slope, etc.) as
well as contextual cues (e.g., laughter or response to
questions) as features.
705
While some of the previous work has identi-
fied specific expressions that correlate with sarcasm,
none has tried to identify contrast between positive
sentiments and negative situations. The novel con-
tributions of our work include explicitly recogniz-
ing contexts that contrast a positive sentiment with a
negative activity or state, as well as a bootstrapped
learning framework to automatically acquire posi-
tive sentiment and negative situation phrases.
3 Bootstrapped Learning of Positive
Sentiments and Negative Situations
Sarcasm is often defined in terms of contrast or ?say-
ing the opposite of what you mean?. Our work fo-
cuses on one specific type of contrast that is common
on Twitter: the expression of a positive sentiment
(e.g., ?love? or ?enjoy?) in reference to a negative
activity or state (e.g., ?taking an exam? or ?being ig-
nored?). Our goal is to create a sarcasm classifier for
tweets that explicitly recognizes contexts that con-
tain a positive sentiment contrasted with a negative
situation.
Our approach learns rich phrasal lexicons of pos-
itive sentiments and negative situations using only
the seed word ?love? and a collection of sarcastic
tweets as input. A key factor that makes the algo-
rithm work is the presumption that if you find a pos-
itive sentiment or a negative situation in a sarcastic
tweet, then you have found the source of the sar-
casm. We further assume that the sarcasm probably
arises from positive/negative contrast and we exploit
syntactic structure to extract phrases that are likely
to have contrasting polarity. Another key factor is
that we focus specifically on tweets. The short na-
ture of tweets limits the search space for the source
of the sarcasm. The brevity of tweets also probably
contributes to the prevalence of this relatively com-
pact form of sarcasm.
3.1 Overview of the Learning Process
Our bootstrapping algorithm operates on the as-
sumption that many sarcastic tweets contain both a
positive sentiment and a negative situation in close
proximity, which is the source of the sarcasm.2 Al-
though sentiments and situations can be expressed
2Sarcasm can arise from a negative sentiment contrasted
with a positive situation too, but our observation is that this is
much less common, at least on Twitter.
Positive
Sentiment
Phrases
Negative
Situation
Phrases
Seed Word
"love"
Sarcastic Tweets
1 2
34
Figure 1: Bootstrapped Learning of Positive Sentiment
and Negative Situation Phrases
in numerous ways, we focus on positive sentiments
that are expressed as a verb phrase or as a predicative
expression (predicate adjective or predicate nomi-
nal), and negative activities or states that can be a
complement to a verb phrase. Ideally, we would
like to parse the text and extract verb complement
phrase structures, but tweets are often informally
written and ungrammatical. Therefore we try to rec-
ognize these syntactic structures heuristically using
only part-of-speech tags and proximity.
The learning process relies on an assumption that
a positive sentiment verb phrase usually appears to
the left of a negative situation phrase and in close
proximity (usually, but not always, adjacent). Picto-
rially, we assume that many sarcastic tweets contain
this structure:
[+ VERB PHRASE] [? SITUATION PHRASE]
This structural assumption drives our bootstrap-
ping algorithm, which is illustrated in Figure 1.
The bootstrapping process begins with a single seed
word, ?love?, which seems to be the most common
positive sentiment term in sarcastic tweets. Given
a sarcastic tweet containing the word ?love?, our
structural assumption infers that ?love? is probably
followed by an expression that refers to a negative
situation. So we harvest the n-grams that follow the
word ?love? as negative situation candidates. We se-
lect the best candidates using a scoring metric, and
add them to a list of negative situation phrases.
Next, we exploit the structural assumption in the
opposite direction. Given a sarcastic tweet that con-
tains a negative situation phrase, we infer that the
negative situation phrase is preceded by a positive
sentiment. We harvest the n-grams that precede the
negative situation phrases as positive sentiment can-
didates, score and select the best candidates, and
706
add them to a list of positive sentiment phrases.
The bootstrapping process then iterates, alternately
learning more positive sentiment phrases and more
negative situation phrases.
We also observed that positive sentiments are fre-
quently expressed as predicative phrases (i.e., pred-
icate adjectives and predicate nominals). For ex-
ample: ?I?m taking calculus. It is awesome. #sar-
casm?. Wiegand et al (2013) offered a related ob-
servation that adjectives occurring in predicate ad-
jective constructions are more likely to convey sub-
jectivity than adjectives occurring in non-predicative
structures. Therefore we also include a step in
the learning process to harvest predicative phrases
that occur in close proximity to a negative situation
phrase. In the following sections, we explain each
step of the bootstrapping process in more detail.
3.2 Bootstrapping Data
For the learning process, we used Twitter?s stream-
ing API to obtain a large set of tweets. We col-
lected 35,000 tweets that contain the hashtag #sar-
casm or #sarcastic to use as positive instances of sar-
casm. We also collected 140,000 additional tweets
from Twitter?s random daily stream. We removed
the tweets that contain a sarcasm hashtag, and con-
sidered the rest to be negative instances of sarcasm.
Of course, there will be some sarcastic tweets that do
not have a sarcasm hashtag, so the negative instances
will contain some noise. But we expect that a very
small percentage of these tweets will be sarcastic, so
the noise should not be a major issue. There will also
be noise in the positive instances because a sarcasm
hashtag does not guarantee that there is sarcasm in
the body of the tweet (e.g., the sarcastic content may
be in a linked url, or in a prior tweet). But again, we
expect the amount of noise to be relatively small.
Our tweet collection therefore contains a total of
175,000 tweets: 20% are labeled as sarcastic and
80% are labeled as not sarcastic. We applied CMU?s
part-of-speech tagger designed for tweets (Owoputi
et al, 2013) to this data set.
3.3 Seeding
The bootstrapping process begins by initializing the
positive sentiment lexicon with one seed word: love.
We chose this seed because it seems to be the most
common positive sentiment word in sarcastic tweets.
3.4 Learning Negative Situation Phrases
The first stage of bootstrapping learns new phrases
that correspond to negative situations. The learning
process consists of two steps: (1) harvesting candi-
date phrases, and (2) scoring and selecting the best
candidates.
To collect candidate phrases for negative situa-
tions, we extract n-grams that follow a positive senti-
ment phrase in a sarcastic tweet. We extract every 1-
gram, 2-gram, and 3-gram that occurs immediately
after (on the right-hand side) of a positive sentiment
phrase. As an example, consider the tweet in Figure
2, where ?love? is the positive sentiment:
I love waiting forever for the doctor #sarcasm
Figure 2: Example Sarcastic Tweet
We extract three n-grams as candidate negative situ-
ation phrases:
waiting, waiting forever, waiting forever for
Next, we apply the part-of-speech (POS) tagger
and filter the candidate list based on POS patterns so
we only keep n-grams that have a desired syntactic
structure. For negative situation phrases, our goal
is to learn possible verb phrase (VP) complements
that are themselves verb phrases because they should
represent activities and states. So we require a can-
didate phrase to be either a unigram tagged as a verb
(V) or the phrase must match one of 7 POS-based
bigram patterns or 20 POS-based trigram patterns
that we created to try to approximate the recogni-
tion of verbal complement structures. The 7 POS bi-
gram patterns are: V+V, V+ADV, ADV+V, ?to?+V,
V+NOUN, V+PRO, V+ADJ. Note that we used
a POS tagger designed for Twitter, which has a
smaller set of POS tags than more traditional POS
taggers. For example there is just a single V tag
that covers all types of verbs. The V+V pattern will
therefore capture negative situation phrases that con-
sist of a present participle verb followed by a past
participle verb, such as ?being ignored? or ?getting
hit?.3 We also allow verb particles to match a V tag
in our patterns. The remaining bigram patterns cap-
ture verb phrases that include a verb and adverb, an
3In some cases it may be more appropriate to consider the
second verb to be an adjective, but in practice they were usually
tagged as verbs.
707
infinitive form (e.g., ?to clean?), a verb and noun
phrase (e.g., ?shoveling snow?), or a verb and ad-
jective (e.g., ?being alone?). We use some simple
heuristics to try to ensure that we are at the end of an
adjective or noun phrase (e.g., if the following word
is tagged as an adjective or noun, then we assume
we are not at the end).
The 20 POS trigram patterns are similar in nature
and are designed to capture seven general types of
verb phrases: verb and adverb mixtures, an infini-
tive VP that includes an adverb, a verb phrase fol-
lowed by a noun phrase, a verb phrase followed by a
prepositional phrase, a verb followed by an adjective
phrase, or an infinitive VP followed by an adjective,
noun, or pronoun.
Returning to Figure 2, only two of the n-grams
match our POS patterns, so we are left with two can-
didate phrases for negative situations:
waiting, waiting forever
Next, we score each negative situation candidate
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase following
a positive sentiment phrase:
| follows(?candidate, +sentiment) & sarcastic |
| follows(?candidate, +sentiment) |
We compute the number of times that the negative
situation candidate immediately follows a positive
sentiment in sarcastic tweets divided by the number
of times that the candidate immediately follows a
positive sentiment in all tweets. We discard phrases
that have a frequency < 3 in the tweet collection
since they are too sparse.
Finally, we rank the candidate phrases based on
this probability, using their frequency as a secondary
key in case of ties. The top 20 phrases with a prob-
ability ? .80 are added to the negative situation
phrase list.4 When we add a phrase to the nega-
tive situation list, we immediately remove all other
candidates that are subsumed by the selected phrase.
For example, if we add the phrase ?waiting?, then
the phrase ?waiting forever? would be removed from
the candidate list because it is subsumed by ?wait-
ing?. This process reduces redundancy in the set of
4Fewer than 20 phrases will be learned if < 20 phrases pass
this threshold.
phrases that we add during each bootstrapping itera-
tion. The bootstrapping process stops when no more
candidate phrases pass the probability threshold.
3.5 Learning Positive Verb Phrases
The procedure for learning positive sentiment
phrases is analogous. First, we collect phrases that
potentially convey a positive sentiment by extract-
ing n-grams that precede a negative situation phrase
in a sarcastic tweet. To learn positive sentiment verb
phrases, we extract every 1-gram and 2-gram that
occurs immediately before (on the left-hand side of)
a negative situation phrase.
Next, we apply the POS tagger and filter the n-
grams using POS tag patterns so that we only keep
n-grams that have a desired syntactic structure. Here
our goal is to learn simple verb phrases (VPs) so we
only retain n-grams that contain at least one verb and
consist only of verbs and (optionally) adverbs. Fi-
nally, we score each candidate sentiment verb phrase
by estimating the probability that a tweet is sarcastic
given that it contains the candidate phrase preceding
a negative situation phrase:
| precedes(+candidateVP,?situation) & sarcastic |
| precedes(+candidateVP,?situation) |
3.6 Learning Positive Predicative Phrases
We also use the negative situation phrases to harvest
predicative expressions (predicate adjective or pred-
icate nominal structures) that occur nearby. Based
on the same assumption that sarcasm often arises
from the contrast between a positive sentiment and
a negative situation, we identify tweets that contain
a negative situation and a predicative expression in
close proximity. We then assume that the predicative
expression is likely to convey a positive sentiment.
To learn predicative expressions, we use 24 copu-
lar verbs from Wikipedia5 and their inflections. We
extract positive sentiment candidates by extracting
1-grams, 2-grams, and 3-grams that appear immedi-
ately after a copular verb and occur within 5 words
of the negative situation phrase, on either side. This
constraint only enforces proximity because predica-
tive expressions often appear in a separate clause or
sentence (e.g., ?It is just great that my iphone was
stolen? or ?My iphone was stolen. This is great.?)
5http://en.wikipedia.org/wiki/List of English copulae
708
We then apply POS patterns to identify n-grams
that correspond to predicate adjective and predicate
nominal phrases. For predicate adjectives, we re-
tain ADJ and ADV+ADJ n-grams. We use a few
heuristics to check that the adjective is not part of a
noun phrase (e.g., we check that the following word
is not a noun). For predicate nominals, we retain
ADV+ADJ+N, DET+ADJ+N and ADJ+N n-grams.
We excluded noun phrases consisting only of nouns
because they rarely seemed to represent a sentiment.
The sentiment in predicate nominals was usually
conveyed by the adjective. We discard all candidates
with frequency < 3 as being too sparse. Finally,
we score each remaining candidate by estimating the
probability that a tweet is sarcastic given that it con-
tains the predicative expression near (within 5 words
of) a negative situation phrase:
| near(+candidatePRED,?situation) & sarcastic |
| near(+candidatePRED,?situation) |
We found that the diversity of positive senti-
ment verb phrases and predicative expressions is
much lower than the diversity of negative situation
phrases. As a result, we sort the candidates by their
probability and conservatively add only the top 5
positive verb phrases and top 5 positive predicative
expressions in each bootstrapping iteration. Both
types of sentiment phrases must pass a probability
threshold of ? .70.
3.7 The Learned Phrase Lists
The bootstrapping process alternately learns pos-
itive sentiments and negative situations until no
more phrases can be learned. In our experiments,
we learned 26 positive sentiment verb phrases, 20
predicative expressions and 239 negative situation
phrases.
Table 1 shows the first 15 positive verb phrases,
the first 15 positive predicative expressions, and the
first 40 negative situation phrases learned by the
bootstrapping algorithm. Some of the negative sit-
uation phrases are not complete expressions, but it
is clear that they will often match negative activities
and states. For example, ?getting yelled? was gener-
ated from sarcastic comments such as ?I love getting
yelled at?, ?being home? occurred in tweets about
?being home alone?, and ?being told? is often be-
ing told what to do. Shorter phrases often outranked
longer phrases because they are more general, and
will therefore match more contexts. But an avenue
for future work is to learn linguistic expressions that
more precisely characterize specific negative situa-
tions.
Positive Verb Phrases (26): missed, loves,
enjoy, cant wait, excited, wanted, can?t wait,
get, appreciate, decided, loving, really like,
looooove, just keeps, loveee, ...
Positive Predicative Expressions (20): great,
so much fun, good, so happy, better, my
favorite thing, cool, funny, nice, always fun,
fun, awesome, the best feeling, amazing,
happy, ...
Negative Situations (239): being ignored, be-
ing sick, waiting, feeling, waking up early, be-
ing woken, fighting, staying, writing, being
home, cleaning, not getting, crying, sitting at
home, being stuck, starting, being told, be-
ing left, getting ignored, being treated, doing
homework, learning, getting up early, going to
bed, getting sick, riding, being ditched, get-
ting ditched, missing, not sleeping, not talking,
trying, falling, walking home, getting yelled,
being awake, being talked, taking care, doing
nothing, wasting, ...
Table 1: Examples of Learned Phrases
4 Evaluation
4.1 Data
For evaluation purposes, we created a gold stan-
dard data set of manually annotated tweets. Even
for people, it is not always easy to identify sarcasm
in tweets because sarcasm often depends on con-
versational context that spans more than a single
tweet. Extracting conversational threads from Twit-
ter, and analyzing conversational exchanges, has its
own challenges and is beyond the scope of this re-
search. We focus on identifying sarcasm that is self-
contained in one tweet and does not depend on prior
conversational context.
We defined annotation guidelines that instructed
human annotators to read isolated tweets and label
709
a tweet as sarcastic if it contains comments judged
to be sarcastic based solely on the content of that
tweet. Tweets that do not contain sarcasm, or where
potential sarcasm is unclear without seeing the prior
conversational context, were labeled as not sarcas-
tic. For example, a tweet such as ?Yes, I meant that
sarcastically.? should be labeled as not sarcastic be-
cause the sarcastic content was (presumably) in a
previous tweet. The guidelines did not contain any
instructions that required positive/negative contrast
to be present in the tweet, so all forms of sarcasm
were considered to be positive examples.
To ensure that our evaluation data had a healthy
mix of both sarcastic and non-sarcastic tweets, we
collected 1,600 tweets with a sarcasm hashtag (#sar-
casm or #sarcastic), and 1,600 tweets without these
sarcasm hashtags from Twitter?s random streaming
API. When presenting the tweets to the annotators,
the sarcasm hashtags were removed so the annota-
tors had to judge whether a tweet was sarcastic or
not without seeing those hashtags.
To ensure that we had high-quality annotations,
three annotators were asked to annotate the same set
of 200 tweets (100 sarcastic + 100 not sarcastic).
We computed inter-annotator agreement (IAA) be-
tween each pair of annotators using Cohen?s kappa
(?). The pairwise IAA scores were ?=0.80, ?=0.81,
and ?=0.82. We then gave each annotator an addi-
tional 1,000 tweets to annotate, yielding a total of
3,200 annotated tweets. We used the first 200 tweets
as our Tuning Set, and the remaining 3000 tweets as
our Test Set.
Our annotators judged 742 of the 3,200 tweets
(23%) to be sarcastic. Only 713 of the 1,600 tweets
with sarcasm hashtags (45%) were judged to be sar-
castic based on our annotation guidelines. There are
several reasons why a tweet with a sarcasm hash-
tag might not have been judged to be sarcastic. Sar-
casm may not be apparent without prior conversa-
tional context (i.e., multiple tweets), or the sarcastic
content may be in a URL and not the tweet itself, or
the tweet?s content may not obviously be sarcastic
without seeing the sarcasm hashtag (e.g., ?The most
boring hockey game ever #sarcasm?).
Of the 1,600 tweets in our data set that were ob-
tained from the random stream and did not have a
sarcasm hashtag, 29 (1.8%) were judged to be sar-
castic based on our annotation guidelines.
4.2 Baselines
Overall, 693 of the 3,000 tweets in our Test Set
were annotated as sarcastic, so a system that classi-
fies every tweet as sarcastic will have 23% precision.
To assess the difficulty of recognizing the sarcastic
tweets in our data set, we evaluated a variety of base-
line systems.
We created two baseline systems that use n-gram
features with supervised machine learning to create
a sarcasm classifier. We used the LIBSVM (Chang
and Lin, 2011) library to train two support vector
machine (SVM) classifiers: one with just unigram
features and one with both unigrams and bigrams.
The features had binary values indicating the pres-
ence or absence of each n-gram in a tweet. The clas-
sifiers were evaluated using 10-fold cross-validation.
We used the RBF kernel, and the cost and gamma
parameters were optimized for accuracy using un-
igram features and 10-fold cross-validation on our
Tuning Set. The first two rows of Table 2 show the
results for these SVM classifiers, which achieved F
scores of 46-48%.
We also conducted experiments with existing sen-
timent and subjectivity lexicons to see whether they
could be leveraged to recognize sarcasm. We exper-
imented with three resources:
Liu05 : A positive and negative opinion lexicon
from (Liu et al, 2005). This lexicon contains
2,007 positive sentiment words and 4,783 neg-
ative sentiment words.
MPQA05 : The MPQA Subjectivity Lexicon that
is part of the OpinionFinder system (Wilson et
al., 2005a; Wilson et al, 2005b). This lexicon
contains 2,718 subjective words with positive
polarity and 4,910 subjective words with nega-
tive polarity.
AFINN11 The AFINN sentiment lexicon designed
for microblogs (Nielsen, 2011; Hansen et al,
2011) contains 2,477 manually labeled words
and phrases with integer values ranging from -5
(negativity) to 5 (positivity). We considered all
words with negative values to have negative po-
larity (1598 words), and all words with positive
values to have positive polarity (879 words).
We performed four sets of experiments with each
resource to see how beneficial existing sentiment
710
System Recall Precision F score
Supervised SVM Classifiers
1grams .35 .64 .46
1+2grams .39 .64 .48
Positive Sentiment Only
Liu05 .77 .34 .47
MPQA05 .78 .30 .43
AFINN11 .75 .32 .44
Negative Sentiment Only
Liu05 .26 .23 .24
MPQA05 .34 .24 .28
AFINN11 .24 .22 .23
Positive and Negative Sentiment, Unordered
Liu05 .19 .37 .25
MPQA05 .27 .30 .29
AFINN11 .17 .30 .22
Positive and Negative Sentiment, Ordered
Liu05 .09 .40 .14
MPQA05 .13 .30 .18
AFINN11 .09 .35 .14
Our Bootstrapped Lexicons
Positive VPs .28 .45 .35
Negative Situations .29 .38 .33
Contrast(+VPs, ?Situations), Unordered .11 .56 .18
Contrast(+VPs, ?Situations), Ordered .09 .70 .15
& Contrast(+Preds, ?Situations) .13 .63 .22
Our Bootstrapped Lexicons ? SVM Classifier
Contrast(+VPs, ?Situations), Ordered .42 .63 .50
& Contrast(+Preds, ?Situations) .44 .62 .51
Table 2: Experimental results on the test set
lexicons could be for sarcasm recognition in tweets.
Since our hypothesis is that sarcasm often arises
from the contrast between something positive and
something negative, we systematically evaluated the
positive and negative phrases individually, jointly,
and jointly in a specific order (a positive phrase fol-
lowed by a negative phrase).
First, we labeled a tweet as sarcastic if it con-
tains any positive term in each resource. The Pos-
itive Sentiment Only section of Table 2 shows that
all three sentiment lexicons achieved high recall (75-
78%) but low precision (30-34%). Second, we la-
beled a tweet as sarcastic if it contains any negative
term from each resource. The Negative Sentiment
Only section of Table 2 shows that this approach
yields much lower recall and also lower precision
of 22-24%, which is what would be expected of a
random classifier since 23% of the tweets are sar-
castic. These results suggest that explicit negative
sentiments are not generally indicative of sarcasm.
Third, we labeled a tweet as sarcastic if it contains
both a positive sentiment term and a negative senti-
ment term, in any order. The Positive and Negative
Sentiment, Unordered section of Table 2 shows that
this approach yields low recall, indicating that rela-
tively few sarcastic tweets contain both positive and
negative sentiments, and low precision as well.
Fourth, we required the contrasting sentiments to
occur in a specific order (the positive term must pre-
cede the negative term) and near each other (no more
than 5 words apart). This criteria reflects our obser-
vation that positive sentiments often closely precede
negative situations in sarcastic tweets, so we wanted
to see if the same ordering tendency holds for neg-
ative sentiments. The Positive and Negative Senti-
ment, Ordered section of Table 2 shows that this or-
dering constraint further decreases recall and only
slightly improves precision, if at all. Our hypothe-
711
sis is that when positive and negative sentiments are
expressed in the same tweet, they are referring to
different things (e.g., different aspects of a product).
Expressing positive and negative sentiments about
the same thing would usually sound contradictory
rather than sarcastic.
4.3 Evaluation of Bootstrapped Phrase Lists
The next set of experiments evaluates the effective-
ness of the positive sentiment and negative situa-
tion phrases learned by our bootstrapping algorithm.
The results are shown in the Our Bootstrapped Lex-
icons section of Table 2. For the sake of compar-
ison with other sentiment resources, we first eval-
uated our positive sentiment verb phrases and neg-
ative situation phrases independently. Our positive
verb phrases achieved much lower recall than the
positive sentiment phrases in the other resources, but
they had higher precision (45%). The low recall
is undoubtedly because our bootstrapped lexicon is
small and contains only verb phrases, while the other
resources are much larger and contain terms with
additional parts-of-speech, such as adjectives and
nouns.
Despite its relatively small size, our list of neg-
ative situation phrases achieved 29% recall, which
is comparable to the negative sentiments, but higher
precision (38%).
Next, we classified a tweet as sarcastic if it con-
tains both a positive verb phrase and a negative sit-
uation phrase from our bootstrapped lists, in any
order. This approach produced low recall (11%)
but higher precision (56%) than the sentiment lex-
icons. Finally, we enforced an ordering constraint
so a tweet is labeled as sarcastic only if it contains
a positive verb phrase that precedes a negative situa-
tion in close proximity (no more than 5 words apart).
This ordering constraint further increased precision
from 56% to 70%, with a decrease of only 2 points
in recall. This precision gain supports our claim that
this particular structure (positive verb phrase fol-
lowed by a negative situation) is strongly indicative
of sarcasm. Note that the same ordering constraint
applied to a positive verb phrase followed by a neg-
ative sentiment produced much lower precision (at
best 40% precision using the Liu05 lexicon). Con-
trasting a positive sentiment with a negative situa-
tion seems to be a key element of sarcasm.
In the last experiment, we added the positive pred-
icative expressions and also labeled a tweet as sar-
castic if a positive predicative appeared in close
proximity to (within 5 words of) a negative situa-
tion. The positive predicatives improved recall to
13%, but decreased precision to 63%, which is com-
parable to the SVM classifiers.
4.4 A Hybrid Approach
Thus far, we have used the bootstrapped lexicons
to recognize sarcasm by looking for phrases in our
lists. We will refer to our approach as the Contrast
method, which labels a tweet as sarcastic if it con-
tains a positive sentiment phrase in close proximity
to a negative situation phrase.
The Contrast method achieved 63% precision but
with low recall (13%). The SVM classifier with un-
igram and bigram features achieved 64% precision
with 39% recall. Since neither approach has high
recall, we decided to see whether they are comple-
mentary and the Contrast method is finding sarcastic
tweets that the SVM classifier overlooks.
In this hybrid approach, a tweet is labeled as sar-
castic if either the SVM classifier or the Contrast
method identifies it as sarcastic. This approach im-
proves recall from 39% to 42% using the Contrast
method with only positive verb phrases. Recall im-
proves to 44% using the Contrast method with both
positive verb phrases and predicative phrases. This
hybrid approach has only a slight drop in precision,
yielding an F score of 51%. This result shows that
our bootstrapped phrase lists are recognizing sarcas-
tic tweets that the SVM classifier misses.
Finally, we ran tests to see if the performance of
the hybrid approach (Contrast ? SVM) is statisti-
cally significantly better than the performance of the
SVM classifier alone. We used paired bootstrap sig-
nificance testing as described in Berg-Kirkpatrick
et al (2012) by drawing 106 samples with repeti-
tion from the test set. These results showed that the
Contrast ? SVM system is statistically significantly
better than the SVM classifier at the p < .01 level
(i.e., the null hypothesis was rejected with 99% con-
fidence).
4.5 Analysis
To get a better sense of the strength and limitations
of our approach, we manually inspected some of the
712
tweets that were labeled as sarcastic using our boot-
strapped phrase lists. Table 3 shows some of the sar-
castic tweets found by the Contrast method but not
by the SVM classifier.
i love fighting with the one i love
love working on my last day of summer
i enjoy tweeting [user] and not getting a reply
working during vacation is awesome .
can?t wait to wake up early to babysit !
Table 3: Five sarcastic tweets found by the Contrast
method but not the SVM
These tweets are good examples of a positive sen-
timent (love, enjoy, awesome, can?t wait) contrast-
ing with a negative situation. However, the negative
situation phrases are not always as specific as they
should be. For example, ?working? was learned as
a negative situation phrase because it is often neg-
ative when it follows a positive sentiment (?I love
working...?). But the attached prepositional phrases
(?on my last day of summer? and ?during vacation?)
should ideally have been captured as well.
We also examined tweets that were incorrectly la-
beled as sarcastic by the Contrast method. Some
false hits come from situations that are frequently
negative but not always negative (e.g., some peo-
ple genuinely like waking up early). However, most
false hits were due to overly general negative situa-
tion phrases (e.g., ?I love working there? was labeled
as sarcastic). We believe that an important direction
for future work will be to learn longer phrases that
represent more specific situations.
5 Conclusions
Sarcasm is a complex and rich linguistic phe-
nomenon. Our work identifies just one type of sar-
casm that is common in tweets: contrast between a
positive sentiment and negative situation. We pre-
sented a bootstrapped learning method to acquire
lists of positive sentiment phrases and negative ac-
tivities and states, and show that these lists can be
used to recognize sarcastic tweets.
This work has only scratched the surface of pos-
sibilities for identifying sarcasm arising from posi-
tive/negative contrast. The phrases that we learned
were limited to specific syntactic structures and we
required the contrasting phrases to appear in a highly
constrained context. We plan to explore methods for
allowing more flexibility and for learning additional
types of phrases and contrasting structures.
We also would like to explore new ways to iden-
tify stereotypically negative activities and states be-
cause we believe this type of world knowledge is
essential to recognize many instances of sarcasm.
For example, sarcasm often arises from a descrip-
tion of a negative event followed by a positive emo-
tion but in a separate clause or sentence, such as:
?Going to the dentist for a root canal this after-
noon. Yay, I can?t wait.? Recognizing the intensity
of the negativity may also be useful to distinguish
strong contrast from weak contrast. Having knowl-
edge about stereotypically undesirable activities and
states could also be important for other natural lan-
guage understanding tasks, such as text summariza-
tion and narrative plot analysis.
6 Acknowledgments
This work was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI
/ NBC) contract number D12PC00285. The U.S.
Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, or the U.S. Government.
References
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL ?12, pages 995?1005.
Paula Carvalho, Lu??s Sarmento, Ma?rio J. Silva, and
Euge?nio de Oliveira. 2009. Clues for detecting irony
in user-generated contents: oh...!! it?s ?so easy? ;-). In
Proceedings of the 1st international CIKM workshop
on Topic-sentiment analysis for mass opinion, TSA
2009.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
713
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Henry S. Cheang and Marc D. Pell. 2008. The sound of
sarcasm. Speech Commun., 50(5):366?381, May.
Henry S. Cheang and Marc D. Pell. 2009. Acous-
tic markers of sarcasm in cantonese and english.
The Journal of the Acoustical Society of America,
126(3):1394?1405.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL 2010.
Elena Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12).
Roger J. Kreuz Gina M. Caucci. 2012. Social and par-
alinguistic cues to sarcasm. online 08/02/2012, 25:1?
22, February.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies.
Lars Kai Hansen, Adam Arvidsson, Finn Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news - affect and virality in twitter. In
The 2011 International Workshop on Social Comput-
ing, Network, and Services (SocialComNet 2011).
Roger Kreuz and Gina Caucci. 2007. Lexical influences
on the perception of sarcasm. In Proceedings of the
Workshop on Computational Approaches to Figurative
Language.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for detect-
ing sarcasm in tweets #not. In Proceedings of the 4th
Workshop on Computational Approaches to Subjec-
tivity, Sentiment and Social Media Analysis, WASSA
2013.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of the 14th International
World Wide Web conference (WWW-2005).
Stephanie Lukin and Marilyn Walker. 2013. Really?
well. apparently bootstrapping improves the perfor-
mance of sarcasm and nastiness classifiers for online
dialogue. In Proceedings of the Workshop on Lan-
guage Analysis in Social Media.
Finn Arup Nielsen. 2011. A new anew: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages (http://arxiv.org/abs/1103.2903).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In The 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL 2013).
Katherine P. Rankin, Andrea Salazar, Maria Luisa Gorno-
Tempini, Marc Sollberger, Stephen M. Wilson, Dani-
jela Pavlic, Christine M. Stanley, Shenly Glenn,
Michael W. Weiner, and Bruce L. Miller. 2009. De-
tecting sarcasm from paralinguistic cues: Anatomic
and cognitive correlates in neurodegenerative disease.
Neuroimage, 47:2005?2015.
Joseph Tepperman, David Traum, and Shrikanth
Narayanan. 2006. ?Yeah right?: Sarcasm recogni-
tion for spoken dialogue systems. In Proceedings of
the INTERSPEECH 2006 - ICSLP, Ninth International
Conference on Spoken Language Processing.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Product
Reviews. In Proceedings of the Fourth International
Conference on Weblogs and Social Media (ICWSM-
2010), ICWSM 2010.
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative adjectives: An unsuper-
vised criterion to extract subjective adjectives. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 534?
539, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patward-
han. 2005a. OpinionFinder: A System for Subjec-
tivity Analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations, pages 34?35, Vancouver,
Canada, October.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the 2005
Human Language Technology Conference / Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
714
Proceedings of the ACL 2010 Conference Short Papers, pages 156?161,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Coreference Resolution with Reconcile
Veselin Stoyanov
Center for Language
and Speech Processing
Johns Hopkins Univ.
Baltimore, MD
ves@cs.jhu.edu
Claire Cardie
Department of
Computer Science
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Nathan Gilbert
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
riloff@cs.utah.edu
David Buttler
David Hysom
Lawrence Livermore
National Laboratory
Livermore, CA
buttler1@llnl.gov
hysom1@llnl.gov
Abstract
Despite the existence of several noun phrase coref-
erence resolution data sets as well as several for-
mal evaluations on the task, it remains frustratingly
difficult to compare results across different corefer-
ence resolution systems. This is due to the high cost
of implementing a complete end-to-end coreference
resolution system, which often forces researchers
to substitute available gold-standard information in
lieu of implementing a module that would compute
that information. Unfortunately, this leads to incon-
sistent and often unrealistic evaluation scenarios.
With the aim to facilitate consistent and realis-
tic experimental evaluations in coreference resolu-
tion, we present Reconcile, an infrastructure for the
development of learning-based noun phrase (NP)
coreference resolution systems. Reconcile is de-
signed to facilitate the rapid creation of corefer-
ence resolution systems, easy implementation of
new feature sets and approaches to coreference res-
olution, and empirical evaluation of coreference re-
solvers across a variety of benchmark data sets and
standard scoring metrics. We describe Reconcile
and present experimental results showing that Rec-
oncile can be used to create a coreference resolver
that achieves performance comparable to state-of-
the-art systems on six benchmark data sets.
1 Introduction
Noun phrase coreference resolution (or simply
coreference resolution) is the problem of identi-
fying all noun phrases (NPs) that refer to the same
entity in a text. The problem of coreference res-
olution is fundamental in the field of natural lan-
guage processing (NLP) because of its usefulness
for other NLP tasks, as well as the theoretical in-
terest in understanding the computational mech-
anisms involved in government, binding and lin-
guistic reference.
Several formal evaluations have been conducted
for the coreference resolution task (e.g., MUC-6
(1995), ACE NIST (2004)), and the data sets cre-
ated for these evaluations have become standard
benchmarks in the field (e.g., MUC and ACE data
sets). However, it is still frustratingly difficult to
compare results across different coreference res-
olution systems. Reported coreference resolu-
tion scores vary wildly across data sets, evaluation
metrics, and system configurations.
We believe that one root cause of these dispar-
ities is the high cost of implementing an end-to-
end coreference resolution system. Coreference
resolution is a complex problem, and successful
systems must tackle a variety of non-trivial sub-
problems that are central to the coreference task ?
e.g., mention/markable detection, anaphor identi-
fication ? and that require substantial implemen-
tation efforts. As a result, many researchers ex-
ploit gold-standard annotations, when available, as
a substitute for component technologies to solve
these subproblems. For example, many published
research results use gold standard annotations to
identify NPs (substituting for mention/markable
detection), to distinguish anaphoric NPs from non-
anaphoric NPs (substituting for anaphoricity de-
termination), to identify named entities (substitut-
ing for named entity recognition), and to identify
the semantic types of NPs (substituting for seman-
tic class identification). Unfortunately, the use of
gold standard annotations for key/critical compo-
nent technologies leads to an unrealistic evalua-
tion setting, and makes it impossible to directly
compare results against coreference resolvers that
solve all of these subproblems from scratch.
Comparison of coreference resolvers is further
hindered by the use of several competing (and
non-trivial) evaluation measures, and data sets that
have substantially different task definitions and
annotation formats. Additionally, coreference res-
olution is a pervasive problem in NLP and many
NLP applications could benefit from an effective
coreference resolver that can be easily configured
and customized.
To address these issues, we have created a plat-
form for coreference resolution, called Reconcile,
that can serve as a software infrastructure to sup-
port the creation of, experimentation with, and
evaluation of coreference resolvers. Reconcile
was designed with the following seven desiderata
in mind:
? implement the basic underlying software ar-
156
chitecture of contemporary state-of-the-art
learning-based coreference resolution sys-
tems;
? support experimentation on most of the stan-
dard coreference resolution data sets;
? implement most popular coreference resolu-
tion scoring metrics;
? exhibit state-of-the-art coreference resolution
performance (i.e., it can be configured to cre-
ate a resolver that achieves performance close
to the best reported results);
? can be easily extended with new methods and
features;
? is relatively fast and easy to configure and
run;
? has a set of pre-built resolvers that can be
used as black-box coreference resolution sys-
tems.
While several other coreference resolution sys-
tems are publicly available (e.g., Poesio and
Kabadjov (2004), Qiu et al (2004) and Versley et
al. (2008)), none meets all seven of these desider-
ata (see Related Work). Reconcile is a modular
software platform that abstracts the basic archi-
tecture of most contemporary supervised learning-
based coreference resolution systems (e.g., Soon
et al (2001), Ng and Cardie (2002), Bengtson and
Roth (2008)) and achieves performance compara-
ble to the state-of-the-art on several benchmark
data sets. Additionally, Reconcile can be eas-
ily reconfigured to use different algorithms, fea-
tures, preprocessing elements, evaluation settings
and metrics.
In the rest of this paper, we review related work
(Section 2), describe Reconcile?s organization and
components (Section 3) and show experimental re-
sults for Reconcile on six data sets and two evalu-
ation metrics (Section 4).
2 Related Work
Several coreference resolution systems are cur-
rently publicly available. JavaRap (Qiu et al,
2004) is an implementation of the Lappin and
Leass? (1994) Resolution of Anaphora Procedure
(RAP). JavaRap resolves only pronouns and, thus,
it is not directly comparable to Reconcile. GuiTaR
(Poesio and Kabadjov, 2004) and BART (Versley
et al, 2008) (which can be considered a succes-
sor of GuiTaR) are both modular systems that tar-
get the full coreference resolution task. As such,
both systems come close to meeting the majority
of the desiderata set forth in Section 1. BART,
in particular, can be considered an alternative to
Reconcile, although we believe that Reconcile?s
approach is more flexible than BART?s. In addi-
tion, the architecture and system components of
Reconcile (including a comprehensive set of fea-
tures that draw on the expertise of state-of-the-art
supervised learning approaches, such as Bengtson
and Roth (2008)) result in performance closer to
the state-of-the-art.
Coreference resolution has received much re-
search attention, resulting in an array of ap-
proaches, algorithms and features. Reconcile
is modeled after typical supervised learning ap-
proaches to coreference resolution (e.g. the archi-
tecture introduced by Soon et al (2001)) because
of the popularity and relatively good performance
of these systems.
However, there have been other approaches
to coreference resolution, including unsupervised
and semi-supervised approaches (e.g. Haghighi
and Klein (2007)), structured approaches (e.g.
McCallum and Wellner (2004) and Finley and
Joachims (2005)), competition approaches (e.g.
Yang et al (2003)) and a bell-tree search approach
(Luo et al (2004)). Most of these approaches rely
on some notion of pairwise feature-based similar-
ity and can be directly implemented in Reconcile.
3 System Description
Reconcile was designed to be a research testbed
capable of implementing most current approaches
to coreference resolution. Reconcile is written in
Java, to be portable across platforms, and was de-
signed to be easily reconfigurable with respect to
subcomponents, feature sets, parameter settings,
etc.
Reconcile?s architecture is illustrated in Figure
1. For simplicity, Figure 1 shows Reconcile?s op-
eration during the classification phase (i.e., assum-
ing that a trained classifier is present).
The basic architecture of the system includes
five major steps. Starting with a corpus of docu-
ments together with a manually annotated corefer-
ence resolution answer key1, Reconcile performs
1Only required during training.
157
Figure 1: The Reconcile classification architecture.
the following steps, in order:
1. Preprocessing. All documents are passed
through a series of (external) linguistic pro-
cessors such as tokenizers, part-of-speech
taggers, syntactic parsers, etc. These com-
ponents produce annotations of the text. Ta-
ble 1 lists the preprocessors currently inter-
faced in Reconcile. Note that Reconcile in-
cludes several in-house NP detectors, that
conform to the different data sets? defini-
tions of what constitutes a NP (e.g., MUC
vs. ACE). All of the extractors utilize a syn-
tactic parse of the text and the output of a
Named Entity (NE) extractor, but extract dif-
ferent constructs as specialized in the corre-
sponding definition. The NP extractors suc-
cessfully recognize about 95% of the NPs in
the MUC and ACE gold standards.
2. Feature generation. Using annotations pro-
duced during preprocessing, Reconcile pro-
duces feature vectors for pairs of NPs. For
example, a feature might denote whether the
two NPs agree in number, or whether they
have any words in common. Reconcile in-
cludes over 80 features, inspired by other suc-
cessful coreference resolution systems such
as Soon et al (2001) and Ng and Cardie
(2002).
3. Classification. Reconcile learns a classifier
that operates on feature vectors representing
Task Systems
Sentence UIUC (CC Group, 2009)
splitter OpenNLP (Baldridge, J., 2005)
Tokenizer OpenNLP (Baldridge, J., 2005)
POS OpenNLP (Baldridge, J., 2005)
Tagger + the two parsers below
Parser Stanford (Klein and Manning, 2003)
Berkeley (Petrov and Klein, 2007)
Dep. parser Stanford (Klein and Manning, 2003)
NE OpenNLP (Baldridge, J., 2005)
Recognizer Stanford (Finkel et al, 2005)
NP Detector In-house
Table 1: Preprocessing components available in
Reconcile.
pairs of NPs and it is trained to assign a score
indicating the likelihood that the NPs in the
pair are coreferent.
4. Clustering. A clustering algorithm consoli-
dates the predictions output by the classifier
and forms the final set of coreference clusters
(chains).2
5. Scoring. Finally, during testing Reconcile
runs scoring algorithms that compare the
chains produced by the system to the gold-
standard chains in the answer key.
Each of the five steps above can invoke differ-
ent components. Reconcile?s modularity makes it
2Some structured coreference resolution algorithms (e.g.,
McCallum and Wellner (2004) and Finley and Joachims
(2005)) combine the classification and clustering steps above.
Reconcile can easily accommodate this modification.
158
Step Available modules
Classification various learners in the Weka toolkit
libSVM (Chang and Lin, 2001)
SVMlight (Joachims, 2002)
Clustering Single-link
Best-First
Most Recent First
Scoring MUC score (Vilain et al, 1995)
B3 score (Bagga and Baldwin, 1998)
CEAF score (Luo, 2005)
Table 2: Available implementations for different
modules available in Reconcile.
easy for new components to be implemented and
existing ones to be removed or replaced. Recon-
cile?s standard distribution comes with a compre-
hensive set of implemented components ? those
available for steps 2?5 are shown in Table 2. Rec-
oncile contains over 38,000 lines of original Java
code. Only about 15% of the code is concerned
with running existing components in the prepro-
cessing step, while the rest deals with NP extrac-
tion, implementations of features, clustering algo-
rithms and scorers. More details about Recon-
cile?s architecture and available components and
features can be found in Stoyanov et al (2010).
4 Evaluation
4.1 Data Sets
Reconcile incorporates the six most commonly
used coreference resolution data sets, two from the
MUC conferences (MUC-6, 1995; MUC-7, 1997)
and four from the ACE Program (NIST, 2004).
For ACE, we incorporate only the newswire por-
tion. When available, Reconcile employs the stan-
dard test/train split. Otherwise, we randomly split
the data into a training and test set following a
70/30 ratio. Performance is evaluated according
to the B3 and MUC scoring metrics.
4.2 The Reconcile2010 Configuration
Reconcile can be easily configured with differ-
ent algorithms for markable detection, anaphoric-
ity determination, feature extraction, etc., and run
against several scoring metrics. For the purpose of
this sample evaluation, we create only one partic-
ular instantiation of Reconcile, which we will call
Reconcile2010 to differentiate it from the general
platform. Reconcile2010 is configured using the
following components:
1. Preprocessing
(a) Sentence Splitter: OpenNLP
(b) Tokenizer: OpenNLP
(c) POS Tagger: OpenNLP
(d) Parser: Berkeley
(e) Named Entity Recognizer: Stanford
2. Feature Set - A hand-selected subset of 60 out of the
more than 80 features available. The features were se-
lected to include most of the features from Soon et al
Soon et al (2001), Ng and Cardie (2002) and Bengtson
and Roth (2008).
3. Classifier - Averaged Perceptron
4. Clustering - Single-link - Positive decision threshold
was tuned by cross validation of the training set.
4.3 Experimental Results
The first two rows of Table 3 show the perfor-
mance of Reconcile2010. For all data sets, B3
scores are higher than MUC scores. The MUC
score is highest for the MUC6 data set, while B3
scores are higher for the ACE data sets as com-
pared to the MUC data sets.
Due to the difficulties outlined in Section 1,
results for Reconcile presented here are directly
comparable only to a limited number of scores
reported in the literature. The bottom three
rows of Table 3 list these comparable scores,
which show that Reconcile2010 exhibits state-of-
the-art performance for supervised learning-based
coreference resolvers. A more detailed study of
Reconcile-based coreference resolution systems
in different evaluation scenarios can be found in
Stoyanov et al (2009).
5 Conclusions
Reconcile is a general architecture for coreference
resolution that can be used to easily create various
coreference resolvers. Reconcile provides broad
support for experimentation in coreference reso-
lution, including implementation of the basic ar-
chitecture of contemporary state-of-the-art coref-
erence systems and a variety of individual mod-
ules employed in these systems. Additionally,
Reconcile handles all of the formatting and scor-
ing peculiarities of the most widely used coref-
erence resolution data sets (those created as part
of the MUC and ACE conferences) and, thus,
allows for easy implementation and evaluation
across these data sets. We hope that Reconcile
will support experimental research in coreference
resolution and provide a state-of-the-art corefer-
ence resolver for both researchers and application
developers. We believe that in this way Recon-
cile will facilitate meaningful and consistent com-
parisons of coreference resolution systems. The
full Reconcile release is available for download at
http://www.cs.utah.edu/nlp/reconcile/.
159
System Score Data sets
MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
Reconcile2010
MUC 68.50 62.80 65.99 67.87 62.03 67.41
B3 70.88 65.86 78.29 79.39 76.50 73.71
Soon et al (2001) MUC 62.6 60.4 ? ? ? ?
Ng and Cardie (2002) MUC 70.4 63.4 ? ? ? ?
Yang et al (2003) MUC 71.3 60.2 ? ? ? ?
Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.
Acknowledgments
This research was supported in part by the Na-
tional Science Foundation under Grant # 0937060
to the Computing Research Association for the
CIFellows Project, Lawrence Livermore National
Laboratory subcontract B573245, Department of
Homeland Security Grant N0014-07-1-0152, and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
The authors would like to thank the anonymous
reviewers for their useful comments.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Workshop
at the Language Resources and Evaluation Conference.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
E. Bengtson and D. Roth. 2008. Understanding the value of
features for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
CC Group. 2009. Sentence Segmentation Tool.
http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.
C. Chang and C. Lin. 2001. LIBSVM: a Li-
brary for Support Vector Machines. Available at
http://www.csie.ntu.edu.tw/cjlin/libsvm.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
Non-local Information into Information Extraction Sys-
tems by Gibbs Sampling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the ACL.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of the Twenty-
second International Conference on Machine Learning
(ICML 2005).
A. Haghighi and D. Klein. 2007. Unsupervised Coreference
Resolution in a Nonparametric Bayesian Model. In Pro-
ceedings of the 45th Annual Meeting of the ACL.
T. Joachims. 2002. SVMLight, http://svmlight.joachims.org.
D. Klein and C. Manning. 2003. Fast Exact Inference with
a Factored Model for Natural Language Parsing. In Ad-
vances in Neural Information Processing (NIPS 2003).
S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535?561.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of the 42nd Annual Meeting of the ACL.
X. Luo. 2005. On Coreference Resolution Performance
Metrics. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP).
A. McCallum and B. Wellner. 2004. Conditional Models
of Identity Uncertainty with Application to Noun Coref-
erence. In Advances in Neural Information Processing
(NIPS 2004).
MUC-6. 1995. Coreference Task Definition. In Proceedings
of the Sixth Message Understanding Conference (MUC-
6).
MUC-7. 1997. Coreference Task Definition. In Proceed-
ings of the Seventh Message Understanding Conference
(MUC-7).
V. Ng and C. Cardie. 2002. Improving Machine Learning
Approaches to Coreference Resolution. In Proceedings of
the 40th Annual Meeting of the ACL.
NIST. 2004. The ACE Evaluation Plan. NIST.
S. Petrov and D. Klein. 2007. Improved Inference for Un-
lexicalized Parsing. In Proceedings of the Joint Meeting
of the Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2007).
M. Poesio and M. Kabadjov. 2004. A general-purpose,
off-the-shelf anaphora resolution module: implementation
and preliminary evaluation. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2004. A public reference
implementation of the rap anaphora resolution algorithm.
In Proceedings of the Language Resources and Evaluation
Conference.
W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Ap-
proach to Coreference of Noun Phrases. Computational
Linguistics, 27(4):521?541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Co-
nundrums in noun phrase coreference resolution: Mak-
ing sense of the state-of-the-art. In Proceedings of
ACL/IJCNLP.
160
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, and
D. Hysom. 2010. Reconcile: A coreference resolution
research platform. Technical report, Cornell University.
Y. Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern,
J. Smith, X. Yang, and A. Moschitti. 2008. BART: A
modular toolkit for coreference resolution. In Proceed-
ings of the Language Resources and Evaluation Confer-
ence.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Coreference
Scoring Theme. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st Annual Meeting of the ACL.
161
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 81?86,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Domain-Specific Coreference Resolution with Lexicalized Features
Nathan Gilbert and Ellen Riloff
School of Computing
University of Utah
50 S. Central Campus Dr.
Salt Lake City, UT 84112
USA
{ngilbert,riloff}@cs.utah.edu
Abstract
Most coreference resolvers rely heavily on
string matching, syntactic properties, and
semantic attributes of words, but they lack
the ability to make decisions based on in-
dividual words. In this paper, we ex-
plore the benefits of lexicalized features
in the setting of domain-specific corefer-
ence resolution. We show that adding
lexicalized features to off-the-shelf coref-
erence resolvers yields significant perfor-
mance gains on four domain-specific data
sets and with two types of coreference res-
olution architectures.
1 Introduction
Coreference resolvers are typically evaluated on
collections of news articles that cover a wide range
of topics, such as the ACE (ACE03, 2003; ACE04,
2004; ACE05, 2005) and OntoNotes (Pradhan
et al, 2007) data sets. Many NLP applica-
tions, however, involve text analysis for special-
ized domains, such as clinical medicine (Gooch
and Roudsari, 2012; Glinos, 2011), legal text anal-
ysis (Bouayad-Agha et al, 2009), and biological
literature (Batista-Navarro and Ananiadou, 2011;
Castan?o et al, 2002). Learning-based corefer-
ence resolvers can be easily retrained for a spe-
cialized domain given annotated training texts for
that domain. However, we found that retraining
an off-the-shelf coreference resolver with domain-
specific texts showed little benefit.
This surprising result led us to question the na-
ture of the feature sets used by noun phrase (NP)
coreference resolvers. Nearly all of the features
employed by recent systems fall into three cate-
gories: string match and word overlap, syntactic
properties (e.g., appositives, predicate nominals,
parse features, etc.), and semantic matching (e.g.,
gender agreement, WordNet similarity, named en-
tity classes, etc.). Conspicuously absent from most
systems are lexical features that allow the classi-
fier to consider the specific words when making a
coreference decision. A few researchers have ex-
perimented with lexical features, but they achieved
mixed results in evaluations on broad-coverage
corpora (Bengston and Roth, 2008; Bjo?rkelund
and Nugues, 2011; Rahman and Ng, 2011a).
We hypothesized that lexicalized features can
have a more substantial impact in domain-specific
settings. Lexical features can capture domain-
specific knowledge and subtle semantic distinc-
tions that may be important within a domain.
For example, based on the resolutions found in
domain-specific training sets, our lexicalized fea-
tures captured the knowledge that ?tomcat? can
be coreferent with ?plane?, ?UAW? can be coref-
erent with ?union?, and ?anthrax? can be coref-
erent with ?diagnosis?. Capturing these types of
domain-specific information is often impossible
using only general-purpose resources. For exam-
ple, WordNet defines ?tomcat? only as an animal,
does not contain an entry for ?UAW?, and catego-
rizes ?anthrax? and ?diagnosis? very differently.1
In this paper, we evaluate the impact of lexi-
calized features on 4 domains: management suc-
cession (MUC-6 data), vehicle launches (MUC-7
data), disease outbreaks (ProMed texts), and ter-
rorism (MUC-4 data). We incorporate lexical-
ized feature sets into two different coreference ar-
chitectures: Reconcile (Stoyanov et al, 2010), a
pairwise coreference classifier, and Sieve (Raghu-
nathan et al, 2010), a rule-based system. Our re-
sults show that lexicalized features significantly
improve performance in all four domains and in
both types of coreference architectures.
2 Related Work
We are not the first researchers to use lexicalized
features for coreference resolution. However, pre-
1WordNet defines ?anthrax? as a disease (condition/state)
and ?diagnosis? as an identification (discovery event).
81
PPPPPPTrain
Test MUC-6 MUC-7 Promed MUC-4
P R F P R F P R F P R F
MUC-6 80.79 62.71 70.61 84.33 61.74 71.29 83.54 70.34 76.37 80.22 60.81 69.18
MUC-7 74.78 65.59 69.88 82.73 64.09 72.23 85.29 71.82 77.98 77.35 64.19 70.16
Promed 73.60 64.20 68.60 82.88 63.37 71.82 80.31 72.66 76.29 74.52 65.65 69.80
MUC-4 69.27 65.66 67.42 71.49 67.22 69.29 76.92 74.25 75.56 71.76 67.37 69.50
Table 1: Cross-domain B3 (Bagga and Baldwin, 1998) results for Reconcile with its general feature set.
The Paired Permutation test (Pesarin, 2001) was used for statistical significance testing and gray cells
represent results that are not significantly different from the best result.
vious work has evaluated the benefit of lexical fea-
tures only for broad-coverage data sets.
Bengston and Roth (2008) incorporated a mem-
orization feature to learn which entities can re-
fer to one another. They created a binary fea-
ture for every pair of head nouns, including pro-
nouns. They reported no significant improvement
from these features on the ACE 2004 data.
Rahman and Ng (2011a) also utilized lexical
features, going beyond strict memorization with
methods to combat data sparseness and incorpo-
rating semantic information. They created a fea-
ture for every ordered pair of head nouns (for
pronouns and nominals) or full NPs (for proper
nouns). Semi-lexical features were also used when
one NP was a Named Entity, and unseen features
were used when the NPs were not in the training
set. Their features did yield improvements on both
the ACE 2005 and OntoNotes-2 data, but the semi-
lexical features included Named Entity classes as
well as word-based features.
Rahman and Ng (2011b) explored the use of
lexical features in greater detail and showed their
benefit on the ACE05 corpus independent of, and
combined with, a conventional set of coreference
features. The ACE05 corpus is drawn from six
sources (Newswire, Broadcast News, Broadcast
Conversations, Conversational Telephone Speech,
Webblogs, and Usenet). The authors experi-
mented with utilizing lexical information drawn
from different sources. The results showed that
the best performance came from training and test-
ing with lexical knowledge drawn from the same
source. Although our approach is similar, this pa-
per focuses on learning lexical information from
different domains as opposed to the different gen-
res found in the six sources of the ACE05 corpus.
Bjo?rkelund and Nugues (2011) used lexical
word pairs for the 2011 CoNLL Shared Task,
showing significant positive impact on perfor-
mance. They used over 2000 annotated docu-
ments from the broad-coverage OntoNotes corpus
for training. Our work aims to show the benefit of
lexical features using much smaller training sets
(< 50 documents) focused on specific domains.
Lexical features have also been used for slightly
different purposes. Florian et al (2004) utilized
lexical information such as mention spelling and
context for entity tracking in ACE. Ng (2007) used
lexical information to assess the likelihood of a
noun phrase being anaphoric, but this did not show
clear improvements on ACE data.
There has been previous work on domain-
specific coreference resolution for several do-
mains, including biological literature (Castan?o et
al., 2002; Liang and Lin, 2005; Gasperin and
Briscoe, 2008; Kim et al, 2011; Batista-Navarro
and Ananiadou, 2011), clinical medicine (He,
2007; Zheng et al, 2011; Glinos, 2011; Gooch and
Roudsari, 2012) and legal documents (Bouayad-
Agha et al, 2009). In addition, BABAR (Bean and
Riloff, 2004) used contextual role knowledge for
coreference resolution in the domains of terrorism
and natural disasters. But BABAR acquired and
used lexical information to match the compatibil-
ity of contexts surrounding NPs, not the NPs them-
selves. To the best of our knowledge, our work is
the first to examine the impact of lexicalized fea-
tures for domain-specific coreference resolution.
3 Exploiting Lexicalized Features
Table 1 shows the performance of a learning-based
coreference resolver, Reconcile (Stoyanov et al,
2010), with its default feature set using different
combinations of training and testing data. Recon-
cile does not include any lexical features, but does
contain over 60 general features covering seman-
tic agreement, syntactic constraints, string match
and recency.
Each row represents a training set, each column
represents a test set, and each cell shows precision
(P), recall (R), and F score results under the B3
metric when using the corresponding training and
test data. The best results for each test set appear
82
MUC-6 MUC-7 ProMED MUC-4
P R F P R F P R F P R F
Reconcile 80.79 62.71 70.61 82.73 64.09 72.23 80.31 72.66 76.29 71.76 67.37 69.50
+LexLookup 87.01 63.40 73.35 87.39 62.86 73.12 86.66 70.95 78.02 82.89 67.53 74.42
+LexSets 86.50 63.76 73.41 85.86 64.35 73.56 86.19 72.14 78.54 81.98 67.73 74.18
Sieve 92.20 61.70 73.90 91.46 59.59 72.16 94.43 67.25 78.55 91.30 59.84 72.30
+LexBegin 91.22 62.97 74.51 91.24 60.28 72.59 93.51 69.15 79.51 89.01 62.84 73.67
+LexEnd 90.59 63.47 74.64 91.17 60.56 72.78 93.99 68.87 79.49 89.04 64.03 74.47
Table 2: B3 results for baselines and lexicalized feature sets across four domains.
in boldface.
We performed statistical significance testing us-
ing the Paired Permutation test (Pesarin, 2001) and
the gray cells represent results where there was
not significant difference from the best results in
the same column. If just one cell is gray in a col-
umn, that indicates the result was significantly bet-
ter than the other results in the same column with
p ? 0.05.
Table 1 does not show much benefit from train-
ing on the same domain as the test set. Three
different training sets produce F scores that are
not significantly different for both the MUC-6
and MUC-4 test data. For ProMed, training on
the MUC-7 data yields significantly better results
than training on all the other data sets, includ-
ing ProMed texts! Based on these results, it
would seem that training on the MUC-7 texts is
likely to yield the best results no matter what do-
main you plan to use the coreference resolver for.
The goal of our work is to investigate whether
lexical features can extract additional knowledge
from domain-specific training texts to help tailor
a coreference resolver to perform better for a spe-
cific domain.
3.1 Extracting Coreferent Training Pairs
We adopt the terminology introduced by Stoyanov
et al (2009) to define a coreference element (CE)
as a noun phrase that can participate in a corefer-
ence relation based on the task definition.
Each training document has manually annotated
gold coreference chains corresponding to the sets
of CEs that are coreferent. For each CE in a gold
chain, we pair that CE with all of the other CEs in
the same chain. We consider the coreference rela-
tion to be bi-directional, so we don?t retain infor-
mation about which CE was the antecedent. We
do not extract CE pairs that share the same head
noun because they are better handled with string
match. For nominal NPs, we retain only the head
noun, but we use the entire NP for proper names.
We discard pairs that include a pronoun, and nor-
malize strings to lower case for consistency.
3.2 Lexicalized Feature Sets
We explore two ways to capture lexicalized infor-
mation as features. The first approach indicates
whether two CEs have ever been coreferent in the
training data. We create a single feature called
LEXLOOKUP(X,Y) that receives a value of 1 when
x and y have been coreferent at least twice, or
a value of 0 otherwise.2 LEXLOOKUP(X,Y) is a
single feature that captures all CE pairs that were
coreferent in the training data.
We also created set-based features that capture
the set of terms that have been coreferent with a
particular CE. The CorefSet(x) is the set of CEs
that have appeared in the same coreference chain
as mention x at least twice.
We create a set of binary-valued features
LEXSET(X,Y), one for each CE x in the training
data. Given a pair of CEs, x and y, LEXSET(X,Y)
= 1 if y ? CorefSet(x), or 0 otherwise. The ben-
efit of the set-based features over a single mono-
lithic feature is that the classifier has one set-based
feature for each mention found in the training data,
so it can learn to handle individual terms differ-
ently.
We also tried encoding a separate feature for
each distinct pair of words, analogous to the mem-
orization feature in Bengston and Roth (2008).
This did not improve performance as much as the
other feature representations presented here.
4 Evaluation
4.1 Data Sets
We evaluated the performance of lexicalized fea-
tures on 4 domain-specific corpora including two
standard coreference benchmarks, the MUC-6 and
MUC-7 data sets. The MUC-6 domain is manage-
ment succession and consists of 30 training texts
and 30 test texts. The MUC-7 domain is vehicle
2We require a frequency ? 2 to minimize overfitting be-
cause many cases occur only once in the training data.
83
launches and consists of 30 training texts and 20
test texts. We used these standard train/test splits
to be consistent with previous work.
We also created 2 new coreference data sets
which we will make freely available. We
manually annotated 45 ProMed-mail articles
(www.promedmail.org) about disease outbreaks
and 45 MUC-4 texts about terrorism, following
the MUC guidelines (Hirschman, 1997). Inter-
annotator agreement between two annotators was
.77 (?) on ProMed and .84 (MUC F Score)(Villain
et al, 1995) on both ProMed and MUC-4.3 We
performed 5-fold cross-validation on both data
sets and report the micro-averaged results.
Gold CE spans were used in all experiments to
factor out issues with markable identification and
anaphoricity across the different domains.
4.2 Coreference Resolution Models
We conducted experiments using two coreference
resolution architectures. Reconcile4 (Stoyanov et
al., 2010) is a freely available pairwise mention
classifier. For classification, we chose Weka?s
(Witten and Frank, 2005) Decision Tree learner
inside Reconcile. Reconcile contains roughly 60
features (none lexical), largely modeled after Ng
and Cardie (2002). We modified Reconcile?s Sin-
gle Link clustering scheme to enforce an addi-
tional rule that non-overlapping proper names can-
not be merged into the same chain.
We also conducted experiments with the Sieve
coreference resolver, which applies high precision
heuristic rules to incrementally build coreference
chains. We implemented the LEXLOOKUP(X,Y)
feature as an additional heuristic rule. We tried
inserting this heuristic before Sieve?s other rules
(LexBegin), and also after Sieve?s other rules
(LexEnd).
4.3 Experimental Results
Table 2 presents results for Reconcile trained with
and without lexical features and when adding
a lexical heuristic with data drawn from same-
domain texts to Sieve.
The first row shows the results without the lex-
icalized features (from Table 1). All F scores
for Reconcile with lexicalized features are signifi-
cantly better than without these features based on
the Paired Permutation test (Pesarin, 2001) with
3We also computed ? on MUC-4, but unfortunately the
score and original data were lost.
4http://www.cs.utah.edu/nlp/reconcile/
p ? 0.05. MUC-4 showed the largest gain for
Reconcile, with the F score increasing from 69.5
to over 74. For most domains, adding the lexical
features to Reconcile substantially increased pre-
cision with comparable levels of recall.
The bottom half of Table 2 contains the results
of adding a lexical heuristic to Sieve. The first
row shows the default system with no lexical in-
formation. All F scores with the lexical heuristic
are significantly better than without it. In Sieve?s
high-precision coreference architecture, the lexi-
cal heuristic yields additional recall gains without
sacrificing much precision.
ACE 2004
P R F
Reconcile 70.59 83.09 76.33
+LexLookup 71.32 82.93 76.69
+LexSets 71.44 83.45 76.98
Sieve 90.09 74.23 81.39
+LexBegin 86.54 75.43 80.61
+LexEnd 87.00 75.45 80.82
Table 3: B3 results for baselines and lexicalized
feature sets on the broad-coverage ACE 2004 data
set.
Table 3 shows the results for Reconcile and
Sieve when training and testing on the ACE 2004
data. Here, we see little improvement from adding
lexical information. For Reconcile, the small dif-
ferences in F scores are not statistically significant.
For Sieve, the unlexicalized system yields a signif-
icantly higher F score than when adding the lexi-
cal heuristic. These results support our hypothesis
that lexicalized information can be beneficial for
capturing domain-specific word associations, but
may not be as helpful in a broad-coverage setting
where the language covers a diverse set of topics.
Table 4 shows a re-evaluation of the cross-
domain experiments from Table 1 for Reconcile
with the LexSet features added. The bottom half
of the table shows cross-domain experiments for
Sieve using the lexical heuristic at the end of its
rule set (LexEnd). Results are presented using
both the B3 metric and the MUC Score (Villain
et al, 1995).
Training and testing on the same domain al-
ways produced the highest recall scores for MUC-
7, ProMed, and MUC-4 when utilizing lexical
features. In all cases, lexical features acquired
from same-domain texts yield results that are ei-
ther clearly the best or not significantly different
from the best.
84
PPPPPPTrain
Test MUC-6 MUC-7 Promed MUC-4
P R F P R F P R F P R F
Reconcile (B3 Score)
MUC-6 86.50 63.76 73.41 90.44 60.75 72.68 89.28 68.14 77.29 84.05 60.61 70.44
MUC-7 80.65 63.42 71.01 85.86 64.46 73.56 89.41 70.05 78.55 80.61 63.26 70.89
Promed 81.69 62.73 70.96 88.32 62.79 73.40 86.19 72.14 78.54 84.81 62.58 72.02
MUC-4 81.20 62.34 70.53 87.23 63.13 73.25 87.52 71.11 78.46 81.98 67.73 74.18
Reconcile (MUC Score)
MUC-6 89.56 71.17 79.32 90.85 67.43 77.41 89.61 65.67 75.79 88.27 66.98 76.16
MUC-7 86.14 72.22 78.57 89.56 72.01 79.83 89.34 68.08 77.27 87.30 70.22 77.83
Promed 86.92 70.68 77.97 90.93 70.33 79.31 88.54 69.55 77.90 88.83 68.89 78.23
MUC-4 85.72 70.50 77.37 88.78 71.24 79.05 88.24 68.18 77.55 87.89 74.18 80.45
Sieve (B3 Score)
MUC-6 90.59 63.47 74.64 91.20 59.91 72.32 94.30 67.25 78.51 91.30 59.90 72.34
MUC-7 91.62 63.67 75.13 91.17 60.56 72.78 94.43 67.35 78.62 91.14 60.44 72.68
Promed 92.14 61.70 73.90 91.46 59.93 72.41 93.99 68.87 79.49 91.27 60.76 72.96
MUC-4 91.76 61.88 73.91 91.26 59.93 72.34 94.30 67.35 78.58 89.04 64.03 74.47
Sieve (MUC Score)
MUC-6 91.80 70.87 79.99 91.38 65.52 76.32 92.08 64.71 76.01 90.38 66.98 77.10
MUC-7 91.82 69.70 79.25 91.68 66.36 76.99 92.20 64.86 76.15 90.71 67.09 77.13
Promed 91.99 69.15 78.95 91.68 65.52 76.42 91.70 66.33 76.98 90.85 67.09 77.18
MUC-4 91.79 69.39 79.03 91.48 65.52 76.36 92.00 64.86 76.08 90.31 69.62 78.62
Table 4: Cross-domain B3 and MUC results for Reconcile and Sieve with lexical features. Gray cells
represent results that are not significantly different from the best results in the column at the 0.05 p-level.
For MUC-6 and MUC-7, the highest F score re-
sults almost always come from training on same-
domain texts, although in some cases these re-
sults are not significantly different from training
on other domains. Lexical features can yield im-
provements when training on a different domain if
there is overlap in the vocabulary across the do-
mains. For the ProMed domain, the Sieve system
performs significantly better, under both metrics,
with same-domain lexical features than with lexi-
cal features acquired from a different domain. For
Reconcile, there is not a significant difference in
the F score for ProMed when training on ProMed,
MUC-4, or MUC-7. In the MUC-4 domain, using
same-domain lexical information always produces
the best F score, under both metrics and in both
coreference systems.
5 Conclusions
We explored the use of lexical information for
domain-specific coreference resolution using 4
domain-specific data sets and 2 coreference re-
solvers. Lexicalized features consistently im-
proved performance for all of the domains and in
both coreference architectures. We see benefits
from lexicalized features in cross-domain training,
but the gains are often more substantial when uti-
lizing same-domain lexical knowledge.
In the future, we plan to explore additional types
of lexical information to benefit domain-specific
coreference resolution.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant
No. IIS-1018314 and the Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0172.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the view
of the DARPA, AFRL, or the U.S. government.
References
ACE03. 2003. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2003.
ACE04. 2004. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2004.
ACE05. 2005. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2005.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreference using the Vector Space
Model. Proceedings of the 17th international con-
ference on Computational Linguistics (COLING).
Riza Theresa Batista-Navarro and Sophia Ananiadou.
2011. Building a coreference-annotated corpus
from the domain of biochemistry. In Proceedings of
BioNLP 2011 Workshop, BioNLP ?11, pages 83?91.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of Contextual Role Knowledge for coref-
erence resolution. Proceedings of the HLT/NAACL
2004.
85
Eric Bengston and Dan Roth. 2008. Understanding the
value of features for coreference resolution. Empir-
ical Methods in Natural Language Processing.
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution.
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50.
Nadjet Bouayad-Agha, Gerard Casamayor, Gabriela
Ferraro, Simon Mille, Vanesa Vidal, and Leo Wan-
ner. 2009. Improving the comprehension of legal
documentation: the case of patent claims. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, pages 78?87.
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature.
International Symposium on Reference Resolution.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, Salim Roukos, and T Zhang. 2004.
A statistical model for multilingual entity detection
and tracking. HLT-NAACL.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. Proceed-
ings of the 22nd Annual Conference on Computa-
tional Linguistics, pages 257?264.
Demetrios G. Glinos. 2011. A search based method for
clinical text coreference resolution. In Proceedings
of the Fifth i2b2/VA Track on Challenges in Natural
Language Processing for Clinical Data (i2b2 2011).
Phil Gooch and Abdul Roudsari. 2012. Lexical pat-
terns, features and knowledge resources for corefer-
ence resolution in clinical notes. Journal of Biomed-
ical Informatics, 45.
Tian Ye He. 2007. Coreference resolution on entities
and events for hospital discharge summaries. Ph.D.
thesis, Massachusetts Institute of Technology.
Lynette Hirschman. 1997. MUC-7 task definition.
Proceedings of MUC-7.
Youngjun Kim, Ellen Riloff, and Nathan Gilbert. 2011.
The taming of Reconcile as a Biomedical corefer-
ence resolver. ACL/HLT 2011 Workshop on Biomed-
ical Natural Language Processing (BioNLP 2011)
Shared Task Paper.
Tyne Liang and Yu-Hsiang Lin. 2005. Anaphora
resolution for biomedical literature by exploiting
multiple resources. Natural Language Processing?
IJCNLP 2005, pages 742?753.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
Proceedings of the 40th Annual Meeting of the ACL,
pages 104?111.
Vincent Ng. 2007. Shallow semantics for corefer-
ence resolution. Proceedings of the Twentieth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07), pages 1689?1694.
Fortunato Pesarin. 2001. Multivariate permutation
tests: with applications in biostatistics, volume 240.
Wiley Chichester.
Sameer S. Pradhan, Lance Ramshaw, Ralph
Weischedel, Jessice MacBride, and Linnea Micci-
ulla. 2007. Unrestricted coreference: Identifying
entities and events in ontonotes. In Proceedings
of the International Conference on Semantic
Computing.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A Multi-
Pass Sieve for coreference resolution. Empirical
Methods in Natural Langugage Processing 2010.
Altaf Rahman and Vincent Ng. 2011a. Coreference
resolution with world knowledge. Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics and Human Language Tech-
nologies (ACL-HLT), pages 814?824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modelling gap: A cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the State-
of-the-Art. Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th IJC-
NLP (ACL-IJCNLP 2009).
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2010. Coreference resolution with
Reconcile. Proceedings of the Joint Conference of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2010).
Marc Villain, John Aberdeen, John Berger, Dennis
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. Proceedings
of the 6th conference on Message understanding.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, 2nd edition.
Jiaping Zheng, Wendy Chapman, Rebecca Crowley,
and Guergana Savova. 2011. Coreference resolu-
tion: A review of general methodologies and appli-
cations in the clinical domain. Journal of Biomedi-
cal Informatics, 44:1113?1122.
86
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 17?25,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Toward Plot Units: Automatic Affect State Analysis
Amit Goyal and Ellen Riloff and Hal Daume III and Nathan Gilbert
School of Computing
University of Utah
Salt Lake City, UT 84112
{amitg,riloff,hal,ngilbert}@cs.utah.edu
Abstract
We present a system called AESOP that au-
tomatically produces affect states associated
with characters in a story. This research repre-
sents a first step toward the automatic genera-
tion of plot unit structures from text. AESOP
incorporates several existing sentiment analy-
sis tools and lexicons to evaluate the effective-
ness of current sentiment technology on this
task. AESOP also includes two novel compo-
nents: a method for acquiring patient polar-
ity verbs, which impart negative affect on their
patients, and affect projection rules to propa-
gate affect tags from surrounding words onto
the characters in the story. We evaluate AE-
SOP on a small collection of fables.
1 Introduction
In the 1980s, plot units (Lehnert, 1981) were pro-
posed as a knowledge structure for representing nar-
rative stories and generating summaries. Plot units
are fundamentally different from the story represen-
tations that preceded them because they focus on the
emotional states and tensions between characters as
the driving force behind interesting plots and cohe-
sive stories. Plot units were used in narrative sum-
marization studies, both in computer science and
psychology (Lehnert et al, 1981), but the compu-
tational models of plot units relied on tremendous
amounts of manual knowledge engineering.
Given the recent swell of activity in automated
methods for sentiment analysis, we embarked on a
project to see whether current techniques could auto-
matically detect the affect states needed for plot unit
analysis. Plot units are complex structures that in-
clude affect states, causal links, and cross-character
links, and generating complete plot unit structures is
beyond the scope of this work. As an initial step to-
ward the long-term goal of automatically generating
plot units, we began by creating a system to automat-
ically identify the affect states associated with char-
acters. An affect state represents the emotional state
of a character, based on their perspective of events
in the story. Plots units include three types of af-
fect states: positive (+) states, negative (-) states, and
mental (M) states that have neutral emotion (these
are often associated with plans and goals).
Our system, called AESOP, pulls together a va-
riety of existing technologies in sentiment analy-
sis to automatically identify words and phrases that
have positive/negative polarity or that correspond
to speech acts (for mental states). However, we
needed to develop a method to automatically map
these affect tags onto characters in the story.1 To
address this issue, we created affect projection rules
that propagate affect tags from words and phrases to
characters in the story via syntactic relations.
During the course of our research, we came to ap-
preciate that affect states, of the type required for
plot units, can represent much more than just di-
rect expressions of emotion. A common phenom-
ena are affect states that result from a character be-
ing acted upon in a positive or negative way. For
example, ?the cat ate the mouse? produces a pos-
itive affect state for the cat and a negative affect
1This is somewhat analogous to, but not exactly the same as,
associating opinion words with their targets or topics (Kim and
Hovy, 2006; Stoyanov and Cardie, 2008).
17
The Father and His Sons
(s1) A father had a family of sons who were perpetually
quarreling among themselves. (s2) When he failed to
heal their disputes by his exhortations, he determined to
give them a practical illustration of the evils of disunion;
and for this purpose he one day told them to bring him a
bundle of sticks. (s3) When they had done so, he placed
the faggot into the hands of each of them in succession,
and ordered them to break it in pieces. (s4) They tried
with all their strength, and were not able to do it. (s5) He
next opened the faggot, took the sticks separately, one by
one, and again put them into his sons? hands, upon which
they broke them easily. (s6) He then addressed them in
these words: ?My sons, if you are of one mind, and unite
to assist each other, you will be as this faggot, uninjured
by all the attempts of your enemies; but if you are divided
among yourselves, you will be broken as easily as these
sticks.?
(a) ?Father and Sons? Fable
Father Sons
(quarreling)a1
(stop quarreling)a3
(annoyed)a2
(exhortations)a4
(exhortations fail)a5
m
m
a
(teach lesson)a6
m
(get sticks & break)a7
m
(get sticks & break)a8
(cannot break sticks)a9
a
(cannot break sticks)a10
a
(bundle & break)a11
(bundle & break)a12
(break sticks)a13
a
(break sticks)a14
a
m
a
shared
request
request
shared
shared
s2
s2
s2
s2
s2
s2
s4
s5
s5
s1
s2
s4
s5
s5
(lesson succeeds)a15s5
(b) Plot Unit Analysis for ?Father and Sons? Fable
state for the mouse because obtaining food is good
but being eaten is bad. This type of world knowl-
edge is difficult to obtain, yet essential for plot unit
analysis. In AESOP, we use corpus statistics to au-
tomatically learn a set of negative patient polarity
verbs which impart a negative polarity on their pa-
tient (e.g., eaten, killed, injured, fired). To acquire
these verbs, we queried a large corpus with patterns
to identify verbs that frequently occur with agents
who stereotypically have evil intent.
We evaulate our complete system on a set of AE-
SOP?s fables. In this paper, we also explain and cat-
egorize different types of situations that can produce
affect states, several of which cannot be automati-
cally recognized by existing sentiment analysis tech-
nology. We hope that one contribution of our work
will be to create a better awareness of, and apprecia-
tion for, the different types of language understand-
ing mechanisms that will ultimately be necessary for
comprehensive affect state analysis.
2 Overview of Plot Units
Narratives can often be understood in terms of the
emotional reactions and affect states of the char-
acters therein. The plot unit formalism (Lehnert,
1981) provides a representational mechanism for af-
fect states and the relationships between them. Plot
unit structures can be used for tasks such as narrative
summarization and question answering.
Plot unit structures consist of affect states for each
character in a narrative, and links explaining the re-
lationships between these affect states. The affect
states themselves each have a type: (+) for positive
states, (-) for negative states, and (M) for mental
states (with neutral affect). Although affect states
are not events per se, events often trigger affect
states. If an event affects multiple characters, it can
trigger multiple affect states, one for each character.
Affect states are further connected by causal links,
which explain how the narrative hangs together.
These include motivations (m), actualizations (a),
terminations (t) and equivalences (e). Causal links
exist between affect states for the same character.
Cross-character links explain how single events af-
fect two characters. For instance, if one character
requests something of the other, this is an M-to-M
link, since it spans a shared mental affect for both
characters. Other speech acts can be represented as
M to + (promise) or M to - (threat).
To get a better feeling of the plot unit represen-
tation, a short fable, ?The Father and His Sons,? is
shown in Figure 1(a) and our annotation of its plot
unit structure is shown in Figure 1(b). In this fa-
ble, there are two characters (the ?Father? and the
?Sons?) who go through a series of affect states, de-
picted chronologically in the two columns.
In this example, the first affect state is a negative
state for the sons, who are quarreling (a1). This state
is shared by the father (via a cross-character link)
who has a negative annoyance state (a2). The fa-
ther then decides that he wants to stop the sons from
quarreling, which is a mental event (a3). The causal
link from a2 to a3 with an m label indicates a ?mo-
tivation.? His first attempt is by exhortations (a4).
18
This produces an M (a3) linked to an M (a4) with
a m (motivation) link, which represents subgoaling.
The father?s overall goal is to stop the quarreling
(a3) and in order to do so, he creates a subgoal of
exhorting the sons to stop (a4). The exhortations
fail, which produces a negative state (a5) for the fa-
ther. The a causal link indicates an ?actualization?,
representing the failure of the plan (a4).
The failure of the father?s exhortations leads to a
new subgoal: to teach the sons a lesson (a6). The m
link from a5 to a6 is an example of ?enablement.?
At a high level, this subgoal has two parts, indicated
by the two gray regions (a7 ? a10 and a11 ? a14).
The first gray region begins with a cross-character
link (M to M), which indicates a request (in this case,
to break a bundle of sticks). The sons fail at this,
which upsets them (a9) but pleases the father (a10).
The second gray region depicts the second part of
the father?s subgoal; he makes a second request (a11
to a12) to separate the bundle and break the sticks,
which the sons successfully do, making them happy
(a13) and the father happy (a14). This latter struc-
ture (the second gray region) is an HONORED RE-
QUEST plot unit. At the end, the father?s plan suc-
ceeds (a15) which is an actualization (a link) of his
goal to teach the sons a lesson (a6).
In this example, as well as the others that we an-
notated in our gold standard, (see Section 5.1), we
annotated conservatively. In particular, in reading
the story, we may assume that the father?s origi-
nal plan of stopping the son?s quarrelling also suc-
ceeded. However, this is not mentioned in the story
and therefore we chose not to represent it. It is also
important to note that plot unit representations can
have t (termination) and e (equivalence) links that
point backwards in time, but they do not occur in
the Father and Sons fable.
3 Where Do Affect States Come From?
We began this research with the hope that recent re-
search in sentiment analysis would supply us with
effective tools to recognize affect states. However,
we soon realized that affect states, as required for
plot unit analysis, go well beyond the notions of pos-
itive/negative polarity and private states that have
been studied in recent sentiment analysis work. In
this section, we explain the wide variety of situa-
tions that can produce an affect state, based on our
observations in working with fables. Most likely, an
even wider variety of situations could produce affect
states in other text genres.
3.1 Direct Expressions of Emotion
Plot units can include affect states that correspond to
explicit expressions of positive/negative emotional
states, as has been studied in the realm of sentiment
analysis. For example, ?Max was disappointed?
produces a negative affect state for Max, and ?Max
was pleased? produces a positive affect state for
Max. However, the affect must relate to an event that
occurs in the story?s plot. For example, a hypotheti-
cal expression of emotion would not yield an affect
state (e.g., ?if the rain stops, she will be pleased?).
3.2 Situational Affect States
Positive and negative affect states also frequently
represent good and bad situational states that char-
acters find themselves in. These states do not rep-
resent emotion, but indicate whether a situation is
good or bad for a character based on world knowl-
edge. For example, ?Wolf, who had a bone stuck
in his throat, ...? produces a negative affect state
for the wolf. Similarly, ?The Old Woman recovered
her sight...? produces a positive affect state. Senti-
ment analysis is not sufficient to generate these af-
fect states. Sometimes, however, a direct expression
of emotion will also be present (e.g., ?Wolf was un-
happy because he had a bone stuck...?), providing
redundancy and multiple opportunities to recognize
the correct affect state for a character.
Situational affect states are common and often
motivate plans and goals that are central to the plot.
3.3 Plans and Goals
Plans and goals are another common reason for
affect states. The existence of a plan or goal is
usually represented as a mental state (M). Plans and
goals can be difficult to detect automatically. A
story may reveal that a character has a plan or goal
in a variety of ways, such as:
Direct expressions of plans/goals: a plan or goal
may be explicitly stated (e.g., ?the lion wanted to
find food?). In this case, a mental state (M) should
19
be generated.
Speech acts: a plan or goal may be revealed
through a speech act between characters. For
example, ?the wolf asked an eagle to extract the
bone? is a directive speech act that indicates the
wolf?s plan to resolve its negative state (having a
bone stuck). This example illustrates how a negative
state (bone stuck) can motivate a mental state (plan).
When a speech act involves multiple characters, it
produces multiple mental states. For example, a
mental state should also be produced for the eagle,
because it now has a plan to help the wolf (by virtue
of being asked).
Inferred plans/goals: plans and goals sometimes
must be inferred from actions. For example, ?the
lion hunted deer? reveals the lion?s plan to obtain
food. Similarly, the serpent spat poison into the
man?s water? implies that the serpent had a plan to
kill the man.
Plans and goals also produce positive/negative af-
fect states when they succeed/fail. For example, if
the eagle successfully extracts the bone from the
wolf?s throat, then both the wolf and the eagle will
have positive affect states, because both were suc-
cessful in their respective goals. A directive speech
act between two characters coupled with positive af-
fect states for both characters is a common plot unit
structure called an HONORED REQUEST, depicted
by the second gray block shown in Fig.1(b).
The affect state for a character is always with
respect to its view of the situation. For example,
consider: ?The owl besought a grasshopper to
stop chirping. The grasshopper refused to desist,
and chirped louder and louder.? Both the owl and
the grasshopper have M affect states representing
the request from the owl to the grasshopper (i.e.,
the owl?s plan to stop the chirping is to ask the
grasshopper to knock it off). The grasshopper
refuses the request, so a negative affect state is
produced for the owl, indicating that its plan failed.
However, a positive affect state is produced for
the grasshopper, because its goal was to continue
chirping which was accomplished by refusing the
request. This scenario is also a common plot unit
structure called a DENIED REQUEST.
3.4 Patient Role Affect States
Many affect states come directly from events. In
particular, when a character is acted upon (the theme
or patient of an event), a positive or negative affect
state often results for the character. These affect
states reflect world knowledge about what situations
are good and bad. For example:
Negative patient roles: killed X, ate X, chased X,
captured X, fired X, tortured X
Positive patient roles: rescued X, fed X, adopted X,
housed X, protected X, rewarded X
For example, ?a man captured a bear? indicates a
negative state for the bear. Overall, this sentence
would generate a SUCCESS plot unit consisting of
an M state and a + state for the man (with an actual-
ization a causal link between them representing the
plan?s success) and a - state for the bear (as a cross-
character link indicating that what was good for the
man was bad for the bear). A tremendous amount of
world knowledge is needed to generate these states
from such a seemingly simple sentence. Similarly,
if a character is rescued, fed, or adopted, then a + af-
fect state should be produced for the character based
on knowledge that these events are desirable. We
are not aware of existing resources that can automat-
ically identify affect polarity with respect to event
roles. In Section 4.1.2, we explain how we automat-
ically acquire Patient Polarity Verbs from a corpus
to identify some of these affect states.
4 AESOP: Automatic Affect State Analysis
We created a system, called AESOP, to try to auto-
matically identify the types of affect states that are
required for plot unit analysis. AESOP incorporates
existing resources for sentiment analysis and speech
act recognition, and includes two novel components:
patient polarity verbs, which we automatically gen-
erate using corpus statistics, and affect projection
rules, which automatically project and infer affect
labels via syntactic relations.
AESOP produces affect states in a 3-step process.
First, AESOP labels individual words and phrases
with an M, +, or - affect tag. Second, it identi-
fies all references to the two main characters of the
20
story. Third, AESOP applies affect projection rules
to propagate affect states onto the characters, and in
some cases, to infer new affect states.
4.1 Step 1: Assigning Affect Tags to Words
4.1.1 Sentiment Analysis Resources
AESOP incorporates several existing sentiment
analysis resources to recognize affect states associ-
ated with emotions and speech acts.
? OpinionFinder2 (Wilson et al, 2005) (Version
1.4) is used to identify all three types of states. We
use the +/- labels assigned by its contextual polar-
ity classifier (Wilson, 2005) to create +/- affect tags.
The MPQASD tags produced by its Direct Subjective
and Speech Event Identifier (Choi et al, 2006) are
used as M affect tags.
? Subjectivity Lexicon3 (Wilson, 2005): The pos-
itive/negative words in this list are assigned +/- af-
fect tags, when they occur with the designated part-
of-speech (POS).
? Semantic Orientation Lexicon4 (Takamura et
al., 2005): The positive/negative words in this list
are assigned +/- affect tags, when they occur with
the designated part-of-speech.
? A list of 228 speech act verbs compiled from
(Wierzbicka, 1987)5, which are used for M states.
4.1.2 Patient Polarity Verbs
As we discussed in Section 3.4, existing resources
are not sufficient to identify affect states that arise
from a character being acted upon. Sentiment lexi-
cons, for example, assign polarity to verbs irrespec-
tive of their agents or patients. To fill this gap,
we tried to automatically acquire verbs that have a
strong patient polarity (i.e., the patient will be in a
good or bad state by virtue of being acted upon).
We used corpus statistics to identify verbs that
frequently occur with agents who typically have
evil (negative) or charitable (positive) intent. First,
we identified 40 words that are stereotypically evil
agents, such as monster, villain, terrorist, and mur-
derer, and 40 words that are stereotypically charita-
ble agents, such as hero, angel, benefactor, and res-
cuer. Next, we searched the google Web 1T 5-gram
2http://www.cs.pitt.edu/mpqa/opinionfinderrelease/
3http://www.cs.pitt.edu/mpqa/lexiconrelease/collectinfo1.html
4http://www.lr.pi.titech.ac.jp/?takamura/pndic en.html
5http://openlibrary.org/b/OL2413134M/English speech act verbs
corpus6 using patterns designed to identify verbs
that co-occur with these words as agents. For each
agent term, we applied the pattern ?*ed by [a,an,the]
AGENT? and extracted the list of matching verbs.7
Next, we rank the extracted verbs by computing
the ratio between the frequency of the verb with a
negative agent versus a positive agent. If this ratio
is > 1, then we save the verb as a negative patient
polarity verb (i.e., it imparts negative polarity to its
patient). This process produced 408 negative patient
polarity verbs, most of which seemed clearly neg-
ative for the patient. Table 1 shows the top 20 ex-
tracted verbs. We also tried to identify positive pa-
tient polarity verbs using a positive-to-negative ra-
tio, but the extracted verbs were often neutral for the
patient, so we did not use them.
scammed damaged disrupted ripped
raided corrupted hindered crippled
slammed chased undermined possesed
dogged tainted grounded levied
patched victimized posessed bothered
Table 1: Top 20 negative patient polarity verbs
4.2 Step 2: Identifying the Characters
The problem of coreference resolution in fables
is somewhat different than for other genres, pri-
marily because characters are often animals (e.g.,
?he?=?owl?). So we hand-crafted a simple rule-
based coreference system. For the sake of this task,
we made two assumptions: (1) There are only two
characters per fable, and (2) Both characters are
mentioned in the fable?s title.
We then apply heuristics to determine number and
gender for the characters based on word lists, Word-
Net (Miller, 1990) and POS tags. If no determina-
tion of a character?s gender or number can be made
from these resources, a process of elimination is em-
ployed. Given the two character assumption, if one
character is known to be male, but there are female
pronouns in the fable, then the other character is as-
sumed to be female. The same is done for number
agreement. Finally, if there is only one character be-
tween a pronoun and the beginning of a document,
6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13
7The corpus is not POS tagged so there is no guarantee these
will be verbs, but they usually are in this construction.
21
the pronoun is assumed to corefer with that char-
acter. The character then assumes the gender and
number of that pronoun. Lastly, WordNet is used
to obtain a small set of non-pronominal, non-string-
match resolutions by exploiting hypernym relations,
for instance, linking Peasant with the man.
4.3 Step 3: Affect Projection
Our goal is to produce affect states for each char-
acter in the story. Therefore every affect tag needs
to be attributed to a character, or discarded. Since
plots typically revolve around actions, we used the
verbs as the basis for projecting affect tags onto the
characters. In some cases, we also spawn new affect
tags associated with mental states to indicate that an
action is likely the manifestation of a plan.
We developed 6 types of affect projection rules
that orchestrate how affect tags are assigned to the
characters based on verb argument structure. We
use the Sundance shallow parsing toolkit (Riloff and
Phillips, 2004) to generate a syntactic analysis of
each sentence, including syntactic chunking, clause
segmentation, and active/passive voice recognition.
We normalize the verb phrases (VPs) with respect to
voice (i.e., we transform the passive voice construc-
tions into an active voice equivalent) to simplify our
rules. We then make the assumption that the Subject
of the VP is its AGENT and the Direct Object of the
VP is its PATIENT.8 The affect projection rules only
project affect states onto AGENTS and PATIENTS
that correspond to a character in the story. The five
types of rules are described below.
1. AGENT VP : This case applies when the VP
has no PATIENT, or a PATIENT that is not a char-
acter in the story, or the PATIENT corefers with
the AGENT. All affect tags associated with the VP
are projected onto the AGENT. For example, ?Mary
laughed (+)? projects a positive affect state onto
Mary.
2. VP PATIENT9: All affect tags associated with
the VP are projected onto the PATIENT, unless both
M and +/- tags exist, in which case only the +/- tags
are projected. For example, ?loved (+) the cat?,
projects a positive affect state onto the cat.
8We are not actually doing thematic role recognition, so this
will not always be correct, but it is a reasonable approximation.
9Agent is missing or not a character.
3. AGENT VP PATIENT: This case applies when
the AGENT and PATIENT refer to different char-
acters. All affect tags associated with the VP are
projected onto the PATIENT, unless both M and +/-
tags exist, in which case only the +/- tags are pro-
jected (as in Rule #2). If the VP has an M tag, then
we also project an M tag onto the AGENT (repre-
senting a shared, cross-character mental state). If
the VP has a +/- tag, then we project a + tag onto
the agent (as an inference that the AGENT accom-
plished some action).
4. AGENT VERB1 to VERB2 PATIENT. We di-
vide this into two cases: (a) If the agent and patient
refer to the same character, then Rule #1 is applied
(e.g., ?Bo decided to teach himself...?). (b) If the
agent and patient are different, we apply Rule #1 to
VERB1 to agent and Rule #2 to VERB2. If no af-
fect tags are assigned to either verb, then we create
an M affect state for the agent (assuming that the VP
represents some sort of plan).
5. If a noun phrase refers to a character and in-
cludes a modifying adjective with an affect tag, then
the affect is mapped onto the character. For exam-
ple, ?the happy (+) fox?.
Finally, if an adverb or adjectival phrase (e.g.,
predicate adjective) has an affect tag, then that affect
tag is mapped onto the preceding VP and the projec-
tion rules above are applied. For all of the rules, if
a clause contains a negation word, then we flip the
polarity of all words in that clause. Our negation list
contains: no, not, never, fail, failed, fails, don?t, and
didn?t.
5 Evaluation
5.1 Data Set
Plot unit analysis of ordinary text is enormously
complex ? even the idea of manually creating gold
standard annotations seemed like a monumental
task. So we began our exploration with simpler and
more constrained texts that seemed particularly ap-
propriate for plot unit analysis: fables. Fables have
two desirable attributes: (1) they have a small cast
of characters, and (2) they typically revolve around
a moral, which is exemplified by a short and concise
plot. Even so, fables are challenging for NLP due to
anthropomorphic characters, flowery language, and
sometimes archaic vocabulary.
22
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bsent baseline .65 .10 .17 .52 .08 .14 .74 .06 .11 .63 .08 .14
Bclause baseline .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
All 4 resources (w/proj. rules) .48 .43 .45 .23 .39 .29 .23 .41 .29 .34 .41 .37
OpinionFinder .36 .42 .39 .00 .00 .00 .00 .00 .00 .15 .35 .21
Subjectivity Lexicon .45 .43 .44 .23 .35 .28 .21 .44 .28 .32 .41 .36
Semantic Dictionary .42 .45 .43 .00 .00 .00 .00 .00 .00 .18 .45 .26
Semantic Orientation Lexicon .41 .43 .42 .17 .53 .26 .08 .43 .13 .25 .45 .32
PPV Lexicon .41 .42 .41 .02 .17 .04 .21 .73 .33 .23 .44 .30
AESOP (All 4 + PPV) .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Table 2: Evaluation results for 2 baselines, 4 sentiment analysis resources with projection rules, and our PPV lexicon
with projection rules. (The # in parentheses is the number of occurrences of that state in the gold standard).
We collected 34 fables from an Aesop?s Fables
web site10, choosing fables that have a true plot
(some only contain quotes) and exactly two charac-
ters. We divided them into a development set of 11
stories, a tuning set of 8 stories, and a test set of 15
stories. The Father and Sons story from Figure 1(a)
is an example from our set.
Creating a gold standard was itself a substantial
undertaking. Plot units are complex structures, and
training non-experts to produce them did not seem
feasible in the short term. So three of the authors
discussed and iteratively refined manual annotations
for the development and tuning set stories until we
became comfortable that we had a common under-
standing for the annotation task. Then to create our
gold standard test set, two authors independently
created annotations for the test set, and a third au-
thor adjudicated the differences. The gold standard
contains complete plot unit annotations, including
affect states, causal links, and cross-character links.
For the experiments in this paper, however, only the
affect state annotations were used.
5.2 Baselines
We created two baselines to measure what would
happen if we use all 4 sentiment analysis resources
without any projection rules. The first one (Bsent)
operates at the sentence level. It naively projects ev-
ery affect tag that occurs in a sentence onto every
character in the same sentence. The second base-
line (Bclause) operates identically, but at the clause
level.
10http://www.pacificnet.net/?johnr/aesop/
5.3 Evaluation
As our evaluation metrics we used recall (R), preci-
sion (P), and F-measure (F). We evaluate each sys-
tem on individual affect states (+, - and M) as well
as across all affect states. The evaluation is done at
the sentence level. Meaning, if a system produces
the same affect state as present in the gold standard
for a sentence, we count it as a correct affect state.
Our main evaluation also requires each affect state
to be associated with the correct character.
Table 2 shows the coverage of our two baseline
systems as well as the four Sentiment Analysis
Resources used with our projection rules. We can
make several observations:
? As expected, the baselines achieve relatively high
recall, but low precision.
? Each of the sentiment analysis resources alone
is useful, and using them with the projection rules
leads to improved performance over the baselines
(10 points in F score for M and 6 points overall).
This shows that the projection rules are helpful
in identifying the characters associated with each
affect state.
? The PPV Lexicon, alone, is quite good at cap-
turing negative affect states. Together with the
projection rules, this leads to good performance on
identifying mental states as well.
To better assess our projection rules, we evaluated
the systems both with respect to characters and with-
out respect to characters. In this evaluation, system-
produced states are correct even if they are assigned
to the wrong character. Table 3 reveals several re-
sults: (1) For the baseline: there is a large drop when
23
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bclause w/o char .65 .37 .47 .50 .25 .33 .77 .19 .30 .63 .26 .37
AESOP w/o char .55 .44 .49 .33 .47 .39 .36 .50 .42 .43 .46 .44
Bclause w/ char .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
AESOP w/ char .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Table 3: Evaluating affect states with and without respect to character.
State M (66) + (52) - (39) All (157)
System R P F R P F R P F R P F
Bclause PCoref .48 .28 .35 .44 .22 .29 .69 .17 .27 .52 .22 .31
AESOP PCoref .48 .40 .44 .25 .36 .30 .33 .46 .38 .37 .40 .38
Bclause ACoref .42 .45 .43 .25 .34 .29 .54 .24 .33 .39 .33 .36
AESOP ACoref .41 .54 .47 .12 .40 .18 .26 .45 .33 .27 .49 .35
Table 4: Final results of Bclause and AESOP systems with perfect and automated coreference
evaluated with respect to the correct character. (2)
For AESOP: there is a smaller drop in both preci-
sion and recall for M and -, suggesting that our pro-
jection rules are doing well for these affect states.
(3) For AESOP: there is a large drop in both preci-
sion and recall for +, suggesting that there is room
for improvement of our projection rules for positive
affect.
Finally, we wish to understand the role that coref-
erence plays. Table 4 summarizes the results with
perfect coreference and with automated coreference.
AESOP is better than both baselines when we use
perfect coreference (PCoref), which indicates that
the affect projection rules are useful. However,
when we use automated coreference (ACoref), re-
call goes down and precision goes up. Recall goes
down because our automated coreference system is
precision oriented: it only says ?coreferent? if it is
sure.
The increase in precision when moving to auto-
mated coreference is bizarre. We suspect it is pri-
marily due to the handling of quotations. Our perfect
coreference system resolves first and second person
pronouns in quotations, but the automated system
does not. Thus, with automated coreference, we al-
most never produce affect states from quotations.
This is a double-edged sword: sometimes quotes
contain important affect states, sometimes they do
not. For example, from the Father and Sons fable,
?if you are divided among yourselves, you will be
broken as easily as these sticks.? Automated coref-
erence does not produce any character resolutions
and therefore AESOP produces no affect states. In
this case this is the right thing to do. However, in
another well-known fable, a tortoise says to a hare:
?although you be as swift as the wind, I have beaten
you in the race.? Here, perfect coreference produces
multiple affect states, which are related to the plot:
the hare recieves a negative affect state for having
been beaten in the race.
6 Conclusions
AESOP demonstrates that sentiment analysis tools
can successfully recognize many affect states when
coupled with syntax-based projection rules to map
the affect states onto characters. We also showed
that negative patient polarity verbs can be harvested
from a corpus to identify characters that are in a neg-
ative state due to an action. However, performance is
still modest, revealing that much work remains to be
done. In future work, new methods will be needed
to represent affect states associated with plans/goals,
events, and inferences.
7 Acknowledgments
The authors thank the anonymous reviewers for
many helpful comments. This work was sup-
ported in part by the Department of Homeland Se-
curity Grant N0014-07-1-0152, the DARPA Ma-
chine Reading program under contract FA8750-09-
C-0172, and the NSF grant IIS-0712764.
24
References
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recogni-
tion. In EMNLP ?06: Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 431?439, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
S. Kim and E. Hovy. 2006. Extracting Opinions, Opin-
ion Holders, and Topics Expressed in Online News
Media Text. In Proceedings of ACL/COLING Work-
shop on Sentiment and Subjectivity in Text.
W. Lehnert, J. Black, and B. Reiser. 1981. Summariz-
ing Narratives. In Proceedings of the Seventh Interna-
tional Joint Conference on Artificial Intelligence.
W. G. Lehnert. 1981. Plot Units and Narrative Summa-
rization. Cognitive Science, 5(4):293?331.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
V. Stoyanov and C. Cardie. 2008. Topic Identification
for Fine-Grained Opinion Analysis. In Conference on
Computational Linguistics (COLING 2008).
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
A. Wierzbicka. 1987. English speech act verbs: a se-
mantic dictionary. Academic Press, Sydney, Orlando.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. OpinionFinder: A system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP 2005
Interactive Demonstrations.
Theresa Wilson. 2005. Recognizing contextual polarity
in phrase-level sentiment analysis. In In Proceedings
of HLT-EMNLP.
25
Proceedings of BioNLP Shared Task 2011 Workshop, pages 89?93,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
The Taming of Reconcile as a Biomedical Coreference Resolver 
  Youngjun Kim,    Ellen Riloff,    Nathan Gilbert School of Computing University of Utah Salt Lake City, UT {youngjun, riloff, ngilbert} @cs.utah.edu       Abstract 
To participate in the Protein Coreference section of the BioNLP 2011 Shared Task, we use Reconcile, a coreference resolution engine, by replacing some pre-processing components and adding a new mention detector. We got some improvement from training two separate classifiers for detecting anaphora and antecedent mentions. Our system yielded the highest score in the task, F-score 34.05% in partial mention, protein links, and system recall mode. We witnessed that specialized mention detection is crucial for coreference resolution in the biomedical domain.  
1 Introduction Coreference resolution is a mechanism that groups entity mentions in a text into coreference chains based on whether they refer to the same real-world entity or concept. Like other NLP applications, which must meet the need for aggressive and sophisticated methods of detecting valuable information in emerging domains, numerous coreference resolvers have been developed, including JavaRap (Qiu et al, 2004), GuiTaR (Poesio and Kabadjov, 2004) and BART (Versley et al, 2008). Our research uses a recently released system, Reconcile (Stoyanov et al 2009; 2010a; 2010b), which was designed as a general architecture for coreference resolution that can be used to easily create learning-based coreference resolvers. Reconcile is based on supervised learning approaches to coreference resolution and 
has showed relatively good performance compared with similar types of systems.  As a first step to adapting Reconcile for the biomedical domain, specifically the BioNLP Shared Task 2011 (Kim et al, 2011), we modified several subcomponents in Reconcile and revised the feature set for this task. Most importantly, we created a specialized mention detector trained for biomedical text. We trained separate classifiers for detecting anaphor and antecedent mentions, and experimented with several clustering techniques to discover the most suitable algorithm for producing coreference chains in this domain. 2 BioNLP 2011 Shared Task  Our system was developed to participate in a Protein Coreference (COREF) task (Nguyen et al, 2011), one of the supporting tasks in the BioNLP Shared Task 2011. The COREF task is to find all mentions participating in the coreference relation and to connect the anaphora-antecedent pairs. The corpus is based on the Genia-Medco coreference corpus. The Genia-Medco corpus was produced for the biomedical domain, and some comparative analysis with this corpus and other newswire domain data have been performed (Yang et al, 2004a; 2004b; Nguyen and Kim, 2008; Nguyen et al, 2008).  The COREF corpus consists of 800 text files for training, 150 for development, and 260 for testing, which all have gene/protein coreference annotations. The training set has 2,313 pairs of coreference links with 4,367 mentions. 2,117 mentions are antecedents, with an average of 4.21 tokens each (delimited by white space), and 2,301 
89
mentions are anaphora, with an average of 1.28 tokens each. The anaphora are much shorter because many of them are pronouns. The five most frequent anaphora are that (686 times), which (526), its (270), their (130), and it (100).  3 Our Coreference Resolver Reconcile was designed to be a research testbed capable of implementing the most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to sub-components, feature sets, parameter settings, etc. A mention detector and an anaphora-antecedent pairs generator are added for the COREF task. 3.1 Preprocessing For pre-processing, we used the Genia Tagger (Tsuruoka and Tsujii. 2005) for sentence splitting, tokenizing, and part-of-speech (POS) tagging. For parsing, we used the Enju parser (Miyao and Tsujii, 2008). We replaced Reconcile?s mention detection module with new classifiers because of poor performance on the biomedical domain with the provided classifiers. We reformatted the training data with IOB tags and trained a sequential classifier using CRF++ (Kudoh, 2007). For this sequence tagging, we borrowed the features generally used for named entity recognition in the biomedical literature (Finkel et al, 2005; Zhou et al, 2005; McDonald and Pereira, 2005), including word, POS, affix, orthographic features and combinations of these features. We extracted features from the target word, as well as two words to its left and two words to its right. Two versions of mention detectors were developed. The first (MD-I) trained one model without differentiating between anaphora and antecedents. For this method, we chose the longest mentions when multiple mentions overlapped. The other detector (MD-II) used two different models for the antecedent and anaphor, classifying them separately. MD-II?s classification result was used when generating the anaphora-antecedent pairs. Table 1 shows the performance of exact matching by these detectors compared with the performance of the Genia Noun Phrase (NP) chunker. Our classifiers did much better, 81.31% precision and 
64.78% recall (MD-II), than the Genia chunker, 6.58% precision and 72.67% recall. Only an average of six mentions occurred in each text, while the Genia chunker detected 66.27 noun phrases on average. The Genia annotation scheme was not limited to specific types of concepts, so the Genia NP chunker identifies every possible concept. In contrast, the COREF shared task only involves a subset of the concepts. Mention boundaries were also frequently mismatched. For example, ?its? was annotated as a mention in the COREF task when it appears as a possessive inside a noun phrase (e.g., ?its activity?), but the Genia NP Chunker tags the entire noun phrase as a mention.   Prec Rec F Genia NP Chunker   6.58 72.67 12.07 Mention Detector-I 80.85 63.33 71.03 Mention Detector-II 81.31 64.78 72.11     Antecedent 65.48 41.35 50.69     Anaphor 91.72 85.07 88.27 Table 1:Mention Detection Results on Dev. Set 3.2 Feature Generation We used the following four types of features: Lexical: String-based comparisons of the two mentions, such as exact string matching and head noun matching.  Proximity: Sentence measures of the distance between two mentions.   Grammatical: A wide variety of syntactic properties of the mentions, either individually or in pairs. These features are based on part-of-speech tags, or parse trees.  Semantic: Semantic information about one or both mentions, such as tests for gender and animacy.  Due to the unavailability of paragraph information in our training data, we excluded Reconcile?s paragraph features. Also, named entity and dependency parsing features were not used for training. Table 2 shows the complete feature set used for this task. In total, we excluded nine existing Reconcile features, mostly semantic features: WordNetClass, WordNetDist, WordNetSense, Subclass, ParNum, SameParagraph, IAntes, Prednom, WordOverlap. Full descriptions of these features can be found in Stoyanov (2010a).  
90
Lexical HeadMatch, PNStr, PNSubstr, ProStr, SoonStr, WordsStr, WordsSubstr Proximity ConsecutiveSentences, SentNum, SameSentence  Syntactic Binding, BothEmbedded, BothInQuotes, BothPronouns, BothProperNouns, BothSubjects, ContainsPN, Contraindices, Definite1, Definite2, Demonstrative2, Embedded1, Embedded2, Indefinite, Indefinite1, InQuote1, InQuote2, MaximalNP, Modifier, PairType, Pronoun, Pronoun1, Pronoun2, ProperNoun, ProResolve, RuleResolve, Span, Subject1, Subject2, Syntax Semantic Agreement, Alias, AlwaysCompatible, Animacy, Appositive, ClosestComp, Constraints, Gender, instClass, Number, ProComp, ProperName, Quantity, WNSynonyms  Table 2: Feature Set for Coreference Resolution 3.3 Clustering After Reconcile makes pairwise decisions linking each anaphor and antecedent, it produces a clustering of the mentions in a document to create coreference chains. Because the format of the COREF task submission was not chains but anaphora-antecedent pairs, it would have been possible to submit the direct results of Reconcile?s pairwise decisions. However, it was easier to use Reconcile as a black-box and post-process the chains to reverse-engineer coreferent pairs from them. Reconcile supports three clustering algorithms: Single-link Clustering (SL) (Transitive Closure) groups together all mentions that are connected by a path of coreferent links.  Best-first (BF) clustering uses the classifier?s confidence value to cluster each noun phrase with its most confident antecedent. Most Recent First (MRF) pairs each noun phrase with the single most recent antecedent that is labeled as coreferent.  Table 3 shows the MUC scores of each clustering method with gold standard mentions and with the mentions automatically detected by each of our two mention detectors. Not surprisingly, using gold mentions produced the highest score of 87.32%. Automatically detected mentions yielded much lower performance. MD-I performed best, in this evaluation, achieving 49.65%. The most recent 
first clustering algorithm produced the best results for both gold mentions and MD-I. The single link clustering algorithm, which is the default method used by Reconcile, produced the lowest results for both gold mentions and MD-I.   SL BF MRF Gold Mention 85.34 86.87 87.32 Mention Detector-I 48.64 48.82 49.65 Mention Detector-II 48.31 48.62 48.07 Table 3: MUC Scores of Dev. Set by Three Different Clustering Methods (SL: Single-link, BF: Best-first, MRF: Most recent first) 3.4 Pair Generation from Chains Reconcile generates coreference chains, but the output for the shared task required anaphora-antecedent pairs. Therefore, we needed to extract individual pairs from the chains. We used the chains produced by the most recent first clustering algorithm for pair generation. When using MD-I output, we took the earliest mention (i.e., the one occurring first in the source document) in the chain and paired it with each of the subsequent mentions in the same chain. Thus, each chain of size N produced N-1 pairs. When using the MD-II predictions, the classifiers gave us two separate lists of antecedent and anaphora mentions. In this case, we paired each anaphor in the chain with every antecedent in the same chain that preceded it in the source document.  3.5 Evaluation and Analysis The mention linking can be evaluated using three different scores: atom coreference links, protein coreference links, and surface coreference links. In the atom link option, only links containing given gene/protein annotations are considered while in the surface link option, every link is a target for the evaluation. Protein links are similar to atom links but loosen the boundary of gene/protein annotations. There were 202 protein links out of 469 surface links in development set.  For mention detection, exact match and partial match are supported in the task evaluation. Recall is measured in two modes. In system mode, every link is calculated for the linking evaluation. In algorithm mode, only links with correctly detected mentions are considered for evaluation. For 
91
detailed information refer to Nguyen et al (2011) or the task web site.1 Table 4 shows the mention linking results (F-score) for the COREF task evaluation using partial match and system recall. The surface link score on gold mentions reached 90.06%. For automatic mention detection, MD-I achieved a score of 45.38% score, but MD-II produced a substantially better score of 50.41%. MD-II, which was trained separately for antecedent and anaphora detection, performed about 5% higher than MD-I in every link mode.    Atom Protein Surface Gold Mention 84.09 84.09 90.06 Mention Detector-I 28.67 34.41 45.38 Mention Detector-II 33.45 39.27 50.41 Table 4: Dev. Set Results by Three Different Evaluation Options Table 5 shows the recall and precision breakdown for the protein evaluation results. Looking behind the composite F-score reveals that our system produced higher precision than recall. Looking back at Table 1, we saw that our anaphor detector performed much better than our antecedent detector. Since every coreference link requires one of each, the relatively poor performance of antecedent detection (especially in terms of recall) is a substantial bottleneck.     Prec Rec F Gold Mention 98.67 73.27 84.09 Mention Detector-I 62.34 23.76 34.41 Mention Detector-II 73.97 26.73 39.27 Table 5: Precision and Recall Breakdown for Protein Evaluation Coreference Links 3.6 Results: Submission for COREF Task We merged the training and development sets to use as training data for Reconcile. We used MD-II for mention detection and the most recent first algorithm for clustering to submit the final output on the test data. Table 6 shows the results of our final submission along with the five other participating teams for the protein evaluation coreference links (Nguyen et al, 2011). Our                                                             1 http://sites.google.com/site/bionlpst/home/protein-gene-coreference-task 
system produced a 34.05% F-score (73.26% precision and 22.18% recall) in protein coreference links and 25.41% F-score in atom links.   Team  Prec  Rec F University of Utah 73.26 22.18 34.05 University of Zurich 55.45 21.48 30.96 Concordia University 63.22 19.37 29.65 University of Turku 67.21 14.44 23.77 University of Szeged   3.47   3.17   3.31 University College Dublin   0.25   0.70   0.37 Table 6: Evaluation Results of Final Submissions (Protein Coreference Links) 4 Conclusions The effort to tame Reconcile as a coreference engine for the biomedical domain was successful and our team?s submission obtained satisfactory results. However, there is ample room for improvement in coreference resolution. We observed that mention detection is crucial - the MUC score reached 87.32% with gold mentions on the development set but only 49.65% with automatically detected mentions (Table 3). One possible avenue for future work is to develop domain-specific features to better identify mentions in biomedical domains.  Acknowledgments  We thank the BioNLP Shared Task 2011 organizers for their efforts, and gratefully acknowledge the support of the National Science Foundation under grants IIS-1018314 and DBI-0849977 and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily express the view of the DARPA, AFRL, NSF, or the U.S. government. References Jenny Finkel, Shipra Dingare, Christopher D Manning, Malvina Nissim, Beatrice Alex, and Claire Grover. 2005. Exploring the Boundaries: Gene and Protein Identifica-tion in Biomedical Text. BMC Bioinformatics. 6:S5. 
92
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, and Jun?ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. ACL 2011. Taku Kudoh. 2007. CRF++. http://crfpp.sourceforge.net/. Ryan McDonald and Fernando Pereira. 2005. Identifying Gene and Protein Mentions in Text Using Condi-tional Random Fields. BMC Bioinformatics. 6:S6. Yusuke Miyao and Jun'ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics, 34(1):35?80. Ngan L. T. Nguyen and Jin-Dong Kim. 2008. Exploring Domain Differences for the Design of Pronoun Resolution Systems for Biomedical Text. Proceedings of COLING 2008:625-632 Ngan L. T. Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2008. Challenges in Pronoun Resolution System for Biomedical Text. Proceedings of LREC 2008.  Ngan L. T. Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011. Overview of the Protein Coreference task in BioNLP Shared Task 2011. Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. ACL 2011. Massimo Poesio and Mijail A. Kabadjov. 2004. A general-purpose, off-the-shelf anaphora resolution module: implementation and preliminary evaluation. Proceedings of LREC 2004. Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A Public Reference Implementation of the Rap Anaphora Resolution Algorithm. Proceedings of LREC 2004. Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010a. Reconcile: A Coreference Resolution Platform. Tech Report. Cornell University. Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010b. Coreference Resolution with Reconcile. Proceedings of ACL 2010. Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art. Proceedings of ACL-IJCNLP 2009. Yoshimasa Tsuruoka and Jun'ichi Tsujii. 2005. Bidirectional Inference with the Easiest-First Strategy for Tagging Sequence Data. Proceedings of HLT/EMNLP 2005:467-474. Yannick Versley,  Simone P. Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng 
Yang, and Alessandro Moschitti. 2008. BART: A modular toolkit for coreference resolution. Proceedings of LREC 2008. Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004a. A NP-Cluster Based Approach to Coreference Resolution. Proceedings of COLING 2004:226-232. XiaoFeng Yang, GuoDong Zhou, Jian Su, and Chew Lim Tan. 2004b. Improving Noun Phrase Coreference Resolution by Matching Strings. Proceedings of IJCNLP 2004:226-333. GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su, and SoonHeng Tan. 2005. Recognition of Protein/Gene Names from Text Using an Ensemble of Classifiers. BMC Bioinformatics. 6:S7. 
93
