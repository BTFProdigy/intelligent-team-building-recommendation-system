Word Sense Disambiguation for Cross-Language Information 
Retrieval 
Mary Xiaoyong Liu, Ted Diamond, and Anne R. Diekema 
School of  Information Studies 
Syracuse University 
Syracuse, NY 13244 
xliu03@mailbox.syr.edu 
t..diar0onl @twcny.rr.com 
diekemar@griailbox, syr. edu 
Abstract 
We have developed a word sense 
disambiguation algorithm, following Cheng and 
Wilensky (1997), to disambiguate among 
WordNet synsets. This algorithm is to be used in 
a cross-language information retrieval system, 
CINDOR, which indexes queries and documents 
in a language-neutral concept representation 
based on WordNet synsets. Our goal is to 
improve retrieval precision through word sense 
disambiguation. An evaluation against human 
disambiguation judgements suggests promise for 
our approach. 
1 Introduction 
The CINDOR cross-language information 
retrieval system (Diekema et al, 1998) uses an 
information structure known as "conceptual 
interlingua" for query and document 
representation. This conceptual interlingua is a 
hierarchically organized multilingual concept 
lexicon, which is structured following WordNet 
(Miller, 1990). By representing query and 
document terms by their WordNet synset 
numbers we arrive at essentially a language 
neutral representation consisting of synset 
numbers representing concepts. This 
representation facilitates cross-language retrieval 
by matching tea-m synonyms in English as well as 
across languages. However, many terms are 
polysemous and belong to multiple synsets, 
resulting in spurious matches in retrieval. The 
nounfigure for example appears in 13 synsets in 
WordNet 1.6. This research paper describes the 
early stages I of our efforts to develop a word 
sense disambiguation (WSD) algorithm aimed at 
improving the precision of our cross-language 
retrieval system. 
2 Related Work  
To determine the sense of a word, a WSD 
algorithm typically uses the context of the 
ambiguous word, external resources such as 
machine-readable dictionaries, or a combination 
of both. Although dictionaries provide useful 
word sense information and thesauri provide 
additional information about relatiomhips 
between words, they lack pragmatic information 
as can be found in corpora. Corpora contain 
examples of words that enable the development of 
statistical models of word senses and their 
contexts (Ide and Veronis, 1998; Leacock and 
Chodorow, 1998). 
There are two general problems with using 
corpora however; 1) corpora typically do not 
come pre-tagged with manually disambiguated 
senses, and 2) corpora are often not large nor 
diverse nough for all senses of a word to appear 
often enough for reliable statistical models (data 
sparseness). Although researchers have tried 
sense-tagging corpora automatically by using 
either supervised or unsupervised training 
methods, we have adopted a WSD algorithm 
which avoids the necessity for a sense-tagged 
training corpus. 
l Please note that he disambiguation research 
described inthis paper has not yet been extended to
multiple language areas. 
35 
P(synsetlcontext(w)) = 
P(context(w) I synset) P(synset) 
P(context(w)) 
(I) 
The problem of data sparseness i usually 
solved by using either smoothing methods, class- 
based methods, or by relying on similarity-based 
methods between words and co-occurrence data. 
Since we are using a WordNet-based resource for 
retrieval, using class-based methods seems a 
natural choice. Appropriate word classes can be 
formed by synsets or groups of synsets. The 
evidence of a certain sense (synset) is then no 
longer dependent on one word but on all the 
members of a particular synset. 
Yarowsky (1992) used Rogets Thesaurus 
categories as classes for WSD. His approach was 
based on selecting the most likely Roget category 
for nouns given their context of 50 words on 
either side. When any of the category indicator 
words appeared in the context of an ambiguous 
word, the indicator weights for each category 
were summed to determine the most likely 
category. The category with the largest sum was 
then selected. 
A similar approach to that of Yarowsky was 
followed by Cheng and Willensky (1997) who 
used a training matrix of associations of words 
with a certain category. Their algorithm was 
appealing to us because it requires no human 
intervention, and more importantly, it avoids the 
use of sense-tagged ata. Our methodology 
described in the next section is therefore based on 
Cheng and Wilensky's approach. 
Methods to reduce (translation) ambiguity in 
cross-language information retrieval have 
included using part-of-speech taggers to restrict 
the translation options (Davis 1997), applying 
pseudo-relevance feedback loops to expand the 
query with better terms aiding translation 
(Ballesteros and Croft 1997), using corpora for 
term translation disambiguation (Ballesteros and 
Croft, 1998), and weighted Boolean models 
which tend to have a self-disambiguating quality 
(Hull, 1997; Diekema et al, 1999; Hiemstra nd 
Kraaij, 1999). 
3 Methodology 
To disambiguate a given word, we would like 
to know the probability that a sense occurs in a 
given context, i.e., P(semse\[context). In this study, 
WordNet synsets are used to represent word 
senses, so P(senselcontext) can be rewritten as 
P(synsetlcontext), for each synset of which that 
word is a member. For nouns, we define the 
context of word w to be the occurrence of words 
in a moving window of I00 words (50 words on 
each side) around w 2. 
By Bayes Theorem, we can obtain the desired 
probability by inversion (see equation (I)). Since 
we are not specifically concerned with getting 
accurate probabilities but rather relative rank 
order for sense selection, we ignore P(context(w)) 
and focus on estimating 
P(context(w)lsymet)P(synset). The event space 
l~om which "context(w)" is drawn is the set of 
sets of words that ever appear with each other in 
the window around w. In other words, w induces 
a partition on the set of words. We define 
"context(w)" to be true whenever any of the 
words in the set appears in the window around w, 
and conversely to be false whenever none of the 
words in the set appears around w. If we assume 
independence of appearance of any two words in 
a given context, then we get: 
P(synset)?(1- l-I(1-P(wilsynset))) (2) 
wiecontext 
Due to the lack of sense-tagged corpora, we 
are not able to directly estimate P(synset) and 
P(wilsymet). Instead, we introduce "noisy 
estimators" (Pdsymet) and Pdwl\]symet)) to 
approximate hese probabilities. In doing so, we 
make two assumptions: l) The presence of any 
word Wk that belongs to synset si signals the 
presence of si; 2) Any word Wk belongs to all its 
synsets simultaneously, and with equal 
probability. Although the assumptions underlying 
the "noisy estimators" are not strictly true, it is 
our belief that the "noisy estimators" hould work 
reasonably well if: 
? The words that belong to symet sitend to 
appear in similar contexts when si is their 
intended sense; 
? These words do not completely overlap 
with the words belonging to some synset 
sj ( i ~ j ) that partially overlaps with si; 
2 For other parts of speech, the window size should 
be much smaller as suggested by previous research. 
36 
The common words between si and sj 
appear in different contexts when si and 
sj are their intended senses. 
4 The WSD Algorithm 
We chose as a basis the algorithms described 
by Yan'owsky (1992) and by Cheng and 
Wilensky (1997). In our variation, we use the 
synset numbers in WordNet to represent he 
senses of a word. Our algorithm learns 
associations of WordNet synsets with words in a 
surrounding context o determine a word sense. It 
consists of two phases. 
During the training phase, the algorithm 
reads in all training documents in collection and 
computes the distance-adjusted weight of co- 
occurrence of each word with each corresponding 
synset. This is done by establishing a 100-word 
window around a target word (50 words on each 
side), and correlating each synset to which the 
target word belongs with each word in the 
surrounding window. The result of the training 
phase is a matrix of associations of words with 
synsets. 
In the sense prediction phase, the algorithm 
takes as input randomly selected testing 
documents or sentences that contain the 
polysemous words we want to disambiguate and 
exploits the context vectors built in the training 
phase by adding up the weighted "votes". It then 
returns a ranked list of probability values 
associated with each synset, and chooses the 
synset with the highest probability as the sense of 
the ambiguous word. 
Figure 1 and Figure 2 show an outline of the 
algorithm. 
In this algorithm, "noisy estimators" are 
employed in the sense prediction phase. They are 
calculated using following formulas: 
M\[w, Ix\] 
Po(wilx)-- LwM\[wIx\] (3) 
where wi is a stem, x is a given synset, 
M\[w\]\[x\] is a cell in the correlation matrix that 
corresponds to word w and synset x, and 
Z.,,,,M\[wlx\] 
P~(x)= Z,,~w.y~rM\[wly \] (4) 
where w is any stem in the collection, x 
is a given symet, y is any synset ever occurred in 
collection. 
For each document d in collection 
read in a noun stem w from d 
for each synset s in which w occurs 
get the column b in the association matrix M that corresponds tos if the column already 
exists; create anew column for s otherwise 
for each word stem j appearing in the 100-word window around w 
get the row a in M that corresponds toj if the row already exists; create anew 
row for j otherwise 
add a distance-adjusted weight o M\[a\]\[b\] 
Figure 1: WSD Algorithm: the training phase 
Set value = 1 
For each word w to be disambiguated 
get synsets of w 
for each synset x ofw 
for each wi in the context ofw (within the 100-window around w) 
calculate Pc(wilx) 
value *= ( 1 - Pc(wilx)) 
P(context(w)lx) = 1 - value 
Calculate pc(x) 
P(xlcontext(w)) =p~(x)* P(eontext(w)lx) 
display a ranked list of the synsets arranged according to their P(xlcontext(w)) in decreasing 
order 
Figure 2: WSD Algorithm: the sense prediction phase 
37 
5 Evaluation 
As suggested by the WSD literature, 
evaluation of word sense disambiguation systems 
is not yet standardized (Resnik and Yarowsky, 
1997). Some WSD evaluations have been done 
using the Brown Corpus as training and testing 
resources and comparing the results against 
SemCor 3, the sense-tagged version of the Brown 
Corpus (Agirre and Rigau, 1996; Gonzalo et al, 
1998). Others have used common test suites such 
as the 2094-word line data of Leacock et al 
(1993). Still others have tended to use their own 
metrics. We chose an evaluation with a user- 
based component that allowed a ranked list of 
sense selection for each target word and enabled 
a comprehensive comparison between automatic 
and manual WSD results. In addition we wanted 
to base the disambiguation matrix on a corpus 
that we use for retrieval. This approach allows 
for a much richer evaluation than a simple hit-or- 
miss test. For vahdation purpose, we will conduct 
a fully automatic evaluation against SemCor in 
our future efforts. 
We use in vitro evaluation in this study, i.e. 
the WSD algorithm is tested independent of the 
retrieval system. The population consists of all 
the nouns in WordNet, after removal of 
monoseanous nouns, and after removal of a 
problematic lass of polysemous nouns. 4 We 
drew a random sample of 87 polysemous nouns 5
from this population. 
In preparation, for each noun in our sample 
we identified all the documents containing that 
noun from the Associated Press (AP) newspaper 
corpus. The testing document set was then 
formed by randomly selecting 10 documents from 
the set of identified ocuments for each of the 87 
nouns. In total, there are 867 documents in the 
3 SemCor is a semantically sense-tagged corpus 
comprising approximately 250, 000 words. The 
reported error rate is around 10% for polysemous 
words. 
4 This class of nouns refers to nouns that are in 
synsets in which they are the sole word, or in synsets 
whose words were subsets of other synsets for that 
noun. This situation makes disambiguation 
extremely problematic. This class of noun will be 
dealt with in a future version of our algorithm but for 
now it is beyond the scope of this evaluation. 
5 A polysemous noun is defined as a noun that 
belongs to two or more synsets. 
testing set. The training document set consists of 
all the documents in the AP corpus excluding the 
above-mentioned 867 documents. For each noun 
in our sample, we selected all its corresponding 
WordNet noun synsets and randomly selected 10 
sentence occurrences with each from one of the 
10 random documents. 
After collecting 87 polysemous nouns with 
10 noun sentences each, we had 870 sentences for 
disambiguation. Four human judges were 
randomly assigned to two groups with two judges 
each, and each judge was asked to disambiguate 
275 word occurrences out of which 160 were 
unique and 115 were shared with the other judge 
in the same group. For each word occurrence, the 
judge put the target word's possible senses in 
rank order according to their appropriateness 
given the context (ties are allowed). 
Our WSD algorithm was also fed with the 
identical set of 870 word occurrences in the sense 
prediction phase and produced a ranked hst of 
senses for each word occurrence. 
Since our study has a matched-group design 
in which the subjects (word occurrences) receive 
both the treatments and control, the measurement 
of variables is on an ordinal scale, and there is no 
apparently applicable parametric statistical 
procedure available, two nonparametric 
procedures -the Friedman two-way analysis of 
variance and the Spearman rank correlation 
coefficient -were originally chosen as candidates 
for the statistical analysis of our results. 
However, the number of ties in our results 
renders the Spearman coefficient unreliable. We 
have therefore concentrated on the Friedman 
analysis of our experimental results. We use the 
two-alternative test with o~=0.05. 
The first tests of interest were aimed at 
estabhshing inter-judge reliability across the 115 
shared sentenees by each pair of judges. The null 
hypothesis can be generalized as "There is no 
difference in judgments on the same word 
occurrences between two judges in the same 
group". Following general steps of conducting a
Friedman test as described by Siegel (1956), we 
cast raw ranks in a two-way table having 2 
conditions/columns (K = 2) with each of the 
human judges in the pair serving as one condition 
and 365 subjects/rows (N = 365) which are all 
the senses of the 115 word occurrences that were 
judged by both human judges. We then ranked 
38 
N K Xr 2 df Rejection region Reject H0? 
First pair of judges 365 2 .003 1 3.84 No 
Second pair of judges 380 2 2.5289 1 3.84 No 
Figure 3." Statistics for significance tests of inter-judge reliability (ct=.05, 2-alt. Test) 
Auto WSD vs man. WSD 
VS sense poo,Ung 
Auto WSD vs man. WSD 
Auto WSD vs sense pooling 
Man. WSD vs sense pooling 
N 
2840 
2840 
2840 
2840 
K x? 
3 73.217 
2 3.7356 
2 5.9507 
2 126.338 
df 
2 
Rejection region Reject H9? 
5.99 Yes 
3.84 No 
3.84 Yes 
3.84 Yes 
Figure 4: Statistics for significance tests among automatic WSD, manual WSD, 
and sense pooling (ct=.05, 2-alt. TesO 
the scores in each row from 1 to K (in this case K 
is 2), summed the derived ranks in each column, 
and calculated X\[ which is .003. For ct=0.05, 
degrees of freedom df = 1 (df = K -1), the 
rejection region starts at 3.84. Since .003 is 
smaller than 3.84, the null hypothesis is not 
rejected. Similar steps were used for analyzing 
reliability between the second pair of judges. In 
both cases, we did not find significant difference 
between judges (see Figure 3). 
Our second area of interest was the 
comparison of automatic WSD, manual WSD, 
and "sense pooling". Sense pooling equates to no 
disambiguation, where each sense of a word is 
considered equally likely (a tie). The null 
hypothesis (H0) is "There is no difference among 
manual WSD, automatic WSD, and sense 
pooling (all the conditions come from the same 
population)". The steps for Friedman analysis 
were similar to what we did for the inter-judge 
reliability test while the conditions and subjects 
were changed in each test according to what we 
would like to compare. Test results are 
summarized in Figure 4. In the three-way 
comparison shown in the first row of the table, 
we rejected H0 so there was at least one condition 
that was from a different population. By further 
conducting tests which examined each two of the 
above three conditions at a time we found that it 
was sense pooling that came from a different 
population while manual and automatic WSD 
were not significantly different. We can therefore 
conclude that our WSD algorithm is better than 
no disambiguation. 
6 Concluding Remarks 
The ambiguity of words may negatively 
impact the retrieval performance of a concept- 
based information retrieval system like CINDOR. 
We have developed a WSD algorithm that uses 
all the words in a WordNet symet as evidence of 
a given sense and builds an association matrix to 
learn the co-occurrence between words and 
senses. An evaluation of our algorithm against 
human judgements of a small sample of nouns 
demonstrated no significant difference between 
our automatic ranking of senses and the human 
judgements. There was, however, a significant 
difference between human judgement and 
rankings produced with no disambiguation where 
all senses were tied. 
These early results are such as to encourage 
us to continue our research in this area. In our 
future work we must tackle issues associated 
with the fine granularity of some WordNet sense 
distinctions, synsets which are proper subsets of 
other synsets and are therefore impossible to 
distinguish, and also extend our evaluation to 
multiple languages and to other parts of speech. 
The next step in our work will be to evaluate our 
WSD algorithm against the manually sense- 
tagged SemCor Corpus for validation, and then 
integrate our WSD algorithm into CINDOR's 
processing and evaluate directly the impact on 
retrieval performance. We hope to verify that 
word sense disambiguation leads to improved 
precision in cross-language retrieval. 
Acknowledgements 
This work was completed under a research 
practicum at MNIS-TextWise Labs, Syracuse, 
NY. We thank Paraie Sheridan for many useful 
discussions and the anonymous reviewers for 
constructive comments on the manuscript. 
39
References 
Agirre, E., and Rigau, G. (1996). Word sense 
disambiguation using conceptual density. In: 
Proceedings of the 16th International 
Conference on Computational Linguistics, 
Copenhagen, 1996. 
Ballesteros, L., and Croft, B. (1997). Phrasal 
Translation and Query Expansion Techniques 
for Cross-Language Information Retrieval. In: 
Proceedings of the Association for Computing 
Machinery Special lnterest Group on 
Information Retrieval (ACM/SIGIR) 20th 
International Conference on Research and 
Development in Information Retrieval; 1997 
July 25-31; Philadelphia, PA. New York, NY: 
ACM, 1997.84-91. 
Ballesteros, L., and CroR, B. (1998). Resolving 
Ambiguity for Cross-language Retrieval. In: 
Proceedings of the Association for Computing 
Machinery Special lnterest Group on 
Information Retrieval (ACM/SIGIR) 21st 
International Conference on Research and 
Development in Information Retrieval; 1998 
August 24-28; Melbourne, Australia. New York, 
NY: ACM, 1998. 64-71. 
Cheng, I., and Wilensky, R. (1997). An Experiment 
in Enhancing Information Access by Natural 
Language Processing. UC Berkeley Computer 
Science Technical Report UCB/CSD 
UCB//CSD-97-963. 
Davis, M. (1997). New Experiments in Cross- 
language Text Retrieval at NMSU's Computing 
Research Lab. In: D.K. Harman, Ed. The Fifth 
Text Retrieval Conference (TREC-5). 1996, 
November. National Institute of Standards and 
Technology (NIST), Gaithersburg, MD. 
Diekema, A., Oroumchian, F., Sheridan, P., and 
Liddy, E. D. (1999). TREC-7 Evaluation of 
Conceptual Interlingua Document Retrieval 
(CINDOR) in English and French. In: E.M. 
Voorhees and D.K. Harman (Eds.) The Seventh 
Text REtrieval Conference (TREC-7). 1998, 
November 9-11; National Institute of Standards 
and Technology 0NIST), Gaithersburg, MD. 
169-180. 
Gonzalo, J., Verdejo, F., Chugur, I., and Cigarran, J. 
(1998). Indexing with WordNet synsets can 
improve text retrieval. In: Proceedings of the 
COLING/ACL Workshop on Usage of WordNet 
in Natural Language Processing Systems, 
Montreal, 1998. 
Hiemstra, D., and Kraaij, W. (1999). Twenty-One at 
TREC-7: Ad-hoc and Cross-language Track. In: 
E.M. Voorhees and D.K. Harman (Eds.) The 
Seventh Text REtrieval Conference (TREC-7). 
1998, November 9-11; National Institute of 
Standards and Technology (NIST), 
Gaithersburg, MD. 227-238. 
Hull, D. A. (1997). Using Structured Queries for 
Disambiguation i  Cross-Language Information 
Retrieval. In: American Association for 
Artificial Intelligence (AAA1) Symposium on 
Cross-Language Text and Speech Retrieval; 
1997 March 24-26; Palo Alto, CA 1997.84-98. 
Ide, N., and Veronis, J. (1998). Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguistics, 
Vol. 24, No. 1, 1-40. 
Leacock, C., and Chodorow, M. (1998). Combining 
Local Context and WordNet Similarity for Word 
Sense Identification. In: Christiane Fellbaum 
rEds.) WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 
l_e, acock, C., ToweU, G., and Voorhees, E. (1993). 
Corpus-based Statistical Sense Resolution. In: 
Proceedings, ARPA Human Language 
Technology Workshop, Plainsboro, NJ. 260-265. 
Miller, G. (1990). WordNet: An On-line Lexical 
Database. International Journal of 
Lexicography, Vol. 3, No. 4, Special Issue. 
Resnik, P., and Yarowsky, D. (1997). A Perspective 
on Word Sense Disambiguation Methods and 
Their Evaluation, position paper presented atthe 
ACL SIGLEX Workshop on Tagging Text with 
Lexical Semantics: Why, What, and Howl held 
April 4-5, 1997 in Washington, D.C., USA in 
conjunction with ANLP-97. 
Siegel, S. (1956). Nonparametric Statistics for the 
Behavioral Sciences. New York: McGraw-Hill, 
1956. 
Yarowsky, D. (1992). Word-Sense Disambiguation 
Using Statistical Models of Roger's Categories 
Trained on Large Corpora. In: Proceedings of 
the Fourteenth International Conference on 
Computational Linguistics. Nantes, France. 454- 
460. 
40
Evaluation of Restricted Domain Question-Answering Systems 
Anne R. Diekema, Ozgur Yilmazel, and Elizabeth D. Liddy 
Center for Natural Language Processing 
School of Information Studies 
Syracuse University 
4-206 Center for Science and Technology 
Syracuse, NY 13244 
{diekemar,liddy,oyilmaz}@syr.edu 
 
 
Abstract 
Question-Answering (QA) evaluation efforts 
have largely been tailored to open-domain 
systems. The TREC QA test collections contain 
newswire articles and the accompanying queries 
cover a wide variety of topics. While some 
apprehension about the limitations of restricted-
domain systems is no doubt justified, the strict 
promotion of unlimited domain QA evaluations 
may have some unintended consequences. 
Simply applying the open domain QA evaluation 
paradigm to a restricted-domain system poses 
problems in the areas of test question 
development, answer key creation, and test 
collection construction. This paper examines the 
evaluation requirements of restricted domain 
systems. It incorporates evaluation criteria 
identified by users of an operational QA system 
in the aerospace engineering domain. While the 
paper demonstrates that user-centered task-based 
evaluations are required for restricted domain 
systems, these evaluations are found to be 
equally applicable to open domain systems. 
1 Introduction 
The Text REtrieval Conference (TREC) 
organized the first QA evaluation (QA track) in 
1999 (Voorhees, 2000) and annual evaluations of 
this nature are ongoing (Voorhees, to appear). 
While the tasks and answer requirements have 
varied slightly from year to year, the purpose 
behind QA evaluations remains the same: to 
move from the traditional document retrieval to 
actual information retrieval by providing an 
answer to a question rather than a ranked list of 
relevant documents. The track was originally 
intended to bring together the fields of 
Information Extraction (IE) and Information 
Retrieval (IR). This legacy still continues in the 
factoid questions that require an IE type answer 
snippet in response, e.g.: ?What country is the 
Aswan High Dam located in?? This style of QA 
evaluation is spreading with very similar 
evaluations in Asia (Fukumoto, Kato, Masui, 
2003) and Europe (Magnini et al, 2003). 
Although these evaluations have a multilingual 
slant, they are strongly modeled after the TREC 
QA track. 
Typical QA systems that participate in these 
evaluations classify the questions into types 
which determine what kind of answer is required. 
After an initial retrieval of documents pertaining 
to the question, some form of text processing is 
then applied to identify possible answer 
sentences in the documents. Sentences that are 
near or contain keywords from the original 
question and contain the desired answer pattern 
are selected for answer extraction. Since it is 
difficult for systems to determine which part of 
the sentence is the correct answer, especially if it 
contains multiple extractions of the desired type, 
many systems have resorted to redundancy 
tactics (Banko et al, 2002; Buchholz, 2002). 
These systems use the Web as an answer 
verification tool by choosing the answer that 
appears most often together with the question 
keywords. While this technique is very 
successful in open domain evaluations, 
restricted-domain systems do not have the luxury 
of using redundancy, making these evaluations 
inappropriate for systems such as these. 
Our QA system participated in the three 
earlier TREC evaluations, e.g. (Diekema et al, 
2002). However, after starting work in the 
restricted-domain of re-usable launch vehicles, 
we found that the TREC evaluation no longer 
suited our system development needs and 
maintaining two different QA systems was too 
costly. 
 
 
2 Restricted-domain system 
characteristics  
The restricted-domain systems of today are 
different from the toy systems from the early 
years of QA (Voorhees and Tice, 2000), which 
might be what first comes to mind when reading 
the term ?restricted-domain?. Early systems like 
LUNAR (with a domain somewhat tangentially 
related to ours, namely lunar archeology) were 
developed by researchers in the field of natural 
language understanding. These early systems 
encoded large amounts of domain knowledge in 
databases. The restricted-domain systems of 
today are far less dependent on large knowledge 
bases and do not aim for language understanding 
per se. Rather, they use specialized extraction 
rules on a domain specific collection. The one 
thing that both types of restricted-domain 
systems have in common is that they are often 
developed with a certain goal or task in mind.  
As we will see later, this task orientation 
becomes equally important in the evaluation of 
these QA systems.  
An example of a modern-day restricted-
domain system is our Knowledge Acquisition 
and Access System (KAAS) QA system. The 
KAAS was developed for use in a collaborative 
learning environment (Advanced Interactive 
Discovery Environment for Engineering 
Education or AIDE) for undergraduate students 
from two universities majoring in aeronautical 
engineering. While students are working within 
the AIDE they can ask questions and quickly get 
answers. The collection against which the 
questions are searched consists of textbooks, 
technical papers, and websites that have been 
pre-selected for relevance and pedagogical value. 
The KAAS system uses a two-stage retrieval 
model to find answers in relevant passages. 
Relevant passages are processed by the Center 
for Natural Language Processing?s eQuery 
information extraction system using additional 
rules in the domain of reusable launch vehicles. 
Users are aided in their question formulations 
through domain specific query expansions. 
 
3 Initiating a restricted domain 
evaluation 
When it came time to evaluate the KAAS 
system, we initially defaulted to the TREC style 
QA evaluation with short, fact-based questions, 
adjudicated answers to these questions, and a test 
collection in which to find those answers. This 
choice of evaluation was not surprising since 
early versions of our system grew out of that 
environment. However, it quickly became 
apparent that this evaluation style posed 
problems for our restricted-domain, specific 
purpose system. 
Developing a set of test questions was easier 
said than done. Unlike the open domain 
evaluations, where test questions can be mined 
from question logs (Encarta, Excite, AskJeeves), 
no question sets are at the disposal of restricted-
domain evaluators. To build a set of test 
questions, we hired two sophomore aerospace 
engineering students. Based on class project 
papers of the previous semester and examples of 
TREC questions, the students were asked to 
create as many short factoid questions as they 
could, i.e ?What is APAS?? However, the real 
user questions that we collected later did not look 
anything like the short test questions in this 
initial evaluation set. The user questions were 
much more complex, e.g. ?How difficult is it to 
mold and shape graphite-epoxies compared with 
alloys or ceramics that may be used for thermal 
protective applications?? A more in depth 
analysis of KAAS question types can be found in 
Diekema et al (to appear). 
Establishing answers for the initial test 
questions proved difficult as well. The students 
did fine at collecting the questions that they had 
while reading the papers, but lacked sufficient 
domain expertise to establish answer correctness. 
Another issue was determining recall because it 
wasn?t always clear whether the (small) corpus 
simply did not contain the answer or whether the 
system was not able to find it. A third student, a 
doctoral student in aerospace engineering, was 
hired to help with these issues. To facilitate 
automatic evaluation we wanted to represent the 
answers in simple patterns but found that 
complex answers are not necessarily suitable for 
such a representation, even though patterns have 
proven feasible for TREC systems.  
While a newswire document collection for 
general domain evaluation is easy to find, a 
collection in our specialized domain needed to be 
created from scratch. Not only did the collection 
of documents take time, the conversion of most 
of these documents to text proved to be quite an 
unexpected hurdle as well. 
As is evident, the TREC style QA evaluation 
did not suit our restricted domain system. It also 
leaves out the user entirely. While information-
based evaluations are necessary to establish the 
ability of the system to answer questions 
correctly, we felt that they were not sufficient for 
evaluating a system with real users. 
4 User-based evaluation dimensions 
Restricted domain systems tend to be situated 
not only within a specific domain, but also within 
a certain user community and within a specific 
task domain. A generic evaluation is neither 
sufficient nor suitable for a restricted domain 
system. The environment in which KAAS is 
situated should drive the evaluation. Unlike 
many of the systems that participate in a TREC 
QA evaluation, the KAAS system has to function 
in real time with real users, not in batch mode 
with surrogate relevance assessors. This brings 
with it additional evaluation criteria such as 
utility and system speed (Nyberg and Mitamura, 
2003). 
KAAS users were asked in two separate 
surveys about their use and experiences with the 
system. The surveys were part of larger scale, 
cross-university course evaluations which looked 
at the students? perceptions of distance learning, 
collaboration at a distance, the collaborative 
software package, the KAAS, and each 
participating faculty member. While there was 
some structure and guidance in the user survey of 
the QA system, it was minimal and the survey is 
mainly characterized by the open nature of the 
responses. There were 25 to 30 students 
participating in each full course survey, but since 
we do not have the actual surveys that were 
turned in, we are not certain as to exactly how 
many students completed the survey section on 
the KAAS. However, it appears that most, if not 
all of the students provided feedback. 
Given the free text nature of the responses, it 
was decided that the three researchers would do a 
content analysis of the responses and 
independently derive a set of evaluation 
dimensions that they detected in the students? 
responses. Through content analysis of the user 
responses and follow-up discussion, we 
identified 5 main areas of importance to KAAS 
users when using the system: system 
performance, answers, database content, display, 
and expectations (see Figure 1). Each of the 
categories will be described in more detail 
below. 
4.1 System Performance 
System Performance is the category that deals 
with system speed and system availability. Users 
indicated that the speed with which answers were 
returned to them mattered. While they did not 
necessarily expect an immediate answer, they 
also did not want to wait, e.g. ?took so long, so I 
gave up?. Whenever users have a question, they 
want to find an answer immediately. If the 
system is down or not available to them at that 
moment, they will not come back later and try 
again. 
Possible system performance metrics are the 
?answer return rate?, and ?up time?. The answer 
return rate measures how long it takes (on 
average) to return an answer after the user has 
submitted a question.  ?Up-time? measures for a 
certain time period how often the system is 
available (system available time divided by the 
length of up-time time period). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 1: User-based evaluation dimensions. 
 
 
1 System Performance 
1.1 Speed 
1.2 Availability / reliability / upness 
2 Answers  
2.1 Completeness 
2.2 Accuracy 
2.3 Relevance 
2.4 Applicability to task / utility / usefulness 
3 Database Content 
3.1 Authority / provenance / Source quality 
3.2 Scope /extensiveness / coverage 
3.3 Size 
3.4 Updatedness 
4 Display (UI) 
4.1 Input 
4.1.1 Question understanding / info need 
understanding 
4.1.2 Querying style 
4.1.2.1 Question 
4.1.2.1.1 NL query 
4.1.2.2 Keywords 
4.1.2.3 Browsing 
4.1.3 Question formulation assistance 
4.1.3.1 Spell Checker 
4.1.3.2 Abbreviation recognition 
4.2 Output 
4.2.1 Organization 
4.2.2 Feedback Solicitation 
5 Expectations 
5.1 Googleness 
 
4.2 Answers  
What users find important in an answer is 
captured in the Answers category. The users not 
only wanted answers to be accurate, they also 
wanted them to be complete and, something that 
is not tested at all in a regular evaluation, 
applicable to their task. e.g. ?in general what I 
received was helpful and accurate?, ?it [the 
system] was useful for the Columbia incident 
exercise??. 
Possible metrics concerning answers are 
?accuracy or correctness?, ?completeness?, 
?relevance?, or ?task suitability?. While the first 
three metrics are used in some shape or form in 
the TREC evaluations, ?task suitability? is not.  
Perhaps this measure requires a certain task 
description with a question to test whether the 
answer provided by the system allowed the user 
to complete the task. 
4.3 Database Content  
Users also shared thoughts about the Database 
Content or source documents that are searched 
for answers. They find it important that these 
documents are reputable. They also shared 
concerns about the size of the database, fearing 
that a limit in size would restrict the number of 
answerable questions, e.g. ?it needs more 
documents?. The same is true for the scope of the 
collection. Users desired extended coverage to 
ensure that a wide range of questions could be 
fielded by the collection, e.g. ?I found the data 
too limited in scope?. 
Possible database content metrics are 
?authority?, ?coverage?, ?size?, and ?up-to-
dateness?. To measure ?authority? one would 
first have to identify the core authors for a 
domain through citation analysis. Once that is 
established, one could measure the percentage of 
database content created by these core 
researchers. ?Coverage? could be measured in a 
similar way after the main research areas within 
a domain are identified.  ?Size? could simply be 
measured in megabytes or gigabytes. ?Up-to-
dateness? could be measured by calculating the 
number of articles per year or simply noting the 
date of the most recent article. 
 
4.4 User Interface 
The User Interface of a system was also found 
of importance. Users were critical about the way 
they were asked to input their questions. They 
did not always want to phrase their question as a 
question but sometimes preferred to use 
keywords, e.g. ?a keyword search would be more 
useful?. They also expected the system to prompt 
them with assistance in case they misspelled 
terms, or when the system did not understand the 
question, e.g ?sometimes very good at correcting 
you to what you need, other times not very 
good?. Users also care about the way in which 
the results are presented to them and whether the 
system desires any additional responses from 
them. They did not like being prompted for 
feedback on a document?s relevance for example, 
e.g. ??the ?was this useful? window was 
disruptive?. 
Measuring UI related aspects can be done 
through observation, questionnaires and 
interviews and does not typically result in actual 
metrics but rather a set of recommendations that 
can be implemented in the next version of the 
system. 
4.5 Expectations 
Another interesting aspect of user criteria is 
Expectations , e.g. ?the documents in the e-Query 
database were useful, but Google is much 
faster?. All users are familiar with Google and 
tend to have little patience with systems that 
have a different look and feel. 
Expectations can be captured by survey so 
that it can be established whether these 
expectations are reasonable and whether they can 
be met.   
5 Restricted domain QA Evaluation  
If we consider a restricted domain QA system 
as a system developed for a certain application, it 
is clear that these systems require a situated 
evaluation. The evaluation has to be situated in 
the task, domain, and user community for which 
the system is developed.  
How then can a restricted domain system best 
be evaluated? We believe that the evaluation 
should be driven by the dimensions identified by 
the users as important: system performance, 
answers, database content, display, and 
expectations. 
The system should be evaluated on its 
performance. How many seconds does it take to 
answer a question? Once the speed is known, one 
can determine how long users are willing to wait 
for an answer. It may very well be that the 
answer-finding capability of a system will need 
to be simplified in order to speed up the system 
and satisfy its users. Similarly, tests to determine 
robustness need to be part of the system 
performance evaluation. Users tend to shy away 
from systems that are periodically unavailable or 
slow to a crawl during peak usage hours.  
Systems should also be evaluated on their 
answer providing ability. This evaluation should 
include measures for answer completeness, 
accuracy, and relevancy. Test questions should 
be within the domain of the QA system in order 
to test the answer quality for that domain. 
Answers to certain questions require a more fine-
grained scoring procedure: answers that are 
explanations or summaries or biographies or 
comparative evaluations cannot be meaningfully 
rated as simply right or wrong. The answer 
providing capability should be evaluated in light 
of the task or purpose of the system. For 
example, users of the KAAS are learners in the 
field and are not well served with exact answer 
snippets. For their task, they need answer context 
information to be able to learn from the answer 
text.  
The evaluation should also include measures 
of the Database Content. Rather than assuming 
relevancy of a collection, it should be evaluated 
whether the content is regularly updated, whether 
the contents are of acceptable quality to the 
users, and whether the coverage of the restricted 
domain is extensive enough. 
Another system component that should be 
evaluated is the User Interface. Is the system 
easy to use? Does the interface provide clear 
guidance and/or assistance to the user? Does it 
allow users to search in multiple ways? 
Finally, it may be pertinent to evaluate how 
far the system goes in living up to user 
expectations. Although it is impossible to satisfy 
everybody, the system developers need to know 
whether there is a large discrepancy between user 
expectations and the actual system, since this 
may influence the use of the system. 
6 Cross-fertilization between evaluations 
How different are restricted-domain 
evaluations from open-domain evaluations? Are 
they so diametrically opposed that restricted-
domain systems require separate evaluations 
from open-domain systems and vice versa? As 
pointed out in Section 1, we stopped 
participating in the TREC QA evaluations 
because that evaluation was not well suited to 
our restricted-domain system. However, we 
regretted this as we believe we could, 
nevertheless, have gained valuable insights. 
Clearly, open-domain systems would benefit 
from the evaluation dimensions discussed in 
Section 4. The difference would be that the test 
questions used for evaluation would be general 
rather than tailored to a specific domain. 
Additionally, it may be harder to evaluate the 
database content (i.e. the collection) for a general 
domain system than would be the case for 
restricted-domain systems.  
To make open-domain evaluations more 
applicable to restricted-domain systems, they 
could be extended to include metrics about 
answer speed, and the ability of answering within 
a certain task. For example, the evaluation could 
include system performance to get an indication 
as to how much processing time, given certain 
hardware, is required in getting the answers. As 
for answer correctness itself, it may be 
interesting to require extensive use of task 
scenarios that would determine aspects such as 
answer length and level of detail. It may also be 
desirable to evaluate runs without redundancy 
techniques separately. Ideally, users would be 
incorporated into the evaluation to assess the user 
interface and the ability of the system to assist 
them in completion of a certain task. 
 
7 Summary 
Restricted-domain systems require a more 
situated evaluation than is generally provided in 
open-domain evaluations. A restricted-domain 
evaluation extends beyond domain specific test 
questions and collections to include the user and 
their task. Users of the restricted-domain KAAS 
system identified five areas that should be 
included in an evaluation: System Performance, 
Answers, Database Content, Display, and 
Expectations. Most of these evaluation 
dimensions could be applied to open-domain 
evaluations as well. Adding system performance 
metrics (such as answer speed) and specific task 
requirements may allow a convergence between 
open domain and restricted domain QA 
evaluations. 
 
Acknowledgements 
Funding for this research has been jointly provided by 
NASA,  NY State, and AT&T. 
 
References  
 
Banko, M., Brill, E., Dumais, S. and Lin, J. 2002.  
AskMSR: Question answering using the worldwide 
Web.  In Proceedings of the 2002 AAAI Spring 
Symposium on Mining Answers from Texts and 
Knowledge Bases, March 2002, Palo Alto, 
California.  
Buchholz, S. 2002. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. In: E. M. Voorhees 
and D. K. Harman (Eds.), The Tenth Text REtrieval 
Conference (TREC 2001), volume 500-250 of 
NIST Special Publication, Gaithersburg, MD. 
National Institute of Standards and Technology, 
2002, pp. 502-509. 
Diekema, A.R., Chen, J., McCracken, N, Ozgencil, 
N.E., Taffet, M.D., Yilmazel, O. and Liddy, E.D. 
2002. Question Answering: CNLP at the TREC-
2002 Question Answering Track. In: Proceedings 
of the Eleventh Text Retrieval Conference (TREC-
2002). E.M. Voorhees and D.K. Harman (Eds.). 
Gaithersburg, MD: Department of Commerce, 
National Institute of Standards and Technology, 
2002. 
Fukumoto, J., Kato, T., and Masui, F. 2003. Question 
Answering Challenge (QAC-1): An Evaluation of 
Question Answering Tasks at the NTCIR 
Workshop 3. In Proceedings of the AAAI Spring 
Symposium: New Directions in Question 
Answering, p.122-133, 2003. 
Diekema, A.R., Yilmazel, O., Chen, J., Harwell, S., 
He, L., and Liddy, E.D. Finding Answers to 
Complex Questions. To appaer. In Maybury, M. 
(Ed.) New Directions in Question Answering. 
AAAI-MIT Press. 
Magnini, B., Romagnoli, S., Vallin, A., Herrera, J. 
Pe?as, A., Peinado, V., Verdejo, F., M. de Rijke, 
The Multiple Language Question Answering Track 
at CLEF 2003. In Carol Peters (Ed.), Working 
Notes for the CLEF 2003 Workshop, 21-22 August, 
Trondheim, Norway, 2003. 
Nyberg E. and T. Mitamura. 2002. Evaluating QA 
Systems on Multiple Dimensions. In Proceedings 
of LREC 2002 Workshop on QA Strategy and 
Resources, May 28th, Las Palmas, Gran Canaria. 
Voorhees, E.M. 2003. DRAFT Overview of the 
TREC 2003 Question Answering Track. To appear 
in Proceedings of TREC 2003. Gaithersburg, MD, 
NIST, to appear. 
Voorhees, E.M. Overview of the TREC-8 Question 
Answering Track Report. In Proceedings of TREC-
8, 77-82. Gaithersburg, MD, NIST, 2000. 
Voorhees, E.M. & Tice, D.M.  Implementing a 
Question Answering Evaluation. In Proceedings of 
LREC?2000 Workshop on Using Evaluation within 
HLT Programs: Results and Trends. 2000. 
Preliminary Lexical Framework for  
English-Arabic Semantic Resource Construction 
Anne R. Diekema 
Center for Natural Language Processing  
4-206 Center for Science & Technology 
Syracuse, NY, 13210 USA 
diekemar@syr.edu 
 
Abstract 
This paper describes preliminary work 
concerning the creation of a Framework to aid 
in lexical semantic resource construction. The 
Framework consists of 9 stages during which 
various lexical resources are collected, 
studied, and combined into a single 
combinatory lexical resource. To evaluate the 
general Framework it was applied to a small 
set of English and Arabic resources, 
automatically combining them into a single 
lexical knowledge base that can be used for 
query translation and disambiguation in Cross-
Language Information Retrieval. 
1 Introduction 
Cross-Language Information Retrieval (CLIR) 
systems facilitate matching between queries and 
documents that do not necessarily share the same 
language. To accomplish this matching between 
distinct vocabularies, a translation step is required. 
The preferred method is to translate the query 
language into the document language by using 
machine translation, or lexicon lookup. While 
machine translation may work reasonably well on 
full sentences, queries tend to be short lists of 
keywords, and are often more suited for lexical 
lookup (Oard and Diekema, 1998). 
 
This paper describes a preliminary framework 
for the creation of a lexical resource through the 
combination of other lexical resources. The 
preliminary Framework will be applied to create a 
translation lexicon for use in an English-Arabic 
CLIR system. The resulting lexicon will be used to 
translate English queries into (unvocalized) Arabic. 
It will also provide the user of the system with 
lexical semantic information about each of the 
possible translations to aid with disambiguation of 
the Arabic query. While the combination of lexical 
resources is nothing new, establishing a sound 
methodology for resource combination, as 
presented in this paper on English-Arabic semantic 
resource construction, is an important contribution. 
Once the Framework has been evaluated for 
English-Arabic resource construction, it can be 
extended to additional languages and resource 
types. 
2 Related Work 
2.1 Arabic-English dictionary combination 
As pointed out previously, translation plays an 
important role in CLIR. Most of the CLIR systems 
participating in the (Arabic) Cross-Language 
Information Retrieval track1 at the Text REtrieval 
Conference (TREC)2 used a query translation 
dictionary-based approach where each source 
query term was looked up in the translation 
resource and replaced by all or a subset of the 
available translations to create the target query 
(Larkey, Ballesteros, and Connell, 2002), (Gey and 
Oard, 2001), (Oard and Gey, 2002). The four main 
sources of translation knowledge that have been 
applied to CLIR are ontologies, bilingual 
dictionaries, machine translation lexicons, and 
corpora. 
 
Research shows that combining translation 
resources increases CLIR performance (Larkey et 
al., 2002) Not only does this combination increase 
translation coverage, it also refines translation 
probability calculations. Chen and Gey  used a 
combination of dictionaries for query translation 
and compared retrieval performance of this 
dictionary combination with machine translation 
(Chen and Gey, 2001). The dictionaries 
outperformed MT. Small bilingual dictionaries 
were created by Larkey and Connell (2001) for 
place names and also inverted an Arabic-English 
dictionary to English-Arabic. They found that 
using dictionaries that have multiple senses, 
                                                   
1
 There have been two large scale Arabic information 
retrieval evaluations as part of TREC. These Arabic 
tracks took place in 2001, and 2002 and had 
approximately 10 participating teams each. 
2
 http://trec.nist.gov 
though not always correct, outperform bilingual 
term lists with only one translation alternative. 
Combining dictionaries is especially important 
when working with ambiguous languages such as 
Arabic. 
 
Many TREC teams used translation probabilities 
to deal with translation ambiguity and term 
weighting issues, especially since a translation 
lexicon with probabilities was provided as a 
standard resource. However, most teams combined 
translation probabilities from different sources and 
achieved better retrieval results that way (Xu, 
Fraser, and Weischedel, 2002), (Chowdhury et al, 
2002), (Darwish and Oard, 2002). Darwish and 
Oard (2002) posit that since there is no such thing 
as a complete translation resource one should 
always use a combination of resources and that 
translation probabilities will be more accurate if 
one uses more resources. 
2.2 Resource combination methodologies 
Ruiz (2000) uses the term lexical triangulation 
to describe the process of mapping a bilingual 
English-Chinese lexicon into an existing WordNet-
based Conceptual Interlingua by using translation 
evidence from multiple sources. Recall that 
WordNet synsets are formed by groups of terms 
with similar meaning (Miller, 1990). By translating 
each of the synonyms into Chinese, Ruiz created a 
frequency-ranked list of translations, and assumed 
that the most frequent translations were most likely 
to be correct. By establishing certain translation 
evidence thresholds, mappings of varying 
reliability were created. This method was later 
augmented with additional translation evidence 
from a Chinese-English parallel corpus. 
 
A methodology to improve query translation is 
described by Chen (2003). The methodology is 
intended to improve translation through the use of 
NLP techniques and the combining of the 
document collection, available translation 
resources, and transliteration techniques. A basic 
mapping was created between the Chinese terms 
from the collection and the English terms in 
WordNet by using a simple Chinese-English 
lexicon. Missing terms such as Named Entities 
were added through the process of transliteration. 
By customizing the translation resources to the 
document collection Chen showed an improvement 
in retrieval performance. 
3 Establishing a Preliminary Framework 
The preliminary Framework provides a 
methodology for the automatic combination of 
various lexical semantic resources such as machine 
readable dictionaries, ontologies, encyclopedias, 
and machine translation lexicons. While these 
individual resources are all valuable individually, 
automatic intelligent lexical combination into one 
single lexical knowledge base will provide an 
enhancement that is larger than the sum of its parts. 
The resulting resource will provide better 
coverage, more reliable translation probability 
information, and additional information leveraged 
through the process of lexical triangulation. In an 
initial evaluation of the preliminary Framework, it 
was applied to the combination of English and 
Arabic lexical resources as described in section 4. 
 
The preliminary Framework consists of 9 stages: 
1) establish goals 
2) collect resources 
3) create resource feature matrix 
4) develop evidence combination strategies 
and thresholds 
5) construct combinatory lexical resource 
6) manage problems that arise during creation 
7) evaluate combinatory lexical resource 
8) implement possible improvements 
9) create final version of combinatory lexical 
resource. 
 
Stage 1: The first stage of the Framework is 
intended to establish the possible usage of the 
combinatory lexical resource (resulting form the 
combination of multiple resources). The 
requirements of this resource will drive the second 
stage: resource collection.  
 
Stage 2: Two types of resources should be 
collected: language processing resources such as 
stemmers and tokenizers; and lexical semantic 
resources such as dictionaries and lexicons. While 
not every resource may seem particularly useful at 
first, different resources can aid in mapping other 
resources together. During the second stage, 
conversion into a single encoding (such as UTF-8) 
will also take place.  
 
Stage 3: Once a set of resources has been 
collected, the resource feature matrix can be 
created. This matrix provides an overview of the 
types of information found in the collected 
resources and of certain resource characteristics. 
For example, it is important to note what base form 
the dictionary entries have. Some dictionaries use 
the singular form (for nouns) or indefinite form 
(for verbs), some use roots, others use stems, and 
free resources from the web often use a 
combination of all of the above. By studying the 
feature matrix the evidence combination strategies 
for stage four can be developed. 
  
A
ra
bi
c 
En
gl
ish
 
w
o
rd
 
st
em
 
ro
o
t 
v
o
ca
liz
ed
 
u
n
v
o
ca
liz
ed
 
po
s 
En
gl
ish
 
de
fin
iti
o
n
 
A
ra
bi
c 
de
fin
iti
o
n
 
sy
n
o
n
ym
s 
se
n
se
 
in
fo
rm
at
io
n
 
Arabeyes x x x    x      
Ajeeb x x x   x  x  x  x 
Buckwalter x x  x  x x x x   x 
Gigaword x  x    x      
WordNet 2.0  x      x x  x x 
 
Table 1:  Resource feature matrix 
 
Stage 4: An intelligent resource combination 
strategy should be informed by the features of the 
different resources. It may be, for example, that 
one resource uses vocalized Arabic only and that 
another resource uses both vocalized and 
unvocalized Arabic. This fact should be taken into 
account by the combination strategy since the 
second resource can serve as an intermediary to 
map the first resource. Thresholding decisions are 
also part of stage four because the certainty of 
some combinations will be higher than others.  
 
Stage 5: Stage five involves writing programs 
based on the findings in stage four that will 
automatically create the combinatory lexical 
resource. The combination programs should 
provide output concerning problematic instances 
that occur during the creation i.e. words that only 
occur in a single resource, so that these problems 
may be handled by alternative strategies in stage 
six.  
 
Stage 6: Most of the problems in stage six are 
likely to be uncommon words, such as named 
entities or transliteration. A transliteration step, 
where for example English letters, i.e. r, are 
mapped to the closest Arabic sounding letters, i.e. 
 , may be applied for languages that do not share 
the same orthographies.  
 
Stage 7: After the initial combinatory lexical 
resource has been created it needs to be evaluated. 
First the accuracy (quality) of the combination 
mappings of the various resources needs to be 
assessed in an intrinsic evaluation. After it has 
been established that the combination has been 
successful, an extrinsic evaluation can be carried 
out. In this evaluation the combinatory lexical 
resource is tested as part of the actual application 
the source was intended for, i.e. CLIR. (For a more 
detailed description of evaluation see Section 5 
below.) 
 
Stage 8: These two evaluations will inform stage 
eight where possible improvements are added to 
the combination process.  
 
Stage 9: The final version of the combinatory 
lexical resource can be created in stage nine. 
4 Application of the Framework to English-
Arabic 
The preliminary Framework as described in 
section 3 was applied to five English and Arabic 
language resources as a kind of feasibility test. 
Following the Framework, we first established the 
goals of the combinatory lexical resource. It was 
determined that the resource would be used as a 
translation resource for CLIR that would aid query 
translation as well as manual translation 
disambiguation by the user. This meant that the 
combinatory lexical resource would need 
translation probabilities as well as English 
definitions for Arabic translations to enable an 
English language user to select the correct Arabic 
translation. We collected five different resources: 
WordNet 2.03, the lexicon included with the 
Buckwalter Stemmer4, translations mined from 
Ajeeb5, the wordlist from the Arabeyes project6, 
and the LDC Arabic Gigaword corpus7. After the 
resources were collected the feature matrix was 
developed (see Table 1). 
                                                   
3
 http://www.cogsci.princeton.edu/~wn 
4
 http://www.qamus.org 
5
 http://english.ajeeb.com 
6
 http://www.arabeyes.org 
7
 
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?cat
alogId=LDC2003T12 
The established combinatory lexical resource 
goals and resource feature matrix were used to 
determine the combination strategy. Since the 
resource should provide the user with definitions 
of Arabic words and WordNet is most 
comprehensive in this regard, it was selected as our 
base resource. The AFP newswire collection from 
the Gigaword corpus was used to mine Ajeeb. As 
is evident in the matrix, all resources contain 
English terms as a common denominator. The 
information used for evidence combination was as 
follows. Evidence used for mapping the Ajeeb and 
Buckwalter lexicons is part-of-speech information. 
Additionally, these two resources also provide 
vocalized Arabic terms/stems that can be used for a 
more reliable (less ambiguous) match. The 
Arabeyes lexicon is not terribly rich but was used 
as additional evidence for a certain translation 
through frequency weighting. The combinatory 
lexical resource was constructed by mapping the 
three lexical resources into WordNet using the 
evidence as discussed above (see Table 2).  
 
 
world, human race, humanity, humankind, human 
beings, humans, mankind, man, all of the 
inhabitants of the earth 
all of the inhabitants of the earth 
	



Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 17?24,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Modeling Reference Interviews as a Basis for Improving Automatic QA 
Systems 
 
Nancy J. McCracken, Anne R. Diekema, Grant Ingersoll, Sarah C. 
Harwell, Eileen E. Allen, Ozgur Yilmazel, Elizabeth D. Liddy 
Center for Natural Language Processing 
Syracuse University 
Syracuse, NY 13244 
{ njmccrac, diekemar, gsingers, scharwel, eeallen, oyilmaz, liddy}@syr.edu 
 
 
 
 
 
Abstract 
 
The automatic QA system described in 
this paper uses a reference interview 
model to allow the user to guide and 
contribute to the QA process.  A set of 
system capabilities was designed and 
implemented that defines how the user?s 
contributions can help improve the 
system.  These include tools, called the 
Query Template Builder and the 
Knowledge Base Builder, that tailor the 
document processing and QA system to 
a particular domain by allowing a 
Subject Matter Expert to contribute to 
the query representation and to the 
domain knowledge.  During the QA 
process, the system can interact with the 
user to improve query terminology by 
using Spell Checking, Answer Type 
verification, Expansions and Acronym 
Clarifications.  The system also has 
capabilities that depend upon, and 
expand the user?s history of interaction 
with the system, including a User 
Profile, Reference Resolution, and 
Question Similarity modules 
 
 
1  Introduction 
 
Reference librarians have successfully fielded 
questions of all types for years using the Reference 
Interview to clarify an unfocused question, narrow 
a broad question, and suggest further information 
that the user might not have thought to ask for.  
The reference interview tries to elicit sufficient 
information about the user?s real need to enable a 
librarian to understand the question enough to 
begin searching.  The question is clarified, made 
more specific, and contextualized with relevant 
detail.  Real questions from real users are often 
?ill-formed? with respect to the information 
system; that is, they do not match the structure of 
?expectations? of the system (Ross et al, 2002). A 
reference interview translates the user?s question 
into a representation that the librarian and the 
library systems can interpret correctly. The human 
reference interview process provides an ideal, 
well-tested model of how questioner and answerer 
work together co-operatively and, we believe, can 
be successfully applied to the digital environment.  
The findings of researchers applying this model in 
online situations (Bates, 1989, Straw, 2004) have 
enabled us to understand how a system might work 
with the user to provide accurate and relevant 
answers to complex questions. 
 Our long term goal in developing Question-
Answering (QA) systems for various user groups is 
to permit, and encourage users to positively 
contribute to the QA process, to more nearly 
mirror what occurs in the reference interview, and 
to develop an automatic QA system that provides 
fuller, more appropriate, individually tailored 
responses than has been available to date. 
 Building on our Natural Language 
Processing (NLP) experience in a range of 
information access applications, we have focused 
our QA work in two areas:  1) modeling the subject 
domain of the collections of interest to a set of 
 
 
 
17
users for whom we are developing the QA system, 
and; 2) modeling the query clarification and 
negotiation interaction between the information 
seeker and the information provider. Examples of 
these implementation environments are: 
 
1. Undergraduate aerospace engineering students 
working in collaborative teams on course 
projects designing reusable launch vehicles, 
who use a QA system in their course-related 
research. 
2. Customers of online business sites who use a 
QA system to learn more about the products or 
services provided by the company, or who 
wish to resolve issues concerning products or 
service delivery. 
 
In this paper, we describe the capabilities we 
have developed for these specific projects in order 
to explicate a more general picture of how we 
model and utilize both the domains of inquiry and 
typical interaction processes observed in these 
diverse user groups. 
 
2 Background and related research 
 
Our work in this paper is based on two premises: 
1) user questions and responsive answers need to 
be understood within a larger model of the user?s 
information needs and requirements, and, 2) a 
good interactive QA system facilitates a dialogue 
with its users to ensure it understands and satisfies 
these information needs. The first premise is based 
on the long-tested and successful model of the 
reference interview (Bates, 1997, Straw, 2004), 
which was again validated by the findings of an 
ARDA-sponsored workshop to increase the 
research community?s understanding of the 
information seeking needs and cognitive processes 
of intelligence analysts (Liddy, 2003). The second 
premise instantiates this model within the digital 
and distributed information environment. 
 Interactive QA assumes an interaction 
between the human and the computer, typically 
through a combination of a clarification dialogue 
and user modeling to capture previous interactions 
of users with the system. De Boni et al (2005) 
view the clarification dialogue mainly as the 
presence or absence of a relationship between the 
question from the user and the answer provided by 
the system. For example, a user may ask a 
question, receive an answer and ask another 
question in order to clarify the meaning, or, the 
user may ask an additional question which expands 
on the previous answer. In their research De Boni 
et al (2005) try to determine automatically 
whether or not there exists a relationship between a 
current question and preceding questions, and if 
there is a relationship, they use this additional 
information in order to determine the correct 
answer.  
 We prefer to view the clarification dialogue 
as more two-sided, where the system and the user 
actually enter a dialogue, similar to the reference 
interview as carried out by reference librarians 
(Diekema et al, 2004). The traditional reference 
interview is a cyclical process in which the 
questioner poses their question, the librarian (or the 
system) questions the questioner, then locates the 
answer based on information provided by the 
questioner, and returns an answer to the user who 
then determines whether this has satisfied their 
information need or whether further clarification or 
further questions are needed.  The HITIQA 
system?s (Small et al, 2004) view of a clarification 
system is closely related to ours?their dialogue 
aligns the understanding of the question between 
system and user. Their research describes three 
types of dialogue strategies: 1) narrowing the 
dialogue, 2) broadening the dialogue, and 3) a fact 
seeking dialogue. 
 Similar research was carried out by Hori et 
al. (2003), although their system automatically 
determines whether there is a need for a dialogue, 
not the user. The system identifies ambiguous 
questions (i.e. questions to which the system could 
not find an answer). By gathering additional 
information, the researchers believe that the system 
can find answers to these questions. Clarifying 
questions are automatically generated based on the 
ambiguous question to solicit additional 
information from the user. This process is 
completely automated and based on templates that 
generate the questions. Still, removing the 
cognitive burden from the user through automation 
is not easy to implement and can be the cause of 
error or misunderstanding. Increasing user 
involvement may help to reduce this error. 
 As described above, it can be seen that 
interactive QA systems have various levels of 
dialogue automation ranging from fully automatic 
(De Boni et al, 2004, Hori et al, 2004) to a strong 
18
user involvement (Small et al, 2004, Diekema et 
al., 2004). Some research suggests that 
clarification dialogues in open-domain systems are 
more unpredictable than those in restricted domain 
systems, the latter lending itself better to 
automation (Hori et al, 2003, J?nsson et al, 2004).  
Incorporating the user?s inherent knowledge of the 
intention of their query is quite feasible in 
restricted domain systems and should improve the 
quality of answers returned, and make the 
experience of the user a less frustrating one. While 
many of the systems described above are 
promising in terms of IQA, we believe that 
incorporating knowledge of the user in the 
question negotiation dialogue is key to developing 
a more accurate and satisfying QA system.   
 
3 System Capabilities 
 
In order to increase the contribution of users to our 
question answering system, we expanded our 
traditional domain independent QA system by 
adding new capabilities that support system-user 
interaction. 
 
3.1  Domain Independent QA 
 
Our traditional domain-independent QA capability 
functions in two stages, the first information 
retrieval stage selecting a set of candidate 
documents, the second stage doing the answer 
finding within the filtered set.  The answer finding 
process draws on models of question types and 
document-based knowledge to seek answers 
without additional feedback from the user.  Again, 
drawing on the modeling of questions as they 
interact with the domain representation, the system 
returns answers of variable lengths on the fly in 
response to the nature of the question since factoid 
questions may be answered with a short answer, 
but complex questions often require longer 
answers.  In addition, since our QA projects were 
based on closed collections, and since closed 
collections may not provide enough redundancy to 
allow for short answers to be returned, the variable 
length answer capability assists in finding answers 
to factoid questions.  The QA system provides 
answers in the form of short answers, sentences, 
and answer-providing passages, as well as links to 
the full answer-providing documents. The user can 
provide relevance feedback by selecting the full 
documents that offer the best information.  Using 
this feedback, the system can reformulate the 
question and look for a better set of documents 
from which to find an answer to the question. 
Multiple answers can be returned, giving the user a 
more complete picture of the information held 
within the collection.   
 One of our first tactics to assist in both 
question and domain modeling for specific user 
needs was to develop tools for Subject Matter 
Experts (SMEs) to tailor our QA systems to a 
particular domain.  Of particular interest to the 
interactive QA community is the Query Template 
Builder (QTB) and the Knowledge Base Builder 
(KBB).  
 Both tools allow a priori alterations to 
question and domain modeling for a community, 
but are not sensitive to particular users.  Then the 
interactive QA system permits question- and user-
specific tailoring of system behavior simply 
because it allows subject matter experts to change 
the way the system understands their need at the 
time of the search. 
 Question Template Builder (QTB) allows 
a subject matter expert to fine tune a question 
representation by adding or removing stopwords 
on a question-by-question basis, adding or masking 
expansions, or changing the answer focus.  The 
QTB displays a list of Question-Answer types, 
allows the addition of new Answer Types, and 
allows users to select the expected answer type for 
specific questions.  For example, the subject matter 
expert may want to adjust particular ?who? 
questions as to whether the expected answer type is 
?person? or ?organization?.  The QTB enables 
organizations to identify questions for which they 
want human intervention and to build specialized 
term expansion sets for terms in the collection.  
They can also adjust the stop word list, and refine 
and build the Frequently or Previously Asked 
Question (FAQ/PAQ) collection. 
 Knowledge Base Builder (KBB) is a suite 
of tools developed for both commercial and 
government customers.  It allows the users to view 
and extract terminology that resides in their 
document collections.  It provides useful statistics 
about the corpus that may indicate portions that 
require attention in customization.  It collects 
frequent / important terms with categorizations to 
enable ontology building (semi-automatic, 
permitting human review), term collocation for use 
19
in identifying which sense of a word is used in the 
collection for use in term expansion and 
categorization review.  KBB allows companies to 
tailor the QA system to the domain vocabulary and 
important concept types for their market.  Users 
are able to customize their QA applications 
through human-assisted automatic procedures.  
The Knowledge Bases built with the tools are  
 
 
IR Answer Providers
Question 
Processing
Session 
Tracking
Reference 
Resolution
User Profile
Question 
Similarity
User
Answer
Spell 
checking
Answer 
Type 
Verification
Expansion 
Clarification
Domain Modeling
QTB KBB
 
Figure 1. System overview 
 
 
primarily lexical semantic taxonomic resources.  
These are used by the system in creating frame 
representations of the text.  Using automatically 
harvested data, customers can review and alter 
categorization of names and entities and expand 
the underlying category taxonomy to the domain of 
interest.  For example, in the NASA QA system, 
experts added categories like ?material?, ?fuel?, 
?spacecraft? and ?RLV?, (Reusable Launch 
Vehicles).  They also could specify that ?RLV? is a 
subcategory of ?spacecraft? and that space shuttles 
like ?Atlantis? have category ?RLV?.  The KBB 
works in tandem with the QTB, where the user can 
find terms in either documents or example queries 
 
3.2 Interactive QA Development 
 
In our current NASA phase, developed for 
undergraduate aerospace engineering students to 
quickly find information in the course of their 
studies on reusable launch vehicles, the user can 
view immediate results, thus bypassing the 
Reference Interviewer, or they may take the 
opportunity to utilize its increased functionality 
and interact with the QA system. The capabilities 
we have developed, represented by modules added 
to the system, fall into two groups. Group One 
includes capabilities that draw on direct interaction 
with the user to clarify what is being asked and that 
address terminological issues.  It includes Spell 
Checking, Expansion Clarification, and Answer 
Type Verification. Answers change dynamically as 
the user provides more input about what was 
meant. Group Two capabilities are dependent 
upon, and expand upon the user?s history of 
interaction with the system and include User 
Profile, Session Tracking, Reference Resolution, 
Question Similarity and User Frustration 
Recognition modules.  These gather knowledge 
about the user, help provide co-reference 
resolution within an extended dialogue, and 
monitor the level of frustration a user is 
experiencing.   
20
 The capabilities are explained in greater 
detail below.  Figure 1 captures the NASA system 
process and flow.  
 
Group One: 
  
In this group of interactive capabilities, after the 
user asks a query, answers are returned as in a 
typical system.  If the answers presented aren?t 
satisfactory, the system will embark on a series of 
interactive steps (described below) in which 
alternative spelling, answertypes, clarifications and 
expansions will be suggested.   The user can 
choose from the system?s suggestions or type in 
their own.  The system will then revise the query 
and return a new set of answers.  If those answers 
aren?t satisfactory, the user can continue 
interacting with the system until appropriate 
answers are found. 
Spell checking: Terms not found in the 
index of the document collection are displayed as 
potentially misspelled words.  In this preliminary 
phase, spelling is checked and users have the 
opportunity to select correct and/or alternative 
spellings.  
 AnswerType verification: The interactive 
QA system displays the type of answer that the 
system is looking for in order to answer the 
question.  For example for the question, Who 
piloted the first space shuttle?, the answer type is 
?person?, and the system will limit the search for 
candidate short answers in the collection to those 
that are a person?s name.  The user can either 
accept the system?s understanding of the question 
or reject the type it suggests.  This is particularly 
useful in semantically ambiguous questions such as 
?Who makes Mountain Dew?? where the system 
might interpret the question as needing a person, 
but the questioner actually wants the name of a 
company.  
Expansion:  This capability allows users to 
review the possible relevant terms (synonyms and 
group members) that could enhance the question-
answering process.  The user can either select or 
deselect terms of interest which do or do not 
express the intent of the question.  For example, if 
the user asks: How will aerobraking change the 
orbit size? then the system can bring back the 
following expansions for ?aerobraking?:  By 
aerobraking do you mean the following: 1) 
aeroassist, 2) aerocapture, 3) aeromaneuvering, 4) 
interplanetary transfer orbits, or 5) transfer orbits. 
Acronym Clarification: For abbreviations 
or acronyms within a query, the full explications 
known by the system for the term can be displayed 
back to the user.  The clarifications implemented 
are a priori limited to those that are relevant to the 
domain.  In the aerospace domain for example, if 
the question was What is used for the TPS of the 
RLV?, the clarifications of TPS would be thermal 
protection system, thermal protection subsystem, 
test preparation sheet, or twisted pair shielded, and 
the clarification of RLV would be reusable launch 
vehicle.  The appropriate clarifications can be 
selected to assist in improving the search.  For a 
more generic domain, the system would offer 
broader choices.  For example, if the user types in 
the question: What educational programs does the 
AIAA offer?, then the system might return: By 
AIAA, do you mean (a) American Institute of 
Aeronautics and Astronautics (b) Australia 
Indonesia Arts Alliance or (c) Americans for 
International Aid & Adoption?   
 
Group Two: 
 
User Profile: The User Profile keeps track of more 
permanent information about the user.  The profile 
includes a small standard set of user attributes, 
such as the user?s name and / or research interests.  
In our commercially funded work, selected 
information gleaned from the question about the 
user was also captured in the profile.  For example, 
if a user asks ?How much protein should my 
husband be getting every day??, the fact that the 
user is married can be added to their profile for 
future marketing, or for a new line of dialogue to 
ask his name or age.  This information is then 
made available as context information for the QA 
system to resolve references that the user makes to 
themselves and their own attributes.  
 For the NASA question-answering 
capability, to assist students in organizing their 
questions and results, there is an area for users to 
save their searches as standing queries, along with 
the results of searching (Davidson, 2006).  This 
information, representing topics and areas of 
interest, can help to focus answer finding for new 
questions the user asks. 
Not yet implemented, but of interest, is the 
ability to save information such as a user?s 
21
preferences (format, reliability, sources), that could 
be used as filters in the answer finding process. 
 Reference Resolution:  A basic feature of 
an interactive QA system is the requirement to 
understand the user?s questions and responsive 
answers as one session. The sequence of questions 
and answers forms a natural language dialogue 
between the user and the system.  This necessitates 
NLP processing at the discourse level, a primary 
task of which is to resolve references across the 
session.  Building on previous work in this area 
done for the Context Track of TREC 2001 
(Harabagiu et al 2001) and additional work (Chai 
and Jin, 2004) suggesting discourse structures are 
needed to understand the question/answer 
sequence, we have developed session-based 
reference resolution capability. In a dialogue, the 
user naturally includes referring phrases that 
require several types of resolution. 
 The simplest case is that of referring 
pronouns, where the user is asking a follow-up 
question, for example: 
 
Q1:  When did Madonna enter the music business? 
A1:  Madonna's first album, Madonna, came out in 
1983 and since then she's had a string of hits, been 
a major influence in the music industry and 
become an international icon. 
Q2:  When did she first move to NYC? 
 
In this question sequence, the second 
question contains a pronoun, ?she?, that refers to 
the person ?Madonna? mentioned both in the 
previous question and its answer.    Reference 
resolution would transform the question into 
?When did Madonna first move to NYC?? 
Another type of referring phrase is the 
definite common noun phrase, as seen in the next 
example: 
 
Q1: If my doctor wants me to take Acyclovir, is it 
expensive?  
A1:  Glaxo-Wellcome, Inc., the company that 
makes Acyclovir, has a program to assist 
individuals that have HIV and Herpes.  
Q2:  Does this company have other assistance 
programs? 
 
The second question has a definite noun 
phrase ?this company? that refers to ?Glaxo-
Wellcome, Inc.? in the previous answer, thus 
transforming the question to ?Does Glaxo-
Wellcome, Inc. have other assistance programs?? 
Currently, we capture a log of the 
question/answer interaction, and the reference 
resolution capability will resolve any references in 
the current question that it can by using linguistic 
techniques on the discourse of the current session.  
This is almost the same as the narrative 
coreference resolution used in documents, with the 
addition of the need to understand first and second 
person pronouns from the dialogue context.  The 
coreference resolution algorithm is based on 
standard linguistic discourse processing techniques 
where referring phrases and candidate resolvents 
are analyzed along a set of features that typically 
includes gender, animacy, number, person and the 
distance between the referring phrase and the 
candidate resolvent. 
Question Similarity: Question Similarity is 
the task of identifying when two or more questions 
are related.  Previous studies (Boydell et al, 2005, 
Balfe and Smyth, 2005) on information retrieval 
have shown that using previously asked questions 
to enhance the current question is often useful for 
improving results among like-minded users.  
Identifying related questions is useful for finding 
matches to Frequently Asked Questions (FAQs) 
and Previously Asked Questions (PAQs) as well as 
detecting when a user is failing to find adequate 
answers and may be getting frustrated.  
Furthermore, similar questions can be used during 
the reference interview process to present 
questions that other users with similar information 
needs have used and any answers that they 
considered useful.   
CNLP?s question similarity capability 
comprises a suite of algorithms designed to 
identify when two or more questions are related.  
The system works by analyzing each query using 
our Language-to-Logic (L2L) module to identify 
and weight keywords in the query, provide 
expansions and clarifications, as well as determine 
the focus of the question and the type of answer the 
user is expecting (Liddy et al, 2003).  We then 
compute a series of similarity measures on two or 
more L2L queries.  Our measures adopt a variety 
of approaches, including those that are based on 
keywords in the query: cosine similarity, keyword 
string matching, expansion analysis, and spelling 
variations.  In addition, two measures are based on 
the representation of the whole query:answer type 
22
and answer frame analysis. An answer frame is our 
representation of the meaningful extractions 
contained in the query, along with metadata about 
where they occur and any other extractions that 
relate to in the query. 
Our system will then combine the weighted 
scores of two or more of these measures to 
determine a composite score for the two queries, 
giving more weight to a measure that testing has 
determined to be more useful for a particular task. 
We have utilized our question similarity 
module for two main tasks.  For FAQ/PAQ (call it 
XAQ) matching, we use question similarity to 
compare the incoming question with our database 
of XAQs.  Through empirical testing, we 
determined a threshold above which we consider 
two questions to be similar. 
Our other use of question similarity is in the 
area of frustration detection.  The goal of 
frustration detection is to identify the signs a user 
may be giving that they are not finding relevant 
answers so that the system can intervene and offer 
alternatives before the user leaves the system, such 
as similar questions from other users that have 
been successful.    
 
4 Implementations:  
 
The refinements to our Question Answering 
system and the addition of interactive elements 
have been implemented in three different, but 
related working systems, one of which is strictly an 
enhanced IR system.  None of the three 
incorporates all of these capabilities.  In our work 
for MySentient, Ltd, we developed the session-
based reference resolution capability, implemented 
the variable length and multiple answer capability, 
modified our processing to facilitate the building 
of a user profile, added FAQ/PAQ capability, and 
our Question Similarity capability for both 
FAQ/PAQ matching and frustration detection.  A 
related project, funded by Syracuse Research 
Corporation, extended the user tools capability to 
include a User Interface for the KBB and basic 
processing technology.  Our NASA project has 
seen several phases.  As the project progressed, we 
added the relevant developed capabilities for 
improved performance.  In the current phase, we 
are implementing the capabilities which draw on 
user choice.  
 
5 Conclusions and Future Work 
 
 The reference interview has been 
implemented as an interactive dialogue between 
the system and the user, and the full system is near 
completion. We are currently working on two 
types of evaluation of our interactive QA 
capabilities. One is a system-based evaluation in 
the form of unit tests, the other is a user-based 
evaluation. The unit tests are designed to verify 
whether each module is working correctly and 
whether any changes to the system adversely affect 
results or performance. Crafting unit tests for 
complex questions has proved challenging, as no 
gold standard for this type of question has yet been 
created.  As the data becomes available, this type 
of evaluation will be ongoing and part of regular 
system development. 
 As appropriate for this evolutionary work 
within specific domains for which there are not 
gold standard test sets, our evaluation of the QA 
systems has focused on qualitative assessments. 
What has been a particularly interesting outcome is 
what we have learned in elicitation from graduate 
students using the NASA QA system, namely that 
they have multiple dimensions on which they 
evaluate a QA system, not just traditional recall 
and precision (Liddy et al 2004). The high level 
dimensions identified include system performance, 
answers, database content, display, and 
expectations. Therefore the evaluation criteria we 
believe appropriate for IQA systems are centered 
around the display (UI) category as described in 
Liddy et al (2004).  We will evaluate aspects of 
the UI input subcategory, including question 
understanding, information need understanding, 
querying style, and question formulation 
assistance. Based on this user evaluation the 
system will be improved and retested.   
 
 
References 
 
Evelyn Balfe and Barry Smyth. 2005. An Analysis 
of Query Similarity in Collaborative Web 
Search. In Proceedings of the 27th European 
Conference on Information Retrieval. Santiago 
de Compostela, Spain. 
 
23
Marcia J. Bates. 1989. The Design of Browsing 
and Berrypicking Techniques for the Online 
Search Interface.  Online Review, 13: 407-424. 
 
Mary Ellen Bates. 1997. The Art of the Reference 
Interview.  Online World. September 15. 
 
Ois?n Boydell, Barry Smyth, Cathal Gurrin, and 
Alan F. Smeaton. 2005. A Study of Selection 
Noise in Collaborative Web Search. In 
Proceedings of the 19th International Joint 
Conference on Artificial Intelligence. Edinburgh, 
Scotland. 
http://www.ijcai.org/papers/post-0214.pdf 
 
Joyce Y. Chai, and Rong Jin. 2004.  Discourse 
Structure for Context Question Answering.  In 
Proceedings of the Workshp on the Pragmatics 
of Quesiton Answering, HST-NAACL, Boston. 
http://www.cse.msu.edu/~rongjin/publications/H
LTQAWorkshop04.pdf   
 
Barry D. Davidson. 2006. An Advanced Interactive 
Discovery Learning Environment for 
Engineering Education: Final Report.  
Submitted to R. E. Gillian, National Aeronautics 
and Space Administration. 
 
Marco De Boni and Suresh Manandhar. 2005. 
Implementing Clarification Dialogues in Open 
Domain Question Answering. Natural Language 
Engineering 11(4): 343-361. 
 
Anne R. Diekema, Ozgur Yilmazel, Jiangping 
Chen, Sarah Harwell, Lan He, and Elizabeth D. 
Liddy. 2004. Finding Answers to Complex 
Questions. In New Directions in Question 
Answering. (Ed.) Mark T. Maybury. The MIT 
Press, 141-152. 
 
Sanda Harabagiu, Dan Moldovan, Marius Pa?ca, 
Mihai Surdeanu, Rada Mihalcea, Roxana G?rju, 
Vasile Rus, Finley L?c?tu?u, Paul Mor?rescu, 
R?zvan Bunescu. 2001. Answering Complex, 
List and Context Questions with LCC?s 
Question-Answering Server, TREC 2001. 
 
Chiori Hori, Takaaki Hori., Hideki Isozaki, Eisaku 
Maeda, Shigeru Katagiri, and Sadaoki Furui. 
2003. Deriving Disambiguous Queries in a 
Spoken Interactive ODQA System. In ICASSP. 
Hongkong, I: 624-627. 
 
Arne J?nsson, Frida And?n, Lars Degerstedt, 
Annika Flycht-Eriksson, Magnus Merkel, and 
Sara Norberg. 2004. Experiences from 
Combining Dialogue System Development With 
Information Extraction Techniques. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 153-164. 
 
Elizabeth D. Liddy. 2003. Question Answering in 
Contexts. Invited Keynote Speaker. ARDA 
AQUAINT Annual Meeting. Washington, DC. 
Dec 2-5, 2003. 
 
Elizabeth D. Liddy, Anne R. Diekema, Jiangping 
Chen, Sarah Harwell, Ozgur Yilmazel, and Lan 
He. 2003. What do You Mean? Finding Answers 
to Complex Questions. Proceedings of New 
Directions in Question Answering. AAAI Spring 
Symposium, March 24-26. 
 
Elizabeth D. Liddy, Anne R. Diekema, and Ozgur 
Yilmazel. 2004. Context-Based Question-
Answering Evaluation. In Proceedings of the 27th 
Annual ACM-SIGIR Conference. Sheffield, 
England 
 
Catherine S. Ross, Kirsti Nilsen, and Patricia 
Dewdney. 2002. Conducting the Reference 
Interview.  Neal-Schuman, New York, NY. 
 
Sharon Small, Tomek Strzalkowski, Ting Liu, 
Nobuyuki Shimizu, and Boris Yamrom. 2004. A 
Data Driven Approach to Interactive QA. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 129-140. 
 
Joseph E. Straw. 2004. Expecting the Stars but 
Getting the Moon: Negotiating around Patron 
Expectations in the Digital Reference 
Environment. In The Virtual Reference 
Experience: Integrating Theory into Practice. 
Eds. R. David Lankes, Joseph Janes, Linda C. 
Smith, and Christina M.  Finneran. Neal-
Schuman, New York, NY. 
 
24
