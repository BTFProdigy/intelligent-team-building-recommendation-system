Proceedings of the Workshop on BioNLP, pages 97?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 
Towards Automatic Generation of Gene Summary 
 
 
Feng Jin Minlie Huang 
Dept. Computer Science and Technology Dept. Computer Science and Technology 
Tsinghua University Tsinghua University 
Beijing 100084, China Beijing 100084, China 
jinfengfeng@gmail.com aihuang@tsinghua.edu.cn 
Zhiyong Lu Xiaoyan Zhu 
National Center for Biotechnology Information Dept. Computer Science and Technology 
National Library of Medicine Tsinghua University 
Bethesda, 20894, USA Beijing 100084, China 
luzh@ncbi.nlm.nih.gov zxy-dcs@tsinghua.edu.cn 
 
  
  
 
 
Abstract 
In this paper we present an extractive system that au-
tomatically generates gene summaries from the biomed-
ical literature. The proposed text summarization system 
selects and ranks sentences from multiple MEDLINE 
abstracts by exploiting gene-specific information and 
similarity relationships between sentences. We evaluate 
our system on a large dataset of 7,294 human genes and 
187,628 MEDLINE abstracts using Recall-Oriented 
Understudy for Gisting Evaluation (ROUGE), a widely 
used automatic evaluation metric in the text summariza-
tion community. Two baseline methods are used for 
comparison. Experimental results show that our system 
significantly outperforms the other two methods with 
regard to all ROUGE metrics. A demo website of our 
system is freely accessible at 
http://60.195.250.72/onbires/summary.jsp.  
1 Introduction 
Entrez Gene is a database for gene-centric infor-
mation maintained at the National Center for Bio-
technology Information (NCBI). It includes genes 
from completely sequenced genomes (e.g. Homo 
sapiens). An important part of a gene record is the 
summary field (shown in Table 1), which is a small 
piece of text that provides a quick synopsis of what 
is known about the gene, the function of its en-
coded protein or RNA products, disease associa-
tions, genetic interactions, etc. The summary field, 
when available, can help biologists to understand 
the target gene quickly by compressing a huge 
amount of knowledge from many papers to a small 
piece of text. At present, gene summaries are gen-
erated manually by the National Library of Medi-
cine (NLM) curators, a time- and labor-intensive 
process. A previous study has concluded that ma-
nual curation is not sufficient for annotation of ge-
nomic databases (Baumgartner et al, 2007). 
Indeed, of the 5 million genes currently in Entrez 
Gene, only about 20,000 genes have a correspond-
ing summary. Even in humans, arguably the most 
important species, the coverage is modest: only 26% 
of human genes are curated in this regard. The goal 
of this work is to develop and evaluate computa-
tional techniques towards automatic generation of 
gene summaries. 
To this end, we developed a text summarization 
system that takes as input MEDLINE documents 
related to a given target gene and outputs a small 
set of genic information rich sentences. Specifical-
ly, it first preprocesses and filters sentences that do 
 
97
 Gene Number  of 
Abstracts 
GO terms Human-writtenSummary 
EFEMP1 26 calcium ion binding 
protein binding 
extracellular region 
proteinaceous extracellu-
lar matrix 
This gene spans approximately 18 kb of genomic DNA and consists of 12 ex-
ons. Alternative splice patterns in the 5\' UTR result in three transcript variants 
encoding the same extracellular matrix protein. Mutations in this gene are asso-
ciated with Doyne honeycomb retinal dystrophy. 
IL20RA 15 blood coagulation 
receptor activity 
integral to membrane 
membrane 
The protein encoded by this gene is a receptor for interleukin 20 (IL20), a cyto-
kine that may be involved in epidermal function. The receptor of IL20 is a hete-
rodimeric receptor complex consisting of this protein and interleukin 20 
receptor beta (IL20B). This gene and IL20B are highly expressed in skin. The 
expression of both genes is found to be upregulated in Psoriasis. 
Table1. Two examples of human-written gene summaries 
not include enough informative words for gene 
summaries. Next, the remaining sentences are 
ranked by the sum of two individual scores: a) an 
authority score from a lexical PageRank algorithm 
(Erkan and Radev, 2004) and b) a similarity score 
between the sentence and the Gene Ontology (GO) 
terms with which the gene is annotated (To date, 
over 190,000 genes have two or more associated 
GO terms). Finally, redundant sentences are re-
moved and top ranked sentences are nominated for 
the target gene.  
In order to evaluate our system, we assembled a 
gold standard dataset consisting of handwritten 
summaries for 7,294 human genes and conducted 
an intrinsic evaluation by measuring the amount of 
overlap between the machine-selected sentences 
and human-written summaries. Our metric for the 
evaluation was ROUGE1, a widely used intrinsic 
summarization evaluation metric. 
2 Related Work 
Summarization systems aim to extract salient text 
fragments, especially sentences, from the original 
documents to form a summary. A number of me-
thods for sentence scoring and ranking have been 
developed. Approaches based on sentence position 
(Edmundson, 1969), cue phrase (McKeown and 
Radev, 1995), word frequency (Teufel and Moens, 
1997), and discourse segmentation (Boguraev and 
Kennedy, 1997) have been reported. Radev et al 
(Radev et al, 2004) developed an extractive multi-
document summarizer, MEAD, which extracts a 
summary from multiple documents based on the 
document cluster centroid, position and first-
sentence overlap. Recently, graph-based ranking 
methods, such as LexPageRank (Erkan and Radev, 
2004) and TextRank (Mihalcea and Tarau, 2004), 
                                                          
1 http://haydn.isi.edu/ROUGE/ 
have been proposed for multi-document summari-
zation. Similar to the original PageRank algorithm, 
these methods make use of similarity relationships 
between sentences and then rank sentences accord-
ing to the ?votes? or ?recommendations? from 
their neighboring sentences. 
Lin and Hovy (2000) first introduced topic sig-
natures which are topic relevant terms for summa-
rization. Afterwards, this technique was 
successfully used in a number of summarization 
systems (Hickl et al, 2007, Gupta and Nenkova et 
al., 2007). In order to improve sentence selection, 
we adopted the idea in a similar way to identify 
terms that tend to appear frequently in gene sum-
maries and subsequently filter sentences that in-
clude none or few such terms. 
Compared with newswire document summariza-
tion, much less attention has been paid to summa-
rizing MEDLINE documents for genic information. 
Ling et al (Ling et al, 2006 and 2007) presented 
an automatic gene summary generation system that 
constructs a summary based on six aspects of a 
gene, such as gene products, mutant phenotype, etc. 
In their system, sentences were ranked according 
to a) the relevance to each category (namely the 
aspect), b) the relevance to the document where 
they are from; and c) the position where sentences 
are located. Although the system performed well 
on a small group of genes (10~20 genes) from Fly-
base, their method relied heavily on high-quality 
training data that is often hard to obtain in practice.  
Yang et al reported a system (Yang et al, 2007 
and 2009) that produces gene summaries by focus-
ing on gene sets from microarray experiments. 
Their system first clustered gene set into functional 
related groups based on free text, Medical Subject 
Headings (MeSH?) and Gene Ontology (GO) fea-
tures. Then, an extractive summary was generated 
for each gene following the Edmundson paradigm 
98
 (Edmundson, 1969). Yang et al also presented 
evaluation results based on human ratings of eight 
gene summaries.  
Another related work is the second task of Text 
REtrieval Conference 2  (TREC) 2003 Genomics 
Track. Participants in the track were required to 
extract GeneRIFs from MEDLINE abstracts 
(Hersh and Bhupatiraju, 2003). Many teams ap-
proached the task as a sentence classification prob-
lem using GeneRIFs in the Entrez database as 
training data (Bhalotia et al, 2003; Jelier et al, 
2003). This task has also been approached as a sin-
gle document summarization problem (Lu et al, 
2006).  
The gene summarization work presented here 
differs from the TREC task in that it deals with 
multiple documents. In contrast to the previously 
described systems for gene summarization, our 
approach has three novel features. First, we are 
able to summarize all aspects of gene-specific in-
formation as opposed to a limited number of prede-
termined aspects. Second, we exploit a lexical 
PageRank algorithm to establish similarity rela-
tionships between sentences. The importance of a 
sentence is based not only on the sentence itself, 
but also on its neighbors in a graph representation. 
Finally, we conducted an intrinsic evaluation on a 
large publicly available dataset. The gold standard 
assembled in this work makes it possible for com-
parisons between different gene summarization 
systems without human judgments.  
3 Method 
To determine if a sentence is extract worthy, we 
consider three different aspects: (1) the number of 
salient or informative words that are frequently 
used by human curators for writing gene summa-
ries; (2) the relative importance of a sentence to be 
included in a gene summary; (3) the gene-specific 
information that is unique between different genes.  
Specifically, we look for signature terms in 
handwritten summaries for the first aspect. Ideally, 
computer generated summaries should resemble 
handwritten summaries. Thus the terms used by 
human curators should also occur frequently in 
automatically generated summaries. In this regard, 
we use a method similar to Lin and Hovy (2000) to 
identify signature terms and subsequently use them 
                                                          
2 http://ir.ohsu.edu/genomics/ 
to discard sentences that contain none or few such 
terms. For the second aspect, we adopt a lexical 
PageRank method to compute the sentence impor-
tance with a graph representation. For the last as-
pect, we treat each gene as having its own 
properties that distinguish it from others. To reflect 
such individual differences in the machine-
generated summaries, we exploit a gene?s GO an-
notations as a surrogate for its unique properties 
and look for their occurrence in abstract sentences.  
Our gene summarization system consists of 
three components: a preprocessing module, a sen-
tence ranking module, and a redundancy removal 
and summary generation module. Given a target 
gene, the preprocessing module retrieves corres-
ponding MEDLINE abstracts and GO terms ac-
cording to the gene2pubmed and gene2go data 
provided by Entrez Gene. Then the abstracts are 
split into sentences by the MEDLINE sentence 
splitter in the LingPipe3 toolkit. The sentence rank-
ing module takes these as input and first filters out 
some non-informative sentences. The remaining 
sentences are then scored according to a linear 
combination of the PageRank score and GO relev-
ance score.  Finally, a gene summary is generated 
after redundant sentences are removed. The system 
is illustrated in Figure 1 and is described in more 
detail in the following sections.  
 
 
Figure 1. System overview  
3.1 Signature Terms Extraction 
There are signature terms for different topic texts 
(Lin and Hovy, 2000). For example, terms such as 
eat, menu and fork that occur frequently in a cor-
pus may signify that the corpus is likely to be 
                                                          
3 http://alias-i.com/lingpipe/ 
Abstracts 
Sentence Segmentation 
Tokenization 
Stemming 
Signature Filtering 
PageRank Scoring 
GO Scoring 
Redundancy Removal 
GO Terms Summary 
99
 about cooking or restaurants. Similarly, there are 
signature terms for gene summaries. 
We use the Pearson?s chi-square test (Manning 
and Sch?tze, 1999) to extract topic signature terms 
from a set of handwritten summaries by comparing 
the occurrence of terms in the handwritten summa-
ries with that of randomly selected MEDLINE ab-
stracts. Let R denote the set of handwritten 
summaries and R denote the set of randomly se-
lected abstracts from MEDLINE. The null hypo-
thesis and alternative hypothesis are as follows:  
0H : ( | ) ( | )i iP t R p P t R= =   
1 1 2H : ( | ) ( | )i iP t R p p P t R= ? =   
The null hypothesis says that the term it appears 
in R and in R with an equal probability and it is 
independent from R . In contrast, the alternative 
hypothesis says that the term it is correlated with
R . We construct the following 2-by-2 contingency 
table:  
 
 R  R  
it  11O  12O  
it  21O  22O  
Table 2. Contingency table for the chi-square test. 
 
where 
11O : the frequency of term it occurring in R ; 
12O : the frequency of it occurring in R ; 
21O  : the frequency of term i it t? occurring in R ; 
22O :  the frequency of it in R .  
Then the Pearson?s chi-square statistic is computed 
by  
22
2
, 1
( )ij ij
i j ij
O E
X
E
=
?
=?  
where ijO is the observed frequency and ijE is the 
expected frequency.  
In our experiments, the significance level is set 
to 0.001, thus the corresponding chi-square value 
is 10.83. Terms with 2X value above 10.83 would 
be selected as signature terms. In total, we obtained 
1,169 unigram terms. The top ranked (by 2X value) 
signature terms are listed in Table 3. Given the set 
of signature terms, sentences containing less than 3 
signature terms are discarded. This parameter was 
determined empirically during the system devel-
opment.  
 
protein 
gene 
encode 
family 
transcription 
member 
variant 
domain 
splice 
subunit 
receptor 
isoform 
alternative 
bind 
involve 
Table 3. A sample of unigram topic signature terms. 
3.2 Lexical PageRank Scoring 
The lexical PageRank algorithm makes use of the 
similarity between sentences and ranks them by 
how similar a sentence is to all other sentences. It 
originates from the original PageRank algorithm 
(Page et al, 1998) that is based on the following 
two hypotheses:  
(1) A web page is important if it is linked by many 
other pages.  
(2) A web page is important if it is linked by im-
portant pages.  
The algorithm views the entire internet as a large 
graph in which a web page is a vertex and a di-
rected edge is connected according to the linkage. 
The salience of a vertex can be computed by a ran-
dom walk on the graph. Such graph-based methods 
have been widely adapted to such Natural Lan-
guage Processing (NLP) problems as text summa-
rization and word sense disambiguation. The 
advantage of such graph-based methods is obvious: 
the importance of a vertex is not only decided by 
itself, but also by its neighbors in a graph represen-
tation.  The random walk on a graph can imply 
more global dependence than other methods. Our 
PageRank scoring method consists of two steps: 
constructing the sentence graph and computing the 
salience score for each vertex of the graph.  
Let { |1 }iS s i N= ? ? be the sentence collec-
tion containing all the sentences to be summarized. 
According to the vector space model (Salton et al, 
1975), each sentence is  can be represented by a 
vector is
G
with each component being the weight of 
a term in is . The weight associated with a term w  
is calculated by ( )* ( )tf w isf w , where ( )tf w is the 
frequency of the term w in sentence is and ( )isf w
100
 is the inverse sentence frequency 4  of term w :
( ) 1 log( / )wisf w N n= + , where N is the total 
number of sentences in S  and wn is the number of 
sentences containing w .The similarity score be-
tween two sentences is computed using the inner 
product of the corresponding sentence vectors, as 
follows:  
( , )
|| || || ||
i j
i j
i j
s s
sim s s
s s
?
=
?
G G
G G  
Taking each sentence as a vertex, and the simi-
larity score as the weight of the edge between two 
sentences, a sentence graph is constructed. The 
graph is fully connected and undirected because 
the similarity score is symmetric.  
The sentence graph can be modeled by an adja-
cency matrix M , in which each element corres-
ponds to the weight of an edge in the graph. Thus
[ ]ij N NM ?=M is defined as:  
,
|| || || ||
0,
i j
i jij
s s
if i j
s sM
otherwise
??
??
?= ???
G G
G G
 
We normalize the row sum of matrix M  in or-
der to assure it is a stochastic matrix such that the 
PageRank iteration algorithm is applicable. The 
normalized matrix is: 
1 1
, 0
0,
N N
ij ij ij
j jij
M M if M
M
otherwise
= =
?
??
= ???
? ? . 
Using the normalized adjacency matrix, the sa-
lience score of a sentence is is computed in an 
iterative manner:  
1
(1 )
( ) ( )
N
i j ji
j
d
score s d score s M
N
=
?
= ? ? +?   
where d is a damping factor that is typically be-
tween 0.8 and 0.9 (Page et al, 1998).  
If we use a column vector p to denote the sa-
lience scores of all the sentences in S , the above 
equation can be written in a matrix form as follows:  
[ (1 ) ]Tp d d p= ? + ? ? ?M U  
                                                          
4 Isf is equivalent to idf if we view each sentence as a docu-
ment. 
where U is a square matrix with all elements being 
equal to 1/ N . The component (1 )d? ?U can be 
considered as a smoothing term which adds a small 
probability for a random walker to jump from the 
current vertex to any vertex in the graph. This 
guarantees that the stochastic transition matrix for 
iteration is irreducible and aperiodic. Therefore the 
iteration can converge to a stable state.  
In our implementation, the damping factor d is 
set to 0.85 as in the PageRank algorithm (Page et 
al., 1998). The column vector p is initialized with 
random values between 0 and 1. After the algo-
rithm converges, each component in the column 
vector p corresponds to the salience score of the 
corresponding sentence. This score is combined 
with the GO relevance score to rank sentences. 
3.3 GO Relevance Scoring 
Up to this point, our system considers only gene-
independent features, in both sentence filtering and 
PageRank-based sentence scoring. These features 
are universal across different genes. However, each 
gene is unique because of its own functional and 
structural properties. Thus we seek to include 
gene-specific features in this next step.  
The GO annotations provide one kind of gene-
specific information and have been shown to be 
useful for selecting GeneRIF candidates (Lu et al, 
2006). A gene?s GO annotations include descrip-
tions in three aspects: molecular function; biologi-
cal process; and cellular component. For example, 
the human gene AANAT (gene ID 15 in Entrez 
Gene) is annotated with the GO terms in Table 4. 
 
GO ID GO term 
GO:0004059 aralkylamine N-acetyltransferase activi-
ty 
GO:0007623 circadian rhythm 
GO:0008152 metabolic process 
GO:0008415 acyltransferase activity 
GO:0016740 transferase activity 
Table 4. GO terms for gene AANAT 
 
The GO relevance score is computed as follows: 
first, the GO terms and the sentences are both 
stemmed and stopwords are removed. For example, 
the GO terms in Table 4 are processed into a set of 
stemmed words: aralkylamin, N, acetyltransferas, 
activ, circadian, rhythm, metabol, process, acyl-
transferas and transferas.  
101
 Second, the total number of occurrence of the 
GO terms appearing in a sentence is counted. Fi-
nally, the GO relevance score is computed as the 
ratio of the total occurrence to the sentence length. 
The entire process can be illustrated by the follow-
ing pseudo codes: 
 
1 tokenize and stem the GO terms; 
2 tokenize and stem all the sentences, remove stop 
words; 
3 for each sentence is , 1,...,i N=  
( ) 0
i
GOScore s =  
for each word w  in is  
if w in the GO term set 
( )
i
GOScore s ++ 
end if 
end for 
( ) ( ) / ( )
i i i
GOScore s GOScore s length s=  
end for  
 
where ( )ilength s is the number of distinct non-stop 
words in is . For each sentence is , the GO relev-
ance score is combined with the PageRank score to 
get the overall score (? is a weight parameter be-
tween 0 and 1; see Section 4.2 for discussion): 
( ) ( ) (1 ) ( )i i iscore s PRScore s GOScore s? ?= ? + ? ? . 
3.4 Redundancy Removal  
A good summary contains as much diverse infor-
mation as possible for a gene, while with as little 
redundancy as possible. For many well-studied 
genes, there are thousands of relevant papers and 
much information is redundant. Hence it is neces-
sary to remove redundant sentences before produc-
ing a final summary.  
We adopt the diversity penalty method (Zhang 
et al, 2005; Wan and Xiao, 2007) for redundancy 
removal. The idea is to penalize the candidate sen-
tences according to their similarity to the ones al-
ready selected. The process is as follows:  
(1) Initialize two sets, A ?= ,
{ | 1, 2,..., }iB s i K= =  containing all the extracted 
sentences;  
(2)  Sort the sentences in B by their scores in des-
cending order;  
(3) Suppose is is the top ranked sentence in B , 
move it from B to A . Then we penalize the re-
maining sentences in B as follows: 
For each sentence js  in B , j i?  
( ) ( ) ( , ) ( )j j j i iScore s Score s sim s s Score s?= ? ? ?  
where 0? > is the penalty degree factor, 
( , )j isim s s  is the similarity between is and js .  
(4) Repeat steps 2 and 3 until enough sentences 
have been selected. 
4 Results and Discussion 
4.1 Evaluation Metrics 
Unlike the newswire summarization, there are no 
gold-standard test collections available for evaluat-
ing gene summarization systems. The two previous 
studies mentioned in Section 2 both conducted ex-
trinsic evaluations by asking human experts to rate 
system outputs. Although it is important to collect 
direct feedback from the users, involving human 
experts makes it difficult to compare different 
summarization systems and to conduct large-scale 
evaluations (both studies evaluated nothing but a 
small number of genes). In contrast, we evaluated 
our system intrinsically on a much larger dataset 
consisting of 7,294 human genes, each with a pre-
existing handwritten summary downloaded from 
the NCBI?s FTP site5.  
The handwritten summaries were used as refer-
ence summaries (i.e. a gold standard) to compare 
with the automatically generated summaries. Al-
though the length of reference summaries varies, 
the majority of these summaries contain 80 to 120 
words. To produce a summary of similar length, 
we decided to select five sentences consisting of 
about 100 words. 
For the intrinsic evaluation of a large number of 
summaries, we made use of the ROUGE metrics 
that has been widely used in automatic evaluation 
of summarization systems (Lin and Hovy, 2003; 
Hickl et al, 2007). It provides a set of evaluation 
metrics to measure the quality of a summary by 
counting overlapping units such as n-grams or 
word sequences between the generated summary 
and its reference summary.  
                                                          
5 ftp://ftp.ncbi.nih.gov/gene/DATA/ASN_BINARY/ 
102
 We computed three ROUGE measures for each 
summary, namely ROUGE-1 (unigram based), 
ROUGE-2 (bigram based) and ROUGE-SU4 
(skip-bigram and unigram) (Lin and Hovy, 2003). 
Among them, ROUGE-1 has been shown to agree 
most with human judgments (Lin and Hovy, 2003). 
However, as biomedical concepts usually contain 
more than one word (e.g. transcription factor), 
ROUGE-2 and ROUGE-SU4 scores are also im-
portant for assessing gene summaries.  
4.2 Determining parameters for best perfor-
mance 
The two important parameters in our system ? the 
linear coefficient ? for the combination of Page-
Rank and GO scores and the diversity penalty de-
gree factor ? in redundancy removal ? are 
investigated in detail on a collection of 100 ran-
domly selected genes. First, by setting ? to values 
from 0 to 1 with an increment of 0.1 while holding 
?  steady at 0.7, we observed the highest ROUGE-
1score when ? was 0.8 (Figure 2). This suggests 
that the two scores (i.e. PageRank and GO score) 
complement to each other and that the PageRank 
score plays a more dominating role in the summed 
score. Next, we varied? gradually from 0 to 5 with 
an increment of 0.25 while holding ? steady at 
0.75.The highest ROUGE-1 score was achieved 
when? was 1.3 (Figure 3). For ROURE-2, the best 
performance was obtained when ? was 0.7 and ?
was 0.5. In order to balance ROUGE-1 and 
ROUGE-2 scores, we set ? to 0.75 and ? to 0.7 
for the remaining experiments.  
 
Figure 2. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.7. 
 
Figure 3. The blue line represents the changes in 
ROUGE-1 scores with different values of ? while ? is 
held at 0.75. 
4.3 Comparison with other methods 
Because there are no publicly available gene sum-
marization systems, we compared our system with 
two baseline methods. The first is a well known 
publicly available summarizer - MEAD (Radev et 
al., 2004). We adopted the latest version of MEAD 
3.11 and used the default setting in MEAD that 
extracts sentences according to three features: cen-
troid, position and length. The second baseline ex-
tracts different sentences randomly from abstracts. 
Comparison results are shown in the following ta-
ble:  
 
System ROUGE-1 ROUGE-2 ROUGE-SU4
Our System 0.4725 0.1247 0.1828 
MEAD 0.3890 0.0961 0.1449 
Random 0.3434 0.0577 0.1091 
Table 5. Systems comparison on 7,294 genes. 
 
As shown in Table 5, our system significantly 
outperformed the two baseline systems in all three 
ROUGE measures. Furthermore, larger perfor-
mance gains are observed in ROUGE-2 and 
ROUGE-SU4 than in ROUGE-1. This is because 
many background words (e.g. gene, protein and 
enzyme) also appeared frequently as unigrams in 
randomly selected summaries. 
 
103
  
Figure 4. ROUGE-1 score distribution 
 
In Figure 4, we show that the majority of the 
summaries have a ROUGE-1 score greater than 0.4. 
Our further analysis revealed that almost half 
summaries with a low score (smaller than 0.3) ei-
ther lacked sufficient relevant abstracts, or the ref-
erence summary was too short or too long. In 
either case, only few overlapping words can be 
found when comparing the generated gene sum-
mary with the reference. The statistics for low 
ROUGE-1 score are listed in Table 6. We also note 
that almost half of the summaries that have low 
ROUGE-1 scores were due to other causes: mostly, 
machine generated summaries differ from human 
summaries in that they describe different function-
al aspects of the same gene product. Take the gene 
TOP2A (ID: 7153) for example. While both sum-
maries (handwritten and machine generated) focus 
on its encoded protein DNA topoisomerase, the 
handwritten summary describes the chromosome 
location of the gene whereas our algorithm selects 
statements about its gene expression when treated 
with a chemotherapy agent. We plan to investigate 
such differences further in our future work. 
 
Causes for Low Score Number of 
genes 
Few (?10) related abstracts 106 
Short reference summary (< 40 words) 27 
Long reference summary (> 150 words) 76 
Other 198 
Total 407 
Table 6. Statistics for low ROUGE-1 scores (<0.3) 
4.4 Results on various summary length 
Figure 5 shows the variations of ROUGE scores as 
the summary length increases. At all lengths and 
for both ROUGE-1 and ROUGE-2 measures, our 
proposed method performed better than the two 
baseline methods. By investigating the scores of 
different summary lengths, it can be seen that the 
advantage of our method is greater when the sum-
mary is short. This is of great importance for a 
summarization system as ordinary users typically 
prefer short content for summaries.  
 
 
Figure 5. Score variation for different summary length 
 
5 Conclusions and Future Work 
In this paper we have presented a system for gene-
rating gene summaries by automatically finding 
extract-worthy sentences from the biomedical lite-
rature. By using the state-of-the-art summarization 
techniques and incorporating gene specific annota-
tions, our system is able to generate gene summa-
ries more accurately than the baseline methods. 
Note that we only evaluated our system for human 
genes in this work. More summaries are available 
for human genes than other organisms, but our me-
thod is organism-independent and can be applied 
to any other species. 
This research has implications for real-world 
applications such as assisting manual database cu-
ration or updating existing gene records. The 
ROUGE scores in our evaluation show comparable 
performance to those in the newswire summariza-
tion (Hickl et al, 2007). Nonetheless, there are 
further steps necessary before making our system 
output readily usable by human curators. For in-
stance, human curators are generally in favor of 
sentences presented in a coherent order. Thus, in-
formation-ordering algorithms in multi-document 
summarization need to be investigated. We also 
plan to study the guidelines and scope of the cura-
tion process, which may provide additional impor-
tant heuristics to further refine our system output.  
Acknowledgments 
104
 The work is supported by NSFC project No. 
60803075, Chinese 973 Project No. 
2007CB311003. ZL is supported by the Intramural 
Program of the National Institutes of Health. The 
authors are grateful to W. John Wilbur and G. 
Craig Murray for their help on the early version of 
this manuscript.  
References 
W. A. Baumgartner, B. K. Cohen, L. M. Fox, G. Ac-
quaah-Mensah, L. Hunter. 2007. Manual Curation Is 
Not Sufficient for Annotation of Genomic Databases. 
Bioinformatics, Vol. 23, No. 13. (July 2007), pp. i41-
48. 
G. Bhalotia, P. I. Nakov, A. S. Schwartz and M. A. 
Hearst, BioText Team Report for the TREC 2003 
Genomics Track. In Proceedings of TREC 2003.  
B. Boguraev and C. Kennedy. 1997. Salience-based 
Content Characterization of Text Documents. In Pro-
ceedings of Workshop on Intelligent Scalable Text 
Summarization (ACL97/EACL97), pp. 2-9. 
J. Carbonell and J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering Docu-
ments and Producing Summaries. In ACM SIGIR, 
pages 335?336, August. 
H. P. Edmundson. 1969. New Methods in Automatic 
Extracting. Journal of the ACM (JACM) archive Vo-
lume 16,  Issue 2  (April 1969) Pages: 264 ? 285. 
G. Erkan and D. R. Radev. 2004. LexPageRank: Pres-
tige in Multi-Document Text Summarization. In Pro-
ceedings of 2004 Conference on Empirical Methods 
in Natural Language Processing (EMNLP 2004), 
Barcelona, Spain. 
S. Gupta, A.Nenkova and D.Jurafsky. 2007. Measuring 
Importance and Query Relevance in Topic-focused 
Multi-document Summarization. Proceedings of 
ACL 2007 short papers, Prague, Czech Republic. 
W. Hersh and R. T. Bhupatiraju. 2003. TREC Genomics 
track Overview. In Proceedings of TheTwelfth Text 
REtrieval Conference, 2003. 
A. Hickl, K. Roberts and F. Lacatusu. 2007. LCC's 
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. 
R. Jelier, M. Schuemie, C. Eijk, M. Weeber, E. Mulli-
gen, B. Schijvenaars, B. Mons, J. Kors. Searching for 
geneRIFs: Concept-based Query Expansion and 
Bayes Classification. In Proceedings of TREC 2003.  
C. Lin and E. Hovy. 2000. The Automated Acquisition 
of Topic Signatures for Text Summarization. In Pro-
ceedings of the COLING Conference. 
C. Lin and E. Hovy. 2003. Automatic Evaluation of 
Summaries Using N-gram Co-Occurrence Statistics. 
In HLT-NAACL, pages 71?78. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2006. Automatically Generating Gene Summaries 
from Biomedical Literature. Proceedings of the Pa-
cific Symposium on Biocomputing 2006. 
X. Ling, J. Jiang, X. He, Q. Mei, C. Zhai and B. Schatz. 
2007. Generating Gene Summaries from Biomedical 
Literature: A Study of Semi-Structured Summariza-
tion. Information Processing and Management 43, 
2007, 1777-1791. 
Z. Lu, K. B. Cohen and L. Hunter. 2006. Finding Ge-
neRIFs via Gene Ontology Annotations. Pac Symp-
Biocomput. 2006:52-63. 
C. Manning and H. Sch?tze. 1999. Foundations of Sta-
tistical Natural Language Processing. Chapter 5, MIT 
Press. Cambridge, MA: May 1999. 
K. R. McKeown and D. R. Radev. 1995. Generating 
Summaries of Multiple News Articles. In Proceed-
ings, ACM Conference on Research and Develop-
ment in Information Retrieval SIGIR'95, pages 74?
82. 
R. Mihalcea and P. Tarau. TextRank: Bringing Order 
into Texts, in Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), Barcelona, Spain, July 2004. 
M. Newman. 2003. The Structure and Function of 
Complex Networks. SIAM Review 45.167?256 
(2003). 
L. Page, S. Brin, R. Motwani and T. Winograd. The 
PageRank Citation Ranking: Bringing Order to the 
Web. Technical report, Stanford University, Stanford, 
CA, 1998. 
D. R. Radev, H. Jing, M. Stys and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management, 40:919?
938. 
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector 
Space Model for Automatic Indexing. Communica-
tions of the ACM, vol. 18, nr.11, pages 613?620. 
S. Teufel and M. Moens. 1997. Sentence Extraction as a 
Classification Task. Workshop ?Intelligent and scala-
ble Text summarization?, ACL/EACL 1997. 
X. Wan and J. Xiao. 2007. Towards a Unified Approach 
Based on Affinity Graph to Various Multi-document 
Summarizations. ECDL 2007: 297-308.  
J. Yang, A. M. Cohen, W. Hersh. Automatic Summari-
zation of Mouse Gene Information by Clustering and 
Sentence Extraction from MEDLINE Abstracts. 
AMIA 2007 Annual Meeting. Nov. 2007 Chicago, IL. 
J. Yang, A. M. Cohen, W. Hersh. 2008. Evaluation of a 
Gene Information Summarization System by Users 
During the Analysis Process of Microarray Datasets. 
In BMC Bioinformatics 2009 10(Suppl 2):S5. 
B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, 
W. Ma. 2005. Improving Web Search Results Using 
Affinity Graph. The 28th Annual International ACM 
SIGIR Conference (SIGIR'2005), August 2005.  
105
Proceedings of the Workshop on BioNLP, pages 144?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Two Biomedical Text Genres for Disease Recognition 
 
 
Aur?lie N?v?ol, Won Kim, W. John Wilbur, Zhiyong Lu* 
National Center for Biotechnology Information 
U.S. National Library of Medicine 
Bethesda, MD 20894, USA 
{neveola,wonkim,wilbur,luzh}@ncbi.nlm.nih.gov 
 
  
 
 
Abstract 
In the framework of contextual information 
retrieval in the biomedical domain, this paper 
reports on the automatic detection of disease 
concepts in two genres of biomedical text: 
sentences from the literature and PubMed user 
queries. A statistical model and a Natural 
Language Processing algorithm for disease 
recognition were applied on both corpora. 
While both methods show good performance 
(F=77% vs. F=76%) on the sentence corpus, 
results on the query corpus indicate that the 
statistical model is more robust (F=74% vs. 
F=70%).  
1 Introduction 
Contextual Information Retrieval (IR) is making 
use of additional information or assumptions about 
the users? needs beyond the obvious intent of the 
query. IR systems need to go beyond the task of 
providing generally relevant information by assist-
ing users in finding information that is relevant to 
them and their specific needs at the time of the 
search. A practical example of a Google contextual 
IR feature is when the search engine returns a map 
showing restaurant locations to a user entering a 
query such as ?Paris restaurants.? 
The contextual aspects of a user?s search were 
defined for example by Saracevic (1997) who dis-
cussed integrating the cognitive, affective, and sit-
uational levels of human computer interaction in 
IR systems. Other research efforts studied users? 
search behavior based on their level of domain 
knowledge (Zhang et al, 2005) or aimed at  mod-
eling users? interests and search habits (Rose and 
Levinson, 2004; Teevan et al, 2005).  
Information about the search context may be 
sought explicitly from the user through profiling or 
relevance feedback (Shen et al, 2005). Recent 
work also exploited query log analysis and basic 
computer environment information (Wen et al 
2004), which involve no explicit interaction with 
the user. In adaptive information retrieval, context 
information is inferred based on query analysis and 
collection characteristics (Bai and Nie 2008).  
In the biomedical domain, a need for contextual 
information retrieval was identified in particular 
for clinical queries submitted to PubMed (Pratt and 
Wasserman, 2000). Building on the idea that a spe-
cific type of document is required for searches with 
a ?clinical? context, the PubMed Clinical Queries 
portal was developed (Haynes and Wilczynski, 
2004). A perhaps more prominent contextual fea-
ture of PubMed is the ?citation sensor?, which 
identifies queries classified by Rose and Levinson 
as reflecting a ?Navigational? or ?Obtain resource? 
goal. For example, the citation sensor will identify 
and retrieve a specific citation if the user enters the 
article title as the query. The analysis of Entrez 
logs shows that MEDLINE is the most popular 
database among the 30 or so databases maintained 
by the National Center for Biotechnology Informa-
tion (NCBI) as it receives most of Entrez traffic. 
This suggests that there is a need to complement 
the information retrieved from MEDLINE by giv-
ing contextual access to other NCBI resources re-
144
levant to users? queries, such as Entrez Gene, Clin-
ical Q&A or BookShelf. In addition, the NLM es-
timated that about 1/3 of PubMed users are not 
biomedical professionals. In this light, providing 
an access point to consumer information such as 
the Genetics Home Reference might also be useful. 
To achieve this, the sensor project was recently 
launched with the goal of recognizing a variety of 
biomedical concepts (e.g. gene, protein and drug 
names) in PubMed queries. These high-level con-
cepts will help characterize users? search context in 
order to provide them with information related to 
their need beyond PubMed. For instance, if a user 
query contains the drug name ?Lipitor?, it will be 
recognized by the drug sensor and additional in-
formation on this drug from Clinical Q&A will be 
shown in the side bar in addition to default 
PubMed results. Since disease names are common 
in PubMed queries, the goal of this work is to in-
vestigate and benchmark computational techniques 
for automatic disease name recognition as an aid to 
implementing PubMed search contexts. 
2 Related Work 
Despite a significant body of literature in biomedi-
cal named entity recognition, most work has been 
focused on gene, protein, drug and chemical names 
through challenges such as BioCreAtIvE1 or the 
TREC Genomics/Chemical tracks (Park and Kim, 
2006). Other work addressed the identification of 
?medical problems? in clinical text (Aronson et al 
2007; Meystre and Haug, 2005). This task was the 
topic of a Medical NLP challenge2, which released 
a corpus of anonymized radiography reports anno-
tated with ICD9 codes. Although there is some 
interest in the biomedical community in the identi-
fication of disease names and more specifically the 
identification of relationships between diseases and 
genes or proteins (Rindflesh and Fizman, 2003), 
there are very few resources available to train or 
evaluate automatic disease recognition systems. To 
the best of our knowledge, the only publicly avail-
able corpus for disease identification in the litera-
ture was developed by Jimeno et al (2008). The 
authors annotated 551 MEDLINE sentences with 
UMLS concepts and used this dataset to bench-
mark three different automatic methods for disease 
name recognition. A MEDLINE corpus annotated 
                                                        
1 http://biocreative.sourceforge.net/ 
2 http://www.computationalmedicine.org/challenge/index.php 
with ?malignancy? mentions and part-of-speech 
tags is also available (Jin et al 2006). This corpus 
is targeted to a very restricted type of diseases. The 
annotations are also domain specific, so that ?can-
cer of the lung? is not considered a malignancy 
mention but a mention of malignancy and a men-
tion of malignancy location. 
As in previous studies, we aim to investigate the 
complexity of automatic disease recognition using 
state-of-the-art computational techniques. This 
work is novel in at least three aspects: first, in ad-
dition to using the MEDLINE sentence corpus 
(Jimeno et al2008), we developed a new corpus 
comprising disease annotations on 500 randomly 
selected PubMed queries. This allowed us to inves-
tigate the influence of local context3 through the 
comparison of system performance between two 
different genres of biomedical text. Second, by 
using a knowledge based tool previously ben-
chmarked on the same MEDLINE corpus (Jimeno 
et al 2008), we show that significant performance 
differences can be observed when parameters are 
adjusted. Finally, a state-of-the-art statistical ap-
proach was adapted for disease name recognition 
and evaluated on both corpora.  
3 Two Biomedical Corpora with disease 
annotations 
The first issue in the development of such a corpus 
is to define the very concept of disease. Among the 
numerous terminological resources available, such 
as Medical Subject Headings (MeSH?, 4,354 dis-
ease concepts) or the International Classification of 
Diseases (ICD9, ~18,000 disease concepts), the 
UMLS Metathesaurus? is the most comprehensive: 
the 2008AB release includes 252,284 concepts in 
the disorder Semantic Group defined by McCray 
et al (2001). The UMLS Metathesaurus is part of 
the Semantic Network, which also includes a set of 
broad subject categories, or Semantic Types, that 
provide a consistent categorization of all concepts 
represented in the Metathesaurus. The Semantic 
Groups aim at providing an even broader categori-
zation for UMLS concepts. For example, the dis-
order Semantic Group comprises 12 Semantic 
Types including Disease or Syndrome, Cell or Mo-
lecular Dysfunction and Congenital Abnormalities.  
                                                        
3 Here, by context, we mean the information surrounding a 
disease mention available in the corpora. This is different from 
the ?search context? previously discussed.   
145
Furthermore, like the gene mention (Morgan et 
al. 2008) and gene normalization (Smith et al 
2008) tasks in BioCreative II, the task of disease 
name recognition can also be performed at two 
different levels: 
 
1. disease mention: the detection of a snippet 
of text that refers to a disease concept (e.g. 
?alzheimer? in the sample query shown in 
Table 2)  
2. disease concept: the recognition of a con-
trolled vocabulary disease concept (e.g. 
?C0002395-alzheimer?s disease? in our Ta-
ble 2 example) in text.  
 
In this work, we evaluate and report system per-
formance at the concept level. 
3.1 Biomedical literature corpus 
Sentence Kniest dysplasia is a moderately 
severe chondrodysplasia pheno-
type that results from mutations 
in the gene for type ii collagen 
col2a1.  
Annotations C0265279-Kniest dysplasia 
C0343284-Chondrodysplasia, 
unspecified 
Table 1: Excerpt of literature corpus (PMID: 7874117) 
 
The corpus made available by Jimeno et al con-
sists of 551 MEDLINE sentences annotated with 
UMLS concepts or concept clusters: concepts that 
were found to be linked to the same term. For ex-
ample, the concepts ?Pancreatic carcinoma? 
(C0235974) and ?Malignant neoplasm of pan-
creas? (C0346647) share the same synonym ?Pan-
creas Cancer?, thus they were clustered. The 
sentences were selected from a set of articles cu-
rated for Online Mendelian Inheritance in Man 
(OMIM) and contain an average of 27(+/- 11) to-
kens, where tokens are defined as sequences of 
characters separated by white space. A set of 
UMLS concepts (or clusters) is associated with 
each sentence in the corpus. However, no boun-
dary information linking a phrase in a sentence to 
an annotation was available. Table 1 shows a sam-
ple sentence and its annotations. 
 
 
 
3.2 Biomedical query corpus 
A total of 500 PubMed queries were randomly se-
lected and divided into two batches of 300 and 200 
queries, respectively. Queries were on average 
3.45(+/- 2.64) tokens long in the 300 query batch 
and 3.58(+/- 4.63) for the 200 query batch, which 
is consistent with the average length of PubMed 
queries (3 tokens) reported by Herskovic et al 
(2007).  
The queries in the first set were annotated using 
Knowtator (Ogren, 2006) by three annotators with 
different backgrounds (one biologist, one informa-
tion scientist, one computational linguist). Two 
annotators annotated the queries using UMLS con-
cepts from the disorder group, while the other an-
notator simply annotated diseases without 
reference to UMLS concepts. Table 2 shows a 
sample query and its annotations. A consensus set 
was obtained after a meeting between the annota-
tors where diverging annotations were discussed 
and annotators agreed on a final, unique, version of 
all annotations.  The consensus set contains 89 dis-
ease concepts (76 unique). 
 
Query alzheimer csf amyloid 
Annotations  Ann. 1: ?alzheimer?; 0-8;  
Ann. 2, 3: ?alzheimer?; 0-8; 
C0002395-alzheimer?s disease 
Table 2: Excerpt of annotated 300-query corpus. Boun-
dary information is given as the character interval of the 
annotated string in the query (here, 0-8). 
 
The queries in the second set were annotated 
with UMLS concepts from the disorder group by 
one of the annotators who also worked on the pre-
vious set. In this set, 53 disease concepts were an-
notated (51 unique). 
4 Automatic disease recognition 
With the perspective of a contextual IR applica-
tion where the disease concepts found in queries 
will be used to refer users to disease-specific in-
formation in databases other than MEDLINE, we 
are concerned with high precision performance. 
For this reason, we decided to experiment with 
methods that showed the highest precision when 
compared to others. In addition, given the size of 
the corpora available and the type of the annota-
146
tions, machine learning methods such as CRFs or 
SVM did not seem applicable.  
Table 3 shows a description of the training and 
test sets for each corpus. 
 
 Table 3: Description of the training and test sets 
4.1 Natural Language Processing 
Disease recognition was performed using the Natu-
ral Language Processing algorithm implemented in 
MetaMap (Aronson, 2001)4. The tool was re-
stricted to retrieve concepts from the disorder 
group, using the UMLS 2008AB release and 
?longest match? feature. 
In practice, MetaMap parses the input text into 
noun phrases, generates variants of these phrases 
using knowledge sources such as the SPECIALIST 
lexicon, and maps the phrases to UMLS concepts.  
4.2 Priority Model 
The priority model was first introduced in (Tanabe 
and Wilbur, 2006) and is adapted here to detect 
disease mentions in free text. Because our evalua-
tion is performed at the concept level, the mentions 
extracted by the model are then mapped to UMLS 
using MetaMap.  
The priority model approach is based on two sets 
of phrases: one names of diseases, D, and one 
names of non-diseases, N. One trains the model to 
assign two numbers, p and q, to each token t that 
appears in a phrase in either D or N. Roughly, p is 
the probability that a phrase from D or N that has 
the token t in it is actually from D and q is the rela-
tive weight that should be assigned to t for this 
purpose and represents a quality estimate. Given a 
phrase 
                                                        
4 Additional information is also available at 
http://metamap.nlm.nih.gov/ 
 
1 2 kph t t t?
    (1) 
and for each it  the corresponding numbers ip  and 
iq  we estimate the probability that ph D  by 
 
1 22 11 1k kkj i i jij j iprob p q q p q  
(2) 
 
The training procedure for the model actually 
chooses the values of all the p and q quantities to 
optimize the 
prob
 values over all of D and N.  
For this work we have extended the approach to 
include a quantity  
21 1 22 11 1k kkj i i jij j iqual q p q q p q prob
(3) 
 
which represents a weighted average of all the 
quality numbers iq . We apply this formula to ob-
tain 
qual
as long as 
0.5.prob
 If 
0.5prob
we 
replace all numbers 
ip  by 1 ip  in (2) and (3) to 
obtain 
qual
.  
For this application we obtained the sets D and 
N from the SEMCAT data (Tanabe, Thom et al 
2006) supplemented with the latest UMLS data. 
We removed any term from D and N that contained 
less than five characters in order to decrease the 
occurrence of ambiguous terms.  Also the 1,000 
most frequent terms from D were examined ma-
nually and the ambiguous ones were removed. The 
end result is a set of 332,984 phrases in D and 
4,253,758 phrases in N. We trained the priority 
model on D and N and applied the resulting train-
ing to compute for each phrase in D and N a vector 
of values 
,prob qual
. In this way D and N are 
converted to 
DV  and NV . We then constructed a 
Mahalanobis classifier (Duda, Hart and Stork, 
2001) for two dimensional vectors as the differ-
ence in the Mahalanobis distance of any such vec-
tor to Gaussian approximations to 
DV  and NV .  We 
refer to the number produced by this classifier as 
the Mahalanobis score.  By randomly dividing both 
D and N into three equal size pieces and training 
on two from each and testing on the third, in a 
three-fold cross validation we found the Mahala-
nobis classifier to perform at 98.4% average preci-
sion and 93.9% precision-recall breakeven point. 
In a final step we applied a simple regression me-
thod to estimate the probability that a given Maha-
Data Lit. Corpus Query Corpus 
Training 276 sentences 
(487 disease con-
cepts, 185 unique) 
300 queries (89 
disease concepts, 
76 unique) 
Testing 275 sentences 
(437 disease con-
cepts, 185 unique) 
200 queries (53 
disease concepts, 
51 unique) 
All 551 sentences 
(924 disease con-
cepts, 280 unique) 
500 queries (142 
disease concepts, 
120 unique) 
147
lanobis score was produced by a phrase belonging 
to D and not N. Given a phrase phr we will denote 
this final probability produced as PMA(phr).  
The second important ingredient of our statistic-
al process is how we produce phrases from a piece 
of text. Given a string of text TX we apply tokeni-
zation to TX to produce an ordered set of tokens 
1 2, , , nt t t?
. Among the tokens produced will be 
punctuation marks and stop words and we denote 
the set of all such tokens by Z . We call a token 
segment 
, ,j kt t?
 maximal if it contains no ele-
ment of Z  and if either 1j  or 
1jt Z
 and 
likewise if k n  or 
1kt Z
. Given text TX we 
will denote the set of all maximal token segments 
produced in this way by 
max ( ).S TX
 Now given a 
maximal token segment mts=
, ,j kt t?
 we define 
two different methods of finding phrases in mts. 
The first assumes we are given an arbitrary set of 
phrases PH.  We recursively define a set of phrases 
,I mts PH
 beginning with this set empty and 
with the parameter 
u j
.  Each iteration consists 
of asking for the largest v k  for which 
, ,u vt t PH?
. If there is such a v  we add 
, ,u vt t?
 to 
,I mts PH
 and set 
1u v
. 
Otherwise we set 
1u u
. We repeat this process 
as long as u k .  The second approach assumes 
we are given an arbitrary set of two token phrases 
P2.  Again we recursively define a set of phrases 
, 2J mts P
 beginning with this set empty and 
with the parameter 
u j
. Each iteration consists 
of asking for the largest v k  for which given any 
,  i u i v
,  
1, 2i it t P
. If there is such a v  
we add 
, ,u vt t?
 to 
, 2J mts P
 and set 
1u v
. Otherwise we set 
1u u
. We repeat 
this process as long as u k .   
In order to apply our phrase extraction proce-
dures we need good sets of phrases. In addition to 
D and N already defined above, we use another set 
of phrases defined as follows. Let R denote the set 
of all token strings with two or more tokens which 
do not contain tokens from Z and for which there 
are at least three MEDLINE records (title and ab-
stract text only) in which the token string is re-
peated at least twice. 
We then define
R R D N
. We make 
use of R  in addition to D and N. For the set 2P  
we take the set of all two token phrases in 
MEDLINE documents for which the two tokens 
co-occur as this phrase much more than expected, 
i.e., with a 
2 10,000
(based on the two-by-two 
contingency table).  
 
 
#Initialization: Given a text TX, set 
maxS S TX
 and .X  
#Processing: While(S ){ 
  I. select mts S  
  II. If( ,I mts D ) ,K I mts D 
       else if( ,I mts R ) ,K I mts R  
        else if( ,I mts N ) K  
        else 
if( , 2J mts P ) , 2K J mts P 
        else K  
  III. X X K  
  IV. S S mts  
     } 
#Return: All pairs , ,  phr PMA phr phr X 
 
Figure 1: Phrase finding algorithm 
 
With these preliminaries, our phrase finding al-
gorithm in pseudo-code is shown in Figure 1. 
The output of this algorithm may then be filtered 
by setting a threshold on the PMA values to accept. 
5 Results  
5.1 Assessing the difficulty of the task 
To assess the difficulty of disease recognition, we 
computed the inter-annotator agreement (IAA) on 
the 300-query corpus. Agreement was computed at 
the disease mention level for all three annotators 
and at the disease concept level for the two annota-
tors who produced UMLS annotations.  
Inter-annotator agreement measures for NLP 
applications have been recently discussed by 
Artstein and Poesio (2008) who advocate for the 
use of chance corrected measures. However, in our 
case, agreement was partly computed on a very 
large set of categories (UMLS concepts) so we 
decided to use Knowtator?s built-in feature, which 
computes IAA as the percentage of agreement and 
148
allows partial string matches. For example, in the 
query ?dog model transient ischemic attacks?, an-
notator 1 selected ?ischemic attacks? as a disorder 
while annotator 2 and 3 selected ?transient ischem-
ic attacks? as UMLS concept C0007787: Attacks, 
Transient Ischemic. In this case, at the subclass 
level (?disorder?) we have a match for this annota-
tion. But at the exact span or exact category level, 
there is no match. Table 4 shows details of IAA at 
the disease mention level when partial matches are 
taken into account. For exact span matches, the 
IAA is lower, at 64.87% on average. 
 
Disorder IAA Ann. 1 Ann. 2 Ann. 3 
Ann. 1 100% 71.77% 75.86% 
Ann. 2  100% 71.68% 
Ann. 3   100% 
Table 4: Agreement on disease mention annotations 
(partial match allowed) ? average is 73.10% 
 
At the concept level, the agreement (when par-
tial matches were allowed) varied significantly 
depending on the semantic types. It ranged be-
tween 33% for Findings and 83% for Mental or 
Behavioral Dysfunction. However, agreement on 
the most frequent category, Disease or Syndrome, 
was 72%, which is close to the annotators? overall 
agreement at the mention level. One major cause 
of disagreement was ambiguity caused by concepts 
that were clustered by Jimeno et al For example, 
in query ?osteoporosis and ?fracture pattern?, an-
notator 2 marked ?osteoporosis? with both 
?C0029456-osteoporosis?(a Disease or Syndrome 
concept) and ?C1962963-osteoporosis adverse 
event?(a Finding concept) while annotator 3 only 
used ?C0029456-osteoporosis?.    
5.2 Results on Literature corpus 
As shown in Table 3, the corpus was randomly 
split into a training set (276 sentences) and a test 
set (275 sentences). The training set was used to 
determine the optimal probability threshold for the 
Priority Model and parameter selection for Meta-
Map, respectively. 
 
Priority Model parameter adjustments: the first 
result observed from applying the Priority Model 
was that D yielded about 90% of the output of the 
algorithm. Also results coming from R  and 2P  
were not well mapped to UMLS concepts by Me-
taMap. As a result, in this work we ignored disease 
candidates retrieved based on R  and 2P . The best 
F-measure was obtained for a threshold of 0.3, 
which was consequently used on the test set.  
Since the Priority Model algorithm does not per-
form any mapping to a controlled vocabulary 
source, the mapping was performed by applying 
MetaMap to the snippets of text returned with a 
probability value above the threshold. 
 
Threshold P R F 
0 64 73 67 
.1 67 73 70 
.2 67 73 70 
.3 68 73 71 
.4 68 73 70 
.5 68 72 69 
.6 68 72 69 
.7 68 72 69 
.8 68 68 68 
.9 65 60 62 
Table 5: Precision (P), Recall (R) and F-measure of the 
Priority Model on the training set for different values of 
the probability threshold. 
 
The results presented in Table 5 were obtained 
before any MetaMap adjustments were made.  
 
MetaMap parameter adjustments: an error anal-
ysis was performed to adjust MetaMap settings. 
Errors fell into the following categories:  
 A more specific disease should have been 
recognized (e.g. ?deficiency? vs. ?C2 defi-
ciency?) 
 The definition of a cluster was lacking 
(e.g. ?G6PD deficiency? comprised 
C0237987- Glucose-6-phosphate dehydro-
genase deficiency anemia and C0017758- 
Glucosphosphate Dehydrogenase Defi-
ciency but not C0017920- Deficiency of 
glucose-6-phosphatase)  
 MetaMap mapping was erroneous (e.g. 
?hereditary breast? was mapped to 
C0729233-Dissecting aneurysm of the 
thoracic aorta instead of C0346153-
Hereditary Breast Cancer)  
 
The results of inter-annotator agreement and fur-
ther study of MetaMap mappings indicated that 
concepts with the semantic type Findings seemed 
149
to be frequently retrieved erroneously. For this rea-
son, we also experimented not taking Findings into 
account as an additional adjustment for MetaMap. 
Table 6 shows the results of applying the MetaMap 
adjustments yielded from the error analysis on the 
training corpus. 
 
Threshold Findings P R F 
.3 Yes 80 78 79 
.3 No 85 78 81 
Table 6: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments?      
 
MetaMap disorder detection was also performed 
directly on the training corpus. An error analysis 
similar to what was presented above was carried 
out to determine the best parameters. Table 7 be-
low shows the results obtained when all concepts 
from the 12 Semantic Types (STs) in the disorder 
group are taken into account with no adjustments 
(?raw?). Then, results including the adjustments 
from the error analysis are shown when all 12 STs 
are taken into account, when Findings are excluded 
(11STs) and when only the most frequent 6STs in 
the training set are taken into account.    
 
Processing P R F 
Raw (12 STs) 50 77 61 
Adjusted (12 STs) 52 75 61 
Adjusted (11 STs) 57 73 64 
Adjusted (6 STs) 77 72 74 
Table 7: Performance of MetaMap on the training set      
 
Finally, Table 8 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 80 74 77 
MetaMap 75 78 76 
Table 8: Precision (P), Recall (R) and F-measure of the 
Priority Model and MetaMap on the test set     
5.3 Results on Query Corpus 
The 300-query corpus was used as a training set 
and the 200-query corpus was used as a test set. 
For consistency with work on the literature corpus, 
we assessed the disease recognition on a gold stan-
dard set including ?clusters? of UMLS concepts 
were appropriate. As previously with the Literature 
corpus, we used the training set to determine the 
best settings for each method. The performance of 
the Priority Model at different values of the proba-
bility threshold, based on the use of D and N as the 
sets of sample phrases is similar to that obtained 
with the literature corpus; 0.3 stands out as one of 
the three values for which the best F-measure is 
obtained (tied with .5 and .8).  
Because of the brevity of queries vs. sentences, 
the MetaMap error analysis was very succinct and 
resulted in:  
 Removal of C0011860-Diabetes mellitus 
type 2  as mapping for ?diabetes? 
 Removal of all occurrences of C0600688-
Toxicity and C0424653-Weight symptom 
(finding)  
 Adjustment on the number of STs taken in-
to account 
 
The difference in performance obtained on the 
training set for the different MetaMap adjustments 
considered is shown in Table 9 when MetaMap 
was applied to Priority Model output and in Table 
10 when it was applied directly on the queries.    
 
Threshold Findings P R F 
.3 Yes 60 72 65 
.3 No 73 70 71 
Table 9: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings 
to Findings are used in the ?adjustments? 
 
Processing P R F 
Raw (12 STs) 41 82 55 
Adjusted (12 STs) 44 82 57 
Adjusted (11 STs) 58 81 68 
Adjusted (6 STs) 64 75 69 
Table 10: performance of MetaMap on the training set 
 
Finally, Table 11 shows the performance of both 
methods on the test set, using the optimal settings 
determined on the training set:  
 
Method P R F 
Priority Model 76 72 74 
MetaMap 66 74 70 
Table 11: Precision (P), Recall (R) and F-measure of 
the Priority Model and MetaMap on the test set 
150
6 Discussion 
Comparing the Two Methods. The performance 
of both methods on the query corpus is comparable 
to inter-annotator agreement (F=70-74 vs. IAA=72 
on Disease and Syndromes). On both corpora, the 
Priority Model achieves higher precision and F-
measure, while MetaMap achieves better recall.  
Comparing the results obtained with MetaMap 
with those reported by Jimeno et al, precision is 
lower, but recall is much higher. This is likely to 
be due to the different MetaMap settings, and the 
use of different UMLS versions - Jimeno et al did 
not provide any of this information, but based on 
the publication date of their paper, it is likely that 
they used one of the 2006 UMLS releases. Meystre 
and Haug (2006) also found that significant per-
formance differences could be obtained with Me-
taMap by adjusting the content of the knowledge 
sources used.   
On both text genres, 0.3 was found to be the op-
timal probability threshold for the Priority Model. 
Based on the performance at different values of the 
threshold, it seems that the model is quite efficient 
at ruling out highly unlikely diseases. However, for 
values above .3 the performance does not vary 
greatly.  
 
Comparing Text Genres. For both methods, 
disease recognition seems more efficient on sen-
tences. This is to be expected: sentences provide 
more context (e.g. more tokens surrounding the 
disease mention are available) and allow for more 
efficient disambiguation, for example on acro-
nyms. Although acronyms are frequent both in 
queries and sentences, more undefined acronyms 
are found in queries. However, the difference in 
performance between the two methods seems 
higher on the query corpus. This indicates that the 
Priority Model could be more robust to sparse con-
text.  
It should be noted that there were diseases in all 
sentences in the literature corpus vs. about 1/3 to 
1/2 of the queries. In addition, the query corpus 
included many author names, which could create 
confusion with disease names (in particular for the 
Priority Model). This difficulty was not found in 
the sentence corpus. However, sentences some-
times contain negated mention of diseases, which 
never occurred in the query corpus where little to 
no syntax is used.  
We also noticed that while Findings seemed to 
be generally problematic concepts in both corpora, 
other concepts such as Injury and Poisoning were 
much more prevalent in the query corpus. For this 
reason, for the general task of disease recognition, 
a drastic restriction to as little as 6 STs is probably 
not advisable.  
 
Limitations of the study. One limitation of our 
study is the relatively small number of disease 
concepts in the query corpus. Although the query 
and sentence corpus contain about 500 que-
ries/sentences each, there are significantly less dis-
ease concepts found in queries compared to 
sentences. As a result, there is also less repetition 
in the disease concept found. This is partly due to 
the brevity of queries compared to sentences but 
mainly to the fact that while all the sentences in the 
literature corpus had at least one disease concept, 
this was not the case for the query corpus. We are 
currently addressing this issue with the ongoing 
development of a large scale query corpus anno-
tated for diseases and other relevant biomedical 
entities.  
7 Conclusions 
We found that of the two steps of disease recogni-
tion, disease mention gets the higher inter-
annotator agreement (vs. concept mapping). We 
have applied a statistical and an NLP method for 
the automatic recognition of disease concepts in 
two genres of biomedical text. While both methods 
show good performance (F=77% vs. F=76%) on 
the sentence corpus, results indicate that the statis-
tical model is more robust on the query corpus 
where very little disease context information is 
available (F=74% vs. F=70%). As a result, the 
priority model will be used for disease detection in 
PubMed queries in order to characterize users? 
search contexts for contextual IR. 
Acknowledgments 
This research was supported by the Intramural Re-
search Program of the NIH, National Library of 
Medicine. The authors would like to thank S. 
Shooshan and T. Tao for their contribution to the 
annotation of the query corpus; colleagues in the 
NCBI engineering branch for their valuable feed-
back at every step of the project.   
151
References  
Alan R. Aronson, Olivier Bodenreider, Dina Demner-
Fushman, Kin Wah Fung, Vivan E. Lee, James G. 
Mork et al 2007. From Indexing the Biomedical Li-
terature to Coding Clinical Text: Experience with 
MTI and Machine Learning Approaches. ACL 
Workshop BioNLP.  
Alan Aronson. 2001. Effective mapping of biomedical 
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proceedings of AMIA Symp:17-21. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder 
Agreement for Computational Linguistics. Compu-
tational Linguistics 34(4): 555-596 
Jing Bai, and Jian-Yun Nie. 2008. Adapting information 
retrieval to query contexts. Information Processing & 
Management. 44(6):1902-22 
Robert O. Duda, Peter. E. Hart and David G. Stork. 
2001. Pattern Classification. New York: John Wiley 
& Sons, Inc. 
R. Brian Haynes and Nancy L. Wilczynski. 2004. Op-
timal search strategies for retrieving scientifically 
strong studies of diagnosis from Medline: analytical 
survey. BMJ. 328(7447):1040. 
Jorge R. Herskovic, Len Y. Tanaka, William Hersh and 
Elmer V. Bernstam. 2007. A day in the life of 
PubMed: analysis of a typical day's query log. Jour-
nal of the American Medical Informatics Association. 
14(2):212-20. 
Antonio Jimeno, Ernesto Jimenez-Ruiz, Vivian Lee, 
Sylvain Gaudan, Rafael Berlanga and Dietrich 
Rebholz-Schuhmann. 2008. Assessment of disease 
named entity recognition on a corpus of annotated 
sentences. BMC Bioinformatics. 11;9 Suppl 3:S3. 
Yang Jin, Ryan T McDonald, Kevin Lerman, Mark A 
Mandel, Steven Carroll, Mark Y Liberman et al 
2006. Automated recognition of malignancy men-
tions in biomedical literature. BMC Bioinformatics.  
7:492. 
Alexa T. McCray, Anita Burgun and Olivier Bodenreid-
er. 2001. Aggregating UMLS semantic types for 
reducing conceptual complexity. Proceedings of 
Medinfo 10(Pt 1):216-20. 
St?phane Meystre and Peter J. Haug. 2006. Natural lan-
guage processing to extract medical problems from 
electronic clinical documents: performance evalua-
tion. J Biomed Inform. 39(6):589-99. 
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang, 
Aaron M. Cohen, Juliane Fluck, Patrick Ruch et al 
2008. Overview of BioCreative II gene normaliza-
tion. Genome Biol. 9 Suppl 2:S3. 
Phillip V. Ogren. 2006. Knowtator: A plug-in for creat-
ing training and evaluation data sets for Biomedical 
Natural Language systems. 9th Intl. Prot?g? Confe-
rence  
Jong C. Park and Jung-Jae Kim. 2006. Named Entity 
Recognition. In S. Ananiadou and J. McNaught 
(Eds), Text Mining for Biology and Biomedicine (pp. 
121-42). Boston|London:Artech House Inc.  
Wanda Pratt and Henry Wasserman. 2000. QueryCat: 
automatic categorization of MEDLINE queries. Pro-
ceedings of AMIA Symp:655-9.  
Tom C. Rindflesh and Marcelo Fiszman. 2003. The 
interaction of domain knowledge and linguistic struc-
ture in natural language processing: interpreting 
hypernymic propositions in biomedical text. J Bio-
med Inform. 36(6):462-77 
Daniel E. Rose and Danny Levinson. 2004. Understand-
ing user goals in web search. In Proceedings of the 
13th international Conference on World Wide 
Web:13-9  
Tefko Saracevic. 1997. The Stratified Model of Infor-
mation Retrieval Interaction: Extension and Applica-
tion. Proceedings of the 60th meeting of the. 
American Society for Information Science:313-27 
Xuehua Shen, Bin Tan and ChengXiang Zhai. 2005 
Context-sensitive information retrieval using impli-
cit feedback, In Proceedings of the 28th annual in-
ternational conference ACM SIGIR conference on 
Research and development in information retrieval: 
43-50.  
Larry Smith, Laurraine K. Tanabe, Rie J. Ando, Cheng-
Ju Kuo, I-Fang Chung , Chun-Nan Hsu et al 2008. 
Overview of BioCreative II gene mention recogni-
tion. Genome Biol. 9 Suppl 2:S2. 
Laurraine K. Tanabe, Lynn. H. Thom, Wayne Matten, 
Donald C. Comeau and W. John Wilbur. 2006. 
SemCat: semantically categorized entities for ge-
nomics. Proceedings of AMIA Symp: 754-8. 
Laurraine K. Tanabe and W. John Wilbur. 2006. A 
Priority Model for Named Entities. Proceedings of 
HLT-NAACL BioNLP Workshop:33-40 
Jaime Teevan, Susan T. Dumais and Eric Horvitz. 2005. 
Personalizing search via automated analysis of in-
terests and activities. In Proceeding of ACM-
SIGIR?05:449?56. 
Ji-Rong Wen, Ni Lao, Wei-Ying Ma. 2004. Probabilis-
tic model for contextual retrieval. Proceedings of 
ACM-SIGIR?04:57?63 
Xiangmin Zhang, Hermina G.B. Anghelescu and Xiao-
jun Yuan. 2005. Domain knowledge, search beha-
vior, and search effectiveness of engineering and 
science students: An exploratory study, Information 
Research 10(2): 217. 
152
Coling 2010: Poster Volume, pages 463?471,
Beijing, August 2010
Learning to Annotate Scientific Publications 
Minlie Huang 
State Key Laboratory of Intelligent 
Technology and Systems, 
Dept. Computer Science and Tech-
nology, Tsinghua University 
aihuang@tsinghua.edu.cn  
Zhiyong Lu 
National Center for Bio-
technology Information (NCBI), 
U.S. National Library of Medi-
cine, National Institutes of Health 
luzh@ncbi.nlm.nih.gov 
 
 
Abstract 
Annotating scientific publications with 
keywords and phrases is of great 
importance to searching, indexing, and 
cataloging such documents. Unlike 
previous studies that focused on user-
centric annotation, this paper presents our 
investigation of various annotation 
characteristics on service-centric anno-
tation. Using a large number of publicly 
available annotated scientific publica-
tions, we characterized and compared the 
two different types of annotation 
processes. Furthermore, we developed an 
automatic approach of annotating 
scientific publications based on a 
machine learning algorithm and a set of 
novel features. When compared to other 
methods, our approach shows significant-
ly improved performance. Experimental 
data sets and evaluation results are pub-
licly available at the supplementary web-
site1. 
1 Introduction 
With the rapid development of the Internet, the 
online document archive is increasing quickly 
with a growing speed. Such a large volume and 
the rapid growth pose great challenges for docu-
ment searching, indexing, and cataloging. To 
facilitate these processes, many concepts have 
been proposed, such as Semantic Web (Berners-
Lee et al, 2001), Ontologies (Gruber, 1993), 
Open Directory Projects like Dmoz2, folksono-
                                                 
1 http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/indexing 
2 http://www.dmoz.org/  
mies (Hotho et al, 2006), and social tagging sys-
tems like Flickr and CiteULike. Annotating doc-
uments or web-pages using Ontologies and Open 
Directories are often limited to a manually con-
trolled vocabulary (developed by service provid-
ers) and a small number of expert annotators, 
which we call service-centric annotation. By 
contrast, social tagging systems in which regis-
tered users can freely use arbitrary words to tag 
images, documents or web-pages, belong to us-
er-centric annotation. Although many advantag-
es have been reported in user-centric annotation, 
low-quality and undesired annotations are always 
observed due to uncontrolled user behaviors (Xu 
et al, 2006; Sigurbj?rnsson and Zwol, 2008). 
Moreover, the vocabulary involved in user-
centric annotation is arbitrary, unlimited, and 
rapid-growing in nature, causing more difficul-
ties in tag-based searching and browsing (Bao et 
al., 2007; Li et al, 2007). 
Service-centric annotation is of importance for 
managing online documents, particularly in serv-
ing high-quality repositories of scientific litera-
ture. For example, in biomedicine, Gene Ontolo-
gy (Ashburner et al, 2000) annotation has been 
for a decade an influential research topic of un-
ifying reliable biological knowledge from the 
vast amount of biomedical literature. Document 
annotation can also greatly help service providers 
such as ACM/IEEE portals to provide better user 
experience of search. Much work has been de-
voted to digital document annotation, such as 
ontology-based (Corcho, 2006) and semantic-
oriented (Eriksson, 2007). 
This paper focuses on service-centric annota-
tion. Our task is to assign an input document a 
list of entries. The entries are pre-defined by a 
controlled vocabulary. Due to the data availabili-
ty, we study the documents and vocabulary in the 
463
biomedical domain. We first analyze human an-
notation behaviors in two millions previously 
annotated documents. When compared to user-
centric annotation, we found that the two annota-
tion processes have major differences and that 
they also share some common grounds. Next, we 
propose to annotate new articles with a learning 
method based on the assumption that documents 
similar in content share similar annotations. To 
this end, we utilize a logistic regression algo-
rithm with a set of novel features. We evaluate 
our approach with extensive experiments and 
compare it to the state of the art. The contribu-
tions of this work are two-fold: First, we present 
an in-depth analysis on annotation behaviors be-
tween service-centric and user-centric annotation. 
Second, we develop an automatic method for 
annotating scientific publications with significant 
improvements over other systems. 
The remainder of the paper is organized as fol-
lows: We present several definitions in Section 2 
and the analysis of annotation behaviors in Sec-
tion 3. In Section 4, we presented the logistic 
regression algorithm for annotation. Benchmark-
ing results are shown in Section 5. We surveyed 
related work in Section 6 and summarized our 
work in Section 7. 
2 Definitions 
A controlled vocabulary: V, a set of pre-
specified entries for describing certain topics. 
Entries in the vocabulary are organized in a hie-
rarchical structure. This vocabulary can be mod-
ified under human supervision.  
Vocabulary Entry: an entry in a controlled 
vocabulary is defined as a triplet: VE = (MT, 
synonyms, NodeLabels). MT is a major term de-
scribing the entry, and NodeLabels are a list of 
node labels in the hierarchical tree. An entry is 
identified by its MT, and a MT may have mul-
tiple node labels as a MT may be mapped to sev-
eral nodes of a hierarchical tree.  
Entry Binary Relation: ISA(VEi, VEj) means 
entry VEj is a child of entry VEi, and SIB(VEi, 
VEj) meaning that VEj is a sibling of entry VEi. A 
set of relations determine the structure of a hie-
rarchy.  
Entry Depth: the depth of an entry relative to 
the root node in the hierarchy. The root node has 
a depth of 1 and the immediate children of a root 
node has a depth of 2, and so on. A major term 
may be mapped to several locations in the hie-
rarchy, thus we have minimal, maximal, and av-
erage depths for each MT.  
Given the above definitions, a controlled vo-
cabulary is defined as {<VEi, ISA(VEi,VEj), 
SIB(VEi,VEj)>|any i, j }. The annotation task is 
stated as follows: given a document D, predicting 
a list of entries VEs that are appropriate for anno-
tating the document. In our framework, we ap-
proach the task as a ranking problem, as detailed 
in Section 4. 
3 Analyzing Service-centric Annotation 
Behavior 
Analyzing annotation behaviors can greatly faci-
litate assessing annotation quality, reliability, and 
consistency. There has been some work on ana-
lyzing social tagging behaviors in user-centric 
annotation systems (Sigurbj?rnsson and Zwol, 
2008; Suchanek et al, 2008). However, to the 
best of our knowledge, there is no such analysis 
on service-centric annotation. In social tagging 
systems, no specific skills are required for partic-
ipating; thus users can tag the resources with ar-
bitrary words (the words may even be totally ir-
relevant to the content, such as ?todo?). By con-
trast, in service-centric annotation, the annotators 
must be trained, and they must comply with a set 
of strict guidelines to assure the consistent anno-
tation quality. Therefore, it is valuable to study 
the differences between the two annotation 
processes. 
3.1 PubMed Document Collection 
To investigate annotation behaviors, we down-
loaded 2 million documents from PubMed3, one 
of the largest search portals for biomedical ar-
ticles. These articles were published from Jan. 1, 
2000 to Dec. 31, 2008. All these documents have 
been manually annotated by National Library 
Medicine (NLM) human curators. The controlled 
vocabulary used in this system is the Medical 
Subject Headings (MeSH?)4, a thesaurus describ-
ing various biomedical topics such as diseases, 
chemicals and drugs, and organisms. There are 
25,588 entries in the vocabulary in 2010, and 
there are updates annually. By comparison, the 
vocabulary used in user-centric annotation is re-
                                                 
3 http://www.ncbi.nlm.nih.gov/pubmed/ 
4 http://www.nlm.nih.gov/mesh/ 
464
markably larger (usually more than 1 million tags) 
and more dynamic (may be updated every day). 
3.2 Annotation Characteristics 
First, we examine the distribution of the number 
of annotated entries in the document collection. 
For each number of annotated entries, we 
counted the number of documents with respect to 
different numbers of annotations. The number of 
annotations per document among these 2 million 
documents varies from 1 (with 176,383 docu-
ments) to 97 (with one document only). The av-
erage number of annotations per document is 
10.10, and the standard deviation is 5.95.  
 
Figure 1. The original distribution and simulated 
normal distribution. Each data point denotes the 
number of documents (y-axis) that has the cor-
responding number of entries (x-axis). 
 
As illustrated in Figure 1, when there are more 
than 4 annotations, the distribution fits a normal 
distribution. Comparing with user-centric annota-
tion, there are three notable observations: a), the 
maximal number of annotations per document 
(97) is much smaller (in social tagging systems 
the number amounts to over 104) due to much 
less annotators involved in service-centric anno-
tation than users in user-centric annotation; b), 
the number of annotations assigned to documents 
conforms to a normal distribution, which has not 
yet been reported in user-centric annotation; c), 
similar to user-centric annotation, the number of 
documents that have only one annotation ac-
counts for a large proportion.  
Second, we investigate whether the Zipf law 
(Zipf, 1949) holds in service-centric annotation. 
To this end, we ranked all the entries according 
to the frequency of being annotated to docu-
ments. We plotted the curve in logarithm scale, 
as illustrated in Figure 2. The curve can be simu-
lated by a linear function in logarithm scale if 
ignoring the tail which corresponds to very infre-
quently used entries. To further justify this find-
ing, we ranked all the documents according to 
the number of assigned annotations and plotted 
the curve in logarithm scale, as shown in Figure 
3. Similar phenomenon is observed. In conclu-
sion, the Zipf law also holds in service-centric 
annotation, just as reported in user-centric anno-
tation (Sigurbj?rnsson and Zwol, 2008).  
 
Figure 2. The distribution of annotated entry 
frequency. X-axis is the rank of entries (ranking 
by the annotation frequency), and y-axis is the 
frequency of an entry being used in annotation.  
 
Figure 3. The distribution of the number of an-
notated entries. X-axis is the rank of a document 
(in log10 scale), and y-axis is the number of anno-
tations assigned to documents (in log2 scale). 
 
Furthermore, as mentioned in Section 2, the 
vocabulary corresponds to a hierarchy tree once a 
set of binary relations were defined.  Thus we 
can easily obtain the minimal, maximal, and av-
erage depth of an entry. The larger depth an entry 
has, the more specific meaning it has. 
Therefore, we investigate whether service-
centric annotation is performed at very specific 
level (with larger depth) or general level (with 
smaller depth). We define prior depth and anno-
tation depth for this study, as follows: 
( )
PriorDepth                    (1)
| |VE V
Dep VE
V?
=?  
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
200000
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52
original
normal
Number of annotated entries 
Num
ber of docum
ents
1
100
10000
1000000
1 10 100 1000 10000 100000
1
2
4
8
16
32
64
128
1 10 100 1000 10000 100000
Rank of the document
Num
ber of annotations
465
AnnoDepth Pr( )* ( )     (2)
VE V
VE Dep VE?=?  
( )
Pr( )                               (3)
( )
VE V
f VE
VE
f VE?
=?  
where Dep(VE) is the minimal, maximal, or av-
erage depth of an entry, f(VE) is the usage fre-
quency of VE in annotation, and |V| is the num-
ber of entries in the vocabulary. The two formu-
las are actually the mathematical expectations of 
the hierarchy?s depth under two distributions re-
spectively: a uniform distribution (1/|V|) and the 
annotation distribution (formula (3)). As shown 
in Table 1, the two expectations are close. This 
means the annotation has not been biased to ei-
ther general or specific level, which suggests that 
the annotation quality is sound.  
Dep(VE) PriorDepth AnnoDepth 
MAX 4.88 4.56 
MIN 4.25 4.02 
AVG 4.56 4.29 
Table 1. Annotation depth comparison. 
 
Figure 4. The imbalance frequency (y-axis) of 
annotated categories (x-axis). 
3.3 Annotation Categorization Imbalance 
We investigate here whether service-centric an-
notation is biased to particular categories in the 
hierarchy. We define a category as the label of 
root nodes in the hierarchy. In our vocabulary, 
there are 11 categories that have at least one an-
notation. The complete list of these categories is 
available at the website5 . Three newly created 
categories have no annotations in the document 
collection. The total number of annotations with-
in a category was divided by the number of en-
                                                 
5 http://www.nlm.nih.gov/mesh/2010/mesh_browser/MeSHtree.Z.html 
tries in that category, as different categories may 
have quite different numbers of entries. If an en-
try is mapped to multiple locations, its annota-
tions will be counted to corresponding categories 
repeatedly.  
From Figure 4, we can see that there is imbal-
ance with respect to the annotations in different 
categories. Category ?diseases? has 473.5 anno-
tations per entry (totally 4408 entries in this cat-
egory). Category ?chemicals and drugs? has 
423.0 annotations per entry (with 8815 entries in 
total). Due to the fact that diseases and chemicals 
and drugs are hot scientific topics, these catego-
ries are largely under-annotated. The most fre-
quently annotated category is: ?named groups? 
(7144.4 annotations per entry), with 199 entries 
in total. The issue of imbalanced categorization 
may be due to that the topics of the document 
collection are of imbalance; and that the vocabu-
lary was updated annually, so that the latest en-
tries were used less frequently. As shown in (Si-
gurbj?rnsson and Zwol, 2008), this imbalance 
issue was also observed in user-centric annota-
tion, such as in Flickr Tagging. 
4 Learning to Annotate 
As shown in Section 3, there are much fewer an-
notations per document in service-centric annota-
tion than in user-centric annotations. Service-
centric annotation is of high quality, and is li-
mited to a controlled vocabulary. However, ma-
nual annotation is time-consuming and labor in-
tensive, particularly when seeking high quality. 
Indeed, our analysis shows that on average it 
takes over 90 days for a PubMed citation to be 
manually annotated with MeSH terms. Thus we 
propose to annotate articles automatically. Spe-
cifically, we approach this task as a ranking 
problem: First, we retrieve k-nearest neighboring 
(KNN) documents for an input document using a 
retrieval model (Lin and Wilbur, 2007). Second, 
we obtain an initial list of annotated entries from 
those retrieved neighboring documents. Third, 
we rank those entries using a logistic regression 
model. Finally, the top N ranked entries are sug-
gested as the annotations for the target document.  
4.1 Logistic Regression 
We propose a probabilistic framework of directly 
estimating the probability that an entry can be 
used to annotate a document. Given a document 
0
1000
2000
3000
4000
5000
6000
7000
8000
An
at
om
y 
[A
]
Or
ga
ni
sm
s [
B]
Di
se
as
es
 [C
]
Ch
em
ica
ls 
an
d 
Dr
ug
s [
D]
An
al
yt
ica
l, 
Di
ag
no
st
ic 
an
d ?
Ps
yc
hi
at
ry
 a
nd
 P
sy
ch
ol
og
y 
[F
]
Ph
en
om
en
a 
an
d 
Pr
oc
es
se
s ?
Di
sc
ip
lin
es
 a
nd
 ?
An
th
ro
po
lo
gy
, E
du
ca
tio
n,
 ?
Te
ch
no
lo
gy
, I
nd
us
try
, ?
In
fo
rm
at
io
n 
Sc
ie
nc
e 
[L
]
Na
m
ed
 G
ro
up
s [
M
]
He
al
th
 C
ar
e 
[N
]
466
D and an entry VE, we compute the probability 
Pr(R(VE)|D) directly using a logistic regression 
algorithm. R(VE) is a binary random variable 
indicating whether VE should be assigned as an 
annotation of the document. According to this 
probability, we can rank the entries obtained 
from neighboring documents. Much work used 
Logistic Regression as classification: Pr(R=1|D) 
>? where ? is a threshold, but it is difficult to 
specify an appropriate value for the threshold in 
this work, as detailed in Section 5.5. 
We applied the logistic regression model to 
this task. Logistic regression has been successful-
ly employed in many applications including mul-
tiple ranking list merging (Si and Callan, 2005) 
and answer validation for question answering 
(Ko et al, 2007). The model gives the following 
probability: 
1 1
Pr( ( ) | ) exp( * ) 1 exp( * )       (4)
m m
i i i i
i i
R VE D b w x b w x
= =
? ?= + + +? ?? ?? ?
where x= (x1, x2, ?, xm) is the feature vector for 
VE and m is the number of features.  
For an input document D, we can obtain an in-
itial list of entries {VE1,VE2,?,VEn} from its 
neighboring documents. Each entry is then 
represented as a feature vector as x= (x1, x2, ?, 
xm). Given a collection of N documents that have 
been annotated manually, each document will 
have a corresponding entry list, {VE1, 
VE2,?,VEn}, and each VEi has gold-standard la-
bel yi=1 if VEi was used to annotate D, or yi=0 
otherwise. Note that the number of entries of la-
bel 0 is much larger than that of label 1 for each 
document. This may bias the learning algorithm. 
We will discuss this in Section 5.5. Given such 
data, the parameters can be estimated using the 
following formula:  
( )* *
, 1 1
, argmax log Pr( ( ) | )          (5)
jLN
i j
w b j i
w b R VE D
= =
= ??


 
where Lj is the number of entries to be ranked for 
Dj, and N is the total number of training docu-
ments. We can use the Quasi-Newton algorithm 
for parameter estimation (Minka, 2003). In this 
paper, we used the WEKA6 package to imple-
ment this model. 
4.2 Features 
We developed various novel features to build 
connections between an entry and the document 
                                                 
6http://www.cs.waikato.ac.nz/ml/weka/ .  
text. When computing these features, both the 
entry?s text (major terms, synonyms) and the 
document text (title and abstract) are tokenized 
and stemmed. To compute these features, we 
collected a set of 13,999 documents (each has 
title, abstract, and annotations) from PubMed. 
Prior probability feature. We compute the 
appearance probability of a major term (MT), 
estimated on the 2 million documents. This prior 
probability reflects the prior quality of an entry. 
Unigram overlap with the title. We count the 
number of unigrams overlapping between the MT 
of an entry and the title, dividing by the total 
number of unigrams in the MT. 
Bigram overlap with the document. We first 
concatenate the title and abstract, then count the 
number of bigram overlaps between the MT and 
the concatenated string, dividing by the total 
number of bigrams in the MT. 
Multinomial distribution feature. This fea-
ture assumes that the words in a major term ap-
pear in the document text with a multinomial 
distribution, as follows:  
#( , )Pr( | )
Pr( | ) | | !*     (6)
#( , )!
w MT
w MT
w Text
MT Text MT
w MT?
= ?
 
#( , )
Pr( | ) (1 ) Pr ( )   (7)
#( , )
i
c
iw
w Text
w Text w
w Text
? ?= ? +?  
where: 
#(w,MT) - The number of times that w appears in 
MT; Similarly for #(w,Text); 
|MT| - The number of single words in MT; 
Text - Either the title or abstract, thus we have 
two features of this type: Pr(MT|Title) and 
Pr(MT|Abstract); 
Prc(w) - The probability of word w occurring in a 
background corpus. This is obtained from a uni-
gram language model that was estimated on the 
13,999 articles; 
? ? A smoothing parameter that was empirically 
set to be 0.2. 
Query-likelihood features. The major term of 
an entry is viewed as a query, and this class of 
features computes likelihood scores between the 
query (as Q) and the article D (either the title or 
the abstract). We used the very classic okapi 
model (Robertson et al 1994), as follows: 
( ) 0.5
( , )*log
( ) 0.5
( , )   (8)
| |
0.5 1.5* ( , )
(| |)
q Q
N df q
tf q D
df q
Okapi Q D
D
tf q D
avg D
?
? ?? +? ?+? ?= ? ?+ +? ?? ?
?  
467
where:  
tf(q,D) - The count of q occurring in document D;  
|D| - The total word counts in document D;  
df(q) - The number of documents containing 
word q;  
avg(|D|) - The average length of documents in 
the collection;  
N - The total number of documents (13,999).  
We have two features: okapi(MT, Title) and 
okapi(MT, Abstract). In other words, the title and 
abstract are processed separately. The advantage 
of using such query-likelihood scores is that they 
give a probability other than a binary judgment 
of whether a major term should be annotated to 
the article, as only indirect evidence exists for 
annotating a vocabulary entry to an article in 
most cases. 
Neighborhood features. The first feature 
represents the number of neighboring documents 
that include the entry to be annotated for a doc-
ument. The second feature, instead of counting 
documents, sums document similarity scores. 
The two features are formulated as follows, re-
spectively: 
{ }( | ) | ,     (9)i i i kfreq MT D D MT D D= ? ??  
; 
( | ) ( , )          (10)
i i k
i
MT D D
sim MT D sim D D
? ??
= ?  
where ?k is the k-nearest neighbors for an input 
document D and sim(Di,Dj) is the similarity score 
between a target document and its neighboring 
document, given by the retrieval model.  
Synonym Features. Each vocabulary entry 
has synonyms. We designed two binary features: 
one judges whether there exists a synonym that 
can be exactly matched to the article text (title 
and abstract); and the other measures whether 
there exists a synonym whose unigram words 
have all been observed in the article text. 
5 Experiment 
5.1 Datasets 
To justify the effectiveness of our method, we 
collected two datasets. We randomly selected a 
set of 200 documents from PubMed to train the 
logistic regression model (named Small200). For 
testing, we used a benchmark dataset, NLM2007, 
which has been previously used in benchmarking 
biomedical document annotation7 (Aronson et al, 
                                                 
7http://ii.nlm.nih.gov/.  
2004; Vasuki and Cohen, 2009; Trieschnigg et 
al., 2009). The two datasets have no overlap with 
the aforementioned 13,999 documents. Each 
document in these two sets has only title and ab-
stract (i.e., no full text). The statistics listed in 
Table 2 show that the two datasets are alike in 
terms of annotations. Note that we also evaluate 
our method on a larger dataset of 1000 docu-
ments, but due to the length limit, the results are 
not presented in this paper. 
Dataset Documents 
Total   
annotations 
Average 
annotations 
Small200 200 2,736 13.7 
NLM2007 200 2,737 13.7 
Table 2. Statistics of the two datasets.  
5.2 Evaluation Metrics 
We use precision, recall, F-score, and mean av-
erage precision (MAP) to evaluate the ranking 
results. As can be seen from Section 3.2, the 
number of annotations per document is about 10. 
Thus we evaluated the performance with top 10 
and top 15 items. 
5.3 Comparison to Other Approaches 
We compare our approach to three methods on 
the benchmark dataset - NLM2007. The first sys-
tem is NLM?s MTI system (Aronson et al, 2004). 
This is a knowledge-rich method that employs 
NLP techniques, biomedical thesauruses, and a 
KNN module. It also utilizes handcrafted filtering 
rules for refinement. The second and third me-
thods rank entries according to Formula (9) and 
(10), respectively (Trieschnigg et al, 2009).  
We trained our model on Small200. All fea-
ture values were normalized to [0,1] using the 
maximum values of each feature. The number of 
neighbors was set to be 20. Neighboring docu-
ments were retrieved from PubMed using the 
retrieval model described in (Lin and Wilbur, 
2007). Existing document annotations were not 
used in retrieving similar documents as they 
should be treated as unavailable for new docu-
ments. As the average number of annotations per 
document is around 13 (see Table 2), we com-
puted precision, recall, F-score, and MAP with 
top 10 and 15 entries, respectively.  
Results in Table 3 demonstrate that our me-
thod outperforms all other methods. It has sub-
stantial improvements over MTI. To justify 
whether the improvement over using neighbor-
468
hood similarity is significant, we conducted the 
Paired t-test (Goulden, 1956). When comparing 
results of using learning vs. neighborhood simi-
larity in Table 3, the p-value is 0.028 for top 10 
and 0.001 for top 15 items. This shows that the 
improvement achieved by our approach is statis-
tically significant (at significance level of 0.05).  
 Methods Pre. Rec. F. MAP 
Top 
10 
MTI .468 .355 .404 .400 
Frequency .635 .464 .536 .598 
Similarity .643 .469 .542 .604 
Learning .657 .480 .555 .622 
Top 
15 
MTI .404 .442 .422 .400 
Frequency .512 .562 .536 .598 
Similarity .524 .574 .548 .604 
Learning .539 .591 .563 .622 
Table 3. Comparative results on NLM2007. 
5.4 Choosing Parameter k 
We demonstrate here our search for the optimal 
number of neighboring documents in this task. 
As shown in Table 4, the more neighbors, the 
larger number of gold-standard annotations 
would be present in neighboring documents. 
With 20 neighbors a fairly high upper-bound re-
call (UBR) is observed (about 85% of gold-
standard annotations of a target document were 
present in its 20 neighbors? annotations), and the 
average number of entries (Avg_VE) to be ranked 
is about 100.  
 
Figure 5. The performance (y-axis) varies with 
the number of neighbors (x-axis). 
Measure 
The number of neighboring documents 
5  10   15  20  25 30 
UBR .704 .793 .832 .856 .871 .882 
Avg_VE 38.8 64.1 83.6 102.2 119.7 136.4 
Table 4. The upper-bound recall (UBR) and av-
erage number of entries (Avg_VE) with different 
number of neighboring documents.  
 
To investigate whether the number of neigh-
boring documents affects performance, we expe-
rimented with different numbers of neighboring 
documents. We trained a model on Small200, 
and tested it on NLM2007. The curves in Figure 
5 show that the performance becomes very close 
when choosing no less than 10 neighbors. This 
infers that reliable performance can be obtained. 
The best performance (F-score of 0.563) is ob-
tained with 20 neighbors. Thus, the parameter k  
is set to be 20. 
5.5 Data Imbalance Issue 
As mentioned in Section 4.1, there is a data im-
balance issue in our task. For each document, we 
obtained an initial list of entries from 20 neigh-
boring documents. The average number of gold-
standard annotations is about 13, while the aver-
age number of entries to be ranked is around 100 
(see Table 4). Thus the number of entries of label 
0 (negative examples) is much larger than that of 
label 1 (positive examples). We did not apply 
any filtering strategy because the gold-standard 
annotations are not proportional to their occur-
ring frequency in the neighboring documents. In 
fact, as shown in Figure 6, the majority of gold-
standard annotations appear in only few docu-
ments among 20 neighbors. For example, there 
are about 250 gold-standard annotations appear-
ing in only one of 20 neighboring documents and 
964 appearing in less than 6 neighboring docu-
ments. Therefore, applying any filtering strategy 
based on their occurrence in neighboring docu-
ments may be harmful to the performance. 
 
Figure 6. The distribution of annotations. X-axis 
is the number of neighboring documents in 
which gold-standard annotations are found. 
5.6 Feature Analysis 
To investigate the impact of different groups of 
features, we performed a feature ablation study. 
The features were divided into four groups. For 
each round of this study, we remove one group 
of features from the entire feature set, re-train the 
model on Small200, and then test the perfor-
mance on NLM2007 with top 15 entries. We di-
vided the features into four independent groups: 
0.45
0.5
0.55
0.6
0.65
5 10 15 20 25 30
MAP
Recall
F-score
Precision
0
50
100
150
200
250
300
1 2 3 4 5 6 7 8 9 1011121314151617181920
G
oldstandard annotations
469
prior probability features, neighborhood features, 
synonym features, and other features (including 
unigram/bigram feature, query likelihood feature, 
etc., see Section 4.2). Results in Table 5 show 
that neighborhood features are dominant: remov-
ing such features leads to a remarkable decrease 
in performance. On the other hand, using only 
neighborhood features (the last row) yields sig-
nificant worse results than using all features. 
This means that combining all features together 
indeed contributes to the optimal performance. 
Feature Set Pre. Rec. F. MAP 
All features .539 .591 .563 .622 
- Prior probability .538 .590 .563  .622 
- Neighborhood features .419* .459* .438*  .467* 
- Synonym features .532 .583 .556  .611 
- Other features .529 .580 .553  .621 
Only neighborhood features .523* .573* .547* .603* 
Table 5. Feature analysis. Those marked by stars 
are significantly worse than the best results. 
5.7 Discussions 
All methods that rely on neighboring documents 
have performance ceilings. Specifically, for the 
NLM2007 dataset, the upper bound recall is 
around 85.6% with 20 neighboring documents, 
as shown in Table 5. Due to the same reason, this 
genre of methods is also limited to recommend 
entries that are recently added to the controlled 
vocabulary as such entries may have not been 
annotated to any document yet. This phenome-
non has been demonstrated in the annotation be-
havior analysis: those latest entries have substan-
tially fewer annotations than older ones.  
6 Related Work 
Our work is closely related to ontology-based or 
semantic-oriented document annotation (Corcho, 
2006; Eriksson, 2007). This work is also related 
to KNN-based tag suggestion or recommendation 
systems (Mishne, 2006). 
The task here is similar to keyword extraction 
(Nguyen and Kan, 2007; Jiang et al, 2009), but 
there is a major difference: keywords are always 
occurring in the document, while when an entry 
of a controlled vocabulary was annotated to a 
document, it may not appear in text at all.  
As for the task tackled in this paper, i.e., anno-
tating biomedical publications, three genres of 
approaches have been proposed: (1) k-Nearest 
Neighbor model: selecting annotations from 
neighboring documents, ranking and filtering 
those annotations (Vasuki and Cohen, 2009; Tri-
eschnigg et al, 2009). (2) Classification model: 
learning the association between the document 
text and an entry (Ruch, 2006). (3) Based on 
knowledge resources: using domain thesauruses 
and NLP techniques to match an entry with con-
cepts in the document text (Aronson, 2001; 
Aronson et al, 2004). (4) LDA-based topic mod-
el: (M?rchen et al, 2008). 
7 Conclusion 
This paper presents a novel study on service-
centric annotation. Based on the analysis results 
of 2 million annotated scientific publications, we 
conclude that service-centric annotation exhibits 
the following unique characteristics: a) the num-
ber of annotation per document is significant 
smaller, but it conforms to a normal distribution; 
and b) entries of different granularity (general vs. 
specific) are used appropriately by the trained 
annotators. Service-centric and user-centric an-
notations have in common that the Zipf law 
holds and categorization imbalance exists. 
Based on these observations, we introduced a 
logistic regression approach to annotate publica-
tions, with novel features. Significant improve-
ments over other systems were obtained on a 
benchmark dataset. Although our features are 
tailored for this task in biomedicine, this ap-
proach may be generalized for similar tasks in 
other domains. 
Acknowledgements 
This work was supported by the Intramural Re-
search Program of the NIH, National Library of 
Medicine. The first author was also supported by 
the Chinese Natural Science Foundation under 
grant No. 60803075 and the grant from the Inter-
national Development Research Center, Ottawa, 
Canada IRCI.  
References 
Alan R. Aronson. Effective mapping of biomedical 
text to the UMLS Metathesaurus: the metamap 
program. In Proc AMIA Symp 2001. p. 17-21. 
Alan Aronson, Alan R. Aronson, James Mork, James 
G. Mork, Clifford Gay, Clifford W. Gay, Susanne 
Humphrey, Susanne M. Humphrey, Willie Rogers, 
Willie J. Rogers. The NLM Indexing Initiative's 
470
Medical Text Indexer. Stud Health Technol In-
form. 2004;107(Pt 1):268-72. 
Michael Ashburner, Catherine A. Ball, Judith A. 
Blake, David Botstein, Heather Butler, et al Gene 
Ontology: tool for the unification of biology. Nat 
Genet. 2000 May; 25(1):25-9. 
Shenghua Bao, Xiaoyuan Wu, Ben Fei, Guirong Xue, 
Zhong Su, and Yong Yu. Optimizing Web Search 
Using Social Annotations. WWW 2007, May 8?12, 
2007, Banff, Alberta, Canada. Pp 501-510. 
Tim Berners-Lee, James Hendler and Ora Lassila. 
The Semantic Web. Scientific American Magazine. 
(May 17,  2001). 
Oscar Corcho. Ontology based document annotation: 
trends and open research problems. International 
Journal of Metadata, Semantics and Ontologies, 
Volume 1,  Issue 1, Pages: 47-57, 2006. 
Henrik Eriksson. An Annotation Tool for Semantic 
Documents. In Proceedings of the 4th European 
conference on The Semantic Web: Research and 
Applications, pages 759-768, 2007. Innsbruck, 
Austria. 
Cyril Harold Goulden. Methods of Statistical Analy-
sis, 2nd ed. New York: Wiley, pp. 50-55, 1956. 
Thomas R. Gruber (1993). A Translation Approach to 
Portable Ontology Specifications. Knowledge Ac-
quisition, 5(2), 1993, pp. 199-220. 
Andreas Hotho, Robert Jaschke, Christoph Schmitz, 
Gerd Stumme. Information Retrieval in Folksono-
mies: Search and Ranking. In ?The Semantic Web: 
Research and Applications?, Vol. 4011 (2006), pp. 
411-426. 
Xin Jiang, Yunhua Hu, Hang Li. A Ranking Ap-
proach to Keyphrase Extraction. SIGIR?09, July 
19?23, 2009, Boston, Massachusetts, USA. 
Jeongwoo Ko, Luo Si, Eric Nyberg. A Probabilistic 
Framework for Answer Selection in Question 
Answering. Proceedings of NAACL HLT 2007, 
pages 524?531, Rochester, NY, April 2007. 
Rui Li, Shenghua Bao, Ben Fei, Zhong Su, and Yong 
Yu. Towards Effective Browsing of Large Scale 
Social Annotations. In WWW ?07: Proceedings of 
the 16th international conference on World Wide 
Web, 2007.  
Jimmy Lin and W. John Wilbur. PubMed related ar-
ticles: a probabilistic topic-based model for content 
similarity. BMC Bioinformatics 8: (2007). 
Thomas P. Minka. A Comparison of Numerical Op-
timizers for Logistic Regression. 2003. Unpub-
lished draft. 
Gilad Mishne. AutoTag: A Collaborative Approach to 
Automated Tag Assignment for Weblog Posts. 
WWW 2006, May 22?26, 2006, Edinburgh, Scot-
land. pages 953?954. 
Fabian M?rchen, Math?us Dejori, Dmitriy Fradkin, 
Julien Etienne, Bernd Wachmann, Markus Bund-
schus. Anticipating annotations and emerging 
trends in biomedical literature. In KDD '08: pp. 
954-962. 
Thuy Dung Nguyen and Min-Yen Kan. Keyphrase 
Extraction in Scientific Publications. In Proc. of In-
ternational Conference on Asian Digital Libraries 
(ICADL ?07), pages 317-326. 
Stephen E. Robertson, Steve Walker, Susan Jones, 
Micheline Hancock-Beaulieu, and Mike Gatford. 
Okapi at TREC-3. In Proceedings of the Third Text 
REtrieval Conference (TREC 1994). Gaithersburg, 
USA, November 1994. 
Patrick Ruch. Automatic assignment of biomedical 
categories: toward a generic approach. Bioinfor-
matics. 2006 Mar 15;22(6):658-64. 
Luo Si and Jamie Callan. 2005 CLEF2005: Multilin-
gual retrieval by combining multiple multilingual 
ranked lists. In Proceedings of Cross-Language 
Evaluation Forum. 
B?rkur Sigurbj?rnsson and Roelof van Zwol. Flickr 
Tag Recommendation based on Collective Know-
ledge. WWW 2008, April 21?25, 2008, Beijing, 
China. Pp. 327-336. 
Fabian M. Suchanek, Milan Vojnovi?c, Dinan Guna-
wardena. Social Tags: Meaning and Suggestions. 
CIKM?08, October 26?30, 2008, Napa Valley, Cal-
ifornia, USA.   
Dolf Trieschnigg, Piotr Pezik, Vivian Lee, Franciska 
de Jong, Wessel Kraaij, Dietrich Rebholz-
Schuhmann. MeSH Up: effective MeSH text clas-
sification for improved document retrieval. Bioin-
formatics, Vol. 25 no. 11 2009, pages 1412?1418. 
 Vidya Vasuki and Trevor Cohen. Reflective Random 
Indexing for Semiautomatic Indexing of the Bio-
medical Literature. AMIA 2009, San Francisco, 
Nov. 14-18, 2009. 
Zhichen Xu, Yun Fu, Jianchang Mao, and Difu Su. 
Towards the Semantic Web: Collaborative Tag 
Suggestions. In WWW2006: Proceedings of the 
Collaborative Web Tagging Workshop (2006). 
George K. Zipf. (1949) Human Behavior and the 
Principle of Least-Effort. Addison-Wesley. 
471
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 103?104,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Automatic extraction of data deposition sentences:  
where do the research results go? 
Aur?lie N?v?ol, W. John Wilbur, Zhiyong Lu 
National Center for Biotechnology Information 
U.S. National Library of Medicine 
Bethesda, MD 20894, USA 
{Aurelie.Neveol,John.Wilbur,zhiyong.lu}@nih.gov 
  
Abstract 
Research in the biomedical domain can have a 
major impact through open sharing of data 
produced. In this study, we use machine learn-
ing for the automatic identification of data 
deposition sentences in research articles. Arti-
cles containing deposition sentences are cor-
rectly identified with 73% f-measure. These 
results show the potential impact of our meth-
od for literature curation.  
1 Background 
Research in the biomedical domain aims at further-
ing the knowledge of biological processes and im-
proving human health. Major contributions 
towards this goal can be achieved by sharing the 
results of research efforts with the community, in-
cluding datasets produced in the course of the re-
search work. While such sharing behavior is 
encouraged by funding agencies and scientific 
journals, recent work has shown that the ratio of 
data sharing is still modest compared to actual data 
production. For instance, Ochsner et al (2008) 
found the deposition rate of microarray data to be 
less than 50% for work published in 2007.  
Information about the declaration of data depo-
sition in research papers can be used both for data 
curation and for the analysis of emerging research 
trends. Our long-term research interest is in as-
sessing the value of deposition sentences for pre-
dicting future trends of data production. The initial 
step of automatically identifying deposition sen-
tences would then lead to an assessment of the 
need for storage space of incoming data in public 
repositories. 
2 Objective 
In this study, we aim at automatically perform-
ing a fine-grained identification of biological data 
deposition sentences in biomedical text. That is, 
we aim at identifying articles containing deposition 
sentences, extracting the specific sentences and 
characterizing the information contained in the 
sentences in terms of data type and deposition lo-
cation (e.g. database, accession numbers).  
3 Material and Methods 
Data deposition sentences . A collection of sen-
tences reporting the deposition of biological data 
(such as microarray data, protein structure, gene 
sequences) in public repositories was compiled 
based on previous work that we extended. We take 
these sentences as a primary method of identifying 
articles reporting on research that produced the 
kind of data deposited in public repositories. (1) 
and (2) show examples of such sentences. In con-
trast, (3) and (4) contain elements related to data 
deposition while focusing on other topics.   
(1) The sequences reported in this paper have been 
deposited in the GenBank database (acces sion 
numbers AF034483 for susceptible strain RC688s 
and AF034484 for resistant strain HD198r). 
(2) The microarray data were submitted to MIAMEx-
press at the EMBL-EBI. 
(3) Histone TAG Arrays are a repurposing of a micro-
array design originally created to represent the 
TAG sequences in the Yeast Knockout collection 
(Yuan et al2005 NCBI GEO Accession Number 
GPL1444). 
(4) The primary sequence of native Acinetobacter 
CMO is identical to the gene sequence for chnB 
deposited under accession number AB006902. 
103
Sentence classification. A Support Vector Ma-
chine (SVM) classifier was built using a corpus of 
583 positive data deposition sentences and 578 
other negative sentences. Several sets of features 
were tested, including the following: sentence to-
kens, associated part-of-speech tags obtained using 
MEDPOST1, relative position of the sentence in 
the article, identification of elements related to data 
deposition (data, deposition action, database, ac-
cession number) obtained using a CRF model2.   
Article classification. The automatic classification 
of articles relied on sentence analysis. The full text 
of articles was segmented into sentences, which 
were then scored by the sentence-level SVM clas-
sifier described above. An article is classified as 
positive if its top-scored sentence is scored higher 
than a threshold, which is predetermined as the 25th 
percentile score for positive sentences in the train-
ing set.  
Evaluation corpus . A corpus composed of 670 
PubMed Central articles was used to evaluate arti-
cle classification. 200 articles were considered as 
?positive? for data deposition based on MEDLINE 
gold standard annotations in the [si] field used to 
curate newly reported accession numbers.  
4 Results  
Table 1 shows the performance of selected SVM 
models for article classification on the test set. 
While differences were very small for cross-
validation on the training set, they are emphasized 
on the test set.   
 
Features P         R           F 
Tokens, position, part-of-
speech tags 
52%      56%     54% 
Token, position, CRF+, 
part-of-speech tags  
65%      58%     62% 
Tokens, position, CRF+/-, 
part-of-speech tags 
69%     78%     73% 
Table 1: Precision, Recall and F-measure of SVM 
models for article classification on test set. 
5 Discussion and Conclusion 
Portability of the method. Although trained 
mainly on microarray data deposition sentences, 
the method adapts well to the identification of oth-
                                                                 
1 http://www.ncbi.nlm.nih.gov/staff/lsmith/MedPost.html 
2 http://mallet.cs.umass.edu/ 
er data deposition sentences, e.g. gene sequences, 
protein coordinates.  
Comparison to other work. Our approach is not 
directly comparable to any of the previous studies. 
At the article level, we perform an automatic clas-
sification of articles containing data deposition 
sentences, in contrast with Oshner et al who per-
formed a one-time manual classification. Piwowar 
et alused machine learning and rule-based algo-
rithms for article classification. However, they re-
lied on identifying the names of five predetermined 
databases in the full text of articles. Our approach 
is generic and aiming at the automatic identifica-
tion of any biological data deposition in any public 
repository. Furthermore, our approach also re-
trieves specific data deposition sentences where 
data and deposition location are identified. At the 
sentence level, this is also different from the classi-
fication of databank accession number sentences 
performed by Kim et al (2010) in two ways: first, 
we focus on retrieving sentences containing acces-
sion numbers if they are deposition sentences (vs. 
data re-use, etc.) and second, we are also interested 
in retrieving data deposition sentences that do not 
contain accession numbers.  
Error analysis . Almost half of the articles clas-
sified as containing a deposition sentence by our 
method but not by the gold standard were found to 
indeed contain a deposition sentence.  
Conclusion. These results show the potential 
impact of our method for literature curation. In 
addition, it provides a robust tool for future work 
assessing the need for storage space of incoming 
data in public repositories. 
Acknowledgments 
This research was supported by the Intramural Re-
search Program of the NIH, NLM.  
References  
Jongwoo Kim, Daniel Le, Georges R. Thoma. Na?ve 
bayes and SVM classifiers for classifying databank 
accession number sentences from online biomedical 
articles. Proc. SPIE 2010 (7534): 7534OU-OU8 
Scott A. Ochsner, Davd L Steffen, Christian J Stoeckert 
Jr, Neil J. McKenna. Much room for improvement 
in deposition rates of expression microarray da-
tasets. Nat Methods. 2008 Dec;5(12):991.  
Heather A. Piwowar, Wendy W. Chapman. Identifying 
data sharing in biomedical literature.AMIA Annu 
Symp Proc. 2008 Nov 6:596-600. 
104
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 91?99,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
An improved corpus of disease mentions in PubMed citations 
 
Rezarta Islamaj Do?an Zhiyong Lu 
National Center for Biotechnology Information National Center for Biotechnology Information 
8600 Rockville Pike 8600 Rockville Pike 
Bethesda, MD 20894, USA Bethesda, MD 20894, USA 
Rezarta.Islamaj@nih.gov Zhiyong.Lu@nih.gov 
 
 
 
 
 
Abstract 
The latest discoveries on diseases and their di-
agnosis/treatment are mostly disseminated in 
the form of scientific publications. However, 
with the rapid growth of the biomedical litera-
ture and a high level of variation and ambigui-
ty in disease names, the task of retrieving 
disease-related articles becomes increasingly 
challenging using the traditional keyword-
based approach. An important first step for 
any disease-related information extraction 
task in the biomedical literature is the disease 
mention recognition task. However, despite 
the strong interest, there has not been enough 
work done on disease name identification, 
perhaps because of the difficulty in obtaining 
adequate corpora. Towards this aim, we creat-
ed a large-scale disease corpus consisting of 
6900 disease mentions in 793 PubMed cita-
tions, derived from an earlier corpus. Our cor-
pus contains rich annotations, was developed 
by a team of 12 annotators (two people per 
annotation) and covers all sentences in a 
PubMed abstract. Disease mentions are cate-
gorized into Specific Disease, Disease Class, 
Composite Mention and Modifier categories. 
When used as the gold standard data for a 
state-of-the-art machine-learning approach, 
significantly higher performance can be found 
on our corpus than the previous one. Such 
characteristics make this disease name corpus 
a valuable resource for mining disease-related 
information from biomedical text. The NCBI 
corpus is available for download at 
http://www.ncbi.nlm.nih.gov/CBBresearch/Fe
llows/Dogan/disease.html. 
1 Introduction 
Identification of biomedical entities has been an 
active area of research in recent years (Rinaldi et 
al., 2011, Smith et al, 2008, Yeh et al, 2005). Au-
tomatic systems, both lexically-based and machine 
learning-based, have been built to identify medi-
cally relevant concepts and/or their relationships. 
Biomedical entity recognition research covers not 
only gene/protein mention recognition (Tanabe et 
al., 2005, Campos et al, 2012), but also other med-
ically relevant concepts such as disease names, 
chemical/drug names, treatments, procedures etc. 
Systems capable of achieving high performance on 
these tasks are highly desirable as entity recogni-
tion precedes all other information extraction and 
text mining tasks.   
Disease information is sought very frequently in 
biomedical search engines. Previous PubMed log 
usage analysis (Islamaj Dogan et al, 2009) has 
shown that disease is the most frequent non-
bibliographic information requested from PubMed 
users. Furthermore, disease information was often 
found to be queried together with Chemical/Drug 
or Gene/Protein information. Automatic recogni-
tion of disease mentions therefore, is essential not 
only for improving retrieval of relevant documents, 
but also for extraction of associations between dis-
eases and genes or between diseases and drugs. 
However, prior research shows that automatic dis-
ease recognition is a challenging task due to varia-
tions and ambiguities in disease names (Leaman et 
al., 2009, Chowdhury and Lavelli 2010).  
Lexically-based systems of disease name recog-
nition, generally refer to the Unified Medical Lan-
guage System (UMLS) (Burgun and Bodenreider 
91
2008). UMLS is a comprehensive resource of med-
ically relevant concepts and relationships and 
METAMAP(Aronson and Lang 2010) is an exam-
ple of a natural language processing (NLP) system 
that provides reliable mapping of the text of a bio-
medical document to UMLS concepts and their 
semantic types.  
Machine learning systems, on the other hand, 
have been employed in order to benefit from the 
flexibility they allow over the rule-based and other 
statistical systems. However, machine learning 
systems are strongly dependent on the data availa-
ble for their training; therefore a comprehensive 
corpus of examples representing as many varia-
tions as possible of the entity of interest is highly 
favorable. 
To our best knowledge, there is one corpus of 
disease mentions in MEDLINE citations developed 
by Leaman et al, 2009. This corpus, AZDC cor-
pus, was inspired by the work of Jimeno et al, 
2008 and its overall characteristics are given in 
Table 1. This corpus has been the study of at least 
two different groups in building automatic systems 
for disease name recognition in biomedical litera-
ture (Leaman et al, 2009, Chowdhury and Lavelli, 
2010). They both reported F-scores around 80% in 
10-fold cross-validation experiments.  
One common encountered difficulty in this do-
main is the fact that ?disease? as a category has a 
very loose definition, and covers a wide range of 
concepts. ?Disease? is a broadly-used term that 
refers to any condition that causes pain, suffering, 
distress, dysfunction, social problems, and/or 
death. In UMLS, the ?disease? concept is covered 
by twelve different semantic types as shown in 
Table 2. The disease definition issue has been dis-
cussed extensively in other studies (Neveol et al, 
2009, Neveol and Lu 2012).   
Disease mentions are also heavily abbreviated in 
biomedical literature (Yeganova et al, 2010). The-
se abbreviations are not always standard; the same 
abbreviated form may represent different defining 
strings in different documents. It is therefore, un-
clear whether these ambiguities could be resolved 
by an abbreviation look-up list from UMLS Me-
tathesaurus and other available databases.  
In this study, we present our efforts in improv-
ing the AZDC corpus by building a richer, broader 
and more complete disease name corpus. The 
NCBI corpus reflects a more representative view 
of what constitutes a disease name as it combines 
the decisions of twelve annotators. It also provides 
four different categories of disease mentions. Our 
work was motivated by the following observations:  
? The need of a pool of experts:  
The AZDC corpus is the work of one annota-
tor. While in terms of consistency this is gen-
erally a good thing, a pool of annotators 
guarantees a more representative view of the 
entity to be annotated and an agreement be-
tween annotators is preferred for categories 
with loose definitions such as ?disease?. 
Moreover, this would ensure that there would 
be fewer missed annotations within the corpus.  
? The need of annotating all sentences in a 
document:  
The AZDC corpus has disease mention annota-
tions of selected sentences in a collection of 
PubMed abstracts. In order to be able to per-
form higher level text mining tasks that ex-
plore relationships between diseases and other 
types of information such as genes or drugs, 
the disease name annotation has to include all 
sentences, as opposed to selected ones. 
Our work is also related to other corpus annota-
tion projects in the biomedical domain (Grouin et 
al., 2011, Tanabe at al., 2005, Thompson et al, 
2009, Neveol at al., 2009, Chapman et al, 2012). 
These studies generally agree on the need of multi-
ple experienced annotators for the project, the need 
of detailed annotation guidelines, and the need of 
large scale high-quality annotation corpora. The 
production of such annotated corpora facilitates the 
development and evaluation of entity recognition 
and information extraction systems. 
2 Methods 
Here we describe the NCBI corpus, and its annota-
tion process. We discuss the annotation guidelines 
and how they evolved through the process. 
2.1 The NCBI disease corpus 
The AZDC corpus contains 2,783 sentences cho-
sen from 793 PubMed abstracts. These selected 
Table 1 AZDC corpus characteristics 
Characteristics of the corpus  
Selected abstracts 793 
Sentences 2,783 
Sentences with disease mentions 1,757 
Total disease mentions 3,224 
 
92
sentences were annotated for disease mentions, 
resulting in 1,202 unique mentions and 3,224 total 
mentions. The NCBI corpus starts with this origi-
nal corpus; however, it is expanded to cover all the 
sentences in all the 793 PubMed abstracts. 
2.2 Annotation guidelines 
One fundamental problem in corpus annotation is 
the definition of what constitutes an entity to be 
tagged. Following the lead of the AZDC annota-
tions, the group of annotators working on the 
NCBI corpus decided that a textual string would be 
annotated as a disease mention if it could be 
mapped to a unique concept in the UMLS Me-
tathesaurus, if it corresponded to at least one of the 
semantic types listed in Table 2, and if it contained 
information that would be helpful to physicians 
and health care professionals. 
Annotators were invited to use their common 
knowledge, use public resources of the National 
Library of Medicine such as UMLS or PubMed 
Health, Disease Ontology (Warren et al, 2006) and 
Wikipedia and consider the viewpoint of an aver-
age user trying to find information on diseases. 
Initially, a set of 20 randomly chosen PubMed 
abstracts was used as a practice set for the devel-
opment of annotation guidelines. After each anno-
tator worked individually on the set, the results 
were shared and discussed among all annotators. 
The final annotation guidelines are summarized 
below and also made available at the corpus down-
load website. 
What to annotate? 
1. Annotate all specific disease mentions. 
A textual string referring to a disease name may 
refer to a Specific Disease, or a Disease Class. 
Disease mentions that could be described as a 
family of many specific diseases were annotated 
with an annotation category called Disease 
Class. The annotation category Specific Disease 
was used for those mentions which could be 
linked to one specific definition that does not in-
clude further categorization.  
e.g. <Specific Disease> Diastrophic dysplasia 
</> is an <Disease Class> autosomal recessive 
disease</> characterized by short stature, very 
short limbs and joint problems that restrict mo-
bility. 
2. Annotate contiguous text strings. 
A textual string may refer to two or more sepa-
rate disease mentions. Such mentions are anno-
tated with the Composite Mention category. 
e.g. The text phrase ?Duchenne and Becker 
muscular dystrophy? refers to two separate dis-
eases. If this phrase is separated into two strings: 
?Duchenne? and ?Becker muscular dystrophy?, 
it results in information loss, because the word 
?Duchenne? on its own is not a disease mention.  
3. Annotate disease mentions that are used as 
modifiers for other concepts 
A textual string may refer to a disease name, but 
it may not be a noun phrase and this is better ex-
pressed with the Modifier annotation category.  
e.g.: Although this mutation was initially de-
tected in four of 33 <Modifier> colorectal can-
cer </> families analysed from eastern England, 
more extensive analysis has reduced the fre-
quency to four of 52 English <Modifier> 
HNPCC </> kindreds analysed. 
4. Annotate duplicate mentions. 
Table 2 The set of UMLS semantic types that collectively cover concepts of the ?disease? category 
UMLS sematic types Disease name example 
Acquired Abnormality Hernia, Varicose Veins 
Anatomical Abnormality Bernheim aneurysm,  Fistula of thoracic duct 
Congenital Abnormality Oppenheim's Disease, Ataxia Telangiectasia 
Cell or Molecular Dysfunction Uniparental disomy, Intestinal metaplasia 
Disease or Syndrome   Acute pancreatitis, Rheumatoid Arthritis 
Experimental Model of Disease Collagen-Induced Arthritis, Jensen Sarcoma 
Injury or Poisoning Contusion and laceration of cerebrum 
Mental or Behavioral Dysfunction Schizophrenia, anxiety disorder, dementia 
Neoplastic Process Colorectal Carcinoma, Burkitt Lymphoma 
Pathologic Function Myocardial degeneration, Adipose Tissue Atrophy 
Sign or Symptom Back Pain, Seizures, Skeletal muscle paralysis 
Finding Abnormal or prolonged bleeding time 
 
93
For each sentence in the PubMed abstract and ti-
tle, the locations of all disease mentions are 
marked, including duplicates within the same 
sentence.  
5. Annotate minimum necessary span of text. 
The minimum span of text necessary to include 
all the tokens expressing the most specific form 
of the disease is preferred. For example, in case 
of the phrase ?insulin-dependent diabetes melli-
tus?, the disease mention including the whole 
phrase was preferred over its substrings such as 
?diabetes mellitus? or ?diabetes?.  
6. Annotate all synonymous mentions.  
Abbreviation definitions such as ?Huntington 
disease? (?HD?) are separated into two annotat-
ed mentions. 
What not to annotate?  
1. Do not annotate organism names. 
Organism names such as ?human? were exclud-
ed from the preferred mention. Viruses, bacteria, 
and other organism names were not annotated 
unless it was clear from the context that the dis-
ease caused by these organisms is discussed.  
e.g. Studies of biopsied tissue for the presence 
of <Specific Disease> Epstein-Barr virus</> and 
<Specific Disease> cytomegalovirus </> were 
negative.  
2. Do not annotate gender.  
Tokens such as ?male? and ?female? were only 
included if they specifically identified a new 
form of the disease, for example ?male breast 
cancer?.  
3. Do not annotate overlapping mentions. 
For example, the phrase ?von Hippel-Lindau 
(VHL) disease? was annotated as one single dis-
ease mention. 
4. Do not annotate general terms.  
Very general terms such as: disease, syndrome, 
deficiency, complications, abnormalities, etc. 
were excluded. However, the terms cancer and 
tumor were retained. 
5. Do not annotate references to biological 
processes.  
For example, terms corresponding to biological 
processes such as ?tumorigenesis? or ?cancero-
genesis?.  
6. Do not annotate disease mentions inter-
rupted by nested mentions.  
Basically, do not break the contiguous text 
rule. E.g. WT1 dysfunction is implicated in both 
neoplastic (Wilms tumor, mesothelioma, leuke-
mia, and breast cancer) and nonneoplastic (glo-
merulosclerosis) disease. 
In this example, the list of all disease mentions 
includes: ?neoplastic disease? and ?nonneo-
plastic disease? in addition to the underlined 
mentions. However, they were not annotated in 
our corpus, because other tokens break up the 
phrase. 
2.3 Annotators and the annotation process 
The annotator group consisted of 12 people with 
background in biomedical informatics research and 
experience in biomedical text corpus annotation. 
The 793 PubMed citations were divided into sets 
of 25 PubMed citations each. Every annotator 
worked on 5 or 6 sets of 25 PubMed abstracts. The 
sets were divided randomly among annotators. 
Each set was shared by two people to annotate. To 
avoid annotator bias, pairs of annotators were cho-
sen randomly for each set of 25 PubMed abstracts.  
As illustrated in Figure 1, first, each abstract 
was pre-annotated using our in-house-developed 
CRF disease mention recognizer trained on the 
AZDC corpus. This process involved a 10-fold 
 
Figure 1. The annotation process 
 
94
cross-validation scheme, where all sentences from 
the same PubMed abstract were assigned to the 
same split. The learning was performed on 9-folds 
and then, the PubMed abstracts assigned to the 
10th fold were annotated for disease mentions on a 
sentence-by-sentence basis.  
Annotation Phase I consisted of each pre-
annotated abstract in the corpus being read and 
reviewed by two annotators working independent-
ly. Annotators could agree with the pre-annotation, 
remove it, or adjust its text span. Annotators could 
also add new annotations. After this initial round 
of annotations, a summary document was created 
highlighting the agreement and differences be-
tween two annotators in the annotations they pro-
duced for each abstract. This constituted the end of 
phase I. The pair of annotators working on the 
same set at this stage was given the summary doc-
ument and their own annotations of Phase I. 
In annotation Phase II, each annotator examined 
and edited his or her own annotations by reviewing 
the different annotations reported in the Phase I 
summary document. This resulted in a new set of 
annotations. After this round, a second summary 
document highlighting the agreement and differ-
ences between two annotators was created for each 
pair of annotators to review.  
After phase II, each pair of annotators organized 
meetings where they reviewed, discussed and re-
solved their differences. After these meetings, a 
reconciled set of annotations was produced for 
each PubMed abstract. The final stage of the anno-
tation process consisted of the first author going 
over all annotated segments and ensuring that an-
notations were consistent both in category and in 
text span across different abstracts and different 
annotation sets. For example if the phrase ?classi-
cal galactosemia? was annotated in one abstract as 
a Specific Disease mention, all occurrences of that 
phrase throughout the corpus should receive con-
sistent annotation. Identified hard cases were dis-
cussed at a meeting where all annotators were 
present and a final decision was made to reconcile 
differences. The final corpus is available at: 
http://www.ncbi.nlm.nih.gov/CBBresearch/Fellow
s/Dogan/disease.html 
 
Figure 2. NCBI corpus annotation software. Each annotator selects a PubMed ID from the current 
working set, and is directed to this screen. Annotation categories are: Specific Disease (highlighted in 
yellow), Disease Class (green), Composite Mention (blue), or Modifier (purple). To annotate a disease 
mention in text, annotators highlight the phrase and click on the appropriate label on top of the editor 
screen. To delete a disease mention, annotators highlight the phrase and click on the Clear label on top 
of the editor. Annotators can retrieve the last saved version of their annotations for each particular 
document by clicking on ?Last Saved? button. Annotators save their work by clicking on Submit but-
ton at the bottom of editor screen.  
 
95
2.4 Annotation software 
Annotation was done using a web interface (the 
prototype of PubTator (Wei et al, 2012)), as 
shown in Figure 2. Each annotator was able to log 
into the system and work independently. The sys-
tem allowed flexibility to make annotations in the 
defined categories, modify annotations, correct the 
text span, delete as well as go back and review the 
process as often as needed. At the end of each an-
notation phase, annotators saved their work, and 
the annotation results were compared to find 
agreement and consistency among annotations.  
2.5 Annotation evaluation metrics  
We measured the annotators? agreement at phase I 
and II of the annotation process. One way to meas-
ure the agreement between two annotators is to 
measure their observed agreement on the sample of 
annotated items, as specified in Equation (1).  
Agreement statistics are measured for each an-
notator pair, for each shared annotation set. Then, 
for each annotator pair the average agreement sta-
tistic is computed over all annotation sets shared 
between the pair of annotators. The final agree-
ment statistic reflects the average and standard de-
viation computed over all annotator pairs. This is 
repeated for both phases.  
Agreement between two annotators is measured 
on two levels: one, both annotators tag the same 
exact phrase based on character indices as a dis-
ease mention, and two, both annotators tag the 
same exact phrase based on character indices as a 
disease mention of the same category. 
2.6  Application of the NCBI corpus 
To compare the two disease corpora with regard to 
their intended primary use in training and testing 
machine learning algorithms, we performed a 10-
fold cross validation experiment with BANNER 
(Leaman et al 2009). We evaluated BANNER per-
formance and compared Precision, Recall and F-
score values for BANNER when trained and tested 
on AZDC corpus and the NCBI disease name cor-
pus, respectively. In these experiments, disease 
mentions of all categories were included and are 
discussed in the Results section.  
To compare the effect of improvement in dis-
ease name recognition, the different disease cate-
gory annotations present in the NCBI corpus were 
        
Figure 3 Inter-annotator annotation consistency measured at the span and span-category level 
 
Table 3 The annotation results and corpus characteristics 
Characteristics of the corpus NCBI corpus AZDC 
Annotators 12 1 
Annotated sentences in citation ALL Selected 
PubMed Citations 793 793 
Sentences 6,651 2,784 
Sentences with disease annotations 3,752 1,757 
Total disease mentions 6,900 3,228 
Specific Disease 3,924 - 
Disease Class 1029 - 
Modifier 1,774 - 
Composite Mention 173 - 
 
96
Table 4 NCBI corpus as training, development and testing sets for disease name recognition 
Corpus Characteristics  Training set Development set Test set 
PubMed Citations 593 100 100 
Total disease mentions 5148 791 961 
Specific Disease 2959 409 556 
Disease Class 781 127 121 
Modifier 1292 218 264 
Composite Mention 116 37 20 
 
flattened into only one single category. This made 
the NCBI corpus compatible with the AZDC cor-
pus. 
3 Results and Discussion 
3.1 Results of Inter-Annotator Agreement  
Figure 3 shows the inter-annotator agreement re-
sults after Phase I and Phase II of the annotations. 
These statistics show a good agreement between 
annotators, especially after phase II of annotations. 
In particular, both span-consistency measure and 
span-category consistency measure is above 80% 
after phase II. These values show that our corpus 
reflects a high quality of annotations and that our 
two-stage annotation steps are effective in improv-
ing corpus consistency.  
3.2 Agreement between automatic pre-
annotation and final annotation results 
In our previous work (Neveol et al 2009) we have 
shown that automatic pre-annotation is found help-
ful by most annotators in assisting large-scale an-
notation projects with regard to speeding up the 
annotation time and improving annotation con-
sistency while maintaining the high quality of the 
final annotations. Thus, we again used pre-
annotation in this work. To demonstrate that hu-
man annotators were not biased towards the com-
puter-generated pre-annotation, we compared the 
final annotation with the pre-annotation results. 
There are a total of 3295 pre-annotated disease 
mentions: 1750 were found also in the final corpus 
while the remaining 1545 were either modified or 
deleted. Furthermore, the final corpus consists of 
additional 3605 new annotations. Overall, the 
agreement between pre-annotation and final anno-
tation results is only 35%. 
3.3 Statistics of the NCBI disease corpus 
After two rounds of annotation, several annotator 
meetings and resolving of inconsistencies, the 
NCBI corpus contains 793 fully annotated PubMed 
citations for disease mentions which are divided 
into these categories: Specific Disease, Disease 
Class, Composite Mention and Modifier. As shown 
in Table 3, the NCBI corpus contains more than 
6K sentences, of which more than half contain dis-
ease mentions. There are 2,161 unique disease 
mentions total, which can be divided into these 
categories: 1,349 unique Specific Disease men-
tions, 608 unique Disease Class mentions, 121 
unique Composite Disease mentions, and 356 
unique Modifier disease mentions. The NCBI dis-
ease name corpus is available for download and 
can be used for development of disease name 
recognition tools, identification of Composite Dis-
ease Mentions, Disease Class or Modifier disease 
mention in biomedical text. 
3.4 Characteristics of the NCBI corpus 
This annotation task was initially undertaken for 
purposes of creating a larger, broader and more 
complete corpus for disease name recognition in 
biomedical literature.  
The NCBI corpus addresses the inconsistencies 
of missed annotations by using a pool of experts 
for annotation and creating the annotation envi-
ronment of multiple discussions and multiple 
rounds of annotation. The NCBI corpus addresses 
the problem of recognition of abbreviated disease 
mentions by delivering annotations for all sentenc-
es in the PubMed abstract. Processing all sentences 
in a document allows for recognition of an abbre-
viated form of a disease name. An abbreviated 
term could be tagged for later occurrences within 
the same document, if an abbreviation definition is 
recognized in one of the preceding sentences.  
NCBI corpus provides a richer level of annota-
tions characterized by four different categories of 
disease mentions: Specific Disease, Disease Class, 
(1) 
s2Annotation12100  ???? sAn otation
AgreementyConsistenc
 
97
Composite Mention and Modifier. Specific Disease 
mentions could be linked to one specific definition 
without further categorization, allowing for future 
normalization tasks. Composite Disease Mentions 
identify intricate lexical strings that express two or 
more disease mentions, allowing for future natural 
language processing tasks to look at them more 
closely. Modifier disease mentions identify non-
noun phrase mentions, again useful for other text 
mining tasks. 
Finally, the corpus can be downloaded and used 
for development and testing for disease name 
recognition and other tasks. To facilitate future 
work, we have divided the corpus into training, 
development and testing sets as shown in Table 4. 
 
3.5 The NCBI corpus as training data for 
disease mention recognition 
We replicated the BANNER experiments by com-
paring their cross-validation results on the original 
corpus (AZDC) and on the NCBI corpus. Our re-
sults reveal that BANNER achieves significantly 
better performance on the NCBI corpus: a 10% 
increase in F-score from 0.764 to 0.840.  Table 5 
shows detailed results for BANNER processing in 
precision, recall and F-score, for both corpora. 
In addition, we performed BANNER experi-
ments on the newly divided NCBI corpus with the 
following results: BANNER achieves an F-score of 
0.845 on a 10 fold cross-validation experiment on 
the NCBI training set, an F-score of 0.819 when 
tested on the NCBI development set, after trained 
on the NCBI training set, and an F-score of 0.818 
when tested on NCBI test set, after trained on 
NCBI training set.   
 
3.6 Limitations of this work 
The NCBI corpus was annotated manually, thus 
the tags assigned were judgment calls by human 
annotators. Annotation guidelines were established 
prior to the annotation process and they were re-
fined during the annotation process, however grey 
areas still remained for which no explicit rules 
were formulated. In particular, inclusion of qualita-
tive terms as part of the disease mention is a matter 
of further investigation as illustrated by the follow-
ing example:  
? Acute meningococcal pericarditis ? Consti-
tutes a disease mention and, exists as a 
separate concept in UMLS, however 
? Acute Neisseria infection ? May or may 
not include the descriptive adjective.  
Similarly: 
? Classical galactosemia ? Includes the de-
scriptive adjective, because it corresponds 
to a particular form of the disease. 
? Inherited spinocerebellar ataxia ? May or 
may not include the descriptive adjective. 
Names containing conjunctions are difficult to 
tag. Although it might seem excessive to require a 
named entity recognizer to identify the whole ex-
pression for cases such as:  
? Adenomatous polyps of the colon and rec-
tum, 
? Fibroepithelial or epithelial hyperplasias, 
? Stage II or stage III colorectal cancer, 
The NCBI disease name corpus rectifies this sit-
uation by annotating them as Composite Mention 
disease name category, thus, allowing for future 
NLP application to develop more precise methods 
in identifying these expressions.  
Moreover, sentences which contained nested 
disease names require further attention, as the cur-
rent annotation rule of annotating only contiguous 
phrases cannot select the outer mentions. 
Finally, our current annotation guideline re-
quires that only one of the four categories be as-
signed to each disease mention. This is not ideal 
because a disease mention may actually fit more 
than one category. For instance, a mention can be 
tagged as both ?Modifier? and ?Disease Class?. In 
practice, for obtaining consistent annotations, the 
priority was given in the order of ?Modifier?, 
?Composite Mention?, ?Disease Class?, and ?Spe-
cific Disease? when more than one category deems 
appropriate. This aspect should be addressed at 
future work.   
4 Conclusions 
We have described the NCBI disease name corpus 
of tagged disease mentions in 793 PubMed titles 
and abstracts. The corpus was designed to capture 
Table 5 BANNER evaluation results on AZDC 
(original) corpus and on the NCBI corpus. 
CRF-
order 
Corpus Precision Recall F-score 
1 AZDC 0.788 0.743 0.764 
1 NCBI 0.859 0.824 0.840 
2 AZDC 0.804 0.752 0.776 
2 NCBI 0.857 0.820 0.838 
 
98
disease mentions in the most common sense of the 
word, and is particularly relevant for biomedical 
information retrieval tasks that involve diseases. 
Annotations were performed for all sentences in a 
document, facilitating the future applications of 
complex information retrieval tasks connecting 
diseases to treatments, causes or other types of in-
formation. Annotation guidelines were designed 
with the goal of allowing flexible matching to 
UMLS concepts, while retaining true meaning of 
the tagged concept. A more detailed definition on 
what constitutes a disease name, accompanied with 
additional annotation rules, could help resolve 
some existing inconsistencies.  The current corpus 
is reviewed several times by several annotators and 
describes a refined scale of annotation categories. 
It allows the separate definition and annotation of 
Composite mentions, Modifiers and distinguishes 
between Disease Class mentions versus Specific 
Diseases. The corpus is available for download1. 
Acknowledgments 
Funding: This research was supported by the Intramural 
Research Program of the NIH, National Library of Med-
icine.  
We sincerely thank Robert Leaman and Graciela Gon-
zalez for their help with BANNER, and the whole team 
of 12 annotators for their time and expertise on annota-
tion of this corpus.   
References  
Aronson, A., Lang, F. 2010. An overview of MetaMap: 
historical perspective and recent advances. J Am Med 
Inform Assoc, 17(3): 229-236. 
Burgun, A., Bodenreider, O. 2008. Accessing and inte-
grating data and knowledge for biomedical research. 
Yearb Med Inform, 91-101. 
Campos, D., Matos, S., Lewin, I., Oliveira, J., Rebholz-
Schuhmann, D. 2012. Harmonisation of gene/protein 
annotations: towards a gold standard MEDLINE. Bi-
oinformatics, 1;28(9):1253-61 
Chapman, W.W., Savova, G.K., Zheng, J., Tharp, M., 
Crowley, R. 2012. Anaphoric reference in clinical re-
ports: Characteristics of an annotated corpus. J Bio-
med Inform  
Chowdhury, F.M., Lavelli, A. 2010. Disease mention 
recognition with specific features. BioNLP, 91-98. 
Grouin, C., Rosset. S., Zweigenbaum, P., Fort, K., Gali-
bert, O., Quintard, L. 2011. Proposal for an extension 
                                                          
1 
http://www.ncbi.nlm.nih.gov/CBBresearch/Fellows/Dogan/dis
ease.html 
of traditional named entities: From guidelines to 
evalua-tion, an overview. 5th law workshop, 92-100.  
Islamaj Dogan, R., Murray, G. C., Neveol, A., Lu, Z. 
2009. Understanding PubMed user search behavior 
through log analysis. Database (Oxford): bap018. 
Jimeno,A., Jimnez-Ruiz, E., Lee, V., Gaudan, S., Ber-
langa,R., Reholz-Schuhmann, D.2008. Assessment of 
disease named entity recognition on a corpus of an-
no-tated sentences. BMC Bioinformatics, 9(S-3). 
Leaman, R., Miller, C., Gonzalez, G. 2009. Enabling 
Recognition of Diseases in Biomedical Text with 
Ma-chine Learning: Corpus and Benchmark. Sympo-
sium on Languages in Biology and Medicine, 82-89.  
Neveol, A., Li, J., Lu, Z. 2012. Linking Multiple Dis-
ease-related resources through UMLS. ACM Interna-
tional Health Informatics. 
Neveol, A., Islamaj Dogan, R., Lu, Z. 2011. Semi-
automatic semantic annotation of PubMed Queries: a 
study on quality, efficiency, satisfaction. J Biomed 
Inform, 44(2):310-8. 
Rinaldi, F., Kaljurand, K., S?tre, R. 2011. Terminologi-
cal resources for text mining over biomedical scien-
tific literature. Artificial intelligence in medicine 
52(2) 
Smith L., Tanabe L.K., Ando R.J., Kuo C.J., Chung I.F., 
Hsu C.N., Lin Y.S., Klinger R., Friedrich C.M., 
Ganchev K., Torii M., Liu H., Haddow B., Struble 
C.A., Povinelli R.J., Vlachos A., Baumgartner W.A. 
Jr., Hunter L., Carpenter B., Tsai R.T., Dai H.J., Liu 
F., Chen Y., Sun C., Katrenko S., Adriaans P., 
Blaschke C., Torres R., Neves M., Nakov P., Divoli 
A., Ma?a-L?pez M., Mata J., Wilbur W.J. 
2008.Overview of BioCreative II gene mention 
recognition. Genome Biology, 9 Suppl 2:S2. 
Tanabe, L., Xie, N., Thom, L., Matten, W., Wilbur, W.J. 
2005. GENETAG: a tagged corpus for gene /protein 
named entity recognition. BMC Bioinformatics, 6:S3. 
Thompson, P., Iqbal, S.A., McNaught, J., Ananiadou, S. 
2009. Construction of an annotated corpus to support 
biomedical information extraction. BMC Bioinfor-
matics, 10:349. 
Warren A., Kibbe J.D.O., Wolf W.A., Smith M.E., Zhu L., 
Lin S., Chisholm R., Disease Ontology. 2006 
Wei C., Kao, H., Lu, Z., 2012. PubTator: A PubMed-
like interactive curation system for document triage 
and literature Curation. In proceedings of BioCrea-
tive workshop, 145-150.  
Yeganova, L., Comeau, D.C., Wilbur, W.J. 2011. Ma-
chine learning with naturally labeled data for identi-
fying abbreviation definitions. BMC Bioinformatics.  
S3:S6 
Yeh, A., Morgan, A., Colosime, M., Hirschman, L. 
2005. BioCreAtIvE Task 1A: gene mention finding 
evaluation. BMC Bioinformatics, 6(Suppl 1):S2 
99
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 24?28,
Baltimore, Maryland USA, June 26-27 2014. c?2014 Association for Computational Linguistics
Automated Disease Normalization with Low Rank Approximations 
 
Robert Leaman Zhiyong Lu 
National Center for Biotechnology Information 
National Library of Medicine 
{robert.leaman, zhiyong.lu}@nih.gov 
 
  
Abstract 
While machine learning methods for 
named entity recognition (mention-level 
detection) have become common, ma-
chine learning methods have rarely been 
applied to normalization (concept-level 
identification). Recent research intro-
duced a machine learning method for 
normalization based on pairwise learning 
to rank. This method, DNorm, uses a lin-
ear model to score the similarity between 
mentions and concept names, and has 
several desirable properties, including 
learning term variation directly from 
training data. In this manuscript we em-
ploy a dimensionality reduction tech-
nique based on low-rank matrix approx-
imation, similar to latent semantic index-
ing. We compare the performance of the 
low rank method to previous work, using 
disease name normalization in the NCBI 
Disease Corpus as the test case, and 
demonstrate increased performance as 
the matrix rank increases. We further 
demonstrate a significant reduction in the 
number of parameters to be learned and 
discuss the implications of this result in 
the context of algorithm scalability. 
1 Introduction 
The data necessary to answer a wide variety of 
biomedical research questions is locked away in 
narrative text. Automating the location (named 
entity recognition) and identification (normaliza-
tion) of key biomedical entities (Do?an et al., 
2009; N?v?ol et al., 2011) such as diseases, pro-
teins and chemicals in narrative text may reduce 
curation costs, enable significantly increased 
scale and ultimately accelerate biomedical dis-
covery (Wei et al., 2012a). 
Named entity recognition (NER) techniques 
have typically focused on machine learning 
methods such as conditional random fields 
(CRFs), which have provided high performance 
when coupled with a rich feature approach. The 
utility of NER for biomedical end users is lim-
ited, however, since many applications require 
each mention to be normalized, that is, identified 
within a specified controlled vocabulary.  
The normalization task has been highlighted in 
the BioCreative challenges (Hirschman et al., 
2005; Lu et al., 2011; Morgan et al., 2008), 
where a variety of methods have been explored 
for normalizing gene names, including string 
matching, pattern matching, and heuristic rules. 
Similar methods have been applied to disease 
names (Do?an & Lu, 2012b; Kang et al., 2012; 
N?v?ol et al., 2009) and species names (Gerner 
et al., 2010; Wei et al., 2012b), and the MetaMap 
program is used to locate and identify concepts 
from the UMLS MetaThesaurus (Aronson, 2001; 
Bodenreider, 2004). 
Machine learning methods for NER have pro-
vided high performance, enhanced system adapt-
ability to new entity types, and abstracted many 
details of specific rule patterns. While machine 
learning methods for normalization have been 
explored (Tsuruoka et al., 2007; Wermter et al., 
2009), these are far less common. This is partial-
ly due to the lack of appropriate training data, 
and also partially due to the need for a general-
izable supporting framework.  
Normalization is frequently decomposed into 
the sub-tasks of candidate generation and disam-
biguation (Lu et al., 2011; Morgan et al., 2008). 
During candidate generation, the set of concept 
names is constrained to a set of possible matches 
using the text of the mention. The primary diffi-
culty addressed in candidate generation is term 
variation: the need to identify terms which are 
semantically similar but textually distinct (e.g. 
?nephropathy? and ?kidney disease?). The dis-
ambiguation step then differentiates between the 
different candidates to remove false positives, 
typically using the context of the mention and the 
article metadata. 
24
Recently, Leaman et al. (2013a) developed an 
algorithm (DNorm) that directly addresses the 
term variation problem with machine learning, 
and used diseases ? an important biomedical en-
tity ? as the first case study. The algorithm learns 
a similarity function between mentions and con-
cept names directly from training data using a 
method based on pairwise learning to rank. The 
method was shown to provide high performance 
on the NCBI Disease Corpus (Do?an et al., 2014; 
Do?an & Lu, 2012a), and was also applied to 
clinical notes in the ShARe / CLEF eHealth task 
(Suominen et al., 2013), where it achieved the 
highest normalization performance out of 17 in-
ternational teams (Leaman et al., 2013b). The 
normalization step does not consider context, and 
therefore must be combined with a disambigua-
tion method for tasks where disambiguation is 
important. However, this method provides high 
performance when paired with a conditional ran-
dom field system for NER, making the combina-
tion a step towards fully adaptable mention 
recognition and normalization systems. 
This manuscript adapts DNorm to use a di-
mensionality reduction technique based on low 
rank matrix approximation. This may provide 
several benefits. First, it may increase the scala-
bility of the method, since the number of pa-
rameters used by the original technique is pro-
portional to the square of the number of unique 
tokens. Second, reducing the number of parame-
ters may, in turn, improve the stability of the 
method and improve its generalization due to the 
induction of a latent ?concept space,? similar to 
latent semantic indexing (Bai et al., 2010). Final-
ly, while the rich feature approach typically used 
with conditional random fields allows it to par-
tially compensate for out-of-vocabulary effects, 
DNorm ignores unknown tokens. This reduces 
the ability of the model to generalize, due to the 
zipfian distribution of text (Manning & Sch?tze, 
1999), and is especially problematic in text 
which contains many misspellings, such as con-
sumer text. Using a richer feature space with 
DNorm would not be feasible, however, unless 
the parameter scalability problem is resolved. 
In this article we expand the DNorm method 
in a pilot study on feasibility of using low rank 
approximation methods for disease name nor-
malization. To make this work comparable to the 
previous work on DNorm, we again employed 
the NCBI Disease Corpus (Do?an et al., 2014). 
This corpus contains nearly 800 abstracts, split 
into training, development, and test sets, as de-
scribed in Table 1. Each disease mention is anno-
tated for span and concept, using the MEDIC 
vocabulary (Davis et al., 2012), which combines 
MeSH? (Coletti & Bleich, 2001) and OMIM? 
(Amberger et al., 2011). The average number of 
concepts for each name in the vocabulary is 5.72. 
Disease names exhibit relatively low ambiguity, 
with an average number of concepts per name of 
1.01. 
 
Subset Abstracts Mentions Concepts 
Training 593 5145 670 
Development 100 787 176 
Test 100 960 203 
 
Table 1. Descriptive statistics for the NCBI Disease 
Corpus. 
2 Methods 
DNorm uses the BANNER NER system 
(Leaman & Gonzalez, 2008) to locate disease 
mentions, and then employs a ranking method to 
normalize each mention found to the disease 
concepts in the lexicon (Leaman et al., 2013a). 
Briefly, we define   to be the set of tokens from 
both the disease mentions in the training data and 
the concept names in the lexicon. We stem each 
token in both disease mentions and concept 
names (Porter, 1980), and then convert each to 
TF-IDF vectors of dimensionality | |, where the 
document frequency for each token is taken to be 
the number of names in the lexicon containing it 
(Manning et al., 2008). All vectors are normal-
ized to unit length. We define a similarity score 
between mention vector   and name vector  , 
     (   ), and each mention is normalized by 
iterating through all concept names and returning 
the disease concept corresponding to the one 
with the highest score. 
In previous work,      (   )      , 
where  is a weight matrix and each entry     
represents the correlation between token    ap-
pearing in a mention and token    appearing in a 
concept name from the lexicon. In this work, 
however, we set  to be a low-rank approxima-
tion of the form       , where   and   
are both   | |  matrices,   being the rank 
(number of linearly independent rows), and 
  | | (Bai et al., 2010). 
For efficiency, the low-rank scoring function 
can be rewritten and evaluated as      (   )  
(  ) (  )     , allowing the respective    
and    vectors to be calculated once and then 
reused. This view provides an intuitive explana-
tion of the purpose of the  and   matrices: to 
25
convert the sparse, high-dimensional mention 
and concept name vectors (  and  ) into dense, 
low dimensional vectors (as    and   ). Under 
this interpretation, we found that performance 
improved if each    and    vector was renor-
malized to unit length. 
This model retains many useful properties of 
the original model, such as the ability to repre-
sent both positive and negative correlations be-
tween tokens, to represent both synonymy and 
polysemy, and to allow the token distributions 
between the mentions and the names to be differ-
ent. The new model also adds one important ad-
ditional property: the number of parameters is 
linear in the number of unique tokens, potentially 
enabling greater scalability.  
2.1 Model Training 
Given any pair of disease names where one (  ) 
is for   , the correct disease concept for 
tion , and the other,   , is for   , an incorrect 
concept , we would like to update the weight ma-
trix   so that            . Following 
Leaman et al. (2013a), we  iterate through each 
?       ? tuple, selecting   and   as the name 
for    and   , respectively, with the highest sim-
ilarity score to , using stochastic gradient de-
scent to make updates to . With a dense weight 
matrix  , the update rule is: if       
       , then   is updated as     
 ( (  )   (  ) ) , where   is the learning 
rate, a parameter controlling the size of the 
change to W. Under the low-rank approximation, 
the update rules are: if             , 
then   is updated as       (     )  , 
and   is updated as        (     ) , 
noting that the updates are applied simultaneous-
ly (Bai et al., 2010). Overfitting is avoided using 
a holdout set, using the average of the ranks of 
the correct concept as the performance measure-
ment, as in previous work. 
We initialize   using values chosen randomly 
from a normal distribution with mean 0 and 
standard deviation 1. We found it useful to ini-
tialize   as   , since this causes the representa-
tion for disease mentions and disease names to 
initially be the same.  
We employed an adaptive learning rate using 
the schedule      
 
   
, where   is the itera-
tion,    is the initial learning rate, and   is the 
discount (Finkel et al., 2008). We used an initial 
learning rate of       
  . This is much lower 
than reported by Leaman et al. (2013a), since we 
found that higher values caused the training to 
found that higher values caused the training to 
diverge. We used a discount parameter of    , 
so that the learning rate is equal to one half the 
initial rate after five iterations. 
3 Results 
Our results were evaluated at the abstract level, 
allowing comparison to the previous work on 
DNorm (Leaman et al., 2013a). This evaluation 
considers the set of disease concepts found in the 
abstract, and ignores the exact location(s) where 
each concept was found. A true positive consists 
of the system returning a disease concept anno-
tated within the NCBI Disease Corpus, and the 
number of false negatives and false positives are 
defined similarly. We calculated the precision, 
recall and F-measure as follows: 
  
  
     
     
  
     
     
   
   
 
We list the micro-averaged results in Table 2. 
 
Rank Precision Recall F-measure 
50 0.648 0.671 0.659 
100 0.673 0.685 0.679 
250 0.697 0.697 0.697 
500 0.702 0.700 0.701 
(Full) 0.828 0.819 0.809 
 
Table 2. Performance measurements for each 
model on the NCBI Disease Test set. Full corre-
sponds with the full-rank matrix used in previous 
work. 
4 Discussion 
There are two primary trends to note. First, the 
performance of the low rank models is about 
10%-15% lower than the full rank model. Sec-
ond, there is a clear trend towards higher preci-
sion and recall as the rank of the matrix increas-
es.  This trend is reinforced in Figure 1, which 
shows the learning curve for all models. These 
describe the performance on the holdout set after 
each iteration through the training data, and are 
measured using the average rank of the correct 
concept in the holdout set, which is dominated 
by a small number of difficult cases. 
Using the low rank approximation, the number 
of parameters is equal to     | |. Since   is 
fixed and independent of | |, the number of pa-
rameters is now linear in the number of tokens, 
effectively solving the parameter scalability 
problem. Table 3 lists the number of parameters 
for each of the models used in this study. 
 
26
 Figure 1. Learning curves showing holdout per-
formance at each iteration through the training 
data. 
 
Rank Parameters 
50 1.8?106 
100 3.7?106 
250 9.1?106 
500 1.8?107 
(Full) 3.3?108 
 
Table 3. Number of model parameters for each 
variant, showing the low rank methods using 1 to 
2 orders of magnitude fewer parameters. 
 
There are two trade-offs for this improvement 
in scalability. First, there is a substantial perfor-
mance reduction, though this might be mitigated 
somewhat in the future by using a richer feature 
set ? a possibility enabled by the use of the low 
rank approximation. Second, training and infer-
ence times are significantly increased; training 
the largest low-rank model (     ) required 
approximately 9 days, though the full-rank mod-
el trains in under an hour.  
The view that the   and   matrices convert the 
TF-IDF vectors to a lower dimensional space 
suggests that the function of   and   is to pro-
vide word embeddings or word representations ? 
a vector space where each word vector encodes 
its relationships with other words. This further 
suggests that one way to provide higher perfor-
mance may be to take advantage of unsupervised 
pre-training (Erhan et al., 2010). Instead of ini-
tializing   and   randomly, they could be initial-
ized using a set of word embeddings trained on a 
large amount of biomedical text, such as with 
neural network language models (Collobert & 
Weston, 2008; Mikolov et al., 2013). 
5 Conclusion 
We performed a pilot study to determine whether 
a low rank approximation may increase the 
scalability of normalization using pairwise learn-
ing to rank. We showed that the reduction in the 
number of parameters is substantial: it is now 
linear to the number of tokens, rather than pro-
portional to the square of the number of tokens. 
We further observed that the precision and recall 
increase as the rank of the matrices is increased. 
We believe that further performance increases 
may be possible through the use of a richer fea-
ture set, unsupervised pre-training, or other di-
mensionality reduction techniques including fea-
ture selection or L1 regularization (Tibshirani, 
1996). We also intend to apply the method to 
additional entity types, using recently released 
corpora such as CRAFT (Bada et al., 2012). 
Acknowledgments 
The authors would like to thank the anonymous 
reviewers for their helpful suggestions. This re-
search was supported by the NIH Intramural Re-
search Program, National Library of Medicine. 
References 
Amberger, J., Bocchini, C., & Hamosh, A. (2011). A 
new face and new challenges for Online 
Mendelian Inheritance in Man (OMIM(R)). Hum 
Mutat, 32(5), 564-567.  
Aronson, A. R. (2001). Effective mapping of 
biomedical text to the UMLS Metathesaurus: the 
MetaMap program. In  Proceedings of the AMIA 
Symposium, 17-21. 
Bada, M., Eckert, M., Evans, D., Garcia, K., Shipley, 
K., Sitnikov, D., et al. (2012). Concept annotation 
in the CRAFT corpus. BMC Bioinformatics, 13, 
161.  
Bai, B., Weston, J., Grangier, D., Collobert, R., 
Sadamasa, K., Qi, Y. J., et al. (2010). Learning to 
rank with (a lot of) word features. Inform. 
Retrieval, 13(3), 291-314.  
Bodenreider, O. (2004). The Unified Medical 
Language System (UMLS): integrating biomedical 
terminology. Nucleic Acids Res, 32, D267-270.  
Coletti, M. H., & Bleich, H. L. (2001). Medical 
subject headings used to search the biomedical 
literature. J Am Med Inform Assoc, 8(4), 317-323.  
Collobert, R., & Weston, J. (2008). A unified 
architecture for natural language processing: 
deep neural networks with multitask learning. In  
Proceedings of the ICML, 160-167. 
Davis, A. P., Wiegers, T. C., Rosenstein, M. C., & 
Mattingly, C. J. (2012). MEDIC: a practical 
disease vocabulary used at the Comparative 
20
30
40
50
60
70
80
90
100
0 5 10
A
ve
ra
ge
 r
an
k 
Iteration 
50
100
250
500
Full
27
Toxicogenomics Database. Database, 2012, 
bar065.  
Do?an, R. I., Leaman, R., & Lu, Z. (2014). NCBI 
disease corpus: A resource for disease name 
recognition and concept normalization. J Biomed 
Inform, 47, 1-10.  
Do?an, R. I., & Lu, Z. (2012a). An improved corpus 
of disease mentions in PubMed citations. In  
Proceedings of the ACL 2012 Workshop on 
BioNLP, 91-99. 
Do?an, R. I., & Lu, Z. (2012b). An Inference Method 
for Disease Name Normalization. In  Proceedings 
of the AAAI 2012 Fall Symposium on Information 
Retrieval and Knowledge Discovery in 
Biomedical Text, 8-13. 
Do?an, R. I., Murray, G. C., N?v?ol, A., & Lu, Z. 
(2009). Understanding PubMed user search 
behavior through log analysis. Database (Oxford), 
2009, bap018.  
Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-
A., Vincent, P., & Bengio, S. (2010). Why does 
unsupervised pre-training help deep learning? J. 
Machine Learning Res., 11, 625-660.  
Finkel, J. R., Kleenman, A., & Manning, C. D. 
(2008). Efficient, Feature-based, Conditional 
Random Field Parsing. In  Proceedings of the 46th 
Annual Meeting of the ACL, 959-967. 
Gerner, M., Nenadic, G., & Bergman, C. M. (2010). 
LINNAEUS: a species name identification system 
for biomedical literature. BMC Bioinformatics, 11, 
85.  
Hirschman, L., Colosimo, M., Morgan, A., & Yeh, A. 
(2005). Overview of BioCreAtIvE task 1B: 
normalized gene lists. BMC Bioinformatics, 6 
Suppl 1, S11.  
Kang, N., Singh, B., Afzal, Z., van Mulligen, E. M., 
& Kors, J. A. (2012). Using rule-based natural 
language processing to improve disease 
normalization in biomedical text. J. Am. Med. 
Inform. Assoc., 20, 876-881.  
Leaman, R., Do?an, R. I., & Lu, Z. (2013a). DNorm: 
Disease name normalization with pairwise 
learning-to-rank. Bioinformatics, 29(22), 2909-
2917.  
Leaman, R., & Gonzalez, G. (2008). BANNER: an 
executable survey of advances in biomedical 
named entity recognition. Pac. Symp. Biocomput., 
652-663.  
Leaman, R., Khare, R., & Lu, Z. (2013b). NCBI at 
2013 ShARe/CLEF eHealth Shared Task: 
Disorder Normalization in Clinical Notes with 
DNorm. In Working Notes of the Conference and 
Labs of the Evaluation Forum Valencia, Spain. 
Lu, Z., Kao, H. Y., Wei, C. H., Huang, M., Liu, J., 
Kuo, C. J., et al. (2011). The gene normalization 
task in BioCreative III. BMC Bioinformatics, 12 
Suppl 8, S2.  
Manning, C., & Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing: 
Massachusetts Institute of Technology. 
Manning, C. D., Raghavan, P., & Sch?tze, H. (2008). 
Introduction to Information Retrieval: Cambridge 
University Press. 
Mikolov, T., Yih, W.-t., & Zweig, G. (2013). 
Linguistic Regularities in Continuous Space Word 
Representations. In  Proceedings of the 2013 
Conference of the NAACL-HLT, 746-751. 
Morgan, A. A., Lu, Z., Wang, X., Cohen, A. M., 
Fluck, J., Ruch, P., et al. (2008). Overview of 
BioCreative II gene normalization. Genome Biol., 
9 Suppl 2, S3.  
N?v?ol, A., Do?an, R. I., & Lu, Z. (2011). Semi-
automatic semantic annotation of PubMed queries: 
a study on quality, efficiency, satisfaction. J 
Biomed Inform, 44(2), 310-318.  
N?v?ol, A., Kim, W., Wilbur, W. J., & Lu, Z. (2009). 
Exploring two biomedical text genres for disease 
recognition. In  Proceedings of the ACL 2009 
BioNLP Workshop, 144-152. 
Porter, M. F. (1980). An algorithm for suffix 
stripping. Program, 14, 130-137.  
Suominen, H., Salanter?, S., Velupillai, S., Chapman, 
W., Savova, G., Elhadad, N., et al. (2013). 
Overview of the ShARe/CLEF eHealth Evaluation 
Lab 2013. In P. Forner, H. M?ller, R. Paredes, P. 
Rosso & B. Stein (Eds.), Information Access 
Evaluation. Multilinguality, Multimodality, and 
Visualization (Vol. 8138, pp. 212-231): Springer 
Berlin Heidelberg. 
Tibshirani, R. (1996). Regression shrinkage and 
selection via the Lasso. Journal of the Royal 
Statistical Society Series B-Methodological, 58(1), 
267-288.  
Tsuruoka, Y., McNaught, J., Tsujii, J., & Ananiadou, 
S. (2007). Learning string similarity measures for 
gene/protein name dictionary look-up using 
logistic regression. Bioinformatics, 23(20), 2768-
2774.  
Wei, C. H., Harris, B. R., Li, D., Berardini, T. Z., 
Huala, E., Kao, H. Y., et al. (2012a). Accelerating 
literature curation with text-mining tools: a case 
study of using PubTator to curate genes in 
PubMed abstracts. Database (Oxford), 2012, 
bas041.  
Wei, C. H., Kao, H. Y., & Lu, Z. (2012b). SR4GN: a 
species recognition software tool for gene 
normalization. PLoS One, 7(6), e38460.  
Wermter, J., Tomanek, K., & Hahn, U. (2009). High-
performance gene name normalization with GeNo. 
Bioinformatics, 25(6), 815-821.  
 
 
28
