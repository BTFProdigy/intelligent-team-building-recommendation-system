Analysis and Processing of Lecture Audio Data: Preliminary Investigations
James Glass, Timothy J. Hazen, Lee Hetherington, and Chao Wang
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar Street, Cambridge, MA 02139, USA
(glass,hazen,hetherington,wang)@csail.mit.edu
Abstract
In this paper we report on our recent efforts to
collect a corpus of spoken lecture material that
will enable research directed towards fast, ac-
curate, and easy access to lecture content. Thus
far, we have collected a corpus of 270 hours of
speech from a variety of undergraduate courses
and seminars. We report on an initial analysis
of the spontaneous speech phenomena present
in these data and the vocabulary usage patterns
across three courses. Finally, we examine lan-
guage model perplexities trained from written
and spoken materials, and describe an initial
recognition experiment on one course.
1 Introduction
In the past decade, we have seen a dramatic increase
in the availability of on-line academic lecture material.
These educational resources can potentially change the
way people learn ? students with disabilities can en-
hance their educational experience, professionals can
keep up with recent advancements in their field, and peo-
ple of all ages can satisfy their thirst for knowledge. In
contrast to many other communicative activities however,
lecture processing has until recently enjoyed little bene-
fit from the development of human language technology.
Although there has been significant research directed to-
ward audio indexing and retrieval (Bacchiani et al, 2001;
Foote, 1999; Jourlin et al, 2000; Makhoul et al, 2000;
Franz et al, 2003; Renals et al, 2000), lecture tran-
scription and analysis is a relatively unexplored area in
speech and natural language research. The most substan-
tial research on lectures has been performed as part of
the Spontaneous Speech Project in Japan (Furui, 2003),
where researchers are processing a variety of Japanese
monologues such as academic and simulated presenta-
tions, news commentaries, etc. There has also been some
work reported on German lectures (Hurst et al, 2002).
One of the reasons for the minimal research in this
area is due to the limited availability of relevant data.
The only publicly available corpus of academic presen-
tations in English is TED, which includes 48 hours of au-
dio recordings of 188 presentations given at Eurospeech
?93 (Lamel et al, 1994). Only 6 of the presenters were
native English speakers however, and only 39 of the lec-
tures have been transcribed. The Corpus of Spontaneous
Japanese currently contains over 2,500 transcribed pre-
sentations (Kawahara et al, 2003). Both of these corpora
focus on conference presentations, which are shorter and
have a lower degree of spontaneity than a one hour or 90
minute classroom lecture.
We have recently initiated a research effort with the
goal of enabling fast, accurate, and easy access to lec-
ture materials. As part of the first phase of this research,
we have begun to create a large corpus of spoken lecture
material. In this paper, we document our ongoing data
collection activities, and describe the results of our pre-
liminary analyses of these data.
2 Corpus Creation and Annotation
In our efforts to date, we have created an initial corpus
of approximately 270 hours containing lectures from six
different courses, and from over 80 seminars given on a
variety of topics. On average, each course contained over
30 lecture sessions. These data were recorded with an
omni-directional microphone (as part of a video record-
ing), and generally occurred in a classroom environment.
To provide data for acoustic and language model train-
ing, we are in the process of generating transcriptions for
the lecture material we have collected to date. An ini-
tial set of transcriptions have been generated by an au-
dio transcription service. The transcription service was
instructed to pay careful attention to generating a correct
literal transcription of what was spoken (and not a ?clean?
transcript with disfluencies such as filled pauses and false
starts removed). In additional to the spoken words, the
transcription service also provided the following anno-
tations: (1) occasional time markers, usually at obvious
pauses or sentence boundaries, (2) locations of speaker
changes (labeled with speaker identities when known),
and (3) punctuation based on the transcribers subjective
assessment of the structure of the spoken utterances.
In addition to the audio data, we have obtained elec-
tronic versions of texts associated with three of these
courses, and over 100 summaries of lecture content for
one of them. We have also obtained electronic notes and
presentations for another course. These resources will be
used for our research involving written and spoken data.
3 Analysis of Lecture Characteristics
3.1 Qualitative Analysis
As illustrated in Figure 1, lecture data has much in com-
mon with casual, or spontaneous speech data, including
false starts, extraneous filler words ( such as ?okay? and
?well?), and non-lexical filled pauses (such as ?uh? or
?um?). One can also easily observe that the colloquial
nature of the data is dramatically different in style from
the same presentation of this material in a text book. For
example, one linear algebra text book covers this material
using a section header that reads, ?8 Rules of Matrix Mul-
tiplication,? followed by text that reads, ?The method for
multiplying two matrices A and B to get C = AB can be
summarized as follows...? The section header and intro-
ductory sentence express the same information as the ten
utterances spoken in Figure 1. In other words, the textual
format is typically more concise and better organized.
Apart from poor planning at the sentence level, lecture
speech often exhibits poor planning at higher structural
levels as well. For example, tangential threads digressing
from the current primary theme are common in sponta-
neous speech. This is exemplified by the brief diversion
into matrix inversion in utterances (4), (5) and (6). This
off-theme digression occurs only three utterances after
the primary theme of ?the rules for matrix multiplication?
is introduced in (1).
3.2 Quantitative Analysis
In order to better quantify the characteristics of lecture
data, we have recently examined a set of 80 lectures taken
from three undergraduate courses in math, physics, and
computer science. The total number of words in each
approximately one hour lecture ranged between 5K and
12K words, with an average of nearly 7K words, and
standard deviation of 1.5K words. The number of unique
words used per lecture ranged from 500 to 1,100 words,
with an average of 800 words, and standard deviation
of 170 words. A preliminary assessment of spontaneous
speech phenomena showed that there tended to be fewer
(1) I?ve been talking ? I?ve been multiplying matrices
already, but certainly time for me to discuss the rules
for matrix multiplication.
(2) And the interesting part is the many ways you can
do it, and they all give the same answer.
(3) So it?s ? and they?re all important.
(4) So matrix multiplication, and then, uh, come in-
verses.
(5) So we?re ? uh, we ? mentioned the inverse of a
matrix, but there?s ? that?s a big deal.
(6) Lots to do about inverses and how to find them.
(7) Okay, so I?ll begin with how to multiply two ma-
trices.
(8) First way, okay, so suppose I have a matrix A mul-
tiplying a matrix B and ? giving me a result ? well, I
could call it C.
(9) A times B. Okay.
(10) Uh, so, l- let me just review the rule for w- for this
entry.
Figure 1: Transcript from a linear algebra lecture.
filled pauses than in Switchboard (1% vs. 3%), although
there were similar amounts of partial words (1%) and
contractions (3-4% vs. 5%) in the data we observed. It
is also clear that the behavior will very much depend on
the lecturer. However, on the basis of these results, we
hypothesize that in terms of spontaneous speech phenom-
ena, the lecture data is closer to Switchboard quality than
it is to a more carefully spoken corpus such as Broadcast
News.
As a preliminary examination of vocabulary usage, we
measured the out-of-vocabulary (OOV) rate of the lec-
ture material as a function of vocabulary size, where the
words in the vocabulary were the most frequently oc-
curring words for a given set of training data. Figure 2
displays the OOV rate vs. vocabulary size for a variety
of speech and text training sources on the latter half of
the computer science lectures (? 10hrs of speech). Each
curve plots the OOV rate as a function of the most fre-
quent words from a particular set of training material.
Curves (A), and (B) show the results using the 64K-word
Broadcast News, and 27K Switchboard lexicons, respec-
tively. Curve (C) was computed from the combined lec-
tures from a math and physics course. The remaining
curves were all computed from subject-specific material.
Curve (D) was computed from a companion textbook,
while curve (E) was computed from the first half of the
computer science lectures. Curve (F) was computed from
a combination of the text and lecture transcripts from the
course (i.e., (D)+(E)).
If one considers the best vocabulary to be one that has
a small OOV rate and a small size, the best matching data
102 103 104 105
10?2
10?1
100
A
B
C
D
E
F
Vocabulary Size
O
O
V 
Ra
te
A: Broadcast News (64k)
B: Switchboard (27k)
C: math + physics lectures (7k)
D: CS companion textbook (5k)
E: CS lectures (3k)
F: CS textbook D + lectures E (6k)
Figure 2: Out-of-vocabulary (OOV) rate vs. vocabulary
size as a function of training material. Each curve plots
the OOV rate in lectures from the latter half of a computer
science (CS) course as a function of the most frequent
words from a particular set of training material. The vo-
cabularies for curves D?F utilize subject-specific material
from a textbook, and/or the first half of the CS lectures.
was obtained, not surprisingly, from subject-specific ma-
terial. Even material from non-subject-related lectures
match the test data better than data from general human-
human conversations or broadcast news. However, we
have also observed (not plotted) that a combination of
general lecture and conversational material, combined
with related text material, can produce behavior similar
to subject-specific speech material.
In order to examine the impact of language model
training data on predicting word usage in lecture material,
we created a 3.3K-word vocabulary exactly covering the
latter half of the computer science lectures. We then cre-
ated trigram language models from a variety of sources
(ignoring OOV words) using the SRILM Toolkit (Stol-
cke, 2002), and measured their perplexity on this data.
The results, as shown in Table 1, show again, not surpris-
ingly, that spoken material provides the most constraints.
Text material from Broadcast News or even the course
textbook are poor predictors of language usage. Models
of general human conversations do significantly better,
although data from general lectures is better than arbi-
trary conversations. It was interesting to observe that a
mixture of subject-specific textbook material and exam-
ple lectures provided the most constraints for new lec-
ture material, although there is still a considerable gap
between this and the case of training the language model
on the test set.
Finally, to investigate the nature of the OOV words for
a general vocabulary, we created a vocabulary of 1,568
words that were common to all three courses. Table 2
Training corpus Perplexity
Broadcast News (A) 380
Switchboard (B) 271
Other Lectures (C) 243
Course Textbook (D) 400
Subject-specific Lectures (E) 161
Textbook & Subject-specific Lectures (F) 137
Test-set Lectures 40
Table 1: Perplexities on CS lectures using trigrams cre-
ated from different training data. Trigram perplexities of
a 3.3K-word vocabulary trained with different text mate-
rials, and tested on 10hrs of CS lectures. Letter designa-
tions correspond to OOV measures plotted in Figure 2.
lists the ten most frequent subject-specific words for each
of the three courses (i.e., OOV words that were not in
the common vocabulary), along with the rank of each
of these words in the Broadcast News and Switchboard
corpora. Not surprisingly, these OOV words tend to be
subject-specific content words, and are likely to be im-
portant words for any kind of summarization or retrieval
task.
4 Preliminary Transcript Generation
The speech recognition processing that has been used to
generate transcripts of spoken lectures has largely been
based on large-vocabulary continuous speech recognition
technology (Hurst et al, 2002; Leeuwis et al, 2003;
Kawahara et al, 2003; Yokoyama et al, 2003). Lan-
guage modeling research has focused on mixing topic-
dependent textual source material (e.g., conference pa-
pers) with unrelated or topic-independent spoken mate-
rial (e.g., Switchboard data, or transcripts of other spoken
material) (Kato et al, 2000).
In our initial speech recognition experiments, we have
developed a recognizer that has been used to align the
transcripts with the speech signal for three courses (ap-
proximately 80 lectures) (Glass, 2003). Based on man-
ual examination, we believe that the alignments of the
16KHz wide-band speech are of good quality, and are
on par with previous alignments we have performed on
Broadcast News, Switchboard, as well as our own in-
ternal spontaneous speech corpora. Using these data as
training material, we have performed a baseline speech
recognition experiment on one course. Using a 5000
word vocabulary and trigram language model (perplex-
ity 120) derived from a portion of lecture transcriptions
and text book, we obtained a 33% word error rate on un-
seen lectures. This result is in line with other lecture word
error rates of 30-40% that have been reported in the liter-
ature (Leeuwis et al, 2003; Kawahara et al, 2003).
Computer Science Physics Linear Algebra
word BN SB word BN SB word BN SB
procedure 2683 5486 field 1029 890 matrix 23752 12918
expression 4211 6935 charge 1004 750 transpose 51305 25829
environment 1268 1055 magnetic 10599 15961 determinant 29023 ?
stream 5409 3210 electric 3520 1733 null 29431 ?
cons 14173 5385 force 434 922 eigenvalues ? ?
program 370 410 volts 33928 ? rows 12440 8272
procedures 3162 5487 energy 1386 1620 matrices ? ?
machine 2201 906 theta ? ? eigen ? ?
arguments 2279 3738 omega 24266 16279 orthogonal ? ?
cdr ? ? maximum 4107 3775 diagonal 34008 14916
Table 2: Top ten most frequent subject-specific words for three courses. Subject-specific words not contained in a
common 1.5K-word vocabulary. Frequency rank for 64K-word Broadcast News (BN) and 27K-word Switchboard
(SB) corpora also shown (? means never occurred).
5 Ongoing and Future Activities
The technical language of academic lectures and lack of
in-domain spoken data for training makes lecture tran-
scription a significant challenge, that will require new
methods for deriving a vocabulary and language model.
To enable effective use of comparable textual material as
a surrogate for in-domain spoken data, we plan to inves-
tigate techniques to transform written text into a conver-
sation style that can be used for language modelling. We
are also exploring a lecture-independent recognizer struc-
ture that uses a small number of words common to lec-
ture discourse along with a sub-word model to represent
subject-specific words.
Finally, we plan to continue to collect and compile
lecture material into a comprehensive annotated corpus.
It is our plan to make this resource available to the
research community, in the hope that it will facilitate
speech and language processing research in this area.
Acknowledgements Support for this research was
provided in part by the MIT/Microsoft iCampus Alliance
for Educational Technology.
References
M. Bacchiani, J. Hirschberg, A. Rosenberg, S. Whittaker,
D. Hindle, P. Isenhour, M. Jones, L. Stark, and G. Zam-
chick. 2001. SCANMail: Audio navigation in the
voicemail domain. In HLT2001, San Diego.
J. Foote. 1999. An overview of audio information re-
trieval. J. ACM Multimedia Systems, 7(1):2?10.
M. Franz, B. Ramabhadran, T. Ward, and M. Picheny.
2003. Automatic transcription and topic segmentation
of large spoken archives. In Proc. Eurospeech, pages
953?956, Geneva.
S. Furui. 2003. Recent advances in spontaneous speech
recognition and understanding. In Proc. IEEE Work-
shop on Spont. Speech Proc. and Rec., pages 1?6,
Tokyo.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer, Speech, and Lan-
guage, 17(2-3):137?152.
W. Hurst, T. Kreuzer, and M. Wiesenhutter. 2002. A
qualitative study towards using large vocabulary auto-
matic speech recognition to index recorded presenta-
tions for search and access over the web. In Proceed-
ings of IADIS WWW/Internet 2002 Conference, Lis-
boa, Portugal.
P. Jourlin, S. E. Johnson, K. S. Jones, and P. C. Wood-
land. 2000. Spoken document representations for
probabilistic retrieval. Speech Communication, 32(1-
2):21?36.
K. Kato, H. Nanjo, and T. Kawahara. 2000. Automatic
transcription of lecture speech using topic-independent
language modeling. In Proc. ICSLP, pages 162?165,
Beijing.
T. Kawahara, K. Shitaoka, T. Kitade, and H. Nanjo.
2003. Automatic indexing of key sentences for lecture
archives. In Proc. ASRU, pages 141?144, St. Thomas.
L. Lamel, F. Schiel, A. Fourcin, J. Mariani, and H. Till-
man. 1994. The translingual English database (TED).
In Proc. ICSLP, pages 1795?1798, Yokohama.
E. Leeuwis, M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the TED corpus
lectures. In Proc. ICASSP, Hong Kong.
J. Makhoul, F. Kubala, T. Leek, D. Liu, L. Nguyen,
R. Schwartz, and A. Srivastava. 2000. Speech and
language technologies for audio indexing and retrieval.
Proc. IEEE, 88(8):1338?1353.
S. Renals, D. Abberley, D. Kirby, and T. Robinson. 2000.
Indexing and retrieval of broadcast news. SpeechCom-
munication, 32(1-2):5?20.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. ICSLP, pages 901?904, Denver.
T. Yokoyama, T. Shinozaki, K. Iwano, and S. Furui.
2003. Unsupervised class-based language model adap-
tation for spontaneous speech recognition. In Proc.
ICASSP, pages 236?239, Hong Kong.
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Multimodal Home Entertainment Interface via a Mobile Device
Alexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie Seneff
Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean Liu
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar St, Cambridge, MA 02139 USA
http://www.sls.csail.mit.edu/
Abstract
We describe a multimodal dialogue system for
interacting with a home entertainment center
via a mobile device. In our working proto-
type, users may utilize both a graphical and
speech user interface to search TV listings,
record and play television programs, and listen
to music. The developed framework is quite
generic, potentially supporting a wide variety
of applications, as we demonstrate by integrat-
ing a weather forecast application. In the pro-
totype, the mobile device serves as the locus
of interaction, providing both a small touch-
screen display, and speech input and output;
while the TV screen features a larger, richer
GUI. The system architecture is agnostic to
the location of the natural language process-
ing components: a consistent user experience
is maintained regardless of whether they run
on a remote server or on the device itself.
1 Introduction
People have access to large libraries of digital con-
tent both in their living rooms and on their mobile
devices. Digital video recorders (DVRs) allow peo-
ple to record TV programs from hundreds of chan-
nels for subsequent viewing at home?or, increas-
ingly, on their mobile devices. Similarly, having
accumulated vast libraries of digital music, people
yearn for an easy way to sift through them from the
comfort of their couches, in their cars, and on the go.
Mobile devices are already central to accessing
digital media libraries while users are away from
home: people listen to music or watch video record-
ings. Mobile devices also play an increasingly im-
portant role in managing digital media libraries. For
instance, a web-enabled mobile phone can be used to
remotely schedule TV recordings through a web site
or via a custom application. Such management tasks
often prove cumbersome, however, as it is challeng-
ing to browse through listings for hundreds of TV
channels on a small display. Indeed, even on a large
screen in the living room, browsing alphabetically,
or by time and channel, for a particular show using
the remote control quickly becomes unwieldy.
Speech and multimodal interfaces provide a nat-
ural means of addressing many of these challenges.
It is effortless for people to say the name of a pro-
gram, for instance, in order to search for existing
recordings. Moreover, such a speech browsing ca-
pability is useful both in the living room and away
from home. Thus, a natural way to provide speech-
based control of a media library is through the user?s
mobile device itself.
In this paper we describe just such a prototype
system. A mobile phone plays a central role in pro-
viding a multimodal, natural language interface to
both a digital video recorder and a music library.
Users can interact with the system?presented as a
dynamic web page on the mobile browser?using
the navigation keys, the stylus, or spoken natural
language. In front of the TV, a much richer GUI is
also available, along with support for playing video
recordings and music.
In the prototype described herein, the mobile de-
vice serves as the locus of natural language in-
teraction, whether a user is in the living room or
walking down the street. Since these environments
may be very different in terms of computational re-
1
sources and network bandwidth, it is important that
the architecture allows for multiple configurations in
terms of the location of the natural language pro-
cessing components. For instance, when a device
is connected to a Wi-Fi network at home, recogni-
tion latency may be reduced by performing speech
and natural language processing on the home me-
dia server. Moreover, a powerful server may enable
more sophisticated processing techniques, such as
multipass speech recognition (Hetherington, 2005;
Chung et al, 2004), for improved accuracy. In sit-
uations with reduced network connectivity, latency
may be improved by performing speech recognition
and natural language processing tasks on the mobile
device itself. Given resource constraints, however,
less detailed acoustic and language models may be
required. We have developed just such a flexible ar-
chitecture, with many of the natural language pro-
cessing components able to run on either a server or
the mobile device itself. Regardless of the configu-
ration, a consistent user experience is maintained.
2 Related Work
Various academic researchers and commercial busi-
nesses have demonstrated speech-enabled interfaces
to entertainment centers. A good deal of the work
focuses on adding a microphone to a remote con-
trol, so that speech input may be used in addition
to a traditional remote control. Much commercial
work, for example (Fujita et al, 2003), tends to fo-
cus on constrained grammar systems, where speech
input is limited to a small set of templates corre-
sponding to menu choices. (Berglund and Johans-
son, 2004) present a remote-control based speech
interface for navigating an existing interactive tele-
vision on-screen menu, though experimenters man-
ually transcribed user utterances as they spoke in-
stead of using a speech recognizer. (Oh et al, 2007)
present a dialogue system for TV control that makes
use of concept spotting and statistical dialogue man-
agement to understand queries. A version of their
system can run independently on low-resource de-
vices such as PDAs; however, it has a smaller vo-
cabulary and supports a limited set of user utterance
templates. Finally, (Wittenburg et al, 2006) look
mainly at the problem of searching for television
programs using speech, an on-screen display, and a
remote control. They explore a Speech-In List-Out
interface to searching for episodes of television pro-
grams.
(Portele et al, 2003) depart from the model of
adding a speech interface component to an exist-
ing on-screen menu. Instead, they a create a tablet
PC interface to an electronic program guide, though
they do not use the television display as well. Users
may search an electronic program guide using con-
straints such as date, time, and genre; however, they
can?t search by title. Users can also perform typi-
cal remote-control tasks like turning the television
on and off, and changing the channel. (Johnston et
al., 2007) also use a tablet PC to provide an inter-
face to television content?in this case a database of
movies. The search can be constrained by attributes
such as title, director, or starring actors. The tablet
PC pen can be used to handwrite queries and to point
at items (such as actor names) while the user speaks.
We were also inspired by previous prototypes in
which mobile devices have been used in conjunc-
tion with larger, shared displays. For instance, (Paek
et al, 2004) demonstrate a framework for building
such applications. The prototype we demonstrate
here fits into their ?Jukebox? model of interaction.
Interactive workspaces, such as the one described in
(Johanson et al, 2002), also demonstrate the utility
of integrating mobile and large screen displays. Our
prototype is a departure from these systems, how-
ever, in that it provides for spoken interactions.
Finally, there is related work in the use of mobile
devices for various kinds of search. For instance, of-
ferings from Microsoft (Acero et al, 2008), Vlingo,1
and Promptu2 allow users to search for items like
businesses and songs using their mobile phones.
These applications differ from ours in that speech
is used only for search, without any accompanying
command and control capabilities. Also, these ser-
vices do not allow interaction with your own de-
vices at home. Efforts have been made to use mo-
bile devices for control of devices in the home, such
as in (Nichols and Myers, 2006), however these ef-
forts have not involved the use of speech as an input
modality.
1http://www.vlingo.com
2http://www.promptu.com
2
Na
vig
ati
on
 pa
d o
r s
tyl
us 
can
 
be
 us
ed
 to
 na
vig
ate
 G
UI
Sta
tus
 ic
on
 in
dic
ate
s c
on
ne
cti
on
 to
 se
rve
r, 
mi
cro
ph
on
e i
co
n w
he
n l
ist
en
ing
Ho
ld-
to-
tal
k 
bu
tto
n
Op
era we
b 
bro
ws
er
Sy
nc
hro
niz
ed
 se
lec
tio
n o
n m
ob
ile
 an
d 
tel
ev
isi
on
 di
spl
ay
s
Fir
efo
x
we
b b
row
ser
Ta
bs 
all
ow
 ea
sy 
sw
itc
hin
g 
am
on
g d
om
ain
s
Me
dia
 pl
ay
ba
ck
 
co
ntr
ols
Figure 1: User interface overview.
(a) Program Guide
(b) Scheduled Recordings
(c) Music Library
U: What?s on tonight at seven o?clock?
S: I found seventy entries. [Displays program guide.]
U: Are there any action movies on HBO tomorrow?
S: I found two entries. [Displays results.]
U: When is American Idol on?
S: I found three entries. [Displays results.]
U: Record the first one.
S: I have added American Idol to the list of programs to be
recorded. [Displays recording settings.]
U: Please record Nova on Wednesday night.
S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.]
U: Show all my scheduled recordings.
S: There are seven scheduled recordings. [Shows list.]
U: Delete BBC World News.
S: I have deleted the scheduled recording: BBC World
News. [Updates List.]
U: Show my videos.
S: [Displays list of videos.]
U: Delete Frasier.
S: I have deleted the recording Frasier. [Updates list.]
U: Play Grey?s Anatomy.
S: Playing Grey?s Anatomy. [Plays recording.]
U: Stop Playing.
S: Stopping. [Stops playback.]
U: [Clicks on Music tab.]
S: [Displays artist list.]
U: Show albums by Billie Holiday.
S: I found three albums by Billie Holiday. [Shows albums.]
U: Please play A Hard Rain?s A-Gonna Fall by Bob Dylan.
S: Playing A Hard Rain?s A-Gonna Fall. [Plays song.]
Figure 2: Screenshots and an example interaction. Utterances are labeled with U for user and S for system.
3
3 User Experience
Our current prototype system implements the basic
functionalities that one expects from a home enter-
tainment center. Users can navigate through and
record programs from the television?s electronic pro-
gram guide, manage recording settings, and play
recorded videos. They can also browse and listen to
selections from their music libraries. However, un-
like existing prototypes, ours employs a smartphone
with a navigation pad, touch-sensitive screen, and
built-in microphone as the remote control. Figure 1
provides an overview of the graphical user interface
on both the TV and mobile device.
Mirroring the TV?s on-screen display, the proto-
type system presents a reduced view on the mobile
device with synchronized cursors. Users can navi-
gate the hierarchical menu structure using the arrow
keys or directly click on the target item with the sty-
lus. While away from the living room, or when a
recording is playing full screen, users can browse
and manage their media libraries using only the mo-
bile device.
While the navigation pad and stylus are great for
basic navigation and control, searching for media
with specific attributes, such as title, remains cum-
bersome. To facilitate such interactions, the cur-
rent system supports spoken natural language inter-
actions. For example, the user can press the hold-to-
talk button located on the side of the mobile device
and ask ?What?s on the National Geographic Chan-
nel this afternoon?? to retrieve a list of shows with
the specified channel and time. The system responds
with a short verbal summary ?I found six entries on
January seventh? and presents the resulting list on
both the TV and mobile displays. The user can then
browse the list using the navigation pad or press the
hold-to-talk button to barge in with another com-
mand, e.g. ?Please record the second one.? Depress-
ing the hold-to-talk button not only terminates any
current spoken response, but also mutes the TV to
minimize interference with speech recognition. As
the previous example demonstrates, contextual in-
formation is used to resolve list position references
and disambiguate commands.
The speech interface to the user?s music library
works in a similar fashion. Users can search by
artist, album, and song name, and then play the
songs found. To demonstrate the extensibility of
the architecture, we have also integrated an exist-
ing weather information system (Zue et al, 2000),
which has been previously deployed as a telephony
application. Users simply click on the Weather tab
to switch to this domain, allowing them to ask a wide
range of weather queries. The system responds ver-
bally and with a simple graphical forecast.
To create a natural user experience, we designed
the multimodal interface to allow users to seam-
lessly switch among the different input modalities
available on the mobile device. Figure 2 demon-
strates an example interaction with the prototype, as
well as several screenshots of the user interface.
4 System Architecture
The system architecture is quite flexible with re-
gards to the placement of the natural language pro-
cessing components. Figure 3 presents two possible
configurations of the system components distributed
across the mobile device, home media server, and
TV display. In 3(a), all speech recognition and nat-
ural language processing components reside on the
server, with the mobile device acting as the micro-
phone, speaker, display, and remote control. In 3(b),
the speech recognizer, language understanding com-
ponent, language generation component, and text-
to-speech (TTS) synthesizer run on the mobile de-
vice. Depending on the capabilities of the mobile
device and network connection, different configu-
rations may be optimal. For instance, on a power-
ful device with slow network connection, recogni-
tion latency may be reduced by performing speech
recognition and natural language processing on the
device. On the other hand, streaming audio via a fast
wireless network to the server for processing may
result in improved accuracy.
In the prototype system, flexible and reusable
speech recognition and natural language processing
capabilities are provided via generic components de-
veloped and deployed in numerous spoken dialogue
systems by our group, with the exception of an off-
the-shelf speech synthesizer. Speech input from the
mobile device is recognized using the landmark-
based SUMMIT system (Glass, 2003). The result-
ing N-best hypotheses are processed by the TINA
language understanding component (Seneff, 1992).
4
Gala
xy
Spee
ch?R
ecog
nize
r
Lang
uage
?Und
ersta
ndin
g
Dialo
gue?
Man
ager
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Web
?
Serv
er
Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Gala
xy
Dialo
gue?
Man
ager
Web
?
Serv
er Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Spee
ch?R
ecog
nize
r
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Lang
uage
?Und
ersta
ndin
g
(a) (b)
Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server,
while in (b) processing is primarily performed on the device.
Based on the resulting meaning representation, the
dialogue manager (Polifroni et al, 2003) incorpo-
rates contextual information (Filisko and Seneff,
2003), and then determines an appropriate response.
The response consists of an update to the graph-
ical display, and a spoken system response which
is realized via the GENESIS (Baptist and Seneff,
2000) language generation module. To support on-
device processing, all the components are linked via
the GALAXY framework (Seneff et al, 1998) with
an additional Mobile Manager component responsi-
ble for coordinating the communication between the
mobile device and the home media server.
In the currently deployed system, we use a mo-
bile phone with a 624 MHz ARM processor run-
ning the Windows Mobile operating system and
Opera Mobile web browser. The TV program and
music databases reside on the home media server
running GNU/Linux. The TV program guide data
and recording capabilities are provided via MythTV,
a full-featured, open-source digital video recorder
software package.3 Daily updates to the program
guide information typically contain hundreds of
unique channel names and thousands of unique pro-
gram names. The music library is comprised of
5,000 songs from over 80 artists and 13 major gen-
res, indexed using the open-source text search en-
gine Lucene.4 Lastly, the TV display can be driven
by a web browser on either the home media server or
a separate computer connected to the server via a fast
Ethernet connection, for high quality video stream-
ing.
3http://www.mythtv.org/
4http://lucene.apache.org/
While the focus of this paper is on the natural lan-
guage processing and user interface aspects of the
system, our work is actually situated within a larger
collaborative project at MIT that also includes sim-
plified device configuration (Mazzola Paluska et al,
2008; Mazzola Paluska et al, 2006), transparent ac-
cess to remote servers (Ford et al, 2006), and im-
proved security.
5 Mobile Natural Language Components
Porting the implementation of the various speech
recognizer and natural language processing com-
ponents to mobile devices with limited computa-
tion and memory presents both a research and en-
gineering challenge. Instead of creating a small vo-
cabulary, fixed phrase dialogue system, we aim to
support?on the mobile device?the same flexible
and natural language interactions currently available
on our desktop, tablet, and telephony systems; see
e.g., (Gruenstein et al, 2006; Seneff, 2002; Zue et
al., 2000). In this section, we summarize our ef-
forts thus far in implementing the SUMMIT speech
recognizer and TINA natural language parser. Ports
of the GENESIS language generation system and of
our dialogue manager are well underway, and we ex-
pect to have these components working on the mo-
bile device in the near future.
5.1 PocketSUMMIT
To significantly reduce the memory footprint and
overall computation, we chose to reimplement our
segment-based speech recognizer from scratch, uti-
lizing fixed-point arithmetic, parameter quantiza-
tion, and bit-packing in the binary model files.
The resulting PocketSUMMIT recognizer (Hether-
5
ington, 2007) utilizes only the landmark features,
initially forgoing segment features such as phonetic
duration, as they introduce algorithmic complexities
for relatively small word error rate (WER) improve-
ments.
In the current system, we quantize the mean and
variance of each Gaussian mixture model dimension
to 5 and 3 bits, respectively. Such quantization not
only results in an 8-fold reduction in model size, but
also yields about a 50% speedup by enabling table
lookups for Gaussian evaluations. Likewise, in the
finite-state transducers (FSTs) used to represent the
language model, lexical, phonological, and class di-
phone constraints, quantizing the FST weights and
bit-packing not only compress the resulting binary
model files, but also reduce the processing time with
improved processor cache locality.
In the aforementioned TV, music, and weather do-
mains with a moderate vocabulary of a few thou-
sand words, the resulting PocketSUMMIT recog-
nizer performs in approximately real-time on 400-
600 MHz ARM processors, using a total of 2-4
MB of memory, including 1-2 MB for memory-
mapped model files. Compared with equivalent non-
quantized models, PocketSUMMIT achieves dra-
matic improvements in speed and memory while
maintaining comparable WER performance.
5.2 PocketTINA
Porting the TINA natural language parser to mobile
devices involved significant software engineering to
reduce the memory and computational requirements
of the core data structures and algorithms. TINA
utilizes a best-first search that explores thousands of
partial parses when processing an input utterance.
To efficiently manage memory allocation given the
unpredictability of pruning invalid parses (e.g. due
to subject-verb agreement), we implemented a mark
and sweep garbage collection mechanism. Com-
bined with a more efficient implementation of the
priority queue and the use of aggressive ?beam?
pruning, the resulting PocketTINA system provides
identical output as server-side TINA, but can parse
a 10-best recognition hypothesis list into the corre-
sponding meaning representation in under 0.1 sec-
onds, using about 2 MB of memory.
6 Rapid Dialogue System Development
Over the course of developing dialogue systems for
many domains, we have built generic natural lan-
guage understanding components that enable the
rapid development of flexible and natural spoken di-
alogue systems for novel domains. Creating such
prototype systems typically involves customizing
the following to the target domain: recognizer lan-
guage model, language understanding parser gram-
mar, context resolution rules, dialogue management
control script, and language generation rules.
Recognizer Language Model Given a new do-
main, we first identify a set of semantic classes
which correspond to the back-end application?s
database, such as artist, album, and genre. Ideally,
we would have a corpus of tagged utterances col-
lected from real users. However, when building pro-
totypes such as the one described here, little or no
training data is usually available. Thus, we create
a domain-specific context-free grammar to generate
a supplemental corpus of synthetic utterances. The
corpus is used to train probabilities for the natural
language parsing grammar (described immediately
below), which in turn is used to derive a class n-
gram language model (Seneff et al, 2003).
Classes in the language model which corre-
spond to contents of the database are marked as
dynamic, and are populated at runtime from the
database (Chung et al, 2004; Hetherington, 2005).
Database entries are heuristically normalized into
spoken forms. Pronunciations not in our 150,000
word lexicon are automatically generated (Seneff,
2007).
Parser Grammar The TINA parser uses a prob-
abilistic context-free grammar enhanced with sup-
port for wh-movement and grammatical agreement
constraints. We have developed a generic syntac-
tic grammar by examining hundreds of thousands
of utterances collected from real user interactions
with various existing dialogue systems. In addition,
we have developed libraries which parse and inter-
pret common semantic classes like dates, times, and
numbers. The grammar and semantic libraries pro-
vide good coverage for spoken dialogue systems in
database-query domains.
6
To build a grammar for a new domain, a devel-
oper extends the generic syntactic grammar by aug-
menting it with domain-specific semantic categories
and their lexical entries. A probability model which
conditions each node category on its left sibling and
parent is then estimated from a training corpus of
utterances (Seneff et al, 2003).
At runtime, the recognizer tags the hypothesized
dynamic class expansions with their class names,
allowing the parser grammar to be independent of
the database contents. Furthermore, each semantic
class is designated either as a semantic entity, or as
an attribute associated with a particular entity. This
enables the generation of a semantic representation
from the parse tree.
Dialogue Management & Language Generation
Once an utterance is recognized and parsed, the
meaning representation is passed to the context res-
olution and dialogue manager component. The con-
text resolution module (Filisko and Seneff, 2003)
applies generic and domain-specific rules to re-
solve anaphora and deixis, and to interpret frag-
ments and ellipsis in context. The dialogue man-
ager then interacts with the application back-end
and database, controlled by a script customized for
the domain (Polifroni et al, 2003). Finally, the
GENESIS module (Baptist and Seneff, 2000) ap-
plies domain-specific rules to generate a natural lan-
guage representation of the dialogue manager?s re-
sponse, which is sent to a speech synthesizer. The
dialogue manager also sends an update to the GUI,
so that, for example, the appropriate database search
results are displayed.
7 Mobile Design Challenges
Dialogue systems for mobile devices present a
unique set of design challenges not found in tele-
phony and desktop applications. Here we describe
some of the design choices made while developing
this prototype, and discuss their tradeoffs.
7.1 Client/Server Tradeoffs
Towards supporting network-less scenarios, we have
begun porting various natural language processing
components to mobile platforms, as discussed in
Section 5. Having efficient mobile implementations
further allows the natural language processing tasks
to be performed on either the mobile device or the
server. While building the prototype, we observed
that the Wi-Fi network performance can often be un-
predictable, resulting in erratic recognition latency
that occasionally exceeds on-device recognition la-
tency. However, utilizing the mobile processor for
computationally intensive tasks rapidly drains the
battery. Currently, the component architecture in the
prototype system is pre-configured. A more robust
implementation would dynamically adjust the con-
figuration to optimize the tradeoffs among network
use, CPU utilization, power consumption, and user-
perceived latency/accuracy.
7.2 Speech User Interface
As neither open-mic nor push-to-talk with automatic
endpoint detection is practical on mobile devices
with limited battery life, our prototype system em-
ploys a hold-to-talk hardware button for microphone
control. To guide users to speak commands only
while the button is depressed, a short beep is played
as an earcon both when the button is pushed and
released. Since users are less likely to talk over
short audio clips, the use of earcons mitigates the
tendency for users to start speaking before pushing
down the microphone button.
In the current system, media audio is played over
the TV speakers, whereas TTS output is sent to
the mobile device speakers. To reduce background
noise captured from the mobile device?s far-field mi-
crophone, the TV is muted while the microphone
button is depressed. Unlike telephony spoken di-
alogue systems where the recognizer has to con-
stantly monitor for barge-in, the use of a hold-to-
talk button significantly simplifies barge-in support,
while reducing power consumption.
7.3 Graphical User Interface
In addition to supporting interactive natural lan-
guage dialogues via the spoken user interface, the
prototype system implements a graphical user in-
terface (GUI) on the mobile device to supplement
the TV?s on-screen interface. To faciliate rapid pro-
totyping, we chose to implement both the mobile
and TV GUI using web pages with AJAX (Asyn-
chronous Javascript and XML) techniques, an ap-
proach we have leveraged in several existing mul-
timodal dialogue systems, e.g. (Gruenstein et al,
7
2006; McGraw and Seneff, 2007). The resulting in-
terface is largely platform-independent and allows
display updates to be ?pushed? to the client browser.
As many users are already familiar with the TV?s
on-screen interface, we chose to mirror the same in-
terface on the mobile device and synchronize the
selection cursor. However, unlike desktop GUIs,
mobile devices are constrained by a small display,
limited computational power, and reduced network
bandwidth. Thus, both the page layout and infor-
mation detail were adjusted for the mobile browser.
Although AJAX is more responsive than traditional
web technology, rendering large formatted pages?
such as the program guide grid?is often still un-
acceptably slow. In the current implementation, we
addressed this problem by displaying only the first
section of the content and providing a ?Show More?
button that downloads and renders the full content.
While browser-based GUIs expedite rapid prototyp-
ing, deployed systems may want to take advantage
of native interfaces specific to the device for more
responsive user interactions. Instead of limiting the
mobile interface to reflect the TV GUI, improved us-
ability may be obtained by designing the interface
for the mobile device first and then expanding the
visual content to the TV display.
7.4 Client/Server Communication
In the current prototype, communication between
the mobile device and the media server consists of
AJAX HTTP and XML-RPC requests. To enable
server-side ?push? updates, the client periodically
pings the server for messages. While such an im-
plementation provides a responsive user interface, it
quickly drains the battery and is not robust to net-
work outages resulting from the device being moved
or switching to power-saving mode. Reestablish-
ing connection with the server further introduces la-
tency. In future implementations, we would like to
examine the use of Bluetooth for lower power con-
sumption, and infrared for immediate response to
common controls and basic navigation.
8 Conclusions & Future Work
We have presented a prototype system that demon-
strates the feasibility of deploying a multimodal,
natural language interface on a mobile device for
browsing and managing one?s home media library.
In developing the prototype, we have experimented
with a novel role for a mobile device?that of a
speech-enabled remote control. We have demon-
strated a flexible natural language understanding ar-
chitecture, in which various processing stages may
be performed on either the server or mobile device,
as networking and processing power considerations
require.
While the mobile platform presents many chal-
lenges, it also provides unique opportunities.
Whereas desktop computers and TV remote controls
tend to be shared by multiple users, a mobile device
is typically used by a single individual. By collect-
ing and adapting to the usage data, the system can
personalize the recognition and understanding mod-
els to improve the system accuracy. In future sys-
tems, we hope to not only explore such adaptation
possibilities, but also study how real users interact
with the system to further improve the user interface.
Acknowledgments
This research is sponsored by the TParty Project,
a joint research program between MIT and Quanta
Computer, Inc.; and by Nokia, as part of a joint MIT-
Nokia collaboration. We are also thankful to three
anonymous reviewers for their constructive feed-
back.
References
A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proc. of ICASSP.
L. Baptist and S. Seneff. 2000. Genesis-II: A versatile
system for language generation in conversational sys-
tem applications. In Proc. of ICSLP.
A. Berglund and P. Johansson. 2004. Using speech and
dialogue for interactive TV navigation. Universal Ac-
cess in the Information Society, 3(3-4):224?238.
G. Chung, S. Seneff, C. Wang, and L. Hetherington.
2004. A dynamic vocabulary spoken dialogue inter-
face. In Proc. of INTERSPEECH, pages 327?330.
E. Filisko and S. Seneff. 2003. A context resolution
server for the GALAXY conversational systems. In
Proc. of EUROSPEECH.
B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,
F. Kaashoek, and R. Morris. 2006. Persistent personal
names for globally connected mobile devices. In Pro-
ceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation (OSDI ?06).
8
K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003.
A new digital TV interface employing speech recog-
nition. IEEE Transactions on Consumer Electronics,
49(3):765?769.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137?152.
A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable
and portable web-based multimodal dialogue interac-
tion with geographical databases. In Proc. of INTER-
SPEECH.
I. L. Hetherington. 2005. A multi-pass, dynamic-
vocabulary approach to real-time, large-vocabulary
speech recognition. In Proc. of INTERSPEECH.
I. L. Hetherington. 2007. PocketSUMMIT: Small-
footprint continuous speech recognition. In Proc. of
INTERSPEECH, pages 1465?1468.
B. Johanson, A. Fox, and T. Winograd. 2002. The in-
teractive workspaces project: Experiences with ubiq-
uitous computing rooms. IEEE Pervasive Computing,
1(2):67?74.
M. Johnston, L. F. D?Haro, M. Levine, and B. Renger.
2007. A multimodal interface for access to content in
the home. In Proc. of ACL, pages 376?383.
J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and
S. Ward. 2006. Reducing configuration overhead with
goal-oriented programming. In PerCom Workshops,
pages 596?599. IEEE Computer Society.
J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-
man, and S. Ward. 2008. Structured decomposition of
adapative applications. In Proc. of 6th IEEE Confer-
ence on Pervasive Computing and Communications.
I. McGraw and S. Seneff. 2007. Immersive second lan-
guage acquisition in narrow domains: A prototype IS-
LAND dialogue system. In Proc. of the Speech and
Language Technology in Education Workshop.
J. Nichols and B. A. Myers. 2006. Controlling home and
office appliances with smartphones. IEEE Pervasive
Computing, special issue on SmartPhones, 5(3):60?
67, July-Sept.
H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007.
An intelligent TV interface based on statistical dia-
logue management. IEEE Transactions on Consumer
Electronics, 53(4).
T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-
son, R. Logan, K. Toyama, and A. Wilson. 2004. To-
ward universal mobile interaction for shared displays.
In Proc. of Computer Supported Cooperative Work.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the automatic generation of mixed-initiative dialogue
systems from web content. In Proc. EUROSPEECH,
pages 193?196.
T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,
and J. te Vrugt. 2003. SmartKom-Home - an ad-
vanced multi-modal interface to home entertainment.
In Proc. of INTERSPEECH.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. IC-
SLP.
S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic in-
duction of n-gram language models from a natural lan-
guage grammar. In Proceedings of EUROSPEECH.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1):61?86.
S. Seneff. 2002. Response planning and generation in
the MERCURY flight reservation system. Computer
Speech and Language, 16:283?312.
S. Seneff. 2007. Reversible sound-to-letter/letter-to-
sound modeling based on syllable structure. In Proc.
of HLT-NAACL.
K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and
A. Vetro. 2006. The prospects for unrestricted speech
input for TV content search. In Proc. of AVI?06.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER: A
telephone-based conversational interface for weather
information. IEEE Transactions on Speech and Audio
Processing, 8(1), January.
9
