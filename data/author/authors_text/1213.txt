Improved-Edit-Distance Kernel for Chinese Relation Extraction
Wanxiang Che
School of Computer Sci. and Tech.
Harbin Institute of Technology
Harbin China, 150001
tliu@ir.hit.edu.cn
Jianmin Jiang, Zhong Su, Yue Pan
IBM CRL
Beijing China, 100085
{jiangjm, suzhong,
panyue}@cn.ibm.com
Ting Liu
School of Computer Sci. and Tech.
Harbin Institute of Technology
Harbin China, 150001
tliu@ir.hit.edu.cn
Abstract
In this paper, a novel kernel-based
method is presented for the problem
of relation extraction between named
entities from Chinese texts. The ker-
nel is defined over the original Chi-
nese string representations around par-
ticular entities. As a kernel func-
tion, the Improved-Edit-Distance (IED)
is used to calculate the similarity be-
tween two Chinese strings. By em-
ploying the Voted Perceptron and Sup-
port Vector Machine (SVM) kernel ma-
chines with the IED kernel as the clas-
sifiers, we tested the method by extract-
ing person-affiliation relation from Chi-
nese texts. By comparing with tradi-
tional feature-based learning methods,
we conclude that our method needs less
manual efforts in feature transformation
and achieves a better performance.
1 Introduction
Relation extraction (RE) is a basic and impor-
tant problem in information extraction field. It
extracts the relations among the named enti-
ties. Examples of relations are person-affiliation,
organization-location, and so on. For example, in
the Chinese sentence ?????IBM????
??? (Gerstner is the chairman of IBM Corpora-
tion.), the named entities are??? (people) and
IBM?? (organization). The relation between
them is person-affiliation.
Usually, we can regard RE as a classification
problem. All particular entity pairs are found
from a text and then decided whether they are a
relation which we need or not.
At the beginning, a number of manually en-
gineered systems were developed for RE prob-
lem (Aone and Ramos-Santacruz, 2000). The
automatic learning methods (Miller et al, 1998;
Soderland, 1999) are not necessary to have some-
one on hand with detailed knowledge of how the
RE system works, or how to write rules for it.
Usually, the machine learning method repre-
sents the NLP objects as feature vectors in the
feature extraction step. The methods are named
feature-based learning methods. But in many
cases, data cannot be easily represented explicitly
via feature vectors. For example, in most NLP
problems, the feature-based representations pro-
duce inherently local representations of objects,
for it is computationally infeasible to generate
features involving long-range dependencies. On
the other hand, finding the suitable features of a
particular problem is a heuristic work. Their ac-
quisition may waste a lot of time.
Different from the feature-based learning
methods, the kernel-based methods do not need
to extract the features from the original text, but
retain the original representation of objects and
use the objects in algorithms only via comput-
ing a kernel (similarity) function between a pair
of objects. Then the kernel-based methods use
existing learning algorithms with dual form, e.g.
the Voted Perceptron (Freund and Schapire, 1998)
or SVM (Cristianini and Shawe-Taylor, 2000), as
kernel machine to do the classification task.
132
Haussler (1999) and Watkins (1999) proposed
a new kernel method based on discrete structures
respectively. Lodhi et al (2002) used string ker-
nels to solve the text classification problem. Ze-
lenko et al (2003) used the kernel methods
for extracting relations from text. They defined
the kernel function over shallow parse represen-
tation of text. And the kernel method is used in
conjunction with the SVM and the Voted Percep-
tron learning algorithms for the task of extracting
person-affiliation and organization-location rela-
tions from text.
As mentioned above, the discrete structure ker-
nel methods are more suitable to RE problems
than the feature-based methods. But the string-
based kernel methods only consider the word
forms without their semantics. Shallow parser
based kernel methods need shallow parser sys-
tems. Because the performance of shallow parser
systems is not high enough until now, especially
for Chinese text, we cannot depend on it com-
pletely.
To cope with these problems, we propose the
Improved-Edit-Distance (IED) algorithm to cal-
culate the kernel (similarity) function. We con-
sider the semantic similarity between two words
in two strings and some structure information of
strings.
The rest of the paper is organized as follows. In
Section 2, we introduce the kernel-based machine
learning algorithms and their application in nat-
ural language processing problems. In Section 3,
we formalize the relation extraction problem as
a machine learning problem. In Section 4, we
give a novel kernel method, named the IED kernel
method. Section 5 describes the experiments and
results on a particular relation extraction problem.
In Section 6, we discuss the reason why the IED
based kernel method yields a better result than
other methods. Finally, in Section 7, we give the
conclusions and comments on the future work.
2 Kernel-based Machine Learning
Most machine learning methods represent an ob-
ject as a feature vector. They are well-known
feature-based learning methods.
Kernel methods (Cristianini and Shawe-Taylor,
2000) are an attractive alternative to feature-based
methods. The kernel methods retain the original
representation of objects and use the object only
via computing a kernel function between a pair
of objects. As we know, a kernel function is a
similarity function satisfying certain properties.
There are a number of learning algorithms that
can operate using only the dot product of exam-
ples. We call them kernel machines. For in-
stance, the Perceptron learning algorithm (Cris-
tianini and Shawe-Taylor, 2000), Support Vector
Machine (SVM) (Vapnik, 1998) and so on.
3 Relation Extraction Problem
We regard the RE problem as a classification
learning problem. We only consider the relation
between two entities in a sentence and no rela-
tions across sentences. For example, the sen-
tence ????????IBM??????
??? (President Bush met Gerstner, the chair-
man of IBM Corporation.) contains three enti-
ties,?? (people), ??? (people) and IBM?
? (organization). The three entities form two
candidate person-affiliation relation pairs: ??-
IBM?? and ???-IBM?? . The con-
texts of the entities pairs produce the examples
for the binary classification problem. Then, from
the context examples, a classifier can decide ??
?-IBM?? is a real person-affiliation relation
but ??-IBM?? is not.
3.1 Feature-based Methods
The feature-based methods have to transform the
context into features. Expert knowledge is re-
quired for deciding which elements or their com-
binations thereof are good features. Usually these
features? values are binary (0 or 1).
The feature-based methods will cost lots of la-
bor to find suitable features for a particular appli-
cation field. Another problem is that we can either
select only the local features with a small win-
dow or we will have to spend much more training
and test time. At the same time, the feature-based
methods will not use the combination of these fea-
tures.
3.2 Kernel-based Methods
Different from the feature-based methods, kernel-
based methods do not require much labor on ex-
tracting the suitable features. As explained in the
introduction to Section 2, we retain the original
133
string form of objects and consider the similarity
function between two objects. For the problem of
the person-affiliation relation extraction, the ob-
jects are the context around people and organiza-
tion with a fixed window size w. It means that
we get w words around each entity as the samples
in the classification problem. Again considering
the example ????????IBM?????
????, with w = 2, the object for the pair ?
?? (people) and IBM?? (organization) can
be written as ??? ? ORG ?? PEO ??
Through the objects transformed from the origi-
nal texts, we can calculate the similarity between
any two objects by using the kernel (similarity)
function.
For the Chinese relation extraction problem,
we must consider the semantic similarity between
words and the structure of strings while comput-
ing similarity. Therefore we must consider the
kernel function which has a good similarity mea-
sure. The methods for computing the similarity
between two strings are: the same-word based
method (Nirenburg et al, 1993), the thesaurus
based method (Qin et al, 2003), the Edit-Distance
method (Ristad and Yianilos, 1998) and the statis-
tical method (Chatterjee, 2001). We know that the
same-word based method cannot solve the prob-
lem of synonyms. The thesaurus based method
can overcome this difficulty but does not con-
sider the structure of the text. Although the Edit-
Distance method uses the structure of the text, it
also has the same problem of the replacement of
synonyms. As for the statistical method, it needs
large corpora of similarity text and thus is difficult
to use for realistic applications.
For the reasons described above, we propose a
novel Improved-Edit-Distance (IED) method for
calculating the similarity between two Chinese
strings.
4 IED Kernel Method
Like normal kernel methods, the new IED ker-
nel method includes two components: the ker-
nel function and the kernel machine. We use the
IED method to calculate the semantic similarity
between two Chinese strings as the kernel func-
tion. As for the kernel machine, we tested the
Voted Perceptron with dual form and SVM with a
customized kernel. In the following subsections,
(a) Edit-Distance (b) Improved-Edit-Distance
Figure 1: The comparison between the Edit-
Distance and the Improved-Edit-Distance
we will introduce the kernel function, the IED
method, and kernel machines.
4.1 Improved-Edit-Distance
Before the introduction to IED, we will give
a brief review of the classical Edit-Distance
method (Ristad and Yianilos, 1998).
The edit distance between two strings is de-
fined as: The minimum number of edit operations
necessary to transform one string into another.
There are three edit operations, Insert, Delete, and
Replace. For example, in Figure 1(a), the edit dis-
tance between ?????(like apples)? and ??
????(like bananas)? is 4, as indicated by the
four dotted lines.
As we see, the method of computing the edit
distance between two Chinese strings cannot re-
flect the actual situation. First, the Edit-Distance
method computes the similarity measured in Chi-
nese character. But in Chinese, most of the char-
acters have no concrete meanings, such as ???,
??? and so on. The single character cannot ex-
press the meanings of words. Second, the cost
of the Replace operation is different for different
words. For example, the operation of ??(love)?
being replace by ???(like)? should have a small
cost, because they are synonyms. At last, if there
are a few words being inserted into a string, the
meaning of it should not be changed too much.
Such as ?????(like apples)? and ?????
?(like sweet apples)? are very similar.
Based on the above idea, we provide the IED
method for computing the similarity between two
Chinese strings. It means that we will use Chinese
words as the basis of our measurement (instead of
characters). By using a thesaurus, the similarity
between two Chinese words can be computed. At
the same time, the cost of the Insert operation is
reduced.
Here, we use the CiLin (Mei et al, 1996) as
134
the thesaurus resource to compute the similarity
between two Chinese words. In CiLin, the se-
mantics of words are divided into High, Middle,
and Low classes to describe a semantic system
from general to special semantic. For example:
???(apple)? is Bh07, ???(banana)? is Bh07,
????(tomato)? is Bh06, and so on.
The semantic distance between word A and
word B can be defined as:
Dist(A, B) = min
a?A,b?B
dist(a, b)
where A and B are the semantic sets of word
A and word B respectively. The distance be-
tween semantic a and b is: dist(a, b) = 2 ?
(3 ? d), where d means that the semantic code
is different from the dth class. If the seman-
tic code is same, then the semantic distance is
0. Therefore, Dist(?????) = 0 and
Dist(??????) = 2.
Table 1 defines the variations of the edit dis-
tance on string ?AB? after doing various edit op-
erations. Where, ??? denotes one to four words,
?A? and ?B? are two words which user inputs. X?
denotes the synonyms of X.
Table 1: The Variations of Edit-Distance with AB
Rank Pattern
1 AB
2 A?B
3 AB?; A?B
4 A?B?; A??B
5 A?; B?
According to Table 1, we can define the cost of
various edit operations in IED. See Table 2, where
??? denotes the delete operation.
Table 2: The Cost of Edit Operation in IED
Edit Operation Cost
A?A 0
Insert 0.1
A?A? Dist(A, A?)/10 + 0.5
Others 1
By the redefinition of the cost of edit opera-
tions, the computation of IED between ????
?? and ??????? is as shown Figure 1(b),
where the Replace cost of ???????? is 0.5
and ????????? is 0.7. Thus the cost of IED
is 1.2. Compared with the cost of classical Edit-
Distance, the cost of IED is much more appropri-
ate in the actual situation.
We use dynamic programming to compute the
IED similar with the computing of edit distance.
In order to compute the similarity between two
strings, we should convert the distance value into
a similarity. Empirically, the maximal similarity
is set to be 10. The similarity is 10 minus the
improved edit distance of two Chinese strings.
4.2 Kernel Machines
We use the Voted Perceptron and SVM algorithms
as the kernel machines here.
The Voted Perceptron algorithm was described
in (Freund and Schapire, 1998). We used
SVMlight (Joachims, 1998) with custom kernel as
the implementation of the SVM method. In our
experiments, we just replaced the custom kernel
with the IED kernel function.
5 Experiments and Results
In this section, we show how to extract the
person-affiliation relation from text and give
some experimental results. It is relatively
straightforward to extend the IED kernel method
to other RE problems.
The corpus for our experiments comes from
Bejing Youth Daily1. We annotated about 500
news with named entities of PEO and ORG. We
selected 4,200 sentences (examples) with both
PEO and ORG pairs as described in Section 3.
There are about 1,200 positive examples and
3,000 negative examples. We took about 2,500
random examples as training data and the rest of
about 1,700 examples as test data.
5.1 Infection of Window Size in Kernel
Methods
The change of the performance of the IED kernel
method varying while the window size w is shown
in Table 3. Here the Voted Perceptron is used as
the kernel machine.
Our experimental results show that the IED
kernel method got the best performance with the
highest F -Score when the window size w =
1http://www.bjyouth.com/
135
2. As w grows, the Precision becomes higher.
With smaller w?s, the Recall becomes higher.
5.2 Comparison between Feature and
Kernel Methods
For the feature-based methods implementation,
we use the words which are around the PEO and
the ORG entities and their POS. The window size
is w (See Section 3). All examples can be trans-
formed into feature vectors. We used the regular-
ized winnow learning algorithm (Zhang, 2001) to
train on the training data and predict the test data.
From the experimental results, we find that when
w = 2, the performance of feature-based method
is highest.
The comparison of the performance between
the feature-based and the kernel-base methods is
shown in Table 4.
Figure 2 displays the change of F -Score for
different methods varying with the training data
size.
Figure 2: The learning curve (of F -Score) for the
person-affiliation relation, comparing IED kernel
with feature-based algorithms
Table 3: The Performance Effected by w
w Precision Recall F -Score
1 66.67% 92.68% 77.55%
2 93.55% 87.80% 90.85%
3 94.23% 74.36% 83.12%
Table 4: The Performance Comparison
Precision Recall F -Score
Regularized Winnow 75.90% 96.92% 85.14%
Voted Perceptron 93.55% 87.80% 90.85%
SVM 94.15% 88.38% 91.17%
From Table 4 and Figure 2, we can see
that the IED kernel methods perform better for
the person-affiliation relation extraction problem
than for the feature-based methods.
Figure 2 shows that the Voted Perceptron
method gets close to, but not as good as, the per-
formance to the SVM method on the RE problem.
But when using the method, we can save signifi-
cantly on computation time and programming ef-
fort.
6 Discussion
Our experimental results show that the kernel-
based and the feature-based methods can get the
best performance with the highest F -Score when
the window size w = 2. This shows that for re-
lation extraction problem, the two words around
entities are the most significant ones. On the other
hand, with w becoming bigger, the Precision be-
comes higher. And with w becoming smaller, the
Recall becomes higher.
From Table 4 and Figure 2, we can see that
the IED kernel methods perform very well for
the person-affiliation relation extraction. Further-
more, it does not need an expensive feature selec-
tion stage like feature-based methods. Because
the IED kernel method uses the semantic similar-
ity between words, it can get a better extension.
We can conclude that the IED kernel method re-
quires much fewer examples than feature-based
methods for achieving the same performance.
For example, there is a test sentence ???
? ?? ?? ? IBM?? ?? ? (Chairman
Hu Jintao met the CEO of IBM Corporation). The
feature-based method judges the ???-IBM?
? as a person-affiliation relation, because the
context around??? and IBM?? is similar
with the context of the person-affiliation relation.
However, the IED kernel method does the correct
judgment based on the structure information. For
this case the IED kernel method gets a higher pre-
cision. At the same time, because the IED kernel
method considers the extension of synonyms, its
recall does not decrease very much.
The speed is a practical problem in apply-
ing kernel-based methods. Kernel-based clas-
sifiers are relatively slow compared to feature-
based classifiers. The main reason is that the com-
puting of kernel (similarity) function takes much
136
time. Therefore, it becomes a key problem to im-
prove the efficiency of the computing of the ker-
nel function.
7 Conclusions
We presented a new approach for using kernel-
based machine learning methods for extracting re-
lations between named entities from Chinese text
sources. We define kernels over the original rep-
resentations of Chinese strings around the partic-
ular entities and use the IED method for comput-
ing the kernel function. The kernel-based meth-
ods need not transform the original expression of
objects into feature vectors, so the methods need
less manual efforts than the feature-based meth-
ods. We applied the Voted Perceptron and the
SVM learning method with custom kernels to ex-
tract the person-affiliation relations. The method
can be extended to extract other relations between
entities, such as organization-location, etc. We
also compared the performance of kernel-based
methods with that of feature-based methods, and
the experimental results show that kernel-based
methods are better than feature-based methods.
Acknowledgements
This research has been supported by National
Natural Science Foundation of China via grant
60435020 and IBM-HIT 2005 joint project.
References
Chinatsu Aone and Mila Ramos-Santacruz. 2000.
Rees: A large-scale relation and event extraction
system. In Proceedings of the 6th Applied Natural
Language Processing Conference, pages 76?83.
Niladri Chatterjee. 2001. A statistical approach
for similarity measurement between sentences for
EBMT. In Proceedings of Symposium on Transla-
tion Support Systems STRANS-2001, Indian Insti-
tute of Technology, Kanpur.
N. Cristianini and J. Shawe-Taylor. 2000. An Intro-
duction to Support Vector Machines. Cambridge
University Press, Cambirdge University.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algo-
rithm. In Computational Learning Theory, pages
209?217.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, 7,.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In Proceedings of ECML-98, number
1398, pages 137?142, Chemnitz, DE.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. J. Mach. Learn.
Res., 2:419?444.
Jiaju Mei, Yiming Lan, Yunqi Gao, and Hongxiang
Yin. 1996. Chinese Thesaurus Tongyici Cilin (2nd
Edtion). Shanghai Thesaurus Press, Shanghai.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract information?
BBN: Description of the SIFT system as used for
MUC. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7).
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993.
Two approaches to matching in example-based ma-
chine translation. In Proceedings of the Fifth In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 47?
57, Kyoto, Japan.
Bing Qin, Ting Liu, Yang Wang, Shifu Zheng, and
Sheng Li. 2003. Chinese question answering sys-
tem based on frequently asked questions. Jour-
nal of Harbin Institute of Technology, 10(35):1179?
1182.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522?
532.
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233?272.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley.
Chris Watkins. 1999. Dynamic alignment kernels.
Technical Report CSD-TR-98-11, 1,.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083?1106.
Tong Zhang. 2001. Regularized winnow methods.
In Advances in Neural Information Processing Sys-
tems 13, pages 703?709.
137
Fast Computing Grammar-driven Convolution Tree Kernel for
Semantic Role Labeling
Wanxiang Che1?, Min Zhang2, Ai Ti Aw2, Chew Lim Tan3, Ting Liu1, Sheng Li1
1School of Computer Science and Technology
Harbin Institute of Technology, China 150001
{car,tliu}@ir.hit.edu.cn, lisheng@hit.edu.cn
2Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore 119613
{mzhang,aaiti}@i2r.a-star.edu.sg
3School of Computing
National University of Singapore, Singapore 117543
tancl@comp.nus.edu.sg
Abstract
Grammar-driven convolution tree kernel
(GTK) has shown promising results for se-
mantic role labeling (SRL). However, the
time complexity of computing the GTK is
exponential in theory. In order to speed
up the computing process, we design two
fast grammar-driven convolution tree kernel
(FGTK) algorithms, which can compute the
GTK in polynomial time. Experimental re-
sults on the CoNLL-2005 SRL data show
that our two FGTK algorithms are much
faster than the GTK.
1 Introduction
Given a sentence, the task of semantic role labeling
(SRL) is to analyze the propositions expressed by
some target verbs or nouns and some constituents
of the sentence. In previous work, data-driven tech-
niques, including feature-based and kernel-based
learning methods, have been extensively studied for
SRL (Carreras and Ma`rquez, 2005).
Although feature-based methods are regarded as
the state-of-the-art methods and achieve much suc-
cess in SRL, kernel-based methods are more effec-
tive in capturing structured features than feature-
based methods. In the meanwhile, the syntactic
structure features hidden in a parse tree have been
suggested as an important feature for SRL and need
to be further explored in SRL (Gildea and Palmer,
2002; Punyakanok et al, 2005). Moschitti (2004)
?The work was mainly done when the author was a visiting
student at I2R
and Che et al (2006) are two reported work to use
convolution tree kernel (TK) methods (Collins and
Duffy, 2001) for SRL and has shown promising re-
sults. However, as a general learning algorithm, the
TK only carries out hard matching between two sub-
trees without considering any linguistic knowledge
in kernel design. To solve the above issue, Zhang
et al (2007) proposed a grammar-driven convolu-
tion tree kernel (GTK) for SRL. The GTK can uti-
lize more grammatical structure features via two
grammar-driven approximate matching mechanisms
over substructures and nodes. Experimental results
show that the GTK significantly outperforms the
TK (Zhang et al, 2007). Theoretically, the GTK
method is applicable to any problem that uses syn-
tax structure features and can be solved by the TK
methods, such as parsing, relation extraction, and so
on. In this paper, we use SRL as an application to
test our proposed algorithms.
Although the GTK shows promising results for
SRL, one big issue for the kernel is that it needs ex-
ponential time to compute the kernel function since
it need to explicitly list all the possible variations
of two sub-trees in kernel calculation (Zhang et al,
2007). Therefore, this method only works efficiently
on such kinds of datasets where there are not too
many optional nodes in production rule set. In order
to solve this computation issue, we propose two fast
algorithms to compute the GTK in polynomial time.
The remainder of the paper is organized as fol-
lows: Section 2 introduces the GTK. In Section 3,
we present our two fast algorithms for computing
the GTK. The experimental results are shown in Sec-
tion 4. Finally, we conclude our work in Section 5.
781
2 Grammar-driven Convolution Tree
Kernel
The GTK features with two grammar-driven ap-
proximate matching mechanisms over substructures
and nodes.
2.1 Grammar-driven Approximate Matching
Grammar-driven Approximate Substructure
Matching: the TK requires exact matching between
two phrase structures. For example, the two phrase
structures ?NP?DT JJ NN? (NP?a red car) and
?NP?DT NN? (NP?a car) are not identical, thus
they contribute nothing to the conventional kernel
although they share core syntactic structure property
and therefore should play the same semantic role
given a predicate. Zhang et al (2007) introduces
the concept of optional node to capture this phe-
nomenon. For example, in the production rule
?NP?DT [JJ] NP?, where [JJ] denotes an optional
node. Based on the concept of optional node, the
grammar-driven approximate substructure matching
mechanism is formulated as follows:
M(r1, r2) =
?
i,j
(IT (T ir1 , T jr2)? ?
ai+bj
1 ) (1)
where r1 is a production rule, representing a two-
layer sub-tree, and likewise for r2. T ir1 is the ith vari-
ation of the sub-tree r1 by removing one ore more
optional nodes, and likewise for T jr2 . IT (?, ?) is a bi-
nary function that is 1 iff the two sub-trees are iden-
tical and zero otherwise. ?1 (0 ? ?1 ? 1) is a small
penalty to penalize optional nodes. ai and bj stand
for the numbers of occurrence of removed optional
nodes in subtrees T ir1 and T jr2 , respectively.
M(r1, r2) returns the similarity (i.e., the kernel
value) between the two sub-trees r1 and r2 by sum-
ming up the similarities between all possible varia-
tions of the sub-trees.
Grammar-driven Approximate Node Match-
ing: the TK needs an exact matching between two
nodes. But, some similar POSs may represent simi-
lar roles, such as NN (dog) and NNS (dogs). Zhang
et al (2007) define some equivalent nodes that can
match each other with a small penalty ?2 (0 ? ?2 ?
1). This case is called node feature mutation. The
approximate node matching can be formulated as:
M(f1, f2) =
?
i,j
(If (f i1, f j2 )? ?ai+bj2 ) (2)
where f1 is a node feature, f i1 is the ith mutation of
f1 and ai is 0 iff f i1 and f1 are identical and 1 oth-
erwise, and likewise for f2 and bj . If (?, ?) is a func-
tion that is 1 iff the two features are identical and
zero otherwise. Eq. (2) sums over all combinations
of feature mutations as the node feature similarity.
2.2 The GTK
Given these two approximate matching mecha-
nisms, the GTK is defined by beginning with the
feature vector representation of a parse tree T as:
??(T ) = (#subtree1(T ), . . . ,#subtreen(T ))
where #subtreei(T ) is the occurrence number of
the ith sub-tree type (subtreei) in T . Now the GTKis defined as follows:
KG(T1, T2) = ???(T1),??(T2)?
=?i #subtreei(T1) ?#subtreei(T2)=?i((
?
n1?N1 I
?
subtreei(n1))
? (?n2?N2 I
?
subtreei(n2)))
=?n1?N1
?
n2?N2 ?
?(n1, n2)
(3)
where N1 and N2 are the sets of nodes in trees T1
and T2, respectively. I ?subtreei(n) is a function that
is ?a1 ??b2 iff there is a subtreei rooted at node n and
zero otherwise, where a and b are the numbers of
removed optional nodes and mutated node features,
respectively. ??(n1, n2) is the number of the com-
mon subtrees rooted at n1 and n2, i.e.,
??(n1, n2) =
?
i
I ?subtreei(n1) ? I ?subtreei(n2) (4)
??(n1, n2) can be further computed by the follow-
ing recursive rules:
R-A: if n1 and n2 are pre-terminals, then:
??(n1, n2) = ??M(f1, f2) (5)
where f1 and f2 are features of nodes n1 and n2
respectively, and M(f1, f2) is defined in Eq. (2),
which can be computed in linear time O(n), where
n is the number of feature mutations.
R-B: else if both n1 and n2 are the same non-terminals, then generate all variations of sub-trees
of depth one rooted at n1 and n2 (denoted by Tn1
782
and Tn2 respectively) by removing different optionalnodes, then:
??(n1, n2) = ??
?
i,j IT (T in1 , T jn2)? ?
ai+bj
1
??nc(n1,i)k=1 (1 + ??(ch(n1, i, k), ch(n2, j, k)))
(6)
where T in1 , T jn2 , IT (?, ?), ai and bj have been ex-
plained in Eq. (1). nc(n1, i) returns the number
of children of n1 in its ith subtree variation T in1 .
ch(n1, i, k) is the kth child of node n1 in its ith vari-
ation subtree T in1 , and likewise for ch(n2, j, k). ?
(0 < ? < 1) is the decay factor.
R-C: else ??(n1, n2) = 0
3 Fast Computation of the GTK
Clearly, directly computing Eq. (6) requires expo-
nential time, since it needs to sum up all possible
variations of the sub-trees with and without optional
nodes. For example, supposing n1 = ?A?a [b] c
[d]?, n2 = ?A?a b c?. To compute the Eq. (6), we
have to list all possible variations of n1 and n2?s sub-
trees, n1: ?A?a b c d?, ?A?a b c?, ?A?a c d?, ?A?a
c?; n2: ?A?a b c?. Unfortunately, Zhang et al
(2007) did not give any theoretical solution for the
issue of exponential computing time. In this paper,
we propose two algorithms to calculate it in polyno-
mial time. Firstly, we recast the issue of computing
Eq. (6) as a problem of finding common sub-trees
with and without optional nodes between two sub-
trees. Following this idea, we rewrite Eq. (6) as:
??(n1, n2) = ?? (1 +
lm?
p=lx
?p(cn1 , cn2)) (7)
where cn1 and cn2 are the child node sequences of
n1 and n2, ?p evaluates the number of common
sub-trees with exactly p children (at least including
all non-optional nodes) rooted at n1 and n2, lx =
max{np(cn1), np(cn2)} and np(?) is the number of
non-optional nodes, lm = min{l(cn1), l(cn2)}and
l(?) returns the number of children.
Now let?s study how to calculate ?p(cn1 , cn2) us-
ing dynamic programming algorithms. Here, we
present two dynamic programming algorithms to
compute it in polynomial time.
3.1 Fast Grammar-driven Convolution Tree
Kernel I (FGTK-I)
Our FGTK-I algorithm is motivated by the string
subsequence kernel (SSK) (Lodhi et al, 2002).
Given two child node sequences sx = cn1 andt = cn2 (x is the last child), the SSK uses the fol-lowing recursive formulas to evaluate the ?p:
??0(s, t) = 1, for all s, t,
??p(s, t) = 0, ifmin(|s|, |t|) < p, (8)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (9)
??p(sx, t) = ????p(sx, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?|t|?j+2)),(10)
p = 1, . . . , n? 1,
?p(sx, t) = ?p(s, t) +?
j:tj=x
(??p?1(s, t[1 : j ? 1]? ?2)). (11)
where ??p is an auxiliary function since it is only
the interior gaps in the subsequences that are penal-
ized; ? is a decay factor only used in the SSK for
weighting each extra length unit. Lodhi et al (2002)
explained the correctness of the recursion defined
above.
Compared with the SSK kernel, the GTK has
three different features:
f1: In the GTK, only optional nodes can be
skipped while the SSK kernel allows any node skip-
ping;
f2: The GTK penalizes skipped optional nodes
only (including both interior and exterior skipped
nodes) while the SSK kernel weights the length of
subsequences (all interior skipped nodes are counted
in, but exterior nodes are ignored);
f3: The GTK needs to further calculate the num-
ber of common sub-trees rooted at each two match-
ing node pair x and t[j].
To reflect the three considerations, we modify the
SSK kernel as follows to calculate the GTK:
?0(s, t) = opt(s)? opt(t)? ?|s|+|t|1 , for all s, t, (12)
?p(s, t) = 0, ifmin(|s|, |t|) < p, (13)
?p(sx, t) = ?1 ??p(sx, t)? opt(x)
+
?
j:tj=x
(?p?1(s, t[1 : j ? 1])? ?|t|?j (14)
?opt(t[j + 1 : |t|])???(x, t[j])).
where opt(w) is a binary function, which is 0 if
non-optional nodes are found in the node sequence
w and 1 otherwise (f1); ?1 is the penalty to penalize
skipped optional nodes and the power of ?1 is the
number of skipped optional nodes (f2); ??(x, t[j])
is defined in Eq. (7) (f3). Now let us compare
783
the FGTK-I and SSK kernel algorithms. Based on
Eqs. (8), (9), (10) and (11), we introduce the opt(?)
function and the penalty ?1 into Eqs. (12), (13) and
(14), respectively. opt(?) is to ensure that in the
GTK only optional nodes are allowed to be skipped.
And only those skipped optional nodes are penal-
ized with ?1. Please note that Eqs. (10) and (11)
are merged into Eq. (14) because of the different
meaning of ? and ?1. From Eq. (8), we can see
that the current path in the recursive call will stop
and its value becomes zero once non-optional node
is skipped (when opt(w) = 0).
Let us use a sample of n1 = ?A?a [b] c [d]?, n2 =
?A?a b c? to exemplify how the FGTK-I algorithm
works. In Eq. (14)?s vocabulary, we have s = ?a [b]
c?, t = ?a b c?, x = ?[d]?, opt(x) = opt([d]) = 1,
p = 3. Then according to Eq (14), ?p(cn1 , cn2) can
be calculated recursively as Eq. (15) (Please refer to
the next page).
Finally, we have ?p(cn1 , cn2) = ?1 ???(a, a)?
??(b, b)???(c, c)
By means of the above algorithm, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |2) (Lodhi et
al., 2002). This means that the worst case complex-
ity of the FGTK-I is O(p?3|N1| ? |N2|2), where ? is
the maximum branching factor of the two trees.
3.2 Fast Grammar-driven Convolution Tree
Kernel II (FGTK-II)
Our FGTK-II algorithm is motivated by the partial
trees (PTs) kernel (Moschitti, 2006). The PT kernel
algorithm uses the following recursive formulas to
evaluate ?p(cn1 , cn2):
?p(cn1 , cn2) =
|cn1 |?
i=1
|cn2 |?
j=1
??p(cn1 [1 : i], cn2 [1 : j]) (16)
where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to i and from 1to j, respectively. Given two child node sequences
s1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and b are
the last children), the PT kernel computes ??p(?, ?) as
follows:
??p(s1a, s2b) =
{
?2??(a, b)Dp(|s1|, |s2|) if a = b
0 else (17)
where ??(a, b) is defined in Eq. (7) and Dp is recur-
sively defined as follows:
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?Dp(k, l ? 1) + ?Dp(k ? 1, l) (18)
??2Dp(k ? 1, l ? 1)
D1(k, l) = 1, for all k, l (19)
where ? used in Eqs. (17) and (18) is a factor to
penalize the length of the child sequences.
Compared with the PT kernel, the GTK has two
different features which are the same as f1 and f2
when defining the FGTK-I.
To reflect the two considerations, based on the PT
kernel algorithm, we define another fast algorithm
of computing the GTK as follows:
?p(cn1 , cn2 ) =
? |cn1 |
i=1
? |cn2 |
j=1 ??p(cn1 [1 : i], cn2 [1 : j])
?opt(cn1 [i+ 1 : |cn1 |])?opt(cn2 [j + 1 : |cn2 |])
??|cn1 |?i+|cn2 |?j1
(20)
??p(s1a, s2b) =
{ ??(a, b)Dp(|s1|, |s2|) if a = b
0 else (21)
Dp(k, l) = ??p?1(s1[1 : k], s2[1 : l])
+?1Dp(k, l ? 1)? opt(s2[l]) (22)
+?1Dp(k ? 1, l)? opt(s1[k])
??21Dp(k ? 1, l ? 1)? opt(s1[k])? opt(s2[l])
D1(k, l) = ?k+l1 ? opt(s1[1 : k])? opt(s2[1 : l]), (23)
for all k, l
??p(s1, s2) = 0, if min(|s1|, |s2|) < p (24)
where opt(w) and ?1 are the same as them in the
FGTK-I.
Now let us compare the FGTK-II and the PT al-
gorithms. Based on Eqs. (16), (18) and (19), we in-
troduce the opt(?) function and the penalty ?1 into
Eqs. (20), (22) and (23), respectively. This is to
ensure that in the GTK only optional nodes are al-
lowed to be skipped and only those skipped optional
nodes are penalized. In addition, compared with
Eq. (17), the penalty ?2 is removed in Eq. (21) in
view that our kernel only penalizes skipped nodes.
Moreover, Eq. (24) is only for fast computing. Fi-
nally, the same as the FGTK-I, in the FGTK-II the
current path in a recursive call will stop and its value
becomes zero once non-optional node is skipped
(when opt(w) = 0). Here, we still can use an ex-
ample to derivate the process of the algorithm step
by step as that for FGTK-I algorithm. Due to space
limitation, here, we do not illustrate it in detail.
By means of the above algorithms, we can com-
pute the ??(n1, n2) in O(p|cn1 | ? |cn2 |) (Moschitti,
784
?p(cn1 , cn2 ) = ?p(?a [b] c [d]? , ?a b c?)
= ?1 ??p(?a [b] c?, ?a b c?) + 0 //Since x * t, the second term is 0
= ?1 ? (0 + ?p?1(?a [b]?, ?a b?)? ?3?31 ???(c, c)) //Since opt(?c?) = 0, the first term is 0
= ?1 ???(c, c)? (0 + ?p?2(?a?, ?a b?)? ?2?21 ???(b, b)) //Since p? 1 > |?a?|,?p?2(?a?, ?a b?) = 0
= ?1 ???(c, c)? (0 + ??(a, a)???(b, b)) //?p?2(?a?, ?a?) = ??(a, a)
(15)
2006). This means that the worst complexity of the
FGTK-II is O(p?2|N1| ? |N2|). It is faster than the
FGTK-I?s O(p?3|N1| ? |N2|2) in theory. Please note
that the average ? in natural language parse trees is
very small and the overall complexity of the FGTKs
can be further reduced by avoiding the computation
of node pairs with different labels (Moschitti, 2006).
4 Experiments
4.1 Experimental Setting
Data: We use the CoNLL-2005 SRL shared task
data (Carreras and Ma`rquez, 2005) as our experi-
mental corpus.
Classifier: SVM (Vapnik, 1998) is selected as our
classifier. In the FGTKs implementation, we mod-
ified the binary Tree Kernels in SVM-Light Tool
(SVM-Light-TK) (Moschitti, 2006) to a grammar-
driven one that encodes the GTK and the two fast dy-
namic algorithms inside the well-known SVM-Light
tool (Joachims, 2002). The parameters are the same
as Zhang et al (2007).
Kernel Setup: We use Che et al (2006)?s hybrid
convolution tree kernel (the best-reported method
for kernel-based SRL) as our baseline kernel. It is
defined as Khybrid = ?Kpath + (1 ? ?)Kcs (0 ?
? ? 1)1. Here, we use the GTK to compute the
Kpath and the Kcs.
In the training data (WSJ sections 02-21), we get
4,734 production rules which appear at least 5 times.
Finally, we use 1,404 rules with optional nodes for
the approximate structure matching. For the node
approximate matching, we use the same equivalent
node sets as Zhang et al (2007).
4.2 Experimental Results
We use 30,000 instances (a subset of the entire train-
ing set) as our training set to compare the different
kernel computing algorithms 2. All experiments are
1Kpath and Kcs are two TKs to describe predicate-
argument link features and argument syntactic structure fea-
tures, respectively. For details, please refer to (Che et al, 2006).
2There are about 450,000 identification instances are ex-
tracted from training data.
conducted on a PC with CPU 2.8GH and memory
1G. Fig. 1 reports the experimental results, where
training curves (time vs. # of instances) of five
kernels are illustrated, namely the TK, the FGTK-
I, the FGTK-II, the GTK and a polynomial kernel
(only for reference). It clearly demonstrates that our
FGTKs are faster than the GTK algorithm as ex-
pected. However, the improvement seems not so
significant. This is not surprising as there are only
30.4% rules (1,404 out of 4,734)3 that have optional
nodes and most of them have only one optional
node4. Therefore, in this case, it is not time con-
suming to list all the possible sub-tree variations and
sum them up. Let us study this issue from computa-
tional complexity viewpoint. Suppose all rules have
exactly one optional node. This means each rule can
only generate two variations. Therefore computing
Eq. (6) is only 4 times (2*2) slower than the GTK
in this case. In other words, we can say that given
the constraint that there is only one optional node
in one rule, the time complexity of the GTK is also
O(|N1| ? |N2|) 5, where N1 and N2 are the numbers
of tree nodes, the same as the TK.
12000
6000
8000
10000
Train
ing T
ime (
S) GTKFGTK-I
2000
4000Tra
ining
 Time
 (S)
FGTK-IITKPoly
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 1: Training time comparison among different
kernels with rule set having less optional nodes.
Moreover, Fig 1 shows that the FGTK-II is faster
than the FGTK-I. This is reasonable since as dis-
3The percentage is even smaller if we consider all produc-
tion (it becomes 14.4% (1,404 out of 9,700)).
4There are 1.6 optional nodes in each rule averagely.
5Indeed it is O(4 ? |N1| ? |N2|). The parameter 4 is omitted
when discussing time complexity.
785
cussed in Subsection 3.2, the FGTK-I?s time com-
plexity is O(p?3|N1| ? |N2|2) while the FGTK-II?s is
O(p?2|N1| ? |N2|).
40000
45000
20000
25000
30000
35000
Train
ing T
ime (
S) GTKFGTK-I
0
5000
10000
15000Trai
ning 
Time
 (S)
FGTK-IITKPoly
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Number of Training Instances (103)
Figure 2: Training time comparison among different
kernels with rule set having more optional nodes.
To further verify the efficiency of our proposed
algorithm, we conduct another experiment. Here we
use the same setting as that in Fig 1 except that we
randomly add more optional nodes in more produc-
tion rules. Table 1 reports the statistics on the two
rule set. Similar to Fig 1, Fig 2 compares the train-
ing time of different algorithms. We can see that
Fig 2 convincingly justify that our algorithms are
much faster than the GTK when the experimental
data has more optional nodes and rules.
Table 1: The rule set comparison between two ex-
periments.
# rules # rule with at
least optional
nodes
# op-
tional
nodes
# average op-
tional nodes per
rule
Exp1 4,734 1,404 2,242 1.6
Exp2 4,734 4,520 10,451 2.3
5 Conclusion
The GTK is a generalization of the TK, which can
capture more linguistic grammar knowledge into the
later and thereby achieve better performance. How-
ever, a biggest issue for the GTK is its comput-
ing speed, which needs exponential time in the-
ory. Therefore, in this paper we design two fast
grammar-driven convolution tree kennel (FGTK-I
and II) algorithms which can compute the GTK in
polynomial time. The experimental results show that
the FGTKs are much faster than the GTK when data
set has more optional nodes. We conclude that our
fast algorithms enable the GTK kernel to easily scale
to larger dataset. Besides the GTK, the idea of our
fast algorithms can be easily used into other similar
problems.
To further our study, we will use the FGTK algo-
rithms for other natural language processing prob-
lems, such as word sense disambiguation, syntactic
parsing, and so on.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for seman-
tic role labeling. In Proceedings of the COLING/ACL
2006, Sydney, Australia, July.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS-
2001.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL-2002, pages 239?246.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL-2004, pages 335?342.
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling case.
In Proceedings of the HHLT-NAACL-2006, June.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role la-
beling. In Proceedings of IJCAI-2005.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for semantic
role classification. In Proceedings of ACL-2007, pages
200?207.
786
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73?80,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Hybrid Convolution Tree Kernel for Semantic Role Labeling
Wanxiang Che
Harbin Inst. of Tech.
Harbin, China, 150001
car@ir.hit.edu.cn
Min Zhang
Inst. for Infocomm Research
Singapore, 119613
mzhang@i2r.a-star.edu.sg
Ting Liu, Sheng Li
Harbin Inst. of Tech.
Harbin, China, 150001
{tliu, ls}@ir.hit.edu.cn
Abstract
A hybrid convolution tree kernel is pro-
posed in this paper to effectively model
syntactic structures for semantic role la-
beling (SRL). The hybrid kernel consists
of two individual convolution kernels: a
Path kernel, which captures predicate-
argument link features, and a Constituent
Structure kernel, which captures the syn-
tactic structure features of arguments.
Evaluation on the datasets of CoNLL-
2005 SRL shared task shows that the
novel hybrid convolution tree kernel out-
performs the previous tree kernels. We
also combine our new hybrid tree ker-
nel based method with the standard rich
flat feature based method. The experi-
mental results show that the combinational
method can get better performance than
each of them individually.
1 Introduction
In the last few years there has been increasing in-
terest in Semantic Role Labeling (SRL). It is cur-
rently a well defined task with a substantial body
of work and comparative evaluation. Given a sen-
tence, the task consists of analyzing the proposi-
tions expressed by some target verbs and some
constituents of the sentence. In particular, for each
target verb (predicate) all the constituents in the
sentence which fill a semantic role (argument) of
the verb have to be recognized.
Figure 1 shows an example of a semantic role
labeling annotation in PropBank (Palmer et al,
2005). The PropBank defines 6 main arguments,
Arg0 is the Agent, Arg1 is Patient, etc. ArgM-
may indicate adjunct arguments, such as Locative,
Temporal.
Many researchers (Gildea and Jurafsky, 2002;
Pradhan et al, 2005a) use feature-based methods

 

 	

 

 



 

 	

Figure 1: Semantic role labeling in a phrase struc-
ture syntactic tree representation
for argument identification and classification in
building SRL systems and participating in eval-
uations, such as Senseval-3 1, CoNLL-2004 and
2005 shared tasks: SRL (Carreras and Ma`rquez,
2004; Carreras and Ma`rquez, 2005), where a
flat feature vector is usually used to represent a
predicate-argument structure. However, it?s hard
for this kind of representation method to explicitly
describe syntactic structure information by a vec-
tor of flat features. As an alternative, convolution
tree kernel methods (Collins and Duffy, 2001)
provide an elegant kernel-based solution to im-
plicitly explore tree structure features by directly
computing the similarity between two trees. In
addition, some machine learning algorithms with
dual form, such as Perceptron and Support Vector
Machines (SVM) (Cristianini and Shawe-Taylor,
2000), which do not need know the exact presen-
tation of objects and only need compute their ker-
nel functions during the process of learning and
prediction. They can be well used as learning al-
gorithms in the kernel-based methods. They are
named kernel machines.
In this paper, we decompose the Moschitti
(2004)?s predicate-argument feature (PAF) kernel
into a Path kernel and a Constituent Structure ker-
1http://www.cs.unt.edu/?rada/senseval/senseval3/
73
nel, and then compose them into a hybrid con-
volution tree kernel. Our hybrid kernel method
using Voted Perceptron kernel machine outper-
forms the PAF kernel in the development sets of
CoNLL-2005 SRL shared task. In addition, the fi-
nal composing kernel between hybrid convolution
tree kernel and standard features? polynomial ker-
nel outperforms each of them individually.
The remainder of the paper is organized as fol-
lows: In Section 2 we review the previous work.
In Section 3 we illustrate the state of the art
feature-based method for SRL. Section 4 discusses
our method. Section 5 shows the experimental re-
sults. We conclude our work in Section 6.
2 Related Work
Automatic semantic role labeling was first intro-
duced by Gildea and Jurafsky (2002). They used
a linear interpolation method and extract features
from a parse tree to identify and classify the con-
stituents in the FrameNet (Baker et al, 1998) with
syntactic parsing results. Here, the basic features
include Phrase Type, Parse Tree Path, Position.
Most of the following works focused on feature
engineering (Xue and Palmer, 2004; Jiang et al,
2005) and machine learning models (Nielsen and
Pradhan, 2004; Pradhan et al, 2005a). Some
other works paid much attention to the robust SRL
(Pradhan et al, 2005b) and post inference (Pun-
yakanok et al, 2004).
These feature-based methods are considered as
the state of the art method for SRL and achieved
much success. However, as we know, the standard
flat features are less effective to model the syntac-
tic structured information. It is sensitive to small
changes of the syntactic structure features. This
can give rise to a data sparseness problem and pre-
vent the learning algorithms from generalizing un-
seen data well.
As an alternative to the standard feature-based
methods, kernel-based methods have been pro-
posed to implicitly explore features in a high-
dimension space by directly calculating the sim-
ilarity between two objects using kernel function.
In particular, the kernel methods could be effective
in reducing the burden of feature engineering for
structured objects in NLP problems. This is be-
cause a kernel can measure the similarity between
two discrete structured objects directly using the
original representation of the objects instead of ex-
plicitly enumerating their features.
Many kernel functions have been proposed in
machine learning community and have been ap-
plied to NLP study. In particular, Haussler (1999)
and Watkins (1999) proposed the best-known con-
volution kernels for a discrete structure. In the
context of convolution kernels, more and more
kernels for restricted syntaxes or specific do-
mains, such as string kernel for text categoriza-
tion (Lodhi et al, 2002), tree kernel for syntactic
parsing (Collins and Duffy, 2001), kernel for re-
lation extraction (Zelenko et al, 2003; Culotta
and Sorensen, 2004) are proposed and explored
in NLP domain. Of special interest here, Mos-
chitti (2004) proposed Predicate Argument Fea-
ture (PAF) kernel under the framework of convo-
lution tree kernel for SRL. In this paper, we fol-
low the same framework and design a novel hybrid
convolution kernel for SRL.
3 Feature-based methods for SRL
Usually feature-based methods refer to the meth-
ods which use the flat features to represent in-
stances. At present, most of the successful SRL
systems use this method. Their features are usu-
ally extended from Gildea and Jurafsky (2002)?s
work, which uses flat information derived from
a parse tree. According to the literature, we
select the Constituent, Predicate, and Predicate-
Constituent related features shown in Table 1.
Feature Description
Constituent related features
Phrase Type syntactic category of the constituent
Head Word head word of the constituent
Last Word last word of the constituent
First Word first word of the constituent
Named Entity named entity type of the constituent?s head word
POS part of speech of the constituent
Previous Word sequence previous word of the constituent
Next Word sequence next word of the constituent
Predicate related features
Predicate predicate lemma
Voice grammatical voice of the predicate, either active or passive
SubCat Sub-category of the predicate?s parent node
Predicate POS part of speech of the predicate
Suffix suffix of the predicate
Predicate-Constituent related features
Path parse tree path from the predicate to the constituent
Position the relative position of the constituent and the predicate, before or after
Path Length the nodes number on the parse tree path
Partial Path some part on the parse tree path
Clause Layers the clause layers from the constituent to the predicate
Table 1: Standard flat features
However, to find relevant features is, as usual,
a complex task. In addition, according to the de-
scription of the standard features, we can see that
the syntactic features, such as Path, Path Length,
bulk large among all features. On the other hand,
the previous researches (Gildea and Palmer, 2002;
Punyakanok et al, 2005) have also recognized the
74
 

 	

 

 



 

 	

Figure 2: Predicate Argument Feature space
necessity of syntactic parsing for semantic role la-
beling. However, the standard flat features cannot
model the syntactic information well. A predicate-
argument pair has two different Path features even
if their paths differ only for a node in the parse
tree. This data sparseness problem prevents the
learning algorithms from generalizing unseen data
well. In order to address this problem, one method
is to list all sub-structures of the parse tree. How-
ever, both space complexity and time complexity
are too high for the algorithm to be realized.
4 Hybrid Convolution Tree Kernels for
SRL
In this section, we introduce the previous ker-
nel method for SRL in Subsection 4.1, discuss
our method in Subsection 4.2 and compare our
method with previous work in Subsection 4.3.
4.1 Convolution Tree Kernels for SRL
Moschitti (2004) proposed to apply convolution
tree kernels (Collins and Duffy, 2001) to SRL.
He selected portions of syntactic parse trees,
which include salient sub-structures of predicate-
arguments, to define convolution kernels for the
task of predicate argument classification. This por-
tions selection method of syntactic parse trees is
named as predicate-arguments feature (PAF) ker-
nel. Figure 2 illustrates the PAF kernel feature
space of the predicate buy and the argument Arg1
in the circled sub-structure.
The kind of convolution tree kernel is similar to
Collins and Duffy (2001)?s tree kernel except the
sub-structure selection strategy. Moschitti (2004)
only selected the relative portion between a predi-
cate and an argument.
Given a tree portion instance defined above, we
design a convolution tree kernel in a way similar
to the parse tree kernel (Collins and Duffy, 2001).
Firstly, a parse tree T can be represented by a vec-
tor of integer counts of each sub-tree type (regard-
less of its ancestors):
?(T ) = (# of sub-trees of type 1, . . . ,
# of sub-trees of type i, . . . ,
# of sub-trees of type n)
This results in a very high dimension since the
number of different subtrees is exponential to the
tree?s size. Thus it is computationally infeasible
to use the feature vector ?(T ) directly. To solve
this problem, we introduce the tree kernel function
which is able to calculate the dot product between
the above high-dimension vectors efficiently. The
kernel function is defined as following:
K(T1, T2) = ??(T1),?(T2)? =
?
i ?i(T1), ?i(T2)=?n1?N1
?
n2?N2
?
i Ii(n1) ? Ii(n2)
where N1 and N2 are the sets of all nodes in
trees T1 and T2, respectively, and Ii(n) is the in-
dicator function whose value is 1 if and only if
there is a sub-tree of type i rooted at node n and
0 otherwise. Collins and Duffy (2001) show that
K(T1, T2) is an instance of convolution kernels
over tree structures, which can be computed in
O(|N1| ? |N2|) by the following recursive defi-
nitions (Let ?(n1, n2) =
?
i Ii(n1) ? Ii(n2)):
(1) if the children of n1 and n2 are different then
?(n1, n2) = 0;
(2) else if their children are the same and they are
leaves, then ?(n1, n2) = ?;
(3) else ?(n1, n2) = ?
?nc(n1)
j=1 (1 +
?(ch(n1, j), ch(n2, j)))
where nc(n1) is the number of the children of
n1, ch(n, j) is the jth child of node n and ?(0 <
? < 1) is the decay factor in order to make the
kernel value less variable with respect to the tree
sizes.
4.2 Hybrid Convolution Tree Kernels
In the PAF kernel, the feature spaces are consid-
ered as an integral portion which includes a pred-
icate and one of its arguments. We note that the
PAF feature consists of two kinds of features: one
is the so-called parse tree Path feature and another
one is the so-called Constituent Structure feature.
These two kinds of feature spaces represent dif-
ferent information. The Path feature describes the
75
 

 	

 

 



 

 	

Figure 3: Path and Constituent Structure feature
spaces
linking information between a predicate and its ar-
guments while the Constituent Structure feature
captures the syntactic structure information of an
argument. We believe that it is more reasonable
to capture the two different kinds of features sepa-
rately since they contribute to SRL in different fea-
ture spaces and it is better to give different weights
to fuse them. Therefore, we propose two convo-
lution kernels to capture the two features, respec-
tively and combine them into one hybrid convolu-
tion kernel for SRL. Figure 3 is an example to il-
lustrate the two feature spaces, where the Path fea-
ture space is circled by solid curves and the Con-
stituent Structure feature spaces is circled by dot-
ted curves. We name them Path kernel and Con-
stituent Structure kernel respectively.
Figure 4 illustrates an example of the distinc-
tion between the PAF kernel and our kernel. In
the PAF kernel, the tree structures are equal when
considering constitutes NP and PRP, as shown in
Figure 4(a). However, the two constituents play
different roles in the sentence and should not be
looked as equal. Figure 4(b) shows the comput-
ing example with our kernel. During computing
the hybrid convolution tree kernel, the NP?PRP
substructure is not computed. Therefore, the two
trees are distinguished correctly.
On the other hand, the constituent structure fea-
ture space reserves the most part in the traditional
PAF feature space usually. Then the Constituent
Structure kernel plays the main role in PAF kernel
computation, as shown in Figure 5. Here, believes
is a predicate and A1 is a long sub-sentence. Ac-
cording to our experimental results in Section 5.2,
we can see that the Constituent Structure kernel
does not perform well. Affected by this, the PAF
kernel cannot perform well, either. However, in
our hybrid method, we can adjust the compromise

 
 	



 
 	


(a) PAF Kernel

 
	


	


 
	



	
 

(b) Hybrid Convolution Tree Kernel
Figure 4: Comparison between PAF and Hybrid
Convolution Tree Kernels
Figure 5: An example of Semantic Role Labeling
of the Path feature and the Constituent Structure
feature by tuning their weights to get an optimal
result.
Having defined two convolution tree kernels,
the Path kernel Kpath and the Constituent Struc-
ture kernel Kcs, we can define a new kernel to
compose and extend the individual kernels. Ac-
cording to Joachims et al (2001), the kernel func-
tion set is closed under linear combination. It
means that the following Khybrid is a valid kernel
if Kpath and Kcs are both valid.
Khybrid = ?Kpath + (1? ?)Kcs (1)
where 0 ? ? ? 1.
According to the definitions of the Path and the
Constituent Structure kernels, each kernel is ex-
plicit. They can be viewed as a matching of fea-
76
tures. Since the features are enumerable on the
given data, the kernels are all valid. Therefore, the
new kernel Khybrid is valid. We name the new ker-
nel hybrid convolution tree kernel, Khybrid.
Since the size of a parse tree is not con-
stant, we normalize K(T1, T2) by dividing it by?K(T1, T1) ?K(T2, T2)
4.3 Comparison with Previous Work
It would be interesting to investigate the differ-
ences between our method and the feature-based
methods. The basic difference between them lies
in the instance representation (parse tree vs. fea-
ture vector) and the similarity calculation mecha-
nism (kernel function vs. dot-product). The main
difference between them is that they belong to dif-
ferent feature spaces. In the kernel methods, we
implicitly represent a parse tree by a vector of in-
teger counts of each sub-tree type. That is to say,
we consider all the sub-tree types and their occur-
ring frequencies. In this way, on the one hand,
the predicate-argument related features, such as
Path, Position, in the flat feature set are embed-
ded in the Path feature space. Additionally, the
Predicate, Predicate POS features are embedded
in the Path feature space, too. The constituent re-
lated features, such as Phrase Type, Head Word,
Last Word, and POS, are embedded in the Con-
stituent Structure feature space. On the other hand,
the other features in the flat feature set, such as
Named Entity, Previous, and Next Word, Voice,
SubCat, Suffix, are not contained in our hybrid
convolution tree kernel. From the syntactic view-
point, the tree representation in our feature space
is more robust than the Parse Tree Path feature in
the flat feature set since the Path feature is sensi-
tive to small changes of the parse trees and it also
does not maintain the hierarchical information of
a parse tree.
It is also worth comparing our method with
the previous kernels. Our method is similar to
the Moschitti (2004)?s predicate-argument feature
(PAF) kernel. However, we differentiate the Path
feature and the Constituent Structure feature in our
hybrid kernel in order to more effectively capture
the syntactic structure information for SRL. In ad-
dition Moschitti (2004) only study the task of ar-
gument classification while in our experiment, we
report the experimental results on both identifica-
tion and classification.
5 Experiments and Discussion
The aim of our experiments is to verify the effec-
tiveness of our hybrid convolution tree kernel and
and its combination with the standard flat features.
5.1 Experimental Setting
5.1.1 Corpus
We use the benchmark corpus provided by
CoNLL-2005 SRL shared task (Carreras and
Ma`rquez, 2005) provided corpus as our training,
development, and test sets. The data consist of
sections of the Wall Street Journal (WSJ) part of
the Penn TreeBank (Marcus et al, 1993), with
information on predicate-argument structures ex-
tracted from the PropBank corpus (Palmer et al,
2005). We followed the standard partition used
in syntactic parsing: sections 02-21 for training,
section 24 for development, and section 23 for
test. In addition, the test set of the shared task
includes three sections of the Brown corpus. Ta-
ble 2 provides counts of sentences, tokens, anno-
tated propositions, and arguments in the four data
sets.
Train Devel tWSJ tBrown
Sentences 39,832 1,346 2,416 426
Tokens 950,028 32,853 56,684 7,159
Propositions 90,750 3,248 5,267 804
Arguments 239,858 8,346 14,077 2,177
Table 2: Counts on the data set
The preprocessing modules used in CONLL-
2005 include an SVM based POS tagger (Gime?nez
and Ma`rquez, 2003), Charniak (2000)?s full syn-
tactic parser, and Chieu and Ng (2003)?s Named
Entity recognizer.
5.1.2 Evaluation
The system is evaluated with respect to
precision, recall, and F?=1 of the predicted ar-
guments. Precision (p) is the proportion of ar-
guments predicted by a system which are cor-
rect. Recall (r) is the proportion of correct ar-
guments which are predicted by a system. F?=1
computes the harmonic mean of precision and
recall, which is the final measure to evaluate the
performances of systems. It is formulated as:
F?=1 = 2pr/(p + r). srl-eval.pl2 is the official
program of the CoNLL-2005 SRL shared task to
evaluate a system performance.
2http://www.lsi.upc.edu/?srlconll/srl-eval.pl
77
5.1.3 SRL Strategies
We use constituents as the labeling units to form
the labeled arguments. In order to speed up the
learning process, we use a four-stage learning ar-
chitecture:
Stage 1: To save time, we use a pruning
stage (Xue and Palmer, 2004) to filter out the
constituents that are clearly not semantic ar-
guments to the predicate.
Stage 2: We then identify the candidates derived
from Stage 1 as either arguments or non-
arguments.
Stage 3: A multi-category classifier is used to
classify the constituents that are labeled as ar-
guments in Stage 2 into one of the argument
classes plus NULL.
Stage 4: A rule-based post-processing stage (Liu
et al, 2005) is used to handle some un-
matched arguments with constituents, such as
AM-MOD, AM-NEG.
5.1.4 Classifier
We use the Voted Perceptron (Freund and
Schapire, 1998) algorithm as the kernel machine.
The performance of the Voted Perceptron is close
to, but not as good as, the performance of SVM on
the same problem, while saving computation time
and programming effort significantly. SVM is too
slow to finish our experiments for tuning parame-
ters.
The Voted Perceptron is a binary classifier. In
order to handle multi-classification problems, we
adopt the one vs. others strategy and select the
one with the largest margin as the final output. The
training parameters are chosen using development
data. After 5 iteration numbers, the best perfor-
mance is achieved. In addition, Moschitti (2004)?s
Tree Kernel Tool is used to compute the tree kernel
function.
5.2 Experimental Results
In order to speed up the training process, in the
following experiments, we ONLY use WSJ sec-
tions 02-05 as training data. The same as Mos-
chitti (2004), we also set the ? = 0.4 in the com-
putation of convolution tree kernels.
In order to study the impact of ? in hybrid con-
volution tree kernel in Eq. 1, we only use the hy-
brid kernel between Kpath and Kcs. The perfor-
mance curve on development set changing with ?
is shown in Figure 6.
Figure 6: The performance curve changing with ?
The performance curve shows that when ? =
0.5, the hybrid convolution tree kernel gets the
best performance. Either the Path kernel (? = 1,
F?=1 = 61.26) or the Constituent Structure kernel
(? = 0, F?=1 = 54.91) cannot perform better than
the hybrid one. It suggests that the two individual
kernels are complementary to each other. In ad-
dition, the Path kernel performs much better than
the Constituent Structure kernel. It indicates that
the predicate-constituent related features are more
effective than the constituent features for SRL.
Table 3 compares the performance comparison
among our Hybrid convolution tree kernel, Mos-
chitti (2004)?s PAF kernel, standard flat features
with Linear kernels, and Poly kernel (d = 2). We
can see that our hybrid convolution tree kernel out-
performs the PAF kernel. It empirically demon-
strates that the weight linear combination in our
hybrid kernel is more effective than PAF kernel for
SRL.
However, our hybrid kernel still performs worse
than the standard feature based system. This is
simple because our kernel only use the syntac-
tic structure information while the feature-based
method use a large number of hand-craft diverse
features, from word, POS, syntax and semantics,
NER, etc. The standard features with polynomial
kernel gets the best performance. The reason is
that the arbitrary binary combination among fea-
tures implicated by the polynomial kernel is useful
to SRL. We believe that combining the two meth-
ods can perform better.
In order to make full use of the syntactic
information and the standard flat features, we
present a composite kernel between hybrid kernel
(Khybrid) and standard features with polynomial
78
Hybrid PAF Linear Poly
Devel 66.01 64.38 68.71 70.25
Table 3: Performance (F?=1) comparison among
various kernels
kernel (Kpoly):
Kcomp = ?Khybrid + (1? ?)Kpoly (2)
where 0 ? ? ? 1.
The performance curve changing with ? in Eq. 2
on development set is shown in Figure 7.
Figure 7: The performance curve changing with ?
We can see that when ? = 0.5, the system
achieves the best performance and F?=1 = 70.78.
It?s statistically significant improvement (?2 test
with p = 0.1) than only using the standard features
with the polynomial kernel (? = 0, F?=1 = 70.25)
and much higher than only using the hybrid con-
volution tree kernel (? = 1, F?=1 = 66.01).
The main reason is that the convolution tree ker-
nel can represent more general syntactic features
than standard flat features, and the standard flat
features include the features that the convolution
tree kernel cannot represent, such as Voice, Sub-
Cat. The two kind features are complementary to
each other.
Finally, we train the composite method using
the above setting (Eq. 2 with when ? = 0.5) on the
entire training set. The final performance is shown
in Table 4.
6 Conclusions and Future Work
In this paper we proposed the hybrid convolu-
tion kernel to model syntactic structure informa-
tion for SRL. Different from the previous convo-
lution tree kernel based methods, our contribution
Precision Recall F?=1
Development 80.71% 68.49% 74.10
Test WSJ 82.46% 70.65% 76.10
Test Brown 73.39% 57.01% 64.17
Test WSJ Precision Recall F?=1
Overall 82.46% 70.65% 76.10
A0 87.97% 82.49% 85.14
A1 80.51% 71.69% 75.84
A2 75.79% 52.16% 61.79
A3 80.85% 43.93% 56.93
A4 83.56% 59.80% 69.71
A5 100.00% 20.00% 33.33
AM-ADV 66.27% 43.87% 52.79
AM-CAU 68.89% 42.47% 52.54
AM-DIR 56.82% 29.41% 38.76
AM-DIS 79.02% 75.31% 77.12
AM-EXT 73.68% 43.75% 54.90
AM-LOC 72.83% 50.96% 59.97
AM-MNR 68.54% 42.44% 52.42
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 49.32% 31.30% 38.30
AM-TMP 82.15% 68.17% 74.51
R-A0 86.28% 87.05% 86.67
R-A1 80.00% 74.36% 77.08
R-A2 100.00% 31.25% 47.62
R-AM-CAU 100.00% 50.00% 66.67
R-AM-EXT 50.00% 100.00% 66.67
R-AM-LOC 92.31% 57.14% 70.59
R-AM-MNR 20.00% 16.67% 18.18
R-AM-TMP 68.75% 63.46% 66.00
V 98.65% 98.65% 98.65
Table 4: Overall results (top) and detailed results
on the WSJ test (bottom).
is that we distinguish between the Path and the
Constituent Structure feature spaces. Evaluation
on the datasets of CoNLL-2005 SRL shared task,
shows that our novel hybrid convolution tree ker-
nel outperforms the PAF kernel method. Although
the hybrid kernel base method is not as good as
the standard rich flat feature based methods, it can
improve the state of the art feature-based methods
by implicating the more generalizing syntactic in-
formation.
Kernel-based methods provide a good frame-
work to use some features which are difficult to
model in the standard flat feature based methods.
For example the semantic similarity of words can
be used in kernels well. We can use general pur-
pose corpus to create clusters of similar words or
use available resources like WordNet. We can also
use the hybrid kernel method into other tasks, such
as relation extraction in the future.
79
Acknowledgements
The authors would like to thank the reviewers for
their helpful comments and Shiqi Zhao, Yanyan
Zhao for their suggestions and useful discussions.
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60435020, 60575042, and 60503072.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the ACL-Coling-1998, pages 86?90.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proceedings of CoNLL-2004, pages 89?
97.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL-2005, pages
152?164.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-2000.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proceedings of CoNLL-2003, pages 160?163.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proceedings
of NIPS-2001.
Nello Cristianini and John Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines. Cambridge
University Press, Cambirdge University.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of ACL-2004, pages 423?429.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In Computational Learning Theory, pages 209?217.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of ACL-2002, pages 239?246.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and
accurate part-of-speech tagging: The svm approach
revisited. In Proceedings of RANLP-2003.
David Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCSC-CRL-99-
10, July.
Zheng Ping Jiang, Jia Li, and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument
interdependence. In Proceedings of IJCAI-2005.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cat-
egorisation. In Proceedings of ICML-2001, pages
250?257.
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, pages 189?192.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, 2:419?444.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of ACL-2004, pages 335?342.
Rodney D. Nielsen and Sameer Pradhan. 2004. Mix-
ing weak learners in semantic parsing. In Proceed-
ings of EMNLP-2004.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005a. Support vector learning for semantic
argument classification. Machine Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Daniel Jurafsky. 2005b. Semantic role
labeling using different syntactic views. In Proceed-
ings of ACL-2005, pages 581?588.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2005.
The necessity of syntactic parsing for semantic role
labeling. In Proceedings of IJCAI-2005, pages
1117?1123.
Chris Watkins. 1999. Dynamic alignment kernels.
Technical Report CSD-TR-98-11, Jan.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP 2004.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
80
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification 
 
Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     
Guodong ZHOU1,4     Ting LIU2     Sheng LI2     
 
1Institute for Infocomm Research   
{mzhang, aaiti}@i2r.a-star.edu.sg 
2Harbin Institute of Technology 
{car, tliu}@ir.hit.edu.cn   
lisheng@hit.edu.cn 
3National University of Singapore 
tancl@comp.nus.edu.sg 
4 Soochow Univ., China 215006 
gdzhou@suda.edu.cn 
 
 
 
 
 
Abstract 
Convolution tree kernel has shown promis-
ing results in semantic role classification. 
However, it only carries out hard matching, 
which may lead to over-fitting and less ac-
curate similarity measure. To remove the 
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic 
role classification by introducing more lin-
guistic knowledge into the standard tree 
kernel. The proposed grammar-driven tree 
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate 
substructure matching and 2) grammar-
driven approximate tree node matching. The 
two improvements enable the grammar-
driven tree kernel explore more linguistically 
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005 
SRL shared task show that the grammar-
driven tree kernel significantly outperforms 
the previous non-grammar-driven one in 
SRL. Moreover, we present a composite 
kernel to integrate feature-based and tree 
kernel-based methods. Experimental results 
show that the composite kernel outperforms 
the previously best-reported methods. 
1 Introduction 
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms 
expressed by some target verbs or nouns and some 
constituents of the sentence. In particular, for each 
predicate (target verb or noun) all the constituents in 
the sentence which fill semantic arguments (roles) 
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc. 
and also adjuncts such as Locative, Temporal, 
Manner, and Cause, etc. Generally, semantic role 
identification and classification are regarded as two 
key steps in semantic role labeling. Semantic role 
identification involves classifying each syntactic 
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument 
identified into a specific semantic role. This paper 
focuses on semantic role classification task with the 
assumption that the semantic arguments have been 
identified correctly. 
Both feature-based and kernel-based learning 
methods have been studied for semantic role classi-
fication (Carreras and M?rquez, 2004; Carreras and 
M?rquez, 2005). In feature-based methods, a flat 
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods, 
a kernel function is used to measure directly the 
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti 
(2004) and Che et al (2006) used a convolution 
tree kernel (Collins and Duffy, 2001) for semantic 
role classification. The convolution tree kernel 
takes sub-tree as its feature and counts the number 
of common sub-trees as the similarity between two 
predicate-arguments. This kernel has shown very 
200
promising results in SRL. However, as a general 
learning algorithm, the tree kernel only carries out 
hard matching between any two sub-trees without 
considering any linguistic knowledge in kernel de-
sign. This makes the kernel fail to handle similar 
phrase structures (e.g., ?buy a car? vs. ?buy a red 
car?) and near-synonymic grammar tags (e.g., the 
POS variations between ?high/JJ degree/NN? 1 and 
?higher/JJR degree/NN?) 2. To some degree, it may 
lead to over-fitting and compromise performance. 
This paper reports our preliminary study in ad-
dressing the above issue by introducing more lin-
guistic knowledge into the convolution tree kernel. 
To our knowledge, this is the first attempt in this 
research direction. In detail, we propose a gram-
mar-driven convolution tree kernel for semantic 
role classification that can carry out more linguisti-
cally motivated substructure matching. Experimental 
results show that the proposed method significantly 
outperforms the standard convolution tree kernel on 
the data set of the CoNLL-2005 SRL shared task. 
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous work and Sec-
tion 3 discusses our grammar-driven convolution 
tree kernel. Section 4 shows the experimental re-
sults. We conclude our work in Section 5. 
2 Previous Work 
Feature-based Methods for SRL: most features 
used in prior SRL research are generally extended 
from Gildea and Jurafsky (2002), who used a linear 
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the 
constituents in the FrameNet (Baker et al, 1998). 
Here, the basic features include Phrase Type, Parse 
Tree Path, and Position. Most of the following work 
focused on feature engineering (Xue and Palmer, 
2004; Jiang et al, 2005) and machine learning 
models (Nielsen and Pradhan, 2004; Pradhan et al, 
2005a). Some other work paid much attention to the 
robust SRL (Pradhan et al, 2005b) and post infer-
ence (Punyakanok et al, 2004). These feature-
based methods are considered as the state of the art 
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the 
                                                          
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the 
detailed definitions of the grammar tags used in the paper. 
2 Some rewrite rules in English grammar are generalizations of 
others: for example, ?NP? DET JJ NN? is a specialized ver-
sion of ?NP? DET NN?. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases. 
syntactic structured information. For example, in 
SRL, the Parse Tree Path feature is sensitive to 
small changes of the syntactic structures. Thus, a 
predicate argument pair will have two different 
Path features even if their paths differ only for one 
node. This may result in data sparseness and model 
generalization problems. 
Kernel-based Methods for SRL: as an alternative, 
kernel methods are more effective in modeling 
structured objects. This is because a kernel can 
measure the similarity between two structured ob-
jects using the original representation of the objects 
instead of explicitly enumerating their features. 
Many kernels have been proposed and applied to 
the NLP study. In particular, Haussler (1999) pro-
posed the well-known convolution kernels for a 
discrete structure. In the context of it, more and 
more kernels for restricted syntaxes or specific do-
mains (Collins and Duffy, 2001; Lodhi et al, 2002; 
Zelenko et al, 2003; Zhang et al, 2006) are pro-
posed and explored in the NLP domain. 
Of special interest here, Moschitti (2004) proposed 
Predicate Argument Feature (PAF) kernel for SRL 
under the framework of convolution tree kernel. He 
selected portions of syntactic parse trees as predicate-
argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al (2006) 
proposed a hybrid convolution tree kernel, which 
consists of two individual convolution kernels: a Path 
kernel and a Constituent Structure kernel. Che et al 
(2006) showed that their method outperformed PAF 
on the CoNLL-2005 SRL dataset.  
The above two kernels are special instances of 
convolution tree kernel for SRL. As discussed in 
Section 1, convolution tree kernel only carries out 
hard matching, so it fails to handle similar phrase 
structures and near-synonymic grammar tags. This 
paper presents a grammar-driven convolution tree 
kernel to solve the two problems 
3 Grammar-driven Convolution Tree 
Kernel 
3.1 Convolution Tree Kernel 
In convolution tree kernel (Collins and Duffy, 
2001), a parse tree T  is represented by a vector of 
integer counts of each sub-tree type (regardless of 
its ancestors): ( )T? = ( ?, # subtreei(T), ?), where 
201
# subtreei(T) is the occurrence number of the ith 
sub-tree type (subtreei) in T. Since the number of 
different sub-trees is exponential with the parse tree 
size, it is computationally infeasible to directly use 
the feature vector ( )T? . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the 
following parse tree kernel to calculate the dot 
product between the above high dimensional vec-
tors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively, and ( )
isubtree
I n  is a function that is 
1 iff the subtreei occurs with root at node n and zero 
otherwise, and 1 2( , )n n?  is the number of the com-
mon subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by the 
following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the decay 
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition, 
the recursive Rule 3 holds because given two 
nodes with the same children, one can construct 
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is 1 2(| | | |)O N N? . 
3.2 Grammar-driven Convolution Tree 
Kernel 
This Subsection introduces the two improvements 
and defines our grammar-driven tree kernel. 
 
Improvement 1: Grammar-driven approximate 
matching between substructures. The conven-
tional tree kernel requires exact matching between 
two contiguous phrase structures. This constraint 
may be too strict. For example, the two phrase 
structures ?NP?DT JJ NN? (NP?a red car) and 
?NP?DT NN? (NP->a car) are not identical, thus 
they contribute nothing to the conventional kernel 
although they should share the same semantic role 
given a predicate. In this paper, we propose a 
grammar-driven approximate matching mechanism 
to capture the similarity between such kinds of 
quasi-structures for SRL. 
First, we construct reduced rule set by defining 
optional nodes, for example, ?NP->DT [JJ] NP? or 
?VP-> VB [ADVP]  PP?, where [*] denotes op-
tional nodes. For convenience, we call ?NP-> DT 
JJ NP? the original rule and ?NP->DT [JJ] NP? the 
reduced rule. Here, we define two grammar-driven 
criteria to select optional nodes: 
1) The reduced rules must be grammatical. It 
means that the reduced rule should be a valid rule 
in the original rule set. For example, ?NP->DT [JJ] 
NP? is valid only when ?NP->DT NP? is a valid 
rule in the original rule set while ?NP->DT [JJ 
NP]? may not be valid since ?NP->DT? is not a 
valid rule in the original rule set. 
2) A valid reduced rule must keep the head 
child of its corresponding original rule and has at 
least two children. This can make the reduced rules 
retain the underlying semantic meaning of their 
corresponding original rules. 
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows: 
11 2 1 2,
( , ) ( ( , ) )
a bi ji j
T r ri j
M r r I T T ?
+
= ??              (1)  
where 1r is a production rule, representing a sub-tree 
of depth one3, and 1
i
rT is the i
th variation of the sub-
tree 1r by removing one ore more optional nodes
4, 
and likewise for 2r and 2
j
rT . ( , )TI ? ? is a function 
that is 1 iff the two sub-trees are identical and zero 
otherwise. 1? (0? 1? ?1) is a small penalty to penal-
                                                          
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one 
can be achieved easily through the matching of sub-structures 
of depth one in the recursively-defined convolution kernel. We 
will discuss this issue when defining our kernel. 
4 To make sure that the new kernel is a proper kernel, we have 
to consider all the possible variations of the original sub-trees. 
Training program converges only when using a proper kernel. 
202
ize optional nodes and the two parameters ia  and 
jb stand for the numbers of occurrence of removed 
optional nodes in subtrees 1
i
rT and 2
j
rT , respectively. 
1 2( , )M r r returns the similarity (ie., the kernel 
value) between the two sub-trees 1r and 2r  by sum-
ming up the similarities between all possible varia-
tions of the sub-trees 1r and 2r . 
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small 
penalty 1? ) if the two structures are identical after 
removing one or more optional nodes. In this case, 
the above example phrase structures ?NP->a red 
car? and ?NP->a car? are matchable with a pen-
alty 1?  in our new kernel. It means that one co-
occurrence of the two structures contributes 1?  to 
our proposed kernel while it contributes zero to the 
traditional one. Therefore, by this improvement, our 
method would be able to explore more linguistically 
appropriate features than the previous one (which is 
formulated as 1 2( , )TI r r ). 
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel 
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs 
may represent similar roles, such as NN (dog) and 
NNS (dogs). In order to capture this phenomenon, 
we allow approximate matching between node fea-
tures. The following illustrates some equivalent 
node feature sets:  
? JJ, JJR, JJS 
? VB, VBD, VBG, VBN, VBP, VBZ 
? ?? 
where POSs in the same line can match each other 
with a small penalty 0? 2? ?1. We call this case 
node feature mutation. This improvement further 
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can 
be formulated as: 
21 2 1 2,
( , ) ( ( , ) )
a bi ji j
fi j
M f f I f f ?
+
= ??           (2) 
where 1f is a node feature, 1
if is the ith mutation 
of 1f and ia is 0 iff 1
if and 1f are identical and 1 oth-
erwise, and likewise for 2f . ( , )fI ? ? is a function 
that is 1 iff the two features are identical and zero 
otherwise. Eq. (2) sums over all combinations of 
feature mutations as the node feature similarity. 
The same as Eq. (1), the reason for taking all the 
possibilities into account in Eq. (2) is to make sure 
that the new kernel is a proper kernel.  
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules. 
 
The Grammar-driven Kernel Definition: Given 
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature 
vector representation of a parse tree T as follows: 
( )T? =? (# subtree1(T), ?, # subtreen(T))       
where # subtreei(T) is the occurrence number of the 
ith sub-tree type (subtreei) in T. Please note that, 
different from the previous tree kernel, here we 
loosen the condition for the occurrence of a subtree 
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example, 
one occurrence of the rule ?NP->DT JJ NP? shall 
contribute 1 times to the feature ?NP->DT JJ NP? 
and 1?  times to the feature ?NP->DT NP? in the 
new kernel while it only contributes 1 times to the 
feature ?NP->DT JJ NP? in the previous one. Now 
we can define the new grammar-driven kernel 
1 2( , )GK T T as follows: 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
( ) ( )
 ( , )
(( ) ( ))
i i
G
subtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
? ?=< >
? ?=
?= ?
?? ? ?
? ?
 (3) 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively. ( )
isubtree
I n?  is a function that is 
1 2
a b? ?? iff the subtreei occurs with root at node n 
and zero otherwise, where a and b are the numbers 
of removed optional nodes and mutated node fea-
tures, respectively. 1 2( , )n n??  is the number of the 
common subtrees rooted at n1 and n2, i.e. , 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ?? = ??         (4) 
Please note that the value of 1 2( , )n n?? is no longer 
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. 1 2( , )n n??  can be further 
computed by the following recursive rules:  
 
203
============================================================================ 
Rule A: if 1n and 2n are pre-terminals, then: 
1 2 1 2( , ) ( , )n n M f f??? = ?                          (5) 
where 1f and 2f are features of nodes 1n and 2n re-
spectively, and 1 2( , )M f f  is defined at Eq. (2).  
Rule B: else if both 1n and 2n are the same non-
terminals, then generate all variations of the subtrees 
of depth one rooted by 1n and 2n (denoted by 1nT  
and 2nT  respectively) by removing different optional 
nodes, then: 
 
1
1
1 2 1 2,
( , )
1 21
( , ) ( ( , )
   (1 ( ( , , ), ( , , )))
a bi ji j
T n ni j
nc n i
k
n n I T T
ch n i k ch n j k
? ?
+
=
?? = ? ?
?? + ?
?
?
(6) 
 
where  
? 1inT and 2jnT stand for the ith and jth variations in 
sub-tree set 1nT and 2nT , respectively. 
? ( , )TI ? ? is a function that is 1 iff the two sub-
trees are identical and zero otherwise.  
? ia and jb stand for the number of removed op-
tional nodes in subtrees 1
i
nT and 2
j
nT , respectively. 
? 1( , )nc n i returns the child number of 1n in its ith 
subtree variation 1
i
nT . 
? 1( , , )ch n i k  is the kth child of node 1n  in its ith 
variation subtree 1
i
nT , and likewise for 2( , , )ch n j k . 
? Finally, the same as the previous tree kernel, 
? (0< ? <1) is the decay factor (see the discussion 
in Subsection 3.1). 
 
Rule C: else 1 2( , ) 0n n?? =  
  
============================================================================ 
 
Rule A accounts for Improvement 2 while Rule 
B accounts for Improvement 1. In Rule B, Eq. (6) 
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4) 
is a convolution kernel according to the definition 
and the proof given in Haussler (1999), and Eqs (5) 
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by 
Eq (3) is also a valid convolution kernel. Finally, 
let us study the computational issue of the new 
convolution tree kernel. Clearly, computing Eq. (6) 
requires exponential time in its worst case. How-
ever, in practice, it may only need  1 2(| | | |)O N N? . 
This is because there are only 9.9% rules (647 out 
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even 
much less and is close to linear in the size of the 
trees since 1 2( , ) 0n n?? =  holds for many node 
pairs (Collins and Duffy, 2001). In theory, we can 
also design an efficient algorithm to compute Eq. 
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work. 
3.3 Comparison with previous work 
In above discussion, we show that the conventional 
convolution tree kernel is a special case of the 
grammar-driven tree kernel. From kernel function 
viewpoint, our kernel can carry out not only exact 
matching (as previous one described by Rules 2 
and 3 in Subsection 3.1) but also approximate 
matching (Eqs. (5) and (6) in Subsection 3.2). From 
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined 
recursively by the phrase parse rules), their feature 
values are different since our kernel captures the 
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge 
in our kernel design. 
Moschitti (2006) proposes a partial tree (PT) 
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much 
larger feature space than both the conventional and 
the grammar-driven kernels. In this point, one can 
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel 
is not grammar-driven, thus many non-
linguistically motivated structures are matched in 
the PT kernel. This may potentially compromise 
the performance since some of the over-generated 
features may possibly be noisy due to the lack of 
linguistic interpretation and constraint. 
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than 
the PT kernel and can explore much larger sub-tree 
features than the PT kernel. However, the same as 
the PT kernel, the labeled order tree kernel is not 
grammar-driven. Thus, it may face the same issues 
204
(such as over-generated features) as the PT kernel 
when used in NLP applications. 
 Shen el al. (2003) proposed a lexicalized tree 
kernel to utilize LTAG-based features in parse 
reranking. Their methods need to obtain a LTAG 
derivation tree for each parse tree before kernel 
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree 
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional. 
4 Experiments 
4.1 Experimental Setting 
Data: We use the CoNLL-2005 SRL shared task 
data (Carreras and M?rquez, 2005) as our experi-
mental corpus. The data consists of sections of the 
Wall Street Journal part of the Penn TreeBank 
(Marcus et al, 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al, 2005). As defined by 
the shared task, we use sections 02-21 for training, 
section 24 for development and section 23 for test. 
There are 35 roles in the data including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference 
(R-) arguments. Table 1 lists counts of sentences 
and arguments in the three data sets. 
  
 Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
Table 1: Counts on the data set 
 
We assume that the semantic role identification 
has been done correctly. In this way, we can focus 
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and 
the one with the largest margin is selected as the 
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the 
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one. 
 
Kernel Setup: We use the Constituent, Predicate, 
and Predicate-Constituent related features, which 
are reported to get the best-reported performance 
(Pradhan et al, 2005a), as the baseline features. We 
use Che et al (2006)?s hybrid convolution tree ker-
nel (the best-reported method for kernel-based 
SRL) as our baseline kernel. It is defined as 
(1 )  (0 1)hybrid path csK K K? ? ?= + ? ? ? (for the de-
tailed definitions of pathK and csK , please refer to 
Che et al (2006)). Here, we use our grammar-
driven tree kernel to compute pathK and csK , and we 
call it grammar-driven hybrid tree kernel while Che 
et al (2006)?s is non-grammar-driven hybrid convo-
lution tree kernel.  
We use a greedy strategy to fine-tune parameters. 
Evaluation on the development set shows that our 
kernel yields the best performance when ? (decay 
factor of tree kernel), 1? and 2? (two penalty factors 
for the grammar-driven kernel), ? (hybrid kernel 
parameter) and c (a SVM training parameter to 
balance training error and margin) are set to 0.4, 
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005 
benchmark data, we get 647 rules with optional 
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below: 
? JJ, JJR, JJS 
? RB, RBR, RBS 
? NN, NNS, NNP, NNPS, NAC, NX 
 
Here, the verb feature set ?VB, VBD, VBG, VBN, 
VBP, VBZ? is removed since the voice information 
is very indicative to the arguments of ARG0 
(Agent, operator) and ARG1 (Thing operated). 
 
Methods Accuracy (%) 
 Baseline: Non-grammar-driven 85.21 
 +Approximate Node Matching 86.27 
 +Approximate Substructure 
Matching 
87.12 
 Ours: Grammar-driven Substruc-
ture and Node Matching 
87.96 
Feature-based method with poly-
nomial kernel (d = 2) 
89.92 
 
Table 2: Performance comparison 
4.2 Experimental Results 
Table 2 compares the performances of different 
methods on the test set. First, we can see that the 
new grammar-driven hybrid convolution tree kernel 
significantly outperforms ( 2? test with p=0.05) the 
205
non-grammar one with an absolute improvement of 
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21)) 
. It suggests that 1) the linguistically motivated 
structure features are very useful for semantic role 
classification and 2) the grammar-driven kernel is 
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the 
grammar-driven approximate node matching and the 
grammar-driven approximate substructure matching 
are very useful in modeling syntactic tree structures 
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and 
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) 
the grammar-driven approximate substructure 
matching is more effective than the grammar-driven 
approximate node matching. However, we find that 
the performance of the grammar-driven kernel is 
still a bit lower than the feature-based method. This 
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this 
paper, it captures the syntactic parse tree structure 
features only while the features used in the feature-
based methods cover more knowledge sources.  
In order to make full use of the syntactic structure 
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the 
grammar-driven hybrid kernel and feature-based 
method with polynomial kernel: 
(1 )      (0 1)comp hybrid polyK K K? ? ?= + ? ? ?  
Evaluation on the development set shows that the 
composite kernel yields the best performance when 
? is set to 0.3. Using the same setting, the system 
achieves the performance of 91.02% in Accuracy 
in the same test set. It shows statistically significant 
improvement (?2 test with p= 0.10) over using the 
standard features with the polynomial kernel (? = 0, 
Accuracy = 89.92%) and using the grammar-driven 
hybrid convolution tree kernel (? = 1, Accuracy = 
87.96%). The main reason is that the tree kernel 
can capture effectively more structure features 
while the standard flat features can cover some 
other useful features, such as Voice, SubCat, which 
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of 
methods are complementary to each other. 
In order to further compare with other methods, 
we also do experiments on the dataset of English 
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of 
Sections 02-21, 00 and 23. Table 3 compares our 
method with other previously best-reported methods 
with the same setting as discussed previously. It 
shows that our method outperforms the previous 
best-reported one with a relative error rate reduction 
of 10.8% (0.97/(100-91)). This further verifies the 
effectiveness of the grammar-driven kernel method 
for semantic role classification. 
  
Method Accuracy (%)
Ours (Composite Kernel)      91.97 
Moschitti (2006): PAF kernel only    87.7 
Jiang et al (2005): feature based    90.50 
Pradhan et al (2005a): feature based    91.0 
 
Table 3: Performance comparison between our 
method and previous work 
 
Training Time Method 
  4 Sections  19 Sections
Ours: grammar-
driven tree kernel 
~8.1 hours ~7.9 days 
Moschitti (2006): 
non-grammar-driven 
tree kernel 
~7.9 hours ~7.1 days 
 
Table 4: Training time comparison 
 
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires 
exponential time in its worst case, however, in 
practice, it may only need 1 2(| | | |)O N N?  or lin-
ear and 2) it is very time-consuming to train a SVM 
classifier in a large dataset.  
5 Conclusion and Future Work 
In this paper, we propose a novel grammar-driven 
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in 
the new kernel design. The experimental results 
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than 
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of 
substructures and node features. We also discuss 
the criteria to determine the optional nodes in a 
206
CFG rule in defining our grammar-driven convolu-
tion tree kernel. 
The extension of our work is to improve the per-
formance of the entire semantic role labeling system 
using the grammar-driven tree kernel, including all 
four stages: pruning, semantic role identification, 
classification and post inference. In addition, a 
more interesting research topic is to study how to 
integrate linguistic knowledge and tree kernel 
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al, 2004). In 
detail, a linguistics and statistics-based theory that 
can suggest the effectiveness of different substruc-
ture features and whether they should be generated 
or not by the tree kernels would be worked out. 
References  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The 
Berkeley FrameNet Project. COLING-ACL-1998  
Xavier Carreras and Llu?s M?rquez. 2004. Introduction to 
the CoNLL-2004 shared task: Semantic role labeling. 
CoNLL-2004  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labeling. 
CoNLL-2005  
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings ofNAACL-2000 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for semantic 
role labeling. COLING-ACL-2006(poster) 
Michael Collins and Nigel Duffy. 2001. Convolution 
kernels for natural language. NIPS-2001 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics, 
28(3):245?288 
David Haussler. 1999. Convolution kernels on discrete 
structures. Technical Report UCSC-CRL-99-10 
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument 
interdependence. IJCAI-2005 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello 
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419?444 
Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational 
Linguistics, 19(2):313?330 
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling 
case. HLT-NAACL-2006 (short paper)  
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing 
weak learners in semantic parsing. EMNLP-2004 
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1) 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne 
Ward, James H. Martin and Daniel Jurafsky. 2005a. 
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James 
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005 
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear 
programming inference. COLING-2004 
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. IJCAI-2005 
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using 
LTAG based features in parse reranking. EMNLP-03 
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. 
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004 
Vladimir N. Vapnik. 1998. Statistical Learning Theory. 
Wiley 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction. 
Machine Learning Research, 3:1083?1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006 
207
A New Chinese Natural Language Understanding Architecture
Based on Multilayer Search Mechanism
Wanxiang Che Ting Liu Sheng Li
School of Computer Science and Technology
Harbin Institute of Technology
P.O. Box 321, HIT
Harbin China, 150001
{car, tliu, ls}@ir.hit.edu.cn
Abstract
A classical Chinese Natural Language Under-
standing (NLU) architecture usually includes
several NLU components which are executed
with some mechanism. A new Multilayer Search
Mechanism (MSM) which integrates and quan-
tifies these components into a uniform multi-
layer treelike architecture is presented in this
paper. The mechanism gets the optimal re-
sult with search algorithms. The components
in MSM affect each other. At last, the per-
formance of each component is enhanced. We
built a practical system ? CUP (Chinese Under-
standing Platform) based on MSM with three
layers. By the experiments on Word Segmen-
tation, a better performance was achieved. In
theory the normal cascade and feedback mech-
anism are just some special cases of MSM.
1 Introduction
At present a classical Chinese NLU architec-
ture usually includes several components, such
as Word Segmentation (Word-Seg), POS Tag-
ging, Phrase Analysis, Parsing, Word Sense Dis-
ambiguation (WSD) and so on. These compo-
nents are executed one by one from lower layers
(such as Word-Seg, POS Tagging) to higher lay-
ers (such as Parsing, WSD) to form a kind of
cascade mechanism. But when people build a
NLU system based on these complex language
analysis, it is a very serious problem since the
errors of each layer component are multiplied.
With more and more analysis components, the
final result becomes too bad to be applicable.
Another problem is that the components in
the system affect each other when people build a
practical but toy NLU system. Here the toy sys-
tem means that each component is ideal enough
with perfect input. But in fact, on the one hand
the lower layer components need the informa-
tion of higher layer components; on the other
hand the incorrect analysis of lower layers must
reduce the accuracy of higher layers. In Chinese
Word-Seg component, many segmentation am-
biguities which cannot be solved using only lexi-
cal information. In order to improve the perfor-
mance of Word-Seg, we have to use some syntax
and even semantic information. Without cor-
rect Word-Seg results, however the syntax and
semantic parser cannot obtain a correct analy-
sis. It is a chain debts problem.
People have tried to solve the error-multiplied
problem by integrating multi-layers into a uni-
form model (Gao et al, 2001; Nagata, 1994).
But with the increasing number of integrated
layers, the model becomes too complex to build
or solve.
The feedback mechanism (Wu and Jiang,
1998) helps to use the information of high lay-
ers to control the final result. If the analysis
at feedback point cannot be passed, the whole
analysis will be denied. This mechanism places
too much burden on the function of feedback
point. This leads to the problems that a correct
lower layer result may be rejected or an error
result may be accepted.
We propose a new Multilayer Search Mecha-
nism (MSM) to solve the problems mentioned
above. Based on the mechanism, we build a
practical Chinese NLU platform ? CUP (Chi-
nese Understanding Platform). Section 2 intro-
duces the background and architecture of the
new mechanism and how to build it up. Exper-
imental results with CUP is given in Section 3.
In Section 4, we discuss why the new mechanism
gets better results than the old ones. Conclu-
sions and the some future work follow in Sec-
tion 5.
2 Multilayer Search Mechanism
The novel Multilayer Search Mechanism (MSM)
integrates and quantifies NLU components into
a uniform multilayer treelike platform, such as
Word-Seg, POS Tagging, Parsing and so on.
These components affect each other by comput-
ing the final score and then get better results.
2.1 Background
Considering a Chinese sentence, the sen-
tence analysis task can be formally defined
as finding a set of word segmentation se-
quence (W ), a POS tagging sequence (POS),
a syntax dependency parsing tree (DP ) and
so on which maximize their joint probability
P (W,POS,DP, ? ? ?). In this paper, we assume
that there are only three layers W , POS and
DP in MSM. It is relatively straightforward,
however, to extend the method to the case for
which there are more than three layers. There-
fore, the sentence analysis task can be described
as finding a triple < W,POS,DP > that max-
imize the joint probability P (W,POS,DP ).
< W,POS,DP >= arg max
W,POS,DP
P (W,POS,DP )
The joint probability distribution
P (W,POS,DP ) can be written in the fol-
lowing form using the chain rule of probability:
P (W,POS,DP )
=P (W )P (POS|W )P (DP |W,POS)
Where P (W ) is considered as the probabil-
ity of the word segmentation layer, P (POS|W )
is the conditional probability of POS Tag-
ging with a given word segmentation result,
P (DP |W,POS) is the conditional probability
of a dependency parsing tree with a given word
segmentation and POS Tagging result similarly.
So the form of < W,POS,DP > can be trans-
formed into:
< W,POS,DP >
= arg max
W,POS,DP
P (W,POS,DP )
= arg max
W,POS,DP
P (W )P (POS|W )P (DP |W,POS)
= arg max
W,POS,DP
logP (W ) + logP (POS|W )
+ logP (DP |W,POS)
= arg min
W,POS,DP
? logP (W )? logP (POS|W )
? logP (DP |W,POS)
We consider that each inversion of probability?s
logarithm at the last step of the above equation
is a score given by a component (Such as Word-
Seg, POS Tagging and so on). So at last, we find
an n-tuple < W,POS,DP, ? ? ? > that minimizes
the last score Sn of a sentence analysis result
with n layers. Sn is defined as:
Sn = s1 + s2 + ? ? ?+ sn (1)
si denotes the score of the ith layer compo-
nent.
2.2 The Architecture of Multilayer
Search Mechanism
Because there are lots of analysis results at each
layer, it?s a combinatorial explosion problem to
find the optimal result. Assuming that each
component produces m results for an input on
average and there are n layers in a NLU system,
the final search space is mn. With the increas-
ing of n, it?s impossible for a system to find the
optimal result in the huge search space.
The classical cascade mechanism uses a
greedy algorithm to solve the problem. It only
keeps the optimal result at each layer. But if
it?s a fault analysis result for the optimal result
at a layer, it?s impossible for this mechanism to
find the final correct analysis result.
To overcome the difficulty, we build a new
Multilayer Search Mechanism (MSM). Different
from the cascade mechanism, MSM maintains a
number of results at each component, so that
the correct analysis should be included in these
results with high probability. Then MSM tries
to use the information of all layer components
to find out the correct analysis result. Different
from the feedback mechanism, the acceptance
of an analysis is not based on a higher layer
components alone. The lower layer components
provide some information to help to find the
correct analysis result as well.
According to the above idea, we design the
architecture of MSM with multilayer treelike
structure. The original input is root and the
several analysis results of the input become
branches. Iterating this progress, we get a big-
ger analysis tree. Figure 1 gives an analysis ex-
ample of a Chinese sentence ????????
??? (He likes beautiful flowers). For the in-
put sentence, there are several Word-Seg results
with scores (the lower the better). Then for each
of Word-Seg results, there are several POS Tag-
ging results, too. And for each of POS Tagging
result, the same thing happens. So we get a big
tree structure and the correct analysis result is a
path in the tree from the root to the leaf except
for there is no correct analysis result in some
analysis components.
A search algorithm can be used to find out the
correct analysis result among the lowest score in
the tree. But because each layer cannot give the
exact score in Equation 1 as the standard score
and the ability of analysis are different with
different layers, we should weight every score.
Then the last score is the linear weighted sum
(Equation 2).
Sn = w1s1 + w2s2 + ? ? ?+ wnsn (2)
si denotes the score of the ith layer compo-
nent which we will introduce in Section 3; wi
denotes the weight of the ith layer components
which we will introduce in the next section.
In order to get the optimal result, all kinds
of tree search algorithms can be used. Here
the BEST-FIRST SEARCH Algorithm (Rus-
sell and Norvig, 1995) is used. Figure 2 shows
the main algorithm steps.
POS Tag
Word-Seg
??
?????????
56.2? ?? ?? ? ??? 87.3? ? ?? ? ? ???
108.3? ?? ?? ? ???  r      v        a      u      n
187.4? ?? ?? ? ???  b      v        a      u      n
Figure 1: An Example of Multilayer Search
Mechanism
1. Add the initial node (starting point) to the
queue.
2. Compare the front node to the goal state. If
they match then the solution is found.
3. If they do not match then expand the front
node by adding all the nodes from its links.
4. If all nodes in the queue are expanded then the
goal state is not found (e.g.there is no solution).
Stop.
5. According to Equation 2 evaluate the score of
expanded nodes and reorder the nodes in the
queue.
6. Go to step 2.
Figure 2: BEST-FIRST SEARCH Algorithm
2.3 Layer Weight
We should find out a group of appropriate
w1, w2, ? ? ? , wn in Equation 2 to maximize the
number of the optimal paths in MSM which can
get the correct results. They are expressed by
W ?.
W ? = arg max
W
ObjFun(minSn) (3)
Here W ? is named as Whole Layer Weight.
ObjFun(?) denotes a function to value the re-
sult that a group ofW can get. Here we can con-
sider that the performance of each layer is pro-
portional to the last performance of the whole
system in MSM. So it maybe the F-Score of
Word-Seg, precision of POS Tagging and so on.
minSn returns the optimal analysis results with
the lowest score.
Here, the F-Score of Word-Seg can be defined
as the harmonic mean of recall and precision of
Word-Seg. That is to say:
Seg.F -Score = 2 ? Seg.Pre ? Seg.RecSeg.Pre+ Seg.Rec
Seg.Pre = #words correctly segmented#words segmented
Seg.Rec = #words correctly segmented#words in input texts
Finding out the most suitable group of W
is an optimization problem. Genetic Algo-
rithms (GAs) (Mitchell, 1996) is just an adap-
tive heuristic search algorithm based on the evo-
lutionary ideas of natural selection and genetics
to solve optimization problems. It exploits his-
torical information to direct the search into the
region of better performance within the search
space.
To use GAs to solve optimization prob-
lems (Wall, 1996) the following three questions
should be answered:
1. How to describ genome?
2. What is the objective function?
3. Which controlling parameters to be se-
lected?
A solution to a problem is represented as a
genome. The genetic algorithm then creates a
population of solutions and applies genetic op-
erators such as mutation and crossover to evolve
the solutions in order to find the best one(s) af-
ter several generations. The numbers of popula-
tion and generation are given by controlling pa-
rameters. The objective function decides which
solution is better than others.
In MSM, the genome is just the group of W
which can be denoted by real numbers between
0 and 1. Because the result is a linear weighted
sum, we should normalize the weights to let w1+
w2+ ? ? ?+wn = 1. The objective function is just
ObjFun(?) in Equation 3. Here the F-Score
of Word-Seg is used to describe it. We set the
genetic generations as 10 and the populations in
one generation as 30. The Whole Layer Weight
shows in the row of WLW in Table 4. The F-
Score of Word-Seg shows as Table 3.
We can see that the Word-Seg layer gets an
obviously large weight. So the final result is
inclined to the result of Word-Seg.
2.4 Self Confidence
Our analysis indicates that the method of
weighting a whole layer uniformly cannot re-
flect the individual information of each sen-
tence to some component. So the F-Score of
Word-Seg drops somewhat comparing with us-
ing Only Word-Seg. For example, the most
sentences which have ambiguities in Word-Seg
component are still weighted high with Word-
Seg layer weight. Then the final result may still
be the same as the result of Word-Seg compo-
nent. It is ambiguous, too. So we must use a
parameter to decrease the weight of a compo-
nent with ambiguity. It is used to describe the
analysis ability of a component for an input. We
name it as Self Confident (SC) of a component.
It is described by the difference between the first
and the second score of a component. Then the
bigger SC of a component, the larger weight of
it.
There are lots of methods to value the differ-
ence between two numbers. So there are many
kinds of definitions of SC. We use A and B to
denote the first and the second score of a compo-
nent respectively. Then the SC can be defined
as B?A, BA and so on. We must select the bet-ter one to represent SC. The better means that
a method which gets a lower Error Rate with a
threshold t? which gets the Minimal Error Rate.
t? = arg min
t
ErrRate(t)
ErrRate(t) denotes the Error Rate with the
threshold t. An error has two definitions:
? SC is higher than t but the first result is
fault
? SC is lower than t but the first result is
right
Then the Error Rate is the ratio between the
error number and the total number of sentences.
Table 2 is the comparison list between differ-
ent definitions of SC and their Minimal Error
Rate of Word-Seg. By this table we select B?A
as the last SC because it gets the minimal Min-
imal Error Rate within the different definitions
of SC.
SC is added into Equation 2 to describe the
individual information of each sentence inten-
sively. Equation 4 shows the new score method
of a path.
Sn = w1sc1s1 + w2sc2s2 + ? ? ?+ wnscnsn (4)
sci denotes the SC of a component in the ith
layer.
3 Experimental Results
3.1 Score of Components
We build a practical system CUP (Chinese
Understanding Platform) based on MSM with
three layers ? Word-Seg, POS Tagging and
Parsing. Each component not only provides the
n-best analysis result, but also the score of each
result.
In the Word-Seg component, we use the uni-
gram model (Liu et al, 1998) to value different
results of Word-Seg. So the score of a result is:
ScoreWord?Seg = ? logP (W ) = ?
?
logP (wi)
wi denotes the ith word in the Word-Seg re-
sult of a sentence.
In the POS Tagging component the classical
Markov Model (Manning and Schu?tze, 1999) is
used to select the n-best POS results of each
Word-Seg result. So the score of a result is:
ScorePOS =? logP (POS|W )
=? log P (W |POS)P (POS)P (W )
=?
?
logP (wi|ti)?
?
logP (ti|ti?1)
+ logP (W )
ti denotes the POS of the ith word in a Word-
Seg result of a sentence.
In the Parsing component, we use a Chinese
Dependency Parser System developed by HIT-
IRLab1. The score of a result is:
ScoreParsing =? logP (DP |W,POS)
=? log P (W,POS,DP )P (W,POS)
=?
?
logP (lij)
+ logP (W,POS)
lij denotes a link between the ith and jth
word in a Word-Seg and POS Tagging result
of a sentence.
Table 1 gives the one and five-best results of
each component with a correct input. The test
data comes from Beijing Univ. and Fujitsu Chi-
nese corpus (Huiming et al, 2000). The F-Score
is used to value the performance of the Word-
Seg, Precision to POS Tagging and the correct
rate of links to Parsing.
Table 1: The five-best results of each compo-
nent
1-best 5-best
Word-Seg 87.83% 94.45%
POS Tag 85.34% 93.28%
Parsing 80.25% 82.13%
3.2 Self Confidence Selection
In order to select a better SC, we test all kinds of
definition form to calculate their Minimal Error
Rate. For example B?A, BA and so on. A and Bdenote the first and the second score of a com-
ponent respectively. Table 2 shows the relation-
ship between definition forms of SC and their
Minimal Error Rate. Here, we experimented
with the first and the second Word-Seg results
of more than 7100 Chinese sentences.
3.3 F-Score of Word-Seg
The result of Word-Seg is used to test our
system?s performance, which means that the
ObjFun(?) returns the F-Score of Word-Seg.
There are 1,500 sentences as training data
and 500 sentences as test data. Among these
data about 10% sentences have ambiguities and
the others come from Beijing Univ. and Fujitsu
1The Parser has not been published still.
Chinese corpus (Huiming et al, 2000). In CUP
the five-best results of each component are se-
lected. Table 3 lists the F-Score of Word-Seg.
They use Only Word-Seg (OWS), Whole Layer
Weight (WLW), SC (SC) and FeedBack mecha-
nism (FB) separately. Using the feedback mech-
anism means that the last analysis result of a
sentence is decided by the Parsing. We select
the result which has the lowest score of Pars-
ing. Table 4 shows the weight distributions in
WLW and SC weighting methods.
3.4 The Efficiency of CUP
The efficiency test of CUP was done with 7112
sentences with 20 Chinese characters averagely.
It costs 58.97 seconds on a PC with PIV 2.0
CPU and 512M memory. The average cost of a
sentence is 0.0083 second.
4 Discussions
According to Table 1, we can see that the per-
formance of each component improved with the
increasing of the number of results. But at the
same time, the processing time must increase.
So we should balance the efficiency and effec-
tiveness with an appropriate number of results.
Thus, it?s more possible for CUP to find out the
correct analysis than the original cascade mech-
anism if we can invent an appropriate method.
We define SC as B ? A which gets the mini-
mal Minimal Error Rate with the analysis of the
Table 2: SC and Minimal Error Rate
Definition Form of SC Minimal
Error Rate
1
A ? 1B 23.85%B ?A 21.07%
B
A 23.98%B
A ? AB 23.98%B?A
length of a sentence 24.12%
B?A
length of a sentence+100 23.71%
Table 3: F-Score of Word-Seg
OWS WLW SC FB
F-Score 86.99% 85.80% 88.13% 80.72%
Table 4: Layer Weight
1-layer 2-layer 3-layer
In WLW 0.84 0.12 0.04
In SC 0.44 0.40 0.16
Table 2. Take the case of Word Segmentation:
B ?A =
?
i
logP (wAi )?
?
j
logP (wBj )
It?s just the difference between logarithms of
different word results? probability of the first
and the second result of Word Segmentation.
Table 3 shows that MSM using SC gets a bet-
ter performance than other methods. For a Chi-
nese sentence ???????????. (There
are some drinks under the table). The CUP
gets the correct analysis ? ???/n ?/nd ?/v
?/u ?/m ?/q ?/n ?/w?. But the cascade
and feedback mechanism?s result is ???/n ?
?/v ?/u ?/m ?/q ?/n ?/w?.
The cascade mechanism uses the Only Word-
Seg result. In this method P (??) is more
than P (?) ? P (?). At the same time, the
wrong analysis is a grammatical sentence and
is accepted by Parsing. These create that these
two mechanisms cannot get the correct result.
But the MSM synthesizes all the information of
Word-Seg, POS Tagging and Parsing. Finally
it gets the correct analysis result.
Now, CUP integrates three layers and its effi-
ciency is high enough for practical applications.
5 Conclusions and Future Work
A new Chinese NLU architecture based on Mul-
tilayer Search Mechanism (MSM) integrates al-
most all of NLU components into a uniform
multilayer treelike platform and quantifies these
components to use the search algorithm to find
out the optimal result. Thus any component
can be added into MSM conveniently. They
only need to accept an input and give several
outputs with scores. By experiments we can see
that a practical system ? CUP based on MSM
improves the performance of Word-Seg to a cer-
tain extent. And its efficiency is high enough for
most practical applications.
The cascade and the feedback mechanism are
JUST the special cases of MSM. If greedy algo-
rithm is used at each layer to expand the result
with the lowest score, MSM becomes the cas-
cade mechanism. If the weight of each layer
except the feedback point is set 0, the MSM be-
comes the feedback mechanism.
In the future we are going to add the Phrase
Analysis, WSD (Word Sense Disambiguation)
and Semantic Analysis components into CUP,
because it is impossible to analyze some sen-
tences correctly without semantic understand-
ing and the Phrase Analysis helps to en-
hance the performance of Parsing. At last,
CUP becomes a whole Chinese NLU platform
with Word-Seg, POS Tagging, Phrase Analy-
sis, Parsing, WSD and Semantic Analysis, six
components from lower layers to higher layers.
Under the framework of MSM, it becomes very
easy to add these components.
With the increasing of layers the handle speed
must decrease. So some heuristic search algo-
rithms will be used to improve the speed of
searching while enhancing the speed of each
component. Under the MSM framework, we can
do these easily.
The performance of each component should
be improved in the future. At least, it is impos-
sible for MSM to find out the correct analysis
result if there is a component which cannot give
a correct result within n-best results with a cor-
rect input. In addition, we are going to evalu-
ate the performance of each component not just
Word-Seg only.
6 Acknowledgements
We thank Liqi Gao and Zhuoran Wang provide
the Word Segmentation tool, Wei He provide
the POS Tagging tool and Jinshan Ma provide
the Parser tool for us. We acknowledge Dekang
Lin for his valuable comments on the earlier ver-
sions of this paper. This work was supported by
NSFC 60203020.
References
Shan Gao, Yan Zhang, Bo Xu, ChengQing
Zong, ZhaoBing Han, and RangShen Zhang.
2001. The research on integrated chinese
words segmentation and labeling based on tri-
gram statistic model. In Proceedings of IJCL-
2001, Tai Yuan, Shan Xi, China.
Duan Huiming, Song Jing, Xu Guowei,
Hu Guoxin, and Yu Shiwen. 2000. The de-
velopment of a large-scale tagged chinese cor-
pus and its applications. Applied Linguistics,
(2):72?77.
Ting Liu, Yan Wu, and Kaizhu Wang. 1998.
The problem and algorithm of maximal prob-
ability word segmentation. Journal of Harbin
Institute of Technology, 30(6):37?41.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, Massachusetts.
Melanie Mitchell. 1996. An Introduction to
Genetic Algorithms. The MIT Press, Cam-
bridge, Massachusetts.
Masaaki Nagata. 1994. A stochastic japanese
morphological analyzer using a forward-dp
backward-A* n-best search algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics, pages
201?207.
Stuart Russell and Peter Norvig. 1995. Artifi-
cial Intelligence: A Modern Approach. Pren-
tice Hall Series in Artificial Intelligence, En-
glewood Cliffs, NJ, USA.
Matthew Wall. 1996. GAlib: A C++ Li-
brary of Genetic Algorithms components.
http://lancet.mit.edu/ga/.
Andi Wu and Zixin Jiang. 1998. Word segmen-
tation in sentence analysis. In Proceedings
of the 1998 International Conference on Chi-
nese Information Processing, pages 169?180,
Beijing, China.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 189?192, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Lableing System using Maximum Entropy Classier ?
Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu and Huaijun Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology
China, 150001
{tliu, car, ls, yxhu, hjliu}@ir.hit.edu.cn
Abstract
A maximum entropy classifier is used in
our semantic role labeling system, which
takes syntactic constituents as the labeling
units. The maximum entropy classifier is
trained to identify and classify the predi-
cates? semantic arguments together. Only
the constituents with the largest probabil-
ity among embedding ones are kept. Af-
ter predicting all arguments which have
matching constituents in full parsing trees,
a simple rule-based post-processing is ap-
plied to correct the arguments which have
no matching constituents in these trees.
Some useful features and their combina-
tions are evaluated.
1 Introduction
The semantic role labeling (SRL) is to assign syn-
tactic constituents with semantic roles (arguments)
of predicates (most frequently verbs) in sentences.
A semantic role is the relationship that a syntactic
constituent has with a predicate. Typical semantic
arguments include Agent, Patient, Instrument, etc.
and also adjunctive arguments indicating Locative,
Temporal, Manner, Cause, etc. It can be used in
lots of natural language processing application sys-
tems in which some kind of semantic interpretation
is needed, such as question and answering, informa-
tion extraction, machine translation, paraphrasing,
and so on.
?This research was supported by National Natural Science
Foundation of China via grant 60435020
Last year, CoNLL-2004 hold a semantic role la-
beling shared task (Carreras and Ma`rquez, 2004)
to test the participant systems? performance based
on shallow syntactic parser results. In 2005, SRL
shared task is continued (Carreras and Ma`rquez,
2005), because it is a complex task and now it is
far from desired performance.
In our SRL system, we select maximum en-
tropy (Berger et al, 1996) as a classifier to im-
plement the semantic role labeling system. Dif-
ferent from the best classifier reported in litera-
tures (Pradhan et al, 2005) ? support vector ma-
chines (SVMs) (Vapnik, 1995), it is much eas-
ier for maximum entropy classifier to handle the
multi-class classification problem without additional
post-processing steps. The classifier is much faster
than training SVMs classifiers. In addition, max-
imum entropy classifier can be tuned to minimize
over-fitting by adjusting gaussian prior. Xue and
Palmer (2004; 2005) and Kwon et al (2004) have
applied the maximum entropy classifier to semantic
role labeling task successfully.
In the following sections, we will describe our
system and report our results on development and
test sets.
2 System Description
2.1 Constituent-by-Constituent
We use syntactic constituent as the unit of labeling.
However, it is impossible for each argument to find
its matching constituent in all auto parsing trees. Ac-
cording to statistics, about 10% arguments have no
matching constituents in the training set of 245,353
189
constituents. The top five arguments with no match-
ing constituents are shown in Table 1. Here, Char-
niak parser got 10.08% no matching arguments and
Collins parser got 11.89%.
Table 1: The top five arguments with no matching
constituents.
Args Cha parser Col parser Both
AM-MOD 9179 9205 9153
A1 5496 7273 3822
AM-NEG 3200 3217 3185
AM-DIS 1451 1482 1404
A0 1416 2811 925
Therefore, we can see that Charniak parser got a
better result than Collins parser in the task of SRL.
So we use the full analysis results created by Char-
niak parser as our classifier?s inputs. Assume that
we could label all AM-MOD and AM-NEG arguments
correctly with simple post processing rules, the up-
per bound of performance could achieve about 95%
recall.
At the same time, we can see that for some ar-
guments, both parsers got lots of no matchings such
as AM-MOD, AM-NEG, and so on. After analyzing
the training data, we can recognize that the perfor-
mance of these arguments can improve a lot after
using some simple post processing rules only, how-
ever other arguments? no matching are caused pri-
marily by parsing errors. The comparison between
using and not using post processing rules is shown
in Section 3.2.
Because of the high speed and no affection in the
number of classes with efficiency of maximum en-
tropy classifier, we just use one stage to label all ar-
guments of predicates. It means that the ?NULL?
tag of constituents is regarded as a class like ?ArgN?
and ?ArgM?.
2.2 Features
The following features, which we refer to as the
basic features modified lightly from Pradhan et
al. (2005), are provided in the shared task data for
each constituent.
? Predicate lemma
? Path: The syntactic path through the parse tree from the
parse constituent to the predicate.
? Phrase type
? Position: The position of the constituent with respect to
its predicate. It has two values, ?before? and ?after?,
for the predicate. For the situation of ?cover?, we use
a heuristic rule to ignore all of them because there is no
chance for them to become an argument of the predicate.
? Voice: Whether the predicate is realized as an active or
passive construction. We use a simple rule to recognize
passive voiced predicates which are labeled with part of
speech ? VBN and sequences with AUX.
? Head word stem: The stemming result of the con-
stituent?s syntactic head. A rule based stemming algo-
rithm (Porter, 1980) is used. Collins Ph.D thesis (Collins,
1999)[Appendix. A] describs some rules to identify the
head word of a constituent. Especially for prepositional
phrase (PP) constituent, the normal head words are not
very discriminative. So we use the last noun in the PP
replacing the traditional head word.
? Sub-categorization
We also use the following additional features.
? Predicate POS
? Predicate suffix: The suffix of the predicate. Here, we
use the last 3 characters as the feature.
? Named entity: The named entity?s type in the constituent
if it ends with a named entity. There are four types: LOC,
ORG, PER and MISC.
? Path length: The length of the path between a constituent
and its predicate.
? Partial path: The part of the path from the constituent
to the lowest common ancestor of the predicate and the
constituent.
? Clause layer: The number of clauses on the path between
a constituent and its predicate.
? Head word POS
? Last word stem: The stemming result of the last word of
the constituent.
? Last word POS
We also use some combinations of the above fea-
tures to build some combinational features. Lots of
combinational features which were supposed to con-
tribute the SRL task of added one by one. At the
same time, we removed ones which made the per-
formance decrease in practical experiments. At last,
we keep the following combinations:
? Position + Voice
? Path length + Clause layer
? Predicate + Path
? Path + Position + Voice
? Path + Position + Voice + Predicate
? Head word stem + Predicate
? Head word stem + Predicate + Path
? Head word stem + Phrase
? Clause layer + Position + Predicate
All of the features and their combinations are used
without feature filtering strategy.
190
2.3 Classifier
Le Zhang?s Maximum Entropy Modeling Toolkit 1,
and the L-BFGS parameter estimation algorithm
with gaussian prior smoothing (Chen and Rosenfeld,
1999) are used as the maximum entropy classifier.
We set gaussian prior to be 2 and use 1,000 itera-
tions in the toolkit to get an optimal result through
some comparative experiments.
2.4 No Embedding
The system described above might label two con-
stituents even if one embeds in another, which is not
allowed by the SRL rule. So we keep only one ar-
gument when more arguments embedding happens.
Because it is easy for maximum entropy classifier to
output each prediction?s probability, we can label the
constituent which has the largest probability among
the embedding ones.
2.5 Post Processing Stage
After labeling the arguments which are matched
with constituents exactly, we have to handle the ar-
guments, such as AM-MOD, AM-NEG and AM-DIS,
which have few matching with the constituents de-
scribed in Section 2.1. So a post processing is given
by using some simply rules:
? Tag target verb and successive particles as V.
? Tag ?not? and ?n?t? in target verb chunk as AM-NEG.
? Tag modal verbs in target verb chunk, such as words with
POS of ?MD?, ?going to?, and so on, as AM-MOD.
? Tag the words with POS of ?CC? and ?RB? at the start of
a clause which include the target verb as AM-DIS.
3 Experiments
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of
PropBank corpus. It consists of the sections from
the Wall Street Journal part of Penn Treebank. Sec-
tions 02-21 are training sets, and Section 24 is devel-
opment set. The results are evaluated for precision,
recall and F?=1 numbers using the srl-eval.pl script
provided by the shared task organizers.
3.2 Post Processing
After using post processing rules, the final F?=1 is
improved from 71.02% to 75.27%.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
3.3 Performance Curve
Because the training corpus is substantially en-
larged, this allows us to test the scalability of
learning-based SRL systems to large data set and
compute learning curves to see how many data are
necessary to train. We divide the training set, 20
sections Penn Treebank into 5 parts with 4 sections
in each part. There are about 8,000 sentences in each
part. Figure 1 shows the change of performance as
a function of training set size. When all of training
data are used, we get the best system performance as
described in Section 3.4.
Figure 1: Our SRL system performance curve (of
F?=1) effecting of the training set size.
We can see that as the training set becomes larger
and larger, so does the performance of SRL system.
However, the rate of increase slackens. So we can
say that at present state, the larger training data has
favorable effect on the improvement of SRL system
performance.
3.4 Best System Results
In all the experiments, all of the features and their
combinations described above are used in our sys-
tem. Table 2 presents our best system performance
on the development and test sets.
From the test results, we can see that our system
gets much worse performance on Brown corpus than
WSJ corpus. The reason is easy to be understood
for the dropping of automatic syntactic parser per-
formance on new corpus but WSJ corpus.
The training time on PIV 2.4G CPU and 1G Mem
machine is about 20 hours on all 20 sections, 39,832-
191
Precision Recall F?=1
Development 79.65% 71.34% 75.27
Test WSJ 80.48% 72.79% 76.44
Test Brown 71.13% 59.99% 65.09
Test WSJ+Brown 79.30% 71.08% 74.97
Test WSJ Precision Recall F?=1
Overall 80.48% 72.79% 76.44
A0 88.14% 83.61% 85.81
A1 79.62% 72.88% 76.10
A2 73.67% 65.05% 69.09
A3 76.03% 53.18% 62.59
A4 78.02% 69.61% 73.58
A5 100.00% 40.00% 57.14
AM-ADV 59.85% 48.02% 53.29
AM-CAU 68.18% 41.10% 51.28
AM-DIR 56.60% 35.29% 43.48
AM-DIS 76.32% 72.50% 74.36
AM-EXT 83.33% 46.88% 60.00
AM-LOC 65.31% 52.89% 58.45
AM-MNR 58.28% 51.16% 54.49
AM-MOD 98.52% 96.37% 97.43
AM-NEG 97.79% 96.09% 96.93
AM-PNC 43.68% 33.04% 37.62
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 78.38% 66.70% 72.07
R-A0 81.70% 85.71% 83.66
R-A1 77.62% 71.15% 74.25
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 83.33% 47.62% 60.61
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 77.27% 65.38% 70.83
V 98.71% 98.71% 98.71
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
sentences training set with 1,000 iterations and more
than 1.5 million samples and 2 million features.
The predicting time is about 160 seconds on 1,346-
sentences development set.
4 Conclusions
We have described a maximum entropy classifier
is our semantic role labeling system, which takes
syntactic constituents as the labeling units. The
fast training speed of the maximum entropy clas-
sifier allows us just use one stage of arguments
identification and classification to build the system.
Some useful features and their combinations are
evaluated. Only the constituents with the largest
probability among embedding ones are kept. Af-
ter predicting all arguments which have matching
constituents in full parsing trees, a simple rule-
based post-processing is applied to correct the ar-
guments which have no matching constituents. The
constituent-based method depends much on the syn-
tactic parsing performance. The comparison be-
tween WSJ and Brown test sets results fully demon-
strates the point of view.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the conll-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL-2004, pages 89?97, Boston,
MA, USA.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2005.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Pennsyl-
vania University.
Namhee Kwon, Michael Fleischman, and Eduard Hovy.
2004. Framenet-based semantic parsing using maxi-
mum entropy models. In Proc. Coling 2004.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3).
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Vladamir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proc. EMNLP
2004.
Nianwen Xue and Martha Palmer. 2005. Automatic se-
mantic role labeling for chinese verbs. In Proc. IJCAI
2005.
192
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 165?168,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT-IR-WSD: A WSD System for English Lexical Sample Task 
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang and Ting Liu 
Information Retrieval Lab 
Harbin Institute of technology 
Harbin, China, 150001 
{yhguo,wxche}@ir.hit.edu.cn 
 
 
Abstract 
HIT-IR-WSD is a word sense disambigua-
tion (WSD) system developed for English 
lexical sample task (Task 11) of Semeval 
2007 by Information Retrieval Lab, Harbin 
Institute of Technology. The system is 
based on a supervised method using an 
SVM classifier. Multi-resources including 
words in the surrounding context, the part-
of-speech of neighboring words, colloca-
tions and syntactic relations are used. The 
final micro-avg raw score achieves 81.9% 
on the test set, the best one among partici-
pating runs. 
1 Introduction 
Lexical sample task is a kind of WSD evaluation 
task providing training and test data in which a 
small pre-selected set of target words is chosen and 
the target words are marked up. In the training data 
the target words? senses are given, but in the test 
data are not and need to be predicted by task par-
ticipants. 
HIT-IR-WSD regards the lexical sample task 
as a classification problem, and devotes to extract 
effective features from the instances. We didn?t use 
any additional training data besides the official 
ones the task organizers provided. Section 2 gives 
the architecture of this system. As the task pro-
vides correct word sense for each instance, a su-
pervised learning approach is used. In this system, 
we choose Support Vector Machine (SVM) as 
classifier. SVM is introduced in section 3. Know-
ledge sources are presented in section 4. The last 
section discusses the experimental results and 
present the main conclusion of the work performed. 
2 The Architecture of the System 
HIT-IR-WSD system consists of 2 parts: feature 
extraction and classification. Figure 1 portrays the 
architecture of the system. 
 
Figure?1:?The?architecture?of?HIT?IR?WSD?
165
Features are extracted from original instances 
and are made into digitized features to feed the 
SVM classifier. The classifier gets the features of 
training data to make a model of the target word. 
Then it uses the model to predict the sense of target 
word in the test data. 
3 Learning Algorithm 
SVM is an effective learning algorithm to WSD 
(Lee and Ng, 2002). The SVM tries to find a 
hyperplane with the largest margin separating the 
training samples into two classes. The instances in 
the same side of the hyperplane have the same 
class label. A test instance?s feature decides the 
position where the sample is in the feature space 
and which side of the hyperplane it is. In this way, 
it leads to get a prediction. SVM could be extended 
to tackle multi-classes problems by using one-
against-one or one-against-rest strategy. 
In the WSD problem, input of SVM is the fea-
ture vector of the instance. Features that appear in 
all the training samples are arranged as a vector 
space. Every instance is mapped to a feature vector. 
If the feature of a certain dimension exists in a 
sample, assign this dimension 1 to this sample, else 
assign it 0. For example, assume the feature vector 
space is <x1, x2, x3, x4, x5, x6, x7>; the instance is 
?x2 x6 x5 x7?. The feature vector of this sample 
should be <0, 1, 0, 0, 1, 1, 1>.  
The implementation of SVM here is libsvm 1 
(Chang and Lin, 2001) for multi-classes. 
4 Knowledge Sources 
We used 4 kinds of features of the target word and 
its context as shown in Table 1. 
Part of the original text of an example is ?? 
This is the <head>age</head> of new media , the 
era of ??. 
Name Extraction Tools Example 
Surrounding 
words 
WordNet 
(morph)2 
?, this, be, age, new, 
medium, ,, era, ? 
Part-of-
speech SVMTool
3 
DT_0, VBZ_0, DT_0, 
NN_t, IN_1, JJ_1, 
NNS_1 
                                                 
1?http://www.csie.ntu.edu.tw/~cjlin/libsvm/?
2?http://wordnet.princeton.edu/man/morph.3WN.html?
3?http://www.lsi.upc.es/~nlp/SVMTool/?
Collocation  
this_0, be_0, the_0, 
age_t, of_1, new_1, 
medium_1, ,_1, the_1 
Syntactic 
relation MaltParser
4 
SYN_HEAD_is 
SYN_HEADPOS_VBZ 
SYN_RELATION_PRD 
SYN_HEADRIGHT 
Table?1:?Features?the?system?extracted?
The next 4 subsections elaborate these features. 
4.1 Words in the Surrounding Context 
We take the neighboring words in the context of 
the target word as a kind of features ignoring their 
exact position information, which is called bag-of-
words approach. 
Mostly, a certain sense of a word is tend to ap-
pear in a certain kind of context, so the context 
words could contain some helpful information to 
disambiguate the sense of the target word. 
Because there would be too many context words 
to be added into the feature vector space, data 
sparseness problem is inevitable. We need to re-
duce the sparseness as possible as we can. A sim-
ple way is to use the words? morphological root 
forms. In addition, we filter the tokens which con-
tain no alphabet character (including punctuation 
symbols) and stop words. The stop words are 
tested separately, and only the effective ones 
would be added into the stop words list. All re-
maining words in the instance are gathered, con-
verted to lower case and replaced by their morpho-
logical root forms. The implementation for getting 
the morphological root forms is WordNet (morph). 
4.2 Part-of-Speechs of Neighboring Words 
As mentioned above, the data sparseness is a se-
rious problem in WSD. Besides changing tokens to 
their morphological root forms, part-of-speech is a 
good choice too. The size of POS tag set is much 
smaller than the size of surrounding words set. 
And the neighboring words? part-of-speeches also 
contain useful information for WSD. In this part, 
we use a POS tagger (Gim?nez and M?rquez, 2004) 
to assign POS tags to those tokens.  
We get the left and right 3 words? POS tags to-
gether with their position information in the target 
words? sentence.  
For example, the word age is to be disambi-
guated in the sentence of ?? This is the 
                                                 
4?http://w3.msi.vxu.se/~nivre/research/MaltParser.html?
166
<head>age</head> of new media , the era of ??. 
The features then will be added to the feature vec-
tor are ?DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1, 
NNS_1?, in which _0/_1 stands for the word with 
current POS tag is in the left/right side of the target 
word. The POS tag set in use here is Penn Tree-
bank Tagset5. 
4.3 Collocations 
Different from bag-of-words, collocation feature 
contains the position information of the target 
words? neighboring words. To make this feature in 
the same form with the bag-of-words, we appended 
a symbol to each of the neighboring words? mor-
phological root forms to mark whether this word is 
in the left or in the right of the target word. Like 
POS feature, collocation was extracted in the sen-
tence where the target word belongs to. The win-
dow size of this feature is 5 to the left and 5 to the 
right of the target word, which is attained by em-
pirical value. In this part, punctuation symbol and 
stop words are not removed. 
Take the same instance last subsection has men-
tioned as example. The features we extracted are 
?this_0, be_0, the_0, age_t, of_1, new_1, me-
dium_1?. Like POS, _0/_1 stands for the word is 
in the left/right side of the target word. Then the 
features were added to the feature vector space. 
4.4 Syntactic Relations 
Many effective context words are not in a short 
distance to the target word, but we shouldn?t en-
large the window size too much in case of includ-
ing too many noises. A solution to this problem is 
to use the syntactic relations of the target word and 
its parent head word. 
We use Nivre et al, (2006)?s dependency parser. 
In this part, we get 4 features from every instance: 
head word of the target word, the head word?s POS, 
the head word?s dependency relation with the tar-
get word and the relative position of the head word 
to the target word. 
Still take the same instance which has been 
mentioned in the las subsection as example. The 
features we extracted are ?SYN_HEAD_is, 
SYN_HEADPOS_VBZ, SYN_RELATION_PRD, 
SYN_HEADRIGHT?, in which SYN_HEAD_is 
stands for is is the head word of age; 
SYN_HEADPOS_VBZ stands for the POS of the 
                                                 
5?http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html?
head word is is VBZ; SYN_RELATION_PRD 
stands for the relationship between the head word 
is and target word age is PRD; and 
SYN_HEADRIGHT stands for the target word age 
is in the right side of the head word is. 
5 Data Set and Results 
This English lexical sample task: Semeval 2007 
task 116 provides two tracks of the data set for par-
ticipants. The first one is from LDC and the second 
from web. 
We took part in this evaluation in the second 
track. The corpus is from web. In this track the task 
organizers provide a training data and test data set 
for 20 nouns and 20 adjectives. 
In order to develop our system, we divided the 
training data into 2 parts: training and development 
sets. The size of the training set is about 2 times of 
the development set. The development set contains 
1,781 instances. 
4 kinds of features were merged into 15 combi-
nations. Here we use a vector (V) to express which 
features are used. The four dimensions stand for 
syntactic relations, POS, surrounding words and 
collocations, respectively. For example, 1010 
means that the syntactic relations feature and the 
surrounding words feature are used. 
V Precision V Precision
0001 78.6% 1001 78.2% 
0010 80.3% 1010 81.9% 
0011 82.0% 1011 82.8% 
0100 70.4% 1100 73.3% 
0101 79.0% 1101 79.1% 
0110 82.1% 1110 82.5% 
0111 82.9% 1111 82.9% 
1000 72.6%   
Table?2:?Results?of?Combinations?of?Features?
From Table 2, we can conclude that the sur-
rounding words feature is the most useful kind of 
features. It obtains much better performance than 
other kinds of features individually. In other words, 
without it, the performance drops a lot. Among 
these features, syntactic relations feature is the 
most unstable one (the improvement with it is un-
stable), partly because the performance of the de-
pendency parser is not good enough. As the ones 
with the vector 0111 and 1111 get the best perfor-
                                                 
6http://nlp.cs.swarthmore.edu/semeval/tasks/task11/descript
ion.shtml?
167
mance, we chose all of these kinds of features for 
our final system. 
A trade-off parameter C in SVM is tuned, and 
the result is shown in Figure 2. We have also tried 
4 types of kernels of the SVM classifier (parame-
ters are set by default). The experimental results 
show that the linear kernel is the most effective as 
Table 3 shows. 
 
Figure?2:?Accuracy?with?different?C?parameters?
Kernel 
Function 
Type 
Linear Poly-nomial RBF
Sig-
moid
Accuracy 82.9% 68.3% 68.3% 68.3%
Table?3:?Accuracy?with?different?kernel?function?
types?
Another experiment (as shown in Figure 3) also 
validate that the linear kernel is the most suitable 
one. We tried using polynomial function. Unlike 
the parameters set by default above (g=1/k, d=3), 
here we set its Gama parameter as 1 (g=1) but oth-
er parameters excepting degree parameter are still 
set by default. The performance gets better when 
the degree parameter is tuned towards 1. That 
means the closer the kernel function to linear func-
tion the better the system performs. 
 
Figure?3:?Accuracy?with?different?degree? in?po?
lynomial?function?
In order to get the relation between the system 
performance and the size of training data, we made 
several groups of training-test data set from the 
training data the organizers provided. Each of them 
has the same test data but different size of training 
data which are 2, 3, 4 and 5 times of the test data 
respectively. Figure 4 shows the performance 
curve with the training data size. Indicated in Fig-
ure 4, the accuracy increases as the size of training 
data enlarge, from which we can infer that we 
could raise the performance by using more training 
data potentially. 
 
Figure?4:?Accuracy?s?trend?with?the?training?da?
ta?size?
Feature extraction is the most time-consuming 
part of the system, especially POS tagging and 
parsing which take 2 hours approximately on the 
training and test data. The classification part (using 
libsvm) takes no more than 5 minutes on the train-
ing and test data. We did our experiment on a PC 
with 2.0GHz CPU and 960 MB system memory. 
Our official result of HIT-IR-WSD is: micro-
avg raw score 81.9% on the test set, the top one 
among the participating runs. 
Acknowledgement 
We gratefully acknowledge the support for this 
study provided by the National Natural Science 
Foundation of China (NSFC) via grant 60435020, 
60575042, 60575042 and 60675034. 
References 
Lee, Y. K., and Ng, H. T. 2002. An empirical evaluation 
of knowledge sources and learning algorithms for 
word sense disambiguation. In Proceedings of 
EMNLP02, 41?48. 
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a 
library for support vector machines. 
Jes?s Gim?nez and Llu?s M?rquez. 2004. SVMTool: A 
general POS tagger generator based on Support Vec-
tor Machines. Proceedings of the 4th International 
Conference on Language Resources and Evaluation 
(LREC'04). Lisbon, Portugal. 
Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 
2006. Labeled Pseudo-Projective Dependency Pars-
ing with Support Vector Machines. In Proceedings of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
168
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency-based Syntactic and Semantic Parsing
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, Ting Liu
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yqli, yhguo, qinb, tliu}@ir.hit.edu.cn
Abstract
Our CoNLL 2009 Shared Task system in-
cludes three cascaded components: syntactic
parsing, predicate classification, and semantic
role labeling. A pseudo-projective high-order
graph-based model is used in our syntactic de-
pendency parser. A support vector machine
(SVM) model is used to classify predicate
senses. Semantic role labeling is achieved us-
ing maximum entropy (MaxEnt) model based
semantic role classification and integer linear
programming (ILP) based post inference. Fi-
nally, we win the first place in the joint task,
including both the closed and open challenges.
1 System Architecture
Our CoNLL 2009 Shared Task (Hajic? et al, 2009):
multilingual syntactic and semantic dependencies
system includes three cascaded components: syn-
tactic parsing, predicate classification, and semantic
role labeling.
2 Syntactic Dependency Parsing
We extend our CoNLL 2008 graph-based
model (Che et al, 2008) in four ways:
1. We use bigram features to choose multiple pos-
sible syntactic labels for one arc, and decide the op-
timal label during decoding.
2. We extend the model with sibling features (Mc-
Donald, 2006).
3. We extend the model with grandchildren fea-
tures. Rather than only using the left-most and right-
most grandchildren as Carreras (2007) and Johans-
son and Nugues (2008) did, we use all left and right
grandchildren in our model.
4. We adopt the pseudo-projective approach in-
troduced in (Nivre and Nilsson, 2005) to handle the
non-projective languages including Czech, German
and English.
2.1 Syntactic Label Determining
The model of (Che et al, 2008) decided one la-
bel for each arc before decoding according to uni-
gram features, which caused lower labeled attach-
ment score (LAS). On the other hand, keeping all
possible labels for each arc made the decoding in-
efficient. Therefore, in the system of this year, we
adopt approximate techniques to compromise, as
shown in the following formulas.
f lbluni(h, c, l) = f lbl1 (h, 1, d, l) ? f lbl1 (c, 0, d, l)
L1(h, c) = arg maxK1l?L(w ? f lbluni(h, c, l))
f lblbi (h, c, l) = f lbl2 (h, c, l)
L2(h, c) = arg maxK2l?L1(h,c)(w ? {f lbluni ? f lblbi })
For each arc, we firstly use unigram features to
choose the K1-best labels. The second parameter of
f lbl1 (?) indicates whether the node is the head of the
arc, and the third parameter indicates the direction.
L denotes the whole label set. Then we re-rank the
labels by combining the bigram features, and choose
K2-best labels. During decoding, we only use the
K2 labels chosen for each arc (K2 ? K1 < |L|).
2.2 High-order Model and Algorithm
Following the Eisner (2000) algorithm, we use spans
as the basic unit. A span is defined as a substring
of the input sentence whose sub-tree is already pro-
duced. Only the start or end words of a span can link
with other spans. In this way, the algorithm parses
the left and the right dependence of a word indepen-
dently, and combines them in the later stage.
We follow McDonald (2006)?s implementation of
first-order Eisner parsing algorithm by modifying its
scoring method to incorporate high-order features.
Our extended algorithm is shown in Algorithm 1.
There are four different span-combining opera-
tions. Here we explain two of them that correspond
to right-arc (s < t), as shown in Figure 1 and 2. We
49
Algorithm 1 High-order Eisner Parsing Algorithm
1: C[s][s][c] = 0, 0 ? s ? N , c ? cp, icp # cp: complete; icp: incomplete
2: for j = 1 to N do
3: for s = 0 to N do
4: t = s+ jL
5: if t > N then
6: break
7: end if
# Create incomplete spans
8: C[s][t][icp] = maxs?r<t;l?L2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l))
9: C[t][s][icp] = maxs?r<t;l?L2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l))
# Create complete spans
10: C[s][t][cp] = maxs<r?t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l))
11: C[t][s][cp] = maxs?r<t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l))
12: end for
13: end for
follow the way of (McDonald, 2006) and (Carreras,
2007) to represent spans. The other two operations
corresponding to left-arc are similar.
 
Figure 1: Combining two spans into an incomplete span
Figure 1 illustrates line 8 of the algorithm in Al-
gorithm 1, which combines two complete spans into
an incomplete span. A complete span means that
only the head word can link with other words fur-
ther, noted as ??? or ???. An incomplete span
indicates that both the start and end words of the
span will link with other spans in the future, noted as
?99K? or ?L99?. In this operation, we combine two
smaller spans, sps?r and spr+1?t, into sps99Kt with
adding arcs?t. As shown in the following formu-
las, the score of sps99Kt is composed of three parts:
the score of sps?r, the score of spr+1?t, and the
score of adding arcs?t. The score of arcs?t is
determined by four different feature sets: unigram
features, bigram features, sibling features and left
grandchildren features (or inside grandchildren fea-
tures, meaning that the grandchildren lie between s
and t). Note that the sibling features are only related
to the nearest sibling node of t, which is denoted as
sck here. And the inside grandchildren features are
related to all the children of t. This is different from
the models used by Carreras (2007) and Johansson
and Nugues (2008). They only used the left-most
child of t, which is tck? here.
ficp(s, r, t, l) = funi(s, t, l) ? fbi(s, t, l)
? fsib(s, sck, t) ? {?k?i=1 fgrand(s, t, tci, l)}
Sicp(s, r, t, l) = w ? ficp(s, r, t, l)
S(sps99Kt) = S(sps?r) + S(spr+1?t)
+ Sicp(s, r, t, l)
In Figure 2 we combine sps99Kr and spr?t into
sps?t, which explains line 10 in Algorithm 1. The
score of sps?t also includes three parts, as shown
in the following formulas. Although there is no new
arc added in this operation, the third part is neces-
sary because it reflects the right (or called outside)
grandchildren information of arcs?r.
r trc1 rcks r s tr rc1 rck
l l
 
Figure 2: Combining two spans into a complete span
fcp(s, r, t, l) = ?ki=1 fgrand(s, r, rci, l)
Scp(s, r, t, l) = w ? fcp(s, r, t, l)
S(sps?t) = S(sps99Kr)
+ S(spr?t) + Scp(s, r, t, l)
50
2.3 Features
As shown above, features used in our model can be
decomposed into four parts: unigram features, bi-
gram features, sibling features, and grandchildren
features. Each part can be seen as two different sets:
arc-related and label-related features, except sibling
features, because we do not consider labels when us-
ing sibling features. Arc-related features can be un-
derstood as back-off of label-related features. Actu-
ally, label-related features are gained by simply at-
taching the label to the arc-features.
The unigram and bigram features used in our
model are similar to those of (Che et al, 2008), ex-
cept that we use bigram label-related features. The
sibling features we use are similar to those of (Mc-
Donald, 2006), and the grandchildren features are
similar to those of (Carreras, 2007).
3 Predicate Classification
The predicate classification is regarded as a super-
vised word sense disambiguation (WSD) task here.
The task is divided into four steps:
1. Target words selection: predicates with multi-
ple senses appearing in the training data are selected
as target words.
2. Feature extraction: features in the context
around these target words are extracted as shown in
Table 4. The detailed explanation about these fea-
tures can be found from (Che et al, 2008).
3. Classification: for each target word, a Support
Vector Machine (SVM) classifier is used to classify
its sense. As reported by Lee and Ng (2002) and
Guo et al (2007), SVM shows good performance on
the WSD task. Here libsvm (Chang and Lin, 2001)
is used. The linear kernel function is used and the
trade off parameter C is 1.
4. Post processing: for each predicate in the test
data which does not appear in the training data, its
first sense in the frame files is used.
4 Semantic Role Labeling
The semantic role labeling (SRL) can be divided
into two separate stages: semantic role classification
(SRC) and post inference (PI).
During the SRC stage, a Maximum en-
tropy (Berger et al, 1996) classifier is used to
predict the probabilities of a word in the sentence
Language No-duplicated-roles
Catalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-loc
Chinese A0, A1, A2, A3, A4, A5,
Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, COND
English A0, A1, A2, A3, A4, A5,
German A0, A1, A2, A3, A4, A5,
Japanese DE, GA, TMP, WO
Spanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr,
arg2-loc, arg2-null, arg4-des, argL-null, argM-
cau, argM-ext, argM-fin
Table 1: No-duplicated-roles for different languages
to be each semantic role. We add a virtual role
?NULL? (presenting none of roles is assigned)
to the roles set, so we do not need semantic role
identification stage anymore. For a predicate
of each language, two classifiers (one for noun
predicates, and the other for verb predicates) predict
probabilities of each word in a sentence to be each
semantic role (including virtual role ?NULL?). The
features used in this stage are listed in Table 4.
The probability of each word to be a semantic role
for a predicate is given by the SRC stage. The re-
sults generated by selecting the roles with the largest
probabilities, however, do not satisfy some con-
strains. As we did in the last year?s system (Che et
al., 2008), we use the ILP (Integer Linear Program-
ming) (Punyakanok et al, 2004) to get the global op-
timization, which is satisfied with three constrains:
C1: Each word should be labeled with one and
only one label (including the virtual label ?NULL?).
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?). The
threshold we use in our system is 0.3.
C3: Statistics show that some roles (except for
the virtual role ?NULL?) usually appear once for
a predicate. We impose a no-duplicate-roles con-
straint with a no-duplicate-roles list, which is con-
structed according to the times of semantic roles?
duplication for each single predicate. Table 1 shows
the no-duplicate-roles for different languages.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit1. The
classifier parameters are tuned with the development
data for different languages respectively. lp solve
5.52 is chosen as our ILP problem solver.
1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
2http://sourceforge.net/projects/lpsolve
51
5 Experiments
5.1 Experimental Setup
We participate in the CoNLL 2009 shared task
with all 7 languages: Catalan (Taule? et al, 2008),
Chinese (Palmer and Xue, 2009), Czech (Hajic? et
al., 2006), English (Surdeanu et al, 2008), Ger-
man (Burchardt et al, 2006), Japanese (Kawahara
et al, 2002), and Spanish (Taule? et al, 2008). Be-
sides the closed challenge, we also submitted the
open challenge results. Our open challenge strategy
is very simple. We add the SRL development data
of each language into their training data. The pur-
pose is to examine the effect of the additional data,
especially for out-of-domain (ood) data.
Three machines (with 2.5GHz Xeon CPU and
16G memory) were used to train our models. Dur-
ing the peak time, Amazon?s EC2 (Elastic Com-
pute Cloud)3 was used, too. Our system requires
15G memory at most and the longest training time
is about 36 hours.
During training the predicate classification (PC)
and the semantic role labeling (SRL) models, golden
syntactic dependency parsing results are used. Pre-
vious experiments show that the PC and SRL test re-
sults based on golden parse trees are slightly worse
than that based on cross trained parse trees. It is,
however, a pity that we have no enough time and ma-
chines to do cross training for so many languages.
5.2 Results and Discussion
In order to examine the performance of the ILP
based post inference (PI) for different languages, we
adopt a simple PI strategy as baseline, which se-
lects the most likely label (including the virtual la-
bel ?NULL?) except for those duplicate non-virtual
labels with lower probabilities (lower than 0.5). Ta-
ble 2 shows their performance on development data.
We can see that the ILP based post inference can
improve the precision but decrease the recall. Ex-
cept for Czech, almost all languages are improved.
Among them, English benefits most.
The final system results are shown in Table 3.
Comparing with our CoNLL 2008 (Che et al, 2008)
syntactic parsing results on English4, we can see that
our new high-order model improves about 1%.
3http://aws.amazon.com/ec2/
4devel: 85.94%, test: 87.51% and ood: 80.73%
Precision Recall F1
Catalan simple 78.68 77.14 77.90
Catalan ILP 79.42 76.49 77.93
Chinese simple 80.74 74.36 77.42
Chinese ILP 81.97 73.92 77.74
Czech simple 88.54 84.68 86.57
Czech ILP 89.23 84.05 86.56
English simple 83.03 83.55 83.29
English ILP 85.63 83.03 84.31
German simple 78.88 75.87 77.34
German ILP 82.04 74.10 77.87
Japanese simple 88.04 70.68 78.41
Japanese ILP 89.23 70.16 78.56
Spanish simple 76.73 75.92 76.33
Spanish ILP 77.71 75.34 76.51
Table 2: Comparison between different PI strategies
For the open challenge, because we did not mod-
ify the syntactic training data, its results are the same
as the closed ones. We can, therefore, examine the
effect of the additional training data on SRL. We can
see that along with the development data are added
into the training data, the performance on the in-
domain test data is increased. However, it is inter-
esting that the additional data is harmful to the ood
test.
6 Conclusion and Future Work
Our CoNLL 2009 Shared Task system is com-
posed of three cascaded components. The pseudo-
projective high-order syntactic dependency model
outperforms our CoNLL 2008 model (in English).
The additional in-domain (devel) SRL data can help
the in-domain test. However, it is harmful to the ood
test. Our final system achieves promising results. In
the future, we will study how to solve the domain
adaptive problem and how to do joint learning be-
tween syntactic and semantic parsing.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60675034, and the ?863? National High-
Tech Research and Development of China via grant
2008AA01Z144.
52
Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Score
devel test ood devel test ood devel test ood
Catalan closed 86.65 86.56 ?? 77.93 77.10 ?? 82.30 81.84 ??open ?? ?? 77.36 ?? 81.97
Chinese closed 75.73 75.49 ?? 77.74 77.15 ?? 76.79 76.38 ??open ?? ?? 77.23 ?? 76.42
Czech closed 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66open ?? ?? 86.57 85.21 ?? 83.31 80.63
English closed 87.09 88.48 81.57 84.30 85.51 73.82 85.70 87.00 77.71open ?? ?? 85.61 73.66 ?? 87.05 77.63
German closed 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19open ?? ?? 78.61 70.09 ?? 82.44 73.20
Japanese closed 92.55 92.57 ?? 78.56 78.26 ?? 85.86 85.65 ??open ?? ?? 78.35 ?? 85.70
Spanish closed 87.22 87.33 ?? 76.51 76.47 ?? 81.87 81.90 ??open ?? ?? 76.66 ?? 82.00
Average closed ?? 85.23 77.90 ?? 79.94 76.38 ?? 82.64 77.19open 80.06 76.32 82.70 77.15
Table 3: Final system results
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In LREC-2006.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP/CoNLL-
2007.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, and Sheng Li. 2008. A cascaded
syntactic and semantic dependency parsing system. In
CoNLL-2008.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies.
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang,
and Ting Liu. 2007. HIT-IR-WSD: A wsd system for
english lexical sample task. In SemEval-2007.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In CoNLL-2009.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP-2008.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In LREC-2002.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In EMNLP-
2002.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In ACL-2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1).
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Coling-2004.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In LREC-2008.
53
Catalan Chinese Czech English German Japanese Spanish
ChildrenPOS ? ? ??
ChildrenPOSNoDup ? ? ? ?
ConstituentPOSPattern ? ? ? ? ? ? ? ? ? ? ? ? ? ?
ConstituentPOSPattern+DepRelation ? ? ? ? ? ?
ConstituentPOSPattern+DepwordLemma ? ? ? ? ? ?
ConstituentPOSPattern+HeadwordLemma ? ? ? ? ? ? ? ? ? ?
DepRelation N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? N M ? ?
DepRelation+DepwordLemma ? ? ? ?
DepRelation+Headword N M N M N N M N M N
DepRelation+HeadwordLemma ? ? ? ? ? ? ? ?
DepRelation+HeadwordLemma+DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepRelation+HeadwordPOS N M N M N M N M N M N
Depword ? ? ? ?
DepwordLemma ? ? ? ? ? ? ? ? ? ? ? ?
DepwordLemma+HeadwordLemma ? ? ? ? ? ?
DepwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ?
DepwordPOS N M N M N M ? ? N M N M ? ? N M
DepwordPOS+HeadwordPOS ? ? ? ?
DownPathLength ? ? ? ?
FirstLemma ? ? ? ? ? ? ? ? ? ? ? ?
FirstPOS ? ? ? ?
FirstPOS+DepwordPOS ? ? ? ? ? ?
FirstWord ? ? ? ?
Headword N M N M N M N M N M ? ? N
HeadwordLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N
HeadwordLemma+RelationPath ? ? ? ? ? ? ? ? ? ? ? ?
HeadwordPOS N M N M N M ? ? N M ? ? N M ? ? N M
LastLemma ? ? ? ? ? ? ? ? ? ?
LastPOS ? ? ? ?
LastWord ? ?
Path ? ? ? ? ? ? ? ? ? ? ? ?
Path+RelationPath ? ? ? ? ? ? ? ? ? ?
PathLength ? ? ? ? ? ? ? ? ? ? ? ?
PFEAT N M N M N M
PFEATSplit N M ? ? N M ? ? N M ? ? N M ? ?
PFEATSplitRemoveNULL N M N M N M
PositionWithPredicate ? ? ? ? ? ? ? ? ? ?
Predicate N M ? ? N M N M ? ? N M N M N M ? ?
Predicate+PredicateFamilyship ? ? ? ? ? ? ? ? ? ?
PredicateBagOfPOSNumbered M N M N M N M
PredicateBagOfPOSNumberedWindow5 N M N M N M N M N M
PredicateBagOfPOSOrdered N M N M N M N M N
PredicateBagOfPOSOrderedWindow5 N M N M N M N M N M N M
PredicateBagOfPOSWindow5 N N M N M N M N M N
PredicateBagOfWords M N M N N M N M
PredicateBagOfWordsAndIsDesOfPRED N M N M M N M N M
PredicateBagOfWordsOrdered M N M N M M N M N M
PredicateChildrenPOS N M ? ? N M N M N M N M N M ? ?
PredicateChildrenPOSNoDup N M N M N M N M N M N M
PredicateChildrenREL N M ? ? N M N M N M N M ? ? N M
PredicateChildrenRELNoDup N M ? ? N M N M N M N M ? ? N M
PredicateFamilyship ? ?
PredicateLemma N M ? ? N M ? ? N M ? ? N M ? ? N M ? ? ? ? N M ? ?
PredicateLemma+PredicateFamilyship ? ? ? ? ? ?
PredicateSense ? ? ? ? ? ? ? ? ? ? ? ? ? ?
PredicateSense+DepRelation ? ? ? ?
PredicateSense+DepwordLemma ? ? ? ?
PredicateSense+DepwordPOS ? ? ? ?
PredicateSiblingsPOS N M N M N N M N M N M
PredicateSiblingsPOSNoDup N M ? ? N M N M N M N M N M ? ?
PredicateSiblingsREL N M ? ? N M N M N M N M N M
PredicateSiblingsRELNoDup N M N M ? ? M N M N M ? ? N M ? ?
PredicateVoiceEn N M
PredicateWindow5Bigram N M N M N M N M
PredicateWindow5BigramPOS N M N M N M N M N M N M
RelationPath ? ? ? ? ? ? ? ? ? ? ? ? ? ?
SiblingsPOS ? ? ? ?
SiblingsREL ?
SiblingsRELNoDup ? ? ? ?
UpPath ? ? ? ? ? ? ?
UpPathLength ? ?
UpRelationPath ? ? ? ? ? ?
UpRelationPath+HeadwordLemma ? ? ? ? ? ? ? ?
Table 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL). N: noun predicate
PC, M: verb predicate PC, ?: noun predicate SRL, ?: verb predicate SRL.
54
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161?169,
Beijing, August 2010
Jointly Modeling WSD and SRL with Markov Logic
Wanxiang Che and Ting Liu
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
{car, tliu}@ir.hit.edu.cn
Abstract
Semantic role labeling (SRL) and word
sense disambiguation (WSD) are two fun-
damental tasks in natural language pro-
cessing to find a sentence-level seman-
tic representation. To date, they have
mostly been modeled in isolation. How-
ever, this approach neglects logical con-
straints between them. We therefore ex-
ploit some pipeline systems which verify
the automatic all word sense disambigua-
tion could help the semantic role label-
ing and vice versa. We further propose a
Markov logic model that jointly labels se-
mantic roles and disambiguates all word
senses. By evaluating our model on the
OntoNotes 3.0 data, we show that this
joint approach leads to a higher perfor-
mance for word sense disambiguation and
semantic role labeling than those pipeline
approaches.
1 Introduction
Semantic role labeling (SRL) and word sense dis-
ambiguation (WSD) are two fundamental tasks in
natural language processing to find a sentence-
level semantic representation. Semantic role la-
beling aims at identifying the relations between
predicates in a sentence and their associated ar-
guments. Word sense disambiguation is the pro-
cess of identifying the correct meaning, or sense
of a word in a given context. For example, for
the sentence in Figure 1, we can find out that the
predicate token ?hitting? at position 3 has sense
?cause to move by striking? and the sense label is
?hit.01?. The argument headed by the token ?cat?
at position 1 with sense ?feline mammal? (cat.01)
is referring to the player (A0), and the argument
headed by the token ?ball? at position 5 with sense
Figure 1: A sample of word sense disambiguation
and semantic role labeling.
?round object that is hit in games? (ball.01) is re-
ferring to the game object (A1) being hit.
Normally, semantic role labeling and word
sense disambiguation are regarded as two inde-
pendent tasks, i.e., the word sense information
is rarely used in a semantic role labeling system
and vice versa. A few researchers have used se-
mantic roles to help the verb sense disambigua-
tion (Dang and Palmer, 2005). More people used
predicate senses in semantic role labeling (Hajic?
et al, 2009; Surdeanu et al, 2008). However, both
of the pipeline methods ignore possible dependen-
cies between the word senses and semantic roles,
and can result in the error propagation problem.
The same problem also appears in other natural
language processing tasks.
In order to make different natural language pro-
cessing tasks be able to help each other, jointly
modeling methods become popular recently, such
as joint Chinese word segmentation and part-of-
speech tagging (Kruengkrai et al, 2009; Zhang
and Clark, 2008; Jiang et al, 2008), joint lemma-
tization and part-of-speech prediction (Toutanova
and Cherry, 2009), joint morphological segmenta-
tion and syntactic parsing (Goldberg and Tsarfaty,
2008), joint text and aspect ratings for sentiment
summarization (Titov and McDonald, 2008), and
joint parsing and named entity recognition (Finkel
and Manning, 2009). For semantic role label-
ing, Dahlmeier et al (2009) proposed a method
to maximize the joint probability of the seman-
161
tic role of preposition phrases and the preposition
sense.
In order to do better joint learning, a novel
statistical relational learning framework, Markov
logic (Domingos and Lowd, 2009) was intro-
duced to join semantic role labeling and predicate
senses (Meza-Ruiz and Riedel, 2009). Markov
logic combines the first order logic and Markov
networks, to develop a joint probability model
over all related rules. Global constraints (intro-
duced by Punyakanok et al (2008)) among se-
mantic roles can be easily added into Markov
logic. And the more important, the jointly model-
ing can be realized using Markov logic naturally.
Besides predicates and prepositions, other word
senses are also important information for recog-
nizing semantic roles. For example, if we know
?cat? is an ?agent? of the predicate ?hit? in a sen-
tence, we can guess that ?dog? can also be an
?agent? of ?hit?, though it does not appear in the
training data. Similarly, the semantic role infor-
mation can also help to disambiguate word senses.
In addition, the predicate sense and the argument
sense can also help each other. In the sentence
?The cat is hitting a ball.?, if we know ?hit? here
has a game related sense, we can guess that the
?ball? should have the sense ?is a round object in
games?. In the same way, the correct ?ball? sense
can help to disambiguate the sense of ?hit?. The
joint probability, that they are disambiguated cor-
rectly simultaneously will be larger than other ab-
normalities.
The release of OntoNotes (Hovy et al, 2006)
provides us an opportunity to jointly model all
word senses disambiguation and semantic role la-
beling. OntoNotes is a large corpus annotated
with constituency trees (based on Penn Tree-
bank), predicate argument structures (based on
Penn PropBank), all word senses, etc. It has been
used in some natural language processing tasks,
such as joint parsing and named entity recogni-
tion (Finkel and Manning, 2009), and word sense
disambiguation (Zhong et al, 2008).
In this paper, we first propose some pipeline
systems which exploit automatic all word sense
disambiguation into semantic role labeling task
and vice versa. Then we present a Markov logic
model which can easily express useful global con-
straints and jointly disambiguate all word senses
and label semantic roles.
Experiments on the OntoNotes 3.0 corpus show
that (1) the automatic all word sense disambigua-
tion and semantic role labeling tasks can help each
other when using pipeline approaches, and more
important, (2) the joint approach using Markov
logic leads to higher accuracy for word sense dis-
ambiguation and performance (F1) for semantic
role labeling than pipeline approaches.
2 Related Work
Joint models were often used in semantic role la-
beling community. Toutanova et al (2008) and
Punyakanok et al (2008) presented a re-ranking
model and an integer linear programming model
respectively to jointly learn a global optimal se-
mantic roles assignment. Besides jointly learning
semantic role assignment of different constituents
for one task (semantic role labeling), their meth-
ods have been used to jointly learn for two tasks
(semantic role labeling and syntactic parsing).
However, it is easy for the re-ranking model to
loss the optimal result, if it is not included in the
top n results. In addition, the integer linear pro-
gramming model can only use hard constraints. A
lot of engineering work is also required in both
models.
Recently, Markov logic (Domingos and Lowd,
2009) became a hot framework for joint model.
It has been successfully used in temporal relations
recognition (Yoshikawa et al, 2009), co-reference
resolution (Poon and Domingos, 2008), etc. It
is very easy to do joint modeling using Markov
logic. The only work is to define relevant formu-
las. Meza-Ruiz and Riedel (2009) have joined se-
mantic role labeling and predicate senses disam-
biguation with Markov logic.
The above idea, that the predicate senses and
the semantic role labeling can help each other,
may be inspired by Hajic? et al (2009), Surdeanu
et al (2008), and Dang and Palmer (2005). They
have shown that semantic role features are helpful
to disambiguate verb senses and vice versa.
Besides predicate senses, Dahlmeier et al
(2009) proposed a joint model to maximize prob-
ability of the preposition senses and the semantic
role of prepositional phrases.
162
Except for predicate and preposition senses,
Che et al (2010) explored all word senses for se-
mantic role labeling. They showed that all word
senses can improve the semantic role labeling per-
formance significantly. However, the golden word
senses were used in their experiments. The results
are still unknown when an automatic word sense
disambiguation system is used.
In this paper, we not only use all word senses
disambiguated by an automatic system, but also
make the semantic role labeling results to help
word sense disambiguation synchronously with a
joint model.
3 Markov Logic
Markov logic can be understood as a knowledge
representation with a weight attached to a first-
order logic formula. Let us describe Markov
logic in the case of the semantic role labeling
task. We can model this task by first introduc-
ing a set of logical predicates such as role(p, a, r)
and lemma(i, l), which means that the argument
at position a has the role r with respect to the
predicate at position p and token at position i has
lemma l respectively. Then we specify a set of
weighted first order formulas that define a distri-
bution over sets of ground atoms of these predi-
cates (or so-called possible worlds).
Ideally, the distribution we define with these
weighted formulas assigns high probability to
possible worlds where semantic role labeling is
correct and a low probability to worlds where this
is not the case. For instance, for the sentence
in Figure 1, a suitable set of weighted formulas
would assign a high probability to the world:
lemma(1, cat), lemma(3, hit), lemma(5, ball)
role(3, 1, A0), role(3, 5, A1)
and low probabilities to other cases.
A Markov logic network (MLN) M is a set
of weighted formulas, i.e., a set of pairs (?, ?),
where ? is a first order formula and ? is the real
weight of the formula. M defines a probability
distribution over possible worlds:
p(y) = 1Z exp(
?
(?,?)?M
?
?
c?C?
f?c (y))
where each c is a binding of free variables in ?
to constants. Each f?c is a binary feature function
that returns 1 if the possible world y includes the
ground formula by replacing the free variables in
? with the constants in c is true, and 0 otherwise.
C? is the set of all bindings for the variables in ?.
Z is a normalization constant.
4 Model
We divide our system into two stages: word sense
disambiguation and semantic role labeling. For
comparison, we can process them with pipeline
strategy, i.e., the word sense disambiguation re-
sults are used in semantic role labeling or the se-
mantic role labeling results are used in word sense
disambiguation. Of course, we can jointly process
them with Markov logic easily.
We define two hidden predicates for the two
stages respectively. For word sense disambigua-
tion, we define the predicate sense(w, s) which
indicates that the word at position w has the
sense s. For semantic role labeling, the predicate
role(p, a, r) is defined as mentioned in above.
Different from Meza-Ruiz and Riedel (2009),
which only used sense number as word sense
representation, we use a triple (lemma, part-of-
speech, sense num) to represent the word sense
s. For example, (hit, v, 01) denotes that the verb
?hit? has sense number 01. Obviously, our rep-
resentation can distinguish different word senses
which have the identical sense number. In ad-
dition, we use one argument classification stage
with predicate role to label semantic roles as Che
et al (2009). Similarly, no argument identifica-
tion stage is used in our model. The approach can
improve the recall of the system.
In addition to the hidden predicates, we define
observable predicates to represent the information
available in the corpus. Table 1 presents these
predicates.
4.1 Local Formula
A local formula means that its groundings relate
any number of observed ground atoms to exactly
one hidden ground atom. For example
lemma(p,+l1)?lemma(a,+l2)? role(p, a,+r)
163
Predicates Description
word(i, w) Token i has word w
pos(i, t) Token i has part-of-speech t
lemma(i, l) Token i has lemma l
chdpos(i, t) The part-of-speech string of to-
ken i?s all children is t
chddep(i, d) The dependency relation string
of token i?s all children is t
firstLemma(i, l) The leftmost lemma of a sub-
tree rooted by token i is l
lastLemma(i, l) The rightmost lemma of a sub-
tree rooted by token i is l
posFrame(i, fr) fr is a part-of-speech frame at
token i
dep(h, a, de) The dependency relation be-
tween an argument a and its
head h is de
isPredicate(p) Token p is a predicate
posPath(p, a, pa) The part-of-speech path be-
tween a predicate p and an ar-
gument a is pa
depPath(p, a, pa) The dependency relation path
between a predicate p and an ar-
gument a is pa
pathLen(p, a, le) The path length between a pred-
icate p and an argument a is le
position(p, a, po) The relative position between a
predicate p and an argument a
is po
family(p, a, fa) The family relation between a
predicate p and an argument a
is fa
wsdCand(i, t) Token i is a word sense disam-
biguation candidate, here t is
?v? or ?n?
uniqe(r) For a predicate, semantic role r
can only appear once
Table 1: Observable Predicates.
means that if the predicate lemma at position p
is l1 and the argument lemma at position a is l2,
then the semantic role between the predicate and
the argument is r with some possibility.
The + notation signifies that Markov logic gen-
erates a separate formula and a separate weight for
each constant of the appropriate type, such as each
possible pair of lemmas (l1, l2, r). This type of
?template-based? formula generation can be per-
formed automatically by a Markov logic engine,
such as the thebeast1 system.
The local formulas are based on features em-
ployed in the state-of-the-art systems. For word
sense disambiguation, we use the basic features
mentioned by Zhong et al (2008). The semantic
role labeling features are from Che et al (2009),
1http://code.google.com/p/thebeast/
Features SRL WSD
Lemma ? ?
POS ? ?
FirstwordLemma ?
HeadwordLemma ?
HeadwordPOS ?
LastwordLemma ?
POSPath ?
PathLength ?
Position ?
PredicateLemma ?
PredicatePOS ?
RelationPath ?
DepRelation ?
POSUpPath ?
POSFrame ?
FamilyShip ?
BagOfWords ?
Window3OrderedWords ?
Window3OrderedPOSs ?
Table 2: Local Features.
the best system of the CoNLL 2009 shared task.
The final features are listed in Table 2.
What follows are some simple examples in or-
der to explain how we implement each feature as
a formula (or a set of formulas).
Consider the ?Position? feature. We first intro-
duce a predicate position(p, a, po) that denotes
the relative position between predicate p and ar-
gument a is po. Then we add a formula
position(p, a,+po)? role(p, a,+r)
for all possible combinations of position and role
relations.
The ?BagOfWords? feature means that the
sense of a word w is determined by all of lemmas
in a sentence. Then, we add the following formula
set:
wsdCand(w,+tw) ? lemma(w,+lw) ? lemma(1,+l1) ? sense(w,+s)
. . .
wsdCand(w,+tw) ? lemma(w,+lw) ? lemma(2,+li) ? sense(w,+s)
. . .
wsdCand(w,+tw) ? lemma(w,+lw) ? lemma(n,+ln) ? sense(w,+s)
where, the w is the position of current word and
tw is its part-of-speech tag, lw is its lemma. li
is the lemma of token i. There are n tokens in a
sentence totally.
4.2 Global Formula
Global formulas relate more than one hidden
ground atoms. We use this type of formula for
two purposes:
164
1. To capture the global constraints among dif-
ferent semantic roles;
2. To reflect the joint relation between word
sense disambiguation and semantic role labeling.
Punyakanok et al (2008) proposed an integer
linear programming (ILP) model to get the global
optimization for semantic role labeling, which sat-
isfies some constraints. This approach has been
successfully transferred into dependency parse
tree based semantic role labeling system by Che
et al (2009). The final results must satisfy two
constraints which can be described with Markov
logic formulas as follows:
C1: Each word should be labeled with one and
only one label.
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
The same unique constraint also happens on the
word sense disambiguation, i.e.,
sense(w, s1) ? s1 6= s2 ? ?sense(w, s2)
C2: Some roles (A0?A5) appear only once fora predicate.
role(p, a1, r) ? uniqe(r) ? a1 6= a2 ? ?role(p, a2, r)
It is also easy to express the joint relation be-
tween word sense disambiguation and semantic
role labeling with Markov logic. What we need
to do is just adding some global formulas. The
relation between them can be shown in Figure 2.
Inspired by CoNLL 2008 (Surdeanu et al, 2008)
and 2009 (Hajic? et al, 2009) shared tasks, where
most of successful participant systems used pred-
icate senses for semantic role labeling, we also
model that the word sense disambiguation impli-
cates the semantic role labeling.
Here, we divide the all word sense disambigua-
tion task into two subtasks: predicate sense dis-
ambiguation and argument sense disambiguation.
The advantages of the division method approach
lie in two aspects. First, it makes us distinguish
the contributions of predicate and argument word
sense disambiguation respectively. Second, as
previous discussed, the predicate and argument
sense disambiguation can help each other. There-
fore, we can reflect the help with the division and
use Markov logic to represent it.
Figure 2: Global model between word sense dis-
ambiguation and semantic role labeling.
Finally, we use three global formulas to imple-
ment the three lines with direction in Figure 2.
They are:
sense(p,+s) ? role(p, a,+r)
sense(a,+s) ? role(p, a,+r)
sense(p,+s) ? sense(a,+s)
5 Experiments
5.1 Experimental Setting
In our experiments, we use the OntoNotes
Release 3.02 corpus, the latest version of
OntoNotes (Hovy et al, 2006). The OntoNotes
project leaders describe it as ?a large, multilingual
richly-annotated corpus constructed at 90% inter-
nanotator agreement.? The corpus has been an-
notated with multiple levels of annotation, includ-
ing constituency trees, predicate argument struc-
ture, word senses, co-reference, and named enti-
ties. For this work, we focus on the constituency
trees, word senses, and predicate argument struc-
tures. The corpus has English, Chinese, and Ara-
bic portions, and we just use the English portion,
which has been split into four sections: broad-
cast conversation (bc), broadcast news (bn), mag-
azine (mz), and newswire (nw). There are several
datasets in each section, such as cnn and voa.
We will do our experiments on all of the
OntoNotes 3.0 English datasets. For each dataset,
we aimed for roughly a 60% train / 20% develop-
ment / 20% test split. See Table 3 for the detailed
statistics. Here, we use the human annotated part-
of-speech and parse trees provided by OntoNotes.
The lemma of each word is extracted using Word-
Net tool3.
2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2009T24
3http://wordnet.princeton.edu/
165
Training Developing Testing
bc
cctv 1,042 (0000-0003) 328 (0004-0004) 333 (0005-0005)
cnn 2,927 (0000-0004) 963 (0005-0006) 880 (0007-0008)
msnbc 2,472 (0000-0003) 1,209 (0004-0005) 1,315 (0006-0007)
phoenix 590 (0000-0001) 240 (0002-0002) 322 (0003-0003)
bn
abc 594 (0001-0040) 146 (0041-0054) 126 (0057-0069)
cnn 1,610 (0001-0234) 835 (0235-0329) 1,068 (0330-0437)
mnb 309 (0001-0015) 111 (0016-0020) 114 (0021-0025)
nbc 281 (0001-0023) 128 (0024-0031) 78 (0032-0039)
pri 1,104 (0001-0068) 399 (0069-0090) 366 (0091-0112)
voa 1,159 (0001-0159) 315 (0160-0212) 315 (0213-0265)
mz sinorama 5,051 (1001-1048) 1,262 (1049-1063) 1,456 (1064-1078)
nw wsj 8,138 (0020-1446) 2,549 (1447-1705) 3,133 (1730-2454)xinhua 2,285 (0001-0195) 724 (0196-0260) 670 (0261-0325)
All 27,562 9,209 10,176
Table 3: Training, developing and testing set sizes for the datasets in sentences. The file ranges (in
parenthesis) refer to the numbers within the names of the original OntoNotes 3.0 files. Here, we remove
4,873 sentences without semantic role labeling annotation.
Because we used semantic role labeling sys-
tem which is based on dependence syntactic trees,
we convert the constituency trees into dependence
trees with an Constituent-to-Dependency Conver-
sion Tool4.
The thebeast system is used in our experiment
as Markov logic engine. It uses cutting planes in-
ference technique (Riedel, 2008) with integer lin-
ear programming. The weights are learned with
MIRA (Crammer and Singer, 2003) online learn-
ing algorithm.
To our knowledge, this is the first word sense
disambiguation and semantic role labeling exper-
iment on OntoNotes 3.0 corpus. In order to com-
pare our joint model with previous work, we build
several systems:
Baseline: There are two independent baseline
systems: word sense disambiguation and seman-
tic role labeling. In each of baseline systems,
we only use the local formulas (Section 4.1) and
the global formulas which only express the global
constraints (Section 4.2).
Pipeline: In a pipeline system, we use ad-
ditional features outputted by preceded stages.
Such as in semantic role labeling pipeline sys-
tem, we use word sense as features, i.e., we set
sense(w, s) as an observable predicate and add
sense(p, s) ? role(p, a, r) and sense(a, s) ?
role(p, a, r) formulas into semantic role label-
ing task. As for word sense disambiguation
4http://nlp.cs.lth.se/software/treebank converter/
task, we add a set of formulas role(p, ai, r) ?
sense(p, s), where ai is the ith argument of
the predicate at position p, and a formula
role(p, a, r) ? sense(p, s) for the argument at
position a respectively.
Jointly: We use all global formulas mentioned
in Section 4.2. With Markov logic, we can add
global constraints and get the word sense disam-
biguation and the semantic role labeling results si-
multaneously.
5.2 Results and Discussion
The performance of these systems on test set is
shown in Table 4. All of the parameters are fine
tuned on the development set.
Here, we only consider the noun and verb word
sense disambiguation, which cover most of multi-
sense words. Therefore, the word sense disam-
biguation performance means the accuracy of all
nouns and verbs in the test set. The performance
of semantic role labeling is calculated using the
semantic evaluation metric of the CoNLL 2009
shared task scorer5. It measures the precision, re-
call, and F1 score of the recovered semantic de-
pendencies. The F1 score is used as the final per-
formance metric. A semantic dependency is cre-
ated for each predicate and its arguments. The la-
bel of such dependency is the role of the argument.
The same with the CoNLL 2009 shared task, we
assume that the predicates have been identified
5http://ufal.mff.cuni.cz/conll2009-st/eval09.pl
166
WSD SRL
Most Frequent Sense 85.58 ?
Baseline 89.37 83.97
PS 89.53 84.17
Pipeline AS 89.41 83.94
PS + AS ? 84.24
Jointly
PS? SRL 89.53 84.27
AS? SRL 89.49 84.16
PS? AS 89.45 ?
PS + AS? SRL 89.54 84.34
Fully 89.55 84.36
Table 4: The results of different systems. Here, PS
means predicate senses and AS means argument
senses.
correctly.
The first row of Table 4 gives the word sense
disambiguation result with the most frequent
sense, i.e., the #01 sense of each candidate word
which normally is the most frequent one in a bal-
ance corpus.
The second row shows the baseline perfor-
mances. Here, we note that the 89.37 word sense
disambiguation accuracy and the 83.97 semantic
role labeling F1 we obtained are comparable to
the state-of-the-art systems, such as the 89.1 word
sense disambiguation accuracy given by Zhong et
al. (2008) and 85.48 semantic role labeling perfor-
mance given by Che et al (2010) on OntoNotes
2.0 respectively, although the corpus used in our
experiments is upgraded version of theirs6. Ad-
ditionally, the performance of word sense dis-
ambiguation is higher than that of the most fre-
quent sense significantly (z-test7 with ? < 0.01).
Therefore, the experimental results show that the
Markov logic can achieve considerable perfor-
mances for word sense disambiguation and se-
mantic role labeling on the latest OntoNotes 3.0
corpus.
There are two kinds of pipeline systems: word
sense disambiguation (WSD) based on semantic
role labeling and semantic role labeling (SRL)
based on word sense disambiguation. For the us-
ing method of word senses, we first only exploit
predicate senses (PS) as mentioned by Surdeanu
et al (2008) and Hajic? et al (2009). Then, in or-
6Compared with OntoNotes 2.0, the version 3.0 incorpo-
rates more corpus.
7http://www.dimensionresearch.com/resources/
calculators/ztest.html
der to examine the contribution of word senses ex-
cept for predicates, we use argument senses (AS)
in isolation. Finally, all word senses (PS + AS)
were considered.
We can see that when the predicate senses (PS)
are used to label semantic role, the performance
of semantic role labeling can be improved from
83.97 to 84.17. The conclusion, that the predi-
cate sense can improve semantic role labeling per-
formance, is similar with CoNLL 2008 (Surdeanu
et al, 2008) and 2009 (Hajic? et al, 2009) shared
tasks. However, the improvement is not signifi-
cant (?2-test8 with ? < 0.1). Additionally, the
semantic role labeling can improve the predicate
sense disambiguation significantly from 89.37 to
89.53 (z-test with ? < 0.1). The same conclusion
was obtained by Dang and Palmer (2005).
However, when we only use argument senses
(AS), both of the word sense disambiguation and
semantic role labeling performances are almost
unchanged (from 89.37 to 89.41 and from 83.97
to 83.94 respectively). For the semantic role la-
beling task, the reason is that the original lemma
and part-of-speech features have been able to de-
scribe the argument related information. This kind
of sense features is just reduplicate. On the other
hand, the argument senses cannot be determined
only by the semantic roles. For example, the
semantic role ?A1? cannot predict the argument
sense of ?ball? exactly. The predicates must be
considered simultaneously.
Therefore, we use the last strategy (PS + AS),
which combines the predicate sense and the ar-
gument sense together to predict semantic roles.
The results show that the performance can be
improved significantly (?2-test with ? < 0.05)
from 83.97 to 84.24. Accordingly, the experi-
ment proves that automatic all word sense disam-
biguation can further improve the semantic role
labeling performance. Different from Che et al
(2010), where the semantic role labeling can be
improved with correct word senses about F1 = 1,
our improvement is much lower. The main reason
is that the performance of our word sense disam-
biguation with the most basic features is not high
enough. Another limitation of the pipeline strat-
8http://graphpad.com/quickcalcs/chisquared1.cfm
167
egy is that it is difficult to predict the combination
between predicate and argument senses. This is
an obvious shortcoming of the pipeline method.
With Markov logic, we can easily join different
tasks with global formulas. As shown in Table 4,
we use five joint strategies:
1. PS ? SRL: means that we jointly disam-
biguate predicate senses and label semantic roles.
Compared with the pipeline PS system, word
sense disambiguation performance is unchanged.
However, the semantic role labeling performance
is improved from 84.17 to 84.27. Compared with
the baseline?s 83.97, the improvement is signifi-
cant (?2-test with ? < 0.05).
2. AS ? SRL: means that we jointly disam-
biguate argument senses and label semantic roles.
Compared with the pipeline AS system, both of
word sense disambiguation and semantic role la-
beling performances are improved (from 89.41 to
89.49 and from 83.94 to 84.16 respectively). Al-
though, the improvement is not significant, it is
observed that the joint model has the capacity to
improve the performance, especially for semantic
role labeling, if we could have a more accurate
word sense disambiguation.
3. PS ? AS: means that we jointly dis-
ambiguate predicate word senses and argument
senses. This kind of joint model does not influ-
ence the performance of semantic role labeling.
The word sense disambiguation outperforms the
baseline system from 89.37 to 89.45. The result
verifies our assumption that the predicate and ar-
gument senses can help each other.
4. PS + AS ? SRL: means that we jointly
disambiguate all word senses and label semantic
roles. Compared with the pipeline method which
uses the PS + AS strategy, the joint method can
further improve the semantic role labeling (from
84.24 to 84.34). Additionally, it can obtain the
predicate and argument senses together. The all
word sense disambiguation performance (89.54)
is higher than the baseline (89.37) significantly (z-
test with ? < 0.1).
5. Fully: finally, we use all of the three global
formulas together, i.e., we jointly disambiguate
predicate senses, argument senses, and label se-
mantic roles. It fully joins all of the tasks. Both of
all word sense disambiguation and semantic role
labeling performances can be further improved.
Although the improvements are not significant
compared with the best pipeline system, they sig-
nificantly (z-test with ? < 0.1 and ?2-test with
? < 0.01 respectively) outperform the baseline
system. Additionally, the performance of the fully
joint system does not outperform partly joint sys-
tems significantly. The reason seems to be that
there is some overlap among the contributions of
the three joint systems.
6 Conclusion
In this paper, we presented a Markov logic model
that jointly models all word sense disambiguation
and semantic role labeling. We got the following
conclusions:
1. The baseline systems with Markov logic is
competitive to the state-of-the-art word sense dis-
ambiguation and semantic role labeling systems
on OntoNotes 3.0 corpus.
2. The predicate sense disambiguation is ben-
eficial to semantic role labeling. However, the
automatic argument sense disambiguation itself is
harmful to the task. It must be combined with the
predicate sense disambiguation.
3. The semantic role labeling not only can help
predicate sense disambiguation, but also argument
sense disambiguation (a little). In contrast, be-
cause of the limitation of the pipeline model, it
is difficult to make semantic role labeling to help
predicate and argument sense disambiguation si-
multaneously.
4. It is easy to implement the joint model of
all word sense disambiguation and semantic role
labeling with Markov logic. More important, the
joint model can further improve the performance
of the all word sense disambiguation and semantic
role labeling than pipeline systems.
Acknowledgement
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
168
References
Che, Wanxiang, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing.
In Proceedings of CoNLL 2009: Shared Task, pages
49?54, June.
Che, Wanxiang, Ting Liu, and Yongqiang Li. 2010.
Improving semantic role labeling with word sense.
In NAACL 2010, pages 246?249, June.
Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Dahlmeier, Daniel, Hwee Tou Ng, and Tanja Schultz.
2009. Joint learning of preposition senses and se-
mantic roles of prepositional phrases. In Proceed-
ings of EMNLP 2009, pages 450?458, August.
Dang, Hoa Trang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL 2005, pages 42?49, Morris-
town, NJ, USA.
Domingos, Pedro and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Synthesis Lectures on Artificial Intelligence and
Machine Learning. Morgan & Claypool Publishers.
Finkel, Jenny Rose and Christopher D. Manning.
2009. Joint parsing and named entity recognition.
In Proceedings of NAACL 2009, pages 326?334,
June.
Goldberg, Yoav and Reut Tsarfaty. 2008. A single
generative model for joint morphological segmenta-
tion and syntactic parsing. In Proceedings of ACL
2008, pages 371?379, June.
Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of CoNLL
2009: Shared Task, pages 1?18, June.
Hovy, Eduard, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
Ontonotes: The 90% solution. In Proceedings of
NAACL 2006, pages 57?60, June.
Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL 2008, pages 897?904, June.
Kruengkrai, Canasai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of ACL-IJCNLP 2009,
pages 513?521, August.
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In Proceedings of NAACL 2009,
pages 155?163, June.
Poon, Hoifung and Pedro Domingos. 2008. Joint
unsupervised coreference resolution with Markov
Logic. In Proceedings of EMNLP 2008, pages 650?
659, October.
Punyakanok, Vasin, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
Riedel, Sebastian. 2008. Improving the accuracy and
efficiency of map inference for markov logic. In
Proceedings of UAI 2008, pages 468?475. AUAI
Press.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of CoNLL
2008, pages 159?177, August.
Titov, Ivan and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL 2008, pages 308?316,
June.
Toutanova, Kristina and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech
prediction. In Proceedings of ACL-IJCNLP 2009,
pages 486?494, August.
Toutanova, Kristina, Aria Haghighi, and Christo-
pher D. Manning. 2008. A global joint model for
semantic role labeling. Computational Linguistics,
34(2).
Yoshikawa, Katsumasa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly iden-
tifying temporal relations with markov logic. In
Proceedings of ACL-IJCNLP 2009, pages 405?413,
August.
Zhang, Yue and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL 2008, pages 888?896,
June.
Zhong, Zhi, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of EMNLP 2008,
pages 1002?1010, October.
169
Coling 2010: Demonstration Volume, pages 13?16,
Beijing, August 2010
LTP: A Chinese Language Technology Platform
Wanxiang Che, Zhenghua Li, Ting Liu
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
Harbin Institute of Technology
{car, lzh, tliu}@ir.hit.edu.cn
Abstract
LTP (Language Technology Platform) is
an integrated Chinese processing platform
which includes a suite of high perfor-
mance natural language processing (NLP)
modules and relevant corpora. Espe-
cially for the syntactic and semantic pars-
ing modules, we achieved good results
in some relevant evaluations, such as
CoNLL and SemEval. Based on XML in-
ternal data representation, users can easily
use these modules and corpora by invok-
ing DLL (Dynamic Link Library) or Web
service APIs (Application Program Inter-
face), and view the processing results di-
rectly by the visualization tool.
1 Introduction
A Chinese natural language processing (NLP)
platform always includes lexical analysis (word
segmentation, part-of-speech tagging, named en-
tity recognition), syntactic parsing and seman-
tic parsing (word sense disambiguation, semantic
role labeling) modules. It is a laborious and time-
consuming work for researchers to develop a full
NLP platform, especially for Chinese, which has
fewer existing NLP tools. Therefore, it should be
of particular concern to build an integrated Chi-
nese processing platform. There are some key
problems for such a platform: providing high per-
formance language processing modules, integrat-
ing these modules smoothly, using processing re-
sults conveniently, and showing processing results
directly.
LTP (Language Technology Platform), a Chi-
nese processing platform, is built to solve the
above mentioned problems. It uses XML to trans-
fer data through modules and provides all sorts
? 
? 	

 ? 



? 




Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 497?507, Dublin, Ireland, August 23-29 2014.
Learning Sense-specific Word Embeddings By Exploiting
Bilingual Resources
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in learning word embeddings with neural network language
models (NNLM). However, the majority of previous NNLMs represent each word with a single
embedding, which fails to capture polysemy. In this paper, we address this problem by represent-
ing words with multiple and sense-specific embeddings, which are learned from bilingual parallel
data. We evaluate our embeddings using the word similarity measurement and show that our ap-
proach is significantly better in capturing the sense-level word similarities. We further feed our
embeddings as features in Chinese named entity recognition and obtain noticeable improvements
against single embeddings.
1 Introduction
Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector rep-
resentations for words. Each dimension of word embedding represents a latent feature of the word, hope-
fully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightfor-
wardly for computing word similarities, which benefits many practical applications (Socher et al., 2011;
Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al.,
2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013).
In recent years, neural network language models (NNLMs) have become popular architectures for
learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most
of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an
attempt to better capture the multiple senses or usages of a word, several multi-prototype models have
been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply
induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed
value. These models still may not capture the real senses of words, because different words may have
different number of senses.
We present a novel and simple method of learning sense-specific word embeddings by using bilingual
parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs.
We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the
source language with different senses is supposed to have different translations in the foreign language.
1
For instance,?? can be translated as investment / overpower / subdue / subjugate / uniform, etc. Among
all of these translations, subdue / overpower / subjugate express the same sense of??, whereas uniform
/ investment express a different sense. Therefore, we could effectively obtain the senses of one word by
clustering its translation words, exhibiting different senses in different clusters.
The created clusters are then projected back into the words in the source language texts, forming a
sense-labeled training data. The sense-labeled data are then trained with recurrent neural network langu-
gae model (RNNLM) (Mikolov, 2012), a kind of NNLM, to obtain sense-specific word embeddings. As
a concrete example, Figure 1 illustrates the process of learning sense-specific embeddings.
?
Email correspondence.
1
In this paper, source language refers to Chinese, whereas foreign language refers to English.
This work is licenced under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
497
? Cluster
Monolingualsense-labeled data
? ?? #2 , ?? ? ?? ?? ?1
?? ? ? ?? #1  ? ?? ?2
? ?? #2 ? ?? ?3
? ?? ? ?? ?? ?? ?  ?? #1 ?4
??5
? Project
? Extract
? RNNLM
Bilingual data(E: English, C: Chinese)
E: ? subdue , conquer or control the opponent .
C: ? ?? , ?? ? ?? ?? ?1
E: The workers wearing the factory 's uniform ? 
C: ?? ? ? ?? ? ?? ? 2
E: She overpowered the burglars .
C: ? ?? ? ?? ?3
E: They wore their priestly vestment in Church .
C: ? ?? ? ?? ?? ?? ? ?? ?4
??5
SL word ??
Translations
subdueuniformoverpowersubjugatevestment??
uniformvestment
overpowersubjugate
subdue
??#1
??#2
(clothes)
(defeat)
Sense-specific word embeddings
< v1#1, v2#1, ..., vN#1 >
< v1#2, v2#2, ..., vN#2 >
?? #1?? #2
Figure 1: An illustration of the proposed method. SL stands for source language.
To evaluate the sense-specific word embeddings we have learned, we manually construct a Chinese
polysemous word similarity dataset that contains 401 pairs of words with human-judged similarities. The
performance of our method on this dataset shows that sense-specific embeddings are significantly better
in capturing the sense-level similarities for polysemous words.
We also evaluate our embeddings by feeding them as features to the task of Chinese named entity
recognition (NER), which is a simple semi-supervised learning mechanism (Turian et al., 2010). In or-
der to use sense-specific embeddings as features, we should discriminate the word senses for the NER
data first. Therefore, we further develop a novel monolingual word sense disambiguation (WSD) algo-
rithm based on the RNNLM we have already trained previously. NER results show that sense-specific
embeddings provide noticeable improvements over traditional single embeddings.
Our contribution in this paper is twofold:
? We propose a novel approach of learning sense-specific word embeddings by utilizing bilingual
parallel data (Section 3). Evaluation on a manually constructed polysemous word similarity dataset
shows that our approach better captures word similarities (Section 5.2).
? To use the sense-specific embeddings in practical applications, we develop a novel WSD algorithm
for monolingual data based on RNNLM (Section 4). Using the algorithm, we feed the sense-specific
embeddings as additional features to NER and achieve significant improvement (Section 5.3).
2 Background: Word Embedding and RNNLM
There has been a line of research on learning word embeddings via NNLMs (Bengio et al., 2003; Mnih
and Hinton, 2008; Mikolov et al., 2013b). NNLMs are language models that exploit neural networks to
make probabilistic predictions of the next word given preceding words. By training NNLMs, we obtain
both high performance language models and word embeddings.
Following Mikolov et al. (2013b), we use the recurrent neural network as the basic framework for train-
ing NNLMs. RNNLM has achieved the state-of-the-art performance in language modeling (Mikolov,
2012) and learned effective word embeddings for several tasks (Mikolov et al., 2013b). The architecture
of RNNLM is shown in Figure 2.
The input layer of RNNLM consists of two components: w(t) and h(t ? 1). w(t) is the one-hot
representation of the word at time step t,
2
h(t ? 1) is the output of hidden layer at the last time step.
Therefore, the input encodes all previous history when predicting the next word at time step t. Compared
2
A feature vector of the same size of the vocabulary, and only one dimension is on.
498
w(t)
y(t)
U
W
V
h(t)
h(t-1)
Figure 2: The basic architecture of RNNLM.
with other feed-forward NNLMs, the RNNLM can theoretically represent longer context patterns. The
output y(t) represents the probability distribution of the next word p(w(t + 1)|w(t),h(t ? 1)). The
output values are computed as follows:
h(t) = f(Uw(t) +Wh(t? 1)) (1)
y(t) = g(Vh(t)) (2)
where f is a sigmoid function and g is a softmax function.
The RNNLM is trained by maximizing the log-likelihood of the training data using stochastic gradi-
ent descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute the
gradients. In the RNNLM, U is the embedding matrix, where each column vector represents a word.
As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon in
natural languages and induce a single embedding for each word. We address this issue and introduce an
effective approach for capturing polysemy in the next section.
3 Sense-specific Word Embedding Learning
In our approach, WSI is performed prior to the training of word embeddings. Inspired by Gale et al.
(1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examples
of WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1. First, we extract
the translations of the source language words from bilingual data (?). Since there may be multiple
translations for the same sense of a source language word, it is straightforward to cluster the translation
words, exhibiting different senses in different clusters (?).
Once word senses are effectively induced for each word, we are able to form the sense-labeled training
data of RNNLMs by tagging each word occurrence in the source language text with its associated sense
cluster (?). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a
standard manner (?).
3.1 Translation Words Extraction
Given bilingual data after word alignment, we present a way of extracting translation words for source
language words by exploiting the translation probability produced by word alignment models (Brown et
al., 1993; Och and Ney, 2003; Liang et al., 2006).
More formally, we notate the Chinese sentence as c = (c
1
, ..., c
I
) and English sentence as e =
(e
1
, ..., e
J
). The alignment models can be generally factored as:
p(c|e) =
?
a
p(a, c|e) (3)
p(a, c|e) =
?
J
j=1
p
d
(a
j
|a
j?
, j)p
t
(c
j
|e
a
j
) (4)
where a is the alignment specifying the position of an English word aligned to each Chinese word,
p
d
(a
j
|a
j?
, j) is the distortion probability, and p
t
(c
j
|e
a
j
) is the translation probability which we use.
499
SL Word Translation Words Translation Word Clusters Nearest Neighbours
?? investment, overpower,
subdue, subjugate, uniform
investment, uniform ??
dress
,??
policeman uniform
subdue, subjugate, overpower ??
defeat
,??
beat
,??
conquer
? blossom, cost, flower,
spend, take, took
flower, blossom ?
greens
,?
leaf
,??
fruit
take, cost, spend ??
cost
,??
save
,??
rest
? act, code, France,
French, law, method
France, French ?
Germany
,?
Russia
,?
Britain
law, act, code ??
ordinance
,??
bill
,??
rule
method ??
concept
,??
scheme
,??
way
??
lead, leader, leadership
leader, leadership ??
chief
,??
boss
,??
chairman
lead ??
supervise
,??
decision
,??
work
Table 1: Results of our approach on a sample of polysemous words. The second column lists the extracted
translation words of the source language word (Section 3.1). The third column lists the clustering results
using affinity propagation (Section 3.2). The last column lists the nearest neighbour words computed
using the learned sense-specific word embeddings (Section 5.2.2).
In this paper, we use the alignment model proposed by Liang et al. (2006). We utilize the bidirectional
translation probabilities for the extraction of translations, where a foreign language word w
e
is deter-
mined as a translation of source language word w
c
only if both translation probabilities p
t
(w
c
|w
e
) and
p
t
(w
e
|w
c
) exceed some threshold 0 < ? < 1.
The second column of Table 1 presents the extraction results on a sample of source language words
with the corresponding translation words.
3.2 Clustering of Translation Words
For each source language word, its translation words are then clustered so as to separate different senses.
At the clustering time, we first represent each translation word with a feature vector (point), so that
we can measure the similarities between points. Then we perform clustering on these feature vectors,
representing different senses in different clusters.
Different from Apidianaki (2008) who represents all occurrences of the translation words with their
contexts in the foreign language for clustering, we adopt the embeddings of the translation words as
the representations and directly perform clustering on the translation words,
3
rather than the contexts of
occurrences. The embedding representation is chosen for two reasons: (1) Word embeddings encode rich
lexical semantics. They can be directly used to measure word similarities. (2) Embedding representation
of the translation words leads to extremely high-efficiency clustering, because the number of translation
words is orders of magnitude less than their occurrences.
Moreover, since the number of senses of different source language words is varied, the commonly-
used k-means algorithm becomes inappropriate for this situation. Instead, we employ affinity propaga-
tion (AP) algorithm (Frey and Dueck, 2007) for clustering. In AP, each cluster is represented by one
of the samples of it, which we call an exemplar. AP finds the exemplars iteratively based on the con-
cept of ?message passing?. AP has the major advantage that the number of the resulting clusters is
dynamic, which mainly depends on the distribution of the data. Compared with other possible clustering
approaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines the
number of resulting clusters automatically without using any partition criterions.
The third column of Table 1 lists the resulting clusters of the translation words for the sampled pol-
ysemous words. We can see that the resulting clusters are meaningful: senses are well represented by
clusters of translation words.
3.3 Cross-lingual Word Sense Projection
The produced clusters are then projected back into the source language to identify word senses.
3
The publicly available word embeddings proposed by Collobert et al. (2011) are used.
500
For each occurrence w
o
of the word w in the source language corpora, we first select the aligned word
with the highest marginal edge posterior (Liang et al., 2006) as its translation. We then identify the sense
of w
o
by computing the similarities of its translation word with each exemplar of the clusters, and select
the one with the maximum similarity. When w
o
is aligned with NULL, we heuristically identify its sense
as the most frequent sense of w that appears in the bilingual dataset.
After projecting the word senses into the source language, we obtain a sense-labeled corpus, which is
used to train the sense-specific word embeddings with RNNLM. The training process is exactly the same
as single embeddings, except that the words in our training corpus has been labeled with senses.
4 Application of Sense-specific Word Embeddings
One of the attractive characteristic of word embeddings is that they can be directly used as word features
in various NLP applications, including NER, chunking, etc. Despite of the usefulness of word embed-
dings on these applications, previous work seldom concerns that words may have multiple senses, which
cannot be effectively represented with single embeddings. In this section, we address this problem by
utilizing sense-specific word embeddings.
We take the task of Chinese NER as a case study. Intuitively, word senses are important in NER. For
instance,? is likely to be an NE of Location when it refers to America. However, when it expresses the
sense of beautiful, it should not be an NE.
Using sense-specific word embedding features for NER is not as straightforward as using single em-
beddings. For each word in the NER data, we first need to determine the correct word sense of it, which
is a typical WSD problem. Then we use the embedding which corresponds to that sense as features.
Here we treat WSD as a sequence labeling problem, and solve it with a very natural algorithm based on
RNNLM we have already trained (Section 3).
4.1 RNNLM-based Word Sense Disambiguation
Given the automatically induced word sense inventories and the RNNLM which has already been trained
on the sense-labeled data of source language, we first develop a greedy decoding algorithm for the
sequential WSD, which works deterministically. Then we improve it using beam search.
Greedy. For word w, we denote the sense-labeled w as w
s
k
, where s
k
represents the k
th
sense of w.
In each step, a single decision is made and the sense of next word (w(t + 1)) which has the maximum
RNNLM output is chosen, given the current (sense-labeled) word w(t)
s
?
and the hidden layer h(t? 1)
at the last time step as input. We simply need to compute a shortlist of y(t) associated with w(t + 1),
that is, y(t)|
w(t+1)
at each step. This process is illustrated in Figure 3.
Beam search. The greedy procedure described above can be improved using a left-to-right beam
search decoding for obtaining a better sequence. The beam-search decoding algorithm keeps B different
sequences of decisions in the agenda, and the sequence with the best overall score is chosen as the final
sense sequence.
Note that the dynamic programming decoding (e.g. viterbi) is not applicable here, because of the
recurrent characteristic of RNNLM. At each step, decisions made by RNNLM depends on all previous
decisions instead of the previous state only, hence markov assumption is not satisfied.
5 Experiments
5.1 Experimental Settings
The Chinese-English parallel datasets we use include LDC03E24, LDC04E12 (1998), the IWSLT 2008
evaluation campaign dataset and the PKU 863 parallel dataset. All corpora are sentence-aligned. After
cleaning and filtering the corpus,
4
we obtain 918,681 pairs of sentences (21.7M words).
In this paper, we use BerkeleyAligner to produce word alignments over the parallel dataset.
5
Berke-
leyAligner also gives translation probabilities and marginal edge posterior probabilities. We adopt the
4
Sentences that are too long (more than 40 words) or too short (less than 10 words) are discarded.
5
code.google.com/p/berkeleyaligner/
501
h(t-1)
U
W
V
h(t)
Next word (sense-labeled)Last word(sense-labeled)
w(t)s*
w(t+1)s1
y(t)|w(t+1)
w(t+1)s2
w(t+1)sK
?
y(t)
?
w(t+1)s*
max
w(t)s* w(t+1)s*
w(t+1)s* w(t+2)s*
w(t+2)s* w(t+3)s*
h(t)
h(t+1)
h(t+2)
h(t-1)
Shortlist? 
Figure 3: Using RNNLM for WSD by sequential labeling (left). Decision at each step of the RNNLM-
based WSD algorithm (right).
scikit-learn tool (Pedregosa et al., 2011) to implement the AP clustering algorithm.
6
The AP algorithm
is not fully automatic in deciding the cluster number. There is a tunable parameter calls preference. A
preference with a larger value encourages more clusters to be produced. We set the preference at the
median value of the input similarity matrix to obtain a moderate number of clusters. The rnnlm toolkit
developed by Mikolov et al. (2011) is used to train RNNLM and obtain word embeddings.
7
We induce
both single and sense-specific embeddings with 50 dimensions. Finally, We obtain embeddings of a
vocabulary of 217K words, with a proportion of 8.4% having multiple sense clusters.
5.2 Evaluation on Word Similarity
Word embeddings can be directly used for computing similarities between words, which benefits many
practical applications. Therefore, we first evaluate our embeddings using a similarity measurement.
Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010):
MaxSim(u, v) = max
1?i?k
u
,1?j?k
v
s(u
i
, v
j
) (5)
AvgSim(u, v) =
1
k
u
?k
v
?
k
u
i=1
?
k
v
j=1
s(u
i
, v
j
) (6)
where k
u
and k
v
are the number of the induced senses for words u and v, respectively. s(?, ?) can be any
standard similarity measure. In this study, we use the cosine similarity.
Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin and
Wu, 2012) for the evaluation of general word similarity. These datasets rarely contain polysemous words,
and thus is unsuitable for our evaluation. To the best of our knowledge, no datasets for polysemous word
similarity evaluation have been published yet, either in English or Chinese. In order to fill this gap in the
research community, we manually construct a Chinese polysemous word similarity dataset.
5.2.1 Chinese Polysemous Word Similarity Dataset Construction
We adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset. HowNet is a Chinese
knowledge database that maintains comprehensive semantic definitions for each word in Chinese. The
process of the dataset construction includes three steps: (1) Commonly used polysemous words are
extracted according to their sense definitions in HowNet. (2) For each polysemous word, we select
several other words to form word pairs with it. (3) Each word pair is manually annotated with similarity.
In step (1), we mainly took advantage of HowNet for the selection of polysemous words. However,
the synsets defined in HowNet are often too fine-grained and many of them are difficult to distinguish,
6
scikit-learn.org
7
www.fit.vutbr.cz/
?
imikolov/rnnlm/
502
particularly for non-experts. Therefore, we manually discard those words with senses that are hard to
distinguish.
In step (2), for each polysemous word w selected in step 1, we sample several other words to form
word pairs withw. The sampled words can be roughly divided into two categories: related and unrelated.
The related words are sampled manually. They can be the hypernym, hyponym, sibling, (near-)synonym,
antonym, or topically related to one sense of w. The unrelated words are sampled randomly.
In step (3), we ask six graduate students who majored in computational linguistics to assign each word
pair a similarity score. Following the setting of WordSim-353, we restrict the similarity score in the range
(0.0, 10.0). To address the inconsistency of the annotations, we discard those word pairs with a standard
deviation greater than 1.0. We end up with 401 word pairs annotated with acceptable consistency. Unlike
the WordSim-353, in which most of the words are nouns, the words in our dataset are more diverse in
terms of part-of-speech tags.
Table 2 lists a sample of word pairs with annotated similarities from the dataset. The whole evaluation
dataset will be publicly available for the research community.
8
Word Paired word Category Mean.Sim Std.Dev
?? ??conquer synonym 8.60 0.29
??
key point
unrelated 0.12 0.19
? ?enter autonym 7.90 0.97
??
publish
near-synonym 7.86 0.76
? ?plant stem sibling 7.80 0.12
??
cost
topic-related 5.86 0.90
? ??
food
hypernym 6.50 0.71
Table 2: Sample word pairs of our dataset. The unrelated words are randomly sampled. Mean.Sim
represents the mean similarity of the annotations, Std.Dev represents the standard deviation.
5.2.2 Evaluation Results
Following Zou et al. (2013), we use Spearman?s ? correlation and Kendall?s ? correlation for evaluation.
The results are shown in Table 3. By utilizing sense-specific embeddings, our approach significantly
outperforms the single-version using either MaxSim or AvgSim measurement.
For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang et
al. (2012), which was first presented by Sch?utze (1998). The occurrences of a word are represented by
the average embeddings of its context words. Following Huang et al.?s settings, we use a context window
of size 10 and all occurrences of a word are clustered using the spherical k-means algorithm, where k is
tuned with a development set and finally set to 2.
System
MaxSim AvgSim
? ?100 ? ?100 ? ?100 ? ?100
Ours 55.4 40.9 49.3 35.2
SingleEmb 42.8 30.6 42.8 30.6
Multi-prototype 40.7 29.1 38.3 27.4
Table 3: Spearman?s ? correlation and Kendall?s ? correlation evaluated on the polysemous dataset.
Surprisingly, the multi-prototype method performs even slightly worse than the single-version, which
suggests that learning a fixed number of embeddings for every word may even harm the embedding.
Additionally, the clustering process of the multi-prototype approach suffers from high memory and time
cost, especially for the high-frequency words.
8
ir.hit.edu.cn/
?
jguo
503
To obtain intuitive insight into the superior performance of sense-specific embeddings, we list in the
last column of Table 1 the nearest neighborhoods of the sampled words in the evaluation dataset. The list
shows that we are able to find the different meanings of a word by using sense-specific embeddings.
5.3 Application on Chinese NER
We further apply the sense-specific embeddings as features to Chinese NER. We first perform WSD on
the NER data using the algorithm introduced in Section 4. For beam search decoding, the beam size B
is tuned on a development set and is finally set to 16.
We conduct our experiments on data from People?s Daily (Jan. and Jun. 1998).
9
The original corpus
contains seven NE types.
10
In this study, we select the three most common NE types: Person, Location,
Organization. The data from January are chosen as the training set (37,426 sentences). The first 2,000
sentences from June are chosen as the development set and the next 8,000 sentences as the test set.
CRF models are used in our NER system and are optimized by L2-regularized SGD. We use the
CRFSuite (Okazaki, 2007) because it accepts feature vectors with numerical values. The state-of-the-art
features (Che et al., 2013) are used in our baseline system. For both single and sense-specific embedding
features, we use a window size of 4 (two words before and two words after).
5.3.1 Results
Table 4 demonstrates the performance of NER on the test set. As desired, the single embedding features
improve the performance of our baseline, which were also shown in (Turian et al., 2010). Furthermore,
the sense-specific embeddings outperform the single word embeddings by nearly 1% F-score (88.56 vs.
87.58), which is statistically significant (p-value < 0.01 using one-tail t-test).
System P R F
Baseline 93.27 81.46 86.97
+SingleEmb 93.55 82.32 87.58
+SenseEmb (greedy) 93.38 83.56 88.20
+SenseEmb (beam search) 93.59 84.05 88.56
Table 4: Performance of NER on test data.
According to our hypothesis, the sense-specific embeddings should bring considerable improvements
to the NER of polysemous words. To verify this, we evaluate the per-token accuracy of the polysemous
words in the NER test data. We again adopt HowNet to determine the polysemy. Words that are defined
with multiple senses are selected as test set. Figure 4 shows that the sense-specific embeddings indeed
improve the NE recognition of the polysemous words, whereas the single embeddings even decrease the
accuracy slightly. We also obtain improvements on the NE recognition of the monosemous words, which
provide evidences that more accurate prediction of polysemous words is beneficial for the prediction of
the monosemous words through contextual influence.
6 Related Work
Previous studies have explored the NNLMs, which predict the next word given some history or future
words as context within a neural network architecture. Schwenk and Gauvain (2002), Bengio et al.
(2003), Mnih and Hinton (2007), and Collobert et al. (2011) proposed language models based on feed-
forward neural networks. Mikolov et al. (2010) studied language models based on RNN, which managed
to represent longer history information for word-predicting and demonstrated outstanding performance.
Besides, researchers have also explored the word embeddings learned by NNLMs. Collobert et al.
(2011) used word embeddings as the input of various NLP tasks, including part-of-speech tagging,
chunking, NER, and semantic role labeling. Turian et al. (2010) made a comprehensive comparison
of various types of word embeddings as features for NER and chunking. In addition, word embeddings
9
www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp
10
Person, Location, Organization, Date, Time, Number and Miscellany
504
Polysemous(2) Polysemous(3) Monosemous
per?t
oken
 acc
urac
y
0.60
0.65
0.70
0.75
0.80
0.85
0.90 Baseline +SingleEmb +SenseEmb
Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polyse-
mous(k) represents the set of words that have more than or equal to k senses defined in HowNet.
are shown to capture many relational similarities, which can be recovered by vector arithmetic in the
embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013)
learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to
constrain translational equivalence.
Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of
languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010)
and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each
word. They did this by clustering the contexts of words. These multi-prototype models simply induced
a fixed number of embeddings for every word, regardless of the real sense capacity of the specific word.
There has been a lot of work on using bilingual resources for word sense disambiguation (Gale et
al., 1992; Chan and Ng, 2005). By using aligned bilingual data along with word sense inventories such
as WordNet, training examples for WSD can be automatically gathered. We employ this idea for word
sense induction in our study, which is free of any pre-defined word sense thesaurus.
The most similar work to our sense induction method is Apidianaki (2008). They presented a method
of sense induction by clustering all occurrences of each word?s translation words. In their approach,
occurrences are represented with their contexts. We suggest that clustering contexts suffer from high
memory and time cost, as well as data sparsity. In our method, by clustering the embeddings of transla-
tion words, we induce word senses much more efficiently.
To evaluate word similarity models, researchers often apply a dataset with human-judged similarities
on word pairs, such as WordSim-353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG
(Rubenstein and Goodenough, 1965) and Jin and Wu (2012). For context-based multi-prototype mod-
els, (Huang et al., 2012) constructs a dataset with context-dependent word similarity. To the best of
our knowledge, there is no publicly available datasets for context-unaware polysemous word similarity
evaluation yet. This paper fills this gap.
7 Conclusion
This paper presents a novel and effective approach of producing sense-specific word embeddings by
exploiting bilingual parallel data. The proposed embeddings are expected to capture the multiple senses
of polysemous words. Evaluation on a manually annotated Chinese polysemous word similarity dataset
shows that the sense-specific embeddings significantly outperforms the single embeddings and the multi-
prototype approach.
Another contribution of this study is the development of a beam-search decoding algorithm based on
RNNLM for monolingual WSD. This algorithm bridges the proposed sense-specific embeddings and
practical applications, where no bilingual information is provided. Experiments on Chinese NER show
that the sense-specific embeddings indeed improve the performance, especially for the recognition of the
polysemous words.
505
Acknowledgments
We are grateful to Dr. Zhenghua Li, Yue Zhang, Shiqi Zhao, Meishan Zhang and the anonymous review-
ers for their insightful comments and suggestions. This work was supported by the National Key Basic
Research Program of China via grant 2014CB340503 and 2014CB340505, the National Natural Science
Foundation of China (NSFC) via grant 61370164.
References
Marianna Apidianaki. 2008. Translation-oriented word sense induction based on parallel corpora. In In Proceed-
ings of the 6th Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. The Journal of Machine Learning Research, 3:1137?1155.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word sense disambiguation via parallel texts. In AAAI,
volume 5, pages 1037?1042.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zhendong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116?
131.
Brendan J Frey and Delbert Dueck. 2007. Clustering by passing messages between data points. science,
315(5814):972?976.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierar-
chies via word embeddings. In Proceedings of the 52th Annual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, Baltimore MD, USA.
William A Gale, Kenneth W Church, and David Yarowsky. 1992. Using bilingual materials to develop word sense
disambiguation methods. In Proceedings of the 4th International Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 101?112.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume 1, pages 873?882.
Peng Jin and Yunfang Wu. 2012. Semeval-2012 task 4: evaluating chinese word similarity. In Proceedings of the
First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,
pages 374?377.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pulman. 2013. Separating disambiguation from composi-
tion in distributional semantics. CoNLL-2013, pages 114?123.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations
of words. In Proceedings of COLING 2012, pages 1459?1474, Mumbai, India, December.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the main conference
on Human Language Technology Conference of the North American Chapter of the Association of Computa-
tional Linguistics, pages 104?111.
Tomas Mikolov, Martin Karafi?at, Luk?a?s Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In Proceedings of Interspeech, pages 1045?1048.
506
Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and J
?
Cernock`y. 2011. Rnnlm-recurrent neural
network language modeling toolkit. In Proc. of the 2011 ASRU Workshop, pages 196?201.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word
representations. In Proceedings of NAACL-HLT, pages 746?751.
Tomas Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Ph. D. thesis, Brno
University of Technology.
George A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and
cognitive processes, 6(1):1?28.
Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th international conference on Machine learning, pages 641?648.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In Advances in
neural information processing systems, pages 1081?1088.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.
chokkan. org/software/crfsuite.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-
learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825?2830.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117.
Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connectionist language modeling for large vocabulary continuous
speech recognition. In Acoustics, Speech, and Signal Processing (ICASSP), 2002 IEEE International Confer-
ence on, volume 1, pages 765?768.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Ng. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing
Systems, pages 801?809.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Dianhai Yu. 2013. Compound embedding features for semi-
supervised learning. In Proceedings of NAACL-HLT, pages 563?568.
Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for
phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, pages 1393?1398, Seattle, Washington, USA, October.
507
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 530?540, Dublin, Ireland, August 23-29 2014.
Jointly or Separately: Which is Better for
Parsing Heterogeneous Dependencies?
Meishan Zhang
?
, Wanxiang Che
?
, Yanqiu Shao
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Beijing Language and Culture University
yqshao163@163.com
Abstract
For languages such as English, several constituent-to-dependency conversion schemes are pro-
posed to construct corpora for dependency parsing. It is hard to determine which scheme is
better because they reflect different views of dependency analysis. We usually obtain dependen-
cy parsers of different schemes by training with the specific corpus separately. It neglects the
correlations between these schemes, which can potentially benefit the parsers. In this paper, we
study how these correlations influence final dependency parsing performances, by proposing a
joint model which can make full use of the correlations between heterogeneous dependencies,
and finally we can answer the following question: parsing heterogeneous dependencies jointly
or separately, which is better? We conduct experiments with two different schemes on the Penn
Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that joint-
ly parsing heterogeneous dependencies can give improved performances for both schemes over
the individual models.
1 Introduction
Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008;
Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et
al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependen-
cy parser is usually constructed according to a specific constituent-to-dependency conversion scheme.
Several conversion schemes for certain languages have been available. For example, the English lan-
guage has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Ya-
mada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme
(de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues, 2007). There are dif-
ferent conversion schemes for the Chinese Penn Treebank (CTB) as well, including the Zhang scheme
(Zhang and Clark, 2008) and the Stanford scheme (de Marneffe and Manning, 2008). It is hard to
judge which scheme is more superior, because each scheme reflects a specific view of dependency analy-
sis, and also there is another fact that different natural language processing (NLP) applications can prefer
different conversion schemes (Elming et al., 2013).
Traditionally, we get dependency parsers of different schemes by training with the specific corpus
separately. The method neglects the correlations between these schemes, which can potentially help
different dependency parsers. On the one hand, there are many consistent dependencies across heteroge-
neous dependency trees. Some dependency structures remain constant in different conversion schemes.
Taking the Yamada and the Stanford schemes as an example, overall 70.27% of the dependencies are
identical (ignoring the dependency labels), according to our experimental analysis. We show a concrete
example for the two heterogeneous dependency trees in Figure 1, where six of the twelve dependencies
are consistent in the two dependency trees (shown by the solid arcs).
On the other hand, differences between heterogeneous dependencies can possibly boost the ev-
idences of the consistent dependencies. For example in Figure 1, the dependencies ?do
VC
xthink?
?
Corresponding author.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
530
We do n?t think at this point anything need to be said
SUB
ROOT
VMOD
VC
VMOD NMOD
PMOD
SUB
VMOD
VMOD
VMOD
VC
nsubj
aux
neg
root
prep det
pobj
nsubj
ccomp
aux
auxpass
xcomp
Figure 1: An example to show the differences and similarities of two dependency schemes. The above
dependency tree is based on the Yamada scheme, while the below dependency tree is based on the
Stanford scheme. The solid arcs show the consistent dependencies between the two dependency
trees, while the dashed arcs show the differences between the two trees.
and ?We
nsubj
x think? from the two trees can both be potential evidences to support the dependency
?thinkyat?. Another example, the label ?PMOD? from the Yamada scheme and the label ?pobj? from
the Stanford scheme on a same dependency ?atypoint? can make it more reliable than one alone.
In this paper, we investigate the influences of the correlations between different dependency schemes
on parsing performances. We propose a joint model to parse heterogeneous dependencies from two
schemes simultaneously, so that the correlations can be fully used by their interactions in a single model.
Joint models have been widely studied to enhance multiple tasks in NLP community, including joint
word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark,
2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word
segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed
over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies
together. To our knowledge, we are the first work to investigate joint models on parallel tasks.
We exploit a transition-based framework with global learning and beam-search decoding to imple-
ment the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art
transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the
Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. The results
show that our joint model gives improved performances over the individual baseline models for both
schemes on both English and Chinese languages, demonstrating positive effects of the correlations be-
tween the two schemes. We make the source code freely available at http://sourceforge.net/
projects/zpar/,version0.7.
2 Baseline
Traditionally, the dependency parsers of different schemes are trained with their corpus separately, using
a state-of-the-art dependency parsing algorithm (Zhang and Clark, 2008; Huang et al., 2009; Koo and
Collins, 2010; Zhang and McDonald, 2012; Choi and McCallum, 2013). In this work, we exploit a
transition-based arc-standard dependency parsing model combined with global learning and beam-search
decoding as the baseline. which is initially proposed by Huang et al. (2009). In the following, we give a
detailed description of the model.
In a typical transition-based system for dependency parsing, we define a transition state, which consists
of a stack to save partial-parsed trees and a queue to save unprocessed words. The parsing is performed
incrementally via a set of transition actions. The transition actions are used to change contents of the
stack and the queue in a transition state. Initially, a start state has an empty stack and all words of a
sentence in its queue. Then transition actions are applied to the start state, and change states step by step.
Finally, we arrive at an end state with only one parsed tree on the stack and no words in the queue. We
score each state by its features generated from the historical actions.
531
S1
? ? ?
? ? ?
S
0
? ? ?
S
AR(l)
AL(l)
P
R
Q
0
Q
1
? ? ?
Q
SH
(a) Arc-standard dependency parsing model for a single dependency tree
S
a
1
? ? ?
? ? ?
S
a
0
? ? ?
S
a
AR
a
(l)
AL
a
(l)
P
R
a
Q
a
0
Q
a
1
? ? ?
Q
a
SH
a
S
b
1
? ? ?
? ? ?
S
b
0
? ? ?
S
b
AR
b
(l)
AL
b
(l)
P
R
b
Q
b
0
Q
b
1
? ? ?
Q
b
SH
b
G
u
i
d
e
d
a
G
u
i
d
e
d
b
(b) The joint model based on arc-standard dependency parsing for two dependency trees
Figure 2: Illustrations for the baseline dependency parsing model and our proposed joint model.
In the baseline arc-standard transition system, we define four kinds of actions, as shown in Figure 2(a).
They are shift (SH), arc-left with dependency label l (AL(l)), arc-right with dependency label l (AR(l))
and pop-root (PR), respectively. The shift action shifts the first element Q
0
of the queue onto the stack;
the action arc-left with dependency label l builds a left arc between the top element S
0
and the second
top element S
1
on the stack, with the dependency label being specified by l; the action arc-right with
dependency label l builds a right arc between the top element S
0
and the second top element S
1
on the
stack, with the dependency label being specified by l; and the pop-root action defines the root node of a
dependency tree when there is only one element on the stack and no element in the queue.
During decoding, each state may have several actions. We employ a fixed beam to reduce the search
space. The low-score states are pruned from the beam when it is full. The feature templates in our
baseline are shown by Table 1, referring to baseline feature templates. We learn the feature weights by
the averaged percepron algorithm with early-update (Collins and Roark, 2004; Zhang and Clark, 2011).
3 The Proposed Joint Model
The aforementioned baseline model can only handle a single dependency tree. In order to parse multiple
dependency trees for a sentence, we usually use individual dependency parsers. This method is not
able to exploit the correlations across different dependency schemes. The joint model to parse multiple
dependency trees with a single model is an elegant way to exploit these correlations fully. Inspired by
this, we make a novel extension to the baseline arc-standard transition system, arriving at a joint model
to parse two heterogeneous dependency trees for a sentence simultaneously.
In the new transition system, we double the original transition state of one stack and one queue into
two stacks and two queues, as shown by Figure 2(b). We use stacks S
a
and S
b
and queues Q
a
and Q
b
to save partial-parsed dependency trees and unprocessed words for two schemes a and b, respectively.
Similarly, the transition actions are doubled as well. We have eight transition actions, where four of them
are aimed for scheme a, and the other four are aimed for scheme b. The concrete action definitions are
similar to the original actions, except an additional constraint that actions should be operated over the
corresponding stack and queue of scheme a or b.
We assume that the actions to build a specific tree of scheme a are A
a
1
A
a
2
? ? ?A
a
n
, and the actions to
532
Baseline feature templates
Unigram features
S
0
w S
0
t S
0
wt S
1
w S
1
t S
1
wt N
0
w N
0
t N
0
wt N
1
w N
1
t N
1
wt
Bigram features
S
0
w?S
1
w S
0
w?S
1
t S
0
t?S
1
w S
0
t?S
1
t S
0
w?N
0
w S
0
w?N
0
t S
0
t?N
0
w S
0
t?N
0
t
Second-order features
S
0l
w S
0r
w S
0l
t S
0r
t S
0l
l S
0r
l S
1l
w S
1r
w S
1l
t S
1r
t S
1l
l S
1r
l
S
0l2
w S
0r2
w S
0l2
t S
0r2
t S
0l2
l2 S
0r2
l2 S
1l2
w S
1r2
w S
1l2
t S
1r2
t S
1l2
l2 S
1r2
l2
Third-order features
S
0
t?S
0l
t?S
0l2
t S
0
t?S
0r
t?S
0r2
t S
1
t?S
1l
t?S
1l2
t S
1
t?S
1r
t?S
1r2
t
S
0
t?S
1
t?S
0l
t S
0
t?S
1
t?S
0l2
t S
0
t?S
1
t?S
0r
t S
0
t?S
1
t?S
0r2
t
S
0
t?S
1
t?S
1l
t S
0
t?S
1
t?S
1l2
t S
0
t?S
1
t?S
1r
t S
0
t?S
1
t?S
1r2
t
Valancy features
S
0
wv
l
S
0
tv
l
S
0
wv
r
S
0
tv
r
S
1
wv
l
S
1
tv
l
S
1
wv
r
S
1
tv
r
Label set features
S
0
ws
r
S
0
ts
r
S
0
ws
l
S
0
ts
l
S
1
ws
l
S
1
ts
l
Proposed new feature templates for the joint model
Guided head features
S
0
w?h
guide
S
0
t?h
guide
S
0
wt?h
guide
S
1
w?h
guide
S
1
t?h
guide
h
guide
Guided label features
S
0
w?S
0
l
guide
S
0
t?S
0
l
guide
S
0
wt?S
0
l
guide
S
1
w?S
0
l
guide
S
1
t?S
0
l
guide
S
0
l
guide
S
0
w?S
1
l
guide
S
0
t?S
1
l
guide
S
0
wt?S
1
l
guide
S
1
w?S
1
l
guide
S
1
t?S
1
l
guide
S
1
l
guide
Table 1: Feature templates for the baseline and joint models, where w denotes the word; t denotes the
POS tag; v
l
and v
r
denote the left and right valencies; l denotes the dependency label; s
l
and s
r
denotes
the label sets of the left and right children; the subscripts l and r denote the left-most and the right-most
children, respectively; the subscripts l2 and r2 denote the second left-most and the second right-most
children, respectively; h
guide
denotes the head direction of the top two elements on the processing stack
in the other tree; l
guide
denotes the label of the same word in the other tree.
build a specific tree of scheme b for the same sentence are A
b
1
A
b
2
? ? ?A
b
n
. We use ST
a
0
ST
a
1
? ? ? ST
a
n
and
ST
b
0
ST
b
1
? ? ? ST
b
n
to denote the historical states for the two action sequences, respectively. A sequence of
actions should consist of A
a
1
A
a
2
? ? ?A
a
n
and A
b
1
A
b
2
? ? ?A
b
n
in a joint model. However, one question that
needs to be answered is that, for a joint state (ST
a
i
,ST
b
j
), which action should be chosen as the next step
to merge the two action sequences into one sequence, A
a
i+1
or A
b
j+1
? To resolve the problem, we employ
a parameter t to limit the next action in the joint model. When t is above zero, an action for scheme b
can be applied only if the last action of scheme a is t steps in advance. For example, the action sequence
is A
a
1
A
b
1
A
a
2
A
b
2
? ? ?A
a
n
A
b
n
when t = 1. t can be negative as well, denoting the reverse constraints.
In the joint model, we extract features separately for the two dependency schemes. When the next
action is aimed for scheme a, we will extract features from S
a
and Q
a
, according to baseline feature
templates in Table 1. In order to make use of the correlations between the two dependency parsing trees,
we introduce several new feature templates, shown in Table 1 referring to proposed new feature templates
for the joint model. The new features are based on two kinds of atomic features: the guided head h
guide
and the guided dependency label l
guide
. Assuming that the currently processing scheme is a, when the
top two elements (S
a
0
and S
a
1
) have both found their heads in Guided
b
(the partial-parsed trees of scheme
b), we can fire the atomic feature h
guide
, which denotes the arc direction between S
0
and S
1
in Guide
b
(S
x
0
S
1
, S
y
0
S
1
or other). When S
a
0
or S
a
1
has its dependency label in Guided
b
, we can fire the atomic
feature l
guide
, which denotes the dependency label of S
a
0
or S
a
1
in Guided
b
. Similarly we can extract the
h
guide
and l
guide
from Guide
a
when we are processing scheme b. When t is infinite, we always have
533
the two atomic features, because the other tree is already parsed. Thus the proposed new features can be
the most effective when t = ? and t = ??. In other conditions, the other tree may not be ready for
the new feature extracting. Similar to the baseline model, we use the beam-search decoding strategy to
reduce the search space, and use the averaged perceptron with early-update to learn the feature weights.
We are especially interested in two cases of the joint models when t is infinite (t =? and t = ??),
where the tree of one specified scheme is always processed after the other tree is finished, because the
new features can be most effectively exploited according to the above analysis. We assume that the first
and second processing schemes are s
1
and s
2
respectively, to facilitate the below descriptions. We can see
that the joint model behaves similarly to a pipeline reranking model, in optimizing scheme s
1
?s parsing
performances. First we get K-best (K equals the beam size of the joint model) candidates for scheme s
1
,
and then employ additional evidences from scheme s
2
?s result, to rerank the K-best candidates, obtaining
a better result. The joint model also behaves similarly to a pipeline feature-based stacking model (Li et
al., 2012), in optimizing scheme s
2
?s parsing performances. After acquiring the best result of scheme
s
1
, we can use it to generate guided features to parse dependencies of scheme s
2
. Thus additional
information from scheme s
1
can be imported into the parsing model of scheme s
2
. Different with the
pipeline reranking and the feature-based stacking models, we employ a single model to achieve the two
goals, making the interactions between the two schemes be better performed.
4 Experiments
4.1 Experimental Settings
In order to evaluate the baseline and joint models, we conduct experiments on English and Chinese da-
ta. For English, we obtain heterogeneous dependencies by the Yamada and the Stanford schemes,
respectively. We transform the bracket constituent trees of English sentences into the Yamada dependen-
cies with the Penn2Malt tool,
1
and into the Stanford dependencies with the Stanford parser version
3.3.1.
2
Following the standard splitting of PTB, we use sections 2-21 as the training data set, section 22 as
the development data set, and section 23 as the final test data set. For Chinese, we obtain heterogeneous
dependencies by the Zhang and the Stanford schemes, respectively. The Zhang dependencies are
obtained by the Penn2Malt tool using the head rules from Zhang and Clark (2008), while the Stanford
dependencies are obtained by the Stanford parser version 3.3.1 similar to English.
We use predicted POS tags in all the experiments. We utilize a linear-CRF POS tagger to obtain
automatic POS tags for English and Chinese datasets.
3
We use a beam size of 64 to train dependency
parsing models. We train the joint models with the Yamada or Zhang dependencies being handled
on stack S
a
and queue Q
a
, and the Stanford dependencies being handled on stack S
b
and queue Q
b
,
referring to Section 3. We follow the standard measures of dependency parsing to evaluate the baseline
and joint models, including unlabeled attachment score (UAS), labeled attachment score (LAS) and
complete match (CM). We ignore the punctuation words for all these measures.
4.2 Development Results
4.2.1 Baseline
Table 2 at the subtable ?Baseline? shows the baseline results on the development data set. The perfor-
mances of the Yamada scheme are better than those of the Stanford scheme. The UAS and LAS of
the Yamada scheme are 92.83 and 91.73 respectively, while they are 92.85 and 90.49 for the Stanford
scheme respectively. The results demonstrate that parsing the Stanford dependencies is more difficult
than parsing the Yamada dependencies because of the lower performances of the Stanford scheme.
1
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html.
2
The tool is available on http://nlp.stanford.edu/software/lex-parser.shtml. We use three options to
perform the conversion: ?-basic? and ?-keepPunct?, respectively.
3
The tagging accuracies are 97.30% on the English test dataset and 93.68% on the Chinese test dataset. We thank Hao
Zhang for sharing the data used in Martins et al. (2013) and Zhang et al. (2013a).
534
Model
Yamada Stanford
UAS LAS CM UAS LAS CM
Baseline 92.83 91.73 47.35 92.85 90.49 50.06
The joint models,
where the Yamada dependencies are processed with priority
t = 1 92.65 91.55 46.35 93.11 90.75 50.24
t = 2 92.65 91.57 46.71 93.15 90.77 50.59
t = 3 92.82 91.74 47.12 93.19 90.82 50.76
t = 4 92.89 91.78 47.35 93.27 90.93 51.29
t =? 93.04 92.01 48.65 93.52 91.15 52.59
The joint models,
where the Stanford dependencies are processed with priority
t = ?1 92.62 91.54 46.71 93.10 90.70 50.76
t = ?2 92.50 91.41 46.18 93.06 90.74 51.12
t = ?3 92.57 91.42 47.00 93.10 90.68 51.35
t = ?4 92.74 91.60 47.41 93.15 90.72 51.29
t = ?? 93.04 91.95 47.88 93.19 90.91 50.71
Table 2: The main results on the development data set of the baseline and proposed joint models.
4.2.2 Parameter Tuning
The proposed joint model has one parameter t to adjust. The parameter t is used to control the decoding in
a joint model, determining which kind of dependencies should be processed at the next step. In our joint
model, if t is larger than zero, scheme a (the Yamada scheme) should be handled t steps in advance,
while when t is smaller than zero, scheme b (the Stanford scheme) should be handled in advance.
When the value of t is infinite, the dependency tree of one scheme is handled until the dependency tree
of the other scheme is finished for a sentence.
As shown by Table 2, we have two major findings. First, the joint models are slightly better when t is
above zero, by decoding with the Yamada scheme in advance. The phenomenon demonstrates that the
decoding sequence is important in the joint parsing models. Second, no matter when t is above or below
zero, the performances arrive at the peak when t is infinite. One benefit of the joint models is that we
can use the correlations between different dependency trees, through the new features proposed by us.
The new features can be the most effective when t is infinite according to the analysis Section 3. Thus
this finding indicates that the new features are crucial in the joint models, since the ineffective utilization
would decrease the model performances a lot. Actually, when the absolute value of t is small, the features
can sometimes be fired and in some other times are not able to be fired, making the training insufficient
and also inconsistent for certain word-pair dependencies when their distances can differ (when t = 1 for
example, the joint model can fire the new features only if the dependency distance equals 1). This would
make the final model deficient, and can even hurt performances of the Yamada scheme.
According to the results on the development data set, we use the t = ? for the final joint model,
which first finishes the Yamada tree and then the Stanford tree for each sentence. Our final model
achieves increases of 0.21 on UAS and 0.28 on LAS for the Yamada scheme, and increases 0.67 on
UAS and 0.66 on LAS for the Stanford scheme.
4.2.3 Feature Ablation
In order to test the effectiveness of the proposed new features, we conduct a feature ablation experiment.
Table 3 shows the results, where the mark ?/wo? denotes the model without the new features proposed
by us. For the Yamada scheme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new
features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS,
respectively. The results demonstrate the new features are effective in the joint model.
535
Model
Yamada Stanford
UAS LAS CM UAS LAS CM
Our joint model 93.04 92.01 48.65 93.52 91.15 52.59
Our joint model/wo 92.89 91.80 48.25 92.95 90.57 50.62
? -0.15 -0.21 -0.40 -0.57 -0.58 -1.97
Table 3: Feature ablation results.
Model
Yamada Stanford
UAS LAS CM UAS LAS CM
Baseline 92.71 91.67 47.48 92.72 90.61 47.76
Our joint model 92.89 91.86 48.39 93.30
?
91.19
?
50.37
Zhang and Nivre (2011) 92.9 91.8 48.0 ? ? ?
Rush and Petrov (2012) ? ? ? 92.7
?
? ?
Martins et al. (2013) 93.07 ? ? 92.82
?
? ?
Zhang et al. (2013a) 93.50 92.41 ? 93.64
?
91.28
?
?
Zhang and McDonald (2014) 93.57 92.48 ? 93.71
?
/93.01
??
91.37
?
/90.64
??
?
Kong and Smith (2014) ? ? ? 92.20
??
89.67
??
?
Table 4: The final results on the test data set, where the results with mark
?
demonstrates that the p-value
is below 10
?3
using t-test. Our Stanford dependencies are slightly different with previous works, where
the results with mark
?
show the numbers for the Stanford dependencies from Stanford parser version
2.0.5 and the results with mark
??
show the numbers for the Stanford dependencies from Stanford parser
version 3.3.0.
4.3 Final Results
Table 4 shows our final results on the English test dataset. The final joint model achieves better per-
formances than the baseline models for both the Yamada and the Stanford schemes, by increases
of 0.18 on UAS and 0.19 on LAS for the Yamada scheme, and increases of 0.58 on UAS and 0.58
on LAS for the Stanford scheme. The results demonstrate that the interactions between the two de-
pendency schemes are useful, and the joint model is superior to separately trained models in handling
heterogeneous dependencies.
We compare our results with some representative previous work of dependency parsing as well. Zhang
and Nivre (2011) is a feature-rich transition-based dependency parser using the arc-eager transition sys-
tem. Rush and Petrov (2012), Zhang et al. (2013a) and Zhang and McDonald (2014) are state-of-the-art
graph-based dependency parsers. Martins et al. (2013) and Kong and Smith (2014) report their results
with the full TurboParser. TurboParser is also a graph-based dependency parser but its decoding algo-
rithm has major differences with the general MST-style decoding.
4.4 Analysis
To better understand the joint model, we conduct analysis work on the Chinese development dataset.
First, we make a comparison to see whether the consistent dependencies give larger increases by the
joint model. As mentioned before, the consistent dependencies can be supported by different evidences
from heterogeneous dependencies. We compute the proportion of the consistent dependencies (ignoring
the dependency labels) between the Yamada and the Stanford dependencies, finding that 70.27% of
the overall dependencies are consistent. Table 5 shows the comparison results. The joint model shows
improvements for the consistent dependencies. However, it does not always show positive effectiveness
for the inconsistent dependencies. The results support our initial motivation that consistent dependencies
can benefit much in joint models .
We also make a comparison between the baseline and joint models with respect to dependency dis-
tance. We use the F-measure value to evaluate the performances. The dependency distances are normal-
536
Yamada Stanford
Consistent Inconsistent Consistent Inconsistent
UAS LAS UAS LAS UAS LAS UAS LAS
Baseline 93.43 92.39 91.44 90.17 93.74 91.35 90.75 88.47
Our joint model 93.81 92.85 91.21 90.02 94.58 92.15 91.01 88.78
? +0.38 +0.46 -0.23 -0.15 +0.84 +0.80 +0.36 +0.31
Table 5: Performances of the baseline and joint models by whether the dependencies are consistent
across the Yamada and the Stanford schemes, where the bold numbers denote the larger increases by
comparisons of consistent and inconsistent dependencies for each scheme.
1 2 3 4 5 6 7
75
80
85
90
95
F
-
m
e
a
s
u
r
e
(
%
)
Baseline Joint
(a) Yamada
1 2 3 4 5 6 7
65
75
85
95
F
-
m
e
a
s
u
r
e
(
%
)
Baseline Joint
(b) Stanford
Figure 3: F-measures of the two heterogeneous dependencies with respect to dependency distance.
ized to a max value of 7. Figure 3 shows the comparison results. We find that the joint model can achieve
consistent better performances for the dependencies of different dependency distance, demonstrating the
robustness of the joint model in improving parsing performances. The joint model performs slightly
better for long-distance dependencies, which is more obvious for the Stanford scheme.
4.5 Parsing Heterogeneous Chinese Dependencies
Table 6 shows our final results on the Chinese test data set. For Chinese, the joint model achieves better
performances with Stanford dependencies being parsed first. The final joint model achieves better
performances than the baseline models for both the Zhang and the Stanford schemes, by increases
of 1.13 on UAS and 0.99 on LAS for the Zhang scheme, and increases of 0.30 on UAS and 0.36 on
LAS for the Stanford scheme. The results also demonstrate similar conclusions with the experiments
on English dataset.
5 Related Work
Our work is mainly inspired by the work of joint models. There are a number of successful studies
on joint modeling pipelined tasks where one task is a prerequisite step of another task, for example,
the joint model of word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009;
Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011;
Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et
Model
Zhang Stanford
UAS LAS CM UAS LAS CM
Baseline 79.07 76.08 27.96 80.33 75.29 31.14
Our joint model 80.20
?
77.07
?
30.10 80.63 75.65 31.20
Table 6: The final results on the test data set, where the results with mark
?
demonstrates that the p-value
is below 10
?3
using t-test.
537
al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic
analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two
heterogeneous dependency trees simultaneously.
There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to
enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a
specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented
a joint inference framework to combine the parsing results based on two different treebanks. All these
work are case studies of annotation adaptation from different sources, which have been done for Chinese
word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their
work, we study the heterogeneous annotations derived from the same source. We use a unified model to
parsing heterogeneous dependencies together.
Our joint parsing model exploits a transition-based framework with global learning and beam-search
decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang
et al., 2009). The transition-based framework is easily adapted to a number of joint models, including
joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing
(Hatori et al., 2012; Bohnet and Nivre, 2012), and also joint word segmentation, POS-tagging and parsing
(Hatori et al., 2012; Zhang et al., 2013b; Zhang et al., 2014).
6 Conclusions
We studied the effectiveness of the correlations between different constituent-to-dependency schemes
for dependency parsing, by exploiting these information with a joint model to parse two heterogeneous
dependency trees simultaneously. We make a novel extension to a transition-based arc-standard depen-
dency parsing algorithm for the joint model. We evaluate our baseline and joint models on both English
and Chinese datasets, based on the Yamada/Zhang and the Stanford dependency schemes. Final
results demonstrate that the joint model which handles two heterogeneous dependencies can give im-
proved performances for dependencies of both schemes. The source code for the joint model is publicly
available at http://sourceforge.net/projects/zpar/,version0.7.
Acknowledgments
We thank Yue Zhang and the anonymous reviewers for their constructive comments, and grateful-
ly acknowledge the support of the National Basic Research Program (973 Program) of China via
Grant 2014CB340503, the National Natural Science Foundation of China (NSFC) via Grant 61133012,
61170144 and 61370164.
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings of the EMNLP-CONLL, pages 1455?1465, Jeju Island,
Korea, July.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich?ard Farkas Filip Ginter, and Jan Hajic. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. TACL, 1.
Jinho D. Choi and Andrew McCallum. 2013. Transition-based dependency parsing with selectional branching. In
Proceedings of ACL, pages 1052?1062, August.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the
ACL, pages 111?118, Barcelona, Spain, July.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,
pages 1?8, Manchester, UK, August.
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi, Hector Martinez Alonso, and Anders
S?gaard. 2013. Down-stream effects of tree-to-dependency conversions. In Proceedings of the NAACL, pages
617?626, Atlanta, Georgia, June.
538
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2011. Incremental joint POS tagging and
dependency parsing in Chinese. In Proceedings of 5th IJCNLP, pages 1216?1224, Chiang Mai, Thailand,
November.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2012. Incremental joint approach to word
segmentation, POS tagging, and dependency parsing in Chinese. In Proceedings of the 50th ACL, pages 1045?
1053, Jeju Island, Korea, July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009. Bilingually-constrained (monolingual) shift-reduce parsing. In
Proceedings of the EMNLP, pages 1222?1231.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?u. 2008. A cascaded linear model for joint Chinese word
segmentation and part-of-speech tagging. In Proceedings of ACL-08, pages 897?904, Columbus, Ohio, June.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word
segmentation and POS tagging: a case study. In Proceedings of the ACL-IJCNLP, pages 522?530.
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In
Proceedings of NODALIDA 2007, Tartu, Estonia.
Lingpeng Kong and Noah A Smith. 2014. An empirical comparison of parsing methods for stanford dependencies.
arXiv preprint arXiv:1404.4314.
Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual
Meeting of the ACL, pages 1?11.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara.
2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In
Proceedings of the ACL-IJCNLP, pages 513?521, Suntec, Singapore, August.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for
Chinese POS tagging and dependency parsing. In Proceedings of the EMNLP, pages 1180?1191, Edinburgh,
Scotland, UK., July.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Exploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th ACL, pages 675?684, Jeju Island, Korea, July.
Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proceedings of the 51st ACL, pages 617?622, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, number June, pages 91?98, Morristown, NJ, USA.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932.
Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics,
34(4):513?553.
Alexander M Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Proceed-
ings of the NAACL, pages 498?507.
Francesco Sartorio, Giorgio Satta, and Joakim Nivre. 2013. A transition-based dependency parser using a dynamic
parsing strategy. In Proceedings of the 51st ACL, pages 135?144, Sofia, Bulgaria, August.
Weiwei Sun and Xiaojun Wan. 2012. Reducing approximation and estimation errors for Chinese lexical processing
with heterogeneous annotations. In Proceedings of the 50th ACL, pages 232?241, Jeju Island, Korea, July.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of IWPT, volume 3.
Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings of EMNLP, pages 562?571, Honolulu, Hawaii, October.
Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single
discriminative model. In Proceedings of the EMNLP, pages 843?852, Cambridge, MA, October.
539
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
Hao Zhang and Ryan McDonald. 2012. Generalized higher-order dependency parsing with cube pruning. In
Proceedings of the EMNLP, pages 320?331.
Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing. In
Proceedings of ACL. Association for Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Pro-
ceedings of the 49th ACL, pages 188?193, Portland, Oregon, USA, June.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDonald. 2013a. Online learning for inexact hypergraph search.
In Proceedings of the EMNLP, pages 908?913, Seattle, Washington, USA, October. Association for Computa-
tional Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013b. Chinese parsing exploiting characters. In
Proceedings of the 51st ACL, pages 125?134, Sofia, Bulgaria, August.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014. Character-level Chinese Dependency Parsing.
In Proceedings of the 52st ACL.
Guangyou Zhou and Jun Zhao. 2013. Joint inference for heterogeneous dependency parsing. In Proceedings of
the 51st ACL, pages 104?109, Sofia, Bulgaria, August.
540
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1360?1369, Dublin, Ireland, August 23-29 2014.
Sentence Compression for Target-Polarity Word Collocation Extraction
Yanyan Zhao
1
, Wanxiang Che
2
, Honglei Guo
3
, Bing Qin
2
, Zhong Su
3
and Ting Liu
2?
1: Department of Media Technology and Art, Harbin Institute of Technology
2: Department of Computer Science and Technology, Harbin Institute of Technology
3: IBM Research-China
{yyzhao, bqin, tliu}@ir.hit.edu.cn, {guohl, suzhong}@cn.ibm.com
Abstract
Target-polarity word (T-P) collocation extraction, a basic sentiment analysis task, relies primarily
on syntactic features to identify the relationships between targets and polarity words. A major
problem of current research is that this task focuses on customer reviews, which are natural or
spontaneous, thus posing a challenge to syntactic parsers. We address this problem by proposing
a framework of adding a sentiment sentence compression (Sent Comp) step before performing
T-P collocation extraction. Sent Comp seeks to remove the unnecessary information for senti-
ment analysis, thereby compressing a complicated sentence into one that is shorter and easier to
parse. We apply a discriminative conditional random field model, with some special sentiment-
related features, in order to automatically compress sentiment sentences. Experiments show that
Sent Comp significantly improves the performance of T-P collocation extraction.
1 Introduction
Sentiment analysis deals with the computational treatment of opinion, sentiment and subjectivity in tex-
t (Pang and Lee, 2008), and has received considerable attention in recent years (Liu, 2012). Target-
Polarity word (T-P) collocation extraction, which aims to extract the collocation of a target and its cor-
responding polarity word in a sentiment sentence, is a basic task in sentiment analysis. For example,
in a sentiment sentence ????????????? (The camera has a novel appearance), ????
(appearance) is the target, and ???? (novel) is the polarity word that modifies ???? (appearance).
According, ???, ??? (?appearance, novel?) is the T-P collocation. Generally, T-P collocation is a
basic and complete sentiment unit, thus is very useful for many sentiment analysis applications.
Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (Ab-
basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation ?Adj
ATT
x Noun?, where the
ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P
collocation ???, ??? (?appearance, novel?) in the above sentiment sentence (Bloom et al., 2007;
Qiu et al., 2011; Xu et al., 2013).
However, one major problem of these approaches is the ?naturalness? of sentiment sentences, that is,
such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge
to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can
further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a)
as an example, because the word ???? (fortunately) is so chatty,1 the parsing result is wrong. Thus,
are unable to extract the T-P collocation ???,?? (?keyboard, good?).
To solve the ?naturalness? problem, we can train a parser on sentiment sentences. Unfortunately, an-
notating such data will cost us a lot of time and effort. Instead, in this paper we produce a sentence
compression model, Sent Comp, which is designed especially to compress complicated sentiment sen-
tences into formal and easier to parse ones, further improving T-P collocation extraction.
?
Correspondence author: tliu@ir.hit.edu.cn
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Note that, in Figure 1, the Chinese word ???? is chatty, although its translated English word ?fortunately? is not. In this
paper, we focus on processing the Chinese data.
1360
?? ?? ?
fortunately keyboard good
SBV
VOB
ROOT
(a) before compression
?? ?
keyboard good
SBV
ROOT
(b) after compression
Figure 1: Parse trees before and after compression.
This idea is motivated by the observation that, current syntactic parsers usually perform accurately
for short, simple and formal sentences, whereas error rates increase for longer, more complex or more
natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing
performance would have a ripple effect over T-P collocation extraction. For example, we can compress
the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part ????
(fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree
is correct, making it easier to accurately extract T-P collocation.
Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining impor-
tant information (usually important grammar structure) (Jing, 2000). For example, the sentence ?Overall,
this is a great camera.? can be compressed into ?This is a camera.? by removing the adverbial ?overall?
and the modifier ?great?. However, the modifier ?great? is a polarity word and very important for sen-
timent analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional
compression models, because it needs to retain the important sentiment information, such as the polarity
word. Hence, using Sent Comp, the above sentence can be compressed into ?This is a great camera.?
We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random
fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other
studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce
some sentiment-related features to retain the sentiment information for Sent Comp.
We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the
sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-the-
art T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese
corpus of four product domains show the effectiveness of our approach.
The main contributions of this paper are as follows:
? We present a framework of using sentiment sentence compression preprocessing step to improve T-
P collocation extraction. This framework can better solve the ?over-natural? problem of sentiment
sentences, which poses a challenge to syntactic parsers. More importantly, the idea of this frame-
work can be applied to some other sentiment analysis tasks that rely heavily on syntactic results.
? We develop a simple yet effective compression model Sent Comp for sentiment sentences. To the
best of our knowledge, this is the first sentiment sentence compression model.
2 Background
For our baseline system, we used the state-of-the-art method to extract T-P collocations introduced by
Qiu et al. (2011), who proposed a double propagation method. This idea is based on the observation
that there is a natural syntactic relationship between polarity words and targets owing to the fact that
polarity words are used to modify targets. Furthermore, they also found that polarity words and targets
themselves have relations in some sentiment sentences (Qiu et al., 2011).
Based on this idea, in the double propagation method, we first used an initial seed polarity word lexicon
and the syntactic relations to extract the targets, which can fall into a new target lexicon. Then we used the
target lexicon and the same syntactic relations to extract the polarity words and to subsequently expand
the polarity word lexicon. This is an iterative procedure, because this method can iteratively produce the
new polarity words and targets back and forth using the syntactic relations.
1361
?? ?? ??
function very powerful
SBV
ADV
ROOT
(a) syntactic structure 1
?? ? ??
powerful function
ATT
RAD
ROOT
(b) syntactic structure 2
?? ? ?? ?
function is powerful
SBV VOB RAD
ROOT
(c) syntactic structure 3
?? ? ?? ?? ??
function and service very powerful
COO
LAD ADV
SBV ROOT
(d) syntactic structure 4
?? ?? ?? ? ??
function very powerful and complete
SBV COO
ADV LAD
ROOT
(e) syntactic structure 5
Figure 2: Example of syntactic structure rules for T-P collocation extraction. We showed five examples
from a total of nine syntactic structures. For each kind of syntactic structure (a) to (e), the target is
shown with a red box and the polarity word is shown with a green box. Syntactic structures (a) to (c)
describe the relations between targets and polarity words. Syntactic structure (d), which is extended
from (a), describes the relation between two targets. Syntactic structure (e), which is also extended from
(a), describes the relation between two polarity words. Similarly, we can summarize the other four rules
extended from (b) and (c) to describe the relations between two targets or two polarity words.
We can see that the syntactic relations are important for this method, and Qiu et al. (2011) proposed
eight rules to describe these relations. However, their work only focused on English sentences, whereas
the relations for Chinese sentences are different. Thus, in accordance with Chinese grammar, we pro-
posed nine syntactic structure rules between target t and polarity word p in a Chinese T-P collocation
?t, p?.
2
The three main rules are shown below and some example rules are illustrated in Figure 2.
Rule 1: t
SBV
x p, the ?subject-verb? structure between t and p, such as the example in Figure 2(a).
Rule 2: p
ATT
x t, that p is an attribute for t, such as the example in Figure 2(b).
Rule 3: t
SBV
x ?
VOB
y p, the ?subject-verb-object? structure between t and p, such as the example in
Figure 2(c). The ? denotes any word.
The other six rules can be extended from the three main rules by obtaining the coordination (COO)
relation of t or p, such as t
SBV
x ?
COO
y p in Figure 2(e). Note that the POS for t should be noun and for p
should be adjective.
As described above, the T-P collocation extraction relies heavily on syntactic parsers. Hence, if we
can use the Sent Comp model to improve the performance of parsers, the performance of T-P collocation
extraction can also be improved accordingly.
3 Sentiment Sentence Compression
3.1 Problem Analysis
First, we conducted an error analysis for the results of current T-P collocation extraction, from which we
observed that the ?naturalness? of sentiment sentences is one of the main problems. For examples:
? Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser.
For example, in the sentence ??????? (fortunately the keyboard is good) shown in Figure 1,
the usage of the chatty word ???? (fortunately) affects the accuracy of the syntactic parser.
2
A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our
dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based
dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05).
1362
?? ?? ?
besides photo good
ADV
POB
ROOT
comp
?? ?
photo good
SBV
ROOT
(a) parse tree 1 before and after compression
?? ? ? ? ?? ??screen for people feel good
SBV
ATT
POBRAD SBV
ROOT
comp
?? ??screen good
SBV
ROOT
(b) parse tree 2 before and after compression
Figure 3: ?Naturalness? problem of sentiment sentences.
? Conjunction word usage: conjunction words are often used in sentiment sentences to show the dis-
course relations between two sentences. However, there are so many conjunction words in Chinese,
some of which can cause errors among parsers. For example, in Figure 3(a), the parse tree of sen-
tence ???????? (besides the photo is good) is wrong because of the usage of the conjunction
word ???? (besides).
? Feeling words/phrase usage: in sentiment sentences, people often use some feeling words/phrase,
such as ??????? (feel like) in Figure 3(b) or ????? (smell like). Given that the current
syntactic parser cannot handle the feeling words/phrases very well, the T-P collocation ???, ?
?? (?screen, good?) in Figure 3(b) cannot be extracted correctly.
To address the ?naturalness? problem, we compressed the sentiment sentences into one that are shorter
and easier to parse. Similar to the examples in Figure 1 and 3, the compressed sentences can be easily
and correctly parsed. The above analysis can be used as the criteria to guide us in compressing sentiment
sentences when annotating, and can also help us exploit more useful features for automatic sentiment
sentence compression.
3.2 Task Definition
We focus on studying the methods for extractive sentence compression.
3
Formally, extractive sentence
compression aims to shorten a sentence x = x
1
? ? ?x
n
into a substring y = y
1
? ? ? y
m
, where y
i
?
{x
1
, ? ? ? , x
n
}, m ? n.
In this paper, similar to Nomoto (2007), we also treated the sentence compression as a sequence
labeling task which can be solved by a CRF model. We assigned a compression tag t
i
to each word x
i
in
an original sentence x, where t
i
= N if x
i
? y, else t
i
= Y.
A first-order linear-chain CRF is used which defines the following conditional probability:
P (t|x) =
1
Z(x)
?
i
M
i
(t
i
, t
i?1
|x) (1)
where x and t are the input and output sequences respectively, Z(x) is the partition function, and M
i
is
the clique potential for edge clique i. Here, we used the CRFsuite toolkit to train the CRF model.
4
3.3 Features
The features for Sent Comp are listed in Table 1. Aside from the basic word (w), POS tag (t) and
their combination context features (01 ? 04), we introduced some sentiment-related features (05 ? 06)
and latent semantic features (07 ? 08) to better handle sentiment analysis data and generalize word
features. Then we added the syntactic parse features (09), which are commonly used in traditional
sentence compression task.
One sentiment-related feature (feeling(?)) indicates whether a word is a feeling word, which is inspired
by the naturalness problem in Figure 3(b). As discussed above, the current parser often produces wrong
parse trees because of these feeling words. Therefore, the feeling words tend to be removed from a
3
Generally, there are two kinds of sentence compression methods: extractive method and abstractive method. Because
abstractive method needs more resource and is more complicated, in this paper, we only focus on extractive approach.
4
www.chokkan.org/software/crfsuite/
1363
Basic Features
01: w
i+k
,?1 ? k ? 1
02: w
i+k?1
? w
i+k
, 0 ? k ? 1
03: t
i+k
,?2 ? k ? 2
04: t
i+k?1
? t
i+k
,?1 ? k ? 2
Sentiment-related Features
05: feeling(w
i
)
06: polarity(w
i
)
Latent Semantic Features
07: suffix(w
i
) if t(w
i
) == n else prefix(w
i
)
08: cluster(w
i
)
Syntactic Features
09: dependency(w
i
)
Table 1: Features of sentiment sentence compression
sentiment sentence for Sent Comp. We can obtain a feeling word lexicon from HowNet,
5
a popular
Chinese sentiment thesaurus, where a feeling word is defined by DEF={perception|??} tag. Finally,
we collected 38 feeling words, such as?? (realize),?? (find), and?? (think).
The other sentiment-related feature (polarity(?)) indicates whether a word is a polarity word. One
of the main differences between a sentiment sentence and a formal sentence is that the former often
contains polarity words. In contrast to the features of feeling(?), polarity words (e.g., ?great? in the
sentence ?Overall, this is a great camera?) tend to be retained, because they are important and special
to sentiment analysis. In this paper, we treat polarity words as important features, considering that they
are often tagged as modifiers and are easily removed by common sentence compression methods. We
can obtain the polarity feature (polarity(?)) from a polarity lexicon, which can also be obtained from
HowNet.
To generalize the words in sentiment sentences, we proposed two kinds of semantic features. The
first one is a suffix or prefix character feature (prefix(?) or suffix(?)). In contrast to English, the suffix
(for noun) or prefix (for non noun) characters of a Chinese word often carry that word?s core semantic
information. For example, ??? (bicycle), ?? (car), and ?? (train) are all various kinds of ?
(vehicle), which is also the suffix of the three words. Given that all of them may become targets, they
tend to be retained in compressed sentences. The verbs, ?? and ??, can be denoted by their prefix
feel (?), and can be removed from original sentences because they are feeling words.
We used word clustering features (cluster(?)) as the other latent semantic feature to further improve
the generalization over common words. Word clustering features contain some semantic information
and have been successfully used in several natural language processing tasks, including NER (Miller et
al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words ??
and ?? (appearance) belong to the same word cluster, although they have a different suffix or prefix.
Both words are important for T-P collocation extraction and should be retained. We used the Brown
word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were
obtained from the fifth edition of Chinese Gigaword (LDC2011T13).
Finally, similar to McDonald (2006), we also added the dependency relation between a word and its
parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence
compression. For example, the ROOT relation typically indicates that the word should not be removed
because it is the main verb of a sentence.
4 Experiments
4.1 Experimental Setup
4.1.1 Corpus
We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3
of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).
6
Table 2 describes the corpus,
5
www.keenage.com
6
www.ir-china.org.cn/coae2008.html
1364
Domain # reviews # sentences # collocations
Camera 138 1,249 1,335
Car 161 1,172 1,312
Notebook 56 623 674
Phone 123 1,350 1,479
All 478 4,394 4,800
Table 2: Corpus statistics for the Chinese corpus of four product domains.
where 4,394 sentiment sentences containing 4,800 T-P collocations are manually found and annotated
from 478 reviews.
We ask annotators to manually compress all the sentiment sentences. Specifically, the annotators
removed some words from a sentiment sentence according to two criteria stated as follows: (1) removing
the word should not change the essential content of the sentence, and (2) removing the word should
not change the sentiment orientation of the sentence. In order to assess the quality of the annotation,
we sampled 500 sentences from this corpus and asked two annotators to perform the annotation. The
resulting word-based Cohen?s kappa (Cohen, 1960) (i.e., a measure of inter-annotator agreement ranging
from zero to one) of 0.7 indicated a good strength of agreement.
4.1.2 Evaluation
Generally, compressions are evaluated using three criteria (McDonald, 2006), namely, grammaticality,
importance, and compression rate. Obviously, the former two are difficult to evaluate objectively. Previ-
ous works used human judgment, which entails a difficult and expensive process. In this paper, similar to
a common sequence labeling task, we simply used the F-score metric of removed words to roughly eval-
uate the performance of sentiment sentence compression. Of course, the final effectiveness of sentence
compression model can be reviewed by the derived T-P collocation extraction task.
For T-P collocation extraction, we applied the traditional P, R and F-score for the final evaluations.
Specially, a fuzzy matching evaluation is adopted for the T-P collocation extraction. That is to say,
given an extracted T-P collocation ?t, p?, whose standard result is ?t
s
, p
s
?, if t is the substring of t
s
, and
meanwhile p is the substring of p
s
, we consider the extracted ?t, p? is a correct T-P collocation.
4.2 Sentiment Sentence Compression Results
Features P(%) R(%) F(%)
Basic (01 ? 04) 76.4 57.4 65.5
+ feeling (05) 75.9 57.6 65.5
+ polarity (06) 76.6 57.6 65.7
+ suffix or prefix (07) 78.4 56.9 66.0
+ cluster (08) 74.9 58.9 65.9
+ dependency (09) 75.3 57.2 65.0
All (01 ? 08) 77.3 59.1 67.0
All - feeling (05) 77.1 58.9 66.8
Table 3: The results of sentiment sentence compression with different features.
Results of Sent Comp with different features are shown in Table 3. All results are reported using five-
fold cross validation. We can see that the performance is improved when we added feeling
7
and polarity
features (05 ? 06) respectively, indicating that the sentiment-related features are useful for sentiment
sentence compression. In addition, the latent semantic features (07 ? 08) are also helpful, especially the
suffix or prefix features, which show better performance than the four other kinds of features.
Nonetheless, the dependency features (09) have a negative on compression performance due to the
specificity of compression for sentiment sentences. That is because the lower dependency parsing per-
formance on sentiment sentences introduces many wrong dependency relations, which counteract the
7
In Table 3, although the performance of adding feeling is comparative to the basic system (Basic (01-04)), the system
without feeling (All - feeling (05), the last line) is worse than the system using all the features (All (01-08)). This can illustrate
the effectiveness of the feeling feature.
1365
Domain Method P(%) R(%) F(%)
no Comp 74.7 58.4 65.6
Camera manual Comp 83.4 62.7 71.6
auto Comp 80.4 62.1 70.1
no Comp 68.2 53.1 59.7
Car manual Comp 76.3 57.7 65.7
auto Comp 72.3 56.1 63.2
no Comp 74.1 56.8 64.3
Notebook manual Comp 82.7 64.5 72.5
auto Comp 79.7 62.8 70.2
no Comp 77.3 60.9 68.1
Phone manual Comp 82.7 65.7 73.2
auto Comp 80.3 63.3 70.8
no Comp 73.7 57.5 64.6
All manual Comp 81.2 62.5 70.6
auto Comp 78.1 60.9 68.4
Table 4: Results on T-P collocation extraction for four product domains.
contribution of the dependency relation features. This is also the reason why we need to compress sen-
timent sentences as the first step for T-P collocation extraction. Finally, when we combine all of useful
features (01 ? 08), the performance achieves the highest score.
It is worth noting that sentiment sentence compression is a new task proposed in this paper. For
simplicity, this paper aims to attempt a simple yet effective sentiment sentence compression model. We
will polish the Sent Comp model in the future work.
4.3 Sent Comp for T-P Collocation Extraction
We designed three comparative systems to demonstrate the effectiveness of Sent Comp for T-P collo-
cation extraction. Note that, Sent Comp is the first step to process the corpus before T-P collocation
extraction. The method for T-P collocation extraction was based on the state-of-the-art method proposed
by Qiu et al. (2011) as described in Section 2.
no Comp - This refers to the system that only uses the T-P collocation extraction method and does not
perform sentence compression as the first step.
manual Comp - This system manually compresses the corpus into a new one as the first step, and then
applies the T-P collocation extraction method on the new compressed corpus.
auto Comp - This system uses Sent Comp as the first step to automatically compress the corpus into a
new one, and then applies the T-P collocation extraction method on the new corpus.
From the descriptions above, we can draw a conclusion that the performance of manual Comp can be
considered as the upper bound for the sentiment sentence compression based T-P collocation extraction
task.
Table 4 shows the experimental results of the three systems on T-P collocation extraction for four prod-
uct domains. Here, manual Comp can significantly (p < 0.01) improved the F-score by approximately
6%,
8
compared with no Comp. This illustrates that the idea of sentiment sentence compression is use-
ful for T-P collocation extraction. Specifically, the proposed method can transform some over-natural
sentences into normal ones, further influencing their final syntactic parsers. Evidently, because the T-P
collocation extraction relies heavily on syntactic features, the more correct syntactic parse trees derived
from the compressed sentences can help to increase the performance of this task.
Compared with no Comp, the auto Comp system also yielded a significantly better results (p < 0.01)
that indicated an improvement of 3.8% in the F-score, despite the fact that the automatic sentence com-
pression model Sent Comp may wrongly compress some sentences. This demonstrates the usefulness
of sentiment sentence compression step in the T-P collocation extraction task and further proves the
effectiveness of our proposed model.
8
We use paired bootstrap resampling significance test (Efron and Tibshirani, 1993).
1366
Moreover, we can observe that the idea of sentence compression and our Sent Comp are useful for
all the four product domains on T-P collocation extraction task, indicating that Sent Comp is domain
adaptive. However, we can find a small gap between auto Comp and manual Comp, which indicates
that the Sent Comp model can still be improved further. In the future, we will explore more effective
sentence compression algorithms to bridge the gap between the two systems.
5 Related Works
5.1 Sentiment Analysis
T-P collocation extraction is a basic task in sentiment analysis. In order to solve this task, most methods
focused on identifying relationships between targets and polarity words. In early studies, researcher-
s recognized the target first, and then chose its polarity word within a window of size k (Hu and Liu,
2004). However, considering that this kind of method is too heuristic, the performance proved to be very
limited. To tackle this problem, many researchers found syntactic patterns that can better describe the
relationships between targets and polarity words. For example, Bloom et al. (2007) constructed a link-
age specification lexicon containing 31 patterns, while Qiu et al. (2011) proposed a double propagation
method that introduced eight heuristic syntactic patterns to extract the collocations. Xu et al. (2013) used
the syntactic patterns to extract the collocation candidates in their two-stage framework.
Based on the above, we can conclude that syntactic features are very important for T-P collocation
extraction. However, the ?naturalness? problem can still seriously affect the performance of syntactic
parser. Once our sentiment sentence compression method can improve the quality of parsing, the perfor-
mance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous
work using a sentence compression model to improve this task.
5.2 Sentence Compression
Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones,
while preserving the essential content (Jing, 2000). There are many applications that can benefit from
a robust compression system, such as summarization systems (Li et al., 2013), semantic role label-
ing (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on.
Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and
Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos,
2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the
syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus,
the compressed tree (after removing constituents from a bad parse) may not produce a good compressed
sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem
by using discriminative models.
Aside from above extractive sentence compression approaches, there is another research line, namely,
abstractive approach, which compresses an original sentence by reordering, substituting, and inserting,
as well as removing (Cohn and Lapata, 2013). This method needs more resource and is more complicat-
ed. Therefore, in this paper, we only focus on extractive approach.
At present, the current sentence compression methods all focus on formal sentences, and few meth-
ods are being proposed to study sentiment sentences. As discussed in the above sections, the current
compression models cannot be directly utilized to T-P collocation extraction owing to the specificity of
sentiment sentences. Therefore, a new compression model for sentiment sentences should be established.
6 Conclusion and Future Work
In this work, we presented a framework that adopted a CRF based sentiment sentence compression mod-
el Sent Comp, as a preprocessing step, to improve the T-P collocation extraction task. Different from
the existing sentence compression models used for formal sentences, Sent Comp incorporated some
sentiment-related features to retain the sentiment information. Experimental results showed that the sys-
tem with the sentence compression step performed better than that without this step, thus demonstrating
the effectiveness of the framework and the compression model Sent Comp.
1367
Generally, the idea of this framework maybe useful for many sentiment analysis tasks that rely heavily
on syntactic results. Thus in the future, we will try to apply the Sent Comp model for these tasks. Besides,
the simplicity and effectiveness of this framework motivates us to pursue the study further. For example,
we will polish the Sent Comp model by exploring more sentiment-related features and exploring other
types of compression models.
Acknowledgments
We thank the anonymous reviewers for their helpful comments. This work was supported by National
Natural Science Foundation of China (NSFC) via grant 61300113, 61133012 and 61273321, the Ministry
of Education Research of Social Sciences Youth funded projects via grant 12YJCZH304, the Fundamen-
tal Research Funds for the Central Universities via grant No.HIT.NSRIF.2013090 and IBM Research-
China Joint Research Project.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature
selection for opinion classification in web forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34, June.
Kenneth Bloom, Navendu Garg, and Shlomo Argamon. 2007. Extracting appraisal expressions. In HLT-NAACL
2007, pages 308?315.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Comput. Linguist., 18(4):467?479, December.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Coling 2010:
Demonstrations, pages 13?16, Beijing, China, August. Coling 2010 Organizing Committee.
Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with
bilingual constraints. In Proceedings of the 2013 Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language Technologies, pages 52?62, Atlanta, Georgia, June.
Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear program-
ming approach. J. Artif. Intell. Res. (JAIR), 31:399?429.
Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement,
20(1):37 ? 46.
Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
Trevor Cohn and Mirella Lapata. 2013. An abstractive approach to sentence compression. ACM Transactions on
Intelligent Systems and Technology, 4(3):1?35.
Adnan Duric and Fei Song. 2012. Feature selection for sentiment analysis based on content and syntax models.
Decis. Support Syst., 53(4):704?711, November.
B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pages 959?967, Columbus, Ohio, June. Association for
Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 885?893, Los Angeles, California, June. Association
for Computational Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In
Human Language Technologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference, pages 180?187, Rochester, New York, April.
Association for Computational Linguistics.
1368
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD-2004,
pages 168?177.
Hongyan Jing. 2000. Sentence reduction for automatic text summarization. In IN PROCEEDINGS OF THE 6TH
APPLIED NATURAL LANGUAGE PROCESSING CONFERENCE, pages 310?315.
Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to
sentence compression. Artif. Intell., 139(1):91?107, July.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus, Ohio, June. Association for Computational Linguistics.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, MIT.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Ryan McDonald. 2006. Discriminative sentence compression with soft syntactic evidence. In In Proc. EACL.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative
training. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,
pages 337?342, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Yusuke Miyao, and Jun?ichi Tsujii. 2010. Entity-focused sentence simplification
for relation extraction. In Proceedings of the 23rd International Conference on Computational Linguistics
(COLING 2010), pages 788?796.
Tadashi Nomoto. 2007. Discriminative sentence compression with conditional random fields. Information Pro-
cessing and Management, 43(6):1571?1587, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational Linguistics, 37(1):9?27.
Kapil Thadani and Kathleen McKeown. 2013. Sentence compression with joint structural inference. In Pro-
ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In
Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 290?
297, Stroudsburg, PA, USA. Association for Computational Linguistics.
David Vickrey and Daphne Koller. 2008. Sentence simplification for semantic role labeling. In ACL, pages
344?352. The Association for Computer Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar
and integer programming. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing, pages 409?420, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang. 2008. Overview of chinese pinion
analysis evaluation 2008. In The First Chinese Opinion Analysis Evaluation (COAE) 2008.
1369
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1180?1191,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Joint Models for Chinese POS Tagging and Dependency Parsing
Zhenghua Li?, Min Zhang?, Wanxiang Che?, Ting Liu?, Wenliang Chen? and Haizhou Li?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{lzh,car,tliu}@ir.hit.edu.cn
?Institute for Infocomm Research, Singapore
{mzhang,wechen,hli}@i2r.a-star.edu.sg
Abstract
Part-of-speech (POS) is an indispensable fea-
ture in dependency parsing. Current research
usually models POS tagging and dependency
parsing independently. This may suffer from
error propagation problem. Our experiments
show that parsing accuracy drops by about
6% when using automatic POS tags instead
of gold ones. To solve this issue, this pa-
per proposes a solution by jointly optimiz-
ing POS tagging and dependency parsing in a
unique model. We design several joint models
and their corresponding decoding algorithms
to incorporate different feature sets. We fur-
ther present an effective pruning strategy to re-
duce the search space of candidate POS tags,
leading to significant improvement of parsing
speed. Experimental results on Chinese Penn
Treebank 5 show that our joint models sig-
nificantly improve the state-of-the-art parsing
accuracy by about 1.5%. Detailed analysis
shows that the joint method is able to choose
such POS tags that are more helpful and dis-
criminative from parsing viewpoint. This is
the fundamental reason of parsing accuracy
improvement.
1 Introduction
In dependency parsing, features consisting of part-
of-speech (POS) tags are very effective, since pure
lexical features lead to severe data sparseness prob-
lem. Typically, POS tagging and dependency pars-
ing are modeled in a pipelined way. However, the
pipelined method is prone to error propagation, es-
pecially for Chinese. Due to the lack of morpholog-
ical features, Chinese POS tagging is even harder
than other languages such as English. The state-of-
the-art accuracy of Chinese POS tagging is about
93.5%, which is much lower than that of English
(about 97% (Collins, 2002)). Our experimental re-
sults show that parsing accuracy decreases by about
6% on Chinese when using automatic POS tagging
results instead of gold ones (see Table 3 in Section
5). Recent research on dependency parsing usually
overlooks this issue by simply adopting gold POS
tags for Chinese data (Duan et al, 2007; Zhang and
Clark, 2008b; Huang and Sagae, 2010). In this pa-
per, we address this issue by jointly optimizing POS
tagging and dependency parsing.
Joint modeling has been a popular and effec-
tive approach to simultaneously solve related tasks.
Recently, many successful joint models have been
proposed, such as joint tokenization and POS tag-
ging (Zhang and Clark, 2008a; Jiang et al, 2008;
Kruengkrai et al, 2009), joint lemmatization and
POS tagging (Toutanova and Cherry, 2009), joint
tokenization and parsing (Cohen and Smith, 2007;
Goldberg and Tsarfaty, 2008), joint named en-
tity recognition and parsing (Finkel and Manning,
2009), joint parsing and semantic role labeling
(SRL) (Li et al, 2010), joint word sense disambigua-
tion and SRL (Che and Liu, 2010), joint tokenization
and machine translation (MT) (Dyer, 2009; Xiao et
al., 2010) and joint parsing and MT (Liu and Liu,
2010). Note that the aforementioned ?parsing? all
refer to constituent parsing.
As far as we know, there are few successful mod-
els for jointly solving dependency parsing and other
tasks. Being facilitated by Conference on Com-
putational Natural Language Learning (CoNLL)
2008 and 2009 shared tasks, several joint models
of dependency parsing and SRL have been pro-
posed. Nevertheless, the top-ranked systems all
adopt pipelined approaches (Surdeanu et al, 2008;
1180
Hajic? et al, 2009). Theoretically, joint modeling
of POS tagging and dependency parsing should be
helpful to the two individual tasks. On the one hand,
syntactic information can help resolve some POS
ambiguities which are difficult to handle for the se-
quential POS tagging models. On the other hand,
more accurate POS tags should further improve de-
pendency parsing.
For joint POS tagging and dependency parsing,
the major issue is to design effective decoding algo-
rithms to capture rich features and efficiently search
out the optimal results from a huge hypothesis
space.1 In this paper, we propose several dynamic
programming (DP) based decoding algorithms for
our joint models by extending existing parsing algo-
rithms. We also present effective pruning techniques
to speed up our decoding algorithms. Experimen-
tal results on Chinese Penn Treebank show that our
joint models can significantly improve the state-of-
the-art parsing accuracy by about 1.5%.
The remainder of this paper is organized as fol-
lows. Section 2 describes the pipelined method, in-
cluding the POS tagging and parsing models. Sec-
tion 3 discusses the joint models and the decod-
ing algorithms, while Section 4 presents the pruning
techniques. Section 5 reports the experimental re-
sults and error analysis. We review previous work
closely related to our method in Section 6, and con-
clude this paper in Section 7.
2 The Baseline Pipelined Method
Given an input sentence x = w1...wn, we denote its
POS tag sequence by t = t1...tn, where ti ? T , 1 ?
i ? n, and T is the POS tag set. A dependency tree
is denoted by d = {(h,m) : 0 ? h ? n, 0 < m ?
n}, where (h,m) represents a dependency wh ?
wm whose head word (or father) is wh and modifier
(or child) is wm. w0 is an artificial root token which
is used to simplify the formalization of the problem.
The pipelined method treats POS tagging and de-
pendency parsing as two cascaded problems. First,
1It should be noted that it is straightforward to simultane-
ously do POS tagging and constituent parsing, as POS tags can
be regarded as non-terminals in the constituent structure (Levy
and Manning, 2003). In addition, Rush et al (2010) describes
an efficient and simple inference algorithm based on dual de-
composition and linear programming relaxation to combine a
lexicalized constituent parser and a trigram POS tagger.
an optimal POS tag sequence t? is determined.
t? = arg max
t
Scorepos(x, t)
Then, an optimal dependency tree d? is determined
based on x and t?.
d? = arg max
d
Scoresyn(x, t?,d)
2.1 POS Tagging
POS tagging is a typical sequence labeling prob-
lem. Many models have been successfully applied
to sequence labeling problems, such as maximum-
entropy (Ratnaparkhi, 1996), conditional random
fields (CRF) (Lafferty et al, 2001) and perceptron
(Collins, 2002). We use perceptron to build our POS
tagging baseline for two reasons. Firstly, as a linear
model, perceptron is simple, fast, and effective. It is
competitive to CRF in tagging accuracy but requires
much less training time (Shen et al, 2007). Sec-
ondly, perceptron has been successfully applied to
dependency parsing as well (Koo and Collins, 2010).
In this paper, perceptron is used in all models includ-
ing the POS tagging model, the dependency parsing
models and the joint models.
In a perceptron, the score of a tag sequence is
Scorepos(x, t) = wpos ? fpos(x, t)
where fpos(x, t) refers to the feature vector andwpos
is the corresponding weight vector.
For POS tagging features, we follow the work of
Zhang and Clark (2008a). Three feature sets are
considered: POS unigram, bigram and trigram fea-
tures. For brevity, we will refer to the three sets as
wi ti, ti?1 ti and ti?2 ti?1 ti.
Given wpos, we adopt the Viterbi algorithm to get
the optimal tagging sequence.
2.2 Dependency Parsing
Recently, graph-based dependency parsing has
gained more and more interest due to its state-of-
the-art accuracy. Graph-based dependency parsing
views the problem as finding the highest scoring tree
from a directed graph. Based on dynamic program-
ming decoding, it can efficiently find an optimal tree
in a huge search space. In a graph-based model, the
1181
score of a dependency tree is factored into scores of
small parts (subtrees).
Scoresyn(x, t,d) = wsyn ? fsyn(x, t,d)
=
?
p?d
Scoresyn(x, t, p)
where p is a scoring part which contains one or more
dependencies in the dependency tree d. Figure 1
shows different types of scoring parts used in current
graph-based models.
h m
dependency
h s
sibling
m g h
grandparent
m
h s
tri-sibling
mth s
grand-sibling
mg
Figure 1: Different types of scoring parts used in current
graph-based models (Koo and Collins, 2010).
Eisner (1996) proposes an O(n3) decoding al-
gorithm for dependency parsing. Based on the al-
gorithm, McDonald et al (2005) propose the first-
order model, in which the scoring parts only con-
tains dependencies. The second-order model of Mc-
Donald and Pereira (2006) incorporates sibling parts
and also needs O(n3) parsing time. The second-
order model of Carreras (2007) incorporates both
sibling and grandparent parts, and needs O(n4)
parsing time. However, the grandparent parts are
restricted to those composed of outermost grand-
children. Koo and Collins (2010) propose efficient
decoding algorithms of O(n4) for third-order mod-
els. In their paper, they implement two versions
of third-order models, Model 1 and Model 2 ac-
cording to their naming. Model 1 incorporates only
grand-sibling parts, while Model 2 incorporates both
grand-sibling and tri-sibling parts. Their experi-
ments on English and Czech show that Model 1 and
Model 2 obtain nearly the same parsing accuracy.
Therefore, we use Model 1 as our third-order model
in this paper.
We use three versions of graph-based dependency
parsing models.
? The first-order model (O1): the same with Mc-
Donald et al (2005).
? The second-order model (O2): the same with
Model 1 in Koo and Collins (2010), but without
using grand-sibling features.2
? The third-order model (O3): the same with
Model 1 in Koo and Collins (2010).
We adopt linear models to define the score of a de-
pendency tree. For the third-order model, the score
of a dependency tree is represented as:
Scoresyn(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s)(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
+
?
{(g,h),(h,s),(h,m)}?d
wgsib ? fgsib(x, t, g, h, s,m)
For the first- and second-order models, the above
formula is modified by deactivating extra parts.
For parsing features, we follow standard prac-
tice for graph-based dependency parsing (McDon-
ald, 2006; Carreras, 2007; Koo and Collins, 2010).
Since these features are highly related with our joint
decoding algorithms, we summarize the features as
follows.
? Dependency Features, fdep(x, t, h,m)
? Unigram Features: whth dir, wmtm dir
? Bigram Features: whth wmtm dir dist
? In Between Features: th tb tm dir dist
? Surrounding Features:
th?1 th th+1 tm?1 tm tm+1 dir dist
? Sibling Features, fsib(x, t, h, s,m)
wh th ws ts wm tm dir
? Grandparent Features, fgrd(x, t, g, h,m)
wg tg wh th wm tm dir gdir
? Grand-sibling Features, fgsib(x, t, g, h, s,m)
wg tg wh th ws ts wm tm dir gdir
2This second-order model incorporates grandparent features
composed of all grandchildren rather than just outermost ones,
and outperforms the one of Carreras (2007) according to the
results in Koo and Collins (2010).
1182
where b denotes an index between h and m; dir
and dist are the direction and distance of (h,m);
gdir is the direction of (g, h). We also use back-
off features by generalizing from very specific fea-
tures over word forms, POS tags, directions and dis-
tances to less sparse features over just POS tags or
considering fewer nodes. To avoid producing too
many sparse features, at most two word forms are
used at the same time in sibling, grandparent and
grand-sibling features, while POS tags are used in-
stead for other nodes; meanwhile, at most four POS
tags are considered at the same time for surrounding
features.
3 Joint Models
In the joint method, we aim to simultaneously solve
the two problems.
(t?, d?) = arg max
t,d
Scorejoint(x, t,d)
Under the linear model, the score of a tagged de-
pendency tree is:
Scorejoint(x, t,d) = Scorepos(x, t)
+ Scoresyn(x, t,d)
= wpos?syn ? fpos?syn(x, t,d)
where fpos?syn(.) means the concatenation of fpos(.)
and fsyn(.). Under the joint model, the weights of
POS and syntactic features, wpos?syn, are simulta-
neously learned. We expect that POS and syntactic
features can interact each other to determine an op-
timal joint result.
Similarly to the baseline dependency parsing
models, we define the first-, second-, and third-order
joint models according to the syntactic features con-
tained in fsyn(.).
In the following, we propose two versions of joint
models which can capture different feature sets and
have different complexity.
3.1 Joint Models of Version 1
The crucial problem for the joint method is to de-
sign effective decoding algorithms to capture rich
features and efficiently search out the optimal re-
sults from a huge hypothesis space. Eisner (2000)
describes a preliminary idea to handle polysemy by
extending parsing algorithms. Based on this idea,
we extend decoding algorithms of McDonald et al
(2005) and Koo and Collins (2010), and propose two
DP based decoding algorithms for our joint models
of version 1.
(b)
(a)
i r r j
r+1 ji ri j
i j
Figure 2: The DP structures and derivations of the first-
order decoding algorithm of joint models of version 1.
We omit symmetric right-headed versions for brevity.
Trapezoids denote incomplete spans. Triangles denote
complete spans. Solid circles denote POS tags of the cor-
responding indices.
The decoding algorithm of O1: As shown in
Figure 2, the first-order joint decoding algorithm
utilizes two types of dynamic programming struc-
tures. (1) Incomplete spans consist of a dependency
and the region between the head and modifier; (2)
Complete spans consist of a headword and its de-
scendants on one side. Each span is recursively cre-
ated by combining two smaller and adjacent spans
in a bottom-up fashion.
The pseudo codes are given in Algorithm 1.
I(i,j)(ti,tj) denotes an incomplete span from i to j
whose boundary POS tags are ti and tj . C(i,j)(ti,tj)
refers to a complete span from i to j whose bound-
ary POS tags are ti and tj . Conversely, I(j,i)(tj ,ti)
andC(j,i)(tj ,ti) represent spans of the other direction.
Note that in these notations the first argument index
always refers to the head of the span.
Line 6 corresponds to the derivation in Figure 2-
(a). Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)}) cap-
tures the joint features invented by this combina-
tion, where p = {(i, j)} means that the newly ob-
served scoring part is the dependency (i, j). The
syntactic features, denoted by fsyn(x, ti, tj , i, j), can
only incorporate syntactic unigram and bigram fea-
tures. The surrounding and in between features
are unavailable, because the context POS tags, such
as tb and ti?1, are not contained in the DP struc-
1183
Algorithm 1 The first-order joint decoding algorithm of version 1
1: ?0 ? i ? n, ti ? T C(i,i)(ti,ti) = 0 ? initialization
2: for w = 1..n do ? span width
3: for i = 0..(n? w) do ? span start index
4: j = i + w ? span end index
5: for (ti, tj) ? T 2 do
6: I(i,j)(ti,tj) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(i, j)})}7: I(j,i)(tj ,ti) = maxi?r<j max(tr,tr+1)?T 2{C(i,r)(ti,tr) + C(j,r+1)(tj ,tr+1) + Scorejoint(x, ti, tr, tr+1, tj , p = {(j, i)})}8: C(i,j)(ti,tj) = maxi<r?j maxtr?T {I(i,r)(ti,tr) + C(r,j)(tr,tj) + Scorejoint(x, ti, tr, tj , p = ?)}
9: C(j,i)(tj ,ti) = maxi?r<j maxtr?T {C(r,i)(tr,ti) + I(j,r)(tj ,tr) + Scorejoint(x, ti, tr, tj , p = ?)}
10: end for
11: end for
12: end for
tures. Therefore, we adopt pseudo surrounding
and in between features by simply fixing the con-
text POS tags as the single most likely ones (Mc-
Donald, 2006). Taking the in between features
as an example, we use ti t?b tj dir dist instead,
where t?b is the 1-best tag determined by the base-
line POS tagger. The POS features, denoted by
fpos(x, ti, tr, tr+1, tj), can only incorporate all POS
unigram and bigram features.3 Similarly, we use
pseudo POS trigram features such as t?r?1 tr tr+1.
Line 8 corresponds to the derivation in Figure 2-
(b). Since this combination invents no scoring part
(p = ?), Scorejoint(x, ti, tr, tj , p = ?) is only com-
posed of POS features.4
Line 7 and Line 9 create spans in the opposite di-
rection, which can be analogously illustrated. The
space and time complexity of the algorithm are re-
spectively O(n2q2) and O(n3q4), where q = |T |.5
The decoding algorithm of O2 & O3: Figure
3 illustrates the second- and third-order decoding
algorithm of joint models of version 1. A new
kind of span, named the sibling span, is used to
capture sibling structures. Furthermore, each span
is augmented with a grandparent-index to capture
both grandparent and grand-sibling structures. It is
straightforward to derive the pseudo codes of the al-
3? wr tr if i ?= r; ? wr+1 tr+1 if r + 1 ?= j; ? tr tr+1
if r ?= i or r + 1 ?= j; ? ti tr if r ? 1 = i; ? tr+1 tj if
r + 2 = j. Note that wi ti, wj tj and ti tj (if i = j ? 1) are
not incorporated here to avoid double counting.
4? wr tr if r ?= j;? ti tr if i = r?1;? tr tj if r+1 = j.
Pseudo trigram features can be added accordingly.
5We can reduce the time complexity to O(n3q3) by strictly
adopting the DP structures in the parsing algorithm of Eisner
(1996). However, that may make the algorithm harder to com-
prehend.
ig j g i i ji+1
(a)
g i j g i k i k j
(b)
i k j i ik jr+1r
(c)
g i j g i r i r j
(d)
Figure 3: The DP structures and derivations of the
second- and third-order joint decoding algorithm of ver-
sion 1. For brevity, we elide the right-headed and right-
grandparented versions. Rectangles represent sibling
spans.
i j i r r j
i j i r r+1 j
(b)
(a)
Figure 4: The DP structures and derivations of the first-
order joint decoding algorithm of version 2. We omit the
right-headed version for brevity.
1184
gorithm from Figure 3. We omit them due to space
limitation. Pseudo surrounding, in between and POS
trigram features are used due to the same reason as
above. The space and time complexity of the algo-
rithm are respectively O(n3q3) and O(n4q5).
3.2 Joint Models of Version 2
To further incorporate genuine syntactic surround-
ing and POS trigram features in the DP structures,
we extend the algorithms of joint models of version
1, and propose our joint models of version 2.
The decoding algorithm of O1: Figure 4 illus-
trates the first-order joint decoding algorithm of ver-
sion 2. Compared with the structures in Figure 2,
each span is augmented with the POS tags surround-
ing the boundary indices. These context POS tags
enable Scorejoint(.) in line 6-9 of Algorithm 1 to
capture the syntactic surrounding and POS trigram
features, but also require enumeration of POS tags
over more indices. For brevity, we skip the pseudo
codes which can be easily derived from Algorithm
1. The space and time complexity of the algorithm
are respectively O(n2q6) and O(n3q10).
The decoding algorithm of O2 & O3: Using the
same idea as above, the second- and third-order joint
decoding algorithms of version 2 can be derived
based on Figure 3. Again, we omit both its DP struc-
tures and pseudo codes for the sake of brevity. Its
space and time complexity are respectively O(n3q7)
and O(n4q11).
In between features, which should be regarded as
non-local features in the joint situation, still cannot
be incorporated in our joint models of version 2.
Again, we adopt the pseudo version.
3.3 Comparison
Based on the above illustration, we can see that joint
models of version 1 are more efficient with regard
to the number of POS tags for each word, but fail to
incorporate syntactic surrounding features and POS
trigram features in the DP structures. On the con-
trary, joint models of version 2 can incorporate both
aforementioned feature sets, but have higher com-
plexity. These two versions of models will be thor-
oughly compared in the experiments.
4 Pruning Techniques
In this section, we introduce two pruning strategies
to constrain the search space of our models due to
their high complexity.
4.1 POS Tag Pruning
The time complexity of the joint decoding algorithm
is unbearably high with regard to the number of can-
didate POS tags for each word (q = |T |). We
find that it would be extremely time-consuming even
when we only use two most likely POS tags for each
word (q = 2) even for joint models of version 1.
To deal with this problem, we propose a pruning
method that can effectively reduce the POS tag space
based on a probabilistic tagging model.
We adopt a conditional log-linear model (Lafferty
et al, 2001), which defines a conditional distribution
of a POS tag sequence t given x:
P (t|x) = e
wpos?fpos(x,t)
?
t ewpos?fpos(x,t)
We use the same feature set fpos defined in Sec-
tion 2.1, and adopt the exponentiated gradient algo-
rithm to learn the weight vector wpos (Collins et al,
2008).
The marginal probability of tagging a word wi as
t is
P (ti = t|x) =
?
t:t[i]?t
P (t|x)
which can be efficiently computed using the
forward-backward algorithm.
We define pmaxi(x) to be the highest marginal
probability of tagging the word wi:
pmaxi(x) = maxt?T P (ti = t|x)
We then define the allowable candidate POS tags
of the word wi to be
Ti(x) = {t : t ? T , P (ti = t|x) ? ?t?pmaxi(x)}
where ?t is the pruning threshold. Ti(x) is used to
constrain the POS search space by replacing T in
Algorithm 1.
1185
4.2 Dependency Pruning
The parsing time grows quickly for the second- and
third-order models (both baseline and joint) when
the input sentence gets longer (O(n4)). Follow-
ing Koo and Collins (2010), we eliminate unlikely
dependencies using a form of coarse-to-fine prun-
ing (Charniak and Johnson, 2005; Petrov and Klein,
2007). On the development set, 68.87% of the de-
pendencies are pruned, while the oracle dependency
accuracy is 99.77%. We use 10-fold cross validation
to do pruning on the training set.
5 Experiments
We use the Penn Chinese Treebank 5.1 (CTB5) (Xue
et al, 2005). Following the setup of Duan et al
(2007), Zhang and Clark (2008b) and Huang and
Sagae (2010), we split CTB5 into training (secs 001-
815 and 1001-1136), development (secs 886-931
and 1148-1151), and test (secs 816-885 and 1137-
1147) sets. We use the head-finding rules of Zhang
and Clark (2008b) to turn the bracketed sentences
into dependency structures.
We use the standard tagging accuracy to evalu-
ate POS tagging. For dependency parsing, we use
word accuracy (also known as dependency accu-
racy), root accuracy and complete match rate (all
excluding punctuation) .
For the averaged training, we train each model for
15 iterations and select the parameters that perform
best on the development set.
5.1 Results of POS Tag Pruning
Figure 5 shows the distribution of words with dif-
ferent number of candidate POS tags and the k-best
oracle tagging accuracy under different ?t. To avoid
dealing with words that have many candidate POS
tags, we further apply a hard criterion that the decod-
ing algorithms only consider top k candidate POS
tags.
To find the best ?t, we train and evaluate the
second-order joint model of version 1 on the train-
ing and development sets pruned with different ?t
(top k = 5). We adopt the second-order joint model
of version 1 because of its efficiency compared with
the third-order models and its capability of captur-
ing rich features compared with the first-order mod-
els. The results are shown in Table 1. The model
0
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 >5
pro
por
tion
 of 
wor
ds (
%)
number of candidate POS tags 
0.1
0.01
0.001
93
94
95
96
97
98
99
100
1 2 3 4 5 ?
k-b
est 
ora
cle 
tagg
ing
 acc
ura
cy (
%)
k
0.1
0.01
0.001
Figure 5: Results of POS tag pruning with different prun-
ing threshold ?t on the development set.
?t word root compl. acc. speed
0.1 81.53 76.88 30.00 94.17 2.5
0.01 81.83 76.62 30.62 93.16 1.2
0.001 81.73 77.38 30.50 93.41 0.5
Table 1: Performance of the second-order joint model of
version 1 with different pruning threshold ?t (top k = 5)
on the development set. ?Acc.? means the tagging accu-
racy. ?Speed? refers to the parsing speed (the number of
sentences processed per second).
with ?t = 0.1 obtains the highest tagging accuracy,
which is much higher than that of both ?t = 0.01
and ?t = 0.001. However, its parsing accuracy
is inferior to the other two. ?t = 0.01 produces
slightly better parsing accuracy than ?t = 0.001,
and is twice faster. Finally, we choose ?t = 0.01
due to the efficiency factor and our priority over the
parsing accuracy.
Then we do experiments to find an optimal top
k. Table 2 shows the results. We decide to choose
k = 3 since it leads to best parsing accuracy.
From Table 1 and 2, we can have an interesting
finding: it seems that the harder we filter the POS
tag space, the higher tagging accuracy we get. In
other words, giving the joint model less flexibility
of choosing POS tags leads to better tagging per-
formance.
Due to time limitation, we do not tune ?t and k for
other joint models. Instead, we simply adopt ?t =
0.01 and top k = 3.
5.2 Final Results
Table 3 shows the final results on the test set. We list
a few state-of-the-art results in the bottom. Duan07
refers to the results of Duan et al (2007). They
enhance the transition-based parsing model with
1186
Syntactic Metrics Tagging Accuracy Parsing Speed
word root compl. all-word known unknown Sent/Sec
Joint Models V2
O3 80.79 75.84 29.11 92.80 93.88 76.80 0.3
O2 80.49 75.49 28.24 92.68 93.77 76.27 0.6
O1 77.37 68.64 23.09 92.96 94.05 76.64 2.0
Joint Models V1
O3 80.69 75.90 29.06 92.89 93.96 76.80 0.5
O2 80.74 75.80 28.24 93.08 94.11 77.53 1.7
O1 77.38 69.69 22.62 93.20 94.23 77.76 8.5
Auto POS
O3 79.29 74.65 27.24
93.51 94.36 80.78
2.0
O2 79.03 74.70 27.19 5.8
O1 75.68 68.06 21.10 17.4
MSTParser2 77.95 72.04 25.50 4.1
MSTParser1 75.84 68.55 21.36 5.2
MaltParser 75.24 65.92 23.19 2.6
Gold POS
O3 86.00 77.59 34.02
100.0 100.0 100.0
-
O2 86.18 78.58 34.07 -
O1 82.24 70.10 26.02 -
MSTParser2 85.24 77.41 33.19 -
MSTParser1 83.04 71.49 27.59 -
MaltParser 82.62 69.34 29.06 -
H&S10 85.20 78.32 33.72 -
Z&C08 single 84.33 76.73 32.79 -
Z&C08 hybrid 85.77 76.26 34.41 -
Duan07 83.88 73.70 32.70 -
Table 3: Final results on the test set. ?Gold POS? means that gold POS tags are used as input by the pipelined parsing
models; while ?Auto POS? means that the POS tags are generated by the baseline POS tagging model.
top k word root compl. acc. speed
2 81.46 76.12 30.50 93.51 2.7
3 82.11 76.75 29.75 93.31 1.7
4 81.75 76.62 30.38 93.25 1.4
5 81.83 76.62 30.62 93.16 1.2
Table 2: Performance of the second-order joint model of
version 1 with different top k (?t = 0.01) on the devel-
opment set.
the beam search. H&S10 refers to the results of
Huang and Sagae (2010). They greatly expand the
search space of the transition-based model by merg-
ing equivalent states with dynamic programming.
Z&C08 refers to the results of Zhang and Clark
(2008b). They use a hybrid model to combine the
advantages of both graph-based and transition-based
models. We also do experiments with two publicly
available and widely-used parsers, MSTParser6 and
MaltParser7. MSTParser1 refers to the first-order
6http://sourceforge.net/projects/mstparser/
7http://maltparser.org/
graph-based model of McDonald et al (2005), while
MSTParser2 is the second-order model of McDon-
ald and Pereira (2006). MaltParser is a transition-
based parsing system. It integrates a number of clas-
sification algorithms and transition strategies. We
adopt the support vector machine classifier and the
arc-standard strategy (Nivre, 2008).
We can see that when using gold tags, our
pipelined second- and third-order parsing models
achieve best parsing accuracy, which is even higher
than the hybrid model of Zhang and Clark (2008b).
It is a little surprising that the second-order model
slightly outperforms the third-order one. This may
be possible, since Koo and Collins (2010) shows that
the third-order model outperforms the second-order
one by only 0.32% on English and 0.07% on Czech.
In addition, we only use basic third-order features.
Both joint models of version 1 and 2 can consis-
tently and significantly improve the parsing accu-
racy by about 1.5% for all first-, second- and third-
order cases. Accidentally, the parsing accuracy of
the second-order joint model of version 2 is lower
1187
error pattern # ? error pattern # ?
DEC ? DEG 237 114 NR ? NN 184 100
NN ? VV 389 73 NN ? NR 106 91
DEG ? DEC 170 39 NN ? JJ 95 70
VV ? NN 453 27 VA ? VV 29 41
P ? VV 52 24 JJ ? NN 126 29
P ? CC 39 13 VV ? VA 67 10
Table 4: Error analysis of POS tagging. # means the
error number of the corresponding pattern made by the
baseline tagging model. ? and ? mean the error number
reduced or increased by the joint model.
than that of its counterparts by about 0.3%. More
experiments and further analysis may be needed to
find out the reason. The two versions of joint models
performs nearly the same, which indicates that using
pseudo surrounding and POS trigram features may
be sufficient for the joint method on this data set.
In summary, we can conclude that the joint frame-
work is certainly helpful for dependency parsing.
It is clearly shown in Table 3 that the joint
method surprisingly hurts the tagging accuracy,
which diverges from our discussion in Section 1.
Some insights into this issue will be given in Sec-
tion 5.3. Moreover, it seems that the more syntac-
tic features the joint method incorporates (from
O1 to O3), the more the tagging accuracy drops.
We suspect that this is because the joint models are
dominated by the syntactic features. Take the first-
order joint model as an example. The dimension of
the syntactic features fsyn is about 3.5 million, while
that of fpos is only about 0.5 million. The gap be-
comes much larger for the second- and third-order
cases.
Comparing the parsing speed, we can find that the
pruning of POS tags is very effective. The second-
order joint model of version 1 can parse 1.7 sen-
tences per second, while the pipelined second-order
parsing model can parse 5.8 sentences per second,
which is rather close considering that there is a fac-
tor of q5.
5.3 Error Analysis
To find out the impact of our joint models on the
individual tasks, we conduct detailed error analy-
sis through comparing the results of the pipelined
second-order parsing model and the second-order
joint model of version 1.
Impact on POS tagging: Table 4 shows how the
joint model changes the quantity of POS tagging er-
ror patterns compared with the pipelined model. An
error pattern ?X ? Y? means that the focus word,
whose true tag is ?X?, is assigned a tag ?Y?. We
choose these patterns with largest reduction or in-
crease in the error number, and rank them in de-
scending order of the variation.
From the left part of Table 4, we can see that
the joint method is clearly better at resolving tag-
ging ambiguities like {VV, NN} and {DEG, DEC}.8
One common characteristic of these ambiguous
pairs is that the local or even whole syntactic struc-
ture will be destructed if the wrong tag is chosen. In
other words, resolving these ambiguities is critical
and helpful from the parsing viewpoint. From an-
other perspective, the joint model is capable of pre-
ferring the right tag with the help of syntactic struc-
tures, which is impossible for the baseline sequential
labeling model.
In contrast, pairs like {NN, NR}, {VV, VA} and
{NN, JJ} only slightly influence the syntactic struc-
ture when mis-tagged. The joint method performs
worse on these ambiguous pairs, as shown in the
right part of Table 4.
Impact on parsing: Table 5 studies the change of
parsing error rates between the pipelined and joint
model on different POS tag patterns. We present the
most typical and prominent patterns in the table, and
rank them in descending order of X?s frequency of
occurrence. We also show the change of proportion
of different patterns, which is consistent with the re-
sults in Table 4.
From the table, we can see the joint model can
achieve a large error reduction (0.8?4.0%) for all
the patterns ?X ? X?. In other words, the joint
model can do better given the correct tags than
the pipelined method.
For all the patterns marked by ?, except for the
ambiguous pair {NN, JJ} (which we find is difficult
to explain even after careful result analysis), the joint
model also reduces the error rates (2.2?15.4%). As
8DEG and DEC are the two POS tags for the frequently used
auxiliary word ??? (de?, of) in Chinese. The associative ???
is tagged as DEG, such as ???/father ? ??/eyes (eyes of
the father)?; while the one in a relative clause is tagged as DEC,
such as ??/he ??/made ? ??/progress (progress that he
made)?.
1188
pattern pipelined jointprop (%) error (%) prop (%) error (%)
NN ? NN 94.6 16.8 -1.1 -1.8
? VV ? 2.9 55.5 -0.5 +15.1
? NR ? 0.8 24.5 +0.7 -2.2
? JJ ? 0.7 17.9 +0.5 +2.1
VV ? VV 89.6 34.2 -0.3 ?4.0
? NN ? 6.6 66.4 -0.4 +0.7
? VA ? 1.0 38.8 +0.1 -15.4
NR ? NR 91.7 15.4 -3.7 -0.8
? NN ? 5.9 21.7 +3.2 -3.7
P ? P 92.8 22.6 +3.4 -3.2
? VV ? 3.0 50.0 -1.4 +10.7
? CC ? 2.3 74.4 -0.7 +21.9
JJ ? JJ 80.5 11.2 -2.8 -2.0
? NN ? 9.8 18.3 +2.2 +1.8
DEG ? DEG 86.5 11.1 +2.8 -3.6
? DEC ? 13.5 61.8 -3.1 +37.4
DEC ? DEC 79.7 17.2 +12.1 -4.0
? DEG ? 20.2 56.5 -9.7 +40.2
Table 5: Comparison of parsing error rates on different
POS tag patterns between the pipelined and joint models.
Given a pattern ?X ? Y?, ?prop? means its proportion in
all occurrence of ?X? (Count(X?Y )Count(X) ), and ?error? refers
to its parsing error rate ( Count(wrongly headed X?Y )Count(X?Y ) ).
The last two columns give the absolute reduction (-) or
increase (+) in proportion and error rate made by the joint
model. ? marks the patterns appearing in the left part of
Table 4, while ? marks those in the right part of Table 4.
discussed earlier, these patterns concern ambiguous
tag pairs which usually play similar roles in syn-
tactic structures. This demonstrates that the joint
model can do better on certain tagging error pat-
terns.
For patterns marked by ?, the error rate of the
joint model usually increases by large margin. How-
ever, the proportion of these patterns is substantially
decreased, since the joint model can better resolve
these ambiguities with the help of syntactic knowl-
edge.
In summary, we can conclude that the joint model
is able to choose such POS tags that are more helpful
and discriminative from parsing viewpoint. This is
the fundamental reason of the parsing performance
improvement.
6 Related Work
Theoretically, Eisner (2000) proposes a preliminary
idea of extending the decoding algorithm for de-
pendency parsing to handle polysemy. Here, word
senses can be understood as POS-tagged words.
Koo and Collins (2010) also briefly discuss that their
third-order decoding algorithm can be modified to
handle word senses using the idea of Eisner (2000).
In his PhD thesis, McDonald (2006) extends his
second-order model with the idea of Eisner (2000)
to study the impact of POS tagging errors on pars-
ing accuracy. To make inference tractable, he uses
top 2 candidate POS tags for each word based on
a maximum entropy tagger, and adopts the single
most likely POS tags for the surrounding and in be-
tween features. He conducts primitive experiments
on English Penn Treebank, and shows that parsing
accuracy can be improved from 91.5% to 91.9%.
However, he finds that the model is unbearably time-
consuming.
7 Conclusions
In this paper, we have systematically investigated
the issue of joint POS tagging and dependency pars-
ing. We propose and compare several joint models
and their corresponding decoding algorithms which
can incorporate different feature sets. We also pro-
pose an effective POS tag pruning method which can
greatly improve the decoding efficiency. The experi-
mental results show that our joint models can signif-
icantly improve the state-of-the-art parsing accuracy
by more than 1.5%. Detailed error analysis shows
that the fundamental reason for the parsing accu-
racy improvement is that the joint method is able to
choose POS tags that are helpful and discriminative
from parsing viewpoint.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 60803093, 60975055, the Natural Scientific
Research Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069) and the Fun-
damental Research Funds for the Central Universi-
ties (HIT.KLOF.2010064).
References
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
1189
EMNLP/CoNLL, pages 141?150.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 161?169.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of EMNLP-CoNLL 2007, pages 208?217.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter Bartlett. 2008. Exponentiated
gradient algorithms for conditional random fields and
max-margin markov networks. JMLR, 9:1775?1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Xiangyu Duan, Jun Zhao, , and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 406?
414.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING 1996, pages 340?345.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
326?334.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, Ohio, June. Association
for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1077?1086,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897?904.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 513?521.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse chinese, or the chinese treebank? In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 439?446,
Sapporo, Japan, July. Association for Computational
Linguistics.
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
Joint syntactic and semantic parsing of chinese. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 1108?
1117.
Yang Liu and Qun Liu. 2010. Joint parsing and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 707?715.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. In Computational Lin-
guistics, volume 34, pages 513?553.
1190
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
760?767, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL-2008.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 486?494.
Xinyan Xiao, Yang Liu, YoungSook Hwang, Qun Liu,
and Shouxun Lin. 2010. Joint tokenization and trans-
lation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (Coling 2010),
pages 1200?1208.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
1191
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110?120,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Revisiting Embedding Features for Simple Semi-supervised Learning
Jiang Guo
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{jguo, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Recent work has shown success in us-
ing continuous word embeddings learned
from unlabeled data as features to improve
supervised NLP systems, which is re-
garded as a simple semi-supervised learn-
ing mechanism. However, fundamen-
tal problems on effectively incorporating
the word embedding features within the
framework of linear models remain. In
this study, we investigate and analyze three
different approaches, including a new pro-
posed distributional prototype approach,
for utilizing the embedding features. The
presented approaches can be integrated
into most of the classical linear models in
NLP. Experiments on the task of named
entity recognition show that each of the
proposed approaches can better utilize the
word embedding features, among which
the distributional prototype approach per-
forms the best. Moreover, the combination
of the approaches provides additive im-
provements, outperforming the dense and
continuous embedding features by nearly
2 points of F1 score.
1 Introduction
Learning generalized representation of words is
an effective way of handling data sparsity caused
by high-dimensional lexical features in NLP sys-
tems, such as named entity recognition (NER)
and dependency parsing. As a typical low-
dimensional and generalized word representa-
tion, Brown clustering of words has been stud-
ied for a long time. For example, Liang (2005)
and Koo et al. (2008) used the Brown cluster
features for semi-supervised learning of various
NLP tasks and achieved significant improvements.
?
Email correspondence.
Recent research has focused on a special fam-
ily of word representations, named ?word embed-
dings?. Word embeddings are conventionally de-
fined as dense, continuous, and low-dimensional
vector representations of words. Word embed-
dings can be learned from large-scale unlabeled
texts through context-predicting models (e.g., neu-
ral network language models) or spectral methods
(e.g., canonical correlation analysis) in an unsu-
pervised setting.
Compared with the so-called one-hot represen-
tation where each word is represented as a sparse
vector of the same size of the vocabulary and only
one dimension is on, word embedding preserves
rich linguistic regularities of words with each di-
mension hopefully representing a latent feature.
Similar words are expected to be distributed close
to one another in the embedding space. Conse-
quently, word embeddings can be beneficial for
a variety of NLP applications in different ways,
among which the most simple and general way is
to be fed as features to enhance existing supervised
NLP systems.
Previous work has demonstrated effectiveness
of the continuous word embedding features in sev-
eral tasks such as chunking and NER using gener-
alized linear models (Turian et al., 2010).
1
How-
ever, there still remain two fundamental problems
that should be addressed:
? Are the continuous embedding features fit for
the generalized linear models that are most
widely adopted in NLP?
? How can the generalized linear models better
utilize the embedding features?
According to the results provided by Turian et
1
Generalized linear models refer to the models that de-
scribe the data as a combination of linear basis functions,
either directly in the input variables space or through some
transformation of the probability distributions (e.g., log-
linear models).
110
al. (2010), the embedding features brought signif-
icantly less improvement than Brown clustering
features. This result is actually not reasonable be-
cause the expressing power of word embeddings
is theoretically stronger than clustering-based rep-
resentations which can be regarded as a kind of
one-hot representation but over a low-dimensional
vocabulary (Bengio et al., 2013).
Wang and Manning (2013) showed that linear
architectures perform better in high-dimensional
discrete feature space than non-linear ones,
whereas non-linear architectures are more effec-
tive in low-dimensional and continuous feature
space. Hence, the previous method that directly
uses the continuous word embeddings as features
in linear models (CRF) is inappropriate. Word
embeddings may be better utilized in the linear
modeling framework by smartly transforming the
embeddings to some relatively higher dimensional
and discrete representations.
Driven by this motivation, we present three
different approaches: binarization (Section 3.2),
clustering (Section 3.3) and a new proposed distri-
butional prototype method (Section 3.4) for better
incorporating the embeddings features. In the bi-
narization approach, we directly binarize the con-
tinuous word embeddings by dimension. In the
clustering approach, we cluster words based on
their embeddings and use the resulting word clus-
ter features instead. In the distributional prototype
approach, we derive task-specific features from
word embeddings by utilizing a set of automati-
cally extracted prototypes for each target label.
We carefully compare and analyze these ap-
proaches in the task of NER. Experimental results
are promising. With each of the three approaches,
we achieve higher performance than directly using
the continuous embedding features, among which
the distributional prototype approach performs the
best. Furthermore, by putting the most effective
two of these features together, we finally outper-
form the continuous embedding features by nearly
2 points of F1 Score (86.21% vs. 88.11%).
The major contribution of this paper is twofold.
(1) We investigate various approaches that can bet-
ter utilize word embeddings for semi-supervised
learning. (2) We propose a novel distributional
prototype approach that shows the great potential
of word embedding features. All the presented ap-
proaches can be easily integrated into most of the
classical linear NLP models.
2 Semi-supervised Learning with Word
Embeddings
Statistical modeling has achieved great success
in most NLP tasks. However, there still remain
some major unsolved problems and challenges,
among which the most widely concerned is the
data sparsity problem. Data sparsity in NLP is
mainly caused by two factors, namely, the lack
of labeled training data and the Zipf distribution
of words. On the one hand, large-scale labeled
training data are typically difficult to obtain, espe-
cially for structure prediction tasks, such as syn-
tactic parsing. Therefore, the supervised mod-
els can only see limited examples and thus make
biased estimation. On the other hand, the nat-
ural language words are Zipf distributed, which
means that most of the words appear a few times
or are completely absent in our texts. For these
low-frequency words, the corresponding parame-
ters usually cannot be fully trained.
More foundationally, the reason for the above
factors lies in the high-dimensional and sparse lex-
ical feature representation, which completely ig-
nores the similarity between features, especially
word features. To overcome this weakness, an ef-
fective way is to learn more generalized represen-
tations of words by exploiting the numerous un-
labeled data, in a semi-supervised manner. After
which, the generalized word representations can
be used as extra features to facilitate the super-
vised systems.
Liang (2005) learned Brown clusters of
words (Brown et al., 1992) from unlabeled data
and use them as features to promote the supervised
NER and Chinese word segmentation. Brown
clusters of words can be seen as a generalized
word representation distributed in a discrete and
low-dimensional vocabulary space. Contextually
similar words are grouped in the same cluster. The
Brown clustering of words was also adopted in de-
pendency parsing (Koo et al., 2008) and POS tag-
ging for online conversational text (Owoputi et al.,
2013), demonstrating significant improvements.
Recently, another kind of word representation
named ?word embeddings? has been widely stud-
ied (Bengio et al., 2003; Mnih and Hinton, 2008).
Using word embeddings, we can evaluate the sim-
ilarity of two words straightforward by comput-
ing the dot-product of two numerical vectors in the
Hilbert space. Two similar words are expected to
111
be distributed close to each other.
2
Word embeddings can be useful as input to an
NLP model (mostly non-linear) or as additional
features to enhance existing systems. Collobert
et al. (2011) used word embeddings as input to a
deep neural network for multi-task learning. De-
spite of the effectiveness, such non-linear models
are hard to build and optimize. Besides, these ar-
chitectures are often specialized for a certain task
and not scalable to general tasks. A simple and
more general way is to feed word embeddings as
augmented features to an existing supervised sys-
tem, which is similar to the semi-supervised learn-
ing with Brown clusters.
As discussed in Section 1, Turian et al. (2010)
is the pioneering work on using word embedding
features for semi-supervised learning. However,
their approach cannot fully exploit the potential
of word embeddings. We revisit this problem
in this study and investigate three different ap-
proaches for better utilizing word embeddings in
semi-supervised learning.
3 Approaches for Utilizing Embedding
Features
3.1 Word Embedding Training
In this paper, we will consider a context-
predicting model, more specifically, the Skip-gram
model (Mikolov et al., 2013a; Mikolov et al.,
2013b) for learning word embeddings, since it is
much more efficient as well as memory-saving
than other approaches.
Let?s denote the embedding matrix to be learned
by C
d?N
, where N is the vocabulary size and d is
the dimension of word embeddings. Each column
of C represents the embedding of a word. The
Skip-gram model takes the current word w as in-
put, and predicts the probability distribution of its
context words within a fixed window size. Con-
cretely, w is first mapped to its embedding v
w
by
selecting the corresponding column vector of C
(or multiplying C with the one-hot vector of w).
The probability of its context word c is then com-
puted using a log-linear function:
P (c|w; ?) =
exp(v
>
c
v
w
)
?
c
?
?V
exp(v
c
?
>
v
w
)
(1)
where V is the vocabulary. The parameters ? are
v
w
i
, v
c
i
for w, c ? V and i = 1, ..., d. Then, the
2
The term similar should be viewed depending on the spe-
cific task.
log-likelihood over the entire training dataset D
can be computed as:
J(?) =
?
(w,c)?D
log p(c|w; ?) (2)
The model can be trained by maximizing J(?).
Here, we suppose that the word embeddings
have already been trained from large-scale unla-
beled texts. We will introduce various approaches
for utilizing the word embeddings as features for
semi-supervised learning. The main idea, as in-
troduced in Section 1, is to transform the continu-
ous word embeddings to some relatively higher di-
mensional and discrete representations. The direct
use of continuous embeddings as features (Turian
et al., 2010) will serve as our baseline setting.
3.2 Binarization of Embeddings
One fairly natural approach for converting the
continuous-valued word embeddings to discrete
values is binarization by dimension.
Formally, we aim to convert the continuous-
valued embedding matrixC
d?N
, to another matrix
M
d?N
which is discrete-valued. There are various
conversion functions. Here, we consider a sim-
ple one. For the i
th
dimension of the word em-
beddings, we divide the corresponding row vector
C
i
into two halves for positive (C
i+
) and nega-
tive (C
i?
), respectively. The conversion function
is then defined as follows:
M
ij
= ?(C
ij
) =
?
?
?
?
?
U
+
, if C
ij
? mean(C
i+
)
B
?
, if C
ij
? mean(C
i?
)
0, otherwise
where mean(v) is the mean value of vector v, U
+
is a string feature which turns on when the value
(C
ij
) falls into the upper part of the positive list.
Similarly, B
?
refers to the bottom part of the neg-
ative list. The insight behind ? is that we only con-
sider the features with strong opinions (i.e., posi-
tive or negative) on each dimension and omit the
values close to zero.
3.3 Clustering of Embeddings
Yu et al. (2013) introduced clustering embeddings
to overcome the disadvantage that word embed-
dings are not suitable for linear models. They sug-
gested that the high-dimensional cluster features
make samples from different classes better sepa-
rated by linear models.
112
In this study, we again investigate this ap-
proach. Concretely, each word is treated as a sin-
gle sample. The batch k-means clustering algo-
rithm (Sculley, 2010) is used,
3
and each cluster
is represented as the mean of the embeddings of
words assigned to it. Similarities between words
and clusters are measured by Euclidean distance.
Moreover, different number of clusters n con-
tain information of different granularities. There-
fore, we combine the cluster features of different
ns to better utilize the embeddings.
3.4 Distributional Prototype Features
We propose a novel kind of embedding features,
named distributional prototype features for su-
pervised models. This is mainly inspired by
prototype-driven learning (Haghighi and Klein,
2006) which was originally introduced as a pri-
marily unsupervised approach for sequence mod-
eling. In prototype-driven learning, a few pro-
totypical examples are specified for each target
label, which can be treated as an injection of
prior knowledge. This sparse prototype informa-
tion is then propagated across an unlabeled corpus
through distributional similarities.
The basic motivation of the distributional pro-
totype features is that similar words are supposed
to be tagged with the same label. This hypothesis
makes great sense in tasks such as NER and POS
tagging. For example, suppose Michael is a pro-
totype of the named entity (NE) type PER. Using
the distributional similarity, we could link similar
words to the same prototypes, so the word David
can be linked to Michael because the two words
have high similarity (exceeds a threshold). Using
this link feature, the model will push David closer
to PER.
To derive the distributional prototype features,
first, we need to construct a few canonical exam-
ples (prototypes) for each target annotation label.
We use the normalized pointwise mutual informa-
tion (NPMI) (Bouma, 2009) between the label and
word, which is a smoothing version of the standard
PMI, to decide the prototypes of each label. Given
the annotated training corpus, the NPMI between
a label and word is computed as follows:
?
n
(label, word) =
?(label, word)
? ln p(label, word)
(3)
3
code.google.com/p/sofia-ml
NE Type Prototypes
B-PER Mark, Michael, David, Paul
I-PER Akram, Ahmed, Khan, Younis
B-ORG Reuters, U.N., Ajax, PSV
I-ORG Newsroom, Inc, Corp, Party
B-LOC U.S., Germany, Britain, Australia
I-LOC States, Republic, Africa, Lanka
B-MISC Russian, German, French, British
I-MISC Cup, Open, League, OPEN
O ., ,, the, to
Table 1: Prototypes extracted from the CoNLL-
2003 NER training data using NPMI.
where,
?(label, word) = ln
p(label, word)
p(label)p(word)
(4)
is the standard PMI.
For each target label l (e.g., PER, ORG, LOC),
we compute the NPMI of l and all words in the
vocabulary, and the top m words are chosen as the
prototypes of l. We should note that the proto-
types are extracted fully automatically, without in-
troducing additional human prior knowledge.
Table 1 shows the top four prototypes extracted
from the NER training corpus of CoNLL-2003
shared task (Tjong Kim Sang and De Meul-
der, 2003), which contains four NE types, namely,
PER, ORG, LOC, and MISC. Non-NEs are denoted
by O. We convert the original annotation to the
standard BIO-style. Thus, the final corpus con-
tains nine labels in total.
Next, we introduce the prototypes as features to
our supervised model. We denote the set of pro-
totypes for all target labels by S
p
. For each proto-
type z ? S
p
, we add a predicate proto = z, which
becomes active at each w if the distributional sim-
ilarity between z and w (DistSim(z, w)) is above
some threshold. DistSim(z, w) can be efficiently
calculated through the cosine similarity of the em-
beddings of z and w. Figure 1 gives an illustra-
tion of the distributional prototype features. Un-
like previous embedding features or Brown clus-
ters, the distributional prototype features are task-
specific because the prototypes of each label are
extracted from the training data.
Moreover, each prototype word is also its own
prototype (since a word has maximum similarity
to itself). Thus, if the prototype is closely related
to a label, all the words that are distributionally
113
i -1x ix1?iy iyO B-LOC
in
/IN
Hague
/NNP
O B-LOC1( , )? ? ?i if y y
( , )
 word = Hague
    pos = NNP
 proto = Britain  B-LOC
 proto = England 
 ...
?
? ?
? ?
? ?? ?
?? ?
? ?
? ?
? ?? ?
i if x y
Figure 1: An example of distributional prototype
features for NER.
similar to that prototype are pushed towards that
label.
4 Supervised Evaluation Task
Various tasks can be considered to compare and
analyze the effectiveness of the above three ap-
proaches. In this study, we partly follow Turian
et al. (2010) and Yu et al. (2013), and take NER as
the supervised evaluation task.
NER identifies and classifies the named entities
such as the names of persons, locations, and orga-
nizations in text. The state-of-the-art systems typ-
ically treat NER as a sequence labeling problem,
where each word is tagged either as a BIO-style
NE or a non-NE category.
Here, we use the linear chain CRF model, which
is most widely used for sequence modeling in the
field of NLP. The CoNLL-2003 shared task dataset
from the Reuters, which was used by Turian et
al. (2010) and Yu et al. (2013), was chosen as
our evaluation dataset. The training set contains
14,987 sentences, the development set contains
3,466 sentences and is used for parameter tuning,
and the test set contains 3,684 sentences.
The baseline features are shown in Table 2.
4.1 Embedding Feature Templates
In this section, we introduce the embedding fea-
tures to the baseline NER system, turning the su-
pervised approach into a semi-supervised one.
Dense embedding features. The dense con-
tinuous embedding features can be fed directly to
the CRF model. These embedding features can
be seen as heterogeneous features from the exist-
ing baseline features, which are discrete. There is
no effective way for dense embedding features to
be combined internally or with other discrete fea-
tures. So we only use the unigram embedding fea-
tures following Turian et al. (2010). Concretely,
the embedding feature template is:
Baseline NER Feature Templates
00: w
i+k
,?2 ? k ? 2
01: w
i+k
? w
i+k+1
,?2 ? k ? 1
02: t
i+k
,?2 ? k ? 2
03: t
i+k
? t
i+k+1
,?2 ? k ? 1
04: chk
i+k
,?2 ? k ? 2
05: chk
i+k
? chk
i+k+1
,?2 ? k ? 1
06: Prefix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
07: Suffix (w
i+k
, l),?2 ? k ? 2, 1 ? l ? 4
08: Type(w
i+k
),?2 ? k ? 2
Unigram Features
y
i
? 00? 08
Bigram Features
y
i?1
? y
i
Table 2: Features used in the NER system. t is
the POS tag. chk is the chunking tag. Prefix
and Suffix are the first and last l characters of a
word. Type indicates if the word is all-capitalized,
is-capitalized, all-digits, etc.
? de
i+k
[d], ?2 ? k ? 2, d ranges over the
dimensions of the dense word embedding de.
Binarized embedding features. The binarized
embedding feature template is similar to the dense
one. The only difference is that the feature val-
ues are discrete and we omit dimensions with zero
value. Therefore, the feature template becomes:
? bi
i+k
[d], ?2 ? k ? 2, where bi
i+k
[d] 6= 0,
d ranges over the dimensions of the binarized
vector bi of word embedding.
In this way, the dimension of the binarized em-
bedding feature space becomes 2 ? d compared
with the originally d of the dense embeddings.
Compound cluster features. The advantage of
the cluster features is that they can be combined
internally or with other features to form compound
features, which can be more discriminative. Fur-
thermore, the number of resulting clusters n can
be tuned, and different ns indicate different granu-
larities. Concretely, the compound cluster feature
template for each specific n is:
? c
i+k
, ?2 ? k ? 2.
? c
i+k
? c
i+k+1
,?2 ? k ? 1.
? c
i?1
? c
i+1
.
Distributional prototype features. The set of
prototypes is again denoted by S
p
, which is de-
114
cided by selecting the topm (NPMI) words as pro-
totypes of each label, where m is tuned on the de-
velopment set. For each word w
i
in a sequence,
we compute the distributional similarity between
w
i
and each prototype in S
p
and select the proto-
types zs that DistSim(z, w) ? ?. We set ? = 0.5
without manual tuning. The distributional proto-
type feature template is then:
? {proto
i+k
=z | DistSim(w
i+k
, z) ? ? & z ?
S
p
}, ?2 ? k ? 2 .
We only use the unigram features, since the
number of active distributional prototype features
varies for different words (positions). Hence,
these features cannot be combined effectively.
4.2 Brown Clustering
Brown clustering has achieved great success in
various NLP applications. At most time, it
provides a strong baseline that is difficult to
beat (Turian et al., 2010). Consequently, in our
study, we conduct comparisons among the embed-
ding features and the Brown clustering features,
along with further investigations of their combina-
tion.
The Brown algorithm is a hierarchical cluster-
ing algorithm which optimizes a class-based bi-
gram language model defined on the word clus-
ters (Brown et al., 1992). The output of the Brown
algorithm is a binary tree, where each word is
uniquely identified by its path from the root. Thus
each word can be represented as a bit-string with
a specific length.
Following the setting of Owoputi et al. (2013),
we will use the prefix features of hierarchical clus-
ters to take advantage of the word similarity in dif-
ferent granularities. Concretely, the Brown cluster
feature template is:
? bc
i+k
, ?2 ? k ? 2.
? prefix (bc
i+k
, p), p ? {2,4,6,...,16}, ?2 ?
k ? 2. prefix takes the p-length prefix of
the Brown cluster coding bc
i+k
.
5 Experiments
5.1 Experimental Setting
We take the English Wikipedia until August 2012
as our unlabeled data to train the word embed-
dings.
4
Little pre-processing is conducted for the
4
download.wikimedia.org.
training of word embeddings. We remove para-
graphs that contain non-roman characters and all
MediaWiki markups. The resulting text is tok-
enized using the Stanford tokenizer,
5
and every
word is converted to lowercase. The final dataset
contains about 30 million sentences and 1.52 bil-
lion words. We use a dictionary that contains
212,779 most common words (frequency ? 80) in
the dataset. An efficient open-source implementa-
tion of the Skip-gram model is adopted.
6
We ap-
ply the negative sampling
7
method for optimiza-
tion, and the asynchronous stochastic gradient de-
scent algorithm (Asynchronous SGD) for parallel
weight updating. In this study, we set the dimen-
sion of the word embeddings to 50. Higher di-
mension is supposed to bring more improvements
in semi-supervised learning, but its comparison is
beyond the scope of this paper.
For the cluster features, we tune the number
of clusters n from 500 to 3000 on the develop-
ment set, and finally use the combination of n =
500, 1000, 1500, 2000, 3000, which achieves the
best results. For the distributional prototype fea-
tures, we use a fixed number of prototype words
(m) for each target label. m is tuned on the devel-
opment set and is finally set to 40.
We induce 1,000 brown clusters of words, the
setting in prior work (Koo et al., 2008; Turian et
al., 2010). The training data of brown clustering is
the same with that of training word embeddings.
5.2 Results
Table 3 shows the performances of NER on the
test dataset. Our baseline is slightly lower than
that of Turian et al. (2010), because they use
the BILOU encoding of NE types which outper-
forms BIO encoding (Ratinov and Roth, 2009).
8
Nonetheless, our conclusions hold. As we can see,
all of the three approaches we investigate in this
study achieve better performance than the direct
use of the dense continuous embedding features.
To our surprise, even the binarized embedding
features (BinarizedEmb) outperform the continu-
ous version (DenseEmb). This provides clear evi-
dence that directly using the dense continuous em-
beddings as features in CRF indeed cannot fully
5
nlp.stanford.edu/software/tokenizer.
shtml.
6
code.google.com/p/word2vec/.
7
More details are analyzed in (Goldberg and Levy, 2014).
8
We use BIO encoding here in order to compare with most
of the reported benchmarks.
115
Setting F1
Baseline 83.43
+DenseEmb? 86.21
+BinarizedEmb 86.75
+ClusterEmb 86.90
+DistPrototype 87.44
+BinarizedEmb+ClusterEmb 87.56
+BinarizedEmb+DistPrototype 87.46
+ClusterEmb+DistPrototype 88.11
+Brown 87.49
+Brown+ClusterEmb 88.17
+Brown+DistPrototype 88.04
+Brown+ClusterEmb+DistPrototype 88.58
Finkel et al. (2005) 86.86
Krishnan and Manning (2006) 87.24
Ando and Zhang (2005) 89.31
Collobert et al. (2011) 88.67
Table 3: The performance of semi-supervised
NER on the CoNLL-2003 test data, using vari-
ous embedding features. ? DenseEmb refers to the
method used by Turian et al. (2010), i.e., the direct
use of the dense and continuous embeddings.
exploit the potential of word embeddings. The
compound cluster features (ClusterEmb) also out-
perform the DenseEmb. The same result is also
shown in (Yu et al., 2013). Further, the distribu-
tional prototype features (DistPrototype) achieve
the best performance among the three approaches
(1.23% higher than DenseEmb).
We should note that the feature templates used
for BinarizedEmb and DistPrototype are merely
unigram features. However, for ClusterEmb, we
form more complex features by combining the
clusters of the context words. We also consider
different number of clusters n, to take advantage
of the different granularities. Consequently, the
dimension of the cluster features is much higher
than that of BinarizedEmb and DistPrototype.
We further combine the proposed features to see
if they are complementary to each other. As shown
in Table 3, the cluster and distributional prototype
features are the most complementary, whereas the
binarized embedding features seem to have large
overlap with the distributional prototype features.
By combining the cluster and distributional pro-
totype features, we further push the performance
to 88.11%, which is nearly two points higher than
the performance of the dense embedding features
(86.21%).
9
We also compare the proposed features with
the Brown cluster features. As shown in Table 3,
the distributional prototype features alone achieve
comparable performance with the Brown clusters.
When the cluster and distributional prototype fea-
tures are used together, we outperform the Brown
clusters. This result is inspiring because we show
that the embedding features indeed have stronger
expressing power than the Brown clusters, as de-
sired. Finally, by combining the Brown cluster
features and the proposed embedding features, the
performance can be improved further (88.58%).
The binarized embedding features are not included
in the final compound features because they are al-
most overlapped with the distributional prototype
features in performance.
We also summarize some of the reported
benchmarks that utilize unlabeled data (with no
gazetteers used), including the Stanford NER tag-
ger (Finkel et al. (2005) and Krishnan and Man-
ning (2006)) with distributional similarity fea-
tures. Ando and Zhang (2005) use unlabeled data
for constructing auxiliary problems that are ex-
pected to capture a good feature representation of
the target problem. Collobert et al. (2011) adjust
the feature embeddings according to the specific
task in a deep neural network architecture. We
can see that both Ando and Zhang (2005) and Col-
lobert et al. (2011) learn task-specific lexical fea-
tures, which is similar to the proposed distribu-
tional prototype method in our study. We suggest
this to be the main reason for the superiority of
these methods.
Another advantage of the proposed discrete fea-
tures over the dense continuous features is tag-
ging efficiency. Table 4 shows the running time
using different kinds of embedding features. We
achieve a significant reduction of the tagging time
per sentence when using the discrete features. This
is mainly due to the dense/sparse battle. Al-
though the dense embedding features are low-
dimensional, the feature vector for each word is
much denser than in the sparse and discrete feature
space. Therefore, we actually need much more
computation during decoding. Similar results can
be observed in the comparison of the DistProto-
type and ClusterEmb features, since the density of
the DistPrototype features is higher. It is possible
9
Statistical significant with p-value < 0.001 by two-tailed
t-test.
116
Setting Time (ms) / sent
Baseline 1.04
+DenseEmb 4.75
+BinarizedEmb 1.25
+ClusterEmb 1.16
+DistPrototype 2.31
Table 4: Running time of different features on a
Intel(R) Xeon(R) E5620 2.40GHz machine.
to accelerate the DistPrototype, by increasing the
threshold of DistSim(z, w). However, this is in-
deed an issue of trade-off between efficiency and
accuracy.
5.3 Analysis
In this section, we conduct analyses to show the
reasons for the improvements.
5.3.1 Rare words
As discussed by Turian et al. (2010), much of the
NER F1 is derived from decisions regarding rare
words. Therefore, in order to show that the three
proposed embedding features have stronger abil-
ity for handling rare words, we first conduct anal-
ysis for the tagging errors of words with differ-
ent frequency in the unlabeled data. We assign the
word frequencies to several buckets, and evaluate
the per-token errors that occurred in each bucket.
Results are shown in Figure 2. In most cases, all
three embedding features result in fewer errors on
rare words than the direct use of dense continuous
embedding features.
Interestingly, we find that for words that are
extremely rare (0?256), the binarized embedding
features incur significantly fewer errors than other
approaches. As we know, the embeddings for the
rare words are close to their initial value, because
they received few updates during training. Hence,
these words are not fully trained. In this case,
we would like to omit these features because their
embeddings are not even trustable. However, all
embedding features that we proposed except Bi-
narizedEmb are unable to handle this.
In order to see how much we have utilized
the embedding features in BinarizedEmb, we cal-
culate the sparsity of the binarized embedding
vectors, i.e., the ratio of zero values in each
vector (Section 3.2). As demonstrated in Fig-
ure 3, the sparsity-frequency curve has good prop-
erties: higher sparsity for very rare words and
very frequent words, while lower sparsity for mid-
frequent words. It indicates that for words that are
very rare or very frequent, BinarizedEmb just omit
most of the features. This is reasonable also for
the very frequent words, since they usually have
rich and diverse context distributions and their
embeddings cannot be well learned by our mod-
els (Huang et al., 2012).
l
l
l
l l
l
l
l
l
l
l
Frequency of word in unlabeled data
Spars
ity
256 1k 4k 16k 64k0
.50
0.55
0.60
0.65
0.70
Figure 3: Sparsity (with confidence interval) of the
binarized embedding vector w.r.t. word frequency
in the unlabeled data.
Figure 2(b) further supports our analysis. Bina-
rizedEmb also reduce much of the errors for the
highly frequent words (32k-64k).
As expected, the distributional prototype fea-
tures produce fewest errors in most cases. The
main reason is that the prototype features are task-
specific. The prototypes are extracted from the
training data and contained indicative information
of the target labels. By contrast, the other em-
bedding features are simply derived from general
word representations and are not specialized for
certain tasks, such as NER.
5.3.2 Linear Separability
Another reason for the superiority of the proposed
embedding features is that the high-dimensional
discrete features are more linear separable than
the low-dimensional continuous embeddings. To
verify the hypothesis, we further carry out experi-
ments to analyze the linear separability of the pro-
posed discrete embedding features against dense
continuous embeddings.
We formalize this problem as a binary classi-
fication task, to determine whether a word is an
NE or not (NE identification). The linear support
vector machine (SVM) is used to build the clas-
sifiers, using different embedding features respec-
117
0?256 256?512 512?1k 1k?2kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
0
50
100
150
200
250 DenseEmbBinarizedEmbClusterEmbDistPrototype
(a)
4k?8k 8k?16k 16k?32k 32k?64kFrequency of word in unlabeled data
num
ber o
f per?
token
 erro
rs
40
60
80
100
120 DenseEmbBinarizedEmbClusterEmbDistPrototype
(b)
Figure 2: The number of per-token errors w.r.t. word frequency in the unlabeled data. (a) For rare words
(frequency ? 2k). (b) For frequent words (frequency ? 4k).
Setting Acc. #features
DenseEmb 95.46 250
BinarizedEmb 94.10 500
ClusterEmb 97.57 482,635
DistPrototype 96.09 1,700
DistPrototype-binary 96.82 4,530
Table 5: Performance of the NE/non-NE classi-
fication on the CoNLL-2003 development dataset
using different embedding features.
tively. We use the LIBLINEAR tool (Fan et al.,
2008) as our SVM implementation. The penalty
parameter C is tuned from 0.1 to 1.0 on the devel-
opment dataset. The results are shown in Table 5.
As we can see, NEs and non-NEs can be better
separated using ClusterEmb or DistPrototype fea-
tures. However, the BinarizedEmb features per-
form worse than the direct use of word embedding
features. The reason might be inferred from the
third column of Table 5. As demonstrated in Wang
and Manning (2013), linear models are more ef-
fective in high-dimensional and discrete feature
space. The dimension of the BinarizedEmb fea-
tures remains small (500), which is merely twice
the DenseEmb. By contrast, feature dimensions
are much higher for ClusterEmb and DistProto-
type, leading to better linear separability and thus
can be better utilized by linear models.
We notice that the DistPrototype features per-
form significantly worse than ClusterEmb in NE
identification. As described in Section 3.4, in
previous experiments, we automatically extracted
prototypes for each label, and propagated the in-
formation via distributional similarities. Intu-
itively, the prototypes we used should be more ef-
fective in determining fine-grained NE types than
identifying whether a word is an NE. To verify
this, we extract new prototypes considering only
two labels, namely, NE and non-NE, using the
same metric in Section 3.4. As shown in the last
row of Table 5, higher performance is achieved.
6 Related Studies
Semi-supervised learning with generalized word
representations is a simple and general way of im-
proving supervised NLP systems. One common
approach for inducing generalized word represen-
tations is to use clustering (e.g., Brown clustering)
(Miller et al., 2004; Liang, 2005; Koo et al., 2008;
Huang and Yates, 2009).
Aside from word clustering, word embeddings
have been widely studied. Bengio et al. (2003)
propose a feed-forward neural network based lan-
guage model (NNLM), which uses an embedding
layer to map each word to a dense continuous-
valued and low-dimensional vector (parameters),
and then use these vectors as the input to predict
the probability distribution of the next word. The
NNLM can be seen as a joint learning framework
for language modeling and word representations.
Alternative models for learning word embed-
dings are mostly inspired by the feed-forward
NNLM, including the Hierarchical Log-Bilinear
Model (Mnih and Hinton, 2008), the recurrent
neural network language model (Mikolov, 2012),
the C&W model (Collobert et al., 2011), the log-
linear models such as the CBOW and the Skip-
118
gram model (Mikolov et al., 2013a; Mikolov et
al., 2013b).
Aside from the NNLMs, word embeddings can
also be induced using spectral methods, such as
latent semantic analysis and canonical correlation
analysis (Dhillon et al., 2011). The spectral meth-
ods are generally faster but much more memory-
consuming than NNLMs.
There has been a plenty of work that exploits
word embeddings as features for semi-supervised
learning, most of which take the continuous fea-
tures directly in linear models (Turian et al., 2010;
Guo et al., 2014). Yu et al. (2013) propose com-
pound k-means cluster features based on word em-
beddings. They show that the high-dimensional
discrete cluster features can be better utilized by
linear models such as CRF. Wu et al. (2013) fur-
ther apply the cluster features to transition-based
dependency parsing.
7 Conclusion and Future Work
This paper revisits the problem of semi-supervised
learning with word embeddings. We present three
different approaches for a careful comparison and
analysis. Using any of the three embedding fea-
tures, we obtain higher performance than the di-
rect use of continuous embeddings, among which
the distributional prototype features perform the
best, showing the great potential of word embed-
dings. Moreover, the combination of the proposed
embedding features provides significant additive
improvements.
We give detailed analysis about the experimen-
tal results. Analysis on rare words and linear sep-
arability provides convincing explanations for the
performance of the embedding features.
For future work, we are exploring a novel and a
theoretically more sounding approach of introduc-
ing embedding kernel into the linear models.
Acknowledgments
We are grateful to Mo Yu for the fruitful discus-
sion on the implementation of the cluster-based
embedding features. We also thank Ruiji Fu,
Meishan Zhang, Sendong Zhao and the anony-
mous reviewers for their insightful comments and
suggestions. This work was supported by the
National Key Basic Research Program of China
via grant 2014CB340503 and the National Natu-
ral Science Foundation of China (NSFC) via grant
61133012 and 61370164.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 1?9. Association for Computational Lin-
guistics.
Yoshua Bengio, R. E. Jean Ducharme, Pascal Vincent,
and Christian Janvin. 2003. A neural probabilistic
language model. The Journal of Machine Learning
Research, 3(Feb):1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. Pattern Analysis and Machine Intelli-
gence, IEEE Transactions on, 35(8):1798?1828.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. Proceedings
of GSCL, pages 31?40.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L. E. On Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (almost)
from scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, volume 24 of NIPS, pages 199?
207.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Yoav Goldberg and Omer Levy. 2014. word2vec ex-
plained: deriving mikolov et al.?s negative-sampling
word-embedding method. CoRR, abs/1402.3722.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 497?507, Dublin, Ireland, August.
Dublin City University and Association for Compu-
tational Linguistics.
119
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
320?327. Association for Computational Linguis-
tics.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
495?503.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 873?882,
Jeju Island, Korea. ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In Kathleen McKeown, Johanna D. Moore, Si-
mone Teufel, James Allan, and Sadaoki Furui, edi-
tors, Proc. of ACL-08: HLT, Proc. of ACL-08: HLT,
pages 595?603, Columbus, Ohio. ACL.
Vijay Krishnan and Christopher D Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 1121?1128. Association for Compu-
tational Linguistics.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master thesis, Massachusetts Institute
of Technology.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proc. of Workshop at
ICLR, Proc. of Workshop at ICLR, Arizona.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. of the NIPS, Proc. of the NIPS, pages
3111?3119, Nevada. MIT Press.
Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph. d. thesis, Brno Uni-
versity of Technology.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scal-
able hierarchical distributed language model. In
Proc. of the NIPS, Proc. of the NIPS, pages 1081?
1088, Vancouver. MIT Press.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, CoNLL ?09,
pages 147?155, Stroudsburg, PA, USA. Association
for Computational Linguistics.
D Sculley. 2010. Combined regression and ranking.
In Proceedings of the 16th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 979?988. ACM.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Jan Hajic, San-
dra Carberry, and Stephen Clark, editors, Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Proc. of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 384?394, Uppsala, Sweden. ACL.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence la-
beling. In Proc. of the Sixth International Joint Con-
ference on Natural Language Processing, Proc. of
the Sixth International Joint Conference on Natural
Language Processing, pages 1285?1291, Nagoya,
Japan. Asian Federation of Natural Language Pro-
cessing.
Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dian-
hai Yu, Hua Wu, and Haifeng Wang. 2013. Gener-
alization of words for chinese dependency parsing.
IWPT-2013, page 73.
Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Di-
anhai Yu. 2013. Compound embedding features for
semi-supervised learning. In Proc. of the NAACL-
HLT, Proc. of the NAACL-HLT, pages 563?568, At-
lanta. NAACL.
120
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864?874,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Domain Adaptation for CRF-based Chinese Word Segmentation using
Free Annotations
Yijia Liu ??, Yue Zhang ?, Wanxiang Che ?, Ting Liu ?, Fan Wu ?
?Singapore University of Technology and Design
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{yjliu,car,tliu}@ir.hit.edu.cn {yue zhang,fan wu}@sutd.edu.sg
Abstract
Supervised methods have been the domi-
nant approach for Chinese word segmen-
tation. The performance can drop signif-
icantly when the test domain is different
from the training domain. In this paper,
we study the problem of obtaining par-
tial annotation from freely available data
to help Chinese word segmentation on dif-
ferent domains. Different sources of free
annotations are transformed into a unified
form of partial annotation and a variant
CRF model is used to leverage both fully
and partially annotated data consistently.
Experimental results show that the Chi-
nese word segmentation model benefits
from free partially annotated data. On the
SIGHAN Bakeoff 2010 data, we achieve
results that are competitive to the best re-
ported in the literature.
1 Introduction
Statistical Chinese word segmentation gains high
accuracies on newswire (Xue and Shen, 2003;
Zhang and Clark, 2007; Jiang et al., 2009; Zhao
et al., 2010; Sun and Xu, 2011). However, man-
ually annotated training data mostly come from
the news domain, and the performance can drop
severely when the test data shift from newswire
to blogs, computer forums and Internet literature
(Liu and Zhang, 2012).
Several methods have been proposed for solv-
ing the domain adaptation problem for segmenta-
tion, which include the traditional token- and type-
supervised methods (Song et al., 2012; Zhang et
al., 2014). While token-supervised methods rely
on manually annotated target-domain sentences,
type-supervised methods leverage manually as-
sembled domain-specific lexicons to improve
target-domain segmentation accuracies. Both
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
Figure 1: The segmentation problem, illustrated
using the sentence ??? (Pudong) ?? (devel-
opment) ? (and) ?? (legal) ?? (construc-
tion)?. Possible segmentation labels are drawn un-
der each character, where b, m, e, s stand for the
beginning, middle, end of a multi-character word,
and a single character word, respectively. The path
shows the correct segmentation by choosing one
label for each character.
methods are competitive given the same amount of
annotation effects (Garrette and Baldridge, 2012;
Zhang et al., 2014). However, obtaining manually
annotated data can be expensive.
On the other hand, there are free data which
contain limited but useful segmentation informa-
tion over the Internet, including large-scale un-
labeled data, domain-specific lexicons and semi-
annotated web pages such as Wikipedia. In the
last case, word-boundary information is contained
in hyperlinks and other markup annotations. Such
free data offer a useful alternative for improving
the segmentation performance, especially on do-
mains that are not identical to newswire, and for
which little annotation is available.
In this paper, we investigate techniques for
adopting freely available data to help improve the
performance on Chinese word segmentation. We
propose a simple but robust method for construct-
ing partial segmentation from different sources
of free data, including unlabeled data and the
Wikipedia. There has been work on making use
of both unlabeled data (Sun and Xu, 2011; Wang
et al., 2011) and Wikipedia (Jiang et al., 2013)
864
to improve segmentation. However, no empiri-
cal results have been reported on a unified ap-
proach to deal with different types of free data.
We use a conditional random fields (Lafferty et al.,
2001; Tsuboi et al., 2008) variant that can lever-
age the partial annotations obtained from different
sources of free annotation. Training is achieved by
a modification to the learning objective, incorpo-
rating partial annotation likelihood, so that a single
model can be trained consistently with a mixture
of full and partial annotation.
Experimental results show that our method of
using partially annotated data can consistently im-
proves cross-domain segmentation performance.
We obtain results which are competitive to the
best reported in the literature. Our segmentor
is freely released at https://github.com/
ExpResults/partial-crfsuite.
2 Obtaining Partially Annotated Data
We model the Chinese word segmentation task as
a character sequence tagging problem, which is to
give each character in a sentence a word-boundary
tag (Xue and Shen, 2003). We adopt four tags, b,
m, e and s, which represent the beginning, middle,
end of a multi-character word, and a single char-
acter word, respectively. A manually segmented
sentence can be represented as a tag sequence, as
shown in Figure 1.
We investigate two major sources of freely-
available annotations: lexicons and natural anno-
tation, both with the help of unannotated data.
To make use of the first source of informa-
tion, we incorporate words from a lexicon into
unannotated sentences by matching of character
sequences, resulting in partially annotated sen-
tences, as shown in Figure 2a. In this example,
the word ???? (the Huqi Mountain)? in the
unannotated sentence matches an item in the lex-
icon. As a result, we obtain a partially-annotated
sentence, in which the segmentation ambiguity of
the characters ?? (fox)?, ?? (brandy road)? and
?? (mountain)? are resolved (??? being the be-
ginning, ??? being the middle and ??? being the
end of the same word). At the same time, the seg-
mentation ambiguity of the surrounding characters
?? (at)? and ?? (save)? are reduced (??? be-
ing either a single-character word or the end of
a multi-character word, and ??? being either a
single-character word or the beginning of a multi-
character word).
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
(a) ?? (at) ??? (Huqi Mountain) ? ? (save) ?
? (Biyao)?, where ????? matches a lexicon word.
. ? ? ? ? ? ? ? ? ?
b b b b b b b b b
m m m m m m m m m
e e e e e e e e e
s s s s s s s s s
(b) ?? (e.g.)???? (lysozyme)? ??? (lactoferrin)?,
where ?????? is a hyperlink.
Figure 2: Examples of partially annotated data.
The paths show possible correct segmentations.
Natural annotation, which refers to word
boundaries that can be inferred from URLs, fonts
or colors on web pages, also result in partially-
annotated sentences. Taking a web page shown
in Figure 2b for example. It can be inferred from
the URL tags on ?????? that ??? should be
either the beginning of a multi-character word or
a single-character word, and ??? should be either
the end a multi-character word or single-character
word. Similarly, possible tags of the surrounding
character ??? and ??? can also be inferred.
We turn both lexicons and natural annotation
into the same form of partial annotation with
same unresolved ambiguities, as shown in Figure
2, and use them together with available full anno-
tation (Figure 1) as the training data for the seg-
mentor. In this section, we describe in detail how
to obtain partially annotated sentences from each
resource, respectively.
2.1 Lexicons
In this scenario, we assume that there are unla-
beled sentences along with a lexicon for the target
domain. We obtain partially segmented sentences
by extracting word boundaries from the unlabeled
sentences with the help of the lexicon. Previous
matching methods (Wu and Tseng, 1993; Wong
and Chan, 1996) for Chinese word segmentation
largely rely on the lexicons, and are generally con-
sidered being weak in ambiguity resolution (Gao
865
People?s
Daily
?? (saw)?? (Hainan)??? (tourist industry)?? (full)?? (hope)
saw tourist industry in Hainan is full of hope
Wikipedia
??(mainly)?(is)?? (tourist)? (industry)?(and)?? (software)??(industry)
mainly is tourist industry and software industry
(a) Case of incompatible annotation on ????(tourist industry)? between People?s Daily and Wikipedia.
Literature
????? (Shuo Wen Jie Zi, a book)?(segmented)?(annotated)?
the segmented and annotated version of Shuo Wen Jie Zi
Computer
??(each)??(record)?(is)??(splitted)?(into)?? (fields)
each record is splitted into several fields
(b) Similar subsequence ???(field)? is segmented differently under different domains in Wikipedia.
Table 1: Examples natural annotation from Wikipedia. Underline marks annotated words.
et al., 2005). But for obtaining the partial labeled
data with lexicon, the matching method can still be
a solution. Since we do not aim to recognize every
word from sentence, we can select a lexicon with
smaller coverage but less ambiguity to achieve rel-
atively precise matching result.
In this paper, we apply two matching schemes
to the same raw sentences to obtain partially an-
notated sentences. The first is a simple forward-
maximum matching (FMM) scheme, which is
very close to the forward maximum matching al-
gorithm ofWu and Tseng (1993) for Chinese word
segmentation. This scheme scans the input sen-
tence from left to right. At each position, it at-
tempts to find the longest subsequence of Chi-
nese characters that matches a lexicon entry. If
such an entry is found, the subsequence is tagged
with the corresponding tags, and its surrounding
characters are also constrained to a smaller set of
tags. If no subsequence is found in the lexicon, the
character is left with all the possible tags. Taking
the sentence in Figure 2a for example. When the
algorithm scans the second character, ???, and
finds the entry ????? in the lexicon, the sub-
sequence of characters is recognized as a word,
and tagged with b, m and e, respectively. At the
same time, the previous character ??? can be in-
ferred as only end of a multi-character word (e) or
a single-character word (s). The second matching
scheme is backward maximum matching, which
can be treated as the application of FMM on the
reverse of unlabeled sentences using a lexicon of
reversed words.
To mitigate the errors resulting from one single
matching scheme, we combine the two matching
results by agreement. The basic idea is that if a
subsequence of sentence is recognized as word by
multiple matching results, it can be considered as a
more precise annotation. Our algorithm reads par-
tial segmentation by different methods and selects
the subsequences that are identified as word by all
methods as annotated words.
2.2 Natural Annotation
We use the Chinese Wikipedia for natural anno-
tation. Partially annotated sentences are readily
formed in Wikipedia by markup syntax, such as
URLs. However, some subtle issues exist if the
sentences are used directly. One problem is in-
compatibility of segmentation standards between
the annotated training data and Wikipedia. Jiang
et al. (2009) discuss this incompatibility problem
between two corpora ? the CTB and the Peo-
ple?s Daily; the problem is even more severe on
Wikipedia because it can be edited by any user.
Table 1a shows a case of incompatible annota-
tion between the People?s Daily data and natural
annotation in Wikipedia, where the three charac-
ters ????? are segmented differently. Both can
be treated as correct, although they have different
segmentation granularities.
Another problem is the intrinsic ambiguity of
segmentation. The same character sequence can
be segmented into different words under differ-
ent contexts. If the training and test data contain
different contexts, the learned model can give in-
correct results on the test data. This is particu-
larly true across different domains. Table 1b gives
such an example, where the character sequence
???? is segmented differently in two of our test
domains, but both cases exist in Wikipedia.
In summary, Wikipedia introduces both use-
ful information for domain adaptation and harm-
ful noise with negative effects on the model. To
866
achieve better performance of domain adaptation
using Wikipedia, one intuitive approach is to se-
lect more domain-related data and less irrelevant
data to minimize the risks that result from incom-
patible annotation and domain difference.
To this end, we assume that there are some raw
sentences on the target domain, which can be used
to evaluate the relevance between Wikipedia and
target domain test data. We assume that URL-
tagged entries reflect the segmentation standards
of Wikipedia sentence, and use them to match
Wikipedia sentences with the raw target domain
data. If the character sequence of any URL-tagged
entry in a Wikipedia sentence matches the target
domain data, the Wikipedia sentence is selected
for training. Another advantage of such data se-
lection is that the training time consumption can
be reduced by reducing the size of training data.
3 CRF for Word Segmentation
We follow the work of Zhao et al. (2010) and Sun
and Xu (2011), and adopt the Conditional Random
Fields (CRF) model (Lafferty et al., 2001) for the
sequence labeling problem of word segmentation.
Given an input characters sequence, the task is to
assign one segmentation label from {b,m, e, s} on
each character. Let x = (x
1
, x
2
, ..., x
T
) be the
sequence of characters in sentence whose length
is T , and y = (y
1
, y
2
, ..., y
T
) be the correspond-
ing label sequence, where y
i
? Y . The linear-
chain conditional random field for Chinese word
segmentation can be formalized as
p(y|x) =
1
Z
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (1)
where ?
k
are the model parameters, f
k
are the fea-
ture functions and Z is the probability normalizer.
Z =
?
y
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (2)
We follow Sun and Xu (2011) and use the fea-
ture templates shown in Table 2 to model the seg-
mented task. For ith character in the sentence, the
n-gram features represent the surrounding charac-
ters of this character; Type categorizes the charac-
ter it into digit, punctuation, english and other;
Identical indicates whether the input character is
the same with its surrounding characters. This
feature captures repetition patterns such as ??
? (try)? or ??? (stroll)?.
Type Template
unigram C
s
(i? 3 < s < i + 3)
bigram C
s
C
s+1
(i? 3 < s < i + 2)
C
s
C
s+2
(i? 3 < s < i + 1)
type Type(C
i
)
Type(C
s
)Type(C
s+1
)
(i? 1 < s < i + 2)
identical Identical(C
s
, C
s+1
) (i ? 3 <
s < i + 1)
Identical(C
s
, C
s+2
) (i ? 3 <
s < i)
Table 2: Feature templates for the ith character.
For fully-annotated training data, the learning
problem of conditional random fields is to maxi-
mize the log likelihood over all the training data
(Lafferty et al., 2001)
L =
N
?
n=1
log p(y
(n)
|x
(n)
)
Here N is the number of training sentences. Both
the likelihood p(y
(n)
|x
(n)
) and its gradient can be
calculated by performing the forward-backward
algorithm (Baum and Petrie, 1966) on the se-
quence, and several optimization algorithm can be
adopted to learn parameters from data, including
L-BFGS (Liu and Nocedal, 1989) and SGD (Bot-
tou, 1991).
4 Training a CRF with partially
annotated data
For word segmentation with partially annotated
data, some characters in a sentence can have
a definite segmentation label, while some can
have multiple labels with ambiguities remain-
ing. Taking the partially annotated sentence
in Figure 2a for example, the corresponding
potential label sequence for ??????? is
{(e, s), (b), (m), (e), (b, s)}, where the characters
???, ??? and ??? have fixed labels but for ???
and ???, some ambiguities exist. Note that the
full annotation in Figure 1 can be regarded as a
special case of partial annotation, where the num-
ber of potential labels for each character is one.
We follow Tsuboi et al. (2008) and model
marginal probabilities over partially annotated
data. Define the possible labels that correspond
to the partial annotation as L = (L
1
, L
2
, ..., L
T
),
where each L
i
is a non-empty subset of Y that cor-
responds to the set of possible labels for x
i
. Let
867
YL
be the set of all possible label sequences where
?y ? Y
L
, y
i
? L
i
. The marginal probability of
Y
L
can be modeled as
p(Y
L
|x) =
1
Z
?
y?Y
L
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x) (3)
Defining the unnormalized marginal probability as
Z
Y
L
=
?
y?Y
L
exp
T
?
t=1
?
k
?
k
f
k
(y
t
, y
t?1
,x),
and the normalizer Z being the same as Equation
2, the log marginal probability of Y
L
over N par-
tially annotated training examples can be formal-
ized as
L
Y
L
=
N
?
n=1
log p(Y
L
|x) =
N
?
n=1
(logZ
Y
L
? logZ)
The gradient of the likelihood can be written as
?L
Y
L
??
k
=
N
?
n=1
T
?
t=1
?
y
Y
L
?L
t
,
y
?
Y
L
?L
t?1
f
k
(y
Y
L
, y
?
Y
L
,x)p
Y
L
(y
Y
L
, y
?
Y
L
|x)
?
N
?
n=1
T
?
t=1
?
y,y
?
f
k
(y, y
?
,x)p(y, y
?
|x)
Both Z
Y
L
and its gradient are similar in form to
Z. By introducing a modification to the forward-
backward algorithm, Z
Y
L
and L
Y
L
can be calcu-
lated. Define the forward variable for partially an-
notated data ?
Y
L
,t
(j) = p
Y
L
(x
?1,...,t?
, y
t
= j). A
modification on the forward algorithm can be for-
malized as
?
Y
L
,t
(j) =
{
0 j /? L
t
?
i?L
t?1
?
t
(j, i, x
t
)?
Y
L
,t?1
(i) j ? L
t
where?
t
(j, i, x) is a potential function that equals
?
k
?
k
f
k
(y
t
= j, y
t?1
= i, x
t
). Similarly, for the
backward variable ?
Y
L
,t
,
?
Y
L
,t
(i) =
{
0 i /? L
t
?
j?L
t+1
?
t
(j, i, x
t+1
)?
Y
L
,t+1
(j) i ? L
t
Z
Y
L
can be calculated by ?
Y
L
(T ),
and p
Y
L
(y, y
?
|x) can be calculated by
?
Y
L
,t?1
(y
?
)?
t
(y, y
?
, x
t
)?
Y
L
,t
(y).
Note that if each element in Y
L
is constrained
to one single label, the CRF model in Equation 3
degrades into Equation 1. So we can train a unified
model with both fully and partially annotated data.
We implement this CRF model based on a open
source toolkit CRFSuite.
1
In our experiments, we
use the L-BFGS (Liu and Nocedal, 1989) algo-
rithm to learn parameters from both fully and par-
tially annotated data.
5 Experiments
We perform our experiments on the domain adap-
tation test data from SIGHANBakeoff 2010 (Zhao
et al., 2010), adapting annotated training sentences
from People?s Daily (PD) (Yu et al., 2001) to
different test domains. The fully annotated data
is selected from the People?s Daily newspaper
in January of 1998, and the four test domains
from the SIGHAN Bakeoff 2010 include finance,
medicine, literature and computer. Sample seg-
mented data in the computer domain from this
bakeoff is used as development set. Statistics of
the data are shown in first half of Table 3. We
use wikidump20140419
2
for the Wikipedia data.
All the traditional Chinese pages in Wikipedia are
converted to simplified Chinese. After filtering
functional pages like redirection and removing du-
plication, 5.45 million sentences are reserved.
For comparison with related work on using a
lexicon to improve segmentation, another set of
test data is chosen for this setting. We use the Chi-
nese Treebank (CTB) as the source domain data,
and Zhuxian (a free Internet novel, also named as
?Jade dynasty?, referred to as ZX henceforth) as
the target domain data.
3
The ZX data are written
in a different style from newswire, and contains
many out-of-vocabulary words. This setting has
been used by Liu and Zhang (2012) and Zhang et
al. (2014) for domain adaptation of segmentation
and POS-tagging. We use the standard training,
development and test split. Statistics of the test
data annotated by Zhang et al. (2014) are shown
in the second half of Table 3.
The data preparation method in Section 2 and
the CRF method in Section 4 are used for all
the experiments. Both recall of out-of-vocabulary
words (R
oov
) and F-score are used to evaluate the
1
http://www.chokkan.org/software/
crfsuite/
2
http://dumps.wikimedia.org/zhwiki/
20140419/
3
Annotated target domain test data and lexicon are avail-
able from http://ir.hit.edu.cn/
?
mszhang/
eacl14mszhang.zip.
868
P
D
?
S
I
G
H
A
N
Data set Train Development Test
PD Computer Finance Medicine Literature Computer
# sent. 19,056 1,000 560 1,308 670 1,329
# words 1,109,734 21,398 33,035 31,499 35,735 35,319
OOV 0.1766 0.0874 0.1102 0.0619 0.1522
C
T
B
5
?
Z
X
Data set Train Development Test Unlabeled
W
i
k
i
p
e
d
i
a
Unlabeled
CTB5 ZX
# sent. 18,086 788 1,394 32,023 5,456,151
# words 493,934 20,393 34,355
OOV 0.1377 0.1550
Table 3: Statistics of data used in this paper.
segmentation performance. There is a mixture of
Chinese characters, English words and numeric
expression in the test data from SIGHAN Bakeoff
2010. To test the influence of Wikipedia data on
Chinese word segmentation alone, we apply reg-
ular expressions to detect English words and nu-
meric expressions, so that they are marked as not
segmented. After performing this preprocessing
step, cleaned test input data are fed to the CRF
model to give a relatively strong baseline.
5.1 Free Lexicons
5.1.1 Obtaining lexicons
For domain adaption from CTB to ZX, we use
a lexicon released by Zhang et al. (2014). The
lexicon is crawled from a online encyclopedia
4
,
and contains the names of 159 characters and ar-
tifacts in the Zhuxian novel. We follow Zhang et
al. (2014) and name it NR for convenience of fur-
ther discussion. The NR lexicon can be treated
as a strongly domain-related, high quality but rel-
atively small lexicon. It?s a typical example of
freely available lexicon over the Internet.
For domain adaptation from PD to medicine and
computer, we collect a list of page titles under
the corresponding categories in Wikipedia. For
medicine, entries under essential medicines, bi-
ological system and diseases are collected. For
computer, entries under computer network, Mi-
crosoft Windows and software widgets are se-
lected. These lexicons are typical freely available
lexicons that we can access to.
5.1.2 Obtaining Unlabeled Sentences
For ZX, partially annotated sentences are obtained
using the NR lexicon and unlabeled ZX sentences
by applying the matching scheme described in
4
http://baike.baidu.com/view/18277.htm
90.1
90.2
90.3
90.4
1 2 4 8 16 32# of sentences * 1000
F sc
ore 
on d
eve
lopm
ent
Figure 3: F-score on the development data when
using different numbers of unlabeled data.
Section 2. The CTB5 training data and the par-
tially annotated data are mixed as the final train-
ing data. Different amounts of unlabeled data are
applied to the development test set, and results are
shown in Figure 3. From this figure we can see
that incorporating 16K sentences gives the high-
est accuracy, and adding more partial labeled data
does not change the accuracy significantly. So for
the ZX experiments, we choose the 16K sentences
as the unlabeled data.
For the medicine and computer experiments, we
selected domain-specific sentences by matching
with the domain-specific lexicons. About 46K out
of the 5.45 million wiki sentences contain subse-
quences in the medicine lexicon and 22K in the
case of the computer domain. We randomly se-
lect 16K sentences as the unlabeled data for each
domain, respectively.
5.1.3 Final results
We incorporate the partially annotated data ob-
tained with the help of lexicon for each of the
test domain. For adaptation from CTB to ZX, we
trained our baseline model on the CTB5 training
data with the feature templates in Table 2. For
adaptation from PD to medicine and computer, we
869
Domain ZX Medicine Computer
F Roov F Roov F Roov
Baseline 87.50 73.65 91.36 72.95 93.16 84.02
Baseline+Lexicon Feature 90.36 80.69 91.60 74.39 93.14 84.27
Baseline+PA (Lex) 90.63 84.88 91.68 74.99 93.47 85.63
Zhang et al. (2014) 88.34 - - - - -
Table 4: Final result for adapting CTB to Zhuxian and adapting PD to the medicine and computer
domains, using partially annotated data (referred to as PA) obtained from unlabeled data and lexicons.
trained our baseline model on the PD training data
with the same feature template setting.
Previous research makes use of a lexicon by
adding lexicon features directly into a model (Sun
and Xu, 2011; Zhang et al., 2014), rather than
transforming them into partially annotated sen-
tences. To make a comparison, we follow Sun and
Xu (2011) and add three lexicon features to repre-
sent whether c
i
is located at the beginning, middle
or the end of a word in the lexicon, respectively.
For each test domain, the lexicon for the lexi-
con feature model consists of the most frequent
words in the source domain training data (about
6.7K for CTB5 and 8K for PD, respectively) and
the domain-specific lexicon we obtained in Sec-
tion 5.1.1.
The results are shown in Table 4, where the first
row shows the performance of the baseline mod-
els and the second row shows the performance
of the model incorporating lexicon feature. The
third row shows our method using partial anno-
tation. On the ZX test set, our method outper-
forms the baseline by more than 3 absolute per-
centage. The model with partially annotated data
performs better than the one with additional lexi-
con features. Similar conclusion is obtained when
adapting from PD to medicine and computer. By
incorporating the partially annotated data, the seg-
mentation of lexicon words, along with the con-
text, is learned.
We also compare our method with the work of
Zhang et al. (2014), who reported results only on
the ZX test data. We use the same lexicon settings.
Our method gives better result than Zhang et al.
(2014), showing that the combination of a lexicon
and unannotated sentence into partially annotated
data can lead to better performance than using a
dictionary alone in type-supervision. Given that
we only explore the use of free resource, combin-
ing a lexicon with unannotated sentences is a bet-
ter option than using the lexicon directly. Zhang
et al.?s concern, on the other hand, is to compare
Method
Com. Dev
F Roov
Baseline 93.56 83.75
Baseline+PA (Random 160K) 94.29 86.58
Baseline+PA (Selected) 95.00 88.28
Table 5: The performance of data selection on the
development set of the computer domain.
type- and token-annotation. Our partial annota-
tion can thus be treated as a compromise to obtain
some pseudo partial token-annotations when full
token annotations are unavailable. Another thing
to note is that the model of Zhang et al. (2014) is
a joint model for segmentation and POS-tagging,
which is generally considered stronger than a sin-
gle segmentation model.
5.2 Free Natural Annotation
When extracting word boundaries from Wikipedia
sentences, we ignore natural annotations on En-
glish words and digits because these words are rec-
ognized by the preprocessor. Following Jiang et
al. (2013), we also recognize a naturally annotated
two-character subsequence as a word.
5.2.1 Effect of data selection
To make better use of more domain-specific data,
and to alleviate noise in partial annotation, we ap-
ply the selection method proposed in Section 2
to the Wikipedia data. On the computer domain
development test data, this selection method re-
sults in 9.4K computer-related sentences with par-
tial annotation. A model is trained with both the
PD training data and the partially annotated com-
puter domain Wikipedia data. For comparison, we
also trained a model with 160K randomly selected
Wikipedia sentences. The experimental result is
shown in Table 5. The model incorporating se-
lected data achieves better performance compared
to the model with randomly sampled data, demon-
strating that data selection is helpful to improving
870
Method
Finance Medicine Literature Computer
Avg-FF Roov F Roov F Roov F Roov
Baseline 95.20 86.90 91.36 72.90 92.27 73.61 93.16 83.48 93.00
Baseline+PA (Ran-
dom 160K)
95.16 87.60 92.41 78.13 92.17 75.30 93.91 83.48 93.41
Baseline+PA
(Selected)
95.54 88.53 92.47 78.28 92.49 76.84 93.93 87.53 93.61
+0.34 +1.11 +0.22 +0.77
Jiang et al. (2013) 93.16 93.34 93.53 91.19 92.80
Table 6: Experimental results on the SIGHAN Bakeoff 2010 data.
the domain adaption accuracy.
5.2.2 Final Result
The final results on the four test domains are
shown in Table 6. From this table, we can see
that significant improvements are achieved with
the help of the partially annotated Wikipedia data,
when compared to the baseline. The models
trained with selected partial annotation perform
better than those trained with random partial an-
notation. Our F-scores are competitive to those re-
ported by Jiang et al. (2013). However, since their
model is trained on a different source domain, the
results are not directly comparable.
5.2.3 Analysis
In this section, we study the effect of Wikipedia on
domain adaptation when no data selection is per-
formed, in order to analyze the effect of partially
annotated data. We randomly sample 10K, 20K,
40K, 80K and 160K sentences from the 5.45 mil-
lion Wikipedia sentences, and incorporate them
into the training process, respectively. Five models
are obtained adding the baseline, and we test their
performances on the four test domains. Figure 4
shows the results.
From the figure we can see that for the medicine
and computer domains, where the OOV rate is rel-
atively high, the F-score generally increases when
more data from Wikipedia are used. The trends
of F-score and OOV recall against the volume of
Wikipedia data are almost identical. However, for
the finance and literature domains, which have low
OOV rates, such a relation between data size and
accuracy is not witnessed. For the literature do-
main, even an opposite trends is shown.
We can draw the following conclusions: (1)
Natural annotation on Wikipedia data contributes
to the recognition of OOV words on domain adap-
tation; (2) target domains with more OOV words
benefit more from Wikipedia data. (3) along with
Method
Med. Com.
F F
Baseline 91.36 93.16
Baseline+PA (Lex) 91.68 93.47
Baseline+PA (Natural) 92.47 93.93
Baseline+PA (Lex+Natural) 92.63 94.07
Table 7: Results by combining different sources of
free annotation.
the positive effect on OOV recognition, Wikipedia
data can also introduce noise, and hence data se-
lection can be useful.
5.3 Combining Lexicon and Natural
Annotation
To make the most use of free annotation, we com-
bine available free lexicon and natural annotation
resources by joining the partially annotated sen-
tences derived using each resource, training our
CRF model with these partially annotated sen-
tences and the fully annotated PD sentences. The
tests are performed on medicine and computer do-
mains. Table 7 shows the results, where further
improvements are made on both domains when the
two types of resources are combined.
6 Related Work
There has been a line of research on making use of
unlabeled data for word segmentation. Zhao and
Kit (2008) improve segmentation performance by
mutual information between characters, collected
from large unlabeled data; Li and Sun (2009) use
punctuation information in a large raw corpus to
learn a segmentation model, and achieve better
recognition of OOVwords; Sun and Xu (2011) ex-
plore several statistical features derived from un-
labeled data to help improve character-based word
segmentation. These investigations mainly focus
on in-domain accuracies. Liu and Zhang (2012)
871
0.9516
0.9519
0.9522
0.9525
0 50 100 150
0.8700
0.8725
0.8750
0.8775
(a) Finance
0.9150
0.9175
0.9200
0.9225
0 50 100 150
0.73
0.74
0.75
0.76
0.77
0.78
(b) Medicine
0.9216
0.9219
0.9222
0.9225
0 50 100 150 0.735
0.740
0.745
0.750
(c) Literature
0.934
0.936
0.938
0 50 100 150
0.84
0.85
0.86
0.87
(d) Computer
Figure 4: Performance of the model incorporating difference sizes of Wikipedia data. The solid line
represents the F-score and dashed line represents the recall of OOV words.
study domain adaptation using an unsupervised
self-training method. In contrast to their work,
we make use of not only unlabeled data, but also
leverage any free annotation to achieve better re-
sults for domain adaptation.
There has also been work on making use of a
dictionary and natural annotation for segmenta-
tion. Zhang et al. (2014) study type-supervised do-
main adaptation for Chinese segmentation. They
categorize domain difference into two types: dif-
ferent vocabulary and different POS distributions.
While the first type of difference can be effec-
tively resolved by using lexicon for each domain,
the second type of difference needs to be resolved
by using annotated sentences. They found that
given the same manual annotation time, a com-
bination of the lexicon and sentence is the most
effective. Jiang et al. (2013) use 160K Wikipedia
sentences to improves segmentation accuracies on
several domains. Both Zhang et al. (2014) and
Jiang et al. (2013) work on discriminative mod-
els using the structure perceptron (Collins, 2002),
although they study two different sources of infor-
mation. In contrast to their work, we unify both
types of information under the CRF framework.
CRF has been used for Chinese word segmenta-
tion (Tseng, 2005; Shi and Wang, 2007; Zhao and
Kit, 2008; Wang et al., 2011). However, most pre-
vious work train a CRF by using full annotation
only. In contrast, we study CRF based segmenta-
tion by using both full and partial annotation.
Several other variants of CRF model has been
proposed in the machine learning literature, such
as the generalized expectation method (Mann and
McCallum, 2008), which introduce knowledge by
incorporating a manually annotated feature dis-
tribution into the regularizer, and the JESS-CM
(Suzuki and Isozaki, 2008), which use a EM-like
method to iteratively optimize the parameter on
both the annotated data and unlabeled data. In
contrast, we directly incorporate the likelihood of
partial annotation into the objective function. The
work that is the most similar to ours is Tsuboi et
al. (2008), who modify the CRF learning objec-
tive for partial data. They focus on Japanese lexi-
cal analysis using manually collected partial data,
while we investigate the effect of partial annota-
tion from freely available sources for Chinese seg-
mentation.
7 Conclusion
In this paper, we investigated the problem of do-
main adaptation for word segmentation, by trans-
ferring various sources of free annotations into a
consistent form of partially annotated data and ap-
plying a variant of CRF that can be trained using
fully- and partially-annotated data simultaneously.
We performed a large set of experiments to study
the effectness of free data, finding that they are
useful for improving segmentation accuracy. Ex-
periments also show that proper data selection can
further benefit the model?s performance.
872
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments. This work was supported
by the National Key Basic Research Program of
China via grant 2014CB340503 and the National
Natural Science Foundation of China (NSFC) via
grant 61133012 and 61370164, the Singapore
Ministry of Education (MOE) AcRF Tier 2 grant
T2MOE201301 and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
References
Leonard E Baum and Ted Petrie. 1966. Statistical
inference for probabilistic functions of finite state
markov chains. The annals of mathematical statis-
tics, pages 1554?1563.
L?eon Bottou. 1991. Stochastic gradient learning in
neural networks. In Proceedings of Neuro-N??mes
91, Nimes, France. EC2.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Comput. Linguist., 31(4):531?574, December.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markovmodels for part-of-speech
tagging with incomplete tag dictionaries. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 821?
831, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 522?530, Suntec, Singapore,
August. Association for Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan L?u, Yating Yang, and
Qun Liu. 2013. Discriminative learning with natu-
ral annotations: Word segmentation as a case study.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 761?769, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Comput. Linguist., 35(4):505?512, December.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528, December.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745?754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870?878, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of the
20th International Joint Conference on Artifical In-
telligence, IJCAI?07, pages 1707?1712, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Yan Song, Prescott Klassen, Fei Xia, and Chunyu Kit.
2012. Entropy-based training data selection for do-
main adaptation. In Proceedings of COLING 2012:
Posters, pages 1191?1200, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?
979, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL-
08: HLT, pages 665?673, Columbus, Ohio, June.
Association for Computational Linguistics.
Huihsin Tseng. 2005. A conditional random field word
segmenter. In In Fourth SIGHAN Workshop on Chi-
nese Language Processing.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
897?904, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
873
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Pak-kwong Wong and Chorkin Chan. 1996. Chinese
word segmentation based on maximum matching
and word binding force. In Proceedings of the 16th
Conference on Computational Linguistics - Volume
1, COLING ?96, pages 200?203, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text
segmentation for text retrieval: Achievements and
problems. J. Am. Soc. Inf. Sci., 44(9):532?542, Oc-
tober.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language
Processing - Volume 17, SIGHAN ?03, pages 176?
179, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 840?
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2014. Type-supervised domain adaptation for
joint segmentation and pos-tagging. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 588?597, Gothenburg, Sweden, April. Asso-
ciation for Computational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In In: The Third International Joint Conference on
Natural Language Processing (IJCNLP-2008.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. 9(2):5:1?5:32,
June.
874
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 588?597,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Type-Supervised Domain Adaptation for Joint Segmentation and
POS-Tagging
Meishan Zhang
?
, Yue Zhang
?
, Wanxiang Che
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
We report an empirical investigation on
type-supervised domain adaptation for
joint Chinese word segmentation and
POS-tagging, making use of domain-
specific tag dictionaries and only un-
labeled target domain data to improve
target-domain accuracies, given a set of
annotated source domain sentences. Pre-
vious work on POS-tagging of other lan-
guages showed that type-supervision can
be a competitive alternative to token-
supervision, while semi-supervised tech-
niques such as label propagation are
important to the effectiveness of type-
supervision. We report similar findings
using a novel approach for joint Chinese
segmentation and POS-tagging, under a
cross-domain setting. With the help of un-
labeled sentences and a lexicon of 3,000
words, we obtain 33% error reduction in
target-domain tagging. In addition, com-
bined type- and token-supervision can lead
to improved cost-effectiveness.
1 Introduction
With accuracies of over 97%, POS-tagging of
WSJ can be treated as a solved problem (Man-
ning, 2011). However, performance is still well
below satisfactory for many other languages and
domains (Petrov et al., 2012; Christodoulopoulos
et al., 2010). There has been a line of research on
using a tag-dictionary for POS-tagging (Merialdo,
1994; Toutanova and Johnson, 2007; Ravi and
Knight, 2009; Garrette and Baldridge, 2012). The
idea is compelling: on the one hand, a list of lex-
icons is often available for special domains, such
as bio-informatics; on the other hand, compiling a
?
Corresponding author.
lexicon of word-tag pairs appears to be less time-
consuming than annotating full sentences.
However, success in type-supervised POS-
tagging turns out to depend on several subtle fac-
tors. For example, recent research has found that
the quality of the tag-dictionary is crucial to the
success of such methods (Banko and Moore, 2004;
Goldberg et al., 2008; Garrette and Baldridge,
2012). Banko and Moore (2004) found that the
accuracies can drop from 96% to 77% when a
hand-crafted tag dictionary is replaced with a raw
tag dictionary gleaned from data, without any hu-
man intervention. These facts indicate that careful
considerations need to be given for effective type-
supervision. In addition, significant manual work
might be required to ensure the quality of lexicons.
To compare type- and token-supervised tagging,
Garrette and Baldridge (2013) performed a set of
experiments by conducting each type of annota-
tion for two hours. They showed that for low-
resource languages, a tag-dictionary can be rea-
sonably effective if label propagation (Talukdar
and Crammer, 2009) and model minimizations
(Ravi and Knight, 2009) are applied to expand and
filter the lexicons. Similar findings were reported
in Garrette et al. (2013).
Do the above findings carry over to the Chi-
nese language? In this paper, we perform an
empirical study on the effects of tag-dictionaries
for domain adaptation of Chinese POS-tagging.
We aim to answer the following research ques-
tions: (a) Is domain adaptation feasible with only
a target-domain lexicon? (b) Can we further im-
prove type-supervised domain adaptation using
unlabeled target-domain sentences? (c) Is craft-
ing a tag dictionary for domain adaptation more
effective than manually annotating target domain
sentences, given similar efforts?
Our investigations are performed under two
Chinese-specific settings. First, unlike low-
resource languages, large amounts of annotation
588
are available for Chinese. For example, the Chi-
nese Treebank (CTB) (Xue et al., 2005) contains
over 50,000 manually tagged news sentences.
Hence rather than studying purely type-supervised
POS-tagging, we make use of CTB as the source
domain, and study domain adaptation to the Inter-
net literature.
Second, one uniqueness of Chinese POS-
tagging, in contrast to the POS-tagging of alpha-
betical languages, is that word segmentation can
be performed jointly to avoid error propagation
(Ng and Low, 2004; Zhang and Clark, 2008; Kru-
engkrai et al., 2009; Zhang and Clark, 2010). We
adopt this approach for a strong baseline. Previous
studies showed that unsupervised domain adap-
tation can give moderate improvements (Liu and
Zhang, 2012). We show that accuracies can be
much more significantly improved by using target-
domain knowledge in the form of lexicons.
Both token-supervised and type-supervised do-
main adaptation rely on a set of source-domain
annotations; while the former makes additional
use of a small set of target annotations, the lat-
ter leverages a target-domain lexicon. We take
a feature-based method, analogous to that of
Daume III (2007), which tunes domain-dependent
versions of features using domain-specific data.
Our method tunes a set of lexicon-based features,
so that domain-dependent models are derived from
inserting domain-specific lexicons.
The conceptually simple method worked highly
effectively on a test set of 1,394 sentences from
the Internet novel ?Zhuxian?. Combined with
the use of unlabeled data, a tag lexicon of 3,000
words gave a 33% error reduction when com-
pared with a strong baseline system trained using
CTB data. We observe that joint use of type- and
token-supervised domain adaptation is more cost-
effective than pure type- or token-supervision.
With 10 hours of annotation, the best error reduc-
tion reaches 47%, with F-score increasing from
80.81% to 89.84%.
2 Baseline
We take as the baseline system a discriminative
joint segmentation and tagging model, proposed
by Zhang and Clark (2010), together with simple
self-training (Liu and Zhang, 2012). While the
baseline discriminative model gives state-of-the-
art joint segmentation and tagging accuracies on
CTB data, the baseline self-training makes use of
unlabeled target domain data to find improved tar-
get domain accuracies over bare CTB training.
2.1 The Baseline Discriminative Chinese
POS-Tagging Model
The baseline discriminative model performs
segmentation and POS-tagging simultaneously.
Given an input sentence c
1
? ? ? c
n
(c
i
refers to the
ith character in the sentence), it operates incre-
mentally, from left to right. At each step, the cur-
rent character can either be appended to the last
word of the existing partial output, or seperated as
the start of a new word with tag p. A beam is used
to maintain the N-best partial results at each step
during decoding. At step i (0 ? i < n), each
item in the beam corresponds to a segmentation
and POS-tagging hypothesis for the first i?1 char-
acters, with the last word being associated with a
POS, but marked as incomplete. When the next
character c
i
is processed, it is combined with all
the partial results from the beam to generate new
partial results, using two types of actions: (1) Ap-
pend, which appends c
i
to the last (partial) word
in a partial result; (2) Separate(p), which makes
the last word in the partial result as completed and
adds c
i
as a new partial word with a POS tag p.
Partial results in the beam are scored globally
over all actions used to build them, so that the N-
best can be put back to the agenda for the next step.
For each action, features are extracted differently.
We use the features from Zhang and Clark (2010).
Discriminative learning with early-update (Collins
and Roark, 2004; Zhang and Clark, 2011) is used
to train the model with beam-search.
2.2 Baseline Unsupervised Adaptation by
Self-Training
A simple unsupervised approach for POS-tagging
with unlabeled data is EM. For a generative model
such as HMM, EM can locally maximize the like-
lihood of training data. Given a good start, EM
can result in a competitive HMM tagging model
(Goldberg et al., 2008).
For discriminative models with source-domain
training examples, an initial model can be trained
using the source-domain data, and self-training
can be applied to find a locally-optimized model
using raw target domain sentences. The training
process is sometimes associated with the EM al-
gorithm. Liu and Zhang (2012) used perplexities
of character trigrams to order unlabeled sentences,
and applied self-training to achieve a 6.3% error
589
Common
Lexicon
Source
Lexicon
Source
Corpus
Training
Model
Target
Sentences
Target
Lexicon
Common
Lexicon
Tagging
Tagging
Results
Training Tagging
Figure 1: Architecture of our lexicon-based model for domain adaptation.
reduction on target-domain data when compared
with source domain training. Their method is sim-
ple to implement, and we take it as our baseline.
3 Type-Supervised Domain Adaptation
To give a formal definition of the domain adap-
tation tasks, we denote by C
s
a set of anno-
tated source-domain sentences, C
t
a set of anno-
tated target-domain sentences, and L
t
an anno-
tated target-domain lexicon. The form of L
t
is a
list of target-domain words, each associated with
a set of POS tags. Token-supervised domain adap-
tation is the task of making use of C
s
and C
t
to
improve target-domain performances, while type-
supervised domain adaptation is to make use of C
s
and L
t
instead for the same purpose.
As described in the introduction, type-
supervised domain adaptation is useful when
annotated sentences are absent, but lexicons are
available. In addition, it is an interesting question
which type of annotation is more cost-effective
when neither is available. We empirically com-
pare the two approaches by proposing a novel
method for type-supervised domain adaptation of
a discriminate tagging model, showing that it can
be a favourable choice in practical situation.
In particular, we split Chinese words into
domain-independent and domain-specific cate-
gories, and define unlexicalized features for
domain-specific words. We train lexicalized
domain-independent and unlexicalized domain-
specific features using the source domain anno-
tated sentences and a source-domain lexicon, and
then apply the resulting model to the target do-
main by replacing the source-domain lexicon with
a target domain lexicon. Combined with unsu-
pervised learning with unlabeled target-domain
of sentences, the conceptually simple method
worked highly effectively. Following Garrette and
Baldridge (2013), we address practical questions
on type-supervised domain adaptation by compar-
ison with token-supervised methods under similar
human annotation efforts.
3.1 System Architecture
Our method is based on the intuition that domain-
specific words of certain types (e.g. proper names)
can behave similarly across domains. For exam-
ple, consider the source-domain sentence ???
?|NR (Jiang Zemin) ??|AD (afterwards) ?
?|VV (visit) ??|NR (Shanghai Automobiles
Corp.)? and the target-domain sentence ??
?|NR (Biyao) ??|AD (afterwards) ??|VV
(arrive) ???|NR (the Bamboo Mountains)?.
???? (Jiang Zemin)? and ??? (Biyao)? are
person names in the two domains, respectively,
whereas ??? (Shanghai Automobiles Corp.)?
and ???? (the Bamboo Mountains)? are loca-
tion names in the two domains, respectively. If the
four words are simply treated as domain-specific
nouns, the two sentences both have the pattern
??domain-NR? AD VV ?domain-NR??, and hence
source domain training data can be useful in train-
ing the distributions of the lexicon-based features
for both domains.
Further, we assume that the syntax structures
and the usage of function words do not vary sig-
nificantly across domains. For example, verbs, ad-
jectives or proper nouns can be different from do-
main to domain, but the subject-verb-object sen-
tence structure does not change. In addition, the
usage of closed-set function words remains sta-
ble across different domains. In the CTB tagset,
closed-set POS tags are the vast majority. Under
this assumption, we introduce a set of unlexical-
ized features into the discriminative model, in or-
der to capture the distributions of domain-specific
dictionary words. Unlexicalized features trained
for source domain words can carry over to the tar-
get domain. The overall architecture of our sys-
590
Action Lexicon Feature templates
Separate in-lex(w
?1
), l(w
?1
) ? in-lex(w
?1
),
in-lex(w
?1
, t
?1
), l(w
?1
) ? in-lex(w
?1
, t
?1
)
Table 1: Dictionary features of the type-
supervised model, where w
?1
and t
?1
denote the
last word and POS tag of a partial result, re-
spectively; l(w) denotes the length of the word
w; in-lex(w, t) denotes whether the word-tag pair
(w, t) is in the lexicon.
tem is shown in Figure 1, where lexicons can be
treated as ?plugins? to the model for different do-
mains, and one model trained from the source do-
main can be applied to many different target do-
mains, as long as a lexicon is available.
The method can be the most effective
when there is a significant amount of domain-
independent words in the data, which provide rich
lexicalized contexts for estimating unlexicalized
features for domain-specific words. For scientific
domains (e.g. the biomedical domain) which
share a significant proportion of common words
with the news domain, and have most domain
specific words being nouns (e.g. ???? (dia-
betes)?), the method can be the most effective.
We choose a comparatively difficult domain pair
(e.g. modern news v.s. ancient style novel),
for which the use of many word types are quite
different. Results on this data can be relatively
more indicative of the usefulness of the method.
3.2 Lexicon-Based Features
Table 1 shows the set of new unlexicalized fea-
tures for the domain-specific lexicons. In addition
to words and POS tags, length information is also
encoded in the features, to capture different dis-
tributions of different word sizes. For example,
a one-character word in the dictionary might not
be identified as confidently using the lexicon as a
three-character word in the dictionary.
To acquire a domain-specific lexicon for the
source domain, we use HowNet (Dong and
Dong, 2006) to classify CTB words into domain-
independent and domain-specific categories. Con-
sisting of semantic information for nearly 100,000
common Chinese words, HowNet can serve as a
resource of domain-independent Chinese words.
We choose out of all words in the source domain
training data those that also occur in HowNet for
domain-independent words, and out of the remain-
ing words those that occur more than 3 times for
words specific to the source domain. We assume
that the domain-independent lexicon applies to all
target domains also. For some target domains,
we can obtain domain-specific terminologies eas-
ily from the Internet. However, this can be a very
small portion depending on the domain. Thus, it
may still be necessary to obtain new lexicons by
manual annotation.
3.3 Lexicon and Self-Training
The lexicon-based features can be combined with
unsupervised learning to further improve target-
domain accuracies. We apply self-training on top
of the lexicon-based features in the following way:
we train a lexicon-based model M using a lexi-
con L
s
of the source domain, and then apply M
together with a target-domain lexicon L
t
to auto-
matically label a set of target domain sentences.
We combine the automatically labeled target sen-
tences with the source-domain training data to ob-
tain an extended set of training data, and train a
final model M
self
, using the lexicon L
s
and L
t
for
source- and target-domain data, respectively.
Different numbers of target domain sentences
can be used for self-training. Liu and Zhang
(2012) showed that an increased amount of tar-
get sentences do not constantly lead to improved
development accuracies. They use character per-
plexity to order target domain sentences, taking
the top K sentences for self-training. They eval-
uate the optimal development accuracies using a
range of different Kvalues, and select the best K
for a final model. This method gave better results
than using sentences in the internet novel in their
original order (Liu and Zhang, 2012). We follow
this method in ranking target domain sentences.
4 Experiments
4.1 Setting
We use annotated sentences from the CTB5 for
source-domain training, splitting the corpus into
training, development and test sections in the same
way as previous work (Kruengkrai et al., 2009;
Zhang and Clark, 2010; Sun, 2011).
Following Liu and Zhang (2012), we use the
free Internet novel ?Zhuxian? (henceforth referred
to as ZX; also known as ?Jade dynasty?) as our tar-
get domain data. The writing style of the novel is
in the literature genre, with the style of Ming and
Qing novels, very different from news in CTB. Ex-
591
CTB sentences ZX sentences
?????????? ??????????????????????
(Qiaoshi meets the Russian delegates.) (The world was big. It held everything. There were fascinating
?????????????? landscapes. There were haunting ghosts.)
(Lipeng stressed on speeding the reform of official regulations.) ??????????????
?????????????? (No time left. Let me call out Zhuxian, the ancient sword.)
(Chinese chemistry industry increases the pace of opening up.) ???????????????(There came suddenly
a gust of wind, out of which was laughters and magic flashes.)
Table 2: Example sentences from CTB and ZX to illustrate the differences between news and novel.
Data Set Chap. IDs # sents # words
CTB5
Train 1-270, 400-931, 10,086 493,930
1001-1151
Devel 301-325 350 6,821
Test 271-300 348 8,008
ZX
Train 6.6-6.10, 2,373 67,648
7.6-7.10, 19
Devel 6.1-6.5 788 20,393
Test 7.1-7.5 1,394 34,355
Table 3: Corpus statistics.
ample sentences from the two corpora are shown
in Table 2. Liu and Zhang (2012) manually anno-
tated 385 sentences as development and test data,
which we download from their website.
1
These
data follow the same annotation guidelines as the
Chinese Treebank (Xue et al., 2000).
To gain more reliable statistics in our results,
we extend their annotation work to a total 4,555
sentences, covering the sections 6, 7 and 19 of the
novel. The annotation work is based on the auto-
matically labeled sentences by our baseline model
trained with CTB5 corpus. It took an experienced
native speaker 80 hours, about one minute on av-
erage to annotate one sentence. We use chapters
1-5 of section 6 as the development data, chap-
ters 1-5 of section 7 as the test data, and the re-
maining data for target-domain training,
2
in order
to compare type-supervised methods with token-
supervised methods. Under permission from the
author of the novel, we release our annotation for
future reference. Statistics of both the source and
the target domain data are shown in Table 3. The
rest of the novel is treated as unlabeled sentences,
used for type-annotation and self-training.
We perform the standard evaluation, using F-
scores for both the segmentation accuracy and the
1
http://faculty.sutd.edu.sg/?yue zhang/emnlp12yang.zip
2
We only use part of the training sentences in our experi-
ments, and the remaining can be used for further research.
overall segmentation and POS tagging accuracy.
4.2 Baseline Performances
The baseline discriminative model can achieve
state-of-the-art performances on the CTB5, with
a 97.62% segmentation accuracy and a 93.85% on
overall segmentation and tagging accuracy. Using
the CTB model, the performance on ZX drops sig-
nificantly, to a 87.71% segmentation accuracy and
a 80.81% overall accuracy. Applying self-training,
the segmentation and overall F-scores can be im-
proved to 88.62% and 81.94% respectively.
4.3 Development Experiments
In this section, we study type-supervised domain
adaptation by conducting a series of experiments
on the development data, addressing the follow-
ing questions. First, what is the influence of tag-
dictionaries through lexicon-based features? Sec-
ond, what is the effect of type-supervised domain
adaptation in contrast to token-supervised domain
adaptation under the same annotation cost? Third,
what is the interaction between tag-dictionary and
self-training? Finally, what is the combined effect
of type- and token-supervised domain adaptation?
4.3.1 The Influence of The Tag Dictionary
We investigate the effects of two different tag dic-
tionaries. The first dictionary contains names of
characters (e.g. ?? (Guili)) and artifacts (e.g.
swords such as?? (Dragonslayer)) in the novel,
which are obtained from an Internet Encyclope-
dia,
3
and requires little human effort. We ex-
tracted 159 words from this page, verified them,
and put them into a tag dictionary. We associate
every word in this tag dictionary with the POS
?NR (proper noun)?, and name the lexicon by NR.
The second dictionary was constructed man-
ually, by first employing our baseline tagger to
tag the unlabeled ZX sentences automatically,
3
http://baike.baidu.com/view/18277.htm
592
Model
Target-Domain
Cost
Supervised +Self-Training
Resources SEG POS SEG POS ER
Baseline ? 0 89.77 82.92 90.35 83.95 6.03
Type-Supervision
NR(T) 0 89.84 83.91 91.18 85.22 8.14
3K(T) 5h 91.93 86.53 92.86 87.67 8.46
ORACLE(T) ? 93.10 88.87 94.00 89.91 9.34
Token-Supervision
300(S) 5h 92.59 86.86 93.33 87.85 7.53
600(S) 10h 93.19 88.13 93.81 89.01 7.41
900(S) 15h 93.53 88.53 94.15 89.33 6.97
Combined 3K(T) + 300(S) 10h 93.49 88.54 94.00 89.21 5.85
Type- and Token-Supervision 3K(T) + 600(S) 15h 93.98 89.27 94.61 89.87 5.59
Table 4: Development test results, where Cost denotes the cost of type- or token-annotation measured
by person hours, ER denotes the error reductions of overall performances brought by self-training, T
denotes type-annotation and S denotes token-annotation.
and then randomly selecting the words that are
not domain-independent for an experienced native
speaker to annotate. To facilitate comparison with
token-supervision, we spent about 5 person hours
in annotating 3,000 word-tag pairs, at about the
same cost as annotating 300 sentences. Finally we
conjoined the 3,000 word-tag pairs with the NR
lexicon, and name the resulting lexicon by 3K.
For the target domain, we mark the words from
both NR and 3K as the domain-specific lexicons.
In all experiments, we use the same domain-
independent lexicon, which is extracted from the
source domain training data by HowNet matching.
The accuracies are shown in Table 4, where
the NR lexicon improved the overall F-score
slightly over the baseline, and the larger lexicon
3K brought more significant improvements. These
experiments agree with the intuition that the size
and the coverage of the tag dictionary is impor-
tant to the accuracies. To understand the extent to
which a lexicon can improve the accuracies, we
perform an oracle test, in which lexicons in the
gold-standard test outputs are included in the dic-
tionary. The accuracy is 88.87%.
4.3.2 Comparing Type-Supervised and
Token-Supervised Domain Adaptation
Table 4 shows that the accuracy improvement by
3,000 annotated word-tag pairs (86.53%) is close
to that by 300 annotated sentences (86.86%). This
suggest that using our method, type-supervised
domain adaptation can be a competitive choice to
the token-supervised methods.
The fact that the token-supervised model gives
slightly better results than our type-annotation
method under similar efforts can probably be ex-
0.6 0.7 0.8 0.9 1
0.6
0.7
0.8
0.9
1
Token-Supervision with 300(S)
T
y
p
e
-
S
u
p
e
r
v
i
s
i
o
n
w
i
t
h
3
K
(
T
)
Figure 2: Sentence accuracy comparisons for
type- and token-supervision with equal cost.
plained by the nature of domain differences. Texts
in the Internet novel are different with CTB news
in not only the vocabulary, but also POS n-gram
distributions. The latter cannot be transferred from
the source-domain training data directly. Texts
from domains such as modern-style novels and
scientific articles might have more similar POS
distributions to the CTB data, and can potentially
benefit more from pure lexicons. We leave the ver-
ification of this intuition to future work.
4.3.3 Making Use of Unlabeled Sentences
Both type- and token-supervised domain adapta-
tion methods can be further improved via unla-
beled target sentences. We apply self-training to
both methods, and find improved results across the
board in Table 4. The results indicate that unla-
beled data is useful in further improving both type-
and token-supervised domain adaptation.
593
Interestingly, the effects of the two methods
on self-training are slightly different. The er-
ror reduction by self-training improves from 6.0%
(baseline) to averaged 7.3% and 8.6% for token-
and type-supervised adaptation, respectively. The
better effect for the type-supervised method may
result from comparatively more uniform cover-
age of the lexicon on sentences, since the target-
domain lexicon is annotated by selecting words
from much more than 300 sentences.
4.3.4 Combined Model of Type- and
Token-Supervision
Figure 2 shows the F-scores of each development
test sentence by type- and token-supervised do-
main adaptation with 5 person hours, respectively.
It indicates that the two methods make different
types of errors, and can potentially be used jointly
for better improvements. We conduct a set of ex-
periments as shown in Table 4, finding that the
combined type- and token-supervised model with
lexicon 3K and 300 labeled sentences achieves
an overall accuracy of 88.54%, exceeding the ac-
curacies of both the type-supervised model with
lexicon 3K and the token-supervised model with
300 labeled sentences. Similar observation can
be found for the combined model with lexicon 3K
and 600 labeled sentences. If combined with self-
training, the same fact can be observed.
More interestingly, the combined model also
exceeds pure type- and token-supervised mod-
els with the same annotation cost. For exam-
ple, the combined model with 3K and 300 la-
beled sentences gives a better accuracy than the
token-supervised model with 600 sentences, with
or without self-training. Similar observations hold
between the combined model with 3K and 600 la-
beled sentences and the token-supervised model
with 900 sentences. The results suggest that the
most cost-effective approach for domain adapta-
tion can be combined type- and token-supervision:
after annotating a set of raw sentences, one could
stop to annotate some words, rather than continu-
ing sentence annotation.
4.4 Final Results
Table 5 shows the final results on test corpus
within ten person hours? annotation. With five per-
son hours (lexicon 3K), the type-supervised model
gave an error reduction of 32.99% compared with
the baseline. The best result was obtained by the
combined type- and token-supervised model, with
SEG POS ER Time
Baseline 87.71 80.81 0.00 0
Baseline+Self-Training 88.62 81.94 5.89 0
Type-Supervision
NR(T) 88.34 82.54 9.02 0
NR(T)+ Self-Training 89.52 83.93 16.26 0
3K(T) 91.11 86.04 27.25 5h
3K(T)+Self-Training 92.11 87.14 32.99 5h
Token-Supervision
300(S) 92.44 86.87 31.58 5h
300(S)+Self-Training 93.24 87.48 34.76 5h
600(S) 93.09 88.05 37.73 10h
600(S)+Self-Training 93.77 88.78 41.53 10h
Combined Type- and Token-Supervision
3K(T)+300(S) 93.27 89.03 42.83 10h
3K(T)+300(S)+Self-Training 93.98 89.84 47.06 10h
Table 5: Final results on test set within ten per-
son hours? annotation, where ER denotes the over-
all error reductions compared with the baseline
model, Time denotes the cost of type- or token-
annotation measured by person hours, T denotes
type-annotation and S denotes token-annotation.
an error reduction of 47.06%, higher than that the
token-supervised model with the same cost under
the same setting (the model of 600 labeled sen-
tences with an error reduction of 41.53%). The
results confirm that the type-supervised model
is a competitive alternative for joint segmenta-
tion and POS-tagging under the cross-domain set-
ting. Combined type- and token-supervised model
yields better results than single models.
5 Related Work
As mentioned in the introduction, tag dictionaries
have been applied to type-supervised POS tagging
of English (Toutanova and Johnson, 2007; Gold-
water and Griffiths, 2007; Ravi and Knight, 2009;
Garrette and Baldridge, 2012), Hebrew (Goldberg
et al., 2008), Kinyarwanda and Malagasy (Gar-
rette and Baldridge, 2013; Garrette et al., 2013),
and other languages (T?ackstr?om et al., 2013).
These methods assume that lexicon can be ob-
tained by manual annotation or semi-supervised
learning, and use the lexicon to induce tag se-
quences on unlabeled sentences. We study type-
supervised Chinese POS-tagging, but under the
setting of domain adaptation. The problem is
how to leverage a target domain lexicon and an
available annotated resources in a different source
domain to improving POS-tagging. Consistent
594
with Garrette et al. (2013), we also find that the
type-supervised method is a competitive choice to
token-supervised adaptation.
There has been a line of work on using graph-
based label propagation to expand tag-lexicons for
POS-tagging (Subramanya et al., 2010; Das and
Petrov, 2011). Similar methods have been ap-
plied to character-level Chinese tagging (Zeng et
al., 2013). We found that label propagation from
neither the source domain nor auto-labeled target
domain sentences can improve domain adaptation.
The main reason could be significant domain dif-
ferences. Due to space limitations, we omit this
negative result in our experiments.
With respect to domain adaptation, existing
methods can be classified into three categories.
The first category does not explicitly model dif-
ferences between the source and target domains,
but use standard semi-supervised learning meth-
ods with labeled source domain data and unla-
beled target domain data (Dai et al., 2007; Raina
et al., 2007). The baseline self-training ap-
proach (Liu and Zhang, 2012) belongs to this cat-
egory. The second considers the differences in the
two domains in terms of features (Blitzer et al.,
2006; Daume III, 2007), classifying features into
domain-independent source domain and target do-
main groups and training these types consistently.
The third considers differences between the dis-
tributions of instances in the two domains, treat-
ing them differently (Jiang and Zhai, 2007). Our
type-supervised method is closer to the second cat-
egory. However, rather than splitting features into
domain-independent and domain-specific types,
we use domain-specific dictionaries to capture do-
main differences, and train a model on the source
domain only. Our method can be treated as an ap-
proach specific to the POS-tagging task.
With respect to Chinese lexical analysis, lit-
tle previous work has been reported on using a
tag dictionary to improve joint segmentation and
POS-tagging. There has been work on using a
lexicon in improving segmentation in a Chinese
analysis pipeline. Peng et al. (2004) used fea-
tures from a set of Chinese words and characters
to improve CRF-based segmentation; Low et al.
(2005) extracted features based on a Chinese lex-
icon from Peking University to help a maximum
segmentor; Sun (2011) collected 12,992 idioms
from Chinese dictionaries, and used them for rule-
based pre-segmentation; Hatori et al. (2012) col-
lected Chinese words from HowNet and the Chi-
nese Wikipedia to enhance segmentation accura-
cies of their joint dependency parsing systems. In
comparison with their work, our lexicon contain
additional POS information, and are used for word
segmentation and POS-tagging simultaneously. In
addition, we separate domain-dependent lexicons
for the source and target lexicons, and use a novel
framework to perform domain adaptation.
Wang et al. (2011) collect word-tag statistics
from automatically labeled texts, and use them as
features to improve POS-tagging. Their word-tag
statistics can be treated as a type of lexicon. How-
ever, their efforts differ from ours in several as-
pects: (1) they focus on in-domain POS-tagging,
while our concern is cross-domain tagging; (2)
they study POS-tagging on segmented sentences,
while we investigate joint segmentation and POS-
tagging for Chinese; (3) their tag-dictionaries are
not tag-dictionaries literally, but statistics of word-
tag associations.
6 Conclusions
We performed an empirical study on the use of
tag-dictionaries for the domain adaptation of joint
Chinese segmentation and POS-tagging, showing
that type-supervised methods can be a compet-
itive alternative to token-supervised methods
in cost-effectiveness. In addition, combination
of the two methods gives the best cost-effect.
Finally, we release our annotation of over 4,000
sentences in the Internet literature domain on-
line at http://faculty.sutd.edu.sg/
?
yue_zhang/eacl14meishan.zip as a
free resource for Chinese POS-tagging.
Acknowledgments
We thank the anonymous reviewers for their con-
structive comments. We gratefully acknowl-
edge the support of the National Key Basic
Research Program (973 Program) of China via
Grant 2014CB340503 and the National Natural
Science Foundation of China (NSFC) via Grant
61133012 and 61370164, the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the Singaporean Ministration of
Education Tier 2 grant T2MOE201301 and SRG
ISTD 2012 038 from Singapore University of
Technology and Design.
595
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In COLING.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128, Sydney, Australia, July.
Association for Computational Linguistics.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsu-
pervised POS induction: How far have we come?
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
575?584, Cambridge, MA, October. Association for
Computational Linguistics.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring Naive Bayes Classifiers for
Text Classification. In AAAI, pages 540?545.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Zhendong Dong and Qiang Dong. 2006. Hownet And
the Computation of Meaning. World Scientific Pub-
lishing Co., Inc., River Edge, NJ, USA.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden markov models for part-of-speech
tagging with incomplete tag dictionaries. In
EMNLP-CoNLL, pages 821?831.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 138?147, Atlanta, Georgia, June. Association
for Computational Linguistics.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 583?592, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08: HLT, pages 746?754, Columbus, Ohio, June.
Association for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1045?
1053, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513?521,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In Proceedings of COLING 2012: Posters, pages
745?754, Mumbai, India, December. The COLING
2012 Organizing Committee.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161?164.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Proceeding of CICLing?11.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2).
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277?284, Barcelona, Spain, July. Association
for Computational Linguistics.
596
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceedings
of Coling 2004, pages 562?568, Geneva, Switzer-
land, Aug 23?Aug 27. COLING.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y. Ng. 2007. Self-taught learn-
ing: transfer learning from unlabeled data. In ICML,
pages 759?766.
Sujith Ravi and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
ACL/IJCNLP, pages 504?512.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
167?176, Cambridge, MA, October. Association for
Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385?
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, McDon-
ald Ryan, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging.
In Transactions of the ACL. Association for Compu-
tational Linguistics, March.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In ECML/PKDD (2), pages 442?457.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In NIPS.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Nianwen Xue, Fei Xia, Shizhe Huang, and Tony Kroch.
2000. The bracketing guidelines for the chinese
treebank. Technical report, University of Pennsyl-
vania.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint chinese word segmentation and part-
of-speech tagging. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 770?
779, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL-08: HLT, pages 888?
896, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843?852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
597
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 246?249,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Semantic Role Labeling with Word Sense
Wanxiang Che, Ting Liu and Yongqiang Li
Research Center for Information Retrieval
MOE-Microsoft Key Laboratory of Natural Language Processing and Speech
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, tliu, yqli}@ir.hit.edu.cn
Abstract
Semantic role labeling (SRL) not only needs
lexical and syntactic information, but also
needs word sense information. However, be-
cause of the lack of corpus annotated with
both word senses and semantic roles, there is
few research on using word sense for SRL.
The release of OntoNotes provides an oppor-
tunity for us to study how to use word sense
for SRL. In this paper, we present some novel
word sense features for SRL and find that they
can improve the performance significantly.
1 Introduction
Semantic role labeling (SRL) is a kind of shallow
sentence-level semantic analysis and is becoming a
hot task in natural language processing. SRL aims at
identifying the relations between the predicates in a
sentence and their associated arguments. At present,
the main stream researches are focusing on feature
engineering or combination of multiple results.
Word senses are important information for rec-
ognizing semantic roles. For example, if we know
?cat? is an ?agent? of the predicate ?eat? in a
sentence, we can guess that ?dog? can also be
an ?agent? of ?eat?. Word sense has been suc-
cessfully used in many natural language process-
ing tasks, such as machine translation (Chan et al,
2007; Carpuat and Wu, 2007). CoNLL 2008 shared
task (Surdeanu et al, 2008) first introduced the pred-
icate classification task, which can be regarded as
the predicate sense disambiguation. Meza-Ruiz and
Riedel (2009) has shown that the predicate sense can
improve the final SRL performance. However, there
is few discussion about the concrete influence of all
word senses, i.e. the words besides predicates. The
major reason is lacking the corpus, which is both an-
notated with all word senses and semantic roles.
The release of OntoNotes corpus provides an op-
portunity for us to verify whether all word senses
can help SRL. OntoNotes is a large corpus annotated
with constituency trees (based on Penn Treebank),
predicate argument structures (based on Penn Prop-
Bank) and word senses. It has been used in some
natural language processing tasks, such as joint pars-
ing and named entity recognition (Finkel and Man-
ning, 2009) and word sense disambiguation (Zhong
et al, 2008).
In this paper, we regard the word sense informa-
tion as additional SRL features. We compare three
categories of word sense features (subtree-word re-
lated sense, predicate sense, and sense path) and find
that the subtree-word related sense feature is ineffec-
tive, however, the predicate sense and the sense path
features can improve the SRL performance signifi-
cantly.
2 Data Preparation
In our experiments, we use the OntoNotes Release
2.01 corpus (Hovy et al, 2006). The OntoNotes
project leaders describe it as ?a large, multilingual
richly-annotated corpus constructed at 90% inter-
nanotator agreement.? The corpus has been an-
notated with multiple levels of annotation, includ-
ing constituency trees, predicate argument struc-
ture, word senses, co-reference, and named entities.
For this work, we focus on the constituency trees,
word senses, and predicate argument structures. The
corpus has English and Chinese portions, and we
just use the English portion, which has been split
into seven sections: ABC, CNN, MNB, NBC, PRI,
VOA, and WSJ. These sections represent a mix of
speech and newswire data.
Because we used SRL system based on depen-
dence syntactic trees, we convert the constituency
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2008T04
246
trees into dependence trees with an Constituent-to-
Dependency Conversion Tool2. In addition, we also
convert the OntoNotes sense of each polysemant
into WordNet sense using sense inventory file pro-
vided by OntoNotes 2.0. For an OntoNotes sense
with more than one WordNet sense, we simply use
the foremost (more popular) one.
3 Semantic Role Labeling System
Our baseline is a state-of-the-art SRL system based
on dependency syntactic tree (Che et al, 2009). A
maximum entropy (Berger et al, 1996) classifier is
used to predict the probabilities of a word in the
sentence to be each semantic role. A virtual role
?NULL? (presenting none of roles is assigned) is
added to the roles set, so it does not need seman-
tic role identification stage anymore. For a predi-
cate, two classifiers (one for noun predicates, and
the other for verb predicates) predict probabilities of
each word in a sentence to be each semantic role (in-
cluding virtual role ?NULL?). The features used in
this stage are listed in Table 1.
Feature Description
FirstwordLemma The lemma of the first word in a
subtree
HeadwordLemma The lemma of the head word in
a subtree
HeadwordPOS The POS of the head word in a
subtree
LastwordLemma The lemma of the last word in a
subtree
POSPath The POS path from a word to a
predicate
PathLength The length of a path
Position The relative position of a word
with a predicate
PredicateLemma The lemma of a predicate
RelationPath The dependency relation path
from a word to a predicate
Table 1: Features that are used in SRL.
4 Word Sense for Semantic Role Labeling
From Table 1, we can see that there are lots of lemma
or POS related features. However, the lemma fea-
ture is very sparse and may result in data sparseness
2http://nlp.cs.lth.se/software/treebank converter/
problem. As for the POS, it represents the syntactic
information, but is not enough to distinguish differ-
ent semantic roles. Therefore, we need a kind of new
feature, which is general than the lemma and special
than the POS.
The word sense just satisfies the requirement.
Thus, we will add some new features related with
word sense for SRL. Generally, the original features
can be classified into three categories:
1. Subtree-word related: FirstwordLemma, Last-
wordLemma, HeadwordLemma, and Head-
wordPOS
2. Predicate related: PredicateLemma
3. Word and predicate related: POSPath, Rela-
tionPath, PathLenght, and Position
Correspondingly, we add three categories of word
sense features by replacing Lemma or POS into
Sense, i.e.
1. Subtree-word related sense: FirstwordSense,
LastwordSense, and HeadwordSense
2. Predicate related sense: PredicateSense
3. Word and predicate related sense: SensePath
Three strategies are designed to adopt these
senses:
1. Lemma+Sense: It is the original word
sense representation in OntoNotes, such as
?dog.n.1?. In fact, This is a specialization of
the lemma.
2. Hypernym(n): It is the hypernym of a word
sense, e.g. the hypernym of ?dog.n.1? is ?ca-
nine.n.1?. The n means the level of the hy-
pernym. With the increasing of n, the sense
becomes more and more general. In theory,
however, this strategy may result in inconsis-
tent sense, e.g. word ?dog? and ?canine? have
different hypernyms. The same problem occurs
with Basic Concepts method (Izquierdo et al,
2007).
3. Root Hyper(n): In order to extract more con-
sistent sense, we use the hypernym of a word
sense counting from the root of a sense tree,
e.g. the root hypernym of ?dog.n.1? is ?en-
tity.n.1?. The n means the level of the root hy-
pernym. With the increasing of n, the sense
247
becomes more and more special. Thus, word
?dog? and ?canine? have the same Root Hyper:
?entity?, ?physical entity?, and ?object? with n
= 1, 2, and 3 respectively.
5 Experiments
We will do our experiments on seven of the
OntoNotes English datasets described in Section 2.
For each dataset, we aimed for roughly a 60% train
/ 20% development / 20% test split. See Table 2
for the detailed statistics. In order to examine the
influence of word senses in isolation, we use the hu-
man annotated POS, parse trees, and word senses
provided by OntoNotes. The lemma of each word is
extracted using WordNet tool.
Training Developing Testing
ABC 669 163 138(0001-0040) (0041-0054) (0057-0069)
CNN 1,691 964 1,146(0001-0234) (0235-0331) (0333-0437)
MNB 381 130 125(0001-0015) (0016-0020) (0021-0025)
NBC 351 129 86(0001-0025) (0026-0032) (0033-0039)
PRI 1,205 384 387(0001-0067) (0068-0090) (0091-0112)
VOA 1,238 325 331(0001-0159) (0160-0212) (0213-0264)
WSJ 8,592 2,552 3,432(0020-1446) (1447-1705) (1730-2454)
All 14,127 4,647 5,645
Table 2: Training, developing and testing set sizes for the
seven datasets in sentences. The file ranges (in parenthe-
sis) refer to the numbers within the names of the original
OntoNotes files.
The baseline SRL system without sense informa-
tion is trained with all the training corpus as de-
scribed in Section 3. Its performance on the devel-
opment data is F1 = 85.48%.
Table 3 shows the performance (F1) comparison
on the development data among different sense ex-
tracting strategies with different feature categories.
The numbers are the parameter n used in Hypernym
and Root Hyper strategies.
From Table 3, we can find that:
1. Both of the predicate sense feature and the
sense path feature can improve the performance. For
Subtree-word Predicate Sense
related sense sense path
Lemma+Sense 85.34% 86.16% 85.69%
1 85.41% 86.12% 85.74%
Hypernym(n) 2 85.48% 86.10% 85.74%
3 85.38% 86.10% 85.69%
1 85.35% 86.07% 85.96%
Root Hyper(n) 2 85.45% 86.13% 85.86%
3 85.46% 86.05% 85.91%
Table 3: The performance comparison on the devel-
opment data among different sense extracting strategies
with different feature categories.
the predicate sense feature, we arrive at the same
conclusion with Meza-Ruiz and Riedel (2009). As
for the sense path feature, it is more special than the
POS, therefore, it can enhance the precision.
2. The subtree-word related sense is almost use-
less. The reason is that the original lemma and POS
features have been able to describe the subtree-word
related information. This kind of sense features is
just reduplicate.
3. For different sense feature categories
(columns), the performance is not very seriously af-
fected by different sense extracting strategies (rows).
That is to say, once the sense of a word is disam-
biguated, the sense expressing form is not important
for SRL.
In order to further improve the performance,
we add the predicate sense and the sense path
features simultaneously. Here, we select the
Lemma+Sense strategy for the predicate sense and
the Root Hyper(1) strategy for the sense path. The
final performance achieves F1 = 86.44%, which is
about 1% higher than the baseline (F1 = 85.48%).
Finally, we compare the baseline (without sense)
result with the word sense result on the test data. In
order to see the contribution of correct word senses,
we introduce a simple sense determining strategy,
which use the first (the most popular)WordNet sense
for each word. The final detailed comparison results
are listed in Table 4.
Averagely, both of the methods with the first sense
and the correct sense can perform better than the
baseline. However, the improvement of the method
with the first sense is not significant (?2-test3 with
3http://graphpad.com/quickcalcs/chisquared1.cfm
248
Precision Recall F1
w/o sense 86.25 83.01 84.60
ABC first sense 84.91 81.71 83.28
word sense 87.13 83.40 85.22
w/o sense 86.67 79.97 83.19
CNN first sense 86.94 80.73 83.72
word sense 87.75 80.64 84.05
w/o sense 85.29 81.69 83.45
MNB first sense 85.04 81.85 83.41
word sense 86.96 82.47 84.66
w/o sense 84.49 76.42 80.26
NBC first sense 84.53 76.63 80.38
word sense 86.20 77.44 81.58
w/o sense 86.48 82.29 84.34
PRI first sense 86.82 83.10 84.92
word sense 87.45 83.14 85.24
w/o sense 89.87 86.65 88.23
VOA first sense 90.01 86.60 88.27
word sense 91.35 87.10 89.18
w/o sense 88.38 82.93 85.57
WSJ first sense 88.72 83.29 85.92
word sense 89.25 84.00 86.54
w/o sense 87.85 82.46 85.07
Avg first sense 88.11 82.85 85.40
word sense 88.84 83.37 86.02
Table 4: The testing performance comparison among
the baseline without (w/o) sense information, the method
with the first sense, and the method with the correct word
sense.
? < 0.01). Especially, for some sections, such as
ABC and MNB, it is harmful to the performance. In
contrast, the correct word sense can improve the per-
formance significantly (?2-test with ? < 0.01)and
consistently. These can further prove that the word
sense can enhance the semantic role labeling.
6 Conclusion
This is the first effort to adopt the word sense
features into semantic role labeling. Experiments
show that the subtree-word related sense features
are ineffective, but the predicate sense and the sense
path features can improve the performance signifi-
cantly. In the future, we will use an automatic word
sense disambiguation (WSD) system to obtain word
senses and study the function of WSD for SRL.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via grant
2008AA01Z144, and Natural Scientific Research
Innovation Foundation in Harbin Institute of Tech-
nology (HIT.NSRIF.2009069).
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of EMNLP/CoNLL-2007, pages
61?72, Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL-2007, pages
33?40, Prague, Czech Republic, June.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL-2009, pages 49?54, Boulder,
Colorado, June.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of NAACL/HLT-2009, pages 326?334, Boul-
der, Colorado, June.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of NAACL/HLT-
2006, pages 57?60, New York City, USA, June.
Rube?n Izquierdo, Armando Sua?rez, and German Rigau.
2007. Exploring the automatic selection of basic level
concepts. In Proceedings of RANLP-2007.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In Proceedings of NAACL/HLT-2009,
pages 155?163, Boulder, Colorado, June.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of CoNLL-2008,
pages 159?177, Manchester, England, August.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An em-
pirical study. In Proceedings of EMNLP-2008, pages
1002?1010, Honolulu, Hawaii, October.
249
Proceedings of NAACL-HLT 2013, pages 52?62,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Named Entity Recognition with Bilingual Constraints
Wanxiang Che? Mengqiu Wang? Christopher D. Manning? Ting Liu?
?{car, tliu}@ir.hit.edu.cn
School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China, 150001
?{mengqiu, manning}@stanford.edu
Computer Science Department
Stanford University
Stanford, CA, 94305
Abstract
Different languages contain complementary
cues about entities, which can be used to im-
prove Named Entity Recognition (NER) sys-
tems. We propose a method that formu-
lates the problem of exploring such signals on
unannotated bilingual text as a simple Inte-
ger Linear Program, which encourages entity
tags to agree via bilingual constraints. Bilin-
gual NER experiments on the large OntoNotes
4.0 Chinese-English corpus show that the pro-
posed method can improve strong baselines
for both Chinese and English. In particular,
Chinese performance improves by over 5%
absolute F1 score. We can then annotate a
large amount of bilingual text (80k sentence
pairs) using our method, and add it as up-
training data to the original monolingual NER
training corpus. The Chinese model retrained
on this new combined dataset outperforms the
strong baseline by over 3% F1 score.
1 Introduction
Named Entity Recognition (NER) is an important
task for many applications, such as information ex-
traction and machine translation. State-of-the-art su-
pervised NER methods require large amounts of an-
notated data, which are difficult and expensive to
produce manually, especially for resource-poor lan-
guages.
A promising approach for improving NER per-
formance without annotating more data is to exploit
unannotated bilingual text (bitext), which are rela-
tively easy to obtain for many language pairs, bor-
rowing from the resources made available by statis-
tical machine translation research.1 Different lan-
guages contain complementary cues about entities.
For example, in Figure 1, the word ?? (Ben)? is
common in Chinese but rarely appears as a trans-
lated foreign name. However, its aligned word on
the English side (?Ben?) provides a strong clue that
this is a person name. Judicious use of this type of
bilingual cues can help to recognize errors a mono-
lingual tagger would make, allowing us to produce
more accurately tagged bitext. Each side of the
tagged bitext can then be used to expand the orig-
inal monolingual training dataset, which may lead
to higher accuracy in the monolingual taggers.
Previous work such as Li et al (2012) and Kim
et al (2012) demonstrated that bilingual corpus an-
notated with NER labels can be used to improve
monolingual tagger performance. But a major draw-
back of their approaches are the need for manual
annotation efforts to create such corpora. To avoid
this requirement, Burkett et al (2010) suggested a
?multi-view? learning scheme based on re-ranking.
Noisy output of a ?strong? tagger is used as training
data to learn parameters of a log-linear re-ranking
model with additional bilingual features, simulated
by a ?weak? tagger. The learned parameters are then
reused with the ?strong? tagger to re-rank its own
outputs for unseen inputs. Designing good ?weak?
taggers so that they complement the ?view? of bilin-
gual features in the log-linear re-ranker is crucial to
the success of this algorithm. Unfortunately there is
no principled way of designing such ?weak? taggers.
In this paper, we would like to explore a conceptu-
ally much simpler idea that can also take advantage
1opus.lingfil.uu.se
52
TheO chairmanO ofO theB?ORG FederalI?ORG ReserveI?ORG isO BenB?PER BernankeI?PER
???B?ORG ??O ?O ?B?PER ???I?PER
Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences.
of the large amount of unannotated bitext, without
complicated machinery. More specifically, we in-
troduce a joint inference method that formulates the
bilingual NER tagging problem as an Integer Linear
Program (ILP) and solves it during decoding. We
propose a set of intuitive and effective bilingual con-
straints that encourage NER results to agree across
the two languages.
Experimental results on the OntoNotes 4.0 named
entity annotated Chinese-English parallel corpus
show that the proposed method can improve the
strong Chinese NER baseline by over 5% F1 score
and also give small improvements over the English
baseline. Moreover, by adding the automatically
tagged data to the original NER training corpus
and retraining the monolingual model using an up-
training regimen (Petrov et al, 2010), we can im-
prove the monolingual Chinese NER performance
by over 3% F1 score.
2 Constraint-based Monolingual NER
NER is a sequence labeling task where we assign
a named entity tag to each word in an input sen-
tence. One commonly used tagging scheme is the
BIO scheme. The tag B-X (Begin) represents the
first word of a named entity of type X, for example,
PER (Person) or LOC (Location). The tag I-X (In-
side) indicates that a word is part of an entity but not
first word. The tag O (Outside) is used for all non-
entity words.2 See Figure 1 for an example tagged
sentence.
Conditional Random Fields (CRF) (Lafferty et al,
2001) is a state-of-the-art sequence labeling model
widely used in NER. A first-order linear-chain CRF
2While the performance of NER is measured at the entity
level (not the tag level).
defines the following conditional probability:
PCRF (y|x) =
1
Z(x)
?
i
Mi(yi, yi?1|x) (1)
where x and y are the input and output sequences,
respectively, Z(x) is the partition function, and Mi
is the clique potential for edge clique i. Decoding
in CRF involves finding the most likely output se-
quence that maximizes this objective, and is com-
monly done by the Viterbi algorithm.
Roth and Yih (2005) proposed an ILP inference
algorithm, which can capture more task-specific and
global constraints than the vanilla Viterbi algorithm.
Our work is inspired by Roth and Yih (2005). But
instead of directly solving the shortest-path problem
in the ILP formulation, we re-define the conditional
probability as:
PMAR(y|x) =
?
i
P (yi|x) (2)
where P (yi|x) is the marginal probability given by
an underlying CRF model computed using forward-
backward inference. Since the early HMM litera-
ture, it has been well known that using the marginal
distributions at each position works well, as opposed
to Viterbi MAP sequence labeling (Me?rialdo, 1994).
Our experimental results also supports this claim, as
we will show in Section 6. Our objective is to find
an optimal NER tag sequence:
y? = argmax
y
PMAR(y|x)
= argmax
y
?
i
logP (yi|x) (3)
Then an ILP can be used to solve the inference
problem as classification problem with constraints.
53
The objective function is:
max
|x|?
i=1
?
y?Y
zyi logP
y
i (4)
where Y is the set of all possible named entity tags.
P yi = P (yi = y|x) is the CRF marginal probabil-
ity that the ith word is tagged with y, and zyi is an
indicator that equals 1 iff the ith word is tagged y;
otherwise, zyi is 0.
If no constraints are identified, then Eq. (4)
achieves maximum when all zyi are assigned to 1,
which violates the condition that each word should
only be assigned a single entity tag. We can express
this with constraints:
?i :
?
y?Y
zyi = 1 (5)
After adding the constraints, the probability of the
sequence is maximized when each word is assigned
the tag with highest probability. However, some in-
valid results may still exist. For example a tag O
may be wrongly followed by a tag I-X, although a
named entity cannot start with I-X. Therefore, we
can add the following constraints:
?i,?X : zB-Xi?1 + zI-Xi?1 ? zI-Xi ? 0 (6)
which specifies that when the ith word is tagged with
I-X (zI-Xi = 1), then the previous word can only be
tagged with B-X or I-X (zB-Xi?1 + zI-Xi?1 ? 1).
3 NER with Bilingual Constraints
This section demonstrates how to jointly perform
NER for two languages with bilingual constraints.
We assume sentences have been aligned into pairs,
and the word alignment between each pair of sen-
tences is also given.
3.1 Hard Bilingual Constraints
We first introduce the simplest hard constraints, i.e.,
each word alignment pair should have the same
named entity tag. For example, in Figure 1, the
Chinese word ????? was aligned with the En-
glish words ?the?, ?Federal? and ?Reserve?. There-
fore, they have the same named entity tags ORG.3
3The prefix B- and I- are ignored.
Similarly, ??? and ?Ben? as well as ????? and
?Bernanke? were all tagged with the tag PER.
The objective function for bilingual NER can be
expressed as follows:
max
|xc|?
i=1
?
y?Y
zyi logP
y
i +
|xe|?
j=1
?
y?Y
zyj logP
y
j (7)
where P yi and P
y
j are the probabilities of the i
th Chi-
nese word and jth English word to be tagged with y,
respectively. xc and xe are respectively the Chinese
and English sentences.
Similar to monolingual constrained NER (Sec-
tion 2), monolingual constraints are added for each
language as shown in Eqs. (8) and (9):
?i :
?
y?Y
zyi = 1;?j :
?
y?Y
zyj = 1 (8)
?i,?X : zB-Xi + zI-Xi ? zB-Xi+1 ? 0 (9)
?j,?X : zB-Xj + zI-Xj ? zB-Xj+1 ? 0
Bilingual constraints are added in Eq. (10):
?(i, j) ? A,?X : zB-Xi + zI-Xi = zB-Xj + zI-Xj (10)
where A = {(i, j)} is the word alignment pair set,
i.e., the ith Chinese word and the jth English word
were aligned together. Chinese word i is tagged with
a named entity type X (zB-Xi + zI-Xi = 1), iff English
word j is tagged with X (zB-Xj +zI-Xj = 1). Therefore,
these hard bilingual constraints guarantee that when
two words are aligned, they are tagged with the same
named entity tag.
However, in practice, aligned word pairs do not
always have the same tag because of the difference
in annotation standards across different languages.
For example, in Figure 2(a), the Chinese word ???
?? is a location. However, it is aligned to the words,
?development? and ?zone?, which are not named en-
tities in English. Word alignment error is another se-
rious problem that can cause violation of hard con-
straints. In Figure 2(b), the English word ?Agency?
is wrongly aligned with the Chinese word ?? (re-
port)?. Thus, these two words cannot be assigned
with the same tag.
To address these two problems, we present a prob-
abilistic model for bilingual NER which can lead to
54
ThisO developmentO zoneO isO locatedO inO . . .
??O ???B?LOC ?O ?O . . .
(a) Inconsistent named entity standards
XinhuaB?ORG NewsI?ORG AgencyI?ORG FebruaryO 16thO
???B?ORG ??B?LOC ???O ?O
(b) Word alignment error
Figure 2: Errors of hard bilingual constraints method.
an optimization problem with two soft bilingual con-
straints:
1) allow word-aligned pairs to have different
named entity tags; 2) consider word alignment prob-
abilities to reduce the influence of wrong word align-
ments.
3.2 Soft Constraints with Tag Uncertainty
The new probabilistic model for bilingual NER is:
P (yc,ye|xc,xe, A) =
P (yc,ye,xc,xe, A)
P (xc,xe, A)
= P (yc,xc,xe, A)
P (xc,xe, A)
? P (ye,xc,xe, A)
P (xc,xe, A)
? P (yc,ye,xc,xe, A)P (xc,xe, A)
P (yc,xc,xe, A)P (ye,xc,xe, A)
(11)
? P (yc|xc)P (ye|xe)
P (yc,ye|A)
P (yc|A)P (ye|A)
(12)
where yc and ye respectively denotes Chinese and
English named entity output sequences. A is the set
of word alignment pairs.
If we assume that named entity tag assignments in
Chinese is only dependent on the observed Chinese
sentence, then we can drop the A and xe term in the
first factor of Eq. (11), and arrive at the first factor of
Eq. (12); similarly we can use the same assumption
to derive the second factor in Eq. (12) for English;
alternatively, if we assume the named entity tag as-
signments are only dependent on the cross-lingual
word associations via word alignment, then we can
drop xc and xe terms in the third factor of Eq. (11)
and arrive at the third factor of Eq. (12). These fac-
tors represent the two major sources of information
in the model: monolingual surface observation, and
cross-lingual word associations.
The first two factors of Eq. (12) can be further
decomposed into the product of probabilities of all
words in each language sentence like Eq. (2).
Assuming that the tags are independent between
different word alignment pairs, then the last factor
of Eq. (12) can be decomposed into:
P (yc,ye|A)
P (yc|A)P (ye|A)
=
?
a?A
P (ycayea)
P (yca)P (yea)
=
?
a?A
?ycyea (13)
where yca and yea respectively denotes Chinese and
English named entity tags in a word alignment pair
a. ?ycye = P (ycye)P (yc)P (ye) is the pointwise mutual infor-
mation (PMI) score between a Chinese named en-
tity tag yc and an English named entity tag ye. If
yc = ye, then the score will be high; otherwise the
score will be low. A number of methods for calculat-
ing the scores are provided at the end of this section.
We use ILP to maximize Eq. (12). The new ob-
jective function is expressed as follow:
max
|xc|?
i=1
?
y?Y
zyi logP
y
i +
|xe|?
j=1
?
y?Y
zyj logP
y
j
+
?
a?A
?
yc?Y
?
ye?Y
zycyea log ?ycyea (14)
where zycyea is an indicator that equals 1 iff the Chi-
nese and English named entity tags are yc and ye
respectively, given a word alignment pair a; other-
wise, zycyea is 0.
Monolingual constraints such as Eqs. (8) and (9)
need to be added. In addition, one and only one pos-
sible named entity tag pair exists for a word align-
ment pair. This condition can be expressed as the
following constraints:
?a ? A :
?
yc?Y
?
ye?Y
zycyea = 1 (15)
When the tag pair of a word alignment pair is de-
termined, the corresponding monolingual named en-
55
tity tags can also be identified. This rule can be ex-
pressed by the following constraints:
?a = (i, j) ? A : zycyea ? zyci , zycyea ? z
ye
j (16)
Thus, if zycyea = 1, then zyci and z
ye
j must be both
equal to 1. Here, the ith Chinese word and the jth
English word are aligned together.
In contrast to hard bilingual constraints, inconsis-
tent named entity tags for an aligned word pair are
allowed in soft bilingual constraints, but are given
lower ?ycye scores.
To calculate the ?ycye score, an annotated bilin-
gual NER corpus is consulted. We count from all
word alignment pairs the number of times yc and ye
occur together (C(ycye)) and separately (C(yc) and
C(ye)). Afterwards, ?ycye is calculated with maxi-
mum likelihood estimation as follows:
?ycye = P (ycye)
P (yc)P (ye)
= N ? C(ycye)
C(yc)C(ye)
(17)
where N is the total number of word alignment
pairs.
However, in this paper, we assume that no named
entity annotated bilingual corpus is available. Thus,
the above method is only used as Oracle. A real-
istic method for calculating the ?ycye score requires
the use of two initial monolingual NER models, such
as baseline CRF, to predict named entity tags for
each language on an unannotated bitext. We count
from this automatically tagged corpus the statistics
mentioned above. This method is henceforth re-
ferred to as Auto.
A simpler approach is to manually set the value
of ?ycye : if yc = ye then we assign a larger value
to ?ycye ; else we assign an ad-hoc smaller value. In
fact, if we set ?ycye = 1 iff yc = ye; otherwise,
?ycye = 0, then the soft constraints backs off to hard
constraints. We refer to this set of soft constraints as
Soft-tag.
3.3 Constraints with Alignment Uncertainty
So far, we assumed that a word alignment set A is
known. In practice, only the word alignment proba-
bility Pa for each word pair a is provided. We can
set a threshold ? for Pa to tune the set A: a ? A
iff Pa ? ?. This condition can be regarded as a
kind of hard word alignment. However, the follow-
ing problem exists: the smaller the ?, the noisier the
word alignments are; the larger the ?, the more pos-
sible word alignments are lost. To ameliorate this
problem, we introduce another set of soft bilingual
constraints.
We can re-express Eq. (13) as follows:
?
a?A
?ycyea =
?
a?A
(?ycyea )Ia (18)
where A is the set of all word pairs between two
languages. Ia = 1 iff Pa ? ?; otherwise, Ia = 0.
We can then replace the hard indicator Ia with
the word alignment probability Pa, Eq. (14) is then
transformed into the following equation:
max
|Wc|?
i
?
y?Y
zyi logP
y
i +
|We|?
j
?
y?Y
zyj logP
y
j
+
?
a?A
?
yc?Y
?
ye?Y
zycyea Pa log ?ycyea (19)
We name the set of constraints above
Soft-align, which has the same constraints
as Soft-tag, i.e., Eqs. (8), (9), (15) and (16).
4 Experimental Setup
We conduct experiments on the latest OntoNotes
4.0 corpus (LDC2011T03). OntoNotes is a large,
manually annotated corpus that contains various text
genres and annotations, such as part-of-speech tags,
named entity labels, syntactic parse trees, predicate-
argument structures and co-references (Hovy et al,
2006). Aside from English, this corpus also con-
tains several Chinese and Arabic corpora. Some of
these corpora contain bilingual parallel documents.
We used the Chinese-English parallel corpus with
named entity labels as our development and test
data. This corpus includes about 400 document pairs
(chtb 0001-0325, ectb 1001-1078). We used odd-
numbered documents as development data and even-
numbered documents as test data. We used all other
portions of the named entity annotated corpus as
training data for the monolingual systems. There
were a total of?660 Chinese documents (?16k sen-
tences) and ?1,400 English documents (?39k sen-
tences). OntoNotes annotates 18 named entity types,
such as person, location, date and money. In this
paper, we selected the four most common named
entity types, i.e., PER (Person), LOC (Location),
56
Chinese NER Templates
00: 1 (class bias param)
01: wi+k,?1 ? k ? 1
02: wi+k?1 ? wi+k, 0 ? k ? 1
03: shape(wi+k),?4 ? k ? 4
04: prefix(wi, k), 1 ? k ? 4
05: prefix(wi?1, k), 1 ? k ? 4
06: suffix(wi, k), 1 ? k ? 4
07: suffix(wi?1, k), 1 ? k ? 4
08: radical(wi, k), 1 ? k ? len(wi)
Unigram Features
yi? 00 ? 08
Bigram Features
yi?1 ? yi? 00 ? 08
Table 1: Basic features of Chinese NER.
ORG (Organization) and GPE (Geo-Political Enti-
ties), and discarded the others.
Since the bilingual corpus is only aligned at the
document level, we performed sentence alignment
using the Champollion Tool Kit (CTK).4 After re-
moving sentences with no aligned sentence, a total
of 8,249 sentence pairs were retained.
We used the BerkeleyAligner,5 to produce
word alignments over the sentence-aligned datasets.
BerkeleyAligner also gives posterior probabilities
Pa for each aligned word pair.
We used the CRF-based Stanford NER tagger (us-
ing Viterbi decoding) as our baseline monolingual
NER tool.6 English features were taken from Finkel
et al (2005). Table 1 lists the basic features of
Chinese NER, where ? means string concatenation
and yi is the named entity tag of the ith word wi.
Moreover, shape(wi) is the shape of wi, such as
date and number. prefix/suffix(wi, k) denotes the
k-characters prefix/suffix of wi. radical(wi, k) de-
notes the radical of the kth Chinese character of wi.7
len(wi) is the number of Chinese characters in wi.
To make the baseline CRF taggers stronger, we
added word clustering features to improve gener-
alization over unseen data for both Chinese and
English. Word clustering features have been suc-
cessfully used in several English tasks, including
4champollion.sourceforge.net
5code.google.com/p/berkeleyaligner
6nlp.stanford.edu/software/CRF-NER.shtml,
which has included our English and Chinese NER implementations.
7The radical of a Chinese character can be found at: www.
unicode.org/charts/unihan.html
NER (Miller et al, 2004) and dependency pars-
ing (Koo et al, 2008). To our knowledge, this work
is the first use of word clustering features for Chi-
nese NER. A C++ implementation of the Brown
word clustering algorithms (Brown et al, 1992) was
used to obtain the word clusters (Liang, 2005).8
Raw text was obtained from the fifth edition of Chi-
nese Gigaword (LDC2011T13). One million para-
graphs from Xinhua news section were randomly
selected, and the Stanford Word Segmenter with
LDC standard was applied to segment Chinese text
into words.9 About 46 million words were obtained
which were clustered into 1,000 word classes.
5 Threshold Tuning
During development, we tuned the word alignment
probability thresholds to find the best value. Figure 3
shows the performance curves.
When the word alignment probability threshold ?
is set to 0.9, the hard bilingual constraints perform
well for both Chinese and English. But as the thresh-
olds value gets smaller, and more noisy word align-
ments are introduced, we see the hard bilingual con-
straints method starts to perform badly.
In Soft-tag setting, where inconsistent tag as-
signments within aligned word pairs are allowed but
penalized, different languages have different optimal
threshold values. For example, Chinese has an opti-
mal threshold of 0.7, whereas English has 0.2. Thus,
the optimal thresholds for different languages need
to be selected with care when Soft-tag is applied
in practice.
Soft-align eliminates the need for careful
tuning of word alignment thresholds, and therefore
can be more easily used in practice. Experimen-
tal results of Soft-align confirms our hypothe-
sis ? the performance of both Chinese and English
NER systems improves with decreasing threshold.
However, we can still improve efficiency by set-
ting a low threshold to prune away very unlikely
word alignments. We set the threshold to 0.1 for
Soft-align to increase speed, and we observed
very minimal performance lost when doing so.
We also found that automatically estimated bilin-
gual tag PMI scores (Auto) gave comparable results
8github.com/percyliang/brown-cluster
9nlp.stanford.edu/software/segmenter.shtml
57
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability
55
60
65
70
75
per
form
anc
e (F
1)
HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)
(a) Chinese
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9threshold of word alignment probability
60
65
70
75
80
per
form
anc
e (F
1)
HardSoft-label (Oracle)Soft-label (Auto)Soft-align (Oracle)Soft-align (Auto)
(b) English
Figure 3: Performance curves of different bilingual constraints methods on development set.
to Oracle. Therefore this technique is effective
for computing the PMI scores, avoiding the need of
manually annotating named entity bilingual corpus.
6 Bilingual NER Results
The main results on Chinese and English test sets
with the optimal word alignment threshold for each
method are shown in Table 2.
The CRF-based Chinese NER with and without
word clustering features are compared here. The
word clustering features significantly (p < 0.01) im-
proved the performance of Chinese NER, 10 giving
us a strong Chinese NER baseline.11 The effective-
ness of word clustering for English NER has been
proved in previous work.
The performance of ILP with only monolingual
constraints is quite comparable with the CRF re-
sults, especially on English. The greater ILP perfor-
mance on English is probably due to more accurate
marginal probabilities estimated by the English CRF
model.
The ILP model with hard bilingual constraints
gives a slight performance improvement on Chi-
nese, but affects performance negatively on English.
Once we introduced tagging uncertainties into the
Soft-tag bilingual constraints, we see a very sig-
10We use paired bootstrap resampling significance test (Efron
and Tibshirani, 1993).
11To the best of our knowledge, there was no performance
report of state-of-the-art NER results on the latest OntoNotes
dataset.
nificant (p < 0.01) performance boost on Chinese.
This method also improves the recall on English,
with a smaller decrease in precision. Overall, it im-
proves English F1 score by about 0.4%, which is un-
fortunately not statistically significant.
Compared with Soft-tag, the final
Soft-align method can further improve
performance on both Chinese and English. This is
likely to be because: 1) Soft-align includes
more word alignment pairs, thereby improving
recall; and 2) uses probabilities to cut wrong
word alignments, thereby improving precision. In
particular, compared with the strong CRF baseline,
the gain on Chinese side is almost 5.5% in absolute
F1 score.
Decoding/inferenc efficiency of different methods
are shown in the last column of Table 2.12 Com-
pared with Viterbi decoding in CRF, monolingual
ILP decoding is about 2.3 times slower. Bilingual
ILP decoding, with either hard or soft constraints, is
significantly slower than the monolingual methods.
The reason is that the number of monolingual ILP
constraints doubles, and there are additionally many
more bilingual constraints. The difference in speed
between the Soft-tag and Soft-align meth-
ods is attributed to the difference in number of word
alignment pairs.
Since each sentence pair can be decoded indepen-
12CPU: Intel Xeon E5-2660 2.20GHz. And the speed cal-
culation of ILP inference methods exclude the time needed to
obtain marginal probabilities from the CRF models.
58
Chinese English Speed
P R F1 P R F1 #sent/s
CRF (No Cluster) 74.74 56.17 64.13 ? ? ? ?
CRF (Word Cluster) 76.90 63.32 69.45 82.95 76.67 79.68 317.3
Monolingual ILP 76.20 63.06 69.01 82.88 76.68 79.66 138.0
Hard 74.38 65.78 69.82 82.66 75.36 78.84 21.1
Soft-tag (Auto) 77.37 71.14 74.13 81.36 78.74 80.03 5.9
Soft-align (Auto) 77.71 72.51 75.02 81.94 78.35 80.10 1.5
Table 2: Results on bilingual parallel test set.
dently, parallelization the decoding process can re-
sult in significant speedup.
7 Semi-supervised NER Results
The above results show the usefulness of our method
in a bilingual setting, where we are presented with
sentence aligned data, and are tagging both lan-
guages at the same time. To have a greater impact
on general monolingual NER systems, we employ
a semi-supervised learning setting. First, we tag a
large amount of unannotated bitext with our bilin-
gual constraint-based NER tagger. Then we mix the
automatically tagged results with the original mono-
lingual Chinese training data to train a new model.
Our bitext is derived from the Chinese-English
part of the Foreign Broadcast Information Service
corpus (FBIS, LDC2003E14). The best perform-
ing bilingual model Soft-align with threshold
? = 0.1 was used under the same experimental set-
ting as described in Section 4
Method #sent P R F1
CRF ?16k 76.90 63.32 69.45
Semi
10k 77.60 66.51 71.62
20k 77.28 67.26 71.92
40k 77.40 67.81 72.29
80k 77.44 68.64 72.77
Table 3: Semi-supervised results on Chinese test set.
Table 3 shows that the performance of the semi-
supervised method improves with more additional
data. We simply appended these data to the orig-
inal training data. We also have done the experi-
ments to down weight the additional training data
by duplicating the original training data. There
was some slight improvements, but not very signif-
icant. Finally, when we add 80k sentences, the F1
score is improved by 3.32%, which is significantly
(p < 0.01) better than the baseline, and most of the
contribution comes from recall improvement.
Before the end of experimental section, let us
summarize the usage of different kinds of data re-
sources used in our experiments, as shown in Ta-
ble 4, where  and ? denote whether the corre-
sponding resources are required. In the bilingual
case, during training, only the monolingual named
entity annotated data (NE-mono) is necessary to
train a monolingual NER tagger. During the test,
unannotated bitext (Bitext) is required by the word
aligner and our bilingual NER tagger. Named entity
annotated bitext (NE-bitext) is used to evaluate our
bilingual model. In the semi-supervised case, be-
sides the original NE-mono data, the Bitext is used
as input to our bilingual NER tagger to product ad-
ditional training data. To evaluate the final NER
model, only NE-mono is needed.
NE-mono Bitext NE-bitext
Bilingual
train  ? ?
test ?  
Semi
train   ?
test  ? ?
Table 4: Summarization of the data resource usage
8 Related Work
Previous work explored the use of bilingual corpora
to improve existing monolingual analyzers. Huang
et al (2009) proposed methods to improve parsing
performance using bilingual parallel corpus. Li et
al. (2012) jointly labeled bilingual named entities
with a cyclic CRF model, where approximate in-
ference was done using loopy belief propagation.
These methods require manually annotated bilingual
59
corpora, which are expensive to construct, and hard
to obtain. Kim et al (2012) proposed a method of
labeling bilingual corpora with named entity labels
automatically based on Wikipedia. However, this
method is restricted to topics covered by Wikipedia.
Similar to our work, Burkett et al (2010) also as-
sumed that annotated bilingual corpora are scarce.
Beyond the difference discussed in Section 1, their
re-ranking strategy may lose the correct named en-
tity results if they are not included in the top-N out-
puts. Furthermore, we consider the word alignment
probabilities in our method which can reduce the in-
fluence of word alignment errors. Finally, we test
our method on a large standard publicly available
corpus (8,249 sentences), while they used a much
smaller (200 sentences) manually annotated bilin-
gual NER corpus for results validation.
In addition to bilingual corpora, bilingual dictio-
naries are also useful resources. Huang and Vo-
gel (2002) and Chen et al (2010) proposed ap-
proaches for extracting bilingual named entity pairs
from unannotated bitext, in which verification is
based on bilingual named entity dictionaries. How-
ever, large-scale bilingual named entity dictionaries
are difficult to obtain for most language pairs.
Yarowsky and Ngai (2001) proposed a projection
method that transforms high-quality analysis results
of one language, such as English, into other lan-
guages on the basis of word alignment. Das and
Petrov (2011) applied the above idea to part-of-
speech tagging with a more complex model. Fu et al
(2011) projected English named entities onto Chi-
nese by carefully designed heuristic rules. Although
this type of method does not require manually an-
notated bilingual corpora or dictionaries, errors in
source language results, wrong word alignments and
inconsistencies between the languages limit applica-
tion of this method.
Constraint-based monolingual methods by using
ILP have been successfully applied to many natural
language processing tasks, such as Semantic Role
Labeling (Punyakanok et al, 2004), Dependency
Parsing (Martins et al, 2009) and Textual Entail-
ment (Berant et al, 2011). Zhuang and Zong (2010)
proposed a joint inference method for bilingual se-
mantic role labeling with ILP. However, their ap-
proach requires training an alignment model with a
manually annotated corpus.
9 Conclusions
We proposed a novel ILP based inference algorithm
with bilingual constraints for NER. This method
can jointly infer bilingual named entities without
using any annotated bilingual corpus. We in-
vestigate various bilingual constraints: hard and
soft constraints. Out empirical study on large-
scale OntoNotes Chinese-English parallel NER data
showed that Soft-align method, which allows
inconsistent named entity tags between two aligned
words and considers word alignment probabilities,
can significantly improve over the performance of
a strong Chinese NER baseline. Our work is the
first to evaluate performance on a large-scale stan-
dard dataset. Finally, we can also improve mono-
lingual Chinese NER performance significantly, by
combining the original monolingual training data
with new data obtained from bitext tagged by our
method. The final ILP-based bilingual NER tag-
ger with soft constraints is publicly available at:
github.com/carfly/bi_ilp
Future work could apply the bilingual constraint-
based method to other tasks, such as part-of-speech
tagging and relation extraction.
Acknowledgments
The authors would like to thank Rob Voigt and the
three anonymous reviewers for their valuable com-
ments and suggestions. We gratefully acknowledge
the support of the National Natural Science Foun-
dation of China (NSFC) via grant 61133012, the
National ?863? Project via grant 2011AA01A207
and 2012AA011102, the Ministry of Education Re-
search of Social Sciences Youth funded projects
via grant 12YJCZH304, the Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Lan-
guage Translation (BOLT) program through IBM.
Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the view of
the DARPA, AFRL, or the US government.
60
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 610?619, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479, December.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 46?54, Uppsala, Sweden, July.
Association for Computational Linguistics.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010.
On jointly recognizing and aligning bilingual named
entities. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 631?639, Uppsala, Sweden, July. Association
for Computational Linguistics.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600?609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 363?370, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Ruiji Fu, Bing Qin, and Ting Liu. 2011. Generating
chinese named entity data from a parallel corpus. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 264?272, Chiang
Mai, Thailand, November. Asian Federation of Natural
Language Processing.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, NAACL-Short ?06,
pages 57?60, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In Proceedings of the 4th IEEE International
Conference on Multimodal Interfaces, ICMI 2002,
Washington, DC, USA. IEEE Computer Society.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1222?1231, Singapore, August. Association for
Computational Linguistics.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 694?702, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Shankar Kumar. 2005. Minimum bayes-risk techniques
in automatic speech recognition and statistical ma-
chine translation. Ph.D. thesis, Baltimore, MD, USA.
AAI3155633.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and Fei
Huang. 2012. Joint bilingual name tagging for paral-
lel corpora. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM 2012), Honolulu, Hawaii, Octo-
ber.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, MIT.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Bernard Me?rialdo. 1994. Tagging english text with a
probabilistic model. Comput. Linguist., 20(2):155?
171.
61
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 337?342, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer lin-
ear programming inference. In Proceedings of Coling
2004, pages 1346?1352, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of the 22nd international conference on
Machine learning, ICML ?05, pages 736?743, New
York, NY, USA. ACM.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the second meeting of the North American Chapter of
the Association for Computational Linguistics on Lan-
guage technologies, NAACL ?01, pages 1?8, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Tao Zhuang and Chengqing Zong. 2010. Joint inference
for bilingual semantic role labeling. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 304?314, Cambridge,
MA, October. Association for Computational Linguis-
tics.
62
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675?684,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Multiple Treebanks for Parsing with Quasi-synchronous
Grammars
Zhenghua Li, Ting Liu?, Wanxiang Che
Research Center for Social Computing and Information Retrieval
School of Computer Science and Technology
Harbin Institute of Technology, China
{lzh,tliu,car}@ir.hit.edu.cn
Abstract
We present a simple and effective framework
for exploiting multiple monolingual treebanks
with different annotation guidelines for pars-
ing. Several types of transformation patterns
(TP) are designed to capture the systematic an-
notation inconsistencies among different tree-
banks. Based on such TPs, we design quasi-
synchronous grammar features to augment the
baseline parsing models. Our approach can
significantly advance the state-of-the-art pars-
ing accuracy on two widely used target tree-
banks (Penn Chinese Treebank 5.1 and 6.0)
using the Chinese Dependency Treebank as
the source treebank. The improvements are
respectively 1.37% and 1.10% with automatic
part-of-speech tags. Moreover, an indirect
comparison indicates that our approach also
outperforms previous work based on treebank
conversion.
1 Introduction
The scale of available labeled data significantly af-
fects the performance of statistical data-driven mod-
els. As a structural classification problem that is
more challenging than binary classification and se-
quence labeling problems, syntactic parsing is more
prone to suffer from the data sparseness problem.
However, the heavy cost of treebanking typically
limits one single treebank in both scale and genre.
At present, learning from one single treebank seems
inadequate for further boosting parsing accuracy.1
?Correspondence author: tliu@ir.hit.edu.cn
1Incorporating an increased number of global features, such
as third-order features in graph-based parsers, slightly affects
parsing accuracy (Koo and Collins, 2010; Li et al, 2011).
Treebanks # of Words Grammar
CTB5 0.51 million Phrase structure
CTB6 0.78 million Phrase structure
CDT 1.11 million Dependency structure
Sinica 0.36 million Phrase structure
TCT about 1 million Phrase structure
Table 1: Several publicly available Chinese treebanks.
Therefore, studies have recently resorted to other re-
sources for the enhancement of parsing models, such
as large-scale unlabeled data (Koo et al, 2008; Chen
et al, 2009; Bansal and Klein, 2011; Zhou et al,
2011), and bilingual texts or cross-lingual treebanks
(Burkett and Klein, 2008; Huang et al, 2009; Bur-
kett et al, 2010; Chen et al, 2010).
The existence of multiple monolingual treebanks
opens another door for this issue. For example, ta-
ble 1 lists a few publicly available Chinese treebanks
that are motivated by different linguistic theories or
applications. In the current paper, we utilize the
first three treebanks, i.e., the Chinese Penn Tree-
bank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al, 2005),
and the Chinese Dependency Treebank (CDT) (Liu
et al, 2006). The Sinica treebank (Chen et al, 2003)
and the Tsinghua Chinese Treebank (TCT) (Qiang,
2004) can be similarly exploited with our proposed
approach, which we leave as future work.
Despite the divergence of annotation philosophy,
these treebanks contain rich human knowledge on
the Chinese syntax, thereby having a great deal of
common ground. Therefore, exploiting multiple
treebanks is very attractive for boosting parsing ac-
curacy. Figure 1 gives an example with different an-
675
??1 ??2 ?3 ??4
VV NN CC NN
promote trade and industry
v n c n
OBJ
NMOD
NMOD
VOB COO
LAD
w0
ROOT
ROOT
Figure 1: Example with annotations from CTB5 (upper)
and CDT (under).
notations from CTB5 and CDT.2 This example illus-
trates that the two treebanks annotate coordination
constructions differently. In CTB5, the last noun is
the head, whereas the first noun is the head in CDT.
One natural idea for multiple treebank exploita-
tion is treebank conversion. First, the annotations
in the source treebank are converted into the style
of the target treebank. Then, both the converted
treebank and the target treebank are combined. Fi-
nally, the combined treebank are used to train a
better parser. However, the inconsistencies among
different treebanks are normally nontrivial, which
makes rule-based conversion infeasible. For exam-
ple, a number of inconsistencies between CTB5 and
CDT are lexicon-sensitive, that is, they adopt dif-
ferent annotations for some particular lexicons (or
word senses). Niu et al (2009) use sophisticated
strategies to reduce the noises of the converted tree-
bank after automatic treebank conversion.
The present paper proposes a simple and effective
framework for this problem. The proposed frame-
work avoids directly addressing the difficult anno-
tation transformation problem, but focuses on mod-
eling the annotation inconsistencies using transfor-
mation patterns (TP). The TPs are used to compose
quasi-synchronous grammar (QG) features, such
that the knowledge of the source treebank can in-
spire the target parser to build better trees. We con-
duct extensive experiments using CDT as the source
treebank to enhance two target treebanks (CTB5 and
CTB6). Results show that our approach can signifi-
cantly boost state-of-the-art parsing accuracy. More-
over, an indirect comparison indicates that our ap-
2CTB5 is converted to dependency structures following the
standard practice of dependency parsing (Zhang and Clark,
2008b). Notably, converting a phrase-structure tree into its
dependency-structure counterpart is straightforward and can be
performed by applying heuristic head-finding rules.
proach also outperforms the treebank conversion ap-
proach of Niu et al (2009).
2 Related Work
The present work is primarily inspired by Jiang et
al. (2009) and Smith and Eisner (2009). Jiang et al
(2009) improve the performance of word segmen-
tation and part-of-speech (POS) tagging on CTB5
using another large-scale corpus of different annota-
tion standards (People?s Daily). Their framework is
similar to ours. However, handling syntactic anno-
tation inconsistencies is significantly more challeng-
ing in our case of parsing. Smith and Eisner (2009)
propose effective QG features for parser adaptation
and projection. The first part of their work is closely
connected with our work, but with a few impor-
tant differences. First, they conduct simulated ex-
periments on one treebank by manually creating a
few trivial annotation inconsistencies based on two
heuristic rules. They then focus on better adapting a
parser to a new annotation style with few sentences
of the target style. In contrast, we experiment with
two real large-scale treebanks, and boost the state-
of-the-art parsing accuracy using QG features. Sec-
ond, we explore much richer QG features to fully
exploit the knowledge of the source treebank. These
features are tailored to the dependency parsing prob-
lem. In summary, the present work makes substan-
tial progress in modeling structural annotation in-
consistencies with QG features for parsing.
Previous work on treebank conversion primar-
ily focuses on converting one grammar formalism
of a treebank into another and then conducting a
study on the converted treebank (Collins et al, 1999;
Xia et al, 2008). The work by Niu et al (2009)
is, to our knowledge, the only study to date that
combines the converted treebank with the existing
target treebank. They automatically convert the
dependency-structure CDT into the phrase-structure
style of CTB5 using a statistical constituency parser
trained on CTB5. Their experiments show that
the combined treebank can significantly improve
the performance of constituency parsers. However,
their method requires several sophisticated strate-
gies, such as corpus weighting and score interpo-
lation, to reduce the influence of conversion errors.
Instead of using the noisy converted treebank as ad-
ditional training data, our approach allows the QG-
676
enhanced parsing models to softly learn the system-
atic inconsistencies based on QG features, making
our approach simpler and more robust.
Our approach is also intuitively related to stacked
learning (SL), a machine learning framework that
has recently been applied to dependency parsing
to integrate two main-stream parsing models, i.e.,
graph-based and transition-based models (Nivre and
McDonald, 2008; Martins et al, 2008). However,
the SL framework trains two parsers on the same
treebank and therefore does not need to consider the
problem of annotation inconsistencies.
3 Dependency Parsing
Given an input sentence x = w0w1...wn and its POS
tag sequence t = t0t1...tn, the goal of dependency
parsing is to build a dependency tree as depicted in
Figure 1, denoted by d = {(h,m, l) : 0 ? h ?
n, 0 < m ? n, l ? L}, where (h,m, l) indicates an
directed arc from the head word (also called father)
wh to the modifier (also called child or dependent)
wm with a dependency label l, and L is the label set.
We omit the label l because we focus on unlabeled
dependency parsing in the present paper. The artifi-
cial node w0, which always points to the root of the
sentence, is used to simplify the formalizations.
In the current research, we adopt the graph-based
parsing models for their state-of-the-art performance
in a variety of languages.3 Graph-based models
view the problem as finding the highest scoring tree
from a directed graph. To guarantee the efficiency of
the decoding algorithms, the score of a dependency
tree is factored into the scores of some small parts
(subtrees).
Scorebs(x, t,d) = wbs ? fbs(x, t,d)
=
?
p?d
wpart ? fpart(x, t, p)
where p is a scoring part which contains one or more
dependencies of d, and fbs(.) denotes the basic pars-
ing features, as opposed to the QG features. Figure
2 lists the scoring parts used in our work, where g,
h, m, and s, are word indices.
We implement three parsing models of varying
strengths in capturing features to better understand
the effect of the proposed QG features.
3Our approach can equally be applied to transition-based
parsing models (Yamada and Matsumoto, 2003; Nivre, 2003)
with minor modifications.
dependency sibling grandparent
h
m
h
ms
h
m
g
Figure 2: Scoring parts used in our graph-based parsing
models.
? The first-order model (O1) only incorporates
dependency parts (McDonald et al, 2005), and
requires O(n3) parsing time.
? The second-order model using only sibling
parts (O2sib) includes both dependency and
sibling parts (McDonald and Pereira, 2006),
and needs O(n3) parsing time.
? The second-order model (O2) uses all the
scoring parts in Figure 2 (Koo and Collins,
2010). The time complexity of the decoding
algorithm is O(n4).4
For the O2 model, the score function is rewritten as:
Scorebs(x, t,d) =
?
{(h,m)}?d
wdep ? fdep(x, t, h,m)
+
?
{(h,s),(h,m)}?d
wsib ? fsib(x, t, h, s,m)
+
?
{(g,h),(h,m)}?d
wgrd ? fgrd(x, t, g, h,m)
where fdep(.), fsib(.) and fgrd(.) correspond to the
features for the three kinds of scoring parts. We
adopt the standard features following Li et al
(2011). For the O1 and O2sib models, the above
formula is modified by deactivating the extra parts.
4 Dependency Parsing with QG Features
Smith and Eisner (2006) propose the QG for ma-
chine translation (MT) problems, allowing greater
syntactic divergences between the two languages.
Given a source sentence x? and its syntactic tree
d?, a QG defines a monolingual grammar that gen-
erates translations of x?, which can be denoted by
p(x,d,a|x?,d?), where x and d refer to a translation
and its parse, and a is a cross-language alignment.
Under a QG, any portion of d can be aligned to any
4We use the coarse-to-fine strategy to prune the search
space, which largely accelerates the decoding procedure (Koo
and Collins, 2010).
677
hm
h
m
m
h
Consistent: 55.4% Reverse: 8.6%Sibling: 10.0%Grand: 11.7% Reverse-grand: 1.4%( ', , )dep d h m? ?
( ', , , )grd d g h m? ?
( ', , , )sib d h s m? ?
i
m
h
i
h
m
28.2%
i
mh
h
ms
h
ms
6.7%
i
m
h
s
h
s
i
m
6.4%
i
msh
4.9%
s
m
h
4.4%
m
s
h
4.2%
h
m
g
h
m
g
30.1% 6.5%
h
mg
6.2%
h
m
i
g
6.1%
i
m
h
g
m
h
g
5.4% 5.3%
i
h
g
m
Syntactic Structures of the Corresponding Source SideTarget Side
Figure 4: Most frequent transformation patterns (TPs) when using CDT as the source treebank and CTB5 as the
target. A TP comprises two syntactic structures, one in the source side and the other in the target side, and denotes
the process by which the left-side subtree is transformed into the right-side structure. Functions ?dep(.), ?sib(.), and
?grd(.) return the specific TP type for a candidate scoring part according to the source tree d?.
Source Parser
ParserS
Target Parser
ParserT
Train
Train
Parse
Target 
Treebank
T={(xj, dj)}j
Source Treebank
S={(xi, di)}i
Parsed 
Treebank
TS={(xj, djS)}j
Target Treebank with 
Source Annotations
T+S={(xj, djS, dj)}j
Out
Figure 3: Framework of our approach.
portion of d?, and the construction of d can be in-
spired by arbitrary substructures of d?. To date, QGs
have been successfully applied to various tasks, such
as word alignment (Smith and Eisner, 2006), ma-
chine translation (Gimpel and Smith, 2011), ques-
tion answering (Wang et al, 2007), and sentence
simplification (Woodsend and Lapata, 2011).
In the present work, we utilize the idea of the QG
for the exploitation of multiple monolingual tree-
banks. The key idea is to let the parse tree of one
style inspire the parsing process of another style.
Different from a MT process, our problem consid-
ers one single sentence (x = x?), and the alignment
a is trivial. Figure 3 shows the framework of our
approach. First, we train a statistical parser on the
source treebank, which is called the source parser.
The source parser is then used to parse the whole tar-
get treebank. At this point, the target treebank con-
tains two sets of annotations, one conforming to the
source style, and the other conforming to the target
style. During both the training and test phases, the
target parser are inspired by the source annotations,
and the score of a target dependency tree becomes
Score(x, t,d?,d) =Scorebs(x, t,d)
+Scoreqg(x, t,d?,d)
The first part corresponds to the baseline model,
whereas the second part is affected by the source tree
d? and can be rewritten as
Scoreqg(x, t,d?,d) = wqg ? fqg(x, t,d?,d)
where fqg(.) denotes the QG features. We expect the
QG features to encourage or penalize certain scor-
ing parts in the target side according to the source
tree d?. Taking Figure 1 as an example, suppose
that the upper structure is the target. The target
parser can raise the score of the candidate depen-
dence ?and? ? ?industry?, because the depen-
678
dency also appears in the source structure, and ev-
idence in the training data shows that both annota-
tion styles handle conjunctions in the same manner.
Similarly, the parser may add weight to ?trade? ?
?industry?, considering that the reverse arc is in
the source structure. Therefore, the QG-enhanced
model must learn the systematic consistencies and
inconsistencies from the training data.
To model such consistency or inconsistency sys-
tematicness, we propose the use of TPs for encoding
the structural correspondence between the source
and target styles. Figure 4 presents the three kinds
of TPs used in our model, which correspond to the
three scoring parts of our parsing models.
Dependency TPs shown in the first row consider
how one dependency in the target side is trans-
formed in the source annotations. We only consider
the five cases shown in the figure. The percentages
in the lower boxes refer to the proportion of the
corresponding pattern, which are counted from the
training data of the target treebank with source anno-
tations T+S . We can see that the noisy source struc-
tures and the gold-standard target structures have
55.4% common dependencies. If the source struc-
ture does not belong to any of the listed five cases,
?dep(d?, h,m) returns ?else? (12.9%). We could
consider more complex structures, such as h being
the grand grand father of m, but statistics show that
more complex transformations become very scarce
in the training data.
For the reason that dependency TPs can only
model how one dependency in the target structure is
transformed, we consider more complex transforma-
tions for the other two kinds of scoring parts of the
target parser, i.e., the sibling and grand TPs shown
in the bottom two rows. We only use high-frequency
TPs of a proportion larger than 1.0%, aggregate oth-
ers as ?else?, which leaves us with 21 sibling TPs
and 22 grand TPs.
Based on these TPs, we propose the QG fea-
tures for enhancing the baseline parsing models,
which are shown in Table 2. The type of the
TP is conjoined with the related words and POS
tags, such that the QG-enhanced parsing models can
make more elaborate decisions based on the context.
Then, the score contributed by the QG features can
be redefined as
Scoreqg(x, t,d?,d) =
?
{(h,m)}?d
wqg-dep ? fqg-dep(x, t,d?, h,m)
+
?
{(h,s),(h,m)}?d
wqg-sib ? fqg-sib(x, t,d?, h, s,m)
+
?
{(g,h),(h,m)}?d
wqg-grd ? fqg-grd(x, t,d?, g, h,m)
which resembles the baseline model and can be nat-
urally handled by the decoding algorithms.
5 Experiments and Analysis
We use the CDT as the source treebank (Liu et
al., 2006). CDT consists of 60,000 sentences from
the People?s Daily in 1990s. For the target tree-
bank, we use two widely used versions of Penn Chi-
nese Treebank, i.e., CTB5 and CTB6, which con-
sist of Xinhua newswire, Hong Kong news and ar-
ticles from Sinarama news magazine (Xue et al,
2005). To facilitate comparison with previous re-
sults, we follow Zhang and Clark (2008b) for data
split and constituency-to-dependency conversion of
CTB5. CTB6 is used as the Chinese data set in the
CoNLL 2009 shared task (Hajic? et al, 2009). There-
fore, we adopt the same setting.
CDT and CTB5/6 adopt different POS tag sets,
and converting from one tag set to another is difficult
(Niu et al, 2009).5 To overcome this problem, we
use the People?s Daily corpus (PD),6 a large-scale
corpus annotated with word segmentation and POS
tags, to train a statistical POS tagger. The tagger
produces a universal layer of POS tags for both the
source and target treebanks. Based on the common
tags, the source parser projects the source annota-
tions into the target treebanks. PD comprises ap-
proximately 300 thousand sentences of with approx-
imately 7 million words from the first half of 1998
of People?s Daily.
Table 3 summarizes the data sets used in the
present work. CTB5X is the same with CTB5 but
follows the data split of Niu et al (2009). We use
CTB5X to compare our approach with their treebank
conversion method (see Table 9).
5The word segmentation standards of the two treebanks also
slightly differs, which are not considered in this work.
6http://icl.pku.edu.cn/icl_groups/
corpustagging.asp
679
fqg-dep(x, t,d?, h,m) fqg-sib(x, t,d?, h, s,m) fqg-grd(x, t,d?, g, h,m)
?dir(h,m) ? dist(h,m) ?dir(h,m) ?dir(h,m) ? dir(g, h)
?dep(d?, h,m) ? th ? tm ?sib(d?, h, s,m) ? th ? ts ? tm ?grd(d?, g, h,m) ? tg ? th ? tm
?dep(d?, h,m) ? wh ? tm ?sib(d?, h, s,m) ? wh ? ts ? tm ?grd(d?, g, h,m) ? wg ? th ? tm
?dep(d?, h,m) ? th ? wm ?sib(d?, h, s,m) ? th ? ws ? tm ?grd(d?, g, h,m) ? tg ? wh ? tm
?dep(d?, h,m) ? wh ? wm ?sib(d?, h, s,m) ? th ? ts ? wm ?grd(d?, g, h,m) ? tg ? th ? wm
?sib(d?, h, s,m) ? ts ? tm ?grd(d?, g, h,m) ? tg ? tm
Table 2: QG features used to enhance the baseline parsing models. dir(h,m) denotes the direction of the dependency
(h,m), whereas dist(h,m) is the distance |h ?m|. ?dir(h,m) ? dist(h,m) indicates that the features listed in the
corresponding column are also conjoined with dir(h,m) ? dist(h,m) to form new features.
Corpus Train Dev Test
PD 281,311 5,000 10,000
CDT 55,500 1,500 3,000
CTB5 16,091 803 1,910
CTB5X 18,104 352 348
CTB6 22,277 1,762 2,556
Table 3: Data used in this work (in sentence number).
We adopt unlabeled attachment score (UAS) as
the primary evaluation metric. We also use Root ac-
curacy (RA) and complete match rate (CM) to give
more insights. All metrics exclude punctuation. We
adopt Dan Bikel?s randomized parsing evaluation
comparator for significance test (Noreen, 1989).7
For all models used in current work (POS tagging
and parsing), we adopt averaged perceptron to train
the feature weights (Collins, 2002). We train each
model for 10 iterations and select the parameters that
perform best on the development set.
5.1 Preliminaries
This subsection describes how we project the source
annotations into the target treebanks. First, we train
a statistical POS tagger on the training set of PD,
which we name TaggerPD .8 The tagging accuracy
on the test set of PD is 98.30%.
We then use TaggerPD to produce POS tags for
all the treebanks (CDT, CTB5, and CTB6).
Based on the common POS tags, we train a
second-order source parser (O2) on CDT, denoted
by ParserCDT . The UAS on CDT-test is 84.45%.
We then use ParserCDT to parse CTB5 and CTB6.
7http://www.cis.upenn.edu/[normal-wave
?
]
dbikel/software.html
8We adopt the Chinese-oriented POS tagging features pro-
posed in Zhang and Clark (2008a).
Models without QG with QG
O2 86.13 86.44 (+0.31, p = 0.06)
O2sib 85.63 86.17 (+0.54, p = 0.003)
O1 83.16 84.40 (+1.24, p < 10?5)
Li11 86.18 ?
Z&N11 86.00 ?
Table 4: Parsing accuracy (UAS) comparison on CTB5-
test with gold-standard POS tags. Li11 refers to the
second-order graph-based model of Li et al (2011),
whereas Z&N11 is the feature-rich transition-based
model of Zhang and Nivre (2011).
At this point, both CTB5 and CTB6 contain depen-
dency structures conforming to the style of CDT.
5.2 CTB5 as the Target Treebank
Table 4 shows the results when the gold-standard
POS tags of CTB5 are adopted by the parsing mod-
els. We aim to analyze the efficacy of QG features
under the ideal scenario wherein the parsing mod-
els suffer from no error propagation of POS tag-
ging. We determine that our baseline O2 model
achieves comparable accuracy with the state-of-the-
art parsers. We also find that QG features can
boost the parsing accuracy by a large margin when
the baseline parser is weak (O1). The improve-
ment shrinks for stronger baselines (O2sib and O2).
This phenomenon is understandable. When gold-
standard POS tags are available, the baseline fea-
tures are very reliable and the QG features becomes
less helpful for more complex models. The p-values
in parentheses present the statistical significance of
the improvements.
We then turn to the more realistic scenario
wherein the gold-standard POS tags of the target
treebank are unavailable. We train a POS tagger on
the training set of CTB5 to produce the automatic
680
Models without QG with QG
O2 79.67 81.04 (+1.37)
O2sib 79.25 80.45 (+1.20)
O1 76.73 79.04 (+2.31)
Li11 joint 80.79 ?
Li11 pipeline 79.29 ?
Table 5: Parsing accuracy (UAS) comparison on CTB5-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p < 10?5).
Setting UAS CM RA
fbs(.) 79.67 26.81 73.82
fqg(.) 79.15 26.34 74.71
fbs(.) + fqg(.) 81.04 29.63 77.17
fbs(.) + fqg-dep(.) 80.82 28.80 76.28
fbs(.) + fqg-sib(.) 80.86 28.48 76.18
fbs(.) + fqg-grd(.) 80.88 28.90 76.34
Table 6: Feature ablation for Parser-O2 on CTB5-test
with automatic POS tags.
POS tags for the development and test sets of CTB5.
The tagging accuracy is 93.88% on the test set. The
automatic POS tags of the training set are produced
using 10-fold cross-validation.9
Table 5 shows the results. We find that QG fea-
tures result in a surprisingly large improvement over
the O1 baseline and can also boost the state-of-
the-art parsing accuracy by a large margin. Li et
al. (2011) show that a joint POS tagging and de-
pendency parsing model can significantly improve
parsing accuracy over a pipeline model. Our QG-
enhanced parser outperforms their best joint model
by 0.25%. Moreover, the QG features can be used to
enhance a joint model and achieve higher accuracy,
which we leave as future work.
5.3 Analysis Using Parser-O2 with AUTO-POS
We then try to gain more insights into the effect of
the QG features through detailed analysis. We se-
lect the state-of-the-art O2 parser and focus on the
realistic scenario with automatic POS tags.
Table 6 compares the efficacy of different feature
sets. The first major row analyzes the efficacy of
9We could use the POS tags produced by TaggerPD in Sec-
tion 5.1, which however would make it difficult to compare our
results with previous ones. Moreover, inferior results may be
gained due to the differences between CTB5 and PD in word
segmentation standards and text sources.
the basic features fbs(.) and the QG features fqg(.).
When using the few QG features in Table 2, the ac-
curacy is very close to that when using the basic
features. Moreover, using both features generates
a large improvement. The second major row com-
pares the efficacy of the three kinds of QG features
corresponding to the three types of scoring parts. We
can see that the three feature sets are similarly effec-
tive and yield comparable accuracies. Combining
these features generate an additional improvement
of approximately 0.2%. These results again demon-
strate that all the proposed QG features are effective.
Figure 5 describes how the performance varies
when the scale of CTB5 and CDT changes. In
the left subfigure, the parsers are trained on part
of the CTB5-train, and ?16? indicates the use of
all the training instances. Meanwhile, the source
parser ParserCDT is trained on the whole CDT-
train. We can see that QG features render larger
improvement when the target treebank is of smaller
scale, which is quite reasonable. More importantly,
the curves indicate that a QG-enhanced parser
trained on a target treebank of 16,000 sentences
may achieve comparable accuracy with a base-
line parser trained on a treebank that is double
the size (32,000), which is very encouraging.
In the right subfigure, the target treebank is
trained on the whole CTB5-train, whereas the source
parser is trained on part of the CDT-train, and ?55.5?
indicates the use of all. The curve clearly demon-
strates that the QG features are more helpful when
the source treebank gets larger, which can be ex-
plained as follows. A larger source treebank can
teach a source parser of higher accuracy; then, the
better source parser can parse the target treebank
more reliably; and finally, the target parser can better
learn the annotation divergences based on QG fea-
tures. These results demonstrate the effectiveness
and stability of our approach.
Table 7 presents the detailed effect of the QG fea-
tures on different dependency patterns. A pattern
?VV ? NN? refers to a right-directed dependency
with the head tagged as ?VV? and the modifier
tagged as ?NN?. whereas ??? means left-directed.
The ?w/o QG? column shows the number of the cor-
responding dependency pattern that appears in the
gold-standard trees but misses in the results of the
baseline parser, whereas the signed figures in the
?+QG? column are the changes made by the QG-
681
71
72
73
74
75
76
77
78
79
80
81
82
1 2 4 8 16
Training Set Size of CTB5
w/o QG
with QG
79.4
79.6
79.8
80
80.2
80.4
80.6
80.8
81
81.2
0 3 6 12 24 55.5
Training Set Size of CDT
with QG
Figure 5: Parsing accuracy (UAS) comparison on CTB5-
test when the scale of CDT and CTB5 varies (thousands
in sentence number).
Dependency w/o QG +QG Descriptions
NN? NN 858 -78 noun modifier or coordinating nouns
VV? VV 777 -41 object clause or coordinating verbs
VV? VV 570 -38 subject clause
VV? NN 509 -79 verb and its object
w0 ? VV 357 -57 verb as sentence root
VV? NN 328 -32 attributive clause
P? VV 278 -37 preposition phrase attachment
VV? DEC 233 -33 attributive clause and auxiliary DE
P? NN 175 -35 preposition and its object
Table 7: Detailed effect of QG features on different de-
pendency patterns.
enhanced parser. We only list the patterns with an
absolute change larger than 30. We find that the QG
features can significantly help a variety of depen-
dency patterns (i.e., reducing the missing number).
5.4 CTB6 as the Target Treebank
We use CTB6 as the target treebank to further verify
the efficacy of our approach. Compared with CTB5,
CTB6 is of larger scale and is converted into de-
pendency structures according to finer-grained head-
finding rules (Hajic? et al, 2009). We directly adopt
the same transformation patterns and features tuned
on CTB5. Table 8 shows results. The improvements
are similar to those on CTB5, demonstrating that our
approach is effective and robust. We list the top three
systems of the CoNLL 2009 shared task in Table 8,
showing that our approach also advances the state-
of-the-art parsing accuracy on this data set.10
10We reproduce their UASs using the data released
by the organizer: http://ufal.mff.cuni.cz/conll2009-st/results/
results.php. The parsing accuracies of the top systems may be
underestimated since the accuracy of the provided POS tags in
CoNLL 2009 is only 92.38% on the test set, while the POS tag-
ger used in our experiments reaches 94.08%.
Models without QG with QG
O2 83.23 84.33 (+1.10)
O2sib 82.87 84.11 (+1.37)
O1 80.29 82.76 (+2.47)
Bohnet (2009) 82.68 ?
Che et al (2009) 82.11 ?
Gesmundo et al (2009) 81.70 ?
Table 8: Parsing accuracy (UAS) comparison on CTB6-
test with automatic POS tags. The improvements shown
in parentheses are all statistically significant (p < 10?5).
Models baseline with another treebank
Ours 84.16 86.67 (+2.51)
GP (Niu et al, 2009) 82.42 84.06 (+1.64)
Table 9: Parsing accuracy (UAS) comparison on the test
set of CTB5X. Niu et al (2009) use the maximum en-
tropy inspired generative parser (GP) of Charniak (2000)
as their constituent parser.
5.5 Comparison with Treebank Conversion
As discussed in Section 2, Niu et al (2009) automat-
ically convert the dependency-structure CDT to the
phrase-structure annotation style of CTB5X and use
the converted treebank as additional labeled data.
We convert their phrase-structure results on CTB5X-
test into dependency structures using the same head-
finding rules. To compare with their results, we
run our baseline and QG-enhanced O2 parsers on
CTB5X. Table 9 presents the results.11 The indirect
comparison indicates that our approach can achieve
larger improvement than their treebank conversion
based method.
6 Conclusions
The current paper proposes a simple and effective
framework for exploiting multiple large-scale tree-
banks of different annotation styles. We design
rich TPs to model the annotation inconsistencies and
consequently propose QG features based on these
TPs. Extensive experiments show that our approach
can effectively utilize the syntactic knowledge from
another treebank and significantly improve the state-
of-the-art parsing accuracy.
11We thank the authors for sharing their results. Niu et al
(2009) also use the reranker (RP) of Charniak and Johnson
(2005) as a stronger baseline, but the results are missing. They
find a less improvement on F score with RP than with GP (0.9%
vs. 1.1%). We refer to their Table 5 and 6 for details.
682
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Major Projects via
grant 2011AA01A207, and the National ?863?
Leading Technology Research Project via grant
2012AA011102.
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 693?702, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Bernd Bohnet. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceedings
of the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2009): Shared Task,
pages 67?72, Boulder, Colorado, June. Association for
Computational Linguistics.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 877?886, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
David Burkett, Slav Petrov, John Blitzer, and Dan Klein.
2010. Learning better monolingual models with unan-
notated bilingual text. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 46?54, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-05, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ANLP?00, pages 132?139.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang
Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In
Proceedings of CoNLL 2009: Shared Task, pages 49?
54.
Keh-Jiann Chen, Chi-Ching Luo, Ming-Chung Chang,
Feng-Yi Chen, Chao-Jan Chen, Chu-Ren Huang, and
Zhao-Ming Gao, 2003. Sinica treebank: Design crite-
ria,representational issues and implementation, chap-
ter 13, pages 231?248. Kluwer Academic Publishers.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
570?579, Singapore, August. Association for Compu-
tational Linguistics.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 21?29, Uppsala, Sweden, July. Association
for Computational Linguistics.
Micheal Collins, Lance Ramshaw, Jan Hajic, and
Christoph Tillmann. 1999. A statistical parser for
czech. In ACL 1999, pages 505?512.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of CoNLL 2009: Shared Task,
pages 37?42.
Kevin Gimpel and Noah A. Smith. 2011. Quasi-
synchronous phrase dependency grammars for ma-
chine translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 474?485, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL
2009.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1222?1231, Singapore, August. Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging ? a case study. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 522?530, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
683
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models
for chinese pos tagging and dependency parsing. In
EMNLP 2011, pages 1180?1191.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building
a dependency treebank for improving Chinese parser.
In Journal of Chinese Language and Computing, vol-
ume 16, pages 207?224.
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
EMNLP?08, pages 157?166.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL 2006.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL 2005, pages 91?98.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 46?54, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL 2008, pages 950?958.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: An introduction. John Wiley &
Sons, Inc., New York. Book (ISBN 0471611360 ).
Zhou Qiang. 2004. Annotation scheme for chinese tree-
bank. Journal of Chinese Information Processing,
18(4):1?8.
David Smith and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntac-
tic dependencies. In Proceedings on the Workshop
on Statistical Machine Translation, pages 23?30, New
York City, June. Association for Computational Lin-
guistics.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 822?831, Singapore, August. Association for
Computational Linguistics.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 22?32,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Kristian Woodsend and Mirella Lapata. 2011. Learning
to simplify sentences with quasi-synchronous gram-
mar and integer programming. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 409?420, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer,
and Dipti Misra. Sharma. 2008. Towards a multi-
representational treebank. In In Proceedings of the 7th
International Workshop on Treebanks and Linguistic
Theories.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of ACL-08: HLT, pages 888?896.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting web-derived selectional preference to im-
prove statistical dependency parsing. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1556?1565, Portland, Oregon, USA,
June. Association for Computational Linguistics.
684
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11?16,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparison of Chinese Parsers for Stanford Dependencies
Wanxiang Che?
car@ir.hit.edu.cn
Valentin I. Spitkovsky?
vals@stanford.edu
Ting Liu?
tliu@ir.hit.edu.cn
?School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China, 150001
?Computer Science Department
Stanford University
Stanford, CA, 94305
Abstract
Stanford dependencies are widely used in nat-
ural language processing as a semantically-
oriented representation, commonly generated
either by (i) converting the output of a con-
stituent parser, or (ii) predicting dependencies
directly. Previous comparisons of the two ap-
proaches for English suggest that starting from
constituents yields higher accuracies. In this
paper, we re-evaluate both methods for Chi-
nese, using more accurate dependency parsers
than in previous work. Our comparison of per-
formance and efficiency across seven popular
open source parsers (four constituent and three
dependency) shows, by contrast, that recent
higher-order graph-based techniques can be
more accurate, though somewhat slower, than
constituent parsers. We demonstrate also that
n-way jackknifing is a useful technique for
producing automatic (rather than gold) part-
of-speech tags to train Chinese dependency
parsers. Finally, we analyze the relations pro-
duced by both kinds of parsing and suggest
which specific parsers to use in practice.
1 Introduction
Stanford dependencies (de Marneffe and Man-
ning, 2008) provide a simple description of rela-
tions between pairs of words in a sentence. This
semantically-oriented representation is intuitive and
easy to apply, requiring little linguistic expertise.
Consequently, Stanford dependencies are widely
used: in biomedical text mining (Kim et al, 2009),
as well as in textual entailment (Androutsopou-
los and Malakasiotis, 2010), information extrac-
tion (Wu and Weld, 2010; Banko et al, 2007) and
sentiment analysis (Meena and Prabhakar, 2007).
In addition to English, there is a Chinese ver-
sion of Stanford dependencies (Chang et al, 2009),
(a) A constituent parse tree.
(b) Stanford dependencies.
Figure 1: A sample Chinese constituent parse tree and its
corresponding Stanford dependencies for the sentence
China (??) encourages (??) private (??)
entrepreneurs (???) to invest (??) in
national (??) infrastructure (??) construction (??).
which is also useful for many applications, such as
Chinese sentiment analysis (Wu et al, 2011; Wu et
al., 2009; Zhuang et al, 2006) and relation extrac-
tion (Huang et al, 2008). Figure 1 shows a sample
constituent parse tree and the corresponding Stan-
ford dependencies for a sentence in Chinese. Al-
though there are several variants of Stanford depen-
dencies for English,1 so far only a basic version (i.e,
dependency tree structures) is available for Chinese.
Stanford dependencies were originally obtained
from constituent trees, using rules (de Marneffe et
al., 2006). But as dependency parsing technolo-
gies mature (Ku?bler et al, 2009), they offer increas-
ingly attractive alternatives that eliminate the need
for an intermediate representation. Cer et al (2010)
reported that Stanford?s implementation (Klein and
Manning, 2003) underperforms other constituent
1nlp.stanford.edu/software/dependencies_manual.pdf
11
Type Parser Version Algorithm URL
Constituent Berkeley 1.1 PCFG code.google.com/p/berkeleyparser
Bikel 1.2 PCFG www.cis.upenn.edu/?dbikel/download.html
Charniak Nov. 2009 PCFG www.cog.brown.edu/?mj/Software.htm
Stanford 2.0 Factored nlp.stanford.edu/software/lex-parser.shtml
Dependency MaltParser 1.6.1 Arc-Eager maltparser.org
Mate 2.0 2nd-order MST code.google.com/p/mate-tools
MSTParser 0.5 MST sourceforge.net/projects/mstparser
Table 1: Basic information for the seven parsers included in our experiments.
parsers, for English, on both accuracy and speed.
Their thorough investigation also showed that con-
stituent parsers systematically outperform parsing
directly to Stanford dependencies. Nevertheless, rel-
ative standings could have changed in recent years:
dependency parsers are now significantly more ac-
curate, thanks to advances like the high-order maxi-
mum spanning tree (MST) model (Koo and Collins,
2010) for graph-based dependency parsing (McDon-
ald and Pereira, 2006). Therefore, we deemed it im-
portant to re-evaluate the performance of constituent
and dependency parsers. But the main purpose of
our work is to apply the more sophisticated depen-
dency parsing algorithms specifically to Chinese.
Number of \in Train Dev Test Total
files 2,083 160 205 2,448
sentences 46,572 2,079 2,796 51,447
tokens 1,039,942 59,955 81,578 1,181,475
Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data.
2 Methodology
We compared seven popular open source constituent
and dependency parsers, focusing on both accuracy
and parsing speed. We hope that our analysis will
help end-users select a suitable method for parsing
to Stanford dependencies in their own applications.
2.1 Parsers
We considered four constituent parsers. They are:
Berkeley (Petrov et al, 2006), Bikel (2004), Char-
niak (2000) and Stanford (Klein and Manning,
2003) chineseFactored, which is also the default
used by Stanford dependencies. The three depen-
dency parsers are: MaltParser (Nivre et al, 2006),
Mate (Bohnet, 2010)2 and MSTParser (McDonald
and Pereira, 2006). Table 1 has more information.
2A second-order MST parser (with the speed optimization).
2.2 Corpus
We used the latest Chinese TreeBank (CTB) 7.0 in
all experiments.3 CTB 7.0 is larger and has more
sources (e.g., web text), compared to previous ver-
sions. We split the data into train/development/test
sets (see Table 2), with gold word segmentation, fol-
lowing the guidelines suggested in documentation.
2.3 Settings
Every parser was run with its own default options.
However, since the default classifier used by Malt-
Parser is libsvm (Chang and Lin, 2011) with a poly-
nomial kernel, it may be too slow for training models
on all of CTB 7.0 training data in acceptable time.
Therefore, we also tested this particular parser with
the faster liblinear (Fan et al, 2008) classifier. All
experiments were performed on a machine with In-
tel?s Xeon E5620 2.40GHz CPU and 24GB RAM.
2.4 Features
Unlike constituent parsers, dependency models re-
quire exogenous part-of-speech (POS) tags, both in
training and in inference. We used the Stanford tag-
ger (Toutanova et al, 2003) v3.1, with the MEMM
model,4 in combination with 10-way jackknifing.5
Word lemmas ? which are generalizations of
words ? are another feature known to be useful
for dependency parsing. Here we lemmatized each
Chinese word down to its last character, since ? in
contrast to English ? a Chinese word?s suffix often
carries that word?s core sense (Tseng et al, 2005).
For example, bicycle (???), car (??) and
train (??) are all various kinds of vehicle (?).
3www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2010T07
4nlp.stanford.edu/software/tagger.shtml
5Training sentences in each fold were tagged using a model
based on the other nine folds; development and test sentences
were tagged using a model based on all ten of the training folds.
12
Dev Test
Type Parser UAS LAS UAS LAS Parsing Time
Constituent Berkeley 82.0 77.0 82.9 77.8 45:56
Bikel 79.4 74.1 80.0 74.3 6,861:31
Charniak 77.8 71.7 78.3 72.3 128:04
Stanford 76.9 71.2 77.3 71.4 330:50
Dependency MaltParser (liblinear) 76.0 71.2 76.3 71.2 0:11
MaltParser (libsvm) 77.3 72.7 78.0 73.1 556:51
Mate (2nd-order) 82.8 78.2 83.1 78.1 87:19
MSTParser (1st-order) 78.8 73.4 78.9 73.1 12:17
Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)
are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen-
eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
3 Results
Table 3 tabulates efficiency and performance for all
parsers; UAS and LAS are unlabeled and labeled at-
tachment scores, respectively ? the standard crite-
ria for evaluating dependencies. They can be com-
puted via a CoNLL-X shared task dependency pars-
ing evaluation tool (without scoring punctuation).6
3.1 Chinese
Mate scored highest, and Berkeley was the most ac-
curate of constituent parsers, slightly behind Mate,
using half of the time. MaltParser (liblinear) was by
far the most efficient but also the least performant; it
scored higher with libsvm but took much more time.
The 1st-order MSTParser was more accurate than
MaltParser (libsvm) ? a result that differs from that
of Cer et al (2010) for English (see ?3.2). The Stan-
ford parser (the default for Stanford dependencies)
was only slightly more accurate than MaltParser (li-
blinear). Bikel?s parser was too slow to be used in
practice; and Charniak?s parser ? which performs
best for English ? did not work well for Chinese.
3.2 English
Our replication of Cer et al?s (2010, Table 1) evalua-
tion revealed a bug: MSTParser normalized all num-
bers to a <num> symbol, which decreased its scores
in the evaluation tool used with Stanford dependen-
cies. After fixing this glitch, MSTParser?s perfor-
mance improved from 78.8 (reported) to 82.5%, thus
making it more accurate than MaltParser (81.1%)
and hence the better dependency parser for English,
consistent with our results for Chinese (see Table 3).
6ilk.uvt.nl/conll/software/eval.pl
Our finding does not contradict the main qualita-
tive result of Cer et al (2010), however, since the
constituent parser of Charniak and Johnson (2005)
still scores substantially higher (89.1%), for English,
compared to all dependency parsers.7 In a separate
experiment (parsing web data),8 we found Mate to
be less accurate than Charniak-Johnson ? and im-
provement from jackknifing smaller ? on English.
4 Analysis
To further compare the constituent and dependency
approaches to generating Stanford dependencies, we
focused on Mate and Berkeley parsers ? the best
of each type. Overall, the difference between their
accuracies is not statistically significant (p > 0.05).9
Table 4 highlights performance (F1 scores) for the
most frequent relation labels. Mate does better on
most relations, noun compound modifiers (nn) and
adjectival modifiers (amod) in particular; and the
Berkeley parser is better at root and dep.10 Mate
seems to excel at short-distance dependencies, pos-
sibly because it uses more local features (even with
a second-order model) than the Berkeley parser,
whose PCFG can capture longer-distance rules.
Since POS-tags are especially informative of Chi-
nese dependencies (Li et al, 2011), we harmonized
training and test data, using 10-way jackknifing (see
?2.4). This method is more robust than training a
7One (small) factor contributing to the difference between
the two languages is that in the Chinese setup we stop with basic
Stanford dependencies ? there is no penalty for further conver-
sion; another is not using discriminative reranking for Chinese.
8sites.google.com/site/sancl2012/home/shared-task
9For LAS, p ? 0.11; and for UAS, p ? 0.25, according to
www.cis.upenn.edu/?dbikel/download/compare.pl
10An unmatched (default) relation (Chang et al, 2009, ?3.1).
13
Relation Count Mate Berkeley
nn 7,783 91.3 89.3
dep 4,651 69.4 70.3
nsubj 4,531 87.1 85.5
advmod 4,028 94.3 93.8
dobj 3,990 86.0 85.0
conj 2,159 76.0 75.8
prep 2,091 94.3 94.1
root 2,079 81.2 82.3
nummod 1,614 97.4 96.7
assmod 1,593 86.3 84.1
assm 1,590 88.9 87.2
pobj 1,532 84.2 82.9
amod 1,440 85.6 81.1
rcmod 1,433 74.0 70.6
cpm 1,371 84.4 83.2
Table 4: Performance (F1 scores) for the fifteen most-
frequent dependency relations in the CTB 7.0 develop-
ment data set attained by both Mate and Berkeley parsers.
parser with gold tags because it improves consis-
tency, particularly for Chinese, where tagging accu-
racies are lower than in English. On development
data, Mate scored worse given gold tags (75.4 versus
78.2%).11 Lemmatization offered additional useful
cues for overcoming data sparseness (77.8 without,
versus 78.2% with lemma features). Unsupervised
word clusters could thus also help (Koo et al, 2008).
5 Discussion
Our results suggest that if accuracy is of primary
concern, then Mate should be preferred;12 however,
Berkeley parser offers a trade-off between accuracy
and speed. If neither parser satisfies the demands
of a practical application (e.g., real-time processing
or bulk-parsing the web), then MaltParser (liblinear)
may be the only viable option. Fortunately, it comes
with much headroom for improving accuracy, in-
cluding a tunable margin parameter C for the classi-
fier, richer feature sets (Zhang and Nivre, 2011) and
ensemble models (Surdeanu and Manning, 2010).
Stanford dependencies are not the only popular
dependency representation. We also considered the
11Berkeley?s performance suffered with jackknifed tags (76.5
versus 77.0%), possibly because it parses and tags better jointly.
12Although Mate?s performance was not significantly better
than Berkeley?s in our setting, it has the potential to tap richer
features and other advantages of dependency parsers (Nivre and
McDonald, 2008) to further boost accuracy, which may be diffi-
cult in the generative framework of a typical constituent parser.
conversion scheme of the Penn2Malt tool,13 used
in a series of CoNLL shared tasks (Buchholz and
Marsi, 2006; Nivre et al, 2007; Surdeanu et al,
2008; Hajic? et al, 2009). However, this tool relies
on function tag information from the CTB in deter-
mining dependency relations. Since these tags usu-
ally cannot be produced by constituent parsers, we
could not, in turn, obtain CoNLL-style dependency
trees from their output. This points to another advan-
tage of dependency parsers: they need only the de-
pendency tree corpus to train and can conveniently
make use of native (unconverted) corpora, such as
the Chinese Dependency Treebank (Liu et al, 2006).
Lastly, we must note that although the Berkeley
parser is on par with Charniak?s (2000) system for
English (Cer et al, 2010, Table 1), its scores for Chi-
nese are substantially higher. There may be subtle
biases in Charniak?s approach (e.g., the conditioning
hierarchy used in smoothing) that could turn out to
be language-specific. The Berkeley parser appears
more general ? without quite as many parameters
or idiosyncratic design decisions ? as evidenced by
a recent application to French (Candito et al, 2010).
6 Conclusion
We compared seven popular open source parsers ?
four constituent and three dependency ? for gen-
erating Stanford dependencies in Chinese. Mate, a
high-order MST dependency parser, with lemmati-
zation and jackknifed POS-tags, appears most accu-
rate; but Berkeley?s faster constituent parser, with
jointly-inferred tags, is statistically no worse. This
outcome is different from English, where constituent
parsers systematically outperform direct methods.
Though Mate scored higher overall, Berkeley?s
parser was better at recovering longer-distance re-
lations, suggesting that a combined approach could
perhaps work better still (Rush et al, 2010, ?4.2).
Acknowledgments
We thank Daniel Cer, for helping us replicate the English ex-
perimental setup and for suggesting that we explore jackknifing
methods, and the anonymous reviewers, for valuable comments.
Supported in part by the National Natural Science Founda-
tion of China (NSFC) via grant 61133012, the National ?863?
Major Project grant 2011AA01A207, and the National ?863?
Leading Technology Research Project grant 2012AA011102.
13w3.msi.vxu.se/?nivre/research/Penn2Malt.html
14
Second author gratefully acknowledges the continued help
and support of his advisor, Dan Jurafsky, and of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program, under the Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions, findings,
and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the views
of DARPA, AFRL, or the US government.
References
Ion Androutsopoulos and Prodromos Malakasiotis. 2010.
A survey of paraphrasing and textual entailment methods.
Journal of Artificial Intelligence Research, 38(1):135?187,
May.
Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt
Broadhead, and Oren Etzioni. 2007. Open information ex-
traction from the web. In Proceedings of the 20th interna-
tional joint conference on Artifical intelligence, IJCAI?07,
pages 2670?2676, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Daniel M. Bikel. 2004. A distributional analysis of a lexi-
calized statistical parsing model. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 182?189,
Barcelona, Spain, July. Association for Computational Lin-
guistics.
Bernd Bohnet. 2010. Top accuracy and fast dependency pars-
ing is not a contradiction. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics (Coling
2010), pages 89?97, Beijing, China, August. Coling 2010
Organizing Committee.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings of
the Tenth Conference on Computational Natural Language
Learning (CoNLL-X), pages 149?164, New York City, June.
Association for Computational Linguistics.
Marie Candito, Joakim Nivre, Pascal Denis, and Enrique Hene-
stroza Anguiano. 2010. Benchmarking of statistical depen-
dency parsers for French. In Coling 2010: Posters, pages
108?116, Beijing, China, August. Coling 2010 Organizing
Committee.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and
Christopher D. Manning. 2010. Parsing to Stanford depen-
dencies: Trade-offs between speed and accuracy. In Pro-
ceedings of the 7th International Conference on Language
Resources and Evaluation (LREC 2010).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A li-
brary for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2(3):27:1?27:27, May.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Discriminative reordering with
Chinese grammatical relations features. In Proceedings of
the Third Workshop on Syntax and Structure in Statistical
Translation, Boulder, Colorado, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 173?180, Ann
Arbor, Michigan, June. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics conference,
NAACL 2000, pages 132?139, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
COLING Workshop on Cross-framework and Cross-domain
Parser Evaluation.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
Fifth International Conference on Language Resources and
Evaluation (LREC?06).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learning
Research, 9:1871?1874, June.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian Pado?, Jan S?te?pa?nek,
Pavel Stran?a?k, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task: Syntac-
tic and semantic dependencies in multiple languages. In
Proceedings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Ruihong Huang, Le Sun, and Yuanyong Feng. 2008. Study
of kernel-based methods for Chinese relation extraction. In
Proceedings of the 4th Asia information retrieval conference
on Information retrieval technology, AIRS?08, pages 598?
604, Berlin, Heidelberg. Springer-Verlag.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu
Kano, and Jun?ichi Tsujii. 2009. Overview of BioNLP?09
shared task on event extraction. In Proceedings of the Work-
shop on Current Trends in Biomedical Natural Language
Processing: Shared Task, BioNLP ?09, pages 1?9, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics - Volume
1, ACL ?03, pages 423?430, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-order de-
pendency parsers. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics, ACL
?10, pages 1?11, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Sim-
ple semi-supervised dependency parsing. In Proceedings of
ACL-08: HLT, pages 595?603, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre. 2009.
Dependency Parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
15
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang
Chen, and Haizhou Li. 2011. Joint models for Chinese POS
tagging and dependency parsing. In Proceedings of the 2011
Conference on Empirical Methods in Natural Language Pro-
cessing, pages 1180?1191, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Ting Liu, Jinshan Ma, and Sheng Li. 2006. Building a de-
pendency treebank for improving Chinese parser. Journal of
Chinese Language and Computing, 16(4).
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proceed-
ings of the 11th Conference of the European Chapter of the
ACL (EACL 2006), pages 81?88.
Arun Meena and T. V. Prabhakar. 2007. Sentence level sen-
timent analysis in the presence of conjuncts using linguistic
analysis. In Proceedings of the 29th European conference on
IR research, ECIR?07, pages 573?580, Berlin, Heidelberg.
Springer-Verlag.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In Proceed-
ings of ACL-08: HLT, pages 950?958, Columbus, Ohio,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser:
A data-driven parser-generator for dependency parsing. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages 2216?
2219.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932, Prague, Czech Republic, June.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics, pages
433?440, Sydney, Australia, July. Association for Computa-
tional Linguistics.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and linear
programming relaxations for natural language processing. In
Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1?11, Cambridge,
MA, October. Association for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble
models for dependency parsing: cheap and good? In Hu-
man Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 649?652, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL 2008 shared
task on joint parsing of syntactic and semantic dependen-
cies. In CoNLL 2008: Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning, pages
159?177, Manchester, England, August. Coling 2008 Orga-
nizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 173?180,
Stroudsburg, PA, USA. Association for Computational Lin-
guistics.
Huihsin Tseng, Daniel Jurafsky, and Christopher Manning.
2005. Morphological features help POS tagging of un-
known words across language varieties. In Proceedings of
the fourth SIGHAN bakeoff.
Fei Wu and Daniel S. Weld. 2010. Open information extraction
using Wikipedia. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics, ACL
?10, pages 118?127, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009.
Phrase dependency parsing for opinion mining. In Proceed-
ings of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 3 - Volume 3, EMNLP
?09, pages 1533?1541, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2011.
Structural opinion mining for graph-based sentiment rep-
resentation. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP ?11,
pages 1332?1341, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proceedings
of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 188?193, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of the 15th
ACM international conference on Information and knowl-
edge management, CIKM ?06, pages 43?50, New York, NY,
USA. ACM.
16
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 125?134,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Chinese Parsing Exploiting Characters
Meishan Zhang?, Yue Zhang??, Wanxiang Che?, Ting Liu?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
Characters play an important role in the
Chinese language, yet computational pro-
cessing of Chinese has been dominated
by word-based approaches, with leaves in
syntax trees being words. We investigate
Chinese parsing from the character-level,
extending the notion of phrase-structure
trees by annotating internal structures of
words. We demonstrate the importance
of character-level information to Chinese
processing by building a joint segmen-
tation, part-of-speech (POS) tagging and
phrase-structure parsing system that inte-
grates character-structure features. Our
joint system significantly outperforms a
state-of-the-art word-based baseline on the
standard CTB5 test, and gives the best
published results for Chinese parsing.
1 Introduction
Characters play an important role in the Chinese
language. They act as basic phonetic, morpho-
syntactic and semantic units in a Chinese sentence.
Frequently-occurring character sequences that ex-
press certain meanings can be treated as words,
while most Chinese words have syntactic struc-
tures. For example, Figure 1(b) shows the struc-
ture of the word ???? (construction and build-
ing industry)?, where the characters ?? (construc-
tion)? and ?? (building)? form a coordination,
and modify the character ?? (industry)?.
However, computational processing of Chinese
is typically based on words. Words are treated
as the atomic units in syntactic parsing, machine
translation, question answering and other NLP
tasks. Manually annotated corpora, such as the
Chinese Treebank (CTB) (Xue et al, 2005), usu-
ally have words as the basic syntactic elements
?Email correspondence.
?? ???
??
? ??
NR NN
VV
JJ NN
NP NP
NPADJP
NP
VPNP
IP
NP NP
NPADJP
NP
VPNP
IP
NN
?
NR-e
?
NR-b
JJ
?
JJ-s
VV
?
VV-e
?
VV-bNN
?
NN-e
?
NN-m
?
NN-b
NR
?
NR-eNR-b
?
NP NP
NPADJP
NP
VPNP
IP
NN-c
?
NR-i
?
NR-b
JJ-t
?
JJ-b
VV-c
?
VV-i
?
VV-bNR-r
?
NR-iNR-b
?
NR-t
NN-r
?
NN-i
?
NN-i
?
NN-b
NN-c
NN-t
VV-t
NN-t
(a) CTB-style word-based syntax tree for ??? (China) ?
?? (architecture industry) ?? (show) ? (new) ??
(pattern)?.
?? ???
??
? ??
NR NN
VV
JJ NN
NP NP
NPADJP
NP
VPNP
IP
NP NP
NPADJP
NP
VPNP
IP
NN
?
NR-e
?
NR-b
JJ
?
JJ-s
VV
?
VV-e
?
VV-bNN
?
NN-e
?
NN-m
?
NN-b
NR
?
NR-eNR-b
?
NP NP
NPADJP
NP
VPNP
IP
NN-c
?
NN-i
?
NN-b
JJ-t
?
JJ-b
VV-c
?
VV-i
?
VV-bNR-r
?
NR-iNR-b
?
NR-t
NN-r
?
NN-i
?
NN-i
?
NN-b
NN-c
NN-t
VV-t
NN-t
(b) character-level syntax tree with hierarchal word structures
for ?? (middle) ? (nation) ? (construction) ? (building)
? (industry) ? (present) ? (show) ? (new) ? (style) ?
(situation)?.
Figure 1: Word-based and character-level phrase-
structure trees for the sentence ????????
??? (China?s architecture industry shows new
patterns)?, where ?l?, ?r?, ?c? denote the direc-
tions of head characters (see section 2).
(Figure 1(a)). This form of annotation does not
give character-level syntactic structures for words,
a source of linguistic information that is more fun-
damental and less sparse than atomic words.
In this paper, we investigate Chinese syn-
tactic parsing with character-level information
by extending the notation of phrase-structure
125
(constituent) trees, adding recursive structures of
characters for words. We manually annotate the
structures of 37,382 words, which cover the entire
CTB5. Using these annotations, we transform
CTB-style constituent trees into character-level
trees (Figure 1(b)). Our word structure corpus,
together with a set of tools to transform CTB-style
trees into character-level trees, is released at
https://github.com/zhangmeishan/wordstructures.
Our annotation work is in line with the work of
Vadas and Curran (2007) and Li (2011), which
provide extended annotations of Penn Treebank
(PTB) noun phrases and CTB words (on the
morphological level), respectively.
We build a character-based Chinese parsing
model to parse the character-level syntax trees.
Given an input Chinese sentence, our parser pro-
duces its character-level syntax trees (Figure 1(b)).
With richer information than word-level trees, this
form of parse trees can be useful for all the afore-
mentioned Chinese NLP applications.
With regard to task of parsing itself, an impor-
tant advantage of the character-level syntax trees is
that they allow word segmentation, part-of-speech
(POS) tagging and parsing to be performed jointly,
using an efficient CKY-style or shift-reduce algo-
rithm. Luo (2003) exploited this advantage by
adding flat word structures without manually an-
notation to CTB trees, and building a generative
character-based parser. Compared to a pipeline
system, the advantages of a joint system include
reduction of error propagation, and the integration
of segmentation, POS tagging and syntax features.
With hierarchical structures and head character in-
formation, our annotated words are more informa-
tive than flat word structures, and hence can bring
further improvements to phrase-structure parsing.
To analyze word structures in addition to phrase
structures, our character-based parser naturally
performs joint word segmentation, POS tagging
and parsing jointly. Our model is based on the
discriminative shift-reduce parser of Zhang and
Clark (2009; 2011), which is a state-of-the-art
word-based phrase-structure parser for Chinese.
We extend their shift-reduce framework, adding
more transition actions for word segmentation and
POS tagging, and defining novel features that cap-
ture character information. Even when trained
using character-level syntax trees with flat word
structures, our joint parser outperforms a strong
pipelined baseline that consists of a state-of-the-
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(a) subject-predicate.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(b) verb-object.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(c) coordination.
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(d) modifier-noun.
Figure 2: Inner word structures of ??? (reper-
tory)?,??? (archaeology)?, ??? (science and
technology)? and ??? (degenerate)?.
art joint segmenter and POS tagger, and our base-
line word-based parser. Our word annotations lead
to further improvements to the joint system, espe-
cially for phrase-structure parsing accuracy.
Our parser work falls in line with recent work
of joint segmentation, POS tagging and parsing
(Hatori et al, 2012; Li and Zhou, 2012; Qian
and Liu, 2012). Compared with related work,
our model gives the best published results for
joint segmentation and POS tagging, as well as
joint phrase-structure parsing on standard CTB5
evaluations. With linear-time complexity, our
parser is highly efficient, processing over 30 sen-
tences per second with a beam size of 16. An
open release of the parser is freely available at
http://sourceforge.net/projects/zpar/, version 0.6.
2 Word Structures and Syntax Trees
The Chinese language is a character-based lan-
guage. Unlike alphabetical languages, Chinese
characters convey meanings, and the meaning of
most Chinese words takes roots in their charac-
ter. For example, the word ???? (computer)? is
composed of the characters ?? (count)?, ?? (cal-
culate)? and ?? (machine)?. An informal name of
?computer? is ????, which is composed of ??
(electronic)? and ?? (brain)?.
Chinese words have internal structures (Xue,
2001; Ma et al, 2012). The way characters inter-
act within words can be similar to the way words
interact within phrases. Figure 2 shows the struc-
tures of the four words ??? (repertory)?, ???
126
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
Figure 3: Character-level word structure of ???
?? (crouching tiger hidden dragon)?.
(archaeology)?, ??? (science and technology)?
and ??? (degenerate)?, which demonstrate
four typical syntactic structures of two-character
words, including subject-predicate, verb-object,
coordination and modifier-noun structures. Multi-
character words can also have recursive syntac-
tic structures. Figure 3 illustrates the structure
of the word ????? (crouching tiger hidden
dragon)?, which is composed of two subwords ??
? (crouching tiger)? and ??? (hidden dragon)?,
both having a modifier-noun structure.
The meaning of characters can be a useful
source of information for computational process-
ing of Chinese, and some recent work has started
to exploit this information. Zhang and Clark
(2010) found that the first character in a Chinese
word is a useful indicator of the word?s POS. They
made use of this information to help joint word
segmentation and POS tagging.
Li (2011) studied the morphological structures
of Chinese words, showing that 35% percent of
the words in CTB5 can be treated as having mor-
phemes. Figure 4(a) illustrates the morphological
structures of the words ? ??? (friends)? and
???? (educational world)?, in which the char-
acters ?? (plural)? and ?? (field)? can be treated
as suffix morphemes. They studied the influence
of such morphology to Chinese dependency pars-
ing (Li and Zhou, 2012).
The aforementioned work explores the influ-
ence of particular types of characters to Chinese
processing, yet not the full potentials of complete
word structures. We take one step further in this
line of work, annotating the full syntactic struc-
tures of 37,382 Chinese words in the form of Fig-
ure 2 and Figure 3. Our annotation covers the
entire vocabulary of CTB5. In addition to dif-
ference in coverage (100% vs 35%), our annota-
tion is structurally more informative than that of
Li (2011), as illustrated in Figure 4(b).
Our annotations are binarized recursive word
NN-c
NN-iNN-b
?
(science)
?
(technology)
VV-l
VV-iVV-b
?
(burn)
?
(up)
NN-r
NN-iNN-b
?
(repository)
?
(saving)
NN-l
VV-iVV-b
?
(investigate)
?
(ancient)
NN-r
NN-iNN-b
?
(bad)
?
(kind)
AD-l
AD-iAD-b
?
(vain)
?
(so)
NN-r
NN-iNN-b
?
(crouching)
?
(tiger)
NN-r
NN-iNN-i
?
(hidden)
?
(dragon)
NN-c
VV-r
VV-iVV-b
?
(fiercely)
?
(sweep)
VV-r
VV-iVV-i
?
(thousands)
?
(troops)
VV-l
NN-c
NN-iNN-b
?
(teach)
?
(education)
NN-i
?
(field)
NN-r
NN
NN-fNN-f
??
(education)
?
(field)
NN-c
NN-iNN-b
?
(friend)
?
(friend)
NN-i
?
(plural)
NN-l
NN
NN-fNN-f
??
(friend)
?
(plural)
(a) morphological-level word structures, where ?f? de-
notes a special mark for fine-grained words.
NN-c
NN-iNN-b
?(science) ?(technology)
VV-l
VV-iVV-b
?(burn) ?(up)
NN-r
NN-iNN-b
?(repository) ?(saving)
NN-l
VV-iVV-b
?(investigate) ?(ancient)
NN-r
NN-iNN-b
?(bad) ?(kind)
AD-l
AD-iAD-b
?(vain) ?(so)
NN-r
NN-iNN-b
?(crouching) ?(tiger)
NN-r
NN-iNN-i
?(hidden) ?(dragon)
NN-c
VV-r
VV-iVV-b
?(fiercely) ?(sweep)
VV-r
VV-iVV-i
?(thousands) ?(troops)
VV-l
NN-c
NN-iNN-b
?(teach) ?(education)
NN-i
?(field)
NN-r
NN
NN-fNN-f
??(education) ?(field)
NN-c
NN-iNN-b
?(friend) ?(friend)
NN-i
?(plural)
NN-l
NN
NN-fNN-f
??(friend) ?(plural)
(b) character-level word structures.
Figure 4: Comparison between character-level and
morphological-level word structures.
structures. For each word or subword, we spec-
ify its POS and head direction. We use ?l?, ?r?
and ?c? to indicate the ?left?, ?right? and ?coordi-
nation? head directions, respectively. The ?coor-
dination? direction is mostly used in coordination
structures, while a very small number of translit-
eration words, such as ???? (Obama)? and ??
?? (Los Angeles)?, have flat structures, and we
use ?coordination? for their left binarization. For
leaf characters, we follow previous work on word
segmentation (Xue, 2003; Ng and Low, 2004), and
use ?b? and ?i? to indicate the beginning and non-
beginning characters of a word, respectively.
The vast majority of words do not have struc-
tural ambiguities. However, the structures of some
words may vary according to different POS. For
example, ???? means ?dominate? when it is
tagged as a verb, of which the head is the left char-
acter; the same word means ?uniform dress? when
tagged as a noun, of which the head is the right
character. Thus the input of the word structure
annotation is a word together with its POS. The
annotation work was conducted by three persons,
with one person annotating the entire corpus, and
the other two checking the annotations.
Using our annotations, we can extend CTB-
style syntax trees (Figure 1(a)) into character-
level trees (Figure 1(b)). In particular, we mark
the original nodes that represent POS tags in CTB-
style trees with ?-t?, and insert our word structures
as unary subnodes of the ?-t? nodes. For the rest
of the paper, we refer to the ?-t? nodes as full-word
nodes, all nodes above full-word nodes as phrase
127
nodes, and all nodes below full-word nodes as sub-
word nodes.
Our character-level trees contain additional syn-
tactic information, which are potentially useful to
Chinese processing. For example, the head char-
acters of words can be populated up to phrase-
level nodes, and serve as an additional source of
information that is less sparse than head words. In
this paper, we build a parser that yields character-
level trees from raw character sequences. In addi-
tion, we use this parser to study the effects of our
annotations to character-based statistical Chinese
parsing, showing that they are useful in improving
parsing accuracies.
3 Character-based Chinese Parsing
To produce character-level trees for Chinese
NLP tasks, we develop a character-based parsing
model, which can jointly perform word segmen-
tation, POS tagging and phrase-structure parsing.
To our knowledge, this is the first work to develop
a transition-based system that jointly performs the
above three tasks. Trained using annotated word
structures, our parser also analyzes the internal
structures of Chinese words.
Our character-based Chinese parsing model is
based on the work of Zhang and Clark (2009),
which is a transition-based model for lexicalized
constituent parsing. They use a beam-search de-
coder so that the transition action sequence can be
globally optimized. The averaged perceptron with
early-update (Collins and Roark, 2004) is used to
train the model parameters. Their transition sys-
tem contains four kinds of actions: (1) SHIFT,
(2) REDUCE-UNARY, (3) REDUCE-BINARY and
(4) TERMINATE. The system can provide bina-
rzied CFG trees in Chomsky Norm Form, and they
present a reversible conversion procedure to map
arbitrary CFG trees into binarized trees.
In this work, we remain consistent with their
work, using the head-finding rules of Zhang and
Clark (2008), and the same binarization algo-
rithm.1 We apply the same beam-search algorithm
for decoding, and employ the averaged perceptron
with early-update to train our model.
We make two extensions to their work to en-
able joint segmentation, POS tagging and phrase-
structure parsing from the character level. First,
we modify the actions of the transition system for
1We use a left-binarization process for flat word structures
that contain more than two characters.
S2
sta ck
. . .
. . .
q u eu e
Q0 Q1 . . .S1
S1l S1r
. . . . . .
S0
S0l S0r
. . . . . .
Figure 5: A state in a transition-based model.
parsing the inner structures of words. Second, we
extend the feature set for our parsing problem.
3.1 The Transition System
In a transition-based system, an input sentence is
processed in a linear left-to-right pass, and the
output is constructed by a state-transition pro-
cess. We learn a model for scoring the transi-
tion Ai from one state STi to the next STi+1. As
shown in Figure 5, a state ST consists of a stack
S and a queue Q, where S = (? ? ? , S1, S0) con-
tains partially constructed parse trees, and Q =
(Q0, Q1, ? ? ? , Qn?j) = (cj , cj+1, ? ? ? , cn) is the
sequence of input characters that have not been
processed. The candidate transition action A at
each step is defined as follows:
? SHIFT-SEPARATE(t): remove the head
character cj from Q, pushing a subword node
S?
cj
2 onto S, assigning S?.t = t. Note that the
parse tree S0 must correspond to a full-word
or a phrase node, and the character cj is the
first character of the next word. The argu-
ment t denotes the POS of S?.
? SHIFT-APPEND: remove the head character
cj from Q, pushing a subword node S?cj onto
S. cj will eventually be combined with all the
subword nodes on top of S to form a word,
and thus we must have S?.t = S0.t.
? REDUCE-SUBWORD(d): pop the top two
nodes S0 and S1 off S, pushing a new sub-
word node S?S1 S0 onto S. The argument ddenotes the head direction of S?, of which
the value can be ?left?, ?right? or ?coordi-
nation?.3 Both S0 and S1 must be subword
nodes and S?.t = S0.t = S1.t.
2We use this notation for a compact representation of a
tree node, where the numerator represents a father node, and
the denominator represents the children.
3For the head direction ?coordination?, we extract the
head character from the left node.
128
Category Feature templates When to Apply
Structure S0ntl S0nwl S1ntl S1nwl S2ntl S2nwl S3ntl S3nwl, All
features Q0c Q1c Q2c Q3c Q0c ?Q1c Q1c ?Q2c Q2c ?Q3c,
S0ltwl S0rtwl S0utwl S1ltwl S1rtwl S1utwl,
S0nw ? S1nw S0nw ? S1nl S0nl ? S1nw S0nl ? S1nl,
S0nw ?Q0c S0nl ?Q0c S1nw ?Q0c S1nlQ0c,
S0nl ? S1nl ? S2nl S0nw ? S1nl ? S2nl S0nl ? S1nw ? S2nl S0nl ? S1nl ? S2nw,
S0nw ? S1nl ?Q0c S0nl ? S1nw ?Q0c S0nl ? S1nl ?Q0c,
S0ncl S0nct S0nctl S1ncl S1nct S1nctl,
S2ncl S2nct S2nctl S3ncl S3nct S3nctl,
S0nc ? S1nc S0ncl ? S1nl S0nl ? S1ncl S0ncl ? S1ncl,
S0nc ? Q0c S0nl ? Q0c S1nc ? Q0c S1nl ? Q0c,
S0nc ? S1nc ? Q0c S0nc ? S1nc ? Q0c ? Q1c
start(S0w) ? start(S1w) start(S0w) ? end(S1w), REDUCE-SUBWORD
indict(S1wS0w) ? len(S1wS0w) indict(S1wS0w, S0t) ? len(S1wS0w)
String t?1 ? t0 t?2 ? t?1t0 w?1 ? t0 c0 ? t0 start(w?1) ? t0 c?1 ? c0 ? t?1 ? t0, SHIFT-SEPARATE
features w?1 w?2 ? w?1 w?1,where len(w?1) = 1 end(w?1) ? c0, REDUCE-WORD
start(w?1) ? len(w?1) end(w?1) ? len(w?1) start(w?1) ? end(w?1),
w?1 ? c0 end(w?2) ? w?1 start(w?1) ? c0 end(w?2) ? end(w?1),
w?1 ? len(w?2) w?2 ? len(w?1) w?1 ? t?1 w?1 ? t?2 w?1 ? t?1 ? c0,
w?1 ? t?1 ? end(w?2) c?2 ? c?1 ? c0 ? t?1,where len(w?1) = 1 end(w?1) ? t?1,
c ? t?1 ? end(w?1),where c ? w?1 and c 6= end(w?1)
c0 ? t?1 c?1 ? c0 start(w?1) ? c0t?1 c?1 ? c0 ? t?1 SHIFT-APPEND
Table 1: Feature templates for the character-level parser. The function start(?), end(?) and len(?) denote
the first character, the last character and the length of a word, respectively.
? REDUCE-WORD: pop the top node S0 off S,
pushing a full-word node S?S0 onto S. This re-duce action generates a full-word node from
S0, which must be a subword node.
? REDUCE-BINARY(d, l): pop the top two
nodes S0 and S1 off S, pushing a binary
phrase node S?S1 S0 onto S. The argument ldenotes the constituent label of S?, and the ar-
gument d specifies the lexical head direction
of S?, which can be either ?left? or ?right?.
Both S0 and S1 must be a full-word node or
a phrase node.
? REDUCE-UNARY(l): pop the top node S0
off S, pushing a unary phrase node S?S0 onto
S. l denotes the constituent label of S?.
? TERMINATE: mark parsing complete.
Compared to set of actions in our baseline
transition-based phrase-structure parser, we have
made three major changes. First, we split the orig-
inal SHIFT action into SHIFT-SEPARATE(t)
and SHIFT-APPEND, which jointly perform the
word segmentation and POS tagging tasks. Sec-
ond, we add an extra REDUCE-SUBWORD(d) op-
eration, which is used for parsing the inner struc-
tures of words. Third, we add REDUCE-WORD,
which applies a unary rule to mark a completed
subword node as a full-word node. The new node
corresponds to a unary ?-t? node in Figure 1(b).
3.2 Features
Table 1 shows the feature templates of our model.
The feature set consists of two categories: (1)
structure features, which encode the structural in-
formation of subwords, full-words and phrases.
(2) string features, which encode the information
of neighboring characters and words.
For the structure features, the symbols S0, S1,
S2, S3 represent the top four nodes on the stack;
Q0, Q1, Q2, Q3 denote the first four characters
in the queue; S0l, S0r, S0u represent the left,
right child for a binary branching S0, and the sin-
gle child for a unary branching S0, respectively;
S1l, S1r, S1u represent the left, right child for
a binary branching S1, and the single child for
a unary branching S1, respectively; n represents
the type for a node; it is a binary value that indi-
cates whether the node is a subword node; c, w,
t and l represent the head character, word (or sub-
word), POS tag and constituent label of a node, re-
spectively. The structure features are mostly taken
129
from the work of Zhang and Clark (2009). The
feature templates in bold are novel, are designed
to encode head character information. In particu-
lar, the indict function denotes whether a word is
in a tag dictionary, which is collected by extract-
ing all multi-character subwords that occur more
than five times in the training corpus.
For string features, c0, c?1 and c?2 represent
the current character and its previous two charac-
ters, respectively; w?1 and w?2 represent the pre-
vious two words to the current character, respec-
tively; t0, t?1 and t?2 represent the POS tags of
the current word and the previous two words, re-
spectively. The string features are used for word
segmentation and POS tagging, and are adapted
from a state-of-the-art joint segmentation and tag-
ging model (Zhang and Clark, 2010).
In summary, our character-based parser con-
tains the word-based features of constituent parser
presented in Zhang and Clark (2009), the word-
based and shallow character-based features of
joint word segmentation and POS tagging pre-
sented in Zhang and Clark (2010), and addition-
ally the deep character-based features that encode
word structure information, which are the first pre-
sented by this paper.
4 Experiments
4.1 Setting
We conduct our experiments on the CTB5 cor-
pus, using the standard split of data, with sections
1?270,400?931 and 1001?1151 for training, sec-
tions 301?325 for system development, and sec-
tions 271?300 for testing. We apply the same pre-
processing step as Harper and Huang (2011), so
that the non-terminal yield unary chains are col-
lapsed to single unary rules.
Since our model can jointly process word seg-
mentation, POS tagging and phrase-structure pars-
ing, we evaluate our model for the three tasks, re-
spectively. For word segmentation and POS tag-
ging, standard metrics of word precision, recall
and F-score are used, where the tagging accuracy
is the joint accuracy of word segmentation and
POS tagging. For phrase-structure parsing, we
use the standard parseval evaluation metrics on
bracketing precision, recall and F-score. As our
constituent trees are based on characters, we fol-
low previous work and redefine the boundary of
a constituent span by its start and end characters.
In addition, we evaluate the performance of word
6570
7580
8590
95
0 10 20 30 40
64b16b4b1b
(a) Joint segmentation and
POS tagging F-scores.
3040
5060
7080
90
0 10 20 30 40
64b16b4b1b
(b) Joint constituent parsing
F-scores.
Figure 6: Accuracies against the training epoch
for joint segmentation and tagging as well as joint
phrase-structure parsing using beam sizes 1, 4, 16
and 64, respectively.
structures, using the word precision, recall and F-
score metrics. A word structure is correct only if
the word and its internal structure are both correct.
4.2 Development Results
Figure 6 shows the accuracies of our model using
different beam sizes with respect to the training
epoch. The performance of our model increases
as the beam size increases. The amount of in-
creases becomes smaller as the size of the beam
grows larger. Tested using gcc 4.7.2 and Fedora
17 on an Intel Core i5-3470 CPU (3.20GHz), the
decoding speeds are 318.2, 98.0, 30.3 and 7.9 sen-
tences per second with beam size 1, 4, 16 and 64,
respectively. Based on this experiment, we set the
beam size 64 for the rest of our experiments.
The character-level parsing model has the ad-
vantage that deep character information can be ex-
tracted as features for parsing. For example, the
head character of a word is exploited in our model.
We conduct feature ablation experiments to eval-
uate the effectiveness of these features. We find
that the parsing accuracy decreases about 0.6%
when the head character related features (the bold
feature templates in Table 1) are removed, which
demonstrates the usefulness of these features.
4.3 Final Results
In this section, we present the final results of our
model, and compare it to two baseline systems, a
pipelined system and a joint system that is trained
with automatically generated flat words structures.
The baseline pipelined system consists of the
joint segmentation and tagging model proposed by
130
Task P R F
Pipeline Seg 97.35 98.02 97.69
Tag 93.51 94.15 93.83
Parse 81.58 82.95 82.26
Flat word Seg 97.32 98.13 97.73
structures Tag 94.09 94.88 94.48
Parse 83.39 83.84 83.61
Annotated Seg 97.49 98.18 97.84
word structures Tag 94.46 95.14 94.80
Parse 84.42 84.43 84.43
WS 94.02 94.69 94.35
Table 2: Final results on test corpus.
Zhang and Clark (2010), and the phrase-structure
parsing model of Zhang and Clark (2009). Both
models give state-of-the-art performances, and are
freely available.4 The model for joint segmen-
tation and POS tagging is trained with a 16-
beam, since it achieves the best performance. The
phrase-structure parsing model is trained with a
64-beam. We train the parsing model using the
automatically generated POS tags by 10-way jack-
knifing, which gives about 1.5% increases in pars-
ing accuracy when tested on automatic segmented
and POS tagged inputs.
The joint system trained with flat word struc-
tures serves to test the effectiveness of our joint
parsing system over the pipelined baseline, since
flat word structures do not contain additional
sources of information over the baseline. It is also
used to test the usefulness of our annotation in im-
proving parsing accuracy.
Table 2 shows the final results of our model
and the two baseline systems on the test data.
We can see that both character-level joint mod-
els outperform the pipelined system; our model
with annotated word structures gives an improve-
ment of 0.97% in tagging accuracy and 2.17% in
phrase-structure parsing accuracy. The results also
demonstrate that the annotated word structures are
highly effective for syntactic parsing, giving an ab-
solute improvement of 0.82% in phrase-structure
parsing accuracy over the joint model with flat
word structures.
Row ?WS? in Table 2 shows the accuracy of
hierarchical word-structure recovery of our joint
system. This figure can be useful for high-level ap-
plications that make use of character-level trees by
4http://sourceforge.net/projects/zpar/, version 0.5.
our parser, as it reflects the capability of our parser
in analyzing word structures. In particular, the per-
formance of parsing OOV word structure is an im-
portant metric of our parser. The recall of OOV
word structures is 60.43%, while if we do not con-
sider the influences of segmentation and tagging
errors, counting only the correctly segmented and
tagged words, the recall is 87.96%.
4.4 Comparison with Previous Work
In this section, we compare our model to previous
systems on the performance of joint word segmen-
tation and POS tagging, and the performance of
joint phrase-structure parsing.
Table 3 shows the results. Kruengkrai+ ?09
denotes the results of Kruengkrai et al (2009),
which is a lattice-based joint word segmentation
and POS tagging model; Sun ?11 denotes a sub-
word based stacking model for joint segmenta-
tion and POS tagging (Sun, 2011), which uses a
dictionary of idioms; Wang+ ?11 denotes a semi-
supervised model proposed by Wang et al (2011),
which additionally uses the Chinese Gigaword
Corpus; Li ?11 denotes a generative model that
can perform word segmentation, POS tagging and
phrase-structure parsing jointly (Li, 2011); Li+
?12 denotes a unified dependency parsing model
that can perform joint word segmentation, POS
tagging and dependency parsing (Li and Zhou,
2012); Li ?11 and Li+ ?12 exploited annotated
morphological-level word structures for Chinese;
Hatori+ ?12 denotes an incremental joint model
for word segmentation, POS tagging and depen-
dency parsing (Hatori et al, 2012); they use exter-
nal dictionary resources including HowNet Word
List and page names from the Chinese Wikipedia;
Qian+ ?12 denotes a joint segmentation, POS tag-
ging and parsing system using a unified frame-
work for decoding, incorporating a word segmen-
tation model, a POS tagging model and a phrase-
structure parsing model together (Qian and Liu,
2012); their word segmentation model is a combi-
nation of character-based model and word-based
model. Our model achieved the best performance
on both joint segmentation and tagging as well as
joint phrase-structure parsing.
Our final performance on constituent parsing is
by far the best that we are aware of for the Chinese
data, and even better than some state-of-the-art
models with gold segmentation. For example, the
un-lexicalized PCFG model of Petrov and Klein
131
System Seg Tag Parse
Kruengkrai+ ?09 97.87 93.67 ?
Sun ?11 98.17* 94.02* ?
Wang+ ?11 98.11* 94.18* ?
Li ?11 97.3 93.5 79.7
Li+ ?12 97.50 93.31 ?
Hatori+ ?12 98.26* 94.64* ?
Qian+ ?12 97.96 93.81 82.85
Ours pipeline 97.69 93.83 82.26
Ours joint flat 97.73 94.48 83.61
Ours joint annotated 97.84 94.80 84.43
Table 3: Comparisons of our final model with
state-of-the-art systems, where ?*? denotes that
external dictionary or corpus has been used.
(2007) achieves 83.45%5 in parsing accuracy on
the test corpus, and our pipeline constituent pars-
ing model achieves 83.55% with gold segmenta-
tion. They are lower than the performance of our
character-level model, which is 84.43% without
gold segmentation. The main differences between
word-based and character-level parsing models are
that character-level model can exploit character
features. This further demonstrates the effective-
ness of characters in Chinese parsing.
5 Related Work
Recent work on using the internal structure of
words to help Chinese processing gives impor-
tant motivations to our work. Zhao (2009) stud-
ied character-level dependencies for Chinese word
segmentation by formalizing segmentsion task in
a dependency parsing framework. Their results
demonstrate that annotated word dependencies
can be helpful for word segmentation. Li (2011)
pointed out that the word?s internal structure is
very important for Chinese NLP. They annotated
morphological-level word structures, and a unified
generative model was proposed to parse the Chi-
nese morphological and phrase-structures. Li and
Zhou (2012) also exploited the morphological-
level word structures for Chinese dependency
parsing. They proposed a unified transition-based
model to parse the morphological and depen-
dency structures of a Chinese sentence in a unified
framework. The morphological-level word struc-
5We rerun the parser and evaluate it using the publicly-
available code on http://code.google.com/p/berkeleyparser
by ourselves, since we have a preprocessing step for the
CTB5 corpus.
tures concern only prefixes and suffixes, which
cover only 35% of entire words in CTB. Accord-
ing to their results, the final performances of their
model on word segmentation and POS tagging are
below the state-of-the-art joint segmentation and
POS tagging models. Compared to their work,
we consider the character-level word structures
for Chinese parsing, presenting a unified frame-
work for segmentation, POS tagging and phrase-
structure parsing. We can achieve improved seg-
mentation and tagging performance.
Our character-level parsing model is inspired
by the work of Zhang and Clark (2009), which
is a transition-based model with a beam-search
decoder for word-based constituent parsing. Our
work is based on the shift-reduce operations of
their work, while we introduce additional opera-
tions for segmentation and POS tagging. By such
an extension, our model can include all the fea-
tures in their work, together with the features for
segmentation and POS tagging. In addition, we
propose novel features related to word structures
and interactions between word segmentation, POS
tagging and word-based constituent parsing.
Luo (2003) was the first work to introduce the
character-based syntax parsing. They use it as
a joint framework to perform Chinese word seg-
mentation, POS tagging and syntax parsing. They
exploit a generative maximum entropy model for
character-based constituent parsing, and find that
POS information is very useful for Chinese word
segmentation, but high-level syntactic information
seems to have little effect on segmentation. Com-
pared to their work, we use a transition-based dis-
criminative model, which can benefit from large
amounts of flexible features. In addition, in-
stead of using flat structures, we manually anno-
tate hierarchal tree structures of Chinese words
for converting word-based constituent trees into
character-based constituent trees.
Hatori et al (2012) proposed the first joint work
for the word segmentation, POS tagging and de-
pendency parsing. They used a single transition-
based model to perform the three tasks. Their
work demonstrates that a joint model can improve
the performance of the three tasks, particularly
for POS tagging and dependency parsing. Qian
and Liu (2012) proposed a joint decoder for word
segmentation, POS tagging and word-based con-
stituent parsing, although they trained models for
the three tasks separately. They reported better
132
performances when using a joint decoder. In our
work, we employ a single character-based dis-
criminative model to perform segmentation, POS
tagging and phrase-structure parsing jointly, and
study the influence of annotated word structures.
6 Conclusions and Future Work
We studied the internal structures of more than
37,382 Chinese words, analyzing their structures
as the recursive combinations of characters. Using
these word structures, we extended the CTB into
character-level trees, and developed a character-
based parser that builds such trees from raw char-
acter sequences. Our character-based parser per-
forms segmentation, POS tagging and parsing
simultaneously, and significantly outperforms a
pipelined baseline. We make both our annotations
and our parser available online.
In summary, our contributions include:
? We annotated the internal structures of Chi-
nese words, which are potentially useful
to character-based studies of Chinese NLP.
We extend CTB-style constituent trees into
character-level trees using our annotations.
? We developed a character-based parsing
model that can produce our character-level
constituent trees. Our parser jointly performs
word segmentation, POS tagging and syntac-
tic parsing.
? We investigated the effectiveness of our joint
parser over pipelined baseline, and the effec-
tiveness of our annotated word structures in
improving parsing accuracies.
Future work includes investigations of our
parser and annotations on Chinese NLP tasks.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Major Projects
via grant 2011AA01A207, the National ?863?
Leading Technology Research Project via grant
2012AA011102, and SRG ISTD 2012 038 from
Singapore University of Technology and Design.
References
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Mary Harper and Zhongqiang Huang. 2011. Chinese
statistical parsing. Handbook of Natural Language
Processing and Machine Translation.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1045?
1053, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 513?521,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmen-
tation. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, pages 1405?1414,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Xiaoqiang Luo. 2003. A maximum entropy Chi-
nese character-based parser. In Michael Collins and
Mark Steedman, editors, Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 192?199.
Jianqiang Ma, Chunyu Kit, and Dale Gerdemann.
2012. Semi-automatic annotation of chinese word
structure. In Proceedings of the Second CIPS-
SIGHAN Joint Conference on Chinese Language
Processing, pages 9?17, Tianjin, China, December.
Association for Computational Linguistics.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277?284, Barcelona, Spain, July. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
133
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
501?511, Jeju Island, Korea, July. Association for
Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385?
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 240?247,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309?317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Nianwen Xue. 2001. Defining and Automatically
Identifying Words in Chinese. Ph.D. thesis, Univer-
sity of Delaware.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, 8(1).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-
based parsing of the chinese treebank using a global
discriminative model. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT?09), pages 162?171, Paris, France, October.
Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843?852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 879?887, Athens, Greece,
March. Association for Computational Linguistics.
134
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1073?1082,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Word Alignment and Bilingual Named Entity Recognition
Using Dual Decomposition
Mengqiu Wang
Stanford University
Stanford, CA 94305
mengqiu@cs.stanford.edu
Wanxiang Che
Harbin Institute of Technology
Harbin, China, 150001
car@ir.hit.edu.cn
Christopher D. Manning
Stanford University
Stanford, CA 94305
manning@cs.stanford.edu
Abstract
Translated bi-texts contain complemen-
tary language cues, and previous work
on Named Entity Recognition (NER)
has demonstrated improvements in perfor-
mance over monolingual taggers by pro-
moting agreement of tagging decisions be-
tween the two languages. However, most
previous approaches to bilingual tagging
assume word alignments are given as fixed
input, which can cause cascading errors.
We observe that NER label information
can be used to correct alignment mis-
takes, and present a graphical model that
performs bilingual NER tagging jointly
with word alignment, by combining two
monolingual tagging models with two uni-
directional alignment models. We intro-
duce additional cross-lingual edge factors
that encourage agreements between tag-
ging and alignment decisions. We design
a dual decomposition inference algorithm
to perform joint decoding over the com-
bined alignment and NER output space.
Experiments on the OntoNotes dataset
demonstrate that our method yields signif-
icant improvements in both NER and word
alignment over state-of-the-art monolin-
gual baselines.
1 Introduction
We study the problem of Named Entity Recogni-
tion (NER) in a bilingual context, where the goal
is to annotate parallel bi-texts with named entity
tags. This is a particularly important problem for
machine translation (MT) since entities such as
person names, locations, organizations, etc. carry
much of the information expressed in the source
sentence. Recognizing them provides useful in-
formation for phrase detection and word sense dis-
ambiguation (e.g., ?melody? as in a female name
has a different translation from the word ?melody?
in a musical sense), and can be directly leveraged
to improve translation quality (Babych and Hart-
ley, 2003). We can also automatically construct a
named entity translation lexicon by annotating and
extracting entities from bi-texts, and use it to im-
prove MT performance (Huang and Vogel, 2002;
Al-Onaizan and Knight, 2002). Previous work
such as Burkett et al (2010b), Li et al (2012) and
Kim et al (2012) have also demonstrated that bi-
texts annotated with NER tags can provide useful
additional training sources for improving the per-
formance of standalone monolingual taggers.
Because human translation in general preserves
semantic equivalence, bi-texts represent two per-
spectives on the same semantic content (Burkett et
al., 2010b). As a result, we can find complemen-
tary cues in the two languages that help to dis-
ambiguate named entity mentions (Brown et al,
1991). For example, the English word ?Jordan?
can be either a last name or a country. Without
sufficient context it can be difficult to distinguish
the two; however, in Chinese, these two senses are
disambiguated: ???? as a last name, and ????
as a country name.
In this work, we first develop a bilingual NER
model (denoted as BI-NER) by embedding two
monolingual CRF-based NER models into a larger
undirected graphical model, and introduce addi-
tional edge factors based on word alignment (WA).
Because the new bilingual model contains many
cyclic cliques, exact inference is intractable. We
employ a dual decomposition (DD) inference al-
gorithm (Bertsekas, 1999; Rush et al, 2010) for
performing approximate inference. Unlike most
1073
f1 f2 f3 f4 f5 f6
e1 e2 e3 e4 e5 e6
Xinhua News Agency Beijing Feb 16
B-ORG I-ORG I-ORG [O] B-LOC O O
??? ? ?? ? ?? ??
B-ORG O B-GPE O O O
Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is
an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of
alignment error.
previous applications of the DD method in NLP,
where the model typically factors over two com-
ponents and agreement is to be sought between the
two (Rush et al, 2010; Koo et al, 2010; DeNero
and Macherey, 2011; Chieu and Teow, 2012), our
method decomposes the larger graphical model
into many overlapping components where each
alignment edge forms a separate factor. We design
clique potentials over the alignment-based edges
to encourage entity tag agreements. Our method
does not require any manual annotation of word
alignments or named entities over the bilingual
training data.
The aforementioned BI-NER model assumes
fixed alignment input given by an underlying word
aligner. But the entity span and type predictions
given by the NER models contain complementary
information for correcting alignment errors. To
capture this source of information, we present a
novel extension that combines the BI-NER model
with two uni-directional HMM-based alignment
models, and perform joint decoding of NER and
word alignments. The new model (denoted as
BI-NER-WA) factors over five components: one
NER model and one word alignment model for
each language, plus a joint NER-alignment model
which not only enforces NER label agreements but
also facilitates message passing among the other
four components. An extended DD decoding algo-
rithm is again employed to perform approximate
inference.
We give a formal definition of the Bi-NER
model in Section 2, and then move to present the
Bi-NER-WA model in Section 3.
2 Bilingual NER by Agreement
The inputs to our models are parallel sentence
pairs (see Figure 1 for an example in English and
Chinese). We denote the sentences as e (for En-
glish) and f (for Chinese). We assume access
to two monolingual linear-chain CRF-based NER
models that are already trained. The English-side
CRF model assigns the following probability for a
tag sequence ye:
PCRFe (ye|e) =
?
vi?Ve
?(vi)
?
(vi,vj)?De
?(vi, vj)
Ze(e)
where Ve is the set of vertices in the CRF and
De is the set of edges. ?(vi) and ?(vi, vj) are
the node and edge clique potentials, and Ze(e)
is the partition function for input sequence e un-
der the English CRF model. We let k(ye) be the
un-normalized log-probability of tag sequence ye,
defined as:
k(ye) = log
?
? ?
vi?Ve
?(vi)
?
(vi,vj)?De
?(vi, vj)
?
?
Similarly, we define model PCRFf and un-
normalized log-probability l(yf) for Chinese.
We also assume that a set of word alignments
(A = {(i, j) : ei ? fj}) is given by a word
aligner and remain fixed in our model.
For clarity, we assume ye and yf are binary vari-
ables in the description of our algorithms. The ex-
tension to the multi-class case is straight-forward
and does not affect the core algorithms.
2.1 Hard Agreement
We define a BI-NER model which imposes hard
agreement of entity labels over aligned word pairs.
At inference time, we solve the following opti-
1074
mization problem:
max
ye,yf
log (PCRFe (ye)) + log
(
PCRFf
(
yf
))
=max
ye,yf
k(ye) + l(yf)? logZe(e)? logZf (f)
'max
ye,yf
k(ye) + l(yf)
3 yei = yfj ?(i, j) ? A
We dropped the Ze(e) and Zf(f) terms because
they remain constant at inference time.
The Lagrangian relaxation of this term is:
L
(
ye,yf,U
)
=
k (ye) + l
(
yf
)
+
?
(i,j)?A
u(i, j)
(
yei ? yfj
)
where u(i, j) are the Lagrangian multipliers.
Instead of solving the Lagrangian directly, we
can form the dual of this problem and solve it us-
ing dual decomposition (Rush et al, 2010):
min
U
(
max
ye
?
?k (ye) +
?
(i,j)?A
u(i, j)yei
?
?
+max
yf
?
?l
(
yf
)
?
?
(i,j)?A
u(i, j)yfj
?
?
)
Similar to previous work, we solve this DD
problem by iteratively updating the sub-gradient
as depicted in Algorithm 1. T is the maximum
number of iterations before early stopping, and ?t
is the learning rate at time t. We adopt a learning
rate update rule from Koo et al (2010) where ?t is
defined as 1N , where N is the number of times weobserved a consecutive dual value increase from
iteration 1 to t.
A thorough introduction to the theoretical foun-
dations of dual decomposition algorithms is be-
yond the scope of this paper; we encourage un-
familiar readers to read Rush and Collins (2012)
for a full tutorial.
2.2 Soft Agreement
The previously discussed hard agreement model
rests on the core assumption that aligned words
must have identical entity tags. In reality, however,
this assumption does not always hold. Firstly, as-
suming words are correctly aligned, their entity
tags may not agree due to inconsistency in anno-
tation standards. In Figure 1, for example, the
Algorithm 1 DD inference algorithm for hard
agreement model.
?(i, j) ? A : u(i, j) = 0
for t? 1 to T do
ye? ? argmax k (ye) + ?
(i,j)?A
u(i, j)yei
yf? ? argmax l
(
yf
)
? ?
(i,j)?A
u(i, j)yfj
if ?(i, j) ? A : ye?i = yf?j then
return (ye?,yf?)
end if
for all (i, j) ? A do
u(i, j)? u(i, j) + ?t
(
yf?j ? ye?i
)
end for
end for
return (ye?(T),yf?(T)
)
word ?Beijing? can be either a Geo-Political En-
tity (GPE) or a location. The Chinese annotation
standard may enforce that ?Beijing? should always
be tagged as GPE when it is mentioned in isola-
tion, while the English standard may require the
annotator to judge based on word usage context.
The assumption in the hard agreement model can
also be violated if there are word alignment errors.
In order to model this uncertainty, we extend
the two previously independent CRF models into a
larger undirected graphical model, by introducing
a cross-lingual edge factor ?(i, j) for every pair of
word positions (i, j) ? A. We associate a clique
potential function h(i,j)(yei , yfj) for ?(i, j):
h(i,j)
(
yei , yfj
)
= pmi
(
yei , yfj
)P? (ei,fj)
where pmi(yei , yfj) is the point-wise mutual in-
formation (PMI) of the tag pair, and we raise it
to the power of a posterior alignment probability
P? (ei, fj). For a pair of NEs that are aligned with
low probability, we cannot be too sure about the
association of the two NEs, therefore the model
should not impose too much influence from the
bilingual agreement model; instead, we will let the
monolingual NE models make their decisions, and
trust that those are the best estimates we can come
up with when we do not have much confidence in
their bilingual association. The use of the poste-
rior alignment probability facilitates this purpose.
Initially, each of the cross-lingual edge factors
will attempt to assign a pair of tags that has the
highest PMI score, but if the monolingual taggers
do not agree, a penalty will start accumulating
over this pair, until some other pair that agrees bet-
ter with the monolingual models takes the top spot.
1075
Simultaneously, the monolingual models will also
be encouraged to agree with the cross-lingual edge
factors. This way, the various components effec-
tively trade penalties indirectly through the cross-
lingual edges, until a tag sequence that maximizes
the joint probability is achieved.
Since we assume no bilingually annotated NER
corpus is available, in order to get an estimate of
the PMI scores, we first tag a collection of unan-
notated bilingual sentence pairs using the mono-
lingual CRF taggers, and collect counts of aligned
entity pairs from this auto-generated tagged data.
Each of the ?(i, j) edge factors (e.g., the edge
between node f3 and e4 in Figure 1) overlaps with
each of the two CRF models over one vertex (e.g.,
f3 on Chinese side and e4 on English side), and
we seek agreement with the Chinese CRF model
over tag assignment of fj , and similarly for ei on
English side. In other words, no direct agreement
between the two CRF models is enforced, but they
both need to agree with the bilingual edge factors.
The updated optimization problem becomes:
max
ye(k)yf(l)ye(h)yf(h)
k
(
ye(k)
)
+ l
(
yf (l)
)
+
?
(i,j)?A
h(i,j)
(
ye(h)i , yf
(h)
j
)
3 ?(i, j) ? A :
(
ye(k)i = ye
(h)
i
)
?
(
yf (l)j = y
f (h)
j
)
where the notation ye(k)i denotes tag assignment to
word ei by the English CRF and ye(h)i denotes as-
signment to word ei by the bilingual factor; yf (l)j
denotes the tag assignment to word fj by the Chi-
nese CRF and yf (h)j denotes assignment to word
fj by the bilingual factor.
The updated DD algorithm is illustrated in Al-
gorithm 2 (case 2). We introduce two separate
sets of dual constraints we and wf, which range
over the set of vertices on their respective half
of the graph. Decoding the edge factor model
h(i,j)(yei , y
f
j) simply involves finding the pair of
tag assignments that gives the highest PMI score,
subject to the dual constraints.
The way DD algorithms work in decomposing
undirected graphical models is analogous to other
message passing algorithms such as loopy belief
propagation, but DD gives a stronger optimality
guarantee upon convergence (Rush et al, 2010).
3 Joint Alignment and NER Decoding
In this section we develop an extended model in
which NER information can in turn be used to
improve alignment accuracy. Although we have
seen more than a handful of recent papers that ap-
ply the dual decomposition method for joint in-
ference problems, all of the past work deals with
cases where the various model components have
the same inference output space (e.g., dependency
parsing (Koo et al, 2010), POS tagging (Rush et
al., 2012), etc.). In our case the output space is
the much more complex joint alignment and NER
tagging space. We propose a novel dual decom-
position variant for performing inference over this
joint space.
Most commonly used alignment models, such
as the IBM models and HMM-based aligner are
unsupervised learners, and can only capture sim-
ple distortion features and lexical translational fea-
tures due to the high complexity of the structure
prediction space. On the other hand, the CRF-
based NER models are trained on manually anno-
tated data, and admit richer sequence and lexical
features. The entity label predictions made by the
NER model can potentially be leveraged to correct
alignment mistakes. For example, in Figure 1, if
the tagger knows that the word ?Agency? is tagged
I-ORG, and if it also knows that the first comma
in the Chinese sentence is not part of any entity,
then we can infer it is very unlikely that there ex-
ists an alignment link between ?Agency? and the
comma.
To capture this intuition, we extend the BI-NER
model to jointly perform word alignment and NER
decoding, and call the resulting model BI-NER-
WA. As a first step, instead of taking the output
from an aligner as fixed input, we incorporate two
uni-directional aligners into our model. We name
the Chinese-to-English aligner model as m(Be)
and the reverse directional model n(Bf ). Be is
a matrix that holds the output of the Chinese-to-
English aligner. Each be(i, j) binary variable in
Be indicates whether fj is aligned to ei; similarly
we define output matrix Bf and bf (i, j) for Chi-
nese. In our experiments, we used two HMM-
based alignment models. But in principle we can
adopt any alignment model as long as we can per-
form efficient inference over it.
We introduce a cross-lingual edge factor ?(i, j)
in the undirected graphical model for every pair of
word indices (i, j), which predicts a binary vari-
1076
Algorithm 2 DD inference algorithm for joint
alignment and NER model. A line marked with (2)
means it applies to the BI-NER model; a line marked with
(3) means it applies to the BI-NER-WA model.
S ? A (2)
S ? {(i, j) : ?i ? |e|, ?j ? |f |} (3)
?i ? |e| : wei = 0; ?j ? |f | : wfj = 0 (2,3)
?(i, j) ? S : de(i, j) = 0, df (i, j) = 0 (3)
for t? 1 to T do
ye(k)? ? argmax k
(
ye(k)
)
+
?
i?|e|
wei ye
(k)
i (2,3)
yf(l)? ? argmax l
(
yf(l)
)
+
?
i?|f |
wfj yf
(l)
j (2,3)
Be??argmax m (Be) + ?
(i,j)
de(i, j)be(i, j) (3)
Bf??argmax n
(
Bf
)
+
?
(i,j)
df(i, j)bf(i, j) (3)
for all (i, j) ? S do
(ye(h)?i yf
(h)?
j )? ?wei ye
(h)
i ? wfj yf
(h)
j
+ argmax h(i,j)(ye
(q)
i yf
(q)
j ) (2)
(ye(q)?i yf
(q)?
j a(i, j)?)? ?wei ye
(q)
i ? wfj yf
(q)
j
+ argmax q(i,j)(ye
(q)
i yf
(q)
j a(i, j))
? de(i, j)a(i, j)? df(i, j)a(i, j) (3)
end for
Conv = (ye(k)=ye(q) ? yf(l)=yf(q)) (2)
Conv = (Be=A=Bf ? ye(k)=ye(q)? yf(l)=yf(q)) (3)
if Conv = true , then
return
(
ye(k)? ,yf(l)?
)
(2)
return
(
ye(k)? ,yf(l)? ,A
)
(3)
else
for all i ? |e| do
wei ? wei + ?t
(
ye(q|h)?i ? ye
(k)?
i
)
(2,3)
end for
for all j ? |f | do
wfj ? wfj + ?t
(
yf
(q|h)?
j ? yf
(l)?
j
)
(2,3)
end for
for all (i, j) ? S do
de(i, j)? de(i, j) + ?t (ae?(i, j)? be?(i, j)) (3)
df(i, j)? df(i, j) + ?t
(
af?(i, j)? bf?(i, j)
) (3)
end for
end if
end for
return
(
ye(k)?(T) ,yf
(l)?
(T)
)
(2)
return
(
ye(k)?(T) ,yf
(l)?
(T) ,A(T )
)
(3)
able a(i, j) for an alignment link between ei and
fj . The edge factor also predicts the entity tags for
ei and fj .
The new edge potential q is defined as:
q(i,j)
(
yei , yfj , a(i, j)
)
=
log(P (a(i, j) = 1)) + S(yei , yfj |a(i, j))P (a(i,j)=1)
S(yei , yfj |a(i, j))=
{
pmi(yei , y
f
j), if a(i, j) = 1
0, else
P (a(i, j) = 1) is the alignment probability as-
signed by the bilingual edge factor between node
ei and fj . We initialize this value to P? (ei, fj) =
1
2(Pm(ei, fj) + Pn(ei, fj)), where Pm(ei, fj) and
Pn(ei, fj) are the posterior probabilities assigned
by the HMM-aligners.
The joint optimization problem is defined as:
max
ye(k)yf(l)ye(h)yf(h)BeBfA
k(ye(k)) + l(yf (l))+
m(Be) + n(Bf) +
?
(i?|e|,j?|f |)
q(i,j)(ye
h
i , yf
(h)
j , a(i, j))
3 ?(i, j) :
(
be(i, j)=a(i, j)
)
?
(
bf (i, j)=a(i, j)
)
? if a(i, j) = 1 then
(
ye(k)i =ye
(h)
i
)
?
(
yf (l)j =y
f (h)
j
)
We include two dual constraints de(i, j) and
df (i, j) over alignments for every bilingual edge
factor ?(i, j), which are applied to the English and
Chinese sides of the alignment space, respectively.
The DD algorithm used for this model is given
in Algorithm 2 (case 3). One special note is that
after each iteration when we consider updates to
the dual constraint for entity tags, we only check
tag agreements for cross-lingual edge factors that
have an alignment assignment value of 1. In other
words, cross-lingual edges that are not aligned do
not affect bilingual NER tagging.
Similar to ?(i, j), ?(i, j) factors do not provide
that much additional information other than some
selectional preferences via PMI score. But the
real power of these cross-language edge cliques
is that they act as a liaison between the NER
and alignment models on each language side, and
encourage these models to indirectly agree with
each other by having them all agree with the edge
cliques.
It is also worth noting that since we decode
the alignment models with Viterbi inference, ad-
ditional constraints such as the neighborhood con-
straint proposed by DeNero and Macherey (2011)
can be easily integrated into our model. The
neighborhood constraint enforces that if fj is
aligned to ei, then fj can only be aligned to ei+1
or ei?1 (with a small penalty), but not any other
word position. We report results of adding neigh-
borhood constraints to our model in Section 6.
4 Experimental Setup
We evaluate on the large OntoNotes (v4.0) cor-
pus (Hovy et al, 2006) which contains manually
1077
annotated NER tags for both Chinese and En-
glish. Document pairs are sentence aligned us-
ing the Champollion Tool Kit (Ma, 2006). Af-
ter discarding sentences with no aligned counter-
part, a total of 402 documents and 8,249 paral-
lel sentence pairs were used for evaluation. We
will refer to this evaluation set as full-set. We use
odd-numbered documents as the dev set and even-
numbered documents as the blind test set. We
did not perform parameter tuning on the dev set
to optimize performance, instead we fix the ini-
tial learning rate to 0.5 and maximum iterations to
1,000 in all DD experiments. We only use the dev
set for model development.
The Stanford CRF-based NER tagger was used
as the monolingual component in our models
(Finkel et al, 2005). It also serves as a state-
of-the-art monolingual baseline for both English
and Chinese. For English, we use the default tag-
ger setting from Finkel et al (2005). For Chi-
nese, we use an improved set of features over the
default tagger, which includes distributional sim-
ilarity features trained on large amounts of non-
overlapping data.1
We train the two CRF models on all portions
of the OntoNotes corpus that are annotated with
named entity tags, except the parallel-aligned por-
tion which we reserve for development and test
purposes. In total, there are about 660 train-
ing documents (?16k sentences) for Chinese and
1,400 documents (?39k sentences) for English.
Out of the 18 named entity types that are an-
notated in OntoNotes, which include person, lo-
cation, date, money, and so on, we select the four
most commonly seen named entity types for evalu-
ation. They are person, location, organization and
GPE. All entities of these four types are converted
to the standard BIO format, and background to-
kens and all other entity types are marked with
tag O. When we consider label agreements over
aligned word pairs in all bilingual agreement mod-
els, we ignore the distinction between B- and I-
tags.
We report standard NER measures (entity pre-
cision (P), recall (R) and F1 score) on the test
set. Statistical significance tests are done using the
paired bootstrap resampling method (Efron and
Tibshirani, 1993).
For alignment experiments, we train two uni-
1The exact feature set and the CRF implementation
can be found here: http://nlp.stanford.edu/
software/CRF-NER.shtml
directional HMM models as our baseline and
monolingual alignment models. The parameters
of the HMM were initialized by IBM Model 1 us-
ing the agreement-based EM training algorithms
from Liang et al (2006). Each model is trained
for 2 iterations over a parallel corpus of 12 mil-
lion English words and Chinese words, almost
twice as much data as used in previous work that
yields state-of-the-art unsupervised alignment re-
sults (DeNero and Klein, 2008; Haghighi et al,
2009; DeNero and Macherey, 2011).
Word alignment evaluation is done over the
sections of OntoNotes that have matching gold-
standard word alignment annotations from GALE
Y1Q4 dataset.2 This subset contains 288 docu-
ments and 3,391 sentence pairs. We will refer
to this subset as wa-subset. This evaluation set
is over 20 times larger than the 150 sentences
set used in most past evaluations (DeNero and
Klein, 2008; Haghighi et al, 2009; DeNero and
Macherey, 2011).
Alignments input to the BI-NER model are
produced by thresholding the averaged posterior
probability at 0.5. In joint NER and alignment ex-
periments, instead of posterior thresholding, we
take the direct intersection of the Viterbi-best
alignment of the two directional models. We re-
port the standard P, R, F1 and Alignment Error
Rate (AER) measures for alignment experiments.
An important past work to make comparisons
with is Burkett et al (2010b). Their method
is similar to ours in that they also model bilin-
gual agreement in conjunction with two CRF-
based monolingual models. But instead of using
just the PMI scores of bilingual NE pairs, as in
our work, they employed a feature-rich log-linear
model to capture bilingual correlations. Parame-
ters in their log-linear model require training with
bilingually annotated data, which is not readily
available. To counter this problem, they proposed
an ?up-training? method which simulates a super-
vised learning environment by pairing a weak clas-
sifier with strong classifiers, and train the bilin-
gual model to rank the output of the strong classi-
fier highly among the N-best outputs of the weak
classifier. In order to compare directly with their
method, we obtained the code behind Burkett et
al. (2010b) and reproduced their experimental set-
ting for the OntoNotes data. An extra set of 5,000
unannotated parallel sentence pairs are used for
2LDC Catalog No. LDC2006E86.
1078
Chinese English
P R F1 P R F1
Mono 76.89 61.64 68.42 81.98 74.59 78.11
Burkett 77.52 65.84 71.20 82.28 76.64 79.36
Bi-soft 79.14 71.55 75.15 82.58 77.96 80.20
Table 1: NER results on bilingual parallel test set.
Best numbers on each measure that are statistically
significantly better than the monolingual baseline
and Burkett et al (2010b) are highlighted in bold.
training the reranker, and the reranker model se-
lection was performed on the development dataset.
5 Bilingual NER Results
The main results on bilingual NER over the test
portion of full-set are shown in Table 1. We
initially experimented with the hard agreement
model, but it performs quite poorly for reasons we
discussed in Section 2.2. The BI-NER model with
soft agreement constraints, however, significantly
outperforms all baselines. In particular, it achieves
an absolute F1 improvement of 6.7% in Chinese
and 2.1% in English over the CRF monolingual
baselines.
A well-known issue with the DD method is
that when the model does not necessarily con-
verge, then the procedure could be very sensi-
tive to hyper-parameters such as initial step size
and early termination criteria. If a model only
gives good performance with well-tuned hyper-
parameters, then we must have manually anno-
tated data for tuning, which would significantly
reduce the applicability and portability of this
method to other language pairs and tasks. To eval-
uate the parameter sensitivity of our model, we
run the model from 50 to 3000 iterations before
early stopping, and with 6 different initial step
sizes from 0.01 to 1. The results are shown in Fig-
ure 2. The soft agreement model does not seem to
be sensitive to initial step size and almost always
converges to a superior solution than the baseline.
6 Joint NER and Alignment Results
We present results for the BI-NER-WA model
in Table 2. By jointly decoding NER with word
alignment, our model not only maintains signifi-
cant improvements in NER performance, but also
yields significant improvements to alignment per-
formance. Overall, joint decoding with NER alone
yields a 10.8% error reduction in AER over the
baseline HMM-aligners, and also gives improve-
0 0.01 0.05
0.1 0.2 0.5
1 2
30001000
800500
300100
5073
74
75
76
77
78
79
80
initial step sizemax no. of iterations
F1 sc
ore
Figure 2: Performance variance of the soft agree-
ment models on the Chinese dev dataset, as a func-
tion of step size (x-axis) and maximum number of
iterations before early stopping (y-axis).
ment over BI-NER in NER. Adding additional
neighborhood constraints gives a further 6% er-
ror reduction in AER, at the cost of a small loss
in Chinese NER. In terms of word alignment re-
sults, we see great increases in F1 and recall, but
precision goes down significantly. This is be-
cause the joint decoding algorithm promotes an ef-
fect of ?soft-union?, by encouraging the two uni-
directional aligners to agree more often. Adding
the neighborhood constraints further enhances this
union effect.
7 Error Analysis and Discussion
We can examine the example in Figure 3 to gain
an understanding of the model?s performance. In
this example, a snippet of a longer sentence pair is
shown with NER and word alignment results. The
monolingual Chinese tagger provides a strong cue
that word f6 is a person name because the unique
4-character word pattern is commonly associated
with foreign names in Chinese, and also the word
is immediately preceded by the word ?president?.
The English monolingual tagger, however, con-
fuses the aligned word e0 with a GPE.
Our bilingual NER model is able to correct this
error as expected. Similarly, the bilingual model
corrects the error over e11. However, the model
also propagates labeling errors from the English
side over the entity ?Tibet Autonomous Region? to
the Chinese side. Nevertheless, the resulting Chi-
nese tags are arguably more useful than the origi-
nal tags assigned by the baseline model.
In terms of word alignment, the HMM models
failed badly on this example because of the long
1079
NER-Chinese NER-English word alignment
P R F1 P R F1 P R F1 AER
HMM-WA - - - - - - 90.43 40.95 56.38 43.62
Mono-CRF 82.50 66.58 73.69 84.24 78.70 81.38 - - - -
Bi-NER 84.87 75.30 79.80 84.47 81.45 82.93 - - - -
Bi-NER-WA 84.42 76.34 80.18 84.25 82.20 83.21 77.45 50.43 61.09 38.91
Bi-NER-WA+NC 84.25 75.09 79.41 84.28 82.17 83.21 76.67 54.44 63.67 36.33
Table 2: Joint alignment and NER test results. +NC means incorporating additional neighbor constraints
from DeNero and Macherey (2011) to the model. Best number in each column is highlighted in bold.
f0 f1 f2 f3 f4 f5 f6
e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11
Suolangdaji , president of Tibet Auto. Region branch of Bank of China
B-PER O O O B-GPE I-GPE I-GPE O O B-ORG I-ORG I-ORG
B-PER O O O [B-LOC] [I-LOC] [I-LOC] O O B-ORG I-ORG I-ORG
[B-GPE] O O O [B-LOC] [I-LOC] [I-LOC] O O [O] [O] [B-GPE]
?? ?? ?? ??? ?? ?? ????
B-ORG I-ORG B-GPE O O O B-PER
B-ORG I-ORG [B-LOC] [I-LOC] O O B-PER
B-ORG I-ORG [O] O O O B-PER
Figure 3: An example output of our BI-NER-WA model. Dotted alignment links are the oracle, dashed
links are alignments from HMM baseline, and solid links are outputs of our model. Entity tags in the
gold line (closest to nodes ei and fj) are the gold-standard tags; in the green line (second closest to
nodes) are output from our model; and in the crimson line (furthest from nodes) are baseline output.
distance swapping phenomena. The two unidirec-
tional HMMs also have strong disagreements over
the alignments, and the resulting baseline aligner
output only recovers two links. If we were to take
this alignment as fixed input, most likely we would
not be able to recover the error over e11, but the
joint decoding method successfully recovered 4
more links, and indirectly resulted in the NER tag-
ging improvement discussed above.
8 Related Work
The idea of employing bilingual resources to im-
prove over monolingual systems has been ex-
plored by much previous work. For example,
Huang et al (2009) improved parsing performance
using a bilingual parallel corpus. In the NER
domain, Li et al (2012) presented a cyclic CRF
model very similar to our BI-NER model, and
performed approximate inference using loopy be-
lief propagation. The feature-rich CRF formula-
tion of bilingual edge potentials in their model is
much more powerful than our simple PMI-based
bilingual edge model. Adding a richer bilingual
edge model might well further improve our results,
and this is a possible direction for further experi-
mentation. However, a big drawback of this ap-
proach is that training such a feature-rich model
requires manually annotated bilingual NER data,
which can be prohibitively expensive to generate.
How and where to obtain training signals with-
out manual supervision is an interesting and open
question. One of the most interesting papers in this
regard is Burkett et al (2010b), which explored
an ?up-training? mechanism by using the outputs
from a strong monolingual model as ground-truth,
and simulated a learning environment where a
bilingual model is trained to help a ?weakened?
monolingual model to recover the results of the
strong model. It is worth mentioning that since
our method does not require additional training
and can take pretty much any existing model as
?black-box? during decoding, the richer and more
accurate bilingual model learned from Burkett et
al. (2010b) can be directly plugged into our model.
A similar dual decomposition algorithm to ours
was proposed by Riedel and McCallum (2011)
for biomedical event detection. In their Model
3, the trigger and argument extraction models
are reminiscent of the two monolingual CRFs in
our model; additional binding agreements are en-
forced over every protein pair, similar to how we
enforce agreement between every aligned word
1080
pair. Martins et al (2011b) presented a new DD
method that combines the power of DD with the
augmented Lagrangian method. They showed
that their method can achieve faster convergence
than traditional sub-gradient methods in models
with many overlapping components (Martins et
al., 2011a). This method is directly applicable to
our work.
Another promising direction for improving
NER performance is in enforcing global label
consistency across documents, which is an idea
that has been greatly explored in the past (Sut-
ton and McCallum, 2004; Bunescu and Mooney,
2004; Finkel et al, 2005). More recently, Rush
et al (2012) and Chieu and Teow (2012) have
shown that combining local prediction models
with global consistency models, and enforcing
agreement via DD is very effective. It is straight-
forward to incorporate an additional global consis-
tency model into our model for further improve-
ments.
Our joint alignment and NER decoding ap-
proach is inspired by prior work on improving
alignment quality through encouraging agreement
between bi-directional models (Liang et al, 2006;
DeNero and Macherey, 2011). Instead of enforc-
ing agreement in the alignment space based on
best sequences found by Viterbi, we could opt
to encourage agreement between posterior prob-
ability distributions, which is related to the pos-
terior regularization work by Grac?a et al (2008).
Cromie`res and Kurohashi (2009) proposed an ap-
proach that takes phrasal bracketing constraints
from parsing outputs, and uses them to enforce
phrasal alignments. This idea is similar to our joint
alignment and NER approach, but in our case the
phrasal constraints are indirectly imposed by en-
tity spans. We also differ in the implementation
details, where in their case belief propagation is
used in both training and Viterbi inference.
Burkett et al (2010a) presented a supervised
learning method for performing joint parsing and
word alignment using log-linear models over parse
trees and an ITG model over alignment. The
model demonstrates performance improvements
in both parsing and alignment, but shares the com-
mon limitations of other supervised work in that it
requires manually annotated bilingual joint pars-
ing and word alignment data.
Chen et al (2010) also tackled the problem of
joint alignment and NER. Their method employs a
set of heuristic rules to expand a candidate named
entity set generated by monolingual taggers, and
then rank those candidates using a bilingual named
entity dictionary. Our approach differs in that we
provide a probabilistic formulation of the problem
and do not require pre-existing NE dictionaries.
9 Conclusion
We introduced a graphical model that combines
two HMM word aligners and two CRF NER tag-
gers into a joint model, and presented a dual de-
composition inference method for performing ef-
ficient decoding over this model. Results from
NER and word alignment experiments suggest that
our method gives significant improvements in both
NER and word alignment. Our techniques make
minimal assumptions about the underlying mono-
lingual components, and can be adapted for many
other tasks such as parsing.
Acknowledgments
The authors would like to thank Rob Voigt and
the three anonymous reviewers for their valuable
comments and suggestions. We gratefully ac-
knowledge the support of the National Natural
Science Foundation of China (NSFC) via grant
61133012, the National ?863? Project via grant
2011AA01A207 and 2012AA011102, the Min-
istry of Education Research of Social Sciences
Youth funded projects via grant 12YJCZH304,
and the support of the U.S. Defense Advanced
Research Projects Agency (DARPA) Broad Op-
erational Language Translation (BOLT) program
through IBM.
Any opinions, findings, and conclusion or rec-
ommendations expressed in this material are those
of the authors and do not necessarily reflect the
view of DARPA, or the US government.
References
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual
resources. In Proceedings of ACL.
Bogdan Babych and Anthony Hartley. 2003. Im-
proving machine translation quality with automatic
named entity recognition. In Proceedings of the
7th International EAMT workshop on MT and other
Language Technology Tools, Improving MT through
other Language Technology Tools: Resources and
Tools for Building MT.
1081
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific, New York.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1991. Word-
sense disambiguation using statistical methods. In
Proceedings of ACL.
Razvan Bunescu and Raymond J. Mooney. 2004.
Collective information extraction with relational
Markov networks. In Proceedings of ACL.
David Burkett, John Blitzer, and Dan Klein. 2010a.
Joint parsing and alignment with weakly synchro-
nized grammars. In Proceedings of NAACL-HLT.
David Burkett, Slav Petrov, John Blitzer, and Dan
Klein. 2010b. Learning better monolingual mod-
els with unannotated bilingual text. In Proceedings
of CoNLL.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su.
2010. On jointly recognizing and aligning bilingual
named entities. In Proceedings of ACL.
Hai Leong Chieu and Loo-Nin Teow. 2012. Com-
bining local and non-local information with dual de-
composition for named entity recognition from text.
In Proceedings of 15th International Conference on
Information Fusion (FUSION).
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proceedings of
EACL/ IJCNLP.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of ACL.
Brad Efron and Robert Tibshirani. 1993. An Introduc-
tion to the Bootstrap. Chapman & Hall, New York.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of ACL.
Joao Grac?a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In Proceedings of NIPS.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
NAACL-HLT.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In Proceedings of the 2002 International Con-
ference on Multimodal Interfaces (ICMI).
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from Wikipedia. In Pro-
ceedings of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and
Fei Huang. 2012. Joint bilingual name tagging for
parallel corpora. In Proceedings of CIKM.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proceedings of LREC.
Andre? F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Ma?rio A. T. Figueiredo. 2011a. Dual
decomposition with many overlapping components.
In Proceedings of EMNLP.
Andre F. T. Martins, Noah A. Smith, Eric P. Xing,
Pedro M. Q. Aguiar, and Ma?rio A. T. Figueiredo.
2011b. Augmenting dual decomposition for map in-
ference. In Proceedings of the International Work-
shop on Optimization for Machine Learning (OPT
2010).
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of EMNLP.
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and Lagrangian relax-
ation for inference in natural language processing.
JAIR, 45:305?362.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proceedings of EMNLP.
Alexander M. Rush, Roi Reichert, Michael Collins, and
Amir Globerson. 2012. Improved parsing and POS
tagging using inter-sentence consistency constraints.
In Proceedings of EMNLP.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant entities
in information extraction. In Proceedings of ICML
Workshop on Statistical Relational Learning and Its
connections to Other Fields.
1082
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199?1209,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Semantic Hierarchies via Word Embeddings
Ruiji Fu
?
, Jiang Guo
?
, Bing Qin
?
, Wanxiang Che
?
, Haifeng Wang
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Baidu Inc., Beijing, China
{rjfu, jguo, bqin, car, tliu}@ir.hit.edu.cn
wanghaifeng@baidu.com
Abstract
Semantic hierarchy construction aims to
build structures of concepts linked by
hypernym?hyponym (?is-a?) relations. A
major challenge for this task is the
automatic discovery of such relations.
This paper proposes a novel and effec-
tive method for the construction of se-
mantic hierarchies based on word em-
beddings, which can be used to mea-
sure the semantic relationship between
words. We identify whether a candidate
word pair has hypernym?hyponym rela-
tion by using the word-embedding-based
semantic projections between words and
their hypernyms. Our result, an F-score
of 73.74%, outperforms the state-of-the-
art methods on a manually labeled test
dataset. Moreover, combining our method
with a previous manually-built hierarchy
extension method can further improve F-
score to 80.29%.
1 Introduction
Semantic hierarchies are natural ways to orga-
nize knowledge. They are the main components
of ontologies or semantic thesauri (Miller, 1995;
Suchanek et al, 2008). In the WordNet hierar-
chy, senses are organized according to the ?is-a?
relations. For example, ?dog? and ?canine? are
connected by a directed edge. Here, ?canine? is
called a hypernym of ?dog.? Conversely, ?dog?
is a hyponym of ?canine.? As key sources
of knowledge, semantic thesauri and ontologies
can support many natural language processing
applications. However, these semantic resources
are limited in its scope and domain, and their
manual construction is knowledge intensive and
time consuming. Therefore, many researchers
?
Email correspondence.
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite ???medicinal plant
??medicine
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
Figure 1: An example of semantic hierarchy con-
struction.
have attempted to automatically extract semantic
relations or to construct taxonomies.
A major challenge for this task is the auto-
matic discovery of hypernym-hyponym relations.
Fu et al (2013) propose a distant supervision
method to extract hypernyms for entities from
multiple sources. The output of their model is
a list of hypernyms for a given enity (left pan-
el, Figure 1). However, there usually also exists
hypernym?hyponym relations among these hy-
pernyms. For instance, ??? (plant)? and
???? (Ranunculaceae)? are both hyper-
nyms of the entity ??? (aconit),? and ??
? (plant)? is also a hypernym of ????
(Ranunculaceae).? Given a list of hypernyms
of an entity, our goal in the present work is to
construct a semantic hierarchy of these hypernyms
(right panel, Figure 1).
1
Some previous works extend and refine
manually-built semantic hierarchies by using other
resources (e.g., Wikipedia) (Suchanek et al,
2008). However, the coverage is limited by the
scope of the resources. Several other works relied
heavily on lexical patterns, which would suffer
from deficiency because such patterns can only
cover a small proportion of complex linguistic cir-
cumstances (Hearst, 1992; Snow et al, 2005).
1
In this study, we focus on Chinese semantic hierarchy
construction. The proposed method can be easily adapted to
other languages.
1199
Besides, distributional similarity methods (Kotler-
man et al, 2010; Lenci and Benotto, 2012) are
based on the assumption that a term can only be
used in contexts where its hypernyms can be used
and that a term might be used in any contexts
where its hyponyms are used. However, it is not
always rational. Our previous method based on
web mining (Fu et al, 2013) works well for hy-
pernym extraction of entity names, but it is unsuit-
able for semantic hierarchy construction which in-
volves many words with broad semantics. More-
over, all of these methods do not use the word
semantics effectively.
This paper proposes a novel approach for se-
mantic hierarchy construction based on word em-
beddings. Word embeddings, also known as dis-
tributed word representations, typically represent
words with dense, low-dimensional and real-
valued vectors. Word embeddings have been
empirically shown to preserve linguistic regular-
ities, such as the semantic relationship between
words (Mikolov et al, 2013b). For example,
v(king) ? v(queen) ? v(man) ? v(woman),
where v(w) is the embedding of the word w. We
observe that a similar property also applies to the
hypernym?hyponym relationship (Section 3.3),
which is the main inspiration of the present study.
However, we further observe that hypernym?
hyponym relations are more complicated than a
single offset can represent. To address this chal-
lenge, we propose a more sophisticated and gen-
eral method ? learning a linear projection which
maps words to their hypernyms (Section 3.3.1).
Furthermore, we propose a piecewise linear pro-
jection method based on relation clustering to
better model hypernym?hyponym relations (Sec-
tion 3.3.2). Subsequently, we identify whether an
unknown word pair is a hypernym?hyponym re-
lation using the projections (Section 3.4). To the
best of our knowledge, we are the first to apply
word embeddings to this task.
For evaluation, we manually annotate a dataset
containing 418 Chinese entities and their hyper-
nym hierarchies, which is the first dataset for this
task as far as we know. The experimental results
show that our method achieves an F-score of
73.74% which significantly outperforms the pre-
vious state-of-the-art methods. Moreover, com-
bining our method with the manually-built hier-
archy extension method proposed by Suchanek et
al. (2008) can further improve F-score to 80.29%.
2 Background
As main components of ontologies, semantic hi-
erarchies have been studied by many researchers.
Some have established concept hierarchies based
on manually-built semantic resources such as
WordNet (Miller, 1995). Such hierarchies have
good structures and high accuracy, but their cov-
erage is limited to fine-grained concepts (e.g.,
?Ranunculaceae? is not included in Word-
Net.). We have made similar obsevation that about
a half of hypernym?hyponym relations are absent
in a Chinese semantic thesaurus. Therefore, a
broader range of resources is needed to supple-
ment the manually built resources. In the construc-
tion of the famous ontology YAGO, Suchanek et
al. (2008) link the categories in Wikipedia onto
WordNet. However, the coverage is still limited
by the scope of Wikipedia.
Several other methods are based on lexical
patterns. They use manually or automatically
constructed lexical patterns to mine hypernym?
hyponym relations from text corpora. A hierarchy
can then be built based on these pairwise relations.
The pioneer work by Hearst (1992) has found
out that linking two noun phrases (NPs) via cer-
tain lexical constructions often implies hypernym
relations. For example, NP
1
is a hypernym of NP
2
in the lexical pattern ?such NP
1
as NP
2
.? Snow et
al. (2005) propose to automatically extract large
numbers of lexico-syntactic patterns and subse-
quently detect hypernym relations from a large
newswire corpus. Their method relies on accurate
syntactic parsers, and the quality of the automat-
ically extracted patterns is difficult to guarantee.
Generally speaking, these pattern-based methods
often suffer from low recall or precision because
of the coverage or the quality of the patterns.
The distributional methods assume that the con-
texts of hypernyms are broader than the ones of
their hyponyms. For distributional similarity com-
puting, each word is represented as a semantic
vector composed of the pointwise mutual infor-
mation (PMI) with its contexts. Kotlerman et al
(2010) design a directional distributional measure
to infer hypernym?hyponym relations based on
the standard IR Average Precision evaluation mea-
sure. Lenci and Benotto (2012) propose anoth-
er measure focusing on the contexts that hyper-
nyms do not share with their hyponyms. However,
broader semantics may not always infer broader
contexts. For example, for terms ?Obama? and
1200
?American people?, it is hard to say whose
contexts are broader.
Our previous work (Fu et al, 2013) applies a
web mining method to discover the hypernyms of
Chinese entities from multiple sources. We as-
sume that the hypernyms of an entity co-occur
with it frequently. It works well for named enti-
ties. But for class names (e.g., singers in Hong
Kong, tropical fruits) with wider range of mean-
ings, this assumption may fail.
In this paper, we aim to identify hypernym?
hyponym relations using word embeddings, which
have been shown to preserve good properties for
capturing semantic relationship between words.
3 Method
In this section, we first define the task formally.
Then we elaborate on our proposed method com-
posed of three major steps, namely, word embed-
ding training, projection learning, and hypernym?
hyponym relation identification.
3.1 Task Definition
Given a list of hypernyms of an entity, our goal is
to construct a semantic hierarchy on it (Figure 1).
We represent the hierarchy as a directed graph
G, in which the nodes denote the words, and the
edges denote the hypernym?hyponym relations.
Hypernym-hyponym relations are asymmetric and
transitive when words are unambiguous:
? ?x, y ? L : x
H
??y ? ?(y
H
??x)
? ?x, y, z ? L : (x
H
??z ? z
H
??y)? x
H
??y
Here, L denotes the list of hypernyms. x, y and
z denote the hypernyms in L. We use
H
?? to
represent a hypernym?hyponym relation in this
paper. Actually, x, y and z are unambiguous as
the hypernyms of a certain entity. Therefore, G
should be a directed acyclic graph (DAG).
3.2 Word Embedding Training
Various models for learning word embeddings
have been proposed, including neural net lan-
guage models (Bengio et al, 2003; Mnih and
Hinton, 2008; Mikolov et al, 2013b) and spec-
tral models (Dhillon et al, 2011). More recent-
ly, Mikolov et al (2013a) propose two log-linear
models, namely the Skip-gram and CBOW model,
to efficiently induce word embeddings. These two
models can be trained very efficiently on a large-
scale corpus because of their low time complexity.
No. Examples
1
v(?)? v(??) ? v(?)? v(??)
v(shrimp)? v(prawn) ? v(fish)? v(gold fish)
2
v(??)? v(??) ? v(??)? v(??)
v(laborer)? v(carpenter) ? v(actor)? v(clown)
3
v(??)? v(??) 6? v(?)? v(??)
v(laborer)? v(carpenter) 6? v(fish)? v(gold fish)
Table 1: Embedding offsets on a sample of
hypernym?hyponym word pairs.
Additionally, their experiment results have shown
that the Skip-gram model performs best in identi-
fying semantic relationship among words. There-
fore, we employ the Skip-gram model for estimat-
ing word embeddings in this study.
The Skip-gram model adopts log-linear classi-
fiers to predict context words given the current
word w(t) as input. First, w(t) is projected to its
embedding. Then, log-linear classifiers are em-
ployed, taking the embedding as input and pre-
dict w(t)?s context words within a certain range,
e.g. k words in the left and k words in the
right. After maximizing the log-likelihood over
the entire dataset using stochastic gradient descent
(SGD), the embeddings are learned.
3.3 Projection Learning
Mikolov et al (2013b) observe that word em-
beddings preserve interesting linguistic regulari-
ties, capturing a considerable amount of syntac-
tic/semantic relations. Looking at the well-known
example: v(king) ? v(queen) ? v(man) ?
v(woman), it indicates that the embedding offsets
indeed represent the shared semantic relation be-
tween the two word pairs.
We observe that the same property also ap-
plies to some hypernym?hyponym relations. As
a preliminary experiment, we compute the em-
bedding offsets between some randomly sampled
hypernym?hyponym word pairs and measure their
similarities. The results are shown in Table 1.
The first two examples imply that a word can
also be mapped to its hypernym by utilizing word
embedding offsets. However, the offset from
?carpenter? to ?laborer? is distant from
the one from ?gold fish? to ?fish,? indicat-
ing that hypernym?hyponym relations should be
more complicated than a single vector offset can
represent. To verify this hypothesis, we com-
pute the embedding offsets over all hypernym?
1201
???-????sportsman - footballer ??-???staff - civil servant??-??laborer - gardener??-???seaman - navigator??-??actor - singer ??-??actor - protagonist??-??actor - clown
??-??position - headmaster
??-???actor - matador
??-???laborer - temporary worker ??-??laborer - carpenter ??-???position ? consul general
??-??staff - airline hostess??-???staff - salesclerk??-???staff - conductor?-??chicken - cock?-????sheep - small-tail Han sheep?-??sheep - ram ?-??equus - zebra ?-??shrimp - prawn
?-??dog - police dog?-???rabbit - wool rabbit
??-???dolphin - white-flag dolphin ?-??fish - shark ?-???fish - tropical fish?-??fish - gold fish
?-??crab - sea crab
?-??donkey - wild ass
Figure 2: Clusters of the vector offsets in training data. The figure shows that the vector offsets distribute
in some clusters. The left cluster shows some hypernym?hyponym relations about animals. The right
one shows some relations about people?s occupations.
hyponym word pairs in our training data and vi-
sualize them.
2
Figure 2 shows that the relations
are adequately distributed in the clusters, which
implies that hypernym?hyponym relations in-
deed can be decomposed into more fine-grained
relations. Moreover, the relations about animals
are spatially close, but separate from the relations
about people?s occupations.
To address this challenge, we propose to learn
the hypernym?hyponym relations using projection
matrices.
3.3.1 A Uniform Linear Projection
Intuitively, we assume that all words can be pro-
jected to their hypernyms based on a uniform tran-
sition matrix. That is, given a word x and its hy-
pernym y, there exists a matrix ? so that y = ?x.
For simplicity, we use the same symbols as the
words to represent the embedding vectors. Ob-
taining a consistent exact ? for the projection of
all hypernym?hyponym pairs is difficult. Instead,
we can learn an approximate ? using Equation 1
on the training data, which minimizes the mean-
squared error:
?
?
= arg min
?
1
N
?
(x,y)
? ?x? y ?
2
(1)
where N is the number of (x, y) word pairs in
the training data. This is a typical linear regres-
sion problem. The only difference is that our pre-
dictions are multi-dimensional vectors instead of
scalar values. We use SGD for optimization.
2
Principal Component Analysis (PCA) is applied for di-
mensionality reduction.
3.3.2 Piecewise Linear Projections
A uniform linear projection may still be under-
representative for fitting all of the hypernym?
hyponym word pairs, because the relations are
rather diverse, as shown in Figure 2. To better
model the various kinds of hypernym?hyponym
relations, we apply the idea of piecewise linear re-
gression (Ritzema, 1994) in this study.
Specifically, the input space is first segmented
into several regions. That is, all word pairs (x, y)
in the training data are first clustered into sever-
al groups, where word pairs in each group are
expected to exhibit similar hypernym?hyponym
relations. Each word pair (x, y) is represented
with their vector offsets: y ? x for clustering.
The reasons are twofold: (1) Mikolov?s work has
shown that the vector offsets imply a certain lev-
el of semantic relationship. (2) The vector off-
sets distribute in clusters well, and the word pairs
which are close indeed represent similar relations,
as shown in Figure 2.
Then we learn a separate projection for each
cluster, respectively (Equation 2).
?
?
k
= arg min
?
k
1
N
k
?
(x,y)?C
k
? ?
k
x? y ?
2
(2)
where N
k
is the amount of word pairs in the k
th
cluster C
k
.
We use the k-means algorithm for clustering,
where k is tuned on a development dataset.
3.3.3 Training Data
To learn the projection matrices, we extract train-
ing data from a Chinese semantic thesaurus,
Tongyi Cilin (Extended) (CilinE for short) which
1202
?
?
?
?
?
Root
L ev el 1
L ev el 2
L ev el 3
L ev el 4
L ev el 5
?  ob j ect
??  animal
??  insect
- -
??  dragonf ly
B
i
18
A
06@
?? : ?? 
( dragonf ly  :  animal)
?? : ?? 
( dragonf ly  :  insect)
C ilinEh y perny m-h y pony m pairs
S ense C ode:  B i1 8 A0 6 @
S ense C ode:  B i1 8 A
S ense C ode:  B i1 8
S ense C ode:  B i
S ense C ode:  B
?? : ?? 
( insect :  animal)
Figure 3: Hierarchy of CilinE and an Example of
Training Data Generation
contains 100,093 words (Che et al, 2010).
3
CilinE
is organized as a hierarchy of five levels, in which
the words are linked by hypernym?hyponym
relations (right panel, Figure 3). Each word in
CilinE has one or more sense codes (some words
are polysemous) that indicate its position in the hi-
erarchy.
The senses of words in the first level, such as
?? (object)? and ??? (time),? are very gen-
eral. The fourth level only has sense codes without
real words. Therefore, we extract words in the sec-
ond, third and fifth levels to constitute hypernym?
hyponym pairs (left panel, Figure 3).
Note that mapping one hyponym to multi-
ple hypernyms with the same projection (?x is
unique) is difficult. Therefore, the pairs with the
same hyponym but different hypernyms are ex-
pected to be clustered into separate groups. Fig-
ure 3 shows that the word ?dragonfly? in the
fifth level has two hypernyms: ?insect? in the
third level and ?animal? in the second level.
Hence the relations dragonfly
H
?? insect and
dragonfly
H
?? animal should fall into differ-
ent clusters.
In our implementation, we apply this constraint
by simply dividing the training data into two cat-
egories, namely, direct and indirect. Hypernym-
hyponym word pair (x, y) is classified into the di-
rect category, only if there doesn?t exist another
word z in the training data, which is a hypernym of
x and a hyponym of y. Otherwise, (x, y) is classi-
fied into the indirect category. Then, data in these
two categories are clustered separately.
3
www.ltp-cloud.com/download/
x
y
?k
? 
x'
?l
Figure 4: In this example, ?
k
x is located in the
circle with center y and radius ?. So y is consid-
ered as a hypernym of x. Conversely, y is not a
hypernym of x
?
.
x
y
z
x
y
(a) (b)
z
x
y
Figure 5: (a) If d(?
j
y, x) > d(?
k
x, y), we re-
move the path from y to x; (b) if d(?
j
y, x) >
d(?
k
x, z) and d(?
j
y, x) > d(?
i
z, y), we reverse
the path from y to x.
3.4 Hypernym-hyponym Relation
Identification
Upon obtaining the clusters of training data and
the corresponding projections, we can identify
whether two words have a hypernym?hyponym re-
lation. Given two words x and y, we find cluster
C
k
whose center is closest to the offset y ? x, and
obtain the corresponding projection ?
k
. For y to
be considered a hypernym of x, one of the two
conditions below must hold.
Condition 1: The projection ?
k
puts ?
k
x close
enough to y (Figure 4). Formally, the euclidean
distance between ?
k
x and y: d(?
k
x, y) must be
less than a threshold ?.
d(?
k
x, y) =? ?
k
x? y ?
2
< ? (3)
Condition 2: There exists another word z sat-
isfying x
H
??z and z
H
??y. In this case, we use the
transitivity of hypernym?hyponym relations.
Besides, the final hierarchy should be a DAG
as discussed in Section 3.1. However, the pro-
jection method cannot guarantee that theoretical-
ly, because the projections are learned from pair-
wise hypernym?hyponym relations without the w-
hole hierarchy structure. All pairwise hypernym?
hyponym relation identification methods would
suffer from this problem actually. It is an inter-
esting problem how to construct a globally opti-
1203
mal semantic hierarchy conforming to the form
of a DAG. But this is not the focus of this paper.
So if some conflicts occur, that is, a relation cir-
cle exists, we remove or reverse the weakest path
heuristically (Figure 5). If a circle has only two
nodes, we remove the weakest path. If a circle has
more than two nodes, we reverse the weakest path
to form an indirect hypernym?hyponym relation.
4 Experimental Setup
4.1 Experimental Data
In this work, we learn word embeddings from a
Chinese encyclopedia corpus named Baidubaike
4
,
which contains about 30 million sentences (about
780 million words). The Chinese segmentation
is provided by the open-source Chinese language
processing platform LTP
5
(Che et al, 2010).
Then, we employ the Skip-gram method (Section
3.2) to train word embeddings. Finally we obtain
the embedding vectors of 0.56 million words.
The training data for projection learning is
collected from CilinE (Section 3.3.3). We ob-
tain 15,247 word pairs of hypernym?hyponym
relations (9,288 for direct relations and 5,959 for
indirect relations).
For evaluation, we collect the hypernyms for
418 entities, which are selected randomly from
Baidubaike, following Fu et al (2013). We then
ask two annotators to manually label the seman-
tic hierarchies of the correct hypernyms. The final
data set contains 655 unique hypernyms and 1,391
hypernym?hyponym relations among them. We
randomly split the labeled data into 1/5 for de-
velopment and 4/5 for testing (Table 2). The hi-
erarchies are represented as relations of pairwise
words. We measure the inter-annotator agreement
using the kappa coefficient (Siegel and Castel-
lan Jr, 1988). The kappa value is 0.96, which indi-
cates a good strength of agreement.
4.2 Evaluation Metrics
We use precision, recall, and F-score as our met-
rics to evaluate the performances of the methods.
Since hypernym?hyponym relations and its re-
verse (hyponym?hypernym) have one-to-one cor-
respondence, their performances are equal. For
4
Baidubaike (baike.baidu.com) is one of the largest
Chinese encyclopedias containing more than 7.05 million en-
tries as of September, 2013.
5
www.ltp-cloud.com/demo/
Relation
# of word pairs
Dev. Test
hypernym?hyponym 312 1,079
hyponym?hypernym
?
312 1,079
unrelated 1,044 3,250
Total 1,668 5,408
Table 2: The evaluation data.
?
Since hypernym?
hyponym relations and hyponym?hypernym
relations have one-to-one correspondence, their
numbers are the same.
1 5 
10 15 
20 
0.45 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
1 10 20 30 40 50 60 Indirect 
F1-
Sco
re 
Direct 
Figure 6: Performance on development data w.r.t.
cluster size.
simplicity, we only report the performance of the
former in the experiments.
5 Results and Analysis
5.1 Varying the Amount of Clusters
We first evaluate the effect of different number of
clusters based on the development data. We vary
the numbers of the clusters both for the direct and
indirect training word pairs.
As shown in Figure 6, the performance of clus-
tering is better than non-clustering (when the clus-
ter number is 1), thus providing evidences that
learning piecewise projections based on clustering
is reasonable. We finally set the numbers of the
clusters of direct and indirect to 20 and 5, respec-
tively, where the best performances are achieved
on the development data.
5.2 Comparison with Previous Work
In this section, we compare the proposed method
with previous methods, including manually-built
hierarchy extension, pairwise relation extraction
1204
P(%) R(%) F(%)
M
Wiki+CilinE
92.41 60.61 73.20
M
Pattern
97.47 21.41 35.11
M
Snow
60.88 25.67 36.11
M
balApinc
54.96 53.38 54.16
M
invCL
49.63 62.84 55.46
M
Fu
87.40 48.19 62.13
M
Emb
80.54 67.99 73.74
M
Emb+CilinE
80.59 72.42 76.29
M
Emb+Wiki+CilinE
79.78 80.81 80.29
Table 3: Comparison of the proposed method with
existing methods in the test set.
Pattern Translation
w?[??|??] h w is a [a kind of] h
w [?]? h w[,] and other h
h [?]?[?] w h[,] called w
h [?] [?]? w h[,] such as w
h [?]??? w h[,] especially w
Table 4: Chinese Hearst-style lexical patterns. The
contents in square brackets are omissible.
based on patterns, word distributions, and web
mining (Section 2). Results are shown in Table 3.
5.2.1 Overall Comparison
M
Wiki+CilinE
refers to the manually-built hierar-
chy extension method of Suchanek et al (2008).
In our experiment, we use the category taxonomy
of Chinese Wikipedia
6
to extend CilinE. Table 3
shows that this method achieves a high precision
but also a low recall, mainly because of the limit-
ed scope of Wikipedia.
M
Pattern
refers to the pattern-based method of
Hearst (1992). We extract hypernym?hyponym
relations in the Baidubaike corpus, which is al-
so used to train word embeddings (Section 4.1).
We use the Chinese Hearst-style patterns (Table
4) proposed by Fu et al (2013), in which w rep-
resents a word, and h represents one of its hy-
pernyms. The result shows that only a small part
of the hypernyms can be extracted based on these
patterns because only a few hypernym relations
are expressed in these fixed patterns, and many are
expressed in highly flexible manners.
In the same corpus, we apply the method
M
Snow
originally proposed by Snow et al (2005).
The same training data for projections learn-
6
dumps.wikimedia.org/zhwiki/20131205/
ing from CilinE (Section 3.3.3) is used as
seed hypernym?hyponym pairs. Lexico-syntactic
patterns are extracted from the Baidubaike corpus
by using the seeds. We then develop a logistic re-
gression classifier based on the patterns to recog-
nize hypernym?hyponym relations. This method
relies on an accurate syntactic parser, and the qual-
ity of the automatically extracted patterns is diffi-
cult to guarantee.
We re-implement two previous distribution-
al methods M
balApinc
(Kotlerman et al, 2010)
and M
invCL
(Lenci and Benotto, 2012) in the
Baidubaike corpus. Each word is represented as a
feature vector in which each dimension is the PMI
value of the word and its context words. We com-
pute a score for each word pair and apply a thresh-
old to identify whether it is a hypernym?hyponym
relation.
M
Fu
refers to our previous web mining
method (Fu et al, 2013). This method mines hy-
pernyms of a given word w from multiple sources
and returns a ranked list of the hypernyms. We
select the hypernyms with scores over a threshold
of each word in the test set for evaluation. This
method assumes that frequent co-occurrence of a
noun or noun phrase n in multiple sources with w
indicate possibility of n being a hypernym of w.
The results presented in Fu et al (2013) show that
the method works well when w is an entity, but
not when w is a word with a common semantic
concept. The main reason may be that there are
relatively more introductory pages about entities
than about common words in the Web.
M
Emb
is the proposed method based on word
embeddings. Table 3 shows that the proposed
method achieves a better recall and F-score than
all of the previous methods do. It can significantly
(p < 0.01) improve the F-score over the state-of-
the-art method M
Wiki+CilinE
.
M
Emb
and M
CilinE
can also be combined. The
combination strategy is to simply merge all pos-
itive results from the two methods together, and
then to infer new relations based on the transitiv-
ity of hypernym?hyponym relations. The F-score
is further improved from 73.74% to 76.29%. Note
that, the combined method achieves a 4.43% re-
call improvement over M
Emb
, but the precision is
almost unchanged. The reason is that the infer-
ence based on the relations identified automatical-
ly may lead to error propagation. For example, the
relation x
H
??y is incorrectly identified by M
Emb
.
1205
P(%) R(%) F(%)
M
Wiki+CilinE
80.39 19.29 31.12
M
Emb+CilinE
71.16 52.80 60.62
M
Emb+Wiki+CilinE
69.13 61.65 65.17
Table 5: Performance on the out-of-CilinE data in
the test set.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Prec
ision
l l lllllll
l
l
l ll
l MEmb+Wiki+Ci l inEMEmb+Ci l inEMWiki+Ci l inE
Figure 7: Precision-Recall curves on the out-of-
CilinE data in the test set.
When the relation y
H
??z from M
CilinE
is added, it
will cause a new incorrect relation x
H
??z.
Combining M
Emb
with M
Wiki+CilinE
achieves
a 7% F-score improvement over the best baseline
M
Wiki+CilinE
. Therefore, the proposed method
is complementary to the manually-built hierarchy
extension method (Suchanek et al, 2008).
5.2.2 Comparison on the Out-of-CilinE Data
We are greatly interested in the practical perfor-
mance of the proposed method on the hypernym?
hyponym relations outside of CilinE. We say a
word pair is outside of CilinE, as long as there
is one word in the pair not existing in CilinE. In
our test data, about 62% word pairs are outside
of CilinE. Table 5 shows the performances of the
best baseline method and our method on the out-
of-CilinE data. The method exploiting the tax-
onomy in Wikipedia, M
Wiki+CilinE
, achieves the
highest precision but has a low recall. By con-
trast, our method can discover more hypernym?
hyponym relations with some loss of precision,
thereby achieving a more than 29% F-score im-
provement. The combination of these two meth-
ods achieves a further 4.5% F-score improvement
over M
Emb+CilinE
. Generally speaking, the pro-
posed method greatly improves the recall but dam-
ages the precision.
Actually, we can get different precisions and re-
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( a)  C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( b )  W ik ipedia+ C ilinE
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( c)  E mb edding
??organism
??plant
???Ranunculaceae
???Aconitum
??aconite
???medicinal plant
??medicine
( d)  E mb edding+ W ik ipedia+ C ilinE
Figure 8: An example for error analysis. The
red paths refer to the relations between the named
entity and its hypernyms extracted using the web
mining method (Fu et al, 2013). The black paths
with hollow arrows denote the relations identified
by the different methods. The boxes with dotted
borders refer to the concepts which are not linked
to correct positions.
calls by adjusting the threshold ? (Equation 3).
Figure 7 shows that M
Emb+CilinE
achieves a high-
er precision than M
Wiki+CilinE
when their recalls
are the same. When they achieve the same preci-
sion, the recall of M
Emb+CilinE
is higher.
5.3 Error Analysis and Discussion
We analyze error cases after experiments. Some
cases are shown in Figure 8. We can see that
there is only one general relation ??? (plant)?
H
?? ??? (organism)? existing in CilinE. Some
fine-grained relations exist in Wikipedia, but the
coverage is limited. Our method based on
word embeddings can discover more hypernym?
hyponym relations than the previous methods can.
When we combine the methods together, we get
the correct hierarchy.
Figure 8 shows that our method loses the
relation ???? (Aconitum)? H?? ????
(Ranunculaceae).? It is because they are
very semantically similar (their cosine similarity
is 0.9038). Their representations are so close to
each other in the embedding space that we have
not find projections suitable for these pairs. The
1206
error statistics show that when the cosine similari-
ties of word pairs are greater than 0.8, the recall is
only 9.5%. This kind of error accounted for about
10.9% among all the errors in our test set. One
possible solution may be adding more data of this
kind to the training set.
6 Related Work
In addition to the works mentioned in Section 2,
we introduce another set of related studies in this
section.
Evans (2004), Ortega-Mendoza et al (2007),
and Sang (2007) consider web data as a large cor-
pus and use search engines to identify hypernyms
based on the lexical patterns of Hearst (1992).
However, the low quality of the sentences in the
search results negatively influence the precision of
hypernym extraction.
Following the method for discovering patterns
automatically (Snow et al, 2005), McNamee et
al. (2008) apply the same method to extract hy-
pernyms of entities in order to improve the perfor-
mance of a question answering system. Ritter et al
(2009) propose a method based on patterns to find
hypernyms on arbitrary noun phrases. They use
a support vector machine classifier to identify the
correct hypernyms from the candidates that match
the patterns. As our experiments show, pattern-
based methods suffer from low recall because of
the low coverage of patterns.
Besides Kotlerman et al (2010) and Lenci and
Benotto (2012), other researchers also propose di-
rectional distributional similarity methods (Weeds
et al, 2004; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor et al, 2007; Clarke, 2009). How-
ever, their basic assumption that a hyponym can
only be used in contexts where its hypernyms can
be used and that a hypernym might be used in all
of the contexts where its hyponyms are used may
not always rational.
Snow et al (2006) provides a global optimiza-
tion scheme for extending WordNet, which is d-
ifferent from the above-mentioned pairwise rela-
tionships identification methods.
Word embeddings have been successfully ap-
plied in many applications, such as in sentiment
analysis (Socher et al, 2011b), paraphrase detec-
tion (Socher et al, 2011a), chunking, and named
entity recognition (Turian et al, 2010; Collobert
et al, 2011). These applications mainly utilize
the representing power of word embeddings to al-
leviate the problem of data sparsity. Mikolov et
al. (2013a) and Mikolov et al (2013b) further ob-
serve that the semantic relationship of words can
be induced by performing simple algebraic oper-
ations with word vectors. Their work indicates
that word embeddings preserve some interesting
linguistic regularities, which might provide sup-
port for many applications. In this paper, we
improve on their work by learning multiple lin-
ear projections in the embedding space, to model
hypernym?hyponym relationships within different
clusters.
7 Conclusion and Future Work
This paper proposes a novel method for seman-
tic hierarchy construction based on word em-
beddings, which are trained using a large-scale
corpus. Using the word embeddings, we learn
the hypernym?hyponym relationship by estimat-
ing projection matrices which map words to their
hypernyms. Further improvements are made us-
ing a cluster-based approach in order to model
the more fine-grained relations. Then we propose
a few simple criteria to identity whether a new
word pair is a hypernym?hyponym relation. Based
on the pairwise hypernym?hyponym relations, we
build semantic hierarchies automatically.
In our experiments, the proposed method signif-
icantly outperforms state-of-the-art methods and
achieves the best F1-score of 73.74% on a manual-
ly labeled test dataset. Further experiments show
that our method is complementary to the previous
manually-built hierarchy extension methods.
For future work, we aim to improve word
embedding learning under the guidance of
hypernym?hyponym relations. By including the
hypernym?hyponym relation constraints while
training word embeddings, we expect to improve
the embeddings such that they become more suit-
able for this task.
Acknowledgments
This work was supported by National Natu-
ral Science Foundation of China (NSFC) via
grant 61133012, 61273321 and the National 863
Leading Technology Research Project via grant
2012AA011102. Special thanks to Shiqi Zhao,
Zhenghua Li, Wei Song and the anonymous re-
viewers for insightful comments and suggestions.
We also thank Xinwei Geng and Hongbo Cai for
their help in the experiments.
1207
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137?1155.
Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorith-
m for learning directionality of inference rules. In
EMNLP-CoNLL, pages 161?170.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Coling
2010: Demonstrations, pages 13?16, Beijing, Chi-
na, August.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, pages 112?119. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199?207.
Richard Evans. 2004. A framework for named entity
recognition in the open domain. Recent Advances in
Natural Language Processing III: Selected Papers
from RANLP 2003, 260:267?274.
Ruiji Fu, Bing Qin, and Ting Liu. 2013. Exploiting
multiple sources for open-domain hypernym discov-
ery. In EMNLP, pages 1224?1234.
Maayan Geffet and Ido Dagan. 2005. The distribution-
al inclusion hypotheses and lexical entailment. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 107?114.
Association for Computational Linguistics.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539?545. Association for Compu-
tational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribution-
al similarity for lexical inference. Natural Language
Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 75?79. Association for
Computational Linguistics.
Paul McNamee, Rion Snow, Patrick Schone, and James
Mayfield. 2008. Learning named entity hyponyms
for question answering. In Proceedings of the
Third International Joint Conference on Natural
Language Processing, pages 799?804.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL-
HLT, pages 746?751.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Rosa M Ortega-Mendoza, Luis Villase?nor-Pineda, and
Manuel Montes-y G?omez. 2007. Using lexical
patterns for extracting hyponyms from the web. In
MICAI 2007: Advances in Artificial Intelligence,
pages 904?911. Springer.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of the 2009 AAAI Spring
Symposium on Learning by Reading and Learning
to Read, pages 88?93.
HP Ritzema. 1994. Drainage principles and
applications.
Erik Tjong Kim Sang. 2007. Extracting hypernym
pairs from the web. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 165?168. Associa-
tion for Computational Linguistics.
Sidney Siegel and N John Castellan Jr. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill, New York.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304. MIT
Press, Cambridge, MA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computation-
al Linguistics, pages 801?808, Sydney, Australia,
July. Association for Computational Linguistics.
1208
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Ng. 2011a. Dynam-
ic pooling and unfolding recursive autoencoders for
paraphrase detection. In Advances in Neural Infor-
mation Processing Systems, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161. Association for
Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
456?463, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th internation-
al conference on Computational Linguistics, page
1015. Association for Computational Linguistics.
1209
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326?1336,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Character-Level Chinese Dependency Parsing
Meishan Zhang
?
, Yue Zhang
?
, Wanxiang Che
?
, Ting Liu
??
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{mszhang, car, tliu}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
Recent work on Chinese analysis has led
to large-scale annotations of the internal
structures of words, enabling character-
level analysis of Chinese syntactic struc-
tures. In this paper, we investigate the
problem of character-level Chinese depen-
dency parsing, building dependency trees
over characters. Character-level infor-
mation can benefit downstream applica-
tions by offering flexible granularities for
word segmentation while improving word-
level dependency parsing accuracies. We
present novel adaptations of two ma-
jor shift-reduce dependency parsing algo-
rithms to character-level parsing. Exper-
imental results on the Chinese Treebank
demonstrate improved performances over
word-based parsing methods.
1 Introduction
As a light-weight formalism offering syntactic
information to downstream applications such as
SMT, the dependency grammar has received in-
creasing interest in the syntax parsing commu-
nity (McDonald et al, 2005; Nivre and Nilsson,
2005; Carreras et al, 2006; Duan et al, 2007; Koo
and Collins, 2010; Zhang and Clark, 2008; Nivre,
2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi
and McCallum, 2013). Chinese dependency trees
were conventionally defined over words (Chang et
al., 2009; Li et al, 2012), requiring word segmen-
tation and POS-tagging as pre-processing steps.
Recent work on Chinese analysis has embarked
on investigating the syntactic roles of characters,
leading to large-scale annotations of word internal
structures (Li, 2011; Zhang et al, 2013). Such an-
notations enable dependency parsing on the char-
acter level, building dependency trees over Chi-
nese characters. Figure 1(c) shows an example of
?
Corresponding author.
??? ??? ? ? ??
forestry administration deputy director meeting in make a speech
(a) a word-based dependency tree
? ? ? ? ? ? ? ? ? ?
woods industry office deputy office manager meeting in make speech
(b) a character-level dependency tree by Zhao (2009) with
real intra-word and pseudo inter-word dependencies
? ? ? ? ? ? ? ? ? ?
woods industry office deputy office manager meeting in make speech
(c) a character-level dependency tree investigated in this pa-
per with both real intra- and inter-word dependencies
Figure 1: An example character-level dependency
tree. ????????????? (The deputy
director of forestry administration make a speech
in the meeting)?.
a character-level dependency tree, where the leaf
nodes are Chinese characters.
Character-level dependency parsing is interest-
ing in at least two aspects. First, character-level
trees circumvent the issue that no universal stan-
dard exists for Chinese word segmentation. In the
well-known Chinese word segmentation bakeoff
tasks, for example, different segmentation stan-
dards have been used by different data sets (Emer-
son, 2005). On the other hand, most disagreement
on segmentation standards boils down to disagree-
ment on segmentation granularity. As demon-
strated by Zhao (2009), one can extract both fine-
grained and coarse-grained words from character-
level dependency trees, and hence can adapt to
flexible segmentation standards using this formal-
ism. In Figure 1(c), for example, ???? (deputy
1326
director)? can be segmented as both ?? (deputy)
| ?? (director)? and ???? (deputy direc-
tor)?, but not ?? (deputy) ? (office) | ? (man-
ager)?, by dependency coherence. Chinese lan-
guage processing tasks, such as machine transla-
tion, can benefit from flexible segmentation stan-
dards (Zhang et al, 2008; Chang et al, 2008).
Second, word internal structures can also be
useful for syntactic parsing. Zhang et al (2013)
have shown the usefulness of word structures in
Chinese constituent parsing. Their results on the
Chinese Treebank (CTB) showed that character-
level constituent parsing can bring increased per-
formances even with the pseudo word structures.
They further showed that better performances can
be achieved when manually annotated word struc-
tures are used instead of pseudo structures.
In this paper, we make an investigation of
character-level Chinese dependency parsing using
Zhang et al (2013)?s annotations and based on
a transition-based parsing framework (Zhang and
Clark, 2011). There are two dominant transition-
based dependency parsing systems, namely the
arc-standard and the arc-eager parsers (Nivre,
2008). We study both algorithms for character-
level dependency parsing in order to make a com-
prehensive investigation. For direct comparison
with word-based parsers, we incorporate the tra-
ditional word segmentation, POS-tagging and de-
pendency parsing stages in our joint parsing mod-
els. We make changes to the original transition
systems, and arrive at two novel transition-based
character-level parsers.
We conduct experiments on three data sets, in-
cluding CTB 5.0, CTB 6.0 and CTB 7.0. Exper-
imental results show that the character-level de-
pendency parsing models outperform the word-
based methods on all the data sets. Moreover,
manually annotated intra-word dependencies can
give improved word-level dependency accuracies
than pseudo intra-word dependencies. These re-
sults confirm the usefulness of character-level
syntax for Chinese analysis. The source codes
are freely available at http://sourceforge.
net/projects/zpar/, version 0.7.
2 Character-Level Dependency Tree
Character-level dependencies were first proposed
by Zhao (2009). They show that by annotat-
ing character dependencies within words, one can
adapt to different segmentation standards. The
dependencies they study are restricted to intra-
word characters, as illustrated in Figure 1(b). For
inter-word dependencies, they use a pseudo right-
headed representation.
In this study, we integrate inter-word syntactic
dependencies and intra-word dependencies using
large-scale annotations of word internal structures
by Zhang et al (2013), and study their interac-
tions. We extract unlabeled dependencies from
bracketed word structures according to Zhang et
al.?s head annotations. In Figure 1(c), the depen-
dencies shown by dashed arcs are intra-word de-
pendencies, which reflect the internal word struc-
tures, while the dependencies with solid arcs are
inter-word dependencies, which reflect the syntac-
tic structures between words.
In this formulation, a character-level depen-
dency tree satisfies the same constraints as the
traditional word-based dependency tree for Chi-
nese, including projectivity. We differentiate intra-
word dependencies and inter-word dependencies
by the arc type, so that our work can be com-
pared with conventional word segmentation, POS-
tagging and dependency parsing pipelines under a
canonical segmentation standard.
The character-level dependency trees hold to a
specific word segmentation standard, but are not
limited to it. We can extract finer-grained words
of different granulities from a coarse-grained word
by taking projective subtrees of different sizes. For
example, taking all the intra-word modifier nodes
of ?? (manager)? in Figure 1(c) results in the
word ???? (deputy director)?, while taking the
first modifier node of ?? (manager)? results in the
word ??? (director)?. Note that ??? (deputy
office)? cannot be a word because it does not form
a projective span without ?? (manager)?.
Inner-word dependencies can also bring bene-
fits to parsing word-level dependencies. The head
character can be a less sparse feature compared
to a word. As intra-word dependencies lead to
fine-grained subwords, we can also use these sub-
words for better parsing. In this work, we use
the innermost left/right subwords as atomic fea-
tures. To extract the subwords, we find the inner-
most left/right modifiers of the head character, re-
spectively, and then conjoin them with all their de-
scendant characters to form the smallest left/right
subwords. Figure 2 shows an example, where the
smallest left subword of ???? (chief lawyer)?
is ??? (lawyer)?, and the smallest right subword
1327
? ? ?
big law officer
(a) smallest left subword
? ? ?
agree with law ize
(b) smallest right subword
Figure 2: An example to illustrate the innermost
left/right subwords.
of ???? (legalize)? is ??? (legal)?.
3 Character-Level Dependency Parsing
A transition-based framework with global learn-
ing and beam search decoding (Zhang and Clark,
2011) has been applied to a number of natural lan-
guage processing tasks, including word segmen-
tation, POS-tagging and syntactic parsing (Zhang
and Clark, 2010; Huang and Sagae, 2010; Bohnet
and Nivre, 2012; Zhang et al, 2013). It models
a task incrementally from a start state to an end
state, where each intermediate state during decod-
ing can be regarded as a partial output. A num-
ber of actions are defined so that the state ad-
vances step by step. To learn the model param-
eters, it usually uses the online perceptron algo-
rithm with early-update under the inexact decod-
ing condition (Collins, 2002; Collins and Roark,
2004). Transition-based dependency parsing can
be modeled under this framework, where the state
consists of a stack and a queue, and the set of ac-
tions can be either the arc-eager (Zhang and Clark,
2008) or the arc-standard (Huang et al, 2009)
transition systems.
When the internal structures of words are an-
notated, character-level dependency parsing can
be treated as a special case of word-level depen-
dency parsing, with ?words? being ?characters?.
A big weakness of this approach is that full words
and POS-tags cannot be used for feature engineer-
ing. Both are crucial to well-established features
for word segmentation, POS-tagging and syntactic
parsing. In this section, we introduce novel exten-
sions to the arc-standard and the arc-eager tran-
sition systems, so that word-based and character-
based features can be used simultaneously for
character-level dependency parsing.
3.1 The Arc-Standard Model
The arc-standard model has been applied to joint
segmentation, POS-tagging and dependency pars-
ing (Hatori et al, 2012), but with pseudo word
structures. For unified processing of annotated
word structures and fair comparison between
character-level arc-eager and arc-standard sys-
tems, we define a different arc-standard transition
system, consistent with our character-level arc-
eager system.
In the word-based arc-standard model, the tran-
sition state includes a stack and a queue, where
the stack contains a sequence of partially-parsed
dependency trees, and the queue consists of un-
processed input words. Four actions are defined
for state transition, including arc-left (AL, which
creates a left arc between the top element s
0
and
the second top element s
1
on the stack), arc-right
(AR, which creates a right arc between s
0
and s
1
),
pop-root (PR, which defines the root node of a de-
pendency tree when there is only one element on
the stack and no element in the queue), and the last
shift (SH, which shifts the first element q
0
of the
queue onto the stack).
For character-level dependency parsing, there
are two types of dependencies: inter-word depen-
dencies and intra-word dependencies. To parse
them with both character and word features, we
extend the original transition actions into two cat-
egories, for inter-word dependencies and intra-
word dependencies, respectively. The actions for
inter-word dependencies include inter-word arc-
left (AL
w
), inter-word arc-right (AR
w
), pop-root
(PR) and inter-word shift (SH
w
). Their definitions
are the same as the word-based model, with one
exception that the inter-word shift operation has
a parameter denoting the POS-tag of the incoming
word, so that POS disambiguation is performed by
the SH
w
action.
The actions for intra-word dependencies in-
clude intra-word arc-left (AL
c
), intra-word arc-
right (AR
c
), pop-word (PW) and inter-word shift
(SH
c
). The definitions of AL
c
, AR
c
and SH
c
are
the same as the word-based arc-standard model,
while PW changes the top element on the stack
into a full-word node, which can only take inter-
word dependencies. One thing to note is that, due
to variable word sizes in character-level parsing,
the number of actions can vary between differ-
ent sequences of actions corresponding to differ-
ent analyses. We use the padding method (Zhu
et al, 2013), adding an IDLE action to finished
transition action sequences, for better alignments
between states in the beam.
In the character-level arc-standard transition
1328
step action stack queue dependencies
0 - ? ? ? ? ? ? ?
1 SH
w
(NR) ?/NR ? ? ? ? ? ?
2 SH
c
?/NR ?/NR ? ? ? ? ? ?
3 AL
c
?/NR ? ? ? ? ? A
1
= {?x?}
4 SH
c
?/NR ?/NR ? ? ? ? ? A
1
5 AL
c
?/NR ? ? ? ? ? A
2
= A
1
?
{?x?}
6 PW ???/NR ? ? ? ? ? A
2
7 SH
w
(NN) ???/NR ?/NN ? ? ? ? ? A
2
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
12 PW ???/NR ???/NN ? ? ? ? ? A
i
13 AL
w
???/NN ? ? ? ? ? A
i+1
= A
i
?
{???/NRx???/NN}
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(a) character-level dependency parsing using the arc-standard algorithm
step action stack deque queue dependencies
0 - ? ? ? ? ? ?
1 SH
c
(NR) ? ?/NR ? ? ? ? ? ?
2 AL
c
? ? ?/NR ? ? ? ? A
1
= {?x?}
3 SH
c
? ?/NR ? ? ? ? ? A
1
4 AL
c
? ? ?/NR ? ? ? ? A
2
= A
1
?
{?x?}
5 SH
c
? ?/NR ? ? ? ? ? A
2
6 PW ? ???/NR ? ? ? ? ? A
2
7 SH
w
???/NR ? ? ? ? ? ? A
2
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
13 PW ???/NR ???/NN ? ? ? ? ? A
i
14 AL
w
? ???/NN ? ? ? ? ? A
i+1
= A
i
?
{???/NRx???/NN}
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(b) character-level dependency parsing using the arc-eager algorithm, t = 1
Figure 3: Character-level dependency parsing of the sentence in Figure 1(c).
system, each word is initialized by the action SH
w
with a POS tag, before being incrementally mod-
ified by a sequence of intra-word actions, and fi-
nally being completed by the action PW. The inter-
word actions can be applied when all the elements
on the stack are full-word nodes, while the intra-
word actions can be applied when at least the top
element on the stack is a partial-word node. For
the actions AL
c
and AR
c
to be valid, the top two
elements on the stack are both partial-word nodes.
For the action PW to be valid, only the top ele-
ment on the stack is a partial-word node. Figure
3(a) gives an example action sequence.
There are three types of features. The first two
types are traditionally established features for the
dependency parsing and joint word segmentation
and POS-tagging tasks. We use the features pro-
posed by Hatori et al (2012). The word-level
dependency parsing features are added when the
inter-word actions are applied, and the features
for joint word segmentation and POS-tagging are
added when the actions PW, SH
w
and SH
c
are ap-
plied. Following the work of Hatori et al (2012),
we have a parameter ? to adjust the weights for
joint word segmentation and POS-tagging fea-
tures. We apply word-based dependency pars-
ing features to intra-word dependency parsing as
well, by using subwords (the conjunction of char-
acters spanning the head node) to replace words in
word features. The third type of features is word-
structure features. We extract the head charac-
ter and the smallest subwords containing the head
character from the intra-word dependencies (Sec-
tion 2). Table 1 summarizes the features.
3.2 The Arc-Eager Model
Similar to the arc-standard case, the state of a
word-based arc-eager model consists of a stack
and a queue, where the stack contains a sequence
of partial dependency trees, and the queue con-
sists of unprocessed input words. Unlike the arc-
standard model, which builds dependencies on the
top two elements on the stack, the arc-eager model
builds dependencies between the top element of
the stack and the first element of the queue. Five
actions are defined for state transformation: arc-
left (AL, which creates a left arc between the top
element of the stack s
0
and the first element in
the queue q
0
, while popping s
0
off the stack),
arc-right (AR, which creates a right arc between
1329
Feature templates
Lc, Lct, Rc, Rct, L
lc1
c, L
rc1
c, R
lc1
c,
Lc ?Rc, L
lc1
ct, L
rc1
ct, R
lc1
ct,
Lc ?Rw, Lw ?Rc, Lct ?Rw,
Lwt ?Rc, Lw ?Rct, Lc ?Rwt,
Lc ?Rc ? L
lc1
c, Lc ?Rc ? L
rc1
c,
Lc ?Rc ? L
lc2
c, Lc ?Rc ? L
rc2
c,
Lc ?Rc ?R
lc1
c, Lc ?Rc ?R
lc2
c,
Llsw, Lrsw, Rlsw, Rrsw, Llswt,
Lrswt, Rlswt, Rrswt, Llsw ?Rw,
Lrsw ?Rw, Lw ?Rlsw, Lw ?Rrsw
Table 1: Feature templates encoding intra-word
dependencies. L and R denote the two elements
over which the dependencies are built; the sub-
scripts lc1 and rc1 denote the left-most and right-
most children, respectively; the subscripts lc2 and
rc2 denote the second left-most and second right-
most children, respectively; w denotes the word;
t denotes the POS tag; c denotes the head charac-
ter; lsw and rsw denote the smallest left and right
subwords respectively, as shown in Figure 2.
s
0
and q
0
, while shifting q
0
from the queue onto
the stack), pop-root (PR, which defines the ROOT
node of the dependency tree when there is only
one element on the stack and no element in the
queue), reduce (RD, which pops s
0
off the stack),
and shift (SH, which shifts q
0
onto the stack).
There is no previous work that exploits the
arc-eager algorithm for jointly performing POS-
tagging and dependency parsing. Since the first
element of the queue can be shifted onto the stack
by either SH or AR, it is more difficult to assign
a POS tag to each word by using a single action.
In this work, we make a change to the configu-
ration state, adding a deque between the stack and
the queue to save partial words with intra-word de-
pendencies. We divide the transition actions into
two categories, one for inter-word dependencies
(AR
w
, AL
w
, SH
w
, RD
w
and PR) and the other
for intra-word dependencies (AR
c
, AL
c
, SH
c
, RD
c
and PW), requiring that the intra-word actions be
operated between the deque and the queue, while
the inter-word actions be operated between the
stack and the deque.
For character-level arc-eager dependency pars-
ing, the inter-word actions are the same as the
word-based methods. The actions AL
c
and AR
c
are the same as AL
w
and AR
w
, except that they
operate on characters, but the SH
c
operation has a
parameter to denote the POS tag of a word. The
PW action recognizes a full-word. We also have
an IDLE action, for the same reason as the arc-
standard model.
In the character-level arc-eager transition sys-
tem, a word is formed in a similar way with that
of character-level arc-standard algorithm. Each
word is initialized by the action SH
c
with a POS
tag, and then incrementally changed a sequence of
intra-word actions, before being finalized by the
action PW. All these actions operate between the
queue and deque. For the action PW, only the
first element in the deque (close to the queue) is
a partial-word node. For the actions AR
c
and AL
c
to be valid, the first element in the deque must be
a partial-word node. The action SH
c
have a POS
tag when shifting the first character of a word,but
does not have such a parameter when shifting the
next characters of a word. For the action SH
c
with
a POS tag to be valid, the first element in the deque
must be a full-word node. Different from the arc-
standard model, at any stage we can choose either
the action SH
c
with a POS tag to initialize a new
word on the deque, or the inter-word actions on
the stack. In order to eliminate the ambiguity, we
define a new parameter t to limit the max size of
the deque. If the deque is full with t words, inter-
word actions are performed; otherwise intra-word
actions are performed. All the inter-word actions
must be applied on full-word nodes between the
stack an the deque. Figure 3(b) gives an example
action sequence.
Similar to the arc-standard case, there are three
types of features, with the first two types being
traditionally established features for dependency
parsing and joint word segmentation and POS-
tagging. The dependency parsing features are
taken from the work of Zhang and Nivre (2011),
and the features for joint word segmentation and
POS-tagging are taken from Zhang and Clark
(2010)
1
. The word-level dependency parsing fea-
tures are triggered when the inter-word actions are
applied, while the features of joint word segmenta-
tion and POS-tagging are added when the actions
SH
c
, AR
c
and PW are applied. Again we use a pa-
rameter ? to adjust the weights for joint word seg-
mentation and POS-tagging features. The word-
level features for dependency parsing are applied
to intra-word dependency parsing as well, by us-
ing subwords to replace words. The third type of
features is word-structure features, which are the
1
Since Hatori et al (2012) also use Zhang and Clark
(2010)?s features, the arc-standard and arc-eager character-
level dependency parsing models have the same features for
joint word segmentation and POS-tagging.
1330
CTB50 CTB60 CTB70
Training
#sent 18k 23k 31k
#word 494k 641k 718k
Development
#sent 350 2.1k 10k
#word 6.8k 60k 237k
#oov 553 3.3k 13k
Test
#sent 348 2.8k 10k
#word 8.0k 82k 245k
#oov 278 4.6k 13k
Table 2: Statistics of datasets.
same as those of the character-level arc-standard
model, shown in Table 1.
4 Experiments
4.1 Experimental Settings
We use the Chinese Penn Treebank 5.0, 6.0 and 7.0
to conduct the experiments, splitting the corpora
into training, development and test sets according
to previous work. Three different splitting meth-
ods are used, namely CTB50 by Zhang and Clark
(2010), CTB60 by the official documentation of
CTB 6.0, and CTB70 by Wang et al (2011). The
dataset statistics are shown in Table 2. We use
the head rules of Zhang and Clark (2008) to con-
vert phrase structures into dependency structures.
The intra-word dependencies are extracted from
the annotations of Zhang et al (2013)
2
.
The standard measures of word-level precision,
recall and F1 score are used to evaluate word seg-
mentation, POS-tagging and dependency parsing,
following Hatori et al (2012). In addition, we use
the same measures to evaluate intra-word depen-
dencies, which indicate the performance of pre-
dicting word structures. A word?s structure is cor-
rect only if all the intra-word dependencies are all
correctly recognized.
4.2 Baseline and Proposed Models
For the baseline, we have two different pipeline
models. The first consists of a joint segmentation
and POS-tagging model (Zhang and Clark, 2010)
and a word-based dependency parsing model us-
ing the arc-standard algorithm (Huang et al,
2009). We name this model STD (pipe). The
second consists of the same joint segmentation
and POS-tagging model and a word-based depen-
dency parsing model using the arc-eager algorithm
2
https://github.com/zhangmeishan/
wordstructures; their annotation was conducted
on CTB 5.0, while we made annotations of the remainder of
the CTB 7.0 words. We also make the annotations publicly
available at the same site.
(Zhang and Nivre, 2011). We name this model
EAG (pipe). For the pipeline models, we use a
beam of size 16 for joint segmentation and POS-
tagging, and a beam of size 64 for dependency
parsing, according to previous work.
We study the following character-level depen-
dency parsing models:
? STD (real, pseudo): the arc-standard model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
? STD (pseudo, real): the arc-standard model
with pseudo intra-word dependencies and
real inter-word dependencies;
? STD (real, real): the arc-standard model with
annotated intra-word dependencies and real
inter-word dependencies;
? EAG (real, pseudo): the arc-eager model
with annotated intra-word dependencies and
pseudo inter-word dependencies;
? EAG (pseudo, real): the arc-eager model
with pseudo intra-word dependencies and
real inter-word dependencies;
? EAG (real, real): the arc-eager model with
annotated intra-word dependencies and real
inter-word dependencies.
The annotated intra-word dependencies refer to
the dependencies extracted from annotated word
structures, while the pseudo intra-word depen-
dencies used in the above models are similar
to those of Hatori et al (2012). For a given
word w = c
1
c
2
? ? ? c
m
, the intra-word depen-
dency structure is c
x
1
c
x
2
? ? ?
x
c
m
3
. The real inter-
word dependencies refer to the syntactic word-
level dependencies by head-finding rules from
CTB, while the pseudo inter-word dependencies
refer to the word-level dependencies used by Zhao
(2009) (w
x
1
w
x
2
? ? ?
x
w
n
). The character-level
models with annotated intra-word dependencies
and pseudo inter-word dependencies are compared
with the pipelines on word segmentation and POS-
tagging accuracies, and are compared with the
character-level models with annotated intra-word
dependencies and real inter-word dependencies
on word segmentation, POS-tagging and word-
structure predicating accuracies. All the proposed
3
We also tried similar structures with right arcs, which
gave lower accuracies.
1331
STD (real, real) SEG POS DEP WS
? = 1 95.85 91.60 76.96 95.14
? = 2 96.09 91.89 77.28 95.29
? = 3 96.02 91.84 77.22 95.23
? = 4 96.10 91.96 77.49 95.29
? = 5 96.07 91.90 77.31 95.21
Table 3: Development test results of the character-
level arc-standard model on CTB60.
EAG (real, real) SEG POS DEP WS
? = 1
t = 1 96.00 91.66 74.63 95.49
t = 2 95.93 91.75 76.60 95.37
t = 3 95.93 91.74 76.94 95.36
t = 4 95.91 91.71 76.82 95.33
t = 5 95.95 91.73 76.84 95.40
t = 3
? = 1 95.93 91.74 76.94 95.36
? = 2 96.11 91.99 77.17 95.56
? = 3 96.16 92.01 77.48 95.62
? = 4 96.11 91.93 77.40 95.53
? = 5 96.00 91.84 77.10 95.43
Table 4: Development test results of the character-
level arc-eager model on CTB60.
models use a beam of size 64 after considering
both speeds and accuracies.
4.3 Development Results
Our development tests are designed for two pur-
poses: adjusting the parameters for the two pro-
posed character-level models and testing the effec-
tiveness of the novel word-structure features. Tun-
ing is conducted by maximizing word-level depen-
dency accuracies. All the tests are conducted on
the CTB60 data set.
4.3.1 Parameter Tuning
For the arc-standard model, there is only one pa-
rameter ? that needs tuning. It adjusts the weights
of segmentation and POS-tagging features, be-
cause the number of feature templates is much less
for the two tasks than for parsing. We set the value
of ? to 1 ? ? ? 5, respectively. Table 3 shows the
accuracies on the CTB60 development set. Ac-
cording to the results, we use ? = 4 for our final
character-level arc-standard model.
For the arc-eager model, there are two parame-
ters t and ?. t denotes the deque size of the arc-
eager model, while ? shares the same meaning as
the arc-standard model. We take two steps for pa-
rameter tuning, first adjusting the more crucial pa-
rameter t and then adjusting ? on the best t. Both
parameters are assigned the values of 1 to 5. Ta-
SEG POS DEP WS
STD (real, real) 96.10 91.96 77.49 95.29
STD (real, real)/wo 95.99 91.79 77.19 95.35
? -0.11 -0.17 -0.30 +0.06
EAG (real, real) 96.16 92.01 77.48 95.62
EAG (real, real)/wo 96.09 91.82 77.12 95.56
? -0.07 -0.19 -0.36 -0.06
Table 5: Feature ablation tests for the novel word-
structure features, where ?/wo? denotes the corre-
sponding models without the novel intra-word de-
pendency features.
ble 4 shows the results. According to results, we
set t = 3 and ? = 3 for the final character-level
arc-eager model, respectively.
4.3.2 Effectiveness of Word-Structure
Features
To test the effectiveness of our novel word-
structure features, we conduct feature ablation ex-
periments on the CTB60 development data set for
the proposed arc-standard and arc-eager models,
respectively. Table 5 shows the results. We can
see that both the two models achieve better accu-
racies on word-level dependencies with the novel
word-structure features, while the features do not
affect word-structure predication significantly.
4.4 Final Results
Table 6 shows the final results on the CTB50,
CTB60 and CTB70 data sets, respectively. The
results demonstrate that the character-level depen-
dency parsing models are significantly better than
the corresponding word-based pipeline models,
for both the arc-standard and arc-eager systems.
Similar to the findings of Zhang et al (2013), we
find that the annotated word structures can give
better accuracies than pseudo word structures. An-
other interesting finding is that, although the arc-
eager algorithm achieves lower accuracies in the
word-based pipeline models, it obtains compara-
tive accuracies in the character-level models.
We also compare our results to those of Hatori
et al (2012), which is comparable to STD (pseudo,
real) since similar arc-standard algorithms and
features are used. The major difference is the
set of transition actions. We rerun their system
on the three datasets
4
. As shown in Table 6, our
arc-standard system with pseudo word structures
4
http://triplet.cc/. We use a different
constituent-to-dependency conversion scheme in com-
parison with Hatori et al (2012)?s work.
1332
Model
CTB50 CTB60 CTB70
SEG POS DEP WS SEG POS DEP WS SEG POS DEP WS
The arc-standard models
STD (pipe) 97.53 93.28 79.72 ? 95.32 90.65 75.35 ? 95.23 89.92 73.93 ?
STD (real, pseudo) 97.78 93.74 ? 97.40 95.77
?
91.24
?
? 95.08 95.59
?
90.49
?
? 94.97
STD (pseudo, real) 97.67 94.28
?
81.63
?
? 95.63
?
91.40
?
76.75
?
? 95.53
?
90.75
?
75.63
?
?
STD (real, real) 97.84 94.62
?
82.14
?
97.30 95.56
?
91.39
?
77.09
?
94.80 95.51
?
90.76
?
75.70
?
94.78
Hatori+ ?12 97.75 94.33 81.56 ? 95.26 91.06 75.93 ? 95.27 90.53 74.73 ?
The arc-eager models
EAG (pipe) 97.53 93.28 79.59 ? 95.32 90.65 74.98 ? 95.23 89.92 73.46 ?
EAG (real, pseudo) 97.75 93.88 ? 97.45 95.63
?
91.07
?
? 95.06 95.50
?
90.36
?
? 95.00
EAG (pseudo, real) 97.76 94.36
?
81.70
?
? 95.63
?
91.34
?
76.87
?
? 95.39
?
90.56
?
75.56
?
?
EAG (real, real) 97.84 94.36
?
82.07
?
97.49 95.71
?
91.51
?
76.99
?
95.16 95.47
?
90.72
?
75.76
?
94.94
Table 6: Main results, where the results marked with ? denote that the p-value is less than 0.001 compared
with the pipeline word-based models using pairwise t-test.
brings consistent better accuracies than their work
on all the three data sets.
Both the pipelines and character-level mod-
els with pseudo inter-word dependencies perform
word segmentation and POS-tagging jointly, with-
out using real word-level syntactic information. A
comparison between them (STD/EAG (pipe) vs.
STD/EAG (real, pseudo)) reflects the effectiveness
of annotated intra-word dependencies on segmen-
tation and POS-tagging. We can see that both the
arc-standard and arc-eager models with annotated
intra-word dependencies can improve the segmen-
tation accuracies by 0.3% and the POS-tagging ac-
curacies by 0.5% on average on the three datasets.
Similarly, a comparison between the character-
level models with pseudo inter-word dependen-
cies and the character-level models with real inter-
word dependencies (STD/EAG (real, pseudo) vs.
STD/EAG (real, real)) can reflect the effectiveness
of annotated inter-word structures on morphology
analysis. We can see that improved POS-tagging
accuracies are achieved using the real inter-word
dependencies when jointly performing inner- and
inter-word dependencies. However, we find that
the inter-word dependencies do not help the word-
structure accuracies.
4.5 Analysis
To better understand the character-level parsing
models, we conduct error analysis in this section.
All the experiments are conducted on the CTB60
test data sets. The new advantage of the character-
level models is that one can parse the internal
word structures of intra-word dependencies. Thus
we are interested in their capabilities of predict-
ing word structures. We study the word-structure
accuracies in two aspects, including OOV, word
length, POS tags and the parsing model.
4.5.1 OOV
The word-structure accuracy of OOV words re-
flects a model?s ability of handling unknown
words. The overall recalls of OOV word structures
are 67.98% by STD (real, real) and 69.01% by
EAG (real, real), respectively. We find that most
errors are caused by failures of word segmenta-
tion. We further investigate the accuracies when
words are correctly segmented, where the accura-
cies of OOV word structures are 87.64% by STD
(real, real) and 89.07% by EAG (real, real). The
results demonstrate that the structures of Chinese
words are not difficult to predict, and confirm the
fact that Chinese word structures have some com-
mon syntactic patterns.
4.5.2 Parsing Model
From the above analysis in terms of OOV, word
lengths and POS tags, we can see that the EAG
(real, real) model and the STD (real, real) mod-
els behave similarly on word-structure accuracies.
Here we study the two models more carefully,
comparing their word accuracies sentence by sen-
tence. Figure 4 shows the results, where each
point denotes a sentential comparison between
STD (real, real) and EAG (real, real), the x-axis
denotes the sentential word-structure accuracy of
STD (real, real), and the y-axis denotes that of
EAG (real, real). The points at the diagonal show
the same accuracies by the two models, while oth-
ers show that the two models perform differently
on the corresponding sentences. We can see that
most points are beyond the diagonal line, indicat-
1333
0.6 0.7 0.8 0.9 1
0.6
0.7
0.8
0.9
1
STD (real, real)
E
A
G
(
r
e
a
l
,
r
e
a
l
)
Figure 4: Sentential word-structure accuracies of
STD (real, real) and EAG (real, real).
ing that the two parsing models can be comple-
mentary in parsing intra-word dependencies.
5 Related Work
Zhao (2009) was the first to study character-level
dependencies; they argue that since no consistent
word boundaries exist over Chinese word segmen-
tation, dependency-based representations of word
structures serve as a good alternative for Chinese
word segmentation. Thus their main concern is
to parse intra-word dependencies. In this work,
we extend their formulation, making use of large-
scale annotations of Zhang et al (2013), so that the
syntactic word-level dependencies can be parsed
together with intra-word dependencies.
Hatori et al (2012) proposed a joint model
for Chinese word segmentation, POS-tagging and
dependency parsing, studying the influence of
joint model and character features for parsing,
Their model is extended from the arc-standard
transition-based model, and can be regarded as
an alternative to the arc-standard model of our
work when pseudo intra-word dependencies are
used. Similar work is done by Li and Zhou (2012).
Our proposed arc-standard model is more concise
while obtaining better performance than Hatori et
al. (2012)?s work. With respect to word structures,
real intra-word dependencies are often more com-
plicated, while pseudo word structures cannot be
used to correctly guide segmentation.
Zhao (2009), Hatori et al (2012) and our
work all study character-level dependency pars-
ing. While Zhao (2009) focus on word internal
structures using pseudo inter-word dependencies,
Hatori et al (2012) investigate a joint model using
pseudo intra-word dependencies. We use manual
dependencies for both inner- and inter-word struc-
tures, studying their influences on each other.
Zhang et al (2013) was the first to perform Chi-
nese syntactic parsing over characters. They ex-
tended word-level constituent trees by annotated
word structures, and proposed a transition-based
approach to parse intra-word structures and word-
level constituent structures jointly. For Hebrew,
Tsarfaty and Goldberg (2008) investigated joint
segmentation and parsing over characters using a
graph-based method. Our work is similar in ex-
ploiting character-level syntax. We study the de-
pendency grammar, another popular syntactic rep-
resentation, and propose two novel transition sys-
tems for character-level dependency parsing.
Nivre (2008) gave a systematic description of
the arc-standard and arc-eager algorithms, cur-
rently two popular transition-based parsing meth-
ods for word-level dependency parsing. We extend
both algorithms to character-level joint word seg-
mentation, POS-tagging and dependency parsing.
To our knowledge, we are the first to apply the arc-
eager system to joint models and achieve compar-
ative performances to the arc-standard model.
6 Conclusions
We studied the character-level Chinese depen-
dency parsing, by making novel extensions to
two commonly-used transition-based dependency
parsing algorithms for word-based dependency
parsing. With both pseudo and annotated word
structures, our character-level models obtained
better accuracies than previous work on seg-
mentation, POS-tagging and word-level depen-
dency parsing. We further analyzed some im-
portant factors for intra-word dependencies, and
found that two proposed character-level pars-
ing models are complementary in parsing intra-
word dependencies. We make the source code
publicly available at http://sourceforge.
net/projects/zpar/, version 0.7.
Acknowledgments
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61370164, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design.
1334
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the EMNLP-CONLL, pages 1455?1465,
Jeju Island, Korea, July.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd COLING, number August, pages
89?97.
Xavier Carreras, Mihai Surdeanu, and Llu??s M`arquez.
2006. Projective dependency parsing with per-
ceptron. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 181?185, New York City, June.
Pi-Chuan Chang, Michel Galley, and Chris Manning.
2008. Optimizing chinese word segmentation for
machine translation performance. In ACL Workshop
on Statistical Machine Translation.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, , and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of ACL, pages
1052?1062, August.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the 7th EMNLP.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, vol-
ume 4701 of Lecture Notes in Computer Science,
pages 559?566.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the Second SIGHAN Workshop on Chinese Lan-
guage Processing, pages 123?133.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental joint approach
to word segmentation, pos tagging, and dependency
parsing in chinese. In Proceedings of the 50th ACL,
pages 1045?1053, Jeju Island, Korea, July.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th ACL, pages 1077?1086, Up-
psala, Sweden, July.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1222?1231. Asso-
ciation for Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the ACL, pages 1?11.
Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445?1454, Jeju Island, Ko-
rea, July.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Ex-
ploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of the 50th
ACL, pages 675?684, Jeju Island, Korea, July.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings of the 49th ACL, pages 1405?
1414, Portland, Oregon, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, number
June, pages 91?98, Morristown, NJ, USA.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
ACL.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Reut Tsarfaty and Yoav Goldberg. 2008. Word-based
or morpheme-based? annotation strategies for mod-
ern hebrew clitics. In LREC. European Language
Resources Association.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmenta-
tion and pos tagging with semi-supervised methods
using large auto-analyzed data. In Proceedings of
5th IJCNLP, pages 309?317, Chiang Mai, Thailand,
November.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
Proceedings of EMNLP, pages 562?571, Honolulu,
Hawaii, October.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
EMNLP, pages 843?852, Cambridge, MA, October.
1335
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th ACL, pages 188?193, Port-
land, Oregon, USA, June.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Chinese word segmentation and statistical
machine translation. IEEE Transactions on Signal
Processing, 5(2).
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proceedings of the 51st ACL, pages 125?134,
Sofia, Bulgaria, August.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the EACL, pages 879?887, Athens, Greece, March.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st ACL, pages 434?443, Sofia, Bulgaria, August.
1336
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 407?410,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
HIT-CIR: An Unsupervised WSD System Based on Domain Most
Frequent Sense Estimation
Yuhang Guo, Wanxiang Che, Wei He, Ting Liu, Sheng Li
Harbin Institute of Technolgy
Harbin, Heilongjiang, PRC
yhguo@ir.hit.edu.cn
Abstract
This paper presents an unsupervised sys-
tem for all-word domain specific word
sense disambiguation task. This system
tags target word with the most frequent
sense which is estimated using a thesaurus
and the word distribution information in
the domain. The thesaurus is automati-
cally constructed from bilingual parallel
corpus using paraphrase technique. The
recall of this system is 43.5% on SemEval-
2 task 17 English data set.
1 Introduction
Tagging polysemous word with its most frequent
sense (MFS) is a popular back-off heuristic in
word sense disambiguation (WSD) systems when
the training data is inadequate. In past evalua-
tions, MFS from WordNet performed even bet-
ter than most of the unsupervised systems (Snyder
and Palmer, 2004; Navigli et al, 2007).
MFS is usually obtained from a large scale
sense tagged corpus, such as SemCor (Miller et al,
1994). However, some polysemous words have
different MFS in different domains. For example,
in the Koeling et al (2005) corpus, target word
coach means ?manager? mostly in the SPORTS
domain but means ?bus? mostly in the FINANCE
domain. So when the MFS is applied to specific
domains, it needs to be re-estimated.
McCarthy et al (2007) proposed an unsuper-
vised predominant word sense acquisition method
which obtains domain specific MFS without sense
tagged corpus. In their method, a thesaurus, in
which words are connected with their distribu-
tional similarity, is constructed from the domain
raw text. Word senses are ranked by their preva-
lence score which is calculated using the thesaurus
and the sense inventory.
In this paper, we propose another way to con-
struct the thesaurus. We use statistical machine
Figure 1: The architecture of HIT-CIR
translation (SMT) techniques to extract paraphrase
pairs from bilingual parallel text. In this way, we
avoid calculating similarities between every pair
of words and could find semantic similar words or
compounds which have dissimilar distributions.
Our system is comprised of two parts: the word
sense ranking part and the word sense tagging part.
Senses are ranked according to their prevalence
score in the target domain, and the predominant
sense is used to tag the occurrences of the target
word in the test data. The architecture of this sys-
tem is shown in Figure 1.
The word sense ranking part includes following
steps.
1. Tag the POS of the background text, count
the word frequency in each POS, and get the
polysemous word list of the POS.
2. Using SMT techniques to extract phrase table
407
Figure 2: Word sense ranking for the noun backbone
from the bilingual corpus. Extract the para-
phrases (called as neighbor words) with the
phrase table for each word in the polysemous
word list.
3. Calculate the prevalence score of each sense
of the target words, rank the senses with the
score and obtain the predominant sense.
We applied our system on the English data set
of SemEval-2 specific domain WSD task. This
task is an all word WSD task in the environ-
mental domain. We employed the domain back-
ground raw text provided by the task organizer as
well as the English WordNet 3.0 (Fellbaum, 1998)
and the English-Spanish parallel corpus from Eu-
roparl (Koehn, 2005).
This paper is organized as follows. Section 2
introduces how to rank word senses. Section 3
presents how to obtain the most related words of
the target words. We describe the system settings
in Section 4 and offer some discussions in Sec-
tion 5.
2 Word Sense Ranking
In our method, word senses are ranked according
to their prevalence score in the specific domain.
According to the assumption of McCarthy et al
(2007), the prevalence score is affected by the fol-
lowing two factors: (1) The relatedness score be-
tween a given sense of the target word and the
target word?s neighbor word. (2) The similarity
between the target word and its neighbor word.
In addition, we add another factor, (3) the impor-
tance of the neighbor word in the specific domain.
In this paper, ?neighbor words? means the words
which are most semantically similar to the target
word.
Figure 2 illustrates the word sense ranking pro-
cess of noun backbone. The contribution of a
neighbor word to a given word sense is measured
by the similarity between them and weighted by
the importance of the neighbor word in the tar-
get domain and the relatedness between the neigh-
bor word and the target word. Sum up the con-
tributions of each neighbor words, and we get the
prevalence score of the word sense.
Formally, the prevalence score of sense s
i
of a
target word w is assigned as follows:
ps(w, s
i
) =
?
n
j
?N
w
rs(w, n
j
) ? ns(s
i
, n
j
) ? dw(n
j
)
(1)
where
ns(s
i
, n
j
) =
sss(s
i
, n
j
)
?
s
i
?
?senses(w)
sss(s
i
?
, n
j
)
, (2)
sss(s
i
, n
j
) = max
s
x
?senses(n
j
)
sss
?
(s
i
, s
x
). (3)
rs(w, n
j
) is the relatedness score between w and
a neighbor word n
j
. N
w
= {n
1
, n
2
, . . . , n
k
}
is the top k relatedness score neighbor word set.
ns(s
i
, n
j
) is the normalized form of the sense sim-
ilarity score between sense s
i
and the neighbor
word n
j
(i.e. sss(s
i
, n
j
)). We define this score
with the maximum WordNet similarity score be-
tween s
i
and the senses of n
j
(i.e. sss
?
(s
i
, n
j
)).
In our system, lesk algorithm is used to measure
the sense similarity score between word senses.
408
Figure 3: Finding the neighbor words of noun backbone
The similarity of this algorithm is the count of
the number of overlap words in the gloss or the
definition of the senses (Banerjee and Pedersen,
2002). The domain importance weight dw(n
j
) is
assigned with the count of n
j
in the domain back-
ground corpus. For the neighbor word that does
not occur in the domain background text, we use
the add-one strategy. We will describe how to ob-
tain n
j
and rs in Section 3.
3 Thesaurus Construction
The neighbor words of the target word as well as
the relatedness score are obtained by extracting
paraphrases from bilingual parallel texts. When
a word is translated from source language to tar-
get language and then translated back to the source
language, the final translation may have the same
meaning to the original word but with different ex-
pressions (e.g. different word or compound). The
translation in the same language could be viewed
as a paraphrase term or, at least, related term of the
original word.
For example, in Figure 3, English noun back-
bone can be translated to columna, columna verte-
bral, pilar and convicciones etc. in Spanish, and
these words also have other relevant translations
in English, such as vertebral column, column, pil-
lar and convictions etc., which are semantically re-
lated to the target word backbone.
We use a statistical machine translation sys-
tem to calculate the translation probability from
English to another language (called as pivot lan-
guage) as well as the translation probability from
that language to English. By multiplying these
two probabilities, we get a paraphrase probabil-
ity. This method was defined in (Bannard and
Callison-Burch, 2005).
In our system, we choose the top k paraphrases
as the neighbor words of the target word, which
have the highest paraphrase probability. Note that
there are two directions of the paraphrase, from
target word to its neighbor word and from the
neighbor word to the target word. We choose
the paraphrase score of the former direction as
the relatedness score (rs). Because the higher
of the score in this direction, the target word is
more likely paraphrased to that neighbor word,
and hence the prevalence of the relevant target
word sense will be higher than other senses. For-
mally, the relatedness score is given by
rs(w, n
j
) =
?
f
p(f |w)p(n
j
|f), (4)
where f is the pivot language word.
We use the English-Spanish parallel text from
Europarl (Koehn, 2005). We choose Spanish as
the pivot language because in the both directions
the BLEU score of the translation between English
and Spanish is relatively higher than other English
and other languages (Koehn, 2005).
4 Data set and System Settings
The organizers of the SemEval-2 specific domain
WSD task provide no training data but raw back-
ground data in the environmental domain. The En-
glish background data is obtained from the offi-
cial web site of World Wide Fund (WWF), Euro-
pean Centre for Nature Conservation (ECNC), Eu-
ropean Commission and the United Nations Eco-
nomic Commission for Europe (UNECE). The
size of the raw text is around 15.5MB after sim-
ple text cleaning. The test data is from WWF and
ECNC, and contains 1398 occurrence of 436 tar-
get words.
For the implementation, we used bpos (Shen et
al., 2007) for the POS tagging. The maximum
409
number of the neighbor word of each target word k
was set to 50. We employed Giza++
1
and Moses
2
to get the phrase table from the bilingual paral-
lel corpus. TheWordNet::Similarity package
3
was
applied for the implement of the lesk word sense
similarity algorithm.
For the target word that is not in the polysemous
word list, we use the MFS from WordNet as the
back-off method.
5 Discussion and Future Work
The recall of our system is 43.5%, which is lower
than that of the MFS baseline, 50.5% (Agirre et
al., 2010). The baseline uses the most frequent
sense from the SemCor corpus (i.e. the MFS of
WordNet). This means that for some target words,
the MFS from SemCor is better than the domain
MFS we estimated in the environmental domain.
In the future, we will analysis errors in detail to
find the effects of the domain on the MFS.
For the domain specific task, it is better to use
parallel text in the domain of the test data in our
method. However, we didn?t find any available
parallel text in the environmental domain yet. In
the future, we will try some parallel corpus acqui-
sition techniques to obtain relevant corpus for en-
vironmental domain for our method.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
References
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ?02: Proceedings
1
http://www.fjoch.com/GIZA++.html
2
http://www.statmt.org/moses/
3
http://wn-similarity.sourceforge.net/
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 597?
604, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 419?426, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590, December.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1994. A semantic concordance. In Proc. ARPA
Human Language Technology Workshop ?93, pages
303?308, Princeton, NJ, March. distributed as Hu-
man Language Technology by San Mateo, CA: Mor-
gan Kaufmann Publishers.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30?35, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
410
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 378?384,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 5: Chinese Semantic Dependency Parsing
Wanxiang Che?, Meishan Zhang?, Yanqiu Shao?, Ting Liu?
?Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{car, mszhang, tliu}@ir.hit.edu.cn
?Beijing City University, China
yqshao@bcu.edu.cn
Abstract
The paper presents the SemEval-2012 Shared
Task 5: Chinese Semantic Dependency Pars-
ing. The goal of this task is to identify the de-
pendency structure of Chinese sentences from
the semantic view. We firstly introduce the
motivation of providing Chinese semantic de-
pendency parsing task, and then describe the
task in detail including data preparation, data
format, task evaluation, and so on. Over ten
thousand sentences were labeled for partici-
pants to train and evaluate their systems. At
last, we briefly describe the submitted systems
and analyze these results.
1 Introduction
Semantic analysis is a long-term goal of Natural
Language Processing, and as such, has been re-
searched for several decades. A number of tasks
for encoding semantic information have been devel-
oped over the years, such as entity type recognition
and word sense disambiguation. Recently, sentence-
level semantics ? in particular, semantic role label-
ing ? has received increasing attention. However,
some problems concerning the semantic representa-
tion method used in semantic role labeling continue
to exist (Xue and Palmer, 2005).
1. Semantic role labeling only considers
predicate-argument relations and ignores
the semantic relations between a noun and its
modifier.
2. The meaning of semantic roles is related to spe-
cial predicates. Therefore, there are infinite se-
mantic roles to be learned, as the number of
predicates is not fixed. Although the Prop-
Bank (Xue and Palmer, 2003) normalizes these
semantic roles into certain symbols, such as
Arg0-Arg5, the same symbol can have different
semantic meanings when paired with different
predicates, and thus cannot be learned well.
Semantic dependency parsing is therefore pro-
posed to solve the two problems above for Chinese.
Firstly, the proposed method analyzes all the words?
semantic roles in a sentence and specifies the con-
crete semantic relation of each word pair. After-
ward, this work analyzes and summarizes all the
possible semantic roles, obtaining over 100 of them,
and then uses these semantic roles to specify the se-
mantic relation for each word pair.
Dependency parsing (Ku?bler et al, 2009) is based
on dependency grammar. It has several advantages,
such as concise formalization, easy comprehension,
high efficiency, and so on. Dependency parsing
has been studied intensively in recent decades, with
most related work focusing on syntactic structure.
Many research papers on Chinese linguistics demon-
strate the remarkable difference between semantics
and syntax (Jin, 2001; Zhou and Zhang, 2003).
Chinese is a meaning-combined language with very
flexible syntax, and semantics are more stable than
syntax. The word is the basic unit of semantics,
and the structure and meaning of a sentence consists
mainly of a series of semantic dependencies between
individual words (Li et al, 2003). Thus, a reason-
able endeavor is to exploit dependency parsing for
semantic analysis of Chinese languages. Figure 1
shows an example of Chinese semantic dependency
parsing.
378
??International ??Monetary ??Fund ??organization ??turn down ?for ??global ??economy ??increasing ?of ??prediction
d-genetived-restrictive d-restrictive agent prep-dependd-genetive d-domain aux-depend
d-restrictivecontent
root
Figure 1: An example of Chinese Semantic Dependency Parsing.
Figure 1 shows that Chinese semantic dependency
parsing looks very similar to traditional syntax-
dominated dependency parsing. Below is a compar-
ison between the two tasks, dealing with three main
points:
1. Semantic relations are more fine-grained than
syntactic ones: the syntactic subject can either
be the agent or experiencer, and the syntactic
object can be the content, patient, possession,
and so on. On the whole, the number of seman-
tic relations is at least twice that of syntactic
relations.
2. Semantic dependency parsing builds the depen-
dency structure of a sentence in terms of se-
mantics, and the word pairs of a dependency
should have a direct semantic relation. This
criterion determines many sizeable differences
between semantics and syntax, especially in
phrases formed by ?XP+DEG?, ?XP+DEV?
and prepositional phrases. For example, in ??
? ? ??? (beautiful country), the head of
???? (beautiful) is ???? (country) in se-
mantic dependency parsing, whereas the head
is ??? (de) in syntax dependency parsing.
3. Semantic relations are independent of position.
For example, in ??? ? ??? (the air is
contaminated) and ??? ? ??? (contami-
nate the air), the patient ???? (the air) can be
before or behind a predicate ???? (contami-
nate).
The rest of the paper is organized as follows. Sec-
tion 2 gives a short overview of data annotation.
Section 3 focuses on the task description. Section
4 describes the participant systems. Section 5 com-
pares and analyzes the results. Finally, Section 6
concludes the paper.
2 Data Annotation
2.1 Corpus Section
10,068 sentences were selected from the Penn Chi-
nese Treebank 6.01 (Xue et al, 2005) (1-121, 1001-
1078, 1100-1151) as the raw corpus from which to
create the Chinese Semantic Dependency Parsing
corpus. These sentences were chosen for the anno-
tation for three reasons. First, gold syntactic depen-
dency structures can be of great help in semantic de-
pendency annotation, as syntactic dependency arcs
are often consistent with semantic ones. Second, the
semantic role labels in PropBank2 can be very use-
ful in the present annotation work. Third, the gold
word segmentation and Part-Of-Speech can be used
as the annotation input in this work.
2.2 Semantic Relations
The semantic relations in the prepared Chinese se-
mantic dependency parsing corpus came mostly
from HowNet3 (Dong and Dong, 2006), a fa-
mous Chinese semantic thesaurus. We also referred
to other sources. Aside from the relations from
HowNet, we defined two kinds of new relations: re-
verse relations and indirect relations. When a verb
modifies a noun, the relation between them is a re-
verse relation, and r-XXX is used to indicate this
kind of relation. For instance, in ???????
?? (the little boy who is playing basketball), the se-
mantic relation between the head word ???? (boy)
1http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalog\\Id=LDC2007T36
2http://verbs.colorado.edu/chinese/cpb/
3http://www.keenage.com/
379
and ??? (playing) is the r-agent. When a verbal
noun is the head word, the relation between it and
the modifier is the indirect relation j-XXX. For in-
stance, in ?????? (business management), the
head word is ???? (management) and the modifier
is ???? (business), their relation is j-patient.
Finally, we defined 84 single-level semantic re-
lations. The number of multi-level semantic rela-
tions that actually appear in the labeled corpus in
this work is 39.
Table 1 summarizes all of the semantic relations
used for annotation.
2.3 Annotation Flow
Our corpus annotation flow can be divided into the
following steps.
1. Conversion of the sentences? constituent struc-
tures into dependency structures according to
a set of rules similar with those used by the
syntactic community to find the head of a
phrase (Collins, 1999).
2. Labeling of the semantic relations for each de-
pendency relation according to another set of
rules using the functional tags in the Penn Chi-
nese Treebank and the semantic roles in the
Chinese PropBank.
3. Six human annotators are asked to check and
adjust the structure and semantic relation errors
introduced in Step 2.
The first two steps were performed automatically
using rules. A high accuracy may be achieved with
dependency structures when semantic labels are not
considered. However, accuracy declines remarkably
when the semantic label is considered. Unlabeled
Attachment Score (UAS) and Labeled Attachment
Score (LAS) can be used to evaluate the perfor-
mance of the automatic conversion. Table 2 gives
the detailed results.
UAS LAS
Conversion Result 90.53 57.38
Table 2: Accuracy after conversion from gold ProbBank.
3 Task Description
3.1 Corpus Statistics
We annotated 10,068 sentences from the Penn Chi-
nese TreeBank for Semantic Dependency Parsing,
and these sentences were divided into training, de-
velopment, and test sections. Table 3 gives the de-
tailed statistical information of the three sections.
Data Set CTB files # sent. # words.
1-10; 36-65;81-121; 8301
Training 1001-1078; 250311
1100-1119;
1126-1140
Devel 66-80; 1120-1125 534 15329
Test 11-35; 1141-1151 1233 34311
Total 1-121; 1001-1078 10068 299951
1100-1151
Table 3: Statistics of training, development and test data.
3.2 Data Format
The data format is identical to that of a syntactic de-
pendency parsing shared task. All the sentences are
in one text file, with each sentence separated by a
blank line. Each sentence consists of one or more to-
kens, and each token is represented on one line con-
sisting of 10 fields. Buchholz and Marsi (2006) pro-
vide more detailed information on the format. Fields
are separated from each other by a tab. Only five of
the 10 fields are used: token id, form, pos tagger,
head, and deprel. Head denotes the semantic depen-
dency of each word, and deprel denotes the corre-
sponding semantic relations of the dependency. In
the data, the lemma column is filled with the form
and the cpostag column with the postag. Figure 2
shows an example.
3.3 Evaluation Method
LAS, which is a method widely used in syntactic
dependency parsing, is used to evaluate the perfor-
mance of the semantic dependency parsing system.
LAS is the proportion of ?scoring? tokens assigned
to both the correct head and correct semantic depen-
dency relation. Punctuation is disregarded during
the evaluation process. UAS is another important
indicator, as it reflects the accuracy of the semantic
dependency structure.
380
Main Semantic Roles
Subject Roles agent, experiencer, causer, possessor, existent, whole, relevant
Object Roles isa, content, possession, patient, OfPart, beneficiary, contrast,
partner, basis, cause, cost, scope, concerning
Auxiliary Semantic Roles
Time Roles duration, TimeFin, TimeIni, time, TimeAdv
Location and State Roles LocationFin, LocationIni, LocationThru, StateFin, state,
StateIni, direction, distance, location
Others Verb Modifiers accompaniment, succeeding, frequency, instrument, material,
means, angle, times, sequence, sequence-p, negation, degree,
modal, emphasis, manner, aspect, comment
Attribute Roles
Direct modifiers d-genetive, d-category, d-member, d-domain, d-quantity-p, d-
quantity, d-deno-p, d-deno, d-host, d-TimePhrase, d-LocPhrase,
d-InstPhrase, d-attribute, d-restrictive, d-material, d-content, d-
sequence, d-sequence-p, qp-mod
Verb Phrase r-{Main Semantic Roles}, eg: r-agent, r-patient, r-possessor
Verb Ellipsis c-{Main Semantic Roles}, eg: c-agent, c-content, c-patient
Noun as Predication j-{Main Semantic Roles}, eg: j-agent, j-patient, j-target
Syntactic Roles and Others
Syntactic Roles s-cause, s-concession, s-condition, s-coordinate, s-or, s-
progression, s-besides, s-succession, s-purpose, s-measure, s-
abandonment, s-preference, s-summary, s-recount, s-concerning,
s-result
Others aux-depend, prep-depend, PU, ROOT
Table 1: Semantic Relations defined for Chinese Semantic Dependency Parsing.
ID FORM LEMMA CPOS PPOS FEAT HEAD REL PHEAD PREL
1 ??? ??? NR NR 2 agent
2 ? ? VV VV 0 ROOT
3 ?? ?? NR NR 4 d-genetive
4 ?? ?? NN NN 7 s-coordinate
5 ? ? CC CC 7 aux-depend
6 ?? ?? NR NR 7 d-genetive
7 ?? ?? NN NN 2 content
Figure 2: Data format of the Chinese Semantic Dependency Parsing corpus.
381
4 Participating Systems
Nine organizations were registered to participate in
the Chinese Semantic Dependency Parsing task. Fi-
nally, nine systems were received from five different
participating teams. These systems are as follows:
1. Zhou Qiaoli-1, Zhou Qiaoli-2, Zhou Qiaoli-3
These three systems propose a divide-and-
conquer strategy for semantic dependency
parsing. The Semantic Role (SR) phrases are
identified (Cai et al, 2011) and then replaced
by their head or the SR of the head. The orig-
inal sentence is thus divided into two types of
parts that can be parsed separately. The first
type is SR phrase parsing, and the second in-
volves the replacement of SR phrases with ei-
ther their head or the SR of the head. Finally,
the paper takes a graph-based parser (Li et al,
2011) as the semantic dependency parser for all
parts. These three systems differ in their phrase
identification strategies.
2. NJU-Parser-1, NJU-Parser-2
The NJU-Parser is based on the state-of-the-
art MSTParser (McDonald, 2006). NJU-Parser
applies three methods to enhance semantic de-
pendency parsing. First, sentences are split
into sub-sentences using commas and semi-
colons: (a) sentences are split using only com-
mas and semicolons, as in the primary sys-
tem, and (b) classifiers are used to determine
whether a comma or semicolon should be used
to split the sentence. Second, the last character
in a Chinese word is extracted as the lemma,
since it usually contains the main sense or se-
mantic class. Third, the multilevel-label is in-
troduced into the semantic relation, for exam-
ple, the r-{Main Semantic Roles}, with NJU-
Parser exploiting special strategies to handle it.
However, this third method does not show pos-
itive performance.
3. Zhijun Wu-1
This system extends the second-order of the
MSTParser by adding third-order features, and
then applying this model to Chinese semantic
dependency parsing. In contrast to Koo and
Collins (2010) this system does not implement
the third-order model using dynamic program-
ming, as it requires O(n4) time. It first first ob-
tained the K-best results of second-order mod-
els and then added the third-order features into
the results.
4. ICT-1
The ICT semantic dependency parser employs
a system-combining strategy to obtain the de-
pendency structure and then uses the classifier
from Le Zhang?s Maximum Entropy Model-
ing Toolkit4 to predict the semantic relation for
each dependency. The system-combining strat-
egy involves three steps:
? Parsing each sentence using Nivre?s arc
standard, Nivre?s arc eager (Nivre and
Nilsson, 2005; Nivre, 2008), and Liang?s
dynamic algorithm (Huang and Sagae,
2010);
? Combining parses given by the three
parsers into a weighted directed graph;
? Using the Chu-Liu-Edmonds algorithm to
search for the final parse for each sen-
tence.
5. Giuseppe Attardi-SVM-1-R, Giuseppe Attardi-
SVM-1-rev
We didn?t receive the system description of
these two systems.
5 Results & Analysis
LAS is the main evaluation metric in Chinese Se-
mantic Dependency Parsing, whereas UAS is the
secondary metric. Table 4 shows the results for these
two indicators in all participating systems.
As shown in Table 4, the Zhou Qiaoli-3 system
achieved the best results with LAS of 61.84. The
LAS values of top systems are very closely. We per-
formed significance tests5 for top six results. Table
5 shows the results , from which we can see that
the performances of top five results are comparative
(p > 0.1) and the rank sixth system is significantly
(p < 10?5) worse than top five results.
4http://homepages.inf.ed.ac.uk/s0450736/
maxenttoolkit.html
5http://www.cis.upenn.edu/?dbikel/
download/compare.pl
382
NJU-Parser-2 NJU-Parser-1 Zhijun Wu-1 Zhou Qiaoli-1 Zhou Qiaoli-2
Zhou Qiaoli-3 ? ? ? ? >
NJU-Parser-2 ? ? ? ? >
NJU-Parser-1 ? ? ? ? >
Zhijun Wu-1 ? ? ? ? >
Zhou Qiaoli-1 ? ? ? ? >
Table 5: Significance tests of the top five systems. ? denotes that the two systems are comparable (p > 0.1), and >
means the system of this row is significantly (p < 10?5) better than the system of this column.
System LAS UAS
Zhou Qiaoli-3 61.84 80.60
NJU-Parser-2 61.64 80.29
NJU-Parser-1 61.63 80.35
Zhijun Wu-1 61.58 80.64
Zhou Qiaoli-1 61.15 80.41
Zhou Qiaoli-2 57.55 78.55
ICT-1 56.31 73.20
Giuseppe Attardi-SVM-1-R 44.46 60.83
Giuseppe Attardi-SVM-1-rev 21.86 40.47
Average 54.22 72.82
Table 4: Results of the submitted systems.
The average LAS for all systems was 54.22.
Chinese Semantic Dependency Parsing performed
much more poorly than Chinese Syntactic Depen-
dency Parsing due to the increased complexity
brought about by the greater number of semantic re-
lations compared with syntactic relations, as well as
greater difficulty in classifying semantic relations.
In general, all the systems employed the tradi-
tional syntax-dominated dependency parsing frame-
works. Some new methods were proposed for
this task. Zhou Qiaoli?s systems first identified
the semantic role phrase in a sentence, and then
employed graph-based dependency parsing to ana-
lyze the semantic structure of the sentence. NJU-
Parser first split the sentence into sub-sentences,
then trained and parsed the sentence based on these
sub-sentences; this was shown to perform well. In
addition, ensemble models were also proposed to
solve the task using ICT systems.
6 Conclusion
We described the Chinese Semantic Dependency
Parsing task for SemEval-2012, which is designed to
parse the semantic structures of Chinese sentences.
Nine results were submitted by five organizations,
with the best result garnering an LAS score of 61.84,
which is far below the performance of Chinese Syn-
tax. This demonstrates that further research on the
structure of Chinese Semantics is needed.
In the future, we will check and improve the anno-
tation standards while building a large, high-quality
corpus for further Chinese semantic research.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This work was supported by Na-
tional Natural Science Foundation of China (NSFC)
via grant 61133012 and 61170144, and the Na-
tional ?863? Leading Technology Research Project
via grant 2012AA011102.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149?164,
New York City, June. Association for Computational
Linguistics.
Dongfeng Cai, Ling Zhang, Qiaoli Zhou, and Yue Zhao.
2011. A collocation based approach for prepositional
phrase identification. IEEE NLPKE.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Pennsyl-
vania University.
Zhendong Dong and Qiang Dong. 2006. Hownet And the
Computation of Meaning. World Scientific Publishing
Co., Inc., River Edge, NJ, USA.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1077?1086,
383
Uppsala, Sweden, July. Association for Computational
Linguistics.
Guangjin Jin. 2001. Theory of modern Chinese verb se-
mantic computation. Beijing University Press.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the ACL, number July, pages 1?11.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. In Synthesis Lectures on
Human Language Technologies.
Mingqin Li, Juanzi Li, Zhendong Dong, Zuoying Wang,
and Dajin Lu. 2003. Building a large chinese corpus
annotated with semantic dependency. In Proceedings
of the second SIGHAN workshop on Chinese language
processing - Volume 17, SIGHAN ?03, pages 84?91,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
chinese pos tagging and dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1180?
1191, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency parsing.
Ph.D. thesis, University of Pennsylvania.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Nianwen Xue and Martha Palmer. 2003. Annotating
the propositions in the penn chinese treebank. In Pro-
ceedings of the Second SIGHAN Workshop on Chinese
Language Processing.
Nianwen Xue and Martha Palmer. 2005. Automatic se-
mantic role labeling for chinese verbs. In Proceedings
of the 19th International Joint Conference on Artificial
Intelligence.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Guoguang Zhou and Linlin Zhang. 2003. The theory
and method of modern Chinese grammar. Guangdong
Higher Education Press.
384
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238?242
Manchester, August 2008
A Cascaded Syntactic and Semantic Dependency Parsing System
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng Li
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cn
Abstract
We describe our CoNLL 2008 Shared Task
system in this paper. The system includes
two cascaded components: a syntactic and
a semantic dependency parsers. A first-
order projective MSTParser is used as our
syntactic dependency parser. In order to
overcome the shortcoming of the MST-
Parser, that it cannot model more global in-
formation, we add a relabeling stage after
the parsing to distinguish some confusable
labels, such as ADV, TMP, and LOC. Be-
sides adding a predicate identification and
a classification stages, our semantic de-
pendency parsing simplifies the traditional
four stages semantic role labeling into two:
a maximum entropy based argument clas-
sification and an ILP-based post inference.
Finally, we gain the overall labeled macro
F1 = 82.66, which ranked the second posi-
tion in the closed challenge.
1 System Architecture
Our CoNLL 2008 Shared Task (Surdeanu et al,
2008) participating system includes two cascaded
components: a syntactic and a semantic depen-
dency parsers. They are described in Section 2
and 3 respectively. Their experimental results are
shown in Section 4. Section 5 gives our conclusion
and future work.
2 Syntactic Dependency Parsing
MSTParser (McDonald, 2006) is selected as our
basic syntactic dependency parser. It views the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
syntactic dependency parsing as a problem of
finding maximum spanning trees (MST) in di-
rected graphs. MSTParser provides the state-of-
the-art performance for both projective and non-
projective tree banks.
2.1 Features
The score of each labeled arc is computed through
the Eq. (1) in MSTParser.
score(h, c, l) = w ? f(h, c, l) (1)
where node h represents the head node of the arc,
while node c is the dependent node (or child node).
l denotes the label of the arc.
There are three major differences between our
feature set and McDonald (2006)?s:
1) We use the lemma as a generalization feature
of a word, while McDonald (2006) use the word?s
prefix.
2) We add two new features: ?bet-pos-h-same-
num? and ?bet-pos-c-same-num?. They represent
the number of nodes which locate between node h
and node c and whose POS tags are the same with
h and c respectively.
3) We use more back-off features than McDon-
ald (2006) by completely enumerating all of the
possible combinatorial features.
2.2 Relabeling
By observing the current results of MSTParser on
the development data, we find that the performance
of some labels are far below average, such as ADV,
TMP, LOC. We think the main reason lies in that
MSTParser only uses local features restricted to a
single arc (as shown in Eq. (1)) and fails to use
more global information. Consider two sentences:
?I read books in the room.? and ?I read books in
the afternoon.?. It is hard to correctly label the arc
238
Deprel Total Mislabeled as
NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],
AMOD [0.1]
OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]
ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],
DIR [1.5]
NAME 1,138 NMOD [2.2]
VC 953 PRD [0.9]
DEP 772 NMOD [4.0]
TMP 755 ADV [9.9], LOC [6.5]
LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]
AMOD 536 ADV [2.2]
PRD 509 VC [4.7]
APPO 444 NMOD [2.5]
OPRD 373 OBJ [4.6]
DIR 119 ADV [18.5]
MNR 109 ADV [28.4]
Table 1: Error Analysis of Each Label
between ?read? and ?in? unless we know the object
of ?in?.
We count the errors of each label, and show the
top ones in Table 1. ?Total? refers to the total num-
ber of the corresponding label in the development
data. The column of ?Mislabeled as? lists the la-
bels that an arc may be mislabeled as. The number
in brackets shows the percentage of mislabeling.
As shown in the table, some labels are often con-
fusable with each other, such as ADV, LOC and
TMP.
2.3 Relabeling using Maximum Entropy
Classifier
We constructed two confusable label set which
have a higher mutual mislabeling proportion:
(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,
OPRD). A maximum entropy classifier is used to
relabel them.
Features are shown in Table 2. The first column
lists local features, which contains information of
the head node h and the dependent node c of an arc.
?+ dir dist? means that conjoining existing features
with arc direction and distance composes new fea-
tures. The second column lists features using the
information of node c?s children. ?word c c? rep-
resents form or lemma of one child of the node
c. ?dir c? and ?dist c? represents the direction and
distance of the arc which links node c to its child.
The back-off technique is also used on these fea-
tures.
Local features (+ dir dist) Global features (+ dir c dist c)
word h word c word h word c word c c
Table 2: Relabeling Feature Set (+ dir dist)
3 Semantic Dependency Parsing
3.1 Architecture
The whole procedure is divided into four separate
stages: Predicate Identification, Predicate Classifi-
cation, Semantic Role Classification, and Post In-
ference.
During the Predicate Identification stage we ex-
amine each word in a sentence to discover target
predicates, including both noun predicates (from
NomBank) and verb predicates (from PropBank).
In the Predicate Classification stage, each predi-
cate is assigned a certain sense number. For each
predicate, the probabilities of a word in the sen-
tence to be each semantic role are predicted in the
Semantic Role Classification stage. Maximum en-
tropy model is selected as our classifiers in these
stages. Finally an ILP (Integer Linear Program-
ming) based method is adopted for post infer-
ence (Punyakanok et al, 2004).
3.2 Predicate Identification
The predicate identification is treated as a binary
classification problem. Each word in a sentence is
predicted to be a predicate or not to be. A set of
features are extracted for each word, and an opti-
mized subset of them are adopted in our final sys-
tem. The following is a full list of the features:
DEPREL (a1): Type of relation to the parent.
WORD (a21), POS (a22), LEMMA (a23),
HEAD (a31), HEAD POS (a32), HEAD LEMMA
(a33): The forms, POS tags and lemmas of a word
and it?s headword (parent) .
FIRST WORD (a41), FIRST POS (a42),
FIRST LEMMA (a43), LAST WORD (a51),
LAST POS (a52), LAST LEMMA (a53): A
corresponding ?constituent? for a word consists
of all descendants of it. The forms, POS tags and
lemmas of both the first and the last words in the
constituent are extracted.
POS PAT (a6): A ?POS pattern? is produced for
the corresponding constituent as follows: a POS
bag is produced with the POS tags of the words
in the constituent except for the first and the last
ones, duplicated tags removed and the original or-
der ignored. Then we have the POS PAT feature
239
by combining the POS tag of the first word, the
bag and the POS tag of the last word.
CHD POS (a71), CHD POS NDUP (a72),
CHD REL (a73), CHD REL NDUP (a74): The
POS tags of the child words are joined to-
gether to form feature CHD POS. With adja-
cently duplicated tags reduced to one, feature
CHD POS NDUP is produced. Similarly we can
get CHD REL and CHD REL NDUP too, with
the relation types substituted for the POS tags.
SIB REL (a81), SIB REL NDUP (a82),
SIB POS (a83), SIB POS NDUP (a84): Sibling
words (including the target word itself) and the
corresponding dependency relations (or POS tags)
are considered as well. Four features are formed
similarly to those of child words.
VERB VOICE (a9): Verbs are examined for
voices: if the headword lemma is either ?be? or
?get?, or else the relation type is ?APPO?, then the
verb is considered passive, otherwise active.
Also we used some ?combined? features which
are combinations of single features. The final op-
timized feature set is (a1, a21, a22, a31, a32, a41,
a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,
a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,
a81+a83).
3.3 Predicate Classification
After predicate identification is done, the resulting
predicates are processed for sense classification. A
classifier is trained for each predicate that has mul-
tiple senses on the training data (There are totally
962 multi-sense predicates on the training corpus,
taking up 14% of all) In additional to those fea-
tures described in the predicate identification sec-
tion, some new ones relating to the predicate word
are introduced:
BAG OF WORD (b11), BAG OF WORD O
(b12): All words in a sentence joined, namely
?Bag of Words?. And an ?ordered? version is in-
troduced where each word is prefixed with a letter
?L?, ?R? or ?T? indicating it?s to the left or right of
the predicate or is the predicate itself.
BAG OF POS O (b21), BAG OF POS N
(b22): The POS tags prefixed with ?L?, ?R? or
?T? indicating the word position joined together,
namely ?Bag of POS (Ordered)?. With the
prefixed letter changed to a number indicating
the distance to the predicate (negative for being
left to the predicate and positive for right), an-
other feature is formed, namely ?Bag of POS
(Numbered)?.
WIND5 BIGRAM (b3): 5 closest words from
both left and right plus the predicate itself, in total
11 words form a ?window?, within which bigrams
are enumerated.
The final optimized feature set for the task of
predicate classification is (a1, a21, a23, a71, a72,
a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,
a71+a9).
3.4 Semantic Role Classification
In our system, the identification and classifica-
tion of semantic roles are achieved in a single
stage (Liu et al, 2005) through one single classi-
fier (actually two, one for noun predicates, and the
other for verb predicates). Each word in a sentence
is given probabilities to be each semantic role (in-
cluding none of the these roles) for a predicate.
Features introduced in addition to those of the pre-
vious subsections are the following:
POS PATH (c11), REL PATH (c12): The ?POS
Path? feature consists of POS tags of the words
along the path from a word to the predicate. Other
than ?Up? and ?Down?, the ?Left? and ?Right? di-
rection of the path is added. Similarly, the ?Re-
lation Path? feature consists of the relation types
along the same path.
UP PATH (c21), UP REL PATH (c22): ?Up-
stream paths? are parts of the above paths that stop
at the common ancestor of a word and the predi-
cate.
PATH LEN (c3): Length of the paths
POSITION (c4): The relative position of a word
to the predicate: Left or Right.
PRED FAMILYSHIP (c5): ?Familyship rela-
tion? between a word and the predicate, being one
of ?self?, ?child?, ?descendant?, ?parent?, ?ances-
tor?, ?sibling?, and ?not-relative?.
PRED SENSE (c6): The lemma plus sense
number of the predicate
As for the task of semantic role classification,
the features of the predicate word in addition to
those of the word under consideration can also
be used; we mark features of the predicate with
an extra ?p?. For example, the head word of
the current word is represented as a31, and the
head word of the predicate is represented as pa31.
So, with no doubt for the representation, our fi-
nal optimized feature set for the task of seman-
tic role classification is (a1, a23, a33, a43, a53,
a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,
240
pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,
a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).
3.5 ILP-based Post Inference
The final semantic role labeling result is gener-
ated through an ILP (Integer Linear Programming)
based post inference method. An ILP problem is
formulated with respect to the probability given by
the above stage. The final labeling is formed at the
same time when the problem is solved.
Let W be the set of words in the sentence, and
R be the set of semantic role labels. A virtual label
?NULL? is also added to R, representing ?none of
the roles is assigned?.
For each word w ? W and semantic role label
r ? R we create a binary variable v
wr
? (0, 1),
whose value indicates whether or not the word w
is labeled as label r. p
wr
denotes the possibil-
ity of word w to be labeled as role r. Obviously,
when objective function f =
?
w,r
log(p
wr
? v
wr
)
is maximized, we can read the optimal labeling for
a predicate from the assignments to the variables
v
wr
. There are three constrains used in our system:
C1: Each relation should be and only be la-
beled with one label (including the virtual label
?NULL?), i.e.:
?
r
v
wr
= 1
C2: Roles with a small probability should never
be labeled (except for the virtual role ?NULL?).
The threshold we use in our system is 0.3, which
is optimized from the development data. i.e.:
v
wr
= 0, if p
wr
< 0.3 and r 6= ?NULL?
C3: Statistics shows that the most roles (ex-
cept for the virtual role ?NULL?) usually appear
only once for a predicate, except for some rare ex-
ception. So we impose a no-duplicated-roles con-
straint with an exception list, which is constructed
according to the times of semantic roles? duplica-
tion for each single predicate (different senses of a
predicate are considered different) and the ratio of
duplication to non-duplication.
?
r
v
wr
? 1,
if < p, r > /? {< p, r > |p ? P, r ? R;
d
pr
c
pr
?d
pr
> 0.3 ? d
pr
> 10}
(2)
where P is the set of predicates; c
pr
denotes the
count of words in the training corpus, which are
Predicate Type Predicate Label
Noun president.01 A3
Verb match.01 A1
Verb tie.01 A1
Verb link.01 A1
Verb rate.01 A0
Verb rate.01 A2
Verb attach.01 A1
Verb connect.01 A1
Verb fit.01 A1
Noun trader.01 SU
Table 3: No-duplicated-roles constraint exception
list (obtained by Eq. (2))
labeled as r ? R for predicate p ? P ; while d
pr
denotes something similar to c
pr
, but what taken
into account are only those words labeled with r,
and there are more than one roles within the sen-
tence for the same predicate. Table 3 lists the com-
plete exception set, which has a size of only 10.
4 Experiments
The original MSTParser
1
is implemented in Java.
We were confronted with memory shortage when
trying to train a model with the entire CoNLL 2008
training data with 4GB memory. Therefore, we
rewrote it with C++ which can manage the mem-
ory more exactly. Since the time was limited, we
only rewrote the projective part without consider-
ing second-order parsing technique.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit
2
. The
classifier parameters: gaussian prior and iterations,
are tuned with the development data for different
stages respectively.
lp solve 5.5
3
is chosen as our ILP problem
solver during the post inference stage.
The training time of the syntactic and the se-
mantic parsers are 22 and 5 hours respectively, on
all training data, with 2.0GHz Xeon CPU and 4G
memory. While the prediction can be done within
10 and 5 minutes on the development data.
4.1 Syntactic Dependency Parsing
The experiments on development data show that
relabeling process is helpful, which improves the
1
http://sourceforge.net/projects/mstparser
2
http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
3
http://sourceforge.net/projects/lpsolve
241
Precision (%) Recall (%) F1
Pred Identification 91.61 91.36 91.48
Pred Classification 86.61 86.37 86.49
Table 4: The performance of predicate identifica-
tion and classification
Precision (%) Recall (%) F1
Simple 81.02 76.00 78.43
ILP-based 82.53 75.26 78.73
Table 5: Comparison between different post infer-
ence strategies
LAS performance from 85.41% to 85.94%. The fi-
nal syntactic dependency parsing performances on
the WSJ and the Brown test data are 87.51% and
80.73% respectively.
4.2 Semantic Dependency Parsing
The semantic dependency parsing component is
based on the last syntactic dependency parsing
component. All stages of the system are trained
with the closed training corpus, while predicted
against the output of the syntactic parsing.
Performance for predicate identification and
classification is given in Table 4, wherein the clas-
sification is done on top of the identification.
Semantic role classification and the post infer-
ence are done on top of the result of predicate iden-
tification and classification. The final performance
is presented in Table 5. A simple post inference
strategy is given for comparison, where the most
possible label (including the virtual label ?NULL?)
is select except for those duplicated non-virtual la-
bels with lower probabilities (lower than 0.5). Our
ILP-based method produces a gain of 0.30 with re-
spect to the F1 score.
The final semantic dependency parsing perfor-
mance on the development and the test (WSJ and
Brown) data are shown in Table 6.
Precision (%) Recall (%) F1
Development 82.53 75.26 78.73
Test (WSJ) 82.67 77.50 80.00
Test (Brown) 64.38 68.50 66.37
Table 6: Semantic dependency parsing perfor-
mances
4.3 Overall Performance
The overall macro scores of our syntactic and se-
mantic dependency parsing system are 82.38%,
83.78% and 73.57% on the development and two
test (WSJ and Brown) data respectively, which is
ranked the second position in the closed challenge.
5 Conclusion and Future Work
We present our CoNLL 2008 Shared Task system
which is composed of two cascaded components:
a syntactic and a semantic dependency parsers,
which are built with some state-of-the-art methods.
Through a fine tuning features and parameters, the
final system achieves promising results. In order
to improve the performance further, we will study
how to make use of more resources and tools (open
challenge) and how to do joint learning between
syntactic and semantic parsing.
Acknowledgments
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(NSFC) via grant 60675034, 60575042, and the
?863? National High-Tech Research and Develop-
ment of China via grant 2006AA01Z145.
References
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, June.
McDonald, Ryan. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346?1352.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
242
