Columbia?s Newsblaster: New Features and Future Directions
Kathleen McKeown, Regina Barzilay, John Chen, David Elson, David Evans,
Judith Klavans, Ani Nenkova, Barry Schiffman and Sergey Sigelman
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, New York, N.Y. 10027
kathy@cs.columbia.edu
Abstract
Columbia?s Newsblaster tracking and summa-
rization system is a robust system that clus-
ters news into events, categorizes events into
broad topics and summarizes multiple articles
on each event. Here we outline our most cur-
rent work on tracking events over days, produc-
ing summaries that update a user on new infor-
mation about an event, outlining the perspec-
tives of news coming from different countries
and clustering and summarizing non-English
sources.
1 Introduction
Columbia?s Newsblaster1 provide news updates on a
daily basis from news published on the Internet; it crawls
news sites, categorizes stories into six broad areas, groups
news into stories on the same event, and generates a sum-
mary of the multiple articles describing each event. In ad-
dition to demonstrating the robustness of current summa-
rization and tracking technology, Newsblaster also serves
as a research environment in which we explore new di-
rections and problems. Currently, we are exploring the
tasks of multilingual summarization where input sources
are drawn frommultiple languages and a summary is gen-
erated in English on the same event (Figure 1), tracking
events across days and generating summaries that update
the user on what is new, and editing generated summaries
to improve fluency and accuracy. Our focus here is on
editing references to people, improving coherency of the
summary and ensuring that references are accurate. Edit-
ing is particularly important as we add multilingual capa-
bilities, given the errors inherent in machine translation.
1http://newsblaster.cs.columbia.edu
2 Multilingual Tracking and
Summarization
The multilingual version of Columbia Newsblaster is
built upon the English version of Columbia Newsblaster,
sharing the same structure and components. To add mul-
tilingual capability, the system first crawls web sites in
foreign languages, and stores both the language and en-
coding for the files. To extract the article text from the
HTML pages, we use a new article extraction component
using language-independent statistical features computed
over text blocks along with a machine learning compo-
nent to classify text blocks as one of ?Article Text?, ?Ti-
tle?, ?Image?, ?Image Caption?, or ?Other?. The article
extraction component has been trained and tested on En-
glish, Japanese, and Russian data, but is also being suc-
cessfully applied to French, Spanish, German, and Ital-
ian data. We plan to train the article extractor on other
languages (Chinese, Arabic, Korean, Spanish, German,
French, etc.) in the near future.
To cluster multilingual documents with English doc-
uments, we use the existing Newsblaster English doc-
ument clustering module. Non-English documents are
translated for clustering after the article extraction phase.
We use simple and fast document translation techniques
for clustering if available, since we potentially process
thousands of documents for a language for each run. We
have developed simple dictionary lookup techniques for
translation for clustering for Japanese and Russian; for
other languages we use an interface to the Systran trans-
lation system via Babelfish. We plan on adding Arabic
translation to the system in the near future.
Summarization is performed using the same summa-
rization strategies in Newsblaster. We are experimenting
with different methods for improving summary quality
when translation of text is noisy. For example, when an
input cluster contains both English and foreign sources,
we weight the English higher in cases where we deter-
mine it is representative of both the English and foreign
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 15-16
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Multilingual Version
input documents. We are also experimenting with meth-
ods for determining similarity across documents using
different levels of translation.
3 Different Perspectives
When news media report on international issues, they re-
flect the perspectives of their own countries. In the past,
Newsblaster has included all international sources as in-
put to its summaries. Recently, we have added a feature
of ?international perspectives? to the system. In addition
to the universal summary for a particular event, which
includes all international sources, Newsblaster now gen-
erates separate summaries for each country, which may
illustrate unique biases or disagree on facts. The News-
blaster interface allows users to view any pair of sum-
maries side by side to compare different perspectives.
4 Summary Rewrite
Newsblaster also currently includes a module for rewrit-
ing summaries to achieve better readability. References
to people are rewritten so that the first mention includes
the person?s full name and a selected description and later
mentions are restricted to last name only. In addition to
improving readability, the rewritten version of the sum-
mary is usually shorter than the version before rewrite,
since multiple verbose descriptions of the same entity are
discarded. These changes can be seen when comparing
the summary sentence with the original document via a
link from the summary using a proxy.
5 Event Tracking and Updates
Newsblaster currently identifies events within a single
day; a new set of clusters is generated each day. We have
designed a new module for tracking events across days,
allowing the system to relate stories published on one day
to closely related stories on other days. In this way, the
user can more easily track events of interest as they un-
fold. The typical approach for tracking events across days
represents each event as one monolithic set of stories. We
have focused instead on a model where events on one day
can divide into related sub-events on the next day. For ex-
ample, a set of stories about the start of the Iraq war is an
event that can branch into multiple sets of stories, each set
representing a different facet of the war. We are currently
determining an appropriate evaluation of this approach as
well as investigating different possible interfaces.
If a user is tracking events across days, it is more useful
to have a summary that provides updates on what is new
as opposed to a summary of similarities across all days.
We have built a prototype update summarizer that scans
new articles extracted by the system and compares these
new articles with a background cluster on the same event.
The summarizer will provide the user with a summary of
only important new developments. As the tracking mod-
ule locates new articles, it will pass these to the update
summarizer, which will determine what, if anything, has
changed. This summarizer uses more syntactic and se-
mantic information about the articles to determine nov-
elty than is used in our other summarization strategies and
thus, efficiency is a challenge. We will demo these com-
ponents in a separately fromNewsblaster as they have not
yet been integrated in the development version.
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 9?12,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
A Tool for Deep Semantic Encoding of Narrative Texts
David K. Elson
Columbia University
New York City
delson@cs.columbia.edu
Kathleen R. McKeown
Columbia University
New York City
kathy@cs.columbia.edu
Abstract
We have developed a novel, publicly avail-
able annotation tool for the semantic en-
coding of texts, especially those in the
narrative domain. Users can create for-
mal propositions to represent spans of text,
as well as temporal relations and other
aspects of narrative. A built-in natural-
language generation component regener-
ates text from the formal structures, which
eases the annotation process. We have
run collection experiments with the tool
and shown that non-experts can easily cre-
ate semantic encodings of short fables.
We present this tool as a stand-alone, re-
usable resource for research in semantics
in which formal encoding of text, espe-
cially in a narrative form, is required.
1 Introduction
Research in language processing has benefited
greatly from the collection of large annotated
corpora such as Penn PropBank (Kingsbury and
Palmer, 2002) and Penn Treebank (Marcus et al,
1993). Such projects typically involve a formal
model (such as a controlled vocabulary of thematic
roles) and a corpus of text that has been anno-
tated against the model. One persistent tradeoff in
building such resources, however, is that a model
with a wider scope is more challenging for anno-
tators. For example, part-of-speech tagging is an
easier task than PropBank annotation. We believe
that careful user interface design can alleviate dif-
ficulties in annotating texts against deep semantic
models. In this demonstration, we present a tool
we have developed, SCHEHERAZADE, for deep
annotation of text.
1
We are using the tool to collect semantic rep-
resentations of narrative text. This domain occurs
1
Available at http://www.cs.columbia.edu/?delson.
frequently, yet is rarely studied in computational
linguistics. Narrative occurs with every other dis-
course type, including dialogue, news, blogs and
multi-party interaction. Given the volume of nar-
rative prose on the Web, a system competent at un-
derstanding narrative structures would be instru-
mental in a range of text processing tasks, such
as summarization or the generation of biographies
for question answering.
In the pursuit of a complete and connected rep-
resentation of the underlying facts of a story, our
annotation process involves the labeling of verb
frames, thematic roles, temporal structure, modal-
ity, causality and other features. This type of anno-
tation allows for machine learning on the thematic
dimension of narrative ? that is, the aspects that
unite a series of related facts into an engaging and
fulfilling experience for a reader. Our methodol-
ogy is novel in its synthesis of several annotation
goals and its focus on content rather than expres-
sion. We aim to separate the narrative?s fabula, the
content dimension of the story, from the rhetori-
cal presentation at the textual surface (sju?zet) (Bal,
1997). To this end, our model incorporates formal
elements found in other discourse-level annotation
projects such as Penn Discourse Treebank (Prasad
et al, 2008) and temporal markup languages such
as TimeML (Mani and Pustejovsky, 2004). We
call the representation a story graph, because these
elements are embodied by nodes and connected by
arcs that represent relationships such as temporal
order and motivation.
More specifically, our annotation process in-
volves the construction of propositions to best ap-
proximate each of the events described in the tex-
tual story. Every element of the representation
is formally defined from controlled vocabularies:
the verb frames, with their thematic roles, are
adapted from VerbNet (Kipper et al, 2006), the
largest verb lexicon available in English. When
the verb frames are filled in to construct action
9
Figure 1: Screenshot from our tool showing the process of creating a formal proposition. On the left, the
user is nesting three action propositions together; on the right, the user selects a particular frame from a
searchable list. The resulting propositions are regenerated in rectangular boxes.
propositions, the arguments are either themselves
propositions or noun synsets from WordNet (the
largest available noun lexicon (Fellbaum, 1998)).
Annotators can also write stative propositions
and modifiers (with adjectives and adverbs culled
from WordNet), and distinguish between goals,
plans, beliefs and other ?hypothetical? modalities.
The representation supports connectives including
causality and motivation between these elements.
Finally, and crucially, each proposition is bound
to a state (time slice) in the story?s main timeline
(a linear sequence of states). Additional timelines
can represent multi-state beliefs, goals or plans. In
the course of authoring actions and statives, an-
notators create a detailed temporal framework to
which they attach their propositions.
2 Description of Tool
The collection process is amenable to community
and non-expert annotation by means of a graphical
encoding tool. We believe this resource can serve
a range of experiments in semantics and human
text comprehension.
As seen in Figure 1, the process of creating a
proposition with our tool involves selecting an ap-
propriate frame and filling the arguments indicated
by the thematic roles of the frame. Annotators are
guided through the process by a natural-language
generation component that is able to realize textual
equivalents of all possible propositions. A search
in the interface for ?flatter,? for example, offers a
list of relevant frames such as<A character> flat-
ters<a character>. Upon selecting this frame, an
annotator is able to supply arguments by choosing
actors from a list of declared characters. ?The fox
flatters the crow,? for one, would be internally rep-
resented with the proposition <flatters>([Fox
1
],
[Crow
1
]) where flatters, Fox and Crow are not
snippets of surface text, but rather selected Word-
Net and VerbNet records. (The subscript indi-
cates that the proposition is invoking a particular
[Fox] instance that was previously declared.) In
this manner an entire story can be encoded.
Figure 2 shows a screenshot from our interface
in which propositions are positioned on a timeline
to indicate temporal relationships. On the right
side of the screen are the original text (used for
reference) and the entire story as regenerated from
10
Figure 2: The main screen of our tool features a graphical timeline, as well as boxes for the reference
text and the story as regenerated by the system from the formal model.
the current state of the formal model. It is also pos-
sible from this screen to invoke modalities such
as goals, plans and beliefs, and to indicate links
between propositions. Annotators are instructed
to construct propositions until the resulting textual
story, as realized by the generation component, is
as close to their own understanding of the story as
permitted by the formal representation.
The tool includes annotation guidelines for con-
structing the best propositions to approximate the
content of the story. Depending on the intended
use of the data, annotators may be instructed to
model just the stated content in the text, or include
the implied content as well. (For example, causal
links between events are often not articulated in a
text.) The resulting story graph is a unified rep-
resentation of the entire fabula, without a story?s
beginning or end. In addition, the tool allows an-
notators to select spans of text and link them to
the corresponding proposition(s). By indicating
which propositions were stated in the original text,
and in what order, the content and presentation di-
mensions of a story are cross-indexed.
3 Evaluation
We have conducted several formative evaluations
and data collection experiments with this inter-
face. In one, four annotators each modeled four of
the fables attributed to Aesop. In another, two an-
notators each modeled twenty fables. We chose to
model stories from the Aesop corpus due to sev-
eral key advantages: the stories are mostly built
from simple declaratives, which are within the ex-
pressive range of our semantic model, yet are rich
in thematic targets for automatic learning (such as
dilemmas where characters must choose from be-
tween competing values).
In the latter collection, both annotators were un-
dergraduates in our engineering school and native
English speakers, with little background in lin-
guistics. For this experiment, we instructed them
to only model stated content (as opposed to includ-
ing inferences), and skip the linking to spans of
source text. On average, they required 35-45 min-
utes to encode a fable, though this decreased with
practice. The 40 encodings include 574 proposi-
tions, excluding those in hypothetical modalities.
The fables average 130 words in length (so the an-
notators created, on average, one proposition for
every nine words).
Both annotators became comfortable with the
tool after a period of training; in surveys that they
completed after each task, they gave Likert-scale
usability scores of 4.25 and 4.30 (averaged over
all 20 tasks, with a score of 5 representing ?easiest
to use?). The most frequently cited deficiencies in
the model were abstract concepts such as fair (in
the sense of a community event), which we plan to
support in a future release.
4 Results and Future Work
The end result from a collection experiment is
a collection of story graphs which are suitable
for machine learning. An example story graph,
based on the state of the tool seen in Figure 2, is
shown in Figure 3. Nodes in the graph represent
states, declared objects and propositions (actions
and statives). Each of the predicates (e.g.,<lion>,
11
????? ?????????????????
???????????
???????????
?????????
????????
?
????????? ?????
??? ?????
????????????
????????????
?????
??????????
?????????
?????
?????
Figure 3: A portion of a story graph representation as created by SCHEHERAZADE.
<watch>, <cunning>) are linked to their corre-
sponding VerbNet and WordNet records.
We are currently experimenting with ap-
proaches for data-driven analysis of narrative con-
tent along the ?thematic? dimension as described
above. In particular, we are interested in the auto-
matic discovery of deep similarities between sto-
ries (such as analogous structures and prototypical
characters). We are also interested in investigat-
ing the selection and ordering of content in the
story?s telling (that is, which elements are stated
and which remain implied), especially as they per-
tain to the reader?s affectual responses. We plan
to make the annotated corpus publicly available in
addition to the tool.
Overall, while more work remains in expanding
the model as well as the graphical interface, we
believe we are providing to the community a valu-
able new tool for eliciting semantic encodings of
narrative texts for machine learning purposes.
5 Script Outline
Our demonstration involves a walk-through of the
SCHEHERAZADE tool. It includes:
1. An outline of the goals of the project and the
innovative aspects of our formal representa-
tion compared to other representations cur-
rently in the field.
2. A tour of the timeline screen (equivalent to
Figure 2) as configured for a particular Aesop
fable.
3. The procedure for reading a text for impor-
tant named entities, and formally declaring
these named entities for the story graph.
4. The process for constructing propositions in
order to encode actions and statives in the
text, as seen in Figure 1.
5. Other features of the software package, such
as the setting of causal links and the ability to
undo/redo.
6. A review of the results of our formative eval-
uations and data collection experiments, in-
cluding surveys of user satisfaction.
References
Mieke Bal. 1997. Narratology: Introduction to the
Theory of Narrative. University of Toronto Press,
Toronto, second edition.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Canary Islands, Spain.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extensive classifications of
english verbs. In Proceedings of the 12th EURALEX
International Congress, Turin, Italy.
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral discourse models for narrative structure. In
Proceedings of the ACL Workshop on Discourse An-
notation, Barcelona, Spain.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
12
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138?147,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Extracting Social Networks from Literary Fiction
David K. Elson
Dept. of Computer Science
Columbia University
delson@cs.columbia.edu
Nicholas Dames
English Department
Columbia University
nd122@columbia.edu
Kathleen R. McKeown
Dept. of Computer Science
Columbia University
kathy@cs.columbia.edu
Abstract
We present a method for extracting so-
cial networks from literature, namely,
nineteenth-century British novels and se-
rials. We derive the networks from di-
alogue interactions, and thus our method
depends on the ability to determine when
two characters are in conversation. Our
approach involves character name chunk-
ing, quoted speech attribution and conver-
sation detection given the set of quotes.
We extract features from the social net-
works and examine their correlation with
one another, as well as with metadata such
as the novel?s setting. Our results provide
evidence that the majority of novels in this
time period do not fit two characterizations
provided by literacy scholars. Instead, our
results suggest an alternative explanation
for differences in social networks.
1 Introduction
Literary studies about the nineteenth-century
British novel are often concerned with the nature
of the community that surrounds the protagonist.
Some theorists have suggested a relationship be-
tween the size of a community and the amount of
dialogue that occurs, positing that ?face to face
time? diminishes as the number of characters in
the novel grows. Others suggest that as the social
setting becomes more urbanized, the quality of di-
alogue also changes, with more interactions occur-
ring in rural communities than urban communities.
Such claims have typically been made, however,
on the basis of a few novels that are studied in
depth. In this paper, we aim to determine whether
an automated study of a much larger sample of
nineteenth century novels supports these claims.
The research presented here is concerned with
the extraction of social networks from literature.
We present a method to automatically construct
a network based on dialogue interactions between
characters in a novel. Our approach includes com-
ponents for finding instances of quoted speech,
attributing each quote to a character, and iden-
tifying when certain characters are in conversa-
tion. We then construct a network where char-
acters are vertices and edges signify an amount
of bilateral conversation between those charac-
ters, with edge weights corresponding to the fre-
quency and length of their exchanges. In contrast
to previous approaches to social network construc-
tion, ours relies on a novel combination of pattern-
based detection, statistical methods, and adapta-
tion of standard natural language tools for the liter-
ary genre. We carried out this work on a corpus of
60 nineteenth-century novels and serials, includ-
ing 31 authors such as Dickens, Austen and Conan
Doyle.
In order to evaluate the literary claims in ques-
tion, we compute various characteristics of the
dialogue-based social network and stratify these
results by categories such as the novel?s setting.
For example, the density of the network provides
evidence about the cohesion of a large or small
community, and cliques may indicate a social frag-
mentation. Our results surprisingly provide evi-
dence that the majority of novels in this time pe-
riod do not fit the suggestions provided by liter-
ary scholars, and we suggest an alternative expla-
nation for our observations of differences across
novels.
In the following sections, we survey related
work on social networks as well as computational
studies of literature. We then present the literary
hypotheses in more detail. We describe the meth-
ods we use to extract dialogue and construct con-
versational networks, along with our approach to
analyzing their characteristics. After we present
the statistical results, we analyze their significance
from a literary perspective.
138
2 Related Work
Computer-assisted literary analysis has typically
occurred at the word level. This level of granular-
ity lends itself to studies of authorial style based
on patterns of word use (Burrows, 2004), and re-
searchers have successfully ?outed? the writers of
anonymous texts by comparing their style to that
of a corpus of known authors (Mostellar and Wal-
lace, 1984). Determining instances of ?text reuse,?
a type of paraphrasing, is also a form of analysis
at the lexical level, and it has recently been used to
validate theories about the lineage of ancient texts
(Lee, 2007).
Analysis of literature using more semantically-
oriented techniques has been rare, most likely be-
cause of the difficulty in automatically determin-
ing meaningful interpretations. Some exceptions
include recent work on learning common event se-
quences in news stories (Chambers and Jurafsky,
2008), an approach based on statistical methods,
and the development of an event calculus for char-
acterizing stories written by children (Halpin et al,
2004), a knowledge-based strategy. On the other
hand, literary theorists, linguists and others have
long developed symbolic but non-computational
models for novels. For example, Moretti (2005)
has graphically mapped out texts according to ge-
ography, social connections and other variables.
While researchers have not attempted the auto-
matic construction of social networks represent-
ing connections between characters in a corpus
of novels, the ACE program has involved entity
and relation extraction in unstructured text (Dod-
dington et al, 2004). Other recent work in so-
cial network construction has explored the use of
structured data such as email headers (McCallum
et al, 2007) and U.S. Senate bill cosponsorship
(Cho and Fowler, 2010). In an analysis of discus-
sion forums, Gruzd and Haythornthwaite (2008)
explored the use of message text as well as posting
data to infer who is talking to whom. In this pa-
per, we also explore how to build a network based
on conversational interaction, but we analyze the
reported dialogue found in novels to determine the
links. The kinds of language that is used to signal
such information is quite different in the two me-
dia. In discussion forums, people tend to use ad-
dresses such as ?Hi Tom,? while in novels, a sys-
tem must determine both the speaker of a quota-
tion and then the intended recipient of the dialogue
act. This is a significantly different problem.
3 Hypotheses
It is commonly held that the novel is a literary
form which tries to produce an accurate represen-
tation of the social world. Within literary stud-
ies, the recurring problem is how that represen-
tation is achieved. Theories about the relation
between novelistic form (the workings of plot,
characters, and dialogue, to take the most basic
categories) and changes to real-world social mi-
lieux abound. Many of these theories center on
nineteenth-century European fiction; innovations
in novelistic form during this period, as well as the
rapid social changes brought about by revolution,
industrialization, and transport development, have
traditionally been linked. These theories, however,
have used only a select few representative novels
as proof. By using statistical methods of analy-
sis, it is possible to move beyond this small corpus
of proof texts. We believe these methods are es-
sential to testing the validity of some core theories
about social interaction and their representation in
literary genres like the novel.
Major versions of the theories about the social
worlds of nineteenth-century fiction tend to cen-
ter on characters, in two specific ways: how many
characters novels tend to have, and how those
characters interact with one another. These two
?formal? facts about novels are usually explained
with reference to a novel?s setting. From the influ-
ential work of the Russian critic Mikhail Bakhtin
to the present, a consensus emerged that as nov-
els are increasingly set in urban areas, the num-
ber of characters and the quality of their interac-
tion change to suit the setting. Bakhtin?s term for
this causal relationship was chronotope: the ?in-
trinsic interconnectedness of temporal and spatial
relationships that are artistically expressed in liter-
ature,? in which ?space becomes charged and re-
sponsive to movements of time, plot, and history?
(Bakhtin, 1981, 84). In Bakhtin?s analysis, dif-
ferent spaces have different social and emotional
potentialities, which in turn affect the most basic
aspects of a novel?s aesthetic technique.
After Bakhtin?s invention of the chronotope,
much literary criticism and theory devoted itself
to filling in, or describing, the qualities of spe-
cific chronotopes, particularly those of the village
or rural environment and the city or urban en-
vironment. Following a suggestion of Bakhtin?s
that the population of village or rural fictions is
modeled on the world of the family, made up of
139
Author/Title/Year Persp. Setting Author/Title/Year Persp. Setting
Ainsworth, Jack Sheppard (1839) 3rd urban Gaskell, North and South (1854) 3rd urban
Austen, Emma (1815) 3rd rural Gissing, In the Year of Jubilee (1894) 3rd urban
Austen, Mansfield Park (1814) 3rd rural Gissing, New Grub Street (1891) 3rd urban
Austen, Persuasion (1817) 3rd rural Hardy, Jude the Obscure (1894) 3rd mixed
Austen, Pride and Prejudice (1813) 3rd rural Hardy, The Return of the Native (1878) 3rd rural
Braddon, Lady Audley?s Secret (1862) 3rd mixed Hardy, Tess of the d?Ubervilles (1891) 3rd rural
Braddon, Aurora Floyd (1863) 3rd rural Hughes, Tom Brown?s School Days (1857) 3rd rural
Bronte?, Anne, The Tenant of Wildfell Hall
(1848)
1st rural James, The Portrait of a Lady (1881) 3rd urban
Bronte?, Charlotte, Jane Eyre (1847) 1st rural James, The Ambassadors (1903) 3rd urban
Bronte?, Charlotte, Villette (1853) 1st mixed James, The Wings of the Dove (1902) 3rd urban
Bronte?, Emily, Wuthering Heights (1847) 1st rural Kingsley, Alton Locke (1860) 1st mixed
Bulwer-Lytton, Paul Clifford (1830) 3rd urban Martineau, Deerbrook (1839) 3rd rural
Collins, The Moonstone (1868) 1st urban Meredith, The Egoist (1879) 3rd rural
Collins, The Woman in White (1859) 1st urban Meredith, The Ordeal of Richard Feverel
(1859)
3rd rural
Conan Doyle, The Sign of the Four (1890) 1st urban Mitford, Our Village (1824) 1st rural
Conan Doyle, A Study in Scarlet (1887) 1st urban Reade, Hard Cash (1863) 3rd urban
Dickens, Bleak House (1852) mixed urban Scott, The Bride of Lammermoor (1819) 3rd rural
Dickens, David Copperfield (1849) 1st mixed Scott, The Heart of Mid-Lothian (1818) 3rd rural
Dickens, Little Dorrit (1855) 3rd urban Scott, Waverley (1814) 3rd rural
Dickens, Oliver Twist (1837) 3rd urban Stevenson, The Strange Case of Dr. Jekyll
and Mr. Hyde (1886)
1st urban
Dickens, The Pickwick Papers (1836) 3rd mixed Stoker, Dracula (1897) 1st urban
Disraeli, Sybil, or the Two Nations (1845) 3rd mixed Thackeray, History of Henry Esmond
(1852)
1st urban
Edgeworth, Belinda (1801) 3rd rural Thackeray, History of Pendennis (1848) 1st urban
Edgeworth, Castle Rackrent (1800) 3rd rural Thackeray, Vanity Fair (1847) 3rd urban
Eliot, Adam Bede (1859) 3rd rural Trollope, Barchester Towers (1857) 3rd rural
Eliot, Daniel Deronda (1876) 3rd urban Trollope, Doctor Thorne (1858) 3rd rural
Eliot, Middlemarch (1871) 3rd rural Trollope, Phineas Finn (1867) 3rd urban
Eliot, The Mill on the Floss (1860) 3rd rural Trollope, The Way We Live Now (1874) 3rd urban
Galt, Annals of the Parish (1821) 1st rural Wilde, The Picture of Dorian Gray (1890) 3rd urban
Gaskell, Mary Barton (1848) 3rd urban Wood, East Lynne (1860) 3rd mixed
Table 1: Properties of the nineteenth-century British novels and serials included in our study.
an intimately related set of characters, many crit-
ics analyzed the formal expression of this world
as constituted by a small set of characters who
express themselves conversationally. Raymond
Williams used the term ?knowable communities?
to describe this world, in which face-to-face rela-
tions of a restricted set of characters are the pri-
mary mode of social interaction (Williams, 1975,
166).
By contrast, the urban world, in this traditional
account, is both larger and more complex. To
describe the social-psychological impact of the
city, Franco Moretti argues, protagonists of urban
novels ?change overnight from ?sons? into ?young
men?: their affective ties are no longer vertical
ones (between successive generations), but hor-
izontal, within the same generation. They are
drawn towards those unknown yet congenial faces
seen in gardens, or at the theater; future friends,
or rivals, or both? (Moretti, 1999, 65). The re-
sult is two-fold: more characters, indeed a mass
of characters, and more interactions, although less
actual conversation; as literary critic Terry Eagle-
ton argues, the city is where ?most of our en-
counters consist of seeing rather than speaking,
glimpsing each other as objects rather than con-
versing as fellow subjects? (Eagleton, 2005, 145).
Moretti argues in similar terms. For him, the
difference in number of characters is ?not just a
matter of quantity... it?s a qualitative, morpho-
logical one? (Moretti, 1999, 68). As the number
of characters increases, Moretti argues (following
Bakhtin in his logic), social interactions of differ-
ent kinds and durations multiply, displacing the
family-centered and conversational logic of vil-
lage or rural fictions. ?The narrative system be-
comes complicated, unstable: the city turns into a
gigantic roulette table, where helpers and antago-
nists mix in unpredictable combinations? (Moretti,
1999, 68). This argument about how novelistic
setting produces different forms of social interac-
tion is precisely what our method seeks to evalu-
ate.
Our corpus of 60 novels was selected for its rep-
resentativeness, particularly in the following cate-
gories: authorial (novels from the major canoni-
140
cal authors of the period), historical (novels from
each decade), generic (from the major sub-genres
of nineteenth-century fiction), sociological (set in
rural, urban, and mixed locales), and technical
(narrated in first-person and third-person form).
The novels, as well as important metadata we as-
signed to them (the perspective and setting), are
shown in Table 1. We define urban to mean set
in a metropolitan zone, characterized by multi-
ple forms of labor (not just agricultural). Here,
social relations are largely financial or commer-
cial in character. We conversely define rural to
describe texts that are set in a country or vil-
lage zone, where agriculture is the primary activ-
ity, and where land-owning, non-productive, rent-
collecting gentry are socially predominant. Social
relations here are still modeled on feudalism (rela-
tions of peasant-lord loyalty and family tie) rather
than the commercial cash nexus. We also explored
other properties of the texts, such as literary genre,
but focus on the results found with setting and per-
spective. We obtained electronic encodings of the
texts from Project Gutenberg. All told, these texts
total more than 10 million words.
We assembled this representative corpus in or-
der to test two hypotheses, which are derived from
the aforementioned theories:
1. That there is an inverse correlation between
the amount of dialogue in a novel and the
number of characters in that novel. One ba-
sic, shared assumption of these theorists is
that as the network of characters expands?
as, in Moretti?s words, a quantitative change
becomes qualitative? the importance, and in
fact amount, of dialogue decreases. With
a method for extracting conversation from a
large corpus of texts, it is possible to test this
hypothesis against a wide range of data.
2. That a significant difference in the
nineteenth-century novel?s representation of
social interaction is geographical: novels set
in urban environments depict a complex but
loose social network, in which numerous
characters share little conversational interac-
tion, while novels set in rural environments
inhabit more tightly bound social networks,
with fewer characters sharing much more
conversational interaction. This hypothesis
is based on the contrast between Williams?s
rural ?knowable communities? and the
sprawling, populous, less conversational
urban fictions or Moretti?s and Eagleton?s
analyses. If true, it would suggest that the
inverse relationship of hypothesis #1 (more
characters means less conversation) can be
correlated to, and perhaps even caused by,
the geography of a novel?s setting. The
claims about novelistic geography and social
interaction have usually been based on
comparisons of a selected few novelists (Jane
Austen and Charles Dickens preeminently).
Do they remain valid when tested against a
larger corpus?
4 Extracting Conversational Networks
from Literature
In order to test these hypotheses, we developed
a novel approach to extracting social networks
from literary texts themselves, building on exist-
ing analysis tools. We defined ?social network?
as ?conversational network? for purposes of eval-
uating these literary theories. In a conversational
network, vertices represent characters (assumed to
be named entities) and edges indicate at least one
instance of dialogue interaction between two char-
acters over the course of the novel. The weight of
each edge is proportional to the amount of inter-
action. We define a conversation as a continuous
span of narrative time featuring a set of characters
in which the following conditions are met:
1. The characters are in the same place at the
same time;
2. The characters take turns speaking; and
3. The characters are mutually aware of each
other and each character?s speech is mutually
intended for the other to hear.
In the following subsections, we discuss the
methods we devised for the three problems in text
processing invoked by this approach: identifying
the characters present in a literary text, assigning
a ?speaker? (if any) to each instance of quoted
speech from among those characters, and con-
structing a social network by detecting conversa-
tions from the set of dialogue acts.
4.1 Character Identification
The first challenge was to identify the candi-
date speakers by ?chunking? names (such as Mr.
Holmes) from the text. We processed each novel
141
with the Stanford NER tagger (Finkel et al, 2005)
and extracted noun phrases that were categorized
as persons or organizations. We then clustered the
noun phrases into coreferents for the same entity
(person or organization). The clustering process is
as follows:
1. For each named entity, we generate varia-
tions on the name that we would expect to
see in a coreferent. Each variation omits cer-
tain parts of multi-word names, respecting ti-
tles and first/last name distinctions, similar to
work by Davis et al (2003). For example,
Mr. Sherlock Holmes may refer to the same
character as Mr. Holmes, Sherlock Holmes,
Sherlock and Holmes.
2. For each named entity, we compile a list of
other named entities that may be coreferents,
either because they are identical or because
one is an expected variation on the other.
3. We then match each named entity to the most
recent of its possible coreferents. In aggre-
gate, this creates a cluster of mentions for
each character.
We also pre-processed the texts to normalize
formatting, detect headings and chapter breaks, re-
move metadata, and identify likely instances of
quoted speech (that is, mark up spans of text that
fall between quotation marks, assumed to be a su-
perset of the quoted speech present in the text).
4.2 Quoted Speech Attribution
In order to programmatically assign a speaker to
each instance of quoted speech, we applied a high-
precision subset of a general approach we describe
elsewhere (Elson and McKeown, 2010). The first
step of this approach was to compile a separate
training and testing corpus of literary texts from
British, American and Russian authors of the nine-
teenth and twentieth centuries. The training cor-
pus consisted of about 111,000 words including
3,176 instances of quoted speech. To obtain gold-
standard annotations, we conducted an online sur-
vey via Amazon?s Mechanical Turk program. For
each quote, we asked three annotators to indepen-
dently choose a speaker from the list of contex-
tual candidates? or, choose ?spoken by an unlisted
character? if the answer was not available, or ?not
spoken by any character? for non-dialogue cases
such as sneer quotes.
We divided this corpus into training and testing
sets, and used the training set to develop a catego-
rizer that assigned one of five syntactic categories
to each quote. For example, if a quote is followed
by a verb that indicates verbal expression (such as
?said?), and then a character mention, a category
called Character trigram is assigned to the quote.
The fifth category is a catch-all for quotes that do
not fall into the other four. In many cases, the an-
swer can be reliably determined based solely on
its syntactic category. For instance, in the Char-
acter trigram category, the mentioned character is
the quote?s speaker in 99% of both the training and
testing sets.
In all, we were able to determine the speaker
of 57% of the testing set with 96% accuracy just
on the basis of syntactic categorization. This is
the technique we used to construct our conversa-
tional networks. In another study, we applied ma-
chine learning tools to the data (one model for
each syntactic category) and achieved an overall
accuracy of 83% over the entire test set (Elson
and McKeown, 2010). The other 43% of quotes
are left here as ?unknown? speakers; however, in
the present study, we are interested in conversa-
tions rather than individual quotes. Each conversa-
tion is likely to consist of multiple quotes by each
speaker, increasing the chances of detecting the in-
teraction. Moreover, this design decision empha-
sizes the precision of the social networks over their
recall. This tilts ?in favor? of hypothesis #1 (that
there are fewer social interactions in larger com-
munities); however, we shall see that despite the
emphasis of precision over recall, we identify a
sufficient mass of interactions in the texts to con-
stitute evidence against this hypothesis.
4.3 Constructing social networks
We then applied the results from our character
identification and quoted speech attribution meth-
ods toward the construction of conversational net-
works from literature. We derived one network
from each text in our corpus.
We first assigned vertices to character enti-
ties that are mentioned repeatedly throughout the
novel. Coreferents for the same name (such as
Mr. Darcy and Darcy) were grouped into the same
vertex. We found that a network that included in-
cidental or single-mention named entities became
too noisy to function effectively, so we filtered out
the entities that are mentioned fewer than three
142
times in the novel or are responsible for less than
1% of the named entity mentions in the novel.
We assigned undirected edges between vertices
that represent adjacency in quoted speech frag-
ments. Specifically, we set the weight of each
undirected edge between two character vertices to
the total length, in words, of all quotes that either
character speaks from among all pairs of adjacent
quotes in which they both speak? implying face to
face conversation. We empirically determined that
the most accurate definition of ?adjacency? is one
where the two characters? quotes fall within 300
words of one another with no attributed quotes in
between. When such an adjacency is found, the
length of the quote is added to the edge weight,
under the hypothesis that the significance of the re-
lationship between two individuals is proportional
to the length of the dialogue that they exchange.
Finally, we normalized each edge?s weight by the
length of the novel.
An example network, automatically constructed
in this manner from Jane Austen?s Mansfield Park,
is shown in Figure 1. The width of each vertex is
drawn to be proportional to the character?s share
of all the named entity mentions in the book (so
that protagonists, who are mentioned frequently,
appear in larger ovals). The width of each edge is
drawn to be proportional to its weight (total con-
versation length).
We also experimented with two alternate meth-
ods for identifying edges, for purposes of a base-
line:
1. The ?correlation? method divides the text
into 10-paragraph segments and counts the
number of mentions of each character in
each segment (excluding mentions inside
quoted speech). It then computes the Pear-
son product-moment correlation coefficient
for the distributions of mentions for each pair
of characters. These coefficients are used for
the edge weights. Characters that tend to ap-
pear together in the same areas of the novel
are taken to be more socially connected, and
have a higher edge weight.
2. The ?spoken mention? method counts occur-
rences when one character refers to another
in his or her quoted speech. These counts,
normalized by the length of the text, are used
as edge weights. The intuition is that charac-
ters who refer to one another are likely to be
in conversation.
??????????????????????????????????????????????????
?????????
??????????
?????
??????
???????????????????
?????????????
???
?????????
??????????????????
?????
??????????
???????????
???????
Figure 1: Automatically extracted conversation
network for Jane Austen?s Mansfield Park.
4.4 Evaluation
To check the accuracy of our method for extracting
conversational networks, we conducted an evalua-
tion involving four of the novels (The Sign of the
Four, Emma, David Copperfield and The Portrait
of a Lady). We did not use these texts when devel-
oping our method for identifying conversations.
For each book, we randomly selected 4-5 chap-
ters from among those with significant amounts
of quoted speech, so that all excerpts from each
novel amounted to at least 10,000 words. We then
asked three annotators to identity all the conversa-
tions that occur in all 44,000 words. We requested
that the annotators include both direct and indi-
rect (unquoted) speech, and define ?conversation?
as in the beginning of Section 4, but exclude ?re-
told? conversations (those that occur within other
dialogue).
We processed the annotation results by breaking
down each multi-way conversation into all of its
unique two-character interactions (for example, a
conversation between four people indicates six bi-
lateral interactions). To calculate inter-annotator
agreement, we first compiled a list of all possi-
ble interactions between all characters in each text.
In this model, each annotator contributed a set of
?yes? or ?no? decisions, one for every character
pair. We then applied the kappa measurement for
agreement in a binary classification problem (Co-
143
Method Precision Recall F
Speech adjacency .95 .51 .67
Correlation .21 .65 .31
Spoken-mention .45 .49 .47
Table 2: Precision, recall, and F-measure of three
methods for detecting bilateral conversations in
literary texts.
hen, 1960). In 95% of character pairs, annota-
tors were unanimous, which is a high agreement
of k = .82.
The precision and recall of our method for de-
tecting conversations is shown in Table 2. Preci-
sion was .95; this indicates that we can be con-
fident in the specificity of the conversational net-
works that we automatically construct. Recall was
.51, indicating a sensitivity of slightly more than
half. There were several reasons that we did not
detect the missing links, including indirect speech,
quotes attributed to anaphoras or coreferents, and
?diffuse? conversations in which the characters do
not speak in turn with one another.
To calculate precision and recall for the two
baseline social networks, we set a threshold t to
derive a binary prediction from the continuous
edge weights. The precision and recall values
shown for the baselines in Table 2 represent the
highest performance we achieved by varying t be-
tween 0 and 1 (maximizing F-measure over t).
Both baselines performed significantly worse in
precision and F-measure than our quoted speech
adjacency method for detecting conversations.
5 Data Analysis
5.1 Feature extraction
We extracted features from the conversational net-
works that emphasize the complexity of the social
interactions found in each novel:
1. The number of characters and the number of
speaking characters
2. The variance of the distribution of quoted
speech (specifically, the proportion of quotes
spoken by the n most frequent speakers, for
1 ? n ? 5)
3. The number of quotes, and proportion of
words in the novel that are quoted speech
4. The number of 3-cliques and 4-cliques in the
social network
5. The average degree of the graph, defined as
?
v?V |Ev|
|V |
=
2|E|
|V |
(1)
where |Ev| is the number of edges incident
on a vertex v, and |V | is the number of ver-
tices. In other words, this determines the
average number of characters connected to
each character in the conversational network
(?with how many people on average does a
character converse??).
6. A variation on graph density that normalizes
the average degree feature by the number of
characters:
?
v?V |Ev|
|V |(|V | ? 1)
=
2|E|
|V |(|V | ? 1)
(2)
By dividing again by |V | ? 1, we use this
as a metric for the overall connectedness of
the graph: ?with what percent of the entire
network (besides herself) does each charac-
ter converse, on average?? The weight of the
edge, as long as it is greater than 0, does not
affect either the network?s average degree or
graph density.
5.2 Results
We derived results from the data in two ways.
First, we examined the strengths of the correla-
tions between the features that we extracted (for
example, between number of character vertices
and the average degree of each vertex). We used
Pearson?s product-moment correlation coefficient
in these calculations. Second, we compared the
extracted features to the metadata we previously
assigned to each text (e.g., urban vs. rural).
Hypothesis #1, which we described in Section
3, claims that there is an inverse correlation be-
tween the amount of dialogue in a nineteenth-
century novel and the number of characters in that
novel. We did not find this to be the case. Rather,
we found a weak but positive correlation (r=.16)
between the number of quotes in a novel and
the number of characters (normalizing the quote
count for text length). There was a stronger pos-
itive correlation (r=.50) between the number of
unique speakers (those characters who speak at
least once) and the normalized number of quotes,
suggesting that larger networks have more conver-
sations than smaller ones. But because the first
144
correlation is weak, we investigated whether fur-
ther analysis could identify other evidence that
confirms or contradicts the hypothesis.
Another way to interpret hypothesis #1 is that
social networks with more characters tend to break
apart and be less connected. However, we found
the opposite to be true. The correlation between
the number of characters in each graph and the av-
erage degree (number of conversation partners) for
each character was a positive, moderately strong
r=.42. This is not a given; a network can easily, for
example, break into minimally connected or mutu-
ally exclusive subnetworks when more characters
are involved. Instead, we found that networks tend
to stay close-knit regardless of their size: even the
density of the graph (the percentage of the com-
munity that each character talks to) grows with
the total population size at r=.30. Moreover, as
the population of speakers grows, the density is
likely to increase at r=.49. A higher number of
characters (speaking or non-speaking) is also cor-
related with a higher rate of 3-cliques per charac-
ter (r=.38), as well as with a more balanced dis-
tribution of dialogue (the share of dialogue spo-
ken by the top three speakers decreases at r=?.61).
This evidence suggests that in nineteenth-century
British literature, it is the small communities,
rather than the large ones, that tend to be discon-
nected.
Hypothesis #2, meanwhile, posited that a
novel?s setting (urban or rural) would have an ef-
fect on the structure of its social network. After
defining ?social network? as a conversational net-
work, we did not find this to be the case. Sur-
prisingly, the numbers of characters and speakers
found in the urban novel were not significantly
greater than those found in the rural novel. More-
over, each of the features we extracted, such as
the rate of cliques, average degree, density, and
rate of characters? mentions of other characters,
did not change in a statistically significant man-
ner between the two genres. For example, Figure
2 shows the mean over all texts of each network?s
average degree, with confidence intervals, sepa-
rated by setting into urban and rural. The increase
in degree seen in urban texts is not significant.
Rather, the only type of metadata variable that
did impact the average degree with any signifi-
cance was the text?s perspective. Figure 2 also sep-
arates texts into first- and third-person tellings and
shows the means and confidence intervals for the
 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
 2 2.2
3rd1sturbanruralAverage De
gree
Setting  /  Perspective
Figure 2: The average degree for each character
as a function of the novel?s setting and its perspec-
tive.
???????????????????????????????????????????????????
?
??????????? ????????????? ?????????????? ????????????
?????
?????????????
????????????
???? ?????????????
Figure 3: Conversational networks for first-person
novels like Collins?s The Woman in White are less
connected due to the structure imposed by the per-
spective.
average degree measure. Stories told in the third
person had much more connected networks than
stories told in the first person: not only did the av-
erage degree increase with statistical significance
(by the homoscedastic t-test to p < .005), so too
did the graph density (p < .05) and the rate of
3-cliques per character (p < .05).
We believe the reason for this can be intuited
with a visual inspection of a first-person graph.
Figure 3 shows the conversational network ex-
tracted for Collins?s The Woman in White, which is
told in the first person. Not surprisingly, the most
oft-repeated named entity in the text is I, referring
to the narrator. More surprising is the lack of con-
versation connections between the auxiliary char-
acters. The story?s structure revolves around the
narrator and each character is understood in terms
of his or her relationship to the narrator. Private
conversations between auxiliary characters would
not include the narrator, and thus do not appear in a
145
first-hand account. An ?omniscient? third person
narrator, by contrast, can eavesdrop on any pair
of characters conversing. This highlights the im-
portance of detecting reported and indirect speech
in future work, as a first-person narrator may hear
about other connections without witnessing them.
6 Literary Interpretation of Results
Our data, therefore, markedly do not confirm hy-
pothesis #1. They also suggest, in relation to hy-
pothesis #2 (also not confirmed by the data), a
strong reason why.
One of the basic assumptions behind hypoth-
esis #2? that urban novels contain more charac-
ters, mirroring the masses of nineteenth-century
cities? is not borne out by our data. Our results do,
however, strongly correlate a point of view (third-
person narration) with more frequently connected
characters, implying tighter and more talkative so-
cial networks.
We would propose that this suggests that the
form of a given novel? the standpoint of the nar-
rative voice, whether the voice is ?omniscient? or
not? is far more determinative of the kind of so-
cial network described in the novel than where it
is set or even the number of characters involved.
Whereas standard accounts of nineteenth-century
fiction, following Bakhtin?s notion of the ?chrono-
tope,? emphasize the content of the novel as de-
terminative (where it is set, whether the novel fits
within a genre of ?village? or ?urban? fiction),
we have found that content to be surprisingly ir-
relevant to the shape of social networks within.
Bakhtin?s influential theory, and its detailed re-
workings by Williams, Moretti, and others, sug-
gests that as the novel becomes more urban, more
centered in (and interested in) populous urban set-
tings, the novel?s form changes to accommodate
the looser, more populated, less conversational
networks of city life. Our data suggests the op-
posite: that the ?urban novel? is not as strongly
distinctive a form as has been asserted, and that in
fact it can look much like the village fictions of the
century, as long as the same method of narration is
used.
This conclusion leads to some further consider-
ations. We are suggesting that the important ele-
ment of social networks in nineteenth-century fic-
tion is not where the networks are set, but from
what standpoint they are imagined or narrated.
Narrative voice, that is, trumps setting.
7 Conclusion
In this paper, we presented a method for char-
acterizing a text of literary fiction by extracting
the network of social conversations that occur be-
tween its characters. This allowed us to take a
systematic and wide look at a large corpus of
texts, an approach which complements the nar-
rower and deeper analysis performed by literary
scholars and can provide evidence for or against
some of their claims. In particular, we described
a high-precision method for detecting face-to-face
conversations between two named characters in a
novel, and showed that as the number of charac-
ters in a novel grows, so too do the cohesion, in-
terconnectedness and balance of their social net-
work. In addition, we showed that the form of the
novel (first- or third-person) is a stronger predictor
of these features than the setting (urban or rural).
Our results thus far suggest further review of our
methods, our corpus and our results for more in-
sights into the social networks found in this and
other genres of fiction.
8 Acknowledgments
This material is based on research supported in
part by the U.S. National Science Foundation
(NSF) under IIS-0935360. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the NSF.
References
Mikhail Bakhtin. 1981. Forms of time and of the
chronotope in the novel. In Trans. Michael Holquist
and Caryl Emerson, editors, The Dialogic Imagi-
nation: Four Essays, pages 84?258. University of
Texas Press, Austin.
John Burrows. 2004. Textual analysis. In Susan
Schreibman, Ray Siemens, and John Unsworth, ed-
itors, A Companion to Digital Humanities. Black-
well, Oxford.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In In
Proceedings of the 46th Annual Meeting of the As-
sociation of Com- putational Linguistics (ACL-08),
pages 789?797, Columbus, Ohio.
Wendy K. Tam Cho and James H. Fowler. 2010. Leg-
islative success in a small world: Social network
analysis and the dynamics of congressional legisla-
tion. The Journal of Politics, 72(1):124?135.
146
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Peter T. Davis, David K. Elson, and Judith L. Klavans.
2003. Methods for precise named entity matching
in digital collections. In Proceedings of the Third
ACM/IEEE Joint Conference on Digital Libraries
(JCDL ?03), Houston, Texas.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content ex-
traction (ace) program tasks, data, and evaluation.
In Proceedings of the Fourth International Confer-
ence on Language Resources and Evaluation (LREC
2004), pages 837?840, Lisbon.
Terry Eagleton. 2005. The English Novel: An Intro-
duction. Blackwell, Oxford.
David K. Elson and Kathleen R. McKeown. 2010. Au-
tomatic attribution of quoted speech in literary nar-
rative. In Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence (AAAI 2010),
Atlanta, Georgia.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In Proceedings of the 43nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pages 363?370.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social net-
works from threaded discussions. In International
Network of Social Network Analysis (INSNA) Con-
ference, St. Pete Beach, Florida.
Harry Halpin, Johanna D. Moore, and Judy Robertson.
2004. Automatic analysis of plot for story rewrit-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
?04), Barcelona.
John Lee. 2007. A computational model of text reuse
in ancient literary texts. In In Proceedings of the
45th Annual Meeting of the Association of Com-
putational Linguistics (ACL 2007), pages 472?479,
Prague.
Andrew McCallum, Xuerui Wang, and Andre?s
Corrada-Emmanual. 2007. Topic and role discovery
in social networks with experiments on enron and
academic email. Journal of Artificial Intelligence
Research, 30:249?272.
Franco Moretti. 1999. Atlas of the European Novel,
1800-1900. Verso, London.
Franco Moretti. 2005. Graphs, Maps, Trees: Abstract
Models for a Literary History. Verso, London.
Frederick Mostellar and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
The Federalist Papers. Springer, New York.
Raymond Williams. 1975. The Country and The City.
Oxford University Press, Oxford.
147
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 230?235,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Detecting Retries of Voice Search Queries
Rivka Levitan
Columbia University
?
rlevitan@cs.columbia.edu
David Elson
Google Inc.
elson@google.com
Abstract
When a system fails to correctly recog-
nize a voice search query, the user will fre-
quently retry the query, either by repeat-
ing it exactly or rephrasing it in an attempt
to adapt to the system?s failure. It is de-
sirable to be able to identify queries as
retries both offline, as a valuable quality
signal, and online, as contextual informa-
tion that can aid recognition. We present
a method than can identify retries offline
with 81% accuracy using similarity mea-
sures between two subsequent queries as
well as system and user signals of recogni-
tion accuracy. The retry rate predicted by
this method correlates significantly with a
gold standard measure of accuracy, sug-
gesting that it may be useful as an offline
predictor of accuracy.
1 Introduction
With ever more capable smartphones connecting
users to cloud-based computing, voice has been a
rapidly growing modality for searching for infor-
mation online. Our voice search application con-
nects a speech recognition service with a search
engine, providing users with structured answers to
questions, Web results, voice actions such as set-
ting an alarm, and more. In the multimodal smart-
phone interface, users can press a button to ac-
tivate the microphone, and then speak the query
when prompted by a beep; after receiving results,
the microphone button is available if they wish to
follow up with a subsequent voice query.
Traditionally, the evaluation of speech recogni-
tion systems has been carried by preparing a test
set of annotated utterances and comparing the ac-
curacy of a system?s transcripts of those utterances
?
This work was done while the first author was an intern
at Google Inc.
against the annotations. In particular, we seek to
measure and minimize the word error rate (WER)
of a system, with a WER of zero indicating perfect
transcription. For voice search interfaces such as
the present one, though, query-level metrics like
WER only tell part of the story. When a user is-
sues two queries in a row, she might be seeking the
same information for a second time due to a sys-
tem failure the first time. When this happens, from
an evaluation standpoint it is helpful to break down
why the first query was unsuccessful: it might be
a speech recognition issue (in particular, a mis-
taken transcription), a search quality issue (where
a correct transcript is interpreted incorrectly by the
semantic understanding systems), a user interface
issue, or another factor. As a second voice query
may also be a new query or a follow-up query, as
opposed to a retry of the first query, the detection
of voice search retry pairs in the query steam is
non-trivial.
Correctly identifying a retry situation in the
query stream has two main benefits. The first
involves offline evaluation and monitoring. We
would like to know the rate at which users were
forced to retry their voice queries, as a measure of
quality. The second has a more immediate ben-
efit for individual users: if we can detect in real
time that a new voice search is really a retry of a
previous voice search, we can take immediate cor-
rective action, such as reranking transcription hy-
potheses to avoid making the same mistake twice,
or presenting alternative searches in the user inter-
face to indicate that the system acknowledges it is
having difficulty.
In this paper, we describe a method for the clas-
sification of subsequent voice searches as either
retry pairs of a certain type, or non-retry pairs. We
identify four salient types of retry pairs, describe
a test set and identify the features we extracted to
build an automatic classifier. We then describe the
models we used to build the classifier and their rel-
230
ative performance on the task, and leave the issue
of real-time corrective action to future work.
2 Related Work
Previous work in voice-enabled information re-
trieval has investigated the problem of identifying
voice retries, and some has taken the additional
step of taking corrective action in instances where
the user is thought to be retrying an earlier utter-
ance. Zweig (2009) describes a system switching
approach in which the second utterance is recog-
nized by a separate model, one trained differently
than the primary model. The ?backup? system is
found to be quite effective at recognizing those
utterances missed by the primary system. Retry
cases are identified with joint language modeling
across multiple transcripts, with the intuition that
retry pairs tend to be closely related or exact dupli-
cates. They also propose a joint acoustic model in
which portions of both utterances are averaged for
feature extraction. Zweig et al (2008) similarly
create a joint decoding model under the assump-
tion that a discrete sent of entities (names of busi-
nesses with directory information) underlies both
queries. While we follow this work in our usage of
joint language modeling, our application encom-
passes open domain voice searches and voice ac-
tions (such as placing calls), so we cannot use sim-
plifying domain assumptions.
Other approaches include Cevik, Weng and Lee
(2008), who use dynamic time warping to de-
fine pattern boundaries using spectral features, and
then consider the best matching patterns to be re-
peated. Williams (2008) measures the overlap be-
tween the two utterances? n-best lists (alternate hy-
potheses) and upweights hypotheses that are com-
mon to both attempts; similarly, Orlandi, Culy and
Franco (2003) remove hypotheses that are seman-
tically equivalent to a previously rejected hypoth-
esis. Unlike these approaches, we do not assume a
strong notion of dialog state to maintain per-state
models.
Another consequence of the open-domain na-
ture of our service is that users are conditioned
to interact with the system as they would with a
search engine, e.g., if the results of a search do
not satisfy their information need, they rephrase
queries in order to refine their results. This can
happen even if the first transcript was correct and
the rephrased query can be easily confused for a
retry of a utterance where the recognition failed.
Figure 1: Retry annotation decision tree.
For purposes of latently monitoring the accuracy
of the recognizer from usage logs, this is a signifi-
cant complicating factor.
3 Data and Annotation
Our data consists of pairs of queries sampled from
anonymized session logs. We consider a pair of
voice searches (spoken queries) to be a potential
retry pair if they are consecutive; we assume that
a voice search cannot be a retry of another voice
search if a typed search occurs between them. We
also exclude pairs for which either member has no
recognition result. For the purpose of our analy-
sis, we further restricted our data to query pairs
whose second member had been previously ran-
domly selected for transcription. A set of 8,254
query pairs met these requirements and are consid-
ered potential retry pairs. 1,000 randomly selected
pairs from this set were separated out and anno-
tated by the authors, leaving a test set of 7,254 po-
tential retry pairs. Among the annotated develop-
ment set, 18 inaudible or unintelligible pairs were
discarded, for a final development set of 982 pairs.
The problem as we have formulated it requires
a labeling system that identifies repetitions and
rephrases as retries, while excluding query pairs
that are superficially similar but have different
search intents. Our system includes five labels.
Figure 1 shows the guidelines for annotation that
define each category.
The first distinction is between query pairs with
the same search intent (?Is the user looking for
the same information??) and those with different
search intents. We define search intent as the re-
sponse the user wants and expects from the sys-
tem. If the second query?s search intent is differ-
ent, it is by definition no retry.
The second distinction we make is between
cases where the first query was recognized cor-
231
rectly and those where it was not. Although
a query that was recognized correctly may be
retried?for example, the user may want to be
reminded of information she already received
(other)?we are only interested in cases where the
system is in error.
If the search intent is the same for both queries,
and the system incorrectly recognized the first,
we consider the second query a retry. We dis-
tinguish between cases where the user repeated
the query exactly, repetition, and where the user
rephrased the query in an attempt to adapt to the
system?s failure, rephrase. This category includes
many kinds of rephrasings, such as adding or drop-
ping terms, or replacing them with synonyms.
The rephrased query may be significantly differ-
ent from the original, as in the following example:
Q1. Navigate to chaparral ease. (?Navigate to Chiappar-
elli?s.?)
Q2. Chipper rally?s Little Italy Baltimore. (?Chiappar-
elli?s Little Italy Baltimore.?)
The rephrased query dropped a term (?Navigate
to?) and added another (?Little Italy Baltimore?).
This example illustrates another difficulty of the
data: the unreliability of the automatic speech
recognition (ASR) means that terms that are in
fact identical (?Chiapparelli?s?) may be recog-
nized very differently (?chaparral ease? or ?chip-
per rally?s?). In the next example, the recognition
hypotheses of two identical queries have only a
single word in common:
Q1. I get in the house Google. (?I did it Google?)
Q2. I did it crash cool. (?I did it Google?)
Conversely, recognition hypotheses that are
nearly identical are not necessarily retries. Often,
these are ?serial queries,? a series of queries the
user is making of the same form or on the same
topic, often to test the system.
Q1. How tall is George Clooney?
Q2. How old is George Clooney?
Q1. Weather in New York.
Q2. Weather in Los Angeles.
These complementary problems mean that we
cannot use na??ve text similarity features to identify
retries. Instead, we combine features that model
the first query?s likely accuracy to broader similar-
ity features to form a more nuanced picture of a
likely retry.
The five granular retry labels were collapsed
into binary categories: search retry, other, and no
retry were mapped to NO RETRY; and repetition
and rephrase were mapped to RETRY. The label
(a) Granular labels
(b) Collapsed (binary) labels
Figure 2: Retry label distribution.
distribution of the final dataset is shown in Figure
2.
4 Features
The features we consider can be divided into three
main categories. The first group of features, sim-
ilarity, is intended to measure the similarity be-
tween the two queries, as similar queries are (with
the above caveats) more likely to be retries. We
calculate the edit distance between the two tran-
scripts at the character and word level, as well as
the two most similar phonetic rewrites. We include
both raw and normalized values as features. We
also count the number of unigrams the two tran-
scripts have in common and the length, absolute
and relative, of the longest unigram overlap.
As we have shown in the previous section, sim-
ilarity features alone cannot identify a retry, since
ASR errors and user rephrases can result in recog-
nition hypotheses that are significantly different
from the original query, while a nearly identical
pair of queries can have different search intents.
Our second group of features, correctness, goes
up a level in our labeling decision tree (Figure 1)
and attempts to instead answer the question: ?Was
the first query transcribed incorrectly?? We use
the confidence score assigned by the recognizer to
the first recognition hypothesis as a measure of the
system?s opinion of its own performance. Since
this score, while informative, may be inaccurate,
we also consider signals from the user that might
indicate the accuracy of the hypothesis. A boolean
feature indicates whether the user interacted with
any of the results (structured or unstructured) that
were presented by the system in response to the
first query, which should constitute an implicit ac-
ceptance of the system?s recognition hypothesis.
The length of the interval between the two queries
is another feature, since a query that occurs imme-
diately after another is likely to be a retry. We also
include the difference and ratio of the two queries?
speaking rate, roughly calculated as the number
of vowels divided by the audio duration in sec-
232
onds, since a speaker is likely to hyperarticulate
(speak more loudly and slowly) after being misun-
derstood ((Wade et al, 1992; Oviatt et al, 1996;
Levow, 1998; Bell and Gustafson, 1999; Soltau
and Waibel, 1998)).
The third feature group, recognizability, at-
tempts to model the characteristics of a query that
is likely to be misrecognized (for the first query
of the pair) or is likely to be a retry of a previ-
ous query (for the second query). We look at the
language model (LM) score and the number of al-
ternate pronunciations of the first query, predicting
that a misrecognized query will have a lower LM
score and more alternate pronunciations. In ad-
dition, we look at the number of characters and
unigrams and the audio duration of each query,
with the intuition that the length of a query may
be correlated with its likelihood of being retried
(or a retry). This feature group also includes
two heuristic features intended to flag the ?serial
queries? mentioned before: the number of capital-
ized words in each query, and whether each one
begins with a question word (who, what, etc.).
5 Prediction task
5.1 Experimental Results
A logistic regression model was trained on these
features to predict the collapsed binary categories
of NO RETRY (search retry, other, no retry) vs.
RETRY (rephrase, repetition). The results of run-
ning this model with each combination of the fea-
ture groups are shown in Table 1.
Features Precision Recall F1 Accuracy
Similarity 0.54 0.65 0.59 0.72
Correctness 0.53 0.67 0.59 0.73
Recognizability 0.49 0.63 0.55 0.70
Sim. & Corr. 0.67 0.71 0.69 0.77
Sim. & Rec. 0.62 0.70 0.66 0.76
Corr. & Rec. 0.65 0.71 0.68 0.77
All Features 0.70 0.76 0.73 0.81
Table 1: Results of the binary prediction task.
Individually, each feature group peformed sig-
nificantly better than the baseline strategy of al-
ways predicting NO RETRY (62.4%). Each pair
of feature groups performed better than any indi-
vidual group, and the final combination of all three
feature groups had the highest precision, recall,
and accuracy, suggesting that each aspect of the
retry conceptualization provides valuable informa-
tion to the model.
Of the similarity features, the ones that con-
tributed significantly in the final model were char-
acter edit distance (normalized) and phoneme edit
distance (raw and normalized); as expected, re-
tries are associated with more similar query pairs.
Of the correctness features, high recognizer con-
fidence, the presence of a positive reaction from
the user such as a link click, and a long inter-
val between queries were all negatively associated
with retries. The significant recognizability fea-
tures included length of the first query in charac-
ters (longer queries were less likely to be retried)
and the number of capital letters in each query (as
our LM is case-sensitive): queries transcribed with
more capital letters were more likely to be retried,
but less likely to themselves be retries. In addition,
the language model likelihood for the first query
was, as expected, significantly lower for retries.
Interestingly, the score of the second query was
lower for retries as well. This accords with our
finding that retries of misrecognized queries are
themselves misrecognized 60%-70% of the time,
which highlights the potential value of corrective
action informed by the retry context.
Several features, though not significant in the
model, are significantly different between the
RETRY and NO RETRY categories, which affords
us further insight into the characteristics of a retry.
T -tests between the two categories showed that all
edit distance features?character, word, reduced,
and phonetic; raw and normalized?are signifi-
cantly more similar between retry query pairs.
1
Similarly, the number of unigrams the two queries
have in common is significantly higher for retries.
The duration of each member of the query pair,
in seconds and word count, is significantly more
similar between retry pairs, and each member of a
retry pair tends to be shorter than members of a no
retry pair. Finally, members of NO RETRY query
pairs were significantly more similar in speaking
rate, and the relative speaking rate of the second
query was significantly slower for RETRY pairs,
possibly due to hyperarticulation.
5.2 Analysis
Figure 3 shows a breakdown of the true granular
labels versus the predicted binary labels. The pri-
mary source of error is the REPHRASE category,
which is identified as a retry with only 16.5% ac-
1
T -tests reported here use a conservative significance
threshold of p < 0.00125 to control for family-wise type I
error (?data dredging? effects).
233
Figure 3: Performance on each of the granular categories.
curacy. This result reflects the fact that although
rephrases conceptually belong in the retry cate-
gory, their characteristics are materially different.
Most notably, all edit distance features are signif-
icantly greater for rephrases. Differences in du-
ration between the two queries in a pair, in sec-
onds and words, are significantly greater as well.
Rephrases also are significantly longer, in seconds
and words, than strict retries. The model includ-
ing only correctness and recognizability features
does significantly better on rephrases than the full
model, identifying them as retries with 25.6% ac-
curacy, confirming that the similarity features are
the primary culprit. Future work may address this
issue by including features crafted to examine the
similarity between substrings of the two queries,
rather than the query as a whole, and by expand-
ing the similarity definition to include synonyms.
To test the model?s performance with a larger,
unseen dataset, we looked at how many retries
it detected in the test set of potential retry pairs
(n=7,254). We do not have retry annotations for
this larger set, but we have transcriptions for the
first member of each query pair, enabling us to cal-
culate the word error rate (WER) of each query?s
recognition hypothesis, and thus obtain ground
truth for half of our retry definition. A perfect
model should never predict RETRY when the first
query is transcribed correctly (WER==0). As
shown in Figure 4, our model assigns a RETRY
label to approximately 14% of the queries follow-
ing an incorrectly recognized search, and only 2%
of queries following a correctly recognized search.
While this provides us with only a lower bound on
our model?s error, this significant correlation with
an orthogonal accuracy metric shows that we have
modeled at least this aspect of retries correctly, and
suggests a correlation between retry rate and tradi-
tional WER-based evaluation.
Figure 4: Performance on unseen data. A perfect model
would have a predicted retry rate of 0 when WER==0.
6 Conclusion
We have presented a method for characterizing re-
tries in an unrestricted voice interface to a search
system. One particular challenge is the lack of
simplifying assumptions based on domain and
state (as users may consider the system to be
stateless when issuing subsequent queries). We
introduce a labeling scheme for retries that en-
compasses rephrases?cases in which the user re-
worded her query to adapt to the system?s error?
as well as repetitions.
Our model identifies retries with 81% accuracy,
significantly above baseline. Our error analysis
confirms that user rephrasings complicate the bi-
nary class separation; an approach that models
typical typed rephrasings may help overcome this
difficulty. However, our model?s performance to-
day correlates strongly with an orthogonal accu-
racy metric, word error rate, on unseen data. This
suggests that ?retry rate? is a reasonable offline
quality metric, to be considered in context among
other metrics and traditional evaluation based on
word error rate.
Acknowledgments
The authors thank Daisy Stanton and Maryam
Kamvar for their helpful comments on this project.
References
Linda Bell and Joakim Gustafson. 1999. Repetition
and its phonetic realizations: Investigating a swedish
database of spontaneous computer-directed speech.
In Proceedings of ICPhS, volume 99, pages 1221?
1224.
Mert Cevik, Fuliang Weng, and Chin-Hui Lee. 2008.
Detection of repetitions in spontaneous speech in di-
234
alogue sessions. In Proceedings of the 9th Annual
Conference of the International Speech Communi-
cation Association (INTERSPEECH 2008), pages
471?474, Brisbane, Australia.
Gina-Anne Levow. 1998. Characterizing and recog-
nizing spoken corrections in human-computer di-
alogue. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 736?742. Association for Computational Lin-
guistics.
Marco Orlandi, Christopher Culy, and Horacio Franco.
2003. Using dialog corrections to improve speech
recognition. In Error Handling in Spoken Language
Dialogue Systems. International Speech Communi-
cation Association.
Sharon Oviatt, G-A Levow, Margaret MacEachern,
and Karen Kuhn. 1996. Modeling hyperarticu-
late speech during human-computer error resolu-
tion. In Spoken Language, 1996. ICSLP 96. Pro-
ceedings., Fourth International Conference on, vol-
ume 2, pages 801?804. IEEE.
Hagen Soltau and Alex Waibel. 1998. On the influ-
ence of hyperarticulated speech on recognition per-
formance. In ICSLP. Citeseer.
Elizabeth Wade, Elizabeth Shriberg, and Patti Price.
1992. User behaviors affecting speech recognition.
In ICSLP.
Jason D. Williams. 2008. Exploiting the asr n-
best by tracking multiple dialog state hypotheses.
In Proceedings of the 9th Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH 2008), pages 191?194, Bris-
bane, Australia.
Geoffrey Zweig, Dan Bohus, Xiao Li, and Patrick
Nguyen. 2008. Structured models for joint
decoding of repeated utterances. In Proceed-
ings of the 9th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2008), pages 1157?1160, Brisbane, Aus-
tralia.
Geoffrey Zweig. 2009. New methods for the
analysis of repeated utterances. In Proceed-
ings of the 10th Annual Conference of the Inter-
national Speech Communication Association (IN-
TERSPEECH 2009), pages 2791?2794, Brighton,
United Kingdom.
235
Tense and Aspect Assignment in Narrative Discourse
David K. Elson and Kathleen R. McKeown
Department of Computer Science
Columbia University
{delson,kathy}@cs.columbia.edu
Abstract
We describe a method for assigning English
tense and aspect in a system that realizes sur-
face text for symbolically encoded narratives. Our
testbed is an encoding interface in which proposi-
tions that are attached to a timeline must be real-
ized from several temporal viewpoints. This in-
volves a mapping from a semantic encoding of
time to a set of tense/aspect permutations. The
encoding tool realizes each permutation to give
a readable, precise description of the narrative so
that users can check whether they have correctly
encoded actions and statives in the formal repre-
sentation. Our method selects tenses and aspects
for individual event intervals as well as subinter-
vals (with multiple reference points), quoted and
unquoted speech (which reassign the temporal fo-
cus), and modal events such as conditionals.
1 Introduction
Generation systems that communicate knowledge
about time must select tense and aspect carefully
in their surface realizations. An incorrect assign-
ment can give the erroneous impression that a con-
tinuous action has ended, or that a previous state
is the current reality. In this paper, we consider
English tense and aspect in the generation of nar-
rative discourse, where statives and actions occur
over connected intervals.
We describe two contributions: first, a general
application of theories of tense, aspect and inter-
val logic to a generation context in which we map
temporal relationships to specific tense/aspect se-
lections. Second, we describe an implementation
of this approach in an interactive environment with
a basic sentence planner and realizer. The first re-
sult does not depend on the second.
The purpose of the system is to allow users who
are na??ve to linguistics and knowledge representa-
tion to create semantic encodings of short stories.
To do this, they construct propositions (predicate-
argument structures) through a graphical, menu-
based interface, and assign them to intervals on a
timeline. Figure 1 shows a session in which the
user is encoding a fable of Aesop. The top-right
panel shows the original fable, and the left-hand
panel shows a graphical timeline with buttons for
constructing new propositions at certain intervals.
The left-hand and bottom-right panels contain au-
tomatically generated text of the encoded story, as
the system understands it, from different points of
view. Users rely on these realizations to check that
they have assigned the formal connections cor-
rectly. The tenses and aspects of these sentences
are a key component of this feedback. We describe
the general purpose of the system, its data model,
and the encoding methodology in a separate paper
(Elson and McKeown, 2010).
The paper is organized as follows: After dis-
cussing related work in Section 2, we describe our
method for selecting tense and aspect for single
events in Section 3. Section 4 follows with more
complex cases involving multiple events and shifts
in temporal focus. We then discuss the results.
2 Related Work
There has been intense interest in the interpre-
tation of tense and aspect into a formal under-
standing of the ordering and duration of events.
This work has been in both linguistics (Dowty,
1979; Nerbonne, 1986; Vlach, 1993) and natu-
ral language understanding. Early systems inves-
tigated rule-based approaches to parsing the du-
rations and orderings of events from the tenses
and aspects of their verbs (Hinrichs, 1987; Web-
ber, 1987; Song and Cohen, 1988; Passonneau,
1988). Allen (1984) and Steedman (1995) focus
on distinguishing between achievements (when an
event culminates in a result, such as John builds
a house) and processes (such as walking). More
Figure 1: Screenshot of our story encoding interface.
recent work has centered on markup languages
for complex temporal information (Mani, 2004)
and corpus-based (statistical) models for predict-
ing temporal relationships on unseen text (Mani et
al., 2006; Lapata and Lascarides, 2006).
Our annotation interface requires a fast realizer
that can be easily integrated into an interactive, on-
line encoding tool. We found that developing a
custom realizer as a module to our Java-based sys-
tem was preferable to integrating a large, general
purpose system such as KPML/Nigel (Matthiessen
and Bateman, 1991) or FUF/SURGE (Elhadad
and Robin, 1996). These realizers, along with Re-
alPro (Lavoie and Rambow, 1997), accept tense as
a parameter, but do not calculate it from a semantic
representation of overlapping time intervals such
as ours (though the Nigel grammar can calculate
tense from speech, event, and reference time or-
derings, discussed below). The statistically trained
FERGUS (Chen et al, 2002) contrasts with our
rule-based approach.
Dorr and Gaasterland (1995) and Grote (1998)
focus on generating temporal connectives, such as
before, based on the relative times and durations of
two events; Gagnon and Lapalme (1996) focus on
temporal adverbials (e.g., when to insert a known
time of day for an event). By comparison, we ex-
tend our approach to cover direct/indirect speech
and the subjunctive/conditional forms, which they
do not report implementing. While our work fo-
cuses on English, Yang and Bateman (2009) de-
scribe a recent system for generating Chinese as-
pect expressions based on a time interval represen-
tation, using KPML as their surface realizer.
Several other projects run tangential to our in-
teractive narrative encoding project. Callaway
and Lester?s STORYBOOK (2002) aims to im-
prove fluency and discourse cohesion in realiz-
ing formally encoded narratives; Ligozat and Zock
(1992) allow users to interactively construct sen-
tences in various temporal scenarios through a
graphical interface.
3 Expressing single events
3.1 Temporal knowledge
The propositions that we aim to realize take the
form of a predicate, one or more arguments, zero
or more attached modifiers (either a negation oper-
ator or an adverbial, which is itself a proposition),
and an assignment in time. Each argument is asso-
ciated with a semantic role (such as Agent or Ex-
periencer), and may include nouns (such as char-
acters) or other propositions. In our implemented
system, the set of predicates available to the an-
notator is adapted from the VerbNet (Kingsbury
and Palmer, 2002) and WordNet (Fellbaum, 1998)
linguistic databanks. These provide both durative
actions and statives (Dowty, 1979); we will refer
to both as events as they occur over intervals. For
example, here are an action and a stative:
walk(Mary, store, 2, 6) (1)
hungry(Julia, 1,?) (2)
The latter two arguments in (1) refer to time
states in a totally ordered sequence; Mary starts
walking to the store at state 2 and finishes walking
at state 6. (2) begins at state 1, but is unbounded
(Julia never ceases being hungry). While this pa-
per does not address the use of reference times
(such as equating a state to 6:00 or yesterday), this
is an area of ongoing work.
(1) and (2), depending on the situation, can be
realized in several aspects and tenses. We adapt
and extend Reichenbach?s (1947) famous system
of symbols for distinguishing between simple and
progressive aspect. Reichenbach identifies three
points that define the temporal position of the
event: the event time E, the speech time S, and
a reference time R which may or may not be in-
dicated by a temporal adverbial. The total order-
ing between these times dictates the appropriate
aspect. For example, the simple past John laughed
has the relation E < S. R = E because there is
no separate reference time involved. The past per-
fect John had laughed [by the end of the play] has
the relation E < R < S, in that it describe ?the
past of the past?, with the nearer ?past? being R
(the end of the play). R can be seen as the tempo-
ral focus of the sentence.
As Reichenbach does not address events with
intervals, we redefine E as the transition (E1..E2)
attached to the proposition (for example, (2,6)
for Mary?s walk). This definition deliberately as-
sumes that no event ever occurs over a single ?in-
stant? of time. The perception of an instantaneous
event, when it is needed, is instead created by di-
lating R into an interval large enough to contain
the entire event, as in Dowty (1979).
We also distinguish between two generation
modes: realizing the story as a complete discourse
(narration mode) and describing the content of a
single state or interval (snapshot mode). Our sys-
tem supports both modes differently. In discourse
mode, we realize the story as if all events occur be-
fore the speech time S, which is the style of most
literary fiction. (We shall see that this does not
preclude the use of the future tense.) In snapshot
mode, speech time is concurrent with reference
time so that the same events are realized as though
they are happening ?now.? The system uses this
mode to allow annotators to inspect and edit what
occurs at any point in the story. In Figure 1, for in-
stance, the lion?s watching of the bull is realized as
both a present, continuing event in snapshot mode
(the lion continues to watch the bull) and narrated
as a past, continuing event (the lion was watching
the bull). In both cases, we aim to precisely trans-
late the propositions and their temporal relation-
ships into text, even if the results are not elegant
rhetoric, so that annotators can see how they have
Diagram Relations Perspective
E 1 R E 2 
R < E1 Before
E 1 R 
E 2 R = E1R < E2 Begin
E 1 R 
E 2 E1 < RR < E2 During
E 1 R E 2 
R = E2
R > E1
Finish
E 1 R E 2 
R > E2 After
Table 1: Perspective assignment for viewing an
event from a reference state.
formally encoded the story. In the remainder of
this section, we describe our method for assigning
tenses and aspects to propositions such as these.
3.2 Reference state
In both snapshot and narration modes, we often
need to render the events that occur at some ref-
erence state R. We would like to know, for in-
stance, what is happening now, or what happened
at 6:00 yesterday evening. The tense and aspect
depend on the perspective of the reference state
on the event, which can be bounded or unbounded.
The two-step process for this scenario is to deter-
mine the correct perspective, then pick the tense
and aspect class that best communicates it.
We define the set of possible perspec-
tives to follow Allen (1983), who describes
seven relationships between two intervals: be-
fore/after, meets/met by, overlaps/overlapped by,
starts/started by, during/contains, finishes/finished
by, and equals. Not all of these map to a relation-
ship between a single reference point and an event
interval. Table 1 maps each possible interaction
between E and R to a perspective, for both
bounded and unbounded events, including the
defining relationships for each interaction. A dia-
mond for E1 indicates at or before, i.e., the event
is either anteriorly unbounded (E1 = ??) or
beginning at a state prior to R and E2. Similarly,
a diamond for E2 indicates at or after.
Once the perspective is determined, covering
Reichenbach?s E and R, speech time S is deter-
mined by the generation mode. Following the
guidelines of Reichenbach and Dowty, we then as-
sign a tense for each perspective/speech time per-
Perspective Generation mode English tense System?s construction Example
After Future Speech Past perfect had {PAST PARTICIPLE} She had walked.
Present Speech Present perfect has/have {PAST PARTICIPLE} She has walked.
Past Speech Future perfect will have {PAST PARTICIPLE} She will have walked.
Modal Infinitive to have {PAST PARTICIPLE} To have walked.
Finish Future Speech ?Finished? stopped {PROGRESSIVE} She stopped walking.
Present Speech ?Finishes? stops {PROGRESSIVE} She stops walking.
Past Speech ?Will finish? will stop {PROGRESSIVE} She will stop walking.
Modal Infinitive to stop {PROGRESSIVE} To stop walking.
During Future Speech Past progressive was/were {PROGRESSIVE} She was walking.
Present Speech Present pro-
gressive
am/is/are {PROGRESSIVE} She is walking.
Past Speech Future progres-
sive
will be {PROGRESSIVE} She will be walking.
Modal Infinitive to be {PROGRESSIVE} To be walking.
During-
After
Future Speech Past perfect
progressive
had been {PROGRESSIVE} She had been walking.
Present Speech Present perfect
progressive
has/have been {PROGRESSIVE} She has been walking.
Past Speech Future perfect
progressive
will have been {PROGRESSIVE} She will have been
walking.
Modal Infinitive to has/have been {PROGRESSIVE} To have been walking.
Begin Future Speech ?Began? began {INFINITIVE} She began to walk.
Present Speech ?Begins? begins {INFINITIVE} She begins to walk.
Past Speech ?Will begin? will begin {INFINITIVE} She will begin to walk.
Modal Infinitive to begin {PROGRESSIVE} To begin walking.
Contains Future Speech Simple past {SIMPLE PAST} She walked.
Present Speech Simple present {SIMPLE PRESENT} She walks.
Past speech Simple future will {INFINITIVE} She will walk.
Modal Infinitive {INFINITIVE} To walk.
Before Future Speech ?Posterior? was/were going {INFINITIVE} She was going to walk.
Present Speech Future am/is/are going {INFINITIVE} She is going to walk.
Past Speech Future-of-
future
will be going {INFINITIVE} She will be going to
walk.
Modal Infinitive to be going {INFINITIVE} To be going to walk.
Table 2: Tense/aspect assignment and realizer constructions for describing an action event from a partic-
ular perspective and speech time. ?Progressive? means ?present participle.?
mutation in Table 2. Not all permutations map to
actual English tenses. Narration mode is shown as
Future Speech, in that S is in the future with re-
spect to all events in the timeline. (This is the case
even if E is unbounded, with E2 = ?.) Snap-
shot mode is realized as Present Speech, in that
R = S. The fourth column indicates the syntac-
tic construction with which our system realizes the
permutation. Each is a sequence of tokens that are
either cue words (began, stopped, etc.) or conjuga-
tions of the predicate?s verb. These constructions
emphasize precision over fluency.
As we have noted, theorists have distinguished
between ?statives? that are descriptive (John was
hungry), ?achievement? actions that culminate in
a state change (John built the house), and ?activi-
ties? that are more continuous and divisible (John
read a book for an hour) (Dowty, 1979). Prior
work in temporal connectives has taken advantage
of lexical information to determine the correct sit-
uation and assign aspect appropriately (Moens and
Steedman, 1988; Dorr and Gaasterland, 1995). In
our case, we only distinguish between actions and
statives, based on information from WordNet and
VerbNet. We use a separate table for statives; it is
similar to Table 2, except the constructions replace
verb conjugations with insertions of be, been, be-
ing, was, were, felt, and so on (with the latter ap-
plying to affective states). We do not currently
distinguish between achievements and activities in
selecting tense and aspect, except that the anno-
tator is tasked with ?manually? indicating a new
state when an event culminates in one (e.g., The
house was complete). Recognizing an achieve-
ment action can benefit lexical choice (better to
say John finished building the house than John
stopped) and content selection for the discourse as
a whole (the house?s completion is implied by fin-
ished and does not need to be stated separately).
To continue our running examples, suppose
propositions (1) and (2) were viewed as a snap-
shot from state R = 2. Table 1 indicates Begin
Diagram Relations Perspective
E 2 R 2 E 1 R 1 
E 2 R 2 E 1 R 1 
R1 ? E2 After
E 2 R 2 E 1 R 1 
R1 > E1
E2 > R1
R2 > E2
Finish
E 2 R 2 E 1 R 1 
E 2 R 2 
E 1 R 1 
R1 ? E1
R2 ? E2
Contains
E 2 R 2 
E 1 R 1 
E1 < R1
E2 > R2
During
E 2 R 2 
E 1 R 1 
R1 < E1
R2 > E1
E2 > R2
Begin
E 2 R 2 
E 1 R 1 
E 2 R 2 
E 1 R 1 
E1 ? R2 Before
Table 3: Perspective assignment for describing an
event from an assigned perspective.
to be the perspective for (1), since E1 = R, and
Table 2 calls for a ?new? tense/aspect permutation
that means ?begins at the present time.? When the
appropriate construction is inserted into the over-
all syntax for walk(Agent, Destination), which we
derive from the VerbNet frame for walk, the result
is Mary begins to walk to the store; similarly, (2) is
realized as Julia is hungry via the During perspec-
tive. Narration mode invokes past-tense verbs.
3.3 Reference interval
Just as events occur over intervals, rather than sin-
gle points, so too can reference times. One may
need to express what occurred when ?Julia entered
the room? (a non-instantaneous action) or ?yes-
terday evening.? Our system allows annotators to
view intervals in snapshot mode to get a sense of
what happens over a certain time span.
The semantics of reference intervals have been
studied as extensions to Reichenbach?s point ap-
proach. Dowty (1979, p.152), for example, posits
that the progressive fits only if the reference in-
terval is completely contained within the event in-
terval. Following this, we construct an alternate
lookup table (Table 3) for assigning the perspec-
Diagram Relations Perspective
E 2 R 2 
E2 > R2
E1 = ??
R1 = ??
During (a priori)
E 2 R 2 
R2 > E2
E1 = ??
R1 = ??
After
E 1 R 1 
R1 > E1
E2 =?
R2 =?
Contains
E 1 R 1 
E1 > R1
E2 =?
R2 =?
Before
Table 4: Perspective assignment if event and ref-
erence intervals are unbounded in like directions.
tive of an event from a reference interval. Table
2 then applies in the same manner. In snapshot
mode, the speech time S also occurs over an inter-
val (namely, R), and Present Speech is still used.
In narration mode, S is assumed to be a point fol-
lowing all event and reference intervals. In our
running example, narrating the interval (1,7) re-
sults in Mary walked to the store and Julia began
to be hungry, using the Contains and Begin per-
spectives respectively.
The notion of an unbounded reference interval,
while unusual, corresponds to a typical perspec-
tive if the event is either bounded or unbounded
in the opposite direction. These scenarios are il-
lustrated in Table 3. Less intuitive are the cases
where event and reference intervals are unbounded
in the same direction. Perspective assignments for
these instances are described in Table 4 and em-
phasize the bounded end of R. These situations
occur rarely in this generation context.
3.4 Event Subintervals
We do not always want to refer to events in their
entirety. We may instead wish to refer to the be-
ginning, middle or end of an event, no matter when
it occurs with respect to the reference time. This
invokes a second reference point in the same inter-
val (Comrie, 1985, p.128), delimiting a subinter-
val. Consider John searches for his glasses versus
John continues to search for his glasses? both in-
dicate an ongoing process, but the latter implies a
subinterval during which time, we are expected to
know, John was already looking for his glasses.
Our handling of subintervals falls along four
alternatives that depend on the interval E1..E2,
the reference R and the subinterval E?1..E
?
2 of E,
where E?1 ? E1 and E
?
2 ? E2.
1. During-After. If E? is not a final subinter-
val of E (E?2 < E2), and R = E
?
2 or R is a
subinterval ofE that is met byE? (R1 = E?2),
the perspective of E? is defined as During-
After. In Table 2, this invokes the perfect-
progressive tense. For example, viewing ex-
ample (1) with E? = (2, 4) from R = 4 in
narration mode (Future Speech) would yield
Mary had been walking to the store.
2. Start. Otherwise, if E? is an initial subin-
terval of E (E?1 = E1 and E
?
2 < E2), the
perspective is defined as Start. These rows
are omitted from Table 2 for space reasons,
but the construction for this case reassigns the
perspective to that between R and E?. Our
realizer reassigns the verb predicate to begin
(or become for statives) with a plan to render
its only argument, the original proposition, in
the infinitive tense. For example, narrating
(2) with E? =(1,2) from R = 3 would yield
Julia had become hungry.
3. Continue. Otherwise, and similarly, if E
strictly contains E? (E?1 > E1 and E
?
2 < E2),
we assign the perspective Continue. To real-
ize this, we reassign the perspective to that
between R and E?, and reassign the verb
predicate to continue (or was still for statives)
with a plan to render its only argument, the
original proposition, in the infinitive.
4. End. Otherwise, if E? is a final subinterval
of E (E?1 > E1 and E
?
2 = E2), we assign the
perspective End. To realize this, we reassign
the perspective to that betweenR andE?, and
reassign the verb predicate to stop (or finish
for cumulative achievements). Similarly, the
predicate?s argument is the original proposi-
tion rendered in the infinitive.
4 Alternate timelines and modalities
This section covers more complex situations in-
volving alternate timelines? the feature of our rep-
resentation by which a proposition in the main
timeline can refer to a second frame of time. Other
models of time have supported similar encapsula-
tions (Crouch and Pulman, 1993; Mani and Puste-
jovsky, 2004). The alternate timeline can contain
references to actual events or modal events (imag-
ined, obligated, desired, planned, etc.) in the past
the future with respect to its point of attachment on
E speech 
R? 
R 
 E hunger 
E? buy 
E? hunger 
reality 
alternate 
S 
Figure 2: Schematic of a speech act attaching to
a alternate timeline with a hypothetical action. R?
and Espeech are attachment points.
the main timeline. This is primarily used in prac-
tice for modeling dialogue acts, but it can also be
used to place real events at uncertain time states
in the past (e.g., the present perfect is used in a
reference story being encoded).
4.1 Reassigning Temporal Focus
Ogihara (1995) describes dialogue acts involving
changes in temporal focus as ?double-access sen-
tences.? We now consider a method for planning
such sentences in such a way that the refocusing
of time (the reassignment of R into a new con-
text) is clear, even if it means changing tense and
aspect mid-sentence. Suppose Mary were to de-
clare that she would buy some eggs because of
Julia?s hunger, but before she returned from the
store, Julia filled up on snacks. If this speech act
is described by a character later in the story, then
we need to carefully separate what is known to
Mary at the time of her speech from what is later
known at R by the teller of the episode. Mary
sees her purchase of eggs as a possible future, even
though it may have already happened by the point
of retelling, and Mary does not know that Julia?s
hunger is to end before long.
Following Hornstein?s treatment of these sce-
narios (Hornstein, 1990), we attach R?, the ref-
erence time for Mary?s statement (in an alternate
timeline), to Espeech, the event of her speaking (in
the main timeline). The act of buying eggs is a
hypothetical event E?buy that falls after R
? on the
alternate (modal) timeline. S is not reassigned.
Figure 2 shows both timelines for this example.
The main timeline is shown on top; Mary?s speech
act is below. The attachment point on the main
timeline is, in this case, the speech event Espeech;
the attachment point on an alternate timeline is al-
ways R?. The placement of R, the main refer-
ence point, is not affected by the alternate time-
line. Real events, such as Julia?s hunger, can be
invoked in the alternate timeline (as drawn with a
vertical line from Ehunger to an E?hunger without
anE?2 known toMary) but they must preserve their
order from the main timeline.
The tense assignment for the event intervals in
the alternate timeline then proceeds as normal,
withR? substituting forR. The hypothetical ?buy?
event is seen in Before perspective, but past tense
(Future Speech), giving the ?posterior? (future-of-
a-past) tense. Julia?s hunger is seen as During as
per Table 1. Further, we assert that connectives
such as Because do not alterR (or in this situation,
R?), and that the E?buy is connected to E
?
hunger
with a causality edge. (Annotators can indicate
connectives between events for causality, motiva-
tion and other features of narrative cohesion.)
The result is: Mary had said that she
was going to buy eggs because Julia was hungry.
The subordinate clause following that sees E?buy
in the future, and E?hunger as ongoing rather than
in the past. It is appropriately ambiguous in both
the symbolic and rendered forms whetherE?buy oc-
curs at all, and if so, whether it occurs before, dur-
ing or after R. A discourse planner would have
the responsibility of pointing out Mary?s mistaken
assumption about the duration of Julia?s hunger.
We assign tense and aspect for quoted speech
differently than for unquoted speech. Instead of
holding S fixed, S? is assigned to R? at the attach-
ment point of the alternate timeline (the ?present
time? for the speech act). If future hypothetical
events are present, they invoke the Past Speech
constructions in Table 2 that have not been used
by either narration or snapshot mode. The content
of the quoted speech then operates totally indepen-
dently of the speech action, since both R? and S?
are detached: Mary said/says/was saying, ?I am
going to buy eggs because Julia is hungry.?
The focus of the sentence can be subsequently
reassigned to deeper nested timelines as necessary
(attaching E? to R??, and so on). Although the
above example uses subordinate clauses, we can
use this nesting technique to construct compos-
ite tenses such as those enumerated by Halliday
(1976). To this end, we conjugate the Modal In-
finitive construction in Table 2 for each alternate
timeline. For instance, Halliday?s complex form
?present in past in future in past in future? (as in
will have been going to have been taking) can be
generated with four timelines in a chain that in-
voke, in order and with Past Speech, the After, Be-
fore, After andDuring perspectives. There are four
Rs, all but the main one attached to a previous E.
4.2 Subjunctives and Conditionals
We finally consider tense and aspect in the case of
subjunctive and conditional statements (if-thens),
which can appear in alternate timelines. The re-
lationship between an if clause and a then clause
is not the same as the relationship between two
clauses joined by because or when. The then
clause? or set of clauses? is predicated on the truth
of the if clause. As linguists have noted (Horn-
stein, 1990, p.74), the if clause serves as an adver-
bial modifier, which has the effect of moving for-
ward the reference point to the last of the if event
intervals (provided that the if refers to a hypotheti-
cal future). Consider the sentence: If John were to
fly to Tokyo, he would have booked a hotel. A cor-
rect model would place E?book before E
?
fly on an
alternate timeline, with E?fly as the if. Since were
to fly is a hypothetical future, R? < E?fly. Dur-
ing regeneration, we set R? to E?fly after rendering
If John were to fly to Tokyo, because we begin to
assume that this event transpired. If R? is left un-
changed, it may be erroneously left before E?book:
Then he would be going to book a hotel.
Our encoding interface allows users to mark one
or more events in an alternate timeline as if events.
If at least one event is marked, all if events are ren-
dered in the subjunctive mood, and the remainder
are rendered in the conditional. For the if clauses
that follow R?, S? and R? itself are reassigned to
the interval for each clause in turn. R? and S? then
remain at the latest if interval (if it is after the origi-
nal R?) for purposes of rendering the then clauses.
In our surface realizer, auxiliary words were and
would are combined with theModal Infinitive con-
structions in Table 2 for events during or following
the original attachment point.
As an example, consider an alternate timeline
with two statives whose start and end points are the
same: Julia is hungry and Julia is unhappy. The
former is marked if. Semantically, we are saying
that hungry(Julia)?unhappy(Julia).
If R? were within these intervals, the rendering
would be: If Julia is hungry, then she is unhappy
(Contains/Present Speech for both clauses). If
R? were prior to these intervals, the rendering
would be: If Julia were to be hungry, then
she would be unhappy. This reassigns R? to
Ehungry, using were as a futurative and would
to indicate a conditional. Because R? and S? are
set to Ehungry, the perspective on both clauses
remains Contains/Present Speech. Finally, if both
intervals are before R?, describing Julia?s previous
emotional states, we avoid shifting R? and S?
backward: If Julia had been hungry, then she had
been unhappy (After perspective, Future Speech
for both statives).
The algorithm is the same for event intervals.
Take (1) and a prior event where Mary runs out of
eggs:
runOut(Mary, eggs, 0, 1) (3)
Suppose they are in an alternate timeline with
attachment point 0? and (1) marked if. We be-
gin by realizing Mary?s walk as an if clause: If
Mary were to walk to the store. We reassign R?
to Ewalk, (2,6), which diverts the perception of
(3) from Begins to After: She would have run out
of eggs. Conversely, suppose the conditional re-
lationship were reversed, with (3) as the only if
action. If the attachment point is 3?, we realize (3)
first in the After perspective, as R? does not shift
backward: If Mary had run out of eggs. The re-
mainder is rendered from the During perspective:
She would be walking to the store. Note that in
casual conversation, we might expect a speaker at
R = 3 to use the past simple: If Mary ran out
of eggs, she would be walking to the store. In this
case, the speaker is attaching the alternate timeline
at a reference interval that subsumes (3), invoking
the Contains perspective by casting a net around
the past. We ask our annotators to select the best
attachment point manually; automatically making
this choice is beyond the scope of this paper.
5 Discussion
As we mentioned earlier, we are describing two
separate methods with a modular relationship to
one another. The first is an abstract mapping from
a conceptual representation of time in a narrative,
including interval and modal logic, to a set of 11
perspectives, including the 7 listed in Table 2 and
the 4 introduced in Section 3.4. These 11 are
crossed with three scenarios for speech time to
give a total of 33 tense/aspect permutations. We
also use an infinitive form for each perspective.
One may take these results and map them from
other time representations with similar specifica-
tions.
The second result is a set of syntactic construc-
tions for realizing these permutations in our story
encoding interface. Our focus here, as we have
noted, is not fluency, but a surface-level render-
ing that reflects the relationships (and, at times,
the ambiguities) present in the conceptual encod-
ing. We consider variations in modality, such as
an indicative reading as opposed to a conditional
or subjunctive reading, to be at the level of the re-
alizer and not another class of tenses.
We have run a collection project with our en-
coding interface and can report success in the
tool?s usability (Elson and McKeown, 2009). Two
annotators each encoded 20 fables into the for-
mal representation, with their only exposure to the
semantic encodings being through the reference
text generator (as in Figure 1). Both annotators
became comfortable with the tool after a period
of training; in surveys that they completed after
each task, they gave Likert-scale usability scores
of 4.25 and 4.30 (averaged over 20 tasks, with
5 meaning ?easiest to use?). These scores are
not specific to the generation component, but they
suggest that annotators could derive satisfactory
tenses from their semantic structures. The most
frequently cited deficiency in the model in terms
of time was the inability to assign reference times
to states and intervals (such as the next morning).
6 Conclusion and Future Work
It has always been the goal in surface realization
to generate sentences from a purely semantic rep-
resentation. Our approach to the generation of
tense and aspect from temporal intervals takes us
closer to that goal. We have applied prior work in
linguistics and interval theory and tested our ap-
proach in an interactive narrative encoding tool.
Our method handles reference intervals and event
intervals, bounded and unbounded, and extends
into subintervals, modal events, conditionals, and
direct and indirect speech where the temporal fo-
cus shifts.
In the future, we will investigate extensions
to the current model, including temporal adver-
bials (which explain the relationship between two
events), reference times, habitual events, achieve-
ments, and discourse-level issues such as prevent-
ing ambiguity as to whether adjacent sentences oc-
cur sequentially (Nerbonne, 1986; Vlach, 1993).
7 Acknowledgments
This material is based on research supported in
part by the U.S. National Science Foundation
(NSF) under IIS-0935360. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the NSF.
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
James F. Allen. 1984. Towards a general theory of
action and time. Artificial Intelligence, 23(2):123?
154.
Charles Callaway and James Lester. 2002. Nar-
rative prose generation. Artificial Intelligence,
139(2):213?252.
John Chen, Srinivas Bangalore, Owen Rambow, and
Marilyn Walker. 2002. Towards automatic gen-
eration of natural language generation systems. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING 2002), Taipei,
Taiwan.
Bernard Comrie. 1985. Tense. Cambridge University
Press.
Richard Crouch and Stephen Pulman. 1993. Time and
modality in a natural language interface to a plan-
ning system. Artificial Intelligence, pages 265?304.
Bonnie J. Dorr and Terry Gaasterland. 1995. Select-
ing tense, aspect, and connecting words in language
generation. In Proceedings of the Fourteenth Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-95), Montreal, Canada.
David R. Dowty. 1979. Word Meaning and Montague
Grammar. D. Reidel, Dordrecht.
Michael Elhadad and Jacques Robin. 1996. An
overview of surge: a reusable comprehensive syn-
tactic realization component. In INLG ?96 Demon-
strations and Posters, pages 1?4, Brighton, UK.
Eighth International Natural Language Generation
Workshop.
David K. Elson and Kathleen R. McKeown. 2009. A
tool for deep semantic encoding of narrative texts.
In Proceedings of the ACL-IJCNLP 2009 Software
Demonstrations, pages 9?12, Suntec, Singapore.
David K. Elson and Kathleen R. McKeown. 2010.
Building a bank of semantically encoded narratives.
In Proceedings of the Seventh International Confer-
ence on Language Resources and Evaluation (LREC
2010), Malta.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Michel Gagnon and Guy Lapalme. 1996. From con-
ceptual time to linguistic time. Computational Lin-
guistics, 22(1):91?127.
Brigitte Grote. 1998. Representing temporal discourse
markers for generation purposes. In Proceedings
of the Discourse Relations and Discourse Markers
Workshop, pages 22?28, Montreal, Canada.
M.A.K. Halliday. 1976. The english verbal group. In
G. R. Kress, editor, Halliday: System and Function
in Language. Oxford University Press, London.
Erhard W. Hinrichs. 1987. A compositional semantics
of temporal expressions in english. In Proceedings
of the 25th Annual Conference of the Association for
Computational Linguistics (ACL-87), Stanford, CA.
Norbert Hornstein. 1990. As Time Goes By: Tense and
Universal Grammar. MIT Press, Cambridge, MA.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC-02), Canary Islands, Spain.
Mirella Lapata and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. Journal of Ar-
tificial Intelligence Research, 27:85?117.
Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Pro-
ceedings of the Fifth Conference on Applied Natural
Language Processing, Washington, DC.
Gerard Ligozat and Michael Zock. 1992. How to vi-
sualize time, tense and aspect? In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING ?92), pages 475?482, Nantes,
France.
Inderjeet Mani and James Pustejovsky. 2004. Tem-
poral discourse models for narrative structure. In
Proceedings of the ACL Workshop on Discourse An-
notation, Barcelona, Spain.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of COLING/ACL 2006, pages 753?760, Sydney,
Australia.
Inderjeet Mani. 2004. Recent developments in tempo-
ral information extraction. In Proceedings of the In-
ternational Conference on Recent Advances in Nat-
ural Language Processing (RANLP ?03), pages 45?
60, Borovets, Bulgaria.
Christian M. I. M. Matthiessen and John A. Bateman.
1991. Text generation and systemic-functional lin-
guistics: experiences from English and Japanese.
Frances Pinter Publishers and St. Martin?s Press,
London and New York.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational Lin-
guistics, 14(2):15?28.
John Nerbonne. 1986. Reference time and time in nar-
ration. Linguistics and Philosophy, 9(1):83?95.
Toshiyuki Ogihara. 1995. Double-access sentences
and reference to states. Natural Language Seman-
tics, 3:177?210.
Rebecca Passonneau. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, 14(2):44?60.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
MacMillan, London.
Fei Song and Robin Cohen. 1988. The interpretation
of temporal relations in narrative. In Proceedings of
the Seventh National Conference on Artificial Intel-
ligence (AAAI-88), St. Paul, Minnesota.
Mark Steedman. 1995. Dynamic semantics for tense
and aspect. In The 1995 International Joint Confer-
ence on AI (IJCAI-95), Montreal, Quebec, Canada.
Frank Vlach. 1993. Temporal adverbials, tenses and
the perfect. Linguistics and Philosophy, 16(3):231?
283.
Bonnie Lynn Webber. 1987. The interpretation of
tense in discourse. In Proceedings of the 25th An-
nual Meeting of the Association for Computational
Linguistics (ACL-87), pages 147?154, Stanford, CA.
Guowen Yang and John Bateman. 2009. The chinese
aspect generation based on aspect selection func-
tions. In Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP (ACL-
IJCNLP 2009), Singapore.
