Coling 2008: Companion volume ? Posters and Demonstrations, pages 23?26
Manchester, August 2008
A Scalable MMR Approach to Sentence Scoring
for Multi-Document Update Summarization
Florian Boudin
\
and Marc El-B
`
eze
\
\
Laboratoire Informatique d?Avignon
339 chemin des Meinajaries, BP1228,
84911 Avignon Cedex 9, France.
florian.boudin@univ-avignon.fr
marc.elbeze@univ-avignon.fr
Juan-Manuel Torres-Moreno
\,[
[
?
Ecole Polytechnique de Montr?eal
CP 6079 Succ. Centre Ville H3C 3A7
Montr?eal (Qu?ebec), Canada.
juan-manuel.torres@univ-avignon.fr
Abstract
We present SMMR, a scalable sentence
scoring method for query-oriented up-
date summarization. Sentences are scored
thanks to a criterion combining query rele-
vance and dissimilarity with already read
documents (history). As the amount of
data in history increases, non-redundancy
is prioritized over query-relevance. We
show that SMMR achieves promising re-
sults on the DUC 2007 update corpus.
1 Introduction
Extensive experiments on query-oriented multi-
document summarization have been carried out
over the past few years. Most of the strategies
to produce summaries are based on an extrac-
tion method, which identifies salient textual seg-
ments, most often sentences, in documents. Sen-
tences containing the most salient concepts are se-
lected, ordered and assembled according to their
relevance to produce summaries (also called ex-
tracts) (Mani and Maybury, 1999).
Recently emerged from the Document Under-
standing Conference (DUC) 2007
1
, update sum-
marization attempts to enhance summarization
when more information about knowledge acquired
by the user is available. It asks the following ques-
tion: has the user already read documents on the
topic? In the case of a positive answer, producing
an extract focusing on only new facts is of inter-
est. In this way, an important issue is introduced:
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
Document Understanding Conferences are conducted
since 2000 by the National Institute of Standards and Tech-
nology (NIST), http://www-nlpir.nist.gov
redundancy with previously read documents (his-
tory) has to be removed from the extract.
A natural way to go about update summarization
would be extracting temporal tags (dates, elapsed
times, temporal expressions...) (Mani and Wilson,
2000) or to automatically construct the timeline
from documents (Swan and Allan, 2000). These
temporal marks could be used to focus extracts on
the most recently written facts. However, most re-
cently written facts are not necessarily new facts.
Machine Reading (MR) was used by (Hickl et
al., 2007) to construct knowledge representations
from clusters of documents. Sentences contain-
ing ?new? information (i.e. that could not be in-
ferred by any previously considered document)
are selected to generate summary. However, this
highly efficient approach (best system in DUC
2007 update) requires large linguistic resources.
(Witte et al, 2007) propose a rule-based system
based on fuzzy coreference cluster graphs. Again,
this approach requires to manually write the sen-
tence ranking scheme. Several strategies remain-
ing on post-processing redundancy removal tech-
niques have been suggested. Extracts constructed
from history were used by (Boudin and Torres-
Moreno, 2007) to minimize history?s redundancy.
(Lin et al, 2007) have proposed a modified Max-
imal Marginal Relevance (MMR) (Carbonell and
Goldstein, 1998) re-ranker during sentence selec-
tion, constructing the summary by incrementally
re-ranking sentences.
In this paper, we propose a scalable sentence
scoring method for update summarization derived
from MMR. Motivated by the need for relevant
novelty, candidate sentences are selected accord-
ing to a combined criterion of query relevance and
dissimilarity with previously read sentences. The
rest of the paper is organized as follows. Section 2
23
introduces our proposed sentence scoring method
and Section 3 presents experiments and evaluates
our approach.
2 Method
The underlying idea of our method is that as the
number of sentences in the history increases, the
likelihood to have redundant information within
candidate sentences also increases. We propose
a scalable sentence scoring method derived from
MMR that, as the size of the history increases,
gives more importance to non-redundancy that to
query relevance. We define H to represent the pre-
viously read documents (history), Q to represent
the query and s the candidate sentence. The fol-
lowing subsections formally define the similarity
measures and the scalable MMR scoring method.
2.1 A query-oriented multi-document
summarizer
We have first started by implementing a simple
summarizer for which the task is to produce query-
focused summaries from clusters of documents.
Each document is pre-processed: documents are
segmented into sentences, sentences are filtered
(words which do not carry meaning are removed
such as functional words or common words) and
normalized using a lemmas database (i.e. inflected
forms ?go?, ?goes?, ?went?, ?gone?... are replaced
by ?go?). An N -dimensional term-space ? , where
N is the number of different terms found in the
cluster, is constructed. Sentences are represented
in ? by vectors in which each component is the
term frequency within the sentence. Sentence scor-
ing can be seen as a passage retrieval task in Infor-
mation Retrieval (IR). Each sentence s is scored by
computing a combination of two similarity mea-
sures between the sentence and the query. The first
measure is the well known cosine angle (Salton et
al., 1975) between the sentence and the query vec-
torial representations in ? (denoted respectively ~s
and
~
Q). The second similarity measure is based
on the Jaro-Winkler distance (Winkler, 1999). The
original Jaro-Winkler measure, denoted JW, uses
the number of matching characters and transposi-
tions to compute a similarity score between two
terms, giving more favourable ratings to terms that
match from the beginning. We have extended this
measure to calculate the similarity between the
sentence s and the query Q:
JW
e
(s,Q) =
1
|Q|
?
?
q?Q
max
m?S
?
JW(q,m) (1)
where S
?
is the term set of s in which the terms
m that already have maximized JW(q,m) are re-
moved. The use of JW
e
smooths normalization and
misspelling errors. Each sentence s is scored using
the linear combination:
Sim
1
(s,Q) = ? ? cosine(~s,
~
Q)
+ (1? ?) ? JW
e
(s,Q) (2)
where ? = 0.7, optimally tuned on the past DUCs
data (2005 and 2006). The system produces a list
of ranked sentences from which the summary is
constructed by arranging the high scored sentences
until the desired size is reached.
2.2 A scalable MMR approach
MMR re-ranking algorithm has been successfully
used in query-oriented summarization (Ye et al,
2005). It strives to reduce redundancy while main-
taining query relevance in selected sentences. The
summary is constructed incrementally from a list
of ranked sentences, at each iteration the sentence
which maximizes MMR is chosen:
MMR = argmax
s?S
[ ? ? Sim
1
(s,Q)
? (1? ?) ?max
s
j
?E
Sim
2
(s, s
j
) ] (3)
where S is the set of candidates sentences and E
is the set of selected sentences. ? represents an
interpolation coefficient between sentence?s rele-
vance and non-redundancy. Sim
2
(s, s
j
) is a nor-
malized Longest Common Substring (LCS) mea-
sure between sentences s and s
j
. Detecting sen-
tence rehearsals, LCS is well adapted for redun-
dancy removal.
We propose an interpretation of MMR to tackle
the update summarization issue. Since Sim
1
and
Sim
2
are ranged in [0, 1], they can be seen as prob-
abilities even though they are not. Just as rewriting
(3) as (NR stands for Novelty Relevance):
NR = argmax
s?S
[ ? ? Sim
1
(s,Q)
+ (1? ?) ? (1? max
s
h
?H
Sim
2
(s, s
h
)) ] (4)
We can understand that (4) equates to an OR com-
bination. But as we are looking for a more intu-
itive AND and since the similarities are indepen-
dent, we have to use the product combination. The
24
scoring method defined in (2) is modified into a
double maximization criterion in which the best
ranked sentence will be the most relevant to the
query AND the most different to the sentences in
H .
SMMR(s) = Sim
1
(s,Q)
?
(
1? max
s
h
?H
Sim
2
(s, s
h
)
)
f(H)
(5)
Decreasing ? in (3) with the length of the sum-
mary was suggested by (Murray et al, 2005) and
successfully used in the DUC 2005 by (Hachey
et al, 2005), thereby emphasizing the relevance
at the outset but increasingly prioritizing redun-
dancy removal as the process continues. Sim-
ilarly, we propose to follow this assumption in
SMMR using a function denoted f that as the
amount of data in history increases, prioritize non-
redundancy (f(H)? 0).
3 Experiments
The method described in the previous section has
been implemented and evaluated by using the
DUC 2007 update corpus
2
. The following subsec-
tions present details of the different experiments
we have conducted.
3.1 The DUC 2007 update corpus
We used for our experiments the DUC 2007 up-
date competition data set. The corpus is composed
of 10 topics, with 25 documents per topic. The up-
date task goal was to produce short (?100 words)
multi-document update summaries of newswire ar-
ticles under the assumption that the user has al-
ready read a set of earlier articles. The purpose
of each update summary will be to inform the
reader of new information about a particular topic.
Given a DUC topic and its 3 document clusters: A
(10 documents), B (8 documents) and C (7 doc-
uments), the task is to create from the documents
three brief, fluent summaries that contribute to sat-
isfying the information need expressed in the topic
statement.
1. A summary of documents in cluster A.
2. An update summary of documents in B, un-
der the assumption that the reader has already
read documents in A.
2
More information about the DUC 2007 corpus is avail-
able at http://duc.nist.gov/.
3. An update summary of documents in C, un-
der the assumption that the reader has already
read documents in A and B.
Within a topic, the document clusters must be pro-
cessed in chronological order. Our system gener-
ates a summary for each cluster by arranging the
high ranked sentences until the limit of 100 words
is reached.
3.2 Evaluation
Most existing automated evaluation methods work
by comparing the generated summaries to one or
more reference summaries (ideally, produced by
humans). To evaluate the quality of our generated
summaries, we choose to use the ROUGE
3
(Lin,
2004) evaluation toolkit, that has been found to be
highly correlated with human judgments. ROUGE-
N is a n-gram recall measure calculated between
a candidate summary and a set of reference sum-
maries. In our experiments ROUGE-1, ROUGE-2
and ROUGE-SU4 will be computed.
3.3 Results
Table 1 reports the results obtained on the DUC
2007 update data set for different sentence scor-
ing methods. cosine + JW
e
stands for the scor-
ing method defined in (2) and NR improves it
with sentence re-ranking defined in equation (4).
SMMR is the combined adaptation we have pro-
posed in (5). The function f(H) used in SMMR is
the simple rational function
1
H
, where H increases
with the number of previous clusters (f(H) = 1
for cluster A,
1
2
for cluster B and
1
3
for cluster C).
This function allows to simply test the assumption
that non-redundancy have to be favoured as the
size of history grows. Baseline results are obtained
on summaries generated by taking the leading sen-
tences of the most recent documents of the cluster,
up to 100 words (official baseline of DUC). The
table also lists the three top performing systems at
DUC 2007 and the lowest scored human reference.
As we can see from these results, SMMR out-
performs the other sentence scoring methods. By
ways of comparison our system would have been
ranked second at the DUC 2007 update competi-
tion. Moreover, no post-processing was applied to
the selected sentences leaving an important margin
of progress. Another interesting result is the high
performance of the non-update specific method
(cosine+ JW
e
) that could be due to the small size
3
ROUGE is available at http://haydn.isi.edu/ROUGE/.
25
of the corpus (little redundancy between clusters).
ROUGE-1 ROUGE-2 ROUGE-SU4
Baseline 0.26232 0.04543 0.08247
3
rd
system 0.35715 0.09622 0.13245
2
nd
system 0.36965 0.09851 0.13509
cosine+ JW
e
0.35905 0.10161 0.13701
NR 0.36207 0.10042 0.13781
SMMR 0.36323 0.10223 0.13886
1
st
system 0.37032 0.11189 0.14306
Worst human 0.40497 0.10511 0.14779
Table 1: ROUGE average recall scores computed
on the DUC 2007 update corpus.
4 Discussion and Future Work
In this paper we have described SMMR, a scal-
able sentence scoring method based on MMR that
achieves very promising results. An important as-
pect of our sentence scoring method is that it does
not requires re-ranking nor linguistic knowledge,
which makes it a simple and fast approach to the
issue of update summarization. It was pointed out
at the DUC 2007 workshop that Question Answer-
ing and query-oriented summarization have been
converging on a common task. The value added
by summarization lies in the linguistic quality. Ap-
proaches mixing IR techniques are well suited for
query-oriented summarization but they require in-
tensive work for making the summary fluent and
coherent. Among the others, this is a point that we
think is worthy of further investigation.
Acknowledgments
This work was supported by the Agence Nationale
de la Recherche, France, project RPM2.
References
Boudin, F. and J.M. Torres-Moreno. 2007. A Co-
sine Maximization-Minimization approach for User-
Oriented Multi-Document Update Summarization.
In Recent Advances in Natural Language Processing
(RANLP), pages 81?87.
Carbonell, J. and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 335?336.
ACM Press New York, NY, USA.
Hachey, B., G. Murray, and D. Reitter. 2005. The
Embra System at DUC 2005: Query-oriented Multi-
document Summarization with a Very Large Latent
Semantic Space. In Document Understanding Con-
ference (DUC).
Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC?s
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. In Document Understanding
Conference (DUC).
Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and
S. Ye. 2007. NUS at DUC 2007: Using Evolu-
tionary Models of Text. In Document Understanding
Conference (DUC).
Lin, C.Y. 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Workshop on Text Sum-
marization Branches Out, pages 25?26.
Mani, I. and M.T. Maybury. 1999. Advances in Auto-
matic Text Summarization. MIT Press.
Mani, I. and G. Wilson. 2000. Robust temporal pro-
cessing of news. In 38th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 69?76.
Association for Computational Linguistics Morris-
town, NJ, USA.
Murray, G., S. Renals, and J. Carletta. 2005. Extractive
Summarization of Meeting Recordings. In Ninth Eu-
ropean Conference on Speech Communication and
Technology. ISCA.
Salton, G., A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18(11):613?620.
Swan, R. and J. Allan. 2000. Automatic generation
of overview timelines. In 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 49?56.
Winkler, W. E. 1999. The state of record linkage and
current research problems. In Survey Methods Sec-
tion, pages 73?79.
Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
ing Update Summaries for DUC 2007. In Document
Understanding Conference (DUC).
Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS
at DUC 2005: Understanding documents via con-
cept links. In Document Understanding Conference
(DUC).
26
	
		

	
	Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1943?1947,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Well-argued recommendation:
adaptive models based on words in recommender systems
Julien Gaillard
University of Avignon
Agorantic
Avignon, France
julien.gaillard@univ-avignon.fr
Marc El-Beze
University of Avignon
Agorantic
Avignon, France
marc.elbeze@univ-avignon.fr
Eitan Altman
INRIA Sophia Antipolis
Agorantic
Sophia-Antipolis, France
eitan.altman@inria.fr
Emmanuel Ethis
University of Avignon
Agorantic
Avignon, France
emmanuel.ethis@univ-avignon.fr
Abstract
Recommendation systems (RS) take advan-
tage of products and users information in order
to propose items to consumers. Collaborative,
content-based and a few hybrid RS have been
developed in the past. In contrast, we propose
a new domain-independent semantic RS. By
providing textually well-argued recommenda-
tions, we aim to give more responsibility to the
end user in his decision. The system includes
a new similarity measure keeping up both the
accuracy of rating predictions and coverage.
We propose an innovative way to apply a fast
adaptation scheme at a semantic level, provid-
ing recommendations and arguments in phase
with the very recent past. We have performed
several experiments on films data, providing
textually well-argued recommendations.
1 Introduction
Recommender systems aim at suggesting appropri-
ate items to users from a large catalog of products.
Those systems are individually adapted by using a
specific profile for each user and item, derived from
the analysis of past ratings. The last decade has
shown a historical change in the way we consume
products. People are getting used to receive recom-
mendations. Nevertheless, after a few bad recom-
mendations, users will not be convinced anymore by
the RS. Moreover, if these suggestions come without
explanations, why people should trust it? Numbers
and figures cannot talk to people.
To answer these key issues, we have designed a
new semantic recommender sytem (SRS) including
at least two innovative features:
? Argumentation: each recommendation relies
on and comes along with a textual argumenta-
tion, providing the reasons that led to that rec-
ommendation.
? Fast adaptation: the system is updated in a con-
tinuous way, as each new review is posted.
In doing so, the system will be perceived as less
intrusive thanks to well-chosen words and its fail-
ures will be smoothed over. It is therefore necessary
to design a new generation of RS providing textu-
ally well-argued recommendations. This way, the
end user will have more elements to make a well-
informed choice. Moreover, the system parameters
have to be dynamically and continuously updated,
in order to provide recommendations and arguments
in phase with the very recent past. To do so, we
have adapted the algorithms we described in Gail-
lard (Gaillard et al, 2013), by including a semantic
level, i.e words, terms and phrases as they are natu-
rally expressed in reviews.
This paper is structured as follows. In the next
section, we present the state of the art in recom-
mendation systems and introduce some of the im-
provements we have made. Then, we present our
approach and define the associated methods in sec-
tion 3. We describe the evaluation protocol and how
we have performed some experiments in section 4.
Finally we report results including a comparison to
a baseline in section 5.
2 Related work and choice of a baseline
We present here some methods used in the litera-
ture. Collaborative Filtering (CF) systems use logs
1943
of users, generally user ratings on items (Burke,
2007; Sarwar et al, 1998). In these systems, the
following assumption is made: if user a and user
b rate n items similarly, they will rate other items
in the same way (Deshpande and Karypis., 2004).
This technique has many well-known issues such
as the ?cold start? problem, i.e when new items or
users appear, it is impossible to make a recommen-
dation, due to the absence of rating data (Schein et
al., 2002). Other limitations of RS are sparsity, scal-
ability, overspecialization and domain-dependency
problems.
In Content Based Filtering (CBF) systems, users are
supposed to be independent (Mehta et al, 2008).
Hence for a given user, recommendations rely only
on items he previously rated.
Some RS incorporate semantic knowledge to im-
prove quality. Generally, they apply a concept-
based approach to enhance the user modeling stage
and employ standard vocabularies and ontology re-
sources. For instance, ePaper (scientific-paper rec-
ommender), computes the matching between the
concepts constituting user interests and the concepts
describing an item by using hierarchical relation-
ships of domain concepts (Maidel et al, 2008). Cod-
ina and Ceccaroni (2010) propose to take advantage
of semantics by using an interest-prediction method
based on user ratings and browsing events.
However, none of them are actually based on the
user opinion as it is expressed in natural language.
2.1 Similarity measures
Similarity measures are the keystone of RS (Her-
locker et al, 2005). Resnick (1997) was one of the
first to introduce the Pearson correlation coefficient
to derive a similarity measure between two entities.
Other similarity measures such as Jaccard and Co-
sine have been proposed (Meyer, 2012). Let Su be
the set of items rated by u, Ti the set of users who
have rated item i, ru,i the rating of user u on item i
and rx the mean of x (user or item). PEA(i,j) stands
for the Pearson similarity between items i and j and
is computed as follows:
?
u?Ti?Tj (ru,i ? ri)(ru,j ? rj)
?
?
u?Ti?Tj (ru,i ? ri)2
?
u?Ti?Tj (ru,j ? rj)2
(1)
In the remainder, the Pearson similarity measure will
be used as a baseline. The Manhattan Weighted and
Corrected similarity (MWC), that we introduced in
(Gaillard et al, 2013), will be used as a point of
comparison as well1. Again, for none of them, tex-
tual content is taken into account.
2.2 Rating prediction
Let i be a given item and u a given user. We suppose
the pair (u, i) is unique. Indeed, most of social net-
works do not allow multiple ratings by the same user
for one item. In this framework, two rating predic-
tion methods have to be defined: one user oriented
and the other item oriented. Sim stands for some
similarity function in the following formula.
rating(u, i) =
?
v?Ti Sim(u, v)? rv,i
?
v?Ti |Sim(u, v)|
(2)
A symmetrical formula for items rating(i, u) is de-
rived from and combined with (2).
r?u,i = ??rating(u, i)+(1??)?rating(i, u) (3)
3 Methods
In this section, we describe the methods we have
used and propose some of the enhancements we
have elaborated in our system. In formula (2),
Sim can be replaced by several similarity such as
Pearson, Cosine or MWC similarity (Tan et al,
2005). All these methods provide a measurement of
the likeness between two objects. We then conclude
if two users (or items) are ?alike? or not. One has
to define what ?alike? should mean in this case. If
two users rate the same movies with equals ratings,
then these similarities will be maximal. However,
they may have rated identically but for completely
different reasons, making them not alike at all.
Moreover, none of these similarity measures can
express why two users or items are similar. This is
due to the fact that they rely on ratings only.
3.1 New similarity based on words
We propose a new similarity method, taking into ac-
count words used by users in their past reviews about
items. In the remainder, we call it the Word Based
Similarity (WBS). Each user x (or item) has a vo-
cabulary set Vx and each word w in it is associated
1Details on MWC can be found in supplementary material.
1944
with a set of ratings Rw,x and an average usage rat-
ing rw. In order to balance the contribution of each
word, we define a weight function Fw, mixing the
well-known Inverse Document Frequency IDF (w)
with the variance ?2w. Common words and words w
associated with very heterogenous ratings Rw,x (i.e
a high variance) will have a smaller weight in the
similarity. Nw is the number of items in which the
word w appears. Ntot is the total number of items.
D is the maximum difference between two ratings.
Note that Fw has to be updated at each iteration.
Fw = ?log
(
Nw
Ntot
)
? 1?2w
(4)
WBS(x, y) =
?
w?Vx?Vy(D ? |rw,x ? rw,y|)Fw
D ? |Vx ? Vy|
?
w?Vx?Vy Fw
(5)
3.2 Adaptation
An adaptive framework proposed in (Gaillard et al,
2013) allows the system to have a dynamic adapta-
tion along time, overcoming most of the drawbacks
due to the cold-start. The authors have designed a
dynamic process following the principle that every
update (u, i) needs to be instantly taken into account
by the system. Consequently, we have to update the
?2w and IDF(w) at each iteration, for every word.
Paying attention to avoid a whole re-estimation of
these two variables, we derived an iterative relation
for the two of them2. We thus reduced the complex-
ity by one degree, keeping our system very well-
fitted to dynamic adaptation.
3.3 Textual recommendation
The main innovative feature of our proposal is to
predict what a user is going to write on an item
we recommend. More precisely, we can tell the
user why he is expected to like or dislike the rec-
ommended item. This is possible thanks to the new
similarity measure we have introduced (WBS). Let
us consider a user u and an item i. To keep it sim-
ple, the system takes into account what u has written
on other items in the past and what other users have
written on item i, by using WBS. The idea consists
in extracting what elements of i have been liked or
disliked by other users, and what u generally likes.
2More details can be found in the supplementary material.
At the intersection of these two pieces of informa-
tion, we extract a set of matching words that we
sort by relevance using Fw. Then, by taking into
account the ratings associated with each word, we
define two sub-sets Pw and Nw. Pw contains what
user u is probably going to like in i and Nw what u
may dislike. Finally, we provide the most relevant
arguments contained in both Pw and Nw, and each
of them is given in the context they have been used
for item i. As an example, some outputs are shown
in section 5.2.
4 Evaluation criteria
We present here the evaluation protocol we de-
signed. It should be noted that we are not able
to make online experiments. Therefore, we can
not measure the feedback on our recommendations.
However, the cornerstone of recommender system is
the accuracy of rating predictions (Herlocker et al,
2004). From this point of view, one could argue that
the quality of a recommender engine could be as-
sessed by its capacity to predict ratings. It is thus
possible to evaluate our system comparing the pre-
diction r?u,i for a given pair (u, i), with the actual
real rating ru,i.
The classical metrics3 (Bell et al, 2007) Root Mean
Square Error (RMSE) and Mean Absolute Error
(MAE) will be used to evaluate our RS.
Last but not least, we make the following assump-
tion: if WBS results are as good as MWC?s, the
words presented by the system to users as arguments
are likely to be relevant.
5 Experiments
This work has been carried out in partnership with
the website Vodkaster 4, a Cinema social network.
Researchers have used other datasets such as the fa-
mous Netflix. Unfortunately, the latter does not in-
clude textual reviews. It is therefore strictly impos-
sible to experiment a SRS on such a dataset.
5.1 Corpus
The corpus has been extracted from Vodkaster?s
database. Users post micro-reviews (MR) to ex-
press their opinion on a movie and rate it, within a
3Details on metrics are given in the supplementary material.
4www.vodkaster.com
1945
140 characters Twitter-like length limit. We divided
the corpus into three parts, chronologically sorted:
training (Tr), development (D) and test (T). Note that
in our experiments, the date is taken into account
since we also work on dynamic adaptation.
Tr D Tr+D T
Size 55486 9892 65378 9729
Nb of Films 8414 3184 9130 3877
Nb of Users 1627 675 1855 706
Table 1: Statistics on the corpus
5.2 Results
Figure 1 compares four different methods: the
classical Pearson (PEA) method that does not
allow quick adaptation, the MWC method with and
without quick adaptation MNA and ours (WBS).
Within the confidence interval, in terms of accuracy,
800 1000 1200 1400 1600 1800 2000
0.86
0.88
0.90
0.92
0.94
0.96
Accuracy as a function of Coverage on DEV
Coverage
Acc
urac
y
WBS
MWC
MNA
PEA
Figure 1: Evolution of accuracy as a function of coverage
for PEA, MWC and WBS methods on D corpus.
the same performances are obtained by MWC and
WBS. Both outperform5 PEA and MNA. Our word
based approach is thus able to offer the arguments
5Note that the key point here is the comparison of results ob-
tained with the baseline and with the method we propose. Both
of them have been evaluated with the same protocol: RMSE is
computed with respect to rating predictions above some empir-
ical threshold as done in (Gaillard et al, 2013).
feature without any loss of perfomances with
respect to any others RS methods that we know of.
In Table 2, we set a constant coverage (2000 pre-
dictions) in order to be able to compare results ob-
tained with different methods.
Corp. Met. RMSE MAE %Acc. CI
D PEA 0.99 0.76 86.41 1.49
E MNA 0.93 0.72 90.75 1.26
V MWC 0.89 0.69 92.95 1.12
WBS 0.89 0.70 92.45 1.16
T PEA 1.01 0.78 86.02 1.51
E MNA 0.98 0.75 90.04 1.30
S MWC 0.92 0.71 91.46 1.22
T WBS 0.94 0.72 91.15 1.24
Table 2: Results with Pearson (PEA), MWC, MWCwith-
out Adaptation (MNA), WBS. CI is the radius confidence
interval estimated in % on accuracy (Acc.).
MNA (MWC without adaptation) being better
and more easily updated than Pearson (PEA), we
have decided to use the adaptive framework only for
MWC. Moreover, for Pearson dynamic adaptation,
the updating algorithm complexity is increased by
one degree.
We want to point out that the results are the same for
both MWC and WBS methods, within a confidence
interval (CI) radius of 1.16%. From a qualitative
point of view, these results can be seen as an
assessment of our approach based on words.
Example of outputs: The movie Apocalypse
Now is recommended to user Theo6 with a rating
prediction equal to 4.3. Why he might like: some
brillant moments (0.99), among the major master-
piece (0.91), Vietnam?s hell (0.8); dislike: did not
understand everything but... (0.71).
The data we have does not contain the informa-
tion on the reaction of the user to the recommen-
dation. In particular, we do not know if the textual
argumentation would have been sufficient for con-
vincing Theo6 to see the film. But we know that
after seeing it, he put a good rating (4.5/5) on this
movie.
1946
6 Conclusion and perspectives
We have presented an innovative proposal for de-
signing a domain-independent SRS relying on a
word based similarity function (WBS), providing
textually well-argued recommendations to users.
Moreover, this system has been developed in a dy-
namic and adaptive framework. This might be the
first step really made towards an anthromorphic and
evolutive recommender. As future work, we plan to
evaluate how the quality is impacted by the time di-
mension (adaptation delay, cache reset,etc.).
Acknowledgment
The authors would like to thank Vodkaster for pro-
viding the data.
This work has been partly supported by the Eu-
ropean Commission within the framework of the
CONGAS Project (FP7- ICT-2011-8-317672), see
www.congas-project.eu.
References
R. Bell, Y. Koren and C. Volinsky. 2007. The BellKor
2008 Solution to the Netflix Prize. The Netflix Prize.
R. Burke. 2007. Hybrid Web Recommender Systems.
The Adaptive Web, 377?408.
V. Codina and Luigi Ceccaroni. 2010. Taking Advan-
tage of Semantics in Recommendation Systems. Pro-
ceedings of the 13th International Conference of the
Catalan Association for A.I.,163?172
M. Deshpande and G. Karypis. 2004. Item based top-
N recommendation algorithms. ACM Transactions on
Information and System Security.
J. Gaillard, M. El-Beze, E. Altman and E. Ethis. 2013.
Flash reactivity: adaptive models in recommender
systems. International Conference on Data Mining
(DMIN), WORLDCOMP.
J. Herlocker, J.A Konstan, L. Terveen and J. Riedl. 2004.
Evaluating collaborative filtering recommender sys-
tems. ACM Transactions on Information Systems
(TOIS).
V. Maidel, P. Shoval, B. Shapira, M. Taieb-Maimon.
2008. Evaluation of an ontology-content based filter-
ing method for a personalized newspaper. RecSys?08:
Proceedings, 91?98.
B. Mehta, T. Hofmann, and W. Nejdl. 2008. Robust col-
laborative filtering. In RecSys
F. Meyer. 2012. Recommender systems in industrial con-
texts. PhD thesis, University of Grenoble, France.
P. Resnick and R. Varian Hal. 1997. Recommender sys-
tems (introduction to special section.) Communica-
tions of the ACM
B.M Sarwar, J.A Konstan, A. Borchers,J. Herlocker, B.
Miller, J. Riedl 1998. Using filtering agents to im-
prove prediction quality in the groupLens research
collaborative filtering system. Proceedings of the
ACM Conference on Computer Supported Coopera-
tive Work
A.I Schein, A. Popescul and L.H Ungar. 2002. Methods
and metrics for cold-start recommendations. ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval.
P. Tan, M. Steinbach and V. Kumar. 2005 Introduction
to Data Mining. Addison-Wesley, 500?524.
C. Ziegler, S.M McNee, J.A Konstan and G. Lausen.
2005. Improving recommendation lists through topic
diversification. Fourteenth International World Wide
Web Conference
1947
